<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250602.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent\n  Large Multimodal Models", "author": "Wufei Ma and Luoxin Ye and Nessa McWeeney and Celso M de Melo and Jieneng Chen and Alan Yuille", "abstract": "  Humans naturally understand 3D spatial relationships, enabling complex\nreasoning like predicting collisions of vehicles from different directions.\nCurrent large multimodal models (LMMs), however, lack of this capability of 3D\nspatial reasoning. This limitation stems from the scarcity of 3D training data\nand the bias in current model designs toward 2D data. In this paper, we\nsystematically study the impact of 3D-informed data, architecture, and training\nsetups, introducing SpatialLLM, a large multi-modal model with advanced 3D\nspatial reasoning abilities. To address data limitations, we develop two types\nof 3D-informed training datasets: (1) 3D-informed probing data focused on\nobject's 3D location and orientation, and (2) 3D-informed conversation data for\ncomplex spatial relationships. Notably, we are the first to curate VQA data\nthat incorporate 3D orientation relationships on real images. Furthermore, we\nsystematically integrate these two types of training data with the\narchitectural and training designs of LMMs, providing a roadmap for optimal\ndesign aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM\nadvances machines toward highly capable 3D-informed reasoning, surpassing\nGPT-4o performance by 8.7%. Our systematic empirical design and the resulting\nfindings offer valuable insights for future research in this direction.\n", "link": "http://arxiv.org/abs/2505.00788v2", "date": "2025-06-02", "relevancy": 3.1689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialLLM%3A%20A%20Compound%203D-Informed%20Design%20towards%20Spatially-Intelligent%0A%20%20Large%20Multimodal%20Models&body=Title%3A%20SpatialLLM%3A%20A%20Compound%203D-Informed%20Design%20towards%20Spatially-Intelligent%0A%20%20Large%20Multimodal%20Models%0AAuthor%3A%20Wufei%20Ma%20and%20Luoxin%20Ye%20and%20Nessa%20McWeeney%20and%20Celso%20M%20de%20Melo%20and%20Jieneng%20Chen%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20Humans%20naturally%20understand%203D%20spatial%20relationships%2C%20enabling%20complex%0Areasoning%20like%20predicting%20collisions%20of%20vehicles%20from%20different%20directions.%0ACurrent%20large%20multimodal%20models%20%28LMMs%29%2C%20however%2C%20lack%20of%20this%20capability%20of%203D%0Aspatial%20reasoning.%20This%20limitation%20stems%20from%20the%20scarcity%20of%203D%20training%20data%0Aand%20the%20bias%20in%20current%20model%20designs%20toward%202D%20data.%20In%20this%20paper%2C%20we%0Asystematically%20study%20the%20impact%20of%203D-informed%20data%2C%20architecture%2C%20and%20training%0Asetups%2C%20introducing%20SpatialLLM%2C%20a%20large%20multi-modal%20model%20with%20advanced%203D%0Aspatial%20reasoning%20abilities.%20To%20address%20data%20limitations%2C%20we%20develop%20two%20types%0Aof%203D-informed%20training%20datasets%3A%20%281%29%203D-informed%20probing%20data%20focused%20on%0Aobject%27s%203D%20location%20and%20orientation%2C%20and%20%282%29%203D-informed%20conversation%20data%20for%0Acomplex%20spatial%20relationships.%20Notably%2C%20we%20are%20the%20first%20to%20curate%20VQA%20data%0Athat%20incorporate%203D%20orientation%20relationships%20on%20real%20images.%20Furthermore%2C%20we%0Asystematically%20integrate%20these%20two%20types%20of%20training%20data%20with%20the%0Aarchitectural%20and%20training%20designs%20of%20LMMs%2C%20providing%20a%20roadmap%20for%20optimal%0Adesign%20aimed%20at%20achieving%20superior%203D%20reasoning%20capabilities.%20Our%20SpatialLLM%0Aadvances%20machines%20toward%20highly%20capable%203D-informed%20reasoning%2C%20surpassing%0AGPT-4o%20performance%20by%208.7%25.%20Our%20systematic%20empirical%20design%20and%20the%20resulting%0Afindings%20offer%20valuable%20insights%20for%20future%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialLLM%253A%2520A%2520Compound%25203D-Informed%2520Design%2520towards%2520Spatially-Intelligent%250A%2520%2520Large%2520Multimodal%2520Models%26entry.906535625%3DWufei%2520Ma%2520and%2520Luoxin%2520Ye%2520and%2520Nessa%2520McWeeney%2520and%2520Celso%2520M%2520de%2520Melo%2520and%2520Jieneng%2520Chen%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520Humans%2520naturally%2520understand%25203D%2520spatial%2520relationships%252C%2520enabling%2520complex%250Areasoning%2520like%2520predicting%2520collisions%2520of%2520vehicles%2520from%2520different%2520directions.%250ACurrent%2520large%2520multimodal%2520models%2520%2528LMMs%2529%252C%2520however%252C%2520lack%2520of%2520this%2520capability%2520of%25203D%250Aspatial%2520reasoning.%2520This%2520limitation%2520stems%2520from%2520the%2520scarcity%2520of%25203D%2520training%2520data%250Aand%2520the%2520bias%2520in%2520current%2520model%2520designs%2520toward%25202D%2520data.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520study%2520the%2520impact%2520of%25203D-informed%2520data%252C%2520architecture%252C%2520and%2520training%250Asetups%252C%2520introducing%2520SpatialLLM%252C%2520a%2520large%2520multi-modal%2520model%2520with%2520advanced%25203D%250Aspatial%2520reasoning%2520abilities.%2520To%2520address%2520data%2520limitations%252C%2520we%2520develop%2520two%2520types%250Aof%25203D-informed%2520training%2520datasets%253A%2520%25281%2529%25203D-informed%2520probing%2520data%2520focused%2520on%250Aobject%2527s%25203D%2520location%2520and%2520orientation%252C%2520and%2520%25282%2529%25203D-informed%2520conversation%2520data%2520for%250Acomplex%2520spatial%2520relationships.%2520Notably%252C%2520we%2520are%2520the%2520first%2520to%2520curate%2520VQA%2520data%250Athat%2520incorporate%25203D%2520orientation%2520relationships%2520on%2520real%2520images.%2520Furthermore%252C%2520we%250Asystematically%2520integrate%2520these%2520two%2520types%2520of%2520training%2520data%2520with%2520the%250Aarchitectural%2520and%2520training%2520designs%2520of%2520LMMs%252C%2520providing%2520a%2520roadmap%2520for%2520optimal%250Adesign%2520aimed%2520at%2520achieving%2520superior%25203D%2520reasoning%2520capabilities.%2520Our%2520SpatialLLM%250Aadvances%2520machines%2520toward%2520highly%2520capable%25203D-informed%2520reasoning%252C%2520surpassing%250AGPT-4o%2520performance%2520by%25208.7%2525.%2520Our%2520systematic%2520empirical%2520design%2520and%2520the%2520resulting%250Afindings%2520offer%2520valuable%2520insights%2520for%2520future%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialLLM%3A%20A%20Compound%203D-Informed%20Design%20towards%20Spatially-Intelligent%0A%20%20Large%20Multimodal%20Models&entry.906535625=Wufei%20Ma%20and%20Luoxin%20Ye%20and%20Nessa%20McWeeney%20and%20Celso%20M%20de%20Melo%20and%20Jieneng%20Chen%20and%20Alan%20Yuille&entry.1292438233=%20%20Humans%20naturally%20understand%203D%20spatial%20relationships%2C%20enabling%20complex%0Areasoning%20like%20predicting%20collisions%20of%20vehicles%20from%20different%20directions.%0ACurrent%20large%20multimodal%20models%20%28LMMs%29%2C%20however%2C%20lack%20of%20this%20capability%20of%203D%0Aspatial%20reasoning.%20This%20limitation%20stems%20from%20the%20scarcity%20of%203D%20training%20data%0Aand%20the%20bias%20in%20current%20model%20designs%20toward%202D%20data.%20In%20this%20paper%2C%20we%0Asystematically%20study%20the%20impact%20of%203D-informed%20data%2C%20architecture%2C%20and%20training%0Asetups%2C%20introducing%20SpatialLLM%2C%20a%20large%20multi-modal%20model%20with%20advanced%203D%0Aspatial%20reasoning%20abilities.%20To%20address%20data%20limitations%2C%20we%20develop%20two%20types%0Aof%203D-informed%20training%20datasets%3A%20%281%29%203D-informed%20probing%20data%20focused%20on%0Aobject%27s%203D%20location%20and%20orientation%2C%20and%20%282%29%203D-informed%20conversation%20data%20for%0Acomplex%20spatial%20relationships.%20Notably%2C%20we%20are%20the%20first%20to%20curate%20VQA%20data%0Athat%20incorporate%203D%20orientation%20relationships%20on%20real%20images.%20Furthermore%2C%20we%0Asystematically%20integrate%20these%20two%20types%20of%20training%20data%20with%20the%0Aarchitectural%20and%20training%20designs%20of%20LMMs%2C%20providing%20a%20roadmap%20for%20optimal%0Adesign%20aimed%20at%20achieving%20superior%203D%20reasoning%20capabilities.%20Our%20SpatialLLM%0Aadvances%20machines%20toward%20highly%20capable%203D-informed%20reasoning%2C%20surpassing%0AGPT-4o%20performance%20by%208.7%25.%20Our%20systematic%20empirical%20design%20and%20the%20resulting%0Afindings%20offer%20valuable%20insights%20for%20future%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00788v2&entry.124074799=Read"},
{"title": "RemoteSAM: Towards Segment Anything for Earth Observation", "author": "Liang Yao and Fan Liu and Delong Chen and Chuanyi Zhang and Yijun Wang and Ziyun Chen and Wei Xu and Shimin Di and Yuhui Zheng", "abstract": "  We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.\n", "link": "http://arxiv.org/abs/2505.18022v3", "date": "2025-06-02", "relevancy": 2.9325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation&body=Title%3A%20RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation%0AAuthor%3A%20Liang%20Yao%20and%20Fan%20Liu%20and%20Delong%20Chen%20and%20Chuanyi%20Zhang%20and%20Yijun%20Wang%20and%20Ziyun%20Chen%20and%20Wei%20Xu%20and%20Shimin%20Di%20and%20Yuhui%20Zheng%0AAbstract%3A%20%20%20We%20aim%20to%20develop%20a%20robust%20yet%20flexible%20visual%20foundation%20model%20for%20Earth%0Aobservation.%20It%20should%20possess%20strong%20capabilities%20in%20recognizing%20and%0Alocalizing%20diverse%20visual%20targets%20while%20providing%20compatibility%20with%20various%0Ainput-output%20interfaces%20required%20across%20different%20task%20scenarios.%20Current%0Asystems%20cannot%20meet%20these%20requirements%2C%20as%20they%20typically%20utilize%20task-specific%0Aarchitecture%20trained%20on%20narrow%20data%20domains%20with%20limited%20semantic%20coverage.%20Our%0Astudy%20addresses%20these%20limitations%20from%20two%20aspects%3A%20data%20and%20modeling.%20We%20first%0Aintroduce%20an%20automatic%20data%20engine%20that%20enjoys%20significantly%20better%20scalability%0Acompared%20to%20previous%20human%20annotation%20or%20rule-based%20approaches.%20It%20has%20enabled%0Aus%20to%20create%20the%20largest%20dataset%20of%20its%20kind%20to%20date%2C%20comprising%20270K%0Aimage-text-mask%20triplets%20covering%20an%20unprecedented%20range%20of%20diverse%20semantic%0Acategories%20and%20attribute%20specifications.%20Based%20on%20this%20data%20foundation%2C%20we%0Afurther%20propose%20a%20task%20unification%20paradigm%20that%20centers%20around%20referring%0Aexpression%20segmentation.%20It%20effectively%20handles%20a%20wide%20range%20of%20vision-centric%0Aperception%20tasks%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20grounding%2C%0Aetc%2C%20using%20a%20single%20model%20without%20any%20task-specific%20heads.%20Combining%20these%0Ainnovations%20on%20data%20and%20modeling%2C%20we%20present%20RemoteSAM%2C%20a%20foundation%20model%20that%0Aestablishes%20new%20SoTA%20on%20several%20earth%20observation%20perception%20benchmarks%2C%0Aoutperforming%20other%20foundation%20models%20such%20as%20Falcon%2C%20GeoChat%2C%20and%20LHRS-Bot%0Awith%20significantly%20higher%20efficiency.%20Models%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/1e12Leon/RemoteSAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18022v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemoteSAM%253A%2520Towards%2520Segment%2520Anything%2520for%2520Earth%2520Observation%26entry.906535625%3DLiang%2520Yao%2520and%2520Fan%2520Liu%2520and%2520Delong%2520Chen%2520and%2520Chuanyi%2520Zhang%2520and%2520Yijun%2520Wang%2520and%2520Ziyun%2520Chen%2520and%2520Wei%2520Xu%2520and%2520Shimin%2520Di%2520and%2520Yuhui%2520Zheng%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520develop%2520a%2520robust%2520yet%2520flexible%2520visual%2520foundation%2520model%2520for%2520Earth%250Aobservation.%2520It%2520should%2520possess%2520strong%2520capabilities%2520in%2520recognizing%2520and%250Alocalizing%2520diverse%2520visual%2520targets%2520while%2520providing%2520compatibility%2520with%2520various%250Ainput-output%2520interfaces%2520required%2520across%2520different%2520task%2520scenarios.%2520Current%250Asystems%2520cannot%2520meet%2520these%2520requirements%252C%2520as%2520they%2520typically%2520utilize%2520task-specific%250Aarchitecture%2520trained%2520on%2520narrow%2520data%2520domains%2520with%2520limited%2520semantic%2520coverage.%2520Our%250Astudy%2520addresses%2520these%2520limitations%2520from%2520two%2520aspects%253A%2520data%2520and%2520modeling.%2520We%2520first%250Aintroduce%2520an%2520automatic%2520data%2520engine%2520that%2520enjoys%2520significantly%2520better%2520scalability%250Acompared%2520to%2520previous%2520human%2520annotation%2520or%2520rule-based%2520approaches.%2520It%2520has%2520enabled%250Aus%2520to%2520create%2520the%2520largest%2520dataset%2520of%2520its%2520kind%2520to%2520date%252C%2520comprising%2520270K%250Aimage-text-mask%2520triplets%2520covering%2520an%2520unprecedented%2520range%2520of%2520diverse%2520semantic%250Acategories%2520and%2520attribute%2520specifications.%2520Based%2520on%2520this%2520data%2520foundation%252C%2520we%250Afurther%2520propose%2520a%2520task%2520unification%2520paradigm%2520that%2520centers%2520around%2520referring%250Aexpression%2520segmentation.%2520It%2520effectively%2520handles%2520a%2520wide%2520range%2520of%2520vision-centric%250Aperception%2520tasks%252C%2520including%2520classification%252C%2520detection%252C%2520segmentation%252C%2520grounding%252C%250Aetc%252C%2520using%2520a%2520single%2520model%2520without%2520any%2520task-specific%2520heads.%2520Combining%2520these%250Ainnovations%2520on%2520data%2520and%2520modeling%252C%2520we%2520present%2520RemoteSAM%252C%2520a%2520foundation%2520model%2520that%250Aestablishes%2520new%2520SoTA%2520on%2520several%2520earth%2520observation%2520perception%2520benchmarks%252C%250Aoutperforming%2520other%2520foundation%2520models%2520such%2520as%2520Falcon%252C%2520GeoChat%252C%2520and%2520LHRS-Bot%250Awith%2520significantly%2520higher%2520efficiency.%2520Models%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/1e12Leon/RemoteSAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18022v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation&entry.906535625=Liang%20Yao%20and%20Fan%20Liu%20and%20Delong%20Chen%20and%20Chuanyi%20Zhang%20and%20Yijun%20Wang%20and%20Ziyun%20Chen%20and%20Wei%20Xu%20and%20Shimin%20Di%20and%20Yuhui%20Zheng&entry.1292438233=%20%20We%20aim%20to%20develop%20a%20robust%20yet%20flexible%20visual%20foundation%20model%20for%20Earth%0Aobservation.%20It%20should%20possess%20strong%20capabilities%20in%20recognizing%20and%0Alocalizing%20diverse%20visual%20targets%20while%20providing%20compatibility%20with%20various%0Ainput-output%20interfaces%20required%20across%20different%20task%20scenarios.%20Current%0Asystems%20cannot%20meet%20these%20requirements%2C%20as%20they%20typically%20utilize%20task-specific%0Aarchitecture%20trained%20on%20narrow%20data%20domains%20with%20limited%20semantic%20coverage.%20Our%0Astudy%20addresses%20these%20limitations%20from%20two%20aspects%3A%20data%20and%20modeling.%20We%20first%0Aintroduce%20an%20automatic%20data%20engine%20that%20enjoys%20significantly%20better%20scalability%0Acompared%20to%20previous%20human%20annotation%20or%20rule-based%20approaches.%20It%20has%20enabled%0Aus%20to%20create%20the%20largest%20dataset%20of%20its%20kind%20to%20date%2C%20comprising%20270K%0Aimage-text-mask%20triplets%20covering%20an%20unprecedented%20range%20of%20diverse%20semantic%0Acategories%20and%20attribute%20specifications.%20Based%20on%20this%20data%20foundation%2C%20we%0Afurther%20propose%20a%20task%20unification%20paradigm%20that%20centers%20around%20referring%0Aexpression%20segmentation.%20It%20effectively%20handles%20a%20wide%20range%20of%20vision-centric%0Aperception%20tasks%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20grounding%2C%0Aetc%2C%20using%20a%20single%20model%20without%20any%20task-specific%20heads.%20Combining%20these%0Ainnovations%20on%20data%20and%20modeling%2C%20we%20present%20RemoteSAM%2C%20a%20foundation%20model%20that%0Aestablishes%20new%20SoTA%20on%20several%20earth%20observation%20perception%20benchmarks%2C%0Aoutperforming%20other%20foundation%20models%20such%20as%20Falcon%2C%20GeoChat%2C%20and%20LHRS-Bot%0Awith%20significantly%20higher%20efficiency.%20Models%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/1e12Leon/RemoteSAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18022v3&entry.124074799=Read"},
{"title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and\n  Generation", "author": "Size Wu and Zhonghua Wu and Zerui Gong and Qingyi Tao and Sheng Jin and Qinyue Li and Wei Li and Chen Change Loy", "abstract": "  In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.\n", "link": "http://arxiv.org/abs/2505.23661v3", "date": "2025-06-02", "relevancy": 2.8222, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&body=Title%3A%20OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenUni%253A%2520A%2520Simple%2520Baseline%2520for%2520Unified%2520Multimodal%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DSize%2520Wu%2520and%2520Zhonghua%2520Wu%2520and%2520Zerui%2520Gong%2520and%2520Qingyi%2520Tao%2520and%2520Sheng%2520Jin%2520and%2520Qinyue%2520Li%2520and%2520Wei%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520OpenUni%252C%2520a%2520simple%252C%2520lightweight%252C%2520and%2520fully%250Aopen-source%2520baseline%2520for%2520unifying%2520multimodal%2520understanding%2520and%2520generation.%250AInspired%2520by%2520prevailing%2520practices%2520in%2520unified%2520model%2520learning%252C%2520we%2520adopt%2520an%250Aefficient%2520training%2520strategy%2520that%2520minimizes%2520the%2520training%2520complexity%2520and%2520overhead%250Aby%2520bridging%2520the%2520off-the-shelf%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Adiffusion%2520models%2520through%2520a%2520set%2520of%2520learnable%2520queries%2520and%2520a%2520light-weight%250Atransformer-based%2520connector.%2520With%2520a%2520minimalist%2520choice%2520of%2520architecture%252C%2520we%250Ademonstrate%2520that%2520OpenUni%2520can%253A%25201%2529%2520generate%2520high-quality%2520and%2520instruction-aligned%250Aimages%252C%2520and%25202%2529%2520achieve%2520exceptional%2520performance%2520on%2520standard%2520benchmarks%2520such%2520as%250AGenEval%252C%2520DPG-%2520Bench%252C%2520and%2520WISE%252C%2520with%2520only%25201.1B%2520and%25203.1B%2520activated%2520parameters.%2520To%250Asupport%2520open%2520research%2520and%2520community%2520advancement%252C%2520we%2520release%2520all%2520model%2520weights%252C%250Atraining%2520code%252C%2520and%2520our%2520curated%2520training%2520datasets%2520%2528including%252023M%2520image-text%250Apairs%2529%2520at%2520https%253A//github.com/wusize/OpenUni.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenUni%3A%20A%20Simple%20Baseline%20for%20Unified%20Multimodal%20Understanding%20and%0A%20%20Generation&entry.906535625=Size%20Wu%20and%20Zhonghua%20Wu%20and%20Zerui%20Gong%20and%20Qingyi%20Tao%20and%20Sheng%20Jin%20and%20Qinyue%20Li%20and%20Wei%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20OpenUni%2C%20a%20simple%2C%20lightweight%2C%20and%20fully%0Aopen-source%20baseline%20for%20unifying%20multimodal%20understanding%20and%20generation.%0AInspired%20by%20prevailing%20practices%20in%20unified%20model%20learning%2C%20we%20adopt%20an%0Aefficient%20training%20strategy%20that%20minimizes%20the%20training%20complexity%20and%20overhead%0Aby%20bridging%20the%20off-the-shelf%20multimodal%20large%20language%20models%20%28LLMs%29%20and%0Adiffusion%20models%20through%20a%20set%20of%20learnable%20queries%20and%20a%20light-weight%0Atransformer-based%20connector.%20With%20a%20minimalist%20choice%20of%20architecture%2C%20we%0Ademonstrate%20that%20OpenUni%20can%3A%201%29%20generate%20high-quality%20and%20instruction-aligned%0Aimages%2C%20and%202%29%20achieve%20exceptional%20performance%20on%20standard%20benchmarks%20such%20as%0AGenEval%2C%20DPG-%20Bench%2C%20and%20WISE%2C%20with%20only%201.1B%20and%203.1B%20activated%20parameters.%20To%0Asupport%20open%20research%20and%20community%20advancement%2C%20we%20release%20all%20model%20weights%2C%0Atraining%20code%2C%20and%20our%20curated%20training%20datasets%20%28including%2023M%20image-text%0Apairs%29%20at%20https%3A//github.com/wusize/OpenUni.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23661v3&entry.124074799=Read"},
{"title": "Monge-Ampere Regularization for Learning Arbitrary Shapes from Point\n  Clouds", "author": "Chuanxiang Yang and Yuanfeng Zhou and Guangshun Wei and Long Ma and Junhui Hou and Yuan Liu and Wenping Wang", "abstract": "  As commonly used implicit geometry representations, the signed distance\nfunction (SDF) is limited to modeling watertight shapes, while the unsigned\ndistance function (UDF) is capable of representing various surfaces. However,\nits inherent theoretical shortcoming, i.e., the non-differentiability at the\nzero level set, would result in sub-optimal reconstruction quality. In this\npaper, we propose the scaled-squared distance function (S$^{2}$DF), a novel\nimplicit surface representation for modeling arbitrary surface types. S$^{2}$DF\ndoes not distinguish between inside and outside regions while effectively\naddressing the non-differentiability issue of UDF at the zero level set. We\ndemonstrate that S$^{2}$DF satisfies a second-order partial differential\nequation of Monge-Ampere-type, allowing us to develop a learning pipeline that\nleverages a novel Monge-Ampere regularization to directly learn S$^{2}$DF from\nraw unoriented point clouds without supervision from ground-truth S$^{2}$DF\nvalues. Extensive experiments across multiple datasets show that our method\nsignificantly outperforms state-of-the-art supervised approaches that require\nground-truth surface information as supervision for training. The source code\nis available at https://github.com/chuanxiang-yang/S2DF.\n", "link": "http://arxiv.org/abs/2410.18477v3", "date": "2025-06-02", "relevancy": 2.7662, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5664}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5632}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monge-Ampere%20Regularization%20for%20Learning%20Arbitrary%20Shapes%20from%20Point%0A%20%20Clouds&body=Title%3A%20Monge-Ampere%20Regularization%20for%20Learning%20Arbitrary%20Shapes%20from%20Point%0A%20%20Clouds%0AAuthor%3A%20Chuanxiang%20Yang%20and%20Yuanfeng%20Zhou%20and%20Guangshun%20Wei%20and%20Long%20Ma%20and%20Junhui%20Hou%20and%20Yuan%20Liu%20and%20Wenping%20Wang%0AAbstract%3A%20%20%20As%20commonly%20used%20implicit%20geometry%20representations%2C%20the%20signed%20distance%0Afunction%20%28SDF%29%20is%20limited%20to%20modeling%20watertight%20shapes%2C%20while%20the%20unsigned%0Adistance%20function%20%28UDF%29%20is%20capable%20of%20representing%20various%20surfaces.%20However%2C%0Aits%20inherent%20theoretical%20shortcoming%2C%20i.e.%2C%20the%20non-differentiability%20at%20the%0Azero%20level%20set%2C%20would%20result%20in%20sub-optimal%20reconstruction%20quality.%20In%20this%0Apaper%2C%20we%20propose%20the%20scaled-squared%20distance%20function%20%28S%24%5E%7B2%7D%24DF%29%2C%20a%20novel%0Aimplicit%20surface%20representation%20for%20modeling%20arbitrary%20surface%20types.%20S%24%5E%7B2%7D%24DF%0Adoes%20not%20distinguish%20between%20inside%20and%20outside%20regions%20while%20effectively%0Aaddressing%20the%20non-differentiability%20issue%20of%20UDF%20at%20the%20zero%20level%20set.%20We%0Ademonstrate%20that%20S%24%5E%7B2%7D%24DF%20satisfies%20a%20second-order%20partial%20differential%0Aequation%20of%20Monge-Ampere-type%2C%20allowing%20us%20to%20develop%20a%20learning%20pipeline%20that%0Aleverages%20a%20novel%20Monge-Ampere%20regularization%20to%20directly%20learn%20S%24%5E%7B2%7D%24DF%20from%0Araw%20unoriented%20point%20clouds%20without%20supervision%20from%20ground-truth%20S%24%5E%7B2%7D%24DF%0Avalues.%20Extensive%20experiments%20across%20multiple%20datasets%20show%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20supervised%20approaches%20that%20require%0Aground-truth%20surface%20information%20as%20supervision%20for%20training.%20The%20source%20code%0Ais%20available%20at%20https%3A//github.com/chuanxiang-yang/S2DF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonge-Ampere%2520Regularization%2520for%2520Learning%2520Arbitrary%2520Shapes%2520from%2520Point%250A%2520%2520Clouds%26entry.906535625%3DChuanxiang%2520Yang%2520and%2520Yuanfeng%2520Zhou%2520and%2520Guangshun%2520Wei%2520and%2520Long%2520Ma%2520and%2520Junhui%2520Hou%2520and%2520Yuan%2520Liu%2520and%2520Wenping%2520Wang%26entry.1292438233%3D%2520%2520As%2520commonly%2520used%2520implicit%2520geometry%2520representations%252C%2520the%2520signed%2520distance%250Afunction%2520%2528SDF%2529%2520is%2520limited%2520to%2520modeling%2520watertight%2520shapes%252C%2520while%2520the%2520unsigned%250Adistance%2520function%2520%2528UDF%2529%2520is%2520capable%2520of%2520representing%2520various%2520surfaces.%2520However%252C%250Aits%2520inherent%2520theoretical%2520shortcoming%252C%2520i.e.%252C%2520the%2520non-differentiability%2520at%2520the%250Azero%2520level%2520set%252C%2520would%2520result%2520in%2520sub-optimal%2520reconstruction%2520quality.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520scaled-squared%2520distance%2520function%2520%2528S%2524%255E%257B2%257D%2524DF%2529%252C%2520a%2520novel%250Aimplicit%2520surface%2520representation%2520for%2520modeling%2520arbitrary%2520surface%2520types.%2520S%2524%255E%257B2%257D%2524DF%250Adoes%2520not%2520distinguish%2520between%2520inside%2520and%2520outside%2520regions%2520while%2520effectively%250Aaddressing%2520the%2520non-differentiability%2520issue%2520of%2520UDF%2520at%2520the%2520zero%2520level%2520set.%2520We%250Ademonstrate%2520that%2520S%2524%255E%257B2%257D%2524DF%2520satisfies%2520a%2520second-order%2520partial%2520differential%250Aequation%2520of%2520Monge-Ampere-type%252C%2520allowing%2520us%2520to%2520develop%2520a%2520learning%2520pipeline%2520that%250Aleverages%2520a%2520novel%2520Monge-Ampere%2520regularization%2520to%2520directly%2520learn%2520S%2524%255E%257B2%257D%2524DF%2520from%250Araw%2520unoriented%2520point%2520clouds%2520without%2520supervision%2520from%2520ground-truth%2520S%2524%255E%257B2%257D%2524DF%250Avalues.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520show%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520supervised%2520approaches%2520that%2520require%250Aground-truth%2520surface%2520information%2520as%2520supervision%2520for%2520training.%2520The%2520source%2520code%250Ais%2520available%2520at%2520https%253A//github.com/chuanxiang-yang/S2DF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monge-Ampere%20Regularization%20for%20Learning%20Arbitrary%20Shapes%20from%20Point%0A%20%20Clouds&entry.906535625=Chuanxiang%20Yang%20and%20Yuanfeng%20Zhou%20and%20Guangshun%20Wei%20and%20Long%20Ma%20and%20Junhui%20Hou%20and%20Yuan%20Liu%20and%20Wenping%20Wang&entry.1292438233=%20%20As%20commonly%20used%20implicit%20geometry%20representations%2C%20the%20signed%20distance%0Afunction%20%28SDF%29%20is%20limited%20to%20modeling%20watertight%20shapes%2C%20while%20the%20unsigned%0Adistance%20function%20%28UDF%29%20is%20capable%20of%20representing%20various%20surfaces.%20However%2C%0Aits%20inherent%20theoretical%20shortcoming%2C%20i.e.%2C%20the%20non-differentiability%20at%20the%0Azero%20level%20set%2C%20would%20result%20in%20sub-optimal%20reconstruction%20quality.%20In%20this%0Apaper%2C%20we%20propose%20the%20scaled-squared%20distance%20function%20%28S%24%5E%7B2%7D%24DF%29%2C%20a%20novel%0Aimplicit%20surface%20representation%20for%20modeling%20arbitrary%20surface%20types.%20S%24%5E%7B2%7D%24DF%0Adoes%20not%20distinguish%20between%20inside%20and%20outside%20regions%20while%20effectively%0Aaddressing%20the%20non-differentiability%20issue%20of%20UDF%20at%20the%20zero%20level%20set.%20We%0Ademonstrate%20that%20S%24%5E%7B2%7D%24DF%20satisfies%20a%20second-order%20partial%20differential%0Aequation%20of%20Monge-Ampere-type%2C%20allowing%20us%20to%20develop%20a%20learning%20pipeline%20that%0Aleverages%20a%20novel%20Monge-Ampere%20regularization%20to%20directly%20learn%20S%24%5E%7B2%7D%24DF%20from%0Araw%20unoriented%20point%20clouds%20without%20supervision%20from%20ground-truth%20S%24%5E%7B2%7D%24DF%0Avalues.%20Extensive%20experiments%20across%20multiple%20datasets%20show%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20supervised%20approaches%20that%20require%0Aground-truth%20surface%20information%20as%20supervision%20for%20training.%20The%20source%20code%0Ais%20available%20at%20https%3A//github.com/chuanxiang-yang/S2DF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18477v3&entry.124074799=Read"},
{"title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language\n  Model Interpretability", "author": "Adam Karvonen and Can Rager and Johnny Lin and Curt Tigges and Joseph Bloom and David Chanin and Yeu-Tong Lau and Eoin Farrell and Callum McDougall and Kola Ayonrinde and Demian Till and Matthew Wearden and Arthur Conmy and Samuel Marks and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across eight\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at:\nwww.neuronpedia.org/sae-bench\n", "link": "http://arxiv.org/abs/2503.09532v3", "date": "2025-06-02", "relevancy": 2.6934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability&body=Title%3A%20SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability%0AAuthor%3A%20Adam%20Karvonen%20and%20Can%20Rager%20and%20Johnny%20Lin%20and%20Curt%20Tigges%20and%20Joseph%20Bloom%20and%20David%20Chanin%20and%20Yeu-Tong%20Lau%20and%20Eoin%20Farrell%20and%20Callum%20McDougall%20and%20Kola%20Ayonrinde%20and%20Demian%20Till%20and%20Matthew%20Wearden%20and%20Arthur%20Conmy%20and%20Samuel%20Marks%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20technique%20for%20interpreting%20language%0Amodel%20activations%2C%20and%20there%20is%20extensive%20recent%20work%20on%20improving%20SAE%0Aeffectiveness.%20However%2C%20most%20prior%20work%20evaluates%20progress%20using%20unsupervised%0Aproxy%20metrics%20with%20unclear%20practical%20relevance.%20We%20introduce%20SAEBench%2C%20a%0Acomprehensive%20evaluation%20suite%20that%20measures%20SAE%20performance%20across%20eight%0Adiverse%20metrics%2C%20spanning%20interpretability%2C%20feature%20disentanglement%20and%0Apractical%20applications%20like%20unlearning.%20To%20enable%20systematic%20comparison%2C%20we%0Aopen-source%20a%20suite%20of%20over%20200%20SAEs%20across%20eight%20recently%20proposed%20SAE%0Aarchitectures%20and%20training%20algorithms.%20Our%20evaluation%20reveals%20that%20gains%20on%0Aproxy%20metrics%20do%20not%20reliably%20translate%20to%20better%20practical%20performance.%20For%0Ainstance%2C%20while%20Matryoshka%20SAEs%20slightly%20underperform%20on%20existing%20proxy%0Ametrics%2C%20they%20substantially%20outperform%20other%20architectures%20on%20feature%0Adisentanglement%20metrics%3B%20moreover%2C%20this%20advantage%20grows%20with%20SAE%20scale.%20By%0Aproviding%20a%20standardized%20framework%20for%20measuring%20progress%20in%20SAE%20development%2C%0ASAEBench%20enables%20researchers%20to%20study%20scaling%20trends%20and%20make%20nuanced%0Acomparisons%20between%20different%20SAE%20architectures%20and%20training%20methodologies.%20Our%0Ainteractive%20interface%20enables%20researchers%20to%20flexibly%20visualize%20relationships%0Abetween%20metrics%20across%20hundreds%20of%20open-source%20SAEs%20at%3A%0Awww.neuronpedia.org/sae-bench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09532v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAEBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Sparse%2520Autoencoders%2520in%2520Language%250A%2520%2520Model%2520Interpretability%26entry.906535625%3DAdam%2520Karvonen%2520and%2520Can%2520Rager%2520and%2520Johnny%2520Lin%2520and%2520Curt%2520Tigges%2520and%2520Joseph%2520Bloom%2520and%2520David%2520Chanin%2520and%2520Yeu-Tong%2520Lau%2520and%2520Eoin%2520Farrell%2520and%2520Callum%2520McDougall%2520and%2520Kola%2520Ayonrinde%2520and%2520Demian%2520Till%2520and%2520Matthew%2520Wearden%2520and%2520Arthur%2520Conmy%2520and%2520Samuel%2520Marks%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520popular%2520technique%2520for%2520interpreting%2520language%250Amodel%2520activations%252C%2520and%2520there%2520is%2520extensive%2520recent%2520work%2520on%2520improving%2520SAE%250Aeffectiveness.%2520However%252C%2520most%2520prior%2520work%2520evaluates%2520progress%2520using%2520unsupervised%250Aproxy%2520metrics%2520with%2520unclear%2520practical%2520relevance.%2520We%2520introduce%2520SAEBench%252C%2520a%250Acomprehensive%2520evaluation%2520suite%2520that%2520measures%2520SAE%2520performance%2520across%2520eight%250Adiverse%2520metrics%252C%2520spanning%2520interpretability%252C%2520feature%2520disentanglement%2520and%250Apractical%2520applications%2520like%2520unlearning.%2520To%2520enable%2520systematic%2520comparison%252C%2520we%250Aopen-source%2520a%2520suite%2520of%2520over%2520200%2520SAEs%2520across%2520eight%2520recently%2520proposed%2520SAE%250Aarchitectures%2520and%2520training%2520algorithms.%2520Our%2520evaluation%2520reveals%2520that%2520gains%2520on%250Aproxy%2520metrics%2520do%2520not%2520reliably%2520translate%2520to%2520better%2520practical%2520performance.%2520For%250Ainstance%252C%2520while%2520Matryoshka%2520SAEs%2520slightly%2520underperform%2520on%2520existing%2520proxy%250Ametrics%252C%2520they%2520substantially%2520outperform%2520other%2520architectures%2520on%2520feature%250Adisentanglement%2520metrics%253B%2520moreover%252C%2520this%2520advantage%2520grows%2520with%2520SAE%2520scale.%2520By%250Aproviding%2520a%2520standardized%2520framework%2520for%2520measuring%2520progress%2520in%2520SAE%2520development%252C%250ASAEBench%2520enables%2520researchers%2520to%2520study%2520scaling%2520trends%2520and%2520make%2520nuanced%250Acomparisons%2520between%2520different%2520SAE%2520architectures%2520and%2520training%2520methodologies.%2520Our%250Ainteractive%2520interface%2520enables%2520researchers%2520to%2520flexibly%2520visualize%2520relationships%250Abetween%2520metrics%2520across%2520hundreds%2520of%2520open-source%2520SAEs%2520at%253A%250Awww.neuronpedia.org/sae-bench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09532v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAEBench%3A%20A%20Comprehensive%20Benchmark%20for%20Sparse%20Autoencoders%20in%20Language%0A%20%20Model%20Interpretability&entry.906535625=Adam%20Karvonen%20and%20Can%20Rager%20and%20Johnny%20Lin%20and%20Curt%20Tigges%20and%20Joseph%20Bloom%20and%20David%20Chanin%20and%20Yeu-Tong%20Lau%20and%20Eoin%20Farrell%20and%20Callum%20McDougall%20and%20Kola%20Ayonrinde%20and%20Demian%20Till%20and%20Matthew%20Wearden%20and%20Arthur%20Conmy%20and%20Samuel%20Marks%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20technique%20for%20interpreting%20language%0Amodel%20activations%2C%20and%20there%20is%20extensive%20recent%20work%20on%20improving%20SAE%0Aeffectiveness.%20However%2C%20most%20prior%20work%20evaluates%20progress%20using%20unsupervised%0Aproxy%20metrics%20with%20unclear%20practical%20relevance.%20We%20introduce%20SAEBench%2C%20a%0Acomprehensive%20evaluation%20suite%20that%20measures%20SAE%20performance%20across%20eight%0Adiverse%20metrics%2C%20spanning%20interpretability%2C%20feature%20disentanglement%20and%0Apractical%20applications%20like%20unlearning.%20To%20enable%20systematic%20comparison%2C%20we%0Aopen-source%20a%20suite%20of%20over%20200%20SAEs%20across%20eight%20recently%20proposed%20SAE%0Aarchitectures%20and%20training%20algorithms.%20Our%20evaluation%20reveals%20that%20gains%20on%0Aproxy%20metrics%20do%20not%20reliably%20translate%20to%20better%20practical%20performance.%20For%0Ainstance%2C%20while%20Matryoshka%20SAEs%20slightly%20underperform%20on%20existing%20proxy%0Ametrics%2C%20they%20substantially%20outperform%20other%20architectures%20on%20feature%0Adisentanglement%20metrics%3B%20moreover%2C%20this%20advantage%20grows%20with%20SAE%20scale.%20By%0Aproviding%20a%20standardized%20framework%20for%20measuring%20progress%20in%20SAE%20development%2C%0ASAEBench%20enables%20researchers%20to%20study%20scaling%20trends%20and%20make%20nuanced%0Acomparisons%20between%20different%20SAE%20architectures%20and%20training%20methodologies.%20Our%0Ainteractive%20interface%20enables%20researchers%20to%20flexibly%20visualize%20relationships%0Abetween%20metrics%20across%20hundreds%20of%20open-source%20SAEs%20at%3A%0Awww.neuronpedia.org/sae-bench%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09532v3&entry.124074799=Read"},
{"title": "MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors", "author": "Riku Murai and Eric Dexheimer and Andrew J. Davison", "abstract": "  We present a real-time monocular dense SLAM system designed bottom-up from\nMASt3R, a two-view 3D reconstruction and matching prior. Equipped with this\nstrong prior, our system is robust on in-the-wild video sequences despite\nmaking no assumption on a fixed or parametric camera model beyond a unique\ncamera centre. We introduce efficient methods for pointmap matching, camera\ntracking and local fusion, graph construction and loop closure, and\nsecond-order global optimisation. With known calibration, a simple modification\nto the system achieves state-of-the-art performance across various benchmarks.\nAltogether, we propose a plug-and-play monocular SLAM system capable of\nproducing globally-consistent poses and dense geometry while operating at 15\nFPS.\n", "link": "http://arxiv.org/abs/2412.12392v2", "date": "2025-06-02", "relevancy": 2.6005, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.721}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6075}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASt3R-SLAM%3A%20Real-Time%20Dense%20SLAM%20with%203D%20Reconstruction%20Priors&body=Title%3A%20MASt3R-SLAM%3A%20Real-Time%20Dense%20SLAM%20with%203D%20Reconstruction%20Priors%0AAuthor%3A%20Riku%20Murai%20and%20Eric%20Dexheimer%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20%20%20We%20present%20a%20real-time%20monocular%20dense%20SLAM%20system%20designed%20bottom-up%20from%0AMASt3R%2C%20a%20two-view%203D%20reconstruction%20and%20matching%20prior.%20Equipped%20with%20this%0Astrong%20prior%2C%20our%20system%20is%20robust%20on%20in-the-wild%20video%20sequences%20despite%0Amaking%20no%20assumption%20on%20a%20fixed%20or%20parametric%20camera%20model%20beyond%20a%20unique%0Acamera%20centre.%20We%20introduce%20efficient%20methods%20for%20pointmap%20matching%2C%20camera%0Atracking%20and%20local%20fusion%2C%20graph%20construction%20and%20loop%20closure%2C%20and%0Asecond-order%20global%20optimisation.%20With%20known%20calibration%2C%20a%20simple%20modification%0Ato%20the%20system%20achieves%20state-of-the-art%20performance%20across%20various%20benchmarks.%0AAltogether%2C%20we%20propose%20a%20plug-and-play%20monocular%20SLAM%20system%20capable%20of%0Aproducing%20globally-consistent%20poses%20and%20dense%20geometry%20while%20operating%20at%2015%0AFPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12392v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASt3R-SLAM%253A%2520Real-Time%2520Dense%2520SLAM%2520with%25203D%2520Reconstruction%2520Priors%26entry.906535625%3DRiku%2520Murai%2520and%2520Eric%2520Dexheimer%2520and%2520Andrew%2520J.%2520Davison%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520real-time%2520monocular%2520dense%2520SLAM%2520system%2520designed%2520bottom-up%2520from%250AMASt3R%252C%2520a%2520two-view%25203D%2520reconstruction%2520and%2520matching%2520prior.%2520Equipped%2520with%2520this%250Astrong%2520prior%252C%2520our%2520system%2520is%2520robust%2520on%2520in-the-wild%2520video%2520sequences%2520despite%250Amaking%2520no%2520assumption%2520on%2520a%2520fixed%2520or%2520parametric%2520camera%2520model%2520beyond%2520a%2520unique%250Acamera%2520centre.%2520We%2520introduce%2520efficient%2520methods%2520for%2520pointmap%2520matching%252C%2520camera%250Atracking%2520and%2520local%2520fusion%252C%2520graph%2520construction%2520and%2520loop%2520closure%252C%2520and%250Asecond-order%2520global%2520optimisation.%2520With%2520known%2520calibration%252C%2520a%2520simple%2520modification%250Ato%2520the%2520system%2520achieves%2520state-of-the-art%2520performance%2520across%2520various%2520benchmarks.%250AAltogether%252C%2520we%2520propose%2520a%2520plug-and-play%2520monocular%2520SLAM%2520system%2520capable%2520of%250Aproducing%2520globally-consistent%2520poses%2520and%2520dense%2520geometry%2520while%2520operating%2520at%252015%250AFPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12392v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASt3R-SLAM%3A%20Real-Time%20Dense%20SLAM%20with%203D%20Reconstruction%20Priors&entry.906535625=Riku%20Murai%20and%20Eric%20Dexheimer%20and%20Andrew%20J.%20Davison&entry.1292438233=%20%20We%20present%20a%20real-time%20monocular%20dense%20SLAM%20system%20designed%20bottom-up%20from%0AMASt3R%2C%20a%20two-view%203D%20reconstruction%20and%20matching%20prior.%20Equipped%20with%20this%0Astrong%20prior%2C%20our%20system%20is%20robust%20on%20in-the-wild%20video%20sequences%20despite%0Amaking%20no%20assumption%20on%20a%20fixed%20or%20parametric%20camera%20model%20beyond%20a%20unique%0Acamera%20centre.%20We%20introduce%20efficient%20methods%20for%20pointmap%20matching%2C%20camera%0Atracking%20and%20local%20fusion%2C%20graph%20construction%20and%20loop%20closure%2C%20and%0Asecond-order%20global%20optimisation.%20With%20known%20calibration%2C%20a%20simple%20modification%0Ato%20the%20system%20achieves%20state-of-the-art%20performance%20across%20various%20benchmarks.%0AAltogether%2C%20we%20propose%20a%20plug-and-play%20monocular%20SLAM%20system%20capable%20of%0Aproducing%20globally-consistent%20poses%20and%20dense%20geometry%20while%20operating%20at%2015%0AFPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12392v2&entry.124074799=Read"},
{"title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders", "author": "David Chanin and James Wilken-Smith and Tom\u00e1\u0161 Dulka and Hardik Bhatnagar and Satvik Golechha and Joseph Bloom", "abstract": "  Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale.\n", "link": "http://arxiv.org/abs/2409.14507v5", "date": "2025-06-02", "relevancy": 2.5722, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders&body=Title%3A%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Satvik%20Golechha%20and%20Joseph%20Bloom%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20aim%20to%20decompose%20the%20activation%20space%20of%20large%0Alanguage%20models%20%28LLMs%29%20into%20human-interpretable%20latent%20directions%20or%20features.%0AAs%20we%20increase%20the%20number%20of%20features%20in%20the%20SAE%2C%20hierarchical%20features%20tend%20to%0Asplit%20into%20finer%20features%20%28%22math%22%20may%20split%20into%20%22algebra%22%2C%20%22geometry%22%2C%20etc.%29%2C%0Aa%20phenomenon%20referred%20to%20as%20feature%20splitting.%20However%2C%20we%20show%20that%20sparse%0Adecomposition%20and%20splitting%20of%20hierarchical%20features%20is%20not%20robust.%0ASpecifically%2C%20we%20show%20that%20seemingly%20monosemantic%20features%20fail%20to%20fire%20where%0Athey%20should%2C%20and%20instead%20get%20%22absorbed%22%20into%20their%20children%20features.%20We%20coin%0Athis%20phenomenon%20feature%20absorption%2C%20and%20show%20that%20it%20is%20caused%20by%20optimizing%0Afor%20sparsity%20in%20SAEs%20whenever%20the%20underlying%20features%20form%20a%20hierarchy.%20We%0Aintroduce%20a%20metric%20to%20detect%20absorption%20in%20SAEs%2C%20and%20validate%20our%20findings%0Aempirically%20on%20hundreds%20of%20LLM%20SAEs.%20Our%20investigation%20suggests%20that%20varying%0ASAE%20sizes%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue.%20We%20discuss%20the%0Aimplications%20of%20feature%20absorption%20in%20SAEs%20and%20some%20potential%20approaches%20to%0Asolve%20the%20fundamental%20theoretical%20issues%20before%20SAEs%20can%20be%20used%20for%0Ainterpreting%20LLMs%20robustly%20and%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14507v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520is%2520for%2520Absorption%253A%2520Studying%2520Feature%2520Splitting%2520and%2520Absorption%2520in%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DDavid%2520Chanin%2520and%2520James%2520Wilken-Smith%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Dulka%2520and%2520Hardik%2520Bhatnagar%2520and%2520Satvik%2520Golechha%2520and%2520Joseph%2520Bloom%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520aim%2520to%2520decompose%2520the%2520activation%2520space%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520into%2520human-interpretable%2520latent%2520directions%2520or%2520features.%250AAs%2520we%2520increase%2520the%2520number%2520of%2520features%2520in%2520the%2520SAE%252C%2520hierarchical%2520features%2520tend%2520to%250Asplit%2520into%2520finer%2520features%2520%2528%2522math%2522%2520may%2520split%2520into%2520%2522algebra%2522%252C%2520%2522geometry%2522%252C%2520etc.%2529%252C%250Aa%2520phenomenon%2520referred%2520to%2520as%2520feature%2520splitting.%2520However%252C%2520we%2520show%2520that%2520sparse%250Adecomposition%2520and%2520splitting%2520of%2520hierarchical%2520features%2520is%2520not%2520robust.%250ASpecifically%252C%2520we%2520show%2520that%2520seemingly%2520monosemantic%2520features%2520fail%2520to%2520fire%2520where%250Athey%2520should%252C%2520and%2520instead%2520get%2520%2522absorbed%2522%2520into%2520their%2520children%2520features.%2520We%2520coin%250Athis%2520phenomenon%2520feature%2520absorption%252C%2520and%2520show%2520that%2520it%2520is%2520caused%2520by%2520optimizing%250Afor%2520sparsity%2520in%2520SAEs%2520whenever%2520the%2520underlying%2520features%2520form%2520a%2520hierarchy.%2520We%250Aintroduce%2520a%2520metric%2520to%2520detect%2520absorption%2520in%2520SAEs%252C%2520and%2520validate%2520our%2520findings%250Aempirically%2520on%2520hundreds%2520of%2520LLM%2520SAEs.%2520Our%2520investigation%2520suggests%2520that%2520varying%250ASAE%2520sizes%2520or%2520sparsity%2520is%2520insufficient%2520to%2520solve%2520this%2520issue.%2520We%2520discuss%2520the%250Aimplications%2520of%2520feature%2520absorption%2520in%2520SAEs%2520and%2520some%2520potential%2520approaches%2520to%250Asolve%2520the%2520fundamental%2520theoretical%2520issues%2520before%2520SAEs%2520can%2520be%2520used%2520for%250Ainterpreting%2520LLMs%2520robustly%2520and%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14507v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders&entry.906535625=David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Satvik%20Golechha%20and%20Joseph%20Bloom&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20aim%20to%20decompose%20the%20activation%20space%20of%20large%0Alanguage%20models%20%28LLMs%29%20into%20human-interpretable%20latent%20directions%20or%20features.%0AAs%20we%20increase%20the%20number%20of%20features%20in%20the%20SAE%2C%20hierarchical%20features%20tend%20to%0Asplit%20into%20finer%20features%20%28%22math%22%20may%20split%20into%20%22algebra%22%2C%20%22geometry%22%2C%20etc.%29%2C%0Aa%20phenomenon%20referred%20to%20as%20feature%20splitting.%20However%2C%20we%20show%20that%20sparse%0Adecomposition%20and%20splitting%20of%20hierarchical%20features%20is%20not%20robust.%0ASpecifically%2C%20we%20show%20that%20seemingly%20monosemantic%20features%20fail%20to%20fire%20where%0Athey%20should%2C%20and%20instead%20get%20%22absorbed%22%20into%20their%20children%20features.%20We%20coin%0Athis%20phenomenon%20feature%20absorption%2C%20and%20show%20that%20it%20is%20caused%20by%20optimizing%0Afor%20sparsity%20in%20SAEs%20whenever%20the%20underlying%20features%20form%20a%20hierarchy.%20We%0Aintroduce%20a%20metric%20to%20detect%20absorption%20in%20SAEs%2C%20and%20validate%20our%20findings%0Aempirically%20on%20hundreds%20of%20LLM%20SAEs.%20Our%20investigation%20suggests%20that%20varying%0ASAE%20sizes%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue.%20We%20discuss%20the%0Aimplications%20of%20feature%20absorption%20in%20SAEs%20and%20some%20potential%20approaches%20to%0Asolve%20the%20fundamental%20theoretical%20issues%20before%20SAEs%20can%20be%20used%20for%0Ainterpreting%20LLMs%20robustly%20and%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14507v5&entry.124074799=Read"},
{"title": "LoRACode: LoRA Adapters for Code Embeddings", "author": "Saumya Chaturvedi and Aman Chadha and Laurent Bindschaedler", "abstract": "  Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations. To foster research in this area, we\nmake our code and pre-trained models publicly available.\n", "link": "http://arxiv.org/abs/2503.05315v2", "date": "2025-06-02", "relevancy": 2.4982, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRACode%3A%20LoRA%20Adapters%20for%20Code%20Embeddings&body=Title%3A%20LoRACode%3A%20LoRA%20Adapters%20for%20Code%20Embeddings%0AAuthor%3A%20Saumya%20Chaturvedi%20and%20Aman%20Chadha%20and%20Laurent%20Bindschaedler%0AAbstract%3A%20%20%20Code%20embeddings%20are%20essential%20for%20semantic%20code%20search%3B%20however%2C%20current%0Aapproaches%20often%20struggle%20to%20capture%20the%20precise%20syntactic%20and%20contextual%0Anuances%20inherent%20in%20code.%20Open-source%20models%20such%20as%20CodeBERT%20and%20UniXcoder%0Aexhibit%20limitations%20in%20scalability%20and%20efficiency%2C%20while%20high-performing%0Aproprietary%20systems%20impose%20substantial%20computational%20costs.%20We%20introduce%20a%0Aparameter-efficient%20fine-tuning%20method%20based%20on%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Aconstruct%20task-specific%20adapters%20for%20code%20retrieval.%20Our%20approach%20reduces%20the%0Anumber%20of%20trainable%20parameters%20to%20less%20than%20two%20percent%20of%20the%20base%20model%2C%0Aenabling%20rapid%20fine-tuning%20on%20extensive%20code%20corpora%20%282%20million%20samples%20in%2025%0Aminutes%20on%20two%20H100%20GPUs%29.%20Experiments%20demonstrate%20an%20increase%20of%20up%20to%209.1%25%20in%0AMean%20Reciprocal%20Rank%20%28MRR%29%20for%20Code2Code%20search%2C%20and%20up%20to%2086.69%25%20for%20Text2Code%0Asearch%20tasks%20across%20multiple%20programming%20languages.%20Distinction%20in%20task-wise%0Aand%20language-wise%20adaptation%20helps%20explore%20the%20sensitivity%20of%20code%20retrieval%0Afor%20syntactical%20and%20linguistic%20variations.%20To%20foster%20research%20in%20this%20area%2C%20we%0Amake%20our%20code%20and%20pre-trained%20models%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRACode%253A%2520LoRA%2520Adapters%2520for%2520Code%2520Embeddings%26entry.906535625%3DSaumya%2520Chaturvedi%2520and%2520Aman%2520Chadha%2520and%2520Laurent%2520Bindschaedler%26entry.1292438233%3D%2520%2520Code%2520embeddings%2520are%2520essential%2520for%2520semantic%2520code%2520search%253B%2520however%252C%2520current%250Aapproaches%2520often%2520struggle%2520to%2520capture%2520the%2520precise%2520syntactic%2520and%2520contextual%250Anuances%2520inherent%2520in%2520code.%2520Open-source%2520models%2520such%2520as%2520CodeBERT%2520and%2520UniXcoder%250Aexhibit%2520limitations%2520in%2520scalability%2520and%2520efficiency%252C%2520while%2520high-performing%250Aproprietary%2520systems%2520impose%2520substantial%2520computational%2520costs.%2520We%2520introduce%2520a%250Aparameter-efficient%2520fine-tuning%2520method%2520based%2520on%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%250Aconstruct%2520task-specific%2520adapters%2520for%2520code%2520retrieval.%2520Our%2520approach%2520reduces%2520the%250Anumber%2520of%2520trainable%2520parameters%2520to%2520less%2520than%2520two%2520percent%2520of%2520the%2520base%2520model%252C%250Aenabling%2520rapid%2520fine-tuning%2520on%2520extensive%2520code%2520corpora%2520%25282%2520million%2520samples%2520in%252025%250Aminutes%2520on%2520two%2520H100%2520GPUs%2529.%2520Experiments%2520demonstrate%2520an%2520increase%2520of%2520up%2520to%25209.1%2525%2520in%250AMean%2520Reciprocal%2520Rank%2520%2528MRR%2529%2520for%2520Code2Code%2520search%252C%2520and%2520up%2520to%252086.69%2525%2520for%2520Text2Code%250Asearch%2520tasks%2520across%2520multiple%2520programming%2520languages.%2520Distinction%2520in%2520task-wise%250Aand%2520language-wise%2520adaptation%2520helps%2520explore%2520the%2520sensitivity%2520of%2520code%2520retrieval%250Afor%2520syntactical%2520and%2520linguistic%2520variations.%2520To%2520foster%2520research%2520in%2520this%2520area%252C%2520we%250Amake%2520our%2520code%2520and%2520pre-trained%2520models%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRACode%3A%20LoRA%20Adapters%20for%20Code%20Embeddings&entry.906535625=Saumya%20Chaturvedi%20and%20Aman%20Chadha%20and%20Laurent%20Bindschaedler&entry.1292438233=%20%20Code%20embeddings%20are%20essential%20for%20semantic%20code%20search%3B%20however%2C%20current%0Aapproaches%20often%20struggle%20to%20capture%20the%20precise%20syntactic%20and%20contextual%0Anuances%20inherent%20in%20code.%20Open-source%20models%20such%20as%20CodeBERT%20and%20UniXcoder%0Aexhibit%20limitations%20in%20scalability%20and%20efficiency%2C%20while%20high-performing%0Aproprietary%20systems%20impose%20substantial%20computational%20costs.%20We%20introduce%20a%0Aparameter-efficient%20fine-tuning%20method%20based%20on%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Aconstruct%20task-specific%20adapters%20for%20code%20retrieval.%20Our%20approach%20reduces%20the%0Anumber%20of%20trainable%20parameters%20to%20less%20than%20two%20percent%20of%20the%20base%20model%2C%0Aenabling%20rapid%20fine-tuning%20on%20extensive%20code%20corpora%20%282%20million%20samples%20in%2025%0Aminutes%20on%20two%20H100%20GPUs%29.%20Experiments%20demonstrate%20an%20increase%20of%20up%20to%209.1%25%20in%0AMean%20Reciprocal%20Rank%20%28MRR%29%20for%20Code2Code%20search%2C%20and%20up%20to%2086.69%25%20for%20Text2Code%0Asearch%20tasks%20across%20multiple%20programming%20languages.%20Distinction%20in%20task-wise%0Aand%20language-wise%20adaptation%20helps%20explore%20the%20sensitivity%20of%20code%20retrieval%0Afor%20syntactical%20and%20linguistic%20variations.%20To%20foster%20research%20in%20this%20area%2C%20we%0Amake%20our%20code%20and%20pre-trained%20models%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05315v2&entry.124074799=Read"},
{"title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels", "author": "Lingxiao Wei and He Yan and Xiangju Lu and Junmin Zhu and Jun Wang and Wei Zhang", "abstract": "  Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)\n", "link": "http://arxiv.org/abs/2412.02819v5", "date": "2025-06-02", "relevancy": 2.4833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels&body=Title%3A%20CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels%0AAuthor%3A%20Lingxiao%20Wei%20and%20He%20Yan%20and%20Xiangju%20Lu%20and%20Junmin%20Zhu%20and%20Jun%20Wang%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20well-researched%20in%20various%0Along-context%20tasks.%20However%2C%20the%20scarcity%20of%20long-context%20summarization%0Adatasets%20hinders%20progress%20in%20this%20area.%20To%20address%20this%2C%20we%20introduce%20CNNSum%2C%20a%0Amulti-scale%20long-context%20summarization%20benchmark%20based%20on%20Chinese%20novels%2C%0Afeaturing%20human-driven%20annotations%20across%20four%20subsets%20totaling%20695%20samples%2C%0Awith%20lengths%20ranging%20from%2016k%20to%20128k.%20We%20benchmark%20numerous%20LLMs%20and%20conduct%0Adetailed%20human%20assessments%20to%20summarize%20abnormal%20output%20types.%20Furthermore%2C%20we%0Aextensively%20explore%20how%20to%20improve%20long-context%20summarization.%20In%20our%20study%3A%0A%281%29%20Advanced%20LLMs%20may%20generate%20much%20subjective%20commentary%2C%20leading%20to%20vague%0Asummaries.%20%282%29%20Currently%2C%20long-context%20summarization%20mainly%20relies%20on%20memory%0Aability.%20The%20advantages%20of%20Large%20LLMs%20are%20hard%20to%20utilize%2C%20thus%20small%20LLMs%20are%0Amore%20cost-effective.%20%283%29%20Different%20prompt%20types%20paired%20with%20various%20version%0Amodels%20may%20cause%20large%20performance%20gaps.%20In%20further%20fine-tuning%2C%20these%20can%20be%0Amitigated%2C%20and%20the%20Base%20version%20models%20perform%20better.%20%284%29%20LLMs%20with%20RoPE-base%0Ascaled%20exhibit%20strong%20extrapolation%20potential%3B%20using%20short-context%20data%20can%0Asignificantly%20improve%20long-context%20summarization%20performance.%20However%2C%20further%0Aapplying%20other%20interpolation%20methods%20requires%20careful%20selection.%20%285%29%20CNNSum%0Aprovides%20more%20reliable%20evaluation%20results%20than%20other%20benchmarks.%20We%20release%0ACNNSum%20to%20advance%20future%20research.%28https%3A//github.com/CxsGhost/CNNSum%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02819v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNNSum%253A%2520Exploring%2520Long-Context%2520Summarization%2520with%2520Large%2520Language%2520Models%250A%2520%2520in%2520Chinese%2520Novels%26entry.906535625%3DLingxiao%2520Wei%2520and%2520He%2520Yan%2520and%2520Xiangju%2520Lu%2520and%2520Junmin%2520Zhu%2520and%2520Jun%2520Wang%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520well-researched%2520in%2520various%250Along-context%2520tasks.%2520However%252C%2520the%2520scarcity%2520of%2520long-context%2520summarization%250Adatasets%2520hinders%2520progress%2520in%2520this%2520area.%2520To%2520address%2520this%252C%2520we%2520introduce%2520CNNSum%252C%2520a%250Amulti-scale%2520long-context%2520summarization%2520benchmark%2520based%2520on%2520Chinese%2520novels%252C%250Afeaturing%2520human-driven%2520annotations%2520across%2520four%2520subsets%2520totaling%2520695%2520samples%252C%250Awith%2520lengths%2520ranging%2520from%252016k%2520to%2520128k.%2520We%2520benchmark%2520numerous%2520LLMs%2520and%2520conduct%250Adetailed%2520human%2520assessments%2520to%2520summarize%2520abnormal%2520output%2520types.%2520Furthermore%252C%2520we%250Aextensively%2520explore%2520how%2520to%2520improve%2520long-context%2520summarization.%2520In%2520our%2520study%253A%250A%25281%2529%2520Advanced%2520LLMs%2520may%2520generate%2520much%2520subjective%2520commentary%252C%2520leading%2520to%2520vague%250Asummaries.%2520%25282%2529%2520Currently%252C%2520long-context%2520summarization%2520mainly%2520relies%2520on%2520memory%250Aability.%2520The%2520advantages%2520of%2520Large%2520LLMs%2520are%2520hard%2520to%2520utilize%252C%2520thus%2520small%2520LLMs%2520are%250Amore%2520cost-effective.%2520%25283%2529%2520Different%2520prompt%2520types%2520paired%2520with%2520various%2520version%250Amodels%2520may%2520cause%2520large%2520performance%2520gaps.%2520In%2520further%2520fine-tuning%252C%2520these%2520can%2520be%250Amitigated%252C%2520and%2520the%2520Base%2520version%2520models%2520perform%2520better.%2520%25284%2529%2520LLMs%2520with%2520RoPE-base%250Ascaled%2520exhibit%2520strong%2520extrapolation%2520potential%253B%2520using%2520short-context%2520data%2520can%250Asignificantly%2520improve%2520long-context%2520summarization%2520performance.%2520However%252C%2520further%250Aapplying%2520other%2520interpolation%2520methods%2520requires%2520careful%2520selection.%2520%25285%2529%2520CNNSum%250Aprovides%2520more%2520reliable%2520evaluation%2520results%2520than%2520other%2520benchmarks.%2520We%2520release%250ACNNSum%2520to%2520advance%2520future%2520research.%2528https%253A//github.com/CxsGhost/CNNSum%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02819v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNNSum%3A%20Exploring%20Long-Context%20Summarization%20with%20Large%20Language%20Models%0A%20%20in%20Chinese%20Novels&entry.906535625=Lingxiao%20Wei%20and%20He%20Yan%20and%20Xiangju%20Lu%20and%20Junmin%20Zhu%20and%20Jun%20Wang%20and%20Wei%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20well-researched%20in%20various%0Along-context%20tasks.%20However%2C%20the%20scarcity%20of%20long-context%20summarization%0Adatasets%20hinders%20progress%20in%20this%20area.%20To%20address%20this%2C%20we%20introduce%20CNNSum%2C%20a%0Amulti-scale%20long-context%20summarization%20benchmark%20based%20on%20Chinese%20novels%2C%0Afeaturing%20human-driven%20annotations%20across%20four%20subsets%20totaling%20695%20samples%2C%0Awith%20lengths%20ranging%20from%2016k%20to%20128k.%20We%20benchmark%20numerous%20LLMs%20and%20conduct%0Adetailed%20human%20assessments%20to%20summarize%20abnormal%20output%20types.%20Furthermore%2C%20we%0Aextensively%20explore%20how%20to%20improve%20long-context%20summarization.%20In%20our%20study%3A%0A%281%29%20Advanced%20LLMs%20may%20generate%20much%20subjective%20commentary%2C%20leading%20to%20vague%0Asummaries.%20%282%29%20Currently%2C%20long-context%20summarization%20mainly%20relies%20on%20memory%0Aability.%20The%20advantages%20of%20Large%20LLMs%20are%20hard%20to%20utilize%2C%20thus%20small%20LLMs%20are%0Amore%20cost-effective.%20%283%29%20Different%20prompt%20types%20paired%20with%20various%20version%0Amodels%20may%20cause%20large%20performance%20gaps.%20In%20further%20fine-tuning%2C%20these%20can%20be%0Amitigated%2C%20and%20the%20Base%20version%20models%20perform%20better.%20%284%29%20LLMs%20with%20RoPE-base%0Ascaled%20exhibit%20strong%20extrapolation%20potential%3B%20using%20short-context%20data%20can%0Asignificantly%20improve%20long-context%20summarization%20performance.%20However%2C%20further%0Aapplying%20other%20interpolation%20methods%20requires%20careful%20selection.%20%285%29%20CNNSum%0Aprovides%20more%20reliable%20evaluation%20results%20than%20other%20benchmarks.%20We%20release%0ACNNSum%20to%20advance%20future%20research.%28https%3A//github.com/CxsGhost/CNNSum%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02819v5&entry.124074799=Read"},
{"title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator", "author": "Guoxuan Chen and Han Shi and Jiawei Li and Yihang Gao and Xiaozhe Ren and Yimeng Chen and Xin Jiang and Zhenguo Li and Weiyang Liu and Chao Huang", "abstract": "  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n", "link": "http://arxiv.org/abs/2412.12094v6", "date": "2025-06-02", "relevancy": 2.4748, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&body=Title%3A%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator%0AAuthor%3A%20Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20separator%20tokens%20%28i.e.%2C%0Apunctuations%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12094v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSepLLM%253A%2520Accelerate%2520Large%2520Language%2520Models%2520by%2520Compressing%2520One%2520Segment%2520into%250A%2520%2520One%2520Separator%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Han%2520Shi%2520and%2520Jiawei%2520Li%2520and%2520Yihang%2520Gao%2520and%2520Xiaozhe%2520Ren%2520and%2520Yimeng%2520Chen%2520and%2520Xin%2520Jiang%2520and%2520Zhenguo%2520Li%2520and%2520Weiyang%2520Liu%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520exceptional%2520performance%2520across%2520a%250Aspectrum%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520their%2520substantial%2520sizes%250Apose%2520considerable%2520challenges%252C%2520particularly%2520in%2520computational%2520demands%2520and%250Ainference%2520speed%252C%2520due%2520to%2520their%2520quadratic%2520complexity.%2520In%2520this%2520work%252C%2520we%2520have%250Aidentified%2520a%2520key%2520pattern%253A%2520certain%2520seemingly%2520meaningless%2520separator%2520tokens%2520%2528i.e.%252C%250Apunctuations%2529%2520contribute%2520disproportionately%2520to%2520attention%2520scores%2520compared%2520to%250Asemantically%2520meaningful%2520tokens.%2520This%2520observation%2520suggests%2520that%2520information%2520of%250Athe%2520segments%2520between%2520these%2520separator%2520tokens%2520can%2520be%2520effectively%2520condensed%2520into%250Athe%2520separator%2520tokens%2520themselves%2520without%2520significant%2520information%2520loss.%2520Guided%2520by%250Athis%2520insight%252C%2520we%2520introduce%2520SepLLM%252C%2520a%2520plug-and-play%2520framework%2520that%2520accelerates%250Ainference%2520by%2520compressing%2520these%2520segments%2520and%2520eliminating%2520redundant%2520tokens.%250AAdditionally%252C%2520we%2520implement%2520efficient%2520kernels%2520for%2520training%2520acceleration.%250AExperimental%2520results%2520across%2520training-free%252C%2520training-from-scratch%252C%2520and%250Apost-training%2520settings%2520demonstrate%2520SepLLM%2527s%2520effectiveness.%2520Notably%252C%2520using%2520the%250ALlama-3-8B%2520backbone%252C%2520SepLLM%2520achieves%2520over%252050%2525%2520reduction%2520in%2520KV%2520cache%2520on%2520the%250AGSM8K-CoT%2520benchmark%2520while%2520maintaining%2520comparable%2520performance.%2520Furthermore%252C%2520in%250Astreaming%2520settings%252C%2520SepLLM%2520effectively%2520processes%2520sequences%2520of%2520up%2520to%25204%2520million%250Atokens%2520or%2520more%2520while%2520maintaining%2520consistent%2520language%2520modeling%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12094v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&entry.906535625=Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20separator%20tokens%20%28i.e.%2C%0Apunctuations%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12094v6&entry.124074799=Read"},
{"title": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens", "author": "Jaehyeon Kim and Taehong Moon and Keon Lee and Jaewoong Cho", "abstract": "  We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based\ngenerative model for high-fidelity generation with fast sampling. RVQ improves\ndata fidelity by increasing the number of quantization steps, referred to as\ndepth, but deeper quantization typically increases inference steps in\ngenerative models. To address this, ResGen directly predicts the vector\nembedding of collective tokens rather than individual ones, ensuring that\ninference steps remain independent of RVQ depth. Additionally, we formulate\ntoken masking and multi-token prediction within a probabilistic framework using\ndiscrete diffusion and variational inference. We validate the efficacy and\ngeneralizability of the proposed method on two challenging tasks across\ndifferent modalities: conditional image generation on ImageNet 256x256 and\nzero-shot text-to-speech synthesis. Experimental results demonstrate that\nResGen outperforms autoregressive counterparts in both tasks, delivering\nsuperior performance without compromising sampling speed. Furthermore, as we\nscale the depth of RVQ, our generative models exhibit enhanced generation\nfidelity or faster sampling speeds compared to similarly sized baseline models.\n", "link": "http://arxiv.org/abs/2412.10208v3", "date": "2025-06-02", "relevancy": 2.4438, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6321}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6186}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens&body=Title%3A%20Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens%0AAuthor%3A%20Jaehyeon%20Kim%20and%20Taehong%20Moon%20and%20Keon%20Lee%20and%20Jaewoong%20Cho%0AAbstract%3A%20%20%20We%20introduce%20ResGen%2C%20an%20efficient%20Residual%20Vector%20Quantization%20%28RVQ%29-based%0Agenerative%20model%20for%20high-fidelity%20generation%20with%20fast%20sampling.%20RVQ%20improves%0Adata%20fidelity%20by%20increasing%20the%20number%20of%20quantization%20steps%2C%20referred%20to%20as%0Adepth%2C%20but%20deeper%20quantization%20typically%20increases%20inference%20steps%20in%0Agenerative%20models.%20To%20address%20this%2C%20ResGen%20directly%20predicts%20the%20vector%0Aembedding%20of%20collective%20tokens%20rather%20than%20individual%20ones%2C%20ensuring%20that%0Ainference%20steps%20remain%20independent%20of%20RVQ%20depth.%20Additionally%2C%20we%20formulate%0Atoken%20masking%20and%20multi-token%20prediction%20within%20a%20probabilistic%20framework%20using%0Adiscrete%20diffusion%20and%20variational%20inference.%20We%20validate%20the%20efficacy%20and%0Ageneralizability%20of%20the%20proposed%20method%20on%20two%20challenging%20tasks%20across%0Adifferent%20modalities%3A%20conditional%20image%20generation%20on%20ImageNet%20256x256%20and%0Azero-shot%20text-to-speech%20synthesis.%20Experimental%20results%20demonstrate%20that%0AResGen%20outperforms%20autoregressive%20counterparts%20in%20both%20tasks%2C%20delivering%0Asuperior%20performance%20without%20compromising%20sampling%20speed.%20Furthermore%2C%20as%20we%0Ascale%20the%20depth%20of%20RVQ%2C%20our%20generative%20models%20exhibit%20enhanced%20generation%0Afidelity%20or%20faster%20sampling%20speeds%20compared%20to%20similarly%20sized%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10208v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Generative%2520Modeling%2520with%2520Residual%2520Vector%2520Quantization-Based%250A%2520%2520Tokens%26entry.906535625%3DJaehyeon%2520Kim%2520and%2520Taehong%2520Moon%2520and%2520Keon%2520Lee%2520and%2520Jaewoong%2520Cho%26entry.1292438233%3D%2520%2520We%2520introduce%2520ResGen%252C%2520an%2520efficient%2520Residual%2520Vector%2520Quantization%2520%2528RVQ%2529-based%250Agenerative%2520model%2520for%2520high-fidelity%2520generation%2520with%2520fast%2520sampling.%2520RVQ%2520improves%250Adata%2520fidelity%2520by%2520increasing%2520the%2520number%2520of%2520quantization%2520steps%252C%2520referred%2520to%2520as%250Adepth%252C%2520but%2520deeper%2520quantization%2520typically%2520increases%2520inference%2520steps%2520in%250Agenerative%2520models.%2520To%2520address%2520this%252C%2520ResGen%2520directly%2520predicts%2520the%2520vector%250Aembedding%2520of%2520collective%2520tokens%2520rather%2520than%2520individual%2520ones%252C%2520ensuring%2520that%250Ainference%2520steps%2520remain%2520independent%2520of%2520RVQ%2520depth.%2520Additionally%252C%2520we%2520formulate%250Atoken%2520masking%2520and%2520multi-token%2520prediction%2520within%2520a%2520probabilistic%2520framework%2520using%250Adiscrete%2520diffusion%2520and%2520variational%2520inference.%2520We%2520validate%2520the%2520efficacy%2520and%250Ageneralizability%2520of%2520the%2520proposed%2520method%2520on%2520two%2520challenging%2520tasks%2520across%250Adifferent%2520modalities%253A%2520conditional%2520image%2520generation%2520on%2520ImageNet%2520256x256%2520and%250Azero-shot%2520text-to-speech%2520synthesis.%2520Experimental%2520results%2520demonstrate%2520that%250AResGen%2520outperforms%2520autoregressive%2520counterparts%2520in%2520both%2520tasks%252C%2520delivering%250Asuperior%2520performance%2520without%2520compromising%2520sampling%2520speed.%2520Furthermore%252C%2520as%2520we%250Ascale%2520the%2520depth%2520of%2520RVQ%252C%2520our%2520generative%2520models%2520exhibit%2520enhanced%2520generation%250Afidelity%2520or%2520faster%2520sampling%2520speeds%2520compared%2520to%2520similarly%2520sized%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10208v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Generative%20Modeling%20with%20Residual%20Vector%20Quantization-Based%0A%20%20Tokens&entry.906535625=Jaehyeon%20Kim%20and%20Taehong%20Moon%20and%20Keon%20Lee%20and%20Jaewoong%20Cho&entry.1292438233=%20%20We%20introduce%20ResGen%2C%20an%20efficient%20Residual%20Vector%20Quantization%20%28RVQ%29-based%0Agenerative%20model%20for%20high-fidelity%20generation%20with%20fast%20sampling.%20RVQ%20improves%0Adata%20fidelity%20by%20increasing%20the%20number%20of%20quantization%20steps%2C%20referred%20to%20as%0Adepth%2C%20but%20deeper%20quantization%20typically%20increases%20inference%20steps%20in%0Agenerative%20models.%20To%20address%20this%2C%20ResGen%20directly%20predicts%20the%20vector%0Aembedding%20of%20collective%20tokens%20rather%20than%20individual%20ones%2C%20ensuring%20that%0Ainference%20steps%20remain%20independent%20of%20RVQ%20depth.%20Additionally%2C%20we%20formulate%0Atoken%20masking%20and%20multi-token%20prediction%20within%20a%20probabilistic%20framework%20using%0Adiscrete%20diffusion%20and%20variational%20inference.%20We%20validate%20the%20efficacy%20and%0Ageneralizability%20of%20the%20proposed%20method%20on%20two%20challenging%20tasks%20across%0Adifferent%20modalities%3A%20conditional%20image%20generation%20on%20ImageNet%20256x256%20and%0Azero-shot%20text-to-speech%20synthesis.%20Experimental%20results%20demonstrate%20that%0AResGen%20outperforms%20autoregressive%20counterparts%20in%20both%20tasks%2C%20delivering%0Asuperior%20performance%20without%20compromising%20sampling%20speed.%20Furthermore%2C%20as%20we%0Ascale%20the%20depth%20of%20RVQ%2C%20our%20generative%20models%20exhibit%20enhanced%20generation%0Afidelity%20or%20faster%20sampling%20speeds%20compared%20to%20similarly%20sized%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10208v3&entry.124074799=Read"},
{"title": "Segment Anything for Histopathology", "author": "Titus Griebel and Anwai Archit and Constantin Pape", "abstract": "  Nucleus segmentation is an important analysis task in digital pathology.\nHowever, methods for automatic segmentation often struggle with new data from a\ndifferent distribution, requiring users to manually annotate nuclei and retrain\ndata-specific models. Vision foundation models (VFMs), such as the Segment\nAnything Model (SAM), offer a more robust alternative for automatic and\ninteractive segmentation. Despite their success in natural images, a foundation\nmodel for nucleus segmentation in histopathology is still missing. Initial\nefforts to adapt SAM have shown some success, but did not yet introduce a\ncomprehensive model for diverse segmentation tasks. To close this gap, we\nintroduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a\ndiverse dataset. Our extensive experiments show that it is the new\nstate-of-the-art model for automatic and interactive nucleus instance\nsegmentation in histopathology. We also demonstrate how it can be adapted for\nother segmentation tasks, including semantic nucleus segmentation. For this\ntask, we show that it yields results better than popular methods, while not yet\nbeating the state-of-the-art, CellViT. Our models are open-source and\ncompatible with popular tools for data annotation. We also provide scripts for\nwhole-slide image segmentation. Our code and models are publicly available at\nhttps://github.com/computational-cell-analytics/patho-sam.\n", "link": "http://arxiv.org/abs/2502.00408v2", "date": "2025-06-02", "relevancy": 2.3976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20for%20Histopathology&body=Title%3A%20Segment%20Anything%20for%20Histopathology%0AAuthor%3A%20Titus%20Griebel%20and%20Anwai%20Archit%20and%20Constantin%20Pape%0AAbstract%3A%20%20%20Nucleus%20segmentation%20is%20an%20important%20analysis%20task%20in%20digital%20pathology.%0AHowever%2C%20methods%20for%20automatic%20segmentation%20often%20struggle%20with%20new%20data%20from%20a%0Adifferent%20distribution%2C%20requiring%20users%20to%20manually%20annotate%20nuclei%20and%20retrain%0Adata-specific%20models.%20Vision%20foundation%20models%20%28VFMs%29%2C%20such%20as%20the%20Segment%0AAnything%20Model%20%28SAM%29%2C%20offer%20a%20more%20robust%20alternative%20for%20automatic%20and%0Ainteractive%20segmentation.%20Despite%20their%20success%20in%20natural%20images%2C%20a%20foundation%0Amodel%20for%20nucleus%20segmentation%20in%20histopathology%20is%20still%20missing.%20Initial%0Aefforts%20to%20adapt%20SAM%20have%20shown%20some%20success%2C%20but%20did%20not%20yet%20introduce%20a%0Acomprehensive%20model%20for%20diverse%20segmentation%20tasks.%20To%20close%20this%20gap%2C%20we%0Aintroduce%20PathoSAM%2C%20a%20VFM%20for%20nucleus%20segmentation%2C%20based%20on%20training%20SAM%20on%20a%0Adiverse%20dataset.%20Our%20extensive%20experiments%20show%20that%20it%20is%20the%20new%0Astate-of-the-art%20model%20for%20automatic%20and%20interactive%20nucleus%20instance%0Asegmentation%20in%20histopathology.%20We%20also%20demonstrate%20how%20it%20can%20be%20adapted%20for%0Aother%20segmentation%20tasks%2C%20including%20semantic%20nucleus%20segmentation.%20For%20this%0Atask%2C%20we%20show%20that%20it%20yields%20results%20better%20than%20popular%20methods%2C%20while%20not%20yet%0Abeating%20the%20state-of-the-art%2C%20CellViT.%20Our%20models%20are%20open-source%20and%0Acompatible%20with%20popular%20tools%20for%20data%20annotation.%20We%20also%20provide%20scripts%20for%0Awhole-slide%20image%20segmentation.%20Our%20code%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/computational-cell-analytics/patho-sam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520for%2520Histopathology%26entry.906535625%3DTitus%2520Griebel%2520and%2520Anwai%2520Archit%2520and%2520Constantin%2520Pape%26entry.1292438233%3D%2520%2520Nucleus%2520segmentation%2520is%2520an%2520important%2520analysis%2520task%2520in%2520digital%2520pathology.%250AHowever%252C%2520methods%2520for%2520automatic%2520segmentation%2520often%2520struggle%2520with%2520new%2520data%2520from%2520a%250Adifferent%2520distribution%252C%2520requiring%2520users%2520to%2520manually%2520annotate%2520nuclei%2520and%2520retrain%250Adata-specific%2520models.%2520Vision%2520foundation%2520models%2520%2528VFMs%2529%252C%2520such%2520as%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%252C%2520offer%2520a%2520more%2520robust%2520alternative%2520for%2520automatic%2520and%250Ainteractive%2520segmentation.%2520Despite%2520their%2520success%2520in%2520natural%2520images%252C%2520a%2520foundation%250Amodel%2520for%2520nucleus%2520segmentation%2520in%2520histopathology%2520is%2520still%2520missing.%2520Initial%250Aefforts%2520to%2520adapt%2520SAM%2520have%2520shown%2520some%2520success%252C%2520but%2520did%2520not%2520yet%2520introduce%2520a%250Acomprehensive%2520model%2520for%2520diverse%2520segmentation%2520tasks.%2520To%2520close%2520this%2520gap%252C%2520we%250Aintroduce%2520PathoSAM%252C%2520a%2520VFM%2520for%2520nucleus%2520segmentation%252C%2520based%2520on%2520training%2520SAM%2520on%2520a%250Adiverse%2520dataset.%2520Our%2520extensive%2520experiments%2520show%2520that%2520it%2520is%2520the%2520new%250Astate-of-the-art%2520model%2520for%2520automatic%2520and%2520interactive%2520nucleus%2520instance%250Asegmentation%2520in%2520histopathology.%2520We%2520also%2520demonstrate%2520how%2520it%2520can%2520be%2520adapted%2520for%250Aother%2520segmentation%2520tasks%252C%2520including%2520semantic%2520nucleus%2520segmentation.%2520For%2520this%250Atask%252C%2520we%2520show%2520that%2520it%2520yields%2520results%2520better%2520than%2520popular%2520methods%252C%2520while%2520not%2520yet%250Abeating%2520the%2520state-of-the-art%252C%2520CellViT.%2520Our%2520models%2520are%2520open-source%2520and%250Acompatible%2520with%2520popular%2520tools%2520for%2520data%2520annotation.%2520We%2520also%2520provide%2520scripts%2520for%250Awhole-slide%2520image%2520segmentation.%2520Our%2520code%2520and%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/computational-cell-analytics/patho-sam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20for%20Histopathology&entry.906535625=Titus%20Griebel%20and%20Anwai%20Archit%20and%20Constantin%20Pape&entry.1292438233=%20%20Nucleus%20segmentation%20is%20an%20important%20analysis%20task%20in%20digital%20pathology.%0AHowever%2C%20methods%20for%20automatic%20segmentation%20often%20struggle%20with%20new%20data%20from%20a%0Adifferent%20distribution%2C%20requiring%20users%20to%20manually%20annotate%20nuclei%20and%20retrain%0Adata-specific%20models.%20Vision%20foundation%20models%20%28VFMs%29%2C%20such%20as%20the%20Segment%0AAnything%20Model%20%28SAM%29%2C%20offer%20a%20more%20robust%20alternative%20for%20automatic%20and%0Ainteractive%20segmentation.%20Despite%20their%20success%20in%20natural%20images%2C%20a%20foundation%0Amodel%20for%20nucleus%20segmentation%20in%20histopathology%20is%20still%20missing.%20Initial%0Aefforts%20to%20adapt%20SAM%20have%20shown%20some%20success%2C%20but%20did%20not%20yet%20introduce%20a%0Acomprehensive%20model%20for%20diverse%20segmentation%20tasks.%20To%20close%20this%20gap%2C%20we%0Aintroduce%20PathoSAM%2C%20a%20VFM%20for%20nucleus%20segmentation%2C%20based%20on%20training%20SAM%20on%20a%0Adiverse%20dataset.%20Our%20extensive%20experiments%20show%20that%20it%20is%20the%20new%0Astate-of-the-art%20model%20for%20automatic%20and%20interactive%20nucleus%20instance%0Asegmentation%20in%20histopathology.%20We%20also%20demonstrate%20how%20it%20can%20be%20adapted%20for%0Aother%20segmentation%20tasks%2C%20including%20semantic%20nucleus%20segmentation.%20For%20this%0Atask%2C%20we%20show%20that%20it%20yields%20results%20better%20than%20popular%20methods%2C%20while%20not%20yet%0Abeating%20the%20state-of-the-art%2C%20CellViT.%20Our%20models%20are%20open-source%20and%0Acompatible%20with%20popular%20tools%20for%20data%20annotation.%20We%20also%20provide%20scripts%20for%0Awhole-slide%20image%20segmentation.%20Our%20code%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/computational-cell-analytics/patho-sam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00408v2&entry.124074799=Read"},
{"title": "PIP: Perturbation-based Iterative Pruning for Large Language Models", "author": "Yi Cao and Wei-Jie Xu and Yucheng Shen and Weijie Shi and Chi-Min Chan and Jianfeng Qu and Jiajie Xu", "abstract": "  The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources.\n", "link": "http://arxiv.org/abs/2501.15278v2", "date": "2025-06-02", "relevancy": 2.3099, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIP%3A%20Perturbation-based%20Iterative%20Pruning%20for%20Large%20Language%20Models&body=Title%3A%20PIP%3A%20Perturbation-based%20Iterative%20Pruning%20for%20Large%20Language%20Models%0AAuthor%3A%20Yi%20Cao%20and%20Wei-Jie%20Xu%20and%20Yucheng%20Shen%20and%20Weijie%20Shi%20and%20Chi-Min%20Chan%20and%20Jianfeng%20Qu%20and%20Jiajie%20Xu%0AAbstract%3A%20%20%20The%20rapid%20increase%20in%20the%20parameter%20counts%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Areaching%20billions%20or%20even%20trillions%2C%20presents%20significant%20challenges%20for%20their%0Apractical%20deployment%2C%20particularly%20in%20resource-constrained%20environments.%20To%0Aease%20this%20issue%2C%20we%20propose%20PIP%20%28Perturbation-based%20Iterative%20Pruning%29%2C%20a%20novel%0Adouble-view%20structured%20pruning%20method%20to%20optimize%20LLMs%2C%20which%20combines%0Ainformation%20from%20two%20different%20views%3A%20the%20unperturbed%20view%20and%20the%20perturbed%0Aview.%20With%20the%20calculation%20of%20gradient%20differences%2C%20PIP%20iteratively%20prunes%0Athose%20that%20struggle%20to%20distinguish%20between%20these%20two%20views.%20Our%20experiments%0Ashow%20that%20PIP%20reduces%20the%20parameter%20count%20by%20approximately%2020%25%20while%20retaining%0Aover%2085%25%20of%20the%20original%20model%27s%20accuracy%20across%20varied%20benchmarks.%20In%20some%0Acases%2C%20the%20performance%20of%20the%20pruned%20model%20is%20within%205%25%20of%20the%20unpruned%0Aversion%2C%20demonstrating%20PIP%27s%20ability%20to%20preserve%20key%20aspects%20of%20model%0Aeffectiveness.%20Moreover%2C%20PIP%20consistently%20outperforms%20existing%20state-of-the-art%0A%28SOTA%29%20structured%20pruning%20methods%2C%20establishing%20it%20as%20a%20leading%20technique%20for%0Aoptimizing%20LLMs%20in%20environments%20with%20constrained%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIP%253A%2520Perturbation-based%2520Iterative%2520Pruning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYi%2520Cao%2520and%2520Wei-Jie%2520Xu%2520and%2520Yucheng%2520Shen%2520and%2520Weijie%2520Shi%2520and%2520Chi-Min%2520Chan%2520and%2520Jianfeng%2520Qu%2520and%2520Jiajie%2520Xu%26entry.1292438233%3D%2520%2520The%2520rapid%2520increase%2520in%2520the%2520parameter%2520counts%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Areaching%2520billions%2520or%2520even%2520trillions%252C%2520presents%2520significant%2520challenges%2520for%2520their%250Apractical%2520deployment%252C%2520particularly%2520in%2520resource-constrained%2520environments.%2520To%250Aease%2520this%2520issue%252C%2520we%2520propose%2520PIP%2520%2528Perturbation-based%2520Iterative%2520Pruning%2529%252C%2520a%2520novel%250Adouble-view%2520structured%2520pruning%2520method%2520to%2520optimize%2520LLMs%252C%2520which%2520combines%250Ainformation%2520from%2520two%2520different%2520views%253A%2520the%2520unperturbed%2520view%2520and%2520the%2520perturbed%250Aview.%2520With%2520the%2520calculation%2520of%2520gradient%2520differences%252C%2520PIP%2520iteratively%2520prunes%250Athose%2520that%2520struggle%2520to%2520distinguish%2520between%2520these%2520two%2520views.%2520Our%2520experiments%250Ashow%2520that%2520PIP%2520reduces%2520the%2520parameter%2520count%2520by%2520approximately%252020%2525%2520while%2520retaining%250Aover%252085%2525%2520of%2520the%2520original%2520model%2527s%2520accuracy%2520across%2520varied%2520benchmarks.%2520In%2520some%250Acases%252C%2520the%2520performance%2520of%2520the%2520pruned%2520model%2520is%2520within%25205%2525%2520of%2520the%2520unpruned%250Aversion%252C%2520demonstrating%2520PIP%2527s%2520ability%2520to%2520preserve%2520key%2520aspects%2520of%2520model%250Aeffectiveness.%2520Moreover%252C%2520PIP%2520consistently%2520outperforms%2520existing%2520state-of-the-art%250A%2528SOTA%2529%2520structured%2520pruning%2520methods%252C%2520establishing%2520it%2520as%2520a%2520leading%2520technique%2520for%250Aoptimizing%2520LLMs%2520in%2520environments%2520with%2520constrained%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIP%3A%20Perturbation-based%20Iterative%20Pruning%20for%20Large%20Language%20Models&entry.906535625=Yi%20Cao%20and%20Wei-Jie%20Xu%20and%20Yucheng%20Shen%20and%20Weijie%20Shi%20and%20Chi-Min%20Chan%20and%20Jianfeng%20Qu%20and%20Jiajie%20Xu&entry.1292438233=%20%20The%20rapid%20increase%20in%20the%20parameter%20counts%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Areaching%20billions%20or%20even%20trillions%2C%20presents%20significant%20challenges%20for%20their%0Apractical%20deployment%2C%20particularly%20in%20resource-constrained%20environments.%20To%0Aease%20this%20issue%2C%20we%20propose%20PIP%20%28Perturbation-based%20Iterative%20Pruning%29%2C%20a%20novel%0Adouble-view%20structured%20pruning%20method%20to%20optimize%20LLMs%2C%20which%20combines%0Ainformation%20from%20two%20different%20views%3A%20the%20unperturbed%20view%20and%20the%20perturbed%0Aview.%20With%20the%20calculation%20of%20gradient%20differences%2C%20PIP%20iteratively%20prunes%0Athose%20that%20struggle%20to%20distinguish%20between%20these%20two%20views.%20Our%20experiments%0Ashow%20that%20PIP%20reduces%20the%20parameter%20count%20by%20approximately%2020%25%20while%20retaining%0Aover%2085%25%20of%20the%20original%20model%27s%20accuracy%20across%20varied%20benchmarks.%20In%20some%0Acases%2C%20the%20performance%20of%20the%20pruned%20model%20is%20within%205%25%20of%20the%20unpruned%0Aversion%2C%20demonstrating%20PIP%27s%20ability%20to%20preserve%20key%20aspects%20of%20model%0Aeffectiveness.%20Moreover%2C%20PIP%20consistently%20outperforms%20existing%20state-of-the-art%0A%28SOTA%29%20structured%20pruning%20methods%2C%20establishing%20it%20as%20a%20leading%20technique%20for%0Aoptimizing%20LLMs%20in%20environments%20with%20constrained%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15278v2&entry.124074799=Read"},
{"title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An\n  Encoder Decoder Perspective", "author": "Seungwook Han and Jinyeop Song and Jeff Gore and Pulkit Agrawal", "abstract": "  Autoregressive transformers exhibit adaptive learning through in-context\nlearning (ICL), which begs the question of how. Prior works have shown that\ntransformers represent the ICL tasks as vectors in their representations. In\nthis paper, we leverage the encoding-decoding framework to study how\ntransformers form task vectors during pretraining and how their task encoding\nquality predicts ICL task performance. On synthetic ICL tasks, we analyze the\ntraining dynamics of a small transformer and report the coupled emergence of\ntask encoding and decoding. As the model learns to encode different latent\ntasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concurrently builds conditional decoding algorithms and\nimproves its ICL performance. We validate this phenomenon across pretrained\nmodels of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the\ncourse of pretraining in OLMo-7B. Further, we demonstrate that the quality of\ntask encoding inferred from representations predicts ICL performance, and that,\nsurprisingly, finetuning the earlier layers can improve the task encoding and\nperformance more than finetuning the latter layers. Our empirical insights shed\nlight into better understanding the success and failure modes of large language\nmodels via their representations.\n", "link": "http://arxiv.org/abs/2412.12276v3", "date": "2025-06-02", "relevancy": 2.308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20and%20Effectiveness%20of%20Task%20Vectors%20in%20In-Context%20Learning%3A%20An%0A%20%20Encoder%20Decoder%20Perspective&body=Title%3A%20Emergence%20and%20Effectiveness%20of%20Task%20Vectors%20in%20In-Context%20Learning%3A%20An%0A%20%20Encoder%20Decoder%20Perspective%0AAuthor%3A%20Seungwook%20Han%20and%20Jinyeop%20Song%20and%20Jeff%20Gore%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Autoregressive%20transformers%20exhibit%20adaptive%20learning%20through%20in-context%0Alearning%20%28ICL%29%2C%20which%20begs%20the%20question%20of%20how.%20Prior%20works%20have%20shown%20that%0Atransformers%20represent%20the%20ICL%20tasks%20as%20vectors%20in%20their%20representations.%20In%0Athis%20paper%2C%20we%20leverage%20the%20encoding-decoding%20framework%20to%20study%20how%0Atransformers%20form%20task%20vectors%20during%20pretraining%20and%20how%20their%20task%20encoding%0Aquality%20predicts%20ICL%20task%20performance.%20On%20synthetic%20ICL%20tasks%2C%20we%20analyze%20the%0Atraining%20dynamics%20of%20a%20small%20transformer%20and%20report%20the%20coupled%20emergence%20of%0Atask%20encoding%20and%20decoding.%20As%20the%20model%20learns%20to%20encode%20different%20latent%0Atasks%20%28e.g.%2C%20%22Finding%20the%20first%20noun%20in%20a%20sentence.%22%29%20into%20distinct%2C%20separable%0Arepresentations%2C%20it%20concurrently%20builds%20conditional%20decoding%20algorithms%20and%0Aimproves%20its%20ICL%20performance.%20We%20validate%20this%20phenomenon%20across%20pretrained%0Amodels%20of%20varying%20scales%20%28Gemma-2%202B/9B/27B%2C%20Llama-3.1%208B/70B%29%20and%20over%20the%0Acourse%20of%20pretraining%20in%20OLMo-7B.%20Further%2C%20we%20demonstrate%20that%20the%20quality%20of%0Atask%20encoding%20inferred%20from%20representations%20predicts%20ICL%20performance%2C%20and%20that%2C%0Asurprisingly%2C%20finetuning%20the%20earlier%20layers%20can%20improve%20the%20task%20encoding%20and%0Aperformance%20more%20than%20finetuning%20the%20latter%20layers.%20Our%20empirical%20insights%20shed%0Alight%20into%20better%20understanding%20the%20success%20and%20failure%20modes%20of%20large%20language%0Amodels%20via%20their%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520and%2520Effectiveness%2520of%2520Task%2520Vectors%2520in%2520In-Context%2520Learning%253A%2520An%250A%2520%2520Encoder%2520Decoder%2520Perspective%26entry.906535625%3DSeungwook%2520Han%2520and%2520Jinyeop%2520Song%2520and%2520Jeff%2520Gore%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Autoregressive%2520transformers%2520exhibit%2520adaptive%2520learning%2520through%2520in-context%250Alearning%2520%2528ICL%2529%252C%2520which%2520begs%2520the%2520question%2520of%2520how.%2520Prior%2520works%2520have%2520shown%2520that%250Atransformers%2520represent%2520the%2520ICL%2520tasks%2520as%2520vectors%2520in%2520their%2520representations.%2520In%250Athis%2520paper%252C%2520we%2520leverage%2520the%2520encoding-decoding%2520framework%2520to%2520study%2520how%250Atransformers%2520form%2520task%2520vectors%2520during%2520pretraining%2520and%2520how%2520their%2520task%2520encoding%250Aquality%2520predicts%2520ICL%2520task%2520performance.%2520On%2520synthetic%2520ICL%2520tasks%252C%2520we%2520analyze%2520the%250Atraining%2520dynamics%2520of%2520a%2520small%2520transformer%2520and%2520report%2520the%2520coupled%2520emergence%2520of%250Atask%2520encoding%2520and%2520decoding.%2520As%2520the%2520model%2520learns%2520to%2520encode%2520different%2520latent%250Atasks%2520%2528e.g.%252C%2520%2522Finding%2520the%2520first%2520noun%2520in%2520a%2520sentence.%2522%2529%2520into%2520distinct%252C%2520separable%250Arepresentations%252C%2520it%2520concurrently%2520builds%2520conditional%2520decoding%2520algorithms%2520and%250Aimproves%2520its%2520ICL%2520performance.%2520We%2520validate%2520this%2520phenomenon%2520across%2520pretrained%250Amodels%2520of%2520varying%2520scales%2520%2528Gemma-2%25202B/9B/27B%252C%2520Llama-3.1%25208B/70B%2529%2520and%2520over%2520the%250Acourse%2520of%2520pretraining%2520in%2520OLMo-7B.%2520Further%252C%2520we%2520demonstrate%2520that%2520the%2520quality%2520of%250Atask%2520encoding%2520inferred%2520from%2520representations%2520predicts%2520ICL%2520performance%252C%2520and%2520that%252C%250Asurprisingly%252C%2520finetuning%2520the%2520earlier%2520layers%2520can%2520improve%2520the%2520task%2520encoding%2520and%250Aperformance%2520more%2520than%2520finetuning%2520the%2520latter%2520layers.%2520Our%2520empirical%2520insights%2520shed%250Alight%2520into%2520better%2520understanding%2520the%2520success%2520and%2520failure%2520modes%2520of%2520large%2520language%250Amodels%2520via%2520their%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20and%20Effectiveness%20of%20Task%20Vectors%20in%20In-Context%20Learning%3A%20An%0A%20%20Encoder%20Decoder%20Perspective&entry.906535625=Seungwook%20Han%20and%20Jinyeop%20Song%20and%20Jeff%20Gore%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Autoregressive%20transformers%20exhibit%20adaptive%20learning%20through%20in-context%0Alearning%20%28ICL%29%2C%20which%20begs%20the%20question%20of%20how.%20Prior%20works%20have%20shown%20that%0Atransformers%20represent%20the%20ICL%20tasks%20as%20vectors%20in%20their%20representations.%20In%0Athis%20paper%2C%20we%20leverage%20the%20encoding-decoding%20framework%20to%20study%20how%0Atransformers%20form%20task%20vectors%20during%20pretraining%20and%20how%20their%20task%20encoding%0Aquality%20predicts%20ICL%20task%20performance.%20On%20synthetic%20ICL%20tasks%2C%20we%20analyze%20the%0Atraining%20dynamics%20of%20a%20small%20transformer%20and%20report%20the%20coupled%20emergence%20of%0Atask%20encoding%20and%20decoding.%20As%20the%20model%20learns%20to%20encode%20different%20latent%0Atasks%20%28e.g.%2C%20%22Finding%20the%20first%20noun%20in%20a%20sentence.%22%29%20into%20distinct%2C%20separable%0Arepresentations%2C%20it%20concurrently%20builds%20conditional%20decoding%20algorithms%20and%0Aimproves%20its%20ICL%20performance.%20We%20validate%20this%20phenomenon%20across%20pretrained%0Amodels%20of%20varying%20scales%20%28Gemma-2%202B/9B/27B%2C%20Llama-3.1%208B/70B%29%20and%20over%20the%0Acourse%20of%20pretraining%20in%20OLMo-7B.%20Further%2C%20we%20demonstrate%20that%20the%20quality%20of%0Atask%20encoding%20inferred%20from%20representations%20predicts%20ICL%20performance%2C%20and%20that%2C%0Asurprisingly%2C%20finetuning%20the%20earlier%20layers%20can%20improve%20the%20task%20encoding%20and%0Aperformance%20more%20than%20finetuning%20the%20latter%20layers.%20Our%20empirical%20insights%20shed%0Alight%20into%20better%20understanding%20the%20success%20and%20failure%20modes%20of%20large%20language%0Amodels%20via%20their%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12276v3&entry.124074799=Read"},
{"title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?", "author": "Kexian Tang and Junyao Gao and Yanhong Zeng and Haodong Duan and Yanan Sun and Zhening Xing and Wenran Liu and Kaifeng Lyu and Kai Chen", "abstract": "  Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed to\nevaluate both spatial understanding and sequential reasoning in MLLMs through\nLEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual\nquestion-answering (VQA) samples spanning 11 distinct tasks, ranging from basic\nspatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles,\nwe conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover\nsignificant limitations in their spatial reasoning capabilities: even the most\npowerful MLLMs can answer only about half of the test cases, whereas human\nparticipants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we\ndesign generation tasks to investigate whether MLLMs can transfer their spatial\nunderstanding and reasoning abilities to image generation. Our experiments show\nthat only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.\n", "link": "http://arxiv.org/abs/2503.19990v2", "date": "2025-06-02", "relevancy": 2.2376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEGO-Puzzles%3A%20How%20Good%20Are%20MLLMs%20at%20Multi-Step%20Spatial%20Reasoning%3F&body=Title%3A%20LEGO-Puzzles%3A%20How%20Good%20Are%20MLLMs%20at%20Multi-Step%20Spatial%20Reasoning%3F%0AAuthor%3A%20Kexian%20Tang%20and%20Junyao%20Gao%20and%20Yanhong%20Zeng%20and%20Haodong%20Duan%20and%20Yanan%20Sun%20and%20Zhening%20Xing%20and%20Wenran%20Liu%20and%20Kaifeng%20Lyu%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Multi-step%20spatial%20reasoning%20entails%20understanding%20and%20reasoning%20about%0Aspatial%20relationships%20across%20multiple%20sequential%20steps%2C%20which%20is%20crucial%20for%0Atackling%20complex%20real-world%20applications%2C%20such%20as%20robotic%20manipulation%2C%0Aautonomous%20navigation%2C%20and%20automated%20assembly.%20To%20assess%20how%20well%20current%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20acquired%20this%20fundamental%0Acapability%2C%20we%20introduce%20LEGO-Puzzles%2C%20a%20scalable%20benchmark%20designed%20to%0Aevaluate%20both%20spatial%20understanding%20and%20sequential%20reasoning%20in%20MLLMs%20through%0ALEGO-based%20tasks.%20LEGO-Puzzles%20consists%20of%201%2C100%20carefully%20curated%20visual%0Aquestion-answering%20%28VQA%29%20samples%20spanning%2011%20distinct%20tasks%2C%20ranging%20from%20basic%0Aspatial%20understanding%20to%20complex%20multi-step%20reasoning.%20Based%20on%20LEGO-Puzzles%2C%0Awe%20conduct%20a%20comprehensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%20and%20uncover%0Asignificant%20limitations%20in%20their%20spatial%20reasoning%20capabilities%3A%20even%20the%20most%0Apowerful%20MLLMs%20can%20answer%20only%20about%20half%20of%20the%20test%20cases%2C%20whereas%20human%0Aparticipants%20achieve%20over%2090%25%20accuracy.%20Furthermore%2C%20based%20on%20LEGO-Puzzles%2C%20we%0Adesign%20generation%20tasks%20to%20investigate%20whether%20MLLMs%20can%20transfer%20their%20spatial%0Aunderstanding%20and%20reasoning%20abilities%20to%20image%20generation.%20Our%20experiments%20show%0Athat%20only%20GPT-4o%20and%20Gemini-2.0-Flash%20exhibit%20a%20limited%20ability%20to%20follow%20these%0Ainstructions%2C%20while%20other%20MLLMs%20either%20replicate%20the%20input%20image%20or%20generate%0Acompletely%20irrelevant%20outputs.%20Overall%2C%20LEGO-Puzzles%20exposes%20critical%0Adeficiencies%20in%20existing%20MLLMs%27%20spatial%20understanding%20and%20sequential%20reasoning%0Acapabilities%2C%20and%20underscores%20the%20need%20for%20further%20advancements%20in%20multimodal%0Aspatial%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEGO-Puzzles%253A%2520How%2520Good%2520Are%2520MLLMs%2520at%2520Multi-Step%2520Spatial%2520Reasoning%253F%26entry.906535625%3DKexian%2520Tang%2520and%2520Junyao%2520Gao%2520and%2520Yanhong%2520Zeng%2520and%2520Haodong%2520Duan%2520and%2520Yanan%2520Sun%2520and%2520Zhening%2520Xing%2520and%2520Wenran%2520Liu%2520and%2520Kaifeng%2520Lyu%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Multi-step%2520spatial%2520reasoning%2520entails%2520understanding%2520and%2520reasoning%2520about%250Aspatial%2520relationships%2520across%2520multiple%2520sequential%2520steps%252C%2520which%2520is%2520crucial%2520for%250Atackling%2520complex%2520real-world%2520applications%252C%2520such%2520as%2520robotic%2520manipulation%252C%250Aautonomous%2520navigation%252C%2520and%2520automated%2520assembly.%2520To%2520assess%2520how%2520well%2520current%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520acquired%2520this%2520fundamental%250Acapability%252C%2520we%2520introduce%2520LEGO-Puzzles%252C%2520a%2520scalable%2520benchmark%2520designed%2520to%250Aevaluate%2520both%2520spatial%2520understanding%2520and%2520sequential%2520reasoning%2520in%2520MLLMs%2520through%250ALEGO-based%2520tasks.%2520LEGO-Puzzles%2520consists%2520of%25201%252C100%2520carefully%2520curated%2520visual%250Aquestion-answering%2520%2528VQA%2529%2520samples%2520spanning%252011%2520distinct%2520tasks%252C%2520ranging%2520from%2520basic%250Aspatial%2520understanding%2520to%2520complex%2520multi-step%2520reasoning.%2520Based%2520on%2520LEGO-Puzzles%252C%250Awe%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%252020%2520state-of-the-art%2520MLLMs%2520and%2520uncover%250Asignificant%2520limitations%2520in%2520their%2520spatial%2520reasoning%2520capabilities%253A%2520even%2520the%2520most%250Apowerful%2520MLLMs%2520can%2520answer%2520only%2520about%2520half%2520of%2520the%2520test%2520cases%252C%2520whereas%2520human%250Aparticipants%2520achieve%2520over%252090%2525%2520accuracy.%2520Furthermore%252C%2520based%2520on%2520LEGO-Puzzles%252C%2520we%250Adesign%2520generation%2520tasks%2520to%2520investigate%2520whether%2520MLLMs%2520can%2520transfer%2520their%2520spatial%250Aunderstanding%2520and%2520reasoning%2520abilities%2520to%2520image%2520generation.%2520Our%2520experiments%2520show%250Athat%2520only%2520GPT-4o%2520and%2520Gemini-2.0-Flash%2520exhibit%2520a%2520limited%2520ability%2520to%2520follow%2520these%250Ainstructions%252C%2520while%2520other%2520MLLMs%2520either%2520replicate%2520the%2520input%2520image%2520or%2520generate%250Acompletely%2520irrelevant%2520outputs.%2520Overall%252C%2520LEGO-Puzzles%2520exposes%2520critical%250Adeficiencies%2520in%2520existing%2520MLLMs%2527%2520spatial%2520understanding%2520and%2520sequential%2520reasoning%250Acapabilities%252C%2520and%2520underscores%2520the%2520need%2520for%2520further%2520advancements%2520in%2520multimodal%250Aspatial%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEGO-Puzzles%3A%20How%20Good%20Are%20MLLMs%20at%20Multi-Step%20Spatial%20Reasoning%3F&entry.906535625=Kexian%20Tang%20and%20Junyao%20Gao%20and%20Yanhong%20Zeng%20and%20Haodong%20Duan%20and%20Yanan%20Sun%20and%20Zhening%20Xing%20and%20Wenran%20Liu%20and%20Kaifeng%20Lyu%20and%20Kai%20Chen&entry.1292438233=%20%20Multi-step%20spatial%20reasoning%20entails%20understanding%20and%20reasoning%20about%0Aspatial%20relationships%20across%20multiple%20sequential%20steps%2C%20which%20is%20crucial%20for%0Atackling%20complex%20real-world%20applications%2C%20such%20as%20robotic%20manipulation%2C%0Aautonomous%20navigation%2C%20and%20automated%20assembly.%20To%20assess%20how%20well%20current%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20acquired%20this%20fundamental%0Acapability%2C%20we%20introduce%20LEGO-Puzzles%2C%20a%20scalable%20benchmark%20designed%20to%0Aevaluate%20both%20spatial%20understanding%20and%20sequential%20reasoning%20in%20MLLMs%20through%0ALEGO-based%20tasks.%20LEGO-Puzzles%20consists%20of%201%2C100%20carefully%20curated%20visual%0Aquestion-answering%20%28VQA%29%20samples%20spanning%2011%20distinct%20tasks%2C%20ranging%20from%20basic%0Aspatial%20understanding%20to%20complex%20multi-step%20reasoning.%20Based%20on%20LEGO-Puzzles%2C%0Awe%20conduct%20a%20comprehensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%20and%20uncover%0Asignificant%20limitations%20in%20their%20spatial%20reasoning%20capabilities%3A%20even%20the%20most%0Apowerful%20MLLMs%20can%20answer%20only%20about%20half%20of%20the%20test%20cases%2C%20whereas%20human%0Aparticipants%20achieve%20over%2090%25%20accuracy.%20Furthermore%2C%20based%20on%20LEGO-Puzzles%2C%20we%0Adesign%20generation%20tasks%20to%20investigate%20whether%20MLLMs%20can%20transfer%20their%20spatial%0Aunderstanding%20and%20reasoning%20abilities%20to%20image%20generation.%20Our%20experiments%20show%0Athat%20only%20GPT-4o%20and%20Gemini-2.0-Flash%20exhibit%20a%20limited%20ability%20to%20follow%20these%0Ainstructions%2C%20while%20other%20MLLMs%20either%20replicate%20the%20input%20image%20or%20generate%0Acompletely%20irrelevant%20outputs.%20Overall%2C%20LEGO-Puzzles%20exposes%20critical%0Adeficiencies%20in%20existing%20MLLMs%27%20spatial%20understanding%20and%20sequential%20reasoning%0Acapabilities%2C%20and%20underscores%20the%20need%20for%20further%20advancements%20in%20multimodal%0Aspatial%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19990v2&entry.124074799=Read"},
{"title": "I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue", "author": "Esam Ghaleb and Bulat Khaertdinov and Asl\u0131 \u00d6zy\u00fcrek and Raquel Fern\u00e1ndez", "abstract": "  In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction.\n", "link": "http://arxiv.org/abs/2503.00071v2", "date": "2025-06-02", "relevancy": 2.1941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20see%20what%20you%20mean%3A%20Co-Speech%20Gestures%20for%20Reference%20Resolution%20in%0A%20%20Multimodal%20Dialogue&body=Title%3A%20I%20see%20what%20you%20mean%3A%20Co-Speech%20Gestures%20for%20Reference%20Resolution%20in%0A%20%20Multimodal%20Dialogue%0AAuthor%3A%20Esam%20Ghaleb%20and%20Bulat%20Khaertdinov%20and%20Asl%C4%B1%20%C3%96zy%C3%BCrek%20and%20Raquel%20Fern%C3%A1ndez%0AAbstract%3A%20%20%20In%20face-to-face%20interaction%2C%20we%20use%20multiple%20modalities%2C%20including%20speech%20and%0Agestures%2C%20to%20communicate%20information%20and%20resolve%20references%20to%20objects.%0AHowever%2C%20how%20representational%20co-speech%20gestures%20refer%20to%20objects%20remains%0Aunderstudied%20from%20a%20computational%20perspective.%20In%20this%20work%2C%20we%20address%20this%0Agap%20by%20introducing%20a%20multimodal%20reference%20resolution%20task%20centred%20on%0Arepresentational%20gestures%2C%20while%20simultaneously%20tackling%20the%20challenge%20of%0Alearning%20robust%20gesture%20embeddings.%20We%20propose%20a%20self-supervised%20pre-training%0Aapproach%20to%20gesture%20representation%20learning%20that%20grounds%20body%20movements%20in%0Aspoken%20language.%20Our%20experiments%20show%20that%20the%20learned%20embeddings%20align%20with%0Aexpert%20annotations%20and%20have%20significant%20predictive%20power.%20Moreover%2C%20reference%0Aresolution%20accuracy%20further%20improves%20when%20%281%29%20using%20multimodal%20gesture%0Arepresentations%2C%20even%20when%20speech%20is%20unavailable%20at%20inference%20time%2C%20and%20%282%29%0Aleveraging%20dialogue%20history.%20Overall%2C%20our%20findings%20highlight%20the%20complementary%0Aroles%20of%20gesture%20and%20speech%20in%20reference%20resolution%2C%20offering%20a%20step%20towards%0Amore%20naturalistic%20models%20of%20human-machine%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520see%2520what%2520you%2520mean%253A%2520Co-Speech%2520Gestures%2520for%2520Reference%2520Resolution%2520in%250A%2520%2520Multimodal%2520Dialogue%26entry.906535625%3DEsam%2520Ghaleb%2520and%2520Bulat%2520Khaertdinov%2520and%2520Asl%25C4%25B1%2520%25C3%2596zy%25C3%25BCrek%2520and%2520Raquel%2520Fern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520In%2520face-to-face%2520interaction%252C%2520we%2520use%2520multiple%2520modalities%252C%2520including%2520speech%2520and%250Agestures%252C%2520to%2520communicate%2520information%2520and%2520resolve%2520references%2520to%2520objects.%250AHowever%252C%2520how%2520representational%2520co-speech%2520gestures%2520refer%2520to%2520objects%2520remains%250Aunderstudied%2520from%2520a%2520computational%2520perspective.%2520In%2520this%2520work%252C%2520we%2520address%2520this%250Agap%2520by%2520introducing%2520a%2520multimodal%2520reference%2520resolution%2520task%2520centred%2520on%250Arepresentational%2520gestures%252C%2520while%2520simultaneously%2520tackling%2520the%2520challenge%2520of%250Alearning%2520robust%2520gesture%2520embeddings.%2520We%2520propose%2520a%2520self-supervised%2520pre-training%250Aapproach%2520to%2520gesture%2520representation%2520learning%2520that%2520grounds%2520body%2520movements%2520in%250Aspoken%2520language.%2520Our%2520experiments%2520show%2520that%2520the%2520learned%2520embeddings%2520align%2520with%250Aexpert%2520annotations%2520and%2520have%2520significant%2520predictive%2520power.%2520Moreover%252C%2520reference%250Aresolution%2520accuracy%2520further%2520improves%2520when%2520%25281%2529%2520using%2520multimodal%2520gesture%250Arepresentations%252C%2520even%2520when%2520speech%2520is%2520unavailable%2520at%2520inference%2520time%252C%2520and%2520%25282%2529%250Aleveraging%2520dialogue%2520history.%2520Overall%252C%2520our%2520findings%2520highlight%2520the%2520complementary%250Aroles%2520of%2520gesture%2520and%2520speech%2520in%2520reference%2520resolution%252C%2520offering%2520a%2520step%2520towards%250Amore%2520naturalistic%2520models%2520of%2520human-machine%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20see%20what%20you%20mean%3A%20Co-Speech%20Gestures%20for%20Reference%20Resolution%20in%0A%20%20Multimodal%20Dialogue&entry.906535625=Esam%20Ghaleb%20and%20Bulat%20Khaertdinov%20and%20Asl%C4%B1%20%C3%96zy%C3%BCrek%20and%20Raquel%20Fern%C3%A1ndez&entry.1292438233=%20%20In%20face-to-face%20interaction%2C%20we%20use%20multiple%20modalities%2C%20including%20speech%20and%0Agestures%2C%20to%20communicate%20information%20and%20resolve%20references%20to%20objects.%0AHowever%2C%20how%20representational%20co-speech%20gestures%20refer%20to%20objects%20remains%0Aunderstudied%20from%20a%20computational%20perspective.%20In%20this%20work%2C%20we%20address%20this%0Agap%20by%20introducing%20a%20multimodal%20reference%20resolution%20task%20centred%20on%0Arepresentational%20gestures%2C%20while%20simultaneously%20tackling%20the%20challenge%20of%0Alearning%20robust%20gesture%20embeddings.%20We%20propose%20a%20self-supervised%20pre-training%0Aapproach%20to%20gesture%20representation%20learning%20that%20grounds%20body%20movements%20in%0Aspoken%20language.%20Our%20experiments%20show%20that%20the%20learned%20embeddings%20align%20with%0Aexpert%20annotations%20and%20have%20significant%20predictive%20power.%20Moreover%2C%20reference%0Aresolution%20accuracy%20further%20improves%20when%20%281%29%20using%20multimodal%20gesture%0Arepresentations%2C%20even%20when%20speech%20is%20unavailable%20at%20inference%20time%2C%20and%20%282%29%0Aleveraging%20dialogue%20history.%20Overall%2C%20our%20findings%20highlight%20the%20complementary%0Aroles%20of%20gesture%20and%20speech%20in%20reference%20resolution%2C%20offering%20a%20step%20towards%0Amore%20naturalistic%20models%20of%20human-machine%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00071v2&entry.124074799=Read"},
{"title": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks", "author": "Sanja Karilanova and Maxime Fabre and Emre Neftci and Ay\u00e7a \u00d6z\u00e7elikkale", "abstract": "  Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation.\n", "link": "http://arxiv.org/abs/2411.04760v2", "date": "2025-06-02", "relevancy": 2.135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5598}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Sanja%20Karilanova%20and%20Maxime%20Fabre%20and%20Emre%20Neftci%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically-inspired%20deep%20neural%20networks%0Athat%20efficiently%20extract%20temporal%20information%20while%20offering%20promising%20gains%20in%0Aterms%20of%20energy%20efficiency%20and%20latency%20when%20deployed%20on%20neuromorphic%20devices.%0AHowever%2C%20SNN%20model%20parameters%20are%20sensitive%20to%20temporal%20resolution%2C%20leading%20to%0Asignificant%20performance%20drops%20when%20the%20temporal%20resolution%20of%20target%20data%20at%0Athe%20edge%20is%20not%20the%20same%20with%20that%20of%20the%20pre-deployment%20source%20data%20used%20for%0Atraining%2C%20especially%20when%20fine-tuning%20is%20not%20possible%20at%20the%20edge.%20To%20address%0Athis%20challenge%2C%20we%20propose%20three%20novel%20domain%20adaptation%20methods%20for%20adapting%0Aneuron%20parameters%20to%20account%20for%20the%20change%20in%20time%20resolution%20without%0Are-training%20on%20target%20time-resolution.%20The%20proposed%20methods%20are%20based%20on%20a%0Amapping%20between%20neuron%20dynamics%20in%20SNNs%20and%20State%20Space%20Models%20%28SSMs%29%3B%20and%20are%0Aapplicable%20to%20general%20neuron%20models.%20We%20evaluate%20the%20proposed%20methods%20under%0Aspatio-temporal%20data%20tasks%2C%20namely%20the%20audio%20keyword%20spotting%20datasets%20SHD%20and%0AMSWC%20as%20well%20as%20the%20image%20classification%20NMINST%20dataset.%20Our%20methods%20provide%20an%0Aalternative%20to%20-%20and%20in%20majority%20of%20the%20cases%20significantly%20outperform%20-%20the%0Aexisting%20reference%20method%20that%20simply%20scales%20the%20time%20constant.%20Moreover%2C%20our%0Aresults%20show%20that%20high%20accuracy%20on%20high%20temporal%20resolution%20data%20can%20be%0Aobtained%20by%20time%20efficient%20training%20on%20lower%20temporal%20resolution%20data%20and%20model%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Temporal%2520Resolution%2520Domain%2520Adaptation%2520for%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSanja%2520Karilanova%2520and%2520Maxime%2520Fabre%2520and%2520Emre%2520Neftci%2520and%2520Ay%25C3%25A7a%2520%25C3%2596z%25C3%25A7elikkale%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520biologically-inspired%2520deep%2520neural%2520networks%250Athat%2520efficiently%2520extract%2520temporal%2520information%2520while%2520offering%2520promising%2520gains%2520in%250Aterms%2520of%2520energy%2520efficiency%2520and%2520latency%2520when%2520deployed%2520on%2520neuromorphic%2520devices.%250AHowever%252C%2520SNN%2520model%2520parameters%2520are%2520sensitive%2520to%2520temporal%2520resolution%252C%2520leading%2520to%250Asignificant%2520performance%2520drops%2520when%2520the%2520temporal%2520resolution%2520of%2520target%2520data%2520at%250Athe%2520edge%2520is%2520not%2520the%2520same%2520with%2520that%2520of%2520the%2520pre-deployment%2520source%2520data%2520used%2520for%250Atraining%252C%2520especially%2520when%2520fine-tuning%2520is%2520not%2520possible%2520at%2520the%2520edge.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520three%2520novel%2520domain%2520adaptation%2520methods%2520for%2520adapting%250Aneuron%2520parameters%2520to%2520account%2520for%2520the%2520change%2520in%2520time%2520resolution%2520without%250Are-training%2520on%2520target%2520time-resolution.%2520The%2520proposed%2520methods%2520are%2520based%2520on%2520a%250Amapping%2520between%2520neuron%2520dynamics%2520in%2520SNNs%2520and%2520State%2520Space%2520Models%2520%2528SSMs%2529%253B%2520and%2520are%250Aapplicable%2520to%2520general%2520neuron%2520models.%2520We%2520evaluate%2520the%2520proposed%2520methods%2520under%250Aspatio-temporal%2520data%2520tasks%252C%2520namely%2520the%2520audio%2520keyword%2520spotting%2520datasets%2520SHD%2520and%250AMSWC%2520as%2520well%2520as%2520the%2520image%2520classification%2520NMINST%2520dataset.%2520Our%2520methods%2520provide%2520an%250Aalternative%2520to%2520-%2520and%2520in%2520majority%2520of%2520the%2520cases%2520significantly%2520outperform%2520-%2520the%250Aexisting%2520reference%2520method%2520that%2520simply%2520scales%2520the%2520time%2520constant.%2520Moreover%252C%2520our%250Aresults%2520show%2520that%2520high%2520accuracy%2520on%2520high%2520temporal%2520resolution%2520data%2520can%2520be%250Aobtained%2520by%2520time%2520efficient%2520training%2520on%2520lower%2520temporal%2520resolution%2520data%2520and%2520model%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Sanja%20Karilanova%20and%20Maxime%20Fabre%20and%20Emre%20Neftci%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically-inspired%20deep%20neural%20networks%0Athat%20efficiently%20extract%20temporal%20information%20while%20offering%20promising%20gains%20in%0Aterms%20of%20energy%20efficiency%20and%20latency%20when%20deployed%20on%20neuromorphic%20devices.%0AHowever%2C%20SNN%20model%20parameters%20are%20sensitive%20to%20temporal%20resolution%2C%20leading%20to%0Asignificant%20performance%20drops%20when%20the%20temporal%20resolution%20of%20target%20data%20at%0Athe%20edge%20is%20not%20the%20same%20with%20that%20of%20the%20pre-deployment%20source%20data%20used%20for%0Atraining%2C%20especially%20when%20fine-tuning%20is%20not%20possible%20at%20the%20edge.%20To%20address%0Athis%20challenge%2C%20we%20propose%20three%20novel%20domain%20adaptation%20methods%20for%20adapting%0Aneuron%20parameters%20to%20account%20for%20the%20change%20in%20time%20resolution%20without%0Are-training%20on%20target%20time-resolution.%20The%20proposed%20methods%20are%20based%20on%20a%0Amapping%20between%20neuron%20dynamics%20in%20SNNs%20and%20State%20Space%20Models%20%28SSMs%29%3B%20and%20are%0Aapplicable%20to%20general%20neuron%20models.%20We%20evaluate%20the%20proposed%20methods%20under%0Aspatio-temporal%20data%20tasks%2C%20namely%20the%20audio%20keyword%20spotting%20datasets%20SHD%20and%0AMSWC%20as%20well%20as%20the%20image%20classification%20NMINST%20dataset.%20Our%20methods%20provide%20an%0Aalternative%20to%20-%20and%20in%20majority%20of%20the%20cases%20significantly%20outperform%20-%20the%0Aexisting%20reference%20method%20that%20simply%20scales%20the%20time%20constant.%20Moreover%2C%20our%0Aresults%20show%20that%20high%20accuracy%20on%20high%20temporal%20resolution%20data%20can%20be%0Aobtained%20by%20time%20efficient%20training%20on%20lower%20temporal%20resolution%20data%20and%20model%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04760v2&entry.124074799=Read"},
{"title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing", "author": "Amirhossein Zhalehmehrabi and Daniele Meli and Francesco Dal Santo and Francesco Trotti and Alessandro Farinelli", "abstract": "  Autonomous Surface Vehicles (ASVs) play a crucial role in maritime\noperations, yet their navigation in shallow-water environments remains\nchallenging due to dynamic disturbances and depth constraints. Traditional\nnavigation strategies struggle with limited sensor information, making safe and\nefficient operation difficult. In this paper, we propose a reinforcement\nlearning (RL) framework for ASV navigation under depth constraints, where the\nvehicle must reach a target while avoiding unsafe areas with only a single\ndepth measurement per timestep from a downward-facing Single Beam Echosounder\n(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)\nregression into the RL framework, enabling the agent to progressively estimate\na bathymetric depth map from sparse sonar readings. This approach improves\ndecision-making by providing a richer representation of the environment.\nFurthermore, we demonstrate effective sim-to-real transfer, ensuring that\ntrained policies generalize well to real-world aquatic conditions. Experimental\nresults validate our method's capability to improve ASV navigation performance\nwhile maintaining safety in challenging shallow-water environments.\n", "link": "http://arxiv.org/abs/2504.18253v2", "date": "2025-06-02", "relevancy": 2.0948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing&body=Title%3A%20Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing%0AAuthor%3A%20Amirhossein%20Zhalehmehrabi%20and%20Daniele%20Meli%20and%20Francesco%20Dal%20Santo%20and%20Francesco%20Trotti%20and%20Alessandro%20Farinelli%0AAbstract%3A%20%20%20Autonomous%20Surface%20Vehicles%20%28ASVs%29%20play%20a%20crucial%20role%20in%20maritime%0Aoperations%2C%20yet%20their%20navigation%20in%20shallow-water%20environments%20remains%0Achallenging%20due%20to%20dynamic%20disturbances%20and%20depth%20constraints.%20Traditional%0Anavigation%20strategies%20struggle%20with%20limited%20sensor%20information%2C%20making%20safe%20and%0Aefficient%20operation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20reinforcement%0Alearning%20%28RL%29%20framework%20for%20ASV%20navigation%20under%20depth%20constraints%2C%20where%20the%0Avehicle%20must%20reach%20a%20target%20while%20avoiding%20unsafe%20areas%20with%20only%20a%20single%0Adepth%20measurement%20per%20timestep%20from%20a%20downward-facing%20Single%20Beam%20Echosounder%0A%28SBES%29.%20To%20enhance%20environmental%20awareness%2C%20we%20integrate%20Gaussian%20Process%20%28GP%29%0Aregression%20into%20the%20RL%20framework%2C%20enabling%20the%20agent%20to%20progressively%20estimate%0Aa%20bathymetric%20depth%20map%20from%20sparse%20sonar%20readings.%20This%20approach%20improves%0Adecision-making%20by%20providing%20a%20richer%20representation%20of%20the%20environment.%0AFurthermore%2C%20we%20demonstrate%20effective%20sim-to-real%20transfer%2C%20ensuring%20that%0Atrained%20policies%20generalize%20well%20to%20real-world%20aquatic%20conditions.%20Experimental%0Aresults%20validate%20our%20method%27s%20capability%20to%20improve%20ASV%20navigation%20performance%0Awhile%20maintaining%20safety%20in%20challenging%20shallow-water%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Constrained%2520ASV%2520Navigation%2520with%2520Deep%2520RL%2520and%2520Limited%2520Sensing%26entry.906535625%3DAmirhossein%2520Zhalehmehrabi%2520and%2520Daniele%2520Meli%2520and%2520Francesco%2520Dal%2520Santo%2520and%2520Francesco%2520Trotti%2520and%2520Alessandro%2520Farinelli%26entry.1292438233%3D%2520%2520Autonomous%2520Surface%2520Vehicles%2520%2528ASVs%2529%2520play%2520a%2520crucial%2520role%2520in%2520maritime%250Aoperations%252C%2520yet%2520their%2520navigation%2520in%2520shallow-water%2520environments%2520remains%250Achallenging%2520due%2520to%2520dynamic%2520disturbances%2520and%2520depth%2520constraints.%2520Traditional%250Anavigation%2520strategies%2520struggle%2520with%2520limited%2520sensor%2520information%252C%2520making%2520safe%2520and%250Aefficient%2520operation%2520difficult.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520reinforcement%250Alearning%2520%2528RL%2529%2520framework%2520for%2520ASV%2520navigation%2520under%2520depth%2520constraints%252C%2520where%2520the%250Avehicle%2520must%2520reach%2520a%2520target%2520while%2520avoiding%2520unsafe%2520areas%2520with%2520only%2520a%2520single%250Adepth%2520measurement%2520per%2520timestep%2520from%2520a%2520downward-facing%2520Single%2520Beam%2520Echosounder%250A%2528SBES%2529.%2520To%2520enhance%2520environmental%2520awareness%252C%2520we%2520integrate%2520Gaussian%2520Process%2520%2528GP%2529%250Aregression%2520into%2520the%2520RL%2520framework%252C%2520enabling%2520the%2520agent%2520to%2520progressively%2520estimate%250Aa%2520bathymetric%2520depth%2520map%2520from%2520sparse%2520sonar%2520readings.%2520This%2520approach%2520improves%250Adecision-making%2520by%2520providing%2520a%2520richer%2520representation%2520of%2520the%2520environment.%250AFurthermore%252C%2520we%2520demonstrate%2520effective%2520sim-to-real%2520transfer%252C%2520ensuring%2520that%250Atrained%2520policies%2520generalize%2520well%2520to%2520real-world%2520aquatic%2520conditions.%2520Experimental%250Aresults%2520validate%2520our%2520method%2527s%2520capability%2520to%2520improve%2520ASV%2520navigation%2520performance%250Awhile%2520maintaining%2520safety%2520in%2520challenging%2520shallow-water%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Constrained%20ASV%20Navigation%20with%20Deep%20RL%20and%20Limited%20Sensing&entry.906535625=Amirhossein%20Zhalehmehrabi%20and%20Daniele%20Meli%20and%20Francesco%20Dal%20Santo%20and%20Francesco%20Trotti%20and%20Alessandro%20Farinelli&entry.1292438233=%20%20Autonomous%20Surface%20Vehicles%20%28ASVs%29%20play%20a%20crucial%20role%20in%20maritime%0Aoperations%2C%20yet%20their%20navigation%20in%20shallow-water%20environments%20remains%0Achallenging%20due%20to%20dynamic%20disturbances%20and%20depth%20constraints.%20Traditional%0Anavigation%20strategies%20struggle%20with%20limited%20sensor%20information%2C%20making%20safe%20and%0Aefficient%20operation%20difficult.%20In%20this%20paper%2C%20we%20propose%20a%20reinforcement%0Alearning%20%28RL%29%20framework%20for%20ASV%20navigation%20under%20depth%20constraints%2C%20where%20the%0Avehicle%20must%20reach%20a%20target%20while%20avoiding%20unsafe%20areas%20with%20only%20a%20single%0Adepth%20measurement%20per%20timestep%20from%20a%20downward-facing%20Single%20Beam%20Echosounder%0A%28SBES%29.%20To%20enhance%20environmental%20awareness%2C%20we%20integrate%20Gaussian%20Process%20%28GP%29%0Aregression%20into%20the%20RL%20framework%2C%20enabling%20the%20agent%20to%20progressively%20estimate%0Aa%20bathymetric%20depth%20map%20from%20sparse%20sonar%20readings.%20This%20approach%20improves%0Adecision-making%20by%20providing%20a%20richer%20representation%20of%20the%20environment.%0AFurthermore%2C%20we%20demonstrate%20effective%20sim-to-real%20transfer%2C%20ensuring%20that%0Atrained%20policies%20generalize%20well%20to%20real-world%20aquatic%20conditions.%20Experimental%0Aresults%20validate%20our%20method%27s%20capability%20to%20improve%20ASV%20navigation%20performance%0Awhile%20maintaining%20safety%20in%20challenging%20shallow-water%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18253v2&entry.124074799=Read"},
{"title": "Survey on Vision-Language-Action Models", "author": "Adilzhan Adilkhanov and Amir Yelenov and Assylkhan Seitzhanov and Ayan Mazhitov and Azamat Abdikarimov and Danissa Sandykbayeva and Daryn Kenzhebek and Dinmukhammed Mukashev and Ilyas Umurbekov and Jabrail Chumakov and Kamila Spanova and Karina Burunchina and Madina Yergibay and Margulan Issa and Moldir Zabirova and Nurdaulet Zhuzbay and Nurlan Kabdyshev and Nurlan Zhaniyar and Rasul Yermagambet and Rustam Chibar and Saltanat Seitzhan and Soibkhon Khajikhanov and Tasbolat Taunyazov and Temirlan Galimzhanov and Temirlan Kaiyrbay and Tleukhan Mussin and Togzhan Syrymova and Valeriya Kostyukova and Yerkebulan Massalim and Yermakhan Kassym and Zerde Nurbayeva and Zhanat Kappassov", "abstract": "  This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable.\n", "link": "http://arxiv.org/abs/2502.06851v3", "date": "2025-06-02", "relevancy": 2.081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20on%20Vision-Language-Action%20Models&body=Title%3A%20Survey%20on%20Vision-Language-Action%20Models%0AAuthor%3A%20Adilzhan%20Adilkhanov%20and%20Amir%20Yelenov%20and%20Assylkhan%20Seitzhanov%20and%20Ayan%20Mazhitov%20and%20Azamat%20Abdikarimov%20and%20Danissa%20Sandykbayeva%20and%20Daryn%20Kenzhebek%20and%20Dinmukhammed%20Mukashev%20and%20Ilyas%20Umurbekov%20and%20Jabrail%20Chumakov%20and%20Kamila%20Spanova%20and%20Karina%20Burunchina%20and%20Madina%20Yergibay%20and%20Margulan%20Issa%20and%20Moldir%20Zabirova%20and%20Nurdaulet%20Zhuzbay%20and%20Nurlan%20Kabdyshev%20and%20Nurlan%20Zhaniyar%20and%20Rasul%20Yermagambet%20and%20Rustam%20Chibar%20and%20Saltanat%20Seitzhan%20and%20Soibkhon%20Khajikhanov%20and%20Tasbolat%20Taunyazov%20and%20Temirlan%20Galimzhanov%20and%20Temirlan%20Kaiyrbay%20and%20Tleukhan%20Mussin%20and%20Togzhan%20Syrymova%20and%20Valeriya%20Kostyukova%20and%20Yerkebulan%20Massalim%20and%20Yermakhan%20Kassym%20and%20Zerde%20Nurbayeva%20and%20Zhanat%20Kappassov%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20AI-generated%20review%20of%20Vision-Language-Action%20%28VLA%29%0Amodels%2C%20summarizing%20key%20methodologies%2C%20findings%2C%20and%20future%20directions.%20The%0Acontent%20is%20produced%20using%20large%20language%20models%20%28LLMs%29%20and%20is%20intended%20only%20for%0Ademonstration%20purposes.%20This%20work%20does%20not%20represent%20original%20research%2C%20but%0Ahighlights%20how%20AI%20can%20help%20automate%20literature%20reviews.%20As%20AI-generated%20content%0Abecomes%20more%20prevalent%2C%20ensuring%20accuracy%2C%20reliability%2C%20and%20proper%20synthesis%0Aremains%20a%20challenge.%20Future%20research%20will%20focus%20on%20developing%20a%20structured%0Aframework%20for%20AI-assisted%20literature%20reviews%2C%20exploring%20techniques%20to%20enhance%0Acitation%20accuracy%2C%20source%20credibility%2C%20and%20contextual%20understanding.%20By%0Aexamining%20the%20potential%20and%20limitations%20of%20LLM%20in%20academic%20writing%2C%20this%20study%0Aaims%20to%20contribute%20to%20the%20broader%20discussion%20of%20integrating%20AI%20into%20research%0Aworkflows.%20This%20work%20serves%20as%20a%20preliminary%20step%20toward%20establishing%0Asystematic%20approaches%20for%20leveraging%20AI%20in%20literature%20review%20generation%2C%20making%0Aacademic%20knowledge%20synthesis%20more%20efficient%20and%20scalable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06851v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520on%2520Vision-Language-Action%2520Models%26entry.906535625%3DAdilzhan%2520Adilkhanov%2520and%2520Amir%2520Yelenov%2520and%2520Assylkhan%2520Seitzhanov%2520and%2520Ayan%2520Mazhitov%2520and%2520Azamat%2520Abdikarimov%2520and%2520Danissa%2520Sandykbayeva%2520and%2520Daryn%2520Kenzhebek%2520and%2520Dinmukhammed%2520Mukashev%2520and%2520Ilyas%2520Umurbekov%2520and%2520Jabrail%2520Chumakov%2520and%2520Kamila%2520Spanova%2520and%2520Karina%2520Burunchina%2520and%2520Madina%2520Yergibay%2520and%2520Margulan%2520Issa%2520and%2520Moldir%2520Zabirova%2520and%2520Nurdaulet%2520Zhuzbay%2520and%2520Nurlan%2520Kabdyshev%2520and%2520Nurlan%2520Zhaniyar%2520and%2520Rasul%2520Yermagambet%2520and%2520Rustam%2520Chibar%2520and%2520Saltanat%2520Seitzhan%2520and%2520Soibkhon%2520Khajikhanov%2520and%2520Tasbolat%2520Taunyazov%2520and%2520Temirlan%2520Galimzhanov%2520and%2520Temirlan%2520Kaiyrbay%2520and%2520Tleukhan%2520Mussin%2520and%2520Togzhan%2520Syrymova%2520and%2520Valeriya%2520Kostyukova%2520and%2520Yerkebulan%2520Massalim%2520and%2520Yermakhan%2520Kassym%2520and%2520Zerde%2520Nurbayeva%2520and%2520Zhanat%2520Kappassov%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520AI-generated%2520review%2520of%2520Vision-Language-Action%2520%2528VLA%2529%250Amodels%252C%2520summarizing%2520key%2520methodologies%252C%2520findings%252C%2520and%2520future%2520directions.%2520The%250Acontent%2520is%2520produced%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520is%2520intended%2520only%2520for%250Ademonstration%2520purposes.%2520This%2520work%2520does%2520not%2520represent%2520original%2520research%252C%2520but%250Ahighlights%2520how%2520AI%2520can%2520help%2520automate%2520literature%2520reviews.%2520As%2520AI-generated%2520content%250Abecomes%2520more%2520prevalent%252C%2520ensuring%2520accuracy%252C%2520reliability%252C%2520and%2520proper%2520synthesis%250Aremains%2520a%2520challenge.%2520Future%2520research%2520will%2520focus%2520on%2520developing%2520a%2520structured%250Aframework%2520for%2520AI-assisted%2520literature%2520reviews%252C%2520exploring%2520techniques%2520to%2520enhance%250Acitation%2520accuracy%252C%2520source%2520credibility%252C%2520and%2520contextual%2520understanding.%2520By%250Aexamining%2520the%2520potential%2520and%2520limitations%2520of%2520LLM%2520in%2520academic%2520writing%252C%2520this%2520study%250Aaims%2520to%2520contribute%2520to%2520the%2520broader%2520discussion%2520of%2520integrating%2520AI%2520into%2520research%250Aworkflows.%2520This%2520work%2520serves%2520as%2520a%2520preliminary%2520step%2520toward%2520establishing%250Asystematic%2520approaches%2520for%2520leveraging%2520AI%2520in%2520literature%2520review%2520generation%252C%2520making%250Aacademic%2520knowledge%2520synthesis%2520more%2520efficient%2520and%2520scalable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06851v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20on%20Vision-Language-Action%20Models&entry.906535625=Adilzhan%20Adilkhanov%20and%20Amir%20Yelenov%20and%20Assylkhan%20Seitzhanov%20and%20Ayan%20Mazhitov%20and%20Azamat%20Abdikarimov%20and%20Danissa%20Sandykbayeva%20and%20Daryn%20Kenzhebek%20and%20Dinmukhammed%20Mukashev%20and%20Ilyas%20Umurbekov%20and%20Jabrail%20Chumakov%20and%20Kamila%20Spanova%20and%20Karina%20Burunchina%20and%20Madina%20Yergibay%20and%20Margulan%20Issa%20and%20Moldir%20Zabirova%20and%20Nurdaulet%20Zhuzbay%20and%20Nurlan%20Kabdyshev%20and%20Nurlan%20Zhaniyar%20and%20Rasul%20Yermagambet%20and%20Rustam%20Chibar%20and%20Saltanat%20Seitzhan%20and%20Soibkhon%20Khajikhanov%20and%20Tasbolat%20Taunyazov%20and%20Temirlan%20Galimzhanov%20and%20Temirlan%20Kaiyrbay%20and%20Tleukhan%20Mussin%20and%20Togzhan%20Syrymova%20and%20Valeriya%20Kostyukova%20and%20Yerkebulan%20Massalim%20and%20Yermakhan%20Kassym%20and%20Zerde%20Nurbayeva%20and%20Zhanat%20Kappassov&entry.1292438233=%20%20This%20paper%20presents%20an%20AI-generated%20review%20of%20Vision-Language-Action%20%28VLA%29%0Amodels%2C%20summarizing%20key%20methodologies%2C%20findings%2C%20and%20future%20directions.%20The%0Acontent%20is%20produced%20using%20large%20language%20models%20%28LLMs%29%20and%20is%20intended%20only%20for%0Ademonstration%20purposes.%20This%20work%20does%20not%20represent%20original%20research%2C%20but%0Ahighlights%20how%20AI%20can%20help%20automate%20literature%20reviews.%20As%20AI-generated%20content%0Abecomes%20more%20prevalent%2C%20ensuring%20accuracy%2C%20reliability%2C%20and%20proper%20synthesis%0Aremains%20a%20challenge.%20Future%20research%20will%20focus%20on%20developing%20a%20structured%0Aframework%20for%20AI-assisted%20literature%20reviews%2C%20exploring%20techniques%20to%20enhance%0Acitation%20accuracy%2C%20source%20credibility%2C%20and%20contextual%20understanding.%20By%0Aexamining%20the%20potential%20and%20limitations%20of%20LLM%20in%20academic%20writing%2C%20this%20study%0Aaims%20to%20contribute%20to%20the%20broader%20discussion%20of%20integrating%20AI%20into%20research%0Aworkflows.%20This%20work%20serves%20as%20a%20preliminary%20step%20toward%20establishing%0Asystematic%20approaches%20for%20leveraging%20AI%20in%20literature%20review%20generation%2C%20making%0Aacademic%20knowledge%20synthesis%20more%20efficient%20and%20scalable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06851v3&entry.124074799=Read"},
{"title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "author": "Xingjun Ma and Yifeng Gao and Yixu Wang and Ruofan Wang and Xin Wang and Ye Sun and Yifan Ding and Hengyuan Xu and Yunhao Chen and Yunhan Zhao and Hanxun Huang and Yige Li and Jiaming Zhang and Xiang Zheng and Yang Bai and Zuxuan Wu and Xipeng Qiu and Jingfeng Zhang and Yiming Li and Xudong Han and Haonan Li and Jun Sun and Cong Wang and Jindong Gu and Baoyuan Wu and Siheng Chen and Tianwei Zhang and Yang Liu and Mingming Gong and Tongliang Liu and Shirui Pan and Cihang Xie and Tianyu Pang and Yinpeng Dong and Ruoxi Jia and Yang Zhang and Shiqing Ma and Xiangyu Zhang and Neil Gong and Chaowei Xiao and Sarah Erfani and Tim Baldwin and Bo Li and Masashi Sugiyama and Dacheng Tao and James Bailey and Yu-Gang Jiang", "abstract": "  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n", "link": "http://arxiv.org/abs/2502.05206v4", "date": "2025-06-02", "relevancy": 2.0597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20at%20Scale%3A%20A%20Comprehensive%20Survey%20of%20Large%20Model%20Safety&body=Title%3A%20Safety%20at%20Scale%3A%20A%20Comprehensive%20Survey%20of%20Large%20Model%20Safety%0AAuthor%3A%20Xingjun%20Ma%20and%20Yifeng%20Gao%20and%20Yixu%20Wang%20and%20Ruofan%20Wang%20and%20Xin%20Wang%20and%20Ye%20Sun%20and%20Yifan%20Ding%20and%20Hengyuan%20Xu%20and%20Yunhao%20Chen%20and%20Yunhan%20Zhao%20and%20Hanxun%20Huang%20and%20Yige%20Li%20and%20Jiaming%20Zhang%20and%20Xiang%20Zheng%20and%20Yang%20Bai%20and%20Zuxuan%20Wu%20and%20Xipeng%20Qiu%20and%20Jingfeng%20Zhang%20and%20Yiming%20Li%20and%20Xudong%20Han%20and%20Haonan%20Li%20and%20Jun%20Sun%20and%20Cong%20Wang%20and%20Jindong%20Gu%20and%20Baoyuan%20Wu%20and%20Siheng%20Chen%20and%20Tianwei%20Zhang%20and%20Yang%20Liu%20and%20Mingming%20Gong%20and%20Tongliang%20Liu%20and%20Shirui%20Pan%20and%20Cihang%20Xie%20and%20Tianyu%20Pang%20and%20Yinpeng%20Dong%20and%20Ruoxi%20Jia%20and%20Yang%20Zhang%20and%20Shiqing%20Ma%20and%20Xiangyu%20Zhang%20and%20Neil%20Gong%20and%20Chaowei%20Xiao%20and%20Sarah%20Erfani%20and%20Tim%20Baldwin%20and%20Bo%20Li%20and%20Masashi%20Sugiyama%20and%20Dacheng%20Tao%20and%20James%20Bailey%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20models%2C%20driven%20by%20their%20exceptional%20abilities%0Ain%20learning%20and%20generalization%20through%20large-scale%20pre-training%2C%20has%20reshaped%0Athe%20landscape%20of%20Artificial%20Intelligence%20%28AI%29.%20These%20models%20are%20now%0Afoundational%20to%20a%20wide%20range%20of%20applications%2C%20including%20conversational%20AI%2C%0Arecommendation%20systems%2C%20autonomous%20driving%2C%20content%20generation%2C%20medical%0Adiagnostics%2C%20and%20scientific%20discovery.%20However%2C%20their%20widespread%20deployment%0Aalso%20exposes%20them%20to%20significant%20safety%20risks%2C%20raising%20concerns%20about%0Arobustness%2C%20reliability%2C%20and%20ethical%20implications.%20This%20survey%20provides%20a%0Asystematic%20review%20of%20current%20safety%20research%20on%20large%20models%2C%20covering%20Vision%0AFoundation%20Models%20%28VFMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20Vision-Language%0APre-training%20%28VLP%29%20models%2C%20Vision-Language%20Models%20%28VLMs%29%2C%20Diffusion%20Models%0A%28DMs%29%2C%20and%20large-model-based%20Agents.%20Our%20contributions%20are%20summarized%20as%0Afollows%3A%20%281%29%20We%20present%20a%20comprehensive%20taxonomy%20of%20safety%20threats%20to%20these%0Amodels%2C%20including%20adversarial%20attacks%2C%20data%20poisoning%2C%20backdoor%20attacks%2C%0Ajailbreak%20and%20prompt%20injection%20attacks%2C%20energy-latency%20attacks%2C%20data%20and%20model%0Aextraction%20attacks%2C%20and%20emerging%20agent-specific%20threats.%20%282%29%20We%20review%20defense%0Astrategies%20proposed%20for%20each%20type%20of%20attacks%20if%20available%20and%20summarize%20the%0Acommonly%20used%20datasets%20and%20benchmarks%20for%20safety%20research.%20%283%29%20Building%20on%0Athis%2C%20we%20identify%20and%20discuss%20the%20open%20challenges%20in%20large%20model%20safety%2C%0Aemphasizing%20the%20need%20for%20comprehensive%20safety%20evaluations%2C%20scalable%20and%0Aeffective%20defense%20mechanisms%2C%20and%20sustainable%20data%20practices.%20More%20importantly%2C%0Awe%20highlight%20the%20necessity%20of%20collective%20efforts%20from%20the%20research%20community%0Aand%20international%20collaboration.%20Our%20work%20can%20serve%20as%20a%20useful%20reference%20for%0Aresearchers%20and%20practitioners%2C%20fostering%20the%20ongoing%20development%20of%0Acomprehensive%20defense%20systems%20and%20platforms%20to%20safeguard%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05206v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520at%2520Scale%253A%2520A%2520Comprehensive%2520Survey%2520of%2520Large%2520Model%2520Safety%26entry.906535625%3DXingjun%2520Ma%2520and%2520Yifeng%2520Gao%2520and%2520Yixu%2520Wang%2520and%2520Ruofan%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Ye%2520Sun%2520and%2520Yifan%2520Ding%2520and%2520Hengyuan%2520Xu%2520and%2520Yunhao%2520Chen%2520and%2520Yunhan%2520Zhao%2520and%2520Hanxun%2520Huang%2520and%2520Yige%2520Li%2520and%2520Jiaming%2520Zhang%2520and%2520Xiang%2520Zheng%2520and%2520Yang%2520Bai%2520and%2520Zuxuan%2520Wu%2520and%2520Xipeng%2520Qiu%2520and%2520Jingfeng%2520Zhang%2520and%2520Yiming%2520Li%2520and%2520Xudong%2520Han%2520and%2520Haonan%2520Li%2520and%2520Jun%2520Sun%2520and%2520Cong%2520Wang%2520and%2520Jindong%2520Gu%2520and%2520Baoyuan%2520Wu%2520and%2520Siheng%2520Chen%2520and%2520Tianwei%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Mingming%2520Gong%2520and%2520Tongliang%2520Liu%2520and%2520Shirui%2520Pan%2520and%2520Cihang%2520Xie%2520and%2520Tianyu%2520Pang%2520and%2520Yinpeng%2520Dong%2520and%2520Ruoxi%2520Jia%2520and%2520Yang%2520Zhang%2520and%2520Shiqing%2520Ma%2520and%2520Xiangyu%2520Zhang%2520and%2520Neil%2520Gong%2520and%2520Chaowei%2520Xiao%2520and%2520Sarah%2520Erfani%2520and%2520Tim%2520Baldwin%2520and%2520Bo%2520Li%2520and%2520Masashi%2520Sugiyama%2520and%2520Dacheng%2520Tao%2520and%2520James%2520Bailey%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520models%252C%2520driven%2520by%2520their%2520exceptional%2520abilities%250Ain%2520learning%2520and%2520generalization%2520through%2520large-scale%2520pre-training%252C%2520has%2520reshaped%250Athe%2520landscape%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529.%2520These%2520models%2520are%2520now%250Afoundational%2520to%2520a%2520wide%2520range%2520of%2520applications%252C%2520including%2520conversational%2520AI%252C%250Arecommendation%2520systems%252C%2520autonomous%2520driving%252C%2520content%2520generation%252C%2520medical%250Adiagnostics%252C%2520and%2520scientific%2520discovery.%2520However%252C%2520their%2520widespread%2520deployment%250Aalso%2520exposes%2520them%2520to%2520significant%2520safety%2520risks%252C%2520raising%2520concerns%2520about%250Arobustness%252C%2520reliability%252C%2520and%2520ethical%2520implications.%2520This%2520survey%2520provides%2520a%250Asystematic%2520review%2520of%2520current%2520safety%2520research%2520on%2520large%2520models%252C%2520covering%2520Vision%250AFoundation%2520Models%2520%2528VFMs%2529%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Vision-Language%250APre-training%2520%2528VLP%2529%2520models%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520Diffusion%2520Models%250A%2528DMs%2529%252C%2520and%2520large-model-based%2520Agents.%2520Our%2520contributions%2520are%2520summarized%2520as%250Afollows%253A%2520%25281%2529%2520We%2520present%2520a%2520comprehensive%2520taxonomy%2520of%2520safety%2520threats%2520to%2520these%250Amodels%252C%2520including%2520adversarial%2520attacks%252C%2520data%2520poisoning%252C%2520backdoor%2520attacks%252C%250Ajailbreak%2520and%2520prompt%2520injection%2520attacks%252C%2520energy-latency%2520attacks%252C%2520data%2520and%2520model%250Aextraction%2520attacks%252C%2520and%2520emerging%2520agent-specific%2520threats.%2520%25282%2529%2520We%2520review%2520defense%250Astrategies%2520proposed%2520for%2520each%2520type%2520of%2520attacks%2520if%2520available%2520and%2520summarize%2520the%250Acommonly%2520used%2520datasets%2520and%2520benchmarks%2520for%2520safety%2520research.%2520%25283%2529%2520Building%2520on%250Athis%252C%2520we%2520identify%2520and%2520discuss%2520the%2520open%2520challenges%2520in%2520large%2520model%2520safety%252C%250Aemphasizing%2520the%2520need%2520for%2520comprehensive%2520safety%2520evaluations%252C%2520scalable%2520and%250Aeffective%2520defense%2520mechanisms%252C%2520and%2520sustainable%2520data%2520practices.%2520More%2520importantly%252C%250Awe%2520highlight%2520the%2520necessity%2520of%2520collective%2520efforts%2520from%2520the%2520research%2520community%250Aand%2520international%2520collaboration.%2520Our%2520work%2520can%2520serve%2520as%2520a%2520useful%2520reference%2520for%250Aresearchers%2520and%2520practitioners%252C%2520fostering%2520the%2520ongoing%2520development%2520of%250Acomprehensive%2520defense%2520systems%2520and%2520platforms%2520to%2520safeguard%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05206v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20at%20Scale%3A%20A%20Comprehensive%20Survey%20of%20Large%20Model%20Safety&entry.906535625=Xingjun%20Ma%20and%20Yifeng%20Gao%20and%20Yixu%20Wang%20and%20Ruofan%20Wang%20and%20Xin%20Wang%20and%20Ye%20Sun%20and%20Yifan%20Ding%20and%20Hengyuan%20Xu%20and%20Yunhao%20Chen%20and%20Yunhan%20Zhao%20and%20Hanxun%20Huang%20and%20Yige%20Li%20and%20Jiaming%20Zhang%20and%20Xiang%20Zheng%20and%20Yang%20Bai%20and%20Zuxuan%20Wu%20and%20Xipeng%20Qiu%20and%20Jingfeng%20Zhang%20and%20Yiming%20Li%20and%20Xudong%20Han%20and%20Haonan%20Li%20and%20Jun%20Sun%20and%20Cong%20Wang%20and%20Jindong%20Gu%20and%20Baoyuan%20Wu%20and%20Siheng%20Chen%20and%20Tianwei%20Zhang%20and%20Yang%20Liu%20and%20Mingming%20Gong%20and%20Tongliang%20Liu%20and%20Shirui%20Pan%20and%20Cihang%20Xie%20and%20Tianyu%20Pang%20and%20Yinpeng%20Dong%20and%20Ruoxi%20Jia%20and%20Yang%20Zhang%20and%20Shiqing%20Ma%20and%20Xiangyu%20Zhang%20and%20Neil%20Gong%20and%20Chaowei%20Xiao%20and%20Sarah%20Erfani%20and%20Tim%20Baldwin%20and%20Bo%20Li%20and%20Masashi%20Sugiyama%20and%20Dacheng%20Tao%20and%20James%20Bailey%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20models%2C%20driven%20by%20their%20exceptional%20abilities%0Ain%20learning%20and%20generalization%20through%20large-scale%20pre-training%2C%20has%20reshaped%0Athe%20landscape%20of%20Artificial%20Intelligence%20%28AI%29.%20These%20models%20are%20now%0Afoundational%20to%20a%20wide%20range%20of%20applications%2C%20including%20conversational%20AI%2C%0Arecommendation%20systems%2C%20autonomous%20driving%2C%20content%20generation%2C%20medical%0Adiagnostics%2C%20and%20scientific%20discovery.%20However%2C%20their%20widespread%20deployment%0Aalso%20exposes%20them%20to%20significant%20safety%20risks%2C%20raising%20concerns%20about%0Arobustness%2C%20reliability%2C%20and%20ethical%20implications.%20This%20survey%20provides%20a%0Asystematic%20review%20of%20current%20safety%20research%20on%20large%20models%2C%20covering%20Vision%0AFoundation%20Models%20%28VFMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20Vision-Language%0APre-training%20%28VLP%29%20models%2C%20Vision-Language%20Models%20%28VLMs%29%2C%20Diffusion%20Models%0A%28DMs%29%2C%20and%20large-model-based%20Agents.%20Our%20contributions%20are%20summarized%20as%0Afollows%3A%20%281%29%20We%20present%20a%20comprehensive%20taxonomy%20of%20safety%20threats%20to%20these%0Amodels%2C%20including%20adversarial%20attacks%2C%20data%20poisoning%2C%20backdoor%20attacks%2C%0Ajailbreak%20and%20prompt%20injection%20attacks%2C%20energy-latency%20attacks%2C%20data%20and%20model%0Aextraction%20attacks%2C%20and%20emerging%20agent-specific%20threats.%20%282%29%20We%20review%20defense%0Astrategies%20proposed%20for%20each%20type%20of%20attacks%20if%20available%20and%20summarize%20the%0Acommonly%20used%20datasets%20and%20benchmarks%20for%20safety%20research.%20%283%29%20Building%20on%0Athis%2C%20we%20identify%20and%20discuss%20the%20open%20challenges%20in%20large%20model%20safety%2C%0Aemphasizing%20the%20need%20for%20comprehensive%20safety%20evaluations%2C%20scalable%20and%0Aeffective%20defense%20mechanisms%2C%20and%20sustainable%20data%20practices.%20More%20importantly%2C%0Awe%20highlight%20the%20necessity%20of%20collective%20efforts%20from%20the%20research%20community%0Aand%20international%20collaboration.%20Our%20work%20can%20serve%20as%20a%20useful%20reference%20for%0Aresearchers%20and%20practitioners%2C%20fostering%20the%20ongoing%20development%20of%0Acomprehensive%20defense%20systems%20and%20platforms%20to%20safeguard%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05206v4&entry.124074799=Read"},
{"title": "Neuroplastic Expansion in Deep Reinforcement Learning", "author": "Jiashun Liu and Johan Obando-Ceron and Aaron Courville and Ling Pan", "abstract": "  The loss of plasticity in learning agents, analogous to the solidification of\nneural pathways in biological brains, significantly impedes learning and\nadaptation in reinforcement learning due to its non-stationary nature. To\naddress this fundamental challenge, we propose a novel approach, {\\it\nNeuroplastic Expansion} (NE), inspired by cortical expansion in cognitive\nscience. NE maintains learnability and adaptability throughout the entire\ntraining process by dynamically growing the network from a smaller initial size\nto its full dimension. Our method is designed with three key components:\n(\\textit{1}) elastic topology generation based on potential gradients,\n(\\textit{2}) dormant neuron pruning to optimize network expressivity, and\n(\\textit{3}) neuron consolidation via experience review to strike a balance in\nthe plasticity-stability dilemma. Extensive experiments demonstrate that NE\neffectively mitigates plasticity loss and outperforms state-of-the-art methods\nacross various tasks in MuJoCo and DeepMind Control Suite environments. NE\nenables more adaptive learning in complex, dynamic environments, which\nrepresents a crucial step towards transitioning deep reinforcement learning\nfrom static, one-time training paradigms to more flexible, continually adapting\nmodels.\n", "link": "http://arxiv.org/abs/2410.07994v3", "date": "2025-06-02", "relevancy": 2.0565, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5095}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuroplastic%20Expansion%20in%20Deep%20Reinforcement%20Learning&body=Title%3A%20Neuroplastic%20Expansion%20in%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Jiashun%20Liu%20and%20Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Ling%20Pan%0AAbstract%3A%20%20%20The%20loss%20of%20plasticity%20in%20learning%20agents%2C%20analogous%20to%20the%20solidification%20of%0Aneural%20pathways%20in%20biological%20brains%2C%20significantly%20impedes%20learning%20and%0Aadaptation%20in%20reinforcement%20learning%20due%20to%20its%20non-stationary%20nature.%20To%0Aaddress%20this%20fundamental%20challenge%2C%20we%20propose%20a%20novel%20approach%2C%20%7B%5Cit%0ANeuroplastic%20Expansion%7D%20%28NE%29%2C%20inspired%20by%20cortical%20expansion%20in%20cognitive%0Ascience.%20NE%20maintains%20learnability%20and%20adaptability%20throughout%20the%20entire%0Atraining%20process%20by%20dynamically%20growing%20the%20network%20from%20a%20smaller%20initial%20size%0Ato%20its%20full%20dimension.%20Our%20method%20is%20designed%20with%20three%20key%20components%3A%0A%28%5Ctextit%7B1%7D%29%20elastic%20topology%20generation%20based%20on%20potential%20gradients%2C%0A%28%5Ctextit%7B2%7D%29%20dormant%20neuron%20pruning%20to%20optimize%20network%20expressivity%2C%20and%0A%28%5Ctextit%7B3%7D%29%20neuron%20consolidation%20via%20experience%20review%20to%20strike%20a%20balance%20in%0Athe%20plasticity-stability%20dilemma.%20Extensive%20experiments%20demonstrate%20that%20NE%0Aeffectively%20mitigates%20plasticity%20loss%20and%20outperforms%20state-of-the-art%20methods%0Aacross%20various%20tasks%20in%20MuJoCo%20and%20DeepMind%20Control%20Suite%20environments.%20NE%0Aenables%20more%20adaptive%20learning%20in%20complex%2C%20dynamic%20environments%2C%20which%0Arepresents%20a%20crucial%20step%20towards%20transitioning%20deep%20reinforcement%20learning%0Afrom%20static%2C%20one-time%20training%20paradigms%20to%20more%20flexible%2C%20continually%20adapting%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07994v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroplastic%2520Expansion%2520in%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DJiashun%2520Liu%2520and%2520Johan%2520Obando-Ceron%2520and%2520Aaron%2520Courville%2520and%2520Ling%2520Pan%26entry.1292438233%3D%2520%2520The%2520loss%2520of%2520plasticity%2520in%2520learning%2520agents%252C%2520analogous%2520to%2520the%2520solidification%2520of%250Aneural%2520pathways%2520in%2520biological%2520brains%252C%2520significantly%2520impedes%2520learning%2520and%250Aadaptation%2520in%2520reinforcement%2520learning%2520due%2520to%2520its%2520non-stationary%2520nature.%2520To%250Aaddress%2520this%2520fundamental%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%2520%257B%255Cit%250ANeuroplastic%2520Expansion%257D%2520%2528NE%2529%252C%2520inspired%2520by%2520cortical%2520expansion%2520in%2520cognitive%250Ascience.%2520NE%2520maintains%2520learnability%2520and%2520adaptability%2520throughout%2520the%2520entire%250Atraining%2520process%2520by%2520dynamically%2520growing%2520the%2520network%2520from%2520a%2520smaller%2520initial%2520size%250Ato%2520its%2520full%2520dimension.%2520Our%2520method%2520is%2520designed%2520with%2520three%2520key%2520components%253A%250A%2528%255Ctextit%257B1%257D%2529%2520elastic%2520topology%2520generation%2520based%2520on%2520potential%2520gradients%252C%250A%2528%255Ctextit%257B2%257D%2529%2520dormant%2520neuron%2520pruning%2520to%2520optimize%2520network%2520expressivity%252C%2520and%250A%2528%255Ctextit%257B3%257D%2529%2520neuron%2520consolidation%2520via%2520experience%2520review%2520to%2520strike%2520a%2520balance%2520in%250Athe%2520plasticity-stability%2520dilemma.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NE%250Aeffectively%2520mitigates%2520plasticity%2520loss%2520and%2520outperforms%2520state-of-the-art%2520methods%250Aacross%2520various%2520tasks%2520in%2520MuJoCo%2520and%2520DeepMind%2520Control%2520Suite%2520environments.%2520NE%250Aenables%2520more%2520adaptive%2520learning%2520in%2520complex%252C%2520dynamic%2520environments%252C%2520which%250Arepresents%2520a%2520crucial%2520step%2520towards%2520transitioning%2520deep%2520reinforcement%2520learning%250Afrom%2520static%252C%2520one-time%2520training%2520paradigms%2520to%2520more%2520flexible%252C%2520continually%2520adapting%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07994v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuroplastic%20Expansion%20in%20Deep%20Reinforcement%20Learning&entry.906535625=Jiashun%20Liu%20and%20Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Ling%20Pan&entry.1292438233=%20%20The%20loss%20of%20plasticity%20in%20learning%20agents%2C%20analogous%20to%20the%20solidification%20of%0Aneural%20pathways%20in%20biological%20brains%2C%20significantly%20impedes%20learning%20and%0Aadaptation%20in%20reinforcement%20learning%20due%20to%20its%20non-stationary%20nature.%20To%0Aaddress%20this%20fundamental%20challenge%2C%20we%20propose%20a%20novel%20approach%2C%20%7B%5Cit%0ANeuroplastic%20Expansion%7D%20%28NE%29%2C%20inspired%20by%20cortical%20expansion%20in%20cognitive%0Ascience.%20NE%20maintains%20learnability%20and%20adaptability%20throughout%20the%20entire%0Atraining%20process%20by%20dynamically%20growing%20the%20network%20from%20a%20smaller%20initial%20size%0Ato%20its%20full%20dimension.%20Our%20method%20is%20designed%20with%20three%20key%20components%3A%0A%28%5Ctextit%7B1%7D%29%20elastic%20topology%20generation%20based%20on%20potential%20gradients%2C%0A%28%5Ctextit%7B2%7D%29%20dormant%20neuron%20pruning%20to%20optimize%20network%20expressivity%2C%20and%0A%28%5Ctextit%7B3%7D%29%20neuron%20consolidation%20via%20experience%20review%20to%20strike%20a%20balance%20in%0Athe%20plasticity-stability%20dilemma.%20Extensive%20experiments%20demonstrate%20that%20NE%0Aeffectively%20mitigates%20plasticity%20loss%20and%20outperforms%20state-of-the-art%20methods%0Aacross%20various%20tasks%20in%20MuJoCo%20and%20DeepMind%20Control%20Suite%20environments.%20NE%0Aenables%20more%20adaptive%20learning%20in%20complex%2C%20dynamic%20environments%2C%20which%0Arepresents%20a%20crucial%20step%20towards%20transitioning%20deep%20reinforcement%20learning%0Afrom%20static%2C%20one-time%20training%20paradigms%20to%20more%20flexible%2C%20continually%20adapting%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07994v3&entry.124074799=Read"},
{"title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via\n  Data Likelihood Maximization", "author": "Mikhail Persiianov and Arip Asadulaev and Nikita Andreev and Nikita Starodubcev and Dmitry Baranchuk and Anastasis Kratsios and Evgeny Burnaev and Alexander Korotin", "abstract": "  Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in\nmachine learning, which is typically approached via supervised methods with\npaired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often\nchallenging, especially in problems such as domain translation. This\nnecessitates the development of $\\textit{semi-supervised}$ models that utilize\nboth limited paired data and additional unpaired i.i.d. samples $x \\sim\n\\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of\nsuch combined data is complex and often relies on heuristic approaches. To\ntackle this issue, we propose a new learning paradigm that integrates both\npaired and unpaired data $\\textbf{seamlessly}$ using the data likelihood\nmaximization techniques. We demonstrate that our approach also connects\nintriguingly with inverse entropic optimal transport (OT). This finding allows\nus to apply recent advances in computational OT to establish an\n$\\textbf{end-to-end}$ learning algorithm to get $\\pi^*(\\cdot|x)$. In addition,\nwe derive the universal approximation property, demonstrating that our approach\ncan theoretically recover true conditional distributions with arbitrarily small\nerror. Furthermore, we demonstrate through empirical tests that our method\neffectively learns conditional distributions using paired and unpaired data\nsimultaneously.\n", "link": "http://arxiv.org/abs/2410.02628v3", "date": "2025-06-02", "relevancy": 2.0235, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5269}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization&body=Title%3A%20Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization%0AAuthor%3A%20Mikhail%20Persiianov%20and%20Arip%20Asadulaev%20and%20Nikita%20Andreev%20and%20Nikita%20Starodubcev%20and%20Dmitry%20Baranchuk%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Learning%20conditional%20distributions%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24%20is%20a%20central%20problem%20in%0Amachine%20learning%2C%20which%20is%20typically%20approached%20via%20supervised%20methods%20with%0Apaired%20data%20%24%28x%2Cy%29%20%5Csim%20%5Cpi%5E%2A%24.%20However%2C%20acquiring%20paired%20data%20samples%20is%20often%0Achallenging%2C%20especially%20in%20problems%20such%20as%20domain%20translation.%20This%0Anecessitates%20the%20development%20of%20%24%5Ctextit%7Bsemi-supervised%7D%24%20models%20that%20utilize%0Aboth%20limited%20paired%20data%20and%20additional%20unpaired%20i.i.d.%20samples%20%24x%20%5Csim%0A%5Cpi%5E%2A_x%24%20and%20%24y%20%5Csim%20%5Cpi%5E%2A_y%24%20from%20the%20marginal%20distributions.%20The%20usage%20of%0Asuch%20combined%20data%20is%20complex%20and%20often%20relies%20on%20heuristic%20approaches.%20To%0Atackle%20this%20issue%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20integrates%20both%0Apaired%20and%20unpaired%20data%20%24%5Ctextbf%7Bseamlessly%7D%24%20using%20the%20data%20likelihood%0Amaximization%20techniques.%20We%20demonstrate%20that%20our%20approach%20also%20connects%0Aintriguingly%20with%20inverse%20entropic%20optimal%20transport%20%28OT%29.%20This%20finding%20allows%0Aus%20to%20apply%20recent%20advances%20in%20computational%20OT%20to%20establish%20an%0A%24%5Ctextbf%7Bend-to-end%7D%24%20learning%20algorithm%20to%20get%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24.%20In%20addition%2C%0Awe%20derive%20the%20universal%20approximation%20property%2C%20demonstrating%20that%20our%20approach%0Acan%20theoretically%20recover%20true%20conditional%20distributions%20with%20arbitrarily%20small%0Aerror.%20Furthermore%2C%20we%20demonstrate%20through%20empirical%20tests%20that%20our%20method%0Aeffectively%20learns%20conditional%20distributions%20using%20paired%20and%20unpaired%20data%0Asimultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02628v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Entropic%2520Optimal%2520Transport%2520Solves%2520Semi-supervised%2520Learning%2520via%250A%2520%2520Data%2520Likelihood%2520Maximization%26entry.906535625%3DMikhail%2520Persiianov%2520and%2520Arip%2520Asadulaev%2520and%2520Nikita%2520Andreev%2520and%2520Nikita%2520Starodubcev%2520and%2520Dmitry%2520Baranchuk%2520and%2520Anastasis%2520Kratsios%2520and%2520Evgeny%2520Burnaev%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Learning%2520conditional%2520distributions%2520%2524%255Cpi%255E%252A%2528%255Ccdot%257Cx%2529%2524%2520is%2520a%2520central%2520problem%2520in%250Amachine%2520learning%252C%2520which%2520is%2520typically%2520approached%2520via%2520supervised%2520methods%2520with%250Apaired%2520data%2520%2524%2528x%252Cy%2529%2520%255Csim%2520%255Cpi%255E%252A%2524.%2520However%252C%2520acquiring%2520paired%2520data%2520samples%2520is%2520often%250Achallenging%252C%2520especially%2520in%2520problems%2520such%2520as%2520domain%2520translation.%2520This%250Anecessitates%2520the%2520development%2520of%2520%2524%255Ctextit%257Bsemi-supervised%257D%2524%2520models%2520that%2520utilize%250Aboth%2520limited%2520paired%2520data%2520and%2520additional%2520unpaired%2520i.i.d.%2520samples%2520%2524x%2520%255Csim%250A%255Cpi%255E%252A_x%2524%2520and%2520%2524y%2520%255Csim%2520%255Cpi%255E%252A_y%2524%2520from%2520the%2520marginal%2520distributions.%2520The%2520usage%2520of%250Asuch%2520combined%2520data%2520is%2520complex%2520and%2520often%2520relies%2520on%2520heuristic%2520approaches.%2520To%250Atackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520learning%2520paradigm%2520that%2520integrates%2520both%250Apaired%2520and%2520unpaired%2520data%2520%2524%255Ctextbf%257Bseamlessly%257D%2524%2520using%2520the%2520data%2520likelihood%250Amaximization%2520techniques.%2520We%2520demonstrate%2520that%2520our%2520approach%2520also%2520connects%250Aintriguingly%2520with%2520inverse%2520entropic%2520optimal%2520transport%2520%2528OT%2529.%2520This%2520finding%2520allows%250Aus%2520to%2520apply%2520recent%2520advances%2520in%2520computational%2520OT%2520to%2520establish%2520an%250A%2524%255Ctextbf%257Bend-to-end%257D%2524%2520learning%2520algorithm%2520to%2520get%2520%2524%255Cpi%255E%252A%2528%255Ccdot%257Cx%2529%2524.%2520In%2520addition%252C%250Awe%2520derive%2520the%2520universal%2520approximation%2520property%252C%2520demonstrating%2520that%2520our%2520approach%250Acan%2520theoretically%2520recover%2520true%2520conditional%2520distributions%2520with%2520arbitrarily%2520small%250Aerror.%2520Furthermore%252C%2520we%2520demonstrate%2520through%2520empirical%2520tests%2520that%2520our%2520method%250Aeffectively%2520learns%2520conditional%2520distributions%2520using%2520paired%2520and%2520unpaired%2520data%250Asimultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02628v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Entropic%20Optimal%20Transport%20Solves%20Semi-supervised%20Learning%20via%0A%20%20Data%20Likelihood%20Maximization&entry.906535625=Mikhail%20Persiianov%20and%20Arip%20Asadulaev%20and%20Nikita%20Andreev%20and%20Nikita%20Starodubcev%20and%20Dmitry%20Baranchuk%20and%20Anastasis%20Kratsios%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin&entry.1292438233=%20%20Learning%20conditional%20distributions%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24%20is%20a%20central%20problem%20in%0Amachine%20learning%2C%20which%20is%20typically%20approached%20via%20supervised%20methods%20with%0Apaired%20data%20%24%28x%2Cy%29%20%5Csim%20%5Cpi%5E%2A%24.%20However%2C%20acquiring%20paired%20data%20samples%20is%20often%0Achallenging%2C%20especially%20in%20problems%20such%20as%20domain%20translation.%20This%0Anecessitates%20the%20development%20of%20%24%5Ctextit%7Bsemi-supervised%7D%24%20models%20that%20utilize%0Aboth%20limited%20paired%20data%20and%20additional%20unpaired%20i.i.d.%20samples%20%24x%20%5Csim%0A%5Cpi%5E%2A_x%24%20and%20%24y%20%5Csim%20%5Cpi%5E%2A_y%24%20from%20the%20marginal%20distributions.%20The%20usage%20of%0Asuch%20combined%20data%20is%20complex%20and%20often%20relies%20on%20heuristic%20approaches.%20To%0Atackle%20this%20issue%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20integrates%20both%0Apaired%20and%20unpaired%20data%20%24%5Ctextbf%7Bseamlessly%7D%24%20using%20the%20data%20likelihood%0Amaximization%20techniques.%20We%20demonstrate%20that%20our%20approach%20also%20connects%0Aintriguingly%20with%20inverse%20entropic%20optimal%20transport%20%28OT%29.%20This%20finding%20allows%0Aus%20to%20apply%20recent%20advances%20in%20computational%20OT%20to%20establish%20an%0A%24%5Ctextbf%7Bend-to-end%7D%24%20learning%20algorithm%20to%20get%20%24%5Cpi%5E%2A%28%5Ccdot%7Cx%29%24.%20In%20addition%2C%0Awe%20derive%20the%20universal%20approximation%20property%2C%20demonstrating%20that%20our%20approach%0Acan%20theoretically%20recover%20true%20conditional%20distributions%20with%20arbitrarily%20small%0Aerror.%20Furthermore%2C%20we%20demonstrate%20through%20empirical%20tests%20that%20our%20method%0Aeffectively%20learns%20conditional%20distributions%20using%20paired%20and%20unpaired%20data%0Asimultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02628v3&entry.124074799=Read"},
{"title": "In the Picture: Medical Imaging Datasets, Artifacts, and their Living\n  Review", "author": "Amelia Jim\u00e9nez-S\u00e1nchez and Natalia-Rozalia Avlona and Sarah de Boer and V\u00edctor M. Campello and Aasa Feragen and Enzo Ferrante and Melanie Ganz and Judy Wawira Gichoya and Camila Gonz\u00e1lez and Steff Groefsema and Alessa Hering and Adam Hulman and Leo Joskowicz and Dovile Juodelyte and Melih Kandemir and Thijs Kooi and Jorge del Pozo L\u00e9rida and Livie Yumeng Li and Andre Pacheco and Tim R\u00e4dsch and Mauricio Reyes and Th\u00e9o Sourget and Bram van Ginneken and David Wen and Nina Weng and Jack Junchi Xu and Hubert Dariusz Zaj\u0105c and Maria A. Zuluaga and Veronika Cheplygina", "abstract": "  Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp://inthepicture.itu.dk/.\n", "link": "http://arxiv.org/abs/2501.10727v2", "date": "2025-06-02", "relevancy": 2.0178, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5097}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5097}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20the%20Picture%3A%20Medical%20Imaging%20Datasets%2C%20Artifacts%2C%20and%20their%20Living%0A%20%20Review&body=Title%3A%20In%20the%20Picture%3A%20Medical%20Imaging%20Datasets%2C%20Artifacts%2C%20and%20their%20Living%0A%20%20Review%0AAuthor%3A%20Amelia%20Jim%C3%A9nez-S%C3%A1nchez%20and%20Natalia-Rozalia%20Avlona%20and%20Sarah%20de%20Boer%20and%20V%C3%ADctor%20M.%20Campello%20and%20Aasa%20Feragen%20and%20Enzo%20Ferrante%20and%20Melanie%20Ganz%20and%20Judy%20Wawira%20Gichoya%20and%20Camila%20Gonz%C3%A1lez%20and%20Steff%20Groefsema%20and%20Alessa%20Hering%20and%20Adam%20Hulman%20and%20Leo%20Joskowicz%20and%20Dovile%20Juodelyte%20and%20Melih%20Kandemir%20and%20Thijs%20Kooi%20and%20Jorge%20del%20Pozo%20L%C3%A9rida%20and%20Livie%20Yumeng%20Li%20and%20Andre%20Pacheco%20and%20Tim%20R%C3%A4dsch%20and%20Mauricio%20Reyes%20and%20Th%C3%A9o%20Sourget%20and%20Bram%20van%20Ginneken%20and%20David%20Wen%20and%20Nina%20Weng%20and%20Jack%20Junchi%20Xu%20and%20Hubert%20Dariusz%20Zaj%C4%85c%20and%20Maria%20A.%20Zuluaga%20and%20Veronika%20Cheplygina%0AAbstract%3A%20%20%20Datasets%20play%20a%20critical%20role%20in%20medical%20imaging%20research%2C%20yet%20issues%20such%20as%0Alabel%20quality%2C%20shortcuts%2C%20and%20metadata%20are%20often%20overlooked.%20This%20lack%20of%0Aattention%20may%20harm%20the%20generalizability%20of%20algorithms%20and%2C%20consequently%2C%0Anegatively%20impact%20patient%20outcomes.%20While%20existing%20medical%20imaging%20literature%0Areviews%20mostly%20focus%20on%20machine%20learning%20%28ML%29%20methods%2C%20with%20only%20a%20few%20focusing%0Aon%20datasets%20for%20specific%20applications%2C%20these%20reviews%20remain%20static%20--%20they%20are%0Apublished%20once%20and%20not%20updated%20thereafter.%20This%20fails%20to%20account%20for%20emerging%0Aevidence%2C%20such%20as%20biases%2C%20shortcuts%2C%20and%20additional%20annotations%20that%20other%0Aresearchers%20may%20contribute%20after%20the%20dataset%20is%20published.%20We%20refer%20to%20these%0Anewly%20discovered%20findings%20of%20datasets%20as%20research%20artifacts.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20living%20review%20that%20continuously%20tracks%20public%20datasets%20and%0Atheir%20associated%20research%20artifacts%20across%20multiple%20medical%20imaging%0Aapplications.%20Our%20approach%20includes%20a%20framework%20for%20the%20living%20review%20to%0Amonitor%20data%20documentation%20artifacts%2C%20and%20an%20SQL%20database%20to%20visualize%20the%0Acitation%20relationships%20between%20research%20artifact%20and%20dataset.%20Lastly%2C%20we%0Adiscuss%20key%20considerations%20for%20creating%20medical%20imaging%20datasets%2C%20review%20best%0Apractices%20for%20data%20annotation%2C%20discuss%20the%20significance%20of%20shortcuts%20and%0Ademographic%20diversity%2C%20and%20emphasize%20the%20importance%20of%20managing%20datasets%0Athroughout%20their%20entire%20lifecycle.%20Our%20demo%20is%20publicly%20available%20at%0Ahttp%3A//inthepicture.itu.dk/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520the%2520Picture%253A%2520Medical%2520Imaging%2520Datasets%252C%2520Artifacts%252C%2520and%2520their%2520Living%250A%2520%2520Review%26entry.906535625%3DAmelia%2520Jim%25C3%25A9nez-S%25C3%25A1nchez%2520and%2520Natalia-Rozalia%2520Avlona%2520and%2520Sarah%2520de%2520Boer%2520and%2520V%25C3%25ADctor%2520M.%2520Campello%2520and%2520Aasa%2520Feragen%2520and%2520Enzo%2520Ferrante%2520and%2520Melanie%2520Ganz%2520and%2520Judy%2520Wawira%2520Gichoya%2520and%2520Camila%2520Gonz%25C3%25A1lez%2520and%2520Steff%2520Groefsema%2520and%2520Alessa%2520Hering%2520and%2520Adam%2520Hulman%2520and%2520Leo%2520Joskowicz%2520and%2520Dovile%2520Juodelyte%2520and%2520Melih%2520Kandemir%2520and%2520Thijs%2520Kooi%2520and%2520Jorge%2520del%2520Pozo%2520L%25C3%25A9rida%2520and%2520Livie%2520Yumeng%2520Li%2520and%2520Andre%2520Pacheco%2520and%2520Tim%2520R%25C3%25A4dsch%2520and%2520Mauricio%2520Reyes%2520and%2520Th%25C3%25A9o%2520Sourget%2520and%2520Bram%2520van%2520Ginneken%2520and%2520David%2520Wen%2520and%2520Nina%2520Weng%2520and%2520Jack%2520Junchi%2520Xu%2520and%2520Hubert%2520Dariusz%2520Zaj%25C4%2585c%2520and%2520Maria%2520A.%2520Zuluaga%2520and%2520Veronika%2520Cheplygina%26entry.1292438233%3D%2520%2520Datasets%2520play%2520a%2520critical%2520role%2520in%2520medical%2520imaging%2520research%252C%2520yet%2520issues%2520such%2520as%250Alabel%2520quality%252C%2520shortcuts%252C%2520and%2520metadata%2520are%2520often%2520overlooked.%2520This%2520lack%2520of%250Aattention%2520may%2520harm%2520the%2520generalizability%2520of%2520algorithms%2520and%252C%2520consequently%252C%250Anegatively%2520impact%2520patient%2520outcomes.%2520While%2520existing%2520medical%2520imaging%2520literature%250Areviews%2520mostly%2520focus%2520on%2520machine%2520learning%2520%2528ML%2529%2520methods%252C%2520with%2520only%2520a%2520few%2520focusing%250Aon%2520datasets%2520for%2520specific%2520applications%252C%2520these%2520reviews%2520remain%2520static%2520--%2520they%2520are%250Apublished%2520once%2520and%2520not%2520updated%2520thereafter.%2520This%2520fails%2520to%2520account%2520for%2520emerging%250Aevidence%252C%2520such%2520as%2520biases%252C%2520shortcuts%252C%2520and%2520additional%2520annotations%2520that%2520other%250Aresearchers%2520may%2520contribute%2520after%2520the%2520dataset%2520is%2520published.%2520We%2520refer%2520to%2520these%250Anewly%2520discovered%2520findings%2520of%2520datasets%2520as%2520research%2520artifacts.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520a%2520living%2520review%2520that%2520continuously%2520tracks%2520public%2520datasets%2520and%250Atheir%2520associated%2520research%2520artifacts%2520across%2520multiple%2520medical%2520imaging%250Aapplications.%2520Our%2520approach%2520includes%2520a%2520framework%2520for%2520the%2520living%2520review%2520to%250Amonitor%2520data%2520documentation%2520artifacts%252C%2520and%2520an%2520SQL%2520database%2520to%2520visualize%2520the%250Acitation%2520relationships%2520between%2520research%2520artifact%2520and%2520dataset.%2520Lastly%252C%2520we%250Adiscuss%2520key%2520considerations%2520for%2520creating%2520medical%2520imaging%2520datasets%252C%2520review%2520best%250Apractices%2520for%2520data%2520annotation%252C%2520discuss%2520the%2520significance%2520of%2520shortcuts%2520and%250Ademographic%2520diversity%252C%2520and%2520emphasize%2520the%2520importance%2520of%2520managing%2520datasets%250Athroughout%2520their%2520entire%2520lifecycle.%2520Our%2520demo%2520is%2520publicly%2520available%2520at%250Ahttp%253A//inthepicture.itu.dk/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20the%20Picture%3A%20Medical%20Imaging%20Datasets%2C%20Artifacts%2C%20and%20their%20Living%0A%20%20Review&entry.906535625=Amelia%20Jim%C3%A9nez-S%C3%A1nchez%20and%20Natalia-Rozalia%20Avlona%20and%20Sarah%20de%20Boer%20and%20V%C3%ADctor%20M.%20Campello%20and%20Aasa%20Feragen%20and%20Enzo%20Ferrante%20and%20Melanie%20Ganz%20and%20Judy%20Wawira%20Gichoya%20and%20Camila%20Gonz%C3%A1lez%20and%20Steff%20Groefsema%20and%20Alessa%20Hering%20and%20Adam%20Hulman%20and%20Leo%20Joskowicz%20and%20Dovile%20Juodelyte%20and%20Melih%20Kandemir%20and%20Thijs%20Kooi%20and%20Jorge%20del%20Pozo%20L%C3%A9rida%20and%20Livie%20Yumeng%20Li%20and%20Andre%20Pacheco%20and%20Tim%20R%C3%A4dsch%20and%20Mauricio%20Reyes%20and%20Th%C3%A9o%20Sourget%20and%20Bram%20van%20Ginneken%20and%20David%20Wen%20and%20Nina%20Weng%20and%20Jack%20Junchi%20Xu%20and%20Hubert%20Dariusz%20Zaj%C4%85c%20and%20Maria%20A.%20Zuluaga%20and%20Veronika%20Cheplygina&entry.1292438233=%20%20Datasets%20play%20a%20critical%20role%20in%20medical%20imaging%20research%2C%20yet%20issues%20such%20as%0Alabel%20quality%2C%20shortcuts%2C%20and%20metadata%20are%20often%20overlooked.%20This%20lack%20of%0Aattention%20may%20harm%20the%20generalizability%20of%20algorithms%20and%2C%20consequently%2C%0Anegatively%20impact%20patient%20outcomes.%20While%20existing%20medical%20imaging%20literature%0Areviews%20mostly%20focus%20on%20machine%20learning%20%28ML%29%20methods%2C%20with%20only%20a%20few%20focusing%0Aon%20datasets%20for%20specific%20applications%2C%20these%20reviews%20remain%20static%20--%20they%20are%0Apublished%20once%20and%20not%20updated%20thereafter.%20This%20fails%20to%20account%20for%20emerging%0Aevidence%2C%20such%20as%20biases%2C%20shortcuts%2C%20and%20additional%20annotations%20that%20other%0Aresearchers%20may%20contribute%20after%20the%20dataset%20is%20published.%20We%20refer%20to%20these%0Anewly%20discovered%20findings%20of%20datasets%20as%20research%20artifacts.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20living%20review%20that%20continuously%20tracks%20public%20datasets%20and%0Atheir%20associated%20research%20artifacts%20across%20multiple%20medical%20imaging%0Aapplications.%20Our%20approach%20includes%20a%20framework%20for%20the%20living%20review%20to%0Amonitor%20data%20documentation%20artifacts%2C%20and%20an%20SQL%20database%20to%20visualize%20the%0Acitation%20relationships%20between%20research%20artifact%20and%20dataset.%20Lastly%2C%20we%0Adiscuss%20key%20considerations%20for%20creating%20medical%20imaging%20datasets%2C%20review%20best%0Apractices%20for%20data%20annotation%2C%20discuss%20the%20significance%20of%20shortcuts%20and%0Ademographic%20diversity%2C%20and%20emphasize%20the%20importance%20of%20managing%20datasets%0Athroughout%20their%20entire%20lifecycle.%20Our%20demo%20is%20publicly%20available%20at%0Ahttp%3A//inthepicture.itu.dk/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10727v2&entry.124074799=Read"},
{"title": "S2A: A Unified Framework for Parameter and Memory Efficient Transfer\n  Learning", "author": "Tian Jin and Enjun Du and Changwei Wang and Wenhao Xu and Ding Luo", "abstract": "  Parameter-efficient transfer learning (PETL) aims to reduce the scales of\npretrained models for multiple downstream tasks. However, as the models keep\nscaling up, the memory footprint of existing PETL methods is not significantly\nreduced compared to the reduction of learnable parameters. This limitation\nhinders the practical deployment of PETL methods on memory-constrained devices.\nTo this end, we proposed a new PETL framework, called Structure to Activation\n(S2A), to reduce the memory footprint of activation during fine-tuning.\nSpecifically, our framework consists of: 1) Activation modules design(i.e.,\nbias, prompt and side modules) in the parametric model structure, which results\nin a significant reduction of adjustable parameters and activation memory; 2)\n4-bit quantization of activations based on their derivatives for non-parametric\nstructures (e.g., nonlinear functions), which maintains accuracy while\nsignificantly reducing memory usage. Our S2A method consequently offers a\nlightweight solution in terms of both parameters and memory footprint. We\nevaluated S2A with different backbones and performed extensive experiments on\nvarious datasets to evaluate the effectiveness. The results show that our\nmethods not only outperform existing PETL techniques, achieving a fourfold\nreduction in GPU memory footprint on average, but also shows competitive\nperformance in accuracy with fewer tunable parameters. These demonstrate that\nour method is highly suitable for practical transfer learning on\nhardware-constrained devices.\n", "link": "http://arxiv.org/abs/2503.08154v3", "date": "2025-06-02", "relevancy": 2.0011, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4917}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2A%3A%20A%20Unified%20Framework%20for%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning&body=Title%3A%20S2A%3A%20A%20Unified%20Framework%20for%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning%0AAuthor%3A%20Tian%20Jin%20and%20Enjun%20Du%20and%20Changwei%20Wang%20and%20Wenhao%20Xu%20and%20Ding%20Luo%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20aims%20to%20reduce%20the%20scales%20of%0Apretrained%20models%20for%20multiple%20downstream%20tasks.%20However%2C%20as%20the%20models%20keep%0Ascaling%20up%2C%20the%20memory%20footprint%20of%20existing%20PETL%20methods%20is%20not%20significantly%0Areduced%20compared%20to%20the%20reduction%20of%20learnable%20parameters.%20This%20limitation%0Ahinders%20the%20practical%20deployment%20of%20PETL%20methods%20on%20memory-constrained%20devices.%0ATo%20this%20end%2C%20we%20proposed%20a%20new%20PETL%20framework%2C%20called%20Structure%20to%20Activation%0A%28S2A%29%2C%20to%20reduce%20the%20memory%20footprint%20of%20activation%20during%20fine-tuning.%0ASpecifically%2C%20our%20framework%20consists%20of%3A%201%29%20Activation%20modules%20design%28i.e.%2C%0Abias%2C%20prompt%20and%20side%20modules%29%20in%20the%20parametric%20model%20structure%2C%20which%20results%0Ain%20a%20significant%20reduction%20of%20adjustable%20parameters%20and%20activation%20memory%3B%202%29%0A4-bit%20quantization%20of%20activations%20based%20on%20their%20derivatives%20for%20non-parametric%0Astructures%20%28e.g.%2C%20nonlinear%20functions%29%2C%20which%20maintains%20accuracy%20while%0Asignificantly%20reducing%20memory%20usage.%20Our%20S2A%20method%20consequently%20offers%20a%0Alightweight%20solution%20in%20terms%20of%20both%20parameters%20and%20memory%20footprint.%20We%0Aevaluated%20S2A%20with%20different%20backbones%20and%20performed%20extensive%20experiments%20on%0Avarious%20datasets%20to%20evaluate%20the%20effectiveness.%20The%20results%20show%20that%20our%0Amethods%20not%20only%20outperform%20existing%20PETL%20techniques%2C%20achieving%20a%20fourfold%0Areduction%20in%20GPU%20memory%20footprint%20on%20average%2C%20but%20also%20shows%20competitive%0Aperformance%20in%20accuracy%20with%20fewer%20tunable%20parameters.%20These%20demonstrate%20that%0Aour%20method%20is%20highly%20suitable%20for%20practical%20transfer%20learning%20on%0Ahardware-constrained%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08154v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2A%253A%2520A%2520Unified%2520Framework%2520for%2520Parameter%2520and%2520Memory%2520Efficient%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DTian%2520Jin%2520and%2520Enjun%2520Du%2520and%2520Changwei%2520Wang%2520and%2520Wenhao%2520Xu%2520and%2520Ding%2520Luo%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520aims%2520to%2520reduce%2520the%2520scales%2520of%250Apretrained%2520models%2520for%2520multiple%2520downstream%2520tasks.%2520However%252C%2520as%2520the%2520models%2520keep%250Ascaling%2520up%252C%2520the%2520memory%2520footprint%2520of%2520existing%2520PETL%2520methods%2520is%2520not%2520significantly%250Areduced%2520compared%2520to%2520the%2520reduction%2520of%2520learnable%2520parameters.%2520This%2520limitation%250Ahinders%2520the%2520practical%2520deployment%2520of%2520PETL%2520methods%2520on%2520memory-constrained%2520devices.%250ATo%2520this%2520end%252C%2520we%2520proposed%2520a%2520new%2520PETL%2520framework%252C%2520called%2520Structure%2520to%2520Activation%250A%2528S2A%2529%252C%2520to%2520reduce%2520the%2520memory%2520footprint%2520of%2520activation%2520during%2520fine-tuning.%250ASpecifically%252C%2520our%2520framework%2520consists%2520of%253A%25201%2529%2520Activation%2520modules%2520design%2528i.e.%252C%250Abias%252C%2520prompt%2520and%2520side%2520modules%2529%2520in%2520the%2520parametric%2520model%2520structure%252C%2520which%2520results%250Ain%2520a%2520significant%2520reduction%2520of%2520adjustable%2520parameters%2520and%2520activation%2520memory%253B%25202%2529%250A4-bit%2520quantization%2520of%2520activations%2520based%2520on%2520their%2520derivatives%2520for%2520non-parametric%250Astructures%2520%2528e.g.%252C%2520nonlinear%2520functions%2529%252C%2520which%2520maintains%2520accuracy%2520while%250Asignificantly%2520reducing%2520memory%2520usage.%2520Our%2520S2A%2520method%2520consequently%2520offers%2520a%250Alightweight%2520solution%2520in%2520terms%2520of%2520both%2520parameters%2520and%2520memory%2520footprint.%2520We%250Aevaluated%2520S2A%2520with%2520different%2520backbones%2520and%2520performed%2520extensive%2520experiments%2520on%250Avarious%2520datasets%2520to%2520evaluate%2520the%2520effectiveness.%2520The%2520results%2520show%2520that%2520our%250Amethods%2520not%2520only%2520outperform%2520existing%2520PETL%2520techniques%252C%2520achieving%2520a%2520fourfold%250Areduction%2520in%2520GPU%2520memory%2520footprint%2520on%2520average%252C%2520but%2520also%2520shows%2520competitive%250Aperformance%2520in%2520accuracy%2520with%2520fewer%2520tunable%2520parameters.%2520These%2520demonstrate%2520that%250Aour%2520method%2520is%2520highly%2520suitable%2520for%2520practical%2520transfer%2520learning%2520on%250Ahardware-constrained%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08154v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2A%3A%20A%20Unified%20Framework%20for%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning&entry.906535625=Tian%20Jin%20and%20Enjun%20Du%20and%20Changwei%20Wang%20and%20Wenhao%20Xu%20and%20Ding%20Luo&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20aims%20to%20reduce%20the%20scales%20of%0Apretrained%20models%20for%20multiple%20downstream%20tasks.%20However%2C%20as%20the%20models%20keep%0Ascaling%20up%2C%20the%20memory%20footprint%20of%20existing%20PETL%20methods%20is%20not%20significantly%0Areduced%20compared%20to%20the%20reduction%20of%20learnable%20parameters.%20This%20limitation%0Ahinders%20the%20practical%20deployment%20of%20PETL%20methods%20on%20memory-constrained%20devices.%0ATo%20this%20end%2C%20we%20proposed%20a%20new%20PETL%20framework%2C%20called%20Structure%20to%20Activation%0A%28S2A%29%2C%20to%20reduce%20the%20memory%20footprint%20of%20activation%20during%20fine-tuning.%0ASpecifically%2C%20our%20framework%20consists%20of%3A%201%29%20Activation%20modules%20design%28i.e.%2C%0Abias%2C%20prompt%20and%20side%20modules%29%20in%20the%20parametric%20model%20structure%2C%20which%20results%0Ain%20a%20significant%20reduction%20of%20adjustable%20parameters%20and%20activation%20memory%3B%202%29%0A4-bit%20quantization%20of%20activations%20based%20on%20their%20derivatives%20for%20non-parametric%0Astructures%20%28e.g.%2C%20nonlinear%20functions%29%2C%20which%20maintains%20accuracy%20while%0Asignificantly%20reducing%20memory%20usage.%20Our%20S2A%20method%20consequently%20offers%20a%0Alightweight%20solution%20in%20terms%20of%20both%20parameters%20and%20memory%20footprint.%20We%0Aevaluated%20S2A%20with%20different%20backbones%20and%20performed%20extensive%20experiments%20on%0Avarious%20datasets%20to%20evaluate%20the%20effectiveness.%20The%20results%20show%20that%20our%0Amethods%20not%20only%20outperform%20existing%20PETL%20techniques%2C%20achieving%20a%20fourfold%0Areduction%20in%20GPU%20memory%20footprint%20on%20average%2C%20but%20also%20shows%20competitive%0Aperformance%20in%20accuracy%20with%20fewer%20tunable%20parameters.%20These%20demonstrate%20that%0Aour%20method%20is%20highly%20suitable%20for%20practical%20transfer%20learning%20on%0Ahardware-constrained%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08154v3&entry.124074799=Read"},
{"title": "JFlow: Model-Independent Spherical Jeans Analysis using Equivariant\n  Continuous Normalizing Flows", "author": "Sung Hak Lim and Kohei Hayashi and Shun'ichi Horigome and Shigeki Matsumoto and Mihoko M. Nojiri", "abstract": "  The kinematics of stars in dwarf spheroidal galaxies have been studied to\nunderstand the structure of dark matter halos. However, the kinematic\ninformation of these stars is often limited to celestial positions and\nline-of-sight velocities, making full phase space analysis challenging.\nConventional methods rely on projected analytic phase space density models with\nseveral parameters and infer dark matter halo structures by solving the\nspherical Jeans equation. In this paper, we introduce an unsupervised machine\nlearning method for solving the spherical Jeans equation in a model-independent\nway as a first step toward model-independent analysis of dwarf spheroidal\ngalaxies. Using equivariant continuous normalizing flows, we demonstrate that\nspherically symmetric stellar phase space densities and velocity dispersions\ncan be estimated without model assumptions. As a proof of concept, we apply our\nmethod to Gaia challenge datasets for spherical models and measure dark matter\nmass densities for given velocity anisotropy profiles. Our method can identify\nhalo structures accurately, even with a small number of tracer stars.\n", "link": "http://arxiv.org/abs/2505.00763v2", "date": "2025-06-02", "relevancy": 1.9563, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5294}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4903}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JFlow%3A%20Model-Independent%20Spherical%20Jeans%20Analysis%20using%20Equivariant%0A%20%20Continuous%20Normalizing%20Flows&body=Title%3A%20JFlow%3A%20Model-Independent%20Spherical%20Jeans%20Analysis%20using%20Equivariant%0A%20%20Continuous%20Normalizing%20Flows%0AAuthor%3A%20Sung%20Hak%20Lim%20and%20Kohei%20Hayashi%20and%20Shun%27ichi%20Horigome%20and%20Shigeki%20Matsumoto%20and%20Mihoko%20M.%20Nojiri%0AAbstract%3A%20%20%20The%20kinematics%20of%20stars%20in%20dwarf%20spheroidal%20galaxies%20have%20been%20studied%20to%0Aunderstand%20the%20structure%20of%20dark%20matter%20halos.%20However%2C%20the%20kinematic%0Ainformation%20of%20these%20stars%20is%20often%20limited%20to%20celestial%20positions%20and%0Aline-of-sight%20velocities%2C%20making%20full%20phase%20space%20analysis%20challenging.%0AConventional%20methods%20rely%20on%20projected%20analytic%20phase%20space%20density%20models%20with%0Aseveral%20parameters%20and%20infer%20dark%20matter%20halo%20structures%20by%20solving%20the%0Aspherical%20Jeans%20equation.%20In%20this%20paper%2C%20we%20introduce%20an%20unsupervised%20machine%0Alearning%20method%20for%20solving%20the%20spherical%20Jeans%20equation%20in%20a%20model-independent%0Away%20as%20a%20first%20step%20toward%20model-independent%20analysis%20of%20dwarf%20spheroidal%0Agalaxies.%20Using%20equivariant%20continuous%20normalizing%20flows%2C%20we%20demonstrate%20that%0Aspherically%20symmetric%20stellar%20phase%20space%20densities%20and%20velocity%20dispersions%0Acan%20be%20estimated%20without%20model%20assumptions.%20As%20a%20proof%20of%20concept%2C%20we%20apply%20our%0Amethod%20to%20Gaia%20challenge%20datasets%20for%20spherical%20models%20and%20measure%20dark%20matter%0Amass%20densities%20for%20given%20velocity%20anisotropy%20profiles.%20Our%20method%20can%20identify%0Ahalo%20structures%20accurately%2C%20even%20with%20a%20small%20number%20of%20tracer%20stars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJFlow%253A%2520Model-Independent%2520Spherical%2520Jeans%2520Analysis%2520using%2520Equivariant%250A%2520%2520Continuous%2520Normalizing%2520Flows%26entry.906535625%3DSung%2520Hak%2520Lim%2520and%2520Kohei%2520Hayashi%2520and%2520Shun%2527ichi%2520Horigome%2520and%2520Shigeki%2520Matsumoto%2520and%2520Mihoko%2520M.%2520Nojiri%26entry.1292438233%3D%2520%2520The%2520kinematics%2520of%2520stars%2520in%2520dwarf%2520spheroidal%2520galaxies%2520have%2520been%2520studied%2520to%250Aunderstand%2520the%2520structure%2520of%2520dark%2520matter%2520halos.%2520However%252C%2520the%2520kinematic%250Ainformation%2520of%2520these%2520stars%2520is%2520often%2520limited%2520to%2520celestial%2520positions%2520and%250Aline-of-sight%2520velocities%252C%2520making%2520full%2520phase%2520space%2520analysis%2520challenging.%250AConventional%2520methods%2520rely%2520on%2520projected%2520analytic%2520phase%2520space%2520density%2520models%2520with%250Aseveral%2520parameters%2520and%2520infer%2520dark%2520matter%2520halo%2520structures%2520by%2520solving%2520the%250Aspherical%2520Jeans%2520equation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520unsupervised%2520machine%250Alearning%2520method%2520for%2520solving%2520the%2520spherical%2520Jeans%2520equation%2520in%2520a%2520model-independent%250Away%2520as%2520a%2520first%2520step%2520toward%2520model-independent%2520analysis%2520of%2520dwarf%2520spheroidal%250Agalaxies.%2520Using%2520equivariant%2520continuous%2520normalizing%2520flows%252C%2520we%2520demonstrate%2520that%250Aspherically%2520symmetric%2520stellar%2520phase%2520space%2520densities%2520and%2520velocity%2520dispersions%250Acan%2520be%2520estimated%2520without%2520model%2520assumptions.%2520As%2520a%2520proof%2520of%2520concept%252C%2520we%2520apply%2520our%250Amethod%2520to%2520Gaia%2520challenge%2520datasets%2520for%2520spherical%2520models%2520and%2520measure%2520dark%2520matter%250Amass%2520densities%2520for%2520given%2520velocity%2520anisotropy%2520profiles.%2520Our%2520method%2520can%2520identify%250Ahalo%2520structures%2520accurately%252C%2520even%2520with%2520a%2520small%2520number%2520of%2520tracer%2520stars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JFlow%3A%20Model-Independent%20Spherical%20Jeans%20Analysis%20using%20Equivariant%0A%20%20Continuous%20Normalizing%20Flows&entry.906535625=Sung%20Hak%20Lim%20and%20Kohei%20Hayashi%20and%20Shun%27ichi%20Horigome%20and%20Shigeki%20Matsumoto%20and%20Mihoko%20M.%20Nojiri&entry.1292438233=%20%20The%20kinematics%20of%20stars%20in%20dwarf%20spheroidal%20galaxies%20have%20been%20studied%20to%0Aunderstand%20the%20structure%20of%20dark%20matter%20halos.%20However%2C%20the%20kinematic%0Ainformation%20of%20these%20stars%20is%20often%20limited%20to%20celestial%20positions%20and%0Aline-of-sight%20velocities%2C%20making%20full%20phase%20space%20analysis%20challenging.%0AConventional%20methods%20rely%20on%20projected%20analytic%20phase%20space%20density%20models%20with%0Aseveral%20parameters%20and%20infer%20dark%20matter%20halo%20structures%20by%20solving%20the%0Aspherical%20Jeans%20equation.%20In%20this%20paper%2C%20we%20introduce%20an%20unsupervised%20machine%0Alearning%20method%20for%20solving%20the%20spherical%20Jeans%20equation%20in%20a%20model-independent%0Away%20as%20a%20first%20step%20toward%20model-independent%20analysis%20of%20dwarf%20spheroidal%0Agalaxies.%20Using%20equivariant%20continuous%20normalizing%20flows%2C%20we%20demonstrate%20that%0Aspherically%20symmetric%20stellar%20phase%20space%20densities%20and%20velocity%20dispersions%0Acan%20be%20estimated%20without%20model%20assumptions.%20As%20a%20proof%20of%20concept%2C%20we%20apply%20our%0Amethod%20to%20Gaia%20challenge%20datasets%20for%20spherical%20models%20and%20measure%20dark%20matter%0Amass%20densities%20for%20given%20velocity%20anisotropy%20profiles.%20Our%20method%20can%20identify%0Ahalo%20structures%20accurately%2C%20even%20with%20a%20small%20number%20of%20tracer%20stars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00763v2&entry.124074799=Read"},
{"title": "PepTune: De Novo Generation of Therapeutic Peptides with\n  Multi-Objective-Guided Discrete Diffusion", "author": "Sophia Tang and Yinuo Zhang and Pranam Chatterjee", "abstract": "  We present PepTune, a multi-objective discrete diffusion model for\nsimultaneous generation and optimization of therapeutic peptide SMILES. Built\non the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid\npeptide structures with a novel bond-dependent masking schedule and invalid\nloss function. To guide the diffusion process, we introduce Monte Carlo Tree\nGuidance (MCTG), an inference-time multi-objective guidance algorithm that\nbalances exploration and exploitation to iteratively refine Pareto-optimal\nsequences. MCTG integrates classifier-based rewards with search-tree expansion,\novercoming gradient estimation challenges and data sparsity. Using PepTune, we\ngenerate diverse, chemically-modified peptides simultaneously optimized for\nmultiple therapeutic properties, including target binding affinity, membrane\npermeability, solubility, hemolysis, and non-fouling for various\ndisease-relevant targets. In total, our results demonstrate that MCTG for\nmasked discrete diffusion is a powerful and modular approach for\nmulti-objective sequence design in discrete state spaces.\n", "link": "http://arxiv.org/abs/2412.17780v4", "date": "2025-06-02", "relevancy": 1.9481, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4973}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4849}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion&body=Title%3A%20PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion%0AAuthor%3A%20Sophia%20Tang%20and%20Yinuo%20Zhang%20and%20Pranam%20Chatterjee%0AAbstract%3A%20%20%20We%20present%20PepTune%2C%20a%20multi-objective%20discrete%20diffusion%20model%20for%0Asimultaneous%20generation%20and%20optimization%20of%20therapeutic%20peptide%20SMILES.%20Built%0Aon%20the%20Masked%20Discrete%20Language%20Model%20%28MDLM%29%20framework%2C%20PepTune%20ensures%20valid%0Apeptide%20structures%20with%20a%20novel%20bond-dependent%20masking%20schedule%20and%20invalid%0Aloss%20function.%20To%20guide%20the%20diffusion%20process%2C%20we%20introduce%20Monte%20Carlo%20Tree%0AGuidance%20%28MCTG%29%2C%20an%20inference-time%20multi-objective%20guidance%20algorithm%20that%0Abalances%20exploration%20and%20exploitation%20to%20iteratively%20refine%20Pareto-optimal%0Asequences.%20MCTG%20integrates%20classifier-based%20rewards%20with%20search-tree%20expansion%2C%0Aovercoming%20gradient%20estimation%20challenges%20and%20data%20sparsity.%20Using%20PepTune%2C%20we%0Agenerate%20diverse%2C%20chemically-modified%20peptides%20simultaneously%20optimized%20for%0Amultiple%20therapeutic%20properties%2C%20including%20target%20binding%20affinity%2C%20membrane%0Apermeability%2C%20solubility%2C%20hemolysis%2C%20and%20non-fouling%20for%20various%0Adisease-relevant%20targets.%20In%20total%2C%20our%20results%20demonstrate%20that%20MCTG%20for%0Amasked%20discrete%20diffusion%20is%20a%20powerful%20and%20modular%20approach%20for%0Amulti-objective%20sequence%20design%20in%20discrete%20state%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17780v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPepTune%253A%2520De%2520Novo%2520Generation%2520of%2520Therapeutic%2520Peptides%2520with%250A%2520%2520Multi-Objective-Guided%2520Discrete%2520Diffusion%26entry.906535625%3DSophia%2520Tang%2520and%2520Yinuo%2520Zhang%2520and%2520Pranam%2520Chatterjee%26entry.1292438233%3D%2520%2520We%2520present%2520PepTune%252C%2520a%2520multi-objective%2520discrete%2520diffusion%2520model%2520for%250Asimultaneous%2520generation%2520and%2520optimization%2520of%2520therapeutic%2520peptide%2520SMILES.%2520Built%250Aon%2520the%2520Masked%2520Discrete%2520Language%2520Model%2520%2528MDLM%2529%2520framework%252C%2520PepTune%2520ensures%2520valid%250Apeptide%2520structures%2520with%2520a%2520novel%2520bond-dependent%2520masking%2520schedule%2520and%2520invalid%250Aloss%2520function.%2520To%2520guide%2520the%2520diffusion%2520process%252C%2520we%2520introduce%2520Monte%2520Carlo%2520Tree%250AGuidance%2520%2528MCTG%2529%252C%2520an%2520inference-time%2520multi-objective%2520guidance%2520algorithm%2520that%250Abalances%2520exploration%2520and%2520exploitation%2520to%2520iteratively%2520refine%2520Pareto-optimal%250Asequences.%2520MCTG%2520integrates%2520classifier-based%2520rewards%2520with%2520search-tree%2520expansion%252C%250Aovercoming%2520gradient%2520estimation%2520challenges%2520and%2520data%2520sparsity.%2520Using%2520PepTune%252C%2520we%250Agenerate%2520diverse%252C%2520chemically-modified%2520peptides%2520simultaneously%2520optimized%2520for%250Amultiple%2520therapeutic%2520properties%252C%2520including%2520target%2520binding%2520affinity%252C%2520membrane%250Apermeability%252C%2520solubility%252C%2520hemolysis%252C%2520and%2520non-fouling%2520for%2520various%250Adisease-relevant%2520targets.%2520In%2520total%252C%2520our%2520results%2520demonstrate%2520that%2520MCTG%2520for%250Amasked%2520discrete%2520diffusion%2520is%2520a%2520powerful%2520and%2520modular%2520approach%2520for%250Amulti-objective%2520sequence%2520design%2520in%2520discrete%2520state%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17780v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion&entry.906535625=Sophia%20Tang%20and%20Yinuo%20Zhang%20and%20Pranam%20Chatterjee&entry.1292438233=%20%20We%20present%20PepTune%2C%20a%20multi-objective%20discrete%20diffusion%20model%20for%0Asimultaneous%20generation%20and%20optimization%20of%20therapeutic%20peptide%20SMILES.%20Built%0Aon%20the%20Masked%20Discrete%20Language%20Model%20%28MDLM%29%20framework%2C%20PepTune%20ensures%20valid%0Apeptide%20structures%20with%20a%20novel%20bond-dependent%20masking%20schedule%20and%20invalid%0Aloss%20function.%20To%20guide%20the%20diffusion%20process%2C%20we%20introduce%20Monte%20Carlo%20Tree%0AGuidance%20%28MCTG%29%2C%20an%20inference-time%20multi-objective%20guidance%20algorithm%20that%0Abalances%20exploration%20and%20exploitation%20to%20iteratively%20refine%20Pareto-optimal%0Asequences.%20MCTG%20integrates%20classifier-based%20rewards%20with%20search-tree%20expansion%2C%0Aovercoming%20gradient%20estimation%20challenges%20and%20data%20sparsity.%20Using%20PepTune%2C%20we%0Agenerate%20diverse%2C%20chemically-modified%20peptides%20simultaneously%20optimized%20for%0Amultiple%20therapeutic%20properties%2C%20including%20target%20binding%20affinity%2C%20membrane%0Apermeability%2C%20solubility%2C%20hemolysis%2C%20and%20non-fouling%20for%20various%0Adisease-relevant%20targets.%20In%20total%2C%20our%20results%20demonstrate%20that%20MCTG%20for%0Amasked%20discrete%20diffusion%20is%20a%20powerful%20and%20modular%20approach%20for%0Amulti-objective%20sequence%20design%20in%20discrete%20state%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17780v4&entry.124074799=Read"},
{"title": "ChitroJera: A Regionally Relevant Visual Question Answering Dataset for\n  Bangla", "author": "Deeparghya Dutta Barua and Md Sakib Ul Rahman Sourove and Md Fahim and Fabiha Haider and Fariha Tanjim Shifat and Md Tasmim Rahman Adib and Anam Borhan Uddin and Md Farhan Ishmam and Md Farhad Alam", "abstract": "  Visual Question Answer (VQA) poses the problem of answering a natural\nlanguage question about a visual context. Bangla, despite being a widely spoken\nlanguage, is considered low-resource in the realm of VQA due to the lack of\nproper benchmarks, challenging models known to be performant in other\nlanguages. Furthermore, existing Bangla VQA datasets offer little regional\nrelevance and are largely adapted from their foreign counterparts. To address\nthese challenges, we introduce a large-scale Bangla VQA dataset, ChitroJera,\ntotaling over 15k samples from diverse and locally relevant data sources. We\nassess the performance of text encoders, image encoders, multimodal models, and\nour novel dual-encoder models. The experiments reveal that the pre-trained\ndual-encoders outperform other models of their scale. We also evaluate the\nperformance of current large vision language models (LVLMs) using prompt-based\ntechniques, achieving the overall best performance. Given the underdeveloped\nstate of existing datasets, we envision ChitroJera expanding the scope of\nVision-Language tasks in Bangla.\n", "link": "http://arxiv.org/abs/2410.14991v2", "date": "2025-06-02", "relevancy": 1.9444, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChitroJera%3A%20A%20Regionally%20Relevant%20Visual%20Question%20Answering%20Dataset%20for%0A%20%20Bangla&body=Title%3A%20ChitroJera%3A%20A%20Regionally%20Relevant%20Visual%20Question%20Answering%20Dataset%20for%0A%20%20Bangla%0AAuthor%3A%20Deeparghya%20Dutta%20Barua%20and%20Md%20Sakib%20Ul%20Rahman%20Sourove%20and%20Md%20Fahim%20and%20Fabiha%20Haider%20and%20Fariha%20Tanjim%20Shifat%20and%20Md%20Tasmim%20Rahman%20Adib%20and%20Anam%20Borhan%20Uddin%20and%20Md%20Farhan%20Ishmam%20and%20Md%20Farhad%20Alam%0AAbstract%3A%20%20%20Visual%20Question%20Answer%20%28VQA%29%20poses%20the%20problem%20of%20answering%20a%20natural%0Alanguage%20question%20about%20a%20visual%20context.%20Bangla%2C%20despite%20being%20a%20widely%20spoken%0Alanguage%2C%20is%20considered%20low-resource%20in%20the%20realm%20of%20VQA%20due%20to%20the%20lack%20of%0Aproper%20benchmarks%2C%20challenging%20models%20known%20to%20be%20performant%20in%20other%0Alanguages.%20Furthermore%2C%20existing%20Bangla%20VQA%20datasets%20offer%20little%20regional%0Arelevance%20and%20are%20largely%20adapted%20from%20their%20foreign%20counterparts.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20large-scale%20Bangla%20VQA%20dataset%2C%20ChitroJera%2C%0Atotaling%20over%2015k%20samples%20from%20diverse%20and%20locally%20relevant%20data%20sources.%20We%0Aassess%20the%20performance%20of%20text%20encoders%2C%20image%20encoders%2C%20multimodal%20models%2C%20and%0Aour%20novel%20dual-encoder%20models.%20The%20experiments%20reveal%20that%20the%20pre-trained%0Adual-encoders%20outperform%20other%20models%20of%20their%20scale.%20We%20also%20evaluate%20the%0Aperformance%20of%20current%20large%20vision%20language%20models%20%28LVLMs%29%20using%20prompt-based%0Atechniques%2C%20achieving%20the%20overall%20best%20performance.%20Given%20the%20underdeveloped%0Astate%20of%20existing%20datasets%2C%20we%20envision%20ChitroJera%20expanding%20the%20scope%20of%0AVision-Language%20tasks%20in%20Bangla.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChitroJera%253A%2520A%2520Regionally%2520Relevant%2520Visual%2520Question%2520Answering%2520Dataset%2520for%250A%2520%2520Bangla%26entry.906535625%3DDeeparghya%2520Dutta%2520Barua%2520and%2520Md%2520Sakib%2520Ul%2520Rahman%2520Sourove%2520and%2520Md%2520Fahim%2520and%2520Fabiha%2520Haider%2520and%2520Fariha%2520Tanjim%2520Shifat%2520and%2520Md%2520Tasmim%2520Rahman%2520Adib%2520and%2520Anam%2520Borhan%2520Uddin%2520and%2520Md%2520Farhan%2520Ishmam%2520and%2520Md%2520Farhad%2520Alam%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answer%2520%2528VQA%2529%2520poses%2520the%2520problem%2520of%2520answering%2520a%2520natural%250Alanguage%2520question%2520about%2520a%2520visual%2520context.%2520Bangla%252C%2520despite%2520being%2520a%2520widely%2520spoken%250Alanguage%252C%2520is%2520considered%2520low-resource%2520in%2520the%2520realm%2520of%2520VQA%2520due%2520to%2520the%2520lack%2520of%250Aproper%2520benchmarks%252C%2520challenging%2520models%2520known%2520to%2520be%2520performant%2520in%2520other%250Alanguages.%2520Furthermore%252C%2520existing%2520Bangla%2520VQA%2520datasets%2520offer%2520little%2520regional%250Arelevance%2520and%2520are%2520largely%2520adapted%2520from%2520their%2520foreign%2520counterparts.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520a%2520large-scale%2520Bangla%2520VQA%2520dataset%252C%2520ChitroJera%252C%250Atotaling%2520over%252015k%2520samples%2520from%2520diverse%2520and%2520locally%2520relevant%2520data%2520sources.%2520We%250Aassess%2520the%2520performance%2520of%2520text%2520encoders%252C%2520image%2520encoders%252C%2520multimodal%2520models%252C%2520and%250Aour%2520novel%2520dual-encoder%2520models.%2520The%2520experiments%2520reveal%2520that%2520the%2520pre-trained%250Adual-encoders%2520outperform%2520other%2520models%2520of%2520their%2520scale.%2520We%2520also%2520evaluate%2520the%250Aperformance%2520of%2520current%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520using%2520prompt-based%250Atechniques%252C%2520achieving%2520the%2520overall%2520best%2520performance.%2520Given%2520the%2520underdeveloped%250Astate%2520of%2520existing%2520datasets%252C%2520we%2520envision%2520ChitroJera%2520expanding%2520the%2520scope%2520of%250AVision-Language%2520tasks%2520in%2520Bangla.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChitroJera%3A%20A%20Regionally%20Relevant%20Visual%20Question%20Answering%20Dataset%20for%0A%20%20Bangla&entry.906535625=Deeparghya%20Dutta%20Barua%20and%20Md%20Sakib%20Ul%20Rahman%20Sourove%20and%20Md%20Fahim%20and%20Fabiha%20Haider%20and%20Fariha%20Tanjim%20Shifat%20and%20Md%20Tasmim%20Rahman%20Adib%20and%20Anam%20Borhan%20Uddin%20and%20Md%20Farhan%20Ishmam%20and%20Md%20Farhad%20Alam&entry.1292438233=%20%20Visual%20Question%20Answer%20%28VQA%29%20poses%20the%20problem%20of%20answering%20a%20natural%0Alanguage%20question%20about%20a%20visual%20context.%20Bangla%2C%20despite%20being%20a%20widely%20spoken%0Alanguage%2C%20is%20considered%20low-resource%20in%20the%20realm%20of%20VQA%20due%20to%20the%20lack%20of%0Aproper%20benchmarks%2C%20challenging%20models%20known%20to%20be%20performant%20in%20other%0Alanguages.%20Furthermore%2C%20existing%20Bangla%20VQA%20datasets%20offer%20little%20regional%0Arelevance%20and%20are%20largely%20adapted%20from%20their%20foreign%20counterparts.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20large-scale%20Bangla%20VQA%20dataset%2C%20ChitroJera%2C%0Atotaling%20over%2015k%20samples%20from%20diverse%20and%20locally%20relevant%20data%20sources.%20We%0Aassess%20the%20performance%20of%20text%20encoders%2C%20image%20encoders%2C%20multimodal%20models%2C%20and%0Aour%20novel%20dual-encoder%20models.%20The%20experiments%20reveal%20that%20the%20pre-trained%0Adual-encoders%20outperform%20other%20models%20of%20their%20scale.%20We%20also%20evaluate%20the%0Aperformance%20of%20current%20large%20vision%20language%20models%20%28LVLMs%29%20using%20prompt-based%0Atechniques%2C%20achieving%20the%20overall%20best%20performance.%20Given%20the%20underdeveloped%0Astate%20of%20existing%20datasets%2C%20we%20envision%20ChitroJera%20expanding%20the%20scope%20of%0AVision-Language%20tasks%20in%20Bangla.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14991v2&entry.124074799=Read"},
{"title": "Beyond Pretty Pictures: Combined Single- and Multi-Image\n  Super-resolution for Sentinel-2 Images", "author": "Aditya Retnanto and Son Le and Sebastian Mueller and Armin Leitner and Michael Riffler and Konrad Schindler and Yohan Iddawela", "abstract": "  Super-resolution aims to increase the resolution of satellite images by\nreconstructing high-frequency details, which go beyond na\\\"ive upsampling. This\nhas particular relevance for Earth observation missions like Sentinel-2, which\noffer frequent, regular coverage at no cost; but at coarse resolution. Its\npixel footprint is too large to capture small features like houses, streets, or\nhedge rows. To address this, we present SEN4X, a hybrid super-resolution\narchitecture that combines the advantages of single-image and multi-image\ntechniques. It combines temporal oversampling from repeated Sentinel-2\nacquisitions with a learned prior from high-resolution Pl\\'eiades Neo data. In\ndoing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.\nWe test the super-resolved images on urban land-cover classification in Hanoi,\nVietnam. We find that they lead to a significant performance improvement over\nstate-of-the-art super-resolution baselines.\n", "link": "http://arxiv.org/abs/2505.24799v2", "date": "2025-06-02", "relevancy": 1.9343, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4894}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4808}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Pretty%20Pictures%3A%20Combined%20Single-%20and%20Multi-Image%0A%20%20Super-resolution%20for%20Sentinel-2%20Images&body=Title%3A%20Beyond%20Pretty%20Pictures%3A%20Combined%20Single-%20and%20Multi-Image%0A%20%20Super-resolution%20for%20Sentinel-2%20Images%0AAuthor%3A%20Aditya%20Retnanto%20and%20Son%20Le%20and%20Sebastian%20Mueller%20and%20Armin%20Leitner%20and%20Michael%20Riffler%20and%20Konrad%20Schindler%20and%20Yohan%20Iddawela%0AAbstract%3A%20%20%20Super-resolution%20aims%20to%20increase%20the%20resolution%20of%20satellite%20images%20by%0Areconstructing%20high-frequency%20details%2C%20which%20go%20beyond%20na%5C%22ive%20upsampling.%20This%0Ahas%20particular%20relevance%20for%20Earth%20observation%20missions%20like%20Sentinel-2%2C%20which%0Aoffer%20frequent%2C%20regular%20coverage%20at%20no%20cost%3B%20but%20at%20coarse%20resolution.%20Its%0Apixel%20footprint%20is%20too%20large%20to%20capture%20small%20features%20like%20houses%2C%20streets%2C%20or%0Ahedge%20rows.%20To%20address%20this%2C%20we%20present%20SEN4X%2C%20a%20hybrid%20super-resolution%0Aarchitecture%20that%20combines%20the%20advantages%20of%20single-image%20and%20multi-image%0Atechniques.%20It%20combines%20temporal%20oversampling%20from%20repeated%20Sentinel-2%0Aacquisitions%20with%20a%20learned%20prior%20from%20high-resolution%20Pl%5C%27eiades%20Neo%20data.%20In%0Adoing%20so%2C%20SEN4X%20upgrades%20Sentinel-2%20imagery%20to%202.5%20m%20ground%20sampling%20distance.%0AWe%20test%20the%20super-resolved%20images%20on%20urban%20land-cover%20classification%20in%20Hanoi%2C%0AVietnam.%20We%20find%20that%20they%20lead%20to%20a%20significant%20performance%20improvement%20over%0Astate-of-the-art%20super-resolution%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Pretty%2520Pictures%253A%2520Combined%2520Single-%2520and%2520Multi-Image%250A%2520%2520Super-resolution%2520for%2520Sentinel-2%2520Images%26entry.906535625%3DAditya%2520Retnanto%2520and%2520Son%2520Le%2520and%2520Sebastian%2520Mueller%2520and%2520Armin%2520Leitner%2520and%2520Michael%2520Riffler%2520and%2520Konrad%2520Schindler%2520and%2520Yohan%2520Iddawela%26entry.1292438233%3D%2520%2520Super-resolution%2520aims%2520to%2520increase%2520the%2520resolution%2520of%2520satellite%2520images%2520by%250Areconstructing%2520high-frequency%2520details%252C%2520which%2520go%2520beyond%2520na%255C%2522ive%2520upsampling.%2520This%250Ahas%2520particular%2520relevance%2520for%2520Earth%2520observation%2520missions%2520like%2520Sentinel-2%252C%2520which%250Aoffer%2520frequent%252C%2520regular%2520coverage%2520at%2520no%2520cost%253B%2520but%2520at%2520coarse%2520resolution.%2520Its%250Apixel%2520footprint%2520is%2520too%2520large%2520to%2520capture%2520small%2520features%2520like%2520houses%252C%2520streets%252C%2520or%250Ahedge%2520rows.%2520To%2520address%2520this%252C%2520we%2520present%2520SEN4X%252C%2520a%2520hybrid%2520super-resolution%250Aarchitecture%2520that%2520combines%2520the%2520advantages%2520of%2520single-image%2520and%2520multi-image%250Atechniques.%2520It%2520combines%2520temporal%2520oversampling%2520from%2520repeated%2520Sentinel-2%250Aacquisitions%2520with%2520a%2520learned%2520prior%2520from%2520high-resolution%2520Pl%255C%2527eiades%2520Neo%2520data.%2520In%250Adoing%2520so%252C%2520SEN4X%2520upgrades%2520Sentinel-2%2520imagery%2520to%25202.5%2520m%2520ground%2520sampling%2520distance.%250AWe%2520test%2520the%2520super-resolved%2520images%2520on%2520urban%2520land-cover%2520classification%2520in%2520Hanoi%252C%250AVietnam.%2520We%2520find%2520that%2520they%2520lead%2520to%2520a%2520significant%2520performance%2520improvement%2520over%250Astate-of-the-art%2520super-resolution%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Pretty%20Pictures%3A%20Combined%20Single-%20and%20Multi-Image%0A%20%20Super-resolution%20for%20Sentinel-2%20Images&entry.906535625=Aditya%20Retnanto%20and%20Son%20Le%20and%20Sebastian%20Mueller%20and%20Armin%20Leitner%20and%20Michael%20Riffler%20and%20Konrad%20Schindler%20and%20Yohan%20Iddawela&entry.1292438233=%20%20Super-resolution%20aims%20to%20increase%20the%20resolution%20of%20satellite%20images%20by%0Areconstructing%20high-frequency%20details%2C%20which%20go%20beyond%20na%5C%22ive%20upsampling.%20This%0Ahas%20particular%20relevance%20for%20Earth%20observation%20missions%20like%20Sentinel-2%2C%20which%0Aoffer%20frequent%2C%20regular%20coverage%20at%20no%20cost%3B%20but%20at%20coarse%20resolution.%20Its%0Apixel%20footprint%20is%20too%20large%20to%20capture%20small%20features%20like%20houses%2C%20streets%2C%20or%0Ahedge%20rows.%20To%20address%20this%2C%20we%20present%20SEN4X%2C%20a%20hybrid%20super-resolution%0Aarchitecture%20that%20combines%20the%20advantages%20of%20single-image%20and%20multi-image%0Atechniques.%20It%20combines%20temporal%20oversampling%20from%20repeated%20Sentinel-2%0Aacquisitions%20with%20a%20learned%20prior%20from%20high-resolution%20Pl%5C%27eiades%20Neo%20data.%20In%0Adoing%20so%2C%20SEN4X%20upgrades%20Sentinel-2%20imagery%20to%202.5%20m%20ground%20sampling%20distance.%0AWe%20test%20the%20super-resolved%20images%20on%20urban%20land-cover%20classification%20in%20Hanoi%2C%0AVietnam.%20We%20find%20that%20they%20lead%20to%20a%20significant%20performance%20improvement%20over%0Astate-of-the-art%20super-resolution%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24799v2&entry.124074799=Read"},
{"title": "Solving Multiagent Path Finding on Highly Centralized Networks", "author": "Foivos Fioravantes and Du\u0161an Knop and Jan Maty\u00e1\u0161 K\u0159i\u0161\u0165an and Nikolaos Melissinos and Michal Opler and Tung Anh Vu", "abstract": "  The Mutliagent Path Finding (MAPF) problem consists of identifying the\ntrajectories that a set of agents should follow inside a given network in order\nto reach their desired destinations as soon as possible, but without colliding\nwith each other. We aim to minimize the maximum time any agent takes to reach\ntheir goal, ensuring optimal path length. In this work, we complement a recent\nthread of results that aim to systematically study the algorithmic behavior of\nthis problem, through the parameterized complexity point of view.\n  First, we show that MAPF is NP-hard when the given network has a star-like\ntopology (bounded vertex cover number) or is a tree with $11$ leaves. Both of\nthese results fill important gaps in our understanding of the tractability of\nthis problem that were left untreated in the recent work of [Fioravantes et al.\nExact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike\nTopology. AAAI'24]. Nevertheless, our main contribution is an exact algorithm\nthat scales well as the input grows (FPT) when the topology of the given\nnetwork is highly centralized (bounded distance to clique). This parameter is\nsignificant as it mirrors real-world networks. In such environments, a bunch of\ncentral hubs (e.g., processing areas) are connected to only few peripheral\nnodes.\n", "link": "http://arxiv.org/abs/2412.09433v2", "date": "2025-06-02", "relevancy": 1.8748, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.467}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks&body=Title%3A%20Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks%0AAuthor%3A%20Foivos%20Fioravantes%20and%20Du%C5%A1an%20Knop%20and%20Jan%20Maty%C3%A1%C5%A1%20K%C5%99i%C5%A1%C5%A5an%20and%20Nikolaos%20Melissinos%20and%20Michal%20Opler%20and%20Tung%20Anh%20Vu%0AAbstract%3A%20%20%20The%20Mutliagent%20Path%20Finding%20%28MAPF%29%20problem%20consists%20of%20identifying%20the%0Atrajectories%20that%20a%20set%20of%20agents%20should%20follow%20inside%20a%20given%20network%20in%20order%0Ato%20reach%20their%20desired%20destinations%20as%20soon%20as%20possible%2C%20but%20without%20colliding%0Awith%20each%20other.%20We%20aim%20to%20minimize%20the%20maximum%20time%20any%20agent%20takes%20to%20reach%0Atheir%20goal%2C%20ensuring%20optimal%20path%20length.%20In%20this%20work%2C%20we%20complement%20a%20recent%0Athread%20of%20results%20that%20aim%20to%20systematically%20study%20the%20algorithmic%20behavior%20of%0Athis%20problem%2C%20through%20the%20parameterized%20complexity%20point%20of%20view.%0A%20%20First%2C%20we%20show%20that%20MAPF%20is%20NP-hard%20when%20the%20given%20network%20has%20a%20star-like%0Atopology%20%28bounded%20vertex%20cover%20number%29%20or%20is%20a%20tree%20with%20%2411%24%20leaves.%20Both%20of%0Athese%20results%20fill%20important%20gaps%20in%20our%20understanding%20of%20the%20tractability%20of%0Athis%20problem%20that%20were%20left%20untreated%20in%20the%20recent%20work%20of%20%5BFioravantes%20et%20al.%0AExact%20Algorithms%20and%20Lowerbounds%20for%20Multiagent%20Path%20Finding%3A%20Power%20of%20Treelike%0ATopology.%20AAAI%2724%5D.%20Nevertheless%2C%20our%20main%20contribution%20is%20an%20exact%20algorithm%0Athat%20scales%20well%20as%20the%20input%20grows%20%28FPT%29%20when%20the%20topology%20of%20the%20given%0Anetwork%20is%20highly%20centralized%20%28bounded%20distance%20to%20clique%29.%20This%20parameter%20is%0Asignificant%20as%20it%20mirrors%20real-world%20networks.%20In%20such%20environments%2C%20a%20bunch%20of%0Acentral%20hubs%20%28e.g.%2C%20processing%20areas%29%20are%20connected%20to%20only%20few%20peripheral%0Anodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Multiagent%2520Path%2520Finding%2520on%2520Highly%2520Centralized%2520Networks%26entry.906535625%3DFoivos%2520Fioravantes%2520and%2520Du%25C5%25A1an%2520Knop%2520and%2520Jan%2520Maty%25C3%25A1%25C5%25A1%2520K%25C5%2599i%25C5%25A1%25C5%25A5an%2520and%2520Nikolaos%2520Melissinos%2520and%2520Michal%2520Opler%2520and%2520Tung%2520Anh%2520Vu%26entry.1292438233%3D%2520%2520The%2520Mutliagent%2520Path%2520Finding%2520%2528MAPF%2529%2520problem%2520consists%2520of%2520identifying%2520the%250Atrajectories%2520that%2520a%2520set%2520of%2520agents%2520should%2520follow%2520inside%2520a%2520given%2520network%2520in%2520order%250Ato%2520reach%2520their%2520desired%2520destinations%2520as%2520soon%2520as%2520possible%252C%2520but%2520without%2520colliding%250Awith%2520each%2520other.%2520We%2520aim%2520to%2520minimize%2520the%2520maximum%2520time%2520any%2520agent%2520takes%2520to%2520reach%250Atheir%2520goal%252C%2520ensuring%2520optimal%2520path%2520length.%2520In%2520this%2520work%252C%2520we%2520complement%2520a%2520recent%250Athread%2520of%2520results%2520that%2520aim%2520to%2520systematically%2520study%2520the%2520algorithmic%2520behavior%2520of%250Athis%2520problem%252C%2520through%2520the%2520parameterized%2520complexity%2520point%2520of%2520view.%250A%2520%2520First%252C%2520we%2520show%2520that%2520MAPF%2520is%2520NP-hard%2520when%2520the%2520given%2520network%2520has%2520a%2520star-like%250Atopology%2520%2528bounded%2520vertex%2520cover%2520number%2529%2520or%2520is%2520a%2520tree%2520with%2520%252411%2524%2520leaves.%2520Both%2520of%250Athese%2520results%2520fill%2520important%2520gaps%2520in%2520our%2520understanding%2520of%2520the%2520tractability%2520of%250Athis%2520problem%2520that%2520were%2520left%2520untreated%2520in%2520the%2520recent%2520work%2520of%2520%255BFioravantes%2520et%2520al.%250AExact%2520Algorithms%2520and%2520Lowerbounds%2520for%2520Multiagent%2520Path%2520Finding%253A%2520Power%2520of%2520Treelike%250ATopology.%2520AAAI%252724%255D.%2520Nevertheless%252C%2520our%2520main%2520contribution%2520is%2520an%2520exact%2520algorithm%250Athat%2520scales%2520well%2520as%2520the%2520input%2520grows%2520%2528FPT%2529%2520when%2520the%2520topology%2520of%2520the%2520given%250Anetwork%2520is%2520highly%2520centralized%2520%2528bounded%2520distance%2520to%2520clique%2529.%2520This%2520parameter%2520is%250Asignificant%2520as%2520it%2520mirrors%2520real-world%2520networks.%2520In%2520such%2520environments%252C%2520a%2520bunch%2520of%250Acentral%2520hubs%2520%2528e.g.%252C%2520processing%2520areas%2529%2520are%2520connected%2520to%2520only%2520few%2520peripheral%250Anodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Multiagent%20Path%20Finding%20on%20Highly%20Centralized%20Networks&entry.906535625=Foivos%20Fioravantes%20and%20Du%C5%A1an%20Knop%20and%20Jan%20Maty%C3%A1%C5%A1%20K%C5%99i%C5%A1%C5%A5an%20and%20Nikolaos%20Melissinos%20and%20Michal%20Opler%20and%20Tung%20Anh%20Vu&entry.1292438233=%20%20The%20Mutliagent%20Path%20Finding%20%28MAPF%29%20problem%20consists%20of%20identifying%20the%0Atrajectories%20that%20a%20set%20of%20agents%20should%20follow%20inside%20a%20given%20network%20in%20order%0Ato%20reach%20their%20desired%20destinations%20as%20soon%20as%20possible%2C%20but%20without%20colliding%0Awith%20each%20other.%20We%20aim%20to%20minimize%20the%20maximum%20time%20any%20agent%20takes%20to%20reach%0Atheir%20goal%2C%20ensuring%20optimal%20path%20length.%20In%20this%20work%2C%20we%20complement%20a%20recent%0Athread%20of%20results%20that%20aim%20to%20systematically%20study%20the%20algorithmic%20behavior%20of%0Athis%20problem%2C%20through%20the%20parameterized%20complexity%20point%20of%20view.%0A%20%20First%2C%20we%20show%20that%20MAPF%20is%20NP-hard%20when%20the%20given%20network%20has%20a%20star-like%0Atopology%20%28bounded%20vertex%20cover%20number%29%20or%20is%20a%20tree%20with%20%2411%24%20leaves.%20Both%20of%0Athese%20results%20fill%20important%20gaps%20in%20our%20understanding%20of%20the%20tractability%20of%0Athis%20problem%20that%20were%20left%20untreated%20in%20the%20recent%20work%20of%20%5BFioravantes%20et%20al.%0AExact%20Algorithms%20and%20Lowerbounds%20for%20Multiagent%20Path%20Finding%3A%20Power%20of%20Treelike%0ATopology.%20AAAI%2724%5D.%20Nevertheless%2C%20our%20main%20contribution%20is%20an%20exact%20algorithm%0Athat%20scales%20well%20as%20the%20input%20grows%20%28FPT%29%20when%20the%20topology%20of%20the%20given%0Anetwork%20is%20highly%20centralized%20%28bounded%20distance%20to%20clique%29.%20This%20parameter%20is%0Asignificant%20as%20it%20mirrors%20real-world%20networks.%20In%20such%20environments%2C%20a%20bunch%20of%0Acentral%20hubs%20%28e.g.%2C%20processing%20areas%29%20are%20connected%20to%20only%20few%20peripheral%0Anodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09433v2&entry.124074799=Read"},
{"title": "Inverse Q-Learning Done Right: Offline Imitation Learning in\n  $Q^\u03c0$-Realizable MDPs", "author": "Antoine Moulin and Gergely Neu and Luca Viano", "abstract": "  We study the problem of offline imitation learning in Markov decision\nprocesses (MDPs), where the goal is to learn a well-performing policy given a\ndataset of state-action pairs generated by an expert policy. Complementing a\nrecent line of work on this topic that assumes the expert belongs to a\ntractable class of known policies, we approach this problem from a new angle\nand leverage a different type of structural assumption about the environment.\nSpecifically, for the class of linear $Q^\\pi$-realizable MDPs, we introduce a\nnew algorithm called saddle-point offline imitation learning (\\SPOIL), which is\nguaranteed to match the performance of any expert up to an additive error\n$\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover,\nwe extend this result to possibly non-linear $Q^\\pi$-realizable MDPs at the\ncost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$.\nFinally, our analysis suggests a new loss function for training critic networks\nfrom expert data in deep imitation learning. Empirical evaluations on standard\nbenchmarks demonstrate that the neural net implementation of \\SPOIL is superior\nto behavior cloning and competitive with state-of-the-art algorithms.\n", "link": "http://arxiv.org/abs/2505.19946v2", "date": "2025-06-02", "relevancy": 1.8699, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&body=Title%3A%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs%0AAuthor%3A%20Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Q-Learning%2520Done%2520Right%253A%2520Offline%2520Imitation%2520Learning%2520in%250A%2520%2520%2524Q%255E%25CF%2580%2524-Realizable%2520MDPs%26entry.906535625%3DAntoine%2520Moulin%2520and%2520Gergely%2520Neu%2520and%2520Luca%2520Viano%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520offline%2520imitation%2520learning%2520in%2520Markov%2520decision%250Aprocesses%2520%2528MDPs%2529%252C%2520where%2520the%2520goal%2520is%2520to%2520learn%2520a%2520well-performing%2520policy%2520given%2520a%250Adataset%2520of%2520state-action%2520pairs%2520generated%2520by%2520an%2520expert%2520policy.%2520Complementing%2520a%250Arecent%2520line%2520of%2520work%2520on%2520this%2520topic%2520that%2520assumes%2520the%2520expert%2520belongs%2520to%2520a%250Atractable%2520class%2520of%2520known%2520policies%252C%2520we%2520approach%2520this%2520problem%2520from%2520a%2520new%2520angle%250Aand%2520leverage%2520a%2520different%2520type%2520of%2520structural%2520assumption%2520about%2520the%2520environment.%250ASpecifically%252C%2520for%2520the%2520class%2520of%2520linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%252C%2520we%2520introduce%2520a%250Anew%2520algorithm%2520called%2520saddle-point%2520offline%2520imitation%2520learning%2520%2528%255CSPOIL%2529%252C%2520which%2520is%250Aguaranteed%2520to%2520match%2520the%2520performance%2520of%2520any%2520expert%2520up%2520to%2520an%2520additive%2520error%250A%2524%255Cvarepsilon%2524%2520with%2520access%2520to%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-2%257D%2529%2524%2520samples.%2520Moreover%252C%250Awe%2520extend%2520this%2520result%2520to%2520possibly%2520non-linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%2520at%2520the%250Acost%2520of%2520a%2520worse%2520sample%2520complexity%2520of%2520order%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-4%257D%2529%2524.%250AFinally%252C%2520our%2520analysis%2520suggests%2520a%2520new%2520loss%2520function%2520for%2520training%2520critic%2520networks%250Afrom%2520expert%2520data%2520in%2520deep%2520imitation%2520learning.%2520Empirical%2520evaluations%2520on%2520standard%250Abenchmarks%2520demonstrate%2520that%2520the%2520neural%2520net%2520implementation%2520of%2520%255CSPOIL%2520is%2520superior%250Ato%2520behavior%2520cloning%2520and%2520competitive%2520with%2520state-of-the-art%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&entry.906535625=Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano&entry.1292438233=%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19946v2&entry.124074799=Read"},
{"title": "How well do LLMs reason over tabular data, really?", "author": "Cornelius Wolff and Madelon Hulsebos", "abstract": "  Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.\n", "link": "http://arxiv.org/abs/2505.07453v2", "date": "2025-06-02", "relevancy": 1.8615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4679}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20well%20do%20LLMs%20reason%20over%20tabular%20data%2C%20really%3F&body=Title%3A%20How%20well%20do%20LLMs%20reason%20over%20tabular%20data%2C%20really%3F%0AAuthor%3A%20Cornelius%20Wolff%20and%20Madelon%20Hulsebos%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20natural%20language%20tasks%2C%20but%20less%20is%0Aknown%20about%20their%20reasoning%20capabilities%20over%20tabular%20data.%20Prior%20analyses%0Adevise%20evaluation%20strategies%20that%20poorly%20reflect%20an%20LLM%27s%20realistic%20performance%0Aon%20tabular%20queries.%20Moreover%2C%20we%20have%20a%20limited%20understanding%20of%20the%20robustness%0Aof%20LLMs%20towards%20realistic%20variations%20in%20tabular%20inputs.%20Therefore%2C%20we%20ask%3A%20Can%0Ageneral-purpose%20LLMs%20reason%20over%20tabular%20data%2C%20really%3F%2C%20and%20focus%20on%20two%0Aquestions%201%29%20are%20tabular%20reasoning%20capabilities%20of%20general-purpose%20LLMs%20robust%0Ato%20real-world%20characteristics%20of%20tabular%20inputs%2C%20and%202%29%20how%20can%20we%0Arealistically%20evaluate%20an%20LLM%27s%20performance%20on%20analytical%20tabular%20queries%3F%0ABuilding%20on%20a%20recent%20tabular%20reasoning%20benchmark%2C%20we%20first%20surface%20shortcomings%0Aof%20its%20multiple-choice%20prompt%20evaluation%20strategy%2C%20as%20well%20as%20commonly%20used%0Afree-form%20text%20metrics%20such%20as%20SacreBleu%20and%20BERT-score.%20We%20show%20that%20an%0ALLM-as-a-judge%20procedure%20yields%20more%20reliable%20performance%20insights%20and%20unveil%20a%0Asignificant%20deficit%20in%20tabular%20reasoning%20performance%20of%20LLMs.%20We%20then%20extend%0Athe%20tabular%20inputs%20reflecting%20three%20common%20characteristics%20in%20practice%3A%201%29%0Amissing%20values%2C%202%29%20duplicate%20entities%2C%20and%203%29%20structural%20variations.%0AExperiments%20show%20that%20the%20tabular%20reasoning%20capabilities%20of%20general-purpose%0ALLMs%20suffer%20from%20these%20variations%2C%20stressing%20the%20importance%20of%20improving%20their%0Arobustness%20for%20realistic%20tabular%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07453v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520well%2520do%2520LLMs%2520reason%2520over%2520tabular%2520data%252C%2520really%253F%26entry.906535625%3DCornelius%2520Wolff%2520and%2520Madelon%2520Hulsebos%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520natural%2520language%2520tasks%252C%2520but%2520less%2520is%250Aknown%2520about%2520their%2520reasoning%2520capabilities%2520over%2520tabular%2520data.%2520Prior%2520analyses%250Adevise%2520evaluation%2520strategies%2520that%2520poorly%2520reflect%2520an%2520LLM%2527s%2520realistic%2520performance%250Aon%2520tabular%2520queries.%2520Moreover%252C%2520we%2520have%2520a%2520limited%2520understanding%2520of%2520the%2520robustness%250Aof%2520LLMs%2520towards%2520realistic%2520variations%2520in%2520tabular%2520inputs.%2520Therefore%252C%2520we%2520ask%253A%2520Can%250Ageneral-purpose%2520LLMs%2520reason%2520over%2520tabular%2520data%252C%2520really%253F%252C%2520and%2520focus%2520on%2520two%250Aquestions%25201%2529%2520are%2520tabular%2520reasoning%2520capabilities%2520of%2520general-purpose%2520LLMs%2520robust%250Ato%2520real-world%2520characteristics%2520of%2520tabular%2520inputs%252C%2520and%25202%2529%2520how%2520can%2520we%250Arealistically%2520evaluate%2520an%2520LLM%2527s%2520performance%2520on%2520analytical%2520tabular%2520queries%253F%250ABuilding%2520on%2520a%2520recent%2520tabular%2520reasoning%2520benchmark%252C%2520we%2520first%2520surface%2520shortcomings%250Aof%2520its%2520multiple-choice%2520prompt%2520evaluation%2520strategy%252C%2520as%2520well%2520as%2520commonly%2520used%250Afree-form%2520text%2520metrics%2520such%2520as%2520SacreBleu%2520and%2520BERT-score.%2520We%2520show%2520that%2520an%250ALLM-as-a-judge%2520procedure%2520yields%2520more%2520reliable%2520performance%2520insights%2520and%2520unveil%2520a%250Asignificant%2520deficit%2520in%2520tabular%2520reasoning%2520performance%2520of%2520LLMs.%2520We%2520then%2520extend%250Athe%2520tabular%2520inputs%2520reflecting%2520three%2520common%2520characteristics%2520in%2520practice%253A%25201%2529%250Amissing%2520values%252C%25202%2529%2520duplicate%2520entities%252C%2520and%25203%2529%2520structural%2520variations.%250AExperiments%2520show%2520that%2520the%2520tabular%2520reasoning%2520capabilities%2520of%2520general-purpose%250ALLMs%2520suffer%2520from%2520these%2520variations%252C%2520stressing%2520the%2520importance%2520of%2520improving%2520their%250Arobustness%2520for%2520realistic%2520tabular%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07453v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20well%20do%20LLMs%20reason%20over%20tabular%20data%2C%20really%3F&entry.906535625=Cornelius%20Wolff%20and%20Madelon%20Hulsebos&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20natural%20language%20tasks%2C%20but%20less%20is%0Aknown%20about%20their%20reasoning%20capabilities%20over%20tabular%20data.%20Prior%20analyses%0Adevise%20evaluation%20strategies%20that%20poorly%20reflect%20an%20LLM%27s%20realistic%20performance%0Aon%20tabular%20queries.%20Moreover%2C%20we%20have%20a%20limited%20understanding%20of%20the%20robustness%0Aof%20LLMs%20towards%20realistic%20variations%20in%20tabular%20inputs.%20Therefore%2C%20we%20ask%3A%20Can%0Ageneral-purpose%20LLMs%20reason%20over%20tabular%20data%2C%20really%3F%2C%20and%20focus%20on%20two%0Aquestions%201%29%20are%20tabular%20reasoning%20capabilities%20of%20general-purpose%20LLMs%20robust%0Ato%20real-world%20characteristics%20of%20tabular%20inputs%2C%20and%202%29%20how%20can%20we%0Arealistically%20evaluate%20an%20LLM%27s%20performance%20on%20analytical%20tabular%20queries%3F%0ABuilding%20on%20a%20recent%20tabular%20reasoning%20benchmark%2C%20we%20first%20surface%20shortcomings%0Aof%20its%20multiple-choice%20prompt%20evaluation%20strategy%2C%20as%20well%20as%20commonly%20used%0Afree-form%20text%20metrics%20such%20as%20SacreBleu%20and%20BERT-score.%20We%20show%20that%20an%0ALLM-as-a-judge%20procedure%20yields%20more%20reliable%20performance%20insights%20and%20unveil%20a%0Asignificant%20deficit%20in%20tabular%20reasoning%20performance%20of%20LLMs.%20We%20then%20extend%0Athe%20tabular%20inputs%20reflecting%20three%20common%20characteristics%20in%20practice%3A%201%29%0Amissing%20values%2C%202%29%20duplicate%20entities%2C%20and%203%29%20structural%20variations.%0AExperiments%20show%20that%20the%20tabular%20reasoning%20capabilities%20of%20general-purpose%0ALLMs%20suffer%20from%20these%20variations%2C%20stressing%20the%20importance%20of%20improving%20their%0Arobustness%20for%20realistic%20tabular%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07453v2&entry.124074799=Read"},
{"title": "E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and\n  Effectiveness", "author": "Yibo Zhao and Jiapeng Zhu and Ye Guo and Kangkang He and Xiang Li", "abstract": "  Graph-based RAG methods like GraphRAG have shown promising global\nunderstanding of the knowledge base by constructing hierarchical entity graphs.\nHowever, they often suffer from inefficiency and rely on manually pre-defined\nquery modes, limiting practical use. In this paper, we propose E^2GraphRAG, a\nstreamlined graph-based RAG framework that improves both Efficiency and\nEffectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree\nwith large language models and an entity graph with SpaCy based on document\nchunks. We then construct bidirectional indexes between entities and chunks to\ncapture their many-to-many relationships, enabling fast lookup during both\nlocal and global retrieval. For the retrieval stage, we design an adaptive\nretrieval strategy that leverages the graph structure to retrieve and select\nbetween local and global modes. Experiments show that E^2GraphRAG achieves up\nto 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG\nin retrieval while maintaining competitive QA performance.\n", "link": "http://arxiv.org/abs/2505.24226v2", "date": "2025-06-02", "relevancy": 1.8234, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.495}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4686}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E%5E2GraphRAG%3A%20Streamlining%20Graph-based%20RAG%20for%20High%20Efficiency%20and%0A%20%20Effectiveness&body=Title%3A%20E%5E2GraphRAG%3A%20Streamlining%20Graph-based%20RAG%20for%20High%20Efficiency%20and%0A%20%20Effectiveness%0AAuthor%3A%20Yibo%20Zhao%20and%20Jiapeng%20Zhu%20and%20Ye%20Guo%20and%20Kangkang%20He%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Graph-based%20RAG%20methods%20like%20GraphRAG%20have%20shown%20promising%20global%0Aunderstanding%20of%20the%20knowledge%20base%20by%20constructing%20hierarchical%20entity%20graphs.%0AHowever%2C%20they%20often%20suffer%20from%20inefficiency%20and%20rely%20on%20manually%20pre-defined%0Aquery%20modes%2C%20limiting%20practical%20use.%20In%20this%20paper%2C%20we%20propose%20E%5E2GraphRAG%2C%20a%0Astreamlined%20graph-based%20RAG%20framework%20that%20improves%20both%20Efficiency%20and%0AEffectiveness.%20During%20the%20indexing%20stage%2C%20E%5E2GraphRAG%20constructs%20a%20summary%20tree%0Awith%20large%20language%20models%20and%20an%20entity%20graph%20with%20SpaCy%20based%20on%20document%0Achunks.%20We%20then%20construct%20bidirectional%20indexes%20between%20entities%20and%20chunks%20to%0Acapture%20their%20many-to-many%20relationships%2C%20enabling%20fast%20lookup%20during%20both%0Alocal%20and%20global%20retrieval.%20For%20the%20retrieval%20stage%2C%20we%20design%20an%20adaptive%0Aretrieval%20strategy%20that%20leverages%20the%20graph%20structure%20to%20retrieve%20and%20select%0Abetween%20local%20and%20global%20modes.%20Experiments%20show%20that%20E%5E2GraphRAG%20achieves%20up%0Ato%2010%20times%20faster%20indexing%20than%20GraphRAG%20and%20100%20times%20speedup%20over%20LightRAG%0Ain%20retrieval%20while%20maintaining%20competitive%20QA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE%255E2GraphRAG%253A%2520Streamlining%2520Graph-based%2520RAG%2520for%2520High%2520Efficiency%2520and%250A%2520%2520Effectiveness%26entry.906535625%3DYibo%2520Zhao%2520and%2520Jiapeng%2520Zhu%2520and%2520Ye%2520Guo%2520and%2520Kangkang%2520He%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Graph-based%2520RAG%2520methods%2520like%2520GraphRAG%2520have%2520shown%2520promising%2520global%250Aunderstanding%2520of%2520the%2520knowledge%2520base%2520by%2520constructing%2520hierarchical%2520entity%2520graphs.%250AHowever%252C%2520they%2520often%2520suffer%2520from%2520inefficiency%2520and%2520rely%2520on%2520manually%2520pre-defined%250Aquery%2520modes%252C%2520limiting%2520practical%2520use.%2520In%2520this%2520paper%252C%2520we%2520propose%2520E%255E2GraphRAG%252C%2520a%250Astreamlined%2520graph-based%2520RAG%2520framework%2520that%2520improves%2520both%2520Efficiency%2520and%250AEffectiveness.%2520During%2520the%2520indexing%2520stage%252C%2520E%255E2GraphRAG%2520constructs%2520a%2520summary%2520tree%250Awith%2520large%2520language%2520models%2520and%2520an%2520entity%2520graph%2520with%2520SpaCy%2520based%2520on%2520document%250Achunks.%2520We%2520then%2520construct%2520bidirectional%2520indexes%2520between%2520entities%2520and%2520chunks%2520to%250Acapture%2520their%2520many-to-many%2520relationships%252C%2520enabling%2520fast%2520lookup%2520during%2520both%250Alocal%2520and%2520global%2520retrieval.%2520For%2520the%2520retrieval%2520stage%252C%2520we%2520design%2520an%2520adaptive%250Aretrieval%2520strategy%2520that%2520leverages%2520the%2520graph%2520structure%2520to%2520retrieve%2520and%2520select%250Abetween%2520local%2520and%2520global%2520modes.%2520Experiments%2520show%2520that%2520E%255E2GraphRAG%2520achieves%2520up%250Ato%252010%2520times%2520faster%2520indexing%2520than%2520GraphRAG%2520and%2520100%2520times%2520speedup%2520over%2520LightRAG%250Ain%2520retrieval%2520while%2520maintaining%2520competitive%2520QA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E%5E2GraphRAG%3A%20Streamlining%20Graph-based%20RAG%20for%20High%20Efficiency%20and%0A%20%20Effectiveness&entry.906535625=Yibo%20Zhao%20and%20Jiapeng%20Zhu%20and%20Ye%20Guo%20and%20Kangkang%20He%20and%20Xiang%20Li&entry.1292438233=%20%20Graph-based%20RAG%20methods%20like%20GraphRAG%20have%20shown%20promising%20global%0Aunderstanding%20of%20the%20knowledge%20base%20by%20constructing%20hierarchical%20entity%20graphs.%0AHowever%2C%20they%20often%20suffer%20from%20inefficiency%20and%20rely%20on%20manually%20pre-defined%0Aquery%20modes%2C%20limiting%20practical%20use.%20In%20this%20paper%2C%20we%20propose%20E%5E2GraphRAG%2C%20a%0Astreamlined%20graph-based%20RAG%20framework%20that%20improves%20both%20Efficiency%20and%0AEffectiveness.%20During%20the%20indexing%20stage%2C%20E%5E2GraphRAG%20constructs%20a%20summary%20tree%0Awith%20large%20language%20models%20and%20an%20entity%20graph%20with%20SpaCy%20based%20on%20document%0Achunks.%20We%20then%20construct%20bidirectional%20indexes%20between%20entities%20and%20chunks%20to%0Acapture%20their%20many-to-many%20relationships%2C%20enabling%20fast%20lookup%20during%20both%0Alocal%20and%20global%20retrieval.%20For%20the%20retrieval%20stage%2C%20we%20design%20an%20adaptive%0Aretrieval%20strategy%20that%20leverages%20the%20graph%20structure%20to%20retrieve%20and%20select%0Abetween%20local%20and%20global%20modes.%20Experiments%20show%20that%20E%5E2GraphRAG%20achieves%20up%0Ato%2010%20times%20faster%20indexing%20than%20GraphRAG%20and%20100%20times%20speedup%20over%20LightRAG%0Ain%20retrieval%20while%20maintaining%20competitive%20QA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24226v2&entry.124074799=Read"},
{"title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph", "author": "Yibo Zhao and Jiapeng Zhu and Can Xu and Yao Liu and Xiang Li", "abstract": "  The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox.\n", "link": "http://arxiv.org/abs/2412.15268v4", "date": "2025-06-02", "relevancy": 1.7853, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4677}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4536}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LLM-based%20Hatred%20and%20Toxicity%20Detection%20with%20Meta-Toxic%0A%20%20Knowledge%20Graph&body=Title%3A%20Enhancing%20LLM-based%20Hatred%20and%20Toxicity%20Detection%20with%20Meta-Toxic%0A%20%20Knowledge%20Graph%0AAuthor%3A%20Yibo%20Zhao%20and%20Jiapeng%20Zhu%20and%20Can%20Xu%20and%20Yao%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20social%20media%20platforms%20has%20raised%20significant%20concerns%0Aregarding%20online%20content%20toxicity.%20When%20Large%20Language%20Models%20%28LLMs%29%20are%20used%0Afor%20toxicity%20detection%2C%20two%20key%20challenges%20emerge%3A%201%29%20the%20absence%20of%0Adomain-specific%20toxic%20knowledge%20leads%20to%20false%20negatives%3B%202%29%20the%20excessive%0Asensitivity%20of%20LLMs%20to%20toxic%20speech%20results%20in%20false%20positives%2C%20limiting%0Afreedom%20of%20speech.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20called%0AMetaTox%2C%20leveraging%20graph%20search%20on%20a%20meta-toxic%20knowledge%20graph%20to%20enhance%0Ahatred%20and%20toxicity%20detection.%20First%2C%20we%20construct%20a%20comprehensive%20meta-toxic%0Aknowledge%20graph%20by%20utilizing%20LLMs%20to%20extract%20toxic%20information%20through%20a%0Athree-step%20pipeline%2C%20with%20toxic%20benchmark%20datasets%20serving%20as%20corpora.%20Second%2C%0Awe%20query%20the%20graph%20via%20retrieval%20and%20ranking%20processes%20to%20supplement%20accurate%2C%0Arelevant%20toxic%20knowledge.%20Extensive%20experiments%20and%20in-depth%20case%20studies%0Aacross%20multiple%20datasets%20demonstrate%20that%20our%20MetaTox%20significantly%20decreases%0Athe%20false%20positive%20rate%20while%20boosting%20overall%20toxicity%20detection%20performance.%0AOur%20code%20is%20available%20at%20https%3A//github.com/YiboZhao624/MetaTox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15268v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LLM-based%2520Hatred%2520and%2520Toxicity%2520Detection%2520with%2520Meta-Toxic%250A%2520%2520Knowledge%2520Graph%26entry.906535625%3DYibo%2520Zhao%2520and%2520Jiapeng%2520Zhu%2520and%2520Can%2520Xu%2520and%2520Yao%2520Liu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520social%2520media%2520platforms%2520has%2520raised%2520significant%2520concerns%250Aregarding%2520online%2520content%2520toxicity.%2520When%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520used%250Afor%2520toxicity%2520detection%252C%2520two%2520key%2520challenges%2520emerge%253A%25201%2529%2520the%2520absence%2520of%250Adomain-specific%2520toxic%2520knowledge%2520leads%2520to%2520false%2520negatives%253B%25202%2529%2520the%2520excessive%250Asensitivity%2520of%2520LLMs%2520to%2520toxic%2520speech%2520results%2520in%2520false%2520positives%252C%2520limiting%250Afreedom%2520of%2520speech.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520method%2520called%250AMetaTox%252C%2520leveraging%2520graph%2520search%2520on%2520a%2520meta-toxic%2520knowledge%2520graph%2520to%2520enhance%250Ahatred%2520and%2520toxicity%2520detection.%2520First%252C%2520we%2520construct%2520a%2520comprehensive%2520meta-toxic%250Aknowledge%2520graph%2520by%2520utilizing%2520LLMs%2520to%2520extract%2520toxic%2520information%2520through%2520a%250Athree-step%2520pipeline%252C%2520with%2520toxic%2520benchmark%2520datasets%2520serving%2520as%2520corpora.%2520Second%252C%250Awe%2520query%2520the%2520graph%2520via%2520retrieval%2520and%2520ranking%2520processes%2520to%2520supplement%2520accurate%252C%250Arelevant%2520toxic%2520knowledge.%2520Extensive%2520experiments%2520and%2520in-depth%2520case%2520studies%250Aacross%2520multiple%2520datasets%2520demonstrate%2520that%2520our%2520MetaTox%2520significantly%2520decreases%250Athe%2520false%2520positive%2520rate%2520while%2520boosting%2520overall%2520toxicity%2520detection%2520performance.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/YiboZhao624/MetaTox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15268v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LLM-based%20Hatred%20and%20Toxicity%20Detection%20with%20Meta-Toxic%0A%20%20Knowledge%20Graph&entry.906535625=Yibo%20Zhao%20and%20Jiapeng%20Zhu%20and%20Can%20Xu%20and%20Yao%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20The%20rapid%20growth%20of%20social%20media%20platforms%20has%20raised%20significant%20concerns%0Aregarding%20online%20content%20toxicity.%20When%20Large%20Language%20Models%20%28LLMs%29%20are%20used%0Afor%20toxicity%20detection%2C%20two%20key%20challenges%20emerge%3A%201%29%20the%20absence%20of%0Adomain-specific%20toxic%20knowledge%20leads%20to%20false%20negatives%3B%202%29%20the%20excessive%0Asensitivity%20of%20LLMs%20to%20toxic%20speech%20results%20in%20false%20positives%2C%20limiting%0Afreedom%20of%20speech.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20called%0AMetaTox%2C%20leveraging%20graph%20search%20on%20a%20meta-toxic%20knowledge%20graph%20to%20enhance%0Ahatred%20and%20toxicity%20detection.%20First%2C%20we%20construct%20a%20comprehensive%20meta-toxic%0Aknowledge%20graph%20by%20utilizing%20LLMs%20to%20extract%20toxic%20information%20through%20a%0Athree-step%20pipeline%2C%20with%20toxic%20benchmark%20datasets%20serving%20as%20corpora.%20Second%2C%0Awe%20query%20the%20graph%20via%20retrieval%20and%20ranking%20processes%20to%20supplement%20accurate%2C%0Arelevant%20toxic%20knowledge.%20Extensive%20experiments%20and%20in-depth%20case%20studies%0Aacross%20multiple%20datasets%20demonstrate%20that%20our%20MetaTox%20significantly%20decreases%0Athe%20false%20positive%20rate%20while%20boosting%20overall%20toxicity%20detection%20performance.%0AOur%20code%20is%20available%20at%20https%3A//github.com/YiboZhao624/MetaTox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15268v4&entry.124074799=Read"},
{"title": "Disentangling Total-Variance and Signal-to-Noise-Ratio Improves\n  Diffusion Models", "author": "Khaled Kahouli and Winfried Ripken and Stefan Gugler and Oliver T. Unke and Klaus-Robert M\u00fcller and Shinichi Nakajima", "abstract": "  The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance/signal-to-noise-ratio\ndisentangled (TV/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that schedules where the TV explodes\nexponentially can often be improved by adopting a constant TV schedule while\npreserving the same SNR schedule. Furthermore, generalizing the SNR schedule of\nthe optimal transport flow matching significantly improves the generation\nperformance. Our findings hold across various reverse diffusion solvers and\ndiverse applications, including molecular structure and image generation.\n", "link": "http://arxiv.org/abs/2502.08598v2", "date": "2025-06-02", "relevancy": 1.7476, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5771}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Total-Variance%20and%20Signal-to-Noise-Ratio%20Improves%0A%20%20Diffusion%20Models&body=Title%3A%20Disentangling%20Total-Variance%20and%20Signal-to-Noise-Ratio%20Improves%0A%20%20Diffusion%20Models%0AAuthor%3A%20Khaled%20Kahouli%20and%20Winfried%20Ripken%20and%20Stefan%20Gugler%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller%20and%20Shinichi%20Nakajima%0AAbstract%3A%20%20%20The%20long%20sampling%20time%20of%20diffusion%20models%20remains%20a%20significant%20bottleneck%2C%0Awhich%20can%20be%20mitigated%20by%20reducing%20the%20number%20of%20diffusion%20time%20steps.%20However%2C%0Athe%20quality%20of%20samples%20with%20fewer%20steps%20is%20highly%20dependent%20on%20the%20noise%0Aschedule%2C%20i.e.%2C%20the%20specific%20manner%20in%20which%20noise%20is%20introduced%20and%20the%20signal%0Ais%20reduced%20at%20each%20step.%20Although%20prior%20work%20has%20improved%20upon%20the%20original%0Avariance-preserving%20and%20variance-exploding%20schedules%2C%20these%20approaches%0A%24%5Ctextit%7Bpassively%7D%24%20adjust%20the%20total%20variance%2C%20without%20direct%20control%20over%20it.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20total-variance/signal-to-noise-ratio%0Adisentangled%20%28TV/SNR%29%20framework%2C%20where%20TV%20and%20SNR%20can%20be%20controlled%0Aindependently.%20Our%20approach%20reveals%20that%20schedules%20where%20the%20TV%20explodes%0Aexponentially%20can%20often%20be%20improved%20by%20adopting%20a%20constant%20TV%20schedule%20while%0Apreserving%20the%20same%20SNR%20schedule.%20Furthermore%2C%20generalizing%20the%20SNR%20schedule%20of%0Athe%20optimal%20transport%20flow%20matching%20significantly%20improves%20the%20generation%0Aperformance.%20Our%20findings%20hold%20across%20various%20reverse%20diffusion%20solvers%20and%0Adiverse%20applications%2C%20including%20molecular%20structure%20and%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Total-Variance%2520and%2520Signal-to-Noise-Ratio%2520Improves%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DKhaled%2520Kahouli%2520and%2520Winfried%2520Ripken%2520and%2520Stefan%2520Gugler%2520and%2520Oliver%2520T.%2520Unke%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Shinichi%2520Nakajima%26entry.1292438233%3D%2520%2520The%2520long%2520sampling%2520time%2520of%2520diffusion%2520models%2520remains%2520a%2520significant%2520bottleneck%252C%250Awhich%2520can%2520be%2520mitigated%2520by%2520reducing%2520the%2520number%2520of%2520diffusion%2520time%2520steps.%2520However%252C%250Athe%2520quality%2520of%2520samples%2520with%2520fewer%2520steps%2520is%2520highly%2520dependent%2520on%2520the%2520noise%250Aschedule%252C%2520i.e.%252C%2520the%2520specific%2520manner%2520in%2520which%2520noise%2520is%2520introduced%2520and%2520the%2520signal%250Ais%2520reduced%2520at%2520each%2520step.%2520Although%2520prior%2520work%2520has%2520improved%2520upon%2520the%2520original%250Avariance-preserving%2520and%2520variance-exploding%2520schedules%252C%2520these%2520approaches%250A%2524%255Ctextit%257Bpassively%257D%2524%2520adjust%2520the%2520total%2520variance%252C%2520without%2520direct%2520control%2520over%2520it.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520total-variance/signal-to-noise-ratio%250Adisentangled%2520%2528TV/SNR%2529%2520framework%252C%2520where%2520TV%2520and%2520SNR%2520can%2520be%2520controlled%250Aindependently.%2520Our%2520approach%2520reveals%2520that%2520schedules%2520where%2520the%2520TV%2520explodes%250Aexponentially%2520can%2520often%2520be%2520improved%2520by%2520adopting%2520a%2520constant%2520TV%2520schedule%2520while%250Apreserving%2520the%2520same%2520SNR%2520schedule.%2520Furthermore%252C%2520generalizing%2520the%2520SNR%2520schedule%2520of%250Athe%2520optimal%2520transport%2520flow%2520matching%2520significantly%2520improves%2520the%2520generation%250Aperformance.%2520Our%2520findings%2520hold%2520across%2520various%2520reverse%2520diffusion%2520solvers%2520and%250Adiverse%2520applications%252C%2520including%2520molecular%2520structure%2520and%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Total-Variance%20and%20Signal-to-Noise-Ratio%20Improves%0A%20%20Diffusion%20Models&entry.906535625=Khaled%20Kahouli%20and%20Winfried%20Ripken%20and%20Stefan%20Gugler%20and%20Oliver%20T.%20Unke%20and%20Klaus-Robert%20M%C3%BCller%20and%20Shinichi%20Nakajima&entry.1292438233=%20%20The%20long%20sampling%20time%20of%20diffusion%20models%20remains%20a%20significant%20bottleneck%2C%0Awhich%20can%20be%20mitigated%20by%20reducing%20the%20number%20of%20diffusion%20time%20steps.%20However%2C%0Athe%20quality%20of%20samples%20with%20fewer%20steps%20is%20highly%20dependent%20on%20the%20noise%0Aschedule%2C%20i.e.%2C%20the%20specific%20manner%20in%20which%20noise%20is%20introduced%20and%20the%20signal%0Ais%20reduced%20at%20each%20step.%20Although%20prior%20work%20has%20improved%20upon%20the%20original%0Avariance-preserving%20and%20variance-exploding%20schedules%2C%20these%20approaches%0A%24%5Ctextit%7Bpassively%7D%24%20adjust%20the%20total%20variance%2C%20without%20direct%20control%20over%20it.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20total-variance/signal-to-noise-ratio%0Adisentangled%20%28TV/SNR%29%20framework%2C%20where%20TV%20and%20SNR%20can%20be%20controlled%0Aindependently.%20Our%20approach%20reveals%20that%20schedules%20where%20the%20TV%20explodes%0Aexponentially%20can%20often%20be%20improved%20by%20adopting%20a%20constant%20TV%20schedule%20while%0Apreserving%20the%20same%20SNR%20schedule.%20Furthermore%2C%20generalizing%20the%20SNR%20schedule%20of%0Athe%20optimal%20transport%20flow%20matching%20significantly%20improves%20the%20generation%0Aperformance.%20Our%20findings%20hold%20across%20various%20reverse%20diffusion%20solvers%20and%0Adiverse%20applications%2C%20including%20molecular%20structure%20and%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08598v2&entry.124074799=Read"},
{"title": "AdaWorld: Learning Adaptable World Models with Latent Actions", "author": "Shenyuan Gao and Siyuan Zhou and Yilun Du and Jun Zhang and Chuang Gan", "abstract": "  World models aim to learn action-controlled future prediction and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this limitation, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.\n", "link": "http://arxiv.org/abs/2503.18938v4", "date": "2025-06-02", "relevancy": 1.7425, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6101}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaWorld%3A%20Learning%20Adaptable%20World%20Models%20with%20Latent%20Actions&body=Title%3A%20AdaWorld%3A%20Learning%20Adaptable%20World%20Models%20with%20Latent%20Actions%0AAuthor%3A%20Shenyuan%20Gao%20and%20Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Jun%20Zhang%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20World%20models%20aim%20to%20learn%20action-controlled%20future%20prediction%20and%20have%20proven%0Aessential%20for%20the%20development%20of%20intelligent%20agents.%20However%2C%20most%20existing%0Aworld%20models%20rely%20heavily%20on%20substantial%20action-labeled%20data%20and%20costly%0Atraining%2C%20making%20it%20challenging%20to%20adapt%20to%20novel%20environments%20with%0Aheterogeneous%20actions%20through%20limited%20interactions.%20This%20limitation%20can%20hinder%0Atheir%20applicability%20across%20broader%20domains.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20AdaWorld%2C%20an%20innovative%20world%20model%20learning%20approach%20that%20enables%0Aefficient%20adaptation.%20The%20key%20idea%20is%20to%20incorporate%20action%20information%20during%0Athe%20pretraining%20of%20world%20models.%20This%20is%20achieved%20by%20extracting%20latent%20actions%0Afrom%20videos%20in%20a%20self-supervised%20manner%2C%20capturing%20the%20most%20critical%0Atransitions%20between%20frames.%20We%20then%20develop%20an%20autoregressive%20world%20model%20that%0Aconditions%20on%20these%20latent%20actions.%20This%20learning%20paradigm%20enables%20highly%0Aadaptable%20world%20models%2C%20facilitating%20efficient%20transfer%20and%20learning%20of%20new%0Aactions%20even%20with%20limited%20interactions%20and%20finetuning.%20Our%20comprehensive%0Aexperiments%20across%20multiple%20environments%20demonstrate%20that%20AdaWorld%20achieves%0Asuperior%20performance%20in%20both%20simulation%20quality%20and%20visual%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18938v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaWorld%253A%2520Learning%2520Adaptable%2520World%2520Models%2520with%2520Latent%2520Actions%26entry.906535625%3DShenyuan%2520Gao%2520and%2520Siyuan%2520Zhou%2520and%2520Yilun%2520Du%2520and%2520Jun%2520Zhang%2520and%2520Chuang%2520Gan%26entry.1292438233%3D%2520%2520World%2520models%2520aim%2520to%2520learn%2520action-controlled%2520future%2520prediction%2520and%2520have%2520proven%250Aessential%2520for%2520the%2520development%2520of%2520intelligent%2520agents.%2520However%252C%2520most%2520existing%250Aworld%2520models%2520rely%2520heavily%2520on%2520substantial%2520action-labeled%2520data%2520and%2520costly%250Atraining%252C%2520making%2520it%2520challenging%2520to%2520adapt%2520to%2520novel%2520environments%2520with%250Aheterogeneous%2520actions%2520through%2520limited%2520interactions.%2520This%2520limitation%2520can%2520hinder%250Atheir%2520applicability%2520across%2520broader%2520domains.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520AdaWorld%252C%2520an%2520innovative%2520world%2520model%2520learning%2520approach%2520that%2520enables%250Aefficient%2520adaptation.%2520The%2520key%2520idea%2520is%2520to%2520incorporate%2520action%2520information%2520during%250Athe%2520pretraining%2520of%2520world%2520models.%2520This%2520is%2520achieved%2520by%2520extracting%2520latent%2520actions%250Afrom%2520videos%2520in%2520a%2520self-supervised%2520manner%252C%2520capturing%2520the%2520most%2520critical%250Atransitions%2520between%2520frames.%2520We%2520then%2520develop%2520an%2520autoregressive%2520world%2520model%2520that%250Aconditions%2520on%2520these%2520latent%2520actions.%2520This%2520learning%2520paradigm%2520enables%2520highly%250Aadaptable%2520world%2520models%252C%2520facilitating%2520efficient%2520transfer%2520and%2520learning%2520of%2520new%250Aactions%2520even%2520with%2520limited%2520interactions%2520and%2520finetuning.%2520Our%2520comprehensive%250Aexperiments%2520across%2520multiple%2520environments%2520demonstrate%2520that%2520AdaWorld%2520achieves%250Asuperior%2520performance%2520in%2520both%2520simulation%2520quality%2520and%2520visual%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18938v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaWorld%3A%20Learning%20Adaptable%20World%20Models%20with%20Latent%20Actions&entry.906535625=Shenyuan%20Gao%20and%20Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Jun%20Zhang%20and%20Chuang%20Gan&entry.1292438233=%20%20World%20models%20aim%20to%20learn%20action-controlled%20future%20prediction%20and%20have%20proven%0Aessential%20for%20the%20development%20of%20intelligent%20agents.%20However%2C%20most%20existing%0Aworld%20models%20rely%20heavily%20on%20substantial%20action-labeled%20data%20and%20costly%0Atraining%2C%20making%20it%20challenging%20to%20adapt%20to%20novel%20environments%20with%0Aheterogeneous%20actions%20through%20limited%20interactions.%20This%20limitation%20can%20hinder%0Atheir%20applicability%20across%20broader%20domains.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20AdaWorld%2C%20an%20innovative%20world%20model%20learning%20approach%20that%20enables%0Aefficient%20adaptation.%20The%20key%20idea%20is%20to%20incorporate%20action%20information%20during%0Athe%20pretraining%20of%20world%20models.%20This%20is%20achieved%20by%20extracting%20latent%20actions%0Afrom%20videos%20in%20a%20self-supervised%20manner%2C%20capturing%20the%20most%20critical%0Atransitions%20between%20frames.%20We%20then%20develop%20an%20autoregressive%20world%20model%20that%0Aconditions%20on%20these%20latent%20actions.%20This%20learning%20paradigm%20enables%20highly%0Aadaptable%20world%20models%2C%20facilitating%20efficient%20transfer%20and%20learning%20of%20new%0Aactions%20even%20with%20limited%20interactions%20and%20finetuning.%20Our%20comprehensive%0Aexperiments%20across%20multiple%20environments%20demonstrate%20that%20AdaWorld%20achieves%0Asuperior%20performance%20in%20both%20simulation%20quality%20and%20visual%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18938v4&entry.124074799=Read"},
{"title": "TextDestroyer: A Training- and Annotation-Free Diffusion Method for\n  Destroying Anomal Text from Images", "author": "Mengcheng Li and Fei Chao and Chia-Wen Lin and Rongrong Ji", "abstract": "  In this paper, we propose TextDestroyer, the first training- and\nannotation-free method for scene text destruction using a pre-trained diffusion\nmodel. Existing scene text removal models require complex annotation and\nretraining, and may leave faint yet recognizable text information, compromising\nprivacy protection and content concealment. TextDestroyer addresses these\nissues by employing a three-stage hierarchical process to obtain accurate text\nmasks. Our method scrambles text areas in the latent start code using a\nGaussian distribution before reconstruction. During the diffusion denoising\nprocess, self-attention key and value are referenced from the original latent\nto restore the compromised background. Latent codes saved at each inversion\nstep are used for replacement during reconstruction, ensuring perfect\nbackground restoration. The advantages of TextDestroyer include: (1) it\neliminates labor-intensive data annotation and resource-intensive training; (2)\nit achieves more thorough text destruction, preventing recognizable traces; and\n(3) it demonstrates better generalization capabilities, performing well on both\nreal-world scenes and generated images.\n", "link": "http://arxiv.org/abs/2411.00355v2", "date": "2025-06-02", "relevancy": 1.7228, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5861}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5743}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextDestroyer%3A%20A%20Training-%20and%20Annotation-Free%20Diffusion%20Method%20for%0A%20%20Destroying%20Anomal%20Text%20from%20Images&body=Title%3A%20TextDestroyer%3A%20A%20Training-%20and%20Annotation-Free%20Diffusion%20Method%20for%0A%20%20Destroying%20Anomal%20Text%20from%20Images%0AAuthor%3A%20Mengcheng%20Li%20and%20Fei%20Chao%20and%20Chia-Wen%20Lin%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20TextDestroyer%2C%20the%20first%20training-%20and%0Aannotation-free%20method%20for%20scene%20text%20destruction%20using%20a%20pre-trained%20diffusion%0Amodel.%20Existing%20scene%20text%20removal%20models%20require%20complex%20annotation%20and%0Aretraining%2C%20and%20may%20leave%20faint%20yet%20recognizable%20text%20information%2C%20compromising%0Aprivacy%20protection%20and%20content%20concealment.%20TextDestroyer%20addresses%20these%0Aissues%20by%20employing%20a%20three-stage%20hierarchical%20process%20to%20obtain%20accurate%20text%0Amasks.%20Our%20method%20scrambles%20text%20areas%20in%20the%20latent%20start%20code%20using%20a%0AGaussian%20distribution%20before%20reconstruction.%20During%20the%20diffusion%20denoising%0Aprocess%2C%20self-attention%20key%20and%20value%20are%20referenced%20from%20the%20original%20latent%0Ato%20restore%20the%20compromised%20background.%20Latent%20codes%20saved%20at%20each%20inversion%0Astep%20are%20used%20for%20replacement%20during%20reconstruction%2C%20ensuring%20perfect%0Abackground%20restoration.%20The%20advantages%20of%20TextDestroyer%20include%3A%20%281%29%20it%0Aeliminates%20labor-intensive%20data%20annotation%20and%20resource-intensive%20training%3B%20%282%29%0Ait%20achieves%20more%20thorough%20text%20destruction%2C%20preventing%20recognizable%20traces%3B%20and%0A%283%29%20it%20demonstrates%20better%20generalization%20capabilities%2C%20performing%20well%20on%20both%0Areal-world%20scenes%20and%20generated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextDestroyer%253A%2520A%2520Training-%2520and%2520Annotation-Free%2520Diffusion%2520Method%2520for%250A%2520%2520Destroying%2520Anomal%2520Text%2520from%2520Images%26entry.906535625%3DMengcheng%2520Li%2520and%2520Fei%2520Chao%2520and%2520Chia-Wen%2520Lin%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520TextDestroyer%252C%2520the%2520first%2520training-%2520and%250Aannotation-free%2520method%2520for%2520scene%2520text%2520destruction%2520using%2520a%2520pre-trained%2520diffusion%250Amodel.%2520Existing%2520scene%2520text%2520removal%2520models%2520require%2520complex%2520annotation%2520and%250Aretraining%252C%2520and%2520may%2520leave%2520faint%2520yet%2520recognizable%2520text%2520information%252C%2520compromising%250Aprivacy%2520protection%2520and%2520content%2520concealment.%2520TextDestroyer%2520addresses%2520these%250Aissues%2520by%2520employing%2520a%2520three-stage%2520hierarchical%2520process%2520to%2520obtain%2520accurate%2520text%250Amasks.%2520Our%2520method%2520scrambles%2520text%2520areas%2520in%2520the%2520latent%2520start%2520code%2520using%2520a%250AGaussian%2520distribution%2520before%2520reconstruction.%2520During%2520the%2520diffusion%2520denoising%250Aprocess%252C%2520self-attention%2520key%2520and%2520value%2520are%2520referenced%2520from%2520the%2520original%2520latent%250Ato%2520restore%2520the%2520compromised%2520background.%2520Latent%2520codes%2520saved%2520at%2520each%2520inversion%250Astep%2520are%2520used%2520for%2520replacement%2520during%2520reconstruction%252C%2520ensuring%2520perfect%250Abackground%2520restoration.%2520The%2520advantages%2520of%2520TextDestroyer%2520include%253A%2520%25281%2529%2520it%250Aeliminates%2520labor-intensive%2520data%2520annotation%2520and%2520resource-intensive%2520training%253B%2520%25282%2529%250Ait%2520achieves%2520more%2520thorough%2520text%2520destruction%252C%2520preventing%2520recognizable%2520traces%253B%2520and%250A%25283%2529%2520it%2520demonstrates%2520better%2520generalization%2520capabilities%252C%2520performing%2520well%2520on%2520both%250Areal-world%2520scenes%2520and%2520generated%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextDestroyer%3A%20A%20Training-%20and%20Annotation-Free%20Diffusion%20Method%20for%0A%20%20Destroying%20Anomal%20Text%20from%20Images&entry.906535625=Mengcheng%20Li%20and%20Fei%20Chao%20and%20Chia-Wen%20Lin%20and%20Rongrong%20Ji&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20TextDestroyer%2C%20the%20first%20training-%20and%0Aannotation-free%20method%20for%20scene%20text%20destruction%20using%20a%20pre-trained%20diffusion%0Amodel.%20Existing%20scene%20text%20removal%20models%20require%20complex%20annotation%20and%0Aretraining%2C%20and%20may%20leave%20faint%20yet%20recognizable%20text%20information%2C%20compromising%0Aprivacy%20protection%20and%20content%20concealment.%20TextDestroyer%20addresses%20these%0Aissues%20by%20employing%20a%20three-stage%20hierarchical%20process%20to%20obtain%20accurate%20text%0Amasks.%20Our%20method%20scrambles%20text%20areas%20in%20the%20latent%20start%20code%20using%20a%0AGaussian%20distribution%20before%20reconstruction.%20During%20the%20diffusion%20denoising%0Aprocess%2C%20self-attention%20key%20and%20value%20are%20referenced%20from%20the%20original%20latent%0Ato%20restore%20the%20compromised%20background.%20Latent%20codes%20saved%20at%20each%20inversion%0Astep%20are%20used%20for%20replacement%20during%20reconstruction%2C%20ensuring%20perfect%0Abackground%20restoration.%20The%20advantages%20of%20TextDestroyer%20include%3A%20%281%29%20it%0Aeliminates%20labor-intensive%20data%20annotation%20and%20resource-intensive%20training%3B%20%282%29%0Ait%20achieves%20more%20thorough%20text%20destruction%2C%20preventing%20recognizable%20traces%3B%20and%0A%283%29%20it%20demonstrates%20better%20generalization%20capabilities%2C%20performing%20well%20on%20both%0Areal-world%20scenes%20and%20generated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00355v2&entry.124074799=Read"},
{"title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous\n  Driving", "author": "Anqing Jiang and Yu Gao and Zhigang Sun and Yiru Wang and Jijun Wang and Jinghao Chai and Qian Cao and Yuweng Heng and Hao Jiang and Zongzheng Zhang and Xianda Guo and Hao Sun and Hao Zhao", "abstract": "  Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS.\n", "link": "http://arxiv.org/abs/2505.19381v3", "date": "2025-06-02", "relevancy": 1.6826, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5809}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffVLA%3A%20Vision-Language%20Guided%20Diffusion%20Planning%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20DiffVLA%3A%20Vision-Language%20Guided%20Diffusion%20Planning%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Anqing%20Jiang%20and%20Yu%20Gao%20and%20Zhigang%20Sun%20and%20Yiru%20Wang%20and%20Jijun%20Wang%20and%20Jinghao%20Chai%20and%20Qian%20Cao%20and%20Yuweng%20Heng%20and%20Hao%20Jiang%20and%20Zongzheng%20Zhang%20and%20Xianda%20Guo%20and%20Hao%20Sun%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Research%20interest%20in%20end-to-end%20autonomous%20driving%20has%20surged%20owing%20to%20its%0Afully%20differentiable%20design%20integrating%20modular%20tasks%2C%20i.e.%20perception%2C%0Aprediction%20and%20planing%2C%20which%20enables%20optimization%20in%20pursuit%20of%20the%20ultimate%0Agoal.%20Despite%20the%20great%20potential%20of%20the%20end-to-end%20paradigm%2C%20existing%20methods%0Asuffer%20from%20several%20aspects%20including%20expensive%20BEV%20%28bird%27s%20eye%20view%29%0Acomputation%2C%20action%20diversity%2C%20and%20sub-optimal%20decision%20in%20complex%20real-world%0Ascenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20hybrid%20sparse-dense%0Adiffusion%20policy%2C%20empowered%20by%20a%20Vision-Language%20Model%20%28VLM%29%2C%20called%20Diff-VLA.%0AWe%20explore%20the%20sparse%20diffusion%20representation%20for%20efficient%20multi-modal%0Adriving%20behavior.%20Moreover%2C%20we%20rethink%20the%20effectiveness%20of%20VLM%20driving%0Adecision%20and%20improve%20the%20trajectory%20generation%20guidance%20through%20deep%0Ainteraction%20across%20agent%2C%20map%20instances%20and%20VLM%20output.%20Our%20method%20shows%0Asuperior%20performance%20in%20Autonomous%20Grand%20Challenge%202025%20which%20contains%0Achallenging%20real%20and%20reactive%20synthetic%20scenarios.%20Our%20methods%20achieves%2045.0%0APDMS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19381v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffVLA%253A%2520Vision-Language%2520Guided%2520Diffusion%2520Planning%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DAnqing%2520Jiang%2520and%2520Yu%2520Gao%2520and%2520Zhigang%2520Sun%2520and%2520Yiru%2520Wang%2520and%2520Jijun%2520Wang%2520and%2520Jinghao%2520Chai%2520and%2520Qian%2520Cao%2520and%2520Yuweng%2520Heng%2520and%2520Hao%2520Jiang%2520and%2520Zongzheng%2520Zhang%2520and%2520Xianda%2520Guo%2520and%2520Hao%2520Sun%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Research%2520interest%2520in%2520end-to-end%2520autonomous%2520driving%2520has%2520surged%2520owing%2520to%2520its%250Afully%2520differentiable%2520design%2520integrating%2520modular%2520tasks%252C%2520i.e.%2520perception%252C%250Aprediction%2520and%2520planing%252C%2520which%2520enables%2520optimization%2520in%2520pursuit%2520of%2520the%2520ultimate%250Agoal.%2520Despite%2520the%2520great%2520potential%2520of%2520the%2520end-to-end%2520paradigm%252C%2520existing%2520methods%250Asuffer%2520from%2520several%2520aspects%2520including%2520expensive%2520BEV%2520%2528bird%2527s%2520eye%2520view%2529%250Acomputation%252C%2520action%2520diversity%252C%2520and%2520sub-optimal%2520decision%2520in%2520complex%2520real-world%250Ascenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520sparse-dense%250Adiffusion%2520policy%252C%2520empowered%2520by%2520a%2520Vision-Language%2520Model%2520%2528VLM%2529%252C%2520called%2520Diff-VLA.%250AWe%2520explore%2520the%2520sparse%2520diffusion%2520representation%2520for%2520efficient%2520multi-modal%250Adriving%2520behavior.%2520Moreover%252C%2520we%2520rethink%2520the%2520effectiveness%2520of%2520VLM%2520driving%250Adecision%2520and%2520improve%2520the%2520trajectory%2520generation%2520guidance%2520through%2520deep%250Ainteraction%2520across%2520agent%252C%2520map%2520instances%2520and%2520VLM%2520output.%2520Our%2520method%2520shows%250Asuperior%2520performance%2520in%2520Autonomous%2520Grand%2520Challenge%25202025%2520which%2520contains%250Achallenging%2520real%2520and%2520reactive%2520synthetic%2520scenarios.%2520Our%2520methods%2520achieves%252045.0%250APDMS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19381v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffVLA%3A%20Vision-Language%20Guided%20Diffusion%20Planning%20for%20Autonomous%0A%20%20Driving&entry.906535625=Anqing%20Jiang%20and%20Yu%20Gao%20and%20Zhigang%20Sun%20and%20Yiru%20Wang%20and%20Jijun%20Wang%20and%20Jinghao%20Chai%20and%20Qian%20Cao%20and%20Yuweng%20Heng%20and%20Hao%20Jiang%20and%20Zongzheng%20Zhang%20and%20Xianda%20Guo%20and%20Hao%20Sun%20and%20Hao%20Zhao&entry.1292438233=%20%20Research%20interest%20in%20end-to-end%20autonomous%20driving%20has%20surged%20owing%20to%20its%0Afully%20differentiable%20design%20integrating%20modular%20tasks%2C%20i.e.%20perception%2C%0Aprediction%20and%20planing%2C%20which%20enables%20optimization%20in%20pursuit%20of%20the%20ultimate%0Agoal.%20Despite%20the%20great%20potential%20of%20the%20end-to-end%20paradigm%2C%20existing%20methods%0Asuffer%20from%20several%20aspects%20including%20expensive%20BEV%20%28bird%27s%20eye%20view%29%0Acomputation%2C%20action%20diversity%2C%20and%20sub-optimal%20decision%20in%20complex%20real-world%0Ascenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20hybrid%20sparse-dense%0Adiffusion%20policy%2C%20empowered%20by%20a%20Vision-Language%20Model%20%28VLM%29%2C%20called%20Diff-VLA.%0AWe%20explore%20the%20sparse%20diffusion%20representation%20for%20efficient%20multi-modal%0Adriving%20behavior.%20Moreover%2C%20we%20rethink%20the%20effectiveness%20of%20VLM%20driving%0Adecision%20and%20improve%20the%20trajectory%20generation%20guidance%20through%20deep%0Ainteraction%20across%20agent%2C%20map%20instances%20and%20VLM%20output.%20Our%20method%20shows%0Asuperior%20performance%20in%20Autonomous%20Grand%20Challenge%202025%20which%20contains%0Achallenging%20real%20and%20reactive%20synthetic%20scenarios.%20Our%20methods%20achieves%2045.0%0APDMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19381v3&entry.124074799=Read"},
{"title": "Towards Collaborative Anti-Money Laundering Among Financial Institutions", "author": "Zhihua Tian and Yuan Ding and Wenjie Qu and Xiang Yu and Enchao Gong and Jiaheng Zhang and Jian Liu and Kui Ren", "abstract": "  Money laundering is the process that intends to legalize the income derived\nfrom illicit activities, thus facilitating their entry into the monetary flow\nof the economy without jeopardizing their source. It is crucial to identify\nsuch activities accurately and reliably in order to enforce anti-money\nlaundering (AML). Despite considerable efforts to AML, a large number of such\nactivities still go undetected. Rule-based methods were first introduced and\nare still widely used in current detection systems. With the rise of machine\nlearning, graph-based learning methods have gained prominence in detecting\nillicit accounts through the analysis of money transfer graphs. Nevertheless,\nthese methods generally assume that the transaction graph is centralized,\nwhereas in practice, money laundering activities usually span multiple\nfinancial institutions. Due to regulatory, legal, commercial, and customer\nprivacy concerns, institutions tend not to share data, restricting their\nutility in practical usage. In this paper, we propose the first algorithm that\nsupports performing AML over multiple institutions while protecting the\nsecurity and privacy of local data. To evaluate, we construct Alipay-ECB, a\nreal-world dataset comprising digital transactions from Alipay, the world's\nlargest mobile payment platform, alongside transactions from E-Commerce Bank\n(ECB). The dataset includes over 200 million accounts and 300 million\ntransactions, covering both intra-institution transactions and those between\nAlipay and ECB. This makes it the largest real-world transaction graph\navailable for analysis. The experimental results demonstrate that our methods\ncan effectively identify cross-institution money laundering subgroups.\nAdditionally, experiments on synthetic datasets also demonstrate that our\nmethod is efficient, requiring only a few minutes on datasets with millions of\ntransactions.\n", "link": "http://arxiv.org/abs/2502.19952v2", "date": "2025-06-02", "relevancy": 1.6578, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4126}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Collaborative%20Anti-Money%20Laundering%20Among%20Financial%20Institutions&body=Title%3A%20Towards%20Collaborative%20Anti-Money%20Laundering%20Among%20Financial%20Institutions%0AAuthor%3A%20Zhihua%20Tian%20and%20Yuan%20Ding%20and%20Wenjie%20Qu%20and%20Xiang%20Yu%20and%20Enchao%20Gong%20and%20Jiaheng%20Zhang%20and%20Jian%20Liu%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Money%20laundering%20is%20the%20process%20that%20intends%20to%20legalize%20the%20income%20derived%0Afrom%20illicit%20activities%2C%20thus%20facilitating%20their%20entry%20into%20the%20monetary%20flow%0Aof%20the%20economy%20without%20jeopardizing%20their%20source.%20It%20is%20crucial%20to%20identify%0Asuch%20activities%20accurately%20and%20reliably%20in%20order%20to%20enforce%20anti-money%0Alaundering%20%28AML%29.%20Despite%20considerable%20efforts%20to%20AML%2C%20a%20large%20number%20of%20such%0Aactivities%20still%20go%20undetected.%20Rule-based%20methods%20were%20first%20introduced%20and%0Aare%20still%20widely%20used%20in%20current%20detection%20systems.%20With%20the%20rise%20of%20machine%0Alearning%2C%20graph-based%20learning%20methods%20have%20gained%20prominence%20in%20detecting%0Aillicit%20accounts%20through%20the%20analysis%20of%20money%20transfer%20graphs.%20Nevertheless%2C%0Athese%20methods%20generally%20assume%20that%20the%20transaction%20graph%20is%20centralized%2C%0Awhereas%20in%20practice%2C%20money%20laundering%20activities%20usually%20span%20multiple%0Afinancial%20institutions.%20Due%20to%20regulatory%2C%20legal%2C%20commercial%2C%20and%20customer%0Aprivacy%20concerns%2C%20institutions%20tend%20not%20to%20share%20data%2C%20restricting%20their%0Autility%20in%20practical%20usage.%20In%20this%20paper%2C%20we%20propose%20the%20first%20algorithm%20that%0Asupports%20performing%20AML%20over%20multiple%20institutions%20while%20protecting%20the%0Asecurity%20and%20privacy%20of%20local%20data.%20To%20evaluate%2C%20we%20construct%20Alipay-ECB%2C%20a%0Areal-world%20dataset%20comprising%20digital%20transactions%20from%20Alipay%2C%20the%20world%27s%0Alargest%20mobile%20payment%20platform%2C%20alongside%20transactions%20from%20E-Commerce%20Bank%0A%28ECB%29.%20The%20dataset%20includes%20over%20200%20million%20accounts%20and%20300%20million%0Atransactions%2C%20covering%20both%20intra-institution%20transactions%20and%20those%20between%0AAlipay%20and%20ECB.%20This%20makes%20it%20the%20largest%20real-world%20transaction%20graph%0Aavailable%20for%20analysis.%20The%20experimental%20results%20demonstrate%20that%20our%20methods%0Acan%20effectively%20identify%20cross-institution%20money%20laundering%20subgroups.%0AAdditionally%2C%20experiments%20on%20synthetic%20datasets%20also%20demonstrate%20that%20our%0Amethod%20is%20efficient%2C%20requiring%20only%20a%20few%20minutes%20on%20datasets%20with%20millions%20of%0Atransactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Collaborative%2520Anti-Money%2520Laundering%2520Among%2520Financial%2520Institutions%26entry.906535625%3DZhihua%2520Tian%2520and%2520Yuan%2520Ding%2520and%2520Wenjie%2520Qu%2520and%2520Xiang%2520Yu%2520and%2520Enchao%2520Gong%2520and%2520Jiaheng%2520Zhang%2520and%2520Jian%2520Liu%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520Money%2520laundering%2520is%2520the%2520process%2520that%2520intends%2520to%2520legalize%2520the%2520income%2520derived%250Afrom%2520illicit%2520activities%252C%2520thus%2520facilitating%2520their%2520entry%2520into%2520the%2520monetary%2520flow%250Aof%2520the%2520economy%2520without%2520jeopardizing%2520their%2520source.%2520It%2520is%2520crucial%2520to%2520identify%250Asuch%2520activities%2520accurately%2520and%2520reliably%2520in%2520order%2520to%2520enforce%2520anti-money%250Alaundering%2520%2528AML%2529.%2520Despite%2520considerable%2520efforts%2520to%2520AML%252C%2520a%2520large%2520number%2520of%2520such%250Aactivities%2520still%2520go%2520undetected.%2520Rule-based%2520methods%2520were%2520first%2520introduced%2520and%250Aare%2520still%2520widely%2520used%2520in%2520current%2520detection%2520systems.%2520With%2520the%2520rise%2520of%2520machine%250Alearning%252C%2520graph-based%2520learning%2520methods%2520have%2520gained%2520prominence%2520in%2520detecting%250Aillicit%2520accounts%2520through%2520the%2520analysis%2520of%2520money%2520transfer%2520graphs.%2520Nevertheless%252C%250Athese%2520methods%2520generally%2520assume%2520that%2520the%2520transaction%2520graph%2520is%2520centralized%252C%250Awhereas%2520in%2520practice%252C%2520money%2520laundering%2520activities%2520usually%2520span%2520multiple%250Afinancial%2520institutions.%2520Due%2520to%2520regulatory%252C%2520legal%252C%2520commercial%252C%2520and%2520customer%250Aprivacy%2520concerns%252C%2520institutions%2520tend%2520not%2520to%2520share%2520data%252C%2520restricting%2520their%250Autility%2520in%2520practical%2520usage.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520algorithm%2520that%250Asupports%2520performing%2520AML%2520over%2520multiple%2520institutions%2520while%2520protecting%2520the%250Asecurity%2520and%2520privacy%2520of%2520local%2520data.%2520To%2520evaluate%252C%2520we%2520construct%2520Alipay-ECB%252C%2520a%250Areal-world%2520dataset%2520comprising%2520digital%2520transactions%2520from%2520Alipay%252C%2520the%2520world%2527s%250Alargest%2520mobile%2520payment%2520platform%252C%2520alongside%2520transactions%2520from%2520E-Commerce%2520Bank%250A%2528ECB%2529.%2520The%2520dataset%2520includes%2520over%2520200%2520million%2520accounts%2520and%2520300%2520million%250Atransactions%252C%2520covering%2520both%2520intra-institution%2520transactions%2520and%2520those%2520between%250AAlipay%2520and%2520ECB.%2520This%2520makes%2520it%2520the%2520largest%2520real-world%2520transaction%2520graph%250Aavailable%2520for%2520analysis.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%2520methods%250Acan%2520effectively%2520identify%2520cross-institution%2520money%2520laundering%2520subgroups.%250AAdditionally%252C%2520experiments%2520on%2520synthetic%2520datasets%2520also%2520demonstrate%2520that%2520our%250Amethod%2520is%2520efficient%252C%2520requiring%2520only%2520a%2520few%2520minutes%2520on%2520datasets%2520with%2520millions%2520of%250Atransactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Collaborative%20Anti-Money%20Laundering%20Among%20Financial%20Institutions&entry.906535625=Zhihua%20Tian%20and%20Yuan%20Ding%20and%20Wenjie%20Qu%20and%20Xiang%20Yu%20and%20Enchao%20Gong%20and%20Jiaheng%20Zhang%20and%20Jian%20Liu%20and%20Kui%20Ren&entry.1292438233=%20%20Money%20laundering%20is%20the%20process%20that%20intends%20to%20legalize%20the%20income%20derived%0Afrom%20illicit%20activities%2C%20thus%20facilitating%20their%20entry%20into%20the%20monetary%20flow%0Aof%20the%20economy%20without%20jeopardizing%20their%20source.%20It%20is%20crucial%20to%20identify%0Asuch%20activities%20accurately%20and%20reliably%20in%20order%20to%20enforce%20anti-money%0Alaundering%20%28AML%29.%20Despite%20considerable%20efforts%20to%20AML%2C%20a%20large%20number%20of%20such%0Aactivities%20still%20go%20undetected.%20Rule-based%20methods%20were%20first%20introduced%20and%0Aare%20still%20widely%20used%20in%20current%20detection%20systems.%20With%20the%20rise%20of%20machine%0Alearning%2C%20graph-based%20learning%20methods%20have%20gained%20prominence%20in%20detecting%0Aillicit%20accounts%20through%20the%20analysis%20of%20money%20transfer%20graphs.%20Nevertheless%2C%0Athese%20methods%20generally%20assume%20that%20the%20transaction%20graph%20is%20centralized%2C%0Awhereas%20in%20practice%2C%20money%20laundering%20activities%20usually%20span%20multiple%0Afinancial%20institutions.%20Due%20to%20regulatory%2C%20legal%2C%20commercial%2C%20and%20customer%0Aprivacy%20concerns%2C%20institutions%20tend%20not%20to%20share%20data%2C%20restricting%20their%0Autility%20in%20practical%20usage.%20In%20this%20paper%2C%20we%20propose%20the%20first%20algorithm%20that%0Asupports%20performing%20AML%20over%20multiple%20institutions%20while%20protecting%20the%0Asecurity%20and%20privacy%20of%20local%20data.%20To%20evaluate%2C%20we%20construct%20Alipay-ECB%2C%20a%0Areal-world%20dataset%20comprising%20digital%20transactions%20from%20Alipay%2C%20the%20world%27s%0Alargest%20mobile%20payment%20platform%2C%20alongside%20transactions%20from%20E-Commerce%20Bank%0A%28ECB%29.%20The%20dataset%20includes%20over%20200%20million%20accounts%20and%20300%20million%0Atransactions%2C%20covering%20both%20intra-institution%20transactions%20and%20those%20between%0AAlipay%20and%20ECB.%20This%20makes%20it%20the%20largest%20real-world%20transaction%20graph%0Aavailable%20for%20analysis.%20The%20experimental%20results%20demonstrate%20that%20our%20methods%0Acan%20effectively%20identify%20cross-institution%20money%20laundering%20subgroups.%0AAdditionally%2C%20experiments%20on%20synthetic%20datasets%20also%20demonstrate%20that%20our%0Amethod%20is%20efficient%2C%20requiring%20only%20a%20few%20minutes%20on%20datasets%20with%20millions%20of%0Atransactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19952v2&entry.124074799=Read"},
{"title": "A Conformal Risk Control Framework for Granular Word Assessment and\n  Uncertainty Calibration of CLIPScore Quality Estimates", "author": "Gon\u00e7alo Gomes and Bruno Martins and Chrysoula Zerva", "abstract": "  This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessments for errors\nwithin captions, and the reliance on single-point quality estimates without\nconsidering uncertainty. To address the limitations, we propose a simple yet\neffective strategy for generating and calibrating distributions of CLIPScore\nvalues. Leveraging a model-agnostic conformal risk control framework, we\ncalibrate CLIPScore values for task-specific control variables, tackling the\naforementioned limitations. Experimental results demonstrate that using\nconformal risk control, over score distributions produced with simple methods\nsuch as input masking, can achieve competitive performance compared to more\ncomplex approaches. Our method effectively detects erroneous words, while\nproviding formal guarantees aligned with desired risk levels. It also improves\nthe correlation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics.\n", "link": "http://arxiv.org/abs/2504.01225v2", "date": "2025-06-02", "relevancy": 1.6214, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5608}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5201}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conformal%20Risk%20Control%20Framework%20for%20Granular%20Word%20Assessment%20and%0A%20%20Uncertainty%20Calibration%20of%20CLIPScore%20Quality%20Estimates&body=Title%3A%20A%20Conformal%20Risk%20Control%20Framework%20for%20Granular%20Word%20Assessment%20and%0A%20%20Uncertainty%20Calibration%20of%20CLIPScore%20Quality%20Estimates%0AAuthor%3A%20Gon%C3%A7alo%20Gomes%20and%20Bruno%20Martins%20and%20Chrysoula%20Zerva%0AAbstract%3A%20%20%20This%20study%20explores%20current%20limitations%20of%20learned%20image%20captioning%0Aevaluation%20metrics%2C%20specifically%20the%20lack%20of%20granular%20assessments%20for%20errors%0Awithin%20captions%2C%20and%20the%20reliance%20on%20single-point%20quality%20estimates%20without%0Aconsidering%20uncertainty.%20To%20address%20the%20limitations%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20strategy%20for%20generating%20and%20calibrating%20distributions%20of%20CLIPScore%0Avalues.%20Leveraging%20a%20model-agnostic%20conformal%20risk%20control%20framework%2C%20we%0Acalibrate%20CLIPScore%20values%20for%20task-specific%20control%20variables%2C%20tackling%20the%0Aaforementioned%20limitations.%20Experimental%20results%20demonstrate%20that%20using%0Aconformal%20risk%20control%2C%20over%20score%20distributions%20produced%20with%20simple%20methods%0Asuch%20as%20input%20masking%2C%20can%20achieve%20competitive%20performance%20compared%20to%20more%0Acomplex%20approaches.%20Our%20method%20effectively%20detects%20erroneous%20words%2C%20while%0Aproviding%20formal%20guarantees%20aligned%20with%20desired%20risk%20levels.%20It%20also%20improves%0Athe%20correlation%20between%20uncertainty%20estimations%20and%20prediction%20errors%2C%20thus%0Aenhancing%20the%20overall%20reliability%20of%20caption%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conformal%2520Risk%2520Control%2520Framework%2520for%2520Granular%2520Word%2520Assessment%2520and%250A%2520%2520Uncertainty%2520Calibration%2520of%2520CLIPScore%2520Quality%2520Estimates%26entry.906535625%3DGon%25C3%25A7alo%2520Gomes%2520and%2520Bruno%2520Martins%2520and%2520Chrysoula%2520Zerva%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520current%2520limitations%2520of%2520learned%2520image%2520captioning%250Aevaluation%2520metrics%252C%2520specifically%2520the%2520lack%2520of%2520granular%2520assessments%2520for%2520errors%250Awithin%2520captions%252C%2520and%2520the%2520reliance%2520on%2520single-point%2520quality%2520estimates%2520without%250Aconsidering%2520uncertainty.%2520To%2520address%2520the%2520limitations%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aeffective%2520strategy%2520for%2520generating%2520and%2520calibrating%2520distributions%2520of%2520CLIPScore%250Avalues.%2520Leveraging%2520a%2520model-agnostic%2520conformal%2520risk%2520control%2520framework%252C%2520we%250Acalibrate%2520CLIPScore%2520values%2520for%2520task-specific%2520control%2520variables%252C%2520tackling%2520the%250Aaforementioned%2520limitations.%2520Experimental%2520results%2520demonstrate%2520that%2520using%250Aconformal%2520risk%2520control%252C%2520over%2520score%2520distributions%2520produced%2520with%2520simple%2520methods%250Asuch%2520as%2520input%2520masking%252C%2520can%2520achieve%2520competitive%2520performance%2520compared%2520to%2520more%250Acomplex%2520approaches.%2520Our%2520method%2520effectively%2520detects%2520erroneous%2520words%252C%2520while%250Aproviding%2520formal%2520guarantees%2520aligned%2520with%2520desired%2520risk%2520levels.%2520It%2520also%2520improves%250Athe%2520correlation%2520between%2520uncertainty%2520estimations%2520and%2520prediction%2520errors%252C%2520thus%250Aenhancing%2520the%2520overall%2520reliability%2520of%2520caption%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conformal%20Risk%20Control%20Framework%20for%20Granular%20Word%20Assessment%20and%0A%20%20Uncertainty%20Calibration%20of%20CLIPScore%20Quality%20Estimates&entry.906535625=Gon%C3%A7alo%20Gomes%20and%20Bruno%20Martins%20and%20Chrysoula%20Zerva&entry.1292438233=%20%20This%20study%20explores%20current%20limitations%20of%20learned%20image%20captioning%0Aevaluation%20metrics%2C%20specifically%20the%20lack%20of%20granular%20assessments%20for%20errors%0Awithin%20captions%2C%20and%20the%20reliance%20on%20single-point%20quality%20estimates%20without%0Aconsidering%20uncertainty.%20To%20address%20the%20limitations%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20strategy%20for%20generating%20and%20calibrating%20distributions%20of%20CLIPScore%0Avalues.%20Leveraging%20a%20model-agnostic%20conformal%20risk%20control%20framework%2C%20we%0Acalibrate%20CLIPScore%20values%20for%20task-specific%20control%20variables%2C%20tackling%20the%0Aaforementioned%20limitations.%20Experimental%20results%20demonstrate%20that%20using%0Aconformal%20risk%20control%2C%20over%20score%20distributions%20produced%20with%20simple%20methods%0Asuch%20as%20input%20masking%2C%20can%20achieve%20competitive%20performance%20compared%20to%20more%0Acomplex%20approaches.%20Our%20method%20effectively%20detects%20erroneous%20words%2C%20while%0Aproviding%20formal%20guarantees%20aligned%20with%20desired%20risk%20levels.%20It%20also%20improves%0Athe%20correlation%20between%20uncertainty%20estimations%20and%20prediction%20errors%2C%20thus%0Aenhancing%20the%20overall%20reliability%20of%20caption%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01225v2&entry.124074799=Read"},
{"title": "Generalized Bayesian deep reinforcement learning", "author": "Shreya Sinha Roy and Richard G. Everitt and Christian P. Robert and Ritabrata Dutta", "abstract": "  Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. As a model-based RL method, it has two key components:\n(1) inferring the posterior distribution of the model for the data-generating\nprocess (DGP) and (2) policy learning using the learned posterior. We propose\nto model the dynamics of the unknown environment through deep generative\nmodels, assuming Markov dependence. In the absence of likelihood functions for\nthese models, we train them by learning a generalized predictive-sequential (or\nprequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC)\nsamplers to draw samples from this generalized Bayesian posterior distribution.\nIn conjunction, to achieve scalability in the high-dimensional parameter space\nof the neural networks, we use the gradient-based Markov kernels within SMC. To\njustify the use of the prequential scoring rule posterior, we prove a\nBernstein-von Mises-type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximising the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions, which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies, assuming a discrete action space.\nFinally, we successfully extended our setup for a challenging problem with a\ncontinuous action space without theoretical guarantees.\n", "link": "http://arxiv.org/abs/2412.11743v2", "date": "2025-06-02", "relevancy": 1.6117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5972}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5309}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Bayesian%20deep%20reinforcement%20learning&body=Title%3A%20Generalized%20Bayesian%20deep%20reinforcement%20learning%0AAuthor%3A%20Shreya%20Sinha%20Roy%20and%20Richard%20G.%20Everitt%20and%20Christian%20P.%20Robert%20and%20Ritabrata%20Dutta%0AAbstract%3A%20%20%20Bayesian%20reinforcement%20learning%20%28BRL%29%20is%20a%20method%20that%20merges%20principles%20from%0ABayesian%20statistics%20and%20reinforcement%20learning%20to%20make%20optimal%20decisions%20in%0Auncertain%20environments.%20As%20a%20model-based%20RL%20method%2C%20it%20has%20two%20key%20components%3A%0A%281%29%20inferring%20the%20posterior%20distribution%20of%20the%20model%20for%20the%20data-generating%0Aprocess%20%28DGP%29%20and%20%282%29%20policy%20learning%20using%20the%20learned%20posterior.%20We%20propose%0Ato%20model%20the%20dynamics%20of%20the%20unknown%20environment%20through%20deep%20generative%0Amodels%2C%20assuming%20Markov%20dependence.%20In%20the%20absence%20of%20likelihood%20functions%20for%0Athese%20models%2C%20we%20train%20them%20by%20learning%20a%20generalized%20predictive-sequential%20%28or%0Aprequential%29%20scoring%20rule%20%28SR%29%20posterior.%20We%20used%20sequential%20Monte%20Carlo%20%28SMC%29%0Asamplers%20to%20draw%20samples%20from%20this%20generalized%20Bayesian%20posterior%20distribution.%0AIn%20conjunction%2C%20to%20achieve%20scalability%20in%20the%20high-dimensional%20parameter%20space%0Aof%20the%20neural%20networks%2C%20we%20use%20the%20gradient-based%20Markov%20kernels%20within%20SMC.%20To%0Ajustify%20the%20use%20of%20the%20prequential%20scoring%20rule%20posterior%2C%20we%20prove%20a%0ABernstein-von%20Mises-type%20theorem.%20For%20policy%20learning%2C%20we%20propose%20expected%0AThompson%20sampling%20%28ETS%29%20to%20learn%20the%20optimal%20policy%20by%20maximising%20the%20expected%0Avalue%20function%20with%20respect%20to%20the%20posterior%20distribution.%20This%20improves%20upon%0Atraditional%20Thompson%20sampling%20%28TS%29%20and%20its%20extensions%2C%20which%20utilize%20only%20one%0Asample%20drawn%20from%20the%20posterior%20distribution.%20This%20improvement%20is%20studied%20both%0Atheoretically%20and%20using%20simulation%20studies%2C%20assuming%20a%20discrete%20action%20space.%0AFinally%2C%20we%20successfully%20extended%20our%20setup%20for%20a%20challenging%20problem%20with%20a%0Acontinuous%20action%20space%20without%20theoretical%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Bayesian%2520deep%2520reinforcement%2520learning%26entry.906535625%3DShreya%2520Sinha%2520Roy%2520and%2520Richard%2520G.%2520Everitt%2520and%2520Christian%2520P.%2520Robert%2520and%2520Ritabrata%2520Dutta%26entry.1292438233%3D%2520%2520Bayesian%2520reinforcement%2520learning%2520%2528BRL%2529%2520is%2520a%2520method%2520that%2520merges%2520principles%2520from%250ABayesian%2520statistics%2520and%2520reinforcement%2520learning%2520to%2520make%2520optimal%2520decisions%2520in%250Auncertain%2520environments.%2520As%2520a%2520model-based%2520RL%2520method%252C%2520it%2520has%2520two%2520key%2520components%253A%250A%25281%2529%2520inferring%2520the%2520posterior%2520distribution%2520of%2520the%2520model%2520for%2520the%2520data-generating%250Aprocess%2520%2528DGP%2529%2520and%2520%25282%2529%2520policy%2520learning%2520using%2520the%2520learned%2520posterior.%2520We%2520propose%250Ato%2520model%2520the%2520dynamics%2520of%2520the%2520unknown%2520environment%2520through%2520deep%2520generative%250Amodels%252C%2520assuming%2520Markov%2520dependence.%2520In%2520the%2520absence%2520of%2520likelihood%2520functions%2520for%250Athese%2520models%252C%2520we%2520train%2520them%2520by%2520learning%2520a%2520generalized%2520predictive-sequential%2520%2528or%250Aprequential%2529%2520scoring%2520rule%2520%2528SR%2529%2520posterior.%2520We%2520used%2520sequential%2520Monte%2520Carlo%2520%2528SMC%2529%250Asamplers%2520to%2520draw%2520samples%2520from%2520this%2520generalized%2520Bayesian%2520posterior%2520distribution.%250AIn%2520conjunction%252C%2520to%2520achieve%2520scalability%2520in%2520the%2520high-dimensional%2520parameter%2520space%250Aof%2520the%2520neural%2520networks%252C%2520we%2520use%2520the%2520gradient-based%2520Markov%2520kernels%2520within%2520SMC.%2520To%250Ajustify%2520the%2520use%2520of%2520the%2520prequential%2520scoring%2520rule%2520posterior%252C%2520we%2520prove%2520a%250ABernstein-von%2520Mises-type%2520theorem.%2520For%2520policy%2520learning%252C%2520we%2520propose%2520expected%250AThompson%2520sampling%2520%2528ETS%2529%2520to%2520learn%2520the%2520optimal%2520policy%2520by%2520maximising%2520the%2520expected%250Avalue%2520function%2520with%2520respect%2520to%2520the%2520posterior%2520distribution.%2520This%2520improves%2520upon%250Atraditional%2520Thompson%2520sampling%2520%2528TS%2529%2520and%2520its%2520extensions%252C%2520which%2520utilize%2520only%2520one%250Asample%2520drawn%2520from%2520the%2520posterior%2520distribution.%2520This%2520improvement%2520is%2520studied%2520both%250Atheoretically%2520and%2520using%2520simulation%2520studies%252C%2520assuming%2520a%2520discrete%2520action%2520space.%250AFinally%252C%2520we%2520successfully%2520extended%2520our%2520setup%2520for%2520a%2520challenging%2520problem%2520with%2520a%250Acontinuous%2520action%2520space%2520without%2520theoretical%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Bayesian%20deep%20reinforcement%20learning&entry.906535625=Shreya%20Sinha%20Roy%20and%20Richard%20G.%20Everitt%20and%20Christian%20P.%20Robert%20and%20Ritabrata%20Dutta&entry.1292438233=%20%20Bayesian%20reinforcement%20learning%20%28BRL%29%20is%20a%20method%20that%20merges%20principles%20from%0ABayesian%20statistics%20and%20reinforcement%20learning%20to%20make%20optimal%20decisions%20in%0Auncertain%20environments.%20As%20a%20model-based%20RL%20method%2C%20it%20has%20two%20key%20components%3A%0A%281%29%20inferring%20the%20posterior%20distribution%20of%20the%20model%20for%20the%20data-generating%0Aprocess%20%28DGP%29%20and%20%282%29%20policy%20learning%20using%20the%20learned%20posterior.%20We%20propose%0Ato%20model%20the%20dynamics%20of%20the%20unknown%20environment%20through%20deep%20generative%0Amodels%2C%20assuming%20Markov%20dependence.%20In%20the%20absence%20of%20likelihood%20functions%20for%0Athese%20models%2C%20we%20train%20them%20by%20learning%20a%20generalized%20predictive-sequential%20%28or%0Aprequential%29%20scoring%20rule%20%28SR%29%20posterior.%20We%20used%20sequential%20Monte%20Carlo%20%28SMC%29%0Asamplers%20to%20draw%20samples%20from%20this%20generalized%20Bayesian%20posterior%20distribution.%0AIn%20conjunction%2C%20to%20achieve%20scalability%20in%20the%20high-dimensional%20parameter%20space%0Aof%20the%20neural%20networks%2C%20we%20use%20the%20gradient-based%20Markov%20kernels%20within%20SMC.%20To%0Ajustify%20the%20use%20of%20the%20prequential%20scoring%20rule%20posterior%2C%20we%20prove%20a%0ABernstein-von%20Mises-type%20theorem.%20For%20policy%20learning%2C%20we%20propose%20expected%0AThompson%20sampling%20%28ETS%29%20to%20learn%20the%20optimal%20policy%20by%20maximising%20the%20expected%0Avalue%20function%20with%20respect%20to%20the%20posterior%20distribution.%20This%20improves%20upon%0Atraditional%20Thompson%20sampling%20%28TS%29%20and%20its%20extensions%2C%20which%20utilize%20only%20one%0Asample%20drawn%20from%20the%20posterior%20distribution.%20This%20improvement%20is%20studied%20both%0Atheoretically%20and%20using%20simulation%20studies%2C%20assuming%20a%20discrete%20action%20space.%0AFinally%2C%20we%20successfully%20extended%20our%20setup%20for%20a%20challenging%20problem%20with%20a%0Acontinuous%20action%20space%20without%20theoretical%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11743v2&entry.124074799=Read"},
{"title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical\n  Systems", "author": "Maksim Zhdanov and Max Welling and Jan-Willem van de Meent", "abstract": "  Large-scale physical systems defined on irregular grids pose significant\nscalability challenges for deep learning methods, especially in the presence of\nlong-range interactions and multi-scale coupling. Traditional approaches that\ncompute all pairwise interactions, such as attention, become computationally\nprohibitive as they scale quadratically with the number of nodes. We present\nErwin, a hierarchical transformer inspired by methods from computational\nmany-body physics, which combines the efficiency of tree-based algorithms with\nthe expressivity of attention mechanisms. Erwin employs ball tree partitioning\nto organize computation, which enables linear-time attention by processing\nnodes in parallel within local neighborhoods of fixed size. Through progressive\ncoarsening and refinement of the ball tree structure, complemented by a novel\ncross-ball interaction mechanism, it captures both fine-grained local details\nand global features. We demonstrate Erwin's effectiveness across multiple\ndomains, including cosmology, molecular dynamics, PDE solving, and particle\nfluid dynamics, where it consistently outperforms baseline methods both in\naccuracy and computational efficiency.\n", "link": "http://arxiv.org/abs/2502.17019v2", "date": "2025-06-02", "relevancy": 1.571, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5664}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems&body=Title%3A%20Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems%0AAuthor%3A%20Maksim%20Zhdanov%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent%0AAbstract%3A%20%20%20Large-scale%20physical%20systems%20defined%20on%20irregular%20grids%20pose%20significant%0Ascalability%20challenges%20for%20deep%20learning%20methods%2C%20especially%20in%20the%20presence%20of%0Along-range%20interactions%20and%20multi-scale%20coupling.%20Traditional%20approaches%20that%0Acompute%20all%20pairwise%20interactions%2C%20such%20as%20attention%2C%20become%20computationally%0Aprohibitive%20as%20they%20scale%20quadratically%20with%20the%20number%20of%20nodes.%20We%20present%0AErwin%2C%20a%20hierarchical%20transformer%20inspired%20by%20methods%20from%20computational%0Amany-body%20physics%2C%20which%20combines%20the%20efficiency%20of%20tree-based%20algorithms%20with%0Athe%20expressivity%20of%20attention%20mechanisms.%20Erwin%20employs%20ball%20tree%20partitioning%0Ato%20organize%20computation%2C%20which%20enables%20linear-time%20attention%20by%20processing%0Anodes%20in%20parallel%20within%20local%20neighborhoods%20of%20fixed%20size.%20Through%20progressive%0Acoarsening%20and%20refinement%20of%20the%20ball%20tree%20structure%2C%20complemented%20by%20a%20novel%0Across-ball%20interaction%20mechanism%2C%20it%20captures%20both%20fine-grained%20local%20details%0Aand%20global%20features.%20We%20demonstrate%20Erwin%27s%20effectiveness%20across%20multiple%0Adomains%2C%20including%20cosmology%2C%20molecular%20dynamics%2C%20PDE%20solving%2C%20and%20particle%0Afluid%20dynamics%2C%20where%20it%20consistently%20outperforms%20baseline%20methods%20both%20in%0Aaccuracy%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErwin%253A%2520A%2520Tree-based%2520Hierarchical%2520Transformer%2520for%2520Large-scale%2520Physical%250A%2520%2520Systems%26entry.906535625%3DMaksim%2520Zhdanov%2520and%2520Max%2520Welling%2520and%2520Jan-Willem%2520van%2520de%2520Meent%26entry.1292438233%3D%2520%2520Large-scale%2520physical%2520systems%2520defined%2520on%2520irregular%2520grids%2520pose%2520significant%250Ascalability%2520challenges%2520for%2520deep%2520learning%2520methods%252C%2520especially%2520in%2520the%2520presence%2520of%250Along-range%2520interactions%2520and%2520multi-scale%2520coupling.%2520Traditional%2520approaches%2520that%250Acompute%2520all%2520pairwise%2520interactions%252C%2520such%2520as%2520attention%252C%2520become%2520computationally%250Aprohibitive%2520as%2520they%2520scale%2520quadratically%2520with%2520the%2520number%2520of%2520nodes.%2520We%2520present%250AErwin%252C%2520a%2520hierarchical%2520transformer%2520inspired%2520by%2520methods%2520from%2520computational%250Amany-body%2520physics%252C%2520which%2520combines%2520the%2520efficiency%2520of%2520tree-based%2520algorithms%2520with%250Athe%2520expressivity%2520of%2520attention%2520mechanisms.%2520Erwin%2520employs%2520ball%2520tree%2520partitioning%250Ato%2520organize%2520computation%252C%2520which%2520enables%2520linear-time%2520attention%2520by%2520processing%250Anodes%2520in%2520parallel%2520within%2520local%2520neighborhoods%2520of%2520fixed%2520size.%2520Through%2520progressive%250Acoarsening%2520and%2520refinement%2520of%2520the%2520ball%2520tree%2520structure%252C%2520complemented%2520by%2520a%2520novel%250Across-ball%2520interaction%2520mechanism%252C%2520it%2520captures%2520both%2520fine-grained%2520local%2520details%250Aand%2520global%2520features.%2520We%2520demonstrate%2520Erwin%2527s%2520effectiveness%2520across%2520multiple%250Adomains%252C%2520including%2520cosmology%252C%2520molecular%2520dynamics%252C%2520PDE%2520solving%252C%2520and%2520particle%250Afluid%2520dynamics%252C%2520where%2520it%2520consistently%2520outperforms%2520baseline%2520methods%2520both%2520in%250Aaccuracy%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems&entry.906535625=Maksim%20Zhdanov%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent&entry.1292438233=%20%20Large-scale%20physical%20systems%20defined%20on%20irregular%20grids%20pose%20significant%0Ascalability%20challenges%20for%20deep%20learning%20methods%2C%20especially%20in%20the%20presence%20of%0Along-range%20interactions%20and%20multi-scale%20coupling.%20Traditional%20approaches%20that%0Acompute%20all%20pairwise%20interactions%2C%20such%20as%20attention%2C%20become%20computationally%0Aprohibitive%20as%20they%20scale%20quadratically%20with%20the%20number%20of%20nodes.%20We%20present%0AErwin%2C%20a%20hierarchical%20transformer%20inspired%20by%20methods%20from%20computational%0Amany-body%20physics%2C%20which%20combines%20the%20efficiency%20of%20tree-based%20algorithms%20with%0Athe%20expressivity%20of%20attention%20mechanisms.%20Erwin%20employs%20ball%20tree%20partitioning%0Ato%20organize%20computation%2C%20which%20enables%20linear-time%20attention%20by%20processing%0Anodes%20in%20parallel%20within%20local%20neighborhoods%20of%20fixed%20size.%20Through%20progressive%0Acoarsening%20and%20refinement%20of%20the%20ball%20tree%20structure%2C%20complemented%20by%20a%20novel%0Across-ball%20interaction%20mechanism%2C%20it%20captures%20both%20fine-grained%20local%20details%0Aand%20global%20features.%20We%20demonstrate%20Erwin%27s%20effectiveness%20across%20multiple%0Adomains%2C%20including%20cosmology%2C%20molecular%20dynamics%2C%20PDE%20solving%2C%20and%20particle%0Afluid%20dynamics%2C%20where%20it%20consistently%20outperforms%20baseline%20methods%20both%20in%0Aaccuracy%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17019v2&entry.124074799=Read"},
{"title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable\n  Multi-Objective Generation", "author": "Guofu Xie and Xiao Zhang and Ting Yao and Yunsheng Shi", "abstract": "  User information needs are often highly diverse and varied. A key challenge\nin current research is how to achieve controllable multi-objective generation\nwhile enabling rapid adaptation to accommodate diverse user demands during test\ntime. Existing solutions, such as Rewarded Soup, focus on merging language\nmodels individually tuned on single objectives. While easy to implement and\nwidely used, these approaches face limitations in achieving optimal performance\ndue to their disregard for the impacts of competing objectives on model tuning.\nTo address this issue, we propose Bone Soup, a novel model merging approach\nthat first seeks a series of backbone models by considering the impacts of\nmultiple objectives and then makes the soup (i.e., merge the backbone models).\nSpecifically, Bone Soup begins by training multiple backbone models for\ndifferent objectives using multi-objective reinforcement learning. Each\nbackbone model is guided by a combination of backbone reward signals. To ensure\nthat these models are optimal for the Pareto front, the backbone rewards are\ncrafted by combining standard reward functions into basis vectors, which can\nthen be modified through a rule-based construction method. Bone Soup leverages\na symmetric circulant matrix mapping to generate the merging coefficients,\nwhich are used to merge the backbone models according to user preferences.\nExtensive experimental results demonstrate that Bone Soup exhibits strong\ncontrollability and Pareto optimality in controllable multi-objective\ngeneration, providing a more effective and efficient approach to addressing\ndiverse user needs at test time.\n", "link": "http://arxiv.org/abs/2502.10762v2", "date": "2025-06-02", "relevancy": 1.5089, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bone%20Soups%3A%20A%20Seek-and-Soup%20Model%20Merging%20Approach%20for%20Controllable%0A%20%20Multi-Objective%20Generation&body=Title%3A%20Bone%20Soups%3A%20A%20Seek-and-Soup%20Model%20Merging%20Approach%20for%20Controllable%0A%20%20Multi-Objective%20Generation%0AAuthor%3A%20Guofu%20Xie%20and%20Xiao%20Zhang%20and%20Ting%20Yao%20and%20Yunsheng%20Shi%0AAbstract%3A%20%20%20User%20information%20needs%20are%20often%20highly%20diverse%20and%20varied.%20A%20key%20challenge%0Ain%20current%20research%20is%20how%20to%20achieve%20controllable%20multi-objective%20generation%0Awhile%20enabling%20rapid%20adaptation%20to%20accommodate%20diverse%20user%20demands%20during%20test%0Atime.%20Existing%20solutions%2C%20such%20as%20Rewarded%20Soup%2C%20focus%20on%20merging%20language%0Amodels%20individually%20tuned%20on%20single%20objectives.%20While%20easy%20to%20implement%20and%0Awidely%20used%2C%20these%20approaches%20face%20limitations%20in%20achieving%20optimal%20performance%0Adue%20to%20their%20disregard%20for%20the%20impacts%20of%20competing%20objectives%20on%20model%20tuning.%0ATo%20address%20this%20issue%2C%20we%20propose%20Bone%20Soup%2C%20a%20novel%20model%20merging%20approach%0Athat%20first%20seeks%20a%20series%20of%20backbone%20models%20by%20considering%20the%20impacts%20of%0Amultiple%20objectives%20and%20then%20makes%20the%20soup%20%28i.e.%2C%20merge%20the%20backbone%20models%29.%0ASpecifically%2C%20Bone%20Soup%20begins%20by%20training%20multiple%20backbone%20models%20for%0Adifferent%20objectives%20using%20multi-objective%20reinforcement%20learning.%20Each%0Abackbone%20model%20is%20guided%20by%20a%20combination%20of%20backbone%20reward%20signals.%20To%20ensure%0Athat%20these%20models%20are%20optimal%20for%20the%20Pareto%20front%2C%20the%20backbone%20rewards%20are%0Acrafted%20by%20combining%20standard%20reward%20functions%20into%20basis%20vectors%2C%20which%20can%0Athen%20be%20modified%20through%20a%20rule-based%20construction%20method.%20Bone%20Soup%20leverages%0Aa%20symmetric%20circulant%20matrix%20mapping%20to%20generate%20the%20merging%20coefficients%2C%0Awhich%20are%20used%20to%20merge%20the%20backbone%20models%20according%20to%20user%20preferences.%0AExtensive%20experimental%20results%20demonstrate%20that%20Bone%20Soup%20exhibits%20strong%0Acontrollability%20and%20Pareto%20optimality%20in%20controllable%20multi-objective%0Ageneration%2C%20providing%20a%20more%20effective%20and%20efficient%20approach%20to%20addressing%0Adiverse%20user%20needs%20at%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBone%2520Soups%253A%2520A%2520Seek-and-Soup%2520Model%2520Merging%2520Approach%2520for%2520Controllable%250A%2520%2520Multi-Objective%2520Generation%26entry.906535625%3DGuofu%2520Xie%2520and%2520Xiao%2520Zhang%2520and%2520Ting%2520Yao%2520and%2520Yunsheng%2520Shi%26entry.1292438233%3D%2520%2520User%2520information%2520needs%2520are%2520often%2520highly%2520diverse%2520and%2520varied.%2520A%2520key%2520challenge%250Ain%2520current%2520research%2520is%2520how%2520to%2520achieve%2520controllable%2520multi-objective%2520generation%250Awhile%2520enabling%2520rapid%2520adaptation%2520to%2520accommodate%2520diverse%2520user%2520demands%2520during%2520test%250Atime.%2520Existing%2520solutions%252C%2520such%2520as%2520Rewarded%2520Soup%252C%2520focus%2520on%2520merging%2520language%250Amodels%2520individually%2520tuned%2520on%2520single%2520objectives.%2520While%2520easy%2520to%2520implement%2520and%250Awidely%2520used%252C%2520these%2520approaches%2520face%2520limitations%2520in%2520achieving%2520optimal%2520performance%250Adue%2520to%2520their%2520disregard%2520for%2520the%2520impacts%2520of%2520competing%2520objectives%2520on%2520model%2520tuning.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520Bone%2520Soup%252C%2520a%2520novel%2520model%2520merging%2520approach%250Athat%2520first%2520seeks%2520a%2520series%2520of%2520backbone%2520models%2520by%2520considering%2520the%2520impacts%2520of%250Amultiple%2520objectives%2520and%2520then%2520makes%2520the%2520soup%2520%2528i.e.%252C%2520merge%2520the%2520backbone%2520models%2529.%250ASpecifically%252C%2520Bone%2520Soup%2520begins%2520by%2520training%2520multiple%2520backbone%2520models%2520for%250Adifferent%2520objectives%2520using%2520multi-objective%2520reinforcement%2520learning.%2520Each%250Abackbone%2520model%2520is%2520guided%2520by%2520a%2520combination%2520of%2520backbone%2520reward%2520signals.%2520To%2520ensure%250Athat%2520these%2520models%2520are%2520optimal%2520for%2520the%2520Pareto%2520front%252C%2520the%2520backbone%2520rewards%2520are%250Acrafted%2520by%2520combining%2520standard%2520reward%2520functions%2520into%2520basis%2520vectors%252C%2520which%2520can%250Athen%2520be%2520modified%2520through%2520a%2520rule-based%2520construction%2520method.%2520Bone%2520Soup%2520leverages%250Aa%2520symmetric%2520circulant%2520matrix%2520mapping%2520to%2520generate%2520the%2520merging%2520coefficients%252C%250Awhich%2520are%2520used%2520to%2520merge%2520the%2520backbone%2520models%2520according%2520to%2520user%2520preferences.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520Bone%2520Soup%2520exhibits%2520strong%250Acontrollability%2520and%2520Pareto%2520optimality%2520in%2520controllable%2520multi-objective%250Ageneration%252C%2520providing%2520a%2520more%2520effective%2520and%2520efficient%2520approach%2520to%2520addressing%250Adiverse%2520user%2520needs%2520at%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bone%20Soups%3A%20A%20Seek-and-Soup%20Model%20Merging%20Approach%20for%20Controllable%0A%20%20Multi-Objective%20Generation&entry.906535625=Guofu%20Xie%20and%20Xiao%20Zhang%20and%20Ting%20Yao%20and%20Yunsheng%20Shi&entry.1292438233=%20%20User%20information%20needs%20are%20often%20highly%20diverse%20and%20varied.%20A%20key%20challenge%0Ain%20current%20research%20is%20how%20to%20achieve%20controllable%20multi-objective%20generation%0Awhile%20enabling%20rapid%20adaptation%20to%20accommodate%20diverse%20user%20demands%20during%20test%0Atime.%20Existing%20solutions%2C%20such%20as%20Rewarded%20Soup%2C%20focus%20on%20merging%20language%0Amodels%20individually%20tuned%20on%20single%20objectives.%20While%20easy%20to%20implement%20and%0Awidely%20used%2C%20these%20approaches%20face%20limitations%20in%20achieving%20optimal%20performance%0Adue%20to%20their%20disregard%20for%20the%20impacts%20of%20competing%20objectives%20on%20model%20tuning.%0ATo%20address%20this%20issue%2C%20we%20propose%20Bone%20Soup%2C%20a%20novel%20model%20merging%20approach%0Athat%20first%20seeks%20a%20series%20of%20backbone%20models%20by%20considering%20the%20impacts%20of%0Amultiple%20objectives%20and%20then%20makes%20the%20soup%20%28i.e.%2C%20merge%20the%20backbone%20models%29.%0ASpecifically%2C%20Bone%20Soup%20begins%20by%20training%20multiple%20backbone%20models%20for%0Adifferent%20objectives%20using%20multi-objective%20reinforcement%20learning.%20Each%0Abackbone%20model%20is%20guided%20by%20a%20combination%20of%20backbone%20reward%20signals.%20To%20ensure%0Athat%20these%20models%20are%20optimal%20for%20the%20Pareto%20front%2C%20the%20backbone%20rewards%20are%0Acrafted%20by%20combining%20standard%20reward%20functions%20into%20basis%20vectors%2C%20which%20can%0Athen%20be%20modified%20through%20a%20rule-based%20construction%20method.%20Bone%20Soup%20leverages%0Aa%20symmetric%20circulant%20matrix%20mapping%20to%20generate%20the%20merging%20coefficients%2C%0Awhich%20are%20used%20to%20merge%20the%20backbone%20models%20according%20to%20user%20preferences.%0AExtensive%20experimental%20results%20demonstrate%20that%20Bone%20Soup%20exhibits%20strong%0Acontrollability%20and%20Pareto%20optimality%20in%20controllable%20multi-objective%0Ageneration%2C%20providing%20a%20more%20effective%20and%20efficient%20approach%20to%20addressing%0Adiverse%20user%20needs%20at%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10762v2&entry.124074799=Read"},
{"title": "Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist:\n  Discovering Diverse Ecosystem Dynamics", "author": "Thomas Michel and Marko Cvjetko and Gautier Hamon and Pierre-Yves Oudeyer and Cl\u00e9ment Moulin-Frier", "abstract": "  We present a method for the automated discovery of system-level dynamics in\nFlow-Lenia--a continuous cellular automaton (CA) with mass conservation and\nparameter localization-using a curiosity--driven AI scientist. This method aims\nto uncover processes leading to self-organization of evolutionary and\necosystemic dynamics in CAs. We build on previous work which uses diversity\nsearch algorithms in Lenia to find self-organized individual patterns, and\nextend it to large environments that support distinct interacting patterns. We\nadapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive\nexploration of diverse Flow-Lenia environments using simulation-wide metrics,\nsuch as evolutionary activity, compression-based complexity, and multi-scale\nentropy. We test our method in two experiments, showcasing its ability to\nilluminate significantly more diverse dynamics compared to random search. We\nshow qualitative results illustrating how ecosystemic simulations enable\nself-organization of complex collective behaviors not captured by previous\nindividual pattern search and analysis. We complement automated discovery with\nan interactive exploration tool, creating an effective human-AI collaborative\nworkflow for scientific investigation. Though demonstrated specifically with\nFlow-Lenia, this methodology provides a framework potentially applicable to\nother parameterizable complex systems where understanding emergent collective\nproperties is of interest.\n", "link": "http://arxiv.org/abs/2505.15998v2", "date": "2025-06-02", "relevancy": 1.4856, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5624}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4796}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Flow-Lenia%20Universes%20with%20a%20Curiosity-driven%20AI%20Scientist%3A%0A%20%20Discovering%20Diverse%20Ecosystem%20Dynamics&body=Title%3A%20Exploring%20Flow-Lenia%20Universes%20with%20a%20Curiosity-driven%20AI%20Scientist%3A%0A%20%20Discovering%20Diverse%20Ecosystem%20Dynamics%0AAuthor%3A%20Thomas%20Michel%20and%20Marko%20Cvjetko%20and%20Gautier%20Hamon%20and%20Pierre-Yves%20Oudeyer%20and%20Cl%C3%A9ment%20Moulin-Frier%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20the%20automated%20discovery%20of%20system-level%20dynamics%20in%0AFlow-Lenia--a%20continuous%20cellular%20automaton%20%28CA%29%20with%20mass%20conservation%20and%0Aparameter%20localization-using%20a%20curiosity--driven%20AI%20scientist.%20This%20method%20aims%0Ato%20uncover%20processes%20leading%20to%20self-organization%20of%20evolutionary%20and%0Aecosystemic%20dynamics%20in%20CAs.%20We%20build%20on%20previous%20work%20which%20uses%20diversity%0Asearch%20algorithms%20in%20Lenia%20to%20find%20self-organized%20individual%20patterns%2C%20and%0Aextend%20it%20to%20large%20environments%20that%20support%20distinct%20interacting%20patterns.%20We%0Aadapt%20Intrinsically%20Motivated%20Goal%20Exploration%20Processes%20%28IMGEPs%29%20to%20drive%0Aexploration%20of%20diverse%20Flow-Lenia%20environments%20using%20simulation-wide%20metrics%2C%0Asuch%20as%20evolutionary%20activity%2C%20compression-based%20complexity%2C%20and%20multi-scale%0Aentropy.%20We%20test%20our%20method%20in%20two%20experiments%2C%20showcasing%20its%20ability%20to%0Ailluminate%20significantly%20more%20diverse%20dynamics%20compared%20to%20random%20search.%20We%0Ashow%20qualitative%20results%20illustrating%20how%20ecosystemic%20simulations%20enable%0Aself-organization%20of%20complex%20collective%20behaviors%20not%20captured%20by%20previous%0Aindividual%20pattern%20search%20and%20analysis.%20We%20complement%20automated%20discovery%20with%0Aan%20interactive%20exploration%20tool%2C%20creating%20an%20effective%20human-AI%20collaborative%0Aworkflow%20for%20scientific%20investigation.%20Though%20demonstrated%20specifically%20with%0AFlow-Lenia%2C%20this%20methodology%20provides%20a%20framework%20potentially%20applicable%20to%0Aother%20parameterizable%20complex%20systems%20where%20understanding%20emergent%20collective%0Aproperties%20is%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Flow-Lenia%2520Universes%2520with%2520a%2520Curiosity-driven%2520AI%2520Scientist%253A%250A%2520%2520Discovering%2520Diverse%2520Ecosystem%2520Dynamics%26entry.906535625%3DThomas%2520Michel%2520and%2520Marko%2520Cvjetko%2520and%2520Gautier%2520Hamon%2520and%2520Pierre-Yves%2520Oudeyer%2520and%2520Cl%25C3%25A9ment%2520Moulin-Frier%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520the%2520automated%2520discovery%2520of%2520system-level%2520dynamics%2520in%250AFlow-Lenia--a%2520continuous%2520cellular%2520automaton%2520%2528CA%2529%2520with%2520mass%2520conservation%2520and%250Aparameter%2520localization-using%2520a%2520curiosity--driven%2520AI%2520scientist.%2520This%2520method%2520aims%250Ato%2520uncover%2520processes%2520leading%2520to%2520self-organization%2520of%2520evolutionary%2520and%250Aecosystemic%2520dynamics%2520in%2520CAs.%2520We%2520build%2520on%2520previous%2520work%2520which%2520uses%2520diversity%250Asearch%2520algorithms%2520in%2520Lenia%2520to%2520find%2520self-organized%2520individual%2520patterns%252C%2520and%250Aextend%2520it%2520to%2520large%2520environments%2520that%2520support%2520distinct%2520interacting%2520patterns.%2520We%250Aadapt%2520Intrinsically%2520Motivated%2520Goal%2520Exploration%2520Processes%2520%2528IMGEPs%2529%2520to%2520drive%250Aexploration%2520of%2520diverse%2520Flow-Lenia%2520environments%2520using%2520simulation-wide%2520metrics%252C%250Asuch%2520as%2520evolutionary%2520activity%252C%2520compression-based%2520complexity%252C%2520and%2520multi-scale%250Aentropy.%2520We%2520test%2520our%2520method%2520in%2520two%2520experiments%252C%2520showcasing%2520its%2520ability%2520to%250Ailluminate%2520significantly%2520more%2520diverse%2520dynamics%2520compared%2520to%2520random%2520search.%2520We%250Ashow%2520qualitative%2520results%2520illustrating%2520how%2520ecosystemic%2520simulations%2520enable%250Aself-organization%2520of%2520complex%2520collective%2520behaviors%2520not%2520captured%2520by%2520previous%250Aindividual%2520pattern%2520search%2520and%2520analysis.%2520We%2520complement%2520automated%2520discovery%2520with%250Aan%2520interactive%2520exploration%2520tool%252C%2520creating%2520an%2520effective%2520human-AI%2520collaborative%250Aworkflow%2520for%2520scientific%2520investigation.%2520Though%2520demonstrated%2520specifically%2520with%250AFlow-Lenia%252C%2520this%2520methodology%2520provides%2520a%2520framework%2520potentially%2520applicable%2520to%250Aother%2520parameterizable%2520complex%2520systems%2520where%2520understanding%2520emergent%2520collective%250Aproperties%2520is%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Flow-Lenia%20Universes%20with%20a%20Curiosity-driven%20AI%20Scientist%3A%0A%20%20Discovering%20Diverse%20Ecosystem%20Dynamics&entry.906535625=Thomas%20Michel%20and%20Marko%20Cvjetko%20and%20Gautier%20Hamon%20and%20Pierre-Yves%20Oudeyer%20and%20Cl%C3%A9ment%20Moulin-Frier&entry.1292438233=%20%20We%20present%20a%20method%20for%20the%20automated%20discovery%20of%20system-level%20dynamics%20in%0AFlow-Lenia--a%20continuous%20cellular%20automaton%20%28CA%29%20with%20mass%20conservation%20and%0Aparameter%20localization-using%20a%20curiosity--driven%20AI%20scientist.%20This%20method%20aims%0Ato%20uncover%20processes%20leading%20to%20self-organization%20of%20evolutionary%20and%0Aecosystemic%20dynamics%20in%20CAs.%20We%20build%20on%20previous%20work%20which%20uses%20diversity%0Asearch%20algorithms%20in%20Lenia%20to%20find%20self-organized%20individual%20patterns%2C%20and%0Aextend%20it%20to%20large%20environments%20that%20support%20distinct%20interacting%20patterns.%20We%0Aadapt%20Intrinsically%20Motivated%20Goal%20Exploration%20Processes%20%28IMGEPs%29%20to%20drive%0Aexploration%20of%20diverse%20Flow-Lenia%20environments%20using%20simulation-wide%20metrics%2C%0Asuch%20as%20evolutionary%20activity%2C%20compression-based%20complexity%2C%20and%20multi-scale%0Aentropy.%20We%20test%20our%20method%20in%20two%20experiments%2C%20showcasing%20its%20ability%20to%0Ailluminate%20significantly%20more%20diverse%20dynamics%20compared%20to%20random%20search.%20We%0Ashow%20qualitative%20results%20illustrating%20how%20ecosystemic%20simulations%20enable%0Aself-organization%20of%20complex%20collective%20behaviors%20not%20captured%20by%20previous%0Aindividual%20pattern%20search%20and%20analysis.%20We%20complement%20automated%20discovery%20with%0Aan%20interactive%20exploration%20tool%2C%20creating%20an%20effective%20human-AI%20collaborative%0Aworkflow%20for%20scientific%20investigation.%20Though%20demonstrated%20specifically%20with%0AFlow-Lenia%2C%20this%20methodology%20provides%20a%20framework%20potentially%20applicable%20to%0Aother%20parameterizable%20complex%20systems%20where%20understanding%20emergent%20collective%0Aproperties%20is%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15998v2&entry.124074799=Read"},
{"title": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs", "author": "Tianheng Ling and Chao Qian and Lukas Johannes Ha\u00dfler and Gregor Schiele", "abstract": "  Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).\n", "link": "http://arxiv.org/abs/2505.17662v2", "date": "2025-06-02", "relevancy": 1.4575, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5237}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.485}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Versatile%20Time-Series%20Analysis%20with%20Tiny%20Transformers%20on%0A%20%20Embedded%20FPGAs&body=Title%3A%20Automating%20Versatile%20Time-Series%20Analysis%20with%20Tiny%20Transformers%20on%0A%20%20Embedded%20FPGAs%0AAuthor%3A%20Tianheng%20Ling%20and%20Chao%20Qian%20and%20Lukas%20Johannes%20Ha%C3%9Fler%20and%20Gregor%20Schiele%0AAbstract%3A%20%20%20Transformer-based%20models%20have%20shown%20strong%20performance%20across%20diverse%0Atime-series%20tasks%2C%20but%20their%20deployment%20on%20resource-constrained%20devices%20remains%0Achallenging%20due%20to%20high%20memory%20and%20computational%20demand.%20While%20prior%20work%0Atargeting%20Microcontroller%20Units%20%28MCUs%29%20has%20explored%20hardware-specific%0Aoptimizations%2C%20such%20approaches%20are%20often%20task-specific%20and%20limited%20to%208-bit%0Afixed-point%20precision.%20Field-Programmable%20Gate%20Arrays%20%28FPGAs%29%20offer%20greater%0Aflexibility%2C%20enabling%20fine-grained%20control%20over%20data%20precision%20and%0Aarchitecture.%20However%2C%20existing%20FPGA-based%20deployments%20of%20Transformers%20for%0Atime-series%20analysis%20typically%20focus%20on%20high-density%20platforms%20with%20manual%0Aconfiguration.%20This%20paper%20presents%20a%20unified%20and%20fully%20automated%20deployment%0Aframework%20for%20Tiny%20Transformers%20on%20embedded%20FPGAs.%20Our%20framework%20supports%20a%0Acompact%20encoder-only%20Transformer%20architecture%20across%20three%20representative%0Atime-series%20tasks%20%28forecasting%2C%20classification%2C%20and%20anomaly%20detection%29.%20It%0Acombines%20quantization-aware%20training%20%28down%20to%204%20bits%29%2C%20hardware-aware%0Ahyperparameter%20search%20using%20Optuna%2C%20and%20automatic%20VHDL%20generation%20for%20seamless%0Adeployment.%20We%20evaluate%20our%20framework%20on%20six%20public%20datasets%20across%20two%0Aembedded%20FPGA%20platforms.%20Results%20show%20that%20our%20framework%20produces%20integer-only%2C%0Atask-specific%20Transformer%20accelerators%20achieving%20as%20low%20as%200.033%20mJ%20per%0Ainference%20with%20millisecond%20latency%20on%20AMD%20Spartan-7%2C%20while%20also%20providing%0Ainsights%20into%20deployment%20feasibility%20on%20Lattice%20iCE40.%20All%20source%20code%20will%20be%0Areleased%20in%20the%20GitHub%20repository%0A%28https%3A//github.com/Edwina1030/TinyTransformer4TS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Versatile%2520Time-Series%2520Analysis%2520with%2520Tiny%2520Transformers%2520on%250A%2520%2520Embedded%2520FPGAs%26entry.906535625%3DTianheng%2520Ling%2520and%2520Chao%2520Qian%2520and%2520Lukas%2520Johannes%2520Ha%25C3%259Fler%2520and%2520Gregor%2520Schiele%26entry.1292438233%3D%2520%2520Transformer-based%2520models%2520have%2520shown%2520strong%2520performance%2520across%2520diverse%250Atime-series%2520tasks%252C%2520but%2520their%2520deployment%2520on%2520resource-constrained%2520devices%2520remains%250Achallenging%2520due%2520to%2520high%2520memory%2520and%2520computational%2520demand.%2520While%2520prior%2520work%250Atargeting%2520Microcontroller%2520Units%2520%2528MCUs%2529%2520has%2520explored%2520hardware-specific%250Aoptimizations%252C%2520such%2520approaches%2520are%2520often%2520task-specific%2520and%2520limited%2520to%25208-bit%250Afixed-point%2520precision.%2520Field-Programmable%2520Gate%2520Arrays%2520%2528FPGAs%2529%2520offer%2520greater%250Aflexibility%252C%2520enabling%2520fine-grained%2520control%2520over%2520data%2520precision%2520and%250Aarchitecture.%2520However%252C%2520existing%2520FPGA-based%2520deployments%2520of%2520Transformers%2520for%250Atime-series%2520analysis%2520typically%2520focus%2520on%2520high-density%2520platforms%2520with%2520manual%250Aconfiguration.%2520This%2520paper%2520presents%2520a%2520unified%2520and%2520fully%2520automated%2520deployment%250Aframework%2520for%2520Tiny%2520Transformers%2520on%2520embedded%2520FPGAs.%2520Our%2520framework%2520supports%2520a%250Acompact%2520encoder-only%2520Transformer%2520architecture%2520across%2520three%2520representative%250Atime-series%2520tasks%2520%2528forecasting%252C%2520classification%252C%2520and%2520anomaly%2520detection%2529.%2520It%250Acombines%2520quantization-aware%2520training%2520%2528down%2520to%25204%2520bits%2529%252C%2520hardware-aware%250Ahyperparameter%2520search%2520using%2520Optuna%252C%2520and%2520automatic%2520VHDL%2520generation%2520for%2520seamless%250Adeployment.%2520We%2520evaluate%2520our%2520framework%2520on%2520six%2520public%2520datasets%2520across%2520two%250Aembedded%2520FPGA%2520platforms.%2520Results%2520show%2520that%2520our%2520framework%2520produces%2520integer-only%252C%250Atask-specific%2520Transformer%2520accelerators%2520achieving%2520as%2520low%2520as%25200.033%2520mJ%2520per%250Ainference%2520with%2520millisecond%2520latency%2520on%2520AMD%2520Spartan-7%252C%2520while%2520also%2520providing%250Ainsights%2520into%2520deployment%2520feasibility%2520on%2520Lattice%2520iCE40.%2520All%2520source%2520code%2520will%2520be%250Areleased%2520in%2520the%2520GitHub%2520repository%250A%2528https%253A//github.com/Edwina1030/TinyTransformer4TS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Versatile%20Time-Series%20Analysis%20with%20Tiny%20Transformers%20on%0A%20%20Embedded%20FPGAs&entry.906535625=Tianheng%20Ling%20and%20Chao%20Qian%20and%20Lukas%20Johannes%20Ha%C3%9Fler%20and%20Gregor%20Schiele&entry.1292438233=%20%20Transformer-based%20models%20have%20shown%20strong%20performance%20across%20diverse%0Atime-series%20tasks%2C%20but%20their%20deployment%20on%20resource-constrained%20devices%20remains%0Achallenging%20due%20to%20high%20memory%20and%20computational%20demand.%20While%20prior%20work%0Atargeting%20Microcontroller%20Units%20%28MCUs%29%20has%20explored%20hardware-specific%0Aoptimizations%2C%20such%20approaches%20are%20often%20task-specific%20and%20limited%20to%208-bit%0Afixed-point%20precision.%20Field-Programmable%20Gate%20Arrays%20%28FPGAs%29%20offer%20greater%0Aflexibility%2C%20enabling%20fine-grained%20control%20over%20data%20precision%20and%0Aarchitecture.%20However%2C%20existing%20FPGA-based%20deployments%20of%20Transformers%20for%0Atime-series%20analysis%20typically%20focus%20on%20high-density%20platforms%20with%20manual%0Aconfiguration.%20This%20paper%20presents%20a%20unified%20and%20fully%20automated%20deployment%0Aframework%20for%20Tiny%20Transformers%20on%20embedded%20FPGAs.%20Our%20framework%20supports%20a%0Acompact%20encoder-only%20Transformer%20architecture%20across%20three%20representative%0Atime-series%20tasks%20%28forecasting%2C%20classification%2C%20and%20anomaly%20detection%29.%20It%0Acombines%20quantization-aware%20training%20%28down%20to%204%20bits%29%2C%20hardware-aware%0Ahyperparameter%20search%20using%20Optuna%2C%20and%20automatic%20VHDL%20generation%20for%20seamless%0Adeployment.%20We%20evaluate%20our%20framework%20on%20six%20public%20datasets%20across%20two%0Aembedded%20FPGA%20platforms.%20Results%20show%20that%20our%20framework%20produces%20integer-only%2C%0Atask-specific%20Transformer%20accelerators%20achieving%20as%20low%20as%200.033%20mJ%20per%0Ainference%20with%20millisecond%20latency%20on%20AMD%20Spartan-7%2C%20while%20also%20providing%0Ainsights%20into%20deployment%20feasibility%20on%20Lattice%20iCE40.%20All%20source%20code%20will%20be%0Areleased%20in%20the%20GitHub%20repository%0A%28https%3A//github.com/Edwina1030/TinyTransformer4TS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17662v2&entry.124074799=Read"},
{"title": "GCoT: Chain-of-Thought Prompt Learning for Graphs", "author": "Xingtong Yu and Chang Zhou and Zhongwei Kuai and Xinming Zhang and Yuan Fang", "abstract": "  Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.\n", "link": "http://arxiv.org/abs/2502.08092v2", "date": "2025-06-02", "relevancy": 1.4467, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4947}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4805}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCoT%3A%20Chain-of-Thought%20Prompt%20Learning%20for%20Graphs&body=Title%3A%20GCoT%3A%20Chain-of-Thought%20Prompt%20Learning%20for%20Graphs%0AAuthor%3A%20Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Zhongwei%20Kuai%20and%20Xinming%20Zhang%20and%20Yuan%20Fang%0AAbstract%3A%20%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29.%20However%2C%20its%20vast%20potential%20remains%20largely%0Aunexplored%20for%20graphs.%20This%20raises%20an%20interesting%20question%3A%20How%20can%20we%20design%0ACoT%20prompting%20for%20graphs%20to%20guide%20graph%20models%20to%20learn%20step%20by%20step%3F%20On%20one%0Ahand%2C%20unlike%20natural%20languages%2C%20graphs%20are%20non-linear%20and%20characterized%20by%0Acomplex%20topological%20structures.%20On%20the%20other%20hand%2C%20many%20graphs%20lack%20textual%0Adata%2C%20making%20it%20difficult%20to%20formulate%20language-based%20CoT%20prompting.%20In%20this%0Awork%2C%20we%20propose%20the%20first%20CoT%20prompt%20learning%20framework%20for%20text-free%20graphs%2C%0AGCoT.%20Specifically%2C%20we%20decompose%20the%20adaptation%20process%20for%20each%20downstream%0Atask%20into%20a%20series%20of%20inference%20steps%2C%20with%20each%20step%20consisting%20of%0Aprompt-based%20inference%2C%20%60%60thought%27%27%20generation%2C%20and%20thought-conditioned%20prompt%0Alearning.%20While%20the%20steps%20mimic%20CoT%20prompting%20in%20NLP%2C%20the%20exact%20mechanism%0Adiffers%20significantly.%20Specifically%2C%20at%20each%20step%2C%20an%20input%20graph%2C%20along%20with%20a%0Aprompt%2C%20is%20first%20fed%20into%20a%20pre-trained%20graph%20encoder%20for%20prompt-based%0Ainference.%20We%20then%20aggregate%20the%20hidden%20layers%20of%20the%20encoder%20to%20construct%20a%0A%60%60thought%27%27%2C%20which%20captures%20the%20working%20state%20of%20each%20node%20in%20the%20current%20step.%0AConditioned%20on%20this%20thought%2C%20we%20learn%20a%20prompt%20specific%20to%20each%20node%20based%20on%0Athe%20current%20state.%20These%20prompts%20are%20fed%20into%20the%20next%20inference%20step%2C%0Arepeating%20the%20cycle.%20To%20evaluate%20and%20analyze%20the%20effectiveness%20of%20GCoT%2C%20we%0Aconduct%20comprehensive%20experiments%20on%20eight%20public%20datasets%2C%20which%20demonstrate%0Athe%20advantage%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCoT%253A%2520Chain-of-Thought%2520Prompt%2520Learning%2520for%2520Graphs%26entry.906535625%3DXingtong%2520Yu%2520and%2520Chang%2520Zhou%2520and%2520Zhongwei%2520Kuai%2520and%2520Xinming%2520Zhang%2520and%2520Yuan%2520Fang%26entry.1292438233%3D%2520%2520Chain-of-thought%2520%2528CoT%2529%2520prompting%2520has%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520processing%2520%2528NLP%2529.%2520However%252C%2520its%2520vast%2520potential%2520remains%2520largely%250Aunexplored%2520for%2520graphs.%2520This%2520raises%2520an%2520interesting%2520question%253A%2520How%2520can%2520we%2520design%250ACoT%2520prompting%2520for%2520graphs%2520to%2520guide%2520graph%2520models%2520to%2520learn%2520step%2520by%2520step%253F%2520On%2520one%250Ahand%252C%2520unlike%2520natural%2520languages%252C%2520graphs%2520are%2520non-linear%2520and%2520characterized%2520by%250Acomplex%2520topological%2520structures.%2520On%2520the%2520other%2520hand%252C%2520many%2520graphs%2520lack%2520textual%250Adata%252C%2520making%2520it%2520difficult%2520to%2520formulate%2520language-based%2520CoT%2520prompting.%2520In%2520this%250Awork%252C%2520we%2520propose%2520the%2520first%2520CoT%2520prompt%2520learning%2520framework%2520for%2520text-free%2520graphs%252C%250AGCoT.%2520Specifically%252C%2520we%2520decompose%2520the%2520adaptation%2520process%2520for%2520each%2520downstream%250Atask%2520into%2520a%2520series%2520of%2520inference%2520steps%252C%2520with%2520each%2520step%2520consisting%2520of%250Aprompt-based%2520inference%252C%2520%2560%2560thought%2527%2527%2520generation%252C%2520and%2520thought-conditioned%2520prompt%250Alearning.%2520While%2520the%2520steps%2520mimic%2520CoT%2520prompting%2520in%2520NLP%252C%2520the%2520exact%2520mechanism%250Adiffers%2520significantly.%2520Specifically%252C%2520at%2520each%2520step%252C%2520an%2520input%2520graph%252C%2520along%2520with%2520a%250Aprompt%252C%2520is%2520first%2520fed%2520into%2520a%2520pre-trained%2520graph%2520encoder%2520for%2520prompt-based%250Ainference.%2520We%2520then%2520aggregate%2520the%2520hidden%2520layers%2520of%2520the%2520encoder%2520to%2520construct%2520a%250A%2560%2560thought%2527%2527%252C%2520which%2520captures%2520the%2520working%2520state%2520of%2520each%2520node%2520in%2520the%2520current%2520step.%250AConditioned%2520on%2520this%2520thought%252C%2520we%2520learn%2520a%2520prompt%2520specific%2520to%2520each%2520node%2520based%2520on%250Athe%2520current%2520state.%2520These%2520prompts%2520are%2520fed%2520into%2520the%2520next%2520inference%2520step%252C%250Arepeating%2520the%2520cycle.%2520To%2520evaluate%2520and%2520analyze%2520the%2520effectiveness%2520of%2520GCoT%252C%2520we%250Aconduct%2520comprehensive%2520experiments%2520on%2520eight%2520public%2520datasets%252C%2520which%2520demonstrate%250Athe%2520advantage%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCoT%3A%20Chain-of-Thought%20Prompt%20Learning%20for%20Graphs&entry.906535625=Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Zhongwei%20Kuai%20and%20Xinming%20Zhang%20and%20Yuan%20Fang&entry.1292438233=%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20%28NLP%29.%20However%2C%20its%20vast%20potential%20remains%20largely%0Aunexplored%20for%20graphs.%20This%20raises%20an%20interesting%20question%3A%20How%20can%20we%20design%0ACoT%20prompting%20for%20graphs%20to%20guide%20graph%20models%20to%20learn%20step%20by%20step%3F%20On%20one%0Ahand%2C%20unlike%20natural%20languages%2C%20graphs%20are%20non-linear%20and%20characterized%20by%0Acomplex%20topological%20structures.%20On%20the%20other%20hand%2C%20many%20graphs%20lack%20textual%0Adata%2C%20making%20it%20difficult%20to%20formulate%20language-based%20CoT%20prompting.%20In%20this%0Awork%2C%20we%20propose%20the%20first%20CoT%20prompt%20learning%20framework%20for%20text-free%20graphs%2C%0AGCoT.%20Specifically%2C%20we%20decompose%20the%20adaptation%20process%20for%20each%20downstream%0Atask%20into%20a%20series%20of%20inference%20steps%2C%20with%20each%20step%20consisting%20of%0Aprompt-based%20inference%2C%20%60%60thought%27%27%20generation%2C%20and%20thought-conditioned%20prompt%0Alearning.%20While%20the%20steps%20mimic%20CoT%20prompting%20in%20NLP%2C%20the%20exact%20mechanism%0Adiffers%20significantly.%20Specifically%2C%20at%20each%20step%2C%20an%20input%20graph%2C%20along%20with%20a%0Aprompt%2C%20is%20first%20fed%20into%20a%20pre-trained%20graph%20encoder%20for%20prompt-based%0Ainference.%20We%20then%20aggregate%20the%20hidden%20layers%20of%20the%20encoder%20to%20construct%20a%0A%60%60thought%27%27%2C%20which%20captures%20the%20working%20state%20of%20each%20node%20in%20the%20current%20step.%0AConditioned%20on%20this%20thought%2C%20we%20learn%20a%20prompt%20specific%20to%20each%20node%20based%20on%0Athe%20current%20state.%20These%20prompts%20are%20fed%20into%20the%20next%20inference%20step%2C%0Arepeating%20the%20cycle.%20To%20evaluate%20and%20analyze%20the%20effectiveness%20of%20GCoT%2C%20we%0Aconduct%20comprehensive%20experiments%20on%20eight%20public%20datasets%2C%20which%20demonstrate%0Athe%20advantage%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08092v2&entry.124074799=Read"},
{"title": "Random Policy Evaluation Uncovers Policies of Generative Flow Networks", "author": "Haoran He and Emmanuel Bengio and Qingpeng Cai and Ling Pan", "abstract": "  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects\nproportionally to an unnormalized reward function. A number of recent works\nexplored connections between GFlowNets and maximum entropy (MaxEnt) RL, which\nmodifies the standard objective of RL agents by learning an entropy-regularized\nobjective. However, the relationship between GFlowNets and standard RL remains\nlargely unexplored, despite the inherent similarities in their sequential\ndecision-making nature. While GFlowNets can discover diverse solutions through\nspecialized flow-matching objectives, connecting them can simplify their\nimplementation through established RL principles and improve RL's diverse\nsolution discovery capabilities. In this paper, we bridge this gap by revealing\na fundamental connection between GFlowNets and one RL's most basic components\n-- policy evaluation. Surprisingly, we find that the value function obtained\nfrom evaluating a uniform policy is closely associated with the flow functions\nin GFlowNets through the lens of flow iteration under certain structural\nconditions. Building upon these insights, we introduce a rectified random\npolicy evaluation (RPE) algorithm, which achieves the same reward-matching\neffect as GFlowNets based on simply evaluating a fixed random policy in these\ncases, offering a new perspective. Empirical results across extensive\nbenchmarks demonstrate that RPE achieves competitive results compared to\nprevious approaches, shedding light on the previously overlooked connection\nbetween (non-MaxEnt) RL and GFlowNets.\n", "link": "http://arxiv.org/abs/2406.02213v3", "date": "2025-06-02", "relevancy": 1.4357, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5145}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Policy%20Evaluation%20Uncovers%20Policies%20of%20Generative%20Flow%20Networks&body=Title%3A%20Random%20Policy%20Evaluation%20Uncovers%20Policies%20of%20Generative%20Flow%20Networks%0AAuthor%3A%20Haoran%20He%20and%20Emmanuel%20Bengio%20and%20Qingpeng%20Cai%20and%20Ling%20Pan%0AAbstract%3A%20%20%20The%20Generative%20Flow%20Network%20%28GFlowNet%29%20is%20a%20probabilistic%20framework%20in%20which%0Aan%20agent%20learns%20a%20stochastic%20policy%20and%20flow%20functions%20to%20sample%20objects%0Aproportionally%20to%20an%20unnormalized%20reward%20function.%20A%20number%20of%20recent%20works%0Aexplored%20connections%20between%20GFlowNets%20and%20maximum%20entropy%20%28MaxEnt%29%20RL%2C%20which%0Amodifies%20the%20standard%20objective%20of%20RL%20agents%20by%20learning%20an%20entropy-regularized%0Aobjective.%20However%2C%20the%20relationship%20between%20GFlowNets%20and%20standard%20RL%20remains%0Alargely%20unexplored%2C%20despite%20the%20inherent%20similarities%20in%20their%20sequential%0Adecision-making%20nature.%20While%20GFlowNets%20can%20discover%20diverse%20solutions%20through%0Aspecialized%20flow-matching%20objectives%2C%20connecting%20them%20can%20simplify%20their%0Aimplementation%20through%20established%20RL%20principles%20and%20improve%20RL%27s%20diverse%0Asolution%20discovery%20capabilities.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20by%20revealing%0Aa%20fundamental%20connection%20between%20GFlowNets%20and%20one%20RL%27s%20most%20basic%20components%0A--%20policy%20evaluation.%20Surprisingly%2C%20we%20find%20that%20the%20value%20function%20obtained%0Afrom%20evaluating%20a%20uniform%20policy%20is%20closely%20associated%20with%20the%20flow%20functions%0Ain%20GFlowNets%20through%20the%20lens%20of%20flow%20iteration%20under%20certain%20structural%0Aconditions.%20Building%20upon%20these%20insights%2C%20we%20introduce%20a%20rectified%20random%0Apolicy%20evaluation%20%28RPE%29%20algorithm%2C%20which%20achieves%20the%20same%20reward-matching%0Aeffect%20as%20GFlowNets%20based%20on%20simply%20evaluating%20a%20fixed%20random%20policy%20in%20these%0Acases%2C%20offering%20a%20new%20perspective.%20Empirical%20results%20across%20extensive%0Abenchmarks%20demonstrate%20that%20RPE%20achieves%20competitive%20results%20compared%20to%0Aprevious%20approaches%2C%20shedding%20light%20on%20the%20previously%20overlooked%20connection%0Abetween%20%28non-MaxEnt%29%20RL%20and%20GFlowNets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02213v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Policy%2520Evaluation%2520Uncovers%2520Policies%2520of%2520Generative%2520Flow%2520Networks%26entry.906535625%3DHaoran%2520He%2520and%2520Emmanuel%2520Bengio%2520and%2520Qingpeng%2520Cai%2520and%2520Ling%2520Pan%26entry.1292438233%3D%2520%2520The%2520Generative%2520Flow%2520Network%2520%2528GFlowNet%2529%2520is%2520a%2520probabilistic%2520framework%2520in%2520which%250Aan%2520agent%2520learns%2520a%2520stochastic%2520policy%2520and%2520flow%2520functions%2520to%2520sample%2520objects%250Aproportionally%2520to%2520an%2520unnormalized%2520reward%2520function.%2520A%2520number%2520of%2520recent%2520works%250Aexplored%2520connections%2520between%2520GFlowNets%2520and%2520maximum%2520entropy%2520%2528MaxEnt%2529%2520RL%252C%2520which%250Amodifies%2520the%2520standard%2520objective%2520of%2520RL%2520agents%2520by%2520learning%2520an%2520entropy-regularized%250Aobjective.%2520However%252C%2520the%2520relationship%2520between%2520GFlowNets%2520and%2520standard%2520RL%2520remains%250Alargely%2520unexplored%252C%2520despite%2520the%2520inherent%2520similarities%2520in%2520their%2520sequential%250Adecision-making%2520nature.%2520While%2520GFlowNets%2520can%2520discover%2520diverse%2520solutions%2520through%250Aspecialized%2520flow-matching%2520objectives%252C%2520connecting%2520them%2520can%2520simplify%2520their%250Aimplementation%2520through%2520established%2520RL%2520principles%2520and%2520improve%2520RL%2527s%2520diverse%250Asolution%2520discovery%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520bridge%2520this%2520gap%2520by%2520revealing%250Aa%2520fundamental%2520connection%2520between%2520GFlowNets%2520and%2520one%2520RL%2527s%2520most%2520basic%2520components%250A--%2520policy%2520evaluation.%2520Surprisingly%252C%2520we%2520find%2520that%2520the%2520value%2520function%2520obtained%250Afrom%2520evaluating%2520a%2520uniform%2520policy%2520is%2520closely%2520associated%2520with%2520the%2520flow%2520functions%250Ain%2520GFlowNets%2520through%2520the%2520lens%2520of%2520flow%2520iteration%2520under%2520certain%2520structural%250Aconditions.%2520Building%2520upon%2520these%2520insights%252C%2520we%2520introduce%2520a%2520rectified%2520random%250Apolicy%2520evaluation%2520%2528RPE%2529%2520algorithm%252C%2520which%2520achieves%2520the%2520same%2520reward-matching%250Aeffect%2520as%2520GFlowNets%2520based%2520on%2520simply%2520evaluating%2520a%2520fixed%2520random%2520policy%2520in%2520these%250Acases%252C%2520offering%2520a%2520new%2520perspective.%2520Empirical%2520results%2520across%2520extensive%250Abenchmarks%2520demonstrate%2520that%2520RPE%2520achieves%2520competitive%2520results%2520compared%2520to%250Aprevious%2520approaches%252C%2520shedding%2520light%2520on%2520the%2520previously%2520overlooked%2520connection%250Abetween%2520%2528non-MaxEnt%2529%2520RL%2520and%2520GFlowNets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02213v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Policy%20Evaluation%20Uncovers%20Policies%20of%20Generative%20Flow%20Networks&entry.906535625=Haoran%20He%20and%20Emmanuel%20Bengio%20and%20Qingpeng%20Cai%20and%20Ling%20Pan&entry.1292438233=%20%20The%20Generative%20Flow%20Network%20%28GFlowNet%29%20is%20a%20probabilistic%20framework%20in%20which%0Aan%20agent%20learns%20a%20stochastic%20policy%20and%20flow%20functions%20to%20sample%20objects%0Aproportionally%20to%20an%20unnormalized%20reward%20function.%20A%20number%20of%20recent%20works%0Aexplored%20connections%20between%20GFlowNets%20and%20maximum%20entropy%20%28MaxEnt%29%20RL%2C%20which%0Amodifies%20the%20standard%20objective%20of%20RL%20agents%20by%20learning%20an%20entropy-regularized%0Aobjective.%20However%2C%20the%20relationship%20between%20GFlowNets%20and%20standard%20RL%20remains%0Alargely%20unexplored%2C%20despite%20the%20inherent%20similarities%20in%20their%20sequential%0Adecision-making%20nature.%20While%20GFlowNets%20can%20discover%20diverse%20solutions%20through%0Aspecialized%20flow-matching%20objectives%2C%20connecting%20them%20can%20simplify%20their%0Aimplementation%20through%20established%20RL%20principles%20and%20improve%20RL%27s%20diverse%0Asolution%20discovery%20capabilities.%20In%20this%20paper%2C%20we%20bridge%20this%20gap%20by%20revealing%0Aa%20fundamental%20connection%20between%20GFlowNets%20and%20one%20RL%27s%20most%20basic%20components%0A--%20policy%20evaluation.%20Surprisingly%2C%20we%20find%20that%20the%20value%20function%20obtained%0Afrom%20evaluating%20a%20uniform%20policy%20is%20closely%20associated%20with%20the%20flow%20functions%0Ain%20GFlowNets%20through%20the%20lens%20of%20flow%20iteration%20under%20certain%20structural%0Aconditions.%20Building%20upon%20these%20insights%2C%20we%20introduce%20a%20rectified%20random%0Apolicy%20evaluation%20%28RPE%29%20algorithm%2C%20which%20achieves%20the%20same%20reward-matching%0Aeffect%20as%20GFlowNets%20based%20on%20simply%20evaluating%20a%20fixed%20random%20policy%20in%20these%0Acases%2C%20offering%20a%20new%20perspective.%20Empirical%20results%20across%20extensive%0Abenchmarks%20demonstrate%20that%20RPE%20achieves%20competitive%20results%20compared%20to%0Aprevious%20approaches%2C%20shedding%20light%20on%20the%20previously%20overlooked%20connection%0Abetween%20%28non-MaxEnt%29%20RL%20and%20GFlowNets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02213v3&entry.124074799=Read"},
{"title": "Humans Coexist, So Must Embodied Artificial Agents", "author": "Hannah Kuehn and Joseph La Delfa and Miguel Vasco and Danica Kragic and Iolanda Leite", "abstract": "  This paper introduces the concept of coexistence for embodied artificial\nagents and argues that it is a prerequisite for long-term, in-the-wild\ninteraction with humans. Contemporary embodied artificial agents excel in\nstatic, predefined tasks but fall short in dynamic and long-term interactions\nwith humans. On the other hand, humans can adapt and evolve continuously,\nexploiting the situated knowledge embedded in their environment and other\nagents, thus contributing to meaningful interactions. We take an\ninterdisciplinary approach at different levels of organization, drawing from\nbiology and design theory, to understand how human and non-human organisms\nfoster entities that coexist within their specific environments. Finally, we\npropose key research directions for the artificial intelligence community to\ndevelop coexisting embodied agents, focusing on the principles, hardware and\nlearning methods responsible for shaping them.\n", "link": "http://arxiv.org/abs/2502.04809v3", "date": "2025-06-02", "relevancy": 1.4315, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humans%20Coexist%2C%20So%20Must%20Embodied%20Artificial%20Agents&body=Title%3A%20Humans%20Coexist%2C%20So%20Must%20Embodied%20Artificial%20Agents%0AAuthor%3A%20Hannah%20Kuehn%20and%20Joseph%20La%20Delfa%20and%20Miguel%20Vasco%20and%20Danica%20Kragic%20and%20Iolanda%20Leite%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20concept%20of%20coexistence%20for%20embodied%20artificial%0Aagents%20and%20argues%20that%20it%20is%20a%20prerequisite%20for%20long-term%2C%20in-the-wild%0Ainteraction%20with%20humans.%20Contemporary%20embodied%20artificial%20agents%20excel%20in%0Astatic%2C%20predefined%20tasks%20but%20fall%20short%20in%20dynamic%20and%20long-term%20interactions%0Awith%20humans.%20On%20the%20other%20hand%2C%20humans%20can%20adapt%20and%20evolve%20continuously%2C%0Aexploiting%20the%20situated%20knowledge%20embedded%20in%20their%20environment%20and%20other%0Aagents%2C%20thus%20contributing%20to%20meaningful%20interactions.%20We%20take%20an%0Ainterdisciplinary%20approach%20at%20different%20levels%20of%20organization%2C%20drawing%20from%0Abiology%20and%20design%20theory%2C%20to%20understand%20how%20human%20and%20non-human%20organisms%0Afoster%20entities%20that%20coexist%20within%20their%20specific%20environments.%20Finally%2C%20we%0Apropose%20key%20research%20directions%20for%20the%20artificial%20intelligence%20community%20to%0Adevelop%20coexisting%20embodied%20agents%2C%20focusing%20on%20the%20principles%2C%20hardware%20and%0Alearning%20methods%20responsible%20for%20shaping%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04809v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumans%2520Coexist%252C%2520So%2520Must%2520Embodied%2520Artificial%2520Agents%26entry.906535625%3DHannah%2520Kuehn%2520and%2520Joseph%2520La%2520Delfa%2520and%2520Miguel%2520Vasco%2520and%2520Danica%2520Kragic%2520and%2520Iolanda%2520Leite%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520coexistence%2520for%2520embodied%2520artificial%250Aagents%2520and%2520argues%2520that%2520it%2520is%2520a%2520prerequisite%2520for%2520long-term%252C%2520in-the-wild%250Ainteraction%2520with%2520humans.%2520Contemporary%2520embodied%2520artificial%2520agents%2520excel%2520in%250Astatic%252C%2520predefined%2520tasks%2520but%2520fall%2520short%2520in%2520dynamic%2520and%2520long-term%2520interactions%250Awith%2520humans.%2520On%2520the%2520other%2520hand%252C%2520humans%2520can%2520adapt%2520and%2520evolve%2520continuously%252C%250Aexploiting%2520the%2520situated%2520knowledge%2520embedded%2520in%2520their%2520environment%2520and%2520other%250Aagents%252C%2520thus%2520contributing%2520to%2520meaningful%2520interactions.%2520We%2520take%2520an%250Ainterdisciplinary%2520approach%2520at%2520different%2520levels%2520of%2520organization%252C%2520drawing%2520from%250Abiology%2520and%2520design%2520theory%252C%2520to%2520understand%2520how%2520human%2520and%2520non-human%2520organisms%250Afoster%2520entities%2520that%2520coexist%2520within%2520their%2520specific%2520environments.%2520Finally%252C%2520we%250Apropose%2520key%2520research%2520directions%2520for%2520the%2520artificial%2520intelligence%2520community%2520to%250Adevelop%2520coexisting%2520embodied%2520agents%252C%2520focusing%2520on%2520the%2520principles%252C%2520hardware%2520and%250Alearning%2520methods%2520responsible%2520for%2520shaping%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04809v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humans%20Coexist%2C%20So%20Must%20Embodied%20Artificial%20Agents&entry.906535625=Hannah%20Kuehn%20and%20Joseph%20La%20Delfa%20and%20Miguel%20Vasco%20and%20Danica%20Kragic%20and%20Iolanda%20Leite&entry.1292438233=%20%20This%20paper%20introduces%20the%20concept%20of%20coexistence%20for%20embodied%20artificial%0Aagents%20and%20argues%20that%20it%20is%20a%20prerequisite%20for%20long-term%2C%20in-the-wild%0Ainteraction%20with%20humans.%20Contemporary%20embodied%20artificial%20agents%20excel%20in%0Astatic%2C%20predefined%20tasks%20but%20fall%20short%20in%20dynamic%20and%20long-term%20interactions%0Awith%20humans.%20On%20the%20other%20hand%2C%20humans%20can%20adapt%20and%20evolve%20continuously%2C%0Aexploiting%20the%20situated%20knowledge%20embedded%20in%20their%20environment%20and%20other%0Aagents%2C%20thus%20contributing%20to%20meaningful%20interactions.%20We%20take%20an%0Ainterdisciplinary%20approach%20at%20different%20levels%20of%20organization%2C%20drawing%20from%0Abiology%20and%20design%20theory%2C%20to%20understand%20how%20human%20and%20non-human%20organisms%0Afoster%20entities%20that%20coexist%20within%20their%20specific%20environments.%20Finally%2C%20we%0Apropose%20key%20research%20directions%20for%20the%20artificial%20intelligence%20community%20to%0Adevelop%20coexisting%20embodied%20agents%2C%20focusing%20on%20the%20principles%2C%20hardware%20and%0Alearning%20methods%20responsible%20for%20shaping%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04809v3&entry.124074799=Read"},
{"title": "NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via\n  Optimal Transport for Cardiovascular Disease Detection", "author": "Peihong Zhang and Zhixin Li and Rui Sang and Yuxuan Liu and Yiqiang Cai and Yizhou Tan and Shengchen Li", "abstract": "  Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a\nlatent coupling signal representing the electrical-to-mechanical cardiac\ntransformation. While valuable for cardiovascular disease (CVD) detection, this\ncoupling signal is traditionally estimated using deconvolution methods that\namplify noise, limiting clinical utility. In this paper, we propose\nNoise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates\nthe problem as distribution matching via optimal transport theory. By jointly\noptimizing amplitude and temporal alignment, NMCSE mitigates noise\namplification without additional preprocessing. Integrated with our\nTemporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal\nCVD detection. Experiments on the PhysioNet 2016 dataset with realistic\nhospital noise demonstrate that NMCSE reduces estimation errors by\napproximately 30% in Mean Squared Error while maintaining higher Pearson\nCorrelation Coefficients across all tested signal-to-noise ratios. Our approach\nachieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming\nstate-of-the-art methods and demonstrating robust performance for real-world\nclinical applications.\n", "link": "http://arxiv.org/abs/2505.18174v2", "date": "2025-06-02", "relevancy": 1.4194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4827}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NMCSE%3A%20Noise-Robust%20Multi-Modal%20Coupling%20Signal%20Estimation%20Method%20via%0A%20%20Optimal%20Transport%20for%20Cardiovascular%20Disease%20Detection&body=Title%3A%20NMCSE%3A%20Noise-Robust%20Multi-Modal%20Coupling%20Signal%20Estimation%20Method%20via%0A%20%20Optimal%20Transport%20for%20Cardiovascular%20Disease%20Detection%0AAuthor%3A%20Peihong%20Zhang%20and%20Zhixin%20Li%20and%20Rui%20Sang%20and%20Yuxuan%20Liu%20and%20Yiqiang%20Cai%20and%20Yizhou%20Tan%20and%20Shengchen%20Li%0AAbstract%3A%20%20%20Electrocardiogram%20%28ECG%29%20and%20Phonocardiogram%20%28PCG%29%20signals%20are%20linked%20by%20a%0Alatent%20coupling%20signal%20representing%20the%20electrical-to-mechanical%20cardiac%0Atransformation.%20While%20valuable%20for%20cardiovascular%20disease%20%28CVD%29%20detection%2C%20this%0Acoupling%20signal%20is%20traditionally%20estimated%20using%20deconvolution%20methods%20that%0Aamplify%20noise%2C%20limiting%20clinical%20utility.%20In%20this%20paper%2C%20we%20propose%0ANoise-Robust%20Multi-Modal%20Coupling%20Signal%20Estimation%20%28NMCSE%29%2C%20which%20reformulates%0Athe%20problem%20as%20distribution%20matching%20via%20optimal%20transport%20theory.%20By%20jointly%0Aoptimizing%20amplitude%20and%20temporal%20alignment%2C%20NMCSE%20mitigates%20noise%0Aamplification%20without%20additional%20preprocessing.%20Integrated%20with%20our%0ATemporal-Spatial%20Feature%20Extraction%20network%2C%20NMCSE%20enables%20robust%20multi-modal%0ACVD%20detection.%20Experiments%20on%20the%20PhysioNet%202016%20dataset%20with%20realistic%0Ahospital%20noise%20demonstrate%20that%20NMCSE%20reduces%20estimation%20errors%20by%0Aapproximately%2030%25%20in%20Mean%20Squared%20Error%20while%20maintaining%20higher%20Pearson%0ACorrelation%20Coefficients%20across%20all%20tested%20signal-to-noise%20ratios.%20Our%20approach%0Aachieves%2097.38%25%20accuracy%20and%200.98%20AUC%20in%20CVD%20detection%2C%20outperforming%0Astate-of-the-art%20methods%20and%20demonstrating%20robust%20performance%20for%20real-world%0Aclinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNMCSE%253A%2520Noise-Robust%2520Multi-Modal%2520Coupling%2520Signal%2520Estimation%2520Method%2520via%250A%2520%2520Optimal%2520Transport%2520for%2520Cardiovascular%2520Disease%2520Detection%26entry.906535625%3DPeihong%2520Zhang%2520and%2520Zhixin%2520Li%2520and%2520Rui%2520Sang%2520and%2520Yuxuan%2520Liu%2520and%2520Yiqiang%2520Cai%2520and%2520Yizhou%2520Tan%2520and%2520Shengchen%2520Li%26entry.1292438233%3D%2520%2520Electrocardiogram%2520%2528ECG%2529%2520and%2520Phonocardiogram%2520%2528PCG%2529%2520signals%2520are%2520linked%2520by%2520a%250Alatent%2520coupling%2520signal%2520representing%2520the%2520electrical-to-mechanical%2520cardiac%250Atransformation.%2520While%2520valuable%2520for%2520cardiovascular%2520disease%2520%2528CVD%2529%2520detection%252C%2520this%250Acoupling%2520signal%2520is%2520traditionally%2520estimated%2520using%2520deconvolution%2520methods%2520that%250Aamplify%2520noise%252C%2520limiting%2520clinical%2520utility.%2520In%2520this%2520paper%252C%2520we%2520propose%250ANoise-Robust%2520Multi-Modal%2520Coupling%2520Signal%2520Estimation%2520%2528NMCSE%2529%252C%2520which%2520reformulates%250Athe%2520problem%2520as%2520distribution%2520matching%2520via%2520optimal%2520transport%2520theory.%2520By%2520jointly%250Aoptimizing%2520amplitude%2520and%2520temporal%2520alignment%252C%2520NMCSE%2520mitigates%2520noise%250Aamplification%2520without%2520additional%2520preprocessing.%2520Integrated%2520with%2520our%250ATemporal-Spatial%2520Feature%2520Extraction%2520network%252C%2520NMCSE%2520enables%2520robust%2520multi-modal%250ACVD%2520detection.%2520Experiments%2520on%2520the%2520PhysioNet%25202016%2520dataset%2520with%2520realistic%250Ahospital%2520noise%2520demonstrate%2520that%2520NMCSE%2520reduces%2520estimation%2520errors%2520by%250Aapproximately%252030%2525%2520in%2520Mean%2520Squared%2520Error%2520while%2520maintaining%2520higher%2520Pearson%250ACorrelation%2520Coefficients%2520across%2520all%2520tested%2520signal-to-noise%2520ratios.%2520Our%2520approach%250Aachieves%252097.38%2525%2520accuracy%2520and%25200.98%2520AUC%2520in%2520CVD%2520detection%252C%2520outperforming%250Astate-of-the-art%2520methods%2520and%2520demonstrating%2520robust%2520performance%2520for%2520real-world%250Aclinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NMCSE%3A%20Noise-Robust%20Multi-Modal%20Coupling%20Signal%20Estimation%20Method%20via%0A%20%20Optimal%20Transport%20for%20Cardiovascular%20Disease%20Detection&entry.906535625=Peihong%20Zhang%20and%20Zhixin%20Li%20and%20Rui%20Sang%20and%20Yuxuan%20Liu%20and%20Yiqiang%20Cai%20and%20Yizhou%20Tan%20and%20Shengchen%20Li&entry.1292438233=%20%20Electrocardiogram%20%28ECG%29%20and%20Phonocardiogram%20%28PCG%29%20signals%20are%20linked%20by%20a%0Alatent%20coupling%20signal%20representing%20the%20electrical-to-mechanical%20cardiac%0Atransformation.%20While%20valuable%20for%20cardiovascular%20disease%20%28CVD%29%20detection%2C%20this%0Acoupling%20signal%20is%20traditionally%20estimated%20using%20deconvolution%20methods%20that%0Aamplify%20noise%2C%20limiting%20clinical%20utility.%20In%20this%20paper%2C%20we%20propose%0ANoise-Robust%20Multi-Modal%20Coupling%20Signal%20Estimation%20%28NMCSE%29%2C%20which%20reformulates%0Athe%20problem%20as%20distribution%20matching%20via%20optimal%20transport%20theory.%20By%20jointly%0Aoptimizing%20amplitude%20and%20temporal%20alignment%2C%20NMCSE%20mitigates%20noise%0Aamplification%20without%20additional%20preprocessing.%20Integrated%20with%20our%0ATemporal-Spatial%20Feature%20Extraction%20network%2C%20NMCSE%20enables%20robust%20multi-modal%0ACVD%20detection.%20Experiments%20on%20the%20PhysioNet%202016%20dataset%20with%20realistic%0Ahospital%20noise%20demonstrate%20that%20NMCSE%20reduces%20estimation%20errors%20by%0Aapproximately%2030%25%20in%20Mean%20Squared%20Error%20while%20maintaining%20higher%20Pearson%0ACorrelation%20Coefficients%20across%20all%20tested%20signal-to-noise%20ratios.%20Our%20approach%0Aachieves%2097.38%25%20accuracy%20and%200.98%20AUC%20in%20CVD%20detection%2C%20outperforming%0Astate-of-the-art%20methods%20and%20demonstrating%20robust%20performance%20for%20real-world%0Aclinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18174v2&entry.124074799=Read"},
{"title": "Causally Reliable Concept Bottleneck Models", "author": "Giovanni De Felice and Arianna Casanova Flores and Francesco De Santis and Silvia Santini and Johannes Schneider and Pietro Barbiero and Alberto Termine", "abstract": "  Concept-based models are an emerging paradigm in deep learning that\nconstrains the inference process to operate through human-interpretable\nvariables, facilitating explainability and human interaction. However, these\narchitectures, on par with popular opaque neural models, fail to account for\nthe true causal mechanisms underlying the target phenomena represented in the\ndata. This hampers their ability to support causal reasoning tasks, limits\nout-of-distribution generalization, and hinders the implementation of fairness\nconstraints. To overcome these issues, we propose Causally reliable Concept\nBottleneck Models (C$^2$BMs), a class of concept-based architectures that\nenforce reasoning through a bottleneck of concepts structured according to a\nmodel of the real-world causal mechanisms. We also introduce a pipeline to\nautomatically learn this structure from observational data and unstructured\nbackground knowledge (e.g., scientific literature). Experimental evidence\nsuggests that C$^2$BMs are more interpretable, causally reliable, and improve\nresponsiveness to interventions w.r.t. standard opaque and concept-based\nmodels, while maintaining their accuracy.\n", "link": "http://arxiv.org/abs/2503.04363v2", "date": "2025-06-02", "relevancy": 1.4126, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causally%20Reliable%20Concept%20Bottleneck%20Models&body=Title%3A%20Causally%20Reliable%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Giovanni%20De%20Felice%20and%20Arianna%20Casanova%20Flores%20and%20Francesco%20De%20Santis%20and%20Silvia%20Santini%20and%20Johannes%20Schneider%20and%20Pietro%20Barbiero%20and%20Alberto%20Termine%0AAbstract%3A%20%20%20Concept-based%20models%20are%20an%20emerging%20paradigm%20in%20deep%20learning%20that%0Aconstrains%20the%20inference%20process%20to%20operate%20through%20human-interpretable%0Avariables%2C%20facilitating%20explainability%20and%20human%20interaction.%20However%2C%20these%0Aarchitectures%2C%20on%20par%20with%20popular%20opaque%20neural%20models%2C%20fail%20to%20account%20for%0Athe%20true%20causal%20mechanisms%20underlying%20the%20target%20phenomena%20represented%20in%20the%0Adata.%20This%20hampers%20their%20ability%20to%20support%20causal%20reasoning%20tasks%2C%20limits%0Aout-of-distribution%20generalization%2C%20and%20hinders%20the%20implementation%20of%20fairness%0Aconstraints.%20To%20overcome%20these%20issues%2C%20we%20propose%20Causally%20reliable%20Concept%0ABottleneck%20Models%20%28C%24%5E2%24BMs%29%2C%20a%20class%20of%20concept-based%20architectures%20that%0Aenforce%20reasoning%20through%20a%20bottleneck%20of%20concepts%20structured%20according%20to%20a%0Amodel%20of%20the%20real-world%20causal%20mechanisms.%20We%20also%20introduce%20a%20pipeline%20to%0Aautomatically%20learn%20this%20structure%20from%20observational%20data%20and%20unstructured%0Abackground%20knowledge%20%28e.g.%2C%20scientific%20literature%29.%20Experimental%20evidence%0Asuggests%20that%20C%24%5E2%24BMs%20are%20more%20interpretable%2C%20causally%20reliable%2C%20and%20improve%0Aresponsiveness%20to%20interventions%20w.r.t.%20standard%20opaque%20and%20concept-based%0Amodels%2C%20while%20maintaining%20their%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausally%2520Reliable%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DGiovanni%2520De%2520Felice%2520and%2520Arianna%2520Casanova%2520Flores%2520and%2520Francesco%2520De%2520Santis%2520and%2520Silvia%2520Santini%2520and%2520Johannes%2520Schneider%2520and%2520Pietro%2520Barbiero%2520and%2520Alberto%2520Termine%26entry.1292438233%3D%2520%2520Concept-based%2520models%2520are%2520an%2520emerging%2520paradigm%2520in%2520deep%2520learning%2520that%250Aconstrains%2520the%2520inference%2520process%2520to%2520operate%2520through%2520human-interpretable%250Avariables%252C%2520facilitating%2520explainability%2520and%2520human%2520interaction.%2520However%252C%2520these%250Aarchitectures%252C%2520on%2520par%2520with%2520popular%2520opaque%2520neural%2520models%252C%2520fail%2520to%2520account%2520for%250Athe%2520true%2520causal%2520mechanisms%2520underlying%2520the%2520target%2520phenomena%2520represented%2520in%2520the%250Adata.%2520This%2520hampers%2520their%2520ability%2520to%2520support%2520causal%2520reasoning%2520tasks%252C%2520limits%250Aout-of-distribution%2520generalization%252C%2520and%2520hinders%2520the%2520implementation%2520of%2520fairness%250Aconstraints.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520Causally%2520reliable%2520Concept%250ABottleneck%2520Models%2520%2528C%2524%255E2%2524BMs%2529%252C%2520a%2520class%2520of%2520concept-based%2520architectures%2520that%250Aenforce%2520reasoning%2520through%2520a%2520bottleneck%2520of%2520concepts%2520structured%2520according%2520to%2520a%250Amodel%2520of%2520the%2520real-world%2520causal%2520mechanisms.%2520We%2520also%2520introduce%2520a%2520pipeline%2520to%250Aautomatically%2520learn%2520this%2520structure%2520from%2520observational%2520data%2520and%2520unstructured%250Abackground%2520knowledge%2520%2528e.g.%252C%2520scientific%2520literature%2529.%2520Experimental%2520evidence%250Asuggests%2520that%2520C%2524%255E2%2524BMs%2520are%2520more%2520interpretable%252C%2520causally%2520reliable%252C%2520and%2520improve%250Aresponsiveness%2520to%2520interventions%2520w.r.t.%2520standard%2520opaque%2520and%2520concept-based%250Amodels%252C%2520while%2520maintaining%2520their%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causally%20Reliable%20Concept%20Bottleneck%20Models&entry.906535625=Giovanni%20De%20Felice%20and%20Arianna%20Casanova%20Flores%20and%20Francesco%20De%20Santis%20and%20Silvia%20Santini%20and%20Johannes%20Schneider%20and%20Pietro%20Barbiero%20and%20Alberto%20Termine&entry.1292438233=%20%20Concept-based%20models%20are%20an%20emerging%20paradigm%20in%20deep%20learning%20that%0Aconstrains%20the%20inference%20process%20to%20operate%20through%20human-interpretable%0Avariables%2C%20facilitating%20explainability%20and%20human%20interaction.%20However%2C%20these%0Aarchitectures%2C%20on%20par%20with%20popular%20opaque%20neural%20models%2C%20fail%20to%20account%20for%0Athe%20true%20causal%20mechanisms%20underlying%20the%20target%20phenomena%20represented%20in%20the%0Adata.%20This%20hampers%20their%20ability%20to%20support%20causal%20reasoning%20tasks%2C%20limits%0Aout-of-distribution%20generalization%2C%20and%20hinders%20the%20implementation%20of%20fairness%0Aconstraints.%20To%20overcome%20these%20issues%2C%20we%20propose%20Causally%20reliable%20Concept%0ABottleneck%20Models%20%28C%24%5E2%24BMs%29%2C%20a%20class%20of%20concept-based%20architectures%20that%0Aenforce%20reasoning%20through%20a%20bottleneck%20of%20concepts%20structured%20according%20to%20a%0Amodel%20of%20the%20real-world%20causal%20mechanisms.%20We%20also%20introduce%20a%20pipeline%20to%0Aautomatically%20learn%20this%20structure%20from%20observational%20data%20and%20unstructured%0Abackground%20knowledge%20%28e.g.%2C%20scientific%20literature%29.%20Experimental%20evidence%0Asuggests%20that%20C%24%5E2%24BMs%20are%20more%20interpretable%2C%20causally%20reliable%2C%20and%20improve%0Aresponsiveness%20to%20interventions%20w.r.t.%20standard%20opaque%20and%20concept-based%0Amodels%2C%20while%20maintaining%20their%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04363v2&entry.124074799=Read"},
{"title": "Enhancing Transformers for Generalizable First-Order Logical Entailment", "author": "Tianshi Zheng and Jiazheng Wang and Zihao Wang and Jiaxin Bai and Hang Yin and Zheye Deng and Yangqiu Song and Jianxin Li", "abstract": "  Transformers, as the fundamental deep learning architecture, have\ndemonstrated great capability in reasoning. This paper studies the\ngeneralizable first-order logical reasoning ability of transformers with their\nparameterized knowledge and how to improve it. Transformers' capability of\nfirst-order reasoning is further captured by whether they can conduct\nfirst-order logical entailment, which is quantitatively measured by their\nperformance in answering knowledge graph queries. We establish the connections\nbetween (1) two types of distribution shifts studied in out-of-distribution\ngeneralization and (2) unseen knowledge and query settings discussed in the\ntask of knowledge graph query answering, which makes it possible to\ncharacterize the fine-grained generalizability. Results on our comprehensive\ndataset showed that transformers outperform previous methods designed\nparticularly for this task and provided detailed empirical evidence about the\nimpact of the input query syntax, token embedding, and transformer\narchitectures on the reasoning capability of transformers. Interestingly, our\nresults revealed the mismatch of positional encoding and other design choices\nof transformer architectures in previous practices. Motivated by this, we\npropose TEGA, a logic-aware architecture that significantly improves the\nperformance in generalizable first-order logical entailment.\n", "link": "http://arxiv.org/abs/2501.00759v2", "date": "2025-06-02", "relevancy": 1.369, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4931}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4483}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Transformers%20for%20Generalizable%20First-Order%20Logical%20Entailment&body=Title%3A%20Enhancing%20Transformers%20for%20Generalizable%20First-Order%20Logical%20Entailment%0AAuthor%3A%20Tianshi%20Zheng%20and%20Jiazheng%20Wang%20and%20Zihao%20Wang%20and%20Jiaxin%20Bai%20and%20Hang%20Yin%20and%20Zheye%20Deng%20and%20Yangqiu%20Song%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Transformers%2C%20as%20the%20fundamental%20deep%20learning%20architecture%2C%20have%0Ademonstrated%20great%20capability%20in%20reasoning.%20This%20paper%20studies%20the%0Ageneralizable%20first-order%20logical%20reasoning%20ability%20of%20transformers%20with%20their%0Aparameterized%20knowledge%20and%20how%20to%20improve%20it.%20Transformers%27%20capability%20of%0Afirst-order%20reasoning%20is%20further%20captured%20by%20whether%20they%20can%20conduct%0Afirst-order%20logical%20entailment%2C%20which%20is%20quantitatively%20measured%20by%20their%0Aperformance%20in%20answering%20knowledge%20graph%20queries.%20We%20establish%20the%20connections%0Abetween%20%281%29%20two%20types%20of%20distribution%20shifts%20studied%20in%20out-of-distribution%0Ageneralization%20and%20%282%29%20unseen%20knowledge%20and%20query%20settings%20discussed%20in%20the%0Atask%20of%20knowledge%20graph%20query%20answering%2C%20which%20makes%20it%20possible%20to%0Acharacterize%20the%20fine-grained%20generalizability.%20Results%20on%20our%20comprehensive%0Adataset%20showed%20that%20transformers%20outperform%20previous%20methods%20designed%0Aparticularly%20for%20this%20task%20and%20provided%20detailed%20empirical%20evidence%20about%20the%0Aimpact%20of%20the%20input%20query%20syntax%2C%20token%20embedding%2C%20and%20transformer%0Aarchitectures%20on%20the%20reasoning%20capability%20of%20transformers.%20Interestingly%2C%20our%0Aresults%20revealed%20the%20mismatch%20of%20positional%20encoding%20and%20other%20design%20choices%0Aof%20transformer%20architectures%20in%20previous%20practices.%20Motivated%20by%20this%2C%20we%0Apropose%20TEGA%2C%20a%20logic-aware%20architecture%20that%20significantly%20improves%20the%0Aperformance%20in%20generalizable%20first-order%20logical%20entailment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Transformers%2520for%2520Generalizable%2520First-Order%2520Logical%2520Entailment%26entry.906535625%3DTianshi%2520Zheng%2520and%2520Jiazheng%2520Wang%2520and%2520Zihao%2520Wang%2520and%2520Jiaxin%2520Bai%2520and%2520Hang%2520Yin%2520and%2520Zheye%2520Deng%2520and%2520Yangqiu%2520Song%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Transformers%252C%2520as%2520the%2520fundamental%2520deep%2520learning%2520architecture%252C%2520have%250Ademonstrated%2520great%2520capability%2520in%2520reasoning.%2520This%2520paper%2520studies%2520the%250Ageneralizable%2520first-order%2520logical%2520reasoning%2520ability%2520of%2520transformers%2520with%2520their%250Aparameterized%2520knowledge%2520and%2520how%2520to%2520improve%2520it.%2520Transformers%2527%2520capability%2520of%250Afirst-order%2520reasoning%2520is%2520further%2520captured%2520by%2520whether%2520they%2520can%2520conduct%250Afirst-order%2520logical%2520entailment%252C%2520which%2520is%2520quantitatively%2520measured%2520by%2520their%250Aperformance%2520in%2520answering%2520knowledge%2520graph%2520queries.%2520We%2520establish%2520the%2520connections%250Abetween%2520%25281%2529%2520two%2520types%2520of%2520distribution%2520shifts%2520studied%2520in%2520out-of-distribution%250Ageneralization%2520and%2520%25282%2529%2520unseen%2520knowledge%2520and%2520query%2520settings%2520discussed%2520in%2520the%250Atask%2520of%2520knowledge%2520graph%2520query%2520answering%252C%2520which%2520makes%2520it%2520possible%2520to%250Acharacterize%2520the%2520fine-grained%2520generalizability.%2520Results%2520on%2520our%2520comprehensive%250Adataset%2520showed%2520that%2520transformers%2520outperform%2520previous%2520methods%2520designed%250Aparticularly%2520for%2520this%2520task%2520and%2520provided%2520detailed%2520empirical%2520evidence%2520about%2520the%250Aimpact%2520of%2520the%2520input%2520query%2520syntax%252C%2520token%2520embedding%252C%2520and%2520transformer%250Aarchitectures%2520on%2520the%2520reasoning%2520capability%2520of%2520transformers.%2520Interestingly%252C%2520our%250Aresults%2520revealed%2520the%2520mismatch%2520of%2520positional%2520encoding%2520and%2520other%2520design%2520choices%250Aof%2520transformer%2520architectures%2520in%2520previous%2520practices.%2520Motivated%2520by%2520this%252C%2520we%250Apropose%2520TEGA%252C%2520a%2520logic-aware%2520architecture%2520that%2520significantly%2520improves%2520the%250Aperformance%2520in%2520generalizable%2520first-order%2520logical%2520entailment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Transformers%20for%20Generalizable%20First-Order%20Logical%20Entailment&entry.906535625=Tianshi%20Zheng%20and%20Jiazheng%20Wang%20and%20Zihao%20Wang%20and%20Jiaxin%20Bai%20and%20Hang%20Yin%20and%20Zheye%20Deng%20and%20Yangqiu%20Song%20and%20Jianxin%20Li&entry.1292438233=%20%20Transformers%2C%20as%20the%20fundamental%20deep%20learning%20architecture%2C%20have%0Ademonstrated%20great%20capability%20in%20reasoning.%20This%20paper%20studies%20the%0Ageneralizable%20first-order%20logical%20reasoning%20ability%20of%20transformers%20with%20their%0Aparameterized%20knowledge%20and%20how%20to%20improve%20it.%20Transformers%27%20capability%20of%0Afirst-order%20reasoning%20is%20further%20captured%20by%20whether%20they%20can%20conduct%0Afirst-order%20logical%20entailment%2C%20which%20is%20quantitatively%20measured%20by%20their%0Aperformance%20in%20answering%20knowledge%20graph%20queries.%20We%20establish%20the%20connections%0Abetween%20%281%29%20two%20types%20of%20distribution%20shifts%20studied%20in%20out-of-distribution%0Ageneralization%20and%20%282%29%20unseen%20knowledge%20and%20query%20settings%20discussed%20in%20the%0Atask%20of%20knowledge%20graph%20query%20answering%2C%20which%20makes%20it%20possible%20to%0Acharacterize%20the%20fine-grained%20generalizability.%20Results%20on%20our%20comprehensive%0Adataset%20showed%20that%20transformers%20outperform%20previous%20methods%20designed%0Aparticularly%20for%20this%20task%20and%20provided%20detailed%20empirical%20evidence%20about%20the%0Aimpact%20of%20the%20input%20query%20syntax%2C%20token%20embedding%2C%20and%20transformer%0Aarchitectures%20on%20the%20reasoning%20capability%20of%20transformers.%20Interestingly%2C%20our%0Aresults%20revealed%20the%20mismatch%20of%20positional%20encoding%20and%20other%20design%20choices%0Aof%20transformer%20architectures%20in%20previous%20practices.%20Motivated%20by%20this%2C%20we%0Apropose%20TEGA%2C%20a%20logic-aware%20architecture%20that%20significantly%20improves%20the%0Aperformance%20in%20generalizable%20first-order%20logical%20entailment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00759v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


