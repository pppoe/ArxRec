<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "AI-Generated Video Detection via Spatio-Temporal Anomaly Learning", "author": "Jianfa Bai and Man Lin and Gang Cao", "abstract": "  The advancement of generation models has led to the emergence of highly\nrealistic artificial intelligence (AI)-generated videos. Malicious users can\neasily create non-existent videos to spread false information. This letter\nproposes an effective AI-generated video detection (AIGVDet) scheme by\ncapturing the forensic traces with a two-branch spatio-temporal convolutional\nneural network (CNN). Specifically, two ResNet sub-detectors are learned\nseparately for identifying the anomalies in spatical and optical flow domains,\nrespectively. Results of such sub-detectors are fused to further enhance the\ndiscrimination ability. A large-scale generated video dataset (GVD) is\nconstructed as a benchmark for model training and evaluation. Extensive\nexperimental results verify the high generalization and robustness of our\nAIGVDet scheme. Code and dataset will be available at\nhttps://github.com/multimediaFor/AIGVDet.\n", "link": "http://arxiv.org/abs/2403.16638v1", "date": "2024-03-25", "relevancy": 2.893, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5965}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5714}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AI-Generated%20Video%20Detection%20via%20Spatio-Temporal%20Anomaly%20Learning&body=Title%3A%20AI-Generated%20Video%20Detection%20via%20Spatio-Temporal%20Anomaly%20Learning%0AAuthor%3A%20Jianfa%20Bai%20and%20Man%20Lin%20and%20Gang%20Cao%0AAbstract%3A%20%20%20The%20advancement%20of%20generation%20models%20has%20led%20to%20the%20emergence%20of%20highly%0Arealistic%20artificial%20intelligence%20%28AI%29-generated%20videos.%20Malicious%20users%20can%0Aeasily%20create%20non-existent%20videos%20to%20spread%20false%20information.%20This%20letter%0Aproposes%20an%20effective%20AI-generated%20video%20detection%20%28AIGVDet%29%20scheme%20by%0Acapturing%20the%20forensic%20traces%20with%20a%20two-branch%20spatio-temporal%20convolutional%0Aneural%20network%20%28CNN%29.%20Specifically%2C%20two%20ResNet%20sub-detectors%20are%20learned%0Aseparately%20for%20identifying%20the%20anomalies%20in%20spatical%20and%20optical%20flow%20domains%2C%0Arespectively.%20Results%20of%20such%20sub-detectors%20are%20fused%20to%20further%20enhance%20the%0Adiscrimination%20ability.%20A%20large-scale%20generated%20video%20dataset%20%28GVD%29%20is%0Aconstructed%20as%20a%20benchmark%20for%20model%20training%20and%20evaluation.%20Extensive%0Aexperimental%20results%20verify%20the%20high%20generalization%20and%20robustness%20of%20our%0AAIGVDet%20scheme.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/multimediaFor/AIGVDet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Generated%20Video%20Detection%20via%20Spatio-Temporal%20Anomaly%20Learning&entry.906535625=Jianfa%20Bai%20and%20Man%20Lin%20and%20Gang%20Cao&entry.1292438233=%20%20The%20advancement%20of%20generation%20models%20has%20led%20to%20the%20emergence%20of%20highly%0Arealistic%20artificial%20intelligence%20%28AI%29-generated%20videos.%20Malicious%20users%20can%0Aeasily%20create%20non-existent%20videos%20to%20spread%20false%20information.%20This%20letter%0Aproposes%20an%20effective%20AI-generated%20video%20detection%20%28AIGVDet%29%20scheme%20by%0Acapturing%20the%20forensic%20traces%20with%20a%20two-branch%20spatio-temporal%20convolutional%0Aneural%20network%20%28CNN%29.%20Specifically%2C%20two%20ResNet%20sub-detectors%20are%20learned%0Aseparately%20for%20identifying%20the%20anomalies%20in%20spatical%20and%20optical%20flow%20domains%2C%0Arespectively.%20Results%20of%20such%20sub-detectors%20are%20fused%20to%20further%20enhance%20the%0Adiscrimination%20ability.%20A%20large-scale%20generated%20video%20dataset%20%28GVD%29%20is%0Aconstructed%20as%20a%20benchmark%20for%20model%20training%20and%20evaluation.%20Extensive%0Aexperimental%20results%20verify%20the%20high%20generalization%20and%20robustness%20of%20our%0AAIGVDet%20scheme.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/multimediaFor/AIGVDet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16638v1&entry.124074799=Read"},
{"title": "Variational Bayes image restoration with compressive autoencoders", "author": "Maud Biquard and Marie Chabert and Thomas Oberlin", "abstract": "  Regularization of inverse problems is of paramount importance in\ncomputational imaging. The ability of neural networks to learn efficient image\nrepresentations has been recently exploited to design powerful data-driven\nregularizers. While state-of-the-art plug-and-play methods rely on an implicit\nregularization provided by neural denoisers, alternative Bayesian approaches\nconsider Maximum A Posteriori (MAP) estimation in the latent space of a\ngenerative model, thus with an explicit regularization. However,\nstate-of-the-art deep generative models require a huge amount of training data\ncompared to denoisers. Besides, their complexity hampers the optimization\ninvolved in latent MAP derivation. In this work, we first propose to use\ncompressive autoencoders instead. These networks, which can be seen as\nvariational autoencoders with a flexible latent prior, are smaller and easier\nto train than state-of-the-art generative models. As a second contribution, we\nintroduce the Variational Bayes Latent Estimation (VBLE) algorithm, which\nperforms latent estimation within the framework of variational inference.\nThanks to a simple yet efficient parameterization of the variational posterior,\nVBLE allows for fast and easy (approximate) posterior sampling. Experimental\nresults on image datasets BSD and FFHQ demonstrate that VBLE reaches similar\nperformance than state-of-the-art plug-and-play methods, while being able to\nquantify uncertainties faster than other existing posterior sampling\ntechniques.\n", "link": "http://arxiv.org/abs/2311.17744v2", "date": "2024-03-25", "relevancy": 2.8543, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5793}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5784}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5548}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Variational%20Bayes%20image%20restoration%20with%20compressive%20autoencoders&body=Title%3A%20Variational%20Bayes%20image%20restoration%20with%20compressive%20autoencoders%0AAuthor%3A%20Maud%20Biquard%20and%20Marie%20Chabert%20and%20Thomas%20Oberlin%0AAbstract%3A%20%20%20Regularization%20of%20inverse%20problems%20is%20of%20paramount%20importance%20in%0Acomputational%20imaging.%20The%20ability%20of%20neural%20networks%20to%20learn%20efficient%20image%0Arepresentations%20has%20been%20recently%20exploited%20to%20design%20powerful%20data-driven%0Aregularizers.%20While%20state-of-the-art%20plug-and-play%20methods%20rely%20on%20an%20implicit%0Aregularization%20provided%20by%20neural%20denoisers%2C%20alternative%20Bayesian%20approaches%0Aconsider%20Maximum%20A%20Posteriori%20%28MAP%29%20estimation%20in%20the%20latent%20space%20of%20a%0Agenerative%20model%2C%20thus%20with%20an%20explicit%20regularization.%20However%2C%0Astate-of-the-art%20deep%20generative%20models%20require%20a%20huge%20amount%20of%20training%20data%0Acompared%20to%20denoisers.%20Besides%2C%20their%20complexity%20hampers%20the%20optimization%0Ainvolved%20in%20latent%20MAP%20derivation.%20In%20this%20work%2C%20we%20first%20propose%20to%20use%0Acompressive%20autoencoders%20instead.%20These%20networks%2C%20which%20can%20be%20seen%20as%0Avariational%20autoencoders%20with%20a%20flexible%20latent%20prior%2C%20are%20smaller%20and%20easier%0Ato%20train%20than%20state-of-the-art%20generative%20models.%20As%20a%20second%20contribution%2C%20we%0Aintroduce%20the%20Variational%20Bayes%20Latent%20Estimation%20%28VBLE%29%20algorithm%2C%20which%0Aperforms%20latent%20estimation%20within%20the%20framework%20of%20variational%20inference.%0AThanks%20to%20a%20simple%20yet%20efficient%20parameterization%20of%20the%20variational%20posterior%2C%0AVBLE%20allows%20for%20fast%20and%20easy%20%28approximate%29%20posterior%20sampling.%20Experimental%0Aresults%20on%20image%20datasets%20BSD%20and%20FFHQ%20demonstrate%20that%20VBLE%20reaches%20similar%0Aperformance%20than%20state-of-the-art%20plug-and-play%20methods%2C%20while%20being%20able%20to%0Aquantify%20uncertainties%20faster%20than%20other%20existing%20posterior%20sampling%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17744v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Bayes%20image%20restoration%20with%20compressive%20autoencoders&entry.906535625=Maud%20Biquard%20and%20Marie%20Chabert%20and%20Thomas%20Oberlin&entry.1292438233=%20%20Regularization%20of%20inverse%20problems%20is%20of%20paramount%20importance%20in%0Acomputational%20imaging.%20The%20ability%20of%20neural%20networks%20to%20learn%20efficient%20image%0Arepresentations%20has%20been%20recently%20exploited%20to%20design%20powerful%20data-driven%0Aregularizers.%20While%20state-of-the-art%20plug-and-play%20methods%20rely%20on%20an%20implicit%0Aregularization%20provided%20by%20neural%20denoisers%2C%20alternative%20Bayesian%20approaches%0Aconsider%20Maximum%20A%20Posteriori%20%28MAP%29%20estimation%20in%20the%20latent%20space%20of%20a%0Agenerative%20model%2C%20thus%20with%20an%20explicit%20regularization.%20However%2C%0Astate-of-the-art%20deep%20generative%20models%20require%20a%20huge%20amount%20of%20training%20data%0Acompared%20to%20denoisers.%20Besides%2C%20their%20complexity%20hampers%20the%20optimization%0Ainvolved%20in%20latent%20MAP%20derivation.%20In%20this%20work%2C%20we%20first%20propose%20to%20use%0Acompressive%20autoencoders%20instead.%20These%20networks%2C%20which%20can%20be%20seen%20as%0Avariational%20autoencoders%20with%20a%20flexible%20latent%20prior%2C%20are%20smaller%20and%20easier%0Ato%20train%20than%20state-of-the-art%20generative%20models.%20As%20a%20second%20contribution%2C%20we%0Aintroduce%20the%20Variational%20Bayes%20Latent%20Estimation%20%28VBLE%29%20algorithm%2C%20which%0Aperforms%20latent%20estimation%20within%20the%20framework%20of%20variational%20inference.%0AThanks%20to%20a%20simple%20yet%20efficient%20parameterization%20of%20the%20variational%20posterior%2C%0AVBLE%20allows%20for%20fast%20and%20easy%20%28approximate%29%20posterior%20sampling.%20Experimental%0Aresults%20on%20image%20datasets%20BSD%20and%20FFHQ%20demonstrate%20that%20VBLE%20reaches%20similar%0Aperformance%20than%20state-of-the-art%20plug-and-play%20methods%2C%20while%20being%20able%20to%0Aquantify%20uncertainties%20faster%20than%20other%20existing%20posterior%20sampling%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17744v2&entry.124074799=Read"},
{"title": "Graph Augmentation for Recommendation", "author": "Qianru Zhang and Lianghao Xia and Xuheng Cai and Siuming Yiu and Chao Huang and Christian S. Jensen", "abstract": "  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n", "link": "http://arxiv.org/abs/2403.16656v1", "date": "2024-03-25", "relevancy": 2.5853, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5131}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4896}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Augmentation%20for%20Recommendation&body=Title%3A%20Graph%20Augmentation%20for%20Recommendation%0AAuthor%3A%20Qianru%20Zhang%20and%20Lianghao%20Xia%20and%20Xuheng%20Cai%20and%20Siuming%20Yiu%20and%20Chao%20Huang%20and%20Christian%20S.%20Jensen%0AAbstract%3A%20%20%20Graph%20augmentation%20with%20contrastive%20learning%20has%20gained%20significant%20attention%0Ain%20the%20field%20of%20recommendation%20systems%20due%20to%20its%20ability%20to%20learn%20expressive%0Auser%20representations%2C%20even%20when%20labeled%20data%20is%20limited.%20However%2C%20directly%0Aapplying%20existing%20GCL%20models%20to%20real-world%20recommendation%20environments%20poses%0Achallenges.%20There%20are%20two%20primary%20issues%20to%20address.%20Firstly%2C%20the%20lack%20of%0Aconsideration%20for%20data%20noise%20in%20contrastive%20learning%20can%20result%20in%20noisy%0Aself-supervised%20signals%2C%20leading%20to%20degraded%20performance.%20Secondly%2C%20many%0Aexisting%20GCL%20approaches%20rely%20on%20graph%20neural%20network%20%28GNN%29%20architectures%2C%20which%0Acan%20suffer%20from%20over-smoothing%20problems%20due%20to%20non-adaptive%20message%20passing.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20principled%20framework%20called%20GraphAug.%0AThis%20framework%20introduces%20a%20robust%20data%20augmentor%20that%20generates%20denoised%0Aself-supervised%20signals%2C%20enhancing%20recommender%20systems.%20The%20GraphAug%20framework%0Aincorporates%20a%20graph%20information%20bottleneck%20%28GIB%29-regularized%20augmentation%0Aparadigm%2C%20which%20automatically%20distills%20informative%20self-supervision%20information%0Aand%20adaptively%20adjusts%20contrastive%20view%20generation.%20Through%20rigorous%0Aexperimentation%20on%20real-world%20datasets%2C%20we%20thoroughly%20assessed%20the%20performance%0Aof%20our%20novel%20GraphAug%20model.%20The%20outcomes%20consistently%20unveil%20its%20superiority%0Aover%20existing%20baseline%20methods.%20The%20source%20code%20for%20our%20model%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/HKUDS/GraphAug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Augmentation%20for%20Recommendation&entry.906535625=Qianru%20Zhang%20and%20Lianghao%20Xia%20and%20Xuheng%20Cai%20and%20Siuming%20Yiu%20and%20Chao%20Huang%20and%20Christian%20S.%20Jensen&entry.1292438233=%20%20Graph%20augmentation%20with%20contrastive%20learning%20has%20gained%20significant%20attention%0Ain%20the%20field%20of%20recommendation%20systems%20due%20to%20its%20ability%20to%20learn%20expressive%0Auser%20representations%2C%20even%20when%20labeled%20data%20is%20limited.%20However%2C%20directly%0Aapplying%20existing%20GCL%20models%20to%20real-world%20recommendation%20environments%20poses%0Achallenges.%20There%20are%20two%20primary%20issues%20to%20address.%20Firstly%2C%20the%20lack%20of%0Aconsideration%20for%20data%20noise%20in%20contrastive%20learning%20can%20result%20in%20noisy%0Aself-supervised%20signals%2C%20leading%20to%20degraded%20performance.%20Secondly%2C%20many%0Aexisting%20GCL%20approaches%20rely%20on%20graph%20neural%20network%20%28GNN%29%20architectures%2C%20which%0Acan%20suffer%20from%20over-smoothing%20problems%20due%20to%20non-adaptive%20message%20passing.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20principled%20framework%20called%20GraphAug.%0AThis%20framework%20introduces%20a%20robust%20data%20augmentor%20that%20generates%20denoised%0Aself-supervised%20signals%2C%20enhancing%20recommender%20systems.%20The%20GraphAug%20framework%0Aincorporates%20a%20graph%20information%20bottleneck%20%28GIB%29-regularized%20augmentation%0Aparadigm%2C%20which%20automatically%20distills%20informative%20self-supervision%20information%0Aand%20adaptively%20adjusts%20contrastive%20view%20generation.%20Through%20rigorous%0Aexperimentation%20on%20real-world%20datasets%2C%20we%20thoroughly%20assessed%20the%20performance%0Aof%20our%20novel%20GraphAug%20model.%20The%20outcomes%20consistently%20unveil%20its%20superiority%0Aover%20existing%20baseline%20methods.%20The%20source%20code%20for%20our%20model%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/HKUDS/GraphAug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16656v1&entry.124074799=Read"},
{"title": "HiFi-123: Towards High-fidelity One Image to 3D Content Generation", "author": "Wangbo Yu and Li Yuan and Yan-Pei Cao and Xiangjun Gao and Xiaoyu Li and Wenbo Hu and Long Quan and Ying Shan and Yonghong Tian", "abstract": "  Recent advances in diffusion models have enabled 3D generation from a single\nimage. However, current methods often produce suboptimal results for novel\nviews, with blurred textures and deviations from the reference image, limiting\ntheir practical applications. In this paper, we introduce HiFi-123, a method\ndesigned for high-fidelity and multi-view consistent 3D generation. Our\ncontributions are twofold: First, we propose a Reference-Guided Novel View\nEnhancement (RGNV) technique that significantly improves the fidelity of\ndiffusion-based zero-shot novel view synthesis methods. Second, capitalizing on\nthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.\nWhen incorporated into the optimization-based image-to-3D pipeline, our method\nsignificantly improves 3D generation quality, achieving state-of-the-art\nperformance. Comprehensive evaluations demonstrate the effectiveness of our\napproach over existing methods, both qualitatively and quantitatively. Video\nresults are available on the project page.\n", "link": "http://arxiv.org/abs/2310.06744v2", "date": "2024-03-25", "relevancy": 2.5482, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6891}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6244}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.59}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HiFi-123%3A%20Towards%20High-fidelity%20One%20Image%20to%203D%20Content%20Generation&body=Title%3A%20HiFi-123%3A%20Towards%20High-fidelity%20One%20Image%20to%203D%20Content%20Generation%0AAuthor%3A%20Wangbo%20Yu%20and%20Li%20Yuan%20and%20Yan-Pei%20Cao%20and%20Xiangjun%20Gao%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Long%20Quan%20and%20Ying%20Shan%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20enabled%203D%20generation%20from%20a%20single%0Aimage.%20However%2C%20current%20methods%20often%20produce%20suboptimal%20results%20for%20novel%0Aviews%2C%20with%20blurred%20textures%20and%20deviations%20from%20the%20reference%20image%2C%20limiting%0Atheir%20practical%20applications.%20In%20this%20paper%2C%20we%20introduce%20HiFi-123%2C%20a%20method%0Adesigned%20for%20high-fidelity%20and%20multi-view%20consistent%203D%20generation.%20Our%0Acontributions%20are%20twofold%3A%20First%2C%20we%20propose%20a%20Reference-Guided%20Novel%20View%0AEnhancement%20%28RGNV%29%20technique%20that%20significantly%20improves%20the%20fidelity%20of%0Adiffusion-based%20zero-shot%20novel%20view%20synthesis%20methods.%20Second%2C%20capitalizing%20on%0Athe%20RGNV%2C%20we%20present%20a%20novel%20Reference-Guided%20State%20Distillation%20%28RGSD%29%20loss.%0AWhen%20incorporated%20into%20the%20optimization-based%20image-to-3D%20pipeline%2C%20our%20method%0Asignificantly%20improves%203D%20generation%20quality%2C%20achieving%20state-of-the-art%0Aperformance.%20Comprehensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20over%20existing%20methods%2C%20both%20qualitatively%20and%20quantitatively.%20Video%0Aresults%20are%20available%20on%20the%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06744v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-123%3A%20Towards%20High-fidelity%20One%20Image%20to%203D%20Content%20Generation&entry.906535625=Wangbo%20Yu%20and%20Li%20Yuan%20and%20Yan-Pei%20Cao%20and%20Xiangjun%20Gao%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Long%20Quan%20and%20Ying%20Shan%20and%20Yonghong%20Tian&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20enabled%203D%20generation%20from%20a%20single%0Aimage.%20However%2C%20current%20methods%20often%20produce%20suboptimal%20results%20for%20novel%0Aviews%2C%20with%20blurred%20textures%20and%20deviations%20from%20the%20reference%20image%2C%20limiting%0Atheir%20practical%20applications.%20In%20this%20paper%2C%20we%20introduce%20HiFi-123%2C%20a%20method%0Adesigned%20for%20high-fidelity%20and%20multi-view%20consistent%203D%20generation.%20Our%0Acontributions%20are%20twofold%3A%20First%2C%20we%20propose%20a%20Reference-Guided%20Novel%20View%0AEnhancement%20%28RGNV%29%20technique%20that%20significantly%20improves%20the%20fidelity%20of%0Adiffusion-based%20zero-shot%20novel%20view%20synthesis%20methods.%20Second%2C%20capitalizing%20on%0Athe%20RGNV%2C%20we%20present%20a%20novel%20Reference-Guided%20State%20Distillation%20%28RGSD%29%20loss.%0AWhen%20incorporated%20into%20the%20optimization-based%20image-to-3D%20pipeline%2C%20our%20method%0Asignificantly%20improves%203D%20generation%20quality%2C%20achieving%20state-of-the-art%0Aperformance.%20Comprehensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20over%20existing%20methods%2C%20both%20qualitatively%20and%20quantitatively.%20Video%0Aresults%20are%20available%20on%20the%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06744v2&entry.124074799=Read"},
{"title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor", "author": "Yi Lu and Xin Zhou and Wei He and Jun Zhao and Tao Ji and Tao Gui and Qi Zhang and Xuanjing Huang", "abstract": "  Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .\n", "link": "http://arxiv.org/abs/2402.10685v2", "date": "2024-03-25", "relevancy": 2.5478, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5105}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LongHeads%3A%20Multi-Head%20Attention%20is%20Secretly%20a%20Long%20Context%20Processor&body=Title%3A%20LongHeads%3A%20Multi-Head%20Attention%20is%20Secretly%20a%20Long%20Context%20Processor%0AAuthor%3A%20Yi%20Lu%20and%20Xin%20Zhou%20and%20Wei%20He%20and%20Jun%20Zhao%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20numerous%0Adomains%20but%20often%20struggle%20to%20process%20lengthy%20inputs%20effectively%20and%0Aefficiently%20due%20to%20limited%20length%20generalization%20and%20attention%27s%20quadratic%0Acomputational%20demands.%20Many%20sought%20to%20mitigate%20this%20by%20restricting%20the%0Aattention%20window%20within%20the%20pre-trained%20length.%20However%2C%20these%20methods%0Aintroduce%20new%20issues%20such%20as%20ignoring%20the%20middle%20context%20and%20requiring%0Aadditional%20training.%20To%20address%20these%20problems%2C%20we%20propose%20LongHeads%2C%20a%0Atraining-free%20framework%20that%20enhances%20LLM%27s%20long%20context%20ability%20by%20unlocking%0Amulti-head%20attention%27s%20untapped%20potential.%20Instead%20of%20allowing%20each%20head%20to%0Aattend%20to%20the%20full%20sentence%2C%20which%20struggles%20with%20generalizing%20to%20longer%0Asequences%20due%20to%20out-of-distribution%20%28OOD%29%20issues%2C%20we%20allow%20each%20head%20to%0Aprocess%20in-distribution%20length%20by%20selecting%20and%20attending%20to%20important%20context%0Achunks.%20To%20this%20end%2C%20we%20propose%20a%20chunk%20selection%20strategy%20that%20relies%20on%20the%0Ainherent%20correlation%20between%20the%20query%20and%20the%20key%20representations%2C%20efficiently%0Adistributing%20context%20chunks%20to%20different%20heads.%20In%20this%20way%2C%20each%20head%20ensures%0Ait%20can%20effectively%20process%20attended%20tokens%20within%20the%20trained%20length%2C%20while%0Adifferent%20heads%20in%20different%20layers%20can%20collectively%20process%20longer%20contexts.%0ALongHeads%20works%20efficiently%20in%20linear%20time%2C%20fits%20seamlessly%20with%20many%20LLMs%20that%0Ause%20relative%20positional%20encoding.%20LongHeads%20achieves%20100%25%20accuracy%20at%20the%20128k%0Alength%20on%20passkey%20retrieval%20task%2C%20verifying%20LongHeads%27s%20efficacy%20in%20extending%0Athe%20usable%20context%20window%20for%20existing%20models.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/LuLuLuyi/LongHeads%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10685v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongHeads%3A%20Multi-Head%20Attention%20is%20Secretly%20a%20Long%20Context%20Processor&entry.906535625=Yi%20Lu%20and%20Xin%20Zhou%20and%20Wei%20He%20and%20Jun%20Zhao%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20in%20numerous%0Adomains%20but%20often%20struggle%20to%20process%20lengthy%20inputs%20effectively%20and%0Aefficiently%20due%20to%20limited%20length%20generalization%20and%20attention%27s%20quadratic%0Acomputational%20demands.%20Many%20sought%20to%20mitigate%20this%20by%20restricting%20the%0Aattention%20window%20within%20the%20pre-trained%20length.%20However%2C%20these%20methods%0Aintroduce%20new%20issues%20such%20as%20ignoring%20the%20middle%20context%20and%20requiring%0Aadditional%20training.%20To%20address%20these%20problems%2C%20we%20propose%20LongHeads%2C%20a%0Atraining-free%20framework%20that%20enhances%20LLM%27s%20long%20context%20ability%20by%20unlocking%0Amulti-head%20attention%27s%20untapped%20potential.%20Instead%20of%20allowing%20each%20head%20to%0Aattend%20to%20the%20full%20sentence%2C%20which%20struggles%20with%20generalizing%20to%20longer%0Asequences%20due%20to%20out-of-distribution%20%28OOD%29%20issues%2C%20we%20allow%20each%20head%20to%0Aprocess%20in-distribution%20length%20by%20selecting%20and%20attending%20to%20important%20context%0Achunks.%20To%20this%20end%2C%20we%20propose%20a%20chunk%20selection%20strategy%20that%20relies%20on%20the%0Ainherent%20correlation%20between%20the%20query%20and%20the%20key%20representations%2C%20efficiently%0Adistributing%20context%20chunks%20to%20different%20heads.%20In%20this%20way%2C%20each%20head%20ensures%0Ait%20can%20effectively%20process%20attended%20tokens%20within%20the%20trained%20length%2C%20while%0Adifferent%20heads%20in%20different%20layers%20can%20collectively%20process%20longer%20contexts.%0ALongHeads%20works%20efficiently%20in%20linear%20time%2C%20fits%20seamlessly%20with%20many%20LLMs%20that%0Ause%20relative%20positional%20encoding.%20LongHeads%20achieves%20100%25%20accuracy%20at%20the%20128k%0Alength%20on%20passkey%20retrieval%20task%2C%20verifying%20LongHeads%27s%20efficacy%20in%20extending%0Athe%20usable%20context%20window%20for%20existing%20models.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/LuLuLuyi/LongHeads%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10685v2&entry.124074799=Read"},
{"title": "DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization", "author": "Yunlong Tang and Yuxuan Wan and Lei Qi and Xin Geng", "abstract": "  Source-Free Domain Generalization (SFDG) aims to develop a model that works\nfor unseen target domains without relying on any source domain. Recent work,\nPromptStyler, employs text prompts to simulate different distribution shifts in\nthe joint vision-language space, allowing the model to generalize effectively\nto unseen domains without using any images. However, 1) PromptStyler's style\ngeneration strategy has limitations, as all style patterns are fixed after the\nfirst training phase. This leads to the training set in the second training\nphase being restricted to a limited set of styles. Additionally, 2) the frozen\ntext encoder in PromptStyler result in the encoder's output varying with the\nstyle of the input text prompts, making it difficult for the model to learn\ndomain-invariant features. In this paper, we introduce Dynamic PromptStyler\n(DPStyler), comprising Style Generation and Style Removal modules to address\nthese issues. The Style Generation module refreshes all styles at every\ntraining epoch, while the Style Removal module eliminates variations in the\nencoder's output features caused by input styles. Moreover, since the Style\nGeneration module, responsible for generating style word vectors using random\nsampling or style mixing, makes the model sensitive to input text prompts, we\nintroduce a model ensemble method to mitigate this sensitivity. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art methods\non benchmark datasets.\n", "link": "http://arxiv.org/abs/2403.16697v1", "date": "2024-03-25", "relevancy": 2.5013, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5007}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4962}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DPStyler%3A%20Dynamic%20PromptStyler%20for%20Source-Free%20Domain%20Generalization&body=Title%3A%20DPStyler%3A%20Dynamic%20PromptStyler%20for%20Source-Free%20Domain%20Generalization%0AAuthor%3A%20Yunlong%20Tang%20and%20Yuxuan%20Wan%20and%20Lei%20Qi%20and%20Xin%20Geng%0AAbstract%3A%20%20%20Source-Free%20Domain%20Generalization%20%28SFDG%29%20aims%20to%20develop%20a%20model%20that%20works%0Afor%20unseen%20target%20domains%20without%20relying%20on%20any%20source%20domain.%20Recent%20work%2C%0APromptStyler%2C%20employs%20text%20prompts%20to%20simulate%20different%20distribution%20shifts%20in%0Athe%20joint%20vision-language%20space%2C%20allowing%20the%20model%20to%20generalize%20effectively%0Ato%20unseen%20domains%20without%20using%20any%20images.%20However%2C%201%29%20PromptStyler%27s%20style%0Ageneration%20strategy%20has%20limitations%2C%20as%20all%20style%20patterns%20are%20fixed%20after%20the%0Afirst%20training%20phase.%20This%20leads%20to%20the%20training%20set%20in%20the%20second%20training%0Aphase%20being%20restricted%20to%20a%20limited%20set%20of%20styles.%20Additionally%2C%202%29%20the%20frozen%0Atext%20encoder%20in%20PromptStyler%20result%20in%20the%20encoder%27s%20output%20varying%20with%20the%0Astyle%20of%20the%20input%20text%20prompts%2C%20making%20it%20difficult%20for%20the%20model%20to%20learn%0Adomain-invariant%20features.%20In%20this%20paper%2C%20we%20introduce%20Dynamic%20PromptStyler%0A%28DPStyler%29%2C%20comprising%20Style%20Generation%20and%20Style%20Removal%20modules%20to%20address%0Athese%20issues.%20The%20Style%20Generation%20module%20refreshes%20all%20styles%20at%20every%0Atraining%20epoch%2C%20while%20the%20Style%20Removal%20module%20eliminates%20variations%20in%20the%0Aencoder%27s%20output%20features%20caused%20by%20input%20styles.%20Moreover%2C%20since%20the%20Style%0AGeneration%20module%2C%20responsible%20for%20generating%20style%20word%20vectors%20using%20random%0Asampling%20or%20style%20mixing%2C%20makes%20the%20model%20sensitive%20to%20input%20text%20prompts%2C%20we%0Aintroduce%20a%20model%20ensemble%20method%20to%20mitigate%20this%20sensitivity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20methods%0Aon%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16697v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPStyler%3A%20Dynamic%20PromptStyler%20for%20Source-Free%20Domain%20Generalization&entry.906535625=Yunlong%20Tang%20and%20Yuxuan%20Wan%20and%20Lei%20Qi%20and%20Xin%20Geng&entry.1292438233=%20%20Source-Free%20Domain%20Generalization%20%28SFDG%29%20aims%20to%20develop%20a%20model%20that%20works%0Afor%20unseen%20target%20domains%20without%20relying%20on%20any%20source%20domain.%20Recent%20work%2C%0APromptStyler%2C%20employs%20text%20prompts%20to%20simulate%20different%20distribution%20shifts%20in%0Athe%20joint%20vision-language%20space%2C%20allowing%20the%20model%20to%20generalize%20effectively%0Ato%20unseen%20domains%20without%20using%20any%20images.%20However%2C%201%29%20PromptStyler%27s%20style%0Ageneration%20strategy%20has%20limitations%2C%20as%20all%20style%20patterns%20are%20fixed%20after%20the%0Afirst%20training%20phase.%20This%20leads%20to%20the%20training%20set%20in%20the%20second%20training%0Aphase%20being%20restricted%20to%20a%20limited%20set%20of%20styles.%20Additionally%2C%202%29%20the%20frozen%0Atext%20encoder%20in%20PromptStyler%20result%20in%20the%20encoder%27s%20output%20varying%20with%20the%0Astyle%20of%20the%20input%20text%20prompts%2C%20making%20it%20difficult%20for%20the%20model%20to%20learn%0Adomain-invariant%20features.%20In%20this%20paper%2C%20we%20introduce%20Dynamic%20PromptStyler%0A%28DPStyler%29%2C%20comprising%20Style%20Generation%20and%20Style%20Removal%20modules%20to%20address%0Athese%20issues.%20The%20Style%20Generation%20module%20refreshes%20all%20styles%20at%20every%0Atraining%20epoch%2C%20while%20the%20Style%20Removal%20module%20eliminates%20variations%20in%20the%0Aencoder%27s%20output%20features%20caused%20by%20input%20styles.%20Moreover%2C%20since%20the%20Style%0AGeneration%20module%2C%20responsible%20for%20generating%20style%20word%20vectors%20using%20random%0Asampling%20or%20style%20mixing%2C%20makes%20the%20model%20sensitive%20to%20input%20text%20prompts%2C%20we%0Aintroduce%20a%20model%20ensemble%20method%20to%20mitigate%20this%20sensitivity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20methods%0Aon%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16697v1&entry.124074799=Read"},
{"title": "Word4Per: Zero-shot Composed Person Retrieval", "author": "Delong Liu and Haiwen Li and Zhicheng Zhao and Fei Su and Yuan Dong", "abstract": "  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n", "link": "http://arxiv.org/abs/2311.16515v2", "date": "2024-03-25", "relevancy": 2.4639, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4826}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval&body=Title%3A%20Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval%0AAuthor%3A%20Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong%0AAbstract%3A%20%20%20Searching%20for%20specific%20person%20has%20great%20social%20benefits%20and%20security%20value%2C%0Aand%20it%20often%20involves%20a%20combination%20of%20visual%20and%20textual%20information.%0AConventional%20person%20retrieval%20methods%2C%20whether%20image-based%20or%20text-based%2C%0Ausually%20fall%20short%20in%20effectively%20harnessing%20both%20types%20of%20information%2C%20leading%0Ato%20the%20loss%20of%20accuracy.%20In%20this%20paper%2C%20a%20whole%20new%20task%20called%20Composed%20Person%0ARetrieval%20%28CPR%29%20is%20proposed%20to%20jointly%20utilize%20both%20image%20and%20text%20information%0Afor%20target%20person%20retrieval.%20However%2C%20the%20supervised%20CPR%20requires%20very%20costly%0Amanual%20annotation%20dataset%2C%20while%20there%20are%20currently%20no%20available%20resources.%20To%0Amitigate%20this%20issue%2C%20we%20firstly%20introduce%20the%20Zero-shot%20Composed%20Person%0ARetrieval%20%28ZS-CPR%29%2C%20which%20leverages%20existing%20domain-related%20data%20to%20resolve%20the%0ACPR%20problem%20without%20expensive%20annotations.%20Secondly%2C%20to%20learn%20ZS-CPR%20model%2C%20we%0Apropose%20a%20two-stage%20learning%20framework%2C%20Word4Per%2C%20where%20a%20lightweight%20Textual%0AInversion%20Network%20%28TINet%29%20and%20a%20text-based%20person%20retrieval%20model%20based%20on%0Afine-tuned%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20network%20are%20learned%0Awithout%20utilizing%20any%20CPR%20data.%20Thirdly%2C%20a%20finely%20annotated%20Image-Text%20Composed%0APerson%20Retrieval%20%28ITCPR%29%20dataset%20is%20built%20as%20the%20benchmark%20to%20assess%20the%0Aperformance%20of%20the%20proposed%20Word4Per%20framework.%20Extensive%20experiments%20under%0Aboth%20Rank-1%20and%20mAP%20demonstrate%20the%20effectiveness%20of%20Word4Per%20for%20the%20ZS-CPR%0Atask%2C%20surpassing%20the%20comparative%20methods%20by%20over%2010%5C%25.%20The%20code%20and%20ITCPR%0Adataset%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Word4Per.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16515v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval&entry.906535625=Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong&entry.1292438233=%20%20Searching%20for%20specific%20person%20has%20great%20social%20benefits%20and%20security%20value%2C%0Aand%20it%20often%20involves%20a%20combination%20of%20visual%20and%20textual%20information.%0AConventional%20person%20retrieval%20methods%2C%20whether%20image-based%20or%20text-based%2C%0Ausually%20fall%20short%20in%20effectively%20harnessing%20both%20types%20of%20information%2C%20leading%0Ato%20the%20loss%20of%20accuracy.%20In%20this%20paper%2C%20a%20whole%20new%20task%20called%20Composed%20Person%0ARetrieval%20%28CPR%29%20is%20proposed%20to%20jointly%20utilize%20both%20image%20and%20text%20information%0Afor%20target%20person%20retrieval.%20However%2C%20the%20supervised%20CPR%20requires%20very%20costly%0Amanual%20annotation%20dataset%2C%20while%20there%20are%20currently%20no%20available%20resources.%20To%0Amitigate%20this%20issue%2C%20we%20firstly%20introduce%20the%20Zero-shot%20Composed%20Person%0ARetrieval%20%28ZS-CPR%29%2C%20which%20leverages%20existing%20domain-related%20data%20to%20resolve%20the%0ACPR%20problem%20without%20expensive%20annotations.%20Secondly%2C%20to%20learn%20ZS-CPR%20model%2C%20we%0Apropose%20a%20two-stage%20learning%20framework%2C%20Word4Per%2C%20where%20a%20lightweight%20Textual%0AInversion%20Network%20%28TINet%29%20and%20a%20text-based%20person%20retrieval%20model%20based%20on%0Afine-tuned%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20network%20are%20learned%0Awithout%20utilizing%20any%20CPR%20data.%20Thirdly%2C%20a%20finely%20annotated%20Image-Text%20Composed%0APerson%20Retrieval%20%28ITCPR%29%20dataset%20is%20built%20as%20the%20benchmark%20to%20assess%20the%0Aperformance%20of%20the%20proposed%20Word4Per%20framework.%20Extensive%20experiments%20under%0Aboth%20Rank-1%20and%20mAP%20demonstrate%20the%20effectiveness%20of%20Word4Per%20for%20the%20ZS-CPR%0Atask%2C%20surpassing%20the%20comparative%20methods%20by%20over%2010%5C%25.%20The%20code%20and%20ITCPR%0Adataset%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Word4Per.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16515v2&entry.124074799=Read"},
{"title": "FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression", "author": "Alireza Furutanpey and Qiyang Zhang and Philipp Raith and Tobias Pfandzelter and Shangguang Wang and Schahram Dustdar", "abstract": "  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n", "link": "http://arxiv.org/abs/2403.16677v1", "date": "2024-03-25", "relevancy": 2.3861, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4985}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4838}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4494}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&body=Title%3A%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression%0AAuthor%3A%20Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar%0AAbstract%3A%20%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%0A%20%20This%20work%20presents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%0Amethod%20that%20preserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%0Asatellite%20imagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%0Aleverages%20inter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%0Aoverhead.%20While%20FOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%0Acompetitive%20scores%20on%20perceptual%20quality%20measures%20at%20lower%20bitrates.%20We%0Aextensively%20evaluate%20transfer%20cost%20reduction%20by%20including%20the%20peculiarity%20of%0Aintermittently%20available%20network%20connections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%0Atest%20the%20feasibility%20of%20our%20system%20for%20standardized%20nanosatellite%20form%20factors.%0AWe%20demonstrate%20that%20FOOL%20permits%20downlinking%20over%20100x%20the%20data%20volume%20without%0Arelying%20on%20prior%20information%20on%20the%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16677v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&entry.906535625=Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar&entry.1292438233=%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%0A%20%20This%20work%20presents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%0Amethod%20that%20preserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%0Asatellite%20imagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%0Aleverages%20inter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%0Aoverhead.%20While%20FOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%0Acompetitive%20scores%20on%20perceptual%20quality%20measures%20at%20lower%20bitrates.%20We%0Aextensively%20evaluate%20transfer%20cost%20reduction%20by%20including%20the%20peculiarity%20of%0Aintermittently%20available%20network%20connections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%0Atest%20the%20feasibility%20of%20our%20system%20for%20standardized%20nanosatellite%20form%20factors.%0AWe%20demonstrate%20that%20FOOL%20permits%20downlinking%20over%20100x%20the%20data%20volume%20without%0Arelying%20on%20prior%20information%20on%20the%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16677v1&entry.124074799=Read"},
{"title": "Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer", "author": "Dominik M\u00fcller and Philip Meyer and Lukas Rentschler and Robin Manz and Daniel Hieber and Jonas B\u00e4cker and Samantha Cramer and Christoph Wengenmayr and Bruno M\u00e4rkl and Ralf Huss and Frank Kramer and I\u00f1aki Soto-Rey and Johannes Raffler", "abstract": "  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n", "link": "http://arxiv.org/abs/2403.16695v1", "date": "2024-03-25", "relevancy": 2.3284, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4812}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4646}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Performance%20of%20Deep%20Learning%20for%20Automated%20Gleason%20Grading%0A%20%20in%20Prostate%20Cancer&body=Title%3A%20Assessing%20the%20Performance%20of%20Deep%20Learning%20for%20Automated%20Gleason%20Grading%0A%20%20in%20Prostate%20Cancer%0AAuthor%3A%20Dominik%20M%C3%BCller%20and%20Philip%20Meyer%20and%20Lukas%20Rentschler%20and%20Robin%20Manz%20and%20Daniel%20Hieber%20and%20Jonas%20B%C3%A4cker%20and%20Samantha%20Cramer%20and%20Christoph%20Wengenmayr%20and%20Bruno%20M%C3%A4rkl%20and%20Ralf%20Huss%20and%20Frank%20Kramer%20and%20I%C3%B1aki%20Soto-Rey%20and%20Johannes%20Raffler%0AAbstract%3A%20%20%20Prostate%20cancer%20is%20a%20dominant%20health%20concern%20calling%20for%20advanced%20diagnostic%0Atools.%20Utilizing%20digital%20pathology%20and%20artificial%20intelligence%2C%20this%20study%0Aexplores%20the%20potential%20of%2011%20deep%20neural%20network%20architectures%20for%20automated%0AGleason%20grading%20in%20prostate%20carcinoma%20focusing%20on%20comparing%20traditional%20and%0Arecent%20architectures.%20A%20standardized%20image%20classification%20pipeline%2C%20based%20on%0Athe%20AUCMEDI%20framework%2C%20facilitated%20robust%20evaluation%20using%20an%20in-house%20dataset%0Aconsisting%20of%2034%2C264%20annotated%20tissue%20tiles.%20The%20results%20indicated%20varying%0Asensitivity%20across%20architectures%2C%20with%20ConvNeXt%20demonstrating%20the%20strongest%0Aperformance.%20Notably%2C%20newer%20architectures%20achieved%20superior%20performance%2C%20even%0Athough%20with%20challenges%20in%20differentiating%20closely%20related%20Gleason%20grades.%20The%0AConvNeXt%20model%20was%20capable%20of%20learning%20a%20balance%20between%20complexity%20and%0Ageneralizability.%20Overall%2C%20this%20study%20lays%20the%20groundwork%20for%20enhanced%20Gleason%0Agrading%20systems%2C%20potentially%20improving%20diagnostic%20efficiency%20for%20prostate%0Acancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Performance%20of%20Deep%20Learning%20for%20Automated%20Gleason%20Grading%0A%20%20in%20Prostate%20Cancer&entry.906535625=Dominik%20M%C3%BCller%20and%20Philip%20Meyer%20and%20Lukas%20Rentschler%20and%20Robin%20Manz%20and%20Daniel%20Hieber%20and%20Jonas%20B%C3%A4cker%20and%20Samantha%20Cramer%20and%20Christoph%20Wengenmayr%20and%20Bruno%20M%C3%A4rkl%20and%20Ralf%20Huss%20and%20Frank%20Kramer%20and%20I%C3%B1aki%20Soto-Rey%20and%20Johannes%20Raffler&entry.1292438233=%20%20Prostate%20cancer%20is%20a%20dominant%20health%20concern%20calling%20for%20advanced%20diagnostic%0Atools.%20Utilizing%20digital%20pathology%20and%20artificial%20intelligence%2C%20this%20study%0Aexplores%20the%20potential%20of%2011%20deep%20neural%20network%20architectures%20for%20automated%0AGleason%20grading%20in%20prostate%20carcinoma%20focusing%20on%20comparing%20traditional%20and%0Arecent%20architectures.%20A%20standardized%20image%20classification%20pipeline%2C%20based%20on%0Athe%20AUCMEDI%20framework%2C%20facilitated%20robust%20evaluation%20using%20an%20in-house%20dataset%0Aconsisting%20of%2034%2C264%20annotated%20tissue%20tiles.%20The%20results%20indicated%20varying%0Asensitivity%20across%20architectures%2C%20with%20ConvNeXt%20demonstrating%20the%20strongest%0Aperformance.%20Notably%2C%20newer%20architectures%20achieved%20superior%20performance%2C%20even%0Athough%20with%20challenges%20in%20differentiating%20closely%20related%20Gleason%20grades.%20The%0AConvNeXt%20model%20was%20capable%20of%20learning%20a%20balance%20between%20complexity%20and%0Ageneralizability.%20Overall%2C%20this%20study%20lays%20the%20groundwork%20for%20enhanced%20Gleason%0Agrading%20systems%2C%20potentially%20improving%20diagnostic%20efficiency%20for%20prostate%0Acancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16695v1&entry.124074799=Read"},
{"title": "SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for\n  Aerial Semantic Segmentation", "author": "Aysim Toker and Marvin Eisenberger and Daniel Cremers and Laura Leal-Taix\u00e9", "abstract": "  In recent years, semantic segmentation has become a pivotal tool in\nprocessing and interpreting satellite imagery. Yet, a prevalent limitation of\nsupervised learning techniques remains the need for extensive manual\nannotations by experts. In this work, we explore the potential of generative\nimage diffusion to address the scarcity of annotated data in earth observation\ntasks. The main idea is to learn the joint data manifold of images and labels,\nleveraging recent advancements in denoising diffusion probabilistic models. To\nthe best of our knowledge, we are the first to generate both images and\ncorresponding masks for satellite segmentation. We find that the obtained pairs\nnot only display high quality in fine-scale features but also ensure a wide\nsampling diversity. Both aspects are crucial for earth observation data, where\nsemantic classes can vary severely in scale and occurrence frequency. We employ\nthe novel data instances for downstream segmentation, as a form of data\naugmentation. In our experiments, we provide comparisons to prior works based\non discriminative diffusion models or GANs. We demonstrate that integrating\ngenerated samples yields significant quantitative improvements for satellite\nsemantic segmentation -- both compared to baselines and when training only on\nthe original data.\n", "link": "http://arxiv.org/abs/2403.16605v1", "date": "2024-03-25", "relevancy": 2.315, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5995}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5829}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5563}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SatSynth%3A%20Augmenting%20Image-Mask%20Pairs%20through%20Diffusion%20Models%20for%0A%20%20Aerial%20Semantic%20Segmentation&body=Title%3A%20SatSynth%3A%20Augmenting%20Image-Mask%20Pairs%20through%20Diffusion%20Models%20for%0A%20%20Aerial%20Semantic%20Segmentation%0AAuthor%3A%20Aysim%20Toker%20and%20Marvin%20Eisenberger%20and%20Daniel%20Cremers%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20In%20recent%20years%2C%20semantic%20segmentation%20has%20become%20a%20pivotal%20tool%20in%0Aprocessing%20and%20interpreting%20satellite%20imagery.%20Yet%2C%20a%20prevalent%20limitation%20of%0Asupervised%20learning%20techniques%20remains%20the%20need%20for%20extensive%20manual%0Aannotations%20by%20experts.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20generative%0Aimage%20diffusion%20to%20address%20the%20scarcity%20of%20annotated%20data%20in%20earth%20observation%0Atasks.%20The%20main%20idea%20is%20to%20learn%20the%20joint%20data%20manifold%20of%20images%20and%20labels%2C%0Aleveraging%20recent%20advancements%20in%20denoising%20diffusion%20probabilistic%20models.%20To%0Athe%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20generate%20both%20images%20and%0Acorresponding%20masks%20for%20satellite%20segmentation.%20We%20find%20that%20the%20obtained%20pairs%0Anot%20only%20display%20high%20quality%20in%20fine-scale%20features%20but%20also%20ensure%20a%20wide%0Asampling%20diversity.%20Both%20aspects%20are%20crucial%20for%20earth%20observation%20data%2C%20where%0Asemantic%20classes%20can%20vary%20severely%20in%20scale%20and%20occurrence%20frequency.%20We%20employ%0Athe%20novel%20data%20instances%20for%20downstream%20segmentation%2C%20as%20a%20form%20of%20data%0Aaugmentation.%20In%20our%20experiments%2C%20we%20provide%20comparisons%20to%20prior%20works%20based%0Aon%20discriminative%20diffusion%20models%20or%20GANs.%20We%20demonstrate%20that%20integrating%0Agenerated%20samples%20yields%20significant%20quantitative%20improvements%20for%20satellite%0Asemantic%20segmentation%20--%20both%20compared%20to%20baselines%20and%20when%20training%20only%20on%0Athe%20original%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SatSynth%3A%20Augmenting%20Image-Mask%20Pairs%20through%20Diffusion%20Models%20for%0A%20%20Aerial%20Semantic%20Segmentation&entry.906535625=Aysim%20Toker%20and%20Marvin%20Eisenberger%20and%20Daniel%20Cremers%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20In%20recent%20years%2C%20semantic%20segmentation%20has%20become%20a%20pivotal%20tool%20in%0Aprocessing%20and%20interpreting%20satellite%20imagery.%20Yet%2C%20a%20prevalent%20limitation%20of%0Asupervised%20learning%20techniques%20remains%20the%20need%20for%20extensive%20manual%0Aannotations%20by%20experts.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20generative%0Aimage%20diffusion%20to%20address%20the%20scarcity%20of%20annotated%20data%20in%20earth%20observation%0Atasks.%20The%20main%20idea%20is%20to%20learn%20the%20joint%20data%20manifold%20of%20images%20and%20labels%2C%0Aleveraging%20recent%20advancements%20in%20denoising%20diffusion%20probabilistic%20models.%20To%0Athe%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20generate%20both%20images%20and%0Acorresponding%20masks%20for%20satellite%20segmentation.%20We%20find%20that%20the%20obtained%20pairs%0Anot%20only%20display%20high%20quality%20in%20fine-scale%20features%20but%20also%20ensure%20a%20wide%0Asampling%20diversity.%20Both%20aspects%20are%20crucial%20for%20earth%20observation%20data%2C%20where%0Asemantic%20classes%20can%20vary%20severely%20in%20scale%20and%20occurrence%20frequency.%20We%20employ%0Athe%20novel%20data%20instances%20for%20downstream%20segmentation%2C%20as%20a%20form%20of%20data%0Aaugmentation.%20In%20our%20experiments%2C%20we%20provide%20comparisons%20to%20prior%20works%20based%0Aon%20discriminative%20diffusion%20models%20or%20GANs.%20We%20demonstrate%20that%20integrating%0Agenerated%20samples%20yields%20significant%20quantitative%20improvements%20for%20satellite%0Asemantic%20segmentation%20--%20both%20compared%20to%20baselines%20and%20when%20training%20only%20on%0Athe%20original%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16605v1&entry.124074799=Read"},
{"title": "One-Shot Domain Incremental Learning", "author": "Yasushi Esaki and Satoshi Koide and Takuro Kutsuna", "abstract": "  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n", "link": "http://arxiv.org/abs/2403.16707v1", "date": "2024-03-25", "relevancy": 2.3023, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5013}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4444}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Domain%20Incremental%20Learning&body=Title%3A%20One-Shot%20Domain%20Incremental%20Learning%0AAuthor%3A%20Yasushi%20Esaki%20and%20Satoshi%20Koide%20and%20Takuro%20Kutsuna%0AAbstract%3A%20%20%20Domain%20incremental%20learning%20%28DIL%29%20has%20been%20discussed%20in%20previous%20studies%20on%0Adeep%20neural%20network%20models%20for%20classification.%20In%20DIL%2C%20we%20assume%20that%20samples%0Aon%20new%20domains%20are%20observed%20over%20time.%20The%20models%20must%20classify%20inputs%20on%20all%0Adomains.%20In%20practice%2C%20however%2C%20we%20may%20encounter%20a%20situation%20where%20we%20need%20to%0Aperform%20DIL%20under%20the%20constraint%20that%20the%20samples%20on%20the%20new%20domain%20are%0Aobserved%20only%20infrequently.%20Therefore%2C%20in%20this%20study%2C%20we%20consider%20the%20extreme%0Acase%20where%20we%20have%20only%20one%20sample%20from%20the%20new%20domain%2C%20which%20we%20call%20one-shot%0ADIL.%20We%20first%20empirically%20show%20that%20existing%20DIL%20methods%20do%20not%20work%20well%20in%0Aone-shot%20DIL.%20We%20have%20analyzed%20the%20reason%20for%20this%20failure%20through%20various%0Ainvestigations.%20According%20to%20our%20analysis%2C%20we%20clarify%20that%20the%20difficulty%20of%0Aone-shot%20DIL%20is%20caused%20by%20the%20statistics%20in%20the%20batch%20normalization%20layers.%0ATherefore%2C%20we%20propose%20a%20technique%20regarding%20these%20statistics%20and%20demonstrate%0Athe%20effectiveness%20of%20our%20technique%20through%20experiments%20on%20open%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16707v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Domain%20Incremental%20Learning&entry.906535625=Yasushi%20Esaki%20and%20Satoshi%20Koide%20and%20Takuro%20Kutsuna&entry.1292438233=%20%20Domain%20incremental%20learning%20%28DIL%29%20has%20been%20discussed%20in%20previous%20studies%20on%0Adeep%20neural%20network%20models%20for%20classification.%20In%20DIL%2C%20we%20assume%20that%20samples%0Aon%20new%20domains%20are%20observed%20over%20time.%20The%20models%20must%20classify%20inputs%20on%20all%0Adomains.%20In%20practice%2C%20however%2C%20we%20may%20encounter%20a%20situation%20where%20we%20need%20to%0Aperform%20DIL%20under%20the%20constraint%20that%20the%20samples%20on%20the%20new%20domain%20are%0Aobserved%20only%20infrequently.%20Therefore%2C%20in%20this%20study%2C%20we%20consider%20the%20extreme%0Acase%20where%20we%20have%20only%20one%20sample%20from%20the%20new%20domain%2C%20which%20we%20call%20one-shot%0ADIL.%20We%20first%20empirically%20show%20that%20existing%20DIL%20methods%20do%20not%20work%20well%20in%0Aone-shot%20DIL.%20We%20have%20analyzed%20the%20reason%20for%20this%20failure%20through%20various%0Ainvestigations.%20According%20to%20our%20analysis%2C%20we%20clarify%20that%20the%20difficulty%20of%0Aone-shot%20DIL%20is%20caused%20by%20the%20statistics%20in%20the%20batch%20normalization%20layers.%0ATherefore%2C%20we%20propose%20a%20technique%20regarding%20these%20statistics%20and%20demonstrate%0Athe%20effectiveness%20of%20our%20technique%20through%20experiments%20on%20open%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16707v1&entry.124074799=Read"},
{"title": "Enabling Uncertainty Estimation in Iterative Neural Networks", "author": "Nikita Durasov and Doruk Oner and Jonathan Donier and Hieu Le and Pascal Fua", "abstract": "  Turning pass-through network architectures into iterative ones, which use\ntheir own output as input, is a well-known approach for boosting performance.\nIn this paper, we argue that such architectures offer an additional benefit:\nThe convergence rate of their successive outputs is highly correlated with the\naccuracy of the value to which they converge. Thus, we can use the convergence\nrate as a useful proxy for uncertainty. This results in an approach to\nuncertainty estimation that provides state-of-the-art estimates at a much lower\ncomputational cost than techniques like Ensembles, and without requiring any\nmodifications to the original iterative model. We demonstrate its practical\nvalue by embedding it in two application domains: road detection in aerial\nimages and the estimation of aerodynamic properties of 2D and 3D shapes.\n", "link": "http://arxiv.org/abs/2403.16732v1", "date": "2024-03-25", "relevancy": 2.2521, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5736}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks&body=Title%3A%20Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks%0AAuthor%3A%20Nikita%20Durasov%20and%20Doruk%20Oner%20and%20Jonathan%20Donier%20and%20Hieu%20Le%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Turning%20pass-through%20network%20architectures%20into%20iterative%20ones%2C%20which%20use%0Atheir%20own%20output%20as%20input%2C%20is%20a%20well-known%20approach%20for%20boosting%20performance.%0AIn%20this%20paper%2C%20we%20argue%20that%20such%20architectures%20offer%20an%20additional%20benefit%3A%0AThe%20convergence%20rate%20of%20their%20successive%20outputs%20is%20highly%20correlated%20with%20the%0Aaccuracy%20of%20the%20value%20to%20which%20they%20converge.%20Thus%2C%20we%20can%20use%20the%20convergence%0Arate%20as%20a%20useful%20proxy%20for%20uncertainty.%20This%20results%20in%20an%20approach%20to%0Auncertainty%20estimation%20that%20provides%20state-of-the-art%20estimates%20at%20a%20much%20lower%0Acomputational%20cost%20than%20techniques%20like%20Ensembles%2C%20and%20without%20requiring%20any%0Amodifications%20to%20the%20original%20iterative%20model.%20We%20demonstrate%20its%20practical%0Avalue%20by%20embedding%20it%20in%20two%20application%20domains%3A%20road%20detection%20in%20aerial%0Aimages%20and%20the%20estimation%20of%20aerodynamic%20properties%20of%202D%20and%203D%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16732v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Uncertainty%20Estimation%20in%20Iterative%20Neural%20Networks&entry.906535625=Nikita%20Durasov%20and%20Doruk%20Oner%20and%20Jonathan%20Donier%20and%20Hieu%20Le%20and%20Pascal%20Fua&entry.1292438233=%20%20Turning%20pass-through%20network%20architectures%20into%20iterative%20ones%2C%20which%20use%0Atheir%20own%20output%20as%20input%2C%20is%20a%20well-known%20approach%20for%20boosting%20performance.%0AIn%20this%20paper%2C%20we%20argue%20that%20such%20architectures%20offer%20an%20additional%20benefit%3A%0AThe%20convergence%20rate%20of%20their%20successive%20outputs%20is%20highly%20correlated%20with%20the%0Aaccuracy%20of%20the%20value%20to%20which%20they%20converge.%20Thus%2C%20we%20can%20use%20the%20convergence%0Arate%20as%20a%20useful%20proxy%20for%20uncertainty.%20This%20results%20in%20an%20approach%20to%0Auncertainty%20estimation%20that%20provides%20state-of-the-art%20estimates%20at%20a%20much%20lower%0Acomputational%20cost%20than%20techniques%20like%20Ensembles%2C%20and%20without%20requiring%20any%0Amodifications%20to%20the%20original%20iterative%20model.%20We%20demonstrate%20its%20practical%0Avalue%20by%20embedding%20it%20in%20two%20application%20domains%3A%20road%20detection%20in%20aerial%0Aimages%20and%20the%20estimation%20of%20aerodynamic%20properties%20of%202D%20and%203D%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16732v1&entry.124074799=Read"},
{"title": "EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation", "author": "Kudaibergen Abutalip and Numan Saeed and Ikboljon Sobirov and Vincent Andrearczyk and Adrien Depeursinge and Mohammad Yaqub", "abstract": "  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n", "link": "http://arxiv.org/abs/2403.16594v1", "date": "2024-03-25", "relevancy": 2.2499, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.7191}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5592}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5031}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EDUE%3A%20Expert%20Disagreement-Guided%20One-Pass%20Uncertainty%20Estimation%20for%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20EDUE%3A%20Expert%20Disagreement-Guided%20One-Pass%20Uncertainty%20Estimation%20for%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Kudaibergen%20Abutalip%20and%20Numan%20Saeed%20and%20Ikboljon%20Sobirov%20and%20Vincent%20Andrearczyk%20and%20Adrien%20Depeursinge%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Deploying%20deep%20learning%20%28DL%29%20models%20in%20medical%20applications%20relies%20on%0Apredictive%20performance%20and%20other%20critical%20factors%2C%20such%20as%20conveying%0Atrustworthy%20predictive%20uncertainty.%20Uncertainty%20estimation%20%28UE%29%20methods%20provide%0Apotential%20solutions%20for%20evaluating%20prediction%20reliability%20and%20improving%20the%0Amodel%20confidence%20calibration.%20Despite%20increasing%20interest%20in%20UE%2C%20challenges%0Apersist%2C%20such%20as%20the%20need%20for%20explicit%20methods%20to%20capture%20aleatoric%20uncertainty%0Aand%20align%20uncertainty%20estimates%20with%20real-life%20disagreements%20among%20domain%0Aexperts.%20This%20paper%20proposes%20an%20Expert%20Disagreement-Guided%20Uncertainty%0AEstimation%20%28EDUE%29%20for%20medical%20image%20segmentation.%20By%20leveraging%20variability%20in%0Aground-truth%20annotations%20from%20multiple%20raters%2C%20we%20guide%20the%20model%20during%0Atraining%20and%20incorporate%20random%20sampling-based%20strategies%20to%20enhance%0Acalibration%20confidence.%20Our%20method%20achieves%2055%25%20and%2023%25%20improvement%20in%0Acorrelation%20on%20average%20with%20expert%20disagreements%20at%20the%20image%20and%20pixel%20levels%2C%0Arespectively%2C%20better%20calibration%2C%20and%20competitive%20segmentation%20performance%0Acompared%20to%20the%20state-of-the-art%20deep%20ensembles%2C%20requiring%20only%20a%20single%0Aforward%20pass.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16594v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDUE%3A%20Expert%20Disagreement-Guided%20One-Pass%20Uncertainty%20Estimation%20for%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Kudaibergen%20Abutalip%20and%20Numan%20Saeed%20and%20Ikboljon%20Sobirov%20and%20Vincent%20Andrearczyk%20and%20Adrien%20Depeursinge%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Deploying%20deep%20learning%20%28DL%29%20models%20in%20medical%20applications%20relies%20on%0Apredictive%20performance%20and%20other%20critical%20factors%2C%20such%20as%20conveying%0Atrustworthy%20predictive%20uncertainty.%20Uncertainty%20estimation%20%28UE%29%20methods%20provide%0Apotential%20solutions%20for%20evaluating%20prediction%20reliability%20and%20improving%20the%0Amodel%20confidence%20calibration.%20Despite%20increasing%20interest%20in%20UE%2C%20challenges%0Apersist%2C%20such%20as%20the%20need%20for%20explicit%20methods%20to%20capture%20aleatoric%20uncertainty%0Aand%20align%20uncertainty%20estimates%20with%20real-life%20disagreements%20among%20domain%0Aexperts.%20This%20paper%20proposes%20an%20Expert%20Disagreement-Guided%20Uncertainty%0AEstimation%20%28EDUE%29%20for%20medical%20image%20segmentation.%20By%20leveraging%20variability%20in%0Aground-truth%20annotations%20from%20multiple%20raters%2C%20we%20guide%20the%20model%20during%0Atraining%20and%20incorporate%20random%20sampling-based%20strategies%20to%20enhance%0Acalibration%20confidence.%20Our%20method%20achieves%2055%25%20and%2023%25%20improvement%20in%0Acorrelation%20on%20average%20with%20expert%20disagreements%20at%20the%20image%20and%20pixel%20levels%2C%0Arespectively%2C%20better%20calibration%2C%20and%20competitive%20segmentation%20performance%0Acompared%20to%20the%20state-of-the-art%20deep%20ensembles%2C%20requiring%20only%20a%20single%0Aforward%20pass.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16594v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Few-Shot Learning via Diffusive Neural Network\n  Generation", "author": "Yuan Yuan and Chenyang Shao and Jingtao Ding and Depeng Jin and Yong Li", "abstract": "  Spatio-temporal modeling is foundational for smart city applications, yet it\nis often hindered by data scarcity in many cities and regions. To bridge this\ngap, we propose a novel generative pre-training framework, GPD, for\nspatio-temporal few-shot learning with urban knowledge transfer. Unlike\nconventional approaches that heavily rely on common feature extraction or\nintricate few-shot learning designs, our solution takes a novel approach by\nperforming generative pre-training on a collection of neural network parameters\noptimized with data from source cities. We recast spatio-temporal few-shot\nlearning as pre-training a generative diffusion model, which generates tailored\nneural networks guided by prompts, allowing for adaptability to diverse data\ndistributions and city-specific characteristics. GPD employs a\nTransformer-based denoising diffusion model, which is model-agnostic to\nintegrate with powerful spatio-temporal neural networks. By addressing\nchallenges arising from data gaps and the complexity of generalizing knowledge\nacross cities, our framework consistently outperforms state-of-the-art\nbaselines on multiple real-world datasets for tasks such as traffic speed\nprediction and crowd flow prediction. The implementation of our approach is\navailable: https://github.com/tsinghua-fib-lab/GPD.\n", "link": "http://arxiv.org/abs/2402.11922v3", "date": "2024-03-25", "relevancy": 2.2356, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5825}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5658}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5325}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Few-Shot%20Learning%20via%20Diffusive%20Neural%20Network%0A%20%20Generation&body=Title%3A%20Spatio-Temporal%20Few-Shot%20Learning%20via%20Diffusive%20Neural%20Network%0A%20%20Generation%0AAuthor%3A%20Yuan%20Yuan%20and%20Chenyang%20Shao%20and%20Jingtao%20Ding%20and%20Depeng%20Jin%20and%20Yong%20Li%0AAbstract%3A%20%20%20Spatio-temporal%20modeling%20is%20foundational%20for%20smart%20city%20applications%2C%20yet%20it%0Ais%20often%20hindered%20by%20data%20scarcity%20in%20many%20cities%20and%20regions.%20To%20bridge%20this%0Agap%2C%20we%20propose%20a%20novel%20generative%20pre-training%20framework%2C%20GPD%2C%20for%0Aspatio-temporal%20few-shot%20learning%20with%20urban%20knowledge%20transfer.%20Unlike%0Aconventional%20approaches%20that%20heavily%20rely%20on%20common%20feature%20extraction%20or%0Aintricate%20few-shot%20learning%20designs%2C%20our%20solution%20takes%20a%20novel%20approach%20by%0Aperforming%20generative%20pre-training%20on%20a%20collection%20of%20neural%20network%20parameters%0Aoptimized%20with%20data%20from%20source%20cities.%20We%20recast%20spatio-temporal%20few-shot%0Alearning%20as%20pre-training%20a%20generative%20diffusion%20model%2C%20which%20generates%20tailored%0Aneural%20networks%20guided%20by%20prompts%2C%20allowing%20for%20adaptability%20to%20diverse%20data%0Adistributions%20and%20city-specific%20characteristics.%20GPD%20employs%20a%0ATransformer-based%20denoising%20diffusion%20model%2C%20which%20is%20model-agnostic%20to%0Aintegrate%20with%20powerful%20spatio-temporal%20neural%20networks.%20By%20addressing%0Achallenges%20arising%20from%20data%20gaps%20and%20the%20complexity%20of%20generalizing%20knowledge%0Aacross%20cities%2C%20our%20framework%20consistently%20outperforms%20state-of-the-art%0Abaselines%20on%20multiple%20real-world%20datasets%20for%20tasks%20such%20as%20traffic%20speed%0Aprediction%20and%20crowd%20flow%20prediction.%20The%20implementation%20of%20our%20approach%20is%0Aavailable%3A%20https%3A//github.com/tsinghua-fib-lab/GPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11922v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Few-Shot%20Learning%20via%20Diffusive%20Neural%20Network%0A%20%20Generation&entry.906535625=Yuan%20Yuan%20and%20Chenyang%20Shao%20and%20Jingtao%20Ding%20and%20Depeng%20Jin%20and%20Yong%20Li&entry.1292438233=%20%20Spatio-temporal%20modeling%20is%20foundational%20for%20smart%20city%20applications%2C%20yet%20it%0Ais%20often%20hindered%20by%20data%20scarcity%20in%20many%20cities%20and%20regions.%20To%20bridge%20this%0Agap%2C%20we%20propose%20a%20novel%20generative%20pre-training%20framework%2C%20GPD%2C%20for%0Aspatio-temporal%20few-shot%20learning%20with%20urban%20knowledge%20transfer.%20Unlike%0Aconventional%20approaches%20that%20heavily%20rely%20on%20common%20feature%20extraction%20or%0Aintricate%20few-shot%20learning%20designs%2C%20our%20solution%20takes%20a%20novel%20approach%20by%0Aperforming%20generative%20pre-training%20on%20a%20collection%20of%20neural%20network%20parameters%0Aoptimized%20with%20data%20from%20source%20cities.%20We%20recast%20spatio-temporal%20few-shot%0Alearning%20as%20pre-training%20a%20generative%20diffusion%20model%2C%20which%20generates%20tailored%0Aneural%20networks%20guided%20by%20prompts%2C%20allowing%20for%20adaptability%20to%20diverse%20data%0Adistributions%20and%20city-specific%20characteristics.%20GPD%20employs%20a%0ATransformer-based%20denoising%20diffusion%20model%2C%20which%20is%20model-agnostic%20to%0Aintegrate%20with%20powerful%20spatio-temporal%20neural%20networks.%20By%20addressing%0Achallenges%20arising%20from%20data%20gaps%20and%20the%20complexity%20of%20generalizing%20knowledge%0Aacross%20cities%2C%20our%20framework%20consistently%20outperforms%20state-of-the-art%0Abaselines%20on%20multiple%20real-world%20datasets%20for%20tasks%20such%20as%20traffic%20speed%0Aprediction%20and%20crowd%20flow%20prediction.%20The%20implementation%20of%20our%20approach%20is%0Aavailable%3A%20https%3A//github.com/tsinghua-fib-lab/GPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11922v3&entry.124074799=Read"},
{"title": "Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network", "author": "Yin Zhang and Jinhong Deng and Peidong Liu and Wen Li and Shiyu Zhao", "abstract": "  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n", "link": "http://arxiv.org/abs/2403.16669v1", "date": "2024-03-25", "relevancy": 2.2183, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5729}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5541}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptive%20Detection%20of%20MAVs%3A%20A%20Benchmark%20and%20Noise%20Suppression%0A%20%20Network&body=Title%3A%20Domain%20Adaptive%20Detection%20of%20MAVs%3A%20A%20Benchmark%20and%20Noise%20Suppression%0A%20%20Network%0AAuthor%3A%20Yin%20Zhang%20and%20Jinhong%20Deng%20and%20Peidong%20Liu%20and%20Wen%20Li%20and%20Shiyu%20Zhao%0AAbstract%3A%20%20%20Visual%20detection%20of%20Micro%20Air%20Vehicles%20%28MAVs%29%20has%20attracted%20increasing%0Aattention%20in%20recent%20years%20due%20to%20its%20important%20application%20in%20various%20tasks.%0AThe%20existing%20methods%20for%20MAV%20detection%20assume%20that%20the%20training%20set%20and%20testing%0Aset%20have%20the%20same%20distribution.%20As%20a%20result%2C%20when%20deployed%20in%20new%20domains%2C%20the%0Adetectors%20would%20have%20a%20significant%20performance%20degradation%20due%20to%20domain%0Adiscrepancy.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20cross-domain%20MAV%20detection.%0AThe%20contributions%20of%20this%20paper%20are%20threefold.%201%29%20We%20propose%20a%0AMulti-MAV-Multi-Domain%20%28M3D%29%20dataset%20consisting%20of%20both%20simulation%20and%0Arealistic%20images.%20Compared%20to%20other%20existing%20datasets%2C%20the%20proposed%20one%20is%20more%0Acomprehensive%20in%20the%20sense%20that%20it%20covers%20rich%20scenes%2C%20diverse%20MAV%20types%2C%20and%0Avarious%20viewing%20angles.%20A%20new%20benchmark%20for%20cross-domain%20MAV%20detection%20is%0Aproposed%20based%20on%20the%20proposed%20dataset.%202%29%20We%20propose%20a%20Noise%20Suppression%0ANetwork%20%28NSN%29%20based%20on%20the%20framework%20of%20pseudo-labeling%20and%20a%20large-to-small%0Atraining%20procedure.%20To%20reduce%20the%20challenging%20pseudo-label%20noises%2C%20two%20novel%0Amodules%20are%20designed%20in%20this%20network.%20The%20first%20is%20a%20prior-based%20curriculum%0Alearning%20module%20for%20allocating%20adaptive%20thresholds%20for%20pseudo%20labels%20with%0Adifferent%20difficulties.%20The%20second%20is%20a%20masked%20copy-paste%20augmentation%20module%0Afor%20pasting%20truly-labeled%20MAVs%20on%20unlabeled%20target%20images%20and%20thus%20decreasing%0Apseudo-label%20noises.%203%29%20Extensive%20experimental%20results%20verify%20the%20superior%0Aperformance%20of%20the%20proposed%20method%20compared%20to%20the%20state-of-the-art%20ones.%20In%0Aparticular%2C%20it%20achieves%20mAP%20of%2046.9%25%28%2B5.8%25%29%2C%2050.5%25%28%2B3.7%25%29%2C%20and%2061.5%25%28%2B11.3%25%29%20on%0Athe%20tasks%20of%20simulation-to-real%20adaptation%2C%20cross-scene%20adaptation%2C%20and%0Across-camera%20adaptation%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16669v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptive%20Detection%20of%20MAVs%3A%20A%20Benchmark%20and%20Noise%20Suppression%0A%20%20Network&entry.906535625=Yin%20Zhang%20and%20Jinhong%20Deng%20and%20Peidong%20Liu%20and%20Wen%20Li%20and%20Shiyu%20Zhao&entry.1292438233=%20%20Visual%20detection%20of%20Micro%20Air%20Vehicles%20%28MAVs%29%20has%20attracted%20increasing%0Aattention%20in%20recent%20years%20due%20to%20its%20important%20application%20in%20various%20tasks.%0AThe%20existing%20methods%20for%20MAV%20detection%20assume%20that%20the%20training%20set%20and%20testing%0Aset%20have%20the%20same%20distribution.%20As%20a%20result%2C%20when%20deployed%20in%20new%20domains%2C%20the%0Adetectors%20would%20have%20a%20significant%20performance%20degradation%20due%20to%20domain%0Adiscrepancy.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20cross-domain%20MAV%20detection.%0AThe%20contributions%20of%20this%20paper%20are%20threefold.%201%29%20We%20propose%20a%0AMulti-MAV-Multi-Domain%20%28M3D%29%20dataset%20consisting%20of%20both%20simulation%20and%0Arealistic%20images.%20Compared%20to%20other%20existing%20datasets%2C%20the%20proposed%20one%20is%20more%0Acomprehensive%20in%20the%20sense%20that%20it%20covers%20rich%20scenes%2C%20diverse%20MAV%20types%2C%20and%0Avarious%20viewing%20angles.%20A%20new%20benchmark%20for%20cross-domain%20MAV%20detection%20is%0Aproposed%20based%20on%20the%20proposed%20dataset.%202%29%20We%20propose%20a%20Noise%20Suppression%0ANetwork%20%28NSN%29%20based%20on%20the%20framework%20of%20pseudo-labeling%20and%20a%20large-to-small%0Atraining%20procedure.%20To%20reduce%20the%20challenging%20pseudo-label%20noises%2C%20two%20novel%0Amodules%20are%20designed%20in%20this%20network.%20The%20first%20is%20a%20prior-based%20curriculum%0Alearning%20module%20for%20allocating%20adaptive%20thresholds%20for%20pseudo%20labels%20with%0Adifferent%20difficulties.%20The%20second%20is%20a%20masked%20copy-paste%20augmentation%20module%0Afor%20pasting%20truly-labeled%20MAVs%20on%20unlabeled%20target%20images%20and%20thus%20decreasing%0Apseudo-label%20noises.%203%29%20Extensive%20experimental%20results%20verify%20the%20superior%0Aperformance%20of%20the%20proposed%20method%20compared%20to%20the%20state-of-the-art%20ones.%20In%0Aparticular%2C%20it%20achieves%20mAP%20of%2046.9%25%28%2B5.8%25%29%2C%2050.5%25%28%2B3.7%25%29%2C%20and%2061.5%25%28%2B11.3%25%29%20on%0Athe%20tasks%20of%20simulation-to-real%20adaptation%2C%20cross-scene%20adaptation%2C%20and%0Across-camera%20adaptation%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16669v1&entry.124074799=Read"},
{"title": "Self-Adaptive Reality-Guided Diffusion for Artifact-Free\n  Super-Resolution", "author": "Qingping Zheng and Ling Zheng and Yuanfan Guo and Ying Li and Songcen Xu and Jiankang Deng and Hang Xu", "abstract": "  Artifact-free super-resolution (SR) aims to translate low-resolution images\ninto their high-resolution counterparts with a strict integrity of the original\ncontent, eliminating any distortions or synthetic details. While traditional\ndiffusion-based SR techniques have demonstrated remarkable abilities to enhance\nimage detail, they are prone to artifact introduction during iterative\nprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,\ndeviate from the true structure of the source image, thus challenging the\nintegrity of the super-resolution process. In this work, we propose\nSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that\ndelves into the latent space to effectively identify and mitigate the\npropagation of artifacts. Our SARGD begins by using an artifact detector to\nidentify implausible pixels, creating a binary mask that highlights artifacts.\nFollowing this, the Reality Guidance Refinement (RGR) process refines artifacts\nby integrating this mask with realistic latent representations, improving\nalignment with the original image. Nonetheless, initial realistic-latent\nrepresentations from lower-quality images result in over-smoothing in the final\noutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.\nIt dynamically computes a reality score, enhancing the sharpness of the\nrealistic latent. These alternating mechanisms collectively achieve\nartifact-free super-resolution. Extensive experiments demonstrate the\nsuperiority of our method, delivering detailed artifact-free high-resolution\nimages while reducing sampling steps by 2X. We release our code at\nhttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.\n", "link": "http://arxiv.org/abs/2403.16643v1", "date": "2024-03-25", "relevancy": 2.188, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5655}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5376}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5323}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Adaptive%20Reality-Guided%20Diffusion%20for%20Artifact-Free%0A%20%20Super-Resolution&body=Title%3A%20Self-Adaptive%20Reality-Guided%20Diffusion%20for%20Artifact-Free%0A%20%20Super-Resolution%0AAuthor%3A%20Qingping%20Zheng%20and%20Ling%20Zheng%20and%20Yuanfan%20Guo%20and%20Ying%20Li%20and%20Songcen%20Xu%20and%20Jiankang%20Deng%20and%20Hang%20Xu%0AAbstract%3A%20%20%20Artifact-free%20super-resolution%20%28SR%29%20aims%20to%20translate%20low-resolution%20images%0Ainto%20their%20high-resolution%20counterparts%20with%20a%20strict%20integrity%20of%20the%20original%0Acontent%2C%20eliminating%20any%20distortions%20or%20synthetic%20details.%20While%20traditional%0Adiffusion-based%20SR%20techniques%20have%20demonstrated%20remarkable%20abilities%20to%20enhance%0Aimage%20detail%2C%20they%20are%20prone%20to%20artifact%20introduction%20during%20iterative%0Aprocedures.%20Such%20artifacts%2C%20ranging%20from%20trivial%20noise%20to%20unauthentic%20textures%2C%0Adeviate%20from%20the%20true%20structure%20of%20the%20source%20image%2C%20thus%20challenging%20the%0Aintegrity%20of%20the%20super-resolution%20process.%20In%20this%20work%2C%20we%20propose%0ASelf-Adaptive%20Reality-Guided%20Diffusion%20%28SARGD%29%2C%20a%20training-free%20method%20that%0Adelves%20into%20the%20latent%20space%20to%20effectively%20identify%20and%20mitigate%20the%0Apropagation%20of%20artifacts.%20Our%20SARGD%20begins%20by%20using%20an%20artifact%20detector%20to%0Aidentify%20implausible%20pixels%2C%20creating%20a%20binary%20mask%20that%20highlights%20artifacts.%0AFollowing%20this%2C%20the%20Reality%20Guidance%20Refinement%20%28RGR%29%20process%20refines%20artifacts%0Aby%20integrating%20this%20mask%20with%20realistic%20latent%20representations%2C%20improving%0Aalignment%20with%20the%20original%20image.%20Nonetheless%2C%20initial%20realistic-latent%0Arepresentations%20from%20lower-quality%20images%20result%20in%20over-smoothing%20in%20the%20final%0Aoutput.%20To%20address%20this%2C%20we%20introduce%20a%20Self-Adaptive%20Guidance%20%28SAG%29%20mechanism.%0AIt%20dynamically%20computes%20a%20reality%20score%2C%20enhancing%20the%20sharpness%20of%20the%0Arealistic%20latent.%20These%20alternating%20mechanisms%20collectively%20achieve%0Aartifact-free%20super-resolution.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%2C%20delivering%20detailed%20artifact-free%20high-resolution%0Aimages%20while%20reducing%20sampling%20steps%20by%202X.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16643v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Adaptive%20Reality-Guided%20Diffusion%20for%20Artifact-Free%0A%20%20Super-Resolution&entry.906535625=Qingping%20Zheng%20and%20Ling%20Zheng%20and%20Yuanfan%20Guo%20and%20Ying%20Li%20and%20Songcen%20Xu%20and%20Jiankang%20Deng%20and%20Hang%20Xu&entry.1292438233=%20%20Artifact-free%20super-resolution%20%28SR%29%20aims%20to%20translate%20low-resolution%20images%0Ainto%20their%20high-resolution%20counterparts%20with%20a%20strict%20integrity%20of%20the%20original%0Acontent%2C%20eliminating%20any%20distortions%20or%20synthetic%20details.%20While%20traditional%0Adiffusion-based%20SR%20techniques%20have%20demonstrated%20remarkable%20abilities%20to%20enhance%0Aimage%20detail%2C%20they%20are%20prone%20to%20artifact%20introduction%20during%20iterative%0Aprocedures.%20Such%20artifacts%2C%20ranging%20from%20trivial%20noise%20to%20unauthentic%20textures%2C%0Adeviate%20from%20the%20true%20structure%20of%20the%20source%20image%2C%20thus%20challenging%20the%0Aintegrity%20of%20the%20super-resolution%20process.%20In%20this%20work%2C%20we%20propose%0ASelf-Adaptive%20Reality-Guided%20Diffusion%20%28SARGD%29%2C%20a%20training-free%20method%20that%0Adelves%20into%20the%20latent%20space%20to%20effectively%20identify%20and%20mitigate%20the%0Apropagation%20of%20artifacts.%20Our%20SARGD%20begins%20by%20using%20an%20artifact%20detector%20to%0Aidentify%20implausible%20pixels%2C%20creating%20a%20binary%20mask%20that%20highlights%20artifacts.%0AFollowing%20this%2C%20the%20Reality%20Guidance%20Refinement%20%28RGR%29%20process%20refines%20artifacts%0Aby%20integrating%20this%20mask%20with%20realistic%20latent%20representations%2C%20improving%0Aalignment%20with%20the%20original%20image.%20Nonetheless%2C%20initial%20realistic-latent%0Arepresentations%20from%20lower-quality%20images%20result%20in%20over-smoothing%20in%20the%20final%0Aoutput.%20To%20address%20this%2C%20we%20introduce%20a%20Self-Adaptive%20Guidance%20%28SAG%29%20mechanism.%0AIt%20dynamically%20computes%20a%20reality%20score%2C%20enhancing%20the%20sharpness%20of%20the%0Arealistic%20latent.%20These%20alternating%20mechanisms%20collectively%20achieve%0Aartifact-free%20super-resolution.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%2C%20delivering%20detailed%20artifact-free%20high-resolution%0Aimages%20while%20reducing%20sampling%20steps%20by%202X.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16643v1&entry.124074799=Read"},
{"title": "Synapse: Learning Preferential Concepts from Visual Demonstrations", "author": "Sadanand Modak and Noah Patton and Isil Dillig and Joydeep Biswas", "abstract": "  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n", "link": "http://arxiv.org/abs/2403.16689v1", "date": "2024-03-25", "relevancy": 2.1652, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5699}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Synapse%3A%20Learning%20Preferential%20Concepts%20from%20Visual%20Demonstrations&body=Title%3A%20Synapse%3A%20Learning%20Preferential%20Concepts%20from%20Visual%20Demonstrations%0AAuthor%3A%20Sadanand%20Modak%20and%20Noah%20Patton%20and%20Isil%20Dillig%20and%20Joydeep%20Biswas%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20preference%20learning%2C%20which%20aims%20to%20learn%0Auser-specific%20preferences%20%28e.g.%2C%20%22good%20parking%20spot%22%2C%20%22convenient%20drop-off%0Alocation%22%29%20from%20visual%20input.%20Despite%20its%20similarity%20to%20learning%20factual%0Aconcepts%20%28e.g.%2C%20%22red%20cube%22%29%2C%20preference%20learning%20is%20a%20fundamentally%20harder%0Aproblem%20due%20to%20its%20subjective%20nature%20and%20the%20paucity%20of%20person-specific%0Atraining%20data.%20We%20address%20this%20problem%20using%20a%20new%20framework%20called%20Synapse%2C%0Awhich%20is%20a%20neuro-symbolic%20approach%20designed%20to%20efficiently%20learn%20preferential%0Aconcepts%20from%20limited%20demonstrations.%20Synapse%20represents%20preferences%20as%0Aneuro-symbolic%20programs%20in%20a%20domain-specific%20language%20%28DSL%29%20that%20operates%20over%0Aimages%2C%20and%20leverages%20a%20novel%20combination%20of%20visual%20parsing%2C%20large%20language%0Amodels%2C%20and%20program%20synthesis%20to%20learn%20programs%20representing%20individual%0Apreferences.%20We%20evaluate%20Synapse%20through%20extensive%20experimentation%20including%20a%0Auser%20case%20study%20focusing%20on%20mobility-related%20concepts%20in%20mobile%20robotics%20and%0Aautonomous%20driving.%20Our%20evaluation%20demonstrates%20that%20Synapse%20significantly%0Aoutperforms%20existing%20baselines%20as%20well%20as%20its%20own%20ablations.%20The%20code%20and%20other%0Adetails%20can%20be%20found%20on%20the%20project%20website%20https%3A//amrl.cs.utexas.edu/synapse%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synapse%3A%20Learning%20Preferential%20Concepts%20from%20Visual%20Demonstrations&entry.906535625=Sadanand%20Modak%20and%20Noah%20Patton%20and%20Isil%20Dillig%20and%20Joydeep%20Biswas&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20preference%20learning%2C%20which%20aims%20to%20learn%0Auser-specific%20preferences%20%28e.g.%2C%20%22good%20parking%20spot%22%2C%20%22convenient%20drop-off%0Alocation%22%29%20from%20visual%20input.%20Despite%20its%20similarity%20to%20learning%20factual%0Aconcepts%20%28e.g.%2C%20%22red%20cube%22%29%2C%20preference%20learning%20is%20a%20fundamentally%20harder%0Aproblem%20due%20to%20its%20subjective%20nature%20and%20the%20paucity%20of%20person-specific%0Atraining%20data.%20We%20address%20this%20problem%20using%20a%20new%20framework%20called%20Synapse%2C%0Awhich%20is%20a%20neuro-symbolic%20approach%20designed%20to%20efficiently%20learn%20preferential%0Aconcepts%20from%20limited%20demonstrations.%20Synapse%20represents%20preferences%20as%0Aneuro-symbolic%20programs%20in%20a%20domain-specific%20language%20%28DSL%29%20that%20operates%20over%0Aimages%2C%20and%20leverages%20a%20novel%20combination%20of%20visual%20parsing%2C%20large%20language%0Amodels%2C%20and%20program%20synthesis%20to%20learn%20programs%20representing%20individual%0Apreferences.%20We%20evaluate%20Synapse%20through%20extensive%20experimentation%20including%20a%0Auser%20case%20study%20focusing%20on%20mobility-related%20concepts%20in%20mobile%20robotics%20and%0Aautonomous%20driving.%20Our%20evaluation%20demonstrates%20that%20Synapse%20significantly%0Aoutperforms%20existing%20baselines%20as%20well%20as%20its%20own%20ablations.%20The%20code%20and%20other%0Adetails%20can%20be%20found%20on%20the%20project%20website%20https%3A//amrl.cs.utexas.edu/synapse%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16689v1&entry.124074799=Read"},
{"title": "Multi-Scale Texture Loss for CT denoising with GANs", "author": "Francesco Di Feola and Lorenzo Tronchin and Valerio Guarrasi and Paolo Soda", "abstract": "  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n", "link": "http://arxiv.org/abs/2403.16640v1", "date": "2024-03-25", "relevancy": 2.1413, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5302}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs&body=Title%3A%20Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs%0AAuthor%3A%20Francesco%20Di%20Feola%20and%20Lorenzo%20Tronchin%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20proved%20as%20a%20powerful%20framework%0Afor%20denoising%20applications%20in%20medical%20imaging.%20However%2C%20GAN-based%20denoising%0Aalgorithms%20still%20suffer%20from%20limitations%20in%20capturing%20complex%20relationships%0Awithin%20the%20images.%20In%20this%20regard%2C%20the%20loss%20function%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20image%20generation%20process%2C%20encompassing%20how%20much%20a%20synthetic%20image%0Adiffers%20from%20a%20real%20image.%20To%20grasp%20highly%20complex%20and%20non-linear%20textural%0Arelationships%20in%20the%20training%20process%2C%20this%20work%20presents%20a%20loss%20function%20that%0Aleverages%20the%20intrinsic%20multi-scale%20nature%20of%20the%20Gray-Level-Co-occurrence%0AMatrix%20%28GLCM%29.%20Although%20the%20recent%20advances%20in%20deep%20learning%20have%20demonstrated%0Asuperior%20performance%20in%20classification%20and%20detection%20tasks%2C%20we%20hypothesize%20that%0Aits%20information%20content%20can%20be%20valuable%20when%20integrated%20into%20GANs%27%20training.%20To%0Athis%20end%2C%20we%20propose%20a%20differentiable%20implementation%20of%20the%20GLCM%20suited%20for%0Agradient-based%20optimization.%20Our%20approach%20also%20introduces%20a%20self-attention%0Alayer%20that%20dynamically%20aggregates%20the%20multi-scale%20texture%20information%20extracted%0Afrom%20the%20images.%20We%20validate%20our%20approach%20by%20carrying%20out%20extensive%20experiments%0Ain%20the%20context%20of%20low-dose%20CT%20denoising%2C%20a%20challenging%20application%20that%20aims%20to%0Aenhance%20the%20quality%20of%20noisy%20CT%20scans.%20We%20utilize%20three%20publicly%20available%0Adatasets%2C%20including%20one%20simulated%20and%20two%20real%20datasets.%20The%20results%20are%0Apromising%20as%20compared%20to%20other%20well-established%20loss%20functions%2C%20being%20also%0Aconsistent%20across%20three%20different%20GAN%20architectures.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FrancescoDiFeola/DenoTextureLoss%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Texture%20Loss%20for%20CT%20denoising%20with%20GANs&entry.906535625=Francesco%20Di%20Feola%20and%20Lorenzo%20Tronchin%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20proved%20as%20a%20powerful%20framework%0Afor%20denoising%20applications%20in%20medical%20imaging.%20However%2C%20GAN-based%20denoising%0Aalgorithms%20still%20suffer%20from%20limitations%20in%20capturing%20complex%20relationships%0Awithin%20the%20images.%20In%20this%20regard%2C%20the%20loss%20function%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20image%20generation%20process%2C%20encompassing%20how%20much%20a%20synthetic%20image%0Adiffers%20from%20a%20real%20image.%20To%20grasp%20highly%20complex%20and%20non-linear%20textural%0Arelationships%20in%20the%20training%20process%2C%20this%20work%20presents%20a%20loss%20function%20that%0Aleverages%20the%20intrinsic%20multi-scale%20nature%20of%20the%20Gray-Level-Co-occurrence%0AMatrix%20%28GLCM%29.%20Although%20the%20recent%20advances%20in%20deep%20learning%20have%20demonstrated%0Asuperior%20performance%20in%20classification%20and%20detection%20tasks%2C%20we%20hypothesize%20that%0Aits%20information%20content%20can%20be%20valuable%20when%20integrated%20into%20GANs%27%20training.%20To%0Athis%20end%2C%20we%20propose%20a%20differentiable%20implementation%20of%20the%20GLCM%20suited%20for%0Agradient-based%20optimization.%20Our%20approach%20also%20introduces%20a%20self-attention%0Alayer%20that%20dynamically%20aggregates%20the%20multi-scale%20texture%20information%20extracted%0Afrom%20the%20images.%20We%20validate%20our%20approach%20by%20carrying%20out%20extensive%20experiments%0Ain%20the%20context%20of%20low-dose%20CT%20denoising%2C%20a%20challenging%20application%20that%20aims%20to%0Aenhance%20the%20quality%20of%20noisy%20CT%20scans.%20We%20utilize%20three%20publicly%20available%0Adatasets%2C%20including%20one%20simulated%20and%20two%20real%20datasets.%20The%20results%20are%0Apromising%20as%20compared%20to%20other%20well-established%20loss%20functions%2C%20being%20also%0Aconsistent%20across%20three%20different%20GAN%20architectures.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FrancescoDiFeola/DenoTextureLoss%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16640v1&entry.124074799=Read"},
{"title": "Mask Grounding for Referring Image Segmentation", "author": "Yong Xien Chng and Henry Zheng and Yizeng Han and Xuchong Qiu and Gao Huang", "abstract": "  Referring Image Segmentation (RIS) is a challenging task that requires an\nalgorithm to segment objects referred by free-form language expressions.\nDespite significant progress in recent years, most state-of-the-art (SOTA)\nmethods still suffer from considerable language-image modality gap at the pixel\nand word level. These methods generally 1) rely on sentence-level language\nfeatures for language-image alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they exhibit weak object-level\ncorrespondence between visual and language features. Without well-grounded\nfeatures, prior methods struggle to understand complex expressions that require\nstrong reasoning over relationships among multiple objects, especially when\ndealing with rarely used or ambiguous clauses. To tackle this challenge, we\nintroduce a novel Mask Grounding auxiliary task that significantly improves\nvisual grounding within language features, by explicitly teaching the model to\nlearn fine-grained correspondence between masked textual tokens and their\nmatching visual objects. Mask Grounding can be directly used on prior RIS\nmethods and consistently bring improvements. Furthermore, to holistically\naddress the modality gap, we also design a cross-modal alignment loss and an\naccompanying alignment module. These additions work synergistically with Mask\nGrounding. With all these techniques, our comprehensive approach culminates in\nMagNet (Mask-grounded Network), an architecture that significantly outperforms\nprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating\nour method's effectiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n", "link": "http://arxiv.org/abs/2312.12198v2", "date": "2024-03-25", "relevancy": 2.0884, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5164}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mask%20Grounding%20for%20Referring%20Image%20Segmentation&body=Title%3A%20Mask%20Grounding%20for%20Referring%20Image%20Segmentation%0AAuthor%3A%20Yong%20Xien%20Chng%20and%20Henry%20Zheng%20and%20Yizeng%20Han%20and%20Xuchong%20Qiu%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%20is%20a%20challenging%20task%20that%20requires%20an%0Aalgorithm%20to%20segment%20objects%20referred%20by%20free-form%20language%20expressions.%0ADespite%20significant%20progress%20in%20recent%20years%2C%20most%20state-of-the-art%20%28SOTA%29%0Amethods%20still%20suffer%20from%20considerable%20language-image%20modality%20gap%20at%20the%20pixel%0Aand%20word%20level.%20These%20methods%20generally%201%29%20rely%20on%20sentence-level%20language%0Afeatures%20for%20language-image%20alignment%20and%202%29%20lack%20explicit%20training%20supervision%0Afor%20fine-grained%20visual%20grounding.%20Consequently%2C%20they%20exhibit%20weak%20object-level%0Acorrespondence%20between%20visual%20and%20language%20features.%20Without%20well-grounded%0Afeatures%2C%20prior%20methods%20struggle%20to%20understand%20complex%20expressions%20that%20require%0Astrong%20reasoning%20over%20relationships%20among%20multiple%20objects%2C%20especially%20when%0Adealing%20with%20rarely%20used%20or%20ambiguous%20clauses.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20Mask%20Grounding%20auxiliary%20task%20that%20significantly%20improves%0Avisual%20grounding%20within%20language%20features%2C%20by%20explicitly%20teaching%20the%20model%20to%0Alearn%20fine-grained%20correspondence%20between%20masked%20textual%20tokens%20and%20their%0Amatching%20visual%20objects.%20Mask%20Grounding%20can%20be%20directly%20used%20on%20prior%20RIS%0Amethods%20and%20consistently%20bring%20improvements.%20Furthermore%2C%20to%20holistically%0Aaddress%20the%20modality%20gap%2C%20we%20also%20design%20a%20cross-modal%20alignment%20loss%20and%20an%0Aaccompanying%20alignment%20module.%20These%20additions%20work%20synergistically%20with%20Mask%0AGrounding.%20With%20all%20these%20techniques%2C%20our%20comprehensive%20approach%20culminates%20in%0AMagNet%20%28Mask-grounded%20Network%29%2C%20an%20architecture%20that%20significantly%20outperforms%0Aprior%20arts%20on%20three%20key%20benchmarks%20%28RefCOCO%2C%20RefCOCO%2B%20and%20G-Ref%29%2C%20demonstrating%0Aour%20method%27s%20effectiveness%20in%20addressing%20current%20limitations%20of%20RIS%20algorithms.%0AOur%20code%20and%20pre-trained%20weights%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12198v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask%20Grounding%20for%20Referring%20Image%20Segmentation&entry.906535625=Yong%20Xien%20Chng%20and%20Henry%20Zheng%20and%20Yizeng%20Han%20and%20Xuchong%20Qiu%20and%20Gao%20Huang&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%20is%20a%20challenging%20task%20that%20requires%20an%0Aalgorithm%20to%20segment%20objects%20referred%20by%20free-form%20language%20expressions.%0ADespite%20significant%20progress%20in%20recent%20years%2C%20most%20state-of-the-art%20%28SOTA%29%0Amethods%20still%20suffer%20from%20considerable%20language-image%20modality%20gap%20at%20the%20pixel%0Aand%20word%20level.%20These%20methods%20generally%201%29%20rely%20on%20sentence-level%20language%0Afeatures%20for%20language-image%20alignment%20and%202%29%20lack%20explicit%20training%20supervision%0Afor%20fine-grained%20visual%20grounding.%20Consequently%2C%20they%20exhibit%20weak%20object-level%0Acorrespondence%20between%20visual%20and%20language%20features.%20Without%20well-grounded%0Afeatures%2C%20prior%20methods%20struggle%20to%20understand%20complex%20expressions%20that%20require%0Astrong%20reasoning%20over%20relationships%20among%20multiple%20objects%2C%20especially%20when%0Adealing%20with%20rarely%20used%20or%20ambiguous%20clauses.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20Mask%20Grounding%20auxiliary%20task%20that%20significantly%20improves%0Avisual%20grounding%20within%20language%20features%2C%20by%20explicitly%20teaching%20the%20model%20to%0Alearn%20fine-grained%20correspondence%20between%20masked%20textual%20tokens%20and%20their%0Amatching%20visual%20objects.%20Mask%20Grounding%20can%20be%20directly%20used%20on%20prior%20RIS%0Amethods%20and%20consistently%20bring%20improvements.%20Furthermore%2C%20to%20holistically%0Aaddress%20the%20modality%20gap%2C%20we%20also%20design%20a%20cross-modal%20alignment%20loss%20and%20an%0Aaccompanying%20alignment%20module.%20These%20additions%20work%20synergistically%20with%20Mask%0AGrounding.%20With%20all%20these%20techniques%2C%20our%20comprehensive%20approach%20culminates%20in%0AMagNet%20%28Mask-grounded%20Network%29%2C%20an%20architecture%20that%20significantly%20outperforms%0Aprior%20arts%20on%20three%20key%20benchmarks%20%28RefCOCO%2C%20RefCOCO%2B%20and%20G-Ref%29%2C%20demonstrating%0Aour%20method%27s%20effectiveness%20in%20addressing%20current%20limitations%20of%20RIS%20algorithms.%0AOur%20code%20and%20pre-trained%20weights%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12198v2&entry.124074799=Read"},
{"title": "V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster", "author": "Si Liu and Zihan Ding and Jiahui Fu and Hongyu Li and Siheng Chen and Shifeng Zhang and Xu Zhou", "abstract": "  The objective of the collaborative vehicle-to-everything perception task is\nto enhance the individual vehicle's perception capability through message\ncommunication among neighboring traffic agents. Previous methods focus on\nachieving optimal performance within bandwidth limitations and typically adopt\nBEV maps as the basic collaborative message units. However, we demonstrate that\ncollaboration with dense representations is plagued by object feature\ndestruction during message packing, inefficient message aggregation for\nlong-range collaboration, and implicit structure representation communication.\nTo tackle these issues, we introduce a brand new message unit, namely point\ncluster, designed to represent the scene sparsely with a combination of\nlow-level structure information and high-level semantic information. The point\ncluster inherently preserves object information while packing messages, with\nweak relevance to the collaboration range, and supports explicit structure\nmodeling. Building upon this representation, we propose a novel framework\nV2X-PC for collaborative perception. This framework includes a Point Cluster\nPacking (PCP) module to keep object feature and manage bandwidth through the\nmanipulation of cluster point numbers. As for effective message aggregation, we\npropose a Point Cluster Aggregation (PCA) module to match and merge point\nclusters associated with the same object. To further handle time latency and\npose errors encountered in real-world scenarios, we propose parameter-free\nsolutions that can adapt to different noisy levels without finetuning.\nExperiments on two widely recognized collaborative perception benchmarks\nshowcase the superior performance of our method compared to the previous\nstate-of-the-art approaches relying on BEV maps.\n", "link": "http://arxiv.org/abs/2403.16635v1", "date": "2024-03-25", "relevancy": 2.0591, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20V2X-PC%3A%20Vehicle-to-everything%20Collaborative%20Perception%20via%20Point%20Cluster&body=Title%3A%20V2X-PC%3A%20Vehicle-to-everything%20Collaborative%20Perception%20via%20Point%20Cluster%0AAuthor%3A%20Si%20Liu%20and%20Zihan%20Ding%20and%20Jiahui%20Fu%20and%20Hongyu%20Li%20and%20Siheng%20Chen%20and%20Shifeng%20Zhang%20and%20Xu%20Zhou%0AAbstract%3A%20%20%20The%20objective%20of%20the%20collaborative%20vehicle-to-everything%20perception%20task%20is%0Ato%20enhance%20the%20individual%20vehicle%27s%20perception%20capability%20through%20message%0Acommunication%20among%20neighboring%20traffic%20agents.%20Previous%20methods%20focus%20on%0Aachieving%20optimal%20performance%20within%20bandwidth%20limitations%20and%20typically%20adopt%0ABEV%20maps%20as%20the%20basic%20collaborative%20message%20units.%20However%2C%20we%20demonstrate%20that%0Acollaboration%20with%20dense%20representations%20is%20plagued%20by%20object%20feature%0Adestruction%20during%20message%20packing%2C%20inefficient%20message%20aggregation%20for%0Along-range%20collaboration%2C%20and%20implicit%20structure%20representation%20communication.%0ATo%20tackle%20these%20issues%2C%20we%20introduce%20a%20brand%20new%20message%20unit%2C%20namely%20point%0Acluster%2C%20designed%20to%20represent%20the%20scene%20sparsely%20with%20a%20combination%20of%0Alow-level%20structure%20information%20and%20high-level%20semantic%20information.%20The%20point%0Acluster%20inherently%20preserves%20object%20information%20while%20packing%20messages%2C%20with%0Aweak%20relevance%20to%20the%20collaboration%20range%2C%20and%20supports%20explicit%20structure%0Amodeling.%20Building%20upon%20this%20representation%2C%20we%20propose%20a%20novel%20framework%0AV2X-PC%20for%20collaborative%20perception.%20This%20framework%20includes%20a%20Point%20Cluster%0APacking%20%28PCP%29%20module%20to%20keep%20object%20feature%20and%20manage%20bandwidth%20through%20the%0Amanipulation%20of%20cluster%20point%20numbers.%20As%20for%20effective%20message%20aggregation%2C%20we%0Apropose%20a%20Point%20Cluster%20Aggregation%20%28PCA%29%20module%20to%20match%20and%20merge%20point%0Aclusters%20associated%20with%20the%20same%20object.%20To%20further%20handle%20time%20latency%20and%0Apose%20errors%20encountered%20in%20real-world%20scenarios%2C%20we%20propose%20parameter-free%0Asolutions%20that%20can%20adapt%20to%20different%20noisy%20levels%20without%20finetuning.%0AExperiments%20on%20two%20widely%20recognized%20collaborative%20perception%20benchmarks%0Ashowcase%20the%20superior%20performance%20of%20our%20method%20compared%20to%20the%20previous%0Astate-of-the-art%20approaches%20relying%20on%20BEV%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X-PC%3A%20Vehicle-to-everything%20Collaborative%20Perception%20via%20Point%20Cluster&entry.906535625=Si%20Liu%20and%20Zihan%20Ding%20and%20Jiahui%20Fu%20and%20Hongyu%20Li%20and%20Siheng%20Chen%20and%20Shifeng%20Zhang%20and%20Xu%20Zhou&entry.1292438233=%20%20The%20objective%20of%20the%20collaborative%20vehicle-to-everything%20perception%20task%20is%0Ato%20enhance%20the%20individual%20vehicle%27s%20perception%20capability%20through%20message%0Acommunication%20among%20neighboring%20traffic%20agents.%20Previous%20methods%20focus%20on%0Aachieving%20optimal%20performance%20within%20bandwidth%20limitations%20and%20typically%20adopt%0ABEV%20maps%20as%20the%20basic%20collaborative%20message%20units.%20However%2C%20we%20demonstrate%20that%0Acollaboration%20with%20dense%20representations%20is%20plagued%20by%20object%20feature%0Adestruction%20during%20message%20packing%2C%20inefficient%20message%20aggregation%20for%0Along-range%20collaboration%2C%20and%20implicit%20structure%20representation%20communication.%0ATo%20tackle%20these%20issues%2C%20we%20introduce%20a%20brand%20new%20message%20unit%2C%20namely%20point%0Acluster%2C%20designed%20to%20represent%20the%20scene%20sparsely%20with%20a%20combination%20of%0Alow-level%20structure%20information%20and%20high-level%20semantic%20information.%20The%20point%0Acluster%20inherently%20preserves%20object%20information%20while%20packing%20messages%2C%20with%0Aweak%20relevance%20to%20the%20collaboration%20range%2C%20and%20supports%20explicit%20structure%0Amodeling.%20Building%20upon%20this%20representation%2C%20we%20propose%20a%20novel%20framework%0AV2X-PC%20for%20collaborative%20perception.%20This%20framework%20includes%20a%20Point%20Cluster%0APacking%20%28PCP%29%20module%20to%20keep%20object%20feature%20and%20manage%20bandwidth%20through%20the%0Amanipulation%20of%20cluster%20point%20numbers.%20As%20for%20effective%20message%20aggregation%2C%20we%0Apropose%20a%20Point%20Cluster%20Aggregation%20%28PCA%29%20module%20to%20match%20and%20merge%20point%0Aclusters%20associated%20with%20the%20same%20object.%20To%20further%20handle%20time%20latency%20and%0Apose%20errors%20encountered%20in%20real-world%20scenarios%2C%20we%20propose%20parameter-free%0Asolutions%20that%20can%20adapt%20to%20different%20noisy%20levels%20without%20finetuning.%0AExperiments%20on%20two%20widely%20recognized%20collaborative%20perception%20benchmarks%0Ashowcase%20the%20superior%20performance%20of%20our%20method%20compared%20to%20the%20previous%0Astate-of-the-art%20approaches%20relying%20on%20BEV%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16635v1&entry.124074799=Read"},
{"title": "Improving Diffusion Models's Data-Corruption Resistance using Scheduled\n  Pseudo-Huber Loss", "author": "Artem Khrapov and Vadim Popov and Tasnima Sadekova and Assel Yermekova and Mikhail Kudinov", "abstract": "  Diffusion models are known to be vulnerable to outliers in training data. In\nthis paper we study an alternative diffusion loss function, which can preserve\nthe high quality of generated data like the original squared $L_{2}$ loss while\nat the same time being robust to outliers. We propose to use pseudo-Huber loss\nfunction with a time-dependent parameter to allow for the trade-off between\nrobustness on the most vulnerable early reverse-diffusion steps and fine\ndetails restoration on the final steps. We show that pseudo-Huber loss with the\ntime-dependent parameter exhibits better performance on corrupted datasets in\nboth image and audio domains. In addition, the loss function we propose can\npotentially help diffusion models to resist dataset corruption while not\nrequiring data filtering or purification compared to conventional training\nalgorithms.\n", "link": "http://arxiv.org/abs/2403.16728v1", "date": "2024-03-25", "relevancy": 2.0443, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Diffusion%20Models%27s%20Data-Corruption%20Resistance%20using%20Scheduled%0A%20%20Pseudo-Huber%20Loss&body=Title%3A%20Improving%20Diffusion%20Models%27s%20Data-Corruption%20Resistance%20using%20Scheduled%0A%20%20Pseudo-Huber%20Loss%0AAuthor%3A%20Artem%20Khrapov%20and%20Vadim%20Popov%20and%20Tasnima%20Sadekova%20and%20Assel%20Yermekova%20and%20Mikhail%20Kudinov%0AAbstract%3A%20%20%20Diffusion%20models%20are%20known%20to%20be%20vulnerable%20to%20outliers%20in%20training%20data.%20In%0Athis%20paper%20we%20study%20an%20alternative%20diffusion%20loss%20function%2C%20which%20can%20preserve%0Athe%20high%20quality%20of%20generated%20data%20like%20the%20original%20squared%20%24L_%7B2%7D%24%20loss%20while%0Aat%20the%20same%20time%20being%20robust%20to%20outliers.%20We%20propose%20to%20use%20pseudo-Huber%20loss%0Afunction%20with%20a%20time-dependent%20parameter%20to%20allow%20for%20the%20trade-off%20between%0Arobustness%20on%20the%20most%20vulnerable%20early%20reverse-diffusion%20steps%20and%20fine%0Adetails%20restoration%20on%20the%20final%20steps.%20We%20show%20that%20pseudo-Huber%20loss%20with%20the%0Atime-dependent%20parameter%20exhibits%20better%20performance%20on%20corrupted%20datasets%20in%0Aboth%20image%20and%20audio%20domains.%20In%20addition%2C%20the%20loss%20function%20we%20propose%20can%0Apotentially%20help%20diffusion%20models%20to%20resist%20dataset%20corruption%20while%20not%0Arequiring%20data%20filtering%20or%20purification%20compared%20to%20conventional%20training%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16728v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Diffusion%20Models%27s%20Data-Corruption%20Resistance%20using%20Scheduled%0A%20%20Pseudo-Huber%20Loss&entry.906535625=Artem%20Khrapov%20and%20Vadim%20Popov%20and%20Tasnima%20Sadekova%20and%20Assel%20Yermekova%20and%20Mikhail%20Kudinov&entry.1292438233=%20%20Diffusion%20models%20are%20known%20to%20be%20vulnerable%20to%20outliers%20in%20training%20data.%20In%0Athis%20paper%20we%20study%20an%20alternative%20diffusion%20loss%20function%2C%20which%20can%20preserve%0Athe%20high%20quality%20of%20generated%20data%20like%20the%20original%20squared%20%24L_%7B2%7D%24%20loss%20while%0Aat%20the%20same%20time%20being%20robust%20to%20outliers.%20We%20propose%20to%20use%20pseudo-Huber%20loss%0Afunction%20with%20a%20time-dependent%20parameter%20to%20allow%20for%20the%20trade-off%20between%0Arobustness%20on%20the%20most%20vulnerable%20early%20reverse-diffusion%20steps%20and%20fine%0Adetails%20restoration%20on%20the%20final%20steps.%20We%20show%20that%20pseudo-Huber%20loss%20with%20the%0Atime-dependent%20parameter%20exhibits%20better%20performance%20on%20corrupted%20datasets%20in%0Aboth%20image%20and%20audio%20domains.%20In%20addition%2C%20the%20loss%20function%20we%20propose%20can%0Apotentially%20help%20diffusion%20models%20to%20resist%20dataset%20corruption%20while%20not%0Arequiring%20data%20filtering%20or%20purification%20compared%20to%20conventional%20training%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16728v1&entry.124074799=Read"},
{"title": "Knowledge Distillation for Road Detection based on cross-model\n  Semi-Supervised Learning", "author": "Wanli Ma and Oktay Karakus and Paul L. Rosin", "abstract": "  The advancement of knowledge distillation has played a crucial role in\nenabling the transfer of knowledge from larger teacher models to smaller and\nmore efficient student models, and is particularly beneficial for online and\nresource-constrained applications. The effectiveness of the student model\nheavily relies on the quality of the distilled knowledge received from the\nteacher. Given the accessibility of unlabelled remote sensing data,\nsemi-supervised learning has become a prevalent strategy for enhancing model\nperformance. However, relying solely on semi-supervised learning with smaller\nmodels may be insufficient due to their limited capacity for feature\nextraction. This limitation restricts their ability to exploit training data.\nTo address this issue, we propose an integrated approach that combines\nknowledge distillation and semi-supervised learning methods. This hybrid\napproach leverages the robust capabilities of large models to effectively\nutilise large unlabelled data whilst subsequently providing the small student\nmodel with rich and informative features for enhancement. The proposed\nsemi-supervised learning-based knowledge distillation (SSLKD) approach\ndemonstrates a notable improvement in the performance of the student model, in\nthe application of road segmentation, surpassing the effectiveness of\ntraditional semi-supervised learning methods.\n", "link": "http://arxiv.org/abs/2402.05305v2", "date": "2024-03-25", "relevancy": 2.0031, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4951}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4903}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Distillation%20for%20Road%20Detection%20based%20on%20cross-model%0A%20%20Semi-Supervised%20Learning&body=Title%3A%20Knowledge%20Distillation%20for%20Road%20Detection%20based%20on%20cross-model%0A%20%20Semi-Supervised%20Learning%0AAuthor%3A%20Wanli%20Ma%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin%0AAbstract%3A%20%20%20The%20advancement%20of%20knowledge%20distillation%20has%20played%20a%20crucial%20role%20in%0Aenabling%20the%20transfer%20of%20knowledge%20from%20larger%20teacher%20models%20to%20smaller%20and%0Amore%20efficient%20student%20models%2C%20and%20is%20particularly%20beneficial%20for%20online%20and%0Aresource-constrained%20applications.%20The%20effectiveness%20of%20the%20student%20model%0Aheavily%20relies%20on%20the%20quality%20of%20the%20distilled%20knowledge%20received%20from%20the%0Ateacher.%20Given%20the%20accessibility%20of%20unlabelled%20remote%20sensing%20data%2C%0Asemi-supervised%20learning%20has%20become%20a%20prevalent%20strategy%20for%20enhancing%20model%0Aperformance.%20However%2C%20relying%20solely%20on%20semi-supervised%20learning%20with%20smaller%0Amodels%20may%20be%20insufficient%20due%20to%20their%20limited%20capacity%20for%20feature%0Aextraction.%20This%20limitation%20restricts%20their%20ability%20to%20exploit%20training%20data.%0ATo%20address%20this%20issue%2C%20we%20propose%20an%20integrated%20approach%20that%20combines%0Aknowledge%20distillation%20and%20semi-supervised%20learning%20methods.%20This%20hybrid%0Aapproach%20leverages%20the%20robust%20capabilities%20of%20large%20models%20to%20effectively%0Autilise%20large%20unlabelled%20data%20whilst%20subsequently%20providing%20the%20small%20student%0Amodel%20with%20rich%20and%20informative%20features%20for%20enhancement.%20The%20proposed%0Asemi-supervised%20learning-based%20knowledge%20distillation%20%28SSLKD%29%20approach%0Ademonstrates%20a%20notable%20improvement%20in%20the%20performance%20of%20the%20student%20model%2C%20in%0Athe%20application%20of%20road%20segmentation%2C%20surpassing%20the%20effectiveness%20of%0Atraditional%20semi-supervised%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05305v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Distillation%20for%20Road%20Detection%20based%20on%20cross-model%0A%20%20Semi-Supervised%20Learning&entry.906535625=Wanli%20Ma%20and%20Oktay%20Karakus%20and%20Paul%20L.%20Rosin&entry.1292438233=%20%20The%20advancement%20of%20knowledge%20distillation%20has%20played%20a%20crucial%20role%20in%0Aenabling%20the%20transfer%20of%20knowledge%20from%20larger%20teacher%20models%20to%20smaller%20and%0Amore%20efficient%20student%20models%2C%20and%20is%20particularly%20beneficial%20for%20online%20and%0Aresource-constrained%20applications.%20The%20effectiveness%20of%20the%20student%20model%0Aheavily%20relies%20on%20the%20quality%20of%20the%20distilled%20knowledge%20received%20from%20the%0Ateacher.%20Given%20the%20accessibility%20of%20unlabelled%20remote%20sensing%20data%2C%0Asemi-supervised%20learning%20has%20become%20a%20prevalent%20strategy%20for%20enhancing%20model%0Aperformance.%20However%2C%20relying%20solely%20on%20semi-supervised%20learning%20with%20smaller%0Amodels%20may%20be%20insufficient%20due%20to%20their%20limited%20capacity%20for%20feature%0Aextraction.%20This%20limitation%20restricts%20their%20ability%20to%20exploit%20training%20data.%0ATo%20address%20this%20issue%2C%20we%20propose%20an%20integrated%20approach%20that%20combines%0Aknowledge%20distillation%20and%20semi-supervised%20learning%20methods.%20This%20hybrid%0Aapproach%20leverages%20the%20robust%20capabilities%20of%20large%20models%20to%20effectively%0Autilise%20large%20unlabelled%20data%20whilst%20subsequently%20providing%20the%20small%20student%0Amodel%20with%20rich%20and%20informative%20features%20for%20enhancement.%20The%20proposed%0Asemi-supervised%20learning-based%20knowledge%20distillation%20%28SSLKD%29%20approach%0Ademonstrates%20a%20notable%20improvement%20in%20the%20performance%20of%20the%20student%20model%2C%20in%0Athe%20application%20of%20road%20segmentation%2C%20surpassing%20the%20effectiveness%20of%0Atraditional%20semi-supervised%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05305v2&entry.124074799=Read"},
{"title": "Clustering Propagation for Universal Medical Image Segmentation", "author": "Yuhang Ding and Liulei Li and Wenguan Wang and Yi Yang", "abstract": "  Prominent solutions for medical image segmentation are typically tailored for\nautomatic or interactive setups, posing challenges in facilitating progress\nachieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$\nnecessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both\ntraining time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$\nissues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$\nuniversal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$\nSlice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive\nsegmentation within a single model and one training session. Inspired by\nclustering-based segmentation techniques, S2VNet makes full use of the\nslice-wise structure of volumetric data by initializing cluster centers from\nthe cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This\nenables knowledge acquired from prior slices to assist in the segmentation of\nthe current slice, further efficiently bridging the communication between\nremote slices using mere 2D networks. Moreover, such a framework readily\naccommodates interactive segmentation with no architectural change, simply by\ninitializing centroids from user inputs. S2VNet distinguishes itself by swift\ninference speeds and reduced memory consumption compared to prevailing 3D\nsolutions. It can also handle multi-class interactions with each of them\nserving to initialize different centroids. Experiments on three benchmarks\ndemonstrate S2VNet surpasses task-specified solutions on both\nautomatic/interactive setups.\n", "link": "http://arxiv.org/abs/2403.16646v1", "date": "2024-03-25", "relevancy": 1.9997, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4852}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4697}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Clustering%20Propagation%20for%20Universal%20Medical%20Image%20Segmentation&body=Title%3A%20Clustering%20Propagation%20for%20Universal%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuhang%20Ding%20and%20Liulei%20Li%20and%20Wenguan%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Prominent%20solutions%20for%20medical%20image%20segmentation%20are%20typically%20tailored%20for%0Aautomatic%20or%20interactive%20setups%2C%20posing%20challenges%20in%20facilitating%20progress%0Aachieved%20in%20one%20task%20to%20another.%24_%7B%5C%21%7D%24%20This%24_%7B%5C%21%7D%24%20also%24_%7B%5C%21%7D%24%0Anecessitates%24_%7B%5C%21%7D%24%20separate%24_%7B%5C%21%7D%24%20models%20for%20each%20task%2C%20duplicating%20both%0Atraining%20time%20and%20parameters.%24_%7B%5C%21%7D%24%20To%24_%7B%5C%21%7D%24%20address%24_%7B%5C%21%7D%24%20above%24_%7B%5C%21%7D%24%0Aissues%2C%24_%7B%5C%21%7D%24%20we%24_%7B%5C%21%7D%24%20introduce%24_%7B%5C%21%7D%24%20S2VNet%2C%24_%7B%5C%21%7D%24%20a%24_%7B%5C%21%7D%24%0Auniversal%24_%7B%5C%21%7D%24%20framework%24_%7B%5C%21%7D%24%20that%24_%7B%5C%21%7D%24%20leverages%24_%7B%5C%21%7D%24%0ASlice-to-Volume%24_%7B%5C%21%7D%24%20propagation%24_%7B%5C%21%7D%24%20to%24_%7B%5C%21%7D%24%20unify%20automatic/interactive%0Asegmentation%20within%20a%20single%20model%20and%20one%20training%20session.%20Inspired%20by%0Aclustering-based%20segmentation%20techniques%2C%20S2VNet%20makes%20full%20use%20of%20the%0Aslice-wise%20structure%20of%20volumetric%20data%20by%20initializing%20cluster%20centers%20from%0Athe%20cluster%24_%7B%5C%21%7D%24%20results%24_%7B%5C%21%7D%24%20of%24_%7B%5C%21%7D%24%20previous%24_%7B%5C%21%7D%24%20slice.%24_%7B%5C%21%7D%24%20This%0Aenables%20knowledge%20acquired%20from%20prior%20slices%20to%20assist%20in%20the%20segmentation%20of%0Athe%20current%20slice%2C%20further%20efficiently%20bridging%20the%20communication%20between%0Aremote%20slices%20using%20mere%202D%20networks.%20Moreover%2C%20such%20a%20framework%20readily%0Aaccommodates%20interactive%20segmentation%20with%20no%20architectural%20change%2C%20simply%20by%0Ainitializing%20centroids%20from%20user%20inputs.%20S2VNet%20distinguishes%20itself%20by%20swift%0Ainference%20speeds%20and%20reduced%20memory%20consumption%20compared%20to%20prevailing%203D%0Asolutions.%20It%20can%20also%20handle%20multi-class%20interactions%20with%20each%20of%20them%0Aserving%20to%20initialize%20different%20centroids.%20Experiments%20on%20three%20benchmarks%0Ademonstrate%20S2VNet%20surpasses%20task-specified%20solutions%20on%20both%0Aautomatic/interactive%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16646v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20Propagation%20for%20Universal%20Medical%20Image%20Segmentation&entry.906535625=Yuhang%20Ding%20and%20Liulei%20Li%20and%20Wenguan%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20Prominent%20solutions%20for%20medical%20image%20segmentation%20are%20typically%20tailored%20for%0Aautomatic%20or%20interactive%20setups%2C%20posing%20challenges%20in%20facilitating%20progress%0Aachieved%20in%20one%20task%20to%20another.%24_%7B%5C%21%7D%24%20This%24_%7B%5C%21%7D%24%20also%24_%7B%5C%21%7D%24%0Anecessitates%24_%7B%5C%21%7D%24%20separate%24_%7B%5C%21%7D%24%20models%20for%20each%20task%2C%20duplicating%20both%0Atraining%20time%20and%20parameters.%24_%7B%5C%21%7D%24%20To%24_%7B%5C%21%7D%24%20address%24_%7B%5C%21%7D%24%20above%24_%7B%5C%21%7D%24%0Aissues%2C%24_%7B%5C%21%7D%24%20we%24_%7B%5C%21%7D%24%20introduce%24_%7B%5C%21%7D%24%20S2VNet%2C%24_%7B%5C%21%7D%24%20a%24_%7B%5C%21%7D%24%0Auniversal%24_%7B%5C%21%7D%24%20framework%24_%7B%5C%21%7D%24%20that%24_%7B%5C%21%7D%24%20leverages%24_%7B%5C%21%7D%24%0ASlice-to-Volume%24_%7B%5C%21%7D%24%20propagation%24_%7B%5C%21%7D%24%20to%24_%7B%5C%21%7D%24%20unify%20automatic/interactive%0Asegmentation%20within%20a%20single%20model%20and%20one%20training%20session.%20Inspired%20by%0Aclustering-based%20segmentation%20techniques%2C%20S2VNet%20makes%20full%20use%20of%20the%0Aslice-wise%20structure%20of%20volumetric%20data%20by%20initializing%20cluster%20centers%20from%0Athe%20cluster%24_%7B%5C%21%7D%24%20results%24_%7B%5C%21%7D%24%20of%24_%7B%5C%21%7D%24%20previous%24_%7B%5C%21%7D%24%20slice.%24_%7B%5C%21%7D%24%20This%0Aenables%20knowledge%20acquired%20from%20prior%20slices%20to%20assist%20in%20the%20segmentation%20of%0Athe%20current%20slice%2C%20further%20efficiently%20bridging%20the%20communication%20between%0Aremote%20slices%20using%20mere%202D%20networks.%20Moreover%2C%20such%20a%20framework%20readily%0Aaccommodates%20interactive%20segmentation%20with%20no%20architectural%20change%2C%20simply%20by%0Ainitializing%20centroids%20from%20user%20inputs.%20S2VNet%20distinguishes%20itself%20by%20swift%0Ainference%20speeds%20and%20reduced%20memory%20consumption%20compared%20to%20prevailing%203D%0Asolutions.%20It%20can%20also%20handle%20multi-class%20interactions%20with%20each%20of%20them%0Aserving%20to%20initialize%20different%20centroids.%20Experiments%20on%20three%20benchmarks%0Ademonstrate%20S2VNet%20surpasses%20task-specified%20solutions%20on%20both%0Aautomatic/interactive%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16646v1&entry.124074799=Read"},
{"title": "CLHA: A Simple yet Effective Contrastive Learning Framework for Human\n  Alignment", "author": "Feiteng Fang and Liang Zhu and Min Yang and Xi Feng and Jinchang Hou and Qixuan Zhao and Chengming Li and Xiping Hu and Ruifeng Xu", "abstract": "  Reinforcement learning from human feedback (RLHF) is a crucial technique in\naligning large language models (LLMs) with human preferences, ensuring these\nLLMs behave in beneficial and comprehensible ways to users. However, a\nlongstanding challenge in human alignment techniques based on reinforcement\nlearning lies in their inherent complexity and difficulty in training. To\naddress this challenge, we present a simple yet effective Contrastive Learning\nFramework for Human Alignment (CLHA) to align LLMs with human preferences\ndirectly. CLHA employs a novel rescoring strategy to evaluate the noise within\nthe data by considering its inherent quality and dynamically adjusting the\ntraining process. Simultaneously, CLHA utilizes pairwise contrastive loss and\nadaptive supervised fine-tuning loss to adaptively modify the likelihood of\ngenerating responses, ensuring enhanced alignment with human preferences. Using\nadvanced methods, CLHA surpasses other algorithms, showcasing superior\nperformance in terms of reward model scores, automatic evaluations, and human\nassessments on the widely used ``\\textit{Helpful and Harmless}'' dataset.\n", "link": "http://arxiv.org/abs/2403.16649v1", "date": "2024-03-25", "relevancy": 1.992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4696}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLHA%3A%20A%20Simple%20yet%20Effective%20Contrastive%20Learning%20Framework%20for%20Human%0A%20%20Alignment&body=Title%3A%20CLHA%3A%20A%20Simple%20yet%20Effective%20Contrastive%20Learning%20Framework%20for%20Human%0A%20%20Alignment%0AAuthor%3A%20Feiteng%20Fang%20and%20Liang%20Zhu%20and%20Min%20Yang%20and%20Xi%20Feng%20and%20Jinchang%20Hou%20and%20Qixuan%20Zhao%20and%20Chengming%20Li%20and%20Xiping%20Hu%20and%20Ruifeng%20Xu%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20a%20crucial%20technique%20in%0Aaligning%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%2C%20ensuring%20these%0ALLMs%20behave%20in%20beneficial%20and%20comprehensible%20ways%20to%20users.%20However%2C%20a%0Alongstanding%20challenge%20in%20human%20alignment%20techniques%20based%20on%20reinforcement%0Alearning%20lies%20in%20their%20inherent%20complexity%20and%20difficulty%20in%20training.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20a%20simple%20yet%20effective%20Contrastive%20Learning%0AFramework%20for%20Human%20Alignment%20%28CLHA%29%20to%20align%20LLMs%20with%20human%20preferences%0Adirectly.%20CLHA%20employs%20a%20novel%20rescoring%20strategy%20to%20evaluate%20the%20noise%20within%0Athe%20data%20by%20considering%20its%20inherent%20quality%20and%20dynamically%20adjusting%20the%0Atraining%20process.%20Simultaneously%2C%20CLHA%20utilizes%20pairwise%20contrastive%20loss%20and%0Aadaptive%20supervised%20fine-tuning%20loss%20to%20adaptively%20modify%20the%20likelihood%20of%0Agenerating%20responses%2C%20ensuring%20enhanced%20alignment%20with%20human%20preferences.%20Using%0Aadvanced%20methods%2C%20CLHA%20surpasses%20other%20algorithms%2C%20showcasing%20superior%0Aperformance%20in%20terms%20of%20reward%20model%20scores%2C%20automatic%20evaluations%2C%20and%20human%0Aassessments%20on%20the%20widely%20used%20%60%60%5Ctextit%7BHelpful%20and%20Harmless%7D%27%27%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLHA%3A%20A%20Simple%20yet%20Effective%20Contrastive%20Learning%20Framework%20for%20Human%0A%20%20Alignment&entry.906535625=Feiteng%20Fang%20and%20Liang%20Zhu%20and%20Min%20Yang%20and%20Xi%20Feng%20and%20Jinchang%20Hou%20and%20Qixuan%20Zhao%20and%20Chengming%20Li%20and%20Xiping%20Hu%20and%20Ruifeng%20Xu&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20is%20a%20crucial%20technique%20in%0Aaligning%20large%20language%20models%20%28LLMs%29%20with%20human%20preferences%2C%20ensuring%20these%0ALLMs%20behave%20in%20beneficial%20and%20comprehensible%20ways%20to%20users.%20However%2C%20a%0Alongstanding%20challenge%20in%20human%20alignment%20techniques%20based%20on%20reinforcement%0Alearning%20lies%20in%20their%20inherent%20complexity%20and%20difficulty%20in%20training.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20a%20simple%20yet%20effective%20Contrastive%20Learning%0AFramework%20for%20Human%20Alignment%20%28CLHA%29%20to%20align%20LLMs%20with%20human%20preferences%0Adirectly.%20CLHA%20employs%20a%20novel%20rescoring%20strategy%20to%20evaluate%20the%20noise%20within%0Athe%20data%20by%20considering%20its%20inherent%20quality%20and%20dynamically%20adjusting%20the%0Atraining%20process.%20Simultaneously%2C%20CLHA%20utilizes%20pairwise%20contrastive%20loss%20and%0Aadaptive%20supervised%20fine-tuning%20loss%20to%20adaptively%20modify%20the%20likelihood%20of%0Agenerating%20responses%2C%20ensuring%20enhanced%20alignment%20with%20human%20preferences.%20Using%0Aadvanced%20methods%2C%20CLHA%20surpasses%20other%20algorithms%2C%20showcasing%20superior%0Aperformance%20in%20terms%20of%20reward%20model%20scores%2C%20automatic%20evaluations%2C%20and%20human%0Aassessments%20on%20the%20widely%20used%20%60%60%5Ctextit%7BHelpful%20and%20Harmless%7D%27%27%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16649v1&entry.124074799=Read"},
{"title": "Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting", "author": "Busra Asan and Abdullah Akgul and Alper Unal and Melih Kandemir and Gozde Unal", "abstract": "  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n", "link": "http://arxiv.org/abs/2403.16612v1", "date": "2024-03-25", "relevancy": 1.954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4779}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting&body=Title%3A%20Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting%0AAuthor%3A%20Busra%20Asan%20and%20Abdullah%20Akgul%20and%20Alper%20Unal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal%0AAbstract%3A%20%20%20Seasonal%20forecasting%20is%20a%20crucial%20task%20when%20it%20comes%20to%20detecting%20the%20extreme%0Aheat%20and%20colds%20that%20occur%20due%20to%20climate%20change.%20Confidence%20in%20the%20predictions%0Ashould%20be%20reliable%20since%20a%20small%20increase%20in%20the%20temperatures%20in%20a%20year%20has%20a%0Abig%20impact%20on%20the%20world.%20Calibration%20of%20the%20neural%20networks%20provides%20a%20way%20to%0Aensure%20our%20confidence%20in%20the%20predictions.%20However%2C%20calibrating%20regression%0Amodels%20is%20an%20under-researched%20topic%2C%20especially%20in%20forecasters.%20We%20calibrate%20a%0AUNet%2B%2B%20based%20architecture%2C%20which%20was%20shown%20to%20outperform%20physics-based%20models%0Ain%20temperature%20anomalies.%20We%20show%20that%20with%20a%20slight%20trade-off%20between%0Aprediction%20error%20and%20calibration%20error%2C%20it%20is%20possible%20to%20get%20more%20reliable%20and%0Asharper%20forecasts.%20We%20believe%20that%20calibration%20should%20be%20an%20important%20part%20of%0Asafety-critical%20machine%20learning%20applications%20such%20as%20weather%20forecasters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20Bayesian%20UNet%2B%2B%20for%20Sub-Seasonal%20Forecasting&entry.906535625=Busra%20Asan%20and%20Abdullah%20Akgul%20and%20Alper%20Unal%20and%20Melih%20Kandemir%20and%20Gozde%20Unal&entry.1292438233=%20%20Seasonal%20forecasting%20is%20a%20crucial%20task%20when%20it%20comes%20to%20detecting%20the%20extreme%0Aheat%20and%20colds%20that%20occur%20due%20to%20climate%20change.%20Confidence%20in%20the%20predictions%0Ashould%20be%20reliable%20since%20a%20small%20increase%20in%20the%20temperatures%20in%20a%20year%20has%20a%0Abig%20impact%20on%20the%20world.%20Calibration%20of%20the%20neural%20networks%20provides%20a%20way%20to%0Aensure%20our%20confidence%20in%20the%20predictions.%20However%2C%20calibrating%20regression%0Amodels%20is%20an%20under-researched%20topic%2C%20especially%20in%20forecasters.%20We%20calibrate%20a%0AUNet%2B%2B%20based%20architecture%2C%20which%20was%20shown%20to%20outperform%20physics-based%20models%0Ain%20temperature%20anomalies.%20We%20show%20that%20with%20a%20slight%20trade-off%20between%0Aprediction%20error%20and%20calibration%20error%2C%20it%20is%20possible%20to%20get%20more%20reliable%20and%0Asharper%20forecasts.%20We%20believe%20that%20calibration%20should%20be%20an%20important%20part%20of%0Asafety-critical%20machine%20learning%20applications%20such%20as%20weather%20forecasters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16612v1&entry.124074799=Read"},
{"title": "BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based\n  Obstacle Avoidance", "author": "Hanna M\u00fcller and Victor Kartsch and Michele Magno and Luca Benini", "abstract": "  Nano-drones, distinguished by their agility, minimal weight, and\ncost-effectiveness, are particularly well-suited for exploration in confined,\ncluttered and narrow spaces. Recognizing transparent, highly reflective or\nabsorbing materials, such as glass and metallic surfaces is challenging, as\nclassical sensors, such as cameras or laser rangers, often do not detect them.\nInspired by bats, which can fly at high speeds in complete darkness with the\nhelp of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering\nsensor-deck employing a lightweight and low-power ultrasonic sensor for\nnano-drone autonomous navigation. This paper first provides insights about\nsensor characteristics, highlighting the influence of motor noise on the\nultrasound readings, then it introduces the results of extensive experimental\ntests for obstacle avoidance (OA) in a diverse environment. Results show that\n\\textit{BatDeck} allows exploration for a flight time of 8 minutes while\ncovering 136m on average before crash in a challenging environment with\ntransparent and reflective obstacles, proving the effectiveness of ultrasonic\nsensors for OA on nano-drones.\n", "link": "http://arxiv.org/abs/2403.16696v1", "date": "2024-03-25", "relevancy": 1.9069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4793}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BatDeck%3A%20Advancing%20Nano-drone%20Navigation%20with%20Low-power%20Ultrasound-based%0A%20%20Obstacle%20Avoidance&body=Title%3A%20BatDeck%3A%20Advancing%20Nano-drone%20Navigation%20with%20Low-power%20Ultrasound-based%0A%20%20Obstacle%20Avoidance%0AAuthor%3A%20Hanna%20M%C3%BCller%20and%20Victor%20Kartsch%20and%20Michele%20Magno%20and%20Luca%20Benini%0AAbstract%3A%20%20%20Nano-drones%2C%20distinguished%20by%20their%20agility%2C%20minimal%20weight%2C%20and%0Acost-effectiveness%2C%20are%20particularly%20well-suited%20for%20exploration%20in%20confined%2C%0Acluttered%20and%20narrow%20spaces.%20Recognizing%20transparent%2C%20highly%20reflective%20or%0Aabsorbing%20materials%2C%20such%20as%20glass%20and%20metallic%20surfaces%20is%20challenging%2C%20as%0Aclassical%20sensors%2C%20such%20as%20cameras%20or%20laser%20rangers%2C%20often%20do%20not%20detect%20them.%0AInspired%20by%20bats%2C%20which%20can%20fly%20at%20high%20speeds%20in%20complete%20darkness%20with%20the%0Ahelp%20of%20ultrasound%2C%20this%20paper%20introduces%20%5Ctextit%7BBatDeck%7D%2C%20a%20pioneering%0Asensor-deck%20employing%20a%20lightweight%20and%20low-power%20ultrasonic%20sensor%20for%0Anano-drone%20autonomous%20navigation.%20This%20paper%20first%20provides%20insights%20about%0Asensor%20characteristics%2C%20highlighting%20the%20influence%20of%20motor%20noise%20on%20the%0Aultrasound%20readings%2C%20then%20it%20introduces%20the%20results%20of%20extensive%20experimental%0Atests%20for%20obstacle%20avoidance%20%28OA%29%20in%20a%20diverse%20environment.%20Results%20show%20that%0A%5Ctextit%7BBatDeck%7D%20allows%20exploration%20for%20a%20flight%20time%20of%208%20minutes%20while%0Acovering%20136m%20on%20average%20before%20crash%20in%20a%20challenging%20environment%20with%0Atransparent%20and%20reflective%20obstacles%2C%20proving%20the%20effectiveness%20of%20ultrasonic%0Asensors%20for%20OA%20on%20nano-drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BatDeck%3A%20Advancing%20Nano-drone%20Navigation%20with%20Low-power%20Ultrasound-based%0A%20%20Obstacle%20Avoidance&entry.906535625=Hanna%20M%C3%BCller%20and%20Victor%20Kartsch%20and%20Michele%20Magno%20and%20Luca%20Benini&entry.1292438233=%20%20Nano-drones%2C%20distinguished%20by%20their%20agility%2C%20minimal%20weight%2C%20and%0Acost-effectiveness%2C%20are%20particularly%20well-suited%20for%20exploration%20in%20confined%2C%0Acluttered%20and%20narrow%20spaces.%20Recognizing%20transparent%2C%20highly%20reflective%20or%0Aabsorbing%20materials%2C%20such%20as%20glass%20and%20metallic%20surfaces%20is%20challenging%2C%20as%0Aclassical%20sensors%2C%20such%20as%20cameras%20or%20laser%20rangers%2C%20often%20do%20not%20detect%20them.%0AInspired%20by%20bats%2C%20which%20can%20fly%20at%20high%20speeds%20in%20complete%20darkness%20with%20the%0Ahelp%20of%20ultrasound%2C%20this%20paper%20introduces%20%5Ctextit%7BBatDeck%7D%2C%20a%20pioneering%0Asensor-deck%20employing%20a%20lightweight%20and%20low-power%20ultrasonic%20sensor%20for%0Anano-drone%20autonomous%20navigation.%20This%20paper%20first%20provides%20insights%20about%0Asensor%20characteristics%2C%20highlighting%20the%20influence%20of%20motor%20noise%20on%20the%0Aultrasound%20readings%2C%20then%20it%20introduces%20the%20results%20of%20extensive%20experimental%0Atests%20for%20obstacle%20avoidance%20%28OA%29%20in%20a%20diverse%20environment.%20Results%20show%20that%0A%5Ctextit%7BBatDeck%7D%20allows%20exploration%20for%20a%20flight%20time%20of%208%20minutes%20while%0Acovering%20136m%20on%20average%20before%20crash%20in%20a%20challenging%20environment%20with%0Atransparent%20and%20reflective%20obstacles%2C%20proving%20the%20effectiveness%20of%20ultrasonic%0Asensors%20for%20OA%20on%20nano-drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16696v1&entry.124074799=Read"},
{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "author": "Yuda Song and Zehao Sun and Xuanwu Yin", "abstract": "  Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.\n", "link": "http://arxiv.org/abs/2403.16627v1", "date": "2024-03-25", "relevancy": 1.8999, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6632}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6321}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5598}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SDXS%3A%20Real-Time%20One-Step%20Latent%20Diffusion%20Models%20with%20Image%20Conditions&body=Title%3A%20SDXS%3A%20Real-Time%20One-Step%20Latent%20Diffusion%20Models%20with%20Image%20Conditions%0AAuthor%3A%20Yuda%20Song%20and%20Zehao%20Sun%20and%20Xuanwu%20Yin%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20positioned%20them%20at%20the%20forefront%0Aof%20image%20generation.%20Despite%20their%20superior%20performance%2C%20diffusion%20models%20are%0Anot%20without%20drawbacks%3B%20they%20are%20characterized%20by%20complex%20architectures%20and%0Asubstantial%20computational%20demands%2C%20resulting%20in%20significant%20latency%20due%20to%0Atheir%20iterative%20sampling%20process.%20To%20mitigate%20these%20limitations%2C%20we%20introduce%20a%0Adual%20approach%20involving%20model%20miniaturization%20and%20a%20reduction%20in%20sampling%0Asteps%2C%20aimed%20at%20significantly%20decreasing%20model%20latency.%20Our%20methodology%0Aleverages%20knowledge%20distillation%20to%20streamline%20the%20U-Net%20and%20image%20decoder%0Aarchitectures%2C%20and%20introduces%20an%20innovative%20one-step%20DM%20training%20technique%20that%0Autilizes%20feature%20matching%20and%20score%20distillation.%20We%20present%20two%20models%2C%0ASDXS-512%20and%20SDXS-1024%2C%20achieving%20inference%20speeds%20of%20approximately%20100%20FPS%0A%2830x%20faster%20than%20SD%20v1.5%29%20and%2030%20FP%20%2860x%20faster%20than%20SDXL%29%20on%20a%20single%20GPU%2C%0Arespectively.%20Moreover%2C%20our%20training%20approach%20offers%20promising%20applications%20in%0Aimage-conditioned%20control%2C%20facilitating%20efficient%20image-to-image%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16627v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDXS%3A%20Real-Time%20One-Step%20Latent%20Diffusion%20Models%20with%20Image%20Conditions&entry.906535625=Yuda%20Song%20and%20Zehao%20Sun%20and%20Xuanwu%20Yin&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20positioned%20them%20at%20the%20forefront%0Aof%20image%20generation.%20Despite%20their%20superior%20performance%2C%20diffusion%20models%20are%0Anot%20without%20drawbacks%3B%20they%20are%20characterized%20by%20complex%20architectures%20and%0Asubstantial%20computational%20demands%2C%20resulting%20in%20significant%20latency%20due%20to%0Atheir%20iterative%20sampling%20process.%20To%20mitigate%20these%20limitations%2C%20we%20introduce%20a%0Adual%20approach%20involving%20model%20miniaturization%20and%20a%20reduction%20in%20sampling%0Asteps%2C%20aimed%20at%20significantly%20decreasing%20model%20latency.%20Our%20methodology%0Aleverages%20knowledge%20distillation%20to%20streamline%20the%20U-Net%20and%20image%20decoder%0Aarchitectures%2C%20and%20introduces%20an%20innovative%20one-step%20DM%20training%20technique%20that%0Autilizes%20feature%20matching%20and%20score%20distillation.%20We%20present%20two%20models%2C%0ASDXS-512%20and%20SDXS-1024%2C%20achieving%20inference%20speeds%20of%20approximately%20100%20FPS%0A%2830x%20faster%20than%20SD%20v1.5%29%20and%2030%20FP%20%2860x%20faster%20than%20SDXL%29%20on%20a%20single%20GPU%2C%0Arespectively.%20Moreover%2C%20our%20training%20approach%20offers%20promising%20applications%20in%0Aimage-conditioned%20control%2C%20facilitating%20efficient%20image-to-image%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16627v1&entry.124074799=Read"},
{"title": "DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks", "author": "Dominik M\u00fcller and Philip Meyer and Lukas Rentschler and Robin Manz and Jonas B\u00e4cker and Samantha Cramer and Christoph Wengenmayr and Bruno M\u00e4rkl and Ralf Huss and I\u00f1aki Soto-Rey and Johannes Raffler", "abstract": "  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n", "link": "http://arxiv.org/abs/2403.16678v1", "date": "2024-03-25", "relevancy": 1.8867, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4635}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeepGleason%3A%20a%20System%20for%20Automated%20Gleason%20Grading%20of%20Prostate%20Cancer%0A%20%20using%20Deep%20Neural%20Networks&body=Title%3A%20DeepGleason%3A%20a%20System%20for%20Automated%20Gleason%20Grading%20of%20Prostate%20Cancer%0A%20%20using%20Deep%20Neural%20Networks%0AAuthor%3A%20Dominik%20M%C3%BCller%20and%20Philip%20Meyer%20and%20Lukas%20Rentschler%20and%20Robin%20Manz%20and%20Jonas%20B%C3%A4cker%20and%20Samantha%20Cramer%20and%20Christoph%20Wengenmayr%20and%20Bruno%20M%C3%A4rkl%20and%20Ralf%20Huss%20and%20I%C3%B1aki%20Soto-Rey%20and%20Johannes%20Raffler%0AAbstract%3A%20%20%20Advances%20in%20digital%20pathology%20and%20artificial%20intelligence%20%28AI%29%20offer%0Apromising%20opportunities%20for%20clinical%20decision%20support%20and%20enhancing%20diagnostic%0Aworkflows.%20Previous%20studies%20already%20demonstrated%20AI%27s%20potential%20for%20automated%0AGleason%20grading%2C%20but%20lack%20state-of-the-art%20methodology%20and%20model%20reusability.%0ATo%20address%20this%20issue%2C%20we%20propose%20DeepGleason%3A%20an%20open-source%20deep%20neural%0Anetwork%20based%20image%20classification%20system%20for%20automated%20Gleason%20grading%20using%0Awhole-slide%20histopathology%20images%20from%20prostate%20tissue%20sections.%20Implemented%0Awith%20the%20standardized%20AUCMEDI%20framework%2C%20our%20tool%20employs%20a%20tile-wise%0Aclassification%20approach%20utilizing%20fine-tuned%20image%20preprocessing%20techniques%20in%0Acombination%20with%20a%20ConvNeXt%20architecture%20which%20was%20compared%20to%20various%0Astate-of-the-art%20architectures.%20The%20neural%20network%20model%20was%20trained%20and%0Avalidated%20on%20an%20in-house%20dataset%20of%2034%2C264%20annotated%20tiles%20from%20369%20prostate%0Acarcinoma%20slides.%20We%20demonstrated%20that%20DeepGleason%20is%20capable%20of%20highly%0Aaccurate%20and%20reliable%20Gleason%20grading%20with%20a%20macro-averaged%20F1-score%20of%200.806%2C%0AAUC%20of%200.991%2C%20and%20Accuracy%20of%200.974.%20The%20internal%20architecture%20comparison%0Arevealed%20that%20the%20ConvNeXt%20model%20was%20superior%20performance-wise%20on%20our%20dataset%0Ato%20established%20and%20other%20modern%20architectures%20like%20transformers.%20Furthermore%2C%0Awe%20were%20able%20to%20outperform%20the%20current%20state-of-the-art%20in%20tile-wise%0Afine-classification%20with%20a%20sensitivity%20and%20specificity%20of%200.94%20and%200.98%20for%0Abenign%20vs%20malignant%20detection%20as%20well%20as%20of%200.91%20and%200.75%20for%20Gleason%203%20vs%0AGleason%204%20%26%205%20classification%2C%20respectively.%20Our%20tool%20contributes%20to%20the%20wider%0Aadoption%20of%20AI-based%20Gleason%20grading%20within%20the%20research%20community%20and%20paves%0Athe%20way%20for%20broader%20clinical%20application%20of%20deep%20learning%20models%20in%20digital%0Apathology.%20DeepGleason%20is%20open-source%20and%20publicly%20available%20for%20research%0Aapplication%20in%20the%20following%20Git%20repository%3A%0Ahttps%3A//github.com/frankkramer-lab/DeepGleason.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16678v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepGleason%3A%20a%20System%20for%20Automated%20Gleason%20Grading%20of%20Prostate%20Cancer%0A%20%20using%20Deep%20Neural%20Networks&entry.906535625=Dominik%20M%C3%BCller%20and%20Philip%20Meyer%20and%20Lukas%20Rentschler%20and%20Robin%20Manz%20and%20Jonas%20B%C3%A4cker%20and%20Samantha%20Cramer%20and%20Christoph%20Wengenmayr%20and%20Bruno%20M%C3%A4rkl%20and%20Ralf%20Huss%20and%20I%C3%B1aki%20Soto-Rey%20and%20Johannes%20Raffler&entry.1292438233=%20%20Advances%20in%20digital%20pathology%20and%20artificial%20intelligence%20%28AI%29%20offer%0Apromising%20opportunities%20for%20clinical%20decision%20support%20and%20enhancing%20diagnostic%0Aworkflows.%20Previous%20studies%20already%20demonstrated%20AI%27s%20potential%20for%20automated%0AGleason%20grading%2C%20but%20lack%20state-of-the-art%20methodology%20and%20model%20reusability.%0ATo%20address%20this%20issue%2C%20we%20propose%20DeepGleason%3A%20an%20open-source%20deep%20neural%0Anetwork%20based%20image%20classification%20system%20for%20automated%20Gleason%20grading%20using%0Awhole-slide%20histopathology%20images%20from%20prostate%20tissue%20sections.%20Implemented%0Awith%20the%20standardized%20AUCMEDI%20framework%2C%20our%20tool%20employs%20a%20tile-wise%0Aclassification%20approach%20utilizing%20fine-tuned%20image%20preprocessing%20techniques%20in%0Acombination%20with%20a%20ConvNeXt%20architecture%20which%20was%20compared%20to%20various%0Astate-of-the-art%20architectures.%20The%20neural%20network%20model%20was%20trained%20and%0Avalidated%20on%20an%20in-house%20dataset%20of%2034%2C264%20annotated%20tiles%20from%20369%20prostate%0Acarcinoma%20slides.%20We%20demonstrated%20that%20DeepGleason%20is%20capable%20of%20highly%0Aaccurate%20and%20reliable%20Gleason%20grading%20with%20a%20macro-averaged%20F1-score%20of%200.806%2C%0AAUC%20of%200.991%2C%20and%20Accuracy%20of%200.974.%20The%20internal%20architecture%20comparison%0Arevealed%20that%20the%20ConvNeXt%20model%20was%20superior%20performance-wise%20on%20our%20dataset%0Ato%20established%20and%20other%20modern%20architectures%20like%20transformers.%20Furthermore%2C%0Awe%20were%20able%20to%20outperform%20the%20current%20state-of-the-art%20in%20tile-wise%0Afine-classification%20with%20a%20sensitivity%20and%20specificity%20of%200.94%20and%200.98%20for%0Abenign%20vs%20malignant%20detection%20as%20well%20as%20of%200.91%20and%200.75%20for%20Gleason%203%20vs%0AGleason%204%20%26%205%20classification%2C%20respectively.%20Our%20tool%20contributes%20to%20the%20wider%0Aadoption%20of%20AI-based%20Gleason%20grading%20within%20the%20research%20community%20and%20paves%0Athe%20way%20for%20broader%20clinical%20application%20of%20deep%20learning%20models%20in%20digital%0Apathology.%20DeepGleason%20is%20open-source%20and%20publicly%20available%20for%20research%0Aapplication%20in%20the%20following%20Git%20repository%3A%0Ahttps%3A//github.com/frankkramer-lab/DeepGleason.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16678v1&entry.124074799=Read"},
{"title": "Distributed collaborative anomalous sound detection by embedding sharing", "author": "Kota Dohi and Yohei Kawaguchi", "abstract": "  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n", "link": "http://arxiv.org/abs/2403.16610v1", "date": "2024-03-25", "relevancy": 1.8566, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4919}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distributed%20collaborative%20anomalous%20sound%20detection%20by%20embedding%20sharing&body=Title%3A%20Distributed%20collaborative%20anomalous%20sound%20detection%20by%20embedding%20sharing%0AAuthor%3A%20Kota%20Dohi%20and%20Yohei%20Kawaguchi%0AAbstract%3A%20%20%20To%20develop%20a%20machine%20sound%20monitoring%20system%2C%20a%20method%20for%20detecting%0Aanomalous%20sound%20is%20proposed.%20In%20this%20paper%2C%20we%20explore%20a%20method%20for%20multiple%0Aclients%20to%20collaboratively%20learn%20an%20anomalous%20sound%20detection%20model%20while%0Akeeping%20their%20raw%20data%20private%20from%20each%20other.%20In%20the%20context%20of%20industrial%0Amachine%20anomalous%20sound%20detection%2C%20each%20client%20possesses%20data%20from%20different%0Amachines%20or%20different%20operational%20states%2C%20making%20it%20challenging%20to%20learn%0Athrough%20federated%20learning%20or%20split%20learning.%20In%20our%20proposed%20method%2C%20each%0Aclient%20calculates%20embeddings%20using%20a%20common%20pre-trained%20model%20developed%20for%0Asound%20data%20classification%2C%20and%20these%20calculated%20embeddings%20are%20aggregated%20on%0Athe%20server%20to%20perform%20anomalous%20sound%20detection%20through%20outlier%20exposure.%0AExperiments%20showed%20that%20our%20proposed%20method%20improves%20the%20AUC%20of%20anomalous%20sound%0Adetection%20by%20an%20average%20of%206.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16610v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20collaborative%20anomalous%20sound%20detection%20by%20embedding%20sharing&entry.906535625=Kota%20Dohi%20and%20Yohei%20Kawaguchi&entry.1292438233=%20%20To%20develop%20a%20machine%20sound%20monitoring%20system%2C%20a%20method%20for%20detecting%0Aanomalous%20sound%20is%20proposed.%20In%20this%20paper%2C%20we%20explore%20a%20method%20for%20multiple%0Aclients%20to%20collaboratively%20learn%20an%20anomalous%20sound%20detection%20model%20while%0Akeeping%20their%20raw%20data%20private%20from%20each%20other.%20In%20the%20context%20of%20industrial%0Amachine%20anomalous%20sound%20detection%2C%20each%20client%20possesses%20data%20from%20different%0Amachines%20or%20different%20operational%20states%2C%20making%20it%20challenging%20to%20learn%0Athrough%20federated%20learning%20or%20split%20learning.%20In%20our%20proposed%20method%2C%20each%0Aclient%20calculates%20embeddings%20using%20a%20common%20pre-trained%20model%20developed%20for%0Asound%20data%20classification%2C%20and%20these%20calculated%20embeddings%20are%20aggregated%20on%0Athe%20server%20to%20perform%20anomalous%20sound%20detection%20through%20outlier%20exposure.%0AExperiments%20showed%20that%20our%20proposed%20method%20improves%20the%20AUC%20of%20anomalous%20sound%0Adetection%20by%20an%20average%20of%206.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16610v1&entry.124074799=Read"},
{"title": "A Novel Loss Function-based Support Vector Machine for Binary\n  Classification", "author": "Yan Li and Liping Zhang", "abstract": "  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2403.16654v1", "date": "2024-03-25", "relevancy": 1.8058, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4359}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Loss%20Function-based%20Support%20Vector%20Machine%20for%20Binary%0A%20%20Classification&body=Title%3A%20A%20Novel%20Loss%20Function-based%20Support%20Vector%20Machine%20for%20Binary%0A%20%20Classification%0AAuthor%3A%20Yan%20Li%20and%20Liping%20Zhang%0AAbstract%3A%20%20%20The%20previous%20support%20vector%20machine%28SVM%29%20including%20%240/1%24%20loss%20SVM%2C%20hinge%20loss%0ASVM%2C%20ramp%20loss%20SVM%2C%20truncated%20pinball%20loss%20SVM%2C%20and%20others%2C%20overlooked%20the%0Adegree%20of%20penalty%20for%20the%20correctly%20classified%20samples%20within%20the%20margin.%20This%0Aoversight%20affects%20the%20generalization%20ability%20of%20the%20SVM%20classifier%20to%20some%0Aextent.%20To%20address%20this%20limitation%2C%20from%20the%20perspective%20of%20confidence%20margin%2C%0Awe%20propose%20a%20novel%20Slide%20loss%20function%20%28%24%5Cell_s%24%29%20to%20construct%20the%20support%0Avector%20machine%20classifier%28%24%5Cell_s%24-SVM%29.%20By%20introducing%20the%20concept%20of%20proximal%0Astationary%20point%2C%20and%20utilizing%20the%20property%20of%20Lipschitz%20continuity%2C%20we%20derive%0Athe%20first-order%20optimality%20conditions%20for%20%24%5Cell_s%24-SVM.%20Based%20on%20this%2C%20we%0Adefine%20the%20%24%5Cell_s%24%20support%20vectors%20and%20working%20set%20of%20%24%5Cell_s%24-SVM.%20To%0Aefficiently%20handle%20%24%5Cell_s%24-SVM%2C%20we%20devise%20a%20fast%20alternating%20direction%20method%0Aof%20multipliers%20with%20the%20working%20set%20%28%24%5Cell_s%24-ADMM%29%2C%20and%20provide%20the%0Aconvergence%20analysis.%20The%20numerical%20experiments%20on%20real%20world%20datasets%20confirm%0Athe%20robustness%20and%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16654v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Loss%20Function-based%20Support%20Vector%20Machine%20for%20Binary%0A%20%20Classification&entry.906535625=Yan%20Li%20and%20Liping%20Zhang&entry.1292438233=%20%20The%20previous%20support%20vector%20machine%28SVM%29%20including%20%240/1%24%20loss%20SVM%2C%20hinge%20loss%0ASVM%2C%20ramp%20loss%20SVM%2C%20truncated%20pinball%20loss%20SVM%2C%20and%20others%2C%20overlooked%20the%0Adegree%20of%20penalty%20for%20the%20correctly%20classified%20samples%20within%20the%20margin.%20This%0Aoversight%20affects%20the%20generalization%20ability%20of%20the%20SVM%20classifier%20to%20some%0Aextent.%20To%20address%20this%20limitation%2C%20from%20the%20perspective%20of%20confidence%20margin%2C%0Awe%20propose%20a%20novel%20Slide%20loss%20function%20%28%24%5Cell_s%24%29%20to%20construct%20the%20support%0Avector%20machine%20classifier%28%24%5Cell_s%24-SVM%29.%20By%20introducing%20the%20concept%20of%20proximal%0Astationary%20point%2C%20and%20utilizing%20the%20property%20of%20Lipschitz%20continuity%2C%20we%20derive%0Athe%20first-order%20optimality%20conditions%20for%20%24%5Cell_s%24-SVM.%20Based%20on%20this%2C%20we%0Adefine%20the%20%24%5Cell_s%24%20support%20vectors%20and%20working%20set%20of%20%24%5Cell_s%24-SVM.%20To%0Aefficiently%20handle%20%24%5Cell_s%24-SVM%2C%20we%20devise%20a%20fast%20alternating%20direction%20method%0Aof%20multipliers%20with%20the%20working%20set%20%28%24%5Cell_s%24-ADMM%29%2C%20and%20provide%20the%0Aconvergence%20analysis.%20The%20numerical%20experiments%20on%20real%20world%20datasets%20confirm%0Athe%20robustness%20and%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16654v1&entry.124074799=Read"},
{"title": "Investigation of the effectiveness of applying ChatGPT in Dialogic\n  Teaching Using Electroencephalography", "author": "Jiayue Zhang and Yiheng Liu and Wenqi Cai and Yali Peng and Senqing Qi and Taotao Long and Bao Ge", "abstract": "  In recent years, the rapid development of artificial intelligence technology,\nespecially the emergence of large language models (LLMs) such as ChatGPT, has\npresented significant prospects for application in the field of education. LLMs\npossess the capability to interpret knowledge, answer questions, and consider\ncontext, thus providing support for dialogic teaching to students. Therefore,\nan examination of the capacity of LLMs to effectively fulfill instructional\nroles, thereby facilitating student learning akin to human educators within\ndialogic teaching scenarios, is an exceptionally valuable research topic. This\nresearch recruited 34 undergraduate students as participants, who were randomly\ndivided into two groups. The experimental group engaged in dialogic teaching\nusing ChatGPT, while the control group interacted with human teachers. Both\ngroups learned the histogram equalization unit in the information-related\ncourse \"Digital Image Processing\". The research findings show comparable scores\nbetween the two groups on the retention test. However, students who engaged in\ndialogue with ChatGPT exhibited lower performance on the transfer test.\nElectroencephalography data revealed that students who interacted with ChatGPT\nexhibited higher levels of cognitive activity, suggesting that ChatGPT could\nhelp students establish a knowledge foundation and stimulate cognitive\nactivity. However, its strengths on promoting students. knowledge application\nand creativity were insignificant. Based upon the research findings, it is\nevident that ChatGPT cannot fully excel in fulfilling teaching tasks in the\ndialogue teaching in information related courses. Combining ChatGPT with\ntraditional human teachers might be a more ideal approach. The synergistic use\nof both can provide students with more comprehensive learning support, thus\ncontributing to enhancing the quality of teaching.\n", "link": "http://arxiv.org/abs/2403.16687v1", "date": "2024-03-25", "relevancy": 1.7698, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4419}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4326}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography&body=Title%3A%20Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography%0AAuthor%3A%20Jiayue%20Zhang%20and%20Yiheng%20Liu%20and%20Wenqi%20Cai%20and%20Yali%20Peng%20and%20Senqing%20Qi%20and%20Taotao%20Long%20and%20Bao%20Ge%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20artificial%20intelligence%20technology%2C%0Aespecially%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%2C%20has%0Apresented%20significant%20prospects%20for%20application%20in%20the%20field%20of%20education.%20LLMs%0Apossess%20the%20capability%20to%20interpret%20knowledge%2C%20answer%20questions%2C%20and%20consider%0Acontext%2C%20thus%20providing%20support%20for%20dialogic%20teaching%20to%20students.%20Therefore%2C%0Aan%20examination%20of%20the%20capacity%20of%20LLMs%20to%20effectively%20fulfill%20instructional%0Aroles%2C%20thereby%20facilitating%20student%20learning%20akin%20to%20human%20educators%20within%0Adialogic%20teaching%20scenarios%2C%20is%20an%20exceptionally%20valuable%20research%20topic.%20This%0Aresearch%20recruited%2034%20undergraduate%20students%20as%20participants%2C%20who%20were%20randomly%0Adivided%20into%20two%20groups.%20The%20experimental%20group%20engaged%20in%20dialogic%20teaching%0Ausing%20ChatGPT%2C%20while%20the%20control%20group%20interacted%20with%20human%20teachers.%20Both%0Agroups%20learned%20the%20histogram%20equalization%20unit%20in%20the%20information-related%0Acourse%20%22Digital%20Image%20Processing%22.%20The%20research%20findings%20show%20comparable%20scores%0Abetween%20the%20two%20groups%20on%20the%20retention%20test.%20However%2C%20students%20who%20engaged%20in%0Adialogue%20with%20ChatGPT%20exhibited%20lower%20performance%20on%20the%20transfer%20test.%0AElectroencephalography%20data%20revealed%20that%20students%20who%20interacted%20with%20ChatGPT%0Aexhibited%20higher%20levels%20of%20cognitive%20activity%2C%20suggesting%20that%20ChatGPT%20could%0Ahelp%20students%20establish%20a%20knowledge%20foundation%20and%20stimulate%20cognitive%0Aactivity.%20However%2C%20its%20strengths%20on%20promoting%20students.%20knowledge%20application%0Aand%20creativity%20were%20insignificant.%20Based%20upon%20the%20research%20findings%2C%20it%20is%0Aevident%20that%20ChatGPT%20cannot%20fully%20excel%20in%20fulfilling%20teaching%20tasks%20in%20the%0Adialogue%20teaching%20in%20information%20related%20courses.%20Combining%20ChatGPT%20with%0Atraditional%20human%20teachers%20might%20be%20a%20more%20ideal%20approach.%20The%20synergistic%20use%0Aof%20both%20can%20provide%20students%20with%20more%20comprehensive%20learning%20support%2C%20thus%0Acontributing%20to%20enhancing%20the%20quality%20of%20teaching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20of%20the%20effectiveness%20of%20applying%20ChatGPT%20in%20Dialogic%0A%20%20Teaching%20Using%20Electroencephalography&entry.906535625=Jiayue%20Zhang%20and%20Yiheng%20Liu%20and%20Wenqi%20Cai%20and%20Yali%20Peng%20and%20Senqing%20Qi%20and%20Taotao%20Long%20and%20Bao%20Ge&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20artificial%20intelligence%20technology%2C%0Aespecially%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%20such%20as%20ChatGPT%2C%20has%0Apresented%20significant%20prospects%20for%20application%20in%20the%20field%20of%20education.%20LLMs%0Apossess%20the%20capability%20to%20interpret%20knowledge%2C%20answer%20questions%2C%20and%20consider%0Acontext%2C%20thus%20providing%20support%20for%20dialogic%20teaching%20to%20students.%20Therefore%2C%0Aan%20examination%20of%20the%20capacity%20of%20LLMs%20to%20effectively%20fulfill%20instructional%0Aroles%2C%20thereby%20facilitating%20student%20learning%20akin%20to%20human%20educators%20within%0Adialogic%20teaching%20scenarios%2C%20is%20an%20exceptionally%20valuable%20research%20topic.%20This%0Aresearch%20recruited%2034%20undergraduate%20students%20as%20participants%2C%20who%20were%20randomly%0Adivided%20into%20two%20groups.%20The%20experimental%20group%20engaged%20in%20dialogic%20teaching%0Ausing%20ChatGPT%2C%20while%20the%20control%20group%20interacted%20with%20human%20teachers.%20Both%0Agroups%20learned%20the%20histogram%20equalization%20unit%20in%20the%20information-related%0Acourse%20%22Digital%20Image%20Processing%22.%20The%20research%20findings%20show%20comparable%20scores%0Abetween%20the%20two%20groups%20on%20the%20retention%20test.%20However%2C%20students%20who%20engaged%20in%0Adialogue%20with%20ChatGPT%20exhibited%20lower%20performance%20on%20the%20transfer%20test.%0AElectroencephalography%20data%20revealed%20that%20students%20who%20interacted%20with%20ChatGPT%0Aexhibited%20higher%20levels%20of%20cognitive%20activity%2C%20suggesting%20that%20ChatGPT%20could%0Ahelp%20students%20establish%20a%20knowledge%20foundation%20and%20stimulate%20cognitive%0Aactivity.%20However%2C%20its%20strengths%20on%20promoting%20students.%20knowledge%20application%0Aand%20creativity%20were%20insignificant.%20Based%20upon%20the%20research%20findings%2C%20it%20is%0Aevident%20that%20ChatGPT%20cannot%20fully%20excel%20in%20fulfilling%20teaching%20tasks%20in%20the%0Adialogue%20teaching%20in%20information%20related%20courses.%20Combining%20ChatGPT%20with%0Atraditional%20human%20teachers%20might%20be%20a%20more%20ideal%20approach.%20The%20synergistic%20use%0Aof%20both%20can%20provide%20students%20with%20more%20comprehensive%20learning%20support%2C%20thus%0Acontributing%20to%20enhancing%20the%20quality%20of%20teaching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16687v1&entry.124074799=Read"},
{"title": "The Implications of Decentralization in Blockchained Federated Learning:\n  Evaluating the Impact of Model Staleness and Inconsistencies", "author": "Francesc Wilhelmi and Nima Afraz and Elia Guerra and Paolo Dini", "abstract": "  Blockchain promises to enhance distributed machine learning (ML) approaches\nsuch as federated learning (FL) by providing further decentralization,\nsecurity, immutability, and trust, which are key properties for enabling\ncollaborative intelligence in next-generation applications. Nonetheless, the\nintrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads\nto an uncharted setting for FL, whereby the concepts of FL round and global\nmodel become meaningless, as devices' synchronization is lost without the\nfigure of a central orchestrating server. In this paper, we study the practical\nimplications of outsourcing the orchestration of FL to a democratic setting\nsuch as in a blockchain. In particular, we focus on the effects that model\nstaleness and inconsistencies, endorsed by blockchains' modus operandi, have on\nthe training procedure held by FL devices asynchronously. Using simulation, we\nevaluate the blockchained FL operation by applying two different ML models\n(ranging from low to high complexity) on the well-known MNIST and CIFAR-10\ndatasets, respectively, and focus on the accuracy and timeliness of the\nsolutions. Our results show the high impact of model inconsistencies on the\naccuracy of the models (up to a ~35% decrease in prediction accuracy), which\nunderscores the importance of properly designing blockchain systems based on\nthe characteristics of the underlying FL application.\n", "link": "http://arxiv.org/abs/2310.07471v2", "date": "2024-03-25", "relevancy": 1.7249, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.423}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Implications%20of%20Decentralization%20in%20Blockchained%20Federated%20Learning%3A%0A%20%20Evaluating%20the%20Impact%20of%20Model%20Staleness%20and%20Inconsistencies&body=Title%3A%20The%20Implications%20of%20Decentralization%20in%20Blockchained%20Federated%20Learning%3A%0A%20%20Evaluating%20the%20Impact%20of%20Model%20Staleness%20and%20Inconsistencies%0AAuthor%3A%20Francesc%20Wilhelmi%20and%20Nima%20Afraz%20and%20Elia%20Guerra%20and%20Paolo%20Dini%0AAbstract%3A%20%20%20Blockchain%20promises%20to%20enhance%20distributed%20machine%20learning%20%28ML%29%20approaches%0Asuch%20as%20federated%20learning%20%28FL%29%20by%20providing%20further%20decentralization%2C%0Asecurity%2C%20immutability%2C%20and%20trust%2C%20which%20are%20key%20properties%20for%20enabling%0Acollaborative%20intelligence%20in%20next-generation%20applications.%20Nonetheless%2C%20the%0Aintrinsic%20decentralized%20operation%20of%20peer-to-peer%20%28P2P%29%20blockchain%20nodes%20leads%0Ato%20an%20uncharted%20setting%20for%20FL%2C%20whereby%20the%20concepts%20of%20FL%20round%20and%20global%0Amodel%20become%20meaningless%2C%20as%20devices%27%20synchronization%20is%20lost%20without%20the%0Afigure%20of%20a%20central%20orchestrating%20server.%20In%20this%20paper%2C%20we%20study%20the%20practical%0Aimplications%20of%20outsourcing%20the%20orchestration%20of%20FL%20to%20a%20democratic%20setting%0Asuch%20as%20in%20a%20blockchain.%20In%20particular%2C%20we%20focus%20on%20the%20effects%20that%20model%0Astaleness%20and%20inconsistencies%2C%20endorsed%20by%20blockchains%27%20modus%20operandi%2C%20have%20on%0Athe%20training%20procedure%20held%20by%20FL%20devices%20asynchronously.%20Using%20simulation%2C%20we%0Aevaluate%20the%20blockchained%20FL%20operation%20by%20applying%20two%20different%20ML%20models%0A%28ranging%20from%20low%20to%20high%20complexity%29%20on%20the%20well-known%20MNIST%20and%20CIFAR-10%0Adatasets%2C%20respectively%2C%20and%20focus%20on%20the%20accuracy%20and%20timeliness%20of%20the%0Asolutions.%20Our%20results%20show%20the%20high%20impact%20of%20model%20inconsistencies%20on%20the%0Aaccuracy%20of%20the%20models%20%28up%20to%20a%20~35%25%20decrease%20in%20prediction%20accuracy%29%2C%20which%0Aunderscores%20the%20importance%20of%20properly%20designing%20blockchain%20systems%20based%20on%0Athe%20characteristics%20of%20the%20underlying%20FL%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07471v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Implications%20of%20Decentralization%20in%20Blockchained%20Federated%20Learning%3A%0A%20%20Evaluating%20the%20Impact%20of%20Model%20Staleness%20and%20Inconsistencies&entry.906535625=Francesc%20Wilhelmi%20and%20Nima%20Afraz%20and%20Elia%20Guerra%20and%20Paolo%20Dini&entry.1292438233=%20%20Blockchain%20promises%20to%20enhance%20distributed%20machine%20learning%20%28ML%29%20approaches%0Asuch%20as%20federated%20learning%20%28FL%29%20by%20providing%20further%20decentralization%2C%0Asecurity%2C%20immutability%2C%20and%20trust%2C%20which%20are%20key%20properties%20for%20enabling%0Acollaborative%20intelligence%20in%20next-generation%20applications.%20Nonetheless%2C%20the%0Aintrinsic%20decentralized%20operation%20of%20peer-to-peer%20%28P2P%29%20blockchain%20nodes%20leads%0Ato%20an%20uncharted%20setting%20for%20FL%2C%20whereby%20the%20concepts%20of%20FL%20round%20and%20global%0Amodel%20become%20meaningless%2C%20as%20devices%27%20synchronization%20is%20lost%20without%20the%0Afigure%20of%20a%20central%20orchestrating%20server.%20In%20this%20paper%2C%20we%20study%20the%20practical%0Aimplications%20of%20outsourcing%20the%20orchestration%20of%20FL%20to%20a%20democratic%20setting%0Asuch%20as%20in%20a%20blockchain.%20In%20particular%2C%20we%20focus%20on%20the%20effects%20that%20model%0Astaleness%20and%20inconsistencies%2C%20endorsed%20by%20blockchains%27%20modus%20operandi%2C%20have%20on%0Athe%20training%20procedure%20held%20by%20FL%20devices%20asynchronously.%20Using%20simulation%2C%20we%0Aevaluate%20the%20blockchained%20FL%20operation%20by%20applying%20two%20different%20ML%20models%0A%28ranging%20from%20low%20to%20high%20complexity%29%20on%20the%20well-known%20MNIST%20and%20CIFAR-10%0Adatasets%2C%20respectively%2C%20and%20focus%20on%20the%20accuracy%20and%20timeliness%20of%20the%0Asolutions.%20Our%20results%20show%20the%20high%20impact%20of%20model%20inconsistencies%20on%20the%0Aaccuracy%20of%20the%20models%20%28up%20to%20a%20~35%25%20decrease%20in%20prediction%20accuracy%29%2C%20which%0Aunderscores%20the%20importance%20of%20properly%20designing%20blockchain%20systems%20based%20on%0Athe%20characteristics%20of%20the%20underlying%20FL%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07471v2&entry.124074799=Read"},
{"title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model", "author": "Ximing Xing and Haitao Zhou and Chuang Wang and Jing Zhang and Dong Xu and Qian Yu", "abstract": "  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\nshape over-smoothing, color over-saturation, limited diversity in results, and\nslow convergence in existing text-to-SVG generation methods. VPSD models SVGs\nas distributions of control points and colors to counteract over-smoothing and\nover-saturation. Furthermore, VPSD leverages a reward model to reweight vector\nparticles, which improves aesthetic appeal and accelerates convergence.\nExtensive experiments have been conducted to validate the effectiveness of\nSVGDreamer, demonstrating its superiority over baseline methods in terms of\neditability, visual quality, and diversity. The code and demo of SVGDreamer can\nbe found at https://ximinng.github.io/SVGDreamer-project/\n", "link": "http://arxiv.org/abs/2312.16476v4", "date": "2024-03-25", "relevancy": 1.7189, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5933}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5479}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5471}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SVGDreamer%3A%20Text%20Guided%20SVG%20Generation%20with%20Diffusion%20Model&body=Title%3A%20SVGDreamer%3A%20Text%20Guided%20SVG%20Generation%20with%20Diffusion%20Model%0AAuthor%3A%20Ximing%20Xing%20and%20Haitao%20Zhou%20and%20Chuang%20Wang%20and%20Jing%20Zhang%20and%20Dong%20Xu%20and%20Qian%20Yu%0AAbstract%3A%20%20%20Recently%2C%20text-guided%20scalable%20vector%20graphics%20%28SVGs%29%20synthesis%20has%20shown%0Apromise%20in%20domains%20such%20as%20iconography%20and%20sketch.%20However%2C%20existing%0Atext-to-SVG%20generation%20methods%20lack%20editability%20and%20struggle%20with%20visual%0Aquality%20and%20result%20diversity.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Atext-guided%20vector%20graphics%20synthesis%20method%20called%20SVGDreamer.%20SVGDreamer%0Aincorporates%20a%20semantic-driven%20image%20vectorization%20%28SIVE%29%20process%20that%20enables%0Athe%20decomposition%20of%20synthesis%20into%20foreground%20objects%20and%20background%2C%20thereby%0Aenhancing%20editability.%20Specifically%2C%20the%20SIVE%20process%20introduce%20attention-based%0Aprimitive%20control%20and%20an%20attention-mask%20loss%20function%20for%20effective%20control%20and%0Amanipulation%20of%20individual%20elements.%20Additionally%2C%20we%20propose%20a%20Vectorized%0AParticle-based%20Score%20Distillation%20%28VPSD%29%20approach%20to%20tackle%20the%20challenges%20of%0Ashape%20over-smoothing%2C%20color%20over-saturation%2C%20limited%20diversity%20in%20results%2C%20and%0Aslow%20convergence%20in%20existing%20text-to-SVG%20generation%20methods.%20VPSD%20models%20SVGs%0Aas%20distributions%20of%20control%20points%20and%20colors%20to%20counteract%20over-smoothing%20and%0Aover-saturation.%20Furthermore%2C%20VPSD%20leverages%20a%20reward%20model%20to%20reweight%20vector%0Aparticles%2C%20which%20improves%20aesthetic%20appeal%20and%20accelerates%20convergence.%0AExtensive%20experiments%20have%20been%20conducted%20to%20validate%20the%20effectiveness%20of%0ASVGDreamer%2C%20demonstrating%20its%20superiority%20over%20baseline%20methods%20in%20terms%20of%0Aeditability%2C%20visual%20quality%2C%20and%20diversity.%20The%20code%20and%20demo%20of%20SVGDreamer%20can%0Abe%20found%20at%20https%3A//ximinng.github.io/SVGDreamer-project/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16476v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVGDreamer%3A%20Text%20Guided%20SVG%20Generation%20with%20Diffusion%20Model&entry.906535625=Ximing%20Xing%20and%20Haitao%20Zhou%20and%20Chuang%20Wang%20and%20Jing%20Zhang%20and%20Dong%20Xu%20and%20Qian%20Yu&entry.1292438233=%20%20Recently%2C%20text-guided%20scalable%20vector%20graphics%20%28SVGs%29%20synthesis%20has%20shown%0Apromise%20in%20domains%20such%20as%20iconography%20and%20sketch.%20However%2C%20existing%0Atext-to-SVG%20generation%20methods%20lack%20editability%20and%20struggle%20with%20visual%0Aquality%20and%20result%20diversity.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Atext-guided%20vector%20graphics%20synthesis%20method%20called%20SVGDreamer.%20SVGDreamer%0Aincorporates%20a%20semantic-driven%20image%20vectorization%20%28SIVE%29%20process%20that%20enables%0Athe%20decomposition%20of%20synthesis%20into%20foreground%20objects%20and%20background%2C%20thereby%0Aenhancing%20editability.%20Specifically%2C%20the%20SIVE%20process%20introduce%20attention-based%0Aprimitive%20control%20and%20an%20attention-mask%20loss%20function%20for%20effective%20control%20and%0Amanipulation%20of%20individual%20elements.%20Additionally%2C%20we%20propose%20a%20Vectorized%0AParticle-based%20Score%20Distillation%20%28VPSD%29%20approach%20to%20tackle%20the%20challenges%20of%0Ashape%20over-smoothing%2C%20color%20over-saturation%2C%20limited%20diversity%20in%20results%2C%20and%0Aslow%20convergence%20in%20existing%20text-to-SVG%20generation%20methods.%20VPSD%20models%20SVGs%0Aas%20distributions%20of%20control%20points%20and%20colors%20to%20counteract%20over-smoothing%20and%0Aover-saturation.%20Furthermore%2C%20VPSD%20leverages%20a%20reward%20model%20to%20reweight%20vector%0Aparticles%2C%20which%20improves%20aesthetic%20appeal%20and%20accelerates%20convergence.%0AExtensive%20experiments%20have%20been%20conducted%20to%20validate%20the%20effectiveness%20of%0ASVGDreamer%2C%20demonstrating%20its%20superiority%20over%20baseline%20methods%20in%20terms%20of%0Aeditability%2C%20visual%20quality%2C%20and%20diversity.%20The%20code%20and%20demo%20of%20SVGDreamer%20can%0Abe%20found%20at%20https%3A//ximinng.github.io/SVGDreamer-project/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16476v4&entry.124074799=Read"},
{"title": "Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception", "author": "Junwen He and Yifan Wang and Lijun Wang and Huchuan Lu and Jun-Yan He and Jin-Peng Lan and Bin Luo and Xuansong Xie", "abstract": "  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a\ncognitive framework for diverse visual-language tasks. Recent efforts have been\nmade to equip MLLMs with visual perceiving and grounding capabilities. However,\nthere still remains a gap in providing fine-grained pixel-level perceptions and\nextending interactions beyond text-specific inputs. In this work, we propose\n{\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object\nperceptions and natural language descriptions from multi-modality references,\nsuch as texts, boxes, images, or audio. This innovation empowers users with\ngreater flexibility to engage with the model beyond textual and regional\nprompts, without modality-specific designs. Through our proposed refocusing\nmechanism, the generated grounding output is guided to better focus on the\nreferenced object, implicitly incorporating additional pixel-level supervision.\nThis simple modification utilizes attention scores generated during the\ninference of LLM, eliminating the need for extra computations while exhibiting\nperformance enhancements in both grounding masks and referring expressions.\nWith only publicly available training data, our model achieves state-of-the-art\nresults across multiple benchmarks, including diverse modality referring\nsegmentation and region-level referring expression generation.\n", "link": "http://arxiv.org/abs/2403.02969v2", "date": "2024-03-25", "relevancy": 1.7142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5335}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Instruction%20Tuned%20LLMs%20with%20Fine-grained%20Visual%20Perception&body=Title%3A%20Multi-modal%20Instruction%20Tuned%20LLMs%20with%20Fine-grained%20Visual%20Perception%0AAuthor%3A%20Junwen%20He%20and%20Yifan%20Wang%20and%20Lijun%20Wang%20and%20Huchuan%20Lu%20and%20Jun-Yan%20He%20and%20Jin-Peng%20Lan%20and%20Bin%20Luo%20and%20Xuansong%20Xie%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Model%20%28MLLMs%29%20leverages%20Large%20Language%20Models%20as%20a%0Acognitive%20framework%20for%20diverse%20visual-language%20tasks.%20Recent%20efforts%20have%20been%0Amade%20to%20equip%20MLLMs%20with%20visual%20perceiving%20and%20grounding%20capabilities.%20However%2C%0Athere%20still%20remains%20a%20gap%20in%20providing%20fine-grained%20pixel-level%20perceptions%20and%0Aextending%20interactions%20beyond%20text-specific%20inputs.%20In%20this%20work%2C%20we%20propose%0A%7B%5Cbf%7BAnyRef%7D%7D%2C%20a%20general%20MLLM%20model%20that%20can%20generate%20pixel-wise%20object%0Aperceptions%20and%20natural%20language%20descriptions%20from%20multi-modality%20references%2C%0Asuch%20as%20texts%2C%20boxes%2C%20images%2C%20or%20audio.%20This%20innovation%20empowers%20users%20with%0Agreater%20flexibility%20to%20engage%20with%20the%20model%20beyond%20textual%20and%20regional%0Aprompts%2C%20without%20modality-specific%20designs.%20Through%20our%20proposed%20refocusing%0Amechanism%2C%20the%20generated%20grounding%20output%20is%20guided%20to%20better%20focus%20on%20the%0Areferenced%20object%2C%20implicitly%20incorporating%20additional%20pixel-level%20supervision.%0AThis%20simple%20modification%20utilizes%20attention%20scores%20generated%20during%20the%0Ainference%20of%20LLM%2C%20eliminating%20the%20need%20for%20extra%20computations%20while%20exhibiting%0Aperformance%20enhancements%20in%20both%20grounding%20masks%20and%20referring%20expressions.%0AWith%20only%20publicly%20available%20training%20data%2C%20our%20model%20achieves%20state-of-the-art%0Aresults%20across%20multiple%20benchmarks%2C%20including%20diverse%20modality%20referring%0Asegmentation%20and%20region-level%20referring%20expression%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02969v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Instruction%20Tuned%20LLMs%20with%20Fine-grained%20Visual%20Perception&entry.906535625=Junwen%20He%20and%20Yifan%20Wang%20and%20Lijun%20Wang%20and%20Huchuan%20Lu%20and%20Jun-Yan%20He%20and%20Jin-Peng%20Lan%20and%20Bin%20Luo%20and%20Xuansong%20Xie&entry.1292438233=%20%20Multimodal%20Large%20Language%20Model%20%28MLLMs%29%20leverages%20Large%20Language%20Models%20as%20a%0Acognitive%20framework%20for%20diverse%20visual-language%20tasks.%20Recent%20efforts%20have%20been%0Amade%20to%20equip%20MLLMs%20with%20visual%20perceiving%20and%20grounding%20capabilities.%20However%2C%0Athere%20still%20remains%20a%20gap%20in%20providing%20fine-grained%20pixel-level%20perceptions%20and%0Aextending%20interactions%20beyond%20text-specific%20inputs.%20In%20this%20work%2C%20we%20propose%0A%7B%5Cbf%7BAnyRef%7D%7D%2C%20a%20general%20MLLM%20model%20that%20can%20generate%20pixel-wise%20object%0Aperceptions%20and%20natural%20language%20descriptions%20from%20multi-modality%20references%2C%0Asuch%20as%20texts%2C%20boxes%2C%20images%2C%20or%20audio.%20This%20innovation%20empowers%20users%20with%0Agreater%20flexibility%20to%20engage%20with%20the%20model%20beyond%20textual%20and%20regional%0Aprompts%2C%20without%20modality-specific%20designs.%20Through%20our%20proposed%20refocusing%0Amechanism%2C%20the%20generated%20grounding%20output%20is%20guided%20to%20better%20focus%20on%20the%0Areferenced%20object%2C%20implicitly%20incorporating%20additional%20pixel-level%20supervision.%0AThis%20simple%20modification%20utilizes%20attention%20scores%20generated%20during%20the%0Ainference%20of%20LLM%2C%20eliminating%20the%20need%20for%20extra%20computations%20while%20exhibiting%0Aperformance%20enhancements%20in%20both%20grounding%20masks%20and%20referring%20expressions.%0AWith%20only%20publicly%20available%20training%20data%2C%20our%20model%20achieves%20state-of-the-art%0Aresults%20across%20multiple%20benchmarks%2C%20including%20diverse%20modality%20referring%0Asegmentation%20and%20region-level%20referring%20expression%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02969v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning and Mean-Variance Strategies for Responsible\n  Portfolio Optimization", "author": "Fernando Acero and Parisa Zehtabi and Nicolas Marchesotti and Michael Cashmore and Daniele Magazzeni and Manuela Veloso", "abstract": "  Portfolio optimization involves determining the optimal allocation of\nportfolio assets in order to maximize a given investment objective.\nTraditionally, some form of mean-variance optimization is used with the aim of\nmaximizing returns while minimizing risk, however, more recently, deep\nreinforcement learning formulations have been explored. Increasingly, investors\nhave demonstrated an interest in incorporating ESG objectives when making\ninvestment decisions, and modifications to the classical mean-variance\noptimization framework have been developed. In this work, we study the use of\ndeep reinforcement learning for responsible portfolio optimization, by\nincorporating ESG states and objectives, and provide comparisons against\nmodified mean-variance approaches. Our results show that deep reinforcement\nlearning policies can provide competitive performance against mean-variance\napproaches for responsible portfolio allocation across additive and\nmultiplicative utility functions of financial and ESG responsibility\nobjectives.\n", "link": "http://arxiv.org/abs/2403.16667v1", "date": "2024-03-25", "relevancy": 1.7113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4718}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4251}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.413}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20and%20Mean-Variance%20Strategies%20for%20Responsible%0A%20%20Portfolio%20Optimization&body=Title%3A%20Deep%20Reinforcement%20Learning%20and%20Mean-Variance%20Strategies%20for%20Responsible%0A%20%20Portfolio%20Optimization%0AAuthor%3A%20Fernando%20Acero%20and%20Parisa%20Zehtabi%20and%20Nicolas%20Marchesotti%20and%20Michael%20Cashmore%20and%20Daniele%20Magazzeni%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20Portfolio%20optimization%20involves%20determining%20the%20optimal%20allocation%20of%0Aportfolio%20assets%20in%20order%20to%20maximize%20a%20given%20investment%20objective.%0ATraditionally%2C%20some%20form%20of%20mean-variance%20optimization%20is%20used%20with%20the%20aim%20of%0Amaximizing%20returns%20while%20minimizing%20risk%2C%20however%2C%20more%20recently%2C%20deep%0Areinforcement%20learning%20formulations%20have%20been%20explored.%20Increasingly%2C%20investors%0Ahave%20demonstrated%20an%20interest%20in%20incorporating%20ESG%20objectives%20when%20making%0Ainvestment%20decisions%2C%20and%20modifications%20to%20the%20classical%20mean-variance%0Aoptimization%20framework%20have%20been%20developed.%20In%20this%20work%2C%20we%20study%20the%20use%20of%0Adeep%20reinforcement%20learning%20for%20responsible%20portfolio%20optimization%2C%20by%0Aincorporating%20ESG%20states%20and%20objectives%2C%20and%20provide%20comparisons%20against%0Amodified%20mean-variance%20approaches.%20Our%20results%20show%20that%20deep%20reinforcement%0Alearning%20policies%20can%20provide%20competitive%20performance%20against%20mean-variance%0Aapproaches%20for%20responsible%20portfolio%20allocation%20across%20additive%20and%0Amultiplicative%20utility%20functions%20of%20financial%20and%20ESG%20responsibility%0Aobjectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20and%20Mean-Variance%20Strategies%20for%20Responsible%0A%20%20Portfolio%20Optimization&entry.906535625=Fernando%20Acero%20and%20Parisa%20Zehtabi%20and%20Nicolas%20Marchesotti%20and%20Michael%20Cashmore%20and%20Daniele%20Magazzeni%20and%20Manuela%20Veloso&entry.1292438233=%20%20Portfolio%20optimization%20involves%20determining%20the%20optimal%20allocation%20of%0Aportfolio%20assets%20in%20order%20to%20maximize%20a%20given%20investment%20objective.%0ATraditionally%2C%20some%20form%20of%20mean-variance%20optimization%20is%20used%20with%20the%20aim%20of%0Amaximizing%20returns%20while%20minimizing%20risk%2C%20however%2C%20more%20recently%2C%20deep%0Areinforcement%20learning%20formulations%20have%20been%20explored.%20Increasingly%2C%20investors%0Ahave%20demonstrated%20an%20interest%20in%20incorporating%20ESG%20objectives%20when%20making%0Ainvestment%20decisions%2C%20and%20modifications%20to%20the%20classical%20mean-variance%0Aoptimization%20framework%20have%20been%20developed.%20In%20this%20work%2C%20we%20study%20the%20use%20of%0Adeep%20reinforcement%20learning%20for%20responsible%20portfolio%20optimization%2C%20by%0Aincorporating%20ESG%20states%20and%20objectives%2C%20and%20provide%20comparisons%20against%0Amodified%20mean-variance%20approaches.%20Our%20results%20show%20that%20deep%20reinforcement%0Alearning%20policies%20can%20provide%20competitive%20performance%20against%20mean-variance%0Aapproaches%20for%20responsible%20portfolio%20allocation%20across%20additive%20and%0Amultiplicative%20utility%20functions%20of%20financial%20and%20ESG%20responsibility%0Aobjectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16667v1&entry.124074799=Read"},
{"title": "A Robotic Skill Learning System Built Upon Diffusion Policies and\n  Foundation Models", "author": "Nils Ingelhag and Jesper Munkeby and Jonne van Haastregt and Anastasia Varava and Michael C. Welle and Danica Kragic", "abstract": "  In this paper, we build upon two major recent developments in the field,\nDiffusion Policies for visuomotor manipulation and large pre-trained multimodal\nfoundational models to obtain a robotic skill learning system. The system can\nobtain new skills via the behavioral cloning approach of visuomotor diffusion\npolicies given teleoperated demonstrations. Foundational models are being used\nto perform skill selection given the user's prompt in natural language. Before\nexecuting a skill the foundational model performs a precondition check given an\nobservation of the workspace. We compare the performance of different\nfoundational models to this end as well as give a detailed experimental\nevaluation of the skills taught by the user in simulation and the real world.\nFinally, we showcase the combined system on a challenging food serving scenario\nin the real world. Videos of all experimental executions, as well as the\nprocess of teaching new skills in simulation and the real world, are available\non the project's website.\n", "link": "http://arxiv.org/abs/2403.16730v1", "date": "2024-03-25", "relevancy": 1.6808, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5906}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Robotic%20Skill%20Learning%20System%20Built%20Upon%20Diffusion%20Policies%20and%0A%20%20Foundation%20Models&body=Title%3A%20A%20Robotic%20Skill%20Learning%20System%20Built%20Upon%20Diffusion%20Policies%20and%0A%20%20Foundation%20Models%0AAuthor%3A%20Nils%20Ingelhag%20and%20Jesper%20Munkeby%20and%20Jonne%20van%20Haastregt%20and%20Anastasia%20Varava%20and%20Michael%20C.%20Welle%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20build%20upon%20two%20major%20recent%20developments%20in%20the%20field%2C%0ADiffusion%20Policies%20for%20visuomotor%20manipulation%20and%20large%20pre-trained%20multimodal%0Afoundational%20models%20to%20obtain%20a%20robotic%20skill%20learning%20system.%20The%20system%20can%0Aobtain%20new%20skills%20via%20the%20behavioral%20cloning%20approach%20of%20visuomotor%20diffusion%0Apolicies%20given%20teleoperated%20demonstrations.%20Foundational%20models%20are%20being%20used%0Ato%20perform%20skill%20selection%20given%20the%20user%27s%20prompt%20in%20natural%20language.%20Before%0Aexecuting%20a%20skill%20the%20foundational%20model%20performs%20a%20precondition%20check%20given%20an%0Aobservation%20of%20the%20workspace.%20We%20compare%20the%20performance%20of%20different%0Afoundational%20models%20to%20this%20end%20as%20well%20as%20give%20a%20detailed%20experimental%0Aevaluation%20of%20the%20skills%20taught%20by%20the%20user%20in%20simulation%20and%20the%20real%20world.%0AFinally%2C%20we%20showcase%20the%20combined%20system%20on%20a%20challenging%20food%20serving%20scenario%0Ain%20the%20real%20world.%20Videos%20of%20all%20experimental%20executions%2C%20as%20well%20as%20the%0Aprocess%20of%20teaching%20new%20skills%20in%20simulation%20and%20the%20real%20world%2C%20are%20available%0Aon%20the%20project%27s%20website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16730v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robotic%20Skill%20Learning%20System%20Built%20Upon%20Diffusion%20Policies%20and%0A%20%20Foundation%20Models&entry.906535625=Nils%20Ingelhag%20and%20Jesper%20Munkeby%20and%20Jonne%20van%20Haastregt%20and%20Anastasia%20Varava%20and%20Michael%20C.%20Welle%20and%20Danica%20Kragic&entry.1292438233=%20%20In%20this%20paper%2C%20we%20build%20upon%20two%20major%20recent%20developments%20in%20the%20field%2C%0ADiffusion%20Policies%20for%20visuomotor%20manipulation%20and%20large%20pre-trained%20multimodal%0Afoundational%20models%20to%20obtain%20a%20robotic%20skill%20learning%20system.%20The%20system%20can%0Aobtain%20new%20skills%20via%20the%20behavioral%20cloning%20approach%20of%20visuomotor%20diffusion%0Apolicies%20given%20teleoperated%20demonstrations.%20Foundational%20models%20are%20being%20used%0Ato%20perform%20skill%20selection%20given%20the%20user%27s%20prompt%20in%20natural%20language.%20Before%0Aexecuting%20a%20skill%20the%20foundational%20model%20performs%20a%20precondition%20check%20given%20an%0Aobservation%20of%20the%20workspace.%20We%20compare%20the%20performance%20of%20different%0Afoundational%20models%20to%20this%20end%20as%20well%20as%20give%20a%20detailed%20experimental%0Aevaluation%20of%20the%20skills%20taught%20by%20the%20user%20in%20simulation%20and%20the%20real%20world.%0AFinally%2C%20we%20showcase%20the%20combined%20system%20on%20a%20challenging%20food%20serving%20scenario%0Ain%20the%20real%20world.%20Videos%20of%20all%20experimental%20executions%2C%20as%20well%20as%20the%0Aprocess%20of%20teaching%20new%20skills%20in%20simulation%20and%20the%20real%20world%2C%20are%20available%0Aon%20the%20project%27s%20website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16730v1&entry.124074799=Read"},
{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "author": "Alberto Baldrati and Davide Morelli and Marcella Cornia and Marco Bertini and Rita Cucchiara", "abstract": "  Fashion illustration is a crucial medium for designers to convey their\ncreative vision and transform design concepts into tangible representations\nthat showcase the interplay between clothing and the human body. In the context\nof fashion design, computer vision techniques have the potential to enhance and\nstreamline the design process. Departing from prior research primarily focused\non virtual try-on, this paper tackles the task of multimodal-conditioned\nfashion image editing. Our approach aims to generate human-centric fashion\nimages guided by multimodal prompts, including text, human body poses, garment\nsketches, and fabric textures. To address this problem, we propose extending\nlatent diffusion models to incorporate these multiple modalities and modifying\nthe structure of the denoising network, taking multimodal prompts as input. To\ncondition the proposed architecture on fabric textures, we employ textual\ninversion techniques and let diverse cross-attention layers of the denoising\nnetwork attend to textual and texture information, thus incorporating different\ngranularity conditioning details. Given the lack of datasets for the task, we\nextend two existing fashion datasets, Dress Code and VITON-HD, with multimodal\nannotations. Experimental evaluations demonstrate the effectiveness of our\nproposed approach in terms of realism and coherence concerning the provided\nmultimodal inputs.\n", "link": "http://arxiv.org/abs/2403.14828v2", "date": "2024-03-25", "relevancy": 1.673, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5897}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5508}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal-Conditioned%20Latent%20Diffusion%20Models%20for%20Fashion%20Image%20Editing&body=Title%3A%20Multimodal-Conditioned%20Latent%20Diffusion%20Models%20for%20Fashion%20Image%20Editing%0AAuthor%3A%20Alberto%20Baldrati%20and%20Davide%20Morelli%20and%20Marcella%20Cornia%20and%20Marco%20Bertini%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Fashion%20illustration%20is%20a%20crucial%20medium%20for%20designers%20to%20convey%20their%0Acreative%20vision%20and%20transform%20design%20concepts%20into%20tangible%20representations%0Athat%20showcase%20the%20interplay%20between%20clothing%20and%20the%20human%20body.%20In%20the%20context%0Aof%20fashion%20design%2C%20computer%20vision%20techniques%20have%20the%20potential%20to%20enhance%20and%0Astreamline%20the%20design%20process.%20Departing%20from%20prior%20research%20primarily%20focused%0Aon%20virtual%20try-on%2C%20this%20paper%20tackles%20the%20task%20of%20multimodal-conditioned%0Afashion%20image%20editing.%20Our%20approach%20aims%20to%20generate%20human-centric%20fashion%0Aimages%20guided%20by%20multimodal%20prompts%2C%20including%20text%2C%20human%20body%20poses%2C%20garment%0Asketches%2C%20and%20fabric%20textures.%20To%20address%20this%20problem%2C%20we%20propose%20extending%0Alatent%20diffusion%20models%20to%20incorporate%20these%20multiple%20modalities%20and%20modifying%0Athe%20structure%20of%20the%20denoising%20network%2C%20taking%20multimodal%20prompts%20as%20input.%20To%0Acondition%20the%20proposed%20architecture%20on%20fabric%20textures%2C%20we%20employ%20textual%0Ainversion%20techniques%20and%20let%20diverse%20cross-attention%20layers%20of%20the%20denoising%0Anetwork%20attend%20to%20textual%20and%20texture%20information%2C%20thus%20incorporating%20different%0Agranularity%20conditioning%20details.%20Given%20the%20lack%20of%20datasets%20for%20the%20task%2C%20we%0Aextend%20two%20existing%20fashion%20datasets%2C%20Dress%20Code%20and%20VITON-HD%2C%20with%20multimodal%0Aannotations.%20Experimental%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approach%20in%20terms%20of%20realism%20and%20coherence%20concerning%20the%20provided%0Amultimodal%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14828v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal-Conditioned%20Latent%20Diffusion%20Models%20for%20Fashion%20Image%20Editing&entry.906535625=Alberto%20Baldrati%20and%20Davide%20Morelli%20and%20Marcella%20Cornia%20and%20Marco%20Bertini%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Fashion%20illustration%20is%20a%20crucial%20medium%20for%20designers%20to%20convey%20their%0Acreative%20vision%20and%20transform%20design%20concepts%20into%20tangible%20representations%0Athat%20showcase%20the%20interplay%20between%20clothing%20and%20the%20human%20body.%20In%20the%20context%0Aof%20fashion%20design%2C%20computer%20vision%20techniques%20have%20the%20potential%20to%20enhance%20and%0Astreamline%20the%20design%20process.%20Departing%20from%20prior%20research%20primarily%20focused%0Aon%20virtual%20try-on%2C%20this%20paper%20tackles%20the%20task%20of%20multimodal-conditioned%0Afashion%20image%20editing.%20Our%20approach%20aims%20to%20generate%20human-centric%20fashion%0Aimages%20guided%20by%20multimodal%20prompts%2C%20including%20text%2C%20human%20body%20poses%2C%20garment%0Asketches%2C%20and%20fabric%20textures.%20To%20address%20this%20problem%2C%20we%20propose%20extending%0Alatent%20diffusion%20models%20to%20incorporate%20these%20multiple%20modalities%20and%20modifying%0Athe%20structure%20of%20the%20denoising%20network%2C%20taking%20multimodal%20prompts%20as%20input.%20To%0Acondition%20the%20proposed%20architecture%20on%20fabric%20textures%2C%20we%20employ%20textual%0Ainversion%20techniques%20and%20let%20diverse%20cross-attention%20layers%20of%20the%20denoising%0Anetwork%20attend%20to%20textual%20and%20texture%20information%2C%20thus%20incorporating%20different%0Agranularity%20conditioning%20details.%20Given%20the%20lack%20of%20datasets%20for%20the%20task%2C%20we%0Aextend%20two%20existing%20fashion%20datasets%2C%20Dress%20Code%20and%20VITON-HD%2C%20with%20multimodal%0Aannotations.%20Experimental%20evaluations%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approach%20in%20terms%20of%20realism%20and%20coherence%20concerning%20the%20provided%0Amultimodal%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14828v2&entry.124074799=Read"},
{"title": "Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation\n  in Unknown Environments", "author": "Hyunki Seong and David Hyunchul Shim", "abstract": "  This paper focuses on the acquisition of mapless navigation skills within\nunknown environments. We introduce the Skill Q-Network (SQN), a novel\nreinforcement learning method featuring an adaptive skill ensemble mechanism.\nUnlike existing methods, our model concurrently learns a high-level skill\ndecision process alongside multiple low-level navigation skills, all without\nthe need for prior knowledge. Leveraging a tailored reward function for mapless\nnavigation, the SQN is capable of learning adaptive maneuvers that incorporate\nboth exploration and goal-directed skills, enabling effective navigation in new\nenvironments. Our experiments demonstrate that our SQN can effectively navigate\ncomplex environments, exhibiting a 40% higher performance compared to baseline\nmodels. Without explicit guidance, SQN discovers how to combine low-level skill\npolicies, showcasing both goal-directed navigations to reach destinations and\nexploration maneuvers to escape from local minimum regions in challenging\nscenarios. Remarkably, our adaptive skill ensemble method enables zero-shot\ntransfer to out-of-distribution domains, characterized by unseen observations\nfrom non-convex obstacles or uneven, subterranean-like environments.\n", "link": "http://arxiv.org/abs/2403.16664v1", "date": "2024-03-25", "relevancy": 1.5889, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5471}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments&body=Title%3A%20Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments%0AAuthor%3A%20Hyunki%20Seong%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20acquisition%20of%20mapless%20navigation%20skills%20within%0Aunknown%20environments.%20We%20introduce%20the%20Skill%20Q-Network%20%28SQN%29%2C%20a%20novel%0Areinforcement%20learning%20method%20featuring%20an%20adaptive%20skill%20ensemble%20mechanism.%0AUnlike%20existing%20methods%2C%20our%20model%20concurrently%20learns%20a%20high-level%20skill%0Adecision%20process%20alongside%20multiple%20low-level%20navigation%20skills%2C%20all%20without%0Athe%20need%20for%20prior%20knowledge.%20Leveraging%20a%20tailored%20reward%20function%20for%20mapless%0Anavigation%2C%20the%20SQN%20is%20capable%20of%20learning%20adaptive%20maneuvers%20that%20incorporate%0Aboth%20exploration%20and%20goal-directed%20skills%2C%20enabling%20effective%20navigation%20in%20new%0Aenvironments.%20Our%20experiments%20demonstrate%20that%20our%20SQN%20can%20effectively%20navigate%0Acomplex%20environments%2C%20exhibiting%20a%2040%25%20higher%20performance%20compared%20to%20baseline%0Amodels.%20Without%20explicit%20guidance%2C%20SQN%20discovers%20how%20to%20combine%20low-level%20skill%0Apolicies%2C%20showcasing%20both%20goal-directed%20navigations%20to%20reach%20destinations%20and%0Aexploration%20maneuvers%20to%20escape%20from%20local%20minimum%20regions%20in%20challenging%0Ascenarios.%20Remarkably%2C%20our%20adaptive%20skill%20ensemble%20method%20enables%20zero-shot%0Atransfer%20to%20out-of-distribution%20domains%2C%20characterized%20by%20unseen%20observations%0Afrom%20non-convex%20obstacles%20or%20uneven%2C%20subterranean-like%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16664v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments&entry.906535625=Hyunki%20Seong%20and%20David%20Hyunchul%20Shim&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20acquisition%20of%20mapless%20navigation%20skills%20within%0Aunknown%20environments.%20We%20introduce%20the%20Skill%20Q-Network%20%28SQN%29%2C%20a%20novel%0Areinforcement%20learning%20method%20featuring%20an%20adaptive%20skill%20ensemble%20mechanism.%0AUnlike%20existing%20methods%2C%20our%20model%20concurrently%20learns%20a%20high-level%20skill%0Adecision%20process%20alongside%20multiple%20low-level%20navigation%20skills%2C%20all%20without%0Athe%20need%20for%20prior%20knowledge.%20Leveraging%20a%20tailored%20reward%20function%20for%20mapless%0Anavigation%2C%20the%20SQN%20is%20capable%20of%20learning%20adaptive%20maneuvers%20that%20incorporate%0Aboth%20exploration%20and%20goal-directed%20skills%2C%20enabling%20effective%20navigation%20in%20new%0Aenvironments.%20Our%20experiments%20demonstrate%20that%20our%20SQN%20can%20effectively%20navigate%0Acomplex%20environments%2C%20exhibiting%20a%2040%25%20higher%20performance%20compared%20to%20baseline%0Amodels.%20Without%20explicit%20guidance%2C%20SQN%20discovers%20how%20to%20combine%20low-level%20skill%0Apolicies%2C%20showcasing%20both%20goal-directed%20navigations%20to%20reach%20destinations%20and%0Aexploration%20maneuvers%20to%20escape%20from%20local%20minimum%20regions%20in%20challenging%0Ascenarios.%20Remarkably%2C%20our%20adaptive%20skill%20ensemble%20method%20enables%20zero-shot%0Atransfer%20to%20out-of-distribution%20domains%2C%20characterized%20by%20unseen%20observations%0Afrom%20non-convex%20obstacles%20or%20uneven%2C%20subterranean-like%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16664v1&entry.124074799=Read"},
{"title": "Bridging the Sim-to-Real Gap with Bayesian Inference", "author": "Jonas Rothfuss and Bhavya Sukhija and Lenart Treven and Florian D\u00f6rfler and Stelian Coros and Andreas Krause", "abstract": "  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n", "link": "http://arxiv.org/abs/2403.16644v1", "date": "2024-03-25", "relevancy": 1.5823, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6676}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4887}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Sim-to-Real%20Gap%20with%20Bayesian%20Inference&body=Title%3A%20Bridging%20the%20Sim-to-Real%20Gap%20with%20Bayesian%20Inference%0AAuthor%3A%20Jonas%20Rothfuss%20and%20Bhavya%20Sukhija%20and%20Lenart%20Treven%20and%20Florian%20D%C3%B6rfler%20and%20Stelian%20Coros%20and%20Andreas%20Krause%0AAbstract%3A%20%20%20We%20present%20SIM-FSVGD%20for%20learning%20robot%20dynamics%20from%20data.%20As%20opposed%20to%0Atraditional%20methods%2C%20SIM-FSVGD%20leverages%20low-fidelity%20physical%20priors%2C%20e.g.%2C%20in%0Athe%20form%20of%20simulators%2C%20to%20regularize%20the%20training%20of%20neural%20network%20models.%0AWhile%20learning%20accurate%20dynamics%20already%20in%20the%20low%20data%20regime%2C%20SIM-FSVGD%0Ascales%20and%20excels%20also%20when%20more%20data%20is%20available.%20We%20empirically%20show%20that%0Alearning%20with%20implicit%20physical%20priors%20results%20in%20accurate%20mean%20model%0Aestimation%20as%20well%20as%20precise%20uncertainty%20quantification.%20We%20demonstrate%20the%0Aeffectiveness%20of%20SIM-FSVGD%20in%20bridging%20the%20sim-to-real%20gap%20on%20a%0Ahigh-performance%20RC%20racecar%20system.%20Using%20model-based%20RL%2C%20we%20demonstrate%20a%0Ahighly%20dynamic%20parking%20maneuver%20with%20drifting%2C%20using%20less%20than%20half%20the%20data%0Acompared%20to%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Sim-to-Real%20Gap%20with%20Bayesian%20Inference&entry.906535625=Jonas%20Rothfuss%20and%20Bhavya%20Sukhija%20and%20Lenart%20Treven%20and%20Florian%20D%C3%B6rfler%20and%20Stelian%20Coros%20and%20Andreas%20Krause&entry.1292438233=%20%20We%20present%20SIM-FSVGD%20for%20learning%20robot%20dynamics%20from%20data.%20As%20opposed%20to%0Atraditional%20methods%2C%20SIM-FSVGD%20leverages%20low-fidelity%20physical%20priors%2C%20e.g.%2C%20in%0Athe%20form%20of%20simulators%2C%20to%20regularize%20the%20training%20of%20neural%20network%20models.%0AWhile%20learning%20accurate%20dynamics%20already%20in%20the%20low%20data%20regime%2C%20SIM-FSVGD%0Ascales%20and%20excels%20also%20when%20more%20data%20is%20available.%20We%20empirically%20show%20that%0Alearning%20with%20implicit%20physical%20priors%20results%20in%20accurate%20mean%20model%0Aestimation%20as%20well%20as%20precise%20uncertainty%20quantification.%20We%20demonstrate%20the%0Aeffectiveness%20of%20SIM-FSVGD%20in%20bridging%20the%20sim-to-real%20gap%20on%20a%0Ahigh-performance%20RC%20racecar%20system.%20Using%20model-based%20RL%2C%20we%20demonstrate%20a%0Ahighly%20dynamic%20parking%20maneuver%20with%20drifting%2C%20using%20less%20than%20half%20the%20data%0Acompared%20to%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16644v1&entry.124074799=Read"},
{"title": "Trajectory Planning of Robotic Manipulator in Dynamic Environment\n  Exploiting DRL", "author": "Osama Ahmad and Zawar Hussain and Hammad Naeem", "abstract": "  This study is about the implementation of a reinforcement learning algorithm\nin the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick\nand place the randomly placed block at a random target point in an unknown\nenvironment. The obstacle is randomly moving which creates a hurdle in picking\nthe object. The objective of the robot is to avoid the obstacle and pick the\nblock with constraints to a fixed timestamp. In this literature, we have\napplied a deep deterministic policy gradient (DDPG) algorithm and compared the\nmodel's efficiency with dense and sparse rewards.\n", "link": "http://arxiv.org/abs/2403.16652v1", "date": "2024-03-25", "relevancy": 1.5735, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5288}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Trajectory%20Planning%20of%20Robotic%20Manipulator%20in%20Dynamic%20Environment%0A%20%20Exploiting%20DRL&body=Title%3A%20Trajectory%20Planning%20of%20Robotic%20Manipulator%20in%20Dynamic%20Environment%0A%20%20Exploiting%20DRL%0AAuthor%3A%20Osama%20Ahmad%20and%20Zawar%20Hussain%20and%20Hammad%20Naeem%0AAbstract%3A%20%20%20This%20study%20is%20about%20the%20implementation%20of%20a%20reinforcement%20learning%20algorithm%0Ain%20the%20trajectory%20planning%20of%20manipulators.%20We%20have%20a%207-DOF%20robotic%20arm%20to%20pick%0Aand%20place%20the%20randomly%20placed%20block%20at%20a%20random%20target%20point%20in%20an%20unknown%0Aenvironment.%20The%20obstacle%20is%20randomly%20moving%20which%20creates%20a%20hurdle%20in%20picking%0Athe%20object.%20The%20objective%20of%20the%20robot%20is%20to%20avoid%20the%20obstacle%20and%20pick%20the%0Ablock%20with%20constraints%20to%20a%20fixed%20timestamp.%20In%20this%20literature%2C%20we%20have%0Aapplied%20a%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20algorithm%20and%20compared%20the%0Amodel%27s%20efficiency%20with%20dense%20and%20sparse%20rewards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20Planning%20of%20Robotic%20Manipulator%20in%20Dynamic%20Environment%0A%20%20Exploiting%20DRL&entry.906535625=Osama%20Ahmad%20and%20Zawar%20Hussain%20and%20Hammad%20Naeem&entry.1292438233=%20%20This%20study%20is%20about%20the%20implementation%20of%20a%20reinforcement%20learning%20algorithm%0Ain%20the%20trajectory%20planning%20of%20manipulators.%20We%20have%20a%207-DOF%20robotic%20arm%20to%20pick%0Aand%20place%20the%20randomly%20placed%20block%20at%20a%20random%20target%20point%20in%20an%20unknown%0Aenvironment.%20The%20obstacle%20is%20randomly%20moving%20which%20creates%20a%20hurdle%20in%20picking%0Athe%20object.%20The%20objective%20of%20the%20robot%20is%20to%20avoid%20the%20obstacle%20and%20pick%20the%0Ablock%20with%20constraints%20to%20a%20fixed%20timestamp.%20In%20this%20literature%2C%20we%20have%0Aapplied%20a%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20algorithm%20and%20compared%20the%0Amodel%27s%20efficiency%20with%20dense%20and%20sparse%20rewards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16652v1&entry.124074799=Read"},
{"title": "Counter-example guided Imitation Learning of Feedback Controllers from\n  Temporal Logic Specifications", "author": "Thao Dang and Alexandre Donz\u00e9 and Inzemamul Haque and Nikolaos Kekatos and Indranil Saha", "abstract": "  We present a novel method for imitation learning for control requirements\nexpressed using Signal Temporal Logic (STL). More concretely we focus on the\nproblem of training a neural network to imitate a complex controller. The\nlearning process is guided by efficient data aggregation based on\ncounter-examples and a coverage measure. Moreover, we introduce a method to\nevaluate the performance of the learned controller via parameterization and\nparameter estimation of the STL requirements. We demonstrate our approach with\na flying robot case study.\n", "link": "http://arxiv.org/abs/2403.16593v1", "date": "2024-03-25", "relevancy": 1.546, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.457}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Counter-example%20guided%20Imitation%20Learning%20of%20Feedback%20Controllers%20from%0A%20%20Temporal%20Logic%20Specifications&body=Title%3A%20Counter-example%20guided%20Imitation%20Learning%20of%20Feedback%20Controllers%20from%0A%20%20Temporal%20Logic%20Specifications%0AAuthor%3A%20Thao%20Dang%20and%20Alexandre%20Donz%C3%A9%20and%20Inzemamul%20Haque%20and%20Nikolaos%20Kekatos%20and%20Indranil%20Saha%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20imitation%20learning%20for%20control%20requirements%0Aexpressed%20using%20Signal%20Temporal%20Logic%20%28STL%29.%20More%20concretely%20we%20focus%20on%20the%0Aproblem%20of%20training%20a%20neural%20network%20to%20imitate%20a%20complex%20controller.%20The%0Alearning%20process%20is%20guided%20by%20efficient%20data%20aggregation%20based%20on%0Acounter-examples%20and%20a%20coverage%20measure.%20Moreover%2C%20we%20introduce%20a%20method%20to%0Aevaluate%20the%20performance%20of%20the%20learned%20controller%20via%20parameterization%20and%0Aparameter%20estimation%20of%20the%20STL%20requirements.%20We%20demonstrate%20our%20approach%20with%0Aa%20flying%20robot%20case%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16593v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counter-example%20guided%20Imitation%20Learning%20of%20Feedback%20Controllers%20from%0A%20%20Temporal%20Logic%20Specifications&entry.906535625=Thao%20Dang%20and%20Alexandre%20Donz%C3%A9%20and%20Inzemamul%20Haque%20and%20Nikolaos%20Kekatos%20and%20Indranil%20Saha&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20imitation%20learning%20for%20control%20requirements%0Aexpressed%20using%20Signal%20Temporal%20Logic%20%28STL%29.%20More%20concretely%20we%20focus%20on%20the%0Aproblem%20of%20training%20a%20neural%20network%20to%20imitate%20a%20complex%20controller.%20The%0Alearning%20process%20is%20guided%20by%20efficient%20data%20aggregation%20based%20on%0Acounter-examples%20and%20a%20coverage%20measure.%20Moreover%2C%20we%20introduce%20a%20method%20to%0Aevaluate%20the%20performance%20of%20the%20learned%20controller%20via%20parameterization%20and%0Aparameter%20estimation%20of%20the%20STL%20requirements.%20We%20demonstrate%20our%20approach%20with%0Aa%20flying%20robot%20case%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16593v1&entry.124074799=Read"},
{"title": "BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis", "author": "Leandro A. Passos and Douglas Rodrigues and Danilo Jodas and Kelton A. P. Costa and Ahsan Adeel and Jo\u00e3o Paulo Papa", "abstract": "  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n", "link": "http://arxiv.org/abs/2402.07310v2", "date": "2024-03-25", "relevancy": 1.5452, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5235}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BioNeRF%3A%20Biologically%20Plausible%20Neural%20Radiance%20Fields%20for%20View%0A%20%20Synthesis&body=Title%3A%20BioNeRF%3A%20Biologically%20Plausible%20Neural%20Radiance%20Fields%20for%20View%0A%20%20Synthesis%0AAuthor%3A%20Leandro%20A.%20Passos%20and%20Douglas%20Rodrigues%20and%20Danilo%20Jodas%20and%20Kelton%20A.%20P.%20Costa%20and%20Ahsan%20Adeel%20and%20Jo%C3%A3o%20Paulo%20Papa%0AAbstract%3A%20%20%20This%20paper%20presents%20BioNeRF%2C%20a%20biologically%20plausible%20architecture%20that%0Amodels%20scenes%20in%20a%203D%20representation%20and%20synthesizes%20new%20views%20through%20radiance%0Afields.%20Since%20NeRF%20relies%20on%20the%20network%20weights%20to%20store%20the%20scene%27s%0A3-dimensional%20representation%2C%20BioNeRF%20implements%20a%20cognitive-inspired%20mechanism%0Athat%20fuses%20inputs%20from%20multiple%20sources%20into%20a%20memory-like%20structure%2C%20improving%0Athe%20storing%20capacity%20and%20extracting%20more%20intrinsic%20and%20correlated%20information.%0ABioNeRF%20also%20mimics%20a%20behavior%20observed%20in%20pyramidal%20cells%20concerning%0Acontextual%20information%2C%20in%20which%20the%20memory%20is%20provided%20as%20the%20context%20and%0Acombined%20with%20the%20inputs%20of%20two%20subsequent%20neural%20models%2C%20one%20responsible%20for%0Aproducing%20the%20volumetric%20densities%20and%20the%20other%20the%20colors%20used%20to%20render%20the%0Ascene.%20Experimental%20results%20show%20that%20BioNeRF%20outperforms%20state-of-the-art%0Aresults%20concerning%20a%20quality%20measure%20that%20encodes%20human%20perception%20in%20two%0Adatasets%3A%20real-world%20images%20and%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07310v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioNeRF%3A%20Biologically%20Plausible%20Neural%20Radiance%20Fields%20for%20View%0A%20%20Synthesis&entry.906535625=Leandro%20A.%20Passos%20and%20Douglas%20Rodrigues%20and%20Danilo%20Jodas%20and%20Kelton%20A.%20P.%20Costa%20and%20Ahsan%20Adeel%20and%20Jo%C3%A3o%20Paulo%20Papa&entry.1292438233=%20%20This%20paper%20presents%20BioNeRF%2C%20a%20biologically%20plausible%20architecture%20that%0Amodels%20scenes%20in%20a%203D%20representation%20and%20synthesizes%20new%20views%20through%20radiance%0Afields.%20Since%20NeRF%20relies%20on%20the%20network%20weights%20to%20store%20the%20scene%27s%0A3-dimensional%20representation%2C%20BioNeRF%20implements%20a%20cognitive-inspired%20mechanism%0Athat%20fuses%20inputs%20from%20multiple%20sources%20into%20a%20memory-like%20structure%2C%20improving%0Athe%20storing%20capacity%20and%20extracting%20more%20intrinsic%20and%20correlated%20information.%0ABioNeRF%20also%20mimics%20a%20behavior%20observed%20in%20pyramidal%20cells%20concerning%0Acontextual%20information%2C%20in%20which%20the%20memory%20is%20provided%20as%20the%20context%20and%0Acombined%20with%20the%20inputs%20of%20two%20subsequent%20neural%20models%2C%20one%20responsible%20for%0Aproducing%20the%20volumetric%20densities%20and%20the%20other%20the%20colors%20used%20to%20render%20the%0Ascene.%20Experimental%20results%20show%20that%20BioNeRF%20outperforms%20state-of-the-art%0Aresults%20concerning%20a%20quality%20measure%20that%20encodes%20human%20perception%20in%20two%0Adatasets%3A%20real-world%20images%20and%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07310v2&entry.124074799=Read"},
{"title": "Research Challenges for Adaptive Architecture: Empowering Occupants of\n  Multi-Occupancy Buildings", "author": "Binh Vinh Duc Nguyen and Andrew Vande Moere", "abstract": "  This positional paper outlines our vision of 'adaptive architecture', which\ninvolves the integration of robotic technology to physically change an\narchitectural space in supporting the changing needs of its occupants, in\nresponse to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data &\nTechnology\" call on \"How do new technologies enable and empower the inhabitants\nof multi-occupancy buildings?\". Specifically, while adaptive architecture holds\npromise for enhancing occupant satisfaction, comfort, and overall health and\nwell-being, there remains a range of research challenges of (1) how it can\neffectively support individual occupants, while (2) mediating the conflicting\nneeds of collocated others, and (3) integrating meaningfully into the\nsociocultural characteristics of their building community.\n", "link": "http://arxiv.org/abs/2403.16600v1", "date": "2024-03-25", "relevancy": 1.5447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Research%20Challenges%20for%20Adaptive%20Architecture%3A%20Empowering%20Occupants%20of%0A%20%20Multi-Occupancy%20Buildings&body=Title%3A%20Research%20Challenges%20for%20Adaptive%20Architecture%3A%20Empowering%20Occupants%20of%0A%20%20Multi-Occupancy%20Buildings%0AAuthor%3A%20Binh%20Vinh%20Duc%20Nguyen%20and%20Andrew%20Vande%20Moere%0AAbstract%3A%20%20%20This%20positional%20paper%20outlines%20our%20vision%20of%20%27adaptive%20architecture%27%2C%20which%0Ainvolves%20the%20integration%20of%20robotic%20technology%20to%20physically%20change%20an%0Aarchitectural%20space%20in%20supporting%20the%20changing%20needs%20of%20its%20occupants%2C%20in%0Aresponse%20to%20the%20CHI%2724%20workshop%20%22HabiTech%20-%20Inhabiting%20Buildings%2C%20Data%20%26%0ATechnology%22%20call%20on%20%22How%20do%20new%20technologies%20enable%20and%20empower%20the%20inhabitants%0Aof%20multi-occupancy%20buildings%3F%22.%20Specifically%2C%20while%20adaptive%20architecture%20holds%0Apromise%20for%20enhancing%20occupant%20satisfaction%2C%20comfort%2C%20and%20overall%20health%20and%0Awell-being%2C%20there%20remains%20a%20range%20of%20research%20challenges%20of%20%281%29%20how%20it%20can%0Aeffectively%20support%20individual%20occupants%2C%20while%20%282%29%20mediating%20the%20conflicting%0Aneeds%20of%20collocated%20others%2C%20and%20%283%29%20integrating%20meaningfully%20into%20the%0Asociocultural%20characteristics%20of%20their%20building%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20Challenges%20for%20Adaptive%20Architecture%3A%20Empowering%20Occupants%20of%0A%20%20Multi-Occupancy%20Buildings&entry.906535625=Binh%20Vinh%20Duc%20Nguyen%20and%20Andrew%20Vande%20Moere&entry.1292438233=%20%20This%20positional%20paper%20outlines%20our%20vision%20of%20%27adaptive%20architecture%27%2C%20which%0Ainvolves%20the%20integration%20of%20robotic%20technology%20to%20physically%20change%20an%0Aarchitectural%20space%20in%20supporting%20the%20changing%20needs%20of%20its%20occupants%2C%20in%0Aresponse%20to%20the%20CHI%2724%20workshop%20%22HabiTech%20-%20Inhabiting%20Buildings%2C%20Data%20%26%0ATechnology%22%20call%20on%20%22How%20do%20new%20technologies%20enable%20and%20empower%20the%20inhabitants%0Aof%20multi-occupancy%20buildings%3F%22.%20Specifically%2C%20while%20adaptive%20architecture%20holds%0Apromise%20for%20enhancing%20occupant%20satisfaction%2C%20comfort%2C%20and%20overall%20health%20and%0Awell-being%2C%20there%20remains%20a%20range%20of%20research%20challenges%20of%20%281%29%20how%20it%20can%0Aeffectively%20support%20individual%20occupants%2C%20while%20%282%29%20mediating%20the%20conflicting%0Aneeds%20of%20collocated%20others%2C%20and%20%283%29%20integrating%20meaningfully%20into%20the%0Asociocultural%20characteristics%20of%20their%20building%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16600v1&entry.124074799=Read"},
{"title": "A note on generalization bounds for losses with finite moments", "author": "Borja Rodr\u00edguez-G\u00e1lvez and Omar Rivasplata and Ragnar Thobaben and Mikael Skoglund", "abstract": "  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n", "link": "http://arxiv.org/abs/2403.16681v1", "date": "2024-03-25", "relevancy": 1.5378, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3988}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3685}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20note%20on%20generalization%20bounds%20for%20losses%20with%20finite%20moments&body=Title%3A%20A%20note%20on%20generalization%20bounds%20for%20losses%20with%20finite%20moments%0AAuthor%3A%20Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Omar%20Rivasplata%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20truncation%20method%20from%20Alquier%20%5B1%5D%20to%20derive%0Ahigh-probability%20PAC-Bayes%20bounds%20for%20unbounded%20losses%20with%20heavy%20tails.%0AAssuming%20that%20the%20%24p%24-th%20moment%20is%20bounded%2C%20the%20resulting%20bounds%20interpolate%0Abetween%20a%20slow%20rate%20%241%20/%20%5Csqrt%7Bn%7D%24%20when%20%24p%3D2%24%2C%20and%20a%20fast%20rate%20%241%20/%20n%24%20when%20%24p%0A%5Cto%20%5Cinfty%24%20and%20the%20loss%20is%20essentially%20bounded.%20Moreover%2C%20the%20paper%20derives%20a%0Ahigh-probability%20PAC-Bayes%20bound%20for%20losses%20with%20a%20bounded%20variance.%20This%20bound%0Ahas%20an%20exponentially%20better%20dependence%20on%20the%20confidence%20parameter%20and%20the%0Adependency%20measure%20than%20previous%20bounds%20in%20the%20literature.%20Finally%2C%20the%20paper%0Aextends%20all%20results%20to%20guarantees%20in%20expectation%20and%20single-draw%20PAC-Bayes.%20In%0Aorder%20to%20so%2C%20it%20obtains%20analogues%20of%20the%20PAC-Bayes%20fast%20rate%20bound%20for%20bounded%0Alosses%20from%20%5B2%5D%20in%20these%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16681v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20note%20on%20generalization%20bounds%20for%20losses%20with%20finite%20moments&entry.906535625=Borja%20Rodr%C3%ADguez-G%C3%A1lvez%20and%20Omar%20Rivasplata%20and%20Ragnar%20Thobaben%20and%20Mikael%20Skoglund&entry.1292438233=%20%20This%20paper%20studies%20the%20truncation%20method%20from%20Alquier%20%5B1%5D%20to%20derive%0Ahigh-probability%20PAC-Bayes%20bounds%20for%20unbounded%20losses%20with%20heavy%20tails.%0AAssuming%20that%20the%20%24p%24-th%20moment%20is%20bounded%2C%20the%20resulting%20bounds%20interpolate%0Abetween%20a%20slow%20rate%20%241%20/%20%5Csqrt%7Bn%7D%24%20when%20%24p%3D2%24%2C%20and%20a%20fast%20rate%20%241%20/%20n%24%20when%20%24p%0A%5Cto%20%5Cinfty%24%20and%20the%20loss%20is%20essentially%20bounded.%20Moreover%2C%20the%20paper%20derives%20a%0Ahigh-probability%20PAC-Bayes%20bound%20for%20losses%20with%20a%20bounded%20variance.%20This%20bound%0Ahas%20an%20exponentially%20better%20dependence%20on%20the%20confidence%20parameter%20and%20the%0Adependency%20measure%20than%20previous%20bounds%20in%20the%20literature.%20Finally%2C%20the%20paper%0Aextends%20all%20results%20to%20guarantees%20in%20expectation%20and%20single-draw%20PAC-Bayes.%20In%0Aorder%20to%20so%2C%20it%20obtains%20analogues%20of%20the%20PAC-Bayes%20fast%20rate%20bound%20for%20bounded%0Alosses%20from%20%5B2%5D%20in%20these%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16681v1&entry.124074799=Read"},
{"title": "A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing", "author": "Hiu Ching Cheung and Ching-Wei Chang and Bailun Jiang and Chih-Yung Wen and Henry K. Chu", "abstract": "  Aerial robots have garnered significant attention due to their potential\napplications in various industries, such as inspection, search and rescue, and\ndrone delivery. Successful missions often depend on the ability of these robots\nto grasp and land effectively. This paper presents a novel modular soft gripper\ndesign tailored explicitly for aerial grasping and landing operations. The\nproposed modular pneumatic soft gripper incorporates a feed-forward\nproportional controller to regulate pressure, enabling compliant gripping\ncapabilities. The modular connectors of the soft fingers offer two\nconfigurations for the 4-tip soft gripper, H-base (cylindrical) and X-base\n(spherical), allowing adaptability to different target objects. Additionally,\nthe gripper can serve as a soft landing gear when deflated, eliminating the\nneed for an extra landing gear. This design reduces weight, simplifies aerial\nmanipulation control, and enhances flight efficiency. We demonstrate the\nefficacy of indoor aerial grasping and achieve a maximum payload of 217 g using\nthe proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).\n", "link": "http://arxiv.org/abs/2311.00390v3", "date": "2024-03-25", "relevancy": 1.506, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5213}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4847}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Modular%20Pneumatic%20Soft%20Gripper%20Design%20for%20Aerial%20Grasping%20and%20Landing&body=Title%3A%20A%20Modular%20Pneumatic%20Soft%20Gripper%20Design%20for%20Aerial%20Grasping%20and%20Landing%0AAuthor%3A%20Hiu%20Ching%20Cheung%20and%20Ching-Wei%20Chang%20and%20Bailun%20Jiang%20and%20Chih-Yung%20Wen%20and%20Henry%20K.%20Chu%0AAbstract%3A%20%20%20Aerial%20robots%20have%20garnered%20significant%20attention%20due%20to%20their%20potential%0Aapplications%20in%20various%20industries%2C%20such%20as%20inspection%2C%20search%20and%20rescue%2C%20and%0Adrone%20delivery.%20Successful%20missions%20often%20depend%20on%20the%20ability%20of%20these%20robots%0Ato%20grasp%20and%20land%20effectively.%20This%20paper%20presents%20a%20novel%20modular%20soft%20gripper%0Adesign%20tailored%20explicitly%20for%20aerial%20grasping%20and%20landing%20operations.%20The%0Aproposed%20modular%20pneumatic%20soft%20gripper%20incorporates%20a%20feed-forward%0Aproportional%20controller%20to%20regulate%20pressure%2C%20enabling%20compliant%20gripping%0Acapabilities.%20The%20modular%20connectors%20of%20the%20soft%20fingers%20offer%20two%0Aconfigurations%20for%20the%204-tip%20soft%20gripper%2C%20H-base%20%28cylindrical%29%20and%20X-base%0A%28spherical%29%2C%20allowing%20adaptability%20to%20different%20target%20objects.%20Additionally%2C%0Athe%20gripper%20can%20serve%20as%20a%20soft%20landing%20gear%20when%20deflated%2C%20eliminating%20the%0Aneed%20for%20an%20extra%20landing%20gear.%20This%20design%20reduces%20weight%2C%20simplifies%20aerial%0Amanipulation%20control%2C%20and%20enhances%20flight%20efficiency.%20We%20demonstrate%20the%0Aefficacy%20of%20indoor%20aerial%20grasping%20and%20achieve%20a%20maximum%20payload%20of%20217%20g%20using%0Athe%20proposed%20soft%20aerial%20vehicle%20and%20its%20H-base%20pneumatic%20soft%20gripper%20%28808%20g%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00390v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Modular%20Pneumatic%20Soft%20Gripper%20Design%20for%20Aerial%20Grasping%20and%20Landing&entry.906535625=Hiu%20Ching%20Cheung%20and%20Ching-Wei%20Chang%20and%20Bailun%20Jiang%20and%20Chih-Yung%20Wen%20and%20Henry%20K.%20Chu&entry.1292438233=%20%20Aerial%20robots%20have%20garnered%20significant%20attention%20due%20to%20their%20potential%0Aapplications%20in%20various%20industries%2C%20such%20as%20inspection%2C%20search%20and%20rescue%2C%20and%0Adrone%20delivery.%20Successful%20missions%20often%20depend%20on%20the%20ability%20of%20these%20robots%0Ato%20grasp%20and%20land%20effectively.%20This%20paper%20presents%20a%20novel%20modular%20soft%20gripper%0Adesign%20tailored%20explicitly%20for%20aerial%20grasping%20and%20landing%20operations.%20The%0Aproposed%20modular%20pneumatic%20soft%20gripper%20incorporates%20a%20feed-forward%0Aproportional%20controller%20to%20regulate%20pressure%2C%20enabling%20compliant%20gripping%0Acapabilities.%20The%20modular%20connectors%20of%20the%20soft%20fingers%20offer%20two%0Aconfigurations%20for%20the%204-tip%20soft%20gripper%2C%20H-base%20%28cylindrical%29%20and%20X-base%0A%28spherical%29%2C%20allowing%20adaptability%20to%20different%20target%20objects.%20Additionally%2C%0Athe%20gripper%20can%20serve%20as%20a%20soft%20landing%20gear%20when%20deflated%2C%20eliminating%20the%0Aneed%20for%20an%20extra%20landing%20gear.%20This%20design%20reduces%20weight%2C%20simplifies%20aerial%0Amanipulation%20control%2C%20and%20enhances%20flight%20efficiency.%20We%20demonstrate%20the%0Aefficacy%20of%20indoor%20aerial%20grasping%20and%20achieve%20a%20maximum%20payload%20of%20217%20g%20using%0Athe%20proposed%20soft%20aerial%20vehicle%20and%20its%20H-base%20pneumatic%20soft%20gripper%20%28808%20g%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00390v3&entry.124074799=Read"},
{"title": "Technical Development of a Semi-Autonomous Robotic Partition", "author": "Binh Vinh Duc Nguyen and Andrew Vande Moere", "abstract": "  This technical description details the design and engineering process of a\nsemi-autonomous robotic partition. This robotic partition prototype was\nsubsequently employed in a longer-term evaluation in-the-wild study conducted\nby the authors in a real-world office setting.\n", "link": "http://arxiv.org/abs/2403.16613v1", "date": "2024-03-25", "relevancy": 1.4885, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5218}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4873}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Technical%20Development%20of%20a%20Semi-Autonomous%20Robotic%20Partition&body=Title%3A%20Technical%20Development%20of%20a%20Semi-Autonomous%20Robotic%20Partition%0AAuthor%3A%20Binh%20Vinh%20Duc%20Nguyen%20and%20Andrew%20Vande%20Moere%0AAbstract%3A%20%20%20This%20technical%20description%20details%20the%20design%20and%20engineering%20process%20of%20a%0Asemi-autonomous%20robotic%20partition.%20This%20robotic%20partition%20prototype%20was%0Asubsequently%20employed%20in%20a%20longer-term%20evaluation%20in-the-wild%20study%20conducted%0Aby%20the%20authors%20in%20a%20real-world%20office%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Development%20of%20a%20Semi-Autonomous%20Robotic%20Partition&entry.906535625=Binh%20Vinh%20Duc%20Nguyen%20and%20Andrew%20Vande%20Moere&entry.1292438233=%20%20This%20technical%20description%20details%20the%20design%20and%20engineering%20process%20of%20a%0Asemi-autonomous%20robotic%20partition.%20This%20robotic%20partition%20prototype%20was%0Asubsequently%20employed%20in%20a%20longer-term%20evaluation%20in-the-wild%20study%20conducted%0Aby%20the%20authors%20in%20a%20real-world%20office%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16613v1&entry.124074799=Read"},
{"title": "Creating a Digital Twin of Spinal Surgery: A Proof of Concept", "author": "Jonas Hein and Frederic Giraud and Lilian Calvet and Alexander Schwarz and Nicola Alessandro Cavalcanti and Sergey Prokudin and Mazda Farshad and Siyu Tang and Marc Pollefeys and Fabio Carrillo and Philipp F\u00fcrnstahl", "abstract": "  Surgery digitalization is the process of creating a virtual replica of\nreal-world surgery, also referred to as a surgical digital twin (SDT). It has\nsignificant applications in various fields such as education and training,\nsurgical planning, and automation of surgical tasks. Given their detailed\nrepresentations of surgical procedures, SDTs are an ideal foundation for\nmachine learning methods, enabling automatic generation of training data. In\nrobotic surgery, SDTs can provide realistic virtual environments in which\nrobots may learn through trial and error. In this paper, we present a proof of\nconcept (PoC) for surgery digitalization that is applied to an ex-vivo spinal\nsurgery performed in realistic conditions. The proposed digitalization focuses\non the acquisition and modelling of the geometry and appearance of the entire\nsurgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of\nthe surgeon, a high-end camera for 3D reconstruction of the anatomy, an\ninfrared stereo camera for surgical instrument tracking, and a laser scanner\nfor 3D reconstruction of the operating room and data fusion. We justify the\nproposed methodology, discuss the challenges faced and further extensions of\nour prototype. While our PoC partially relies on manual data curation, its high\nquality and great potential motivate the development of automated methods for\nthe creation of SDTs. The quality of our SDT can be assessed in a rendered\nvideo available at https://youtu.be/LqVaWGgaTMY .\n", "link": "http://arxiv.org/abs/2403.16736v1", "date": "2024-03-25", "relevancy": 1.4852, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5082}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20Digital%20Twin%20of%20Spinal%20Surgery%3A%20A%20Proof%20of%20Concept&body=Title%3A%20Creating%20a%20Digital%20Twin%20of%20Spinal%20Surgery%3A%20A%20Proof%20of%20Concept%0AAuthor%3A%20Jonas%20Hein%20and%20Frederic%20Giraud%20and%20Lilian%20Calvet%20and%20Alexander%20Schwarz%20and%20Nicola%20Alessandro%20Cavalcanti%20and%20Sergey%20Prokudin%20and%20Mazda%20Farshad%20and%20Siyu%20Tang%20and%20Marc%20Pollefeys%20and%20Fabio%20Carrillo%20and%20Philipp%20F%C3%BCrnstahl%0AAbstract%3A%20%20%20Surgery%20digitalization%20is%20the%20process%20of%20creating%20a%20virtual%20replica%20of%0Areal-world%20surgery%2C%20also%20referred%20to%20as%20a%20surgical%20digital%20twin%20%28SDT%29.%20It%20has%0Asignificant%20applications%20in%20various%20fields%20such%20as%20education%20and%20training%2C%0Asurgical%20planning%2C%20and%20automation%20of%20surgical%20tasks.%20Given%20their%20detailed%0Arepresentations%20of%20surgical%20procedures%2C%20SDTs%20are%20an%20ideal%20foundation%20for%0Amachine%20learning%20methods%2C%20enabling%20automatic%20generation%20of%20training%20data.%20In%0Arobotic%20surgery%2C%20SDTs%20can%20provide%20realistic%20virtual%20environments%20in%20which%0Arobots%20may%20learn%20through%20trial%20and%20error.%20In%20this%20paper%2C%20we%20present%20a%20proof%20of%0Aconcept%20%28PoC%29%20for%20surgery%20digitalization%20that%20is%20applied%20to%20an%20ex-vivo%20spinal%0Asurgery%20performed%20in%20realistic%20conditions.%20The%20proposed%20digitalization%20focuses%0Aon%20the%20acquisition%20and%20modelling%20of%20the%20geometry%20and%20appearance%20of%20the%20entire%0Asurgical%20scene.%20We%20employ%20five%20RGB-D%20cameras%20for%20dynamic%203D%20reconstruction%20of%0Athe%20surgeon%2C%20a%20high-end%20camera%20for%203D%20reconstruction%20of%20the%20anatomy%2C%20an%0Ainfrared%20stereo%20camera%20for%20surgical%20instrument%20tracking%2C%20and%20a%20laser%20scanner%0Afor%203D%20reconstruction%20of%20the%20operating%20room%20and%20data%20fusion.%20We%20justify%20the%0Aproposed%20methodology%2C%20discuss%20the%20challenges%20faced%20and%20further%20extensions%20of%0Aour%20prototype.%20While%20our%20PoC%20partially%20relies%20on%20manual%20data%20curation%2C%20its%20high%0Aquality%20and%20great%20potential%20motivate%20the%20development%20of%20automated%20methods%20for%0Athe%20creation%20of%20SDTs.%20The%20quality%20of%20our%20SDT%20can%20be%20assessed%20in%20a%20rendered%0Avideo%20available%20at%20https%3A//youtu.be/LqVaWGgaTMY%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16736v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20Digital%20Twin%20of%20Spinal%20Surgery%3A%20A%20Proof%20of%20Concept&entry.906535625=Jonas%20Hein%20and%20Frederic%20Giraud%20and%20Lilian%20Calvet%20and%20Alexander%20Schwarz%20and%20Nicola%20Alessandro%20Cavalcanti%20and%20Sergey%20Prokudin%20and%20Mazda%20Farshad%20and%20Siyu%20Tang%20and%20Marc%20Pollefeys%20and%20Fabio%20Carrillo%20and%20Philipp%20F%C3%BCrnstahl&entry.1292438233=%20%20Surgery%20digitalization%20is%20the%20process%20of%20creating%20a%20virtual%20replica%20of%0Areal-world%20surgery%2C%20also%20referred%20to%20as%20a%20surgical%20digital%20twin%20%28SDT%29.%20It%20has%0Asignificant%20applications%20in%20various%20fields%20such%20as%20education%20and%20training%2C%0Asurgical%20planning%2C%20and%20automation%20of%20surgical%20tasks.%20Given%20their%20detailed%0Arepresentations%20of%20surgical%20procedures%2C%20SDTs%20are%20an%20ideal%20foundation%20for%0Amachine%20learning%20methods%2C%20enabling%20automatic%20generation%20of%20training%20data.%20In%0Arobotic%20surgery%2C%20SDTs%20can%20provide%20realistic%20virtual%20environments%20in%20which%0Arobots%20may%20learn%20through%20trial%20and%20error.%20In%20this%20paper%2C%20we%20present%20a%20proof%20of%0Aconcept%20%28PoC%29%20for%20surgery%20digitalization%20that%20is%20applied%20to%20an%20ex-vivo%20spinal%0Asurgery%20performed%20in%20realistic%20conditions.%20The%20proposed%20digitalization%20focuses%0Aon%20the%20acquisition%20and%20modelling%20of%20the%20geometry%20and%20appearance%20of%20the%20entire%0Asurgical%20scene.%20We%20employ%20five%20RGB-D%20cameras%20for%20dynamic%203D%20reconstruction%20of%0Athe%20surgeon%2C%20a%20high-end%20camera%20for%203D%20reconstruction%20of%20the%20anatomy%2C%20an%0Ainfrared%20stereo%20camera%20for%20surgical%20instrument%20tracking%2C%20and%20a%20laser%20scanner%0Afor%203D%20reconstruction%20of%20the%20operating%20room%20and%20data%20fusion.%20We%20justify%20the%0Aproposed%20methodology%2C%20discuss%20the%20challenges%20faced%20and%20further%20extensions%20of%0Aour%20prototype.%20While%20our%20PoC%20partially%20relies%20on%20manual%20data%20curation%2C%20its%20high%0Aquality%20and%20great%20potential%20motivate%20the%20development%20of%20automated%20methods%20for%0Athe%20creation%20of%20SDTs.%20The%20quality%20of%20our%20SDT%20can%20be%20assessed%20in%20a%20rendered%0Avideo%20available%20at%20https%3A//youtu.be/LqVaWGgaTMY%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16736v1&entry.124074799=Read"},
{"title": "ROXIE: Defining a Robotic eXplanation and Interpretability Engine", "author": "Francisco J. Rodr\u00edguez-Lera and Miguel A. Gonz\u00e1lez-Santamarta and Alejandro Gonz\u00e1lez-Cant\u00f3n and Laura Fern\u00e1ndez-Becerra and David Sobr\u00edn-Hidalgo and Angel Manuel Guerrero-Higueras", "abstract": "  In an era where autonomous robots increasingly inhabit public spaces, the\nimperative for transparency and interpretability in their decision-making\nprocesses becomes paramount. This paper presents the overview of a Robotic\neXplanation and Interpretability Engine (ROXIE), which addresses this critical\nneed, aiming to demystify the opaque nature of complex robotic behaviors. This\npaper elucidates the key features and requirements needed for providing\ninformation and explanations about robot decision-making processes. It also\noverviews the suite of software components and libraries available for\ndeployment with ROS 2, empowering users to provide comprehensive explanations\nand interpretations of robot processes and behaviors, thereby fostering trust\nand collaboration in human-robot interactions.\n", "link": "http://arxiv.org/abs/2403.16606v1", "date": "2024-03-25", "relevancy": 1.478, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4568}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ROXIE%3A%20Defining%20a%20Robotic%20eXplanation%20and%20Interpretability%20Engine&body=Title%3A%20ROXIE%3A%20Defining%20a%20Robotic%20eXplanation%20and%20Interpretability%20Engine%0AAuthor%3A%20Francisco%20J.%20Rodr%C3%ADguez-Lera%20and%20Miguel%20A.%20Gonz%C3%A1lez-Santamarta%20and%20Alejandro%20Gonz%C3%A1lez-Cant%C3%B3n%20and%20Laura%20Fern%C3%A1ndez-Becerra%20and%20David%20Sobr%C3%ADn-Hidalgo%20and%20Angel%20Manuel%20Guerrero-Higueras%0AAbstract%3A%20%20%20In%20an%20era%20where%20autonomous%20robots%20increasingly%20inhabit%20public%20spaces%2C%20the%0Aimperative%20for%20transparency%20and%20interpretability%20in%20their%20decision-making%0Aprocesses%20becomes%20paramount.%20This%20paper%20presents%20the%20overview%20of%20a%20Robotic%0AeXplanation%20and%20Interpretability%20Engine%20%28ROXIE%29%2C%20which%20addresses%20this%20critical%0Aneed%2C%20aiming%20to%20demystify%20the%20opaque%20nature%20of%20complex%20robotic%20behaviors.%20This%0Apaper%20elucidates%20the%20key%20features%20and%20requirements%20needed%20for%20providing%0Ainformation%20and%20explanations%20about%20robot%20decision-making%20processes.%20It%20also%0Aoverviews%20the%20suite%20of%20software%20components%20and%20libraries%20available%20for%0Adeployment%20with%20ROS%202%2C%20empowering%20users%20to%20provide%20comprehensive%20explanations%0Aand%20interpretations%20of%20robot%20processes%20and%20behaviors%2C%20thereby%20fostering%20trust%0Aand%20collaboration%20in%20human-robot%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROXIE%3A%20Defining%20a%20Robotic%20eXplanation%20and%20Interpretability%20Engine&entry.906535625=Francisco%20J.%20Rodr%C3%ADguez-Lera%20and%20Miguel%20A.%20Gonz%C3%A1lez-Santamarta%20and%20Alejandro%20Gonz%C3%A1lez-Cant%C3%B3n%20and%20Laura%20Fern%C3%A1ndez-Becerra%20and%20David%20Sobr%C3%ADn-Hidalgo%20and%20Angel%20Manuel%20Guerrero-Higueras&entry.1292438233=%20%20In%20an%20era%20where%20autonomous%20robots%20increasingly%20inhabit%20public%20spaces%2C%20the%0Aimperative%20for%20transparency%20and%20interpretability%20in%20their%20decision-making%0Aprocesses%20becomes%20paramount.%20This%20paper%20presents%20the%20overview%20of%20a%20Robotic%0AeXplanation%20and%20Interpretability%20Engine%20%28ROXIE%29%2C%20which%20addresses%20this%20critical%0Aneed%2C%20aiming%20to%20demystify%20the%20opaque%20nature%20of%20complex%20robotic%20behaviors.%20This%0Apaper%20elucidates%20the%20key%20features%20and%20requirements%20needed%20for%20providing%0Ainformation%20and%20explanations%20about%20robot%20decision-making%20processes.%20It%20also%0Aoverviews%20the%20suite%20of%20software%20components%20and%20libraries%20available%20for%0Adeployment%20with%20ROS%202%2C%20empowering%20users%20to%20provide%20comprehensive%20explanations%0Aand%20interpretations%20of%20robot%20processes%20and%20behaviors%2C%20thereby%20fostering%20trust%0Aand%20collaboration%20in%20human-robot%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16606v1&entry.124074799=Read"},
{"title": "Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus", "author": "Chen Li and Ruijie Ma and Xiang Qian and Xiaohao Wang and Xinghui Li", "abstract": "  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n", "link": "http://arxiv.org/abs/2403.16607v1", "date": "2024-03-25", "relevancy": 1.4234, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4899}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4714}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4695}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Industrial%20Transfer%20Learning%20with%20Style%20Filter%3A%20Cost%20Reduction%0A%20%20and%20Defect-Focus&body=Title%3A%20Enhancing%20Industrial%20Transfer%20Learning%20with%20Style%20Filter%3A%20Cost%20Reduction%0A%20%20and%20Defect-Focus%0AAuthor%3A%20Chen%20Li%20and%20Ruijie%20Ma%20and%20Xiang%20Qian%20and%20Xiaohao%20Wang%20and%20Xinghui%20Li%0AAbstract%3A%20%20%20Addressing%20the%20challenge%20of%20data%20scarcity%20in%20industrial%20domains%2C%20transfer%0Alearning%20emerges%20as%20a%20pivotal%20paradigm.%20This%20work%20introduces%20Style%20Filter%2C%20a%0Atailored%20methodology%20for%20industrial%20contexts.%20By%20selectively%20filtering%20source%0Adomain%20data%20before%20knowledge%20transfer%2C%20Style%20Filter%20reduces%20the%20quantity%20of%0Adata%20while%20maintaining%20or%20even%20enhancing%20the%20performance%20of%20transfer%20learning%0Astrategy.%20Offering%20label-free%20operation%2C%20minimal%20reliance%20on%20prior%20knowledge%2C%0Aindependence%20from%20specific%20models%2C%20and%20re-utilization%2C%20Style%20Filter%20is%0Aevaluated%20on%20authentic%20industrial%20datasets%2C%20highlighting%20its%20effectiveness%20when%0Aemployed%20before%20conventional%20transfer%20strategies%20in%20the%20deep%20learning%20domain.%0AThe%20results%20underscore%20the%20effectiveness%20of%20Style%20Filter%20in%20real-world%0Aindustrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16607v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Industrial%20Transfer%20Learning%20with%20Style%20Filter%3A%20Cost%20Reduction%0A%20%20and%20Defect-Focus&entry.906535625=Chen%20Li%20and%20Ruijie%20Ma%20and%20Xiang%20Qian%20and%20Xiaohao%20Wang%20and%20Xinghui%20Li&entry.1292438233=%20%20Addressing%20the%20challenge%20of%20data%20scarcity%20in%20industrial%20domains%2C%20transfer%0Alearning%20emerges%20as%20a%20pivotal%20paradigm.%20This%20work%20introduces%20Style%20Filter%2C%20a%0Atailored%20methodology%20for%20industrial%20contexts.%20By%20selectively%20filtering%20source%0Adomain%20data%20before%20knowledge%20transfer%2C%20Style%20Filter%20reduces%20the%20quantity%20of%0Adata%20while%20maintaining%20or%20even%20enhancing%20the%20performance%20of%20transfer%20learning%0Astrategy.%20Offering%20label-free%20operation%2C%20minimal%20reliance%20on%20prior%20knowledge%2C%0Aindependence%20from%20specific%20models%2C%20and%20re-utilization%2C%20Style%20Filter%20is%0Aevaluated%20on%20authentic%20industrial%20datasets%2C%20highlighting%20its%20effectiveness%20when%0Aemployed%20before%20conventional%20transfer%20strategies%20in%20the%20deep%20learning%20domain.%0AThe%20results%20underscore%20the%20effectiveness%20of%20Style%20Filter%20in%20real-world%0Aindustrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16607v1&entry.124074799=Read"},
{"title": "A comparative analysis of embedding models for patent similarity", "author": "Grazia Sveva Ascione and Valerio Sterzi", "abstract": "  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n", "link": "http://arxiv.org/abs/2403.16630v1", "date": "2024-03-25", "relevancy": 1.4212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4958}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20comparative%20analysis%20of%20embedding%20models%20for%20patent%20similarity&body=Title%3A%20A%20comparative%20analysis%20of%20embedding%20models%20for%20patent%20similarity%0AAuthor%3A%20Grazia%20Sveva%20Ascione%20and%20Valerio%20Sterzi%0AAbstract%3A%20%20%20This%20paper%20makes%20two%20contributions%20to%20the%20field%20of%20text-based%20patent%0Asimilarity.%20First%2C%20it%20compares%20the%20performance%20of%20different%20kinds%20of%0Apatent-specific%20pretrained%20embedding%20models%2C%20namely%20static%20word%20embeddings%0A%28such%20as%20word2vec%20and%20doc2vec%20models%29%20and%20contextual%20word%20embeddings%20%28such%20as%0Atransformers%20based%20models%29%2C%20on%20the%20task%20of%20patent%20similarity%20calculation.%0ASecond%2C%20it%20compares%20specifically%20the%20performance%20of%20Sentence%20Transformers%0A%28SBERT%29%20architectures%20with%20different%20training%20phases%20on%20the%20patent%20similarity%0Atask.%20To%20assess%20the%20models%27%20performance%2C%20we%20use%20information%20about%20patent%0Ainterferences%2C%20a%20phenomenon%20in%20which%20two%20or%20more%20patent%20claims%20belonging%20to%0Adifferent%20patent%20applications%20are%20proven%20to%20be%20overlapping%20by%20patent%20examiners.%0ATherefore%2C%20we%20use%20these%20interferences%20cases%20as%20a%20proxy%20for%20maximum%20similarity%0Abetween%20two%20patents%2C%20treating%20them%20as%20ground-truth%20to%20evaluate%20the%20performance%0Aof%20the%20different%20embedding%20models.%20Our%20results%20point%20out%20that%2C%20first%2C%20Patent%0ASBERT-adapt-ub%2C%20the%20domain%20adaptation%20of%20the%20pretrained%20Sentence%20Transformer%0Aarchitecture%20proposed%20in%20this%20research%2C%20outperforms%20the%20current%0Astate-of-the-art%20in%20patent%20similarity.%20Second%2C%20they%20show%20that%2C%20in%20some%20cases%2C%0Alarge%20static%20models%20performances%20are%20still%20comparable%20to%20contextual%20ones%20when%0Atrained%20on%20extensive%20data%3B%20thus%2C%20we%20believe%20that%20the%20superiority%20in%20the%0Aperformance%20of%20contextual%20embeddings%20may%20not%20be%20related%20to%20the%20actual%0Aarchitecture%20but%20rather%20to%20the%20way%20the%20training%20phase%20is%20performed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16630v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comparative%20analysis%20of%20embedding%20models%20for%20patent%20similarity&entry.906535625=Grazia%20Sveva%20Ascione%20and%20Valerio%20Sterzi&entry.1292438233=%20%20This%20paper%20makes%20two%20contributions%20to%20the%20field%20of%20text-based%20patent%0Asimilarity.%20First%2C%20it%20compares%20the%20performance%20of%20different%20kinds%20of%0Apatent-specific%20pretrained%20embedding%20models%2C%20namely%20static%20word%20embeddings%0A%28such%20as%20word2vec%20and%20doc2vec%20models%29%20and%20contextual%20word%20embeddings%20%28such%20as%0Atransformers%20based%20models%29%2C%20on%20the%20task%20of%20patent%20similarity%20calculation.%0ASecond%2C%20it%20compares%20specifically%20the%20performance%20of%20Sentence%20Transformers%0A%28SBERT%29%20architectures%20with%20different%20training%20phases%20on%20the%20patent%20similarity%0Atask.%20To%20assess%20the%20models%27%20performance%2C%20we%20use%20information%20about%20patent%0Ainterferences%2C%20a%20phenomenon%20in%20which%20two%20or%20more%20patent%20claims%20belonging%20to%0Adifferent%20patent%20applications%20are%20proven%20to%20be%20overlapping%20by%20patent%20examiners.%0ATherefore%2C%20we%20use%20these%20interferences%20cases%20as%20a%20proxy%20for%20maximum%20similarity%0Abetween%20two%20patents%2C%20treating%20them%20as%20ground-truth%20to%20evaluate%20the%20performance%0Aof%20the%20different%20embedding%20models.%20Our%20results%20point%20out%20that%2C%20first%2C%20Patent%0ASBERT-adapt-ub%2C%20the%20domain%20adaptation%20of%20the%20pretrained%20Sentence%20Transformer%0Aarchitecture%20proposed%20in%20this%20research%2C%20outperforms%20the%20current%0Astate-of-the-art%20in%20patent%20similarity.%20Second%2C%20they%20show%20that%2C%20in%20some%20cases%2C%0Alarge%20static%20models%20performances%20are%20still%20comparable%20to%20contextual%20ones%20when%0Atrained%20on%20extensive%20data%3B%20thus%2C%20we%20believe%20that%20the%20superiority%20in%20the%0Aperformance%20of%20contextual%20embeddings%20may%20not%20be%20related%20to%20the%20actual%0Aarchitecture%20but%20rather%20to%20the%20way%20the%20training%20phase%20is%20performed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16630v1&entry.124074799=Read"},
{"title": "Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations", "author": "Nicolas Lazzari and Stefano De Giorgis and Aldo Gangemi and Valentina Presutti", "abstract": "  This paper presents sandra, a neuro-symbolic reasoner combining vectorial\nrepresentations with deductive reasoning. Sandra builds a vector space\nconstrained by an ontology and performs reasoning over it. The geometric nature\nof the reasoner allows its combination with neural networks, bridging the gap\nwith symbolic knowledge representations. Sandra is based on the Description and\nSituation (DnS) ontology design pattern, a formalization of frame semantics.\nGiven a set of facts (a situation) it allows to infer all possible perspectives\n(descriptions) that can provide a plausible interpretation for it, even in\npresence of incomplete information. We prove that our method is correct with\nrespect to the DnS model. We experiment with two different tasks and their\nstandard benchmarks, demonstrating that, without increasing complexity, sandra\n(i) outperforms all the baselines (ii) provides interpretability in the\nclassification process, and (iii) allows control over the vector space, which\nis designed a priori.\n", "link": "http://arxiv.org/abs/2402.00591v3", "date": "2024-03-25", "relevancy": 1.4041, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.435}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sandra%20--%20A%20Neuro-Symbolic%20Reasoner%20Based%20On%20Descriptions%20And%20Situations&body=Title%3A%20Sandra%20--%20A%20Neuro-Symbolic%20Reasoner%20Based%20On%20Descriptions%20And%20Situations%0AAuthor%3A%20Nicolas%20Lazzari%20and%20Stefano%20De%20Giorgis%20and%20Aldo%20Gangemi%20and%20Valentina%20Presutti%0AAbstract%3A%20%20%20This%20paper%20presents%20sandra%2C%20a%20neuro-symbolic%20reasoner%20combining%20vectorial%0Arepresentations%20with%20deductive%20reasoning.%20Sandra%20builds%20a%20vector%20space%0Aconstrained%20by%20an%20ontology%20and%20performs%20reasoning%20over%20it.%20The%20geometric%20nature%0Aof%20the%20reasoner%20allows%20its%20combination%20with%20neural%20networks%2C%20bridging%20the%20gap%0Awith%20symbolic%20knowledge%20representations.%20Sandra%20is%20based%20on%20the%20Description%20and%0ASituation%20%28DnS%29%20ontology%20design%20pattern%2C%20a%20formalization%20of%20frame%20semantics.%0AGiven%20a%20set%20of%20facts%20%28a%20situation%29%20it%20allows%20to%20infer%20all%20possible%20perspectives%0A%28descriptions%29%20that%20can%20provide%20a%20plausible%20interpretation%20for%20it%2C%20even%20in%0Apresence%20of%20incomplete%20information.%20We%20prove%20that%20our%20method%20is%20correct%20with%0Arespect%20to%20the%20DnS%20model.%20We%20experiment%20with%20two%20different%20tasks%20and%20their%0Astandard%20benchmarks%2C%20demonstrating%20that%2C%20without%20increasing%20complexity%2C%20sandra%0A%28i%29%20outperforms%20all%20the%20baselines%20%28ii%29%20provides%20interpretability%20in%20the%0Aclassification%20process%2C%20and%20%28iii%29%20allows%20control%20over%20the%20vector%20space%2C%20which%0Ais%20designed%20a%20priori.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00591v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sandra%20--%20A%20Neuro-Symbolic%20Reasoner%20Based%20On%20Descriptions%20And%20Situations&entry.906535625=Nicolas%20Lazzari%20and%20Stefano%20De%20Giorgis%20and%20Aldo%20Gangemi%20and%20Valentina%20Presutti&entry.1292438233=%20%20This%20paper%20presents%20sandra%2C%20a%20neuro-symbolic%20reasoner%20combining%20vectorial%0Arepresentations%20with%20deductive%20reasoning.%20Sandra%20builds%20a%20vector%20space%0Aconstrained%20by%20an%20ontology%20and%20performs%20reasoning%20over%20it.%20The%20geometric%20nature%0Aof%20the%20reasoner%20allows%20its%20combination%20with%20neural%20networks%2C%20bridging%20the%20gap%0Awith%20symbolic%20knowledge%20representations.%20Sandra%20is%20based%20on%20the%20Description%20and%0ASituation%20%28DnS%29%20ontology%20design%20pattern%2C%20a%20formalization%20of%20frame%20semantics.%0AGiven%20a%20set%20of%20facts%20%28a%20situation%29%20it%20allows%20to%20infer%20all%20possible%20perspectives%0A%28descriptions%29%20that%20can%20provide%20a%20plausible%20interpretation%20for%20it%2C%20even%20in%0Apresence%20of%20incomplete%20information.%20We%20prove%20that%20our%20method%20is%20correct%20with%0Arespect%20to%20the%20DnS%20model.%20We%20experiment%20with%20two%20different%20tasks%20and%20their%0Astandard%20benchmarks%2C%20demonstrating%20that%2C%20without%20increasing%20complexity%2C%20sandra%0A%28i%29%20outperforms%20all%20the%20baselines%20%28ii%29%20provides%20interpretability%20in%20the%0Aclassification%20process%2C%20and%20%28iii%29%20allows%20control%20over%20the%20vector%20space%2C%20which%0Ais%20designed%20a%20priori.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00591v3&entry.124074799=Read"},
{"title": "Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics", "author": "Rene Winchenbach and Nils Thuerey", "abstract": "  Learning physical simulations has been an essential and central aspect of\nmany recent research efforts in machine learning, particularly for\nNavier-Stokes-based fluid mechanics. Classic numerical solvers have\ntraditionally been computationally expensive and challenging to use in inverse\nproblems, whereas Neural solvers aim to address both concerns through machine\nlearning. We propose a general formulation for continuous convolutions using\nseparable basis functions as a superset of existing methods and evaluate a\nlarge set of basis functions in the context of (a) a compressible 1D SPH\nsimulation, (b) a weakly compressible 2D SPH simulation, and (c) an\nincompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\nincluded in the basis functions are key aspects of stability and accuracy. Our\nbroad evaluation shows that Fourier-based continuous convolutions outperform\nall other architectures regarding accuracy and generalization. Finally, using\nthese Fourier-based networks, we show that prior inductive biases, such as\nwindow functions, are no longer necessary. An implementation of our approach,\nas well as complete datasets and solver implementations, is available at\nhttps://github.com/tum-pbs/SFBC.\n", "link": "http://arxiv.org/abs/2403.16680v1", "date": "2024-03-25", "relevancy": 1.4024, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4817}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4462}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Symmetric%20Basis%20Convolutions%20for%20Learning%20Lagrangian%20Fluid%20Mechanics&body=Title%3A%20Symmetric%20Basis%20Convolutions%20for%20Learning%20Lagrangian%20Fluid%20Mechanics%0AAuthor%3A%20Rene%20Winchenbach%20and%20Nils%20Thuerey%0AAbstract%3A%20%20%20Learning%20physical%20simulations%20has%20been%20an%20essential%20and%20central%20aspect%20of%0Amany%20recent%20research%20efforts%20in%20machine%20learning%2C%20particularly%20for%0ANavier-Stokes-based%20fluid%20mechanics.%20Classic%20numerical%20solvers%20have%0Atraditionally%20been%20computationally%20expensive%20and%20challenging%20to%20use%20in%20inverse%0Aproblems%2C%20whereas%20Neural%20solvers%20aim%20to%20address%20both%20concerns%20through%20machine%0Alearning.%20We%20propose%20a%20general%20formulation%20for%20continuous%20convolutions%20using%0Aseparable%20basis%20functions%20as%20a%20superset%20of%20existing%20methods%20and%20evaluate%20a%0Alarge%20set%20of%20basis%20functions%20in%20the%20context%20of%20%28a%29%20a%20compressible%201D%20SPH%0Asimulation%2C%20%28b%29%20a%20weakly%20compressible%202D%20SPH%20simulation%2C%20and%20%28c%29%20an%0Aincompressible%202D%20SPH%20Simulation.%20We%20demonstrate%20that%20even%20and%20odd%20symmetries%0Aincluded%20in%20the%20basis%20functions%20are%20key%20aspects%20of%20stability%20and%20accuracy.%20Our%0Abroad%20evaluation%20shows%20that%20Fourier-based%20continuous%20convolutions%20outperform%0Aall%20other%20architectures%20regarding%20accuracy%20and%20generalization.%20Finally%2C%20using%0Athese%20Fourier-based%20networks%2C%20we%20show%20that%20prior%20inductive%20biases%2C%20such%20as%0Awindow%20functions%2C%20are%20no%20longer%20necessary.%20An%20implementation%20of%20our%20approach%2C%0Aas%20well%20as%20complete%20datasets%20and%20solver%20implementations%2C%20is%20available%20at%0Ahttps%3A//github.com/tum-pbs/SFBC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16680v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetric%20Basis%20Convolutions%20for%20Learning%20Lagrangian%20Fluid%20Mechanics&entry.906535625=Rene%20Winchenbach%20and%20Nils%20Thuerey&entry.1292438233=%20%20Learning%20physical%20simulations%20has%20been%20an%20essential%20and%20central%20aspect%20of%0Amany%20recent%20research%20efforts%20in%20machine%20learning%2C%20particularly%20for%0ANavier-Stokes-based%20fluid%20mechanics.%20Classic%20numerical%20solvers%20have%0Atraditionally%20been%20computationally%20expensive%20and%20challenging%20to%20use%20in%20inverse%0Aproblems%2C%20whereas%20Neural%20solvers%20aim%20to%20address%20both%20concerns%20through%20machine%0Alearning.%20We%20propose%20a%20general%20formulation%20for%20continuous%20convolutions%20using%0Aseparable%20basis%20functions%20as%20a%20superset%20of%20existing%20methods%20and%20evaluate%20a%0Alarge%20set%20of%20basis%20functions%20in%20the%20context%20of%20%28a%29%20a%20compressible%201D%20SPH%0Asimulation%2C%20%28b%29%20a%20weakly%20compressible%202D%20SPH%20simulation%2C%20and%20%28c%29%20an%0Aincompressible%202D%20SPH%20Simulation.%20We%20demonstrate%20that%20even%20and%20odd%20symmetries%0Aincluded%20in%20the%20basis%20functions%20are%20key%20aspects%20of%20stability%20and%20accuracy.%20Our%0Abroad%20evaluation%20shows%20that%20Fourier-based%20continuous%20convolutions%20outperform%0Aall%20other%20architectures%20regarding%20accuracy%20and%20generalization.%20Finally%2C%20using%0Athese%20Fourier-based%20networks%2C%20we%20show%20that%20prior%20inductive%20biases%2C%20such%20as%0Awindow%20functions%2C%20are%20no%20longer%20necessary.%20An%20implementation%20of%20our%20approach%2C%0Aas%20well%20as%20complete%20datasets%20and%20solver%20implementations%2C%20is%20available%20at%0Ahttps%3A//github.com/tum-pbs/SFBC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16680v1&entry.124074799=Read"},
{"title": "LOCOST: State-Space Models for Long Document Abstractive Summarization", "author": "Florian Le Bronnec and Song Duong and Mathieu Ravaut and Alexandre Allauzen and Nancy F. Chen and Vincent Guigue and Alberto Lumbreras and Laure Soulier and Patrick Gallinari", "abstract": "  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n", "link": "http://arxiv.org/abs/2401.17919v3", "date": "2024-03-25", "relevancy": 1.3915, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4688}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4678}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LOCOST%3A%20State-Space%20Models%20for%20Long%20Document%20Abstractive%20Summarization&body=Title%3A%20LOCOST%3A%20State-Space%20Models%20for%20Long%20Document%20Abstractive%20Summarization%0AAuthor%3A%20Florian%20Le%20Bronnec%20and%20Song%20Duong%20and%20Mathieu%20Ravaut%20and%20Alexandre%20Allauzen%20and%20Nancy%20F.%20Chen%20and%20Vincent%20Guigue%20and%20Alberto%20Lumbreras%20and%20Laure%20Soulier%20and%20Patrick%20Gallinari%0AAbstract%3A%20%20%20State-space%20models%20are%20a%20low-complexity%20alternative%20to%20transformers%20for%0Aencoding%20long%20sequences%20and%20capturing%20long-term%20dependencies.%20We%20propose%0ALOCOST%3A%20an%20encoder-decoder%20architecture%20based%20on%20state-space%20models%20for%0Aconditional%20text%20generation%20with%20long%20context%20inputs.%20With%20a%20computational%0Acomplexity%20of%20%24O%28L%20%5Clog%20L%29%24%2C%20this%20architecture%20can%20handle%20significantly%20longer%0Asequences%20than%20state-of-the-art%20models%20that%20are%20based%20on%20sparse%20attention%0Apatterns.%20We%20evaluate%20our%20model%20on%20a%20series%20of%20long%20document%20abstractive%0Asummarization%20tasks.%20The%20model%20reaches%20a%20performance%20level%20that%20is%2093-96%25%0Acomparable%20to%20the%20top-performing%20sparse%20transformers%20of%20the%20same%20size%20while%0Asaving%20up%20to%2050%25%20memory%20during%20training%20and%20up%20to%2087%25%20during%20inference.%0AAdditionally%2C%20LOCOST%20effectively%20handles%20input%20texts%20exceeding%20600K%20tokens%20at%0Ainference%20time%2C%20setting%20new%20state-of-the-art%20results%20on%20full-book%20summarization%0Aand%20opening%20new%20perspectives%20for%20long%20input%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17919v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOCOST%3A%20State-Space%20Models%20for%20Long%20Document%20Abstractive%20Summarization&entry.906535625=Florian%20Le%20Bronnec%20and%20Song%20Duong%20and%20Mathieu%20Ravaut%20and%20Alexandre%20Allauzen%20and%20Nancy%20F.%20Chen%20and%20Vincent%20Guigue%20and%20Alberto%20Lumbreras%20and%20Laure%20Soulier%20and%20Patrick%20Gallinari&entry.1292438233=%20%20State-space%20models%20are%20a%20low-complexity%20alternative%20to%20transformers%20for%0Aencoding%20long%20sequences%20and%20capturing%20long-term%20dependencies.%20We%20propose%0ALOCOST%3A%20an%20encoder-decoder%20architecture%20based%20on%20state-space%20models%20for%0Aconditional%20text%20generation%20with%20long%20context%20inputs.%20With%20a%20computational%0Acomplexity%20of%20%24O%28L%20%5Clog%20L%29%24%2C%20this%20architecture%20can%20handle%20significantly%20longer%0Asequences%20than%20state-of-the-art%20models%20that%20are%20based%20on%20sparse%20attention%0Apatterns.%20We%20evaluate%20our%20model%20on%20a%20series%20of%20long%20document%20abstractive%0Asummarization%20tasks.%20The%20model%20reaches%20a%20performance%20level%20that%20is%2093-96%25%0Acomparable%20to%20the%20top-performing%20sparse%20transformers%20of%20the%20same%20size%20while%0Asaving%20up%20to%2050%25%20memory%20during%20training%20and%20up%20to%2087%25%20during%20inference.%0AAdditionally%2C%20LOCOST%20effectively%20handles%20input%20texts%20exceeding%20600K%20tokens%20at%0Ainference%20time%2C%20setting%20new%20state-of-the-art%20results%20on%20full-book%20summarization%0Aand%20opening%20new%20perspectives%20for%20long%20input%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17919v3&entry.124074799=Read"},
{"title": "Design-Space Exploration of SNN Models using Application-Specific\n  Multi-Core Architectures", "author": " Sanaullah and Shamini Koravuna and Ulrich R\u00fcckert and Thorsten Jungeblut", "abstract": "  With the motivation and the difficulties that currently exist in\ncomprehending and utilizing the promising features of SNNs, we proposed a novel\nrun-time multi-core architecture-based simulator called \"RAVSim\" (Runtime\nAnalysis and Visualization Simulator), a cutting-edge SNN simulator, developed\nusing LabVIEW and it is publicly available on their website as an official\nmodule. RAVSim is a runtime virtual simulation environment tool that enables\nthe user to interact with the model, observe its behavior of output\nconcentration, and modify the set of parametric values at any time while the\nsimulation is in execution. Recently some popular tools have been presented,\nbut we believe that none of the tools allow users to interact with the model\nsimulation in run time.\n", "link": "http://arxiv.org/abs/2403.12061v2", "date": "2024-03-25", "relevancy": 1.3829, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.482}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4718}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Design-Space%20Exploration%20of%20SNN%20Models%20using%20Application-Specific%0A%20%20Multi-Core%20Architectures&body=Title%3A%20Design-Space%20Exploration%20of%20SNN%20Models%20using%20Application-Specific%0A%20%20Multi-Core%20Architectures%0AAuthor%3A%20%20Sanaullah%20and%20Shamini%20Koravuna%20and%20Ulrich%20R%C3%BCckert%20and%20Thorsten%20Jungeblut%0AAbstract%3A%20%20%20With%20the%20motivation%20and%20the%20difficulties%20that%20currently%20exist%20in%0Acomprehending%20and%20utilizing%20the%20promising%20features%20of%20SNNs%2C%20we%20proposed%20a%20novel%0Arun-time%20multi-core%20architecture-based%20simulator%20called%20%22RAVSim%22%20%28Runtime%0AAnalysis%20and%20Visualization%20Simulator%29%2C%20a%20cutting-edge%20SNN%20simulator%2C%20developed%0Ausing%20LabVIEW%20and%20it%20is%20publicly%20available%20on%20their%20website%20as%20an%20official%0Amodule.%20RAVSim%20is%20a%20runtime%20virtual%20simulation%20environment%20tool%20that%20enables%0Athe%20user%20to%20interact%20with%20the%20model%2C%20observe%20its%20behavior%20of%20output%0Aconcentration%2C%20and%20modify%20the%20set%20of%20parametric%20values%20at%20any%20time%20while%20the%0Asimulation%20is%20in%20execution.%20Recently%20some%20popular%20tools%20have%20been%20presented%2C%0Abut%20we%20believe%20that%20none%20of%20the%20tools%20allow%20users%20to%20interact%20with%20the%20model%0Asimulation%20in%20run%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12061v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design-Space%20Exploration%20of%20SNN%20Models%20using%20Application-Specific%0A%20%20Multi-Core%20Architectures&entry.906535625=%20Sanaullah%20and%20Shamini%20Koravuna%20and%20Ulrich%20R%C3%BCckert%20and%20Thorsten%20Jungeblut&entry.1292438233=%20%20With%20the%20motivation%20and%20the%20difficulties%20that%20currently%20exist%20in%0Acomprehending%20and%20utilizing%20the%20promising%20features%20of%20SNNs%2C%20we%20proposed%20a%20novel%0Arun-time%20multi-core%20architecture-based%20simulator%20called%20%22RAVSim%22%20%28Runtime%0AAnalysis%20and%20Visualization%20Simulator%29%2C%20a%20cutting-edge%20SNN%20simulator%2C%20developed%0Ausing%20LabVIEW%20and%20it%20is%20publicly%20available%20on%20their%20website%20as%20an%20official%0Amodule.%20RAVSim%20is%20a%20runtime%20virtual%20simulation%20environment%20tool%20that%20enables%0Athe%20user%20to%20interact%20with%20the%20model%2C%20observe%20its%20behavior%20of%20output%0Aconcentration%2C%20and%20modify%20the%20set%20of%20parametric%20values%20at%20any%20time%20while%20the%0Asimulation%20is%20in%20execution.%20Recently%20some%20popular%20tools%20have%20been%20presented%2C%0Abut%20we%20believe%20that%20none%20of%20the%20tools%20allow%20users%20to%20interact%20with%20the%20model%0Asimulation%20in%20run%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12061v2&entry.124074799=Read"},
{"title": "Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy", "author": "Xiaojin Zhang and Yulin Fei and Wei Chen and Hai Jin", "abstract": "  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n", "link": "http://arxiv.org/abs/2403.16591v1", "date": "2024-03-25", "relevancy": 1.3717, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4626}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4558}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4556}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy&body=Title%3A%20Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy%0AAuthor%3A%20Xiaojin%20Zhang%20and%20Yulin%20Fei%20and%20Wei%20Chen%20and%20Hai%20Jin%0AAbstract%3A%20%20%20The%20swift%20evolution%20of%20machine%20learning%20has%20led%20to%20emergence%20of%20various%0Adefinitions%20of%20privacy%20due%20to%20the%20threats%20it%20poses%20to%20privacy%2C%20including%20the%0Aconcept%20of%20local%20differential%20privacy%20%28LDP%29.%20Although%20widely%20embraced%20and%0Autilized%20across%20numerous%20domains%2C%20this%20conventional%20approach%20to%20measure%20privacy%0Astill%20exhibits%20certain%20limitations%2C%20spanning%20from%20failure%20to%20prevent%0Ainferential%20disclosure%20to%20lack%20of%20consideration%20for%20the%20adversary%27s%20background%0Aknowledge.%20In%20this%20comprehensive%20study%2C%20we%20introduce%20Bayesian%20privacy%20and%20delve%0Ainto%20the%20intricate%20relationship%20between%20local%20differential%20privacy%20and%20its%0ABayesian%20counterparts%2C%20unveiling%20novel%20insights%20into%20utility-privacy%0Atrade-offs.%20We%20introduce%20a%20framework%20that%20encapsulates%20both%20attack%20and%20defense%0Astrategies%2C%20highlighting%20their%20interplay%20and%20effectiveness.%20Our%20theoretical%0Acontributions%20are%20anchored%20in%20the%20rigorous%20definitions%20and%20relationships%0Abetween%20Average%20Bayesian%20Privacy%20%28ABP%29%20and%20Maximum%20Bayesian%20Privacy%20%28MBP%29%2C%0Aencapsulated%20by%20equations%20%24%5Cepsilon_%7Bp%2Ca%7D%20%5Cleq%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Csqrt%7B%28%5Cepsilon_%7Bp%2Cm%7D%20%2B%20%5Cepsilon%29%5Ccdot%28e%5E%7B%5Cepsilon_%7Bp%2Cm%7D%20%2B%0A%5Cepsilon%7D%20-%201%29%7D%24%20and%20the%20equivalence%20between%20%24%5Cxi%24-MBP%20and%20%242%5Cxi%24-LDP%0Aestablished%20under%20uniform%20prior%20distribution.%20These%20relationships%20fortify%20our%0Aunderstanding%20of%20the%20privacy%20guarantees%20provided%20by%20various%20mechanisms%2C%20leading%0Ato%20the%20realization%20that%20a%20mechanism%20satisfying%20%24%5Cxi%24-LDP%20also%20confers%0A%24%5Cxi%24-MBP%2C%20and%20vice%20versa.%20Our%20work%20not%20only%20lays%20the%20groundwork%20for%20future%0Aempirical%20exploration%20but%20also%20promises%20to%20enhance%20the%20design%20of%0Aprivacy-preserving%20algorithms%20that%20do%20not%20compromise%20on%20utility%2C%20thereby%0Afostering%20the%20development%20of%20trustworthy%20machine%20learning%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16591v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20the%20Interplay%20between%20Local%20Differential%20Privacy%2C%20Average%0A%20%20Bayesian%20Privacy%2C%20and%20Maximum%20Bayesian%20Privacy&entry.906535625=Xiaojin%20Zhang%20and%20Yulin%20Fei%20and%20Wei%20Chen%20and%20Hai%20Jin&entry.1292438233=%20%20The%20swift%20evolution%20of%20machine%20learning%20has%20led%20to%20emergence%20of%20various%0Adefinitions%20of%20privacy%20due%20to%20the%20threats%20it%20poses%20to%20privacy%2C%20including%20the%0Aconcept%20of%20local%20differential%20privacy%20%28LDP%29.%20Although%20widely%20embraced%20and%0Autilized%20across%20numerous%20domains%2C%20this%20conventional%20approach%20to%20measure%20privacy%0Astill%20exhibits%20certain%20limitations%2C%20spanning%20from%20failure%20to%20prevent%0Ainferential%20disclosure%20to%20lack%20of%20consideration%20for%20the%20adversary%27s%20background%0Aknowledge.%20In%20this%20comprehensive%20study%2C%20we%20introduce%20Bayesian%20privacy%20and%20delve%0Ainto%20the%20intricate%20relationship%20between%20local%20differential%20privacy%20and%20its%0ABayesian%20counterparts%2C%20unveiling%20novel%20insights%20into%20utility-privacy%0Atrade-offs.%20We%20introduce%20a%20framework%20that%20encapsulates%20both%20attack%20and%20defense%0Astrategies%2C%20highlighting%20their%20interplay%20and%20effectiveness.%20Our%20theoretical%0Acontributions%20are%20anchored%20in%20the%20rigorous%20definitions%20and%20relationships%0Abetween%20Average%20Bayesian%20Privacy%20%28ABP%29%20and%20Maximum%20Bayesian%20Privacy%20%28MBP%29%2C%0Aencapsulated%20by%20equations%20%24%5Cepsilon_%7Bp%2Ca%7D%20%5Cleq%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Csqrt%7B%28%5Cepsilon_%7Bp%2Cm%7D%20%2B%20%5Cepsilon%29%5Ccdot%28e%5E%7B%5Cepsilon_%7Bp%2Cm%7D%20%2B%0A%5Cepsilon%7D%20-%201%29%7D%24%20and%20the%20equivalence%20between%20%24%5Cxi%24-MBP%20and%20%242%5Cxi%24-LDP%0Aestablished%20under%20uniform%20prior%20distribution.%20These%20relationships%20fortify%20our%0Aunderstanding%20of%20the%20privacy%20guarantees%20provided%20by%20various%20mechanisms%2C%20leading%0Ato%20the%20realization%20that%20a%20mechanism%20satisfying%20%24%5Cxi%24-LDP%20also%20confers%0A%24%5Cxi%24-MBP%2C%20and%20vice%20versa.%20Our%20work%20not%20only%20lays%20the%20groundwork%20for%20future%0Aempirical%20exploration%20but%20also%20promises%20to%20enhance%20the%20design%20of%0Aprivacy-preserving%20algorithms%20that%20do%20not%20compromise%20on%20utility%2C%20thereby%0Afostering%20the%20development%20of%20trustworthy%20machine%20learning%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16591v1&entry.124074799=Read"},
{"title": "A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study", "author": "Osman Alperen Kora\u015f and J\u00f6rg Schl\u00f6tterer and Christin Seifert", "abstract": "  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n", "link": "http://arxiv.org/abs/2403.02930v2", "date": "2024-03-25", "relevancy": 1.3405, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4413}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Second%20Look%20on%20BASS%20--%20Boosting%20Abstractive%20Summarization%20with%20Unified%0A%20%20Semantic%20Graphs%20--%20A%20Replication%20Study&body=Title%3A%20A%20Second%20Look%20on%20BASS%20--%20Boosting%20Abstractive%20Summarization%20with%20Unified%0A%20%20Semantic%20Graphs%20--%20A%20Replication%20Study%0AAuthor%3A%20Osman%20Alperen%20Kora%C5%9F%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Christin%20Seifert%0AAbstract%3A%20%20%20We%20present%20a%20detailed%20replication%20study%20of%20the%20BASS%20framework%2C%20an%20abstractive%0Asummarization%20system%20based%20on%20the%20notion%20of%20Unified%20Semantic%20Graphs.%20Our%0Ainvestigation%20includes%20challenges%20in%20replicating%20key%20components%20and%20an%20ablation%0Astudy%20to%20systematically%20isolate%20error%20sources%20rooted%20in%20replicating%20novel%0Acomponents.%20Our%20findings%20reveal%20discrepancies%20in%20performance%20compared%20to%20the%0Aoriginal%20work.%20We%20highlight%20the%20significance%20of%20paying%20careful%20attention%20even%0Ato%20reasonably%20omitted%20details%20for%20replicating%20advanced%20frameworks%20like%20BASS%2C%0Aand%20emphasize%20key%20practices%20for%20writing%20replicable%20papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02930v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Second%20Look%20on%20BASS%20--%20Boosting%20Abstractive%20Summarization%20with%20Unified%0A%20%20Semantic%20Graphs%20--%20A%20Replication%20Study&entry.906535625=Osman%20Alperen%20Kora%C5%9F%20and%20J%C3%B6rg%20Schl%C3%B6tterer%20and%20Christin%20Seifert&entry.1292438233=%20%20We%20present%20a%20detailed%20replication%20study%20of%20the%20BASS%20framework%2C%20an%20abstractive%0Asummarization%20system%20based%20on%20the%20notion%20of%20Unified%20Semantic%20Graphs.%20Our%0Ainvestigation%20includes%20challenges%20in%20replicating%20key%20components%20and%20an%20ablation%0Astudy%20to%20systematically%20isolate%20error%20sources%20rooted%20in%20replicating%20novel%0Acomponents.%20Our%20findings%20reveal%20discrepancies%20in%20performance%20compared%20to%20the%0Aoriginal%20work.%20We%20highlight%20the%20significance%20of%20paying%20careful%20attention%20even%0Ato%20reasonably%20omitted%20details%20for%20replicating%20advanced%20frameworks%20like%20BASS%2C%0Aand%20emphasize%20key%20practices%20for%20writing%20replicable%20papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02930v2&entry.124074799=Read"},
{"title": "Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks", "author": "Huifeng Yin and Hanle Zheng and Jiayi Mao and Siyuan Ding and Xing Liu and Mingkun Xu and Yifan Hu and Jing Pei and Lei Deng", "abstract": "  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n", "link": "http://arxiv.org/abs/2403.16674v1", "date": "2024-03-25", "relevancy": 1.3381, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4346}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Functional%20Roles%20of%20Modelling%20Components%20in%20Spiking%0A%20%20Neural%20Networks&body=Title%3A%20Understanding%20the%20Functional%20Roles%20of%20Modelling%20Components%20in%20Spiking%0A%20%20Neural%20Networks%0AAuthor%3A%20Huifeng%20Yin%20and%20Hanle%20Zheng%20and%20Jiayi%20Mao%20and%20Siyuan%20Ding%20and%20Xing%20Liu%20and%20Mingkun%20Xu%20and%20Yifan%20Hu%20and%20Jing%20Pei%20and%20Lei%20Deng%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%2C%20inspired%20by%20the%20neural%20circuits%20of%20the%20brain%2C%0Aare%20promising%20in%20achieving%20high%20computational%20efficiency%20with%20biological%0Afidelity.%20Nevertheless%2C%20it%20is%20quite%20difficult%20to%20optimize%20SNNs%20because%20the%0Afunctional%20roles%20of%20their%20modelling%20components%20remain%20unclear.%20By%20designing%20and%0Aevaluating%20several%20variants%20of%20the%20classic%20model%2C%20we%20systematically%20investigate%0Athe%20functional%20roles%20of%20key%20modelling%20components%2C%20leakage%2C%20reset%2C%20and%0Arecurrence%2C%20in%20leaky%20integrate-and-fire%20%28LIF%29%20based%20SNNs.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20how%20these%20components%20influence%20the%20accuracy%2C%0Ageneralization%2C%20and%20robustness%20of%20SNNs.%20Specifically%2C%20we%20find%20that%20the%20leakage%0Aplays%20a%20crucial%20role%20in%20balancing%20memory%20retention%20and%20robustness%2C%20the%20reset%0Amechanism%20is%20essential%20for%20uninterrupted%20temporal%20processing%20and%20computational%0Aefficiency%2C%20and%20the%20recurrence%20enriches%20the%20capability%20to%20model%20complex%0Adynamics%20at%20a%20cost%20of%20robustness%20degradation.%20With%20these%20interesting%0Aobservations%2C%20we%20provide%20optimization%20suggestions%20for%20enhancing%20the%20performance%0Aof%20SNNs%20in%20different%20scenarios.%20This%20work%20deepens%20the%20understanding%20of%20how%20SNNs%0Awork%2C%20which%20offers%20valuable%20guidance%20for%20the%20development%20of%20more%20effective%20and%0Arobust%20neuromorphic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Functional%20Roles%20of%20Modelling%20Components%20in%20Spiking%0A%20%20Neural%20Networks&entry.906535625=Huifeng%20Yin%20and%20Hanle%20Zheng%20and%20Jiayi%20Mao%20and%20Siyuan%20Ding%20and%20Xing%20Liu%20and%20Mingkun%20Xu%20and%20Yifan%20Hu%20and%20Jing%20Pei%20and%20Lei%20Deng&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%2C%20inspired%20by%20the%20neural%20circuits%20of%20the%20brain%2C%0Aare%20promising%20in%20achieving%20high%20computational%20efficiency%20with%20biological%0Afidelity.%20Nevertheless%2C%20it%20is%20quite%20difficult%20to%20optimize%20SNNs%20because%20the%0Afunctional%20roles%20of%20their%20modelling%20components%20remain%20unclear.%20By%20designing%20and%0Aevaluating%20several%20variants%20of%20the%20classic%20model%2C%20we%20systematically%20investigate%0Athe%20functional%20roles%20of%20key%20modelling%20components%2C%20leakage%2C%20reset%2C%20and%0Arecurrence%2C%20in%20leaky%20integrate-and-fire%20%28LIF%29%20based%20SNNs.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20how%20these%20components%20influence%20the%20accuracy%2C%0Ageneralization%2C%20and%20robustness%20of%20SNNs.%20Specifically%2C%20we%20find%20that%20the%20leakage%0Aplays%20a%20crucial%20role%20in%20balancing%20memory%20retention%20and%20robustness%2C%20the%20reset%0Amechanism%20is%20essential%20for%20uninterrupted%20temporal%20processing%20and%20computational%0Aefficiency%2C%20and%20the%20recurrence%20enriches%20the%20capability%20to%20model%20complex%0Adynamics%20at%20a%20cost%20of%20robustness%20degradation.%20With%20these%20interesting%0Aobservations%2C%20we%20provide%20optimization%20suggestions%20for%20enhancing%20the%20performance%0Aof%20SNNs%20in%20different%20scenarios.%20This%20work%20deepens%20the%20understanding%20of%20how%20SNNs%0Awork%2C%20which%20offers%20valuable%20guidance%20for%20the%20development%20of%20more%20effective%20and%0Arobust%20neuromorphic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16674v1&entry.124074799=Read"},
{"title": "Robust Integral Consensus Control of Multi-Agent Networks Perturbed by\n  Matched and Unmatched Disturbances: The Case of Directed Graphs", "author": "Jose Guadalupe Romero and David Navarro-Alarcon", "abstract": "  This work presents a new method to design consensus controllers for perturbed\ndouble integrator systems whose interconnection is described by a directed\ngraph containing a rooted spanning tree. We propose new robust controllers to\nsolve the consensus and synchronization problems when the systems are under the\neffects of matched and unmatched disturbances. In both problems, we present\nsimple continuous controllers, whose integral actions allow us to handle the\ndisturbances. A rigorous stability analysis based on Lyapunov's direct method\nfor unperturbed networked systems is presented. To assess the performance of\nour result, a representative simulation study is presented.\n", "link": "http://arxiv.org/abs/2310.00262v2", "date": "2024-03-25", "relevancy": 1.3264, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4525}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.438}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Integral%20Consensus%20Control%20of%20Multi-Agent%20Networks%20Perturbed%20by%0A%20%20Matched%20and%20Unmatched%20Disturbances%3A%20The%20Case%20of%20Directed%20Graphs&body=Title%3A%20Robust%20Integral%20Consensus%20Control%20of%20Multi-Agent%20Networks%20Perturbed%20by%0A%20%20Matched%20and%20Unmatched%20Disturbances%3A%20The%20Case%20of%20Directed%20Graphs%0AAuthor%3A%20Jose%20Guadalupe%20Romero%20and%20David%20Navarro-Alarcon%0AAbstract%3A%20%20%20This%20work%20presents%20a%20new%20method%20to%20design%20consensus%20controllers%20for%20perturbed%0Adouble%20integrator%20systems%20whose%20interconnection%20is%20described%20by%20a%20directed%0Agraph%20containing%20a%20rooted%20spanning%20tree.%20We%20propose%20new%20robust%20controllers%20to%0Asolve%20the%20consensus%20and%20synchronization%20problems%20when%20the%20systems%20are%20under%20the%0Aeffects%20of%20matched%20and%20unmatched%20disturbances.%20In%20both%20problems%2C%20we%20present%0Asimple%20continuous%20controllers%2C%20whose%20integral%20actions%20allow%20us%20to%20handle%20the%0Adisturbances.%20A%20rigorous%20stability%20analysis%20based%20on%20Lyapunov%27s%20direct%20method%0Afor%20unperturbed%20networked%20systems%20is%20presented.%20To%20assess%20the%20performance%20of%0Aour%20result%2C%20a%20representative%20simulation%20study%20is%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00262v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Integral%20Consensus%20Control%20of%20Multi-Agent%20Networks%20Perturbed%20by%0A%20%20Matched%20and%20Unmatched%20Disturbances%3A%20The%20Case%20of%20Directed%20Graphs&entry.906535625=Jose%20Guadalupe%20Romero%20and%20David%20Navarro-Alarcon&entry.1292438233=%20%20This%20work%20presents%20a%20new%20method%20to%20design%20consensus%20controllers%20for%20perturbed%0Adouble%20integrator%20systems%20whose%20interconnection%20is%20described%20by%20a%20directed%0Agraph%20containing%20a%20rooted%20spanning%20tree.%20We%20propose%20new%20robust%20controllers%20to%0Asolve%20the%20consensus%20and%20synchronization%20problems%20when%20the%20systems%20are%20under%20the%0Aeffects%20of%20matched%20and%20unmatched%20disturbances.%20In%20both%20problems%2C%20we%20present%0Asimple%20continuous%20controllers%2C%20whose%20integral%20actions%20allow%20us%20to%20handle%20the%0Adisturbances.%20A%20rigorous%20stability%20analysis%20based%20on%20Lyapunov%27s%20direct%20method%0Afor%20unperturbed%20networked%20systems%20is%20presented.%20To%20assess%20the%20performance%20of%0Aour%20result%2C%20a%20representative%20simulation%20study%20is%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00262v2&entry.124074799=Read"},
{"title": "Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure", "author": "Lucas English and Mahdi Abolghasemi", "abstract": "  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n", "link": "http://arxiv.org/abs/2308.03472v2", "date": "2024-03-25", "relevancy": 1.2294, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4019}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20forecast%20accuracy%20of%20wind%20power%20by%20leveraging%20multiple%0A%20%20hierarchical%20structure&body=Title%3A%20Improving%20the%20forecast%20accuracy%20of%20wind%20power%20by%20leveraging%20multiple%0A%20%20hierarchical%20structure%0AAuthor%3A%20Lucas%20English%20and%20Mahdi%20Abolghasemi%0AAbstract%3A%20%20%20Renewable%20energy%20generation%20is%20of%20utmost%20importance%20for%20global%0Adecarbonization.%20Forecasting%20renewable%20energies%2C%20particularly%20wind%20energy%2C%20is%0Achallenging%20due%20to%20the%20inherent%20uncertainty%20in%20wind%20energy%20generation%2C%20which%0Adepends%20on%20weather%20conditions.%20Recent%20advances%20in%20hierarchical%20forecasting%0Athrough%20reconciliation%20have%20demonstrated%20a%20significant%20increase%20in%20the%20quality%0Aof%20wind%20energy%20forecasts%20for%20short-term%20periods.%20We%20leverage%20the%0Across-sectional%20and%20temporal%20hierarchical%20structure%20of%20turbines%20in%20wind%20farms%0Aand%20build%20cross-temporal%20hierarchies%20to%20further%20investigate%20how%20integrated%0Across-sectional%20and%20temporal%20dimensions%20can%20add%20value%20to%20forecast%20accuracy%20in%0Awind%20farms.%20We%20found%20that%20cross-temporal%20reconciliation%20was%20superior%20to%0Aindividual%20cross-sectional%20reconciliation%20at%20multiple%20temporal%20aggregations.%0AAdditionally%2C%20machine%20learning%20based%20forecasts%20that%20were%20cross-temporally%0Areconciled%20demonstrated%20high%20accuracy%20at%20coarser%20temporal%20granularities%2C%20which%0Amay%20encourage%20adoption%20for%20short-term%20wind%20forecasts.%20Empirically%2C%20we%20provide%0Ainsights%20for%20decision-makers%20on%20the%20best%20methods%20for%20forecasting%20high-frequency%0Awind%20data%20across%20different%20forecasting%20horizons%20and%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03472v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20forecast%20accuracy%20of%20wind%20power%20by%20leveraging%20multiple%0A%20%20hierarchical%20structure&entry.906535625=Lucas%20English%20and%20Mahdi%20Abolghasemi&entry.1292438233=%20%20Renewable%20energy%20generation%20is%20of%20utmost%20importance%20for%20global%0Adecarbonization.%20Forecasting%20renewable%20energies%2C%20particularly%20wind%20energy%2C%20is%0Achallenging%20due%20to%20the%20inherent%20uncertainty%20in%20wind%20energy%20generation%2C%20which%0Adepends%20on%20weather%20conditions.%20Recent%20advances%20in%20hierarchical%20forecasting%0Athrough%20reconciliation%20have%20demonstrated%20a%20significant%20increase%20in%20the%20quality%0Aof%20wind%20energy%20forecasts%20for%20short-term%20periods.%20We%20leverage%20the%0Across-sectional%20and%20temporal%20hierarchical%20structure%20of%20turbines%20in%20wind%20farms%0Aand%20build%20cross-temporal%20hierarchies%20to%20further%20investigate%20how%20integrated%0Across-sectional%20and%20temporal%20dimensions%20can%20add%20value%20to%20forecast%20accuracy%20in%0Awind%20farms.%20We%20found%20that%20cross-temporal%20reconciliation%20was%20superior%20to%0Aindividual%20cross-sectional%20reconciliation%20at%20multiple%20temporal%20aggregations.%0AAdditionally%2C%20machine%20learning%20based%20forecasts%20that%20were%20cross-temporally%0Areconciled%20demonstrated%20high%20accuracy%20at%20coarser%20temporal%20granularities%2C%20which%0Amay%20encourage%20adoption%20for%20short-term%20wind%20forecasts.%20Empirically%2C%20we%20provide%0Ainsights%20for%20decision-makers%20on%20the%20best%20methods%20for%20forecasting%20high-frequency%0Awind%20data%20across%20different%20forecasting%20horizons%20and%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03472v2&entry.124074799=Read"},
{"title": "The Adaptive Workplace: Orchestrating Architectural Services around the\n  Wellbeing of Individual Occupants", "author": "Andrew Vande Moere and Sara Arko and Alena Safrova Drasilova and Tom\u00e1\u0161 Ondr\u00e1\u010dek and Ilaria Pigliautile and Benedetta Pioppi and Anna Laura Pisello and Jakub Prochazka and Paula Acuna Roncancio and Davide Schaumann and Marcel Schweiker and Binh Vinh Duc Nguyen", "abstract": "  As the academic consortia members of the EU Horizon project SONATA\n(\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the\nworkshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\"\nby proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to\nadapt a workplace to the ever-changing needs of individual occupants, instead\nof that occupants are expected to adapt to their workplace.\n", "link": "http://arxiv.org/abs/2403.16595v1", "date": "2024-03-25", "relevancy": 1.2042, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4294}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3775}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Adaptive%20Workplace%3A%20Orchestrating%20Architectural%20Services%20around%20the%0A%20%20Wellbeing%20of%20Individual%20Occupants&body=Title%3A%20The%20Adaptive%20Workplace%3A%20Orchestrating%20Architectural%20Services%20around%20the%0A%20%20Wellbeing%20of%20Individual%20Occupants%0AAuthor%3A%20Andrew%20Vande%20Moere%20and%20Sara%20Arko%20and%20Alena%20Safrova%20Drasilova%20and%20Tom%C3%A1%C5%A1%20Ondr%C3%A1%C4%8Dek%20and%20Ilaria%20Pigliautile%20and%20Benedetta%20Pioppi%20and%20Anna%20Laura%20Pisello%20and%20Jakub%20Prochazka%20and%20Paula%20Acuna%20Roncancio%20and%20Davide%20Schaumann%20and%20Marcel%20Schweiker%20and%20Binh%20Vinh%20Duc%20Nguyen%0AAbstract%3A%20%20%20As%20the%20academic%20consortia%20members%20of%20the%20EU%20Horizon%20project%20SONATA%0A%28%22Situation-aware%20OrchestratioN%20of%20AdapTive%20Architecture%22%29%2C%20we%20respond%20to%20the%0Aworkshop%20call%20for%20%22Office%20Wellbeing%20by%20Design%3A%20Don%27t%20Stand%20for%20Anything%20Less%22%0Aby%20proposing%20the%20%22Adaptive%20Workplace%22%20concept.%20In%20essence%2C%20our%20vision%20aims%20to%0Aadapt%20a%20workplace%20to%20the%20ever-changing%20needs%20of%20individual%20occupants%2C%20instead%0Aof%20that%20occupants%20are%20expected%20to%20adapt%20to%20their%20workplace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16595v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Adaptive%20Workplace%3A%20Orchestrating%20Architectural%20Services%20around%20the%0A%20%20Wellbeing%20of%20Individual%20Occupants&entry.906535625=Andrew%20Vande%20Moere%20and%20Sara%20Arko%20and%20Alena%20Safrova%20Drasilova%20and%20Tom%C3%A1%C5%A1%20Ondr%C3%A1%C4%8Dek%20and%20Ilaria%20Pigliautile%20and%20Benedetta%20Pioppi%20and%20Anna%20Laura%20Pisello%20and%20Jakub%20Prochazka%20and%20Paula%20Acuna%20Roncancio%20and%20Davide%20Schaumann%20and%20Marcel%20Schweiker%20and%20Binh%20Vinh%20Duc%20Nguyen&entry.1292438233=%20%20As%20the%20academic%20consortia%20members%20of%20the%20EU%20Horizon%20project%20SONATA%0A%28%22Situation-aware%20OrchestratioN%20of%20AdapTive%20Architecture%22%29%2C%20we%20respond%20to%20the%0Aworkshop%20call%20for%20%22Office%20Wellbeing%20by%20Design%3A%20Don%27t%20Stand%20for%20Anything%20Less%22%0Aby%20proposing%20the%20%22Adaptive%20Workplace%22%20concept.%20In%20essence%2C%20our%20vision%20aims%20to%0Aadapt%20a%20workplace%20to%20the%20ever-changing%20needs%20of%20individual%20occupants%2C%20instead%0Aof%20that%20occupants%20are%20expected%20to%20adapt%20to%20their%20workplace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16595v1&entry.124074799=Read"},
{"title": "An Analysis of Linear Time Series Forecasting Models", "author": "William Toner and Luke Darlow", "abstract": "  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n", "link": "http://arxiv.org/abs/2403.14587v2", "date": "2024-03-25", "relevancy": 1.1846, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4155}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models&body=Title%3A%20An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models%0AAuthor%3A%20William%20Toner%20and%20Luke%20Darlow%0AAbstract%3A%20%20%20Despite%20their%20simplicity%2C%20linear%20models%20perform%20well%20at%20time%20series%0Aforecasting%2C%20even%20when%20pitted%20against%20deeper%20and%20more%20expensive%20models.%20A%0Anumber%20of%20variations%20to%20the%20linear%20model%20have%20been%20proposed%2C%20often%20including%0Asome%20form%20of%20feature%20normalisation%20that%20improves%20model%20generalisation.%20In%20this%0Apaper%20we%20analyse%20the%20sets%20of%20functions%20expressible%20using%20these%20linear%20model%0Aarchitectures.%20In%20so%20doing%20we%20show%20that%20several%20popular%20variants%20of%20linear%0Amodels%20for%20time%20series%20forecasting%20are%20equivalent%20and%20functionally%0Aindistinguishable%20from%20standard%2C%20unconstrained%20linear%20regression.%20We%0Acharacterise%20the%20model%20classes%20for%20each%20linear%20variant.%20We%20demonstrate%20that%0Aeach%20model%20can%20be%20reinterpreted%20as%20unconstrained%20linear%20regression%20over%20a%0Asuitably%20augmented%20feature%20set%2C%20and%20therefore%20admit%20closed-form%20solutions%20when%0Ausing%20a%20mean-squared%20loss%20function.%20We%20provide%20experimental%20evidence%20that%20the%0Amodels%20under%20inspection%20learn%20nearly%20identical%20solutions%2C%20and%20finally%0Ademonstrate%20that%20the%20simpler%20closed%20form%20solutions%20are%20superior%20forecasters%0Aacross%2072%25%20of%20test%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14587v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Analysis%20of%20Linear%20Time%20Series%20Forecasting%20Models&entry.906535625=William%20Toner%20and%20Luke%20Darlow&entry.1292438233=%20%20Despite%20their%20simplicity%2C%20linear%20models%20perform%20well%20at%20time%20series%0Aforecasting%2C%20even%20when%20pitted%20against%20deeper%20and%20more%20expensive%20models.%20A%0Anumber%20of%20variations%20to%20the%20linear%20model%20have%20been%20proposed%2C%20often%20including%0Asome%20form%20of%20feature%20normalisation%20that%20improves%20model%20generalisation.%20In%20this%0Apaper%20we%20analyse%20the%20sets%20of%20functions%20expressible%20using%20these%20linear%20model%0Aarchitectures.%20In%20so%20doing%20we%20show%20that%20several%20popular%20variants%20of%20linear%0Amodels%20for%20time%20series%20forecasting%20are%20equivalent%20and%20functionally%0Aindistinguishable%20from%20standard%2C%20unconstrained%20linear%20regression.%20We%0Acharacterise%20the%20model%20classes%20for%20each%20linear%20variant.%20We%20demonstrate%20that%0Aeach%20model%20can%20be%20reinterpreted%20as%20unconstrained%20linear%20regression%20over%20a%0Asuitably%20augmented%20feature%20set%2C%20and%20therefore%20admit%20closed-form%20solutions%20when%0Ausing%20a%20mean-squared%20loss%20function.%20We%20provide%20experimental%20evidence%20that%20the%0Amodels%20under%20inspection%20learn%20nearly%20identical%20solutions%2C%20and%20finally%0Ademonstrate%20that%20the%20simpler%20closed%20form%20solutions%20are%20superior%20forecasters%0Aacross%2072%25%20of%20test%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14587v2&entry.124074799=Read"},
{"title": "Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for\n  Computations in Matlab", "author": "Manel Velasco and Isiah Zaplana and Arnau D\u00f3ria-Cerezo and Pau Mart\u00ed", "abstract": "  Geometric algebra (GA) is a mathematical tool for geometric computing,\nproviding a framework that allows a unified and compact approach to geometric\nrelations which in other mathematical systems are typically described using\ndifferent more complicated elements. This fact has led to an increasing\nadoption of GA in applied mathematics and engineering problems. However, the\nscarcity of symbolic implementations of GA and its inherent complexity,\nrequiring a specific mathematical background, make it challenging and less\nintuitive for engineers to work with. This prevents wider adoption among more\napplied professionals. To address this challenge, this paper introduces SUGAR\n(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox\ndesigned for Matlab and licensed under the MIT License. SUGAR facilitates the\ntranslation of GA concepts into Matlab and provides a collection of\nuser-friendly functions tailored for GA computations, including support for\nsymbolic operations. It supports both numeric and symbolic computations in\nhigh-dimensional GAs. Specifically tailored for applied mathematics and\nengineering applications, SUGAR has been meticulously engineered to represent\ngeometric elements and transformations within two and three-dimensional\nprojective and conformal geometric algebras, aligning with established\ncomputational methodologies in the literature. Furthermore, SUGAR efficiently\nhandles functions of multivectors, such as exponential, logarithmic,\nsinusoidal, and cosine functions, enhancing its applicability across various\nengineering domains, including robotics, control systems, and power\nelectronics. Finally, this work includes four distinct validation examples,\ndemonstrating SUGAR's capabilities across the above-mentioned fields and its\npractical utility in addressing real-world applied mathematics and engineering\nproblems.\n", "link": "http://arxiv.org/abs/2403.16634v1", "date": "2024-03-25", "relevancy": 1.1673, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4027}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3771}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Symbolic%20and%20User-friendly%20Geometric%20Algebra%20Routines%20%28SUGAR%29%20for%0A%20%20Computations%20in%20Matlab&body=Title%3A%20Symbolic%20and%20User-friendly%20Geometric%20Algebra%20Routines%20%28SUGAR%29%20for%0A%20%20Computations%20in%20Matlab%0AAuthor%3A%20Manel%20Velasco%20and%20Isiah%20Zaplana%20and%20Arnau%20D%C3%B3ria-Cerezo%20and%20Pau%20Mart%C3%AD%0AAbstract%3A%20%20%20Geometric%20algebra%20%28GA%29%20is%20a%20mathematical%20tool%20for%20geometric%20computing%2C%0Aproviding%20a%20framework%20that%20allows%20a%20unified%20and%20compact%20approach%20to%20geometric%0Arelations%20which%20in%20other%20mathematical%20systems%20are%20typically%20described%20using%0Adifferent%20more%20complicated%20elements.%20This%20fact%20has%20led%20to%20an%20increasing%0Aadoption%20of%20GA%20in%20applied%20mathematics%20and%20engineering%20problems.%20However%2C%20the%0Ascarcity%20of%20symbolic%20implementations%20of%20GA%20and%20its%20inherent%20complexity%2C%0Arequiring%20a%20specific%20mathematical%20background%2C%20make%20it%20challenging%20and%20less%0Aintuitive%20for%20engineers%20to%20work%20with.%20This%20prevents%20wider%20adoption%20among%20more%0Aapplied%20professionals.%20To%20address%20this%20challenge%2C%20this%20paper%20introduces%20SUGAR%0A%28Symbolic%20and%20User-friendly%20Geometric%20Algebra%20Routines%29%2C%20an%20open-source%20toolbox%0Adesigned%20for%20Matlab%20and%20licensed%20under%20the%20MIT%20License.%20SUGAR%20facilitates%20the%0Atranslation%20of%20GA%20concepts%20into%20Matlab%20and%20provides%20a%20collection%20of%0Auser-friendly%20functions%20tailored%20for%20GA%20computations%2C%20including%20support%20for%0Asymbolic%20operations.%20It%20supports%20both%20numeric%20and%20symbolic%20computations%20in%0Ahigh-dimensional%20GAs.%20Specifically%20tailored%20for%20applied%20mathematics%20and%0Aengineering%20applications%2C%20SUGAR%20has%20been%20meticulously%20engineered%20to%20represent%0Ageometric%20elements%20and%20transformations%20within%20two%20and%20three-dimensional%0Aprojective%20and%20conformal%20geometric%20algebras%2C%20aligning%20with%20established%0Acomputational%20methodologies%20in%20the%20literature.%20Furthermore%2C%20SUGAR%20efficiently%0Ahandles%20functions%20of%20multivectors%2C%20such%20as%20exponential%2C%20logarithmic%2C%0Asinusoidal%2C%20and%20cosine%20functions%2C%20enhancing%20its%20applicability%20across%20various%0Aengineering%20domains%2C%20including%20robotics%2C%20control%20systems%2C%20and%20power%0Aelectronics.%20Finally%2C%20this%20work%20includes%20four%20distinct%20validation%20examples%2C%0Ademonstrating%20SUGAR%27s%20capabilities%20across%20the%20above-mentioned%20fields%20and%20its%0Apractical%20utility%20in%20addressing%20real-world%20applied%20mathematics%20and%20engineering%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20and%20User-friendly%20Geometric%20Algebra%20Routines%20%28SUGAR%29%20for%0A%20%20Computations%20in%20Matlab&entry.906535625=Manel%20Velasco%20and%20Isiah%20Zaplana%20and%20Arnau%20D%C3%B3ria-Cerezo%20and%20Pau%20Mart%C3%AD&entry.1292438233=%20%20Geometric%20algebra%20%28GA%29%20is%20a%20mathematical%20tool%20for%20geometric%20computing%2C%0Aproviding%20a%20framework%20that%20allows%20a%20unified%20and%20compact%20approach%20to%20geometric%0Arelations%20which%20in%20other%20mathematical%20systems%20are%20typically%20described%20using%0Adifferent%20more%20complicated%20elements.%20This%20fact%20has%20led%20to%20an%20increasing%0Aadoption%20of%20GA%20in%20applied%20mathematics%20and%20engineering%20problems.%20However%2C%20the%0Ascarcity%20of%20symbolic%20implementations%20of%20GA%20and%20its%20inherent%20complexity%2C%0Arequiring%20a%20specific%20mathematical%20background%2C%20make%20it%20challenging%20and%20less%0Aintuitive%20for%20engineers%20to%20work%20with.%20This%20prevents%20wider%20adoption%20among%20more%0Aapplied%20professionals.%20To%20address%20this%20challenge%2C%20this%20paper%20introduces%20SUGAR%0A%28Symbolic%20and%20User-friendly%20Geometric%20Algebra%20Routines%29%2C%20an%20open-source%20toolbox%0Adesigned%20for%20Matlab%20and%20licensed%20under%20the%20MIT%20License.%20SUGAR%20facilitates%20the%0Atranslation%20of%20GA%20concepts%20into%20Matlab%20and%20provides%20a%20collection%20of%0Auser-friendly%20functions%20tailored%20for%20GA%20computations%2C%20including%20support%20for%0Asymbolic%20operations.%20It%20supports%20both%20numeric%20and%20symbolic%20computations%20in%0Ahigh-dimensional%20GAs.%20Specifically%20tailored%20for%20applied%20mathematics%20and%0Aengineering%20applications%2C%20SUGAR%20has%20been%20meticulously%20engineered%20to%20represent%0Ageometric%20elements%20and%20transformations%20within%20two%20and%20three-dimensional%0Aprojective%20and%20conformal%20geometric%20algebras%2C%20aligning%20with%20established%0Acomputational%20methodologies%20in%20the%20literature.%20Furthermore%2C%20SUGAR%20efficiently%0Ahandles%20functions%20of%20multivectors%2C%20such%20as%20exponential%2C%20logarithmic%2C%0Asinusoidal%2C%20and%20cosine%20functions%2C%20enhancing%20its%20applicability%20across%20various%0Aengineering%20domains%2C%20including%20robotics%2C%20control%20systems%2C%20and%20power%0Aelectronics.%20Finally%2C%20this%20work%20includes%20four%20distinct%20validation%20examples%2C%0Ademonstrating%20SUGAR%27s%20capabilities%20across%20the%20above-mentioned%20fields%20and%20its%0Apractical%20utility%20in%20addressing%20real-world%20applied%20mathematics%20and%20engineering%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16634v1&entry.124074799=Read"},
{"title": "Towards a Formalisation of Value-based Actions and Consequentialist\n  Ethics", "author": "Adam Wyner and Tomasz Zurek and DOrota Stachura-Zurek", "abstract": "  Agents act to bring about a state of the world that is more compatible with\ntheir personal or institutional values. To formalise this intuition, the paper\nproposes an action framework based on the STRIPS formalisation. Technically,\nthe contribution expresses actions in terms of Value-based Formal Reasoning\n(VFR), which provides a set of propositions derived from an Agent's value\nprofile and the Agent's assessment of propositions with respect to the profile.\nConceptually, the contribution provides a computational framework for a form of\nconsequentialist ethics which is satisficing, luralistic, act-based, and\npreferential.\n", "link": "http://arxiv.org/abs/2403.16719v1", "date": "2024-03-25", "relevancy": 1.0851, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4013}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Formalisation%20of%20Value-based%20Actions%20and%20Consequentialist%0A%20%20Ethics&body=Title%3A%20Towards%20a%20Formalisation%20of%20Value-based%20Actions%20and%20Consequentialist%0A%20%20Ethics%0AAuthor%3A%20Adam%20Wyner%20and%20Tomasz%20Zurek%20and%20DOrota%20Stachura-Zurek%0AAbstract%3A%20%20%20Agents%20act%20to%20bring%20about%20a%20state%20of%20the%20world%20that%20is%20more%20compatible%20with%0Atheir%20personal%20or%20institutional%20values.%20To%20formalise%20this%20intuition%2C%20the%20paper%0Aproposes%20an%20action%20framework%20based%20on%20the%20STRIPS%20formalisation.%20Technically%2C%0Athe%20contribution%20expresses%20actions%20in%20terms%20of%20Value-based%20Formal%20Reasoning%0A%28VFR%29%2C%20which%20provides%20a%20set%20of%20propositions%20derived%20from%20an%20Agent%27s%20value%0Aprofile%20and%20the%20Agent%27s%20assessment%20of%20propositions%20with%20respect%20to%20the%20profile.%0AConceptually%2C%20the%20contribution%20provides%20a%20computational%20framework%20for%20a%20form%20of%0Aconsequentialist%20ethics%20which%20is%20satisficing%2C%20luralistic%2C%20act-based%2C%20and%0Apreferential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16719v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Formalisation%20of%20Value-based%20Actions%20and%20Consequentialist%0A%20%20Ethics&entry.906535625=Adam%20Wyner%20and%20Tomasz%20Zurek%20and%20DOrota%20Stachura-Zurek&entry.1292438233=%20%20Agents%20act%20to%20bring%20about%20a%20state%20of%20the%20world%20that%20is%20more%20compatible%20with%0Atheir%20personal%20or%20institutional%20values.%20To%20formalise%20this%20intuition%2C%20the%20paper%0Aproposes%20an%20action%20framework%20based%20on%20the%20STRIPS%20formalisation.%20Technically%2C%0Athe%20contribution%20expresses%20actions%20in%20terms%20of%20Value-based%20Formal%20Reasoning%0A%28VFR%29%2C%20which%20provides%20a%20set%20of%20propositions%20derived%20from%20an%20Agent%27s%20value%0Aprofile%20and%20the%20Agent%27s%20assessment%20of%20propositions%20with%20respect%20to%20the%20profile.%0AConceptually%2C%20the%20contribution%20provides%20a%20computational%20framework%20for%20a%20form%20of%0Aconsequentialist%20ethics%20which%20is%20satisficing%2C%20luralistic%2C%20act-based%2C%20and%0Apreferential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16719v1&entry.124074799=Read"},
{"title": "Revisiting the Sleeping Beauty problem", "author": "Paulo S. Piva and Gabriel Ruffolo", "abstract": "  The Sleeping Beauty problem is a probability riddle with no definite solution\nfor more than two decades and its solution is of great interest in many fields\nof knowledge. There are two main competing solutions to the problem: the halfer\napproach, and the thirder approach. The main reason for disagreement in the\nliterature is connected to the use of different probability spaces to represent\nthe same probabilistic riddle. In this work, we analyse the problem from a\nmathematical perspective, identifying probability distributions induced\ndirectly from the thought experiment's rules. The precise choices of\nprobability spaces provide both halfer and thirder solutions to the problem. To\ntry and decide on which approach to follow, a criterion involving the\ninformation available to Sleeping Beauty is proposed.\n", "link": "http://arxiv.org/abs/2403.16666v1", "date": "2024-03-25", "relevancy": 1.0738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3861}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3557}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Sleeping%20Beauty%20problem&body=Title%3A%20Revisiting%20the%20Sleeping%20Beauty%20problem%0AAuthor%3A%20Paulo%20S.%20Piva%20and%20Gabriel%20Ruffolo%0AAbstract%3A%20%20%20The%20Sleeping%20Beauty%20problem%20is%20a%20probability%20riddle%20with%20no%20definite%20solution%0Afor%20more%20than%20two%20decades%20and%20its%20solution%20is%20of%20great%20interest%20in%20many%20fields%0Aof%20knowledge.%20There%20are%20two%20main%20competing%20solutions%20to%20the%20problem%3A%20the%20halfer%0Aapproach%2C%20and%20the%20thirder%20approach.%20The%20main%20reason%20for%20disagreement%20in%20the%0Aliterature%20is%20connected%20to%20the%20use%20of%20different%20probability%20spaces%20to%20represent%0Athe%20same%20probabilistic%20riddle.%20In%20this%20work%2C%20we%20analyse%20the%20problem%20from%20a%0Amathematical%20perspective%2C%20identifying%20probability%20distributions%20induced%0Adirectly%20from%20the%20thought%20experiment%27s%20rules.%20The%20precise%20choices%20of%0Aprobability%20spaces%20provide%20both%20halfer%20and%20thirder%20solutions%20to%20the%20problem.%20To%0Atry%20and%20decide%20on%20which%20approach%20to%20follow%2C%20a%20criterion%20involving%20the%0Ainformation%20available%20to%20Sleeping%20Beauty%20is%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16666v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Sleeping%20Beauty%20problem&entry.906535625=Paulo%20S.%20Piva%20and%20Gabriel%20Ruffolo&entry.1292438233=%20%20The%20Sleeping%20Beauty%20problem%20is%20a%20probability%20riddle%20with%20no%20definite%20solution%0Afor%20more%20than%20two%20decades%20and%20its%20solution%20is%20of%20great%20interest%20in%20many%20fields%0Aof%20knowledge.%20There%20are%20two%20main%20competing%20solutions%20to%20the%20problem%3A%20the%20halfer%0Aapproach%2C%20and%20the%20thirder%20approach.%20The%20main%20reason%20for%20disagreement%20in%20the%0Aliterature%20is%20connected%20to%20the%20use%20of%20different%20probability%20spaces%20to%20represent%0Athe%20same%20probabilistic%20riddle.%20In%20this%20work%2C%20we%20analyse%20the%20problem%20from%20a%0Amathematical%20perspective%2C%20identifying%20probability%20distributions%20induced%0Adirectly%20from%20the%20thought%20experiment%27s%20rules.%20The%20precise%20choices%20of%0Aprobability%20spaces%20provide%20both%20halfer%20and%20thirder%20solutions%20to%20the%20problem.%20To%0Atry%20and%20decide%20on%20which%20approach%20to%20follow%2C%20a%20criterion%20involving%20the%0Ainformation%20available%20to%20Sleeping%20Beauty%20is%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16666v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


