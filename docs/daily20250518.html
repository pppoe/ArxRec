<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250515.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "author": "Yanbo Ding", "abstract": "  Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.\n", "link": "http://arxiv.org/abs/2505.10238v1", "date": "2025-05-15", "relevancy": 3.3094, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6881}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTVCrafter%3A%204D%20Motion%20Tokenization%20for%20Open-World%20Human%20Image%20Animation&body=Title%3A%20MTVCrafter%3A%204D%20Motion%20Tokenization%20for%20Open-World%20Human%20Image%20Animation%0AAuthor%3A%20Yanbo%20Ding%0AAbstract%3A%20%20%20Human%20image%20animation%20has%20gained%20increasing%20attention%20and%20developed%20rapidly%0Adue%20to%20its%20broad%20applications%20in%20digital%20humans.%20However%2C%20existing%20methods%20rely%0Alargely%20on%202D-rendered%20pose%20images%20for%20motion%20guidance%2C%20which%20limits%0Ageneralization%20and%20discards%20essential%203D%20information%20for%20open-world%20animation.%0ATo%20tackle%20this%20problem%2C%20we%20propose%20MTVCrafter%20%28Motion%20Tokenization%20Video%0ACrafter%29%2C%20the%20first%20framework%20that%20directly%20models%20raw%203D%20motion%20sequences%0A%28i.e.%2C%204D%20motion%29%20for%20human%20image%20animation.%20Specifically%2C%20we%20introduce%204DMoT%0A%284D%20motion%20tokenizer%29%20to%20quantize%203D%20motion%20sequences%20into%204D%20motion%20tokens.%0ACompared%20to%202D-rendered%20pose%20images%2C%204D%20motion%20tokens%20offer%20more%20robust%0Aspatio-temporal%20cues%20and%20avoid%20strict%20pixel-level%20alignment%20between%20pose%20image%0Aand%20character%2C%20enabling%20more%20flexible%20and%20disentangled%20control.%20Then%2C%20we%0Aintroduce%20MV-DiT%20%28Motion-aware%20Video%20DiT%29.%20By%20designing%20unique%20motion%20attention%0Awith%204D%20positional%20encodings%2C%20MV-DiT%20can%20effectively%20leverage%20motion%20tokens%20as%0A4D%20compact%20yet%20expressive%20context%20for%20human%20image%20animation%20in%20the%20complex%203D%0Aworld.%20Hence%2C%20it%20marks%20a%20significant%20step%20forward%20in%20this%20field%20and%20opens%20a%20new%0Adirection%20for%20pose-guided%20human%20video%20generation.%20Experiments%20show%20that%20our%0AMTVCrafter%20achieves%20state-of-the-art%20results%20with%20an%20FID-VID%20of%206.98%2C%0Asurpassing%20the%20second-best%20by%2065%25.%20Powered%20by%20robust%20motion%20tokens%2C%20MTVCrafter%0Aalso%20generalizes%20well%20to%20diverse%20open-world%20characters%20%28single/multiple%2C%0Afull/half-body%29%20across%20various%20styles%20and%20scenarios.%20Our%20video%20demos%20and%20code%0Aare%20provided%20in%20the%20supplementary%20material%20and%20at%20this%20anonymous%20GitHub%20link%3A%0Ahttps%3A//anonymous.4open.science/r/MTVCrafter-1B13.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTVCrafter%253A%25204D%2520Motion%2520Tokenization%2520for%2520Open-World%2520Human%2520Image%2520Animation%26entry.906535625%3DYanbo%2520Ding%26entry.1292438233%3D%2520%2520Human%2520image%2520animation%2520has%2520gained%2520increasing%2520attention%2520and%2520developed%2520rapidly%250Adue%2520to%2520its%2520broad%2520applications%2520in%2520digital%2520humans.%2520However%252C%2520existing%2520methods%2520rely%250Alargely%2520on%25202D-rendered%2520pose%2520images%2520for%2520motion%2520guidance%252C%2520which%2520limits%250Ageneralization%2520and%2520discards%2520essential%25203D%2520information%2520for%2520open-world%2520animation.%250ATo%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520MTVCrafter%2520%2528Motion%2520Tokenization%2520Video%250ACrafter%2529%252C%2520the%2520first%2520framework%2520that%2520directly%2520models%2520raw%25203D%2520motion%2520sequences%250A%2528i.e.%252C%25204D%2520motion%2529%2520for%2520human%2520image%2520animation.%2520Specifically%252C%2520we%2520introduce%25204DMoT%250A%25284D%2520motion%2520tokenizer%2529%2520to%2520quantize%25203D%2520motion%2520sequences%2520into%25204D%2520motion%2520tokens.%250ACompared%2520to%25202D-rendered%2520pose%2520images%252C%25204D%2520motion%2520tokens%2520offer%2520more%2520robust%250Aspatio-temporal%2520cues%2520and%2520avoid%2520strict%2520pixel-level%2520alignment%2520between%2520pose%2520image%250Aand%2520character%252C%2520enabling%2520more%2520flexible%2520and%2520disentangled%2520control.%2520Then%252C%2520we%250Aintroduce%2520MV-DiT%2520%2528Motion-aware%2520Video%2520DiT%2529.%2520By%2520designing%2520unique%2520motion%2520attention%250Awith%25204D%2520positional%2520encodings%252C%2520MV-DiT%2520can%2520effectively%2520leverage%2520motion%2520tokens%2520as%250A4D%2520compact%2520yet%2520expressive%2520context%2520for%2520human%2520image%2520animation%2520in%2520the%2520complex%25203D%250Aworld.%2520Hence%252C%2520it%2520marks%2520a%2520significant%2520step%2520forward%2520in%2520this%2520field%2520and%2520opens%2520a%2520new%250Adirection%2520for%2520pose-guided%2520human%2520video%2520generation.%2520Experiments%2520show%2520that%2520our%250AMTVCrafter%2520achieves%2520state-of-the-art%2520results%2520with%2520an%2520FID-VID%2520of%25206.98%252C%250Asurpassing%2520the%2520second-best%2520by%252065%2525.%2520Powered%2520by%2520robust%2520motion%2520tokens%252C%2520MTVCrafter%250Aalso%2520generalizes%2520well%2520to%2520diverse%2520open-world%2520characters%2520%2528single/multiple%252C%250Afull/half-body%2529%2520across%2520various%2520styles%2520and%2520scenarios.%2520Our%2520video%2520demos%2520and%2520code%250Aare%2520provided%2520in%2520the%2520supplementary%2520material%2520and%2520at%2520this%2520anonymous%2520GitHub%2520link%253A%250Ahttps%253A//anonymous.4open.science/r/MTVCrafter-1B13.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTVCrafter%3A%204D%20Motion%20Tokenization%20for%20Open-World%20Human%20Image%20Animation&entry.906535625=Yanbo%20Ding&entry.1292438233=%20%20Human%20image%20animation%20has%20gained%20increasing%20attention%20and%20developed%20rapidly%0Adue%20to%20its%20broad%20applications%20in%20digital%20humans.%20However%2C%20existing%20methods%20rely%0Alargely%20on%202D-rendered%20pose%20images%20for%20motion%20guidance%2C%20which%20limits%0Ageneralization%20and%20discards%20essential%203D%20information%20for%20open-world%20animation.%0ATo%20tackle%20this%20problem%2C%20we%20propose%20MTVCrafter%20%28Motion%20Tokenization%20Video%0ACrafter%29%2C%20the%20first%20framework%20that%20directly%20models%20raw%203D%20motion%20sequences%0A%28i.e.%2C%204D%20motion%29%20for%20human%20image%20animation.%20Specifically%2C%20we%20introduce%204DMoT%0A%284D%20motion%20tokenizer%29%20to%20quantize%203D%20motion%20sequences%20into%204D%20motion%20tokens.%0ACompared%20to%202D-rendered%20pose%20images%2C%204D%20motion%20tokens%20offer%20more%20robust%0Aspatio-temporal%20cues%20and%20avoid%20strict%20pixel-level%20alignment%20between%20pose%20image%0Aand%20character%2C%20enabling%20more%20flexible%20and%20disentangled%20control.%20Then%2C%20we%0Aintroduce%20MV-DiT%20%28Motion-aware%20Video%20DiT%29.%20By%20designing%20unique%20motion%20attention%0Awith%204D%20positional%20encodings%2C%20MV-DiT%20can%20effectively%20leverage%20motion%20tokens%20as%0A4D%20compact%20yet%20expressive%20context%20for%20human%20image%20animation%20in%20the%20complex%203D%0Aworld.%20Hence%2C%20it%20marks%20a%20significant%20step%20forward%20in%20this%20field%20and%20opens%20a%20new%0Adirection%20for%20pose-guided%20human%20video%20generation.%20Experiments%20show%20that%20our%0AMTVCrafter%20achieves%20state-of-the-art%20results%20with%20an%20FID-VID%20of%206.98%2C%0Asurpassing%20the%20second-best%20by%2065%25.%20Powered%20by%20robust%20motion%20tokens%2C%20MTVCrafter%0Aalso%20generalizes%20well%20to%20diverse%20open-world%20characters%20%28single/multiple%2C%0Afull/half-body%29%20across%20various%20styles%20and%20scenarios.%20Our%20video%20demos%20and%20code%0Aare%20provided%20in%20the%20supplementary%20material%20and%20at%20this%20anonymous%20GitHub%20link%3A%0Ahttps%3A//anonymous.4open.science/r/MTVCrafter-1B13.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10238v1&entry.124074799=Read"},
{"title": "PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for\n  View-Adaptive Rendering", "author": "Junxi Jin and Xiulai Li and Haiping Huang and Lianjun Liu and Yujie Sun and Logan Liu", "abstract": "  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in\nreal-time, high-quality 3D scene rendering. However, it faces several\nchallenges, including Gaussian redundancy, limited ability to capture\nview-dependent effects, and difficulties in handling complex lighting and\nspecular reflections. Additionally, methods that use spherical harmonics for\ncolor representation often struggle to effectively capture anisotropic\ncomponents, especially when modeling view-dependent colors under complex\nlighting conditions, leading to insufficient contrast and unnatural color\nsaturation. To address these limitations, we introduce PEP-GS, a\nperceptually-enhanced framework that dynamically predicts Gaussian attributes,\nincluding opacity, color, and covariance. We replace traditional spherical\nharmonics with a Hierarchical Granular-Structural Attention mechanism, which\nenables more accurate modeling of complex view-dependent color effects. By\nemploying a stable and interpretable framework for opacity and covariance\nestimation, PEP-GS avoids the removal of essential Gaussians prematurely,\nensuring a more accurate scene representation. Furthermore, perceptual\noptimization is applied to the final rendered images, enhancing perceptual\nconsistency across different views and ensuring high-quality renderings with\nimproved texture fidelity and fine-scale detail preservation. Experimental\nresults demonstrate that PEP-GS outperforms state-of-the-art methods,\nparticularly in challenging scenarios involving view-dependent effects and\nfine-scale details.\n", "link": "http://arxiv.org/abs/2411.05731v3", "date": "2025-05-15", "relevancy": 3.3032, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6821}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.655}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&body=Title%3A%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering%0AAuthor%3A%20Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun%20and%20Logan%20Liu%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20achieved%20significant%20success%20in%0Areal-time%2C%20high-quality%203D%20scene%20rendering.%20However%2C%20it%20faces%20several%0Achallenges%2C%20including%20Gaussian%20redundancy%2C%20limited%20ability%20to%20capture%0Aview-dependent%20effects%2C%20and%20difficulties%20in%20handling%20complex%20lighting%20and%0Aspecular%20reflections.%20Additionally%2C%20methods%20that%20use%20spherical%20harmonics%20for%0Acolor%20representation%20often%20struggle%20to%20effectively%20capture%20anisotropic%0Acomponents%2C%20especially%20when%20modeling%20view-dependent%20colors%20under%20complex%0Alighting%20conditions%2C%20leading%20to%20insufficient%20contrast%20and%20unnatural%20color%0Asaturation.%20To%20address%20these%20limitations%2C%20we%20introduce%20PEP-GS%2C%20a%0Aperceptually-enhanced%20framework%20that%20dynamically%20predicts%20Gaussian%20attributes%2C%0Aincluding%20opacity%2C%20color%2C%20and%20covariance.%20We%20replace%20traditional%20spherical%0Aharmonics%20with%20a%20Hierarchical%20Granular-Structural%20Attention%20mechanism%2C%20which%0Aenables%20more%20accurate%20modeling%20of%20complex%20view-dependent%20color%20effects.%20By%0Aemploying%20a%20stable%20and%20interpretable%20framework%20for%20opacity%20and%20covariance%0Aestimation%2C%20PEP-GS%20avoids%20the%20removal%20of%20essential%20Gaussians%20prematurely%2C%0Aensuring%20a%20more%20accurate%20scene%20representation.%20Furthermore%2C%20perceptual%0Aoptimization%20is%20applied%20to%20the%20final%20rendered%20images%2C%20enhancing%20perceptual%0Aconsistency%20across%20different%20views%20and%20ensuring%20high-quality%20renderings%20with%0Aimproved%20texture%20fidelity%20and%20fine-scale%20detail%20preservation.%20Experimental%0Aresults%20demonstrate%20that%20PEP-GS%20outperforms%20state-of-the-art%20methods%2C%0Aparticularly%20in%20challenging%20scenarios%20involving%20view-dependent%20effects%20and%0Afine-scale%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05731v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEP-GS%253A%2520Perceptually-Enhanced%2520Precise%2520Structured%25203D%2520Gaussians%2520for%250A%2520%2520View-Adaptive%2520Rendering%26entry.906535625%3DJunxi%2520Jin%2520and%2520Xiulai%2520Li%2520and%2520Haiping%2520Huang%2520and%2520Lianjun%2520Liu%2520and%2520Yujie%2520Sun%2520and%2520Logan%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520has%2520achieved%2520significant%2520success%2520in%250Areal-time%252C%2520high-quality%25203D%2520scene%2520rendering.%2520However%252C%2520it%2520faces%2520several%250Achallenges%252C%2520including%2520Gaussian%2520redundancy%252C%2520limited%2520ability%2520to%2520capture%250Aview-dependent%2520effects%252C%2520and%2520difficulties%2520in%2520handling%2520complex%2520lighting%2520and%250Aspecular%2520reflections.%2520Additionally%252C%2520methods%2520that%2520use%2520spherical%2520harmonics%2520for%250Acolor%2520representation%2520often%2520struggle%2520to%2520effectively%2520capture%2520anisotropic%250Acomponents%252C%2520especially%2520when%2520modeling%2520view-dependent%2520colors%2520under%2520complex%250Alighting%2520conditions%252C%2520leading%2520to%2520insufficient%2520contrast%2520and%2520unnatural%2520color%250Asaturation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520PEP-GS%252C%2520a%250Aperceptually-enhanced%2520framework%2520that%2520dynamically%2520predicts%2520Gaussian%2520attributes%252C%250Aincluding%2520opacity%252C%2520color%252C%2520and%2520covariance.%2520We%2520replace%2520traditional%2520spherical%250Aharmonics%2520with%2520a%2520Hierarchical%2520Granular-Structural%2520Attention%2520mechanism%252C%2520which%250Aenables%2520more%2520accurate%2520modeling%2520of%2520complex%2520view-dependent%2520color%2520effects.%2520By%250Aemploying%2520a%2520stable%2520and%2520interpretable%2520framework%2520for%2520opacity%2520and%2520covariance%250Aestimation%252C%2520PEP-GS%2520avoids%2520the%2520removal%2520of%2520essential%2520Gaussians%2520prematurely%252C%250Aensuring%2520a%2520more%2520accurate%2520scene%2520representation.%2520Furthermore%252C%2520perceptual%250Aoptimization%2520is%2520applied%2520to%2520the%2520final%2520rendered%2520images%252C%2520enhancing%2520perceptual%250Aconsistency%2520across%2520different%2520views%2520and%2520ensuring%2520high-quality%2520renderings%2520with%250Aimproved%2520texture%2520fidelity%2520and%2520fine-scale%2520detail%2520preservation.%2520Experimental%250Aresults%2520demonstrate%2520that%2520PEP-GS%2520outperforms%2520state-of-the-art%2520methods%252C%250Aparticularly%2520in%2520challenging%2520scenarios%2520involving%2520view-dependent%2520effects%2520and%250Afine-scale%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05731v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&entry.906535625=Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun%20and%20Logan%20Liu&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20achieved%20significant%20success%20in%0Areal-time%2C%20high-quality%203D%20scene%20rendering.%20However%2C%20it%20faces%20several%0Achallenges%2C%20including%20Gaussian%20redundancy%2C%20limited%20ability%20to%20capture%0Aview-dependent%20effects%2C%20and%20difficulties%20in%20handling%20complex%20lighting%20and%0Aspecular%20reflections.%20Additionally%2C%20methods%20that%20use%20spherical%20harmonics%20for%0Acolor%20representation%20often%20struggle%20to%20effectively%20capture%20anisotropic%0Acomponents%2C%20especially%20when%20modeling%20view-dependent%20colors%20under%20complex%0Alighting%20conditions%2C%20leading%20to%20insufficient%20contrast%20and%20unnatural%20color%0Asaturation.%20To%20address%20these%20limitations%2C%20we%20introduce%20PEP-GS%2C%20a%0Aperceptually-enhanced%20framework%20that%20dynamically%20predicts%20Gaussian%20attributes%2C%0Aincluding%20opacity%2C%20color%2C%20and%20covariance.%20We%20replace%20traditional%20spherical%0Aharmonics%20with%20a%20Hierarchical%20Granular-Structural%20Attention%20mechanism%2C%20which%0Aenables%20more%20accurate%20modeling%20of%20complex%20view-dependent%20color%20effects.%20By%0Aemploying%20a%20stable%20and%20interpretable%20framework%20for%20opacity%20and%20covariance%0Aestimation%2C%20PEP-GS%20avoids%20the%20removal%20of%20essential%20Gaussians%20prematurely%2C%0Aensuring%20a%20more%20accurate%20scene%20representation.%20Furthermore%2C%20perceptual%0Aoptimization%20is%20applied%20to%20the%20final%20rendered%20images%2C%20enhancing%20perceptual%0Aconsistency%20across%20different%20views%20and%20ensuring%20high-quality%20renderings%20with%0Aimproved%20texture%20fidelity%20and%20fine-scale%20detail%20preservation.%20Experimental%0Aresults%20demonstrate%20that%20PEP-GS%20outperforms%20state-of-the-art%20methods%2C%0Aparticularly%20in%20challenging%20scenarios%20involving%20view-dependent%20effects%20and%0Afine-scale%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05731v3&entry.124074799=Read"},
{"title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality", "author": "Xuechang Tu and Lukas Radl and Michael Steiner and Markus Steinberger and Bernhard Kerbl and Fernando de la Torre", "abstract": "  3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.\n", "link": "http://arxiv.org/abs/2505.10144v1", "date": "2025-05-15", "relevancy": 3.2522, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.716}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6267}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRSplat%3A%20Fast%20and%20Robust%20Gaussian%20Splatting%20for%20Virtual%20Reality&body=Title%3A%20VRSplat%3A%20Fast%20and%20Robust%20Gaussian%20Splatting%20for%20Virtual%20Reality%0AAuthor%3A%20Xuechang%20Tu%20and%20Lukas%20Radl%20and%20Michael%20Steiner%20and%20Markus%20Steinberger%20and%20Bernhard%20Kerbl%20and%20Fernando%20de%20la%20Torre%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20rapidly%20become%20a%20leading%20technique%20for%0Anovel-view%20synthesis%2C%20providing%20exceptional%20performance%20through%20efficient%0Asoftware-based%20GPU%20rasterization.%20Its%20versatility%20enables%20real-time%0Aapplications%2C%20including%20on%20mobile%20and%20lower-powered%20devices.%20However%2C%203DGS%0Afaces%20key%20challenges%20in%20virtual%20reality%20%28VR%29%3A%20%281%29%20temporal%20artifacts%2C%20such%20as%0Apopping%20during%20head%20movements%2C%20%282%29%20projection-based%20distortions%20that%20result%20in%0Adisturbing%20and%20view-inconsistent%20floaters%2C%20and%20%283%29%20reduced%20framerates%20when%0Arendering%20large%20numbers%20of%20Gaussians%2C%20falling%20below%20the%20critical%20threshold%20for%0AVR.%20Compared%20to%20desktop%20environments%2C%20these%20issues%20are%20drastically%20amplified%20by%0Alarge%20field-of-view%2C%20constant%20head%20movements%2C%20and%20high%20resolution%20of%0Ahead-mounted%20displays%20%28HMDs%29.%20In%20this%20work%2C%20we%20introduce%20VRSplat%3A%20we%20combine%0Aand%20extend%20several%20recent%20advancements%20in%203DGS%20to%20address%20challenges%20of%20VR%0Aholistically.%20We%20show%20how%20the%20ideas%20of%20Mini-Splatting%2C%20StopThePop%2C%20and%20Optimal%0AProjection%20can%20complement%20each%20other%2C%20by%20modifying%20the%20individual%20techniques%0Aand%20core%203DGS%20rasterizer.%20Additionally%2C%20we%20propose%20an%20efficient%20foveated%0Arasterizer%20that%20handles%20focus%20and%20peripheral%20areas%20in%20a%20single%20GPU%20launch%2C%0Aavoiding%20redundant%20computations%20and%20improving%20GPU%20utilization.%20Our%20method%20also%0Aincorporates%20a%20fine-tuning%20step%20that%20optimizes%20Gaussian%20parameters%20based%20on%0AStopThePop%20depth%20evaluations%20and%20Optimal%20Projection.%20We%20validate%20our%20method%0Athrough%20a%20controlled%20user%20study%20with%2025%20participants%2C%20showing%20a%20strong%0Apreference%20for%20VRSplat%20over%20other%20configurations%20of%20Mini-Splatting.%20VRSplat%20is%0Athe%20first%2C%20systematically%20evaluated%203DGS%20approach%20capable%20of%20supporting%20modern%0AVR%20applications%2C%20achieving%2072%2B%20FPS%20while%20eliminating%20popping%20and%0Astereo-disrupting%20floaters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRSplat%253A%2520Fast%2520and%2520Robust%2520Gaussian%2520Splatting%2520for%2520Virtual%2520Reality%26entry.906535625%3DXuechang%2520Tu%2520and%2520Lukas%2520Radl%2520and%2520Michael%2520Steiner%2520and%2520Markus%2520Steinberger%2520and%2520Bernhard%2520Kerbl%2520and%2520Fernando%2520de%2520la%2520Torre%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520rapidly%2520become%2520a%2520leading%2520technique%2520for%250Anovel-view%2520synthesis%252C%2520providing%2520exceptional%2520performance%2520through%2520efficient%250Asoftware-based%2520GPU%2520rasterization.%2520Its%2520versatility%2520enables%2520real-time%250Aapplications%252C%2520including%2520on%2520mobile%2520and%2520lower-powered%2520devices.%2520However%252C%25203DGS%250Afaces%2520key%2520challenges%2520in%2520virtual%2520reality%2520%2528VR%2529%253A%2520%25281%2529%2520temporal%2520artifacts%252C%2520such%2520as%250Apopping%2520during%2520head%2520movements%252C%2520%25282%2529%2520projection-based%2520distortions%2520that%2520result%2520in%250Adisturbing%2520and%2520view-inconsistent%2520floaters%252C%2520and%2520%25283%2529%2520reduced%2520framerates%2520when%250Arendering%2520large%2520numbers%2520of%2520Gaussians%252C%2520falling%2520below%2520the%2520critical%2520threshold%2520for%250AVR.%2520Compared%2520to%2520desktop%2520environments%252C%2520these%2520issues%2520are%2520drastically%2520amplified%2520by%250Alarge%2520field-of-view%252C%2520constant%2520head%2520movements%252C%2520and%2520high%2520resolution%2520of%250Ahead-mounted%2520displays%2520%2528HMDs%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VRSplat%253A%2520we%2520combine%250Aand%2520extend%2520several%2520recent%2520advancements%2520in%25203DGS%2520to%2520address%2520challenges%2520of%2520VR%250Aholistically.%2520We%2520show%2520how%2520the%2520ideas%2520of%2520Mini-Splatting%252C%2520StopThePop%252C%2520and%2520Optimal%250AProjection%2520can%2520complement%2520each%2520other%252C%2520by%2520modifying%2520the%2520individual%2520techniques%250Aand%2520core%25203DGS%2520rasterizer.%2520Additionally%252C%2520we%2520propose%2520an%2520efficient%2520foveated%250Arasterizer%2520that%2520handles%2520focus%2520and%2520peripheral%2520areas%2520in%2520a%2520single%2520GPU%2520launch%252C%250Aavoiding%2520redundant%2520computations%2520and%2520improving%2520GPU%2520utilization.%2520Our%2520method%2520also%250Aincorporates%2520a%2520fine-tuning%2520step%2520that%2520optimizes%2520Gaussian%2520parameters%2520based%2520on%250AStopThePop%2520depth%2520evaluations%2520and%2520Optimal%2520Projection.%2520We%2520validate%2520our%2520method%250Athrough%2520a%2520controlled%2520user%2520study%2520with%252025%2520participants%252C%2520showing%2520a%2520strong%250Apreference%2520for%2520VRSplat%2520over%2520other%2520configurations%2520of%2520Mini-Splatting.%2520VRSplat%2520is%250Athe%2520first%252C%2520systematically%2520evaluated%25203DGS%2520approach%2520capable%2520of%2520supporting%2520modern%250AVR%2520applications%252C%2520achieving%252072%252B%2520FPS%2520while%2520eliminating%2520popping%2520and%250Astereo-disrupting%2520floaters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRSplat%3A%20Fast%20and%20Robust%20Gaussian%20Splatting%20for%20Virtual%20Reality&entry.906535625=Xuechang%20Tu%20and%20Lukas%20Radl%20and%20Michael%20Steiner%20and%20Markus%20Steinberger%20and%20Bernhard%20Kerbl%20and%20Fernando%20de%20la%20Torre&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20rapidly%20become%20a%20leading%20technique%20for%0Anovel-view%20synthesis%2C%20providing%20exceptional%20performance%20through%20efficient%0Asoftware-based%20GPU%20rasterization.%20Its%20versatility%20enables%20real-time%0Aapplications%2C%20including%20on%20mobile%20and%20lower-powered%20devices.%20However%2C%203DGS%0Afaces%20key%20challenges%20in%20virtual%20reality%20%28VR%29%3A%20%281%29%20temporal%20artifacts%2C%20such%20as%0Apopping%20during%20head%20movements%2C%20%282%29%20projection-based%20distortions%20that%20result%20in%0Adisturbing%20and%20view-inconsistent%20floaters%2C%20and%20%283%29%20reduced%20framerates%20when%0Arendering%20large%20numbers%20of%20Gaussians%2C%20falling%20below%20the%20critical%20threshold%20for%0AVR.%20Compared%20to%20desktop%20environments%2C%20these%20issues%20are%20drastically%20amplified%20by%0Alarge%20field-of-view%2C%20constant%20head%20movements%2C%20and%20high%20resolution%20of%0Ahead-mounted%20displays%20%28HMDs%29.%20In%20this%20work%2C%20we%20introduce%20VRSplat%3A%20we%20combine%0Aand%20extend%20several%20recent%20advancements%20in%203DGS%20to%20address%20challenges%20of%20VR%0Aholistically.%20We%20show%20how%20the%20ideas%20of%20Mini-Splatting%2C%20StopThePop%2C%20and%20Optimal%0AProjection%20can%20complement%20each%20other%2C%20by%20modifying%20the%20individual%20techniques%0Aand%20core%203DGS%20rasterizer.%20Additionally%2C%20we%20propose%20an%20efficient%20foveated%0Arasterizer%20that%20handles%20focus%20and%20peripheral%20areas%20in%20a%20single%20GPU%20launch%2C%0Aavoiding%20redundant%20computations%20and%20improving%20GPU%20utilization.%20Our%20method%20also%0Aincorporates%20a%20fine-tuning%20step%20that%20optimizes%20Gaussian%20parameters%20based%20on%0AStopThePop%20depth%20evaluations%20and%20Optimal%20Projection.%20We%20validate%20our%20method%0Athrough%20a%20controlled%20user%20study%20with%2025%20participants%2C%20showing%20a%20strong%0Apreference%20for%20VRSplat%20over%20other%20configurations%20of%20Mini-Splatting.%20VRSplat%20is%0Athe%20first%2C%20systematically%20evaluated%203DGS%20approach%20capable%20of%20supporting%20modern%0AVR%20applications%2C%20achieving%2072%2B%20FPS%20while%20eliminating%20popping%20and%0Astereo-disrupting%20floaters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10144v1&entry.124074799=Read"},
{"title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "author": "Yen-Chi Cheng and Krishna Kumar Singh and Jae Shin Yoon and Alex Schwing and Liangyan Gui and Matheus Gadelha and Paul Guerrero and Nanxuan Zhao", "abstract": "  Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/\n", "link": "http://arxiv.org/abs/2505.10566v1", "date": "2025-05-15", "relevancy": 3.2325, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6724}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6336}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Fixup%3A%20Advancing%20Photo%20Editing%20with%203D%20Priors&body=Title%3A%203D-Fixup%3A%20Advancing%20Photo%20Editing%20with%203D%20Priors%0AAuthor%3A%20Yen-Chi%20Cheng%20and%20Krishna%20Kumar%20Singh%20and%20Jae%20Shin%20Yoon%20and%20Alex%20Schwing%20and%20Liangyan%20Gui%20and%20Matheus%20Gadelha%20and%20Paul%20Guerrero%20and%20Nanxuan%20Zhao%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20modeling%20image%20priors%20via%20diffusion%20models%2C%0A3D-aware%20image%20editing%20remains%20challenging%2C%20in%20part%20because%20the%20object%20is%20only%0Aspecified%20via%20a%20single%20image.%20To%20tackle%20this%20challenge%2C%20we%20propose%203D-Fixup%2C%20a%0Anew%20framework%20for%20editing%202D%20images%20guided%20by%20learned%203D%20priors.%20The%20framework%0Asupports%20difficult%20editing%20situations%20such%20as%20object%20translation%20and%203D%0Arotation.%20To%20achieve%20this%2C%20we%20leverage%20a%20training-based%20approach%20that%20harnesses%0Athe%20generative%20power%20of%20diffusion%20models.%20As%20video%20data%20naturally%20encodes%0Areal-world%20physical%20dynamics%2C%20we%20turn%20to%20video%20data%20for%20generating%20training%0Adata%20pairs%2C%20i.e.%2C%20a%20source%20and%20a%20target%20frame.%20Rather%20than%20relying%20solely%20on%20a%0Asingle%20trained%20model%20to%20infer%20transformations%20between%20source%20and%20target%20frames%2C%0Awe%20incorporate%203D%20guidance%20from%20an%20Image-to-3D%20model%2C%20which%20bridges%20this%0Achallenging%20task%20by%20explicitly%20projecting%202D%20information%20into%203D%20space.%20We%0Adesign%20a%20data%20generation%20pipeline%20to%20ensure%20high-quality%203D%20guidance%20throughout%0Atraining.%20Results%20show%20that%20by%20integrating%20these%203D%20priors%2C%203D-Fixup%0Aeffectively%20supports%20complex%2C%20identity%20coherent%203D-aware%20edits%2C%20achieving%0Ahigh-quality%20results%20and%20advancing%20the%20application%20of%20diffusion%20models%20in%0Arealistic%20image%20manipulation.%20The%20code%20is%20provided%20at%0Ahttps%3A//3dfixup.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Fixup%253A%2520Advancing%2520Photo%2520Editing%2520with%25203D%2520Priors%26entry.906535625%3DYen-Chi%2520Cheng%2520and%2520Krishna%2520Kumar%2520Singh%2520and%2520Jae%2520Shin%2520Yoon%2520and%2520Alex%2520Schwing%2520and%2520Liangyan%2520Gui%2520and%2520Matheus%2520Gadelha%2520and%2520Paul%2520Guerrero%2520and%2520Nanxuan%2520Zhao%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520modeling%2520image%2520priors%2520via%2520diffusion%2520models%252C%250A3D-aware%2520image%2520editing%2520remains%2520challenging%252C%2520in%2520part%2520because%2520the%2520object%2520is%2520only%250Aspecified%2520via%2520a%2520single%2520image.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%25203D-Fixup%252C%2520a%250Anew%2520framework%2520for%2520editing%25202D%2520images%2520guided%2520by%2520learned%25203D%2520priors.%2520The%2520framework%250Asupports%2520difficult%2520editing%2520situations%2520such%2520as%2520object%2520translation%2520and%25203D%250Arotation.%2520To%2520achieve%2520this%252C%2520we%2520leverage%2520a%2520training-based%2520approach%2520that%2520harnesses%250Athe%2520generative%2520power%2520of%2520diffusion%2520models.%2520As%2520video%2520data%2520naturally%2520encodes%250Areal-world%2520physical%2520dynamics%252C%2520we%2520turn%2520to%2520video%2520data%2520for%2520generating%2520training%250Adata%2520pairs%252C%2520i.e.%252C%2520a%2520source%2520and%2520a%2520target%2520frame.%2520Rather%2520than%2520relying%2520solely%2520on%2520a%250Asingle%2520trained%2520model%2520to%2520infer%2520transformations%2520between%2520source%2520and%2520target%2520frames%252C%250Awe%2520incorporate%25203D%2520guidance%2520from%2520an%2520Image-to-3D%2520model%252C%2520which%2520bridges%2520this%250Achallenging%2520task%2520by%2520explicitly%2520projecting%25202D%2520information%2520into%25203D%2520space.%2520We%250Adesign%2520a%2520data%2520generation%2520pipeline%2520to%2520ensure%2520high-quality%25203D%2520guidance%2520throughout%250Atraining.%2520Results%2520show%2520that%2520by%2520integrating%2520these%25203D%2520priors%252C%25203D-Fixup%250Aeffectively%2520supports%2520complex%252C%2520identity%2520coherent%25203D-aware%2520edits%252C%2520achieving%250Ahigh-quality%2520results%2520and%2520advancing%2520the%2520application%2520of%2520diffusion%2520models%2520in%250Arealistic%2520image%2520manipulation.%2520The%2520code%2520is%2520provided%2520at%250Ahttps%253A//3dfixup.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Fixup%3A%20Advancing%20Photo%20Editing%20with%203D%20Priors&entry.906535625=Yen-Chi%20Cheng%20and%20Krishna%20Kumar%20Singh%20and%20Jae%20Shin%20Yoon%20and%20Alex%20Schwing%20and%20Liangyan%20Gui%20and%20Matheus%20Gadelha%20and%20Paul%20Guerrero%20and%20Nanxuan%20Zhao&entry.1292438233=%20%20Despite%20significant%20advances%20in%20modeling%20image%20priors%20via%20diffusion%20models%2C%0A3D-aware%20image%20editing%20remains%20challenging%2C%20in%20part%20because%20the%20object%20is%20only%0Aspecified%20via%20a%20single%20image.%20To%20tackle%20this%20challenge%2C%20we%20propose%203D-Fixup%2C%20a%0Anew%20framework%20for%20editing%202D%20images%20guided%20by%20learned%203D%20priors.%20The%20framework%0Asupports%20difficult%20editing%20situations%20such%20as%20object%20translation%20and%203D%0Arotation.%20To%20achieve%20this%2C%20we%20leverage%20a%20training-based%20approach%20that%20harnesses%0Athe%20generative%20power%20of%20diffusion%20models.%20As%20video%20data%20naturally%20encodes%0Areal-world%20physical%20dynamics%2C%20we%20turn%20to%20video%20data%20for%20generating%20training%0Adata%20pairs%2C%20i.e.%2C%20a%20source%20and%20a%20target%20frame.%20Rather%20than%20relying%20solely%20on%20a%0Asingle%20trained%20model%20to%20infer%20transformations%20between%20source%20and%20target%20frames%2C%0Awe%20incorporate%203D%20guidance%20from%20an%20Image-to-3D%20model%2C%20which%20bridges%20this%0Achallenging%20task%20by%20explicitly%20projecting%202D%20information%20into%203D%20space.%20We%0Adesign%20a%20data%20generation%20pipeline%20to%20ensure%20high-quality%203D%20guidance%20throughout%0Atraining.%20Results%20show%20that%20by%20integrating%20these%203D%20priors%2C%203D-Fixup%0Aeffectively%20supports%20complex%2C%20identity%20coherent%203D-aware%20edits%2C%20achieving%0Ahigh-quality%20results%20and%20advancing%20the%20application%20of%20diffusion%20models%20in%0Arealistic%20image%20manipulation.%20The%20code%20is%20provided%20at%0Ahttps%3A//3dfixup.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10566v1&entry.124074799=Read"},
{"title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting", "author": "Fengdi Zhang and Hongkun Cao and Ruqi Huang", "abstract": "  To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.\n", "link": "http://arxiv.org/abs/2505.10473v1", "date": "2025-05-15", "relevancy": 3.2311, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6614}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Quantity-Quality%20Control%20across%20Scenes%20for%20Deployment-Aware%0A%20%20Gaussian%20Splatting&body=Title%3A%20Consistent%20Quantity-Quality%20Control%20across%20Scenes%20for%20Deployment-Aware%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Fengdi%20Zhang%20and%20Hongkun%20Cao%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20To%20reduce%20storage%20and%20computational%20costs%2C%203D%20Gaussian%20splatting%20%283DGS%29%20seeks%0Ato%20minimize%20the%20number%20of%20Gaussians%20used%20while%20preserving%20high%20rendering%0Aquality%2C%20introducing%20an%20inherent%20trade-off%20between%20Gaussian%20quantity%20and%0Arendering%20quality.%20Existing%20methods%20strive%20for%20better%20quantity-quality%0Aperformance%2C%20but%20lack%20the%20ability%20for%20users%20to%20intuitively%20adjust%20this%0Atrade-off%20to%20suit%20practical%20needs%20such%20as%20model%20deployment%20under%20diverse%0Ahardware%20and%20communication%20constraints.%20Here%2C%20we%20present%20ControlGS%2C%20a%203DGS%0Aoptimization%20method%20that%20achieves%20semantically%20meaningful%20and%20cross-scene%0Aconsistent%20quantity-quality%20control%20while%20maintaining%20strong%20quantity-quality%0Aperformance.%20Through%20a%20single%20training%20run%20using%20a%20fixed%20setup%20and%20a%0Auser-specified%20hyperparameter%20reflecting%20quantity-quality%20preference%2C%20ControlGS%0Acan%20automatically%20find%20desirable%20quantity-quality%20trade-off%20points%20across%0Adiverse%20scenes%2C%20from%20compact%20objects%20to%20large%20outdoor%20scenes.%20It%20also%0Aoutperforms%20baselines%20by%20achieving%20higher%20rendering%20quality%20with%20fewer%0AGaussians%2C%20and%20supports%20a%20broad%20adjustment%20range%20with%20stepless%20control%20over%20the%0Atrade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Quantity-Quality%2520Control%2520across%2520Scenes%2520for%2520Deployment-Aware%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DFengdi%2520Zhang%2520and%2520Hongkun%2520Cao%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520To%2520reduce%2520storage%2520and%2520computational%2520costs%252C%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520seeks%250Ato%2520minimize%2520the%2520number%2520of%2520Gaussians%2520used%2520while%2520preserving%2520high%2520rendering%250Aquality%252C%2520introducing%2520an%2520inherent%2520trade-off%2520between%2520Gaussian%2520quantity%2520and%250Arendering%2520quality.%2520Existing%2520methods%2520strive%2520for%2520better%2520quantity-quality%250Aperformance%252C%2520but%2520lack%2520the%2520ability%2520for%2520users%2520to%2520intuitively%2520adjust%2520this%250Atrade-off%2520to%2520suit%2520practical%2520needs%2520such%2520as%2520model%2520deployment%2520under%2520diverse%250Ahardware%2520and%2520communication%2520constraints.%2520Here%252C%2520we%2520present%2520ControlGS%252C%2520a%25203DGS%250Aoptimization%2520method%2520that%2520achieves%2520semantically%2520meaningful%2520and%2520cross-scene%250Aconsistent%2520quantity-quality%2520control%2520while%2520maintaining%2520strong%2520quantity-quality%250Aperformance.%2520Through%2520a%2520single%2520training%2520run%2520using%2520a%2520fixed%2520setup%2520and%2520a%250Auser-specified%2520hyperparameter%2520reflecting%2520quantity-quality%2520preference%252C%2520ControlGS%250Acan%2520automatically%2520find%2520desirable%2520quantity-quality%2520trade-off%2520points%2520across%250Adiverse%2520scenes%252C%2520from%2520compact%2520objects%2520to%2520large%2520outdoor%2520scenes.%2520It%2520also%250Aoutperforms%2520baselines%2520by%2520achieving%2520higher%2520rendering%2520quality%2520with%2520fewer%250AGaussians%252C%2520and%2520supports%2520a%2520broad%2520adjustment%2520range%2520with%2520stepless%2520control%2520over%2520the%250Atrade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Quantity-Quality%20Control%20across%20Scenes%20for%20Deployment-Aware%0A%20%20Gaussian%20Splatting&entry.906535625=Fengdi%20Zhang%20and%20Hongkun%20Cao%20and%20Ruqi%20Huang&entry.1292438233=%20%20To%20reduce%20storage%20and%20computational%20costs%2C%203D%20Gaussian%20splatting%20%283DGS%29%20seeks%0Ato%20minimize%20the%20number%20of%20Gaussians%20used%20while%20preserving%20high%20rendering%0Aquality%2C%20introducing%20an%20inherent%20trade-off%20between%20Gaussian%20quantity%20and%0Arendering%20quality.%20Existing%20methods%20strive%20for%20better%20quantity-quality%0Aperformance%2C%20but%20lack%20the%20ability%20for%20users%20to%20intuitively%20adjust%20this%0Atrade-off%20to%20suit%20practical%20needs%20such%20as%20model%20deployment%20under%20diverse%0Ahardware%20and%20communication%20constraints.%20Here%2C%20we%20present%20ControlGS%2C%20a%203DGS%0Aoptimization%20method%20that%20achieves%20semantically%20meaningful%20and%20cross-scene%0Aconsistent%20quantity-quality%20control%20while%20maintaining%20strong%20quantity-quality%0Aperformance.%20Through%20a%20single%20training%20run%20using%20a%20fixed%20setup%20and%20a%0Auser-specified%20hyperparameter%20reflecting%20quantity-quality%20preference%2C%20ControlGS%0Acan%20automatically%20find%20desirable%20quantity-quality%20trade-off%20points%20across%0Adiverse%20scenes%2C%20from%20compact%20objects%20to%20large%20outdoor%20scenes.%20It%20also%0Aoutperforms%20baselines%20by%20achieving%20higher%20rendering%20quality%20with%20fewer%0AGaussians%2C%20and%20supports%20a%20broad%20adjustment%20range%20with%20stepless%20control%20over%20the%0Atrade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10473v1&entry.124074799=Read"},
{"title": "DINO-X: A Unified Vision Model for Open-World Object Detection and\n  Understanding", "author": "Tianhe Ren and Yihao Chen and Qing Jiang and Zhaoyang Zeng and Yuda Xiong and Wenlong Liu and Zhengyu Ma and Junyi Shen and Yuan Gao and Xiaoke Jiang and Xingyu Chen and Zhuheng Song and Yuhong Zhang and Hongjie Huang and Han Gao and Shilong Liu and Hao Zhang and Feng Li and Kent Yu and Lei Zhang", "abstract": "  In this paper, we introduce DINO-X, which is a unified object-centric vision\nmodel developed by IDEA Research with the best open-world object detection\nperformance to date. DINO-X employs the same Transformer-based encoder-decoder\narchitecture as Grounding DINO 1.5 to pursue an object-level representation for\nopen-world object understanding. To make long-tailed object detection easy,\nDINO-X extends its input options to support text prompt, visual prompt, and\ncustomized prompt. With such flexible prompt options, we develop a universal\nobject prompt to support prompt-free open-world detection, making it possible\nto detect anything in an image without requiring users to provide any prompt.\nTo enhance the model's core grounding capability, we have constructed a\nlarge-scale dataset with over 100 million high-quality grounding samples,\nreferred to as Grounding-100M, for advancing the model's open-vocabulary\ndetection performance. Pre-training on such a large-scale grounding dataset\nleads to a foundational object-level representation, which enables DINO-X to\nintegrate multiple perception heads to simultaneously support multiple object\nperception and understanding tasks, including detection, segmentation, pose\nestimation, object captioning, object-based QA, etc. Experimental results\ndemonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro\nmodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and\nLVIS-val zero-shot object detection benchmarks, respectively. Notably, it\nscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val\nbenchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such\na result underscores its significantly improved capacity for recognizing\nlong-tailed objects.\n", "link": "http://arxiv.org/abs/2411.14347v3", "date": "2025-05-15", "relevancy": 3.1945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.667}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding&body=Title%3A%20DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding%0AAuthor%3A%20Tianhe%20Ren%20and%20Yihao%20Chen%20and%20Qing%20Jiang%20and%20Zhaoyang%20Zeng%20and%20Yuda%20Xiong%20and%20Wenlong%20Liu%20and%20Zhengyu%20Ma%20and%20Junyi%20Shen%20and%20Yuan%20Gao%20and%20Xiaoke%20Jiang%20and%20Xingyu%20Chen%20and%20Zhuheng%20Song%20and%20Yuhong%20Zhang%20and%20Hongjie%20Huang%20and%20Han%20Gao%20and%20Shilong%20Liu%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Kent%20Yu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20DINO-X%2C%20which%20is%20a%20unified%20object-centric%20vision%0Amodel%20developed%20by%20IDEA%20Research%20with%20the%20best%20open-world%20object%20detection%0Aperformance%20to%20date.%20DINO-X%20employs%20the%20same%20Transformer-based%20encoder-decoder%0Aarchitecture%20as%20Grounding%20DINO%201.5%20to%20pursue%20an%20object-level%20representation%20for%0Aopen-world%20object%20understanding.%20To%20make%20long-tailed%20object%20detection%20easy%2C%0ADINO-X%20extends%20its%20input%20options%20to%20support%20text%20prompt%2C%20visual%20prompt%2C%20and%0Acustomized%20prompt.%20With%20such%20flexible%20prompt%20options%2C%20we%20develop%20a%20universal%0Aobject%20prompt%20to%20support%20prompt-free%20open-world%20detection%2C%20making%20it%20possible%0Ato%20detect%20anything%20in%20an%20image%20without%20requiring%20users%20to%20provide%20any%20prompt.%0ATo%20enhance%20the%20model%27s%20core%20grounding%20capability%2C%20we%20have%20constructed%20a%0Alarge-scale%20dataset%20with%20over%20100%20million%20high-quality%20grounding%20samples%2C%0Areferred%20to%20as%20Grounding-100M%2C%20for%20advancing%20the%20model%27s%20open-vocabulary%0Adetection%20performance.%20Pre-training%20on%20such%20a%20large-scale%20grounding%20dataset%0Aleads%20to%20a%20foundational%20object-level%20representation%2C%20which%20enables%20DINO-X%20to%0Aintegrate%20multiple%20perception%20heads%20to%20simultaneously%20support%20multiple%20object%0Aperception%20and%20understanding%20tasks%2C%20including%20detection%2C%20segmentation%2C%20pose%0Aestimation%2C%20object%20captioning%2C%20object-based%20QA%2C%20etc.%20Experimental%20results%0Ademonstrate%20the%20superior%20performance%20of%20DINO-X.%20Specifically%2C%20the%20DINO-X%20Pro%0Amodel%20achieves%2056.0%20AP%2C%2059.8%20AP%2C%20and%2052.4%20AP%20on%20the%20COCO%2C%20LVIS-minival%2C%20and%0ALVIS-val%20zero-shot%20object%20detection%20benchmarks%2C%20respectively.%20Notably%2C%20it%0Ascores%2063.3%20AP%20and%2056.5%20AP%20on%20the%20rare%20classes%20of%20LVIS-minival%20and%20LVIS-val%0Abenchmarks%2C%20improving%20the%20previous%20SOTA%20performance%20by%205.8%20AP%20and%205.0%20AP.%20Such%0Aa%20result%20underscores%20its%20significantly%20improved%20capacity%20for%20recognizing%0Along-tailed%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-X%253A%2520A%2520Unified%2520Vision%2520Model%2520for%2520Open-World%2520Object%2520Detection%2520and%250A%2520%2520Understanding%26entry.906535625%3DTianhe%2520Ren%2520and%2520Yihao%2520Chen%2520and%2520Qing%2520Jiang%2520and%2520Zhaoyang%2520Zeng%2520and%2520Yuda%2520Xiong%2520and%2520Wenlong%2520Liu%2520and%2520Zhengyu%2520Ma%2520and%2520Junyi%2520Shen%2520and%2520Yuan%2520Gao%2520and%2520Xiaoke%2520Jiang%2520and%2520Xingyu%2520Chen%2520and%2520Zhuheng%2520Song%2520and%2520Yuhong%2520Zhang%2520and%2520Hongjie%2520Huang%2520and%2520Han%2520Gao%2520and%2520Shilong%2520Liu%2520and%2520Hao%2520Zhang%2520and%2520Feng%2520Li%2520and%2520Kent%2520Yu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DINO-X%252C%2520which%2520is%2520a%2520unified%2520object-centric%2520vision%250Amodel%2520developed%2520by%2520IDEA%2520Research%2520with%2520the%2520best%2520open-world%2520object%2520detection%250Aperformance%2520to%2520date.%2520DINO-X%2520employs%2520the%2520same%2520Transformer-based%2520encoder-decoder%250Aarchitecture%2520as%2520Grounding%2520DINO%25201.5%2520to%2520pursue%2520an%2520object-level%2520representation%2520for%250Aopen-world%2520object%2520understanding.%2520To%2520make%2520long-tailed%2520object%2520detection%2520easy%252C%250ADINO-X%2520extends%2520its%2520input%2520options%2520to%2520support%2520text%2520prompt%252C%2520visual%2520prompt%252C%2520and%250Acustomized%2520prompt.%2520With%2520such%2520flexible%2520prompt%2520options%252C%2520we%2520develop%2520a%2520universal%250Aobject%2520prompt%2520to%2520support%2520prompt-free%2520open-world%2520detection%252C%2520making%2520it%2520possible%250Ato%2520detect%2520anything%2520in%2520an%2520image%2520without%2520requiring%2520users%2520to%2520provide%2520any%2520prompt.%250ATo%2520enhance%2520the%2520model%2527s%2520core%2520grounding%2520capability%252C%2520we%2520have%2520constructed%2520a%250Alarge-scale%2520dataset%2520with%2520over%2520100%2520million%2520high-quality%2520grounding%2520samples%252C%250Areferred%2520to%2520as%2520Grounding-100M%252C%2520for%2520advancing%2520the%2520model%2527s%2520open-vocabulary%250Adetection%2520performance.%2520Pre-training%2520on%2520such%2520a%2520large-scale%2520grounding%2520dataset%250Aleads%2520to%2520a%2520foundational%2520object-level%2520representation%252C%2520which%2520enables%2520DINO-X%2520to%250Aintegrate%2520multiple%2520perception%2520heads%2520to%2520simultaneously%2520support%2520multiple%2520object%250Aperception%2520and%2520understanding%2520tasks%252C%2520including%2520detection%252C%2520segmentation%252C%2520pose%250Aestimation%252C%2520object%2520captioning%252C%2520object-based%2520QA%252C%2520etc.%2520Experimental%2520results%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520DINO-X.%2520Specifically%252C%2520the%2520DINO-X%2520Pro%250Amodel%2520achieves%252056.0%2520AP%252C%252059.8%2520AP%252C%2520and%252052.4%2520AP%2520on%2520the%2520COCO%252C%2520LVIS-minival%252C%2520and%250ALVIS-val%2520zero-shot%2520object%2520detection%2520benchmarks%252C%2520respectively.%2520Notably%252C%2520it%250Ascores%252063.3%2520AP%2520and%252056.5%2520AP%2520on%2520the%2520rare%2520classes%2520of%2520LVIS-minival%2520and%2520LVIS-val%250Abenchmarks%252C%2520improving%2520the%2520previous%2520SOTA%2520performance%2520by%25205.8%2520AP%2520and%25205.0%2520AP.%2520Such%250Aa%2520result%2520underscores%2520its%2520significantly%2520improved%2520capacity%2520for%2520recognizing%250Along-tailed%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-X%3A%20A%20Unified%20Vision%20Model%20for%20Open-World%20Object%20Detection%20and%0A%20%20Understanding&entry.906535625=Tianhe%20Ren%20and%20Yihao%20Chen%20and%20Qing%20Jiang%20and%20Zhaoyang%20Zeng%20and%20Yuda%20Xiong%20and%20Wenlong%20Liu%20and%20Zhengyu%20Ma%20and%20Junyi%20Shen%20and%20Yuan%20Gao%20and%20Xiaoke%20Jiang%20and%20Xingyu%20Chen%20and%20Zhuheng%20Song%20and%20Yuhong%20Zhang%20and%20Hongjie%20Huang%20and%20Han%20Gao%20and%20Shilong%20Liu%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Kent%20Yu%20and%20Lei%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20DINO-X%2C%20which%20is%20a%20unified%20object-centric%20vision%0Amodel%20developed%20by%20IDEA%20Research%20with%20the%20best%20open-world%20object%20detection%0Aperformance%20to%20date.%20DINO-X%20employs%20the%20same%20Transformer-based%20encoder-decoder%0Aarchitecture%20as%20Grounding%20DINO%201.5%20to%20pursue%20an%20object-level%20representation%20for%0Aopen-world%20object%20understanding.%20To%20make%20long-tailed%20object%20detection%20easy%2C%0ADINO-X%20extends%20its%20input%20options%20to%20support%20text%20prompt%2C%20visual%20prompt%2C%20and%0Acustomized%20prompt.%20With%20such%20flexible%20prompt%20options%2C%20we%20develop%20a%20universal%0Aobject%20prompt%20to%20support%20prompt-free%20open-world%20detection%2C%20making%20it%20possible%0Ato%20detect%20anything%20in%20an%20image%20without%20requiring%20users%20to%20provide%20any%20prompt.%0ATo%20enhance%20the%20model%27s%20core%20grounding%20capability%2C%20we%20have%20constructed%20a%0Alarge-scale%20dataset%20with%20over%20100%20million%20high-quality%20grounding%20samples%2C%0Areferred%20to%20as%20Grounding-100M%2C%20for%20advancing%20the%20model%27s%20open-vocabulary%0Adetection%20performance.%20Pre-training%20on%20such%20a%20large-scale%20grounding%20dataset%0Aleads%20to%20a%20foundational%20object-level%20representation%2C%20which%20enables%20DINO-X%20to%0Aintegrate%20multiple%20perception%20heads%20to%20simultaneously%20support%20multiple%20object%0Aperception%20and%20understanding%20tasks%2C%20including%20detection%2C%20segmentation%2C%20pose%0Aestimation%2C%20object%20captioning%2C%20object-based%20QA%2C%20etc.%20Experimental%20results%0Ademonstrate%20the%20superior%20performance%20of%20DINO-X.%20Specifically%2C%20the%20DINO-X%20Pro%0Amodel%20achieves%2056.0%20AP%2C%2059.8%20AP%2C%20and%2052.4%20AP%20on%20the%20COCO%2C%20LVIS-minival%2C%20and%0ALVIS-val%20zero-shot%20object%20detection%20benchmarks%2C%20respectively.%20Notably%2C%20it%0Ascores%2063.3%20AP%20and%2056.5%20AP%20on%20the%20rare%20classes%20of%20LVIS-minival%20and%20LVIS-val%0Abenchmarks%2C%20improving%20the%20previous%20SOTA%20performance%20by%205.8%20AP%20and%205.0%20AP.%20Such%0Aa%20result%20underscores%20its%20significantly%20improved%20capacity%20for%20recognizing%0Along-tailed%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14347v3&entry.124074799=Read"},
{"title": "Vision language models have difficulty recognizing virtual objects", "author": "Tyler Tran and Sangeet Khemlani and J. G. Trafton", "abstract": "  Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.\n", "link": "http://arxiv.org/abs/2505.10453v1", "date": "2025-05-15", "relevancy": 3.128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6689}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20language%20models%20have%20difficulty%20recognizing%20virtual%20objects&body=Title%3A%20Vision%20language%20models%20have%20difficulty%20recognizing%20virtual%20objects%0AAuthor%3A%20Tyler%20Tran%20and%20Sangeet%20Khemlani%20and%20J.%20G.%20Trafton%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20are%20AI%20systems%20paired%20with%20both%20language%20and%0Avision%20encoders%20to%20process%20multimodal%20input.%20They%20are%20capable%20of%20performing%0Acomplex%20semantic%20tasks%20such%20as%20automatic%20captioning%2C%20but%20it%20remains%20an%20open%0Aquestion%20about%20how%20well%20they%20comprehend%20the%20visuospatial%20properties%20of%20scenes%0Adepicted%20in%20the%20images%20they%20process.%20We%20argue%20that%20descriptions%20of%20virtual%0Aobjects%20--%20objects%20that%20are%20not%20visually%20represented%20in%20an%20image%20--%20can%20help%0Atest%20scene%20comprehension%20in%20these%20AI%20systems.%20For%20example%2C%20an%20image%20that%0Adepicts%20a%20person%20standing%20under%20a%20tree%20can%20be%20paired%20with%20the%20following%20prompt%3A%0Aimagine%20that%20a%20kite%20is%20stuck%20in%20the%20tree.%20VLMs%20that%20comprehend%20the%20scene%20should%0Aupdate%20their%20representations%20and%20reason%20sensibly%20about%20the%20spatial%20relations%0Abetween%20all%20three%20objects.%20We%20describe%20systematic%20evaluations%20of%0Astate-of-the-art%20VLMs%20and%20show%20that%20their%20ability%20to%20process%20virtual%20objects%20is%0Ainadequate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520language%2520models%2520have%2520difficulty%2520recognizing%2520virtual%2520objects%26entry.906535625%3DTyler%2520Tran%2520and%2520Sangeet%2520Khemlani%2520and%2520J.%2520G.%2520Trafton%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520are%2520AI%2520systems%2520paired%2520with%2520both%2520language%2520and%250Avision%2520encoders%2520to%2520process%2520multimodal%2520input.%2520They%2520are%2520capable%2520of%2520performing%250Acomplex%2520semantic%2520tasks%2520such%2520as%2520automatic%2520captioning%252C%2520but%2520it%2520remains%2520an%2520open%250Aquestion%2520about%2520how%2520well%2520they%2520comprehend%2520the%2520visuospatial%2520properties%2520of%2520scenes%250Adepicted%2520in%2520the%2520images%2520they%2520process.%2520We%2520argue%2520that%2520descriptions%2520of%2520virtual%250Aobjects%2520--%2520objects%2520that%2520are%2520not%2520visually%2520represented%2520in%2520an%2520image%2520--%2520can%2520help%250Atest%2520scene%2520comprehension%2520in%2520these%2520AI%2520systems.%2520For%2520example%252C%2520an%2520image%2520that%250Adepicts%2520a%2520person%2520standing%2520under%2520a%2520tree%2520can%2520be%2520paired%2520with%2520the%2520following%2520prompt%253A%250Aimagine%2520that%2520a%2520kite%2520is%2520stuck%2520in%2520the%2520tree.%2520VLMs%2520that%2520comprehend%2520the%2520scene%2520should%250Aupdate%2520their%2520representations%2520and%2520reason%2520sensibly%2520about%2520the%2520spatial%2520relations%250Abetween%2520all%2520three%2520objects.%2520We%2520describe%2520systematic%2520evaluations%2520of%250Astate-of-the-art%2520VLMs%2520and%2520show%2520that%2520their%2520ability%2520to%2520process%2520virtual%2520objects%2520is%250Ainadequate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20language%20models%20have%20difficulty%20recognizing%20virtual%20objects&entry.906535625=Tyler%20Tran%20and%20Sangeet%20Khemlani%20and%20J.%20G.%20Trafton&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20are%20AI%20systems%20paired%20with%20both%20language%20and%0Avision%20encoders%20to%20process%20multimodal%20input.%20They%20are%20capable%20of%20performing%0Acomplex%20semantic%20tasks%20such%20as%20automatic%20captioning%2C%20but%20it%20remains%20an%20open%0Aquestion%20about%20how%20well%20they%20comprehend%20the%20visuospatial%20properties%20of%20scenes%0Adepicted%20in%20the%20images%20they%20process.%20We%20argue%20that%20descriptions%20of%20virtual%0Aobjects%20--%20objects%20that%20are%20not%20visually%20represented%20in%20an%20image%20--%20can%20help%0Atest%20scene%20comprehension%20in%20these%20AI%20systems.%20For%20example%2C%20an%20image%20that%0Adepicts%20a%20person%20standing%20under%20a%20tree%20can%20be%20paired%20with%20the%20following%20prompt%3A%0Aimagine%20that%20a%20kite%20is%20stuck%20in%20the%20tree.%20VLMs%20that%20comprehend%20the%20scene%20should%0Aupdate%20their%20representations%20and%20reason%20sensibly%20about%20the%20spatial%20relations%0Abetween%20all%20three%20objects.%20We%20describe%20systematic%20evaluations%20of%0Astate-of-the-art%20VLMs%20and%20show%20that%20their%20ability%20to%20process%20virtual%20objects%20is%0Ainadequate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10453v1&entry.124074799=Read"},
{"title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language\n  Models through Attention Analysis", "author": "Pengfei Wang and Guohai Xu and Weinong Wang and Junjie Yang and Jie Lou and Yunhua Xue", "abstract": "  Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.\n", "link": "http://arxiv.org/abs/2505.10541v1", "date": "2025-05-15", "relevancy": 2.9871, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis&body=Title%3A%20Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis%0AAuthor%3A%20Pengfei%20Wang%20and%20Guohai%20Xu%20and%20Weinong%20Wang%20and%20Junjie%20Yang%20and%20Jie%20Lou%20and%20Yunhua%20Xue%0AAbstract%3A%20%20%20Recent%20advancements%20have%20enhanced%20the%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20comprehend%20multi-image%20information.%20However%2C%20existing%0Abenchmarks%20primarily%20evaluate%20answer%20correctness%2C%20overlooking%20whether%20models%0Agenuinely%20comprehend%20the%20visual%20input.%20To%20address%20this%2C%20we%20define%20implicit%0Avisual%20misunderstanding%20%28IVM%29%2C%20where%20MLLMs%20provide%20correct%20answers%20without%0Afully%20comprehending%20the%20visual%20input.%20Through%20our%20analysis%2C%20we%20decouple%20the%0Avisual%20and%20textual%20modalities%20within%20the%20causal%20attention%20module%2C%20revealing%0Athat%20attention%20distribution%20increasingly%20converges%20on%20the%20image%20associated%20with%0Athe%20correct%20answer%20as%20the%20network%20layers%20deepen.%20This%20insight%20leads%20to%20the%0Aintroduction%20of%20a%20scale-agnostic%20metric%2C%20%5Ctextit%7Battention%20accuracy%7D%2C%20and%20a%0Anovel%20benchmark%20for%20quantifying%20IVMs.%20Attention%20accuracy%20directly%20evaluates%20the%0Amodel%27s%20visual%20understanding%20via%20internal%20mechanisms%2C%20remaining%20robust%20to%0Apositional%20biases%20for%20more%20reliable%20assessments.%20Furthermore%2C%20we%20extend%20our%0Aapproach%20to%20finer%20granularities%20and%20demonstrate%20its%20effectiveness%20in%20unimodal%0Ascenarios%2C%20underscoring%20its%20versatility%20and%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Implicit%2520Visual%2520Misunderstandings%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520through%2520Attention%2520Analysis%26entry.906535625%3DPengfei%2520Wang%2520and%2520Guohai%2520Xu%2520and%2520Weinong%2520Wang%2520and%2520Junjie%2520Yang%2520and%2520Jie%2520Lou%2520and%2520Yunhua%2520Xue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520enhanced%2520the%2520capability%2520of%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520to%2520comprehend%2520multi-image%2520information.%2520However%252C%2520existing%250Abenchmarks%2520primarily%2520evaluate%2520answer%2520correctness%252C%2520overlooking%2520whether%2520models%250Agenuinely%2520comprehend%2520the%2520visual%2520input.%2520To%2520address%2520this%252C%2520we%2520define%2520implicit%250Avisual%2520misunderstanding%2520%2528IVM%2529%252C%2520where%2520MLLMs%2520provide%2520correct%2520answers%2520without%250Afully%2520comprehending%2520the%2520visual%2520input.%2520Through%2520our%2520analysis%252C%2520we%2520decouple%2520the%250Avisual%2520and%2520textual%2520modalities%2520within%2520the%2520causal%2520attention%2520module%252C%2520revealing%250Athat%2520attention%2520distribution%2520increasingly%2520converges%2520on%2520the%2520image%2520associated%2520with%250Athe%2520correct%2520answer%2520as%2520the%2520network%2520layers%2520deepen.%2520This%2520insight%2520leads%2520to%2520the%250Aintroduction%2520of%2520a%2520scale-agnostic%2520metric%252C%2520%255Ctextit%257Battention%2520accuracy%257D%252C%2520and%2520a%250Anovel%2520benchmark%2520for%2520quantifying%2520IVMs.%2520Attention%2520accuracy%2520directly%2520evaluates%2520the%250Amodel%2527s%2520visual%2520understanding%2520via%2520internal%2520mechanisms%252C%2520remaining%2520robust%2520to%250Apositional%2520biases%2520for%2520more%2520reliable%2520assessments.%2520Furthermore%252C%2520we%2520extend%2520our%250Aapproach%2520to%2520finer%2520granularities%2520and%2520demonstrate%2520its%2520effectiveness%2520in%2520unimodal%250Ascenarios%252C%2520underscoring%2520its%2520versatility%2520and%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis&entry.906535625=Pengfei%20Wang%20and%20Guohai%20Xu%20and%20Weinong%20Wang%20and%20Junjie%20Yang%20and%20Jie%20Lou%20and%20Yunhua%20Xue&entry.1292438233=%20%20Recent%20advancements%20have%20enhanced%20the%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20comprehend%20multi-image%20information.%20However%2C%20existing%0Abenchmarks%20primarily%20evaluate%20answer%20correctness%2C%20overlooking%20whether%20models%0Agenuinely%20comprehend%20the%20visual%20input.%20To%20address%20this%2C%20we%20define%20implicit%0Avisual%20misunderstanding%20%28IVM%29%2C%20where%20MLLMs%20provide%20correct%20answers%20without%0Afully%20comprehending%20the%20visual%20input.%20Through%20our%20analysis%2C%20we%20decouple%20the%0Avisual%20and%20textual%20modalities%20within%20the%20causal%20attention%20module%2C%20revealing%0Athat%20attention%20distribution%20increasingly%20converges%20on%20the%20image%20associated%20with%0Athe%20correct%20answer%20as%20the%20network%20layers%20deepen.%20This%20insight%20leads%20to%20the%0Aintroduction%20of%20a%20scale-agnostic%20metric%2C%20%5Ctextit%7Battention%20accuracy%7D%2C%20and%20a%0Anovel%20benchmark%20for%20quantifying%20IVMs.%20Attention%20accuracy%20directly%20evaluates%20the%0Amodel%27s%20visual%20understanding%20via%20internal%20mechanisms%2C%20remaining%20robust%20to%0Apositional%20biases%20for%20more%20reliable%20assessments.%20Furthermore%2C%20we%20extend%20our%0Aapproach%20to%20finer%20granularities%20and%20demonstrate%20its%20effectiveness%20in%20unimodal%0Ascenarios%2C%20underscoring%20its%20versatility%20and%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10541v1&entry.124074799=Read"},
{"title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct\n  Preference Optimization", "author": "Wenhao Shen and Wanqi Yin and Xiaofeng Yang and Cheng Chen and Chaoyue Song and Zhongang Cai and Lei Yang and Hao Wang and Guosheng Lin", "abstract": "  Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.\n", "link": "http://arxiv.org/abs/2505.10250v1", "date": "2025-05-15", "relevancy": 2.9367, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6097}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5769}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADHMR%3A%20Aligning%20Diffusion-based%20Human%20Mesh%20Recovery%20via%20Direct%0A%20%20Preference%20Optimization&body=Title%3A%20ADHMR%3A%20Aligning%20Diffusion-based%20Human%20Mesh%20Recovery%20via%20Direct%0A%20%20Preference%20Optimization%0AAuthor%3A%20Wenhao%20Shen%20and%20Wanqi%20Yin%20and%20Xiaofeng%20Yang%20and%20Cheng%20Chen%20and%20Chaoyue%20Song%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Hao%20Wang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Human%20mesh%20recovery%20%28HMR%29%20from%20a%20single%20image%20is%20inherently%20ill-posed%20due%20to%0Adepth%20ambiguity%20and%20occlusions.%20Probabilistic%20methods%20have%20tried%20to%20solve%20this%0Aby%20generating%20numerous%20plausible%203D%20human%20mesh%20predictions%2C%20but%20they%20often%0Aexhibit%20misalignment%20with%202D%20image%20observations%20and%20weak%20robustness%20to%0Ain-the-wild%20images.%20To%20address%20these%20issues%2C%20we%20propose%20ADHMR%2C%20a%20framework%20that%0AAligns%20a%20Diffusion-based%20HMR%20model%20in%20a%20preference%20optimization%20manner.%20First%2C%0Awe%20train%20a%20human%20mesh%20prediction%20assessment%20model%2C%20HMR-Scorer%2C%20capable%20of%0Aevaluating%20predictions%20even%20for%20in-the-wild%20images%20without%203D%20annotations.%20We%0Athen%20use%20HMR-Scorer%20to%20create%20a%20preference%20dataset%2C%20where%20each%20input%20image%20has%0Aa%20pair%20of%20winner%20and%20loser%20mesh%20predictions.%20This%20dataset%20is%20used%20to%20finetune%0Athe%20base%20model%20using%20direct%20preference%20optimization.%20Moreover%2C%20HMR-Scorer%20also%0Ahelps%20improve%20existing%20HMR%20models%20by%20data%20cleaning%2C%20even%20with%20fewer%20training%0Asamples.%20Extensive%20experiments%20show%20that%20ADHMR%20outperforms%20current%0Astate-of-the-art%20methods.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/shenwenhao01/ADHMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADHMR%253A%2520Aligning%2520Diffusion-based%2520Human%2520Mesh%2520Recovery%2520via%2520Direct%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DWenhao%2520Shen%2520and%2520Wanqi%2520Yin%2520and%2520Xiaofeng%2520Yang%2520and%2520Cheng%2520Chen%2520and%2520Chaoyue%2520Song%2520and%2520Zhongang%2520Cai%2520and%2520Lei%2520Yang%2520and%2520Hao%2520Wang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Human%2520mesh%2520recovery%2520%2528HMR%2529%2520from%2520a%2520single%2520image%2520is%2520inherently%2520ill-posed%2520due%2520to%250Adepth%2520ambiguity%2520and%2520occlusions.%2520Probabilistic%2520methods%2520have%2520tried%2520to%2520solve%2520this%250Aby%2520generating%2520numerous%2520plausible%25203D%2520human%2520mesh%2520predictions%252C%2520but%2520they%2520often%250Aexhibit%2520misalignment%2520with%25202D%2520image%2520observations%2520and%2520weak%2520robustness%2520to%250Ain-the-wild%2520images.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520ADHMR%252C%2520a%2520framework%2520that%250AAligns%2520a%2520Diffusion-based%2520HMR%2520model%2520in%2520a%2520preference%2520optimization%2520manner.%2520First%252C%250Awe%2520train%2520a%2520human%2520mesh%2520prediction%2520assessment%2520model%252C%2520HMR-Scorer%252C%2520capable%2520of%250Aevaluating%2520predictions%2520even%2520for%2520in-the-wild%2520images%2520without%25203D%2520annotations.%2520We%250Athen%2520use%2520HMR-Scorer%2520to%2520create%2520a%2520preference%2520dataset%252C%2520where%2520each%2520input%2520image%2520has%250Aa%2520pair%2520of%2520winner%2520and%2520loser%2520mesh%2520predictions.%2520This%2520dataset%2520is%2520used%2520to%2520finetune%250Athe%2520base%2520model%2520using%2520direct%2520preference%2520optimization.%2520Moreover%252C%2520HMR-Scorer%2520also%250Ahelps%2520improve%2520existing%2520HMR%2520models%2520by%2520data%2520cleaning%252C%2520even%2520with%2520fewer%2520training%250Asamples.%2520Extensive%2520experiments%2520show%2520that%2520ADHMR%2520outperforms%2520current%250Astate-of-the-art%2520methods.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/shenwenhao01/ADHMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADHMR%3A%20Aligning%20Diffusion-based%20Human%20Mesh%20Recovery%20via%20Direct%0A%20%20Preference%20Optimization&entry.906535625=Wenhao%20Shen%20and%20Wanqi%20Yin%20and%20Xiaofeng%20Yang%20and%20Cheng%20Chen%20and%20Chaoyue%20Song%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Hao%20Wang%20and%20Guosheng%20Lin&entry.1292438233=%20%20Human%20mesh%20recovery%20%28HMR%29%20from%20a%20single%20image%20is%20inherently%20ill-posed%20due%20to%0Adepth%20ambiguity%20and%20occlusions.%20Probabilistic%20methods%20have%20tried%20to%20solve%20this%0Aby%20generating%20numerous%20plausible%203D%20human%20mesh%20predictions%2C%20but%20they%20often%0Aexhibit%20misalignment%20with%202D%20image%20observations%20and%20weak%20robustness%20to%0Ain-the-wild%20images.%20To%20address%20these%20issues%2C%20we%20propose%20ADHMR%2C%20a%20framework%20that%0AAligns%20a%20Diffusion-based%20HMR%20model%20in%20a%20preference%20optimization%20manner.%20First%2C%0Awe%20train%20a%20human%20mesh%20prediction%20assessment%20model%2C%20HMR-Scorer%2C%20capable%20of%0Aevaluating%20predictions%20even%20for%20in-the-wild%20images%20without%203D%20annotations.%20We%0Athen%20use%20HMR-Scorer%20to%20create%20a%20preference%20dataset%2C%20where%20each%20input%20image%20has%0Aa%20pair%20of%20winner%20and%20loser%20mesh%20predictions.%20This%20dataset%20is%20used%20to%20finetune%0Athe%20base%20model%20using%20direct%20preference%20optimization.%20Moreover%2C%20HMR-Scorer%20also%0Ahelps%20improve%20existing%20HMR%20models%20by%20data%20cleaning%2C%20even%20with%20fewer%20training%0Asamples.%20Extensive%20experiments%20show%20that%20ADHMR%20outperforms%20current%0Astate-of-the-art%20methods.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/shenwenhao01/ADHMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10250v1&entry.124074799=Read"},
{"title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning", "author": "Ke Wang and Junting Pan and Linda Wei and Aojun Zhou and Weikang Shi and Zimu Lu and Han Xiao and Yunqiao Yang and Houxing Ren and Mingjie Zhan and Hongsheng Li", "abstract": "  Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.\n", "link": "http://arxiv.org/abs/2505.10557v1", "date": "2025-05-15", "relevancy": 2.9197, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathCoder-VL%3A%20Bridging%20Vision%20and%20Code%20for%20Enhanced%20Multimodal%0A%20%20Mathematical%20Reasoning&body=Title%3A%20MathCoder-VL%3A%20Bridging%20Vision%20and%20Code%20for%20Enhanced%20Multimodal%0A%20%20Mathematical%20Reasoning%0AAuthor%3A%20Ke%20Wang%20and%20Junting%20Pan%20and%20Linda%20Wei%20and%20Aojun%20Zhou%20and%20Weikang%20Shi%20and%20Zimu%20Lu%20and%20Han%20Xiao%20and%20Yunqiao%20Yang%20and%20Houxing%20Ren%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Natural%20language%20image-caption%20datasets%2C%20widely%20used%20for%20training%20Large%0AMultimodal%20Models%2C%20mainly%20focus%20on%20natural%20scenarios%20and%20overlook%20the%20intricate%0Adetails%20of%20mathematical%20figures%20that%20are%20critical%20for%20problem-solving%2C%0Ahindering%20the%20advancement%20of%20current%20LMMs%20in%20multimodal%20mathematical%20reasoning.%0ATo%20this%20end%2C%20we%20propose%20leveraging%20code%20as%20supervision%20for%20cross-modal%0Aalignment%2C%20since%20code%20inherently%20encodes%20all%20information%20needed%20to%20generate%0Acorresponding%20figures%2C%20establishing%20a%20precise%20connection%20between%20the%20two%0Amodalities.%20Specifically%2C%20we%20co-develop%20our%20image-to-code%20model%20and%20dataset%0Awith%20model-in-the-loop%20approach%2C%20resulting%20in%20an%20image-to-code%20model%2C%0AFigCodifier%20and%20ImgCode-8.6M%20dataset%2C%20the%20largest%20image-code%20dataset%20to%20date.%0AFurthermore%2C%20we%20utilize%20FigCodifier%20to%20synthesize%20novel%20mathematical%20figures%0Aand%20then%20construct%20MM-MathInstruct-3M%2C%20a%20high-quality%20multimodal%20math%0Ainstruction%20fine-tuning%20dataset.%20Finally%2C%20we%20present%20MathCoder-VL%2C%20trained%20with%0AImgCode-8.6M%20for%20cross-modal%20alignment%20and%20subsequently%20fine-tuned%20on%0AMM-MathInstruct-3M%20for%20multimodal%20math%20problem%20solving.%20Our%20model%20achieves%20a%0Anew%20open-source%20SOTA%20across%20all%20six%20metrics.%20Notably%2C%20it%20surpasses%20GPT-4o%20and%0AClaude%203.5%20Sonnet%20in%20the%20geometry%20problem-solving%20subset%20of%20MathVista%2C%0Aachieving%20improvements%20of%208.9%25%20and%209.2%25.%20The%20dataset%20and%20models%20will%20be%0Areleased%20at%20https%3A//github.com/mathllm/MathCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathCoder-VL%253A%2520Bridging%2520Vision%2520and%2520Code%2520for%2520Enhanced%2520Multimodal%250A%2520%2520Mathematical%2520Reasoning%26entry.906535625%3DKe%2520Wang%2520and%2520Junting%2520Pan%2520and%2520Linda%2520Wei%2520and%2520Aojun%2520Zhou%2520and%2520Weikang%2520Shi%2520and%2520Zimu%2520Lu%2520and%2520Han%2520Xiao%2520and%2520Yunqiao%2520Yang%2520and%2520Houxing%2520Ren%2520and%2520Mingjie%2520Zhan%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Natural%2520language%2520image-caption%2520datasets%252C%2520widely%2520used%2520for%2520training%2520Large%250AMultimodal%2520Models%252C%2520mainly%2520focus%2520on%2520natural%2520scenarios%2520and%2520overlook%2520the%2520intricate%250Adetails%2520of%2520mathematical%2520figures%2520that%2520are%2520critical%2520for%2520problem-solving%252C%250Ahindering%2520the%2520advancement%2520of%2520current%2520LMMs%2520in%2520multimodal%2520mathematical%2520reasoning.%250ATo%2520this%2520end%252C%2520we%2520propose%2520leveraging%2520code%2520as%2520supervision%2520for%2520cross-modal%250Aalignment%252C%2520since%2520code%2520inherently%2520encodes%2520all%2520information%2520needed%2520to%2520generate%250Acorresponding%2520figures%252C%2520establishing%2520a%2520precise%2520connection%2520between%2520the%2520two%250Amodalities.%2520Specifically%252C%2520we%2520co-develop%2520our%2520image-to-code%2520model%2520and%2520dataset%250Awith%2520model-in-the-loop%2520approach%252C%2520resulting%2520in%2520an%2520image-to-code%2520model%252C%250AFigCodifier%2520and%2520ImgCode-8.6M%2520dataset%252C%2520the%2520largest%2520image-code%2520dataset%2520to%2520date.%250AFurthermore%252C%2520we%2520utilize%2520FigCodifier%2520to%2520synthesize%2520novel%2520mathematical%2520figures%250Aand%2520then%2520construct%2520MM-MathInstruct-3M%252C%2520a%2520high-quality%2520multimodal%2520math%250Ainstruction%2520fine-tuning%2520dataset.%2520Finally%252C%2520we%2520present%2520MathCoder-VL%252C%2520trained%2520with%250AImgCode-8.6M%2520for%2520cross-modal%2520alignment%2520and%2520subsequently%2520fine-tuned%2520on%250AMM-MathInstruct-3M%2520for%2520multimodal%2520math%2520problem%2520solving.%2520Our%2520model%2520achieves%2520a%250Anew%2520open-source%2520SOTA%2520across%2520all%2520six%2520metrics.%2520Notably%252C%2520it%2520surpasses%2520GPT-4o%2520and%250AClaude%25203.5%2520Sonnet%2520in%2520the%2520geometry%2520problem-solving%2520subset%2520of%2520MathVista%252C%250Aachieving%2520improvements%2520of%25208.9%2525%2520and%25209.2%2525.%2520The%2520dataset%2520and%2520models%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/mathllm/MathCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathCoder-VL%3A%20Bridging%20Vision%20and%20Code%20for%20Enhanced%20Multimodal%0A%20%20Mathematical%20Reasoning&entry.906535625=Ke%20Wang%20and%20Junting%20Pan%20and%20Linda%20Wei%20and%20Aojun%20Zhou%20and%20Weikang%20Shi%20and%20Zimu%20Lu%20and%20Han%20Xiao%20and%20Yunqiao%20Yang%20and%20Houxing%20Ren%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li&entry.1292438233=%20%20Natural%20language%20image-caption%20datasets%2C%20widely%20used%20for%20training%20Large%0AMultimodal%20Models%2C%20mainly%20focus%20on%20natural%20scenarios%20and%20overlook%20the%20intricate%0Adetails%20of%20mathematical%20figures%20that%20are%20critical%20for%20problem-solving%2C%0Ahindering%20the%20advancement%20of%20current%20LMMs%20in%20multimodal%20mathematical%20reasoning.%0ATo%20this%20end%2C%20we%20propose%20leveraging%20code%20as%20supervision%20for%20cross-modal%0Aalignment%2C%20since%20code%20inherently%20encodes%20all%20information%20needed%20to%20generate%0Acorresponding%20figures%2C%20establishing%20a%20precise%20connection%20between%20the%20two%0Amodalities.%20Specifically%2C%20we%20co-develop%20our%20image-to-code%20model%20and%20dataset%0Awith%20model-in-the-loop%20approach%2C%20resulting%20in%20an%20image-to-code%20model%2C%0AFigCodifier%20and%20ImgCode-8.6M%20dataset%2C%20the%20largest%20image-code%20dataset%20to%20date.%0AFurthermore%2C%20we%20utilize%20FigCodifier%20to%20synthesize%20novel%20mathematical%20figures%0Aand%20then%20construct%20MM-MathInstruct-3M%2C%20a%20high-quality%20multimodal%20math%0Ainstruction%20fine-tuning%20dataset.%20Finally%2C%20we%20present%20MathCoder-VL%2C%20trained%20with%0AImgCode-8.6M%20for%20cross-modal%20alignment%20and%20subsequently%20fine-tuned%20on%0AMM-MathInstruct-3M%20for%20multimodal%20math%20problem%20solving.%20Our%20model%20achieves%20a%0Anew%20open-source%20SOTA%20across%20all%20six%20metrics.%20Notably%2C%20it%20surpasses%20GPT-4o%20and%0AClaude%203.5%20Sonnet%20in%20the%20geometry%20problem-solving%20subset%20of%20MathVista%2C%0Aachieving%20improvements%20of%208.9%25%20and%209.2%25.%20The%20dataset%20and%20models%20will%20be%0Areleased%20at%20https%3A//github.com/mathllm/MathCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10557v1&entry.124074799=Read"},
{"title": "Highly Efficient 3D Human Pose Tracking from Events with Spiking\n  Spatiotemporal Transformer", "author": "Shihao Zou and Yuxuan Mu and Wei Ji and Zi-An Wang and Xinxin Zuo and Sen Wang and Weixin Si and Li Cheng", "abstract": "  Event camera, as an asynchronous vision sensor capturing scene dynamics,\npresents new opportunities for highly efficient 3D human pose tracking.\nExisting approaches typically adopt modern-day Artificial Neural Networks\n(ANNs), such as CNNs or Transformer, where sparse events are converted into\ndense images or paired with additional gray-scale images as input. Such\npractices, however, ignore the inherent sparsity of events, resulting in\nredundant computations, increased energy consumption, and potentially degraded\nperformance. Motivated by these observations, we introduce the first sparse\nSpiking Neural Networks (SNNs) framework for 3D human pose tracking based\nsolely on events. Our approach eliminates the need to convert sparse data to\ndense formats or incorporate additional images, thereby fully exploiting the\ninnate sparsity of input events. Central to our framework is a novel Spiking\nSpatiotemporal Transformer, which enables bi-directional spatiotemporal fusion\nof spike pose features and provides a guaranteed similarity measurement between\nbinary spike features in spiking attention. Moreover, we have constructed a\nlarge-scale synthetic dataset, SynEventHPD, that features a broad and diverse\nset of 3D human motions, as well as much longer hours of event streams.\nEmpirical experiments demonstrate the superiority of our approach over existing\nstate-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%\nenergy cost. Furthermore, our approach outperforms existing SNN-based\nbenchmarks in this task, highlighting the effectiveness of our proposed SNN\nframework. The dataset will be released upon acceptance, and code can be found\nat https://github.com/JimmyZou/HumanPoseTracking_SNN.\n", "link": "http://arxiv.org/abs/2303.09681v5", "date": "2025-05-15", "relevancy": 2.9129, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5977}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5928}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20Efficient%203D%20Human%20Pose%20Tracking%20from%20Events%20with%20Spiking%0A%20%20Spatiotemporal%20Transformer&body=Title%3A%20Highly%20Efficient%203D%20Human%20Pose%20Tracking%20from%20Events%20with%20Spiking%0A%20%20Spatiotemporal%20Transformer%0AAuthor%3A%20Shihao%20Zou%20and%20Yuxuan%20Mu%20and%20Wei%20Ji%20and%20Zi-An%20Wang%20and%20Xinxin%20Zuo%20and%20Sen%20Wang%20and%20Weixin%20Si%20and%20Li%20Cheng%0AAbstract%3A%20%20%20Event%20camera%2C%20as%20an%20asynchronous%20vision%20sensor%20capturing%20scene%20dynamics%2C%0Apresents%20new%20opportunities%20for%20highly%20efficient%203D%20human%20pose%20tracking.%0AExisting%20approaches%20typically%20adopt%20modern-day%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20such%20as%20CNNs%20or%20Transformer%2C%20where%20sparse%20events%20are%20converted%20into%0Adense%20images%20or%20paired%20with%20additional%20gray-scale%20images%20as%20input.%20Such%0Apractices%2C%20however%2C%20ignore%20the%20inherent%20sparsity%20of%20events%2C%20resulting%20in%0Aredundant%20computations%2C%20increased%20energy%20consumption%2C%20and%20potentially%20degraded%0Aperformance.%20Motivated%20by%20these%20observations%2C%20we%20introduce%20the%20first%20sparse%0ASpiking%20Neural%20Networks%20%28SNNs%29%20framework%20for%203D%20human%20pose%20tracking%20based%0Asolely%20on%20events.%20Our%20approach%20eliminates%20the%20need%20to%20convert%20sparse%20data%20to%0Adense%20formats%20or%20incorporate%20additional%20images%2C%20thereby%20fully%20exploiting%20the%0Ainnate%20sparsity%20of%20input%20events.%20Central%20to%20our%20framework%20is%20a%20novel%20Spiking%0ASpatiotemporal%20Transformer%2C%20which%20enables%20bi-directional%20spatiotemporal%20fusion%0Aof%20spike%20pose%20features%20and%20provides%20a%20guaranteed%20similarity%20measurement%20between%0Abinary%20spike%20features%20in%20spiking%20attention.%20Moreover%2C%20we%20have%20constructed%20a%0Alarge-scale%20synthetic%20dataset%2C%20SynEventHPD%2C%20that%20features%20a%20broad%20and%20diverse%0Aset%20of%203D%20human%20motions%2C%20as%20well%20as%20much%20longer%20hours%20of%20event%20streams.%0AEmpirical%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Astate-of-the-art%20%28SOTA%29%20ANN-based%20methods%2C%20requiring%20only%2019.1%25%20FLOPs%20and%203.6%25%0Aenergy%20cost.%20Furthermore%2C%20our%20approach%20outperforms%20existing%20SNN-based%0Abenchmarks%20in%20this%20task%2C%20highlighting%20the%20effectiveness%20of%20our%20proposed%20SNN%0Aframework.%20The%20dataset%20will%20be%20released%20upon%20acceptance%2C%20and%20code%20can%20be%20found%0Aat%20https%3A//github.com/JimmyZou/HumanPoseTracking_SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09681v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520Efficient%25203D%2520Human%2520Pose%2520Tracking%2520from%2520Events%2520with%2520Spiking%250A%2520%2520Spatiotemporal%2520Transformer%26entry.906535625%3DShihao%2520Zou%2520and%2520Yuxuan%2520Mu%2520and%2520Wei%2520Ji%2520and%2520Zi-An%2520Wang%2520and%2520Xinxin%2520Zuo%2520and%2520Sen%2520Wang%2520and%2520Weixin%2520Si%2520and%2520Li%2520Cheng%26entry.1292438233%3D%2520%2520Event%2520camera%252C%2520as%2520an%2520asynchronous%2520vision%2520sensor%2520capturing%2520scene%2520dynamics%252C%250Apresents%2520new%2520opportunities%2520for%2520highly%2520efficient%25203D%2520human%2520pose%2520tracking.%250AExisting%2520approaches%2520typically%2520adopt%2520modern-day%2520Artificial%2520Neural%2520Networks%250A%2528ANNs%2529%252C%2520such%2520as%2520CNNs%2520or%2520Transformer%252C%2520where%2520sparse%2520events%2520are%2520converted%2520into%250Adense%2520images%2520or%2520paired%2520with%2520additional%2520gray-scale%2520images%2520as%2520input.%2520Such%250Apractices%252C%2520however%252C%2520ignore%2520the%2520inherent%2520sparsity%2520of%2520events%252C%2520resulting%2520in%250Aredundant%2520computations%252C%2520increased%2520energy%2520consumption%252C%2520and%2520potentially%2520degraded%250Aperformance.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520introduce%2520the%2520first%2520sparse%250ASpiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520framework%2520for%25203D%2520human%2520pose%2520tracking%2520based%250Asolely%2520on%2520events.%2520Our%2520approach%2520eliminates%2520the%2520need%2520to%2520convert%2520sparse%2520data%2520to%250Adense%2520formats%2520or%2520incorporate%2520additional%2520images%252C%2520thereby%2520fully%2520exploiting%2520the%250Ainnate%2520sparsity%2520of%2520input%2520events.%2520Central%2520to%2520our%2520framework%2520is%2520a%2520novel%2520Spiking%250ASpatiotemporal%2520Transformer%252C%2520which%2520enables%2520bi-directional%2520spatiotemporal%2520fusion%250Aof%2520spike%2520pose%2520features%2520and%2520provides%2520a%2520guaranteed%2520similarity%2520measurement%2520between%250Abinary%2520spike%2520features%2520in%2520spiking%2520attention.%2520Moreover%252C%2520we%2520have%2520constructed%2520a%250Alarge-scale%2520synthetic%2520dataset%252C%2520SynEventHPD%252C%2520that%2520features%2520a%2520broad%2520and%2520diverse%250Aset%2520of%25203D%2520human%2520motions%252C%2520as%2520well%2520as%2520much%2520longer%2520hours%2520of%2520event%2520streams.%250AEmpirical%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520existing%250Astate-of-the-art%2520%2528SOTA%2529%2520ANN-based%2520methods%252C%2520requiring%2520only%252019.1%2525%2520FLOPs%2520and%25203.6%2525%250Aenergy%2520cost.%2520Furthermore%252C%2520our%2520approach%2520outperforms%2520existing%2520SNN-based%250Abenchmarks%2520in%2520this%2520task%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520proposed%2520SNN%250Aframework.%2520The%2520dataset%2520will%2520be%2520released%2520upon%2520acceptance%252C%2520and%2520code%2520can%2520be%2520found%250Aat%2520https%253A//github.com/JimmyZou/HumanPoseTracking_SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09681v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20Efficient%203D%20Human%20Pose%20Tracking%20from%20Events%20with%20Spiking%0A%20%20Spatiotemporal%20Transformer&entry.906535625=Shihao%20Zou%20and%20Yuxuan%20Mu%20and%20Wei%20Ji%20and%20Zi-An%20Wang%20and%20Xinxin%20Zuo%20and%20Sen%20Wang%20and%20Weixin%20Si%20and%20Li%20Cheng&entry.1292438233=%20%20Event%20camera%2C%20as%20an%20asynchronous%20vision%20sensor%20capturing%20scene%20dynamics%2C%0Apresents%20new%20opportunities%20for%20highly%20efficient%203D%20human%20pose%20tracking.%0AExisting%20approaches%20typically%20adopt%20modern-day%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20such%20as%20CNNs%20or%20Transformer%2C%20where%20sparse%20events%20are%20converted%20into%0Adense%20images%20or%20paired%20with%20additional%20gray-scale%20images%20as%20input.%20Such%0Apractices%2C%20however%2C%20ignore%20the%20inherent%20sparsity%20of%20events%2C%20resulting%20in%0Aredundant%20computations%2C%20increased%20energy%20consumption%2C%20and%20potentially%20degraded%0Aperformance.%20Motivated%20by%20these%20observations%2C%20we%20introduce%20the%20first%20sparse%0ASpiking%20Neural%20Networks%20%28SNNs%29%20framework%20for%203D%20human%20pose%20tracking%20based%0Asolely%20on%20events.%20Our%20approach%20eliminates%20the%20need%20to%20convert%20sparse%20data%20to%0Adense%20formats%20or%20incorporate%20additional%20images%2C%20thereby%20fully%20exploiting%20the%0Ainnate%20sparsity%20of%20input%20events.%20Central%20to%20our%20framework%20is%20a%20novel%20Spiking%0ASpatiotemporal%20Transformer%2C%20which%20enables%20bi-directional%20spatiotemporal%20fusion%0Aof%20spike%20pose%20features%20and%20provides%20a%20guaranteed%20similarity%20measurement%20between%0Abinary%20spike%20features%20in%20spiking%20attention.%20Moreover%2C%20we%20have%20constructed%20a%0Alarge-scale%20synthetic%20dataset%2C%20SynEventHPD%2C%20that%20features%20a%20broad%20and%20diverse%0Aset%20of%203D%20human%20motions%2C%20as%20well%20as%20much%20longer%20hours%20of%20event%20streams.%0AEmpirical%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Astate-of-the-art%20%28SOTA%29%20ANN-based%20methods%2C%20requiring%20only%2019.1%25%20FLOPs%20and%203.6%25%0Aenergy%20cost.%20Furthermore%2C%20our%20approach%20outperforms%20existing%20SNN-based%0Abenchmarks%20in%20this%20task%2C%20highlighting%20the%20effectiveness%20of%20our%20proposed%20SNN%0Aframework.%20The%20dataset%20will%20be%20released%20upon%20acceptance%2C%20and%20code%20can%20be%20found%0Aat%20https%3A//github.com/JimmyZou/HumanPoseTracking_SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09681v5&entry.124074799=Read"},
{"title": "A Unified and Scalable Membership Inference Method for Visual\n  Self-supervised Encoder via Part-aware Capability", "author": "Jie Zhu and Jirong Zha and Ding Li and Leye Wang", "abstract": "  Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.\n", "link": "http://arxiv.org/abs/2505.10351v1", "date": "2025-05-15", "relevancy": 2.9001, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6014}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5791}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20and%20Scalable%20Membership%20Inference%20Method%20for%20Visual%0A%20%20Self-supervised%20Encoder%20via%20Part-aware%20Capability&body=Title%3A%20A%20Unified%20and%20Scalable%20Membership%20Inference%20Method%20for%20Visual%0A%20%20Self-supervised%20Encoder%20via%20Part-aware%20Capability%0AAuthor%3A%20Jie%20Zhu%20and%20Jirong%20Zha%20and%20Ding%20Li%20and%20Leye%20Wang%0AAbstract%3A%20%20%20Self-supervised%20learning%20shows%20promise%20in%20harnessing%20extensive%20unlabeled%0Adata%2C%20but%20it%20also%20confronts%20significant%20privacy%20concerns%2C%20especially%20in%20vision.%0AIn%20this%20paper%2C%20we%20perform%20membership%20inference%20on%20visual%20self-supervised%20models%0Ain%20a%20more%20realistic%20setting%3A%20self-supervised%20training%20method%20and%20details%20are%0Aunknown%20for%20an%20adversary%20when%20attacking%20as%20he%20usually%20faces%20a%20black-box%20system%0Ain%20practice.%20In%20this%20setting%2C%20considering%20that%20self-supervised%20model%20could%20be%0Atrained%20by%20completely%20different%20self-supervised%20paradigms%2C%20e.g.%2C%20masked%20image%0Amodeling%20and%20contrastive%20learning%2C%20with%20complex%20training%20details%2C%20we%20propose%20a%0Aunified%20membership%20inference%20method%20called%20PartCrop.%20It%20is%20motivated%20by%20the%0Ashared%20part-aware%20capability%20among%20models%20and%20stronger%20part%20response%20on%20the%0Atraining%20data.%20Specifically%2C%20PartCrop%20crops%20parts%20of%20objects%20in%20an%20image%20to%0Aquery%20responses%20within%20the%20image%20in%20representation%20space.%20We%20conduct%20extensive%0Aattacks%20on%20self-supervised%20models%20with%20different%20training%20protocols%20and%0Astructures%20using%20three%20widely%20used%20image%20datasets.%20The%20results%20verify%20the%0Aeffectiveness%20and%20generalization%20of%20PartCrop.%20Moreover%2C%20to%20defend%20against%0APartCrop%2C%20we%20evaluate%20two%20common%20approaches%2C%20i.e.%2C%20early%20stop%20and%20differential%0Aprivacy%2C%20and%20propose%20a%20tailored%20method%20called%20shrinking%20crop%20scale%20range.%20The%0Adefense%20experiments%20indicate%20that%20all%20of%20them%20are%20effective.%20Finally%2C%20besides%0Aprototype%20testing%20on%20toy%20visual%20encoders%20and%20small-scale%20image%20datasets%2C%20we%0Aquantitatively%20study%20the%20impacts%20of%20scaling%20from%20both%20data%20and%20model%20aspects%20in%0Aa%20realistic%20scenario%20and%20propose%20a%20scalable%20PartCrop-v2%20by%20introducing%20two%0Astructural%20improvements%20to%20PartCrop.%20Our%20code%20is%20at%0Ahttps%3A//github.com/JiePKU/PartCrop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520and%2520Scalable%2520Membership%2520Inference%2520Method%2520for%2520Visual%250A%2520%2520Self-supervised%2520Encoder%2520via%2520Part-aware%2520Capability%26entry.906535625%3DJie%2520Zhu%2520and%2520Jirong%2520Zha%2520and%2520Ding%2520Li%2520and%2520Leye%2520Wang%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520shows%2520promise%2520in%2520harnessing%2520extensive%2520unlabeled%250Adata%252C%2520but%2520it%2520also%2520confronts%2520significant%2520privacy%2520concerns%252C%2520especially%2520in%2520vision.%250AIn%2520this%2520paper%252C%2520we%2520perform%2520membership%2520inference%2520on%2520visual%2520self-supervised%2520models%250Ain%2520a%2520more%2520realistic%2520setting%253A%2520self-supervised%2520training%2520method%2520and%2520details%2520are%250Aunknown%2520for%2520an%2520adversary%2520when%2520attacking%2520as%2520he%2520usually%2520faces%2520a%2520black-box%2520system%250Ain%2520practice.%2520In%2520this%2520setting%252C%2520considering%2520that%2520self-supervised%2520model%2520could%2520be%250Atrained%2520by%2520completely%2520different%2520self-supervised%2520paradigms%252C%2520e.g.%252C%2520masked%2520image%250Amodeling%2520and%2520contrastive%2520learning%252C%2520with%2520complex%2520training%2520details%252C%2520we%2520propose%2520a%250Aunified%2520membership%2520inference%2520method%2520called%2520PartCrop.%2520It%2520is%2520motivated%2520by%2520the%250Ashared%2520part-aware%2520capability%2520among%2520models%2520and%2520stronger%2520part%2520response%2520on%2520the%250Atraining%2520data.%2520Specifically%252C%2520PartCrop%2520crops%2520parts%2520of%2520objects%2520in%2520an%2520image%2520to%250Aquery%2520responses%2520within%2520the%2520image%2520in%2520representation%2520space.%2520We%2520conduct%2520extensive%250Aattacks%2520on%2520self-supervised%2520models%2520with%2520different%2520training%2520protocols%2520and%250Astructures%2520using%2520three%2520widely%2520used%2520image%2520datasets.%2520The%2520results%2520verify%2520the%250Aeffectiveness%2520and%2520generalization%2520of%2520PartCrop.%2520Moreover%252C%2520to%2520defend%2520against%250APartCrop%252C%2520we%2520evaluate%2520two%2520common%2520approaches%252C%2520i.e.%252C%2520early%2520stop%2520and%2520differential%250Aprivacy%252C%2520and%2520propose%2520a%2520tailored%2520method%2520called%2520shrinking%2520crop%2520scale%2520range.%2520The%250Adefense%2520experiments%2520indicate%2520that%2520all%2520of%2520them%2520are%2520effective.%2520Finally%252C%2520besides%250Aprototype%2520testing%2520on%2520toy%2520visual%2520encoders%2520and%2520small-scale%2520image%2520datasets%252C%2520we%250Aquantitatively%2520study%2520the%2520impacts%2520of%2520scaling%2520from%2520both%2520data%2520and%2520model%2520aspects%2520in%250Aa%2520realistic%2520scenario%2520and%2520propose%2520a%2520scalable%2520PartCrop-v2%2520by%2520introducing%2520two%250Astructural%2520improvements%2520to%2520PartCrop.%2520Our%2520code%2520is%2520at%250Ahttps%253A//github.com/JiePKU/PartCrop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20and%20Scalable%20Membership%20Inference%20Method%20for%20Visual%0A%20%20Self-supervised%20Encoder%20via%20Part-aware%20Capability&entry.906535625=Jie%20Zhu%20and%20Jirong%20Zha%20and%20Ding%20Li%20and%20Leye%20Wang&entry.1292438233=%20%20Self-supervised%20learning%20shows%20promise%20in%20harnessing%20extensive%20unlabeled%0Adata%2C%20but%20it%20also%20confronts%20significant%20privacy%20concerns%2C%20especially%20in%20vision.%0AIn%20this%20paper%2C%20we%20perform%20membership%20inference%20on%20visual%20self-supervised%20models%0Ain%20a%20more%20realistic%20setting%3A%20self-supervised%20training%20method%20and%20details%20are%0Aunknown%20for%20an%20adversary%20when%20attacking%20as%20he%20usually%20faces%20a%20black-box%20system%0Ain%20practice.%20In%20this%20setting%2C%20considering%20that%20self-supervised%20model%20could%20be%0Atrained%20by%20completely%20different%20self-supervised%20paradigms%2C%20e.g.%2C%20masked%20image%0Amodeling%20and%20contrastive%20learning%2C%20with%20complex%20training%20details%2C%20we%20propose%20a%0Aunified%20membership%20inference%20method%20called%20PartCrop.%20It%20is%20motivated%20by%20the%0Ashared%20part-aware%20capability%20among%20models%20and%20stronger%20part%20response%20on%20the%0Atraining%20data.%20Specifically%2C%20PartCrop%20crops%20parts%20of%20objects%20in%20an%20image%20to%0Aquery%20responses%20within%20the%20image%20in%20representation%20space.%20We%20conduct%20extensive%0Aattacks%20on%20self-supervised%20models%20with%20different%20training%20protocols%20and%0Astructures%20using%20three%20widely%20used%20image%20datasets.%20The%20results%20verify%20the%0Aeffectiveness%20and%20generalization%20of%20PartCrop.%20Moreover%2C%20to%20defend%20against%0APartCrop%2C%20we%20evaluate%20two%20common%20approaches%2C%20i.e.%2C%20early%20stop%20and%20differential%0Aprivacy%2C%20and%20propose%20a%20tailored%20method%20called%20shrinking%20crop%20scale%20range.%20The%0Adefense%20experiments%20indicate%20that%20all%20of%20them%20are%20effective.%20Finally%2C%20besides%0Aprototype%20testing%20on%20toy%20visual%20encoders%20and%20small-scale%20image%20datasets%2C%20we%0Aquantitatively%20study%20the%20impacts%20of%20scaling%20from%20both%20data%20and%20model%20aspects%20in%0Aa%20realistic%20scenario%20and%20propose%20a%20scalable%20PartCrop-v2%20by%20introducing%20two%0Astructural%20improvements%20to%20PartCrop.%20Our%20code%20is%20at%0Ahttps%3A//github.com/JiePKU/PartCrop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10351v1&entry.124074799=Read"},
{"title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot\n  Learning", "author": "Yue Wang and Shuai Xu and Xuelin Zhu and Yicong Li", "abstract": "  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.\n", "link": "http://arxiv.org/abs/2505.10289v1", "date": "2025-05-15", "relevancy": 2.8785, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCI%3A%20Addressing%20CLIP%27s%20Inherent%20Limitations%20for%20Compositional%20Zero-Shot%0A%20%20Learning&body=Title%3A%20MSCI%3A%20Addressing%20CLIP%27s%20Inherent%20Limitations%20for%20Compositional%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Yue%20Wang%20and%20Shuai%20Xu%20and%20Xuelin%20Zhu%20and%20Yicong%20Li%0AAbstract%3A%20%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20unseen%20state-object%0Acombinations%20by%20leveraging%20known%20combinations.%20Existing%20studies%20basically%20rely%0Aon%20the%20cross-modal%20alignment%20capabilities%20of%20CLIP%20but%20tend%20to%20overlook%20its%0Alimitations%20in%20capturing%20fine-grained%20local%20features%2C%20which%20arise%20from%20its%0Aarchitectural%20and%20training%20paradigm.%20To%20address%20this%20issue%2C%20we%20propose%20a%0AMulti-Stage%20Cross-modal%20Interaction%20%28MSCI%29%20model%20that%20effectively%20explores%20and%0Autilizes%20intermediate-layer%20information%20from%20CLIP%27s%20visual%20encoder.%0ASpecifically%2C%20we%20design%20two%20self-adaptive%20aggregators%20to%20extract%20local%0Ainformation%20from%20low-level%20visual%20features%20and%20integrate%20global%20information%0Afrom%20high-level%20visual%20features%2C%20respectively.%20These%20key%20information%20are%0Aprogressively%20incorporated%20into%20textual%20representations%20through%20a%0Astage-by-stage%20interaction%20mechanism%2C%20significantly%20enhancing%20the%20model%27s%0Aperception%20capability%20for%20fine-grained%20local%20visual%20information.%20Additionally%2C%0AMSCI%20dynamically%20adjusts%20the%20attention%20weights%20between%20global%20and%20local%20visual%0Ainformation%20based%20on%20different%20combinations%2C%20as%20well%20as%20different%20elements%0Awithin%20the%20same%20combination%2C%20allowing%20it%20to%20flexibly%20adapt%20to%20diverse%0Ascenarios.%20Experiments%20on%20three%20widely%20used%20datasets%20fully%20validate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20model.%20Data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/ltpwy/MSCI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCI%253A%2520Addressing%2520CLIP%2527s%2520Inherent%2520Limitations%2520for%2520Compositional%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DYue%2520Wang%2520and%2520Shuai%2520Xu%2520and%2520Xuelin%2520Zhu%2520and%2520Yicong%2520Li%26entry.1292438233%3D%2520%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%2520aims%2520to%2520recognize%2520unseen%2520state-object%250Acombinations%2520by%2520leveraging%2520known%2520combinations.%2520Existing%2520studies%2520basically%2520rely%250Aon%2520the%2520cross-modal%2520alignment%2520capabilities%2520of%2520CLIP%2520but%2520tend%2520to%2520overlook%2520its%250Alimitations%2520in%2520capturing%2520fine-grained%2520local%2520features%252C%2520which%2520arise%2520from%2520its%250Aarchitectural%2520and%2520training%2520paradigm.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250AMulti-Stage%2520Cross-modal%2520Interaction%2520%2528MSCI%2529%2520model%2520that%2520effectively%2520explores%2520and%250Autilizes%2520intermediate-layer%2520information%2520from%2520CLIP%2527s%2520visual%2520encoder.%250ASpecifically%252C%2520we%2520design%2520two%2520self-adaptive%2520aggregators%2520to%2520extract%2520local%250Ainformation%2520from%2520low-level%2520visual%2520features%2520and%2520integrate%2520global%2520information%250Afrom%2520high-level%2520visual%2520features%252C%2520respectively.%2520These%2520key%2520information%2520are%250Aprogressively%2520incorporated%2520into%2520textual%2520representations%2520through%2520a%250Astage-by-stage%2520interaction%2520mechanism%252C%2520significantly%2520enhancing%2520the%2520model%2527s%250Aperception%2520capability%2520for%2520fine-grained%2520local%2520visual%2520information.%2520Additionally%252C%250AMSCI%2520dynamically%2520adjusts%2520the%2520attention%2520weights%2520between%2520global%2520and%2520local%2520visual%250Ainformation%2520based%2520on%2520different%2520combinations%252C%2520as%2520well%2520as%2520different%2520elements%250Awithin%2520the%2520same%2520combination%252C%2520allowing%2520it%2520to%2520flexibly%2520adapt%2520to%2520diverse%250Ascenarios.%2520Experiments%2520on%2520three%2520widely%2520used%2520datasets%2520fully%2520validate%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520model.%2520Data%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/ltpwy/MSCI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCI%3A%20Addressing%20CLIP%27s%20Inherent%20Limitations%20for%20Compositional%20Zero-Shot%0A%20%20Learning&entry.906535625=Yue%20Wang%20and%20Shuai%20Xu%20and%20Xuelin%20Zhu%20and%20Yicong%20Li&entry.1292438233=%20%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%20aims%20to%20recognize%20unseen%20state-object%0Acombinations%20by%20leveraging%20known%20combinations.%20Existing%20studies%20basically%20rely%0Aon%20the%20cross-modal%20alignment%20capabilities%20of%20CLIP%20but%20tend%20to%20overlook%20its%0Alimitations%20in%20capturing%20fine-grained%20local%20features%2C%20which%20arise%20from%20its%0Aarchitectural%20and%20training%20paradigm.%20To%20address%20this%20issue%2C%20we%20propose%20a%0AMulti-Stage%20Cross-modal%20Interaction%20%28MSCI%29%20model%20that%20effectively%20explores%20and%0Autilizes%20intermediate-layer%20information%20from%20CLIP%27s%20visual%20encoder.%0ASpecifically%2C%20we%20design%20two%20self-adaptive%20aggregators%20to%20extract%20local%0Ainformation%20from%20low-level%20visual%20features%20and%20integrate%20global%20information%0Afrom%20high-level%20visual%20features%2C%20respectively.%20These%20key%20information%20are%0Aprogressively%20incorporated%20into%20textual%20representations%20through%20a%0Astage-by-stage%20interaction%20mechanism%2C%20significantly%20enhancing%20the%20model%27s%0Aperception%20capability%20for%20fine-grained%20local%20visual%20information.%20Additionally%2C%0AMSCI%20dynamically%20adjusts%20the%20attention%20weights%20between%20global%20and%20local%20visual%0Ainformation%20based%20on%20different%20combinations%2C%20as%20well%20as%20different%20elements%0Awithin%20the%20same%20combination%2C%20allowing%20it%20to%20flexibly%20adapt%20to%20diverse%0Ascenarios.%20Experiments%20on%20three%20widely%20used%20datasets%20fully%20validate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20model.%20Data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/ltpwy/MSCI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10289v1&entry.124074799=Read"},
{"title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume\n  Estimation", "author": "Umair Haroon and Ahmad AlMughrabi and Thanasis Zoumpekas and Ricardo Marques and Petia Radeva", "abstract": "  Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.\n", "link": "http://arxiv.org/abs/2505.10205v1", "date": "2025-05-15", "relevancy": 2.7993, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VolE%3A%20A%20Point-cloud%20Framework%20for%20Food%203D%20Reconstruction%20and%20Volume%0A%20%20Estimation&body=Title%3A%20VolE%3A%20A%20Point-cloud%20Framework%20for%20Food%203D%20Reconstruction%20and%20Volume%0A%20%20Estimation%0AAuthor%3A%20Umair%20Haroon%20and%20Ahmad%20AlMughrabi%20and%20Thanasis%20Zoumpekas%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%0AAbstract%3A%20%20%20Accurate%20food%20volume%20estimation%20is%20crucial%20for%20medical%20nutrition%20management%0Aand%20health%20monitoring%20applications%2C%20but%20current%20food%20volume%20estimation%20methods%0Aare%20often%20limited%20by%20mononuclear%20data%2C%20leveraging%20single-purpose%20hardware%20such%0Aas%203D%20scanners%2C%20gathering%20sensor-oriented%20information%20such%20as%20depth%0Ainformation%2C%20or%20relying%20on%20camera%20calibration%20using%20a%20reference%20object.%20In%20this%0Apaper%2C%20we%20present%20VolE%2C%20a%20novel%20framework%20that%20leverages%20mobile%20device-driven%0A3D%20reconstruction%20to%20estimate%20food%20volume.%20VolE%20captures%20images%20and%20camera%0Alocations%20in%20free%20motion%20to%20generate%20precise%203D%20models%2C%20thanks%20to%20AR-capable%0Amobile%20devices.%20To%20achieve%20real-world%20measurement%2C%20VolE%20is%20a%20reference-%20and%0Adepth-free%20framework%20that%20leverages%20food%20video%20segmentation%20for%20food%20mask%0Ageneration.%20We%20also%20introduce%20a%20new%20food%20dataset%20encompassing%20the%20challenging%0Ascenarios%20absent%20in%20the%20previous%20benchmarks.%20Our%20experiments%20demonstrate%20that%0AVolE%20outperforms%20the%20existing%20volume%20estimation%20techniques%20across%20multiple%0Adatasets%20by%20achieving%202.22%20%25%20MAPE%2C%20highlighting%20its%20superior%20performance%20in%0Afood%20volume%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolE%253A%2520A%2520Point-cloud%2520Framework%2520for%2520Food%25203D%2520Reconstruction%2520and%2520Volume%250A%2520%2520Estimation%26entry.906535625%3DUmair%2520Haroon%2520and%2520Ahmad%2520AlMughrabi%2520and%2520Thanasis%2520Zoumpekas%2520and%2520Ricardo%2520Marques%2520and%2520Petia%2520Radeva%26entry.1292438233%3D%2520%2520Accurate%2520food%2520volume%2520estimation%2520is%2520crucial%2520for%2520medical%2520nutrition%2520management%250Aand%2520health%2520monitoring%2520applications%252C%2520but%2520current%2520food%2520volume%2520estimation%2520methods%250Aare%2520often%2520limited%2520by%2520mononuclear%2520data%252C%2520leveraging%2520single-purpose%2520hardware%2520such%250Aas%25203D%2520scanners%252C%2520gathering%2520sensor-oriented%2520information%2520such%2520as%2520depth%250Ainformation%252C%2520or%2520relying%2520on%2520camera%2520calibration%2520using%2520a%2520reference%2520object.%2520In%2520this%250Apaper%252C%2520we%2520present%2520VolE%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520mobile%2520device-driven%250A3D%2520reconstruction%2520to%2520estimate%2520food%2520volume.%2520VolE%2520captures%2520images%2520and%2520camera%250Alocations%2520in%2520free%2520motion%2520to%2520generate%2520precise%25203D%2520models%252C%2520thanks%2520to%2520AR-capable%250Amobile%2520devices.%2520To%2520achieve%2520real-world%2520measurement%252C%2520VolE%2520is%2520a%2520reference-%2520and%250Adepth-free%2520framework%2520that%2520leverages%2520food%2520video%2520segmentation%2520for%2520food%2520mask%250Ageneration.%2520We%2520also%2520introduce%2520a%2520new%2520food%2520dataset%2520encompassing%2520the%2520challenging%250Ascenarios%2520absent%2520in%2520the%2520previous%2520benchmarks.%2520Our%2520experiments%2520demonstrate%2520that%250AVolE%2520outperforms%2520the%2520existing%2520volume%2520estimation%2520techniques%2520across%2520multiple%250Adatasets%2520by%2520achieving%25202.22%2520%2525%2520MAPE%252C%2520highlighting%2520its%2520superior%2520performance%2520in%250Afood%2520volume%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VolE%3A%20A%20Point-cloud%20Framework%20for%20Food%203D%20Reconstruction%20and%20Volume%0A%20%20Estimation&entry.906535625=Umair%20Haroon%20and%20Ahmad%20AlMughrabi%20and%20Thanasis%20Zoumpekas%20and%20Ricardo%20Marques%20and%20Petia%20Radeva&entry.1292438233=%20%20Accurate%20food%20volume%20estimation%20is%20crucial%20for%20medical%20nutrition%20management%0Aand%20health%20monitoring%20applications%2C%20but%20current%20food%20volume%20estimation%20methods%0Aare%20often%20limited%20by%20mononuclear%20data%2C%20leveraging%20single-purpose%20hardware%20such%0Aas%203D%20scanners%2C%20gathering%20sensor-oriented%20information%20such%20as%20depth%0Ainformation%2C%20or%20relying%20on%20camera%20calibration%20using%20a%20reference%20object.%20In%20this%0Apaper%2C%20we%20present%20VolE%2C%20a%20novel%20framework%20that%20leverages%20mobile%20device-driven%0A3D%20reconstruction%20to%20estimate%20food%20volume.%20VolE%20captures%20images%20and%20camera%0Alocations%20in%20free%20motion%20to%20generate%20precise%203D%20models%2C%20thanks%20to%20AR-capable%0Amobile%20devices.%20To%20achieve%20real-world%20measurement%2C%20VolE%20is%20a%20reference-%20and%0Adepth-free%20framework%20that%20leverages%20food%20video%20segmentation%20for%20food%20mask%0Ageneration.%20We%20also%20introduce%20a%20new%20food%20dataset%20encompassing%20the%20challenging%0Ascenarios%20absent%20in%20the%20previous%20benchmarks.%20Our%20experiments%20demonstrate%20that%0AVolE%20outperforms%20the%20existing%20volume%20estimation%20techniques%20across%20multiple%0Adatasets%20by%20achieving%202.22%20%25%20MAPE%2C%20highlighting%20its%20superior%20performance%20in%0Afood%20volume%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10205v1&entry.124074799=Read"},
{"title": "End-to-End Vision Tokenizer Tuning", "author": "Wenxuan Wang and Fan Zhang and Yufeng Cui and Haiwen Diao and Zhuoyan Luo and Huchuan Lu and Jing Liu and Xinlong Wang", "abstract": "  Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.\n", "link": "http://arxiv.org/abs/2505.10562v1", "date": "2025-05-15", "relevancy": 2.7801, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6042}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Vision%20Tokenizer%20Tuning&body=Title%3A%20End-to-End%20Vision%20Tokenizer%20Tuning%0AAuthor%3A%20Wenxuan%20Wang%20and%20Fan%20Zhang%20and%20Yufeng%20Cui%20and%20Haiwen%20Diao%20and%20Zhuoyan%20Luo%20and%20Huchuan%20Lu%20and%20Jing%20Liu%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Existing%20vision%20tokenization%20isolates%20the%20optimization%20of%20vision%20tokenizers%0Afrom%20downstream%20training%2C%20implicitly%20assuming%20the%20visual%20tokens%20can%20generalize%0Awell%20across%20various%20tasks%2C%20e.g.%2C%20image%20generation%20and%20visual%20question%0Aanswering.%20The%20vision%20tokenizer%20optimized%20for%20low-level%20reconstruction%20is%0Aagnostic%20to%20downstream%20tasks%20requiring%20varied%20representations%20and%20semantics.%0AThis%20decoupled%20paradigm%20introduces%20a%20critical%20misalignment%3A%20The%20loss%20of%20the%0Avision%20tokenization%20can%20be%20the%20representation%20bottleneck%20for%20target%20tasks.%20For%0Aexample%2C%20errors%20in%20tokenizing%20text%20in%20a%20given%20image%20lead%20to%20poor%20results%20when%0Arecognizing%20or%20generating%20them.%20To%20address%20this%2C%20we%20propose%20ETT%2C%20an%20end-to-end%0Avision%20tokenizer%20tuning%20approach%20that%20enables%20joint%20optimization%20between%20vision%0Atokenization%20and%20target%20autoregressive%20tasks.%20Unlike%20prior%20autoregressive%0Amodels%20that%20use%20only%20discrete%20indices%20from%20a%20frozen%20vision%20tokenizer%2C%20ETT%0Aleverages%20the%20visual%20embeddings%20of%20the%20tokenizer%20codebook%2C%20and%20optimizes%20the%0Avision%20tokenizers%20end-to-end%20with%20both%20reconstruction%20and%20caption%20objectives.%0AETT%20can%20be%20seamlessly%20integrated%20into%20existing%20training%20pipelines%20with%20minimal%0Aarchitecture%20modifications.%20Our%20ETT%20is%20simple%20to%20implement%20and%20integrate%2C%0Awithout%20the%20need%20to%20adjust%20the%20original%20codebooks%20or%20architectures%20of%20the%0Aemployed%20large%20language%20models.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20end-to-end%20vision%20tokenizer%20tuning%20unlocks%20significant%20performance%0Agains%2C%20i.e.%2C%202-6%25%20for%20multimodal%20understanding%20and%20visual%20generation%20tasks%0Acompared%20to%20frozen%20tokenizer%20baselines%2C%20while%20preserving%20the%20original%0Areconstruction%20capability.%20We%20hope%20this%20very%20simple%20and%20strong%20method%20can%0Aempower%20multimodal%20foundation%20models%20besides%20image%20generation%20and%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Vision%2520Tokenizer%2520Tuning%26entry.906535625%3DWenxuan%2520Wang%2520and%2520Fan%2520Zhang%2520and%2520Yufeng%2520Cui%2520and%2520Haiwen%2520Diao%2520and%2520Zhuoyan%2520Luo%2520and%2520Huchuan%2520Lu%2520and%2520Jing%2520Liu%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520vision%2520tokenization%2520isolates%2520the%2520optimization%2520of%2520vision%2520tokenizers%250Afrom%2520downstream%2520training%252C%2520implicitly%2520assuming%2520the%2520visual%2520tokens%2520can%2520generalize%250Awell%2520across%2520various%2520tasks%252C%2520e.g.%252C%2520image%2520generation%2520and%2520visual%2520question%250Aanswering.%2520The%2520vision%2520tokenizer%2520optimized%2520for%2520low-level%2520reconstruction%2520is%250Aagnostic%2520to%2520downstream%2520tasks%2520requiring%2520varied%2520representations%2520and%2520semantics.%250AThis%2520decoupled%2520paradigm%2520introduces%2520a%2520critical%2520misalignment%253A%2520The%2520loss%2520of%2520the%250Avision%2520tokenization%2520can%2520be%2520the%2520representation%2520bottleneck%2520for%2520target%2520tasks.%2520For%250Aexample%252C%2520errors%2520in%2520tokenizing%2520text%2520in%2520a%2520given%2520image%2520lead%2520to%2520poor%2520results%2520when%250Arecognizing%2520or%2520generating%2520them.%2520To%2520address%2520this%252C%2520we%2520propose%2520ETT%252C%2520an%2520end-to-end%250Avision%2520tokenizer%2520tuning%2520approach%2520that%2520enables%2520joint%2520optimization%2520between%2520vision%250Atokenization%2520and%2520target%2520autoregressive%2520tasks.%2520Unlike%2520prior%2520autoregressive%250Amodels%2520that%2520use%2520only%2520discrete%2520indices%2520from%2520a%2520frozen%2520vision%2520tokenizer%252C%2520ETT%250Aleverages%2520the%2520visual%2520embeddings%2520of%2520the%2520tokenizer%2520codebook%252C%2520and%2520optimizes%2520the%250Avision%2520tokenizers%2520end-to-end%2520with%2520both%2520reconstruction%2520and%2520caption%2520objectives.%250AETT%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520training%2520pipelines%2520with%2520minimal%250Aarchitecture%2520modifications.%2520Our%2520ETT%2520is%2520simple%2520to%2520implement%2520and%2520integrate%252C%250Awithout%2520the%2520need%2520to%2520adjust%2520the%2520original%2520codebooks%2520or%2520architectures%2520of%2520the%250Aemployed%2520large%2520language%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aproposed%2520end-to-end%2520vision%2520tokenizer%2520tuning%2520unlocks%2520significant%2520performance%250Agains%252C%2520i.e.%252C%25202-6%2525%2520for%2520multimodal%2520understanding%2520and%2520visual%2520generation%2520tasks%250Acompared%2520to%2520frozen%2520tokenizer%2520baselines%252C%2520while%2520preserving%2520the%2520original%250Areconstruction%2520capability.%2520We%2520hope%2520this%2520very%2520simple%2520and%2520strong%2520method%2520can%250Aempower%2520multimodal%2520foundation%2520models%2520besides%2520image%2520generation%2520and%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Vision%20Tokenizer%20Tuning&entry.906535625=Wenxuan%20Wang%20and%20Fan%20Zhang%20and%20Yufeng%20Cui%20and%20Haiwen%20Diao%20and%20Zhuoyan%20Luo%20and%20Huchuan%20Lu%20and%20Jing%20Liu%20and%20Xinlong%20Wang&entry.1292438233=%20%20Existing%20vision%20tokenization%20isolates%20the%20optimization%20of%20vision%20tokenizers%0Afrom%20downstream%20training%2C%20implicitly%20assuming%20the%20visual%20tokens%20can%20generalize%0Awell%20across%20various%20tasks%2C%20e.g.%2C%20image%20generation%20and%20visual%20question%0Aanswering.%20The%20vision%20tokenizer%20optimized%20for%20low-level%20reconstruction%20is%0Aagnostic%20to%20downstream%20tasks%20requiring%20varied%20representations%20and%20semantics.%0AThis%20decoupled%20paradigm%20introduces%20a%20critical%20misalignment%3A%20The%20loss%20of%20the%0Avision%20tokenization%20can%20be%20the%20representation%20bottleneck%20for%20target%20tasks.%20For%0Aexample%2C%20errors%20in%20tokenizing%20text%20in%20a%20given%20image%20lead%20to%20poor%20results%20when%0Arecognizing%20or%20generating%20them.%20To%20address%20this%2C%20we%20propose%20ETT%2C%20an%20end-to-end%0Avision%20tokenizer%20tuning%20approach%20that%20enables%20joint%20optimization%20between%20vision%0Atokenization%20and%20target%20autoregressive%20tasks.%20Unlike%20prior%20autoregressive%0Amodels%20that%20use%20only%20discrete%20indices%20from%20a%20frozen%20vision%20tokenizer%2C%20ETT%0Aleverages%20the%20visual%20embeddings%20of%20the%20tokenizer%20codebook%2C%20and%20optimizes%20the%0Avision%20tokenizers%20end-to-end%20with%20both%20reconstruction%20and%20caption%20objectives.%0AETT%20can%20be%20seamlessly%20integrated%20into%20existing%20training%20pipelines%20with%20minimal%0Aarchitecture%20modifications.%20Our%20ETT%20is%20simple%20to%20implement%20and%20integrate%2C%0Awithout%20the%20need%20to%20adjust%20the%20original%20codebooks%20or%20architectures%20of%20the%0Aemployed%20large%20language%20models.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20end-to-end%20vision%20tokenizer%20tuning%20unlocks%20significant%20performance%0Agains%2C%20i.e.%2C%202-6%25%20for%20multimodal%20understanding%20and%20visual%20generation%20tasks%0Acompared%20to%20frozen%20tokenizer%20baselines%2C%20while%20preserving%20the%20original%0Areconstruction%20capability.%20We%20hope%20this%20very%20simple%20and%20strong%20method%20can%0Aempower%20multimodal%20foundation%20models%20besides%20image%20generation%20and%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10562v1&entry.124074799=Read"},
{"title": "Visual Fidelity Index for Generative Semantic Communications with\n  Critical Information Embedding", "author": "Jianhao Huang and Qunsong Zeng and Kaibin Huang", "abstract": "  Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.\n", "link": "http://arxiv.org/abs/2505.10405v1", "date": "2025-05-15", "relevancy": 2.7576, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5791}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Fidelity%20Index%20for%20Generative%20Semantic%20Communications%20with%0A%20%20Critical%20Information%20Embedding&body=Title%3A%20Visual%20Fidelity%20Index%20for%20Generative%20Semantic%20Communications%20with%0A%20%20Critical%20Information%20Embedding%0AAuthor%3A%20Jianhao%20Huang%20and%20Qunsong%20Zeng%20and%20Kaibin%20Huang%0AAbstract%3A%20%20%20Generative%20semantic%20communication%20%28Gen-SemCom%29%20with%20large%20artificial%0Aintelligence%20%28AI%29%20model%20promises%20a%20transformative%20paradigm%20for%206G%20networks%2C%0Awhich%20reduces%20communication%20costs%20by%20transmitting%20low-dimensional%20prompts%0Arather%20than%20raw%20data.%20However%2C%20purely%20prompt-driven%20generation%20loses%0Afine-grained%20visual%20details.%20Additionally%2C%20there%20is%20a%20lack%20of%20systematic%0Ametrics%20to%20evaluate%20the%20performance%20of%20Gen-SemCom%20systems.%20To%20address%20these%0Aissues%2C%20we%20develop%20a%20hybrid%20Gen-SemCom%20system%20with%20a%20critical%20information%0Aembedding%20%28CIE%29%20framework%2C%20where%20both%20text%20prompts%20and%20semantically%20critical%0Afeatures%20are%20extracted%20for%20transmissions.%20First%2C%20a%20novel%20approach%20of%20semantic%0Afiltering%20is%20proposed%20to%20select%20and%20transmit%20the%20semantically%20critical%20features%0Aof%20images%20relevant%20to%20semantic%20label.%20By%20integrating%20the%20text%20prompt%20and%0Acritical%20features%2C%20the%20receiver%20reconstructs%20high-fidelity%20images%20using%20a%0Adiffusion-based%20generative%20model.%20Next%2C%20we%20propose%20the%20generative%20visual%0Ainformation%20fidelity%20%28GVIF%29%20metric%20to%20evaluate%20the%20visual%20quality%20of%20the%0Agenerated%20image.%20By%20characterizing%20the%20statistical%20models%20of%20image%20features%2C%0Athe%20GVIF%20metric%20quantifies%20the%20mutual%20information%20between%20the%20distorted%0Afeatures%20and%20their%20original%20counterparts.%20By%20maximizing%20the%20GVIF%20metric%2C%20we%0Adesign%20a%20channel-adaptive%20Gen-SemCom%20system%20that%20adaptively%20control%20the%20volume%0Aof%20features%20and%20compression%20rate%20according%20to%20the%20channel%20state.%20Experimental%0Aresults%20validate%20the%20GVIF%20metric%27s%20sensitivity%20to%20visual%20fidelity%2C%20correlating%0Awith%20both%20the%20PSNR%20and%20critical%20information%20volume.%20In%20addition%2C%20the%20optimized%0Asystem%20achieves%20superior%20performance%20over%20benchmarking%20schemes%20in%20terms%20of%0Ahigher%20PSNR%20and%20lower%20FID%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Fidelity%2520Index%2520for%2520Generative%2520Semantic%2520Communications%2520with%250A%2520%2520Critical%2520Information%2520Embedding%26entry.906535625%3DJianhao%2520Huang%2520and%2520Qunsong%2520Zeng%2520and%2520Kaibin%2520Huang%26entry.1292438233%3D%2520%2520Generative%2520semantic%2520communication%2520%2528Gen-SemCom%2529%2520with%2520large%2520artificial%250Aintelligence%2520%2528AI%2529%2520model%2520promises%2520a%2520transformative%2520paradigm%2520for%25206G%2520networks%252C%250Awhich%2520reduces%2520communication%2520costs%2520by%2520transmitting%2520low-dimensional%2520prompts%250Arather%2520than%2520raw%2520data.%2520However%252C%2520purely%2520prompt-driven%2520generation%2520loses%250Afine-grained%2520visual%2520details.%2520Additionally%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%250Ametrics%2520to%2520evaluate%2520the%2520performance%2520of%2520Gen-SemCom%2520systems.%2520To%2520address%2520these%250Aissues%252C%2520we%2520develop%2520a%2520hybrid%2520Gen-SemCom%2520system%2520with%2520a%2520critical%2520information%250Aembedding%2520%2528CIE%2529%2520framework%252C%2520where%2520both%2520text%2520prompts%2520and%2520semantically%2520critical%250Afeatures%2520are%2520extracted%2520for%2520transmissions.%2520First%252C%2520a%2520novel%2520approach%2520of%2520semantic%250Afiltering%2520is%2520proposed%2520to%2520select%2520and%2520transmit%2520the%2520semantically%2520critical%2520features%250Aof%2520images%2520relevant%2520to%2520semantic%2520label.%2520By%2520integrating%2520the%2520text%2520prompt%2520and%250Acritical%2520features%252C%2520the%2520receiver%2520reconstructs%2520high-fidelity%2520images%2520using%2520a%250Adiffusion-based%2520generative%2520model.%2520Next%252C%2520we%2520propose%2520the%2520generative%2520visual%250Ainformation%2520fidelity%2520%2528GVIF%2529%2520metric%2520to%2520evaluate%2520the%2520visual%2520quality%2520of%2520the%250Agenerated%2520image.%2520By%2520characterizing%2520the%2520statistical%2520models%2520of%2520image%2520features%252C%250Athe%2520GVIF%2520metric%2520quantifies%2520the%2520mutual%2520information%2520between%2520the%2520distorted%250Afeatures%2520and%2520their%2520original%2520counterparts.%2520By%2520maximizing%2520the%2520GVIF%2520metric%252C%2520we%250Adesign%2520a%2520channel-adaptive%2520Gen-SemCom%2520system%2520that%2520adaptively%2520control%2520the%2520volume%250Aof%2520features%2520and%2520compression%2520rate%2520according%2520to%2520the%2520channel%2520state.%2520Experimental%250Aresults%2520validate%2520the%2520GVIF%2520metric%2527s%2520sensitivity%2520to%2520visual%2520fidelity%252C%2520correlating%250Awith%2520both%2520the%2520PSNR%2520and%2520critical%2520information%2520volume.%2520In%2520addition%252C%2520the%2520optimized%250Asystem%2520achieves%2520superior%2520performance%2520over%2520benchmarking%2520schemes%2520in%2520terms%2520of%250Ahigher%2520PSNR%2520and%2520lower%2520FID%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Fidelity%20Index%20for%20Generative%20Semantic%20Communications%20with%0A%20%20Critical%20Information%20Embedding&entry.906535625=Jianhao%20Huang%20and%20Qunsong%20Zeng%20and%20Kaibin%20Huang&entry.1292438233=%20%20Generative%20semantic%20communication%20%28Gen-SemCom%29%20with%20large%20artificial%0Aintelligence%20%28AI%29%20model%20promises%20a%20transformative%20paradigm%20for%206G%20networks%2C%0Awhich%20reduces%20communication%20costs%20by%20transmitting%20low-dimensional%20prompts%0Arather%20than%20raw%20data.%20However%2C%20purely%20prompt-driven%20generation%20loses%0Afine-grained%20visual%20details.%20Additionally%2C%20there%20is%20a%20lack%20of%20systematic%0Ametrics%20to%20evaluate%20the%20performance%20of%20Gen-SemCom%20systems.%20To%20address%20these%0Aissues%2C%20we%20develop%20a%20hybrid%20Gen-SemCom%20system%20with%20a%20critical%20information%0Aembedding%20%28CIE%29%20framework%2C%20where%20both%20text%20prompts%20and%20semantically%20critical%0Afeatures%20are%20extracted%20for%20transmissions.%20First%2C%20a%20novel%20approach%20of%20semantic%0Afiltering%20is%20proposed%20to%20select%20and%20transmit%20the%20semantically%20critical%20features%0Aof%20images%20relevant%20to%20semantic%20label.%20By%20integrating%20the%20text%20prompt%20and%0Acritical%20features%2C%20the%20receiver%20reconstructs%20high-fidelity%20images%20using%20a%0Adiffusion-based%20generative%20model.%20Next%2C%20we%20propose%20the%20generative%20visual%0Ainformation%20fidelity%20%28GVIF%29%20metric%20to%20evaluate%20the%20visual%20quality%20of%20the%0Agenerated%20image.%20By%20characterizing%20the%20statistical%20models%20of%20image%20features%2C%0Athe%20GVIF%20metric%20quantifies%20the%20mutual%20information%20between%20the%20distorted%0Afeatures%20and%20their%20original%20counterparts.%20By%20maximizing%20the%20GVIF%20metric%2C%20we%0Adesign%20a%20channel-adaptive%20Gen-SemCom%20system%20that%20adaptively%20control%20the%20volume%0Aof%20features%20and%20compression%20rate%20according%20to%20the%20channel%20state.%20Experimental%0Aresults%20validate%20the%20GVIF%20metric%27s%20sensitivity%20to%20visual%20fidelity%2C%20correlating%0Awith%20both%20the%20PSNR%20and%20critical%20information%20volume.%20In%20addition%2C%20the%20optimized%0Asystem%20achieves%20superior%20performance%20over%20benchmarking%20schemes%20in%20terms%20of%0Ahigher%20PSNR%20and%20lower%20FID%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10405v1&entry.124074799=Read"},
{"title": "Large Wireless Localization Model (LWLM): A Foundation Model for\n  Positioning in 6G Networks", "author": "Guangjin Pan and Kaixuan Huang and Hui Chen and Shunqing Zhang and Christian H\u00e4ger and Henk Wymeersch", "abstract": "  Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.\n", "link": "http://arxiv.org/abs/2505.10134v1", "date": "2025-05-15", "relevancy": 2.7402, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Wireless%20Localization%20Model%20%28LWLM%29%3A%20A%20Foundation%20Model%20for%0A%20%20Positioning%20in%206G%20Networks&body=Title%3A%20Large%20Wireless%20Localization%20Model%20%28LWLM%29%3A%20A%20Foundation%20Model%20for%0A%20%20Positioning%20in%206G%20Networks%0AAuthor%3A%20Guangjin%20Pan%20and%20Kaixuan%20Huang%20and%20Hui%20Chen%20and%20Shunqing%20Zhang%20and%20Christian%20H%C3%A4ger%20and%20Henk%20Wymeersch%0AAbstract%3A%20%20%20Accurate%20and%20robust%20localization%20is%20a%20critical%20enabler%20for%20emerging%205G%20and%206G%0Aapplications%2C%20including%20autonomous%20driving%2C%20extended%20reality%20%28XR%29%2C%20and%20smart%0Amanufacturing.%20While%20data-driven%20approaches%20have%20shown%20promise%2C%20most%20existing%0Amodels%20require%20large%20amounts%20of%20labeled%20data%20and%20struggle%20to%20generalize%20across%0Adeployment%20scenarios%20and%20wireless%20configurations.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20foundation-model-based%20solution%20tailored%20for%20wireless%0Alocalization.%20We%20first%20analyze%20how%20different%20self-supervised%20learning%20%28SSL%29%0Atasks%20acquire%20general-purpose%20and%20task-specific%20semantic%20features%20based%20on%0Ainformation%20bottleneck%20%28IB%29%20theory.%20Building%20on%20this%20foundation%2C%20we%20design%20a%0Apretraining%20methodology%20for%20the%20proposed%20Large%20Wireless%20Localization%20Model%0A%28LWLM%29.%20Specifically%2C%20we%20propose%20an%20SSL%20framework%20that%20jointly%20optimizes%20three%0Acomplementary%20objectives%3A%20%28i%29%20spatial-frequency%20masked%20channel%20modeling%0A%28SF-MCM%29%2C%20%28ii%29%20domain-transformation%20invariance%20%28DTI%29%2C%20and%20%28iii%29%0Aposition-invariant%20contrastive%20learning%20%28PICL%29.%20These%20objectives%20jointly%0Acapture%20the%20underlying%20semantics%20of%20wireless%20channel%20from%20multiple%0Aperspectives.%20We%20further%20design%20lightweight%20decoders%20for%20key%20downstream%20tasks%2C%0Aincluding%20time-of-arrival%20%28ToA%29%20estimation%2C%20angle-of-arrival%20%28AoA%29%20estimation%2C%0Asingle%20base%20station%20%28BS%29%20localization%2C%20and%20multiple%20BS%20localization.%0AComprehensive%20experimental%20results%20confirm%20that%20LWLM%20consistently%20surpasses%0Aboth%20model-based%20and%20supervised%20learning%20baselines%20across%20all%20localization%0Atasks.%20In%20particular%2C%20LWLM%20achieves%2026.0%25--87.5%25%20improvement%20over%20transformer%0Amodels%20without%20pretraining%2C%20and%20exhibits%20strong%20generalization%20under%0Alabel-limited%20fine-tuning%20and%20unseen%20BS%20configurations%2C%20confirming%20its%0Apotential%20as%20a%20foundation%20model%20for%20wireless%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Wireless%2520Localization%2520Model%2520%2528LWLM%2529%253A%2520A%2520Foundation%2520Model%2520for%250A%2520%2520Positioning%2520in%25206G%2520Networks%26entry.906535625%3DGuangjin%2520Pan%2520and%2520Kaixuan%2520Huang%2520and%2520Hui%2520Chen%2520and%2520Shunqing%2520Zhang%2520and%2520Christian%2520H%25C3%25A4ger%2520and%2520Henk%2520Wymeersch%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520localization%2520is%2520a%2520critical%2520enabler%2520for%2520emerging%25205G%2520and%25206G%250Aapplications%252C%2520including%2520autonomous%2520driving%252C%2520extended%2520reality%2520%2528XR%2529%252C%2520and%2520smart%250Amanufacturing.%2520While%2520data-driven%2520approaches%2520have%2520shown%2520promise%252C%2520most%2520existing%250Amodels%2520require%2520large%2520amounts%2520of%2520labeled%2520data%2520and%2520struggle%2520to%2520generalize%2520across%250Adeployment%2520scenarios%2520and%2520wireless%2520configurations.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520a%2520foundation-model-based%2520solution%2520tailored%2520for%2520wireless%250Alocalization.%2520We%2520first%2520analyze%2520how%2520different%2520self-supervised%2520learning%2520%2528SSL%2529%250Atasks%2520acquire%2520general-purpose%2520and%2520task-specific%2520semantic%2520features%2520based%2520on%250Ainformation%2520bottleneck%2520%2528IB%2529%2520theory.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520design%2520a%250Apretraining%2520methodology%2520for%2520the%2520proposed%2520Large%2520Wireless%2520Localization%2520Model%250A%2528LWLM%2529.%2520Specifically%252C%2520we%2520propose%2520an%2520SSL%2520framework%2520that%2520jointly%2520optimizes%2520three%250Acomplementary%2520objectives%253A%2520%2528i%2529%2520spatial-frequency%2520masked%2520channel%2520modeling%250A%2528SF-MCM%2529%252C%2520%2528ii%2529%2520domain-transformation%2520invariance%2520%2528DTI%2529%252C%2520and%2520%2528iii%2529%250Aposition-invariant%2520contrastive%2520learning%2520%2528PICL%2529.%2520These%2520objectives%2520jointly%250Acapture%2520the%2520underlying%2520semantics%2520of%2520wireless%2520channel%2520from%2520multiple%250Aperspectives.%2520We%2520further%2520design%2520lightweight%2520decoders%2520for%2520key%2520downstream%2520tasks%252C%250Aincluding%2520time-of-arrival%2520%2528ToA%2529%2520estimation%252C%2520angle-of-arrival%2520%2528AoA%2529%2520estimation%252C%250Asingle%2520base%2520station%2520%2528BS%2529%2520localization%252C%2520and%2520multiple%2520BS%2520localization.%250AComprehensive%2520experimental%2520results%2520confirm%2520that%2520LWLM%2520consistently%2520surpasses%250Aboth%2520model-based%2520and%2520supervised%2520learning%2520baselines%2520across%2520all%2520localization%250Atasks.%2520In%2520particular%252C%2520LWLM%2520achieves%252026.0%2525--87.5%2525%2520improvement%2520over%2520transformer%250Amodels%2520without%2520pretraining%252C%2520and%2520exhibits%2520strong%2520generalization%2520under%250Alabel-limited%2520fine-tuning%2520and%2520unseen%2520BS%2520configurations%252C%2520confirming%2520its%250Apotential%2520as%2520a%2520foundation%2520model%2520for%2520wireless%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Wireless%20Localization%20Model%20%28LWLM%29%3A%20A%20Foundation%20Model%20for%0A%20%20Positioning%20in%206G%20Networks&entry.906535625=Guangjin%20Pan%20and%20Kaixuan%20Huang%20and%20Hui%20Chen%20and%20Shunqing%20Zhang%20and%20Christian%20H%C3%A4ger%20and%20Henk%20Wymeersch&entry.1292438233=%20%20Accurate%20and%20robust%20localization%20is%20a%20critical%20enabler%20for%20emerging%205G%20and%206G%0Aapplications%2C%20including%20autonomous%20driving%2C%20extended%20reality%20%28XR%29%2C%20and%20smart%0Amanufacturing.%20While%20data-driven%20approaches%20have%20shown%20promise%2C%20most%20existing%0Amodels%20require%20large%20amounts%20of%20labeled%20data%20and%20struggle%20to%20generalize%20across%0Adeployment%20scenarios%20and%20wireless%20configurations.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20foundation-model-based%20solution%20tailored%20for%20wireless%0Alocalization.%20We%20first%20analyze%20how%20different%20self-supervised%20learning%20%28SSL%29%0Atasks%20acquire%20general-purpose%20and%20task-specific%20semantic%20features%20based%20on%0Ainformation%20bottleneck%20%28IB%29%20theory.%20Building%20on%20this%20foundation%2C%20we%20design%20a%0Apretraining%20methodology%20for%20the%20proposed%20Large%20Wireless%20Localization%20Model%0A%28LWLM%29.%20Specifically%2C%20we%20propose%20an%20SSL%20framework%20that%20jointly%20optimizes%20three%0Acomplementary%20objectives%3A%20%28i%29%20spatial-frequency%20masked%20channel%20modeling%0A%28SF-MCM%29%2C%20%28ii%29%20domain-transformation%20invariance%20%28DTI%29%2C%20and%20%28iii%29%0Aposition-invariant%20contrastive%20learning%20%28PICL%29.%20These%20objectives%20jointly%0Acapture%20the%20underlying%20semantics%20of%20wireless%20channel%20from%20multiple%0Aperspectives.%20We%20further%20design%20lightweight%20decoders%20for%20key%20downstream%20tasks%2C%0Aincluding%20time-of-arrival%20%28ToA%29%20estimation%2C%20angle-of-arrival%20%28AoA%29%20estimation%2C%0Asingle%20base%20station%20%28BS%29%20localization%2C%20and%20multiple%20BS%20localization.%0AComprehensive%20experimental%20results%20confirm%20that%20LWLM%20consistently%20surpasses%0Aboth%20model-based%20and%20supervised%20learning%20baselines%20across%20all%20localization%0Atasks.%20In%20particular%2C%20LWLM%20achieves%2026.0%25--87.5%25%20improvement%20over%20transformer%0Amodels%20without%20pretraining%2C%20and%20exhibits%20strong%20generalization%20under%0Alabel-limited%20fine-tuning%20and%20unseen%20BS%20configurations%2C%20confirming%20its%0Apotential%20as%20a%20foundation%20model%20for%20wireless%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10134v1&entry.124074799=Read"},
{"title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "author": "Ilya Ovodov and Petr Surovtsev and Karina Kvanchiani and Alexander Kapitanov and Alexander Nagaev", "abstract": "  This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.\n", "link": "http://arxiv.org/abs/2505.10481v1", "date": "2025-05-15", "relevancy": 2.7376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition&body=Title%3A%20Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition%0AAuthor%3A%20Ilya%20Ovodov%20and%20Petr%20Surovtsev%20and%20Karina%20Kvanchiani%20and%20Alexander%20Kapitanov%20and%20Alexander%20Nagaev%0AAbstract%3A%20%20%20This%20paper%20examines%20two%20aspects%20of%20the%20isolated%20sign%20language%20recognition%0A%28ISLR%29%20task.%20First%2C%20despite%20the%20availability%20of%20a%20number%20of%20datasets%2C%20the%0Aamount%20of%20data%20for%20most%20individual%20sign%20languages%20is%20limited.%20It%20poses%20the%0Achallenge%20of%20cross-language%20ISLR%20model%20training%2C%20including%20transfer%20learning.%0ASecond%2C%20similar%20signs%20can%20have%20different%20semantic%20meanings.%20It%20leads%20to%0Aambiguity%20in%20dataset%20labeling%20and%20raises%20the%20question%20of%20the%20best%20policy%20for%0Aannotating%20such%20signs.%20To%20address%20these%20issues%2C%20this%20study%20presents%20Logos%2C%20a%0Anovel%20Russian%20Sign%20Language%20%28RSL%29%20dataset%2C%20the%20most%20extensive%20ISLR%20dataset%20by%0Athe%20number%20of%20signers%20and%20one%20of%20the%20largest%20available%20datasets%20while%20also%20the%0Alargest%20RSL%20dataset%20in%20size%20and%20vocabulary.%20It%20is%20shown%20that%20a%20model%2C%0Apre-trained%20on%20the%20Logos%20dataset%20can%20be%20used%20as%20a%20universal%20encoder%20for%20other%0Alanguage%20SLR%20tasks%2C%20including%20few-shot%20learning.%20We%20explore%20cross-language%0Atransfer%20learning%20approaches%20and%20find%20that%20joint%20training%20using%20multiple%0Aclassification%20heads%20benefits%20accuracy%20for%20the%20target%20lowresource%20datasets%20the%0Amost.%20The%20key%20feature%20of%20the%20Logos%20dataset%20is%20explicitly%20annotated%20visually%0Asimilar%20sign%20groups.%20We%20show%20that%20explicitly%20labeling%20visually%20similar%20signs%0Aimproves%20trained%20model%20quality%20as%20a%20visual%20encoder%20for%20downstream%20tasks.%20Based%0Aon%20the%20proposed%20contributions%2C%20we%20outperform%20current%20state-of-the-art%20results%0Afor%20the%20WLASL%20dataset%20and%20get%20competitive%20results%20for%20the%20AUTSL%20dataset%2C%20with%20a%0Asingle%20stream%20model%20processing%20solely%20RGB%20video.%20The%20source%20code%2C%20dataset%2C%20and%0Apre-trained%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogos%2520as%2520a%2520Well-Tempered%2520Pre-train%2520for%2520Sign%2520Language%2520Recognition%26entry.906535625%3DIlya%2520Ovodov%2520and%2520Petr%2520Surovtsev%2520and%2520Karina%2520Kvanchiani%2520and%2520Alexander%2520Kapitanov%2520and%2520Alexander%2520Nagaev%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520two%2520aspects%2520of%2520the%2520isolated%2520sign%2520language%2520recognition%250A%2528ISLR%2529%2520task.%2520First%252C%2520despite%2520the%2520availability%2520of%2520a%2520number%2520of%2520datasets%252C%2520the%250Aamount%2520of%2520data%2520for%2520most%2520individual%2520sign%2520languages%2520is%2520limited.%2520It%2520poses%2520the%250Achallenge%2520of%2520cross-language%2520ISLR%2520model%2520training%252C%2520including%2520transfer%2520learning.%250ASecond%252C%2520similar%2520signs%2520can%2520have%2520different%2520semantic%2520meanings.%2520It%2520leads%2520to%250Aambiguity%2520in%2520dataset%2520labeling%2520and%2520raises%2520the%2520question%2520of%2520the%2520best%2520policy%2520for%250Aannotating%2520such%2520signs.%2520To%2520address%2520these%2520issues%252C%2520this%2520study%2520presents%2520Logos%252C%2520a%250Anovel%2520Russian%2520Sign%2520Language%2520%2528RSL%2529%2520dataset%252C%2520the%2520most%2520extensive%2520ISLR%2520dataset%2520by%250Athe%2520number%2520of%2520signers%2520and%2520one%2520of%2520the%2520largest%2520available%2520datasets%2520while%2520also%2520the%250Alargest%2520RSL%2520dataset%2520in%2520size%2520and%2520vocabulary.%2520It%2520is%2520shown%2520that%2520a%2520model%252C%250Apre-trained%2520on%2520the%2520Logos%2520dataset%2520can%2520be%2520used%2520as%2520a%2520universal%2520encoder%2520for%2520other%250Alanguage%2520SLR%2520tasks%252C%2520including%2520few-shot%2520learning.%2520We%2520explore%2520cross-language%250Atransfer%2520learning%2520approaches%2520and%2520find%2520that%2520joint%2520training%2520using%2520multiple%250Aclassification%2520heads%2520benefits%2520accuracy%2520for%2520the%2520target%2520lowresource%2520datasets%2520the%250Amost.%2520The%2520key%2520feature%2520of%2520the%2520Logos%2520dataset%2520is%2520explicitly%2520annotated%2520visually%250Asimilar%2520sign%2520groups.%2520We%2520show%2520that%2520explicitly%2520labeling%2520visually%2520similar%2520signs%250Aimproves%2520trained%2520model%2520quality%2520as%2520a%2520visual%2520encoder%2520for%2520downstream%2520tasks.%2520Based%250Aon%2520the%2520proposed%2520contributions%252C%2520we%2520outperform%2520current%2520state-of-the-art%2520results%250Afor%2520the%2520WLASL%2520dataset%2520and%2520get%2520competitive%2520results%2520for%2520the%2520AUTSL%2520dataset%252C%2520with%2520a%250Asingle%2520stream%2520model%2520processing%2520solely%2520RGB%2520video.%2520The%2520source%2520code%252C%2520dataset%252C%2520and%250Apre-trained%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logos%20as%20a%20Well-Tempered%20Pre-train%20for%20Sign%20Language%20Recognition&entry.906535625=Ilya%20Ovodov%20and%20Petr%20Surovtsev%20and%20Karina%20Kvanchiani%20and%20Alexander%20Kapitanov%20and%20Alexander%20Nagaev&entry.1292438233=%20%20This%20paper%20examines%20two%20aspects%20of%20the%20isolated%20sign%20language%20recognition%0A%28ISLR%29%20task.%20First%2C%20despite%20the%20availability%20of%20a%20number%20of%20datasets%2C%20the%0Aamount%20of%20data%20for%20most%20individual%20sign%20languages%20is%20limited.%20It%20poses%20the%0Achallenge%20of%20cross-language%20ISLR%20model%20training%2C%20including%20transfer%20learning.%0ASecond%2C%20similar%20signs%20can%20have%20different%20semantic%20meanings.%20It%20leads%20to%0Aambiguity%20in%20dataset%20labeling%20and%20raises%20the%20question%20of%20the%20best%20policy%20for%0Aannotating%20such%20signs.%20To%20address%20these%20issues%2C%20this%20study%20presents%20Logos%2C%20a%0Anovel%20Russian%20Sign%20Language%20%28RSL%29%20dataset%2C%20the%20most%20extensive%20ISLR%20dataset%20by%0Athe%20number%20of%20signers%20and%20one%20of%20the%20largest%20available%20datasets%20while%20also%20the%0Alargest%20RSL%20dataset%20in%20size%20and%20vocabulary.%20It%20is%20shown%20that%20a%20model%2C%0Apre-trained%20on%20the%20Logos%20dataset%20can%20be%20used%20as%20a%20universal%20encoder%20for%20other%0Alanguage%20SLR%20tasks%2C%20including%20few-shot%20learning.%20We%20explore%20cross-language%0Atransfer%20learning%20approaches%20and%20find%20that%20joint%20training%20using%20multiple%0Aclassification%20heads%20benefits%20accuracy%20for%20the%20target%20lowresource%20datasets%20the%0Amost.%20The%20key%20feature%20of%20the%20Logos%20dataset%20is%20explicitly%20annotated%20visually%0Asimilar%20sign%20groups.%20We%20show%20that%20explicitly%20labeling%20visually%20similar%20signs%0Aimproves%20trained%20model%20quality%20as%20a%20visual%20encoder%20for%20downstream%20tasks.%20Based%0Aon%20the%20proposed%20contributions%2C%20we%20outperform%20current%20state-of-the-art%20results%0Afor%20the%20WLASL%20dataset%20and%20get%20competitive%20results%20for%20the%20AUTSL%20dataset%2C%20with%20a%0Asingle%20stream%20model%20processing%20solely%20RGB%20video.%20The%20source%20code%2C%20dataset%2C%20and%0Apre-trained%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10481v1&entry.124074799=Read"},
{"title": "MapExplorer: New Content Generation from Low-Dimensional Visualizations", "author": "Xingjian Zhang and Ziyang Xiong and Shixuan Liu and Yutong Xie and Tolga Ergen and Dongsub Shim and Hua Xu and Honglak Lee and Qiaozhu Me", "abstract": "  Low-dimensional visualizations, or \"projection maps,\" are widely used in\nscientific and creative domains to interpret large-scale and complex datasets.\nThese visualizations not only aid in understanding existing knowledge spaces\nbut also implicitly guide exploration into unknown areas. Although techniques\nsuch as t-SNE and UMAP can generate these maps, there exists no systematic\nmethod for leveraging them to generate new content. To address this, we\nintroduce MapExplorer, a novel knowledge discovery task that translates\ncoordinates within any projection map into coherent, contextually aligned\ntextual content. This allows users to interactively explore and uncover\ninsights embedded in the maps. To evaluate the performance of MapExplorer\nmethods, we propose Atometric, a fine-grained metric inspired by ROUGE that\nquantifies logical coherence and alignment between generated and reference\ntext. Experiments on diverse datasets demonstrate the versatility of\nMapExplorer in generating scientific hypotheses, crafting synthetic personas,\nand devising strategies for attacking large language models-even with simple\nbaseline methods. By bridging visualization and generation, our work highlights\nthe potential of MapExplorer to enable intuitive human-AI collaboration in\nlarge-scale data exploration.\n", "link": "http://arxiv.org/abs/2412.18673v2", "date": "2025-05-15", "relevancy": 2.7329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapExplorer%3A%20New%20Content%20Generation%20from%20Low-Dimensional%20Visualizations&body=Title%3A%20MapExplorer%3A%20New%20Content%20Generation%20from%20Low-Dimensional%20Visualizations%0AAuthor%3A%20Xingjian%20Zhang%20and%20Ziyang%20Xiong%20and%20Shixuan%20Liu%20and%20Yutong%20Xie%20and%20Tolga%20Ergen%20and%20Dongsub%20Shim%20and%20Hua%20Xu%20and%20Honglak%20Lee%20and%20Qiaozhu%20Me%0AAbstract%3A%20%20%20Low-dimensional%20visualizations%2C%20or%20%22projection%20maps%2C%22%20are%20widely%20used%20in%0Ascientific%20and%20creative%20domains%20to%20interpret%20large-scale%20and%20complex%20datasets.%0AThese%20visualizations%20not%20only%20aid%20in%20understanding%20existing%20knowledge%20spaces%0Abut%20also%20implicitly%20guide%20exploration%20into%20unknown%20areas.%20Although%20techniques%0Asuch%20as%20t-SNE%20and%20UMAP%20can%20generate%20these%20maps%2C%20there%20exists%20no%20systematic%0Amethod%20for%20leveraging%20them%20to%20generate%20new%20content.%20To%20address%20this%2C%20we%0Aintroduce%20MapExplorer%2C%20a%20novel%20knowledge%20discovery%20task%20that%20translates%0Acoordinates%20within%20any%20projection%20map%20into%20coherent%2C%20contextually%20aligned%0Atextual%20content.%20This%20allows%20users%20to%20interactively%20explore%20and%20uncover%0Ainsights%20embedded%20in%20the%20maps.%20To%20evaluate%20the%20performance%20of%20MapExplorer%0Amethods%2C%20we%20propose%20Atometric%2C%20a%20fine-grained%20metric%20inspired%20by%20ROUGE%20that%0Aquantifies%20logical%20coherence%20and%20alignment%20between%20generated%20and%20reference%0Atext.%20Experiments%20on%20diverse%20datasets%20demonstrate%20the%20versatility%20of%0AMapExplorer%20in%20generating%20scientific%20hypotheses%2C%20crafting%20synthetic%20personas%2C%0Aand%20devising%20strategies%20for%20attacking%20large%20language%20models-even%20with%20simple%0Abaseline%20methods.%20By%20bridging%20visualization%20and%20generation%2C%20our%20work%20highlights%0Athe%20potential%20of%20MapExplorer%20to%20enable%20intuitive%20human-AI%20collaboration%20in%0Alarge-scale%20data%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapExplorer%253A%2520New%2520Content%2520Generation%2520from%2520Low-Dimensional%2520Visualizations%26entry.906535625%3DXingjian%2520Zhang%2520and%2520Ziyang%2520Xiong%2520and%2520Shixuan%2520Liu%2520and%2520Yutong%2520Xie%2520and%2520Tolga%2520Ergen%2520and%2520Dongsub%2520Shim%2520and%2520Hua%2520Xu%2520and%2520Honglak%2520Lee%2520and%2520Qiaozhu%2520Me%26entry.1292438233%3D%2520%2520Low-dimensional%2520visualizations%252C%2520or%2520%2522projection%2520maps%252C%2522%2520are%2520widely%2520used%2520in%250Ascientific%2520and%2520creative%2520domains%2520to%2520interpret%2520large-scale%2520and%2520complex%2520datasets.%250AThese%2520visualizations%2520not%2520only%2520aid%2520in%2520understanding%2520existing%2520knowledge%2520spaces%250Abut%2520also%2520implicitly%2520guide%2520exploration%2520into%2520unknown%2520areas.%2520Although%2520techniques%250Asuch%2520as%2520t-SNE%2520and%2520UMAP%2520can%2520generate%2520these%2520maps%252C%2520there%2520exists%2520no%2520systematic%250Amethod%2520for%2520leveraging%2520them%2520to%2520generate%2520new%2520content.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520MapExplorer%252C%2520a%2520novel%2520knowledge%2520discovery%2520task%2520that%2520translates%250Acoordinates%2520within%2520any%2520projection%2520map%2520into%2520coherent%252C%2520contextually%2520aligned%250Atextual%2520content.%2520This%2520allows%2520users%2520to%2520interactively%2520explore%2520and%2520uncover%250Ainsights%2520embedded%2520in%2520the%2520maps.%2520To%2520evaluate%2520the%2520performance%2520of%2520MapExplorer%250Amethods%252C%2520we%2520propose%2520Atometric%252C%2520a%2520fine-grained%2520metric%2520inspired%2520by%2520ROUGE%2520that%250Aquantifies%2520logical%2520coherence%2520and%2520alignment%2520between%2520generated%2520and%2520reference%250Atext.%2520Experiments%2520on%2520diverse%2520datasets%2520demonstrate%2520the%2520versatility%2520of%250AMapExplorer%2520in%2520generating%2520scientific%2520hypotheses%252C%2520crafting%2520synthetic%2520personas%252C%250Aand%2520devising%2520strategies%2520for%2520attacking%2520large%2520language%2520models-even%2520with%2520simple%250Abaseline%2520methods.%2520By%2520bridging%2520visualization%2520and%2520generation%252C%2520our%2520work%2520highlights%250Athe%2520potential%2520of%2520MapExplorer%2520to%2520enable%2520intuitive%2520human-AI%2520collaboration%2520in%250Alarge-scale%2520data%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapExplorer%3A%20New%20Content%20Generation%20from%20Low-Dimensional%20Visualizations&entry.906535625=Xingjian%20Zhang%20and%20Ziyang%20Xiong%20and%20Shixuan%20Liu%20and%20Yutong%20Xie%20and%20Tolga%20Ergen%20and%20Dongsub%20Shim%20and%20Hua%20Xu%20and%20Honglak%20Lee%20and%20Qiaozhu%20Me&entry.1292438233=%20%20Low-dimensional%20visualizations%2C%20or%20%22projection%20maps%2C%22%20are%20widely%20used%20in%0Ascientific%20and%20creative%20domains%20to%20interpret%20large-scale%20and%20complex%20datasets.%0AThese%20visualizations%20not%20only%20aid%20in%20understanding%20existing%20knowledge%20spaces%0Abut%20also%20implicitly%20guide%20exploration%20into%20unknown%20areas.%20Although%20techniques%0Asuch%20as%20t-SNE%20and%20UMAP%20can%20generate%20these%20maps%2C%20there%20exists%20no%20systematic%0Amethod%20for%20leveraging%20them%20to%20generate%20new%20content.%20To%20address%20this%2C%20we%0Aintroduce%20MapExplorer%2C%20a%20novel%20knowledge%20discovery%20task%20that%20translates%0Acoordinates%20within%20any%20projection%20map%20into%20coherent%2C%20contextually%20aligned%0Atextual%20content.%20This%20allows%20users%20to%20interactively%20explore%20and%20uncover%0Ainsights%20embedded%20in%20the%20maps.%20To%20evaluate%20the%20performance%20of%20MapExplorer%0Amethods%2C%20we%20propose%20Atometric%2C%20a%20fine-grained%20metric%20inspired%20by%20ROUGE%20that%0Aquantifies%20logical%20coherence%20and%20alignment%20between%20generated%20and%20reference%0Atext.%20Experiments%20on%20diverse%20datasets%20demonstrate%20the%20versatility%20of%0AMapExplorer%20in%20generating%20scientific%20hypotheses%2C%20crafting%20synthetic%20personas%2C%0Aand%20devising%20strategies%20for%20attacking%20large%20language%20models-even%20with%20simple%0Abaseline%20methods.%20By%20bridging%20visualization%20and%20generation%2C%20our%20work%20highlights%0Athe%20potential%20of%20MapExplorer%20to%20enable%20intuitive%20human-AI%20collaboration%20in%0Alarge-scale%20data%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18673v2&entry.124074799=Read"},
{"title": "Self-supervised Learning for Acoustic Few-Shot Classification", "author": "Jingyong Liang and Bernd Meyer and Isaac Ning Lee and Thanh-Toan Do", "abstract": "  Labelled data are limited and self-supervised learning is one of the most\nimportant approaches for reducing labelling requirements. While it has been\nextensively explored in the image domain, it has so far not received the same\namount of attention in the acoustic domain. Yet, reducing labelling is a key\nrequirement for many acoustic applications. Specifically in bioacoustic, there\nare rarely sufficient labels for fully supervised learning available. This has\nled to the widespread use of acoustic recognisers that have been pre-trained on\nunrelated data for bioacoustic tasks. We posit that training on the actual task\ndata and combining self-supervised pre-training with few-shot classification is\na superior approach that has the ability to deliver high accuracy even when\nonly a few labels are available. To this end, we introduce and evaluate a new\narchitecture that combines CNN-based preprocessing with feature extraction\nbased on state space models (SSMs). This combination is motivated by the fact\nthat CNN-based networks alone struggle to capture temporal information\neffectively, which is crucial for classifying acoustic signals. SSMs,\nspecifically S4 and Mamba, on the other hand, have been shown to have an\nexcellent ability to capture long-range dependencies in sequence data. We\npre-train this architecture using contrastive learning on the actual task data\nand subsequent fine-tuning with an extremely small amount of labelled data. We\nevaluate the performance of this proposed architecture for ($n$-shot,\n$n$-class) classification on standard benchmarks as well as real-world data.\nOur evaluation shows that it outperforms state-of-the-art architectures on the\nfew-shot classification problem.\n", "link": "http://arxiv.org/abs/2409.09647v2", "date": "2025-05-15", "relevancy": 2.7241, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5591}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5468}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20for%20Acoustic%20Few-Shot%20Classification&body=Title%3A%20Self-supervised%20Learning%20for%20Acoustic%20Few-Shot%20Classification%0AAuthor%3A%20Jingyong%20Liang%20and%20Bernd%20Meyer%20and%20Isaac%20Ning%20Lee%20and%20Thanh-Toan%20Do%0AAbstract%3A%20%20%20Labelled%20data%20are%20limited%20and%20self-supervised%20learning%20is%20one%20of%20the%20most%0Aimportant%20approaches%20for%20reducing%20labelling%20requirements.%20While%20it%20has%20been%0Aextensively%20explored%20in%20the%20image%20domain%2C%20it%20has%20so%20far%20not%20received%20the%20same%0Aamount%20of%20attention%20in%20the%20acoustic%20domain.%20Yet%2C%20reducing%20labelling%20is%20a%20key%0Arequirement%20for%20many%20acoustic%20applications.%20Specifically%20in%20bioacoustic%2C%20there%0Aare%20rarely%20sufficient%20labels%20for%20fully%20supervised%20learning%20available.%20This%20has%0Aled%20to%20the%20widespread%20use%20of%20acoustic%20recognisers%20that%20have%20been%20pre-trained%20on%0Aunrelated%20data%20for%20bioacoustic%20tasks.%20We%20posit%20that%20training%20on%20the%20actual%20task%0Adata%20and%20combining%20self-supervised%20pre-training%20with%20few-shot%20classification%20is%0Aa%20superior%20approach%20that%20has%20the%20ability%20to%20deliver%20high%20accuracy%20even%20when%0Aonly%20a%20few%20labels%20are%20available.%20To%20this%20end%2C%20we%20introduce%20and%20evaluate%20a%20new%0Aarchitecture%20that%20combines%20CNN-based%20preprocessing%20with%20feature%20extraction%0Abased%20on%20state%20space%20models%20%28SSMs%29.%20This%20combination%20is%20motivated%20by%20the%20fact%0Athat%20CNN-based%20networks%20alone%20struggle%20to%20capture%20temporal%20information%0Aeffectively%2C%20which%20is%20crucial%20for%20classifying%20acoustic%20signals.%20SSMs%2C%0Aspecifically%20S4%20and%20Mamba%2C%20on%20the%20other%20hand%2C%20have%20been%20shown%20to%20have%20an%0Aexcellent%20ability%20to%20capture%20long-range%20dependencies%20in%20sequence%20data.%20We%0Apre-train%20this%20architecture%20using%20contrastive%20learning%20on%20the%20actual%20task%20data%0Aand%20subsequent%20fine-tuning%20with%20an%20extremely%20small%20amount%20of%20labelled%20data.%20We%0Aevaluate%20the%20performance%20of%20this%20proposed%20architecture%20for%20%28%24n%24-shot%2C%0A%24n%24-class%29%20classification%20on%20standard%20benchmarks%20as%20well%20as%20real-world%20data.%0AOur%20evaluation%20shows%20that%20it%20outperforms%20state-of-the-art%20architectures%20on%20the%0Afew-shot%20classification%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520for%2520Acoustic%2520Few-Shot%2520Classification%26entry.906535625%3DJingyong%2520Liang%2520and%2520Bernd%2520Meyer%2520and%2520Isaac%2520Ning%2520Lee%2520and%2520Thanh-Toan%2520Do%26entry.1292438233%3D%2520%2520Labelled%2520data%2520are%2520limited%2520and%2520self-supervised%2520learning%2520is%2520one%2520of%2520the%2520most%250Aimportant%2520approaches%2520for%2520reducing%2520labelling%2520requirements.%2520While%2520it%2520has%2520been%250Aextensively%2520explored%2520in%2520the%2520image%2520domain%252C%2520it%2520has%2520so%2520far%2520not%2520received%2520the%2520same%250Aamount%2520of%2520attention%2520in%2520the%2520acoustic%2520domain.%2520Yet%252C%2520reducing%2520labelling%2520is%2520a%2520key%250Arequirement%2520for%2520many%2520acoustic%2520applications.%2520Specifically%2520in%2520bioacoustic%252C%2520there%250Aare%2520rarely%2520sufficient%2520labels%2520for%2520fully%2520supervised%2520learning%2520available.%2520This%2520has%250Aled%2520to%2520the%2520widespread%2520use%2520of%2520acoustic%2520recognisers%2520that%2520have%2520been%2520pre-trained%2520on%250Aunrelated%2520data%2520for%2520bioacoustic%2520tasks.%2520We%2520posit%2520that%2520training%2520on%2520the%2520actual%2520task%250Adata%2520and%2520combining%2520self-supervised%2520pre-training%2520with%2520few-shot%2520classification%2520is%250Aa%2520superior%2520approach%2520that%2520has%2520the%2520ability%2520to%2520deliver%2520high%2520accuracy%2520even%2520when%250Aonly%2520a%2520few%2520labels%2520are%2520available.%2520To%2520this%2520end%252C%2520we%2520introduce%2520and%2520evaluate%2520a%2520new%250Aarchitecture%2520that%2520combines%2520CNN-based%2520preprocessing%2520with%2520feature%2520extraction%250Abased%2520on%2520state%2520space%2520models%2520%2528SSMs%2529.%2520This%2520combination%2520is%2520motivated%2520by%2520the%2520fact%250Athat%2520CNN-based%2520networks%2520alone%2520struggle%2520to%2520capture%2520temporal%2520information%250Aeffectively%252C%2520which%2520is%2520crucial%2520for%2520classifying%2520acoustic%2520signals.%2520SSMs%252C%250Aspecifically%2520S4%2520and%2520Mamba%252C%2520on%2520the%2520other%2520hand%252C%2520have%2520been%2520shown%2520to%2520have%2520an%250Aexcellent%2520ability%2520to%2520capture%2520long-range%2520dependencies%2520in%2520sequence%2520data.%2520We%250Apre-train%2520this%2520architecture%2520using%2520contrastive%2520learning%2520on%2520the%2520actual%2520task%2520data%250Aand%2520subsequent%2520fine-tuning%2520with%2520an%2520extremely%2520small%2520amount%2520of%2520labelled%2520data.%2520We%250Aevaluate%2520the%2520performance%2520of%2520this%2520proposed%2520architecture%2520for%2520%2528%2524n%2524-shot%252C%250A%2524n%2524-class%2529%2520classification%2520on%2520standard%2520benchmarks%2520as%2520well%2520as%2520real-world%2520data.%250AOur%2520evaluation%2520shows%2520that%2520it%2520outperforms%2520state-of-the-art%2520architectures%2520on%2520the%250Afew-shot%2520classification%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20for%20Acoustic%20Few-Shot%20Classification&entry.906535625=Jingyong%20Liang%20and%20Bernd%20Meyer%20and%20Isaac%20Ning%20Lee%20and%20Thanh-Toan%20Do&entry.1292438233=%20%20Labelled%20data%20are%20limited%20and%20self-supervised%20learning%20is%20one%20of%20the%20most%0Aimportant%20approaches%20for%20reducing%20labelling%20requirements.%20While%20it%20has%20been%0Aextensively%20explored%20in%20the%20image%20domain%2C%20it%20has%20so%20far%20not%20received%20the%20same%0Aamount%20of%20attention%20in%20the%20acoustic%20domain.%20Yet%2C%20reducing%20labelling%20is%20a%20key%0Arequirement%20for%20many%20acoustic%20applications.%20Specifically%20in%20bioacoustic%2C%20there%0Aare%20rarely%20sufficient%20labels%20for%20fully%20supervised%20learning%20available.%20This%20has%0Aled%20to%20the%20widespread%20use%20of%20acoustic%20recognisers%20that%20have%20been%20pre-trained%20on%0Aunrelated%20data%20for%20bioacoustic%20tasks.%20We%20posit%20that%20training%20on%20the%20actual%20task%0Adata%20and%20combining%20self-supervised%20pre-training%20with%20few-shot%20classification%20is%0Aa%20superior%20approach%20that%20has%20the%20ability%20to%20deliver%20high%20accuracy%20even%20when%0Aonly%20a%20few%20labels%20are%20available.%20To%20this%20end%2C%20we%20introduce%20and%20evaluate%20a%20new%0Aarchitecture%20that%20combines%20CNN-based%20preprocessing%20with%20feature%20extraction%0Abased%20on%20state%20space%20models%20%28SSMs%29.%20This%20combination%20is%20motivated%20by%20the%20fact%0Athat%20CNN-based%20networks%20alone%20struggle%20to%20capture%20temporal%20information%0Aeffectively%2C%20which%20is%20crucial%20for%20classifying%20acoustic%20signals.%20SSMs%2C%0Aspecifically%20S4%20and%20Mamba%2C%20on%20the%20other%20hand%2C%20have%20been%20shown%20to%20have%20an%0Aexcellent%20ability%20to%20capture%20long-range%20dependencies%20in%20sequence%20data.%20We%0Apre-train%20this%20architecture%20using%20contrastive%20learning%20on%20the%20actual%20task%20data%0Aand%20subsequent%20fine-tuning%20with%20an%20extremely%20small%20amount%20of%20labelled%20data.%20We%0Aevaluate%20the%20performance%20of%20this%20proposed%20architecture%20for%20%28%24n%24-shot%2C%0A%24n%24-class%29%20classification%20on%20standard%20benchmarks%20as%20well%20as%20real-world%20data.%0AOur%20evaluation%20shows%20that%20it%20outperforms%20state-of-the-art%20architectures%20on%20the%0Afew-shot%20classification%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09647v2&entry.124074799=Read"},
{"title": "Latent Action Pretraining from Videos", "author": "Seonghyeon Ye and Joel Jang and Byeongguk Jeon and Sejune Joo and Jianwei Yang and Baolin Peng and Ajay Mandlekar and Reuben Tan and Yu-Wei Chao and Bill Yuchen Lin and Lars Liden and Kimin Lee and Jianfeng Gao and Luke Zettlemoyer and Dieter Fox and Minjoon Seo", "abstract": "  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n", "link": "http://arxiv.org/abs/2410.11758v2", "date": "2025-05-15", "relevancy": 2.6868, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5691}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Action%20Pretraining%20from%20Videos&body=Title%3A%20Latent%20Action%20Pretraining%20from%20Videos%0AAuthor%3A%20Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Byeongguk%20Jeon%20and%20Sejune%20Joo%20and%20Jianwei%20Yang%20and%20Baolin%20Peng%20and%20Ajay%20Mandlekar%20and%20Reuben%20Tan%20and%20Yu-Wei%20Chao%20and%20Bill%20Yuchen%20Lin%20and%20Lars%20Liden%20and%20Kimin%20Lee%20and%20Jianfeng%20Gao%20and%20Luke%20Zettlemoyer%20and%20Dieter%20Fox%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20We%20introduce%20Latent%20Action%20Pretraining%20for%20general%20Action%20models%20%28LAPA%29%2C%20an%0Aunsupervised%20method%20for%20pretraining%20Vision-Language-Action%20%28VLA%29%20models%20without%0Aground-truth%20robot%20action%20labels.%20Existing%20Vision-Language-Action%20models%0Arequire%20action%20labels%20typically%20collected%20by%20human%20teleoperators%20during%0Apretraining%2C%20which%20significantly%20limits%20possible%20data%20sources%20and%20scale.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20learn%20from%20internet-scale%20videos%20that%20do%20not%0Ahave%20robot%20action%20labels.%20We%20first%20train%20an%20action%20quantization%20model%0Aleveraging%20VQ-VAE-based%20objective%20to%20learn%20discrete%20latent%20actions%20between%0Aimage%20frames%2C%20then%20pretrain%20a%20latent%20VLA%20model%20to%20predict%20these%20latent%20actions%0Afrom%20observations%20and%20task%20descriptions%2C%20and%20finally%20finetune%20the%20VLA%20on%0Asmall-scale%20robot%20manipulation%20data%20to%20map%20from%20latent%20to%20robot%20actions.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aexisting%20techniques%20that%20train%20robot%20manipulation%20policies%20from%20large-scale%0Avideos.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20VLA%20model%20trained%20with%0Arobotic%20action%20labels%20on%20real-world%20manipulation%20tasks%20that%20require%20language%0Aconditioning%2C%20generalization%20to%20unseen%20objects%2C%20and%20semantic%20generalization%20to%0Aunseen%20instructions.%20Training%20only%20on%20human%20manipulation%20videos%20also%20shows%0Apositive%20transfer%2C%20opening%20up%20the%20potential%20for%20leveraging%20web-scale%20data%20for%0Arobotics%20foundation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Action%2520Pretraining%2520from%2520Videos%26entry.906535625%3DSeonghyeon%2520Ye%2520and%2520Joel%2520Jang%2520and%2520Byeongguk%2520Jeon%2520and%2520Sejune%2520Joo%2520and%2520Jianwei%2520Yang%2520and%2520Baolin%2520Peng%2520and%2520Ajay%2520Mandlekar%2520and%2520Reuben%2520Tan%2520and%2520Yu-Wei%2520Chao%2520and%2520Bill%2520Yuchen%2520Lin%2520and%2520Lars%2520Liden%2520and%2520Kimin%2520Lee%2520and%2520Jianfeng%2520Gao%2520and%2520Luke%2520Zettlemoyer%2520and%2520Dieter%2520Fox%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520We%2520introduce%2520Latent%2520Action%2520Pretraining%2520for%2520general%2520Action%2520models%2520%2528LAPA%2529%252C%2520an%250Aunsupervised%2520method%2520for%2520pretraining%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520without%250Aground-truth%2520robot%2520action%2520labels.%2520Existing%2520Vision-Language-Action%2520models%250Arequire%2520action%2520labels%2520typically%2520collected%2520by%2520human%2520teleoperators%2520during%250Apretraining%252C%2520which%2520significantly%2520limits%2520possible%2520data%2520sources%2520and%2520scale.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520method%2520to%2520learn%2520from%2520internet-scale%2520videos%2520that%2520do%2520not%250Ahave%2520robot%2520action%2520labels.%2520We%2520first%2520train%2520an%2520action%2520quantization%2520model%250Aleveraging%2520VQ-VAE-based%2520objective%2520to%2520learn%2520discrete%2520latent%2520actions%2520between%250Aimage%2520frames%252C%2520then%2520pretrain%2520a%2520latent%2520VLA%2520model%2520to%2520predict%2520these%2520latent%2520actions%250Afrom%2520observations%2520and%2520task%2520descriptions%252C%2520and%2520finally%2520finetune%2520the%2520VLA%2520on%250Asmall-scale%2520robot%2520manipulation%2520data%2520to%2520map%2520from%2520latent%2520to%2520robot%2520actions.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Aexisting%2520techniques%2520that%2520train%2520robot%2520manipulation%2520policies%2520from%2520large-scale%250Avideos.%2520Furthermore%252C%2520it%2520outperforms%2520the%2520state-of-the-art%2520VLA%2520model%2520trained%2520with%250Arobotic%2520action%2520labels%2520on%2520real-world%2520manipulation%2520tasks%2520that%2520require%2520language%250Aconditioning%252C%2520generalization%2520to%2520unseen%2520objects%252C%2520and%2520semantic%2520generalization%2520to%250Aunseen%2520instructions.%2520Training%2520only%2520on%2520human%2520manipulation%2520videos%2520also%2520shows%250Apositive%2520transfer%252C%2520opening%2520up%2520the%2520potential%2520for%2520leveraging%2520web-scale%2520data%2520for%250Arobotics%2520foundation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Action%20Pretraining%20from%20Videos&entry.906535625=Seonghyeon%20Ye%20and%20Joel%20Jang%20and%20Byeongguk%20Jeon%20and%20Sejune%20Joo%20and%20Jianwei%20Yang%20and%20Baolin%20Peng%20and%20Ajay%20Mandlekar%20and%20Reuben%20Tan%20and%20Yu-Wei%20Chao%20and%20Bill%20Yuchen%20Lin%20and%20Lars%20Liden%20and%20Kimin%20Lee%20and%20Jianfeng%20Gao%20and%20Luke%20Zettlemoyer%20and%20Dieter%20Fox%20and%20Minjoon%20Seo&entry.1292438233=%20%20We%20introduce%20Latent%20Action%20Pretraining%20for%20general%20Action%20models%20%28LAPA%29%2C%20an%0Aunsupervised%20method%20for%20pretraining%20Vision-Language-Action%20%28VLA%29%20models%20without%0Aground-truth%20robot%20action%20labels.%20Existing%20Vision-Language-Action%20models%0Arequire%20action%20labels%20typically%20collected%20by%20human%20teleoperators%20during%0Apretraining%2C%20which%20significantly%20limits%20possible%20data%20sources%20and%20scale.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20to%20learn%20from%20internet-scale%20videos%20that%20do%20not%0Ahave%20robot%20action%20labels.%20We%20first%20train%20an%20action%20quantization%20model%0Aleveraging%20VQ-VAE-based%20objective%20to%20learn%20discrete%20latent%20actions%20between%0Aimage%20frames%2C%20then%20pretrain%20a%20latent%20VLA%20model%20to%20predict%20these%20latent%20actions%0Afrom%20observations%20and%20task%20descriptions%2C%20and%20finally%20finetune%20the%20VLA%20on%0Asmall-scale%20robot%20manipulation%20data%20to%20map%20from%20latent%20to%20robot%20actions.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Aexisting%20techniques%20that%20train%20robot%20manipulation%20policies%20from%20large-scale%0Avideos.%20Furthermore%2C%20it%20outperforms%20the%20state-of-the-art%20VLA%20model%20trained%20with%0Arobotic%20action%20labels%20on%20real-world%20manipulation%20tasks%20that%20require%20language%0Aconditioning%2C%20generalization%20to%20unseen%20objects%2C%20and%20semantic%20generalization%20to%0Aunseen%20instructions.%20Training%20only%20on%20human%20manipulation%20videos%20also%20shows%0Apositive%20transfer%2C%20opening%20up%20the%20potential%20for%20leveraging%20web-scale%20data%20for%0Arobotics%20foundation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11758v2&entry.124074799=Read"},
{"title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "author": "Aaryan Sharma and Shivansh Gupta and Samar Agarwal and Vishak Prasad C. and Ganesh Ramakrishnan", "abstract": "  Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.\n", "link": "http://arxiv.org/abs/2505.10533v1", "date": "2025-05-15", "relevancy": 2.6846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multi-Image%20Question%20Answering%20via%20Submodular%20Subset%20Selection&body=Title%3A%20Enhancing%20Multi-Image%20Question%20Answering%20via%20Submodular%20Subset%20Selection%0AAuthor%3A%20Aaryan%20Sharma%20and%20Shivansh%20Gupta%20and%20Samar%20Agarwal%20and%20Vishak%20Prasad%20C.%20and%20Ganesh%20Ramakrishnan%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20achieved%20high%20performance%20in%0Avision-language%20tasks%20involving%20single%20image%20but%20they%20struggle%20when%20presented%0Awith%20a%20collection%20of%20multiple%20images%20%28Multiple%20Image%20Question%20Answering%0Ascenario%29.%20These%20tasks%2C%20which%20involve%20reasoning%20over%20large%20number%20of%20images%2C%0Apresent%20issues%20in%20scalability%20%28with%20increasing%20number%20of%20images%29%20and%20retrieval%0Aperformance.%20In%20this%20work%2C%20we%20propose%20an%20enhancement%20for%20retriever%20framework%0Aintroduced%20in%20MIRAGE%20model%20using%20submodular%20subset%20selection%20techniques.%20Our%0Amethod%20leverages%20query-aware%20submodular%20functions%2C%20such%20as%20GraphCut%2C%20to%0Apre-select%20a%20subset%20of%20semantically%20relevant%20images%20before%20main%20retrieval%0Acomponent.%20We%20demonstrate%20that%20using%20anchor-based%20queries%20and%20augmenting%20the%0Adata%20improves%20submodular-retriever%20pipeline%20effectiveness%2C%20particularly%20in%0Alarge%20haystack%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multi-Image%2520Question%2520Answering%2520via%2520Submodular%2520Subset%2520Selection%26entry.906535625%3DAaryan%2520Sharma%2520and%2520Shivansh%2520Gupta%2520and%2520Samar%2520Agarwal%2520and%2520Vishak%2520Prasad%2520C.%2520and%2520Ganesh%2520Ramakrishnan%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520achieved%2520high%2520performance%2520in%250Avision-language%2520tasks%2520involving%2520single%2520image%2520but%2520they%2520struggle%2520when%2520presented%250Awith%2520a%2520collection%2520of%2520multiple%2520images%2520%2528Multiple%2520Image%2520Question%2520Answering%250Ascenario%2529.%2520These%2520tasks%252C%2520which%2520involve%2520reasoning%2520over%2520large%2520number%2520of%2520images%252C%250Apresent%2520issues%2520in%2520scalability%2520%2528with%2520increasing%2520number%2520of%2520images%2529%2520and%2520retrieval%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520enhancement%2520for%2520retriever%2520framework%250Aintroduced%2520in%2520MIRAGE%2520model%2520using%2520submodular%2520subset%2520selection%2520techniques.%2520Our%250Amethod%2520leverages%2520query-aware%2520submodular%2520functions%252C%2520such%2520as%2520GraphCut%252C%2520to%250Apre-select%2520a%2520subset%2520of%2520semantically%2520relevant%2520images%2520before%2520main%2520retrieval%250Acomponent.%2520We%2520demonstrate%2520that%2520using%2520anchor-based%2520queries%2520and%2520augmenting%2520the%250Adata%2520improves%2520submodular-retriever%2520pipeline%2520effectiveness%252C%2520particularly%2520in%250Alarge%2520haystack%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multi-Image%20Question%20Answering%20via%20Submodular%20Subset%20Selection&entry.906535625=Aaryan%20Sharma%20and%20Shivansh%20Gupta%20and%20Samar%20Agarwal%20and%20Vishak%20Prasad%20C.%20and%20Ganesh%20Ramakrishnan&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20achieved%20high%20performance%20in%0Avision-language%20tasks%20involving%20single%20image%20but%20they%20struggle%20when%20presented%0Awith%20a%20collection%20of%20multiple%20images%20%28Multiple%20Image%20Question%20Answering%0Ascenario%29.%20These%20tasks%2C%20which%20involve%20reasoning%20over%20large%20number%20of%20images%2C%0Apresent%20issues%20in%20scalability%20%28with%20increasing%20number%20of%20images%29%20and%20retrieval%0Aperformance.%20In%20this%20work%2C%20we%20propose%20an%20enhancement%20for%20retriever%20framework%0Aintroduced%20in%20MIRAGE%20model%20using%20submodular%20subset%20selection%20techniques.%20Our%0Amethod%20leverages%20query-aware%20submodular%20functions%2C%20such%20as%20GraphCut%2C%20to%0Apre-select%20a%20subset%20of%20semantically%20relevant%20images%20before%20main%20retrieval%0Acomponent.%20We%20demonstrate%20that%20using%20anchor-based%20queries%20and%20augmenting%20the%0Adata%20improves%20submodular-retriever%20pipeline%20effectiveness%2C%20particularly%20in%0Alarge%20haystack%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10533v1&entry.124074799=Read"},
{"title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention", "author": "Jintian Shao and Hongyi Huang and Jiayi Wu and Beiwen Zhang and ZhiYu Wu and You Shan and MingKai Zheng", "abstract": "  Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.\n", "link": "http://arxiv.org/abs/2505.10222v1", "date": "2025-05-15", "relevancy": 2.6771, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComplexFormer%3A%20Disruptively%20Advancing%20Transformer%20Inference%20Ability%20via%0A%20%20Head-Specific%20Complex%20Vector%20Attention&body=Title%3A%20ComplexFormer%3A%20Disruptively%20Advancing%20Transformer%20Inference%20Ability%20via%0A%20%20Head-Specific%20Complex%20Vector%20Attention%0AAuthor%3A%20Jintian%20Shao%20and%20Hongyi%20Huang%20and%20Jiayi%20Wu%20and%20Beiwen%20Zhang%20and%20ZhiYu%20Wu%20and%20You%20Shan%20and%20MingKai%20Zheng%0AAbstract%3A%20%20%20Transformer%20models%20rely%20on%20self-attention%20to%20capture%20token%20dependencies%20but%0Aface%20challenges%20in%20effectively%20integrating%20positional%20information%20while%0Aallowing%20multi-head%20attention%20%28MHA%29%20flexibility.%20Prior%20methods%20often%20model%0Asemantic%20and%20positional%20differences%20disparately%20or%20apply%20uniform%20positional%0Aadjustments%20across%20heads%2C%20potentially%20limiting%20representational%20capacity.%20This%0Apaper%20introduces%20ComplexFormer%2C%20featuring%20Complex%20Multi-Head%20Attention-CMHA.%0ACMHA%20empowers%20each%20head%20to%20independently%20model%20semantic%20and%20positional%0Adifferences%20unified%20within%20the%20complex%20plane%2C%20representing%20interactions%20as%0Arotations%20and%20scaling.%20ComplexFormer%20incorporates%20two%20key%20improvements%3A%20%281%29%20a%0Aper-head%20Euler%20transformation%2C%20converting%20real-valued%20query/key%20projections%0Ainto%20polar-form%20complex%20vectors%20for%20head-specific%20complex%20subspace%20operation%3B%0Aand%20%282%29%20a%20per-head%20adaptive%20differential%20rotation%20mechanism%2C%0Aexp%5Bi%28Adapt%28ASmn%2Ci%29%20%2B%20Delta%28Pmn%29%2Ci%29%5D%2C%20allowing%20each%20head%20to%20learn%20distinct%0Astrategies%20for%20integrating%20semantic%20angle%20differences%20%28ASmn%2Ci%29%20with%20relative%0Apositional%20encodings%20%28Delta%28Pmn%29%2Ci%29.%20Extensive%20experiments%20on%20language%0Amodeling%2C%20text%20generation%2C%20code%20generation%2C%20and%20mathematical%20reasoning%20show%0AComplexFormer%20achieves%20superior%20performance%2C%20significantly%20lower%20generation%0Aperplexity%20%2C%20and%20improved%20long-context%20coherence%20compared%20to%20strong%20baselines%0Alike%20RoPE-Transformers.%20ComplexFormer%20demonstrates%20strong%20parameter%20efficiency%2C%0Aoffering%20a%20more%20expressive%2C%20adaptable%20attention%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplexFormer%253A%2520Disruptively%2520Advancing%2520Transformer%2520Inference%2520Ability%2520via%250A%2520%2520Head-Specific%2520Complex%2520Vector%2520Attention%26entry.906535625%3DJintian%2520Shao%2520and%2520Hongyi%2520Huang%2520and%2520Jiayi%2520Wu%2520and%2520Beiwen%2520Zhang%2520and%2520ZhiYu%2520Wu%2520and%2520You%2520Shan%2520and%2520MingKai%2520Zheng%26entry.1292438233%3D%2520%2520Transformer%2520models%2520rely%2520on%2520self-attention%2520to%2520capture%2520token%2520dependencies%2520but%250Aface%2520challenges%2520in%2520effectively%2520integrating%2520positional%2520information%2520while%250Aallowing%2520multi-head%2520attention%2520%2528MHA%2529%2520flexibility.%2520Prior%2520methods%2520often%2520model%250Asemantic%2520and%2520positional%2520differences%2520disparately%2520or%2520apply%2520uniform%2520positional%250Aadjustments%2520across%2520heads%252C%2520potentially%2520limiting%2520representational%2520capacity.%2520This%250Apaper%2520introduces%2520ComplexFormer%252C%2520featuring%2520Complex%2520Multi-Head%2520Attention-CMHA.%250ACMHA%2520empowers%2520each%2520head%2520to%2520independently%2520model%2520semantic%2520and%2520positional%250Adifferences%2520unified%2520within%2520the%2520complex%2520plane%252C%2520representing%2520interactions%2520as%250Arotations%2520and%2520scaling.%2520ComplexFormer%2520incorporates%2520two%2520key%2520improvements%253A%2520%25281%2529%2520a%250Aper-head%2520Euler%2520transformation%252C%2520converting%2520real-valued%2520query/key%2520projections%250Ainto%2520polar-form%2520complex%2520vectors%2520for%2520head-specific%2520complex%2520subspace%2520operation%253B%250Aand%2520%25282%2529%2520a%2520per-head%2520adaptive%2520differential%2520rotation%2520mechanism%252C%250Aexp%255Bi%2528Adapt%2528ASmn%252Ci%2529%2520%252B%2520Delta%2528Pmn%2529%252Ci%2529%255D%252C%2520allowing%2520each%2520head%2520to%2520learn%2520distinct%250Astrategies%2520for%2520integrating%2520semantic%2520angle%2520differences%2520%2528ASmn%252Ci%2529%2520with%2520relative%250Apositional%2520encodings%2520%2528Delta%2528Pmn%2529%252Ci%2529.%2520Extensive%2520experiments%2520on%2520language%250Amodeling%252C%2520text%2520generation%252C%2520code%2520generation%252C%2520and%2520mathematical%2520reasoning%2520show%250AComplexFormer%2520achieves%2520superior%2520performance%252C%2520significantly%2520lower%2520generation%250Aperplexity%2520%252C%2520and%2520improved%2520long-context%2520coherence%2520compared%2520to%2520strong%2520baselines%250Alike%2520RoPE-Transformers.%2520ComplexFormer%2520demonstrates%2520strong%2520parameter%2520efficiency%252C%250Aoffering%2520a%2520more%2520expressive%252C%2520adaptable%2520attention%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComplexFormer%3A%20Disruptively%20Advancing%20Transformer%20Inference%20Ability%20via%0A%20%20Head-Specific%20Complex%20Vector%20Attention&entry.906535625=Jintian%20Shao%20and%20Hongyi%20Huang%20and%20Jiayi%20Wu%20and%20Beiwen%20Zhang%20and%20ZhiYu%20Wu%20and%20You%20Shan%20and%20MingKai%20Zheng&entry.1292438233=%20%20Transformer%20models%20rely%20on%20self-attention%20to%20capture%20token%20dependencies%20but%0Aface%20challenges%20in%20effectively%20integrating%20positional%20information%20while%0Aallowing%20multi-head%20attention%20%28MHA%29%20flexibility.%20Prior%20methods%20often%20model%0Asemantic%20and%20positional%20differences%20disparately%20or%20apply%20uniform%20positional%0Aadjustments%20across%20heads%2C%20potentially%20limiting%20representational%20capacity.%20This%0Apaper%20introduces%20ComplexFormer%2C%20featuring%20Complex%20Multi-Head%20Attention-CMHA.%0ACMHA%20empowers%20each%20head%20to%20independently%20model%20semantic%20and%20positional%0Adifferences%20unified%20within%20the%20complex%20plane%2C%20representing%20interactions%20as%0Arotations%20and%20scaling.%20ComplexFormer%20incorporates%20two%20key%20improvements%3A%20%281%29%20a%0Aper-head%20Euler%20transformation%2C%20converting%20real-valued%20query/key%20projections%0Ainto%20polar-form%20complex%20vectors%20for%20head-specific%20complex%20subspace%20operation%3B%0Aand%20%282%29%20a%20per-head%20adaptive%20differential%20rotation%20mechanism%2C%0Aexp%5Bi%28Adapt%28ASmn%2Ci%29%20%2B%20Delta%28Pmn%29%2Ci%29%5D%2C%20allowing%20each%20head%20to%20learn%20distinct%0Astrategies%20for%20integrating%20semantic%20angle%20differences%20%28ASmn%2Ci%29%20with%20relative%0Apositional%20encodings%20%28Delta%28Pmn%29%2Ci%29.%20Extensive%20experiments%20on%20language%0Amodeling%2C%20text%20generation%2C%20code%20generation%2C%20and%20mathematical%20reasoning%20show%0AComplexFormer%20achieves%20superior%20performance%2C%20significantly%20lower%20generation%0Aperplexity%20%2C%20and%20improved%20long-context%20coherence%20compared%20to%20strong%20baselines%0Alike%20RoPE-Transformers.%20ComplexFormer%20demonstrates%20strong%20parameter%20efficiency%2C%0Aoffering%20a%20more%20expressive%2C%20adaptable%20attention%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10222v1&entry.124074799=Read"},
{"title": "An unsupervised method for MRI recovery: Deep image prior with\n  structured sparsity", "author": "Muhammad Ahmad Sultan and Chong Chen and Yingmin Liu and Katarzyna Gil and Karolina Zareba and Rizwan Ahmad", "abstract": "  Objective: To propose and validate an unsupervised MRI reconstruction method\nthat does not require fully sampled k-space data. Materials and Methods: The\nproposed method, deep image prior with structured sparsity (DISCUS), extends\nthe deep image prior (DIP) by introducing group sparsity to frame-specific code\nvectors, enabling the discovery of a low-dimensional manifold for capturing\ntemporal variations. \\discus was validated using four studies: (I) simulation\nof a dynamic Shepp-Logan phantom to demonstrate its manifold discovery\ncapabilities, (II) comparison with compressed sensing and DIP-based methods\nusing simulated single-shot late gadolinium enhancement (LGE) image series from\nsix distinct digital cardiac phantoms in terms of normalized mean square error\n(NMSE) and structural similarity index measure (SSIM), (III) evaluation on\nretrospectively undersampled single-shot LGE data from eight patients, and (IV)\nevaluation on prospectively undersampled single-shot LGE data from eight\npatients, assessed via blind scoring from two expert readers. Results: DISCUS\noutperformed competing methods, demonstrating superior reconstruction quality\nin terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study\nIV). Discussion: An unsupervised image reconstruction method is presented and\nvalidated on simulated and measured data. These developments can benefit\napplications where acquiring fully sampled data is challenging.\n", "link": "http://arxiv.org/abs/2501.01482v2", "date": "2025-05-15", "relevancy": 2.6674, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5339}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20unsupervised%20method%20for%20MRI%20recovery%3A%20Deep%20image%20prior%20with%0A%20%20structured%20sparsity&body=Title%3A%20An%20unsupervised%20method%20for%20MRI%20recovery%3A%20Deep%20image%20prior%20with%0A%20%20structured%20sparsity%0AAuthor%3A%20Muhammad%20Ahmad%20Sultan%20and%20Chong%20Chen%20and%20Yingmin%20Liu%20and%20Katarzyna%20Gil%20and%20Karolina%20Zareba%20and%20Rizwan%20Ahmad%0AAbstract%3A%20%20%20Objective%3A%20To%20propose%20and%20validate%20an%20unsupervised%20MRI%20reconstruction%20method%0Athat%20does%20not%20require%20fully%20sampled%20k-space%20data.%20Materials%20and%20Methods%3A%20The%0Aproposed%20method%2C%20deep%20image%20prior%20with%20structured%20sparsity%20%28DISCUS%29%2C%20extends%0Athe%20deep%20image%20prior%20%28DIP%29%20by%20introducing%20group%20sparsity%20to%20frame-specific%20code%0Avectors%2C%20enabling%20the%20discovery%20of%20a%20low-dimensional%20manifold%20for%20capturing%0Atemporal%20variations.%20%5Cdiscus%20was%20validated%20using%20four%20studies%3A%20%28I%29%20simulation%0Aof%20a%20dynamic%20Shepp-Logan%20phantom%20to%20demonstrate%20its%20manifold%20discovery%0Acapabilities%2C%20%28II%29%20comparison%20with%20compressed%20sensing%20and%20DIP-based%20methods%0Ausing%20simulated%20single-shot%20late%20gadolinium%20enhancement%20%28LGE%29%20image%20series%20from%0Asix%20distinct%20digital%20cardiac%20phantoms%20in%20terms%20of%20normalized%20mean%20square%20error%0A%28NMSE%29%20and%20structural%20similarity%20index%20measure%20%28SSIM%29%2C%20%28III%29%20evaluation%20on%0Aretrospectively%20undersampled%20single-shot%20LGE%20data%20from%20eight%20patients%2C%20and%20%28IV%29%0Aevaluation%20on%20prospectively%20undersampled%20single-shot%20LGE%20data%20from%20eight%0Apatients%2C%20assessed%20via%20blind%20scoring%20from%20two%20expert%20readers.%20Results%3A%20DISCUS%0Aoutperformed%20competing%20methods%2C%20demonstrating%20superior%20reconstruction%20quality%0Ain%20terms%20of%20NMSE%20and%20SSIM%20%28Studies%20I--III%29%20and%20expert%20reader%20scoring%20%28Study%0AIV%29.%20Discussion%3A%20An%20unsupervised%20image%20reconstruction%20method%20is%20presented%20and%0Avalidated%20on%20simulated%20and%20measured%20data.%20These%20developments%20can%20benefit%0Aapplications%20where%20acquiring%20fully%20sampled%20data%20is%20challenging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520unsupervised%2520method%2520for%2520MRI%2520recovery%253A%2520Deep%2520image%2520prior%2520with%250A%2520%2520structured%2520sparsity%26entry.906535625%3DMuhammad%2520Ahmad%2520Sultan%2520and%2520Chong%2520Chen%2520and%2520Yingmin%2520Liu%2520and%2520Katarzyna%2520Gil%2520and%2520Karolina%2520Zareba%2520and%2520Rizwan%2520Ahmad%26entry.1292438233%3D%2520%2520Objective%253A%2520To%2520propose%2520and%2520validate%2520an%2520unsupervised%2520MRI%2520reconstruction%2520method%250Athat%2520does%2520not%2520require%2520fully%2520sampled%2520k-space%2520data.%2520Materials%2520and%2520Methods%253A%2520The%250Aproposed%2520method%252C%2520deep%2520image%2520prior%2520with%2520structured%2520sparsity%2520%2528DISCUS%2529%252C%2520extends%250Athe%2520deep%2520image%2520prior%2520%2528DIP%2529%2520by%2520introducing%2520group%2520sparsity%2520to%2520frame-specific%2520code%250Avectors%252C%2520enabling%2520the%2520discovery%2520of%2520a%2520low-dimensional%2520manifold%2520for%2520capturing%250Atemporal%2520variations.%2520%255Cdiscus%2520was%2520validated%2520using%2520four%2520studies%253A%2520%2528I%2529%2520simulation%250Aof%2520a%2520dynamic%2520Shepp-Logan%2520phantom%2520to%2520demonstrate%2520its%2520manifold%2520discovery%250Acapabilities%252C%2520%2528II%2529%2520comparison%2520with%2520compressed%2520sensing%2520and%2520DIP-based%2520methods%250Ausing%2520simulated%2520single-shot%2520late%2520gadolinium%2520enhancement%2520%2528LGE%2529%2520image%2520series%2520from%250Asix%2520distinct%2520digital%2520cardiac%2520phantoms%2520in%2520terms%2520of%2520normalized%2520mean%2520square%2520error%250A%2528NMSE%2529%2520and%2520structural%2520similarity%2520index%2520measure%2520%2528SSIM%2529%252C%2520%2528III%2529%2520evaluation%2520on%250Aretrospectively%2520undersampled%2520single-shot%2520LGE%2520data%2520from%2520eight%2520patients%252C%2520and%2520%2528IV%2529%250Aevaluation%2520on%2520prospectively%2520undersampled%2520single-shot%2520LGE%2520data%2520from%2520eight%250Apatients%252C%2520assessed%2520via%2520blind%2520scoring%2520from%2520two%2520expert%2520readers.%2520Results%253A%2520DISCUS%250Aoutperformed%2520competing%2520methods%252C%2520demonstrating%2520superior%2520reconstruction%2520quality%250Ain%2520terms%2520of%2520NMSE%2520and%2520SSIM%2520%2528Studies%2520I--III%2529%2520and%2520expert%2520reader%2520scoring%2520%2528Study%250AIV%2529.%2520Discussion%253A%2520An%2520unsupervised%2520image%2520reconstruction%2520method%2520is%2520presented%2520and%250Avalidated%2520on%2520simulated%2520and%2520measured%2520data.%2520These%2520developments%2520can%2520benefit%250Aapplications%2520where%2520acquiring%2520fully%2520sampled%2520data%2520is%2520challenging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20unsupervised%20method%20for%20MRI%20recovery%3A%20Deep%20image%20prior%20with%0A%20%20structured%20sparsity&entry.906535625=Muhammad%20Ahmad%20Sultan%20and%20Chong%20Chen%20and%20Yingmin%20Liu%20and%20Katarzyna%20Gil%20and%20Karolina%20Zareba%20and%20Rizwan%20Ahmad&entry.1292438233=%20%20Objective%3A%20To%20propose%20and%20validate%20an%20unsupervised%20MRI%20reconstruction%20method%0Athat%20does%20not%20require%20fully%20sampled%20k-space%20data.%20Materials%20and%20Methods%3A%20The%0Aproposed%20method%2C%20deep%20image%20prior%20with%20structured%20sparsity%20%28DISCUS%29%2C%20extends%0Athe%20deep%20image%20prior%20%28DIP%29%20by%20introducing%20group%20sparsity%20to%20frame-specific%20code%0Avectors%2C%20enabling%20the%20discovery%20of%20a%20low-dimensional%20manifold%20for%20capturing%0Atemporal%20variations.%20%5Cdiscus%20was%20validated%20using%20four%20studies%3A%20%28I%29%20simulation%0Aof%20a%20dynamic%20Shepp-Logan%20phantom%20to%20demonstrate%20its%20manifold%20discovery%0Acapabilities%2C%20%28II%29%20comparison%20with%20compressed%20sensing%20and%20DIP-based%20methods%0Ausing%20simulated%20single-shot%20late%20gadolinium%20enhancement%20%28LGE%29%20image%20series%20from%0Asix%20distinct%20digital%20cardiac%20phantoms%20in%20terms%20of%20normalized%20mean%20square%20error%0A%28NMSE%29%20and%20structural%20similarity%20index%20measure%20%28SSIM%29%2C%20%28III%29%20evaluation%20on%0Aretrospectively%20undersampled%20single-shot%20LGE%20data%20from%20eight%20patients%2C%20and%20%28IV%29%0Aevaluation%20on%20prospectively%20undersampled%20single-shot%20LGE%20data%20from%20eight%0Apatients%2C%20assessed%20via%20blind%20scoring%20from%20two%20expert%20readers.%20Results%3A%20DISCUS%0Aoutperformed%20competing%20methods%2C%20demonstrating%20superior%20reconstruction%20quality%0Ain%20terms%20of%20NMSE%20and%20SSIM%20%28Studies%20I--III%29%20and%20expert%20reader%20scoring%20%28Study%0AIV%29.%20Discussion%3A%20An%20unsupervised%20image%20reconstruction%20method%20is%20presented%20and%0Avalidated%20on%20simulated%20and%20measured%20data.%20These%20developments%20can%20benefit%0Aapplications%20where%20acquiring%20fully%20sampled%20data%20is%20challenging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01482v2&entry.124074799=Read"},
{"title": "SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human\n  Activity Recognition", "author": "Anh Tuan Ha and Hoang Khang Phan and Thai Minh Tien Ngo and Anh Phan Truong and Nhat Tan Le", "abstract": "  In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.\n", "link": "http://arxiv.org/abs/2505.10312v1", "date": "2025-05-15", "relevancy": 2.6336, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5347}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5241}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOS%3A%20A%20Shuffle%20Order%20Strategy%20for%20Data%20Augmentation%20in%20Industrial%20Human%0A%20%20Activity%20Recognition&body=Title%3A%20SOS%3A%20A%20Shuffle%20Order%20Strategy%20for%20Data%20Augmentation%20in%20Industrial%20Human%0A%20%20Activity%20Recognition%0AAuthor%3A%20Anh%20Tuan%20Ha%20and%20Hoang%20Khang%20Phan%20and%20Thai%20Minh%20Tien%20Ngo%20and%20Anh%20Phan%20Truong%20and%20Nhat%20Tan%20Le%0AAbstract%3A%20%20%20In%20the%20realm%20of%20Human%20Activity%20Recognition%20%28HAR%29%2C%20obtaining%20high%20quality%20and%0Avariance%20data%20is%20still%20a%20persistent%20challenge%20due%20to%20high%20costs%20and%20the%0Ainherent%20variability%20of%20real-world%20activities.%20This%20study%20introduces%20a%0Ageneration%20dataset%20by%20deep%20learning%20approaches%20%28Attention%20Autoencoder%20and%0Aconditional%20Generative%20Adversarial%20Networks%29.%20Another%20problem%20that%20data%0Aheterogeneity%20is%20a%20critical%20challenge%2C%20one%20of%20the%20solutions%20is%20to%20shuffle%20the%0Adata%20to%20homogenize%20the%20distribution.%20Experimental%20results%20demonstrate%20that%20the%0Arandom%20sequence%20strategy%20significantly%20improves%20classification%20performance%2C%0Aachieving%20an%20accuracy%20of%20up%20to%200.70%20%24%5Cpm%24%200.03%20and%20a%20macro%20F1%20score%20of%200.64%0A%24%5Cpm%24%200.01.%20For%20that%2C%20disrupting%20temporal%20dependencies%20through%20random%20sequence%0Areordering%20compels%20the%20model%20to%20focus%20on%20instantaneous%20recognition%2C%20thereby%0Aimproving%20robustness%20against%20activity%20transitions.%20This%20approach%20not%20only%0Abroadens%20the%20effective%20training%20dataset%20but%20also%20offers%20promising%20avenues%20for%0Aenhancing%20HAR%20systems%20in%20complex%2C%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOS%253A%2520A%2520Shuffle%2520Order%2520Strategy%2520for%2520Data%2520Augmentation%2520in%2520Industrial%2520Human%250A%2520%2520Activity%2520Recognition%26entry.906535625%3DAnh%2520Tuan%2520Ha%2520and%2520Hoang%2520Khang%2520Phan%2520and%2520Thai%2520Minh%2520Tien%2520Ngo%2520and%2520Anh%2520Phan%2520Truong%2520and%2520Nhat%2520Tan%2520Le%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%252C%2520obtaining%2520high%2520quality%2520and%250Avariance%2520data%2520is%2520still%2520a%2520persistent%2520challenge%2520due%2520to%2520high%2520costs%2520and%2520the%250Ainherent%2520variability%2520of%2520real-world%2520activities.%2520This%2520study%2520introduces%2520a%250Ageneration%2520dataset%2520by%2520deep%2520learning%2520approaches%2520%2528Attention%2520Autoencoder%2520and%250Aconditional%2520Generative%2520Adversarial%2520Networks%2529.%2520Another%2520problem%2520that%2520data%250Aheterogeneity%2520is%2520a%2520critical%2520challenge%252C%2520one%2520of%2520the%2520solutions%2520is%2520to%2520shuffle%2520the%250Adata%2520to%2520homogenize%2520the%2520distribution.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Arandom%2520sequence%2520strategy%2520significantly%2520improves%2520classification%2520performance%252C%250Aachieving%2520an%2520accuracy%2520of%2520up%2520to%25200.70%2520%2524%255Cpm%2524%25200.03%2520and%2520a%2520macro%2520F1%2520score%2520of%25200.64%250A%2524%255Cpm%2524%25200.01.%2520For%2520that%252C%2520disrupting%2520temporal%2520dependencies%2520through%2520random%2520sequence%250Areordering%2520compels%2520the%2520model%2520to%2520focus%2520on%2520instantaneous%2520recognition%252C%2520thereby%250Aimproving%2520robustness%2520against%2520activity%2520transitions.%2520This%2520approach%2520not%2520only%250Abroadens%2520the%2520effective%2520training%2520dataset%2520but%2520also%2520offers%2520promising%2520avenues%2520for%250Aenhancing%2520HAR%2520systems%2520in%2520complex%252C%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOS%3A%20A%20Shuffle%20Order%20Strategy%20for%20Data%20Augmentation%20in%20Industrial%20Human%0A%20%20Activity%20Recognition&entry.906535625=Anh%20Tuan%20Ha%20and%20Hoang%20Khang%20Phan%20and%20Thai%20Minh%20Tien%20Ngo%20and%20Anh%20Phan%20Truong%20and%20Nhat%20Tan%20Le&entry.1292438233=%20%20In%20the%20realm%20of%20Human%20Activity%20Recognition%20%28HAR%29%2C%20obtaining%20high%20quality%20and%0Avariance%20data%20is%20still%20a%20persistent%20challenge%20due%20to%20high%20costs%20and%20the%0Ainherent%20variability%20of%20real-world%20activities.%20This%20study%20introduces%20a%0Ageneration%20dataset%20by%20deep%20learning%20approaches%20%28Attention%20Autoencoder%20and%0Aconditional%20Generative%20Adversarial%20Networks%29.%20Another%20problem%20that%20data%0Aheterogeneity%20is%20a%20critical%20challenge%2C%20one%20of%20the%20solutions%20is%20to%20shuffle%20the%0Adata%20to%20homogenize%20the%20distribution.%20Experimental%20results%20demonstrate%20that%20the%0Arandom%20sequence%20strategy%20significantly%20improves%20classification%20performance%2C%0Aachieving%20an%20accuracy%20of%20up%20to%200.70%20%24%5Cpm%24%200.03%20and%20a%20macro%20F1%20score%20of%200.64%0A%24%5Cpm%24%200.01.%20For%20that%2C%20disrupting%20temporal%20dependencies%20through%20random%20sequence%0Areordering%20compels%20the%20model%20to%20focus%20on%20instantaneous%20recognition%2C%20thereby%0Aimproving%20robustness%20against%20activity%20transitions.%20This%20approach%20not%20only%0Abroadens%20the%20effective%20training%20dataset%20but%20also%20offers%20promising%20avenues%20for%0Aenhancing%20HAR%20systems%20in%20complex%2C%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10312v1&entry.124074799=Read"},
{"title": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments", "author": "Tuan Dung Nguyen and Duncan J. Watts and Mark E. Whiting", "abstract": "  Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.\n", "link": "http://arxiv.org/abs/2505.10309v1", "date": "2025-05-15", "relevancy": 2.6129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirically%20evaluating%20commonsense%20intelligence%20in%20large%20language%20models%0A%20%20with%20large-scale%20human%20judgments&body=Title%3A%20Empirically%20evaluating%20commonsense%20intelligence%20in%20large%20language%20models%0A%20%20with%20large-scale%20human%20judgments%0AAuthor%3A%20Tuan%20Dung%20Nguyen%20and%20Duncan%20J.%20Watts%20and%20Mark%20E.%20Whiting%0AAbstract%3A%20%20%20Commonsense%20intelligence%20in%20machines%20is%20often%20assessed%20by%20static%20benchmarks%0Athat%20compare%20a%20model%27s%20output%20against%20human-prescribed%20correct%20labels.%20An%0Aimportant%2C%20albeit%20implicit%2C%20assumption%20of%20these%20labels%20is%20that%20they%20accurately%0Acapture%20what%20any%20human%20would%20think%2C%20effectively%20treating%20human%20common%20sense%20as%0Ahomogeneous.%20However%2C%20recent%20empirical%20work%20has%20shown%20that%20humans%20vary%0Aenormously%20in%20what%20they%20consider%20commonsensical%3B%20thus%20what%20appears%20self-evident%0Ato%20one%20benchmark%20designer%20may%20not%20be%20so%20to%20another.%20Here%2C%20we%20propose%20a%20novel%0Amethod%20for%20evaluating%20common%20sense%20in%20artificial%20intelligence%20%28AI%29%2C%0Aspecifically%20in%20large%20language%20models%20%28LLMs%29%2C%20that%20incorporates%20empirically%0Aobserved%20heterogeneity%20among%20humans%20by%20measuring%20the%20correspondence%20between%20a%0Amodel%27s%20judgment%20and%20that%20of%20a%20human%20population.%20We%20first%20find%20that%2C%20when%0Atreated%20as%20independent%20survey%20respondents%2C%20most%20LLMs%20remain%20below%20the%20human%0Amedian%20in%20their%20individual%20commonsense%20competence.%20Second%2C%20when%20used%20as%0Asimulators%20of%20a%20hypothetical%20population%2C%20LLMs%20correlate%20with%20real%20humans%20only%0Amodestly%20in%20the%20extent%20to%20which%20they%20agree%20on%20the%20same%20set%20of%20statements.%20In%0Aboth%20cases%2C%20smaller%2C%20open-weight%20models%20are%20surprisingly%20more%20competitive%20than%0Alarger%2C%20proprietary%20frontier%20models.%20Our%20evaluation%20framework%2C%20which%20ties%0Acommonsense%20intelligence%20to%20its%20cultural%20basis%2C%20contributes%20to%20the%20growing%20call%0Afor%20adapting%20AI%20models%20to%20human%20collectivities%20that%20possess%20different%2C%20often%0Aincompatible%2C%20social%20stocks%20of%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirically%2520evaluating%2520commonsense%2520intelligence%2520in%2520large%2520language%2520models%250A%2520%2520with%2520large-scale%2520human%2520judgments%26entry.906535625%3DTuan%2520Dung%2520Nguyen%2520and%2520Duncan%2520J.%2520Watts%2520and%2520Mark%2520E.%2520Whiting%26entry.1292438233%3D%2520%2520Commonsense%2520intelligence%2520in%2520machines%2520is%2520often%2520assessed%2520by%2520static%2520benchmarks%250Athat%2520compare%2520a%2520model%2527s%2520output%2520against%2520human-prescribed%2520correct%2520labels.%2520An%250Aimportant%252C%2520albeit%2520implicit%252C%2520assumption%2520of%2520these%2520labels%2520is%2520that%2520they%2520accurately%250Acapture%2520what%2520any%2520human%2520would%2520think%252C%2520effectively%2520treating%2520human%2520common%2520sense%2520as%250Ahomogeneous.%2520However%252C%2520recent%2520empirical%2520work%2520has%2520shown%2520that%2520humans%2520vary%250Aenormously%2520in%2520what%2520they%2520consider%2520commonsensical%253B%2520thus%2520what%2520appears%2520self-evident%250Ato%2520one%2520benchmark%2520designer%2520may%2520not%2520be%2520so%2520to%2520another.%2520Here%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520for%2520evaluating%2520common%2520sense%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%250Aspecifically%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520that%2520incorporates%2520empirically%250Aobserved%2520heterogeneity%2520among%2520humans%2520by%2520measuring%2520the%2520correspondence%2520between%2520a%250Amodel%2527s%2520judgment%2520and%2520that%2520of%2520a%2520human%2520population.%2520We%2520first%2520find%2520that%252C%2520when%250Atreated%2520as%2520independent%2520survey%2520respondents%252C%2520most%2520LLMs%2520remain%2520below%2520the%2520human%250Amedian%2520in%2520their%2520individual%2520commonsense%2520competence.%2520Second%252C%2520when%2520used%2520as%250Asimulators%2520of%2520a%2520hypothetical%2520population%252C%2520LLMs%2520correlate%2520with%2520real%2520humans%2520only%250Amodestly%2520in%2520the%2520extent%2520to%2520which%2520they%2520agree%2520on%2520the%2520same%2520set%2520of%2520statements.%2520In%250Aboth%2520cases%252C%2520smaller%252C%2520open-weight%2520models%2520are%2520surprisingly%2520more%2520competitive%2520than%250Alarger%252C%2520proprietary%2520frontier%2520models.%2520Our%2520evaluation%2520framework%252C%2520which%2520ties%250Acommonsense%2520intelligence%2520to%2520its%2520cultural%2520basis%252C%2520contributes%2520to%2520the%2520growing%2520call%250Afor%2520adapting%2520AI%2520models%2520to%2520human%2520collectivities%2520that%2520possess%2520different%252C%2520often%250Aincompatible%252C%2520social%2520stocks%2520of%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirically%20evaluating%20commonsense%20intelligence%20in%20large%20language%20models%0A%20%20with%20large-scale%20human%20judgments&entry.906535625=Tuan%20Dung%20Nguyen%20and%20Duncan%20J.%20Watts%20and%20Mark%20E.%20Whiting&entry.1292438233=%20%20Commonsense%20intelligence%20in%20machines%20is%20often%20assessed%20by%20static%20benchmarks%0Athat%20compare%20a%20model%27s%20output%20against%20human-prescribed%20correct%20labels.%20An%0Aimportant%2C%20albeit%20implicit%2C%20assumption%20of%20these%20labels%20is%20that%20they%20accurately%0Acapture%20what%20any%20human%20would%20think%2C%20effectively%20treating%20human%20common%20sense%20as%0Ahomogeneous.%20However%2C%20recent%20empirical%20work%20has%20shown%20that%20humans%20vary%0Aenormously%20in%20what%20they%20consider%20commonsensical%3B%20thus%20what%20appears%20self-evident%0Ato%20one%20benchmark%20designer%20may%20not%20be%20so%20to%20another.%20Here%2C%20we%20propose%20a%20novel%0Amethod%20for%20evaluating%20common%20sense%20in%20artificial%20intelligence%20%28AI%29%2C%0Aspecifically%20in%20large%20language%20models%20%28LLMs%29%2C%20that%20incorporates%20empirically%0Aobserved%20heterogeneity%20among%20humans%20by%20measuring%20the%20correspondence%20between%20a%0Amodel%27s%20judgment%20and%20that%20of%20a%20human%20population.%20We%20first%20find%20that%2C%20when%0Atreated%20as%20independent%20survey%20respondents%2C%20most%20LLMs%20remain%20below%20the%20human%0Amedian%20in%20their%20individual%20commonsense%20competence.%20Second%2C%20when%20used%20as%0Asimulators%20of%20a%20hypothetical%20population%2C%20LLMs%20correlate%20with%20real%20humans%20only%0Amodestly%20in%20the%20extent%20to%20which%20they%20agree%20on%20the%20same%20set%20of%20statements.%20In%0Aboth%20cases%2C%20smaller%2C%20open-weight%20models%20are%20surprisingly%20more%20competitive%20than%0Alarger%2C%20proprietary%20frontier%20models.%20Our%20evaluation%20framework%2C%20which%20ties%0Acommonsense%20intelligence%20to%20its%20cultural%20basis%2C%20contributes%20to%20the%20growing%20call%0Afor%20adapting%20AI%20models%20to%20human%20collectivities%20that%20possess%20different%2C%20often%0Aincompatible%2C%20social%20stocks%20of%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10309v1&entry.124074799=Read"},
{"title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning", "author": "Yoichi Ishibashi and Taro Yano and Masafumi Oyamada", "abstract": "  Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.\n", "link": "http://arxiv.org/abs/2505.10182v1", "date": "2025-05-15", "relevancy": 2.5925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mining%20Hidden%20Thoughts%20from%20Texts%3A%20Evaluating%20Continual%20Pretraining%20with%0A%20%20Synthetic%20Data%20for%20LLM%20Reasoning&body=Title%3A%20Mining%20Hidden%20Thoughts%20from%20Texts%3A%20Evaluating%20Continual%20Pretraining%20with%0A%20%20Synthetic%20Data%20for%20LLM%20Reasoning%0AAuthor%3A%20Yoichi%20Ishibashi%20and%20Taro%20Yano%20and%20Masafumi%20Oyamada%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20improvements%20in%0Areasoning%20capabilities%20through%20supervised%20fine-tuning%20and%20reinforcement%0Alearning.%20However%2C%20when%20training%20reasoning%20models%2C%20these%20approaches%20are%0Aprimarily%20applicable%20to%20specific%20domains%20such%20as%20mathematics%20and%20programming%2C%0Awhich%20imposes%20fundamental%20constraints%20on%20the%20breadth%20and%20scalability%20of%0Atraining%20data.%20In%20contrast%2C%20continual%20pretraining%20%28CPT%29%20offers%20the%20advantage%20of%0Anot%20requiring%20task-specific%20signals.%20Nevertheless%2C%20how%20to%20effectively%0Asynthesize%20training%20data%20for%20reasoning%20and%20how%20such%20data%20affect%20a%20wide%20range%20of%0Adomains%20remain%20largely%20unexplored.%20This%20study%20provides%20a%20detailed%20evaluation%20of%0AReasoning%20CPT%2C%20a%20form%20of%20CPT%20that%20uses%20synthetic%20data%20to%20reconstruct%20the%20hidden%0Athought%20processes%20underlying%20texts%2C%20based%20on%20the%20premise%20that%20texts%20are%20the%0Aresult%20of%20the%20author%27s%20thinking%20process.%20Specifically%2C%20we%20apply%20Reasoning%20CPT%0Ato%20Gemma2-9B%20using%20synthetic%20data%20with%20hidden%20thoughts%20derived%20from%20STEM%20and%0ALaw%20corpora%2C%20and%20compare%20it%20to%20standard%20CPT%20on%20the%20MMLU%20benchmark.%20Our%20analysis%0Areveals%20that%20Reasoning%20CPT%20consistently%20improves%20performance%20across%20all%0Aevaluated%20domains.%20Notably%2C%20reasoning%20skills%20acquired%20in%20one%20domain%20transfer%0Aeffectively%20to%20others%3B%20the%20performance%20gap%20with%20conventional%20methods%20widens%20as%0Aproblem%20difficulty%20increases%2C%20with%20gains%20of%20up%20to%208%20points%20on%20the%20most%0Achallenging%20problems.%20Furthermore%2C%20models%20trained%20with%20hidden%20thoughts%20learn%20to%0Aadjust%20the%20depth%20of%20their%20reasoning%20according%20to%20problem%20difficulty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMining%2520Hidden%2520Thoughts%2520from%2520Texts%253A%2520Evaluating%2520Continual%2520Pretraining%2520with%250A%2520%2520Synthetic%2520Data%2520for%2520LLM%2520Reasoning%26entry.906535625%3DYoichi%2520Ishibashi%2520and%2520Taro%2520Yano%2520and%2520Masafumi%2520Oyamada%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520improvements%2520in%250Areasoning%2520capabilities%2520through%2520supervised%2520fine-tuning%2520and%2520reinforcement%250Alearning.%2520However%252C%2520when%2520training%2520reasoning%2520models%252C%2520these%2520approaches%2520are%250Aprimarily%2520applicable%2520to%2520specific%2520domains%2520such%2520as%2520mathematics%2520and%2520programming%252C%250Awhich%2520imposes%2520fundamental%2520constraints%2520on%2520the%2520breadth%2520and%2520scalability%2520of%250Atraining%2520data.%2520In%2520contrast%252C%2520continual%2520pretraining%2520%2528CPT%2529%2520offers%2520the%2520advantage%2520of%250Anot%2520requiring%2520task-specific%2520signals.%2520Nevertheless%252C%2520how%2520to%2520effectively%250Asynthesize%2520training%2520data%2520for%2520reasoning%2520and%2520how%2520such%2520data%2520affect%2520a%2520wide%2520range%2520of%250Adomains%2520remain%2520largely%2520unexplored.%2520This%2520study%2520provides%2520a%2520detailed%2520evaluation%2520of%250AReasoning%2520CPT%252C%2520a%2520form%2520of%2520CPT%2520that%2520uses%2520synthetic%2520data%2520to%2520reconstruct%2520the%2520hidden%250Athought%2520processes%2520underlying%2520texts%252C%2520based%2520on%2520the%2520premise%2520that%2520texts%2520are%2520the%250Aresult%2520of%2520the%2520author%2527s%2520thinking%2520process.%2520Specifically%252C%2520we%2520apply%2520Reasoning%2520CPT%250Ato%2520Gemma2-9B%2520using%2520synthetic%2520data%2520with%2520hidden%2520thoughts%2520derived%2520from%2520STEM%2520and%250ALaw%2520corpora%252C%2520and%2520compare%2520it%2520to%2520standard%2520CPT%2520on%2520the%2520MMLU%2520benchmark.%2520Our%2520analysis%250Areveals%2520that%2520Reasoning%2520CPT%2520consistently%2520improves%2520performance%2520across%2520all%250Aevaluated%2520domains.%2520Notably%252C%2520reasoning%2520skills%2520acquired%2520in%2520one%2520domain%2520transfer%250Aeffectively%2520to%2520others%253B%2520the%2520performance%2520gap%2520with%2520conventional%2520methods%2520widens%2520as%250Aproblem%2520difficulty%2520increases%252C%2520with%2520gains%2520of%2520up%2520to%25208%2520points%2520on%2520the%2520most%250Achallenging%2520problems.%2520Furthermore%252C%2520models%2520trained%2520with%2520hidden%2520thoughts%2520learn%2520to%250Aadjust%2520the%2520depth%2520of%2520their%2520reasoning%2520according%2520to%2520problem%2520difficulty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Hidden%20Thoughts%20from%20Texts%3A%20Evaluating%20Continual%20Pretraining%20with%0A%20%20Synthetic%20Data%20for%20LLM%20Reasoning&entry.906535625=Yoichi%20Ishibashi%20and%20Taro%20Yano%20and%20Masafumi%20Oyamada&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%20improvements%20in%0Areasoning%20capabilities%20through%20supervised%20fine-tuning%20and%20reinforcement%0Alearning.%20However%2C%20when%20training%20reasoning%20models%2C%20these%20approaches%20are%0Aprimarily%20applicable%20to%20specific%20domains%20such%20as%20mathematics%20and%20programming%2C%0Awhich%20imposes%20fundamental%20constraints%20on%20the%20breadth%20and%20scalability%20of%0Atraining%20data.%20In%20contrast%2C%20continual%20pretraining%20%28CPT%29%20offers%20the%20advantage%20of%0Anot%20requiring%20task-specific%20signals.%20Nevertheless%2C%20how%20to%20effectively%0Asynthesize%20training%20data%20for%20reasoning%20and%20how%20such%20data%20affect%20a%20wide%20range%20of%0Adomains%20remain%20largely%20unexplored.%20This%20study%20provides%20a%20detailed%20evaluation%20of%0AReasoning%20CPT%2C%20a%20form%20of%20CPT%20that%20uses%20synthetic%20data%20to%20reconstruct%20the%20hidden%0Athought%20processes%20underlying%20texts%2C%20based%20on%20the%20premise%20that%20texts%20are%20the%0Aresult%20of%20the%20author%27s%20thinking%20process.%20Specifically%2C%20we%20apply%20Reasoning%20CPT%0Ato%20Gemma2-9B%20using%20synthetic%20data%20with%20hidden%20thoughts%20derived%20from%20STEM%20and%0ALaw%20corpora%2C%20and%20compare%20it%20to%20standard%20CPT%20on%20the%20MMLU%20benchmark.%20Our%20analysis%0Areveals%20that%20Reasoning%20CPT%20consistently%20improves%20performance%20across%20all%0Aevaluated%20domains.%20Notably%2C%20reasoning%20skills%20acquired%20in%20one%20domain%20transfer%0Aeffectively%20to%20others%3B%20the%20performance%20gap%20with%20conventional%20methods%20widens%20as%0Aproblem%20difficulty%20increases%2C%20with%20gains%20of%20up%20to%208%20points%20on%20the%20most%0Achallenging%20problems.%20Furthermore%2C%20models%20trained%20with%20hidden%20thoughts%20learn%20to%0Aadjust%20the%20depth%20of%20their%20reasoning%20according%20to%20problem%20difficulty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10182v1&entry.124074799=Read"},
{"title": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization", "author": "Dixuan Wang and Yanda Li and Junyuan Jiang and Zepeng Ding and Ziqin Luo and Guochao Jiang and Jiaqing Liang and Deqing Yang", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.\n", "link": "http://arxiv.org/abs/2405.17067v2", "date": "2025-05-15", "relevancy": 2.5865, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5402}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization&body=Title%3A%20Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization%0AAuthor%3A%20Dixuan%20Wang%20and%20Yanda%20Li%20and%20Junyuan%20Jiang%20and%20Zepeng%20Ding%20and%20Ziqin%20Luo%20and%20Guochao%20Jiang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20it%20was%20also%20witnessed%20that%20LLMs%20tend%0Ato%20produce%20inaccurate%20responses%20to%20specific%20queries.%20This%20deficiency%20can%20be%0Atraced%20to%20the%20tokenization%20step%20LLMs%20must%20undergo%2C%20which%20is%20an%20inevitable%0Alimitation%20inherent%20to%20all%20LLMs.%20In%20fact%2C%20incorrect%20tokenization%20is%20the%0Acritical%20point%20that%20hinders%20LLMs%20in%20understanding%20the%20input%20precisely%2C%20thus%0Aleading%20to%20unsatisfactory%20output.%20This%20defect%20is%20more%20obvious%20in%20Chinese%0Ascenarios.%20To%20demonstrate%20this%20flaw%20of%20LLMs%2C%20we%20construct%20an%20adversarial%0Adataset%2C%20named%20as%20%24%5Ctextbf%7BADT%20%28Adversarial%20Dataset%20for%20Tokenizer%29%7D%24%2C%20which%0Adraws%20upon%20the%20vocabularies%20of%20various%20open-source%20LLMs%20to%20challenge%20LLMs%27%0Atokenization.%20ADT%20consists%20of%20two%20subsets%3A%20the%20manually%20constructed%20ADT-Human%0Aand%20the%20automatically%20generated%20ADT-Auto.%20Our%20empirical%20results%20reveal%20that%20our%0AADT%20is%20highly%20effective%20on%20challenging%20the%20tokenization%20of%20leading%20LLMs%2C%0Aincluding%20GPT-4o%2C%20Llama-3%2C%20Deepseek-R1%20and%20so%20on%2C%20thus%20degrading%20these%20LLMs%27%0Acapabilities.%20Moreover%2C%20our%20method%20of%20automatic%20data%20generation%20has%20been%20proven%0Aefficient%20and%20robust%2C%20which%20can%20be%20applied%20to%20any%20open-source%20LLMs.%20In%20this%0Apaper%2C%20we%20substantially%20investigate%20LLMs%27%20vulnerability%20in%20terms%20of%20challenging%0Atheir%20token%20segmentation%2C%20which%20will%20shed%20light%20on%20the%20subsequent%20research%20of%0Aimproving%20LLMs%27%20capabilities%20through%20optimizing%20their%20tokenization%20process%20and%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenization%2520Matters%2521%2520Degrading%2520Large%2520Language%2520Models%2520through%250A%2520%2520Challenging%2520Their%2520Tokenization%26entry.906535625%3DDixuan%2520Wang%2520and%2520Yanda%2520Li%2520and%2520Junyuan%2520Jiang%2520and%2520Zepeng%2520Ding%2520and%2520Ziqin%2520Luo%2520and%2520Guochao%2520Jiang%2520and%2520Jiaqing%2520Liang%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520language%250Aunderstanding%2520and%2520generation.%2520Nonetheless%252C%2520it%2520was%2520also%2520witnessed%2520that%2520LLMs%2520tend%250Ato%2520produce%2520inaccurate%2520responses%2520to%2520specific%2520queries.%2520This%2520deficiency%2520can%2520be%250Atraced%2520to%2520the%2520tokenization%2520step%2520LLMs%2520must%2520undergo%252C%2520which%2520is%2520an%2520inevitable%250Alimitation%2520inherent%2520to%2520all%2520LLMs.%2520In%2520fact%252C%2520incorrect%2520tokenization%2520is%2520the%250Acritical%2520point%2520that%2520hinders%2520LLMs%2520in%2520understanding%2520the%2520input%2520precisely%252C%2520thus%250Aleading%2520to%2520unsatisfactory%2520output.%2520This%2520defect%2520is%2520more%2520obvious%2520in%2520Chinese%250Ascenarios.%2520To%2520demonstrate%2520this%2520flaw%2520of%2520LLMs%252C%2520we%2520construct%2520an%2520adversarial%250Adataset%252C%2520named%2520as%2520%2524%255Ctextbf%257BADT%2520%2528Adversarial%2520Dataset%2520for%2520Tokenizer%2529%257D%2524%252C%2520which%250Adraws%2520upon%2520the%2520vocabularies%2520of%2520various%2520open-source%2520LLMs%2520to%2520challenge%2520LLMs%2527%250Atokenization.%2520ADT%2520consists%2520of%2520two%2520subsets%253A%2520the%2520manually%2520constructed%2520ADT-Human%250Aand%2520the%2520automatically%2520generated%2520ADT-Auto.%2520Our%2520empirical%2520results%2520reveal%2520that%2520our%250AADT%2520is%2520highly%2520effective%2520on%2520challenging%2520the%2520tokenization%2520of%2520leading%2520LLMs%252C%250Aincluding%2520GPT-4o%252C%2520Llama-3%252C%2520Deepseek-R1%2520and%2520so%2520on%252C%2520thus%2520degrading%2520these%2520LLMs%2527%250Acapabilities.%2520Moreover%252C%2520our%2520method%2520of%2520automatic%2520data%2520generation%2520has%2520been%2520proven%250Aefficient%2520and%2520robust%252C%2520which%2520can%2520be%2520applied%2520to%2520any%2520open-source%2520LLMs.%2520In%2520this%250Apaper%252C%2520we%2520substantially%2520investigate%2520LLMs%2527%2520vulnerability%2520in%2520terms%2520of%2520challenging%250Atheir%2520token%2520segmentation%252C%2520which%2520will%2520shed%2520light%2520on%2520the%2520subsequent%2520research%2520of%250Aimproving%2520LLMs%2527%2520capabilities%2520through%2520optimizing%2520their%2520tokenization%2520process%2520and%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization&entry.906535625=Dixuan%20Wang%20and%20Yanda%20Li%20and%20Junyuan%20Jiang%20and%20Zepeng%20Ding%20and%20Ziqin%20Luo%20and%20Guochao%20Jiang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20it%20was%20also%20witnessed%20that%20LLMs%20tend%0Ato%20produce%20inaccurate%20responses%20to%20specific%20queries.%20This%20deficiency%20can%20be%0Atraced%20to%20the%20tokenization%20step%20LLMs%20must%20undergo%2C%20which%20is%20an%20inevitable%0Alimitation%20inherent%20to%20all%20LLMs.%20In%20fact%2C%20incorrect%20tokenization%20is%20the%0Acritical%20point%20that%20hinders%20LLMs%20in%20understanding%20the%20input%20precisely%2C%20thus%0Aleading%20to%20unsatisfactory%20output.%20This%20defect%20is%20more%20obvious%20in%20Chinese%0Ascenarios.%20To%20demonstrate%20this%20flaw%20of%20LLMs%2C%20we%20construct%20an%20adversarial%0Adataset%2C%20named%20as%20%24%5Ctextbf%7BADT%20%28Adversarial%20Dataset%20for%20Tokenizer%29%7D%24%2C%20which%0Adraws%20upon%20the%20vocabularies%20of%20various%20open-source%20LLMs%20to%20challenge%20LLMs%27%0Atokenization.%20ADT%20consists%20of%20two%20subsets%3A%20the%20manually%20constructed%20ADT-Human%0Aand%20the%20automatically%20generated%20ADT-Auto.%20Our%20empirical%20results%20reveal%20that%20our%0AADT%20is%20highly%20effective%20on%20challenging%20the%20tokenization%20of%20leading%20LLMs%2C%0Aincluding%20GPT-4o%2C%20Llama-3%2C%20Deepseek-R1%20and%20so%20on%2C%20thus%20degrading%20these%20LLMs%27%0Acapabilities.%20Moreover%2C%20our%20method%20of%20automatic%20data%20generation%20has%20been%20proven%0Aefficient%20and%20robust%2C%20which%20can%20be%20applied%20to%20any%20open-source%20LLMs.%20In%20this%0Apaper%2C%20we%20substantially%20investigate%20LLMs%27%20vulnerability%20in%20terms%20of%20challenging%0Atheir%20token%20segmentation%2C%20which%20will%20shed%20light%20on%20the%20subsequent%20research%20of%0Aimproving%20LLMs%27%20capabilities%20through%20optimizing%20their%20tokenization%20process%20and%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17067v2&entry.124074799=Read"},
{"title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "author": "Pavel Korotaev and Petr Surovtsev and Alexander Kapitanov and Karina Kvanchiani and Aleksandr Nagaev", "abstract": "  Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.\n", "link": "http://arxiv.org/abs/2505.10267v1", "date": "2025-05-15", "relevancy": 2.5774, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandReader%3A%20Advanced%20Techniques%20for%20Efficient%20Fingerspelling%20Recognition&body=Title%3A%20HandReader%3A%20Advanced%20Techniques%20for%20Efficient%20Fingerspelling%20Recognition%0AAuthor%3A%20Pavel%20Korotaev%20and%20Petr%20Surovtsev%20and%20Alexander%20Kapitanov%20and%20Karina%20Kvanchiani%20and%20Aleksandr%20Nagaev%0AAbstract%3A%20%20%20Fingerspelling%20is%20a%20significant%20component%20of%20Sign%20Language%20%28SL%29%2C%20allowing%20the%0Ainterpretation%20of%20proper%20names%2C%20characterized%20by%20fast%20hand%20movements%20during%0Asigning.%20Although%20previous%20works%20on%20fingerspelling%20recognition%20have%20focused%20on%0Aprocessing%20the%20temporal%20dimension%20of%20videos%2C%20there%20remains%20room%20for%20improving%0Athe%20accuracy%20of%20these%20approaches.%20This%20paper%20introduces%20HandReader%2C%20a%20group%20of%0Athree%20architectures%20designed%20to%20address%20the%20fingerspelling%20recognition%20task.%0AHandReader%24_%7BRGB%7D%24%20employs%20the%20novel%20Temporal%20Shift-Adaptive%20Module%20%28TSAM%29%20to%0Aprocess%20RGB%20features%20from%20videos%20of%20varying%20lengths%20while%20preserving%20important%0Asequential%20information.%20HandReader%24_%7BKP%7D%24%20is%20built%20on%20the%20proposed%20Temporal%0APose%20Encoder%20%28TPE%29%20operated%20on%20keypoints%20as%20tensors.%20Such%20keypoints%20composition%0Ain%20a%20batch%20allows%20the%20encoder%20to%20pass%20them%20through%202D%20and%203D%20convolution%0Alayers%2C%20utilizing%20temporal%20and%20spatial%20information%20and%20accumulating%20keypoints%0Acoordinates.%20We%20also%20introduce%20HandReader_RGB%2BKP%20-%20architecture%20with%20a%20joint%0Aencoder%20to%20benefit%20from%20RGB%20and%20keypoint%20modalities.%20Each%20HandReader%20model%0Apossesses%20distinct%20advantages%20and%20achieves%20state-of-the-art%20results%20on%20the%0AChicagoFSWild%20and%20ChicagoFSWild%2B%20datasets.%20Moreover%2C%20the%20models%20demonstrate%0Ahigh%20performance%20on%20the%20first%20open%20dataset%20for%20Russian%20fingerspelling%2C%20Znaki%2C%0Apresented%20in%20this%20paper.%20The%20Znaki%20dataset%20and%20HandReader%20pre-trained%20models%0Aare%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandReader%253A%2520Advanced%2520Techniques%2520for%2520Efficient%2520Fingerspelling%2520Recognition%26entry.906535625%3DPavel%2520Korotaev%2520and%2520Petr%2520Surovtsev%2520and%2520Alexander%2520Kapitanov%2520and%2520Karina%2520Kvanchiani%2520and%2520Aleksandr%2520Nagaev%26entry.1292438233%3D%2520%2520Fingerspelling%2520is%2520a%2520significant%2520component%2520of%2520Sign%2520Language%2520%2528SL%2529%252C%2520allowing%2520the%250Ainterpretation%2520of%2520proper%2520names%252C%2520characterized%2520by%2520fast%2520hand%2520movements%2520during%250Asigning.%2520Although%2520previous%2520works%2520on%2520fingerspelling%2520recognition%2520have%2520focused%2520on%250Aprocessing%2520the%2520temporal%2520dimension%2520of%2520videos%252C%2520there%2520remains%2520room%2520for%2520improving%250Athe%2520accuracy%2520of%2520these%2520approaches.%2520This%2520paper%2520introduces%2520HandReader%252C%2520a%2520group%2520of%250Athree%2520architectures%2520designed%2520to%2520address%2520the%2520fingerspelling%2520recognition%2520task.%250AHandReader%2524_%257BRGB%257D%2524%2520employs%2520the%2520novel%2520Temporal%2520Shift-Adaptive%2520Module%2520%2528TSAM%2529%2520to%250Aprocess%2520RGB%2520features%2520from%2520videos%2520of%2520varying%2520lengths%2520while%2520preserving%2520important%250Asequential%2520information.%2520HandReader%2524_%257BKP%257D%2524%2520is%2520built%2520on%2520the%2520proposed%2520Temporal%250APose%2520Encoder%2520%2528TPE%2529%2520operated%2520on%2520keypoints%2520as%2520tensors.%2520Such%2520keypoints%2520composition%250Ain%2520a%2520batch%2520allows%2520the%2520encoder%2520to%2520pass%2520them%2520through%25202D%2520and%25203D%2520convolution%250Alayers%252C%2520utilizing%2520temporal%2520and%2520spatial%2520information%2520and%2520accumulating%2520keypoints%250Acoordinates.%2520We%2520also%2520introduce%2520HandReader_RGB%252BKP%2520-%2520architecture%2520with%2520a%2520joint%250Aencoder%2520to%2520benefit%2520from%2520RGB%2520and%2520keypoint%2520modalities.%2520Each%2520HandReader%2520model%250Apossesses%2520distinct%2520advantages%2520and%2520achieves%2520state-of-the-art%2520results%2520on%2520the%250AChicagoFSWild%2520and%2520ChicagoFSWild%252B%2520datasets.%2520Moreover%252C%2520the%2520models%2520demonstrate%250Ahigh%2520performance%2520on%2520the%2520first%2520open%2520dataset%2520for%2520Russian%2520fingerspelling%252C%2520Znaki%252C%250Apresented%2520in%2520this%2520paper.%2520The%2520Znaki%2520dataset%2520and%2520HandReader%2520pre-trained%2520models%250Aare%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandReader%3A%20Advanced%20Techniques%20for%20Efficient%20Fingerspelling%20Recognition&entry.906535625=Pavel%20Korotaev%20and%20Petr%20Surovtsev%20and%20Alexander%20Kapitanov%20and%20Karina%20Kvanchiani%20and%20Aleksandr%20Nagaev&entry.1292438233=%20%20Fingerspelling%20is%20a%20significant%20component%20of%20Sign%20Language%20%28SL%29%2C%20allowing%20the%0Ainterpretation%20of%20proper%20names%2C%20characterized%20by%20fast%20hand%20movements%20during%0Asigning.%20Although%20previous%20works%20on%20fingerspelling%20recognition%20have%20focused%20on%0Aprocessing%20the%20temporal%20dimension%20of%20videos%2C%20there%20remains%20room%20for%20improving%0Athe%20accuracy%20of%20these%20approaches.%20This%20paper%20introduces%20HandReader%2C%20a%20group%20of%0Athree%20architectures%20designed%20to%20address%20the%20fingerspelling%20recognition%20task.%0AHandReader%24_%7BRGB%7D%24%20employs%20the%20novel%20Temporal%20Shift-Adaptive%20Module%20%28TSAM%29%20to%0Aprocess%20RGB%20features%20from%20videos%20of%20varying%20lengths%20while%20preserving%20important%0Asequential%20information.%20HandReader%24_%7BKP%7D%24%20is%20built%20on%20the%20proposed%20Temporal%0APose%20Encoder%20%28TPE%29%20operated%20on%20keypoints%20as%20tensors.%20Such%20keypoints%20composition%0Ain%20a%20batch%20allows%20the%20encoder%20to%20pass%20them%20through%202D%20and%203D%20convolution%0Alayers%2C%20utilizing%20temporal%20and%20spatial%20information%20and%20accumulating%20keypoints%0Acoordinates.%20We%20also%20introduce%20HandReader_RGB%2BKP%20-%20architecture%20with%20a%20joint%0Aencoder%20to%20benefit%20from%20RGB%20and%20keypoint%20modalities.%20Each%20HandReader%20model%0Apossesses%20distinct%20advantages%20and%20achieves%20state-of-the-art%20results%20on%20the%0AChicagoFSWild%20and%20ChicagoFSWild%2B%20datasets.%20Moreover%2C%20the%20models%20demonstrate%0Ahigh%20performance%20on%20the%20first%20open%20dataset%20for%20Russian%20fingerspelling%2C%20Znaki%2C%0Apresented%20in%20this%20paper.%20The%20Znaki%20dataset%20and%20HandReader%20pre-trained%20models%0Aare%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10267v1&entry.124074799=Read"},
{"title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs", "author": "Jingyao Wang and Wenwen Qiang and Zeen Song and Changwen Zheng and Hui Xiong", "abstract": "  Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.\n", "link": "http://arxiv.org/abs/2505.10425v1", "date": "2025-05-15", "relevancy": 2.5478, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Think%3A%20Information-Theoretic%20Reinforcement%20Fine-Tuning%20for%0A%20%20LLMs&body=Title%3A%20Learning%20to%20Think%3A%20Information-Theoretic%20Reinforcement%20Fine-Tuning%20for%0A%20%20LLMs%0AAuthor%3A%20Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Zeen%20Song%20and%20Changwen%20Zheng%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20complex%20tasks%20thanks%20to%20advances%20in%0Areasoning%20abilities.%20However%2C%20existing%20methods%20overlook%20the%20trade-off%20between%0Areasoning%20effectiveness%20and%20computational%20efficiency%2C%20often%20encouraging%0Aunnecessarily%20long%20reasoning%20chains%20and%20wasting%20tokens.%20To%20address%20this%2C%20we%0Apropose%20Learning%20to%20Think%20%28L2T%29%2C%20an%20information-theoretic%20reinforcement%0Afine-tuning%20framework%20for%20LLMs%20to%20make%20the%20models%20achieve%20optimal%20reasoning%0Awith%20fewer%20tokens.%20Specifically%2C%20L2T%20treats%20each%20query-response%20interaction%20as%0Aa%20hierarchical%20session%20of%20multiple%20episodes%20and%20proposes%20a%20universal%20dense%0Aprocess%20reward%2C%20i.e.%2C%20quantifies%20the%20episode-wise%20information%20gain%20in%0Aparameters%2C%20requiring%20no%20extra%20annotations%20or%20task-specific%20evaluators.%20We%0Apropose%20a%20method%20to%20quickly%20estimate%20this%20reward%20based%20on%20PAC-Bayes%20bounds%20and%0Athe%20Fisher%20information%20matrix.%20Theoretical%20analyses%20show%20that%20it%20significantly%0Areduces%20computational%20complexity%20with%20high%20estimation%20accuracy.%20By%20immediately%0Arewarding%20each%20episode%27s%20contribution%20and%20penalizing%20excessive%20updates%2C%20L2T%0Aoptimizes%20the%20model%20via%20reinforcement%20learning%20to%20maximize%20the%20use%20of%20each%0Aepisode%20and%20achieve%20effective%20updates.%20Empirical%20results%20on%20various%20reasoning%0Abenchmarks%20and%20base%20models%20demonstrate%20the%20advantage%20of%20L2T%20across%20different%0Atasks%2C%20boosting%20both%20reasoning%20effectiveness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Think%253A%2520Information-Theoretic%2520Reinforcement%2520Fine-Tuning%2520for%250A%2520%2520LLMs%26entry.906535625%3DJingyao%2520Wang%2520and%2520Wenwen%2520Qiang%2520and%2520Zeen%2520Song%2520and%2520Changwen%2520Zheng%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520complex%2520tasks%2520thanks%2520to%2520advances%2520in%250Areasoning%2520abilities.%2520However%252C%2520existing%2520methods%2520overlook%2520the%2520trade-off%2520between%250Areasoning%2520effectiveness%2520and%2520computational%2520efficiency%252C%2520often%2520encouraging%250Aunnecessarily%2520long%2520reasoning%2520chains%2520and%2520wasting%2520tokens.%2520To%2520address%2520this%252C%2520we%250Apropose%2520Learning%2520to%2520Think%2520%2528L2T%2529%252C%2520an%2520information-theoretic%2520reinforcement%250Afine-tuning%2520framework%2520for%2520LLMs%2520to%2520make%2520the%2520models%2520achieve%2520optimal%2520reasoning%250Awith%2520fewer%2520tokens.%2520Specifically%252C%2520L2T%2520treats%2520each%2520query-response%2520interaction%2520as%250Aa%2520hierarchical%2520session%2520of%2520multiple%2520episodes%2520and%2520proposes%2520a%2520universal%2520dense%250Aprocess%2520reward%252C%2520i.e.%252C%2520quantifies%2520the%2520episode-wise%2520information%2520gain%2520in%250Aparameters%252C%2520requiring%2520no%2520extra%2520annotations%2520or%2520task-specific%2520evaluators.%2520We%250Apropose%2520a%2520method%2520to%2520quickly%2520estimate%2520this%2520reward%2520based%2520on%2520PAC-Bayes%2520bounds%2520and%250Athe%2520Fisher%2520information%2520matrix.%2520Theoretical%2520analyses%2520show%2520that%2520it%2520significantly%250Areduces%2520computational%2520complexity%2520with%2520high%2520estimation%2520accuracy.%2520By%2520immediately%250Arewarding%2520each%2520episode%2527s%2520contribution%2520and%2520penalizing%2520excessive%2520updates%252C%2520L2T%250Aoptimizes%2520the%2520model%2520via%2520reinforcement%2520learning%2520to%2520maximize%2520the%2520use%2520of%2520each%250Aepisode%2520and%2520achieve%2520effective%2520updates.%2520Empirical%2520results%2520on%2520various%2520reasoning%250Abenchmarks%2520and%2520base%2520models%2520demonstrate%2520the%2520advantage%2520of%2520L2T%2520across%2520different%250Atasks%252C%2520boosting%2520both%2520reasoning%2520effectiveness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Think%3A%20Information-Theoretic%20Reinforcement%20Fine-Tuning%20for%0A%20%20LLMs&entry.906535625=Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Zeen%20Song%20and%20Changwen%20Zheng%20and%20Hui%20Xiong&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20complex%20tasks%20thanks%20to%20advances%20in%0Areasoning%20abilities.%20However%2C%20existing%20methods%20overlook%20the%20trade-off%20between%0Areasoning%20effectiveness%20and%20computational%20efficiency%2C%20often%20encouraging%0Aunnecessarily%20long%20reasoning%20chains%20and%20wasting%20tokens.%20To%20address%20this%2C%20we%0Apropose%20Learning%20to%20Think%20%28L2T%29%2C%20an%20information-theoretic%20reinforcement%0Afine-tuning%20framework%20for%20LLMs%20to%20make%20the%20models%20achieve%20optimal%20reasoning%0Awith%20fewer%20tokens.%20Specifically%2C%20L2T%20treats%20each%20query-response%20interaction%20as%0Aa%20hierarchical%20session%20of%20multiple%20episodes%20and%20proposes%20a%20universal%20dense%0Aprocess%20reward%2C%20i.e.%2C%20quantifies%20the%20episode-wise%20information%20gain%20in%0Aparameters%2C%20requiring%20no%20extra%20annotations%20or%20task-specific%20evaluators.%20We%0Apropose%20a%20method%20to%20quickly%20estimate%20this%20reward%20based%20on%20PAC-Bayes%20bounds%20and%0Athe%20Fisher%20information%20matrix.%20Theoretical%20analyses%20show%20that%20it%20significantly%0Areduces%20computational%20complexity%20with%20high%20estimation%20accuracy.%20By%20immediately%0Arewarding%20each%20episode%27s%20contribution%20and%20penalizing%20excessive%20updates%2C%20L2T%0Aoptimizes%20the%20model%20via%20reinforcement%20learning%20to%20maximize%20the%20use%20of%20each%0Aepisode%20and%20achieve%20effective%20updates.%20Empirical%20results%20on%20various%20reasoning%0Abenchmarks%20and%20base%20models%20demonstrate%20the%20advantage%20of%20L2T%20across%20different%0Atasks%2C%20boosting%20both%20reasoning%20effectiveness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10425v1&entry.124074799=Read"},
{"title": "An Introduction to Discrete Variational Autoencoders", "author": "Alan Jeffares and Liyuan Liu", "abstract": "  Variational Autoencoders (VAEs) are well-established as a principled approach\nto probabilistic unsupervised learning with neural networks. Typically, an\nencoder network defines the parameters of a Gaussian distributed latent space\nfrom which we can sample and pass realizations to a decoder network. This model\nis trained to reconstruct its inputs and is optimized through the evidence\nlower bound. In recent years, discrete latent spaces have grown in popularity,\nsuggesting that they may be a natural choice for many data modalities (e.g.\ntext). In this tutorial, we provide a rigorous, yet practical, introduction to\ndiscrete variational autoencoders -- specifically, VAEs in which the latent\nspace is made up of latent variables that follow a categorical distribution. We\nassume only a basic mathematical background with which we carefully derive each\nstep from first principles. From there, we develop a concrete training recipe\nand provide an example implementation, hosted at\nhttps://github.com/alanjeffares/discreteVAE.\n", "link": "http://arxiv.org/abs/2505.10344v1", "date": "2025-05-15", "relevancy": 2.5142, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5409}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4903}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Introduction%20to%20Discrete%20Variational%20Autoencoders&body=Title%3A%20An%20Introduction%20to%20Discrete%20Variational%20Autoencoders%0AAuthor%3A%20Alan%20Jeffares%20and%20Liyuan%20Liu%0AAbstract%3A%20%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20well-established%20as%20a%20principled%20approach%0Ato%20probabilistic%20unsupervised%20learning%20with%20neural%20networks.%20Typically%2C%20an%0Aencoder%20network%20defines%20the%20parameters%20of%20a%20Gaussian%20distributed%20latent%20space%0Afrom%20which%20we%20can%20sample%20and%20pass%20realizations%20to%20a%20decoder%20network.%20This%20model%0Ais%20trained%20to%20reconstruct%20its%20inputs%20and%20is%20optimized%20through%20the%20evidence%0Alower%20bound.%20In%20recent%20years%2C%20discrete%20latent%20spaces%20have%20grown%20in%20popularity%2C%0Asuggesting%20that%20they%20may%20be%20a%20natural%20choice%20for%20many%20data%20modalities%20%28e.g.%0Atext%29.%20In%20this%20tutorial%2C%20we%20provide%20a%20rigorous%2C%20yet%20practical%2C%20introduction%20to%0Adiscrete%20variational%20autoencoders%20--%20specifically%2C%20VAEs%20in%20which%20the%20latent%0Aspace%20is%20made%20up%20of%20latent%20variables%20that%20follow%20a%20categorical%20distribution.%20We%0Aassume%20only%20a%20basic%20mathematical%20background%20with%20which%20we%20carefully%20derive%20each%0Astep%20from%20first%20principles.%20From%20there%2C%20we%20develop%20a%20concrete%20training%20recipe%0Aand%20provide%20an%20example%20implementation%2C%20hosted%20at%0Ahttps%3A//github.com/alanjeffares/discreteVAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Introduction%2520to%2520Discrete%2520Variational%2520Autoencoders%26entry.906535625%3DAlan%2520Jeffares%2520and%2520Liyuan%2520Liu%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520well-established%2520as%2520a%2520principled%2520approach%250Ato%2520probabilistic%2520unsupervised%2520learning%2520with%2520neural%2520networks.%2520Typically%252C%2520an%250Aencoder%2520network%2520defines%2520the%2520parameters%2520of%2520a%2520Gaussian%2520distributed%2520latent%2520space%250Afrom%2520which%2520we%2520can%2520sample%2520and%2520pass%2520realizations%2520to%2520a%2520decoder%2520network.%2520This%2520model%250Ais%2520trained%2520to%2520reconstruct%2520its%2520inputs%2520and%2520is%2520optimized%2520through%2520the%2520evidence%250Alower%2520bound.%2520In%2520recent%2520years%252C%2520discrete%2520latent%2520spaces%2520have%2520grown%2520in%2520popularity%252C%250Asuggesting%2520that%2520they%2520may%2520be%2520a%2520natural%2520choice%2520for%2520many%2520data%2520modalities%2520%2528e.g.%250Atext%2529.%2520In%2520this%2520tutorial%252C%2520we%2520provide%2520a%2520rigorous%252C%2520yet%2520practical%252C%2520introduction%2520to%250Adiscrete%2520variational%2520autoencoders%2520--%2520specifically%252C%2520VAEs%2520in%2520which%2520the%2520latent%250Aspace%2520is%2520made%2520up%2520of%2520latent%2520variables%2520that%2520follow%2520a%2520categorical%2520distribution.%2520We%250Aassume%2520only%2520a%2520basic%2520mathematical%2520background%2520with%2520which%2520we%2520carefully%2520derive%2520each%250Astep%2520from%2520first%2520principles.%2520From%2520there%252C%2520we%2520develop%2520a%2520concrete%2520training%2520recipe%250Aand%2520provide%2520an%2520example%2520implementation%252C%2520hosted%2520at%250Ahttps%253A//github.com/alanjeffares/discreteVAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Introduction%20to%20Discrete%20Variational%20Autoencoders&entry.906535625=Alan%20Jeffares%20and%20Liyuan%20Liu&entry.1292438233=%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20well-established%20as%20a%20principled%20approach%0Ato%20probabilistic%20unsupervised%20learning%20with%20neural%20networks.%20Typically%2C%20an%0Aencoder%20network%20defines%20the%20parameters%20of%20a%20Gaussian%20distributed%20latent%20space%0Afrom%20which%20we%20can%20sample%20and%20pass%20realizations%20to%20a%20decoder%20network.%20This%20model%0Ais%20trained%20to%20reconstruct%20its%20inputs%20and%20is%20optimized%20through%20the%20evidence%0Alower%20bound.%20In%20recent%20years%2C%20discrete%20latent%20spaces%20have%20grown%20in%20popularity%2C%0Asuggesting%20that%20they%20may%20be%20a%20natural%20choice%20for%20many%20data%20modalities%20%28e.g.%0Atext%29.%20In%20this%20tutorial%2C%20we%20provide%20a%20rigorous%2C%20yet%20practical%2C%20introduction%20to%0Adiscrete%20variational%20autoencoders%20--%20specifically%2C%20VAEs%20in%20which%20the%20latent%0Aspace%20is%20made%20up%20of%20latent%20variables%20that%20follow%20a%20categorical%20distribution.%20We%0Aassume%20only%20a%20basic%20mathematical%20background%20with%20which%20we%20carefully%20derive%20each%0Astep%20from%20first%20principles.%20From%20there%2C%20we%20develop%20a%20concrete%20training%20recipe%0Aand%20provide%20an%20example%20implementation%2C%20hosted%20at%0Ahttps%3A//github.com/alanjeffares/discreteVAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10344v1&entry.124074799=Read"},
{"title": "The Mosaic Memory of Large Language Models", "author": "Igor Shilov and Matthieu Meeus and Yves-Alexandre de Montjoye", "abstract": "  As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation.\n", "link": "http://arxiv.org/abs/2405.15523v2", "date": "2025-05-15", "relevancy": 2.4793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Mosaic%20Memory%20of%20Large%20Language%20Models&body=Title%3A%20The%20Mosaic%20Memory%20of%20Large%20Language%20Models%0AAuthor%3A%20Igor%20Shilov%20and%20Matthieu%20Meeus%20and%20Yves-Alexandre%20de%20Montjoye%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20widely%20adopted%2C%20understanding%20how%20they%0Alearn%20from%2C%20and%20memorize%2C%20training%20data%20becomes%20crucial.%20Memorization%20in%20LLMs%0Ais%20widely%20assumed%20to%20only%20occur%20as%20a%20result%20of%20sequences%20being%20repeated%20in%20the%0Atraining%20data.%20Instead%2C%20we%20show%20that%20LLMs%20memorize%20by%20assembling%20information%0Afrom%20similar%20sequences%2C%20a%20phenomena%20we%20call%20mosaic%20memory.%20We%20show%20major%20LLMs%0Ato%20exhibit%20mosaic%20memory%2C%20with%20fuzzy%20duplicates%20contributing%20to%20memorization%20as%0Amuch%20as%200.8%20of%20an%20exact%20duplicate%20and%20even%20heavily%20modified%20sequences%0Acontributing%20substantially%20to%20memorization.%20Despite%20models%20display%20reasoning%0Acapabilities%2C%20we%20somewhat%20surprisingly%20show%20memorization%20to%20be%20predominantly%0Asyntactic%20rather%20than%20semantic.%20We%20finally%20show%20fuzzy%20duplicates%20to%20be%0Aubiquitous%20in%20real-world%20data%2C%20untouched%20by%20deduplication%20techniques.%20Taken%0Atogether%2C%20our%20results%20challenge%20widely%20held%20beliefs%20and%20show%20memorization%20to%20be%0Aa%20more%20complex%2C%20mosaic%20process%2C%20with%20real-world%20implications%20for%20privacy%2C%0Aconfidentiality%2C%20model%20utility%20and%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Mosaic%2520Memory%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DIgor%2520Shilov%2520and%2520Matthieu%2520Meeus%2520and%2520Yves-Alexandre%2520de%2520Montjoye%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520widely%2520adopted%252C%2520understanding%2520how%2520they%250Alearn%2520from%252C%2520and%2520memorize%252C%2520training%2520data%2520becomes%2520crucial.%2520Memorization%2520in%2520LLMs%250Ais%2520widely%2520assumed%2520to%2520only%2520occur%2520as%2520a%2520result%2520of%2520sequences%2520being%2520repeated%2520in%2520the%250Atraining%2520data.%2520Instead%252C%2520we%2520show%2520that%2520LLMs%2520memorize%2520by%2520assembling%2520information%250Afrom%2520similar%2520sequences%252C%2520a%2520phenomena%2520we%2520call%2520mosaic%2520memory.%2520We%2520show%2520major%2520LLMs%250Ato%2520exhibit%2520mosaic%2520memory%252C%2520with%2520fuzzy%2520duplicates%2520contributing%2520to%2520memorization%2520as%250Amuch%2520as%25200.8%2520of%2520an%2520exact%2520duplicate%2520and%2520even%2520heavily%2520modified%2520sequences%250Acontributing%2520substantially%2520to%2520memorization.%2520Despite%2520models%2520display%2520reasoning%250Acapabilities%252C%2520we%2520somewhat%2520surprisingly%2520show%2520memorization%2520to%2520be%2520predominantly%250Asyntactic%2520rather%2520than%2520semantic.%2520We%2520finally%2520show%2520fuzzy%2520duplicates%2520to%2520be%250Aubiquitous%2520in%2520real-world%2520data%252C%2520untouched%2520by%2520deduplication%2520techniques.%2520Taken%250Atogether%252C%2520our%2520results%2520challenge%2520widely%2520held%2520beliefs%2520and%2520show%2520memorization%2520to%2520be%250Aa%2520more%2520complex%252C%2520mosaic%2520process%252C%2520with%2520real-world%2520implications%2520for%2520privacy%252C%250Aconfidentiality%252C%2520model%2520utility%2520and%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Mosaic%20Memory%20of%20Large%20Language%20Models&entry.906535625=Igor%20Shilov%20and%20Matthieu%20Meeus%20and%20Yves-Alexandre%20de%20Montjoye&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20widely%20adopted%2C%20understanding%20how%20they%0Alearn%20from%2C%20and%20memorize%2C%20training%20data%20becomes%20crucial.%20Memorization%20in%20LLMs%0Ais%20widely%20assumed%20to%20only%20occur%20as%20a%20result%20of%20sequences%20being%20repeated%20in%20the%0Atraining%20data.%20Instead%2C%20we%20show%20that%20LLMs%20memorize%20by%20assembling%20information%0Afrom%20similar%20sequences%2C%20a%20phenomena%20we%20call%20mosaic%20memory.%20We%20show%20major%20LLMs%0Ato%20exhibit%20mosaic%20memory%2C%20with%20fuzzy%20duplicates%20contributing%20to%20memorization%20as%0Amuch%20as%200.8%20of%20an%20exact%20duplicate%20and%20even%20heavily%20modified%20sequences%0Acontributing%20substantially%20to%20memorization.%20Despite%20models%20display%20reasoning%0Acapabilities%2C%20we%20somewhat%20surprisingly%20show%20memorization%20to%20be%20predominantly%0Asyntactic%20rather%20than%20semantic.%20We%20finally%20show%20fuzzy%20duplicates%20to%20be%0Aubiquitous%20in%20real-world%20data%2C%20untouched%20by%20deduplication%20techniques.%20Taken%0Atogether%2C%20our%20results%20challenge%20widely%20held%20beliefs%20and%20show%20memorization%20to%20be%0Aa%20more%20complex%2C%20mosaic%20process%2C%20with%20real-world%20implications%20for%20privacy%2C%0Aconfidentiality%2C%20model%20utility%20and%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15523v2&entry.124074799=Read"},
{"title": "Negative Metric Learning for Graphs", "author": "Yiyang Zhao and Chengpei Wu and Lilin Zhang and Ning Yang", "abstract": "  Graph contrastive learning (GCL) often suffers from false negatives, which\ndegrades the performance on downstream tasks. The existing methods addressing\nthe false negative issue usually rely on human prior knowledge, still leading\nGCL to suboptimal results. In this paper, we propose a novel Negative Metric\nLearning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative\nMetric Network (NMN) to build a negative metric space, in which false negatives\ncan be distinguished better from true negatives based on their distance to\nanchor node. To overcome the lack of explicit supervision signals for NML, we\npropose a joint training scheme with bi-level optimization objective, which\nimplicitly utilizes the self-supervision signals to iteratively optimize the\nencoder and the negative metric network. The solid theoretical analysis and the\nextensive experiments conducted on widely used benchmarks verify the\nsuperiority of the proposed method.\n", "link": "http://arxiv.org/abs/2505.10307v1", "date": "2025-05-15", "relevancy": 2.4507, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.484}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Negative%20Metric%20Learning%20for%20Graphs&body=Title%3A%20Negative%20Metric%20Learning%20for%20Graphs%0AAuthor%3A%20Yiyang%20Zhao%20and%20Chengpei%20Wu%20and%20Lilin%20Zhang%20and%20Ning%20Yang%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20often%20suffers%20from%20false%20negatives%2C%20which%0Adegrades%20the%20performance%20on%20downstream%20tasks.%20The%20existing%20methods%20addressing%0Athe%20false%20negative%20issue%20usually%20rely%20on%20human%20prior%20knowledge%2C%20still%20leading%0AGCL%20to%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Negative%20Metric%0ALearning%20%28NML%29%20enhanced%20GCL%20%28NML-GCL%29.%20NML-GCL%20employs%20a%20learnable%20Negative%0AMetric%20Network%20%28NMN%29%20to%20build%20a%20negative%20metric%20space%2C%20in%20which%20false%20negatives%0Acan%20be%20distinguished%20better%20from%20true%20negatives%20based%20on%20their%20distance%20to%0Aanchor%20node.%20To%20overcome%20the%20lack%20of%20explicit%20supervision%20signals%20for%20NML%2C%20we%0Apropose%20a%20joint%20training%20scheme%20with%20bi-level%20optimization%20objective%2C%20which%0Aimplicitly%20utilizes%20the%20self-supervision%20signals%20to%20iteratively%20optimize%20the%0Aencoder%20and%20the%20negative%20metric%20network.%20The%20solid%20theoretical%20analysis%20and%20the%0Aextensive%20experiments%20conducted%20on%20widely%20used%20benchmarks%20verify%20the%0Asuperiority%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNegative%2520Metric%2520Learning%2520for%2520Graphs%26entry.906535625%3DYiyang%2520Zhao%2520and%2520Chengpei%2520Wu%2520and%2520Lilin%2520Zhang%2520and%2520Ning%2520Yang%26entry.1292438233%3D%2520%2520Graph%2520contrastive%2520learning%2520%2528GCL%2529%2520often%2520suffers%2520from%2520false%2520negatives%252C%2520which%250Adegrades%2520the%2520performance%2520on%2520downstream%2520tasks.%2520The%2520existing%2520methods%2520addressing%250Athe%2520false%2520negative%2520issue%2520usually%2520rely%2520on%2520human%2520prior%2520knowledge%252C%2520still%2520leading%250AGCL%2520to%2520suboptimal%2520results.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Negative%2520Metric%250ALearning%2520%2528NML%2529%2520enhanced%2520GCL%2520%2528NML-GCL%2529.%2520NML-GCL%2520employs%2520a%2520learnable%2520Negative%250AMetric%2520Network%2520%2528NMN%2529%2520to%2520build%2520a%2520negative%2520metric%2520space%252C%2520in%2520which%2520false%2520negatives%250Acan%2520be%2520distinguished%2520better%2520from%2520true%2520negatives%2520based%2520on%2520their%2520distance%2520to%250Aanchor%2520node.%2520To%2520overcome%2520the%2520lack%2520of%2520explicit%2520supervision%2520signals%2520for%2520NML%252C%2520we%250Apropose%2520a%2520joint%2520training%2520scheme%2520with%2520bi-level%2520optimization%2520objective%252C%2520which%250Aimplicitly%2520utilizes%2520the%2520self-supervision%2520signals%2520to%2520iteratively%2520optimize%2520the%250Aencoder%2520and%2520the%2520negative%2520metric%2520network.%2520The%2520solid%2520theoretical%2520analysis%2520and%2520the%250Aextensive%2520experiments%2520conducted%2520on%2520widely%2520used%2520benchmarks%2520verify%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Negative%20Metric%20Learning%20for%20Graphs&entry.906535625=Yiyang%20Zhao%20and%20Chengpei%20Wu%20and%20Lilin%20Zhang%20and%20Ning%20Yang&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20often%20suffers%20from%20false%20negatives%2C%20which%0Adegrades%20the%20performance%20on%20downstream%20tasks.%20The%20existing%20methods%20addressing%0Athe%20false%20negative%20issue%20usually%20rely%20on%20human%20prior%20knowledge%2C%20still%20leading%0AGCL%20to%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Negative%20Metric%0ALearning%20%28NML%29%20enhanced%20GCL%20%28NML-GCL%29.%20NML-GCL%20employs%20a%20learnable%20Negative%0AMetric%20Network%20%28NMN%29%20to%20build%20a%20negative%20metric%20space%2C%20in%20which%20false%20negatives%0Acan%20be%20distinguished%20better%20from%20true%20negatives%20based%20on%20their%20distance%20to%0Aanchor%20node.%20To%20overcome%20the%20lack%20of%20explicit%20supervision%20signals%20for%20NML%2C%20we%0Apropose%20a%20joint%20training%20scheme%20with%20bi-level%20optimization%20objective%2C%20which%0Aimplicitly%20utilizes%20the%20self-supervision%20signals%20to%20iteratively%20optimize%20the%0Aencoder%20and%20the%20negative%20metric%20network.%20The%20solid%20theoretical%20analysis%20and%20the%0Aextensive%20experiments%20conducted%20on%20widely%20used%20benchmarks%20verify%20the%0Asuperiority%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10307v1&entry.124074799=Read"},
{"title": "SEAL: Searching Expandable Architectures for Incremental Learning", "author": "Matteo Gambella and Vicente Javier Castro Solar and Manuel Roveri", "abstract": "  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n", "link": "http://arxiv.org/abs/2505.10457v1", "date": "2025-05-15", "relevancy": 2.4352, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4879}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning&body=Title%3A%20SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning%0AAuthor%3A%20Matteo%20Gambella%20and%20Vicente%20Javier%20Castro%20Solar%20and%20Manuel%20Roveri%0AAbstract%3A%20%20%20Incremental%20learning%20is%20a%20machine%20learning%20paradigm%20where%20a%20model%20learns%20from%0Aa%20sequential%20stream%20of%20tasks.%20This%20setting%20poses%20a%20key%20challenge%3A%20balancing%0Aplasticity%20%28learning%20new%20tasks%29%20and%20stability%20%28preserving%20past%20knowledge%29.%0ANeural%20Architecture%20Search%20%28NAS%29%2C%20a%20branch%20of%20AutoML%2C%20automates%20the%20design%20of%0Athe%20architecture%20of%20Deep%20Neural%20Networks%20and%20has%20shown%20success%20in%20static%0Asettings.%20However%2C%20existing%20NAS-based%20approaches%20to%20incremental%20learning%20often%0Arely%20on%20expanding%20the%20model%20at%20every%20task%2C%20making%20them%20impractical%20in%0Aresource-constrained%20environments.%20In%20this%20work%2C%20we%20introduce%20SEAL%2C%20a%20NAS-based%0Aframework%20tailored%20for%20data-incremental%20learning%2C%20a%20scenario%20where%20disjoint%0Adata%20samples%20arrive%20sequentially%20and%20are%20not%20stored%20for%20future%20access.%20SEAL%0Aadapts%20the%20model%20structure%20dynamically%20by%20expanding%20it%20only%20when%20necessary%2C%0Abased%20on%20a%20capacity%20estimation%20metric.%20Stability%20is%20preserved%20through%0Across-distillation%20training%20after%20each%20expansion%20step.%20The%20NAS%20component%0Ajointly%20searches%20for%20both%20the%20architecture%20and%20the%20optimal%20expansion%20policy.%0AExperiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SEAL%20effectively%0Areduces%20forgetting%20and%20enhances%20accuracy%20while%20maintaining%20a%20lower%20model%20size%0Acompared%20to%20prior%20methods.%20These%20results%20highlight%20the%20promise%20of%20combining%20NAS%0Aand%20selective%20expansion%20for%20efficient%2C%20adaptive%20learning%20in%20incremental%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Searching%2520Expandable%2520Architectures%2520for%2520Incremental%2520Learning%26entry.906535625%3DMatteo%2520Gambella%2520and%2520Vicente%2520Javier%2520Castro%2520Solar%2520and%2520Manuel%2520Roveri%26entry.1292438233%3D%2520%2520Incremental%2520learning%2520is%2520a%2520machine%2520learning%2520paradigm%2520where%2520a%2520model%2520learns%2520from%250Aa%2520sequential%2520stream%2520of%2520tasks.%2520This%2520setting%2520poses%2520a%2520key%2520challenge%253A%2520balancing%250Aplasticity%2520%2528learning%2520new%2520tasks%2529%2520and%2520stability%2520%2528preserving%2520past%2520knowledge%2529.%250ANeural%2520Architecture%2520Search%2520%2528NAS%2529%252C%2520a%2520branch%2520of%2520AutoML%252C%2520automates%2520the%2520design%2520of%250Athe%2520architecture%2520of%2520Deep%2520Neural%2520Networks%2520and%2520has%2520shown%2520success%2520in%2520static%250Asettings.%2520However%252C%2520existing%2520NAS-based%2520approaches%2520to%2520incremental%2520learning%2520often%250Arely%2520on%2520expanding%2520the%2520model%2520at%2520every%2520task%252C%2520making%2520them%2520impractical%2520in%250Aresource-constrained%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SEAL%252C%2520a%2520NAS-based%250Aframework%2520tailored%2520for%2520data-incremental%2520learning%252C%2520a%2520scenario%2520where%2520disjoint%250Adata%2520samples%2520arrive%2520sequentially%2520and%2520are%2520not%2520stored%2520for%2520future%2520access.%2520SEAL%250Aadapts%2520the%2520model%2520structure%2520dynamically%2520by%2520expanding%2520it%2520only%2520when%2520necessary%252C%250Abased%2520on%2520a%2520capacity%2520estimation%2520metric.%2520Stability%2520is%2520preserved%2520through%250Across-distillation%2520training%2520after%2520each%2520expansion%2520step.%2520The%2520NAS%2520component%250Ajointly%2520searches%2520for%2520both%2520the%2520architecture%2520and%2520the%2520optimal%2520expansion%2520policy.%250AExperiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520SEAL%2520effectively%250Areduces%2520forgetting%2520and%2520enhances%2520accuracy%2520while%2520maintaining%2520a%2520lower%2520model%2520size%250Acompared%2520to%2520prior%2520methods.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520combining%2520NAS%250Aand%2520selective%2520expansion%2520for%2520efficient%252C%2520adaptive%2520learning%2520in%2520incremental%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning&entry.906535625=Matteo%20Gambella%20and%20Vicente%20Javier%20Castro%20Solar%20and%20Manuel%20Roveri&entry.1292438233=%20%20Incremental%20learning%20is%20a%20machine%20learning%20paradigm%20where%20a%20model%20learns%20from%0Aa%20sequential%20stream%20of%20tasks.%20This%20setting%20poses%20a%20key%20challenge%3A%20balancing%0Aplasticity%20%28learning%20new%20tasks%29%20and%20stability%20%28preserving%20past%20knowledge%29.%0ANeural%20Architecture%20Search%20%28NAS%29%2C%20a%20branch%20of%20AutoML%2C%20automates%20the%20design%20of%0Athe%20architecture%20of%20Deep%20Neural%20Networks%20and%20has%20shown%20success%20in%20static%0Asettings.%20However%2C%20existing%20NAS-based%20approaches%20to%20incremental%20learning%20often%0Arely%20on%20expanding%20the%20model%20at%20every%20task%2C%20making%20them%20impractical%20in%0Aresource-constrained%20environments.%20In%20this%20work%2C%20we%20introduce%20SEAL%2C%20a%20NAS-based%0Aframework%20tailored%20for%20data-incremental%20learning%2C%20a%20scenario%20where%20disjoint%0Adata%20samples%20arrive%20sequentially%20and%20are%20not%20stored%20for%20future%20access.%20SEAL%0Aadapts%20the%20model%20structure%20dynamically%20by%20expanding%20it%20only%20when%20necessary%2C%0Abased%20on%20a%20capacity%20estimation%20metric.%20Stability%20is%20preserved%20through%0Across-distillation%20training%20after%20each%20expansion%20step.%20The%20NAS%20component%0Ajointly%20searches%20for%20both%20the%20architecture%20and%20the%20optimal%20expansion%20policy.%0AExperiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SEAL%20effectively%0Areduces%20forgetting%20and%20enhances%20accuracy%20while%20maintaining%20a%20lower%20model%20size%0Acompared%20to%20prior%20methods.%20These%20results%20highlight%20the%20promise%20of%20combining%20NAS%0Aand%20selective%20expansion%20for%20efficient%2C%20adaptive%20learning%20in%20incremental%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10457v1&entry.124074799=Read"},
{"title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M", "author": "Dario Di Palma and Felice Antonio Merra and Maurizio Sfilio and Vito Walter Anelli and Fedelucio Narducci and Tommaso Di Noia", "abstract": "  Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector\n", "link": "http://arxiv.org/abs/2505.10212v1", "date": "2025-05-15", "relevancy": 2.4207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Memorize%20Recommendation%20Datasets%3F%20A%20Preliminary%20Study%20on%0A%20%20MovieLens-1M&body=Title%3A%20Do%20LLMs%20Memorize%20Recommendation%20Datasets%3F%20A%20Preliminary%20Study%20on%0A%20%20MovieLens-1M%0AAuthor%3A%20Dario%20Di%20Palma%20and%20Felice%20Antonio%20Merra%20and%20Maurizio%20Sfilio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20increasingly%20central%20to%0Arecommendation%20scenarios%20due%20to%20their%20remarkable%20natural%20language%20understanding%0Aand%20generation%20capabilities.%20Although%20significant%20research%20has%20explored%20the%20use%0Aof%20LLMs%20for%20various%20recommendation%20tasks%2C%20little%20effort%20has%20been%20dedicated%20to%0Averifying%20whether%20they%20have%20memorized%20public%20recommendation%20dataset%20as%20part%20of%0Atheir%20training%20data.%20This%20is%20undesirable%20because%20memorization%20reduces%20the%0Ageneralizability%20of%20research%20findings%2C%20as%20benchmarking%20on%20memorized%20datasets%0Adoes%20not%20guarantee%20generalization%20to%20unseen%20datasets.%20Furthermore%2C%20memorization%0Acan%20amplify%20biases%2C%20for%20example%2C%20some%20popular%20items%20may%20be%20recommended%20more%0Afrequently%20than%20others.%0A%20%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%20have%20memorized%20public%0Arecommendation%20datasets.%20Specifically%2C%20we%20examine%20two%20model%20families%20%28GPT%20and%0ALlama%29%20across%20multiple%20sizes%2C%20focusing%20on%20one%20of%20the%20most%20widely%20used%20dataset%0Ain%20recommender%20systems%3A%20MovieLens-1M.%20First%2C%20we%20define%20dataset%20memorization%20as%0Athe%20extent%20to%20which%20item%20attributes%2C%20user%20profiles%2C%20and%20user-item%20interactions%0Acan%20be%20retrieved%20by%20prompting%20the%20LLMs.%20Second%2C%20we%20analyze%20the%20impact%20of%0Amemorization%20on%20recommendation%20performance.%20Lastly%2C%20we%20examine%20whether%0Amemorization%20varies%20across%20model%20families%20and%20model%20sizes.%20Our%20results%20reveal%0Athat%20all%20models%20exhibit%20some%20degree%20of%20memorization%20of%20MovieLens-1M%2C%20and%20that%0Arecommendation%20performance%20is%20related%20to%20the%20extent%20of%20memorization.%20We%20have%0Amade%20all%20the%20code%20publicly%20available%20at%3A%0Ahttps%3A//github.com/sisinflab/LLM-MemoryInspector%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Memorize%2520Recommendation%2520Datasets%253F%2520A%2520Preliminary%2520Study%2520on%250A%2520%2520MovieLens-1M%26entry.906535625%3DDario%2520Di%2520Palma%2520and%2520Felice%2520Antonio%2520Merra%2520and%2520Maurizio%2520Sfilio%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520increasingly%2520central%2520to%250Arecommendation%2520scenarios%2520due%2520to%2520their%2520remarkable%2520natural%2520language%2520understanding%250Aand%2520generation%2520capabilities.%2520Although%2520significant%2520research%2520has%2520explored%2520the%2520use%250Aof%2520LLMs%2520for%2520various%2520recommendation%2520tasks%252C%2520little%2520effort%2520has%2520been%2520dedicated%2520to%250Averifying%2520whether%2520they%2520have%2520memorized%2520public%2520recommendation%2520dataset%2520as%2520part%2520of%250Atheir%2520training%2520data.%2520This%2520is%2520undesirable%2520because%2520memorization%2520reduces%2520the%250Ageneralizability%2520of%2520research%2520findings%252C%2520as%2520benchmarking%2520on%2520memorized%2520datasets%250Adoes%2520not%2520guarantee%2520generalization%2520to%2520unseen%2520datasets.%2520Furthermore%252C%2520memorization%250Acan%2520amplify%2520biases%252C%2520for%2520example%252C%2520some%2520popular%2520items%2520may%2520be%2520recommended%2520more%250Afrequently%2520than%2520others.%250A%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520LLMs%2520have%2520memorized%2520public%250Arecommendation%2520datasets.%2520Specifically%252C%2520we%2520examine%2520two%2520model%2520families%2520%2528GPT%2520and%250ALlama%2529%2520across%2520multiple%2520sizes%252C%2520focusing%2520on%2520one%2520of%2520the%2520most%2520widely%2520used%2520dataset%250Ain%2520recommender%2520systems%253A%2520MovieLens-1M.%2520First%252C%2520we%2520define%2520dataset%2520memorization%2520as%250Athe%2520extent%2520to%2520which%2520item%2520attributes%252C%2520user%2520profiles%252C%2520and%2520user-item%2520interactions%250Acan%2520be%2520retrieved%2520by%2520prompting%2520the%2520LLMs.%2520Second%252C%2520we%2520analyze%2520the%2520impact%2520of%250Amemorization%2520on%2520recommendation%2520performance.%2520Lastly%252C%2520we%2520examine%2520whether%250Amemorization%2520varies%2520across%2520model%2520families%2520and%2520model%2520sizes.%2520Our%2520results%2520reveal%250Athat%2520all%2520models%2520exhibit%2520some%2520degree%2520of%2520memorization%2520of%2520MovieLens-1M%252C%2520and%2520that%250Arecommendation%2520performance%2520is%2520related%2520to%2520the%2520extent%2520of%2520memorization.%2520We%2520have%250Amade%2520all%2520the%2520code%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/sisinflab/LLM-MemoryInspector%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Memorize%20Recommendation%20Datasets%3F%20A%20Preliminary%20Study%20on%0A%20%20MovieLens-1M&entry.906535625=Dario%20Di%20Palma%20and%20Felice%20Antonio%20Merra%20and%20Maurizio%20Sfilio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20increasingly%20central%20to%0Arecommendation%20scenarios%20due%20to%20their%20remarkable%20natural%20language%20understanding%0Aand%20generation%20capabilities.%20Although%20significant%20research%20has%20explored%20the%20use%0Aof%20LLMs%20for%20various%20recommendation%20tasks%2C%20little%20effort%20has%20been%20dedicated%20to%0Averifying%20whether%20they%20have%20memorized%20public%20recommendation%20dataset%20as%20part%20of%0Atheir%20training%20data.%20This%20is%20undesirable%20because%20memorization%20reduces%20the%0Ageneralizability%20of%20research%20findings%2C%20as%20benchmarking%20on%20memorized%20datasets%0Adoes%20not%20guarantee%20generalization%20to%20unseen%20datasets.%20Furthermore%2C%20memorization%0Acan%20amplify%20biases%2C%20for%20example%2C%20some%20popular%20items%20may%20be%20recommended%20more%0Afrequently%20than%20others.%0A%20%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%20have%20memorized%20public%0Arecommendation%20datasets.%20Specifically%2C%20we%20examine%20two%20model%20families%20%28GPT%20and%0ALlama%29%20across%20multiple%20sizes%2C%20focusing%20on%20one%20of%20the%20most%20widely%20used%20dataset%0Ain%20recommender%20systems%3A%20MovieLens-1M.%20First%2C%20we%20define%20dataset%20memorization%20as%0Athe%20extent%20to%20which%20item%20attributes%2C%20user%20profiles%2C%20and%20user-item%20interactions%0Acan%20be%20retrieved%20by%20prompting%20the%20LLMs.%20Second%2C%20we%20analyze%20the%20impact%20of%0Amemorization%20on%20recommendation%20performance.%20Lastly%2C%20we%20examine%20whether%0Amemorization%20varies%20across%20model%20families%20and%20model%20sizes.%20Our%20results%20reveal%0Athat%20all%20models%20exhibit%20some%20degree%20of%20memorization%20of%20MovieLens-1M%2C%20and%20that%0Arecommendation%20performance%20is%20related%20to%20the%20extent%20of%20memorization.%20We%20have%0Amade%20all%20the%20code%20publicly%20available%20at%3A%0Ahttps%3A//github.com/sisinflab/LLM-MemoryInspector%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10212v1&entry.124074799=Read"},
{"title": "System Log Parsing with Large Language Models: A Review", "author": "Viktor Beck and Max Landauer and Markus Wurzenberger and Florian Skopik and Andreas Rauber", "abstract": "  Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency.\n", "link": "http://arxiv.org/abs/2504.04877v2", "date": "2025-05-15", "relevancy": 2.3823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%20Log%20Parsing%20with%20Large%20Language%20Models%3A%20A%20Review&body=Title%3A%20System%20Log%20Parsing%20with%20Large%20Language%20Models%3A%20A%20Review%0AAuthor%3A%20Viktor%20Beck%20and%20Max%20Landauer%20and%20Markus%20Wurzenberger%20and%20Florian%20Skopik%20and%20Andreas%20Rauber%0AAbstract%3A%20%20%20Log%20data%20provides%20crucial%20insights%20for%20tasks%20like%20monitoring%2C%20root%20cause%0Aanalysis%2C%20and%20anomaly%20detection.%20Due%20to%20the%20vast%20volume%20of%20logs%2C%20automated%20log%0Aparsing%20is%20essential%20to%20transform%20semi-structured%20log%20messages%20into%20structured%0Arepresentations.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%0Aintroduced%20the%20new%20research%20field%20of%20LLM-based%20log%20parsing.%20Despite%20promising%0Aresults%2C%20there%20is%20no%20structured%20overview%20of%20the%20approaches%20in%20this%20relatively%0Anew%20research%20field%20with%20the%20earliest%20advances%20published%20in%20late%202023.%20This%20work%0Asystematically%20reviews%2029%20LLM-based%20log%20parsing%20methods.%20We%20benchmark%20seven%20of%0Athem%20on%20public%20datasets%20and%20critically%20assess%20their%20comparability%20and%20the%0Areproducibility%20of%20their%20reported%20results.%20Our%20findings%20summarize%20the%20advances%0Aof%20this%20new%20research%20field%2C%20with%20insights%20on%20how%20to%20report%20results%2C%20which%20data%0Asets%2C%20metrics%20and%20which%20terminology%20to%20use%2C%20and%20which%20inconsistencies%20to%20avoid%2C%0Awith%20code%20and%20results%20made%20publicly%20available%20for%20transparency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%2520Log%2520Parsing%2520with%2520Large%2520Language%2520Models%253A%2520A%2520Review%26entry.906535625%3DViktor%2520Beck%2520and%2520Max%2520Landauer%2520and%2520Markus%2520Wurzenberger%2520and%2520Florian%2520Skopik%2520and%2520Andreas%2520Rauber%26entry.1292438233%3D%2520%2520Log%2520data%2520provides%2520crucial%2520insights%2520for%2520tasks%2520like%2520monitoring%252C%2520root%2520cause%250Aanalysis%252C%2520and%2520anomaly%2520detection.%2520Due%2520to%2520the%2520vast%2520volume%2520of%2520logs%252C%2520automated%2520log%250Aparsing%2520is%2520essential%2520to%2520transform%2520semi-structured%2520log%2520messages%2520into%2520structured%250Arepresentations.%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Aintroduced%2520the%2520new%2520research%2520field%2520of%2520LLM-based%2520log%2520parsing.%2520Despite%2520promising%250Aresults%252C%2520there%2520is%2520no%2520structured%2520overview%2520of%2520the%2520approaches%2520in%2520this%2520relatively%250Anew%2520research%2520field%2520with%2520the%2520earliest%2520advances%2520published%2520in%2520late%25202023.%2520This%2520work%250Asystematically%2520reviews%252029%2520LLM-based%2520log%2520parsing%2520methods.%2520We%2520benchmark%2520seven%2520of%250Athem%2520on%2520public%2520datasets%2520and%2520critically%2520assess%2520their%2520comparability%2520and%2520the%250Areproducibility%2520of%2520their%2520reported%2520results.%2520Our%2520findings%2520summarize%2520the%2520advances%250Aof%2520this%2520new%2520research%2520field%252C%2520with%2520insights%2520on%2520how%2520to%2520report%2520results%252C%2520which%2520data%250Asets%252C%2520metrics%2520and%2520which%2520terminology%2520to%2520use%252C%2520and%2520which%2520inconsistencies%2520to%2520avoid%252C%250Awith%2520code%2520and%2520results%2520made%2520publicly%2520available%2520for%2520transparency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Log%20Parsing%20with%20Large%20Language%20Models%3A%20A%20Review&entry.906535625=Viktor%20Beck%20and%20Max%20Landauer%20and%20Markus%20Wurzenberger%20and%20Florian%20Skopik%20and%20Andreas%20Rauber&entry.1292438233=%20%20Log%20data%20provides%20crucial%20insights%20for%20tasks%20like%20monitoring%2C%20root%20cause%0Aanalysis%2C%20and%20anomaly%20detection.%20Due%20to%20the%20vast%20volume%20of%20logs%2C%20automated%20log%0Aparsing%20is%20essential%20to%20transform%20semi-structured%20log%20messages%20into%20structured%0Arepresentations.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%0Aintroduced%20the%20new%20research%20field%20of%20LLM-based%20log%20parsing.%20Despite%20promising%0Aresults%2C%20there%20is%20no%20structured%20overview%20of%20the%20approaches%20in%20this%20relatively%0Anew%20research%20field%20with%20the%20earliest%20advances%20published%20in%20late%202023.%20This%20work%0Asystematically%20reviews%2029%20LLM-based%20log%20parsing%20methods.%20We%20benchmark%20seven%20of%0Athem%20on%20public%20datasets%20and%20critically%20assess%20their%20comparability%20and%20the%0Areproducibility%20of%20their%20reported%20results.%20Our%20findings%20summarize%20the%20advances%0Aof%20this%20new%20research%20field%2C%20with%20insights%20on%20how%20to%20report%20results%2C%20which%20data%0Asets%2C%20metrics%20and%20which%20terminology%20to%20use%2C%20and%20which%20inconsistencies%20to%20avoid%2C%0Awith%20code%20and%20results%20made%20publicly%20available%20for%20transparency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04877v2&entry.124074799=Read"},
{"title": "UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System", "author": "Yitao Zhu and Yuan Yin and Zhenrong Shen and Zihao Zhao and Haiyu Song and Sheng Wang and Dinggang Shen and Qian Wang", "abstract": "  The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.\n", "link": "http://arxiv.org/abs/2505.09178v2", "date": "2025-05-15", "relevancy": 2.3699, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.601}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCAD%3A%20Efficient%20and%20Extendable%20Architecture%20for%20Multi-Task%0A%20%20Computer-Aided%20Diagnosis%20System&body=Title%3A%20UniCAD%3A%20Efficient%20and%20Extendable%20Architecture%20for%20Multi-Task%0A%20%20Computer-Aided%20Diagnosis%20System%0AAuthor%3A%20Yitao%20Zhu%20and%20Yuan%20Yin%20and%20Zhenrong%20Shen%20and%20Zihao%20Zhao%20and%20Haiyu%20Song%20and%20Sheng%20Wang%20and%20Dinggang%20Shen%20and%20Qian%20Wang%0AAbstract%3A%20%20%20The%20growing%20complexity%20and%20scale%20of%20visual%20model%20pre-training%20have%20made%0Adeveloping%20and%20deploying%20multi-task%20computer-aided%20diagnosis%20%28CAD%29%20systems%0Aincreasingly%20challenging%20and%20resource-intensive.%20Furthermore%2C%20the%20medical%0Aimaging%20community%20lacks%20an%20open-source%20CAD%20platform%20to%20enable%20the%20rapid%0Acreation%20of%20efficient%20and%20extendable%20diagnostic%20models.%20To%20address%20these%0Aissues%2C%20we%20propose%20UniCAD%2C%20a%20unified%20architecture%20that%20leverages%20the%20robust%0Acapabilities%20of%20pre-trained%20vision%20foundation%20models%20to%20seamlessly%20handle%20both%0A2D%20and%203D%20medical%20images%20while%20requiring%20only%20minimal%20task-specific%20parameters.%0AUniCAD%20introduces%20two%20key%20innovations%3A%20%281%29%20Efficiency%3A%20A%20low-rank%20adaptation%0Astrategy%20is%20employed%20to%20adapt%20a%20pre-trained%20visual%20model%20to%20the%20medical%20image%0Adomain%2C%20achieving%20performance%20on%20par%20with%20fully%20fine-tuned%20counterparts%20while%0Aintroducing%20only%200.17%25%20trainable%20parameters.%20%282%29%20Plug-and-Play%3A%20A%20modular%0Aarchitecture%20that%20combines%20a%20frozen%20foundation%20model%20with%20multiple%0Aplug-and-play%20experts%2C%20enabling%20diverse%20tasks%20and%20seamless%20functionality%0Aexpansion.%20Building%20on%20this%20unified%20CAD%20architecture%2C%20we%20establish%20an%0Aopen-source%20platform%20where%20researchers%20can%20share%20and%20access%20lightweight%20CAD%0Aexperts%2C%20fostering%20a%20more%20equitable%20and%20efficient%20research%20ecosystem.%0AComprehensive%20experiments%20across%2012%20diverse%20medical%20datasets%20demonstrate%20that%0AUniCAD%20consistently%20outperforms%20existing%20methods%20in%20both%20accuracy%20and%0Adeployment%20efficiency.%20The%20source%20code%20and%20project%20page%20are%20available%20at%0Ahttps%3A//mii-laboratory.github.io/UniCAD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCAD%253A%2520Efficient%2520and%2520Extendable%2520Architecture%2520for%2520Multi-Task%250A%2520%2520Computer-Aided%2520Diagnosis%2520System%26entry.906535625%3DYitao%2520Zhu%2520and%2520Yuan%2520Yin%2520and%2520Zhenrong%2520Shen%2520and%2520Zihao%2520Zhao%2520and%2520Haiyu%2520Song%2520and%2520Sheng%2520Wang%2520and%2520Dinggang%2520Shen%2520and%2520Qian%2520Wang%26entry.1292438233%3D%2520%2520The%2520growing%2520complexity%2520and%2520scale%2520of%2520visual%2520model%2520pre-training%2520have%2520made%250Adeveloping%2520and%2520deploying%2520multi-task%2520computer-aided%2520diagnosis%2520%2528CAD%2529%2520systems%250Aincreasingly%2520challenging%2520and%2520resource-intensive.%2520Furthermore%252C%2520the%2520medical%250Aimaging%2520community%2520lacks%2520an%2520open-source%2520CAD%2520platform%2520to%2520enable%2520the%2520rapid%250Acreation%2520of%2520efficient%2520and%2520extendable%2520diagnostic%2520models.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520UniCAD%252C%2520a%2520unified%2520architecture%2520that%2520leverages%2520the%2520robust%250Acapabilities%2520of%2520pre-trained%2520vision%2520foundation%2520models%2520to%2520seamlessly%2520handle%2520both%250A2D%2520and%25203D%2520medical%2520images%2520while%2520requiring%2520only%2520minimal%2520task-specific%2520parameters.%250AUniCAD%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520Efficiency%253A%2520A%2520low-rank%2520adaptation%250Astrategy%2520is%2520employed%2520to%2520adapt%2520a%2520pre-trained%2520visual%2520model%2520to%2520the%2520medical%2520image%250Adomain%252C%2520achieving%2520performance%2520on%2520par%2520with%2520fully%2520fine-tuned%2520counterparts%2520while%250Aintroducing%2520only%25200.17%2525%2520trainable%2520parameters.%2520%25282%2529%2520Plug-and-Play%253A%2520A%2520modular%250Aarchitecture%2520that%2520combines%2520a%2520frozen%2520foundation%2520model%2520with%2520multiple%250Aplug-and-play%2520experts%252C%2520enabling%2520diverse%2520tasks%2520and%2520seamless%2520functionality%250Aexpansion.%2520Building%2520on%2520this%2520unified%2520CAD%2520architecture%252C%2520we%2520establish%2520an%250Aopen-source%2520platform%2520where%2520researchers%2520can%2520share%2520and%2520access%2520lightweight%2520CAD%250Aexperts%252C%2520fostering%2520a%2520more%2520equitable%2520and%2520efficient%2520research%2520ecosystem.%250AComprehensive%2520experiments%2520across%252012%2520diverse%2520medical%2520datasets%2520demonstrate%2520that%250AUniCAD%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520both%2520accuracy%2520and%250Adeployment%2520efficiency.%2520The%2520source%2520code%2520and%2520project%2520page%2520are%2520available%2520at%250Ahttps%253A//mii-laboratory.github.io/UniCAD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCAD%3A%20Efficient%20and%20Extendable%20Architecture%20for%20Multi-Task%0A%20%20Computer-Aided%20Diagnosis%20System&entry.906535625=Yitao%20Zhu%20and%20Yuan%20Yin%20and%20Zhenrong%20Shen%20and%20Zihao%20Zhao%20and%20Haiyu%20Song%20and%20Sheng%20Wang%20and%20Dinggang%20Shen%20and%20Qian%20Wang&entry.1292438233=%20%20The%20growing%20complexity%20and%20scale%20of%20visual%20model%20pre-training%20have%20made%0Adeveloping%20and%20deploying%20multi-task%20computer-aided%20diagnosis%20%28CAD%29%20systems%0Aincreasingly%20challenging%20and%20resource-intensive.%20Furthermore%2C%20the%20medical%0Aimaging%20community%20lacks%20an%20open-source%20CAD%20platform%20to%20enable%20the%20rapid%0Acreation%20of%20efficient%20and%20extendable%20diagnostic%20models.%20To%20address%20these%0Aissues%2C%20we%20propose%20UniCAD%2C%20a%20unified%20architecture%20that%20leverages%20the%20robust%0Acapabilities%20of%20pre-trained%20vision%20foundation%20models%20to%20seamlessly%20handle%20both%0A2D%20and%203D%20medical%20images%20while%20requiring%20only%20minimal%20task-specific%20parameters.%0AUniCAD%20introduces%20two%20key%20innovations%3A%20%281%29%20Efficiency%3A%20A%20low-rank%20adaptation%0Astrategy%20is%20employed%20to%20adapt%20a%20pre-trained%20visual%20model%20to%20the%20medical%20image%0Adomain%2C%20achieving%20performance%20on%20par%20with%20fully%20fine-tuned%20counterparts%20while%0Aintroducing%20only%200.17%25%20trainable%20parameters.%20%282%29%20Plug-and-Play%3A%20A%20modular%0Aarchitecture%20that%20combines%20a%20frozen%20foundation%20model%20with%20multiple%0Aplug-and-play%20experts%2C%20enabling%20diverse%20tasks%20and%20seamless%20functionality%0Aexpansion.%20Building%20on%20this%20unified%20CAD%20architecture%2C%20we%20establish%20an%0Aopen-source%20platform%20where%20researchers%20can%20share%20and%20access%20lightweight%20CAD%0Aexperts%2C%20fostering%20a%20more%20equitable%20and%20efficient%20research%20ecosystem.%0AComprehensive%20experiments%20across%2012%20diverse%20medical%20datasets%20demonstrate%20that%0AUniCAD%20consistently%20outperforms%20existing%20methods%20in%20both%20accuracy%20and%0Adeployment%20efficiency.%20The%20source%20code%20and%20project%20page%20are%20available%20at%0Ahttps%3A//mii-laboratory.github.io/UniCAD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09178v2&entry.124074799=Read"},
{"title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models", "author": "Peichao Lai and Kexuan Zhang and Yi Lin and Linyihan Zhang and Feiyang Ye and Jinhao Yan and Yanwei Xu and Conghui He and Yilei Wang and Wentao Zhang and Bin Cui", "abstract": "  Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.\n", "link": "http://arxiv.org/abs/2505.07247v2", "date": "2025-05-15", "relevancy": 2.3543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4785}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAS-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Short%20Answer%20Scoring%0A%20%20with%20Large%20Language%20Models&body=Title%3A%20SAS-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Short%20Answer%20Scoring%0A%20%20with%20Large%20Language%20Models%0AAuthor%3A%20Peichao%20Lai%20and%20Kexuan%20Zhang%20and%20Yi%20Lin%20and%20Linyihan%20Zhang%20and%20Feiyang%20Ye%20and%20Jinhao%20Yan%20and%20Yanwei%20Xu%20and%20Conghui%20He%20and%20Yilei%20Wang%20and%20Wentao%20Zhang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Subjective%20Answer%20Grading%20%28SAG%29%20plays%20a%20crucial%20role%20in%20education%2C%0Astandardized%20testing%2C%20and%20automated%20assessment%20systems%2C%20particularly%20for%0Aevaluating%20short-form%20responses%20in%20Short%20Answer%20Scoring%20%28SAS%29.%20However%2C%0Aexisting%20approaches%20often%20produce%20coarse-grained%20scores%20and%20lack%20detailed%0Areasoning.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20potential%20as%0Azero-shot%20evaluators%2C%20they%20remain%20susceptible%20to%20bias%2C%20inconsistencies%20with%0Ahuman%20judgment%2C%20and%20limited%20transparency%20in%20scoring%20decisions.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20SAS-Bench%2C%20a%20benchmark%20specifically%20designed%0Afor%20LLM-based%20SAS%20tasks.%20SAS-Bench%20provides%20fine-grained%2C%20step-wise%20scoring%2C%0Aexpert-annotated%20error%20categories%2C%20and%20a%20diverse%20range%20of%20question%20types%0Aderived%20from%20real-world%20subject-specific%20exams.%20This%20benchmark%20facilitates%0Adetailed%20evaluation%20of%20model%20reasoning%20processes%20and%20explainability.%20We%20also%0Arelease%20an%20open-source%20dataset%20containing%201%2C030%20questions%20and%204%2C109%20student%0Aresponses%2C%20each%20annotated%20by%20domain%20experts.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20experiments%20with%20various%20LLMs%2C%20identifying%20major%20challenges%20in%0Ascoring%20science-related%20questions%20and%20highlighting%20the%20effectiveness%20of%0Afew-shot%20prompting%20in%20improving%20scoring%20accuracy.%20Our%20work%20offers%20valuable%0Ainsights%20into%20the%20development%20of%20more%20robust%2C%20fair%2C%20and%20educationally%0Ameaningful%20LLM-based%20evaluation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAS-Bench%253A%2520A%2520Fine-Grained%2520Benchmark%2520for%2520Evaluating%2520Short%2520Answer%2520Scoring%250A%2520%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DPeichao%2520Lai%2520and%2520Kexuan%2520Zhang%2520and%2520Yi%2520Lin%2520and%2520Linyihan%2520Zhang%2520and%2520Feiyang%2520Ye%2520and%2520Jinhao%2520Yan%2520and%2520Yanwei%2520Xu%2520and%2520Conghui%2520He%2520and%2520Yilei%2520Wang%2520and%2520Wentao%2520Zhang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Subjective%2520Answer%2520Grading%2520%2528SAG%2529%2520plays%2520a%2520crucial%2520role%2520in%2520education%252C%250Astandardized%2520testing%252C%2520and%2520automated%2520assessment%2520systems%252C%2520particularly%2520for%250Aevaluating%2520short-form%2520responses%2520in%2520Short%2520Answer%2520Scoring%2520%2528SAS%2529.%2520However%252C%250Aexisting%2520approaches%2520often%2520produce%2520coarse-grained%2520scores%2520and%2520lack%2520detailed%250Areasoning.%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520potential%2520as%250Azero-shot%2520evaluators%252C%2520they%2520remain%2520susceptible%2520to%2520bias%252C%2520inconsistencies%2520with%250Ahuman%2520judgment%252C%2520and%2520limited%2520transparency%2520in%2520scoring%2520decisions.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520SAS-Bench%252C%2520a%2520benchmark%2520specifically%2520designed%250Afor%2520LLM-based%2520SAS%2520tasks.%2520SAS-Bench%2520provides%2520fine-grained%252C%2520step-wise%2520scoring%252C%250Aexpert-annotated%2520error%2520categories%252C%2520and%2520a%2520diverse%2520range%2520of%2520question%2520types%250Aderived%2520from%2520real-world%2520subject-specific%2520exams.%2520This%2520benchmark%2520facilitates%250Adetailed%2520evaluation%2520of%2520model%2520reasoning%2520processes%2520and%2520explainability.%2520We%2520also%250Arelease%2520an%2520open-source%2520dataset%2520containing%25201%252C030%2520questions%2520and%25204%252C109%2520student%250Aresponses%252C%2520each%2520annotated%2520by%2520domain%2520experts.%2520Furthermore%252C%2520we%2520conduct%250Acomprehensive%2520experiments%2520with%2520various%2520LLMs%252C%2520identifying%2520major%2520challenges%2520in%250Ascoring%2520science-related%2520questions%2520and%2520highlighting%2520the%2520effectiveness%2520of%250Afew-shot%2520prompting%2520in%2520improving%2520scoring%2520accuracy.%2520Our%2520work%2520offers%2520valuable%250Ainsights%2520into%2520the%2520development%2520of%2520more%2520robust%252C%2520fair%252C%2520and%2520educationally%250Ameaningful%2520LLM-based%2520evaluation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAS-Bench%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Short%20Answer%20Scoring%0A%20%20with%20Large%20Language%20Models&entry.906535625=Peichao%20Lai%20and%20Kexuan%20Zhang%20and%20Yi%20Lin%20and%20Linyihan%20Zhang%20and%20Feiyang%20Ye%20and%20Jinhao%20Yan%20and%20Yanwei%20Xu%20and%20Conghui%20He%20and%20Yilei%20Wang%20and%20Wentao%20Zhang%20and%20Bin%20Cui&entry.1292438233=%20%20Subjective%20Answer%20Grading%20%28SAG%29%20plays%20a%20crucial%20role%20in%20education%2C%0Astandardized%20testing%2C%20and%20automated%20assessment%20systems%2C%20particularly%20for%0Aevaluating%20short-form%20responses%20in%20Short%20Answer%20Scoring%20%28SAS%29.%20However%2C%0Aexisting%20approaches%20often%20produce%20coarse-grained%20scores%20and%20lack%20detailed%0Areasoning.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20potential%20as%0Azero-shot%20evaluators%2C%20they%20remain%20susceptible%20to%20bias%2C%20inconsistencies%20with%0Ahuman%20judgment%2C%20and%20limited%20transparency%20in%20scoring%20decisions.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20SAS-Bench%2C%20a%20benchmark%20specifically%20designed%0Afor%20LLM-based%20SAS%20tasks.%20SAS-Bench%20provides%20fine-grained%2C%20step-wise%20scoring%2C%0Aexpert-annotated%20error%20categories%2C%20and%20a%20diverse%20range%20of%20question%20types%0Aderived%20from%20real-world%20subject-specific%20exams.%20This%20benchmark%20facilitates%0Adetailed%20evaluation%20of%20model%20reasoning%20processes%20and%20explainability.%20We%20also%0Arelease%20an%20open-source%20dataset%20containing%201%2C030%20questions%20and%204%2C109%20student%0Aresponses%2C%20each%20annotated%20by%20domain%20experts.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20experiments%20with%20various%20LLMs%2C%20identifying%20major%20challenges%20in%0Ascoring%20science-related%20questions%20and%20highlighting%20the%20effectiveness%20of%0Afew-shot%20prompting%20in%20improving%20scoring%20accuracy.%20Our%20work%20offers%20valuable%0Ainsights%20into%20the%20development%20of%20more%20robust%2C%20fair%2C%20and%20educationally%0Ameaningful%20LLM-based%20evaluation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07247v2&entry.124074799=Read"},
{"title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling", "author": "Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Yucheng Li and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Junjie Hu and Wen Xiao", "abstract": "  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.\n", "link": "http://arxiv.org/abs/2406.02069v4", "date": "2025-05-15", "relevancy": 2.3396, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyramidKV%3A%20Dynamic%20KV%20Cache%20Compression%20based%20on%20Pyramidal%20Information%0A%20%20Funneling&body=Title%3A%20PyramidKV%3A%20Dynamic%20KV%20Cache%20Compression%20based%20on%20Pyramidal%20Information%0A%20%20Funneling%0AAuthor%3A%20Zefan%20Cai%20and%20Yichi%20Zhang%20and%20Bofei%20Gao%20and%20Yuliang%20Liu%20and%20Yucheng%20Li%20and%20Tianyu%20Liu%20and%20Keming%20Lu%20and%20Wayne%20Xiong%20and%20Yue%20Dong%20and%20Junjie%20Hu%20and%20Wen%20Xiao%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigate%20whether%20attention-based%20information%20flow%20inside%0Alarge%20language%20models%20%28LLMs%29%20is%20aggregated%20through%20noticeable%20patterns%20for%20long%0Acontext%20processing.%20Our%20observations%20reveal%20that%20LLMs%20aggregate%20information%0Athrough%20Pyramidal%20Information%20Funneling%20where%20attention%20is%20scattering%20widely%20in%0Alower%20layers%2C%20progressively%20consolidating%20within%20specific%20contexts%2C%20and%0Aultimately%20focusing%20on%20critical%20tokens%20%28a.k.a%20massive%20activation%20or%20attention%0Asink%29%20in%20higher%20layers.%20Motivated%20by%20these%20insights%2C%20we%20developed%20PyramidKV%2C%20a%0Anovel%20and%20effective%20KV%20cache%20compression%20method.%20This%20approach%20dynamically%0Aadjusts%20the%20KV%20cache%20size%20across%20different%20layers%2C%20allocating%20more%20cache%20in%0Alower%20layers%20and%20less%20in%20higher%20ones%2C%20diverging%20from%20traditional%20methods%20that%0Amaintain%20a%20uniform%20KV%20cache%20size.%20Our%20experimental%20evaluations%2C%20utilizing%20the%0ALongBench%20benchmark%2C%20show%20that%20PyramidKV%20matches%20the%20performance%20of%20models%20with%0Aa%20full%20KV%20cache%20while%20retaining%20only%2012%25%20of%20the%20KV%20cache%2C%20thus%20significantly%0Areducing%20memory%20usage.%20In%20scenarios%20emphasizing%20memory%20efficiency%2C%20where%20only%0A0.7%25%20of%20the%20KV%20cache%20is%20maintained%2C%20PyramidKV%20surpasses%20other%20KV%20cache%0Acompression%20techniques%2C%20achieving%20up%20to%20a%2020.5%20absolute%20accuracy%20improvement%20on%0ATREC%20dataset.%20In%20the%20Needle-in-a-Haystack%20experiment%2C%20PyramidKV%20outperforms%0Acompeting%20methods%20in%20maintaining%20long-context%20comprehension%20in%20LLMs%3B%20notably%2C%0Aretaining%20just%20128%20KV%20cache%20entries%20enables%20the%20LLAMA-3-70B%20model%20to%20achieve%0A100.0%20Acc.%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02069v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyramidKV%253A%2520Dynamic%2520KV%2520Cache%2520Compression%2520based%2520on%2520Pyramidal%2520Information%250A%2520%2520Funneling%26entry.906535625%3DZefan%2520Cai%2520and%2520Yichi%2520Zhang%2520and%2520Bofei%2520Gao%2520and%2520Yuliang%2520Liu%2520and%2520Yucheng%2520Li%2520and%2520Tianyu%2520Liu%2520and%2520Keming%2520Lu%2520and%2520Wayne%2520Xiong%2520and%2520Yue%2520Dong%2520and%2520Junjie%2520Hu%2520and%2520Wen%2520Xiao%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520whether%2520attention-based%2520information%2520flow%2520inside%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520is%2520aggregated%2520through%2520noticeable%2520patterns%2520for%2520long%250Acontext%2520processing.%2520Our%2520observations%2520reveal%2520that%2520LLMs%2520aggregate%2520information%250Athrough%2520Pyramidal%2520Information%2520Funneling%2520where%2520attention%2520is%2520scattering%2520widely%2520in%250Alower%2520layers%252C%2520progressively%2520consolidating%2520within%2520specific%2520contexts%252C%2520and%250Aultimately%2520focusing%2520on%2520critical%2520tokens%2520%2528a.k.a%2520massive%2520activation%2520or%2520attention%250Asink%2529%2520in%2520higher%2520layers.%2520Motivated%2520by%2520these%2520insights%252C%2520we%2520developed%2520PyramidKV%252C%2520a%250Anovel%2520and%2520effective%2520KV%2520cache%2520compression%2520method.%2520This%2520approach%2520dynamically%250Aadjusts%2520the%2520KV%2520cache%2520size%2520across%2520different%2520layers%252C%2520allocating%2520more%2520cache%2520in%250Alower%2520layers%2520and%2520less%2520in%2520higher%2520ones%252C%2520diverging%2520from%2520traditional%2520methods%2520that%250Amaintain%2520a%2520uniform%2520KV%2520cache%2520size.%2520Our%2520experimental%2520evaluations%252C%2520utilizing%2520the%250ALongBench%2520benchmark%252C%2520show%2520that%2520PyramidKV%2520matches%2520the%2520performance%2520of%2520models%2520with%250Aa%2520full%2520KV%2520cache%2520while%2520retaining%2520only%252012%2525%2520of%2520the%2520KV%2520cache%252C%2520thus%2520significantly%250Areducing%2520memory%2520usage.%2520In%2520scenarios%2520emphasizing%2520memory%2520efficiency%252C%2520where%2520only%250A0.7%2525%2520of%2520the%2520KV%2520cache%2520is%2520maintained%252C%2520PyramidKV%2520surpasses%2520other%2520KV%2520cache%250Acompression%2520techniques%252C%2520achieving%2520up%2520to%2520a%252020.5%2520absolute%2520accuracy%2520improvement%2520on%250ATREC%2520dataset.%2520In%2520the%2520Needle-in-a-Haystack%2520experiment%252C%2520PyramidKV%2520outperforms%250Acompeting%2520methods%2520in%2520maintaining%2520long-context%2520comprehension%2520in%2520LLMs%253B%2520notably%252C%250Aretaining%2520just%2520128%2520KV%2520cache%2520entries%2520enables%2520the%2520LLAMA-3-70B%2520model%2520to%2520achieve%250A100.0%2520Acc.%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02069v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyramidKV%3A%20Dynamic%20KV%20Cache%20Compression%20based%20on%20Pyramidal%20Information%0A%20%20Funneling&entry.906535625=Zefan%20Cai%20and%20Yichi%20Zhang%20and%20Bofei%20Gao%20and%20Yuliang%20Liu%20and%20Yucheng%20Li%20and%20Tianyu%20Liu%20and%20Keming%20Lu%20and%20Wayne%20Xiong%20and%20Yue%20Dong%20and%20Junjie%20Hu%20and%20Wen%20Xiao&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigate%20whether%20attention-based%20information%20flow%20inside%0Alarge%20language%20models%20%28LLMs%29%20is%20aggregated%20through%20noticeable%20patterns%20for%20long%0Acontext%20processing.%20Our%20observations%20reveal%20that%20LLMs%20aggregate%20information%0Athrough%20Pyramidal%20Information%20Funneling%20where%20attention%20is%20scattering%20widely%20in%0Alower%20layers%2C%20progressively%20consolidating%20within%20specific%20contexts%2C%20and%0Aultimately%20focusing%20on%20critical%20tokens%20%28a.k.a%20massive%20activation%20or%20attention%0Asink%29%20in%20higher%20layers.%20Motivated%20by%20these%20insights%2C%20we%20developed%20PyramidKV%2C%20a%0Anovel%20and%20effective%20KV%20cache%20compression%20method.%20This%20approach%20dynamically%0Aadjusts%20the%20KV%20cache%20size%20across%20different%20layers%2C%20allocating%20more%20cache%20in%0Alower%20layers%20and%20less%20in%20higher%20ones%2C%20diverging%20from%20traditional%20methods%20that%0Amaintain%20a%20uniform%20KV%20cache%20size.%20Our%20experimental%20evaluations%2C%20utilizing%20the%0ALongBench%20benchmark%2C%20show%20that%20PyramidKV%20matches%20the%20performance%20of%20models%20with%0Aa%20full%20KV%20cache%20while%20retaining%20only%2012%25%20of%20the%20KV%20cache%2C%20thus%20significantly%0Areducing%20memory%20usage.%20In%20scenarios%20emphasizing%20memory%20efficiency%2C%20where%20only%0A0.7%25%20of%20the%20KV%20cache%20is%20maintained%2C%20PyramidKV%20surpasses%20other%20KV%20cache%0Acompression%20techniques%2C%20achieving%20up%20to%20a%2020.5%20absolute%20accuracy%20improvement%20on%0ATREC%20dataset.%20In%20the%20Needle-in-a-Haystack%20experiment%2C%20PyramidKV%20outperforms%0Acompeting%20methods%20in%20maintaining%20long-context%20comprehension%20in%20LLMs%3B%20notably%2C%0Aretaining%20just%20128%20KV%20cache%20entries%20enables%20the%20LLAMA-3-70B%20model%20to%20achieve%0A100.0%20Acc.%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02069v4&entry.124074799=Read"},
{"title": "AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera\n  in Surgical Robotics", "author": "Alexandre Banks and Randy Moore and Sayem Nazmuz Zaman and Alaa Eldin Abdelaal and Septimiu E. Salcudean", "abstract": "  Incorporating an autonomous auxiliary camera into robot-assisted minimally\ninvasive surgery (RAMIS) enhances spatial awareness and eliminates manual\nviewpoint control. Existing path planning methods for auxiliary cameras track\ntwo-dimensional surgical features but do not simultaneously account for camera\norientation, workspace constraints, and robot joint limits. This study presents\nAutoCam: an automatic auxiliary camera placement method to improve\nvisualization in RAMIS. Implemented on the da Vinci Research Kit, the system\nuses a priority-based, workspace-constrained control algorithm that combines\nheuristic geometric placement with nonlinear optimization to ensure robust\ncamera tracking. A user study (N=6) demonstrated that the system maintained\n99.84% visibility of a salient feature and achieved a pose error of 4.36 $\\pm$\n2.11 degrees and 1.95 $\\pm$ 5.66 mm. The controller was computationally\nefficient, with a loop time of 6.8 $\\pm$ 12.8 ms. An additional pilot study\n(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training\ntask, suggests that users can teleoperate just as effectively from AutoCam's\nviewpoint as from the endoscope's while still benefiting from AutoCam's\nimproved visual coverage of the scene. These results indicate that an auxiliary\ncamera can be autonomously controlled using the da Vinci patient-side\nmanipulators to track a salient feature, laying the groundwork for new\nmulti-camera visualization methods in RAMIS.\n", "link": "http://arxiv.org/abs/2505.10398v1", "date": "2025-05-15", "relevancy": 2.3368, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6223}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5658}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoCam%3A%20Hierarchical%20Path%20Planning%20for%20an%20Autonomous%20Auxiliary%20Camera%0A%20%20in%20Surgical%20Robotics&body=Title%3A%20AutoCam%3A%20Hierarchical%20Path%20Planning%20for%20an%20Autonomous%20Auxiliary%20Camera%0A%20%20in%20Surgical%20Robotics%0AAuthor%3A%20Alexandre%20Banks%20and%20Randy%20Moore%20and%20Sayem%20Nazmuz%20Zaman%20and%20Alaa%20Eldin%20Abdelaal%20and%20Septimiu%20E.%20Salcudean%0AAbstract%3A%20%20%20Incorporating%20an%20autonomous%20auxiliary%20camera%20into%20robot-assisted%20minimally%0Ainvasive%20surgery%20%28RAMIS%29%20enhances%20spatial%20awareness%20and%20eliminates%20manual%0Aviewpoint%20control.%20Existing%20path%20planning%20methods%20for%20auxiliary%20cameras%20track%0Atwo-dimensional%20surgical%20features%20but%20do%20not%20simultaneously%20account%20for%20camera%0Aorientation%2C%20workspace%20constraints%2C%20and%20robot%20joint%20limits.%20This%20study%20presents%0AAutoCam%3A%20an%20automatic%20auxiliary%20camera%20placement%20method%20to%20improve%0Avisualization%20in%20RAMIS.%20Implemented%20on%20the%20da%20Vinci%20Research%20Kit%2C%20the%20system%0Auses%20a%20priority-based%2C%20workspace-constrained%20control%20algorithm%20that%20combines%0Aheuristic%20geometric%20placement%20with%20nonlinear%20optimization%20to%20ensure%20robust%0Acamera%20tracking.%20A%20user%20study%20%28N%3D6%29%20demonstrated%20that%20the%20system%20maintained%0A99.84%25%20visibility%20of%20a%20salient%20feature%20and%20achieved%20a%20pose%20error%20of%204.36%20%24%5Cpm%24%0A2.11%20degrees%20and%201.95%20%24%5Cpm%24%205.66%20mm.%20The%20controller%20was%20computationally%0Aefficient%2C%20with%20a%20loop%20time%20of%206.8%20%24%5Cpm%24%2012.8%20ms.%20An%20additional%20pilot%20study%0A%28N%3D6%29%2C%20where%20novices%20completed%20a%20Fundamentals%20of%20Laparoscopic%20Surgery%20training%0Atask%2C%20suggests%20that%20users%20can%20teleoperate%20just%20as%20effectively%20from%20AutoCam%27s%0Aviewpoint%20as%20from%20the%20endoscope%27s%20while%20still%20benefiting%20from%20AutoCam%27s%0Aimproved%20visual%20coverage%20of%20the%20scene.%20These%20results%20indicate%20that%20an%20auxiliary%0Acamera%20can%20be%20autonomously%20controlled%20using%20the%20da%20Vinci%20patient-side%0Amanipulators%20to%20track%20a%20salient%20feature%2C%20laying%20the%20groundwork%20for%20new%0Amulti-camera%20visualization%20methods%20in%20RAMIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoCam%253A%2520Hierarchical%2520Path%2520Planning%2520for%2520an%2520Autonomous%2520Auxiliary%2520Camera%250A%2520%2520in%2520Surgical%2520Robotics%26entry.906535625%3DAlexandre%2520Banks%2520and%2520Randy%2520Moore%2520and%2520Sayem%2520Nazmuz%2520Zaman%2520and%2520Alaa%2520Eldin%2520Abdelaal%2520and%2520Septimiu%2520E.%2520Salcudean%26entry.1292438233%3D%2520%2520Incorporating%2520an%2520autonomous%2520auxiliary%2520camera%2520into%2520robot-assisted%2520minimally%250Ainvasive%2520surgery%2520%2528RAMIS%2529%2520enhances%2520spatial%2520awareness%2520and%2520eliminates%2520manual%250Aviewpoint%2520control.%2520Existing%2520path%2520planning%2520methods%2520for%2520auxiliary%2520cameras%2520track%250Atwo-dimensional%2520surgical%2520features%2520but%2520do%2520not%2520simultaneously%2520account%2520for%2520camera%250Aorientation%252C%2520workspace%2520constraints%252C%2520and%2520robot%2520joint%2520limits.%2520This%2520study%2520presents%250AAutoCam%253A%2520an%2520automatic%2520auxiliary%2520camera%2520placement%2520method%2520to%2520improve%250Avisualization%2520in%2520RAMIS.%2520Implemented%2520on%2520the%2520da%2520Vinci%2520Research%2520Kit%252C%2520the%2520system%250Auses%2520a%2520priority-based%252C%2520workspace-constrained%2520control%2520algorithm%2520that%2520combines%250Aheuristic%2520geometric%2520placement%2520with%2520nonlinear%2520optimization%2520to%2520ensure%2520robust%250Acamera%2520tracking.%2520A%2520user%2520study%2520%2528N%253D6%2529%2520demonstrated%2520that%2520the%2520system%2520maintained%250A99.84%2525%2520visibility%2520of%2520a%2520salient%2520feature%2520and%2520achieved%2520a%2520pose%2520error%2520of%25204.36%2520%2524%255Cpm%2524%250A2.11%2520degrees%2520and%25201.95%2520%2524%255Cpm%2524%25205.66%2520mm.%2520The%2520controller%2520was%2520computationally%250Aefficient%252C%2520with%2520a%2520loop%2520time%2520of%25206.8%2520%2524%255Cpm%2524%252012.8%2520ms.%2520An%2520additional%2520pilot%2520study%250A%2528N%253D6%2529%252C%2520where%2520novices%2520completed%2520a%2520Fundamentals%2520of%2520Laparoscopic%2520Surgery%2520training%250Atask%252C%2520suggests%2520that%2520users%2520can%2520teleoperate%2520just%2520as%2520effectively%2520from%2520AutoCam%2527s%250Aviewpoint%2520as%2520from%2520the%2520endoscope%2527s%2520while%2520still%2520benefiting%2520from%2520AutoCam%2527s%250Aimproved%2520visual%2520coverage%2520of%2520the%2520scene.%2520These%2520results%2520indicate%2520that%2520an%2520auxiliary%250Acamera%2520can%2520be%2520autonomously%2520controlled%2520using%2520the%2520da%2520Vinci%2520patient-side%250Amanipulators%2520to%2520track%2520a%2520salient%2520feature%252C%2520laying%2520the%2520groundwork%2520for%2520new%250Amulti-camera%2520visualization%2520methods%2520in%2520RAMIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoCam%3A%20Hierarchical%20Path%20Planning%20for%20an%20Autonomous%20Auxiliary%20Camera%0A%20%20in%20Surgical%20Robotics&entry.906535625=Alexandre%20Banks%20and%20Randy%20Moore%20and%20Sayem%20Nazmuz%20Zaman%20and%20Alaa%20Eldin%20Abdelaal%20and%20Septimiu%20E.%20Salcudean&entry.1292438233=%20%20Incorporating%20an%20autonomous%20auxiliary%20camera%20into%20robot-assisted%20minimally%0Ainvasive%20surgery%20%28RAMIS%29%20enhances%20spatial%20awareness%20and%20eliminates%20manual%0Aviewpoint%20control.%20Existing%20path%20planning%20methods%20for%20auxiliary%20cameras%20track%0Atwo-dimensional%20surgical%20features%20but%20do%20not%20simultaneously%20account%20for%20camera%0Aorientation%2C%20workspace%20constraints%2C%20and%20robot%20joint%20limits.%20This%20study%20presents%0AAutoCam%3A%20an%20automatic%20auxiliary%20camera%20placement%20method%20to%20improve%0Avisualization%20in%20RAMIS.%20Implemented%20on%20the%20da%20Vinci%20Research%20Kit%2C%20the%20system%0Auses%20a%20priority-based%2C%20workspace-constrained%20control%20algorithm%20that%20combines%0Aheuristic%20geometric%20placement%20with%20nonlinear%20optimization%20to%20ensure%20robust%0Acamera%20tracking.%20A%20user%20study%20%28N%3D6%29%20demonstrated%20that%20the%20system%20maintained%0A99.84%25%20visibility%20of%20a%20salient%20feature%20and%20achieved%20a%20pose%20error%20of%204.36%20%24%5Cpm%24%0A2.11%20degrees%20and%201.95%20%24%5Cpm%24%205.66%20mm.%20The%20controller%20was%20computationally%0Aefficient%2C%20with%20a%20loop%20time%20of%206.8%20%24%5Cpm%24%2012.8%20ms.%20An%20additional%20pilot%20study%0A%28N%3D6%29%2C%20where%20novices%20completed%20a%20Fundamentals%20of%20Laparoscopic%20Surgery%20training%0Atask%2C%20suggests%20that%20users%20can%20teleoperate%20just%20as%20effectively%20from%20AutoCam%27s%0Aviewpoint%20as%20from%20the%20endoscope%27s%20while%20still%20benefiting%20from%20AutoCam%27s%0Aimproved%20visual%20coverage%20of%20the%20scene.%20These%20results%20indicate%20that%20an%20auxiliary%0Acamera%20can%20be%20autonomously%20controlled%20using%20the%20da%20Vinci%20patient-side%0Amanipulators%20to%20track%20a%20salient%20feature%2C%20laying%20the%20groundwork%20for%20new%0Amulti-camera%20visualization%20methods%20in%20RAMIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10398v1&entry.124074799=Read"},
{"title": "Emergence of Structure in Ensembles of Random Neural Networks", "author": "Luca Muscarnera and Luigi Loreti and Giovanni Todeschini and Alessio Fumagalli and Francesco Regazzoni", "abstract": "  Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.\n", "link": "http://arxiv.org/abs/2505.10331v1", "date": "2025-05-15", "relevancy": 2.3146, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4681}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.463}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Structure%20in%20Ensembles%20of%20Random%20Neural%20Networks&body=Title%3A%20Emergence%20of%20Structure%20in%20Ensembles%20of%20Random%20Neural%20Networks%0AAuthor%3A%20Luca%20Muscarnera%20and%20Luigi%20Loreti%20and%20Giovanni%20Todeschini%20and%20Alessio%20Fumagalli%20and%20Francesco%20Regazzoni%0AAbstract%3A%20%20%20Randomness%20is%20ubiquitous%20in%20many%20applications%20across%20data%20science%20and%20machine%0Alearning.%20Remarkably%2C%20systems%20composed%20of%20random%20components%20often%20display%0Aemergent%20global%20behaviors%20that%20appear%20deterministic%2C%20manifesting%20a%20transition%0Afrom%20microscopic%20disorder%20to%20macroscopic%20organization.%20In%20this%20work%2C%20we%0Aintroduce%20a%20theoretical%20model%20for%20studying%20the%20emergence%20of%20collective%0Abehaviors%20in%20ensembles%20of%20random%20classifiers.%20We%20argue%20that%2C%20if%20the%20ensemble%20is%0Aweighted%20through%20the%20Gibbs%20measure%20defined%20by%20adopting%20the%20classification%20loss%0Aas%20an%20energy%2C%20then%20there%20exists%20a%20finite%20temperature%20parameter%20for%20the%0Adistribution%20such%20that%20the%20classification%20is%20optimal%2C%20with%20respect%20to%20the%20loss%0A%28or%20the%20energy%29.%20Interestingly%2C%20for%20the%20case%20in%20which%20samples%20are%20generated%20by%0Aa%20Gaussian%20distribution%20and%20labels%20are%20constructed%20by%20employing%20a%20teacher%0Aperceptron%2C%20we%20analytically%20prove%20and%20numerically%20confirm%20that%20such%20optimal%0Atemperature%20does%20not%20depend%20neither%20on%20the%20teacher%20classifier%20%28which%20is%2C%20by%0Aconstruction%20of%20the%20learning%20problem%2C%20unknown%29%2C%20nor%20on%20the%20number%20of%20random%0Aclassifiers%2C%20highlighting%20the%20universal%20nature%20of%20the%20observed%20behavior.%0AExperiments%20on%20the%20MNIST%20dataset%20underline%20the%20relevance%20of%20this%20phenomenon%20in%0Ahigh-quality%2C%20noiseless%2C%20datasets.%20Finally%2C%20a%20physical%20analogy%20allows%20us%20to%0Ashed%20light%20on%20the%20self-organizing%20nature%20of%20the%20studied%20phenomenon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Structure%2520in%2520Ensembles%2520of%2520Random%2520Neural%2520Networks%26entry.906535625%3DLuca%2520Muscarnera%2520and%2520Luigi%2520Loreti%2520and%2520Giovanni%2520Todeschini%2520and%2520Alessio%2520Fumagalli%2520and%2520Francesco%2520Regazzoni%26entry.1292438233%3D%2520%2520Randomness%2520is%2520ubiquitous%2520in%2520many%2520applications%2520across%2520data%2520science%2520and%2520machine%250Alearning.%2520Remarkably%252C%2520systems%2520composed%2520of%2520random%2520components%2520often%2520display%250Aemergent%2520global%2520behaviors%2520that%2520appear%2520deterministic%252C%2520manifesting%2520a%2520transition%250Afrom%2520microscopic%2520disorder%2520to%2520macroscopic%2520organization.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520theoretical%2520model%2520for%2520studying%2520the%2520emergence%2520of%2520collective%250Abehaviors%2520in%2520ensembles%2520of%2520random%2520classifiers.%2520We%2520argue%2520that%252C%2520if%2520the%2520ensemble%2520is%250Aweighted%2520through%2520the%2520Gibbs%2520measure%2520defined%2520by%2520adopting%2520the%2520classification%2520loss%250Aas%2520an%2520energy%252C%2520then%2520there%2520exists%2520a%2520finite%2520temperature%2520parameter%2520for%2520the%250Adistribution%2520such%2520that%2520the%2520classification%2520is%2520optimal%252C%2520with%2520respect%2520to%2520the%2520loss%250A%2528or%2520the%2520energy%2529.%2520Interestingly%252C%2520for%2520the%2520case%2520in%2520which%2520samples%2520are%2520generated%2520by%250Aa%2520Gaussian%2520distribution%2520and%2520labels%2520are%2520constructed%2520by%2520employing%2520a%2520teacher%250Aperceptron%252C%2520we%2520analytically%2520prove%2520and%2520numerically%2520confirm%2520that%2520such%2520optimal%250Atemperature%2520does%2520not%2520depend%2520neither%2520on%2520the%2520teacher%2520classifier%2520%2528which%2520is%252C%2520by%250Aconstruction%2520of%2520the%2520learning%2520problem%252C%2520unknown%2529%252C%2520nor%2520on%2520the%2520number%2520of%2520random%250Aclassifiers%252C%2520highlighting%2520the%2520universal%2520nature%2520of%2520the%2520observed%2520behavior.%250AExperiments%2520on%2520the%2520MNIST%2520dataset%2520underline%2520the%2520relevance%2520of%2520this%2520phenomenon%2520in%250Ahigh-quality%252C%2520noiseless%252C%2520datasets.%2520Finally%252C%2520a%2520physical%2520analogy%2520allows%2520us%2520to%250Ashed%2520light%2520on%2520the%2520self-organizing%2520nature%2520of%2520the%2520studied%2520phenomenon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Structure%20in%20Ensembles%20of%20Random%20Neural%20Networks&entry.906535625=Luca%20Muscarnera%20and%20Luigi%20Loreti%20and%20Giovanni%20Todeschini%20and%20Alessio%20Fumagalli%20and%20Francesco%20Regazzoni&entry.1292438233=%20%20Randomness%20is%20ubiquitous%20in%20many%20applications%20across%20data%20science%20and%20machine%0Alearning.%20Remarkably%2C%20systems%20composed%20of%20random%20components%20often%20display%0Aemergent%20global%20behaviors%20that%20appear%20deterministic%2C%20manifesting%20a%20transition%0Afrom%20microscopic%20disorder%20to%20macroscopic%20organization.%20In%20this%20work%2C%20we%0Aintroduce%20a%20theoretical%20model%20for%20studying%20the%20emergence%20of%20collective%0Abehaviors%20in%20ensembles%20of%20random%20classifiers.%20We%20argue%20that%2C%20if%20the%20ensemble%20is%0Aweighted%20through%20the%20Gibbs%20measure%20defined%20by%20adopting%20the%20classification%20loss%0Aas%20an%20energy%2C%20then%20there%20exists%20a%20finite%20temperature%20parameter%20for%20the%0Adistribution%20such%20that%20the%20classification%20is%20optimal%2C%20with%20respect%20to%20the%20loss%0A%28or%20the%20energy%29.%20Interestingly%2C%20for%20the%20case%20in%20which%20samples%20are%20generated%20by%0Aa%20Gaussian%20distribution%20and%20labels%20are%20constructed%20by%20employing%20a%20teacher%0Aperceptron%2C%20we%20analytically%20prove%20and%20numerically%20confirm%20that%20such%20optimal%0Atemperature%20does%20not%20depend%20neither%20on%20the%20teacher%20classifier%20%28which%20is%2C%20by%0Aconstruction%20of%20the%20learning%20problem%2C%20unknown%29%2C%20nor%20on%20the%20number%20of%20random%0Aclassifiers%2C%20highlighting%20the%20universal%20nature%20of%20the%20observed%20behavior.%0AExperiments%20on%20the%20MNIST%20dataset%20underline%20the%20relevance%20of%20this%20phenomenon%20in%0Ahigh-quality%2C%20noiseless%2C%20datasets.%20Finally%2C%20a%20physical%20analogy%20allows%20us%20to%0Ashed%20light%20on%20the%20self-organizing%20nature%20of%20the%20studied%20phenomenon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10331v1&entry.124074799=Read"},
{"title": "Pose Priors from Language Models", "author": "Sanjay Subramanian and Evonne Ng and Lea M\u00fcller and Dan Klein and Shiry Ginosar and Trevor Darrell", "abstract": "  Language is often used to describe physical interaction, yet most 3D human\npose estimation methods overlook this rich source of information. We bridge\nthis gap by leveraging large multimodal models (LMMs) as priors for\nreconstructing contact poses, offering a scalable alternative to traditional\nmethods that rely on human annotations or motion capture data. Our approach\nextracts contact-relevant descriptors from an LMM and translates them into\ntractable losses to constrain 3D human pose optimization. Despite its\nsimplicity, our method produces compelling reconstructions for both two-person\ninteractions and self-contact scenarios, accurately capturing the semantics of\nphysical and social interactions. Our results demonstrate that LMMs can serve\nas powerful tools for contact prediction and pose estimation, offering an\nalternative to costly manual human annotations or motion capture data. Our code\nis publicly available at https://prosepose.github.io.\n", "link": "http://arxiv.org/abs/2405.03689v2", "date": "2025-05-15", "relevancy": 2.3055, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%20Priors%20from%20Language%20Models&body=Title%3A%20Pose%20Priors%20from%20Language%20Models%0AAuthor%3A%20Sanjay%20Subramanian%20and%20Evonne%20Ng%20and%20Lea%20M%C3%BCller%20and%20Dan%20Klein%20and%20Shiry%20Ginosar%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20Language%20is%20often%20used%20to%20describe%20physical%20interaction%2C%20yet%20most%203D%20human%0Apose%20estimation%20methods%20overlook%20this%20rich%20source%20of%20information.%20We%20bridge%0Athis%20gap%20by%20leveraging%20large%20multimodal%20models%20%28LMMs%29%20as%20priors%20for%0Areconstructing%20contact%20poses%2C%20offering%20a%20scalable%20alternative%20to%20traditional%0Amethods%20that%20rely%20on%20human%20annotations%20or%20motion%20capture%20data.%20Our%20approach%0Aextracts%20contact-relevant%20descriptors%20from%20an%20LMM%20and%20translates%20them%20into%0Atractable%20losses%20to%20constrain%203D%20human%20pose%20optimization.%20Despite%20its%0Asimplicity%2C%20our%20method%20produces%20compelling%20reconstructions%20for%20both%20two-person%0Ainteractions%20and%20self-contact%20scenarios%2C%20accurately%20capturing%20the%20semantics%20of%0Aphysical%20and%20social%20interactions.%20Our%20results%20demonstrate%20that%20LMMs%20can%20serve%0Aas%20powerful%20tools%20for%20contact%20prediction%20and%20pose%20estimation%2C%20offering%20an%0Aalternative%20to%20costly%20manual%20human%20annotations%20or%20motion%20capture%20data.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//prosepose.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%2520Priors%2520from%2520Language%2520Models%26entry.906535625%3DSanjay%2520Subramanian%2520and%2520Evonne%2520Ng%2520and%2520Lea%2520M%25C3%25BCller%2520and%2520Dan%2520Klein%2520and%2520Shiry%2520Ginosar%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520Language%2520is%2520often%2520used%2520to%2520describe%2520physical%2520interaction%252C%2520yet%2520most%25203D%2520human%250Apose%2520estimation%2520methods%2520overlook%2520this%2520rich%2520source%2520of%2520information.%2520We%2520bridge%250Athis%2520gap%2520by%2520leveraging%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520as%2520priors%2520for%250Areconstructing%2520contact%2520poses%252C%2520offering%2520a%2520scalable%2520alternative%2520to%2520traditional%250Amethods%2520that%2520rely%2520on%2520human%2520annotations%2520or%2520motion%2520capture%2520data.%2520Our%2520approach%250Aextracts%2520contact-relevant%2520descriptors%2520from%2520an%2520LMM%2520and%2520translates%2520them%2520into%250Atractable%2520losses%2520to%2520constrain%25203D%2520human%2520pose%2520optimization.%2520Despite%2520its%250Asimplicity%252C%2520our%2520method%2520produces%2520compelling%2520reconstructions%2520for%2520both%2520two-person%250Ainteractions%2520and%2520self-contact%2520scenarios%252C%2520accurately%2520capturing%2520the%2520semantics%2520of%250Aphysical%2520and%2520social%2520interactions.%2520Our%2520results%2520demonstrate%2520that%2520LMMs%2520can%2520serve%250Aas%2520powerful%2520tools%2520for%2520contact%2520prediction%2520and%2520pose%2520estimation%252C%2520offering%2520an%250Aalternative%2520to%2520costly%2520manual%2520human%2520annotations%2520or%2520motion%2520capture%2520data.%2520Our%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//prosepose.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%20Priors%20from%20Language%20Models&entry.906535625=Sanjay%20Subramanian%20and%20Evonne%20Ng%20and%20Lea%20M%C3%BCller%20and%20Dan%20Klein%20and%20Shiry%20Ginosar%20and%20Trevor%20Darrell&entry.1292438233=%20%20Language%20is%20often%20used%20to%20describe%20physical%20interaction%2C%20yet%20most%203D%20human%0Apose%20estimation%20methods%20overlook%20this%20rich%20source%20of%20information.%20We%20bridge%0Athis%20gap%20by%20leveraging%20large%20multimodal%20models%20%28LMMs%29%20as%20priors%20for%0Areconstructing%20contact%20poses%2C%20offering%20a%20scalable%20alternative%20to%20traditional%0Amethods%20that%20rely%20on%20human%20annotations%20or%20motion%20capture%20data.%20Our%20approach%0Aextracts%20contact-relevant%20descriptors%20from%20an%20LMM%20and%20translates%20them%20into%0Atractable%20losses%20to%20constrain%203D%20human%20pose%20optimization.%20Despite%20its%0Asimplicity%2C%20our%20method%20produces%20compelling%20reconstructions%20for%20both%20two-person%0Ainteractions%20and%20self-contact%20scenarios%2C%20accurately%20capturing%20the%20semantics%20of%0Aphysical%20and%20social%20interactions.%20Our%20results%20demonstrate%20that%20LMMs%20can%20serve%0Aas%20powerful%20tools%20for%20contact%20prediction%20and%20pose%20estimation%2C%20offering%20an%0Aalternative%20to%20costly%20manual%20human%20annotations%20or%20motion%20capture%20data.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//prosepose.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03689v2&entry.124074799=Read"},
{"title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent", "author": "Xiao Xia and Dan Zhang and Zibo Liao and Zhenyu Hou and Tianrui Sun and Jing Li and Ling Fu and Yuxiao Dong", "abstract": "  The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .\n", "link": "http://arxiv.org/abs/2410.21909v2", "date": "2025-05-15", "relevancy": 2.3016, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent&body=Title%3A%20SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent%0AAuthor%3A%20Xiao%20Xia%20and%20Dan%20Zhang%20and%20Zibo%20Liao%20and%20Zhenyu%20Hou%20and%20Tianrui%20Sun%20and%20Jing%20Li%20and%20Ling%20Fu%20and%20Yuxiao%20Dong%0AAbstract%3A%20%20%20The%20modeling%20of%20industrial%20scenes%20is%20essential%20for%20simulations%20in%20industrial%0Amanufacturing.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Aprogress%20in%20generating%20general%203D%20scenes%20from%20textual%20descriptions%2C%20generating%0Aindustrial%20scenes%20with%20LLMs%20poses%20a%20unique%20challenge%20due%20to%20their%20demand%20for%0Aprecise%20measurements%20and%20positioning%2C%20requiring%20complex%20planning%20over%20spatial%0Aarrangement.%20To%20address%20this%20challenge%2C%20we%20introduce%20SceneGenAgent%2C%20an%0ALLM-based%20agent%20for%20generating%20industrial%20scenes%20through%20C%23%20code.%20SceneGenAgent%0Aensures%20precise%20layout%20planning%20through%20a%20structured%20and%20calculable%20format%2C%0Alayout%20verification%2C%20and%20iterative%20refinement%20to%20meet%20the%20quantitative%0Arequirements%20of%20industrial%20scenarios.%20Experiment%20results%20demonstrate%20that%20LLMs%0Apowered%20by%20SceneGenAgent%20exceed%20their%20original%20performance%2C%20reaching%20up%20to%0A81.0%25%20success%20rate%20in%20real-world%20industrial%20scene%20generation%20tasks%20and%0Aeffectively%20meeting%20most%20scene%20generation%20requirements.%20To%20further%20enhance%0Aaccessibility%2C%20we%20construct%20SceneInstruct%2C%20a%20dataset%20designed%20for%20fine-tuning%0Aopen-source%20LLMs%20to%20integrate%20into%20SceneGenAgent.%20Experiments%20show%20that%0Afine-tuning%20open-source%20LLMs%20on%20SceneInstruct%20yields%20significant%20performance%0Aimprovements%2C%20with%20Llama3.1-70B%20approaching%20the%20capabilities%20of%20GPT-4o.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/THUDM/SceneGenAgent%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneGenAgent%253A%2520Precise%2520Industrial%2520Scene%2520Generation%2520with%2520Coding%2520Agent%26entry.906535625%3DXiao%2520Xia%2520and%2520Dan%2520Zhang%2520and%2520Zibo%2520Liao%2520and%2520Zhenyu%2520Hou%2520and%2520Tianrui%2520Sun%2520and%2520Jing%2520Li%2520and%2520Ling%2520Fu%2520and%2520Yuxiao%2520Dong%26entry.1292438233%3D%2520%2520The%2520modeling%2520of%2520industrial%2520scenes%2520is%2520essential%2520for%2520simulations%2520in%2520industrial%250Amanufacturing.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%250Aprogress%2520in%2520generating%2520general%25203D%2520scenes%2520from%2520textual%2520descriptions%252C%2520generating%250Aindustrial%2520scenes%2520with%2520LLMs%2520poses%2520a%2520unique%2520challenge%2520due%2520to%2520their%2520demand%2520for%250Aprecise%2520measurements%2520and%2520positioning%252C%2520requiring%2520complex%2520planning%2520over%2520spatial%250Aarrangement.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SceneGenAgent%252C%2520an%250ALLM-based%2520agent%2520for%2520generating%2520industrial%2520scenes%2520through%2520C%2523%2520code.%2520SceneGenAgent%250Aensures%2520precise%2520layout%2520planning%2520through%2520a%2520structured%2520and%2520calculable%2520format%252C%250Alayout%2520verification%252C%2520and%2520iterative%2520refinement%2520to%2520meet%2520the%2520quantitative%250Arequirements%2520of%2520industrial%2520scenarios.%2520Experiment%2520results%2520demonstrate%2520that%2520LLMs%250Apowered%2520by%2520SceneGenAgent%2520exceed%2520their%2520original%2520performance%252C%2520reaching%2520up%2520to%250A81.0%2525%2520success%2520rate%2520in%2520real-world%2520industrial%2520scene%2520generation%2520tasks%2520and%250Aeffectively%2520meeting%2520most%2520scene%2520generation%2520requirements.%2520To%2520further%2520enhance%250Aaccessibility%252C%2520we%2520construct%2520SceneInstruct%252C%2520a%2520dataset%2520designed%2520for%2520fine-tuning%250Aopen-source%2520LLMs%2520to%2520integrate%2520into%2520SceneGenAgent.%2520Experiments%2520show%2520that%250Afine-tuning%2520open-source%2520LLMs%2520on%2520SceneInstruct%2520yields%2520significant%2520performance%250Aimprovements%252C%2520with%2520Llama3.1-70B%2520approaching%2520the%2520capabilities%2520of%2520GPT-4o.%2520Our%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/THUDM/SceneGenAgent%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneGenAgent%3A%20Precise%20Industrial%20Scene%20Generation%20with%20Coding%20Agent&entry.906535625=Xiao%20Xia%20and%20Dan%20Zhang%20and%20Zibo%20Liao%20and%20Zhenyu%20Hou%20and%20Tianrui%20Sun%20and%20Jing%20Li%20and%20Ling%20Fu%20and%20Yuxiao%20Dong&entry.1292438233=%20%20The%20modeling%20of%20industrial%20scenes%20is%20essential%20for%20simulations%20in%20industrial%0Amanufacturing.%20While%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Aprogress%20in%20generating%20general%203D%20scenes%20from%20textual%20descriptions%2C%20generating%0Aindustrial%20scenes%20with%20LLMs%20poses%20a%20unique%20challenge%20due%20to%20their%20demand%20for%0Aprecise%20measurements%20and%20positioning%2C%20requiring%20complex%20planning%20over%20spatial%0Aarrangement.%20To%20address%20this%20challenge%2C%20we%20introduce%20SceneGenAgent%2C%20an%0ALLM-based%20agent%20for%20generating%20industrial%20scenes%20through%20C%23%20code.%20SceneGenAgent%0Aensures%20precise%20layout%20planning%20through%20a%20structured%20and%20calculable%20format%2C%0Alayout%20verification%2C%20and%20iterative%20refinement%20to%20meet%20the%20quantitative%0Arequirements%20of%20industrial%20scenarios.%20Experiment%20results%20demonstrate%20that%20LLMs%0Apowered%20by%20SceneGenAgent%20exceed%20their%20original%20performance%2C%20reaching%20up%20to%0A81.0%25%20success%20rate%20in%20real-world%20industrial%20scene%20generation%20tasks%20and%0Aeffectively%20meeting%20most%20scene%20generation%20requirements.%20To%20further%20enhance%0Aaccessibility%2C%20we%20construct%20SceneInstruct%2C%20a%20dataset%20designed%20for%20fine-tuning%0Aopen-source%20LLMs%20to%20integrate%20into%20SceneGenAgent.%20Experiments%20show%20that%0Afine-tuning%20open-source%20LLMs%20on%20SceneInstruct%20yields%20significant%20performance%0Aimprovements%2C%20with%20Llama3.1-70B%20approaching%20the%20capabilities%20of%20GPT-4o.%20Our%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/THUDM/SceneGenAgent%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21909v2&entry.124074799=Read"},
{"title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models", "author": "Mugilan Ganesan and Shane Segal and Ankur Aggarwal and Nish Sinnadurai and Sean Lie and Vithursan Thangarasa", "abstract": "  Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.\n", "link": "http://arxiv.org/abs/2505.10526v1", "date": "2025-05-15", "relevancy": 2.2886, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASSV%3A%20Multimodal%20Adaptation%20and%20Self-Data%20Distillation%20for%20Speculative%0A%20%20Decoding%20of%20Vision-Language%20Models&body=Title%3A%20MASSV%3A%20Multimodal%20Adaptation%20and%20Self-Data%20Distillation%20for%20Speculative%0A%20%20Decoding%20of%20Vision-Language%20Models%0AAuthor%3A%20Mugilan%20Ganesan%20and%20Shane%20Segal%20and%20Ankur%20Aggarwal%20and%20Nish%20Sinnadurai%20and%20Sean%20Lie%20and%20Vithursan%20Thangarasa%0AAbstract%3A%20%20%20Speculative%20decoding%20significantly%20accelerates%20language%20model%20inference%20by%0Aenabling%20a%20lightweight%20draft%20model%20to%20propose%20multiple%20tokens%20that%20a%20larger%0Atarget%20model%20verifies%20simultaneously.%20However%2C%20applying%20this%20technique%20to%0Avision-language%20models%20%28VLMs%29%20presents%20two%20fundamental%20challenges%3A%20small%0Alanguage%20models%20that%20could%20serve%20as%20efficient%20drafters%20lack%20the%20architectural%0Acomponents%20to%20process%20visual%20inputs%2C%20and%20their%20token%20predictions%20fail%20to%20match%0Athose%20of%20VLM%20target%20models%20that%20consider%20visual%20context.%20We%20introduce%0AMultimodal%20Adaptation%20and%20Self-Data%20Distillation%20for%20Speculative%20Decoding%20of%0AVision-Language%20Models%20%28MASSV%29%2C%20which%20transforms%20existing%20small%20language%20models%0Ainto%20effective%20multimodal%20drafters%20through%20a%20two-phase%20approach.%20MASSV%20first%0Aconnects%20the%20target%20VLM%27s%20vision%20encoder%20to%20the%20draft%20model%20via%20a%20lightweight%0Atrainable%20projector%2C%20then%20applies%20self-distilled%20visual%20instruction%20tuning%0Ausing%20responses%20generated%20by%20the%20target%20VLM%20to%20align%20token%20predictions.%0AComprehensive%20experiments%20across%20the%20Qwen2.5-VL%20and%20Gemma3%20model%20families%0Ademonstrate%20that%20MASSV%20increases%20accepted%20length%20by%20up%20to%2030%25%20and%20delivers%0Aend-to-end%20inference%20speedups%20of%20up%20to%201.46x%20on%20visually-grounded%20tasks.%20MASSV%0Aprovides%20a%20scalable%2C%20architecture-compatible%20method%20for%20accelerating%20both%0Acurrent%20and%20future%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASSV%253A%2520Multimodal%2520Adaptation%2520and%2520Self-Data%2520Distillation%2520for%2520Speculative%250A%2520%2520Decoding%2520of%2520Vision-Language%2520Models%26entry.906535625%3DMugilan%2520Ganesan%2520and%2520Shane%2520Segal%2520and%2520Ankur%2520Aggarwal%2520and%2520Nish%2520Sinnadurai%2520and%2520Sean%2520Lie%2520and%2520Vithursan%2520Thangarasa%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520significantly%2520accelerates%2520language%2520model%2520inference%2520by%250Aenabling%2520a%2520lightweight%2520draft%2520model%2520to%2520propose%2520multiple%2520tokens%2520that%2520a%2520larger%250Atarget%2520model%2520verifies%2520simultaneously.%2520However%252C%2520applying%2520this%2520technique%2520to%250Avision-language%2520models%2520%2528VLMs%2529%2520presents%2520two%2520fundamental%2520challenges%253A%2520small%250Alanguage%2520models%2520that%2520could%2520serve%2520as%2520efficient%2520drafters%2520lack%2520the%2520architectural%250Acomponents%2520to%2520process%2520visual%2520inputs%252C%2520and%2520their%2520token%2520predictions%2520fail%2520to%2520match%250Athose%2520of%2520VLM%2520target%2520models%2520that%2520consider%2520visual%2520context.%2520We%2520introduce%250AMultimodal%2520Adaptation%2520and%2520Self-Data%2520Distillation%2520for%2520Speculative%2520Decoding%2520of%250AVision-Language%2520Models%2520%2528MASSV%2529%252C%2520which%2520transforms%2520existing%2520small%2520language%2520models%250Ainto%2520effective%2520multimodal%2520drafters%2520through%2520a%2520two-phase%2520approach.%2520MASSV%2520first%250Aconnects%2520the%2520target%2520VLM%2527s%2520vision%2520encoder%2520to%2520the%2520draft%2520model%2520via%2520a%2520lightweight%250Atrainable%2520projector%252C%2520then%2520applies%2520self-distilled%2520visual%2520instruction%2520tuning%250Ausing%2520responses%2520generated%2520by%2520the%2520target%2520VLM%2520to%2520align%2520token%2520predictions.%250AComprehensive%2520experiments%2520across%2520the%2520Qwen2.5-VL%2520and%2520Gemma3%2520model%2520families%250Ademonstrate%2520that%2520MASSV%2520increases%2520accepted%2520length%2520by%2520up%2520to%252030%2525%2520and%2520delivers%250Aend-to-end%2520inference%2520speedups%2520of%2520up%2520to%25201.46x%2520on%2520visually-grounded%2520tasks.%2520MASSV%250Aprovides%2520a%2520scalable%252C%2520architecture-compatible%2520method%2520for%2520accelerating%2520both%250Acurrent%2520and%2520future%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASSV%3A%20Multimodal%20Adaptation%20and%20Self-Data%20Distillation%20for%20Speculative%0A%20%20Decoding%20of%20Vision-Language%20Models&entry.906535625=Mugilan%20Ganesan%20and%20Shane%20Segal%20and%20Ankur%20Aggarwal%20and%20Nish%20Sinnadurai%20and%20Sean%20Lie%20and%20Vithursan%20Thangarasa&entry.1292438233=%20%20Speculative%20decoding%20significantly%20accelerates%20language%20model%20inference%20by%0Aenabling%20a%20lightweight%20draft%20model%20to%20propose%20multiple%20tokens%20that%20a%20larger%0Atarget%20model%20verifies%20simultaneously.%20However%2C%20applying%20this%20technique%20to%0Avision-language%20models%20%28VLMs%29%20presents%20two%20fundamental%20challenges%3A%20small%0Alanguage%20models%20that%20could%20serve%20as%20efficient%20drafters%20lack%20the%20architectural%0Acomponents%20to%20process%20visual%20inputs%2C%20and%20their%20token%20predictions%20fail%20to%20match%0Athose%20of%20VLM%20target%20models%20that%20consider%20visual%20context.%20We%20introduce%0AMultimodal%20Adaptation%20and%20Self-Data%20Distillation%20for%20Speculative%20Decoding%20of%0AVision-Language%20Models%20%28MASSV%29%2C%20which%20transforms%20existing%20small%20language%20models%0Ainto%20effective%20multimodal%20drafters%20through%20a%20two-phase%20approach.%20MASSV%20first%0Aconnects%20the%20target%20VLM%27s%20vision%20encoder%20to%20the%20draft%20model%20via%20a%20lightweight%0Atrainable%20projector%2C%20then%20applies%20self-distilled%20visual%20instruction%20tuning%0Ausing%20responses%20generated%20by%20the%20target%20VLM%20to%20align%20token%20predictions.%0AComprehensive%20experiments%20across%20the%20Qwen2.5-VL%20and%20Gemma3%20model%20families%0Ademonstrate%20that%20MASSV%20increases%20accepted%20length%20by%20up%20to%2030%25%20and%20delivers%0Aend-to-end%20inference%20speedups%20of%20up%20to%201.46x%20on%20visually-grounded%20tasks.%20MASSV%0Aprovides%20a%20scalable%2C%20architecture-compatible%20method%20for%20accelerating%20both%0Acurrent%20and%20future%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10526v1&entry.124074799=Read"},
{"title": "Estimating the number of household TV profiles based in customer\n  behaviour using Gaussian mixture model averaging", "author": "Gabriel R. Palma and Sally McClean and Brahim Allan and Zeeshan Tariq and Rafael A. Moral", "abstract": "  TV customers today face many choices from many live channels and on-demand\nservices. Providing a personalised experience that saves customers time when\ndiscovering content is essential for TV providers. However, a reliable\nunderstanding of their behaviour and preferences is key. When creating\npersonalised recommendations for TV, the biggest challenge is understanding\nviewing behaviour within households when multiple people are watching. The\nobjective is to detect and combine individual profiles to make\nbetter-personalised recommendations for group viewing. Our challenge is that we\nhave little explicit information about who is watching the devices at any time\n(individuals or groups). Also, we do not have a way to combine more than one\nindividual profile to make better recommendations for group viewing. We propose\na novel framework using a Gaussian mixture model averaging to obtain point\nestimates for the number of household TV profiles and a Bayesian random walk\nmodel to introduce uncertainty. We applied our approach using data from real\ncustomers whose TV-watching data totalled approximately half a million\nobservations. Our results indicate that combining our framework with the\nselected features provides a means to estimate the number of household TV\nprofiles and their characteristics, including shifts over time and\nquantification of uncertainty.\n", "link": "http://arxiv.org/abs/2505.10279v1", "date": "2025-05-15", "relevancy": 2.2788, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.456}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.456}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20number%20of%20household%20TV%20profiles%20based%20in%20customer%0A%20%20behaviour%20using%20Gaussian%20mixture%20model%20averaging&body=Title%3A%20Estimating%20the%20number%20of%20household%20TV%20profiles%20based%20in%20customer%0A%20%20behaviour%20using%20Gaussian%20mixture%20model%20averaging%0AAuthor%3A%20Gabriel%20R.%20Palma%20and%20Sally%20McClean%20and%20Brahim%20Allan%20and%20Zeeshan%20Tariq%20and%20Rafael%20A.%20Moral%0AAbstract%3A%20%20%20TV%20customers%20today%20face%20many%20choices%20from%20many%20live%20channels%20and%20on-demand%0Aservices.%20Providing%20a%20personalised%20experience%20that%20saves%20customers%20time%20when%0Adiscovering%20content%20is%20essential%20for%20TV%20providers.%20However%2C%20a%20reliable%0Aunderstanding%20of%20their%20behaviour%20and%20preferences%20is%20key.%20When%20creating%0Apersonalised%20recommendations%20for%20TV%2C%20the%20biggest%20challenge%20is%20understanding%0Aviewing%20behaviour%20within%20households%20when%20multiple%20people%20are%20watching.%20The%0Aobjective%20is%20to%20detect%20and%20combine%20individual%20profiles%20to%20make%0Abetter-personalised%20recommendations%20for%20group%20viewing.%20Our%20challenge%20is%20that%20we%0Ahave%20little%20explicit%20information%20about%20who%20is%20watching%20the%20devices%20at%20any%20time%0A%28individuals%20or%20groups%29.%20Also%2C%20we%20do%20not%20have%20a%20way%20to%20combine%20more%20than%20one%0Aindividual%20profile%20to%20make%20better%20recommendations%20for%20group%20viewing.%20We%20propose%0Aa%20novel%20framework%20using%20a%20Gaussian%20mixture%20model%20averaging%20to%20obtain%20point%0Aestimates%20for%20the%20number%20of%20household%20TV%20profiles%20and%20a%20Bayesian%20random%20walk%0Amodel%20to%20introduce%20uncertainty.%20We%20applied%20our%20approach%20using%20data%20from%20real%0Acustomers%20whose%20TV-watching%20data%20totalled%20approximately%20half%20a%20million%0Aobservations.%20Our%20results%20indicate%20that%20combining%20our%20framework%20with%20the%0Aselected%20features%20provides%20a%20means%20to%20estimate%20the%20number%20of%20household%20TV%0Aprofiles%20and%20their%20characteristics%2C%20including%20shifts%20over%20time%20and%0Aquantification%20of%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520number%2520of%2520household%2520TV%2520profiles%2520based%2520in%2520customer%250A%2520%2520behaviour%2520using%2520Gaussian%2520mixture%2520model%2520averaging%26entry.906535625%3DGabriel%2520R.%2520Palma%2520and%2520Sally%2520McClean%2520and%2520Brahim%2520Allan%2520and%2520Zeeshan%2520Tariq%2520and%2520Rafael%2520A.%2520Moral%26entry.1292438233%3D%2520%2520TV%2520customers%2520today%2520face%2520many%2520choices%2520from%2520many%2520live%2520channels%2520and%2520on-demand%250Aservices.%2520Providing%2520a%2520personalised%2520experience%2520that%2520saves%2520customers%2520time%2520when%250Adiscovering%2520content%2520is%2520essential%2520for%2520TV%2520providers.%2520However%252C%2520a%2520reliable%250Aunderstanding%2520of%2520their%2520behaviour%2520and%2520preferences%2520is%2520key.%2520When%2520creating%250Apersonalised%2520recommendations%2520for%2520TV%252C%2520the%2520biggest%2520challenge%2520is%2520understanding%250Aviewing%2520behaviour%2520within%2520households%2520when%2520multiple%2520people%2520are%2520watching.%2520The%250Aobjective%2520is%2520to%2520detect%2520and%2520combine%2520individual%2520profiles%2520to%2520make%250Abetter-personalised%2520recommendations%2520for%2520group%2520viewing.%2520Our%2520challenge%2520is%2520that%2520we%250Ahave%2520little%2520explicit%2520information%2520about%2520who%2520is%2520watching%2520the%2520devices%2520at%2520any%2520time%250A%2528individuals%2520or%2520groups%2529.%2520Also%252C%2520we%2520do%2520not%2520have%2520a%2520way%2520to%2520combine%2520more%2520than%2520one%250Aindividual%2520profile%2520to%2520make%2520better%2520recommendations%2520for%2520group%2520viewing.%2520We%2520propose%250Aa%2520novel%2520framework%2520using%2520a%2520Gaussian%2520mixture%2520model%2520averaging%2520to%2520obtain%2520point%250Aestimates%2520for%2520the%2520number%2520of%2520household%2520TV%2520profiles%2520and%2520a%2520Bayesian%2520random%2520walk%250Amodel%2520to%2520introduce%2520uncertainty.%2520We%2520applied%2520our%2520approach%2520using%2520data%2520from%2520real%250Acustomers%2520whose%2520TV-watching%2520data%2520totalled%2520approximately%2520half%2520a%2520million%250Aobservations.%2520Our%2520results%2520indicate%2520that%2520combining%2520our%2520framework%2520with%2520the%250Aselected%2520features%2520provides%2520a%2520means%2520to%2520estimate%2520the%2520number%2520of%2520household%2520TV%250Aprofiles%2520and%2520their%2520characteristics%252C%2520including%2520shifts%2520over%2520time%2520and%250Aquantification%2520of%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20number%20of%20household%20TV%20profiles%20based%20in%20customer%0A%20%20behaviour%20using%20Gaussian%20mixture%20model%20averaging&entry.906535625=Gabriel%20R.%20Palma%20and%20Sally%20McClean%20and%20Brahim%20Allan%20and%20Zeeshan%20Tariq%20and%20Rafael%20A.%20Moral&entry.1292438233=%20%20TV%20customers%20today%20face%20many%20choices%20from%20many%20live%20channels%20and%20on-demand%0Aservices.%20Providing%20a%20personalised%20experience%20that%20saves%20customers%20time%20when%0Adiscovering%20content%20is%20essential%20for%20TV%20providers.%20However%2C%20a%20reliable%0Aunderstanding%20of%20their%20behaviour%20and%20preferences%20is%20key.%20When%20creating%0Apersonalised%20recommendations%20for%20TV%2C%20the%20biggest%20challenge%20is%20understanding%0Aviewing%20behaviour%20within%20households%20when%20multiple%20people%20are%20watching.%20The%0Aobjective%20is%20to%20detect%20and%20combine%20individual%20profiles%20to%20make%0Abetter-personalised%20recommendations%20for%20group%20viewing.%20Our%20challenge%20is%20that%20we%0Ahave%20little%20explicit%20information%20about%20who%20is%20watching%20the%20devices%20at%20any%20time%0A%28individuals%20or%20groups%29.%20Also%2C%20we%20do%20not%20have%20a%20way%20to%20combine%20more%20than%20one%0Aindividual%20profile%20to%20make%20better%20recommendations%20for%20group%20viewing.%20We%20propose%0Aa%20novel%20framework%20using%20a%20Gaussian%20mixture%20model%20averaging%20to%20obtain%20point%0Aestimates%20for%20the%20number%20of%20household%20TV%20profiles%20and%20a%20Bayesian%20random%20walk%0Amodel%20to%20introduce%20uncertainty.%20We%20applied%20our%20approach%20using%20data%20from%20real%0Acustomers%20whose%20TV-watching%20data%20totalled%20approximately%20half%20a%20million%0Aobservations.%20Our%20results%20indicate%20that%20combining%20our%20framework%20with%20the%0Aselected%20features%20provides%20a%20means%20to%20estimate%20the%20number%20of%20household%20TV%0Aprofiles%20and%20their%20characteristics%2C%20including%20shifts%20over%20time%20and%0Aquantification%20of%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10279v1&entry.124074799=Read"},
{"title": "Rethinking Repetition Problems of LLMs in Code Generation", "author": "Yihong Dong and Yuchen Liu and Xue Jiang and Zhi Jin and Ge Li", "abstract": "  With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.\n", "link": "http://arxiv.org/abs/2505.10402v1", "date": "2025-05-15", "relevancy": 2.2517, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4512}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Repetition%20Problems%20of%20LLMs%20in%20Code%20Generation&body=Title%3A%20Rethinking%20Repetition%20Problems%20of%20LLMs%20in%20Code%20Generation%0AAuthor%3A%20Yihong%20Dong%20and%20Yuchen%20Liu%20and%20Xue%20Jiang%20and%20Zhi%20Jin%20and%20Ge%20Li%0AAbstract%3A%20%20%20With%20the%20advent%20of%20neural%20language%20models%2C%20the%20performance%20of%20code%20generation%0Ahas%20been%20significantly%20boosted.%20However%2C%20the%20problem%20of%20repetitions%20during%20the%0Ageneration%20process%20continues%20to%20linger.%20Previous%20work%20has%20primarily%20focused%20on%0Acontent%20repetition%2C%20which%20is%20merely%20a%20fraction%20of%20the%20broader%20repetition%0Aproblem%20in%20code%20generation.%20A%20more%20prevalent%20and%20challenging%20problem%20is%0Astructural%20repetition.%20In%20structural%20repetition%2C%20the%20repeated%20code%20appears%20in%0Avarious%20patterns%20but%20possesses%20a%20fixed%20structure%2C%20which%20can%20be%20inherently%0Areflected%20in%20grammar.%20In%20this%20paper%2C%20we%20formally%20define%20structural%20repetition%0Aand%20propose%20an%20efficient%20decoding%20approach%20called%20RPG%2C%20which%20stands%20for%0ARepetition%20Penalization%20based%20on%20Grammar%2C%20to%20alleviate%20the%20repetition%20problems%0Ain%20code%20generation%20for%20LLMs.%20Specifically%2C%20RPG%20first%20leverages%20grammar%20rules%20to%0Aidentify%20repetition%20problems%20during%20code%20generation%2C%20and%20then%20strategically%0Adecays%20the%20likelihood%20of%20critical%20tokens%20that%20contribute%20to%20repetitions%2C%0Athereby%20mitigating%20them%20in%20code%20generation.%20To%20facilitate%20this%20study%2C%20we%0Aconstruct%20a%20new%20dataset%20CodeRepetEval%20to%20comprehensively%20evaluate%20approaches%0Afor%20mitigating%20the%20repetition%20problems%20in%20code%20generation.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20RPG%20substantially%20outperforms%20the%0Abest-performing%20baselines%20on%20CodeRepetEval%20dataset%20as%20well%20as%20HumanEval%20and%0AMBPP%20benchmarks%2C%20effectively%20reducing%20repetitions%20and%20enhancing%20the%20quality%20of%0Agenerated%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Repetition%2520Problems%2520of%2520LLMs%2520in%2520Code%2520Generation%26entry.906535625%3DYihong%2520Dong%2520and%2520Yuchen%2520Liu%2520and%2520Xue%2520Jiang%2520and%2520Zhi%2520Jin%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520neural%2520language%2520models%252C%2520the%2520performance%2520of%2520code%2520generation%250Ahas%2520been%2520significantly%2520boosted.%2520However%252C%2520the%2520problem%2520of%2520repetitions%2520during%2520the%250Ageneration%2520process%2520continues%2520to%2520linger.%2520Previous%2520work%2520has%2520primarily%2520focused%2520on%250Acontent%2520repetition%252C%2520which%2520is%2520merely%2520a%2520fraction%2520of%2520the%2520broader%2520repetition%250Aproblem%2520in%2520code%2520generation.%2520A%2520more%2520prevalent%2520and%2520challenging%2520problem%2520is%250Astructural%2520repetition.%2520In%2520structural%2520repetition%252C%2520the%2520repeated%2520code%2520appears%2520in%250Avarious%2520patterns%2520but%2520possesses%2520a%2520fixed%2520structure%252C%2520which%2520can%2520be%2520inherently%250Areflected%2520in%2520grammar.%2520In%2520this%2520paper%252C%2520we%2520formally%2520define%2520structural%2520repetition%250Aand%2520propose%2520an%2520efficient%2520decoding%2520approach%2520called%2520RPG%252C%2520which%2520stands%2520for%250ARepetition%2520Penalization%2520based%2520on%2520Grammar%252C%2520to%2520alleviate%2520the%2520repetition%2520problems%250Ain%2520code%2520generation%2520for%2520LLMs.%2520Specifically%252C%2520RPG%2520first%2520leverages%2520grammar%2520rules%2520to%250Aidentify%2520repetition%2520problems%2520during%2520code%2520generation%252C%2520and%2520then%2520strategically%250Adecays%2520the%2520likelihood%2520of%2520critical%2520tokens%2520that%2520contribute%2520to%2520repetitions%252C%250Athereby%2520mitigating%2520them%2520in%2520code%2520generation.%2520To%2520facilitate%2520this%2520study%252C%2520we%250Aconstruct%2520a%2520new%2520dataset%2520CodeRepetEval%2520to%2520comprehensively%2520evaluate%2520approaches%250Afor%2520mitigating%2520the%2520repetition%2520problems%2520in%2520code%2520generation.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520RPG%2520substantially%2520outperforms%2520the%250Abest-performing%2520baselines%2520on%2520CodeRepetEval%2520dataset%2520as%2520well%2520as%2520HumanEval%2520and%250AMBPP%2520benchmarks%252C%2520effectively%2520reducing%2520repetitions%2520and%2520enhancing%2520the%2520quality%2520of%250Agenerated%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Repetition%20Problems%20of%20LLMs%20in%20Code%20Generation&entry.906535625=Yihong%20Dong%20and%20Yuchen%20Liu%20and%20Xue%20Jiang%20and%20Zhi%20Jin%20and%20Ge%20Li&entry.1292438233=%20%20With%20the%20advent%20of%20neural%20language%20models%2C%20the%20performance%20of%20code%20generation%0Ahas%20been%20significantly%20boosted.%20However%2C%20the%20problem%20of%20repetitions%20during%20the%0Ageneration%20process%20continues%20to%20linger.%20Previous%20work%20has%20primarily%20focused%20on%0Acontent%20repetition%2C%20which%20is%20merely%20a%20fraction%20of%20the%20broader%20repetition%0Aproblem%20in%20code%20generation.%20A%20more%20prevalent%20and%20challenging%20problem%20is%0Astructural%20repetition.%20In%20structural%20repetition%2C%20the%20repeated%20code%20appears%20in%0Avarious%20patterns%20but%20possesses%20a%20fixed%20structure%2C%20which%20can%20be%20inherently%0Areflected%20in%20grammar.%20In%20this%20paper%2C%20we%20formally%20define%20structural%20repetition%0Aand%20propose%20an%20efficient%20decoding%20approach%20called%20RPG%2C%20which%20stands%20for%0ARepetition%20Penalization%20based%20on%20Grammar%2C%20to%20alleviate%20the%20repetition%20problems%0Ain%20code%20generation%20for%20LLMs.%20Specifically%2C%20RPG%20first%20leverages%20grammar%20rules%20to%0Aidentify%20repetition%20problems%20during%20code%20generation%2C%20and%20then%20strategically%0Adecays%20the%20likelihood%20of%20critical%20tokens%20that%20contribute%20to%20repetitions%2C%0Athereby%20mitigating%20them%20in%20code%20generation.%20To%20facilitate%20this%20study%2C%20we%0Aconstruct%20a%20new%20dataset%20CodeRepetEval%20to%20comprehensively%20evaluate%20approaches%0Afor%20mitigating%20the%20repetition%20problems%20in%20code%20generation.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20RPG%20substantially%20outperforms%20the%0Abest-performing%20baselines%20on%20CodeRepetEval%20dataset%20as%20well%20as%20HumanEval%20and%0AMBPP%20benchmarks%2C%20effectively%20reducing%20repetitions%20and%20enhancing%20the%20quality%20of%0Agenerated%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10402v1&entry.124074799=Read"},
{"title": "UniEval: Unified Holistic Evaluation for Unified Multimodal\n  Understanding and Generation", "author": "Yi Li and Haonan Wang and Qixiang Zhang and Boyu Xiao and Chenchang Hu and Hualiang Wang and Xiaomeng Li", "abstract": "  The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.\n", "link": "http://arxiv.org/abs/2505.10483v1", "date": "2025-05-15", "relevancy": 2.2491, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniEval%3A%20Unified%20Holistic%20Evaluation%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation&body=Title%3A%20UniEval%3A%20Unified%20Holistic%20Evaluation%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation%0AAuthor%3A%20Yi%20Li%20and%20Haonan%20Wang%20and%20Qixiang%20Zhang%20and%20Boyu%20Xiao%20and%20Chenchang%20Hu%20and%20Hualiang%20Wang%20and%20Xiaomeng%20Li%0AAbstract%3A%20%20%20The%20emergence%20of%20unified%20multimodal%20understanding%20and%20generation%20models%20is%0Arapidly%20attracting%20attention%20because%20of%20their%20ability%20to%20enhance%0Ainstruction-following%20capabilities%20while%20minimizing%20model%20redundancy.%20However%2C%0Athere%20is%20a%20lack%20of%20a%20unified%20evaluation%20framework%20for%20these%20models%2C%20which%20would%0Aenable%20an%20elegant%2C%20simplified%2C%20and%20overall%20evaluation.%20Current%20models%20conduct%0Aevaluations%20on%20multiple%20task-specific%20benchmarks%2C%20but%20there%20are%20significant%0Alimitations%2C%20such%20as%20the%20lack%20of%20overall%20results%2C%20errors%20from%20extra%20evaluation%0Amodels%2C%20reliance%20on%20extensive%20labeled%20images%2C%20benchmarks%20that%20lack%20diversity%2C%0Aand%20metrics%20with%20limited%20capacity%20for%20instruction-following%20evaluation.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%20UniEval%2C%20the%20first%20evaluation%20framework%0Adesigned%20for%20unified%20multimodal%20models%20without%20extra%20models%2C%20images%2C%20or%0Aannotations.%20This%20facilitates%20a%20simplified%20and%20unified%20evaluation%20process.%20The%0AUniEval%20framework%20contains%20a%20holistic%20benchmark%2C%20UniBench%20%28supports%20both%0Aunified%20and%20visual%20generation%20models%29%2C%20along%20with%20the%20corresponding%20UniScore%0Ametric.%20UniBench%20includes%2081%20fine-grained%20tags%20contributing%20to%20high%20diversity.%0AExperimental%20results%20indicate%20that%20UniBench%20is%20more%20challenging%20than%20existing%0Abenchmarks%2C%20and%20UniScore%20aligns%20closely%20with%20human%20evaluations%2C%20surpassing%0Acurrent%20metrics.%20Moreover%2C%20we%20extensively%20evaluated%20SoTA%20unified%20and%20visual%0Ageneration%20models%2C%20uncovering%20new%20insights%20into%20Univeral%27s%20unique%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniEval%253A%2520Unified%2520Holistic%2520Evaluation%2520for%2520Unified%2520Multimodal%250A%2520%2520Understanding%2520and%2520Generation%26entry.906535625%3DYi%2520Li%2520and%2520Haonan%2520Wang%2520and%2520Qixiang%2520Zhang%2520and%2520Boyu%2520Xiao%2520and%2520Chenchang%2520Hu%2520and%2520Hualiang%2520Wang%2520and%2520Xiaomeng%2520Li%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520unified%2520multimodal%2520understanding%2520and%2520generation%2520models%2520is%250Arapidly%2520attracting%2520attention%2520because%2520of%2520their%2520ability%2520to%2520enhance%250Ainstruction-following%2520capabilities%2520while%2520minimizing%2520model%2520redundancy.%2520However%252C%250Athere%2520is%2520a%2520lack%2520of%2520a%2520unified%2520evaluation%2520framework%2520for%2520these%2520models%252C%2520which%2520would%250Aenable%2520an%2520elegant%252C%2520simplified%252C%2520and%2520overall%2520evaluation.%2520Current%2520models%2520conduct%250Aevaluations%2520on%2520multiple%2520task-specific%2520benchmarks%252C%2520but%2520there%2520are%2520significant%250Alimitations%252C%2520such%2520as%2520the%2520lack%2520of%2520overall%2520results%252C%2520errors%2520from%2520extra%2520evaluation%250Amodels%252C%2520reliance%2520on%2520extensive%2520labeled%2520images%252C%2520benchmarks%2520that%2520lack%2520diversity%252C%250Aand%2520metrics%2520with%2520limited%2520capacity%2520for%2520instruction-following%2520evaluation.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520introduce%2520UniEval%252C%2520the%2520first%2520evaluation%2520framework%250Adesigned%2520for%2520unified%2520multimodal%2520models%2520without%2520extra%2520models%252C%2520images%252C%2520or%250Aannotations.%2520This%2520facilitates%2520a%2520simplified%2520and%2520unified%2520evaluation%2520process.%2520The%250AUniEval%2520framework%2520contains%2520a%2520holistic%2520benchmark%252C%2520UniBench%2520%2528supports%2520both%250Aunified%2520and%2520visual%2520generation%2520models%2529%252C%2520along%2520with%2520the%2520corresponding%2520UniScore%250Ametric.%2520UniBench%2520includes%252081%2520fine-grained%2520tags%2520contributing%2520to%2520high%2520diversity.%250AExperimental%2520results%2520indicate%2520that%2520UniBench%2520is%2520more%2520challenging%2520than%2520existing%250Abenchmarks%252C%2520and%2520UniScore%2520aligns%2520closely%2520with%2520human%2520evaluations%252C%2520surpassing%250Acurrent%2520metrics.%2520Moreover%252C%2520we%2520extensively%2520evaluated%2520SoTA%2520unified%2520and%2520visual%250Ageneration%2520models%252C%2520uncovering%2520new%2520insights%2520into%2520Univeral%2527s%2520unique%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniEval%3A%20Unified%20Holistic%20Evaluation%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation&entry.906535625=Yi%20Li%20and%20Haonan%20Wang%20and%20Qixiang%20Zhang%20and%20Boyu%20Xiao%20and%20Chenchang%20Hu%20and%20Hualiang%20Wang%20and%20Xiaomeng%20Li&entry.1292438233=%20%20The%20emergence%20of%20unified%20multimodal%20understanding%20and%20generation%20models%20is%0Arapidly%20attracting%20attention%20because%20of%20their%20ability%20to%20enhance%0Ainstruction-following%20capabilities%20while%20minimizing%20model%20redundancy.%20However%2C%0Athere%20is%20a%20lack%20of%20a%20unified%20evaluation%20framework%20for%20these%20models%2C%20which%20would%0Aenable%20an%20elegant%2C%20simplified%2C%20and%20overall%20evaluation.%20Current%20models%20conduct%0Aevaluations%20on%20multiple%20task-specific%20benchmarks%2C%20but%20there%20are%20significant%0Alimitations%2C%20such%20as%20the%20lack%20of%20overall%20results%2C%20errors%20from%20extra%20evaluation%0Amodels%2C%20reliance%20on%20extensive%20labeled%20images%2C%20benchmarks%20that%20lack%20diversity%2C%0Aand%20metrics%20with%20limited%20capacity%20for%20instruction-following%20evaluation.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%20UniEval%2C%20the%20first%20evaluation%20framework%0Adesigned%20for%20unified%20multimodal%20models%20without%20extra%20models%2C%20images%2C%20or%0Aannotations.%20This%20facilitates%20a%20simplified%20and%20unified%20evaluation%20process.%20The%0AUniEval%20framework%20contains%20a%20holistic%20benchmark%2C%20UniBench%20%28supports%20both%0Aunified%20and%20visual%20generation%20models%29%2C%20along%20with%20the%20corresponding%20UniScore%0Ametric.%20UniBench%20includes%2081%20fine-grained%20tags%20contributing%20to%20high%20diversity.%0AExperimental%20results%20indicate%20that%20UniBench%20is%20more%20challenging%20than%20existing%0Abenchmarks%2C%20and%20UniScore%20aligns%20closely%20with%20human%20evaluations%2C%20surpassing%0Acurrent%20metrics.%20Moreover%2C%20we%20extensively%20evaluated%20SoTA%20unified%20and%20visual%0Ageneration%20models%2C%20uncovering%20new%20insights%20into%20Univeral%27s%20unique%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10483v1&entry.124074799=Read"},
{"title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding\n  and Grounded Story Generation", "author": "Daniel A. P. Oliveira and David Martins de Matos", "abstract": "  Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.\n", "link": "http://arxiv.org/abs/2505.10292v1", "date": "2025-05-15", "relevancy": 2.2416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryReasoning%20Dataset%3A%20Using%20Chain-of-Thought%20for%20Scene%20Understanding%0A%20%20and%20Grounded%20Story%20Generation&body=Title%3A%20StoryReasoning%20Dataset%3A%20Using%20Chain-of-Thought%20for%20Scene%20Understanding%0A%20%20and%20Grounded%20Story%20Generation%0AAuthor%3A%20Daniel%20A.%20P.%20Oliveira%20and%20David%20Martins%20de%20Matos%0AAbstract%3A%20%20%20Visual%20storytelling%20systems%20struggle%20to%20maintain%20character%20identity%20across%0Aframes%20and%20link%20actions%20to%20appropriate%20subjects%2C%20frequently%20leading%20to%0Areferential%20hallucinations.%20These%20issues%20can%20be%20addressed%20through%20grounding%20of%0Acharacters%2C%20objects%2C%20and%20other%20entities%20on%20the%20visual%20elements.%20We%20propose%0AStoryReasoning%2C%20a%20dataset%20containing%204%2C178%20stories%20derived%20from%2052%2C016%20movie%0Aimages%2C%20with%20both%20structured%20scene%20analyses%20and%20grounded%20stories.%20Each%20story%0Amaintains%20character%20and%20object%20consistency%20across%20frames%20while%20explicitly%0Amodeling%20multi-frame%20relationships%20through%20structured%20tabular%20representations.%0AOur%20approach%20features%20cross-frame%20object%20re-identification%20using%20visual%0Asimilarity%20and%20face%20recognition%2C%20chain-of-thought%20reasoning%20for%20explicit%0Anarrative%20modeling%2C%20and%20a%20grounding%20scheme%20that%20links%20textual%20elements%20to%0Avisual%20entities%20across%20multiple%20frames.%20We%20establish%20baseline%20performance%20by%0Afine-tuning%20Qwen2.5-VL%207B%2C%20creating%20Qwen%20Storyteller%2C%20which%20performs%20end-to-end%0Aobject%20detection%2C%20re-identification%2C%20and%20landmark%20detection%20while%20maintaining%0Aconsistent%20object%20references%20throughout%20the%20story.%20Evaluation%20demonstrates%20a%0Areduction%20from%204.06%20to%203.56%20%28-12.3%25%29%20hallucinations%20on%20average%20per%20story%20when%0Acompared%20to%20a%20non-fine-tuned%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryReasoning%2520Dataset%253A%2520Using%2520Chain-of-Thought%2520for%2520Scene%2520Understanding%250A%2520%2520and%2520Grounded%2520Story%2520Generation%26entry.906535625%3DDaniel%2520A.%2520P.%2520Oliveira%2520and%2520David%2520Martins%2520de%2520Matos%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520systems%2520struggle%2520to%2520maintain%2520character%2520identity%2520across%250Aframes%2520and%2520link%2520actions%2520to%2520appropriate%2520subjects%252C%2520frequently%2520leading%2520to%250Areferential%2520hallucinations.%2520These%2520issues%2520can%2520be%2520addressed%2520through%2520grounding%2520of%250Acharacters%252C%2520objects%252C%2520and%2520other%2520entities%2520on%2520the%2520visual%2520elements.%2520We%2520propose%250AStoryReasoning%252C%2520a%2520dataset%2520containing%25204%252C178%2520stories%2520derived%2520from%252052%252C016%2520movie%250Aimages%252C%2520with%2520both%2520structured%2520scene%2520analyses%2520and%2520grounded%2520stories.%2520Each%2520story%250Amaintains%2520character%2520and%2520object%2520consistency%2520across%2520frames%2520while%2520explicitly%250Amodeling%2520multi-frame%2520relationships%2520through%2520structured%2520tabular%2520representations.%250AOur%2520approach%2520features%2520cross-frame%2520object%2520re-identification%2520using%2520visual%250Asimilarity%2520and%2520face%2520recognition%252C%2520chain-of-thought%2520reasoning%2520for%2520explicit%250Anarrative%2520modeling%252C%2520and%2520a%2520grounding%2520scheme%2520that%2520links%2520textual%2520elements%2520to%250Avisual%2520entities%2520across%2520multiple%2520frames.%2520We%2520establish%2520baseline%2520performance%2520by%250Afine-tuning%2520Qwen2.5-VL%25207B%252C%2520creating%2520Qwen%2520Storyteller%252C%2520which%2520performs%2520end-to-end%250Aobject%2520detection%252C%2520re-identification%252C%2520and%2520landmark%2520detection%2520while%2520maintaining%250Aconsistent%2520object%2520references%2520throughout%2520the%2520story.%2520Evaluation%2520demonstrates%2520a%250Areduction%2520from%25204.06%2520to%25203.56%2520%2528-12.3%2525%2529%2520hallucinations%2520on%2520average%2520per%2520story%2520when%250Acompared%2520to%2520a%2520non-fine-tuned%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryReasoning%20Dataset%3A%20Using%20Chain-of-Thought%20for%20Scene%20Understanding%0A%20%20and%20Grounded%20Story%20Generation&entry.906535625=Daniel%20A.%20P.%20Oliveira%20and%20David%20Martins%20de%20Matos&entry.1292438233=%20%20Visual%20storytelling%20systems%20struggle%20to%20maintain%20character%20identity%20across%0Aframes%20and%20link%20actions%20to%20appropriate%20subjects%2C%20frequently%20leading%20to%0Areferential%20hallucinations.%20These%20issues%20can%20be%20addressed%20through%20grounding%20of%0Acharacters%2C%20objects%2C%20and%20other%20entities%20on%20the%20visual%20elements.%20We%20propose%0AStoryReasoning%2C%20a%20dataset%20containing%204%2C178%20stories%20derived%20from%2052%2C016%20movie%0Aimages%2C%20with%20both%20structured%20scene%20analyses%20and%20grounded%20stories.%20Each%20story%0Amaintains%20character%20and%20object%20consistency%20across%20frames%20while%20explicitly%0Amodeling%20multi-frame%20relationships%20through%20structured%20tabular%20representations.%0AOur%20approach%20features%20cross-frame%20object%20re-identification%20using%20visual%0Asimilarity%20and%20face%20recognition%2C%20chain-of-thought%20reasoning%20for%20explicit%0Anarrative%20modeling%2C%20and%20a%20grounding%20scheme%20that%20links%20textual%20elements%20to%0Avisual%20entities%20across%20multiple%20frames.%20We%20establish%20baseline%20performance%20by%0Afine-tuning%20Qwen2.5-VL%207B%2C%20creating%20Qwen%20Storyteller%2C%20which%20performs%20end-to-end%0Aobject%20detection%2C%20re-identification%2C%20and%20landmark%20detection%20while%20maintaining%0Aconsistent%20object%20references%20throughout%20the%20story.%20Evaluation%20demonstrates%20a%0Areduction%20from%204.06%20to%203.56%20%28-12.3%25%29%20hallucinations%20on%20average%20per%20story%20when%0Acompared%20to%20a%20non-fine-tuned%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10292v1&entry.124074799=Read"},
{"title": "R2VF: A Two-Step Regularization Algorithm to Cluster Categories in GLMs", "author": "Yuval Ben Dror", "abstract": "  Over recent decades, extensive research has aimed to overcome the restrictive\nunderlying assumptions required for a Generalized Linear Model to generate\naccurate and meaningful predictions. These efforts include regularizing\ncoefficients, selecting features, and clustering ordinal categories, among\nother approaches. Despite these advances, efficiently clustering nominal\ncategories in GLMs without incurring high computational costs remains a\nchallenge. This paper introduces Ranking to Variable Fusion (R2VF), a two-step\nmethod designed to efficiently fuse nominal and ordinal categories in GLMs. By\nfirst transforming nominal features into an ordinal framework via regularized\nregression and then applying variable fusion, R2VF strikes a balance between\nmodel complexity and interpretability. We demonstrate the effectiveness of R2VF\nthrough comparisons with other methods, highlighting its performance in\naddressing overfitting and identifying an appropriate set of covariates.\n", "link": "http://arxiv.org/abs/2503.01521v2", "date": "2025-05-15", "relevancy": 2.2327, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4649}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2VF%3A%20A%20Two-Step%20Regularization%20Algorithm%20to%20Cluster%20Categories%20in%20GLMs&body=Title%3A%20R2VF%3A%20A%20Two-Step%20Regularization%20Algorithm%20to%20Cluster%20Categories%20in%20GLMs%0AAuthor%3A%20Yuval%20Ben%20Dror%0AAbstract%3A%20%20%20Over%20recent%20decades%2C%20extensive%20research%20has%20aimed%20to%20overcome%20the%20restrictive%0Aunderlying%20assumptions%20required%20for%20a%20Generalized%20Linear%20Model%20to%20generate%0Aaccurate%20and%20meaningful%20predictions.%20These%20efforts%20include%20regularizing%0Acoefficients%2C%20selecting%20features%2C%20and%20clustering%20ordinal%20categories%2C%20among%0Aother%20approaches.%20Despite%20these%20advances%2C%20efficiently%20clustering%20nominal%0Acategories%20in%20GLMs%20without%20incurring%20high%20computational%20costs%20remains%20a%0Achallenge.%20This%20paper%20introduces%20Ranking%20to%20Variable%20Fusion%20%28R2VF%29%2C%20a%20two-step%0Amethod%20designed%20to%20efficiently%20fuse%20nominal%20and%20ordinal%20categories%20in%20GLMs.%20By%0Afirst%20transforming%20nominal%20features%20into%20an%20ordinal%20framework%20via%20regularized%0Aregression%20and%20then%20applying%20variable%20fusion%2C%20R2VF%20strikes%20a%20balance%20between%0Amodel%20complexity%20and%20interpretability.%20We%20demonstrate%20the%20effectiveness%20of%20R2VF%0Athrough%20comparisons%20with%20other%20methods%2C%20highlighting%20its%20performance%20in%0Aaddressing%20overfitting%20and%20identifying%20an%20appropriate%20set%20of%20covariates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2VF%253A%2520A%2520Two-Step%2520Regularization%2520Algorithm%2520to%2520Cluster%2520Categories%2520in%2520GLMs%26entry.906535625%3DYuval%2520Ben%2520Dror%26entry.1292438233%3D%2520%2520Over%2520recent%2520decades%252C%2520extensive%2520research%2520has%2520aimed%2520to%2520overcome%2520the%2520restrictive%250Aunderlying%2520assumptions%2520required%2520for%2520a%2520Generalized%2520Linear%2520Model%2520to%2520generate%250Aaccurate%2520and%2520meaningful%2520predictions.%2520These%2520efforts%2520include%2520regularizing%250Acoefficients%252C%2520selecting%2520features%252C%2520and%2520clustering%2520ordinal%2520categories%252C%2520among%250Aother%2520approaches.%2520Despite%2520these%2520advances%252C%2520efficiently%2520clustering%2520nominal%250Acategories%2520in%2520GLMs%2520without%2520incurring%2520high%2520computational%2520costs%2520remains%2520a%250Achallenge.%2520This%2520paper%2520introduces%2520Ranking%2520to%2520Variable%2520Fusion%2520%2528R2VF%2529%252C%2520a%2520two-step%250Amethod%2520designed%2520to%2520efficiently%2520fuse%2520nominal%2520and%2520ordinal%2520categories%2520in%2520GLMs.%2520By%250Afirst%2520transforming%2520nominal%2520features%2520into%2520an%2520ordinal%2520framework%2520via%2520regularized%250Aregression%2520and%2520then%2520applying%2520variable%2520fusion%252C%2520R2VF%2520strikes%2520a%2520balance%2520between%250Amodel%2520complexity%2520and%2520interpretability.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520R2VF%250Athrough%2520comparisons%2520with%2520other%2520methods%252C%2520highlighting%2520its%2520performance%2520in%250Aaddressing%2520overfitting%2520and%2520identifying%2520an%2520appropriate%2520set%2520of%2520covariates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2VF%3A%20A%20Two-Step%20Regularization%20Algorithm%20to%20Cluster%20Categories%20in%20GLMs&entry.906535625=Yuval%20Ben%20Dror&entry.1292438233=%20%20Over%20recent%20decades%2C%20extensive%20research%20has%20aimed%20to%20overcome%20the%20restrictive%0Aunderlying%20assumptions%20required%20for%20a%20Generalized%20Linear%20Model%20to%20generate%0Aaccurate%20and%20meaningful%20predictions.%20These%20efforts%20include%20regularizing%0Acoefficients%2C%20selecting%20features%2C%20and%20clustering%20ordinal%20categories%2C%20among%0Aother%20approaches.%20Despite%20these%20advances%2C%20efficiently%20clustering%20nominal%0Acategories%20in%20GLMs%20without%20incurring%20high%20computational%20costs%20remains%20a%0Achallenge.%20This%20paper%20introduces%20Ranking%20to%20Variable%20Fusion%20%28R2VF%29%2C%20a%20two-step%0Amethod%20designed%20to%20efficiently%20fuse%20nominal%20and%20ordinal%20categories%20in%20GLMs.%20By%0Afirst%20transforming%20nominal%20features%20into%20an%20ordinal%20framework%20via%20regularized%0Aregression%20and%20then%20applying%20variable%20fusion%2C%20R2VF%20strikes%20a%20balance%20between%0Amodel%20complexity%20and%20interpretability.%20We%20demonstrate%20the%20effectiveness%20of%20R2VF%0Athrough%20comparisons%20with%20other%20methods%2C%20highlighting%20its%20performance%20in%0Aaddressing%20overfitting%20and%20identifying%20an%20appropriate%20set%20of%20covariates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01521v2&entry.124074799=Read"},
{"title": "TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile\n  Graphics for Individuals with Vision Impairment", "author": "Adnan Khan and Alireza Choubineh and Mai A. Shaaban and Abbas Akkasi and Majid Komeili", "abstract": "  Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss. Traditional methods for\ncreating these graphics are labor-intensive and cannot meet growing demand. We\nintroduce TactileNet, the first comprehensive dataset and AI-driven framework\nfor generating embossing-ready 2D tactile templates using text-to-image Stable\nDiffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and\nDreamBooth, our method fine-tunes SD models to produce high-fidelity,\nguideline-compliant graphics while reducing computational costs. Quantitative\nevaluations with tactile experts show 92.86% adherence to accessibility\nstandards. Structural fidelity analysis revealed near-human design similarity,\nwith an SSIM of 0.538 between generated graphics and expert-designed tactile\nimages. Notably, our method preserves object silhouettes better than human\ndesigns (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation\nof manual tactile abstraction. The framework scales to 32,000 images (7,050\nhigh-quality) across 66 classes, with prompt editing enabling customizable\noutputs (e.g., adding or removing details). By automating the 2D template\ngeneration step-compatible with standard embossing workflows-TactileNet\naccelerates production while preserving design flexibility. This work\ndemonstrates how AI can augment (not replace) human expertise to bridge the\naccessibility gap in education and beyond. Code, data, and models will be\npublicly released to foster further research.\n", "link": "http://arxiv.org/abs/2504.04722v2", "date": "2025-05-15", "relevancy": 2.2308, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5638}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5557}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TactileNet%3A%20Bridging%20the%20Accessibility%20Gap%20with%20AI-Generated%20Tactile%0A%20%20Graphics%20for%20Individuals%20with%20Vision%20Impairment&body=Title%3A%20TactileNet%3A%20Bridging%20the%20Accessibility%20Gap%20with%20AI-Generated%20Tactile%0A%20%20Graphics%20for%20Individuals%20with%20Vision%20Impairment%0AAuthor%3A%20Adnan%20Khan%20and%20Alireza%20Choubineh%20and%20Mai%20A.%20Shaaban%20and%20Abbas%20Akkasi%20and%20Majid%20Komeili%0AAbstract%3A%20%20%20Tactile%20graphics%20are%20essential%20for%20providing%20access%20to%20visual%20information%20for%0Athe%2043%20million%20people%20globally%20living%20with%20vision%20loss.%20Traditional%20methods%20for%0Acreating%20these%20graphics%20are%20labor-intensive%20and%20cannot%20meet%20growing%20demand.%20We%0Aintroduce%20TactileNet%2C%20the%20first%20comprehensive%20dataset%20and%20AI-driven%20framework%0Afor%20generating%20embossing-ready%202D%20tactile%20templates%20using%20text-to-image%20Stable%0ADiffusion%20%28SD%29%20models.%20By%20integrating%20Low-Rank%20Adaptation%20%28LoRA%29%20and%0ADreamBooth%2C%20our%20method%20fine-tunes%20SD%20models%20to%20produce%20high-fidelity%2C%0Aguideline-compliant%20graphics%20while%20reducing%20computational%20costs.%20Quantitative%0Aevaluations%20with%20tactile%20experts%20show%2092.86%25%20adherence%20to%20accessibility%0Astandards.%20Structural%20fidelity%20analysis%20revealed%20near-human%20design%20similarity%2C%0Awith%20an%20SSIM%20of%200.538%20between%20generated%20graphics%20and%20expert-designed%20tactile%0Aimages.%20Notably%2C%20our%20method%20preserves%20object%20silhouettes%20better%20than%20human%0Adesigns%20%28SSIM%20%3D%200.259%20vs.%200.215%20for%20binary%20masks%29%2C%20addressing%20a%20key%20limitation%0Aof%20manual%20tactile%20abstraction.%20The%20framework%20scales%20to%2032%2C000%20images%20%287%2C050%0Ahigh-quality%29%20across%2066%20classes%2C%20with%20prompt%20editing%20enabling%20customizable%0Aoutputs%20%28e.g.%2C%20adding%20or%20removing%20details%29.%20By%20automating%20the%202D%20template%0Ageneration%20step-compatible%20with%20standard%20embossing%20workflows-TactileNet%0Aaccelerates%20production%20while%20preserving%20design%20flexibility.%20This%20work%0Ademonstrates%20how%20AI%20can%20augment%20%28not%20replace%29%20human%20expertise%20to%20bridge%20the%0Aaccessibility%20gap%20in%20education%20and%20beyond.%20Code%2C%20data%2C%20and%20models%20will%20be%0Apublicly%20released%20to%20foster%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactileNet%253A%2520Bridging%2520the%2520Accessibility%2520Gap%2520with%2520AI-Generated%2520Tactile%250A%2520%2520Graphics%2520for%2520Individuals%2520with%2520Vision%2520Impairment%26entry.906535625%3DAdnan%2520Khan%2520and%2520Alireza%2520Choubineh%2520and%2520Mai%2520A.%2520Shaaban%2520and%2520Abbas%2520Akkasi%2520and%2520Majid%2520Komeili%26entry.1292438233%3D%2520%2520Tactile%2520graphics%2520are%2520essential%2520for%2520providing%2520access%2520to%2520visual%2520information%2520for%250Athe%252043%2520million%2520people%2520globally%2520living%2520with%2520vision%2520loss.%2520Traditional%2520methods%2520for%250Acreating%2520these%2520graphics%2520are%2520labor-intensive%2520and%2520cannot%2520meet%2520growing%2520demand.%2520We%250Aintroduce%2520TactileNet%252C%2520the%2520first%2520comprehensive%2520dataset%2520and%2520AI-driven%2520framework%250Afor%2520generating%2520embossing-ready%25202D%2520tactile%2520templates%2520using%2520text-to-image%2520Stable%250ADiffusion%2520%2528SD%2529%2520models.%2520By%2520integrating%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%250ADreamBooth%252C%2520our%2520method%2520fine-tunes%2520SD%2520models%2520to%2520produce%2520high-fidelity%252C%250Aguideline-compliant%2520graphics%2520while%2520reducing%2520computational%2520costs.%2520Quantitative%250Aevaluations%2520with%2520tactile%2520experts%2520show%252092.86%2525%2520adherence%2520to%2520accessibility%250Astandards.%2520Structural%2520fidelity%2520analysis%2520revealed%2520near-human%2520design%2520similarity%252C%250Awith%2520an%2520SSIM%2520of%25200.538%2520between%2520generated%2520graphics%2520and%2520expert-designed%2520tactile%250Aimages.%2520Notably%252C%2520our%2520method%2520preserves%2520object%2520silhouettes%2520better%2520than%2520human%250Adesigns%2520%2528SSIM%2520%253D%25200.259%2520vs.%25200.215%2520for%2520binary%2520masks%2529%252C%2520addressing%2520a%2520key%2520limitation%250Aof%2520manual%2520tactile%2520abstraction.%2520The%2520framework%2520scales%2520to%252032%252C000%2520images%2520%25287%252C050%250Ahigh-quality%2529%2520across%252066%2520classes%252C%2520with%2520prompt%2520editing%2520enabling%2520customizable%250Aoutputs%2520%2528e.g.%252C%2520adding%2520or%2520removing%2520details%2529.%2520By%2520automating%2520the%25202D%2520template%250Ageneration%2520step-compatible%2520with%2520standard%2520embossing%2520workflows-TactileNet%250Aaccelerates%2520production%2520while%2520preserving%2520design%2520flexibility.%2520This%2520work%250Ademonstrates%2520how%2520AI%2520can%2520augment%2520%2528not%2520replace%2529%2520human%2520expertise%2520to%2520bridge%2520the%250Aaccessibility%2520gap%2520in%2520education%2520and%2520beyond.%2520Code%252C%2520data%252C%2520and%2520models%2520will%2520be%250Apublicly%2520released%2520to%2520foster%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TactileNet%3A%20Bridging%20the%20Accessibility%20Gap%20with%20AI-Generated%20Tactile%0A%20%20Graphics%20for%20Individuals%20with%20Vision%20Impairment&entry.906535625=Adnan%20Khan%20and%20Alireza%20Choubineh%20and%20Mai%20A.%20Shaaban%20and%20Abbas%20Akkasi%20and%20Majid%20Komeili&entry.1292438233=%20%20Tactile%20graphics%20are%20essential%20for%20providing%20access%20to%20visual%20information%20for%0Athe%2043%20million%20people%20globally%20living%20with%20vision%20loss.%20Traditional%20methods%20for%0Acreating%20these%20graphics%20are%20labor-intensive%20and%20cannot%20meet%20growing%20demand.%20We%0Aintroduce%20TactileNet%2C%20the%20first%20comprehensive%20dataset%20and%20AI-driven%20framework%0Afor%20generating%20embossing-ready%202D%20tactile%20templates%20using%20text-to-image%20Stable%0ADiffusion%20%28SD%29%20models.%20By%20integrating%20Low-Rank%20Adaptation%20%28LoRA%29%20and%0ADreamBooth%2C%20our%20method%20fine-tunes%20SD%20models%20to%20produce%20high-fidelity%2C%0Aguideline-compliant%20graphics%20while%20reducing%20computational%20costs.%20Quantitative%0Aevaluations%20with%20tactile%20experts%20show%2092.86%25%20adherence%20to%20accessibility%0Astandards.%20Structural%20fidelity%20analysis%20revealed%20near-human%20design%20similarity%2C%0Awith%20an%20SSIM%20of%200.538%20between%20generated%20graphics%20and%20expert-designed%20tactile%0Aimages.%20Notably%2C%20our%20method%20preserves%20object%20silhouettes%20better%20than%20human%0Adesigns%20%28SSIM%20%3D%200.259%20vs.%200.215%20for%20binary%20masks%29%2C%20addressing%20a%20key%20limitation%0Aof%20manual%20tactile%20abstraction.%20The%20framework%20scales%20to%2032%2C000%20images%20%287%2C050%0Ahigh-quality%29%20across%2066%20classes%2C%20with%20prompt%20editing%20enabling%20customizable%0Aoutputs%20%28e.g.%2C%20adding%20or%20removing%20details%29.%20By%20automating%20the%202D%20template%0Ageneration%20step-compatible%20with%20standard%20embossing%20workflows-TactileNet%0Aaccelerates%20production%20while%20preserving%20design%20flexibility.%20This%20work%0Ademonstrates%20how%20AI%20can%20augment%20%28not%20replace%29%20human%20expertise%20to%20bridge%20the%0Aaccessibility%20gap%20in%20education%20and%20beyond.%20Code%2C%20data%2C%20and%20models%20will%20be%0Apublicly%20released%20to%20foster%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04722v2&entry.124074799=Read"},
{"title": "Multi-contrast laser endoscopy for in vivo gastrointestinal imaging", "author": "Taylor L. Bobrow and Mayank Golhar and Suchapa Arayakarnkul and Anthony A. Song and Saowanee Ngamruengphong and Nicholas J. Durr", "abstract": "  White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.\n", "link": "http://arxiv.org/abs/2505.10492v1", "date": "2025-05-15", "relevancy": 2.2208, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-contrast%20laser%20endoscopy%20for%20in%20vivo%20gastrointestinal%20imaging&body=Title%3A%20Multi-contrast%20laser%20endoscopy%20for%20in%20vivo%20gastrointestinal%20imaging%0AAuthor%3A%20Taylor%20L.%20Bobrow%20and%20Mayank%20Golhar%20and%20Suchapa%20Arayakarnkul%20and%20Anthony%20A.%20Song%20and%20Saowanee%20Ngamruengphong%20and%20Nicholas%20J.%20Durr%0AAbstract%3A%20%20%20White%20light%20endoscopy%20is%20the%20clinical%20gold%20standard%20for%20detecting%20diseases%20in%0Athe%20gastrointestinal%20tract.%20Most%20applications%20involve%20identifying%20visual%0Aabnormalities%20in%20tissue%20color%2C%20texture%2C%20and%20shape.%20Unfortunately%2C%20the%20contrast%0Aof%20these%20features%20is%20often%20subtle%2C%20causing%20many%20clinically%20relevant%20cases%20to%20go%0Aundetected.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Multi-contrast%20Laser%0AEndoscopy%20%28MLE%29%3A%20a%20platform%20for%20widefield%20clinical%20imaging%20with%20rapidly%20tunable%0Aspectral%2C%20coherent%2C%20and%20directional%20illumination.%20We%20demonstrate%20three%0Acapabilities%20of%20MLE%3A%20enhancing%20tissue%20chromophore%20contrast%20with%20multispectral%0Adiffuse%20reflectance%2C%20quantifying%20blood%20flow%20using%20laser%20speckle%20contrast%0Aimaging%2C%20and%20characterizing%20mucosal%20topography%20using%20photometric%20stereo.%20We%0Avalidate%20MLE%20with%20benchtop%20models%2C%20then%20demonstrate%20MLE%20in%20vivo%20during%20clinical%0Acolonoscopies.%20MLE%20images%20from%2031%20polyps%20demonstrate%20an%20approximate%20three-fold%0Aimprovement%20in%20contrast%20and%20a%20five-fold%20improvement%20in%20color%20difference%0Acompared%20to%20white%20light%20and%20narrow%20band%20imaging.%20With%20the%20ability%20to%20reveal%0Amultiple%20complementary%20types%20of%20tissue%20contrast%20while%20seamlessly%20integrating%0Ainto%20the%20clinical%20environment%2C%20MLE%20shows%20promise%20as%20an%20investigative%20tool%20to%0Aimprove%20gastrointestinal%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-contrast%2520laser%2520endoscopy%2520for%2520in%2520vivo%2520gastrointestinal%2520imaging%26entry.906535625%3DTaylor%2520L.%2520Bobrow%2520and%2520Mayank%2520Golhar%2520and%2520Suchapa%2520Arayakarnkul%2520and%2520Anthony%2520A.%2520Song%2520and%2520Saowanee%2520Ngamruengphong%2520and%2520Nicholas%2520J.%2520Durr%26entry.1292438233%3D%2520%2520White%2520light%2520endoscopy%2520is%2520the%2520clinical%2520gold%2520standard%2520for%2520detecting%2520diseases%2520in%250Athe%2520gastrointestinal%2520tract.%2520Most%2520applications%2520involve%2520identifying%2520visual%250Aabnormalities%2520in%2520tissue%2520color%252C%2520texture%252C%2520and%2520shape.%2520Unfortunately%252C%2520the%2520contrast%250Aof%2520these%2520features%2520is%2520often%2520subtle%252C%2520causing%2520many%2520clinically%2520relevant%2520cases%2520to%2520go%250Aundetected.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520Multi-contrast%2520Laser%250AEndoscopy%2520%2528MLE%2529%253A%2520a%2520platform%2520for%2520widefield%2520clinical%2520imaging%2520with%2520rapidly%2520tunable%250Aspectral%252C%2520coherent%252C%2520and%2520directional%2520illumination.%2520We%2520demonstrate%2520three%250Acapabilities%2520of%2520MLE%253A%2520enhancing%2520tissue%2520chromophore%2520contrast%2520with%2520multispectral%250Adiffuse%2520reflectance%252C%2520quantifying%2520blood%2520flow%2520using%2520laser%2520speckle%2520contrast%250Aimaging%252C%2520and%2520characterizing%2520mucosal%2520topography%2520using%2520photometric%2520stereo.%2520We%250Avalidate%2520MLE%2520with%2520benchtop%2520models%252C%2520then%2520demonstrate%2520MLE%2520in%2520vivo%2520during%2520clinical%250Acolonoscopies.%2520MLE%2520images%2520from%252031%2520polyps%2520demonstrate%2520an%2520approximate%2520three-fold%250Aimprovement%2520in%2520contrast%2520and%2520a%2520five-fold%2520improvement%2520in%2520color%2520difference%250Acompared%2520to%2520white%2520light%2520and%2520narrow%2520band%2520imaging.%2520With%2520the%2520ability%2520to%2520reveal%250Amultiple%2520complementary%2520types%2520of%2520tissue%2520contrast%2520while%2520seamlessly%2520integrating%250Ainto%2520the%2520clinical%2520environment%252C%2520MLE%2520shows%2520promise%2520as%2520an%2520investigative%2520tool%2520to%250Aimprove%2520gastrointestinal%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-contrast%20laser%20endoscopy%20for%20in%20vivo%20gastrointestinal%20imaging&entry.906535625=Taylor%20L.%20Bobrow%20and%20Mayank%20Golhar%20and%20Suchapa%20Arayakarnkul%20and%20Anthony%20A.%20Song%20and%20Saowanee%20Ngamruengphong%20and%20Nicholas%20J.%20Durr&entry.1292438233=%20%20White%20light%20endoscopy%20is%20the%20clinical%20gold%20standard%20for%20detecting%20diseases%20in%0Athe%20gastrointestinal%20tract.%20Most%20applications%20involve%20identifying%20visual%0Aabnormalities%20in%20tissue%20color%2C%20texture%2C%20and%20shape.%20Unfortunately%2C%20the%20contrast%0Aof%20these%20features%20is%20often%20subtle%2C%20causing%20many%20clinically%20relevant%20cases%20to%20go%0Aundetected.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Multi-contrast%20Laser%0AEndoscopy%20%28MLE%29%3A%20a%20platform%20for%20widefield%20clinical%20imaging%20with%20rapidly%20tunable%0Aspectral%2C%20coherent%2C%20and%20directional%20illumination.%20We%20demonstrate%20three%0Acapabilities%20of%20MLE%3A%20enhancing%20tissue%20chromophore%20contrast%20with%20multispectral%0Adiffuse%20reflectance%2C%20quantifying%20blood%20flow%20using%20laser%20speckle%20contrast%0Aimaging%2C%20and%20characterizing%20mucosal%20topography%20using%20photometric%20stereo.%20We%0Avalidate%20MLE%20with%20benchtop%20models%2C%20then%20demonstrate%20MLE%20in%20vivo%20during%20clinical%0Acolonoscopies.%20MLE%20images%20from%2031%20polyps%20demonstrate%20an%20approximate%20three-fold%0Aimprovement%20in%20contrast%20and%20a%20five-fold%20improvement%20in%20color%20difference%0Acompared%20to%20white%20light%20and%20narrow%20band%20imaging.%20With%20the%20ability%20to%20reveal%0Amultiple%20complementary%20types%20of%20tissue%20contrast%20while%20seamlessly%20integrating%0Ainto%20the%20clinical%20environment%2C%20MLE%20shows%20promise%20as%20an%20investigative%20tool%20to%0Aimprove%20gastrointestinal%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10492v1&entry.124074799=Read"},
{"title": "Towards a Deeper Understanding of Reasoning Capabilities in Large\n  Language Models", "author": "Annie Wong and Thomas B\u00e4ck and Aske Plaat and Niki van Stein and Anna V. Kononova", "abstract": "  While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.\n", "link": "http://arxiv.org/abs/2505.10543v1", "date": "2025-05-15", "relevancy": 2.1944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Deeper%20Understanding%20of%20Reasoning%20Capabilities%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Towards%20a%20Deeper%20Understanding%20of%20Reasoning%20Capabilities%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Annie%20Wong%20and%20Thomas%20B%C3%A4ck%20and%20Aske%20Plaat%20and%20Niki%20van%20Stein%20and%20Anna%20V.%20Kononova%0AAbstract%3A%20%20%20While%20large%20language%20models%20demonstrate%20impressive%20performance%20on%20static%0Abenchmarks%2C%20the%20true%20potential%20of%20large%20language%20models%20as%20self-learning%20and%0Areasoning%20agents%20in%20dynamic%20environments%20remains%20unclear.%20This%20study%0Asystematically%20evaluates%20the%20efficacy%20of%20self-reflection%2C%20heuristic%20mutation%2C%0Aand%20planning%20as%20prompting%20techniques%20to%20test%20the%20adaptive%20capabilities%20of%0Aagents.%20We%20conduct%20experiments%20with%20various%20open-source%20language%20models%20in%0Adynamic%20environments%20and%20find%20that%20larger%20models%20generally%20outperform%20smaller%0Aones%2C%20but%20that%20strategic%20prompting%20can%20close%20this%20performance%20gap.%20Second%2C%20a%0Atoo-long%20prompt%20can%20negatively%20impact%20smaller%20models%20on%20basic%20reactive%20tasks%2C%0Awhile%20larger%20models%20show%20more%20robust%20behaviour.%20Third%2C%20advanced%20prompting%0Atechniques%20primarily%20benefit%20smaller%20models%20on%20complex%20games%2C%20but%20offer%20less%0Aimprovement%20for%20already%20high-performing%20large%20language%20models.%20Yet%2C%20we%20find%0Athat%20advanced%20reasoning%20methods%20yield%20highly%20variable%20outcomes%3A%20while%20capable%0Aof%20significantly%20improving%20performance%20when%20reasoning%20and%20decision-making%0Aalign%2C%20they%20also%20introduce%20instability%20and%20can%20lead%20to%20big%20performance%20drops.%0ACompared%20to%20human%20performance%2C%20our%20findings%20reveal%20little%20evidence%20of%20true%0Aemergent%20reasoning.%20Instead%2C%20large%20language%20model%20performance%20exhibits%0Apersistent%20limitations%20in%20crucial%20areas%20such%20as%20planning%2C%20reasoning%2C%20and%0Aspatial%20coordination%2C%20suggesting%20that%20current-generation%20large%20language%20models%0Astill%20suffer%20fundamental%20shortcomings%20that%20may%20not%20be%20fully%20overcome%20through%0Aself-reflective%20prompting%20alone.%20Reasoning%20is%20a%20multi-faceted%20task%2C%20and%20while%0Areasoning%20methods%20like%20Chain%20of%20thought%20improves%20multi-step%20reasoning%20on%20math%0Aword%20problems%2C%20our%20findings%20using%20dynamic%20benchmarks%20highlight%20important%0Ashortcomings%20in%20general%20reasoning%20capabilities%2C%20indicating%20a%20need%20to%20move%0Abeyond%20static%20benchmarks%20to%20capture%20the%20complexity%20of%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Deeper%2520Understanding%2520of%2520Reasoning%2520Capabilities%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DAnnie%2520Wong%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Aske%2520Plaat%2520and%2520Niki%2520van%2520Stein%2520and%2520Anna%2520V.%2520Kononova%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520demonstrate%2520impressive%2520performance%2520on%2520static%250Abenchmarks%252C%2520the%2520true%2520potential%2520of%2520large%2520language%2520models%2520as%2520self-learning%2520and%250Areasoning%2520agents%2520in%2520dynamic%2520environments%2520remains%2520unclear.%2520This%2520study%250Asystematically%2520evaluates%2520the%2520efficacy%2520of%2520self-reflection%252C%2520heuristic%2520mutation%252C%250Aand%2520planning%2520as%2520prompting%2520techniques%2520to%2520test%2520the%2520adaptive%2520capabilities%2520of%250Aagents.%2520We%2520conduct%2520experiments%2520with%2520various%2520open-source%2520language%2520models%2520in%250Adynamic%2520environments%2520and%2520find%2520that%2520larger%2520models%2520generally%2520outperform%2520smaller%250Aones%252C%2520but%2520that%2520strategic%2520prompting%2520can%2520close%2520this%2520performance%2520gap.%2520Second%252C%2520a%250Atoo-long%2520prompt%2520can%2520negatively%2520impact%2520smaller%2520models%2520on%2520basic%2520reactive%2520tasks%252C%250Awhile%2520larger%2520models%2520show%2520more%2520robust%2520behaviour.%2520Third%252C%2520advanced%2520prompting%250Atechniques%2520primarily%2520benefit%2520smaller%2520models%2520on%2520complex%2520games%252C%2520but%2520offer%2520less%250Aimprovement%2520for%2520already%2520high-performing%2520large%2520language%2520models.%2520Yet%252C%2520we%2520find%250Athat%2520advanced%2520reasoning%2520methods%2520yield%2520highly%2520variable%2520outcomes%253A%2520while%2520capable%250Aof%2520significantly%2520improving%2520performance%2520when%2520reasoning%2520and%2520decision-making%250Aalign%252C%2520they%2520also%2520introduce%2520instability%2520and%2520can%2520lead%2520to%2520big%2520performance%2520drops.%250ACompared%2520to%2520human%2520performance%252C%2520our%2520findings%2520reveal%2520little%2520evidence%2520of%2520true%250Aemergent%2520reasoning.%2520Instead%252C%2520large%2520language%2520model%2520performance%2520exhibits%250Apersistent%2520limitations%2520in%2520crucial%2520areas%2520such%2520as%2520planning%252C%2520reasoning%252C%2520and%250Aspatial%2520coordination%252C%2520suggesting%2520that%2520current-generation%2520large%2520language%2520models%250Astill%2520suffer%2520fundamental%2520shortcomings%2520that%2520may%2520not%2520be%2520fully%2520overcome%2520through%250Aself-reflective%2520prompting%2520alone.%2520Reasoning%2520is%2520a%2520multi-faceted%2520task%252C%2520and%2520while%250Areasoning%2520methods%2520like%2520Chain%2520of%2520thought%2520improves%2520multi-step%2520reasoning%2520on%2520math%250Aword%2520problems%252C%2520our%2520findings%2520using%2520dynamic%2520benchmarks%2520highlight%2520important%250Ashortcomings%2520in%2520general%2520reasoning%2520capabilities%252C%2520indicating%2520a%2520need%2520to%2520move%250Abeyond%2520static%2520benchmarks%2520to%2520capture%2520the%2520complexity%2520of%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Deeper%20Understanding%20of%20Reasoning%20Capabilities%20in%20Large%0A%20%20Language%20Models&entry.906535625=Annie%20Wong%20and%20Thomas%20B%C3%A4ck%20and%20Aske%20Plaat%20and%20Niki%20van%20Stein%20and%20Anna%20V.%20Kononova&entry.1292438233=%20%20While%20large%20language%20models%20demonstrate%20impressive%20performance%20on%20static%0Abenchmarks%2C%20the%20true%20potential%20of%20large%20language%20models%20as%20self-learning%20and%0Areasoning%20agents%20in%20dynamic%20environments%20remains%20unclear.%20This%20study%0Asystematically%20evaluates%20the%20efficacy%20of%20self-reflection%2C%20heuristic%20mutation%2C%0Aand%20planning%20as%20prompting%20techniques%20to%20test%20the%20adaptive%20capabilities%20of%0Aagents.%20We%20conduct%20experiments%20with%20various%20open-source%20language%20models%20in%0Adynamic%20environments%20and%20find%20that%20larger%20models%20generally%20outperform%20smaller%0Aones%2C%20but%20that%20strategic%20prompting%20can%20close%20this%20performance%20gap.%20Second%2C%20a%0Atoo-long%20prompt%20can%20negatively%20impact%20smaller%20models%20on%20basic%20reactive%20tasks%2C%0Awhile%20larger%20models%20show%20more%20robust%20behaviour.%20Third%2C%20advanced%20prompting%0Atechniques%20primarily%20benefit%20smaller%20models%20on%20complex%20games%2C%20but%20offer%20less%0Aimprovement%20for%20already%20high-performing%20large%20language%20models.%20Yet%2C%20we%20find%0Athat%20advanced%20reasoning%20methods%20yield%20highly%20variable%20outcomes%3A%20while%20capable%0Aof%20significantly%20improving%20performance%20when%20reasoning%20and%20decision-making%0Aalign%2C%20they%20also%20introduce%20instability%20and%20can%20lead%20to%20big%20performance%20drops.%0ACompared%20to%20human%20performance%2C%20our%20findings%20reveal%20little%20evidence%20of%20true%0Aemergent%20reasoning.%20Instead%2C%20large%20language%20model%20performance%20exhibits%0Apersistent%20limitations%20in%20crucial%20areas%20such%20as%20planning%2C%20reasoning%2C%20and%0Aspatial%20coordination%2C%20suggesting%20that%20current-generation%20large%20language%20models%0Astill%20suffer%20fundamental%20shortcomings%20that%20may%20not%20be%20fully%20overcome%20through%0Aself-reflective%20prompting%20alone.%20Reasoning%20is%20a%20multi-faceted%20task%2C%20and%20while%0Areasoning%20methods%20like%20Chain%20of%20thought%20improves%20multi-step%20reasoning%20on%20math%0Aword%20problems%2C%20our%20findings%20using%20dynamic%20benchmarks%20highlight%20important%0Ashortcomings%20in%20general%20reasoning%20capabilities%2C%20indicating%20a%20need%20to%20move%0Abeyond%20static%20benchmarks%20to%20capture%20the%20complexity%20of%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10543v1&entry.124074799=Read"},
{"title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric\n  Lesion Segmentation", "author": "Jiaming Liang and Lihuan Dai and Xiaoqi Sheng and Xiangguang Chen and Chun Yao and Guihua Tao and Qibin Leng and Honming Cai and Xi Zhong", "abstract": "  Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.\n", "link": "http://arxiv.org/abs/2505.10464v1", "date": "2025-05-15", "relevancy": 2.1943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5429}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HWA-UNETR%3A%20Hierarchical%20Window%20Aggregate%20UNETR%20for%203D%20Multimodal%20Gastric%0A%20%20Lesion%20Segmentation&body=Title%3A%20HWA-UNETR%3A%20Hierarchical%20Window%20Aggregate%20UNETR%20for%203D%20Multimodal%20Gastric%0A%20%20Lesion%20Segmentation%0AAuthor%3A%20Jiaming%20Liang%20and%20Lihuan%20Dai%20and%20Xiaoqi%20Sheng%20and%20Xiangguang%20Chen%20and%20Chun%20Yao%20and%20Guihua%20Tao%20and%20Qibin%20Leng%20and%20Honming%20Cai%20and%20Xi%20Zhong%0AAbstract%3A%20%20%20Multimodal%20medical%20image%20segmentation%20faces%20significant%20challenges%20in%20the%0Acontext%20of%20gastric%20cancer%20lesion%20analysis.%20This%20clinical%20context%20is%20defined%20by%0Athe%20scarcity%20of%20independent%20multimodal%20datasets%20and%20the%20imperative%20to%0Aamalgamate%20inherently%20misaligned%20modalities.%20As%20a%20result%2C%20algorithms%20are%0Aconstrained%20to%20train%20on%20approximate%20data%20and%20depend%20on%20application%20migration%2C%0Aleading%20to%20substantial%20resource%20expenditure%20and%20a%20potential%20decline%20in%20analysis%0Aaccuracy.%20To%20address%20those%20challenges%2C%20we%20have%20made%20two%20major%20contributions%3A%0AFirst%2C%20we%20publicly%20disseminate%20the%20GCM%202025%20dataset%2C%20which%20serves%20as%20the%20first%0Alarge-scale%2C%20open-source%20collection%20of%20gastric%20cancer%20multimodal%20MRI%20scans%2C%0Afeaturing%20professionally%20annotated%20FS-T2W%2C%20CE-T1W%2C%20and%20ADC%20images%20from%20500%0Apatients.%20Second%2C%20we%20introduce%20HWA-UNETR%2C%20a%20novel%203D%20segmentation%20framework%0Athat%20employs%20an%20original%20HWA%20block%20with%20learnable%20window%20aggregation%20layers%20to%0Aestablish%20dynamic%20feature%20correspondences%20between%20different%20modalities%27%0Aanatomical%20structures%2C%20and%20leverages%20the%20innovative%20tri-orientated%20fusion%20mamba%0Amechanism%20for%20context%20modeling%20and%20capturing%20long-range%20spatial%20dependencies.%0AExtensive%20experiments%20on%20our%20GCM%202025%20dataset%20and%20the%20publicly%20BraTS%202021%0Adataset%20validate%20the%20performance%20of%20our%20framework%2C%20demonstrating%20that%20the%20new%0Aapproach%20surpasses%20existing%20methods%20by%20up%20to%201.68%5C%25%20in%20the%20Dice%20score%20while%0Amaintaining%20solid%20robustness.%20The%20dataset%20and%20code%20are%20public%20via%0Ahttps%3A//github.com/JeMing-creater/HWA-UNETR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHWA-UNETR%253A%2520Hierarchical%2520Window%2520Aggregate%2520UNETR%2520for%25203D%2520Multimodal%2520Gastric%250A%2520%2520Lesion%2520Segmentation%26entry.906535625%3DJiaming%2520Liang%2520and%2520Lihuan%2520Dai%2520and%2520Xiaoqi%2520Sheng%2520and%2520Xiangguang%2520Chen%2520and%2520Chun%2520Yao%2520and%2520Guihua%2520Tao%2520and%2520Qibin%2520Leng%2520and%2520Honming%2520Cai%2520and%2520Xi%2520Zhong%26entry.1292438233%3D%2520%2520Multimodal%2520medical%2520image%2520segmentation%2520faces%2520significant%2520challenges%2520in%2520the%250Acontext%2520of%2520gastric%2520cancer%2520lesion%2520analysis.%2520This%2520clinical%2520context%2520is%2520defined%2520by%250Athe%2520scarcity%2520of%2520independent%2520multimodal%2520datasets%2520and%2520the%2520imperative%2520to%250Aamalgamate%2520inherently%2520misaligned%2520modalities.%2520As%2520a%2520result%252C%2520algorithms%2520are%250Aconstrained%2520to%2520train%2520on%2520approximate%2520data%2520and%2520depend%2520on%2520application%2520migration%252C%250Aleading%2520to%2520substantial%2520resource%2520expenditure%2520and%2520a%2520potential%2520decline%2520in%2520analysis%250Aaccuracy.%2520To%2520address%2520those%2520challenges%252C%2520we%2520have%2520made%2520two%2520major%2520contributions%253A%250AFirst%252C%2520we%2520publicly%2520disseminate%2520the%2520GCM%25202025%2520dataset%252C%2520which%2520serves%2520as%2520the%2520first%250Alarge-scale%252C%2520open-source%2520collection%2520of%2520gastric%2520cancer%2520multimodal%2520MRI%2520scans%252C%250Afeaturing%2520professionally%2520annotated%2520FS-T2W%252C%2520CE-T1W%252C%2520and%2520ADC%2520images%2520from%2520500%250Apatients.%2520Second%252C%2520we%2520introduce%2520HWA-UNETR%252C%2520a%2520novel%25203D%2520segmentation%2520framework%250Athat%2520employs%2520an%2520original%2520HWA%2520block%2520with%2520learnable%2520window%2520aggregation%2520layers%2520to%250Aestablish%2520dynamic%2520feature%2520correspondences%2520between%2520different%2520modalities%2527%250Aanatomical%2520structures%252C%2520and%2520leverages%2520the%2520innovative%2520tri-orientated%2520fusion%2520mamba%250Amechanism%2520for%2520context%2520modeling%2520and%2520capturing%2520long-range%2520spatial%2520dependencies.%250AExtensive%2520experiments%2520on%2520our%2520GCM%25202025%2520dataset%2520and%2520the%2520publicly%2520BraTS%25202021%250Adataset%2520validate%2520the%2520performance%2520of%2520our%2520framework%252C%2520demonstrating%2520that%2520the%2520new%250Aapproach%2520surpasses%2520existing%2520methods%2520by%2520up%2520to%25201.68%255C%2525%2520in%2520the%2520Dice%2520score%2520while%250Amaintaining%2520solid%2520robustness.%2520The%2520dataset%2520and%2520code%2520are%2520public%2520via%250Ahttps%253A//github.com/JeMing-creater/HWA-UNETR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HWA-UNETR%3A%20Hierarchical%20Window%20Aggregate%20UNETR%20for%203D%20Multimodal%20Gastric%0A%20%20Lesion%20Segmentation&entry.906535625=Jiaming%20Liang%20and%20Lihuan%20Dai%20and%20Xiaoqi%20Sheng%20and%20Xiangguang%20Chen%20and%20Chun%20Yao%20and%20Guihua%20Tao%20and%20Qibin%20Leng%20and%20Honming%20Cai%20and%20Xi%20Zhong&entry.1292438233=%20%20Multimodal%20medical%20image%20segmentation%20faces%20significant%20challenges%20in%20the%0Acontext%20of%20gastric%20cancer%20lesion%20analysis.%20This%20clinical%20context%20is%20defined%20by%0Athe%20scarcity%20of%20independent%20multimodal%20datasets%20and%20the%20imperative%20to%0Aamalgamate%20inherently%20misaligned%20modalities.%20As%20a%20result%2C%20algorithms%20are%0Aconstrained%20to%20train%20on%20approximate%20data%20and%20depend%20on%20application%20migration%2C%0Aleading%20to%20substantial%20resource%20expenditure%20and%20a%20potential%20decline%20in%20analysis%0Aaccuracy.%20To%20address%20those%20challenges%2C%20we%20have%20made%20two%20major%20contributions%3A%0AFirst%2C%20we%20publicly%20disseminate%20the%20GCM%202025%20dataset%2C%20which%20serves%20as%20the%20first%0Alarge-scale%2C%20open-source%20collection%20of%20gastric%20cancer%20multimodal%20MRI%20scans%2C%0Afeaturing%20professionally%20annotated%20FS-T2W%2C%20CE-T1W%2C%20and%20ADC%20images%20from%20500%0Apatients.%20Second%2C%20we%20introduce%20HWA-UNETR%2C%20a%20novel%203D%20segmentation%20framework%0Athat%20employs%20an%20original%20HWA%20block%20with%20learnable%20window%20aggregation%20layers%20to%0Aestablish%20dynamic%20feature%20correspondences%20between%20different%20modalities%27%0Aanatomical%20structures%2C%20and%20leverages%20the%20innovative%20tri-orientated%20fusion%20mamba%0Amechanism%20for%20context%20modeling%20and%20capturing%20long-range%20spatial%20dependencies.%0AExtensive%20experiments%20on%20our%20GCM%202025%20dataset%20and%20the%20publicly%20BraTS%202021%0Adataset%20validate%20the%20performance%20of%20our%20framework%2C%20demonstrating%20that%20the%20new%0Aapproach%20surpasses%20existing%20methods%20by%20up%20to%201.68%5C%25%20in%20the%20Dice%20score%20while%0Amaintaining%20solid%20robustness.%20The%20dataset%20and%20code%20are%20public%20via%0Ahttps%3A//github.com/JeMing-creater/HWA-UNETR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10464v1&entry.124074799=Read"},
{"title": "IMITATE: Image Registration with Context for unknown time frame recovery", "author": "Ziad Kheil and Lucas Robinet and Laurent Risser and Soleakhena Ken", "abstract": "  In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .\n", "link": "http://arxiv.org/abs/2505.10124v1", "date": "2025-05-15", "relevancy": 2.1805, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5587}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5515}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMITATE%3A%20Image%20Registration%20with%20Context%20for%20unknown%20time%20frame%20recovery&body=Title%3A%20IMITATE%3A%20Image%20Registration%20with%20Context%20for%20unknown%20time%20frame%20recovery%0AAuthor%3A%20Ziad%20Kheil%20and%20Lucas%20Robinet%20and%20Laurent%20Risser%20and%20Soleakhena%20Ken%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20formulate%20a%20novel%20image%20registration%20formalism%20dedicated%20to%0Athe%20estimation%20of%20unknown%20condition-related%20images%2C%20based%20on%20two%20or%20more%20known%0Aimages%20and%20their%20associated%20conditions.%20We%20show%20how%20to%20practically%20model%20this%0Aformalism%20by%20using%20a%20new%20conditional%20U-Net%20architecture%2C%20which%20fully%20takes%20into%0Aaccount%20the%20conditional%20information%20and%20does%20not%20need%20any%20fixed%20image.%20Our%0Aformalism%20is%20then%20applied%20to%20image%20moving%20tumors%20for%20radiotherapy%20treatment%20at%0Adifferent%20breathing%20amplitude%20using%204D-CT%20%283D%2Bt%29%20scans%20in%20thoracoabdominal%0Aregions.%20This%20driving%20application%20is%20particularly%20complex%20as%20it%20requires%20to%0Astitch%20a%20collection%20of%20sequential%202D%20slices%20into%20several%203D%20volumes%20at%0Adifferent%20organ%20positions.%20Movement%20interpolation%20with%20standard%20methods%20then%0Agenerates%20well%20known%20reconstruction%20artefacts%20in%20the%20assembled%20volumes%20due%20to%0Airregular%20patient%20breathing%2C%20hysteresis%20and%20poor%20correlation%20of%20breathing%0Asignal%20to%20internal%20motion.%20Results%20obtained%20on%204D-CT%20clinical%20data%20showcase%0Aartefact-free%20volumes%20achieved%20through%20real-time%20latencies.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/Kheil-Z/IMITATE%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMITATE%253A%2520Image%2520Registration%2520with%2520Context%2520for%2520unknown%2520time%2520frame%2520recovery%26entry.906535625%3DZiad%2520Kheil%2520and%2520Lucas%2520Robinet%2520and%2520Laurent%2520Risser%2520and%2520Soleakhena%2520Ken%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520formulate%2520a%2520novel%2520image%2520registration%2520formalism%2520dedicated%2520to%250Athe%2520estimation%2520of%2520unknown%2520condition-related%2520images%252C%2520based%2520on%2520two%2520or%2520more%2520known%250Aimages%2520and%2520their%2520associated%2520conditions.%2520We%2520show%2520how%2520to%2520practically%2520model%2520this%250Aformalism%2520by%2520using%2520a%2520new%2520conditional%2520U-Net%2520architecture%252C%2520which%2520fully%2520takes%2520into%250Aaccount%2520the%2520conditional%2520information%2520and%2520does%2520not%2520need%2520any%2520fixed%2520image.%2520Our%250Aformalism%2520is%2520then%2520applied%2520to%2520image%2520moving%2520tumors%2520for%2520radiotherapy%2520treatment%2520at%250Adifferent%2520breathing%2520amplitude%2520using%25204D-CT%2520%25283D%252Bt%2529%2520scans%2520in%2520thoracoabdominal%250Aregions.%2520This%2520driving%2520application%2520is%2520particularly%2520complex%2520as%2520it%2520requires%2520to%250Astitch%2520a%2520collection%2520of%2520sequential%25202D%2520slices%2520into%2520several%25203D%2520volumes%2520at%250Adifferent%2520organ%2520positions.%2520Movement%2520interpolation%2520with%2520standard%2520methods%2520then%250Agenerates%2520well%2520known%2520reconstruction%2520artefacts%2520in%2520the%2520assembled%2520volumes%2520due%2520to%250Airregular%2520patient%2520breathing%252C%2520hysteresis%2520and%2520poor%2520correlation%2520of%2520breathing%250Asignal%2520to%2520internal%2520motion.%2520Results%2520obtained%2520on%25204D-CT%2520clinical%2520data%2520showcase%250Aartefact-free%2520volumes%2520achieved%2520through%2520real-time%2520latencies.%2520The%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/Kheil-Z/IMITATE%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMITATE%3A%20Image%20Registration%20with%20Context%20for%20unknown%20time%20frame%20recovery&entry.906535625=Ziad%20Kheil%20and%20Lucas%20Robinet%20and%20Laurent%20Risser%20and%20Soleakhena%20Ken&entry.1292438233=%20%20In%20this%20paper%2C%20we%20formulate%20a%20novel%20image%20registration%20formalism%20dedicated%20to%0Athe%20estimation%20of%20unknown%20condition-related%20images%2C%20based%20on%20two%20or%20more%20known%0Aimages%20and%20their%20associated%20conditions.%20We%20show%20how%20to%20practically%20model%20this%0Aformalism%20by%20using%20a%20new%20conditional%20U-Net%20architecture%2C%20which%20fully%20takes%20into%0Aaccount%20the%20conditional%20information%20and%20does%20not%20need%20any%20fixed%20image.%20Our%0Aformalism%20is%20then%20applied%20to%20image%20moving%20tumors%20for%20radiotherapy%20treatment%20at%0Adifferent%20breathing%20amplitude%20using%204D-CT%20%283D%2Bt%29%20scans%20in%20thoracoabdominal%0Aregions.%20This%20driving%20application%20is%20particularly%20complex%20as%20it%20requires%20to%0Astitch%20a%20collection%20of%20sequential%202D%20slices%20into%20several%203D%20volumes%20at%0Adifferent%20organ%20positions.%20Movement%20interpolation%20with%20standard%20methods%20then%0Agenerates%20well%20known%20reconstruction%20artefacts%20in%20the%20assembled%20volumes%20due%20to%0Airregular%20patient%20breathing%2C%20hysteresis%20and%20poor%20correlation%20of%20breathing%0Asignal%20to%20internal%20motion.%20Results%20obtained%20on%204D-CT%20clinical%20data%20showcase%0Aartefact-free%20volumes%20achieved%20through%20real-time%20latencies.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/Kheil-Z/IMITATE%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10124v1&entry.124074799=Read"},
{"title": "Deconstructing Subset Construction -- Reducing While Determinizing", "author": "John Nicol and Markus Frohme", "abstract": "  We present a novel perspective on the NFA canonization problem, which\nintroduces intermediate minimization steps to reduce the exploration space\non-the-fly. Essential to our approach are so-called equivalence registries\nwhich manage information about equivalent states and allow for incorporating\nfurther optimization techniques such as convexity closures or simulation to\nboost performance. Due to the generality of our approach, these concepts can be\nembedded in classic subset construction or Brzozowski's approach. We evaluate\nour approach on a set of real-world examples from automatic sequences and\nobserve that we are able to improve especially worst-case scenarios. We\nimplement our approach in an open-source library for users to experiment with.\n", "link": "http://arxiv.org/abs/2505.10319v1", "date": "2025-05-15", "relevancy": 2.1599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4334}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconstructing%20Subset%20Construction%20--%20Reducing%20While%20Determinizing&body=Title%3A%20Deconstructing%20Subset%20Construction%20--%20Reducing%20While%20Determinizing%0AAuthor%3A%20John%20Nicol%20and%20Markus%20Frohme%0AAbstract%3A%20%20%20We%20present%20a%20novel%20perspective%20on%20the%20NFA%20canonization%20problem%2C%20which%0Aintroduces%20intermediate%20minimization%20steps%20to%20reduce%20the%20exploration%20space%0Aon-the-fly.%20Essential%20to%20our%20approach%20are%20so-called%20equivalence%20registries%0Awhich%20manage%20information%20about%20equivalent%20states%20and%20allow%20for%20incorporating%0Afurther%20optimization%20techniques%20such%20as%20convexity%20closures%20or%20simulation%20to%0Aboost%20performance.%20Due%20to%20the%20generality%20of%20our%20approach%2C%20these%20concepts%20can%20be%0Aembedded%20in%20classic%20subset%20construction%20or%20Brzozowski%27s%20approach.%20We%20evaluate%0Aour%20approach%20on%20a%20set%20of%20real-world%20examples%20from%20automatic%20sequences%20and%0Aobserve%20that%20we%20are%20able%20to%20improve%20especially%20worst-case%20scenarios.%20We%0Aimplement%20our%20approach%20in%20an%20open-source%20library%20for%20users%20to%20experiment%20with.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconstructing%2520Subset%2520Construction%2520--%2520Reducing%2520While%2520Determinizing%26entry.906535625%3DJohn%2520Nicol%2520and%2520Markus%2520Frohme%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520perspective%2520on%2520the%2520NFA%2520canonization%2520problem%252C%2520which%250Aintroduces%2520intermediate%2520minimization%2520steps%2520to%2520reduce%2520the%2520exploration%2520space%250Aon-the-fly.%2520Essential%2520to%2520our%2520approach%2520are%2520so-called%2520equivalence%2520registries%250Awhich%2520manage%2520information%2520about%2520equivalent%2520states%2520and%2520allow%2520for%2520incorporating%250Afurther%2520optimization%2520techniques%2520such%2520as%2520convexity%2520closures%2520or%2520simulation%2520to%250Aboost%2520performance.%2520Due%2520to%2520the%2520generality%2520of%2520our%2520approach%252C%2520these%2520concepts%2520can%2520be%250Aembedded%2520in%2520classic%2520subset%2520construction%2520or%2520Brzozowski%2527s%2520approach.%2520We%2520evaluate%250Aour%2520approach%2520on%2520a%2520set%2520of%2520real-world%2520examples%2520from%2520automatic%2520sequences%2520and%250Aobserve%2520that%2520we%2520are%2520able%2520to%2520improve%2520especially%2520worst-case%2520scenarios.%2520We%250Aimplement%2520our%2520approach%2520in%2520an%2520open-source%2520library%2520for%2520users%2520to%2520experiment%2520with.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconstructing%20Subset%20Construction%20--%20Reducing%20While%20Determinizing&entry.906535625=John%20Nicol%20and%20Markus%20Frohme&entry.1292438233=%20%20We%20present%20a%20novel%20perspective%20on%20the%20NFA%20canonization%20problem%2C%20which%0Aintroduces%20intermediate%20minimization%20steps%20to%20reduce%20the%20exploration%20space%0Aon-the-fly.%20Essential%20to%20our%20approach%20are%20so-called%20equivalence%20registries%0Awhich%20manage%20information%20about%20equivalent%20states%20and%20allow%20for%20incorporating%0Afurther%20optimization%20techniques%20such%20as%20convexity%20closures%20or%20simulation%20to%0Aboost%20performance.%20Due%20to%20the%20generality%20of%20our%20approach%2C%20these%20concepts%20can%20be%0Aembedded%20in%20classic%20subset%20construction%20or%20Brzozowski%27s%20approach.%20We%20evaluate%0Aour%20approach%20on%20a%20set%20of%20real-world%20examples%20from%20automatic%20sequences%20and%0Aobserve%20that%20we%20are%20able%20to%20improve%20especially%20worst-case%20scenarios.%20We%0Aimplement%20our%20approach%20in%20an%20open-source%20library%20for%20users%20to%20experiment%20with.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10319v1&entry.124074799=Read"},
{"title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "author": "Michael Hubbertz and Pascal Colling and Qi Han and Tobias Meisen", "abstract": "  High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.\n", "link": "http://arxiv.org/abs/2505.10258v1", "date": "2025-05-15", "relevancy": 2.1531, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.554}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20Driving%20Maps%20by%20Deep%20Learning-based%20Trail%20Map%20Extraction&body=Title%3A%20Inferring%20Driving%20Maps%20by%20Deep%20Learning-based%20Trail%20Map%20Extraction%0AAuthor%3A%20Michael%20Hubbertz%20and%20Pascal%20Colling%20and%20Qi%20Han%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20High-definition%20%28HD%29%20maps%20offer%20extensive%20and%20accurate%20environmental%0Ainformation%20about%20the%20driving%20scene%2C%20making%20them%20a%20crucial%20and%20essential%0Aelement%20for%20planning%20within%20autonomous%20driving%20systems.%20To%20avoid%20extensive%0Aefforts%20from%20manual%20labeling%2C%20methods%20for%20automating%20the%20map%20creation%20have%0Aemerged.%20Recent%20trends%20have%20moved%20from%20offline%20mapping%20to%20online%20mapping%2C%0Aensuring%20availability%20and%20actuality%20of%20the%20utilized%20maps.%20While%20the%20performance%0Ahas%20increased%20in%20recent%20years%2C%20online%20mapping%20still%20faces%20challenges%20regarding%0Atemporal%20consistency%2C%20sensor%20occlusion%2C%20runtime%2C%20and%20generalization.%20We%20propose%0Aa%20novel%20offline%20mapping%20approach%20that%20integrates%20trails%20-%20informal%20routes%20used%0Aby%20drivers%20-%20into%20the%20map%20creation%20process.%20Our%20method%20aggregates%20trail%20data%0Afrom%20the%20ego%20vehicle%20and%20other%20traffic%20participants%20to%20construct%20a%0Acomprehensive%20global%20map%20using%20transformer-based%20deep%20learning%20models.%20Unlike%0Atraditional%20offline%20mapping%2C%20our%20approach%20enables%20continuous%20updates%20while%0Aremaining%20sensor-agnostic%2C%20facilitating%20efficient%20data%20transfer.%20Our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20online%20mapping%0Aapproaches%2C%20achieving%20improved%20generalization%20to%20previously%20unseen%20environments%0Aand%20sensor%20configurations.%20We%20validate%20our%20approach%20on%20two%20benchmark%20datasets%2C%0Ahighlighting%20its%20robustness%20and%20applicability%20in%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520Driving%2520Maps%2520by%2520Deep%2520Learning-based%2520Trail%2520Map%2520Extraction%26entry.906535625%3DMichael%2520Hubbertz%2520and%2520Pascal%2520Colling%2520and%2520Qi%2520Han%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520High-definition%2520%2528HD%2529%2520maps%2520offer%2520extensive%2520and%2520accurate%2520environmental%250Ainformation%2520about%2520the%2520driving%2520scene%252C%2520making%2520them%2520a%2520crucial%2520and%2520essential%250Aelement%2520for%2520planning%2520within%2520autonomous%2520driving%2520systems.%2520To%2520avoid%2520extensive%250Aefforts%2520from%2520manual%2520labeling%252C%2520methods%2520for%2520automating%2520the%2520map%2520creation%2520have%250Aemerged.%2520Recent%2520trends%2520have%2520moved%2520from%2520offline%2520mapping%2520to%2520online%2520mapping%252C%250Aensuring%2520availability%2520and%2520actuality%2520of%2520the%2520utilized%2520maps.%2520While%2520the%2520performance%250Ahas%2520increased%2520in%2520recent%2520years%252C%2520online%2520mapping%2520still%2520faces%2520challenges%2520regarding%250Atemporal%2520consistency%252C%2520sensor%2520occlusion%252C%2520runtime%252C%2520and%2520generalization.%2520We%2520propose%250Aa%2520novel%2520offline%2520mapping%2520approach%2520that%2520integrates%2520trails%2520-%2520informal%2520routes%2520used%250Aby%2520drivers%2520-%2520into%2520the%2520map%2520creation%2520process.%2520Our%2520method%2520aggregates%2520trail%2520data%250Afrom%2520the%2520ego%2520vehicle%2520and%2520other%2520traffic%2520participants%2520to%2520construct%2520a%250Acomprehensive%2520global%2520map%2520using%2520transformer-based%2520deep%2520learning%2520models.%2520Unlike%250Atraditional%2520offline%2520mapping%252C%2520our%2520approach%2520enables%2520continuous%2520updates%2520while%250Aremaining%2520sensor-agnostic%252C%2520facilitating%2520efficient%2520data%2520transfer.%2520Our%2520method%250Ademonstrates%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520online%2520mapping%250Aapproaches%252C%2520achieving%2520improved%2520generalization%2520to%2520previously%2520unseen%2520environments%250Aand%2520sensor%2520configurations.%2520We%2520validate%2520our%2520approach%2520on%2520two%2520benchmark%2520datasets%252C%250Ahighlighting%2520its%2520robustness%2520and%2520applicability%2520in%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Driving%20Maps%20by%20Deep%20Learning-based%20Trail%20Map%20Extraction&entry.906535625=Michael%20Hubbertz%20and%20Pascal%20Colling%20and%20Qi%20Han%20and%20Tobias%20Meisen&entry.1292438233=%20%20High-definition%20%28HD%29%20maps%20offer%20extensive%20and%20accurate%20environmental%0Ainformation%20about%20the%20driving%20scene%2C%20making%20them%20a%20crucial%20and%20essential%0Aelement%20for%20planning%20within%20autonomous%20driving%20systems.%20To%20avoid%20extensive%0Aefforts%20from%20manual%20labeling%2C%20methods%20for%20automating%20the%20map%20creation%20have%0Aemerged.%20Recent%20trends%20have%20moved%20from%20offline%20mapping%20to%20online%20mapping%2C%0Aensuring%20availability%20and%20actuality%20of%20the%20utilized%20maps.%20While%20the%20performance%0Ahas%20increased%20in%20recent%20years%2C%20online%20mapping%20still%20faces%20challenges%20regarding%0Atemporal%20consistency%2C%20sensor%20occlusion%2C%20runtime%2C%20and%20generalization.%20We%20propose%0Aa%20novel%20offline%20mapping%20approach%20that%20integrates%20trails%20-%20informal%20routes%20used%0Aby%20drivers%20-%20into%20the%20map%20creation%20process.%20Our%20method%20aggregates%20trail%20data%0Afrom%20the%20ego%20vehicle%20and%20other%20traffic%20participants%20to%20construct%20a%0Acomprehensive%20global%20map%20using%20transformer-based%20deep%20learning%20models.%20Unlike%0Atraditional%20offline%20mapping%2C%20our%20approach%20enables%20continuous%20updates%20while%0Aremaining%20sensor-agnostic%2C%20facilitating%20efficient%20data%20transfer.%20Our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20online%20mapping%0Aapproaches%2C%20achieving%20improved%20generalization%20to%20previously%20unseen%20environments%0Aand%20sensor%20configurations.%20We%20validate%20our%20approach%20on%20two%20benchmark%20datasets%2C%0Ahighlighting%20its%20robustness%20and%20applicability%20in%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10258v1&entry.124074799=Read"},
{"title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion\n  Timesteps", "author": "Ningyuan Yang and Jiaxuan Gao and Feng Gao and Yi Wu and Chao Yu", "abstract": "  Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.\n", "link": "http://arxiv.org/abs/2505.10482v1", "date": "2025-05-15", "relevancy": 2.1513, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.544}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20Diffusion%20Policies%20with%20Backpropagation%20Through%20Diffusion%0A%20%20Timesteps&body=Title%3A%20Fine-tuning%20Diffusion%20Policies%20with%20Backpropagation%20Through%20Diffusion%0A%20%20Timesteps%0AAuthor%3A%20Ningyuan%20Yang%20and%20Jiaxuan%20Gao%20and%20Feng%20Gao%20and%20Yi%20Wu%20and%20Chao%20Yu%0AAbstract%3A%20%20%20Diffusion%20policies%2C%20widely%20adopted%20in%20decision-making%20scenarios%20such%20as%0Arobotics%2C%20gaming%20and%20autonomous%20driving%2C%20are%20capable%20of%20learning%20diverse%20skills%0Afrom%20demonstration%20data%20due%20to%20their%20high%20representation%20power.%20However%2C%20the%0Asub-optimal%20and%20limited%20coverage%20of%20demonstration%20data%20could%20lead%20to%20diffusion%0Apolicies%20that%20generate%20sub-optimal%20trajectories%20and%20even%20catastrophic%20failures.%0AWhile%20reinforcement%20learning%20%28RL%29-based%20fine-tuning%20has%20emerged%20as%20a%20promising%0Asolution%20to%20address%20these%20limitations%2C%20existing%20approaches%20struggle%20to%0Aeffectively%20adapt%20Proximal%20Policy%20Optimization%20%28PPO%29%20to%20diffusion%20models.%20This%0Achallenge%20stems%20from%20the%20computational%20intractability%20of%20action%20likelihood%0Aestimation%20during%20the%20denoising%20process%2C%20which%20leads%20to%20complicated%0Aoptimization%20objectives.%20In%20our%20experiments%20starting%20from%20randomly%20initialized%0Apolicies%2C%20we%20find%20that%20online%20tuning%20of%20Diffusion%20Policies%20demonstrates%20much%0Alower%20sample%20efficiency%20compared%20to%20directly%20applying%20PPO%20on%20MLP%20policies%0A%28MLP%2BPPO%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20NCDPO%2C%20a%20novel%20framework%0Athat%20reformulates%20Diffusion%20Policy%20as%20a%20noise-conditioned%20deterministic%20policy.%0ABy%20treating%20each%20denoising%20step%20as%20a%20differentiable%20transformation%20conditioned%0Aon%20pre-sampled%20noise%2C%20NCDPO%20enables%20tractable%20likelihood%20evaluation%20and%0Agradient%20backpropagation%20through%20all%20diffusion%20timesteps.%20Our%20experiments%0Ademonstrate%20that%20NCDPO%20achieves%20sample%20efficiency%20comparable%20to%20MLP%2BPPO%20when%0Atraining%20from%20scratch%2C%20outperforming%20existing%20methods%20in%20both%20sample%20efficiency%0Aand%20final%20performance%20across%20diverse%20benchmarks%2C%20including%20continuous%20robot%0Acontrol%20and%20multi-agent%20game%20scenarios.%20Furthermore%2C%20our%20experimental%20results%0Ashow%20that%20our%20method%20is%20robust%20to%20the%20number%20denoising%20timesteps%20in%20the%0ADiffusion%20Policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520Diffusion%2520Policies%2520with%2520Backpropagation%2520Through%2520Diffusion%250A%2520%2520Timesteps%26entry.906535625%3DNingyuan%2520Yang%2520and%2520Jiaxuan%2520Gao%2520and%2520Feng%2520Gao%2520and%2520Yi%2520Wu%2520and%2520Chao%2520Yu%26entry.1292438233%3D%2520%2520Diffusion%2520policies%252C%2520widely%2520adopted%2520in%2520decision-making%2520scenarios%2520such%2520as%250Arobotics%252C%2520gaming%2520and%2520autonomous%2520driving%252C%2520are%2520capable%2520of%2520learning%2520diverse%2520skills%250Afrom%2520demonstration%2520data%2520due%2520to%2520their%2520high%2520representation%2520power.%2520However%252C%2520the%250Asub-optimal%2520and%2520limited%2520coverage%2520of%2520demonstration%2520data%2520could%2520lead%2520to%2520diffusion%250Apolicies%2520that%2520generate%2520sub-optimal%2520trajectories%2520and%2520even%2520catastrophic%2520failures.%250AWhile%2520reinforcement%2520learning%2520%2528RL%2529-based%2520fine-tuning%2520has%2520emerged%2520as%2520a%2520promising%250Asolution%2520to%2520address%2520these%2520limitations%252C%2520existing%2520approaches%2520struggle%2520to%250Aeffectively%2520adapt%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520to%2520diffusion%2520models.%2520This%250Achallenge%2520stems%2520from%2520the%2520computational%2520intractability%2520of%2520action%2520likelihood%250Aestimation%2520during%2520the%2520denoising%2520process%252C%2520which%2520leads%2520to%2520complicated%250Aoptimization%2520objectives.%2520In%2520our%2520experiments%2520starting%2520from%2520randomly%2520initialized%250Apolicies%252C%2520we%2520find%2520that%2520online%2520tuning%2520of%2520Diffusion%2520Policies%2520demonstrates%2520much%250Alower%2520sample%2520efficiency%2520compared%2520to%2520directly%2520applying%2520PPO%2520on%2520MLP%2520policies%250A%2528MLP%252BPPO%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520NCDPO%252C%2520a%2520novel%2520framework%250Athat%2520reformulates%2520Diffusion%2520Policy%2520as%2520a%2520noise-conditioned%2520deterministic%2520policy.%250ABy%2520treating%2520each%2520denoising%2520step%2520as%2520a%2520differentiable%2520transformation%2520conditioned%250Aon%2520pre-sampled%2520noise%252C%2520NCDPO%2520enables%2520tractable%2520likelihood%2520evaluation%2520and%250Agradient%2520backpropagation%2520through%2520all%2520diffusion%2520timesteps.%2520Our%2520experiments%250Ademonstrate%2520that%2520NCDPO%2520achieves%2520sample%2520efficiency%2520comparable%2520to%2520MLP%252BPPO%2520when%250Atraining%2520from%2520scratch%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520sample%2520efficiency%250Aand%2520final%2520performance%2520across%2520diverse%2520benchmarks%252C%2520including%2520continuous%2520robot%250Acontrol%2520and%2520multi-agent%2520game%2520scenarios.%2520Furthermore%252C%2520our%2520experimental%2520results%250Ashow%2520that%2520our%2520method%2520is%2520robust%2520to%2520the%2520number%2520denoising%2520timesteps%2520in%2520the%250ADiffusion%2520Policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20Diffusion%20Policies%20with%20Backpropagation%20Through%20Diffusion%0A%20%20Timesteps&entry.906535625=Ningyuan%20Yang%20and%20Jiaxuan%20Gao%20and%20Feng%20Gao%20and%20Yi%20Wu%20and%20Chao%20Yu&entry.1292438233=%20%20Diffusion%20policies%2C%20widely%20adopted%20in%20decision-making%20scenarios%20such%20as%0Arobotics%2C%20gaming%20and%20autonomous%20driving%2C%20are%20capable%20of%20learning%20diverse%20skills%0Afrom%20demonstration%20data%20due%20to%20their%20high%20representation%20power.%20However%2C%20the%0Asub-optimal%20and%20limited%20coverage%20of%20demonstration%20data%20could%20lead%20to%20diffusion%0Apolicies%20that%20generate%20sub-optimal%20trajectories%20and%20even%20catastrophic%20failures.%0AWhile%20reinforcement%20learning%20%28RL%29-based%20fine-tuning%20has%20emerged%20as%20a%20promising%0Asolution%20to%20address%20these%20limitations%2C%20existing%20approaches%20struggle%20to%0Aeffectively%20adapt%20Proximal%20Policy%20Optimization%20%28PPO%29%20to%20diffusion%20models.%20This%0Achallenge%20stems%20from%20the%20computational%20intractability%20of%20action%20likelihood%0Aestimation%20during%20the%20denoising%20process%2C%20which%20leads%20to%20complicated%0Aoptimization%20objectives.%20In%20our%20experiments%20starting%20from%20randomly%20initialized%0Apolicies%2C%20we%20find%20that%20online%20tuning%20of%20Diffusion%20Policies%20demonstrates%20much%0Alower%20sample%20efficiency%20compared%20to%20directly%20applying%20PPO%20on%20MLP%20policies%0A%28MLP%2BPPO%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20NCDPO%2C%20a%20novel%20framework%0Athat%20reformulates%20Diffusion%20Policy%20as%20a%20noise-conditioned%20deterministic%20policy.%0ABy%20treating%20each%20denoising%20step%20as%20a%20differentiable%20transformation%20conditioned%0Aon%20pre-sampled%20noise%2C%20NCDPO%20enables%20tractable%20likelihood%20evaluation%20and%0Agradient%20backpropagation%20through%20all%20diffusion%20timesteps.%20Our%20experiments%0Ademonstrate%20that%20NCDPO%20achieves%20sample%20efficiency%20comparable%20to%20MLP%2BPPO%20when%0Atraining%20from%20scratch%2C%20outperforming%20existing%20methods%20in%20both%20sample%20efficiency%0Aand%20final%20performance%20across%20diverse%20benchmarks%2C%20including%20continuous%20robot%0Acontrol%20and%20multi-agent%20game%20scenarios.%20Furthermore%2C%20our%20experimental%20results%0Ashow%20that%20our%20method%20is%20robust%20to%20the%20number%20denoising%20timesteps%20in%20the%0ADiffusion%20Policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10482v1&entry.124074799=Read"},
{"title": "Intelligently Augmented Contrastive Tensor Factorization: Empowering\n  Multi-dimensional Time Series Classification in Low-Data Environments", "author": "Anushiya Arunan and Yan Qin and Xiaoli Li and Yuen Chau", "abstract": "  Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved.\n", "link": "http://arxiv.org/abs/2505.03825v2", "date": "2025-05-15", "relevancy": 2.1447, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligently%20Augmented%20Contrastive%20Tensor%20Factorization%3A%20Empowering%0A%20%20Multi-dimensional%20Time%20Series%20Classification%20in%20Low-Data%20Environments&body=Title%3A%20Intelligently%20Augmented%20Contrastive%20Tensor%20Factorization%3A%20Empowering%0A%20%20Multi-dimensional%20Time%20Series%20Classification%20in%20Low-Data%20Environments%0AAuthor%3A%20Anushiya%20Arunan%20and%20Yan%20Qin%20and%20Xiaoli%20Li%20and%20Yuen%20Chau%0AAbstract%3A%20%20%20Classification%20of%20multi-dimensional%20time%20series%20from%20real-world%20systems%0Arequire%20fine-grained%20learning%20of%20complex%20features%20such%20as%20cross-dimensional%0Adependencies%20and%20intra-class%20variations-all%20under%20the%20practical%20challenge%20of%0Alow%20training%20data%20availability.%20However%2C%20standard%20deep%20learning%20%28DL%29%20struggles%0Ato%20learn%20generalizable%20features%20in%20low-data%20environments%20due%20to%20model%0Aoverfitting.%20We%20propose%20a%20versatile%20yet%20data-efficient%20framework%2C%20Intelligently%0AAugmented%20Contrastive%20Tensor%20Factorization%20%28ITA-CTF%29%2C%20to%20learn%20effective%0Arepresentations%20from%20multi-dimensional%20time%20series.%20The%20CTF%20module%20learns%20core%0Aexplanatory%20components%20of%20the%20time%20series%20%28e.g.%2C%20sensor%20factors%2C%20temporal%0Afactors%29%2C%20and%20importantly%2C%20their%20joint%20dependencies.%20Notably%2C%20unlike%20standard%0Atensor%20factorization%20%28TF%29%2C%20the%20CTF%20module%20incorporates%20a%20new%20contrastive%20loss%0Aoptimization%20to%20induce%20similarity%20learning%20and%20class-awareness%20into%20the%20learnt%0Arepresentations%20for%20better%20classification%20performance.%20To%20strengthen%20this%0Acontrastive%20learning%2C%20the%20preceding%20ITA%20module%20generates%20targeted%20but%0Ainformative%20augmentations%20that%20highlight%20realistic%20intra-class%20patterns%20in%20the%0Aoriginal%20data%2C%20while%20preserving%20class-wise%20properties.%20This%20is%20achieved%20by%0Adynamically%20sampling%20a%20%22soft%22%20class%20prototype%20to%20guide%20the%20warping%20of%20each%0Aquery%20data%20sample%2C%20which%20results%20in%20an%20augmentation%20that%20is%20intelligently%0Apattern-mixed%20between%20the%20%22soft%22%20class%20prototype%20and%20the%20query%20sample.%20These%0Aaugmentations%20enable%20the%20CTF%20module%20to%20recognize%20complex%20intra-class%20variations%0Adespite%20the%20limited%20original%20training%20data%2C%20and%20seek%20out%20invariant%20class-wise%0Aproperties%20for%20accurate%20classification%20performance.%20The%20proposed%20method%20is%0Acomprehensively%20evaluated%20on%20five%20different%20classification%20tasks.%20Compared%20to%0Astandard%20TF%20and%20several%20DL%20benchmarks%2C%20notable%20performance%20improvements%20up%20to%0A18.7%25%20were%20achieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligently%2520Augmented%2520Contrastive%2520Tensor%2520Factorization%253A%2520Empowering%250A%2520%2520Multi-dimensional%2520Time%2520Series%2520Classification%2520in%2520Low-Data%2520Environments%26entry.906535625%3DAnushiya%2520Arunan%2520and%2520Yan%2520Qin%2520and%2520Xiaoli%2520Li%2520and%2520Yuen%2520Chau%26entry.1292438233%3D%2520%2520Classification%2520of%2520multi-dimensional%2520time%2520series%2520from%2520real-world%2520systems%250Arequire%2520fine-grained%2520learning%2520of%2520complex%2520features%2520such%2520as%2520cross-dimensional%250Adependencies%2520and%2520intra-class%2520variations-all%2520under%2520the%2520practical%2520challenge%2520of%250Alow%2520training%2520data%2520availability.%2520However%252C%2520standard%2520deep%2520learning%2520%2528DL%2529%2520struggles%250Ato%2520learn%2520generalizable%2520features%2520in%2520low-data%2520environments%2520due%2520to%2520model%250Aoverfitting.%2520We%2520propose%2520a%2520versatile%2520yet%2520data-efficient%2520framework%252C%2520Intelligently%250AAugmented%2520Contrastive%2520Tensor%2520Factorization%2520%2528ITA-CTF%2529%252C%2520to%2520learn%2520effective%250Arepresentations%2520from%2520multi-dimensional%2520time%2520series.%2520The%2520CTF%2520module%2520learns%2520core%250Aexplanatory%2520components%2520of%2520the%2520time%2520series%2520%2528e.g.%252C%2520sensor%2520factors%252C%2520temporal%250Afactors%2529%252C%2520and%2520importantly%252C%2520their%2520joint%2520dependencies.%2520Notably%252C%2520unlike%2520standard%250Atensor%2520factorization%2520%2528TF%2529%252C%2520the%2520CTF%2520module%2520incorporates%2520a%2520new%2520contrastive%2520loss%250Aoptimization%2520to%2520induce%2520similarity%2520learning%2520and%2520class-awareness%2520into%2520the%2520learnt%250Arepresentations%2520for%2520better%2520classification%2520performance.%2520To%2520strengthen%2520this%250Acontrastive%2520learning%252C%2520the%2520preceding%2520ITA%2520module%2520generates%2520targeted%2520but%250Ainformative%2520augmentations%2520that%2520highlight%2520realistic%2520intra-class%2520patterns%2520in%2520the%250Aoriginal%2520data%252C%2520while%2520preserving%2520class-wise%2520properties.%2520This%2520is%2520achieved%2520by%250Adynamically%2520sampling%2520a%2520%2522soft%2522%2520class%2520prototype%2520to%2520guide%2520the%2520warping%2520of%2520each%250Aquery%2520data%2520sample%252C%2520which%2520results%2520in%2520an%2520augmentation%2520that%2520is%2520intelligently%250Apattern-mixed%2520between%2520the%2520%2522soft%2522%2520class%2520prototype%2520and%2520the%2520query%2520sample.%2520These%250Aaugmentations%2520enable%2520the%2520CTF%2520module%2520to%2520recognize%2520complex%2520intra-class%2520variations%250Adespite%2520the%2520limited%2520original%2520training%2520data%252C%2520and%2520seek%2520out%2520invariant%2520class-wise%250Aproperties%2520for%2520accurate%2520classification%2520performance.%2520The%2520proposed%2520method%2520is%250Acomprehensively%2520evaluated%2520on%2520five%2520different%2520classification%2520tasks.%2520Compared%2520to%250Astandard%2520TF%2520and%2520several%2520DL%2520benchmarks%252C%2520notable%2520performance%2520improvements%2520up%2520to%250A18.7%2525%2520were%2520achieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligently%20Augmented%20Contrastive%20Tensor%20Factorization%3A%20Empowering%0A%20%20Multi-dimensional%20Time%20Series%20Classification%20in%20Low-Data%20Environments&entry.906535625=Anushiya%20Arunan%20and%20Yan%20Qin%20and%20Xiaoli%20Li%20and%20Yuen%20Chau&entry.1292438233=%20%20Classification%20of%20multi-dimensional%20time%20series%20from%20real-world%20systems%0Arequire%20fine-grained%20learning%20of%20complex%20features%20such%20as%20cross-dimensional%0Adependencies%20and%20intra-class%20variations-all%20under%20the%20practical%20challenge%20of%0Alow%20training%20data%20availability.%20However%2C%20standard%20deep%20learning%20%28DL%29%20struggles%0Ato%20learn%20generalizable%20features%20in%20low-data%20environments%20due%20to%20model%0Aoverfitting.%20We%20propose%20a%20versatile%20yet%20data-efficient%20framework%2C%20Intelligently%0AAugmented%20Contrastive%20Tensor%20Factorization%20%28ITA-CTF%29%2C%20to%20learn%20effective%0Arepresentations%20from%20multi-dimensional%20time%20series.%20The%20CTF%20module%20learns%20core%0Aexplanatory%20components%20of%20the%20time%20series%20%28e.g.%2C%20sensor%20factors%2C%20temporal%0Afactors%29%2C%20and%20importantly%2C%20their%20joint%20dependencies.%20Notably%2C%20unlike%20standard%0Atensor%20factorization%20%28TF%29%2C%20the%20CTF%20module%20incorporates%20a%20new%20contrastive%20loss%0Aoptimization%20to%20induce%20similarity%20learning%20and%20class-awareness%20into%20the%20learnt%0Arepresentations%20for%20better%20classification%20performance.%20To%20strengthen%20this%0Acontrastive%20learning%2C%20the%20preceding%20ITA%20module%20generates%20targeted%20but%0Ainformative%20augmentations%20that%20highlight%20realistic%20intra-class%20patterns%20in%20the%0Aoriginal%20data%2C%20while%20preserving%20class-wise%20properties.%20This%20is%20achieved%20by%0Adynamically%20sampling%20a%20%22soft%22%20class%20prototype%20to%20guide%20the%20warping%20of%20each%0Aquery%20data%20sample%2C%20which%20results%20in%20an%20augmentation%20that%20is%20intelligently%0Apattern-mixed%20between%20the%20%22soft%22%20class%20prototype%20and%20the%20query%20sample.%20These%0Aaugmentations%20enable%20the%20CTF%20module%20to%20recognize%20complex%20intra-class%20variations%0Adespite%20the%20limited%20original%20training%20data%2C%20and%20seek%20out%20invariant%20class-wise%0Aproperties%20for%20accurate%20classification%20performance.%20The%20proposed%20method%20is%0Acomprehensively%20evaluated%20on%20five%20different%20classification%20tasks.%20Compared%20to%0Astandard%20TF%20and%20several%20DL%20benchmarks%2C%20notable%20performance%20improvements%20up%20to%0A18.7%25%20were%20achieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03825v2&entry.124074799=Read"},
{"title": "Learning Progress Driven Multi-Agent Curriculum", "author": "Wenshuai Zhao and Zhiyuan Li and Joni Pajarinen", "abstract": "  The number of agents can be an effective curriculum variable for controlling\nthe difficulty of multi-agent reinforcement learning (MARL) tasks. Existing\nwork typically uses manually defined curricula such as linear schemes. We\nidentify two potential flaws while applying existing reward-based automatic\ncurriculum learning methods in MARL: (1) The expected episode return used to\nmeasure task difficulty has high variance; (2) Credit assignment difficulty can\nbe exacerbated in tasks where increasing the number of agents yields higher\nreturns which is common in many MARL tasks. To address these issues, we propose\nto control the curriculum by using a TD-error based *learning progress* measure\nand by letting the curriculum proceed from an initial context distribution to\nthe final task specific one. Since our approach maintains a distribution over\nthe number of agents and measures learning progress rather than absolute\nperformance, which often increases with the number of agents, we alleviate\nproblem (2). Moreover, the learning progress measure naturally alleviates\nproblem (1) by aggregating returns. In three challenging sparse-reward MARL\nbenchmarks, our approach outperforms state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2205.10016v3", "date": "2025-05-15", "relevancy": 2.1423, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Progress%20Driven%20Multi-Agent%20Curriculum&body=Title%3A%20Learning%20Progress%20Driven%20Multi-Agent%20Curriculum%0AAuthor%3A%20Wenshuai%20Zhao%20and%20Zhiyuan%20Li%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20The%20number%20of%20agents%20can%20be%20an%20effective%20curriculum%20variable%20for%20controlling%0Athe%20difficulty%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%20tasks.%20Existing%0Awork%20typically%20uses%20manually%20defined%20curricula%20such%20as%20linear%20schemes.%20We%0Aidentify%20two%20potential%20flaws%20while%20applying%20existing%20reward-based%20automatic%0Acurriculum%20learning%20methods%20in%20MARL%3A%20%281%29%20The%20expected%20episode%20return%20used%20to%0Ameasure%20task%20difficulty%20has%20high%20variance%3B%20%282%29%20Credit%20assignment%20difficulty%20can%0Abe%20exacerbated%20in%20tasks%20where%20increasing%20the%20number%20of%20agents%20yields%20higher%0Areturns%20which%20is%20common%20in%20many%20MARL%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%0Ato%20control%20the%20curriculum%20by%20using%20a%20TD-error%20based%20%2Alearning%20progress%2A%20measure%0Aand%20by%20letting%20the%20curriculum%20proceed%20from%20an%20initial%20context%20distribution%20to%0Athe%20final%20task%20specific%20one.%20Since%20our%20approach%20maintains%20a%20distribution%20over%0Athe%20number%20of%20agents%20and%20measures%20learning%20progress%20rather%20than%20absolute%0Aperformance%2C%20which%20often%20increases%20with%20the%20number%20of%20agents%2C%20we%20alleviate%0Aproblem%20%282%29.%20Moreover%2C%20the%20learning%20progress%20measure%20naturally%20alleviates%0Aproblem%20%281%29%20by%20aggregating%20returns.%20In%20three%20challenging%20sparse-reward%20MARL%0Abenchmarks%2C%20our%20approach%20outperforms%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.10016v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Progress%2520Driven%2520Multi-Agent%2520Curriculum%26entry.906535625%3DWenshuai%2520Zhao%2520and%2520Zhiyuan%2520Li%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520The%2520number%2520of%2520agents%2520can%2520be%2520an%2520effective%2520curriculum%2520variable%2520for%2520controlling%250Athe%2520difficulty%2520of%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520tasks.%2520Existing%250Awork%2520typically%2520uses%2520manually%2520defined%2520curricula%2520such%2520as%2520linear%2520schemes.%2520We%250Aidentify%2520two%2520potential%2520flaws%2520while%2520applying%2520existing%2520reward-based%2520automatic%250Acurriculum%2520learning%2520methods%2520in%2520MARL%253A%2520%25281%2529%2520The%2520expected%2520episode%2520return%2520used%2520to%250Ameasure%2520task%2520difficulty%2520has%2520high%2520variance%253B%2520%25282%2529%2520Credit%2520assignment%2520difficulty%2520can%250Abe%2520exacerbated%2520in%2520tasks%2520where%2520increasing%2520the%2520number%2520of%2520agents%2520yields%2520higher%250Areturns%2520which%2520is%2520common%2520in%2520many%2520MARL%2520tasks.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250Ato%2520control%2520the%2520curriculum%2520by%2520using%2520a%2520TD-error%2520based%2520%252Alearning%2520progress%252A%2520measure%250Aand%2520by%2520letting%2520the%2520curriculum%2520proceed%2520from%2520an%2520initial%2520context%2520distribution%2520to%250Athe%2520final%2520task%2520specific%2520one.%2520Since%2520our%2520approach%2520maintains%2520a%2520distribution%2520over%250Athe%2520number%2520of%2520agents%2520and%2520measures%2520learning%2520progress%2520rather%2520than%2520absolute%250Aperformance%252C%2520which%2520often%2520increases%2520with%2520the%2520number%2520of%2520agents%252C%2520we%2520alleviate%250Aproblem%2520%25282%2529.%2520Moreover%252C%2520the%2520learning%2520progress%2520measure%2520naturally%2520alleviates%250Aproblem%2520%25281%2529%2520by%2520aggregating%2520returns.%2520In%2520three%2520challenging%2520sparse-reward%2520MARL%250Abenchmarks%252C%2520our%2520approach%2520outperforms%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.10016v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Progress%20Driven%20Multi-Agent%20Curriculum&entry.906535625=Wenshuai%20Zhao%20and%20Zhiyuan%20Li%20and%20Joni%20Pajarinen&entry.1292438233=%20%20The%20number%20of%20agents%20can%20be%20an%20effective%20curriculum%20variable%20for%20controlling%0Athe%20difficulty%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%20tasks.%20Existing%0Awork%20typically%20uses%20manually%20defined%20curricula%20such%20as%20linear%20schemes.%20We%0Aidentify%20two%20potential%20flaws%20while%20applying%20existing%20reward-based%20automatic%0Acurriculum%20learning%20methods%20in%20MARL%3A%20%281%29%20The%20expected%20episode%20return%20used%20to%0Ameasure%20task%20difficulty%20has%20high%20variance%3B%20%282%29%20Credit%20assignment%20difficulty%20can%0Abe%20exacerbated%20in%20tasks%20where%20increasing%20the%20number%20of%20agents%20yields%20higher%0Areturns%20which%20is%20common%20in%20many%20MARL%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%0Ato%20control%20the%20curriculum%20by%20using%20a%20TD-error%20based%20%2Alearning%20progress%2A%20measure%0Aand%20by%20letting%20the%20curriculum%20proceed%20from%20an%20initial%20context%20distribution%20to%0Athe%20final%20task%20specific%20one.%20Since%20our%20approach%20maintains%20a%20distribution%20over%0Athe%20number%20of%20agents%20and%20measures%20learning%20progress%20rather%20than%20absolute%0Aperformance%2C%20which%20often%20increases%20with%20the%20number%20of%20agents%2C%20we%20alleviate%0Aproblem%20%282%29.%20Moreover%2C%20the%20learning%20progress%20measure%20naturally%20alleviates%0Aproblem%20%281%29%20by%20aggregating%20returns.%20In%20three%20challenging%20sparse-reward%20MARL%0Abenchmarks%2C%20our%20approach%20outperforms%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.10016v3&entry.124074799=Read"},
{"title": "Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection", "author": "Hanzhe Liang and Aoran Wang and Jie Zhou and Xin Jin and Can Gao and Jinbao Wang", "abstract": "  In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD\n", "link": "http://arxiv.org/abs/2505.05901v2", "date": "2025-05-15", "relevancy": 2.1169, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5287}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20the%20Source%20of%20Defects%20from%20a%20Mechanical%20Perspective%20for%203D%0A%20%20Anomaly%20Detection&body=Title%3A%20Examining%20the%20Source%20of%20Defects%20from%20a%20Mechanical%20Perspective%20for%203D%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Hanzhe%20Liang%20and%20Aoran%20Wang%20and%20Jie%20Zhou%20and%20Xin%20Jin%20and%20Can%20Gao%20and%20Jinbao%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20a%20novel%20approach%20to%203D%20anomaly%20detection%20%28AD%29%20that%0Agoes%20beyond%20merely%20identifying%20anomalies%20based%20on%20structural%20characteristics.%0AOur%20primary%20perspective%20is%20that%20most%20anomalies%20arise%20from%20unpredictable%0Adefective%20forces%20originating%20from%20both%20internal%20and%20external%20sources.%20To%0Aaddress%20these%20anomalies%2C%20we%20seek%20out%20opposing%20forces%20that%20can%20help%20correct%0Athem.%20Therefore%2C%20we%20introduce%20the%20Mechanics%20Complementary%20Model-based%20Framework%0Afor%20the%203D-AD%20task%20%28MC4AD%29%2C%20which%20generates%20internal%20and%20external%20corrective%0Aforces%20for%20each%20point.%20We%20first%20propose%20a%20Diverse%20Anomaly-Generation%20%28DA-Gen%29%0Amodule%20designed%20to%20simulate%20various%20types%20of%20anomalies.%20Next%2C%20we%20present%20the%0ACorrective%20Force%20Prediction%20Network%20%28CFP-Net%29%2C%20which%20uses%20complementary%0Arepresentations%20for%20point-level%20analysis%20to%20simulate%20the%20different%0Acontributions%20from%20internal%20and%20external%20corrective%20forces.%20To%20ensure%20the%0Acorrective%20forces%20are%20constrained%20effectively%2C%20we%20have%20developed%20a%20combined%0Aloss%20function%20that%20includes%20a%20new%20symmetric%20loss%20and%20an%20overall%20loss.%20Notably%2C%0Awe%20implement%20a%20Hierarchical%20Quality%20Control%20%28HQC%29%20strategy%20based%20on%20a%20three-way%0Adecision%20process%20and%20contribute%20a%20dataset%20titled%20Anomaly-IntraVariance%2C%20which%0Aincorporates%20intraclass%20variance%20to%20evaluate%20our%20model.%20As%20a%20result%2C%20the%0Aproposed%20MC4AD%20has%20been%20proven%20effective%20through%20theory%20and%20experimentation.%0AThe%20experimental%20results%20demonstrate%20that%20our%20approach%20yields%20nine%0Astate-of-the-art%20performances%2C%20achieving%20optimal%20results%20with%20minimal%0Aparameters%20and%20the%20fastest%20inference%20speed%20across%20five%20existing%20datasets%2C%20in%0Aaddition%20to%20the%20proposed%20Anomaly-IntraVariance%20dataset.%20The%20source%20is%20available%0Aat%20https%3A//github.com/hzzzzzhappy/MC4AD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520the%2520Source%2520of%2520Defects%2520from%2520a%2520Mechanical%2520Perspective%2520for%25203D%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DHanzhe%2520Liang%2520and%2520Aoran%2520Wang%2520and%2520Jie%2520Zhou%2520and%2520Xin%2520Jin%2520and%2520Can%2520Gao%2520and%2520Jinbao%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520novel%2520approach%2520to%25203D%2520anomaly%2520detection%2520%2528AD%2529%2520that%250Agoes%2520beyond%2520merely%2520identifying%2520anomalies%2520based%2520on%2520structural%2520characteristics.%250AOur%2520primary%2520perspective%2520is%2520that%2520most%2520anomalies%2520arise%2520from%2520unpredictable%250Adefective%2520forces%2520originating%2520from%2520both%2520internal%2520and%2520external%2520sources.%2520To%250Aaddress%2520these%2520anomalies%252C%2520we%2520seek%2520out%2520opposing%2520forces%2520that%2520can%2520help%2520correct%250Athem.%2520Therefore%252C%2520we%2520introduce%2520the%2520Mechanics%2520Complementary%2520Model-based%2520Framework%250Afor%2520the%25203D-AD%2520task%2520%2528MC4AD%2529%252C%2520which%2520generates%2520internal%2520and%2520external%2520corrective%250Aforces%2520for%2520each%2520point.%2520We%2520first%2520propose%2520a%2520Diverse%2520Anomaly-Generation%2520%2528DA-Gen%2529%250Amodule%2520designed%2520to%2520simulate%2520various%2520types%2520of%2520anomalies.%2520Next%252C%2520we%2520present%2520the%250ACorrective%2520Force%2520Prediction%2520Network%2520%2528CFP-Net%2529%252C%2520which%2520uses%2520complementary%250Arepresentations%2520for%2520point-level%2520analysis%2520to%2520simulate%2520the%2520different%250Acontributions%2520from%2520internal%2520and%2520external%2520corrective%2520forces.%2520To%2520ensure%2520the%250Acorrective%2520forces%2520are%2520constrained%2520effectively%252C%2520we%2520have%2520developed%2520a%2520combined%250Aloss%2520function%2520that%2520includes%2520a%2520new%2520symmetric%2520loss%2520and%2520an%2520overall%2520loss.%2520Notably%252C%250Awe%2520implement%2520a%2520Hierarchical%2520Quality%2520Control%2520%2528HQC%2529%2520strategy%2520based%2520on%2520a%2520three-way%250Adecision%2520process%2520and%2520contribute%2520a%2520dataset%2520titled%2520Anomaly-IntraVariance%252C%2520which%250Aincorporates%2520intraclass%2520variance%2520to%2520evaluate%2520our%2520model.%2520As%2520a%2520result%252C%2520the%250Aproposed%2520MC4AD%2520has%2520been%2520proven%2520effective%2520through%2520theory%2520and%2520experimentation.%250AThe%2520experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520yields%2520nine%250Astate-of-the-art%2520performances%252C%2520achieving%2520optimal%2520results%2520with%2520minimal%250Aparameters%2520and%2520the%2520fastest%2520inference%2520speed%2520across%2520five%2520existing%2520datasets%252C%2520in%250Aaddition%2520to%2520the%2520proposed%2520Anomaly-IntraVariance%2520dataset.%2520The%2520source%2520is%2520available%250Aat%2520https%253A//github.com/hzzzzzhappy/MC4AD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20the%20Source%20of%20Defects%20from%20a%20Mechanical%20Perspective%20for%203D%0A%20%20Anomaly%20Detection&entry.906535625=Hanzhe%20Liang%20and%20Aoran%20Wang%20and%20Jie%20Zhou%20and%20Xin%20Jin%20and%20Can%20Gao%20and%20Jinbao%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20a%20novel%20approach%20to%203D%20anomaly%20detection%20%28AD%29%20that%0Agoes%20beyond%20merely%20identifying%20anomalies%20based%20on%20structural%20characteristics.%0AOur%20primary%20perspective%20is%20that%20most%20anomalies%20arise%20from%20unpredictable%0Adefective%20forces%20originating%20from%20both%20internal%20and%20external%20sources.%20To%0Aaddress%20these%20anomalies%2C%20we%20seek%20out%20opposing%20forces%20that%20can%20help%20correct%0Athem.%20Therefore%2C%20we%20introduce%20the%20Mechanics%20Complementary%20Model-based%20Framework%0Afor%20the%203D-AD%20task%20%28MC4AD%29%2C%20which%20generates%20internal%20and%20external%20corrective%0Aforces%20for%20each%20point.%20We%20first%20propose%20a%20Diverse%20Anomaly-Generation%20%28DA-Gen%29%0Amodule%20designed%20to%20simulate%20various%20types%20of%20anomalies.%20Next%2C%20we%20present%20the%0ACorrective%20Force%20Prediction%20Network%20%28CFP-Net%29%2C%20which%20uses%20complementary%0Arepresentations%20for%20point-level%20analysis%20to%20simulate%20the%20different%0Acontributions%20from%20internal%20and%20external%20corrective%20forces.%20To%20ensure%20the%0Acorrective%20forces%20are%20constrained%20effectively%2C%20we%20have%20developed%20a%20combined%0Aloss%20function%20that%20includes%20a%20new%20symmetric%20loss%20and%20an%20overall%20loss.%20Notably%2C%0Awe%20implement%20a%20Hierarchical%20Quality%20Control%20%28HQC%29%20strategy%20based%20on%20a%20three-way%0Adecision%20process%20and%20contribute%20a%20dataset%20titled%20Anomaly-IntraVariance%2C%20which%0Aincorporates%20intraclass%20variance%20to%20evaluate%20our%20model.%20As%20a%20result%2C%20the%0Aproposed%20MC4AD%20has%20been%20proven%20effective%20through%20theory%20and%20experimentation.%0AThe%20experimental%20results%20demonstrate%20that%20our%20approach%20yields%20nine%0Astate-of-the-art%20performances%2C%20achieving%20optimal%20results%20with%20minimal%0Aparameters%20and%20the%20fastest%20inference%20speed%20across%20five%20existing%20datasets%2C%20in%0Aaddition%20to%20the%20proposed%20Anomaly-IntraVariance%20dataset.%20The%20source%20is%20available%0Aat%20https%3A//github.com/hzzzzzhappy/MC4AD%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05901v2&entry.124074799=Read"},
{"title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets", "author": "Chenwei Zhang and Khanh Dao Duc", "abstract": "  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n", "link": "http://arxiv.org/abs/2503.20291v2", "date": "2025-05-15", "relevancy": 2.1134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CryoSAMU%3A%20Enhancing%203D%20Cryo-EM%20Density%20Maps%20of%20Protein%20Structures%20at%0A%20%20Intermediate%20Resolution%20with%20Structure-Aware%20Multimodal%20U-Nets&body=Title%3A%20CryoSAMU%3A%20Enhancing%203D%20Cryo-EM%20Density%20Maps%20of%20Protein%20Structures%20at%0A%20%20Intermediate%20Resolution%20with%20Structure-Aware%20Multimodal%20U-Nets%0AAuthor%3A%20Chenwei%20Zhang%20and%20Khanh%20Dao%20Duc%0AAbstract%3A%20%20%20Enhancing%20cryogenic%20electron%20microscopy%20%28cryo-EM%29%203D%20density%20maps%20at%0Aintermediate%20resolution%20%284-8%20%7B%5CAA%7D%29%20is%20crucial%20in%20protein%20structure%0Adetermination.%20Recent%20advances%20in%20deep%20learning%20have%20led%20to%20the%20development%20of%0Aautomated%20approaches%20for%20enhancing%20experimental%20cryo-EM%20density%20maps.%20Yet%2C%0Athese%20methods%20are%20not%20optimized%20for%20intermediate-resolution%20maps%20and%20rely%20on%0Amap%20density%20features%20alone.%20To%20address%20this%2C%20we%20propose%20CryoSAMU%2C%20a%20novel%0Amethod%20designed%20to%20enhance%203D%20cryo-EM%20density%20maps%20of%20protein%20structures%20using%0Astructure-aware%20multimodal%20U-Nets%20and%20trained%20on%20curated%0Aintermediate-resolution%20density%20maps.%20We%20comprehensively%20evaluate%20CryoSAMU%0Aacross%20various%20metrics%20and%20demonstrate%20its%20competitive%20performance%20compared%20to%0Astate-of-the-art%20methods.%20Notably%2C%20CryoSAMU%20achieves%20significantly%20faster%0Aprocessing%20speed%2C%20showing%20promise%20for%20future%20practical%20applications.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/chenwei-zhang/CryoSAMU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryoSAMU%253A%2520Enhancing%25203D%2520Cryo-EM%2520Density%2520Maps%2520of%2520Protein%2520Structures%2520at%250A%2520%2520Intermediate%2520Resolution%2520with%2520Structure-Aware%2520Multimodal%2520U-Nets%26entry.906535625%3DChenwei%2520Zhang%2520and%2520Khanh%2520Dao%2520Duc%26entry.1292438233%3D%2520%2520Enhancing%2520cryogenic%2520electron%2520microscopy%2520%2528cryo-EM%2529%25203D%2520density%2520maps%2520at%250Aintermediate%2520resolution%2520%25284-8%2520%257B%255CAA%257D%2529%2520is%2520crucial%2520in%2520protein%2520structure%250Adetermination.%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520led%2520to%2520the%2520development%2520of%250Aautomated%2520approaches%2520for%2520enhancing%2520experimental%2520cryo-EM%2520density%2520maps.%2520Yet%252C%250Athese%2520methods%2520are%2520not%2520optimized%2520for%2520intermediate-resolution%2520maps%2520and%2520rely%2520on%250Amap%2520density%2520features%2520alone.%2520To%2520address%2520this%252C%2520we%2520propose%2520CryoSAMU%252C%2520a%2520novel%250Amethod%2520designed%2520to%2520enhance%25203D%2520cryo-EM%2520density%2520maps%2520of%2520protein%2520structures%2520using%250Astructure-aware%2520multimodal%2520U-Nets%2520and%2520trained%2520on%2520curated%250Aintermediate-resolution%2520density%2520maps.%2520We%2520comprehensively%2520evaluate%2520CryoSAMU%250Aacross%2520various%2520metrics%2520and%2520demonstrate%2520its%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods.%2520Notably%252C%2520CryoSAMU%2520achieves%2520significantly%2520faster%250Aprocessing%2520speed%252C%2520showing%2520promise%2520for%2520future%2520practical%2520applications.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/chenwei-zhang/CryoSAMU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CryoSAMU%3A%20Enhancing%203D%20Cryo-EM%20Density%20Maps%20of%20Protein%20Structures%20at%0A%20%20Intermediate%20Resolution%20with%20Structure-Aware%20Multimodal%20U-Nets&entry.906535625=Chenwei%20Zhang%20and%20Khanh%20Dao%20Duc&entry.1292438233=%20%20Enhancing%20cryogenic%20electron%20microscopy%20%28cryo-EM%29%203D%20density%20maps%20at%0Aintermediate%20resolution%20%284-8%20%7B%5CAA%7D%29%20is%20crucial%20in%20protein%20structure%0Adetermination.%20Recent%20advances%20in%20deep%20learning%20have%20led%20to%20the%20development%20of%0Aautomated%20approaches%20for%20enhancing%20experimental%20cryo-EM%20density%20maps.%20Yet%2C%0Athese%20methods%20are%20not%20optimized%20for%20intermediate-resolution%20maps%20and%20rely%20on%0Amap%20density%20features%20alone.%20To%20address%20this%2C%20we%20propose%20CryoSAMU%2C%20a%20novel%0Amethod%20designed%20to%20enhance%203D%20cryo-EM%20density%20maps%20of%20protein%20structures%20using%0Astructure-aware%20multimodal%20U-Nets%20and%20trained%20on%20curated%0Aintermediate-resolution%20density%20maps.%20We%20comprehensively%20evaluate%20CryoSAMU%0Aacross%20various%20metrics%20and%20demonstrate%20its%20competitive%20performance%20compared%20to%0Astate-of-the-art%20methods.%20Notably%2C%20CryoSAMU%20achieves%20significantly%20faster%0Aprocessing%20speed%2C%20showing%20promise%20for%20future%20practical%20applications.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/chenwei-zhang/CryoSAMU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20291v2&entry.124074799=Read"},
{"title": "A systematic review of challenges and proposed solutions in modeling\n  multimodal data", "author": "Maryam Farhadizadeh and Maria Weymann and Michael Bla\u00df and Johann Kraus and Christopher Gundler and Sebastian Walter and Noah Hempen and Harald Binder and Nadine Binder", "abstract": "  Multimodal data modeling has emerged as a powerful approach in clinical\nresearch, enabling the integration of diverse data types such as imaging,\ngenomics, wearable sensors, and electronic health records. Despite its\npotential to improve diagnostic accuracy and support personalized care,\nmodeling such heterogeneous data presents significant technical challenges.\nThis systematic review synthesizes findings from 69 studies to identify common\nobstacles, including missing modalities, limited sample sizes, dimensionality\nimbalance, interpretability issues, and finding the optimal fusion techniques.\nWe highlight recent methodological advances, such as transfer learning,\ngenerative models, attention mechanisms, and neural architecture search that\noffer promising solutions. By mapping current trends and innovations, this\nreview provides a comprehensive overview of the field and offers practical\ninsights to guide future research and development in multimodal modeling for\nmedical applications.\n", "link": "http://arxiv.org/abs/2505.06945v2", "date": "2025-05-15", "relevancy": 2.1081, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20systematic%20review%20of%20challenges%20and%20proposed%20solutions%20in%20modeling%0A%20%20multimodal%20data&body=Title%3A%20A%20systematic%20review%20of%20challenges%20and%20proposed%20solutions%20in%20modeling%0A%20%20multimodal%20data%0AAuthor%3A%20Maryam%20Farhadizadeh%20and%20Maria%20Weymann%20and%20Michael%20Bla%C3%9F%20and%20Johann%20Kraus%20and%20Christopher%20Gundler%20and%20Sebastian%20Walter%20and%20Noah%20Hempen%20and%20Harald%20Binder%20and%20Nadine%20Binder%0AAbstract%3A%20%20%20Multimodal%20data%20modeling%20has%20emerged%20as%20a%20powerful%20approach%20in%20clinical%0Aresearch%2C%20enabling%20the%20integration%20of%20diverse%20data%20types%20such%20as%20imaging%2C%0Agenomics%2C%20wearable%20sensors%2C%20and%20electronic%20health%20records.%20Despite%20its%0Apotential%20to%20improve%20diagnostic%20accuracy%20and%20support%20personalized%20care%2C%0Amodeling%20such%20heterogeneous%20data%20presents%20significant%20technical%20challenges.%0AThis%20systematic%20review%20synthesizes%20findings%20from%2069%20studies%20to%20identify%20common%0Aobstacles%2C%20including%20missing%20modalities%2C%20limited%20sample%20sizes%2C%20dimensionality%0Aimbalance%2C%20interpretability%20issues%2C%20and%20finding%20the%20optimal%20fusion%20techniques.%0AWe%20highlight%20recent%20methodological%20advances%2C%20such%20as%20transfer%20learning%2C%0Agenerative%20models%2C%20attention%20mechanisms%2C%20and%20neural%20architecture%20search%20that%0Aoffer%20promising%20solutions.%20By%20mapping%20current%20trends%20and%20innovations%2C%20this%0Areview%20provides%20a%20comprehensive%20overview%20of%20the%20field%20and%20offers%20practical%0Ainsights%20to%20guide%20future%20research%20and%20development%20in%20multimodal%20modeling%20for%0Amedical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520systematic%2520review%2520of%2520challenges%2520and%2520proposed%2520solutions%2520in%2520modeling%250A%2520%2520multimodal%2520data%26entry.906535625%3DMaryam%2520Farhadizadeh%2520and%2520Maria%2520Weymann%2520and%2520Michael%2520Bla%25C3%259F%2520and%2520Johann%2520Kraus%2520and%2520Christopher%2520Gundler%2520and%2520Sebastian%2520Walter%2520and%2520Noah%2520Hempen%2520and%2520Harald%2520Binder%2520and%2520Nadine%2520Binder%26entry.1292438233%3D%2520%2520Multimodal%2520data%2520modeling%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520in%2520clinical%250Aresearch%252C%2520enabling%2520the%2520integration%2520of%2520diverse%2520data%2520types%2520such%2520as%2520imaging%252C%250Agenomics%252C%2520wearable%2520sensors%252C%2520and%2520electronic%2520health%2520records.%2520Despite%2520its%250Apotential%2520to%2520improve%2520diagnostic%2520accuracy%2520and%2520support%2520personalized%2520care%252C%250Amodeling%2520such%2520heterogeneous%2520data%2520presents%2520significant%2520technical%2520challenges.%250AThis%2520systematic%2520review%2520synthesizes%2520findings%2520from%252069%2520studies%2520to%2520identify%2520common%250Aobstacles%252C%2520including%2520missing%2520modalities%252C%2520limited%2520sample%2520sizes%252C%2520dimensionality%250Aimbalance%252C%2520interpretability%2520issues%252C%2520and%2520finding%2520the%2520optimal%2520fusion%2520techniques.%250AWe%2520highlight%2520recent%2520methodological%2520advances%252C%2520such%2520as%2520transfer%2520learning%252C%250Agenerative%2520models%252C%2520attention%2520mechanisms%252C%2520and%2520neural%2520architecture%2520search%2520that%250Aoffer%2520promising%2520solutions.%2520By%2520mapping%2520current%2520trends%2520and%2520innovations%252C%2520this%250Areview%2520provides%2520a%2520comprehensive%2520overview%2520of%2520the%2520field%2520and%2520offers%2520practical%250Ainsights%2520to%2520guide%2520future%2520research%2520and%2520development%2520in%2520multimodal%2520modeling%2520for%250Amedical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20systematic%20review%20of%20challenges%20and%20proposed%20solutions%20in%20modeling%0A%20%20multimodal%20data&entry.906535625=Maryam%20Farhadizadeh%20and%20Maria%20Weymann%20and%20Michael%20Bla%C3%9F%20and%20Johann%20Kraus%20and%20Christopher%20Gundler%20and%20Sebastian%20Walter%20and%20Noah%20Hempen%20and%20Harald%20Binder%20and%20Nadine%20Binder&entry.1292438233=%20%20Multimodal%20data%20modeling%20has%20emerged%20as%20a%20powerful%20approach%20in%20clinical%0Aresearch%2C%20enabling%20the%20integration%20of%20diverse%20data%20types%20such%20as%20imaging%2C%0Agenomics%2C%20wearable%20sensors%2C%20and%20electronic%20health%20records.%20Despite%20its%0Apotential%20to%20improve%20diagnostic%20accuracy%20and%20support%20personalized%20care%2C%0Amodeling%20such%20heterogeneous%20data%20presents%20significant%20technical%20challenges.%0AThis%20systematic%20review%20synthesizes%20findings%20from%2069%20studies%20to%20identify%20common%0Aobstacles%2C%20including%20missing%20modalities%2C%20limited%20sample%20sizes%2C%20dimensionality%0Aimbalance%2C%20interpretability%20issues%2C%20and%20finding%20the%20optimal%20fusion%20techniques.%0AWe%20highlight%20recent%20methodological%20advances%2C%20such%20as%20transfer%20learning%2C%0Agenerative%20models%2C%20attention%20mechanisms%2C%20and%20neural%20architecture%20search%20that%0Aoffer%20promising%20solutions.%20By%20mapping%20current%20trends%20and%20innovations%2C%20this%0Areview%20provides%20a%20comprehensive%20overview%20of%20the%20field%20and%20offers%20practical%0Ainsights%20to%20guide%20future%20research%20and%20development%20in%20multimodal%20modeling%20for%0Amedical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06945v2&entry.124074799=Read"},
{"title": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories", "author": "Sejin Kim and Hosung Lee and Sundong Kim", "abstract": "  Understanding misalignments in human task-solving trajectories is critical\nfor improving AI models trained to mimic human reasoning. This study\ncategorizes such misalignments into three types: \\textbf{(1) Lack of functions\nto express intent}, \\textbf{(2) Inefficient action sequences}, and \\textbf{(3)\nIncorrect intentions that cannot solve the task}. To address these issues, we\nfirst formalize and define these three types of misalignments. We then propose\na heuristic algorithm to detect these misalignments in O2ARC trajectories and\nconduct a hierarchical and quantitative analysis of their impact. Furthermore,\nwe introduce an intention estimation algorithm that predicts missing alignment\ninformation between user actions and inferred intentions, leveraging our\nformalized framework. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the potential of intention learning.\n", "link": "http://arxiv.org/abs/2409.14191v3", "date": "2025-05-15", "relevancy": 2.1024, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5313}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories&body=Title%3A%20Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories%0AAuthor%3A%20Sejin%20Kim%20and%20Hosung%20Lee%20and%20Sundong%20Kim%0AAbstract%3A%20%20%20Understanding%20misalignments%20in%20human%20task-solving%20trajectories%20is%20critical%0Afor%20improving%20AI%20models%20trained%20to%20mimic%20human%20reasoning.%20This%20study%0Acategorizes%20such%20misalignments%20into%20three%20types%3A%20%5Ctextbf%7B%281%29%20Lack%20of%20functions%0Ato%20express%20intent%7D%2C%20%5Ctextbf%7B%282%29%20Inefficient%20action%20sequences%7D%2C%20and%20%5Ctextbf%7B%283%29%0AIncorrect%20intentions%20that%20cannot%20solve%20the%20task%7D.%20To%20address%20these%20issues%2C%20we%0Afirst%20formalize%20and%20define%20these%20three%20types%20of%20misalignments.%20We%20then%20propose%0Aa%20heuristic%20algorithm%20to%20detect%20these%20misalignments%20in%20O2ARC%20trajectories%20and%0Aconduct%20a%20hierarchical%20and%20quantitative%20analysis%20of%20their%20impact.%20Furthermore%2C%0Awe%20introduce%20an%20intention%20estimation%20algorithm%20that%20predicts%20missing%20alignment%0Ainformation%20between%20user%20actions%20and%20inferred%20intentions%2C%20leveraging%20our%0Aformalized%20framework.%20Through%20trajectory%20alignment%2C%20we%20experimentally%0Ademonstrate%20that%20AI%20models%20trained%20on%20human%20task-solving%20trajectories%20improve%0Aperformance%20in%20mimicking%20human%20reasoning.%20Based%20on%20hierarchical%20analysis%20and%0Aexperiments%2C%20we%20highlight%20the%20importance%20of%20trajectory-intention%20alignment%20and%0Ademonstrate%20the%20potential%20of%20intention%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14191v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520and%2520Visualizing%2520Misalignments%2520in%2520Human%2520Task-Solving%250A%2520%2520Trajectories%26entry.906535625%3DSejin%2520Kim%2520and%2520Hosung%2520Lee%2520and%2520Sundong%2520Kim%26entry.1292438233%3D%2520%2520Understanding%2520misalignments%2520in%2520human%2520task-solving%2520trajectories%2520is%2520critical%250Afor%2520improving%2520AI%2520models%2520trained%2520to%2520mimic%2520human%2520reasoning.%2520This%2520study%250Acategorizes%2520such%2520misalignments%2520into%2520three%2520types%253A%2520%255Ctextbf%257B%25281%2529%2520Lack%2520of%2520functions%250Ato%2520express%2520intent%257D%252C%2520%255Ctextbf%257B%25282%2529%2520Inefficient%2520action%2520sequences%257D%252C%2520and%2520%255Ctextbf%257B%25283%2529%250AIncorrect%2520intentions%2520that%2520cannot%2520solve%2520the%2520task%257D.%2520To%2520address%2520these%2520issues%252C%2520we%250Afirst%2520formalize%2520and%2520define%2520these%2520three%2520types%2520of%2520misalignments.%2520We%2520then%2520propose%250Aa%2520heuristic%2520algorithm%2520to%2520detect%2520these%2520misalignments%2520in%2520O2ARC%2520trajectories%2520and%250Aconduct%2520a%2520hierarchical%2520and%2520quantitative%2520analysis%2520of%2520their%2520impact.%2520Furthermore%252C%250Awe%2520introduce%2520an%2520intention%2520estimation%2520algorithm%2520that%2520predicts%2520missing%2520alignment%250Ainformation%2520between%2520user%2520actions%2520and%2520inferred%2520intentions%252C%2520leveraging%2520our%250Aformalized%2520framework.%2520Through%2520trajectory%2520alignment%252C%2520we%2520experimentally%250Ademonstrate%2520that%2520AI%2520models%2520trained%2520on%2520human%2520task-solving%2520trajectories%2520improve%250Aperformance%2520in%2520mimicking%2520human%2520reasoning.%2520Based%2520on%2520hierarchical%2520analysis%2520and%250Aexperiments%252C%2520we%2520highlight%2520the%2520importance%2520of%2520trajectory-intention%2520alignment%2520and%250Ademonstrate%2520the%2520potential%2520of%2520intention%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14191v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20and%20Visualizing%20Misalignments%20in%20Human%20Task-Solving%0A%20%20Trajectories&entry.906535625=Sejin%20Kim%20and%20Hosung%20Lee%20and%20Sundong%20Kim&entry.1292438233=%20%20Understanding%20misalignments%20in%20human%20task-solving%20trajectories%20is%20critical%0Afor%20improving%20AI%20models%20trained%20to%20mimic%20human%20reasoning.%20This%20study%0Acategorizes%20such%20misalignments%20into%20three%20types%3A%20%5Ctextbf%7B%281%29%20Lack%20of%20functions%0Ato%20express%20intent%7D%2C%20%5Ctextbf%7B%282%29%20Inefficient%20action%20sequences%7D%2C%20and%20%5Ctextbf%7B%283%29%0AIncorrect%20intentions%20that%20cannot%20solve%20the%20task%7D.%20To%20address%20these%20issues%2C%20we%0Afirst%20formalize%20and%20define%20these%20three%20types%20of%20misalignments.%20We%20then%20propose%0Aa%20heuristic%20algorithm%20to%20detect%20these%20misalignments%20in%20O2ARC%20trajectories%20and%0Aconduct%20a%20hierarchical%20and%20quantitative%20analysis%20of%20their%20impact.%20Furthermore%2C%0Awe%20introduce%20an%20intention%20estimation%20algorithm%20that%20predicts%20missing%20alignment%0Ainformation%20between%20user%20actions%20and%20inferred%20intentions%2C%20leveraging%20our%0Aformalized%20framework.%20Through%20trajectory%20alignment%2C%20we%20experimentally%0Ademonstrate%20that%20AI%20models%20trained%20on%20human%20task-solving%20trajectories%20improve%0Aperformance%20in%20mimicking%20human%20reasoning.%20Based%20on%20hierarchical%20analysis%20and%0Aexperiments%2C%20we%20highlight%20the%20importance%20of%20trajectory-intention%20alignment%20and%0Ademonstrate%20the%20potential%20of%20intention%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14191v3&entry.124074799=Read"},
{"title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity", "author": "Huy Q. Le and Latif U. Khan and Choong Seon Hong", "abstract": "  Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.\n", "link": "http://arxiv.org/abs/2505.10128v1", "date": "2025-05-15", "relevancy": 2.0983, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5304}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5224}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Federated%20Learning%20on%20Edge%20Devices%20with%20Domain%20Heterogeneity&body=Title%3A%20Robust%20Federated%20Learning%20on%20Edge%20Devices%20with%20Domain%20Heterogeneity%0AAuthor%3A%20Huy%20Q.%20Le%20and%20Latif%20U.%20Khan%20and%20Choong%20Seon%20Hong%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20allows%20collaborative%20training%20while%20ensuring%20data%0Aprivacy%20across%20distributed%20edge%20devices%2C%20making%20it%20a%20popular%20solution%20for%0Aprivacy-sensitive%20applications.%20However%2C%20FL%20faces%20significant%20challenges%20due%20to%0Astatistical%20heterogeneity%2C%20particularly%20domain%20heterogeneity%2C%20which%20impedes%20the%0Aglobal%20mode%27s%20convergence.%20In%20this%20study%2C%20we%20introduce%20a%20new%20framework%20to%0Aaddress%20this%20challenge%20by%20improving%20the%20generalization%20ability%20of%20the%20FL%20global%0Amodel%20under%20domain%20heterogeneity%2C%20using%20prototype%20augmentation.%20Specifically%2C%0Awe%20introduce%20FedAPC%20%28Federated%20Augmented%20Prototype%20Contrastive%20Learning%29%2C%20a%0Aprototype-based%20FL%20framework%20designed%20to%20enhance%20feature%20diversity%20and%20model%0Arobustness.%20FedAPC%20leverages%20prototypes%20derived%20from%20the%20mean%20features%20of%0Aaugmented%20data%20to%20capture%20richer%20representations.%20By%20aligning%20local%20features%0Awith%20global%20prototypes%2C%20we%20enable%20the%20model%20to%20learn%20meaningful%20semantic%0Afeatures%20while%20reducing%20overfitting%20to%20any%20specific%20domain.%20Experimental%0Aresults%20on%20the%20Office-10%20and%20Digits%20datasets%20illustrate%20that%20our%20framework%0Aoutperforms%20SOTA%20baselines%2C%20demonstrating%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Federated%2520Learning%2520on%2520Edge%2520Devices%2520with%2520Domain%2520Heterogeneity%26entry.906535625%3DHuy%2520Q.%2520Le%2520and%2520Latif%2520U.%2520Khan%2520and%2520Choong%2520Seon%2520Hong%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520allows%2520collaborative%2520training%2520while%2520ensuring%2520data%250Aprivacy%2520across%2520distributed%2520edge%2520devices%252C%2520making%2520it%2520a%2520popular%2520solution%2520for%250Aprivacy-sensitive%2520applications.%2520However%252C%2520FL%2520faces%2520significant%2520challenges%2520due%2520to%250Astatistical%2520heterogeneity%252C%2520particularly%2520domain%2520heterogeneity%252C%2520which%2520impedes%2520the%250Aglobal%2520mode%2527s%2520convergence.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520new%2520framework%2520to%250Aaddress%2520this%2520challenge%2520by%2520improving%2520the%2520generalization%2520ability%2520of%2520the%2520FL%2520global%250Amodel%2520under%2520domain%2520heterogeneity%252C%2520using%2520prototype%2520augmentation.%2520Specifically%252C%250Awe%2520introduce%2520FedAPC%2520%2528Federated%2520Augmented%2520Prototype%2520Contrastive%2520Learning%2529%252C%2520a%250Aprototype-based%2520FL%2520framework%2520designed%2520to%2520enhance%2520feature%2520diversity%2520and%2520model%250Arobustness.%2520FedAPC%2520leverages%2520prototypes%2520derived%2520from%2520the%2520mean%2520features%2520of%250Aaugmented%2520data%2520to%2520capture%2520richer%2520representations.%2520By%2520aligning%2520local%2520features%250Awith%2520global%2520prototypes%252C%2520we%2520enable%2520the%2520model%2520to%2520learn%2520meaningful%2520semantic%250Afeatures%2520while%2520reducing%2520overfitting%2520to%2520any%2520specific%2520domain.%2520Experimental%250Aresults%2520on%2520the%2520Office-10%2520and%2520Digits%2520datasets%2520illustrate%2520that%2520our%2520framework%250Aoutperforms%2520SOTA%2520baselines%252C%2520demonstrating%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Learning%20on%20Edge%20Devices%20with%20Domain%20Heterogeneity&entry.906535625=Huy%20Q.%20Le%20and%20Latif%20U.%20Khan%20and%20Choong%20Seon%20Hong&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20allows%20collaborative%20training%20while%20ensuring%20data%0Aprivacy%20across%20distributed%20edge%20devices%2C%20making%20it%20a%20popular%20solution%20for%0Aprivacy-sensitive%20applications.%20However%2C%20FL%20faces%20significant%20challenges%20due%20to%0Astatistical%20heterogeneity%2C%20particularly%20domain%20heterogeneity%2C%20which%20impedes%20the%0Aglobal%20mode%27s%20convergence.%20In%20this%20study%2C%20we%20introduce%20a%20new%20framework%20to%0Aaddress%20this%20challenge%20by%20improving%20the%20generalization%20ability%20of%20the%20FL%20global%0Amodel%20under%20domain%20heterogeneity%2C%20using%20prototype%20augmentation.%20Specifically%2C%0Awe%20introduce%20FedAPC%20%28Federated%20Augmented%20Prototype%20Contrastive%20Learning%29%2C%20a%0Aprototype-based%20FL%20framework%20designed%20to%20enhance%20feature%20diversity%20and%20model%0Arobustness.%20FedAPC%20leverages%20prototypes%20derived%20from%20the%20mean%20features%20of%0Aaugmented%20data%20to%20capture%20richer%20representations.%20By%20aligning%20local%20features%0Awith%20global%20prototypes%2C%20we%20enable%20the%20model%20to%20learn%20meaningful%20semantic%0Afeatures%20while%20reducing%20overfitting%20to%20any%20specific%20domain.%20Experimental%0Aresults%20on%20the%20Office-10%20and%20Digits%20datasets%20illustrate%20that%20our%20framework%0Aoutperforms%20SOTA%20baselines%2C%20demonstrating%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10128v1&entry.124074799=Read"},
{"title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think", "author": "Seongyun Lee and Seungone Kim and Minju Seo and Yongrae Jo and Dongyoung Go and Hyeonbin Hwang and Jinho Park and Xiang Yue and Sean Welleck and Graham Neubig and Moontae Lee and Minjoon Seo", "abstract": "  Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.\n", "link": "http://arxiv.org/abs/2505.10185v1", "date": "2025-05-15", "relevancy": 2.0828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20CoT%20Encyclopedia%3A%20Analyzing%2C%20Predicting%2C%20and%20Controlling%20how%20a%0A%20%20Reasoning%20Model%20will%20Think&body=Title%3A%20The%20CoT%20Encyclopedia%3A%20Analyzing%2C%20Predicting%2C%20and%20Controlling%20how%20a%0A%20%20Reasoning%20Model%20will%20Think%0AAuthor%3A%20Seongyun%20Lee%20and%20Seungone%20Kim%20and%20Minju%20Seo%20and%20Yongrae%20Jo%20and%20Dongyoung%20Go%20and%20Hyeonbin%20Hwang%20and%20Jinho%20Park%20and%20Xiang%20Yue%20and%20Sean%20Welleck%20and%20Graham%20Neubig%20and%20Moontae%20Lee%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20Long%20chain-of-thought%20%28CoT%29%20is%20an%20essential%20ingredient%20in%20effective%20usage%20of%0Amodern%20large%20language%20models%2C%20but%20our%20understanding%20of%20the%20reasoning%20strategies%0Aunderlying%20these%20capabilities%20remains%20limited.%20While%20some%20prior%20works%20have%0Aattempted%20to%20categorize%20CoTs%20using%20predefined%20strategy%20types%2C%20such%20approaches%0Aare%20constrained%20by%20human%20intuition%20and%20fail%20to%20capture%20the%20full%20diversity%20of%0Amodel%20behaviors.%20In%20this%20work%2C%20we%20introduce%20the%20CoT%20Encyclopedia%2C%20a%20bottom-up%0Aframework%20for%20analyzing%20and%20steering%20model%20reasoning.%20Our%20method%20automatically%0Aextracts%20diverse%20reasoning%20criteria%20from%20model-generated%20CoTs%2C%20embeds%20them%20into%0Aa%20semantic%20space%2C%20clusters%20them%20into%20representative%20categories%2C%20and%20derives%0Acontrastive%20rubrics%20to%20interpret%20reasoning%20behavior.%20Human%20evaluations%20show%0Athat%20this%20framework%20produces%20more%20interpretable%20and%20comprehensive%20analyses%20than%0Aexisting%20methods.%20Moreover%2C%20we%20demonstrate%20that%20this%20understanding%20enables%0Aperformance%20gains%3A%20we%20can%20predict%20which%20strategy%20a%20model%20is%20likely%20to%20use%20and%0Aguide%20it%20toward%20more%20effective%20alternatives.%20Finally%2C%20we%20provide%20practical%0Ainsights%2C%20such%20as%20that%20training%20data%20format%20%28e.g.%2C%20free-form%20vs.%0Amultiple-choice%29%20has%20a%20far%20greater%20impact%20on%20reasoning%20behavior%20than%20data%0Adomain%2C%20underscoring%20the%20importance%20of%20format-aware%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520CoT%2520Encyclopedia%253A%2520Analyzing%252C%2520Predicting%252C%2520and%2520Controlling%2520how%2520a%250A%2520%2520Reasoning%2520Model%2520will%2520Think%26entry.906535625%3DSeongyun%2520Lee%2520and%2520Seungone%2520Kim%2520and%2520Minju%2520Seo%2520and%2520Yongrae%2520Jo%2520and%2520Dongyoung%2520Go%2520and%2520Hyeonbin%2520Hwang%2520and%2520Jinho%2520Park%2520and%2520Xiang%2520Yue%2520and%2520Sean%2520Welleck%2520and%2520Graham%2520Neubig%2520and%2520Moontae%2520Lee%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520Long%2520chain-of-thought%2520%2528CoT%2529%2520is%2520an%2520essential%2520ingredient%2520in%2520effective%2520usage%2520of%250Amodern%2520large%2520language%2520models%252C%2520but%2520our%2520understanding%2520of%2520the%2520reasoning%2520strategies%250Aunderlying%2520these%2520capabilities%2520remains%2520limited.%2520While%2520some%2520prior%2520works%2520have%250Aattempted%2520to%2520categorize%2520CoTs%2520using%2520predefined%2520strategy%2520types%252C%2520such%2520approaches%250Aare%2520constrained%2520by%2520human%2520intuition%2520and%2520fail%2520to%2520capture%2520the%2520full%2520diversity%2520of%250Amodel%2520behaviors.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520CoT%2520Encyclopedia%252C%2520a%2520bottom-up%250Aframework%2520for%2520analyzing%2520and%2520steering%2520model%2520reasoning.%2520Our%2520method%2520automatically%250Aextracts%2520diverse%2520reasoning%2520criteria%2520from%2520model-generated%2520CoTs%252C%2520embeds%2520them%2520into%250Aa%2520semantic%2520space%252C%2520clusters%2520them%2520into%2520representative%2520categories%252C%2520and%2520derives%250Acontrastive%2520rubrics%2520to%2520interpret%2520reasoning%2520behavior.%2520Human%2520evaluations%2520show%250Athat%2520this%2520framework%2520produces%2520more%2520interpretable%2520and%2520comprehensive%2520analyses%2520than%250Aexisting%2520methods.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520this%2520understanding%2520enables%250Aperformance%2520gains%253A%2520we%2520can%2520predict%2520which%2520strategy%2520a%2520model%2520is%2520likely%2520to%2520use%2520and%250Aguide%2520it%2520toward%2520more%2520effective%2520alternatives.%2520Finally%252C%2520we%2520provide%2520practical%250Ainsights%252C%2520such%2520as%2520that%2520training%2520data%2520format%2520%2528e.g.%252C%2520free-form%2520vs.%250Amultiple-choice%2529%2520has%2520a%2520far%2520greater%2520impact%2520on%2520reasoning%2520behavior%2520than%2520data%250Adomain%252C%2520underscoring%2520the%2520importance%2520of%2520format-aware%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20CoT%20Encyclopedia%3A%20Analyzing%2C%20Predicting%2C%20and%20Controlling%20how%20a%0A%20%20Reasoning%20Model%20will%20Think&entry.906535625=Seongyun%20Lee%20and%20Seungone%20Kim%20and%20Minju%20Seo%20and%20Yongrae%20Jo%20and%20Dongyoung%20Go%20and%20Hyeonbin%20Hwang%20and%20Jinho%20Park%20and%20Xiang%20Yue%20and%20Sean%20Welleck%20and%20Graham%20Neubig%20and%20Moontae%20Lee%20and%20Minjoon%20Seo&entry.1292438233=%20%20Long%20chain-of-thought%20%28CoT%29%20is%20an%20essential%20ingredient%20in%20effective%20usage%20of%0Amodern%20large%20language%20models%2C%20but%20our%20understanding%20of%20the%20reasoning%20strategies%0Aunderlying%20these%20capabilities%20remains%20limited.%20While%20some%20prior%20works%20have%0Aattempted%20to%20categorize%20CoTs%20using%20predefined%20strategy%20types%2C%20such%20approaches%0Aare%20constrained%20by%20human%20intuition%20and%20fail%20to%20capture%20the%20full%20diversity%20of%0Amodel%20behaviors.%20In%20this%20work%2C%20we%20introduce%20the%20CoT%20Encyclopedia%2C%20a%20bottom-up%0Aframework%20for%20analyzing%20and%20steering%20model%20reasoning.%20Our%20method%20automatically%0Aextracts%20diverse%20reasoning%20criteria%20from%20model-generated%20CoTs%2C%20embeds%20them%20into%0Aa%20semantic%20space%2C%20clusters%20them%20into%20representative%20categories%2C%20and%20derives%0Acontrastive%20rubrics%20to%20interpret%20reasoning%20behavior.%20Human%20evaluations%20show%0Athat%20this%20framework%20produces%20more%20interpretable%20and%20comprehensive%20analyses%20than%0Aexisting%20methods.%20Moreover%2C%20we%20demonstrate%20that%20this%20understanding%20enables%0Aperformance%20gains%3A%20we%20can%20predict%20which%20strategy%20a%20model%20is%20likely%20to%20use%20and%0Aguide%20it%20toward%20more%20effective%20alternatives.%20Finally%2C%20we%20provide%20practical%0Ainsights%2C%20such%20as%20that%20training%20data%20format%20%28e.g.%2C%20free-form%20vs.%0Amultiple-choice%29%20has%20a%20far%20greater%20impact%20on%20reasoning%20behavior%20than%20data%0Adomain%2C%20underscoring%20the%20importance%20of%20format-aware%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10185v1&entry.124074799=Read"},
{"title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models", "author": "Saikat Barua and Mostafizur Rahman and Shehenaz Khaled and Md Jafor Sadek and Rafiul Islam and Shahnewaz Siddique", "abstract": "  The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.\n", "link": "http://arxiv.org/abs/2505.10167v1", "date": "2025-05-15", "relevancy": 2.06, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuXAI%3A%20Explainers%20for%20Hybrid%20Quantum%20Machine%20Learning%20Models&body=Title%3A%20QuXAI%3A%20Explainers%20for%20Hybrid%20Quantum%20Machine%20Learning%20Models%0AAuthor%3A%20Saikat%20Barua%20and%20Mostafizur%20Rahman%20and%20Shehenaz%20Khaled%20and%20Md%20Jafor%20Sadek%20and%20Rafiul%20Islam%20and%20Shahnewaz%20Siddique%0AAbstract%3A%20%20%20The%20emergence%20of%20hybrid%20quantum-classical%20machine%20learning%20%28HQML%29%20models%0Aopens%20new%20horizons%20of%20computational%20intelligence%20but%20their%20fundamental%0Acomplexity%20frequently%20leads%20to%20black%20box%20behavior%20that%20undermines%20transparency%0Aand%20reliability%20in%20their%20application.%20Although%20XAI%20for%20quantum%20systems%20still%20in%0Aits%20infancy%2C%20a%20major%20research%20gap%20is%20evident%20in%20robust%20global%20and%20local%0Aexplainability%20approaches%20that%20are%20designed%20for%20HQML%20architectures%20that%20employ%0Aquantized%20feature%20encoding%20followed%20by%20classical%20learning.%20The%20gap%20is%20the%20focus%0Aof%20this%20work%2C%20which%20introduces%20QuXAI%2C%20an%20framework%20based%20upon%20Q-MEDLEY%2C%20an%0Aexplainer%20for%20explaining%20feature%20importance%20in%20these%20hybrid%20systems.%20Our%20model%0Aentails%20the%20creation%20of%20HQML%20models%20incorporating%20quantum%20feature%20maps%2C%20the%20use%0Aof%20Q-MEDLEY%2C%20which%20combines%20feature%20based%20inferences%2C%20preserving%20the%20quantum%0Atransformation%20stage%20and%20visualizing%20the%20resulting%20attributions.%20Our%20result%0Ashows%20that%20Q-MEDLEY%20delineates%20influential%20classical%20aspects%20in%20HQML%20models%2C%20as%0Awell%20as%20separates%20their%20noise%2C%20and%20competes%20well%20against%20established%20XAI%0Atechniques%20in%20classical%20validation%20settings.%20Ablation%20studies%20more%0Asignificantly%20expose%20the%20virtues%20of%20the%20composite%20structure%20used%20in%20Q-MEDLEY.%0AThe%20implications%20of%20this%20work%20are%20critically%20important%2C%20as%20it%20provides%20a%20route%0Ato%20improve%20the%20interpretability%20and%20reliability%20of%20HQML%20models%2C%20thus%20promoting%0Agreater%20confidence%20and%20being%20able%20to%20engage%20in%20safer%20and%20more%20responsible%20use%0Aof%20quantum-enhanced%20AI%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuXAI%253A%2520Explainers%2520for%2520Hybrid%2520Quantum%2520Machine%2520Learning%2520Models%26entry.906535625%3DSaikat%2520Barua%2520and%2520Mostafizur%2520Rahman%2520and%2520Shehenaz%2520Khaled%2520and%2520Md%2520Jafor%2520Sadek%2520and%2520Rafiul%2520Islam%2520and%2520Shahnewaz%2520Siddique%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520hybrid%2520quantum-classical%2520machine%2520learning%2520%2528HQML%2529%2520models%250Aopens%2520new%2520horizons%2520of%2520computational%2520intelligence%2520but%2520their%2520fundamental%250Acomplexity%2520frequently%2520leads%2520to%2520black%2520box%2520behavior%2520that%2520undermines%2520transparency%250Aand%2520reliability%2520in%2520their%2520application.%2520Although%2520XAI%2520for%2520quantum%2520systems%2520still%2520in%250Aits%2520infancy%252C%2520a%2520major%2520research%2520gap%2520is%2520evident%2520in%2520robust%2520global%2520and%2520local%250Aexplainability%2520approaches%2520that%2520are%2520designed%2520for%2520HQML%2520architectures%2520that%2520employ%250Aquantized%2520feature%2520encoding%2520followed%2520by%2520classical%2520learning.%2520The%2520gap%2520is%2520the%2520focus%250Aof%2520this%2520work%252C%2520which%2520introduces%2520QuXAI%252C%2520an%2520framework%2520based%2520upon%2520Q-MEDLEY%252C%2520an%250Aexplainer%2520for%2520explaining%2520feature%2520importance%2520in%2520these%2520hybrid%2520systems.%2520Our%2520model%250Aentails%2520the%2520creation%2520of%2520HQML%2520models%2520incorporating%2520quantum%2520feature%2520maps%252C%2520the%2520use%250Aof%2520Q-MEDLEY%252C%2520which%2520combines%2520feature%2520based%2520inferences%252C%2520preserving%2520the%2520quantum%250Atransformation%2520stage%2520and%2520visualizing%2520the%2520resulting%2520attributions.%2520Our%2520result%250Ashows%2520that%2520Q-MEDLEY%2520delineates%2520influential%2520classical%2520aspects%2520in%2520HQML%2520models%252C%2520as%250Awell%2520as%2520separates%2520their%2520noise%252C%2520and%2520competes%2520well%2520against%2520established%2520XAI%250Atechniques%2520in%2520classical%2520validation%2520settings.%2520Ablation%2520studies%2520more%250Asignificantly%2520expose%2520the%2520virtues%2520of%2520the%2520composite%2520structure%2520used%2520in%2520Q-MEDLEY.%250AThe%2520implications%2520of%2520this%2520work%2520are%2520critically%2520important%252C%2520as%2520it%2520provides%2520a%2520route%250Ato%2520improve%2520the%2520interpretability%2520and%2520reliability%2520of%2520HQML%2520models%252C%2520thus%2520promoting%250Agreater%2520confidence%2520and%2520being%2520able%2520to%2520engage%2520in%2520safer%2520and%2520more%2520responsible%2520use%250Aof%2520quantum-enhanced%2520AI%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuXAI%3A%20Explainers%20for%20Hybrid%20Quantum%20Machine%20Learning%20Models&entry.906535625=Saikat%20Barua%20and%20Mostafizur%20Rahman%20and%20Shehenaz%20Khaled%20and%20Md%20Jafor%20Sadek%20and%20Rafiul%20Islam%20and%20Shahnewaz%20Siddique&entry.1292438233=%20%20The%20emergence%20of%20hybrid%20quantum-classical%20machine%20learning%20%28HQML%29%20models%0Aopens%20new%20horizons%20of%20computational%20intelligence%20but%20their%20fundamental%0Acomplexity%20frequently%20leads%20to%20black%20box%20behavior%20that%20undermines%20transparency%0Aand%20reliability%20in%20their%20application.%20Although%20XAI%20for%20quantum%20systems%20still%20in%0Aits%20infancy%2C%20a%20major%20research%20gap%20is%20evident%20in%20robust%20global%20and%20local%0Aexplainability%20approaches%20that%20are%20designed%20for%20HQML%20architectures%20that%20employ%0Aquantized%20feature%20encoding%20followed%20by%20classical%20learning.%20The%20gap%20is%20the%20focus%0Aof%20this%20work%2C%20which%20introduces%20QuXAI%2C%20an%20framework%20based%20upon%20Q-MEDLEY%2C%20an%0Aexplainer%20for%20explaining%20feature%20importance%20in%20these%20hybrid%20systems.%20Our%20model%0Aentails%20the%20creation%20of%20HQML%20models%20incorporating%20quantum%20feature%20maps%2C%20the%20use%0Aof%20Q-MEDLEY%2C%20which%20combines%20feature%20based%20inferences%2C%20preserving%20the%20quantum%0Atransformation%20stage%20and%20visualizing%20the%20resulting%20attributions.%20Our%20result%0Ashows%20that%20Q-MEDLEY%20delineates%20influential%20classical%20aspects%20in%20HQML%20models%2C%20as%0Awell%20as%20separates%20their%20noise%2C%20and%20competes%20well%20against%20established%20XAI%0Atechniques%20in%20classical%20validation%20settings.%20Ablation%20studies%20more%0Asignificantly%20expose%20the%20virtues%20of%20the%20composite%20structure%20used%20in%20Q-MEDLEY.%0AThe%20implications%20of%20this%20work%20are%20critically%20important%2C%20as%20it%20provides%20a%20route%0Ato%20improve%20the%20interpretability%20and%20reliability%20of%20HQML%20models%2C%20thus%20promoting%0Agreater%20confidence%20and%20being%20able%20to%20engage%20in%20safer%20and%20more%20responsible%20use%0Aof%20quantum-enhanced%20AI%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10167v1&entry.124074799=Read"},
{"title": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs", "author": "Yuxuan Gu and Wuyang Zhou and Giorgos Iacovides and Danilo Mandic", "abstract": "  The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.\n", "link": "http://arxiv.org/abs/2501.15674v2", "date": "2025-05-15", "relevancy": 2.0581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5033}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TensorLLM%3A%20Tensorising%20Multi-Head%20Attention%20for%20Enhanced%20Reasoning%20and%0A%20%20Compression%20in%20LLMs&body=Title%3A%20TensorLLM%3A%20Tensorising%20Multi-Head%20Attention%20for%20Enhanced%20Reasoning%20and%0A%20%20Compression%20in%20LLMs%0AAuthor%3A%20Yuxuan%20Gu%20and%20Wuyang%20Zhou%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic%0AAbstract%3A%20%20%20The%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20improved%20by%0Astructurally%20denoising%20their%20weights%2C%20yet%20existing%20techniques%20primarily%20focus%0Aon%20denoising%20the%20feed-forward%20network%20%28FFN%29%20of%20the%20transformer%20block%2C%20and%20can%0Anot%20efficiently%20utilise%20the%20Multi-head%20Attention%20%28MHA%29%20block%2C%20which%20is%20the%20core%0Aof%20transformer%20architectures.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aintuitive%20framework%20that%2C%20at%20its%20very%20core%2C%20performs%20MHA%20compression%20through%20a%0Amulti-head%20tensorisation%20process%20and%20the%20Tucker%20decomposition.%20This%20enables%0Aboth%20higher-dimensional%20structured%20denoising%20and%20compression%20of%20the%20MHA%0Aweights%2C%20by%20enforcing%20a%20shared%20higher-dimensional%20subspace%20across%20the%20weights%0Aof%20the%20multiple%20attention%20heads.%20We%20demonstrate%20that%20this%20approach%20consistently%0Aenhances%20the%20reasoning%20capabilities%20of%20LLMs%20across%20multiple%20benchmark%20datasets%2C%0Aand%20for%20both%20encoder-only%20and%20decoder-only%20architectures%2C%20while%20achieving%0Acompression%20rates%20of%20up%20to%20%24%5Csim%20250%24%20times%20in%20the%20MHA%20weights%2C%20all%20without%0Arequiring%20any%20additional%20data%2C%20training%2C%20or%20fine-tuning.%20Furthermore%2C%20we%20show%0Athat%20the%20proposed%20method%20can%20be%20seamlessly%20combined%20with%20existing%0AFFN-only-based%20denoising%20techniques%20to%20achieve%20further%20improvements%20in%20LLM%0Areasoning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensorLLM%253A%2520Tensorising%2520Multi-Head%2520Attention%2520for%2520Enhanced%2520Reasoning%2520and%250A%2520%2520Compression%2520in%2520LLMs%26entry.906535625%3DYuxuan%2520Gu%2520and%2520Wuyang%2520Zhou%2520and%2520Giorgos%2520Iacovides%2520and%2520Danilo%2520Mandic%26entry.1292438233%3D%2520%2520The%2520reasoning%2520abilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520be%2520improved%2520by%250Astructurally%2520denoising%2520their%2520weights%252C%2520yet%2520existing%2520techniques%2520primarily%2520focus%250Aon%2520denoising%2520the%2520feed-forward%2520network%2520%2528FFN%2529%2520of%2520the%2520transformer%2520block%252C%2520and%2520can%250Anot%2520efficiently%2520utilise%2520the%2520Multi-head%2520Attention%2520%2528MHA%2529%2520block%252C%2520which%2520is%2520the%2520core%250Aof%2520transformer%2520architectures.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250Aintuitive%2520framework%2520that%252C%2520at%2520its%2520very%2520core%252C%2520performs%2520MHA%2520compression%2520through%2520a%250Amulti-head%2520tensorisation%2520process%2520and%2520the%2520Tucker%2520decomposition.%2520This%2520enables%250Aboth%2520higher-dimensional%2520structured%2520denoising%2520and%2520compression%2520of%2520the%2520MHA%250Aweights%252C%2520by%2520enforcing%2520a%2520shared%2520higher-dimensional%2520subspace%2520across%2520the%2520weights%250Aof%2520the%2520multiple%2520attention%2520heads.%2520We%2520demonstrate%2520that%2520this%2520approach%2520consistently%250Aenhances%2520the%2520reasoning%2520capabilities%2520of%2520LLMs%2520across%2520multiple%2520benchmark%2520datasets%252C%250Aand%2520for%2520both%2520encoder-only%2520and%2520decoder-only%2520architectures%252C%2520while%2520achieving%250Acompression%2520rates%2520of%2520up%2520to%2520%2524%255Csim%2520250%2524%2520times%2520in%2520the%2520MHA%2520weights%252C%2520all%2520without%250Arequiring%2520any%2520additional%2520data%252C%2520training%252C%2520or%2520fine-tuning.%2520Furthermore%252C%2520we%2520show%250Athat%2520the%2520proposed%2520method%2520can%2520be%2520seamlessly%2520combined%2520with%2520existing%250AFFN-only-based%2520denoising%2520techniques%2520to%2520achieve%2520further%2520improvements%2520in%2520LLM%250Areasoning%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TensorLLM%3A%20Tensorising%20Multi-Head%20Attention%20for%20Enhanced%20Reasoning%20and%0A%20%20Compression%20in%20LLMs&entry.906535625=Yuxuan%20Gu%20and%20Wuyang%20Zhou%20and%20Giorgos%20Iacovides%20and%20Danilo%20Mandic&entry.1292438233=%20%20The%20reasoning%20abilities%20of%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20improved%20by%0Astructurally%20denoising%20their%20weights%2C%20yet%20existing%20techniques%20primarily%20focus%0Aon%20denoising%20the%20feed-forward%20network%20%28FFN%29%20of%20the%20transformer%20block%2C%20and%20can%0Anot%20efficiently%20utilise%20the%20Multi-head%20Attention%20%28MHA%29%20block%2C%20which%20is%20the%20core%0Aof%20transformer%20architectures.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aintuitive%20framework%20that%2C%20at%20its%20very%20core%2C%20performs%20MHA%20compression%20through%20a%0Amulti-head%20tensorisation%20process%20and%20the%20Tucker%20decomposition.%20This%20enables%0Aboth%20higher-dimensional%20structured%20denoising%20and%20compression%20of%20the%20MHA%0Aweights%2C%20by%20enforcing%20a%20shared%20higher-dimensional%20subspace%20across%20the%20weights%0Aof%20the%20multiple%20attention%20heads.%20We%20demonstrate%20that%20this%20approach%20consistently%0Aenhances%20the%20reasoning%20capabilities%20of%20LLMs%20across%20multiple%20benchmark%20datasets%2C%0Aand%20for%20both%20encoder-only%20and%20decoder-only%20architectures%2C%20while%20achieving%0Acompression%20rates%20of%20up%20to%20%24%5Csim%20250%24%20times%20in%20the%20MHA%20weights%2C%20all%20without%0Arequiring%20any%20additional%20data%2C%20training%2C%20or%20fine-tuning.%20Furthermore%2C%20we%20show%0Athat%20the%20proposed%20method%20can%20be%20seamlessly%20combined%20with%20existing%0AFFN-only-based%20denoising%20techniques%20to%20achieve%20further%20improvements%20in%20LLM%0Areasoning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15674v2&entry.124074799=Read"},
{"title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face\n  Morphing Attacks", "author": "Iurii Medvedev and Nuno Goncalves", "abstract": "  Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.\n", "link": "http://arxiv.org/abs/2505.10497v1", "date": "2025-05-15", "relevancy": 2.05, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5039}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphGuard%3A%20Morph%20Specific%20Margin%20Loss%20for%20Enhancing%20Robustness%20to%20Face%0A%20%20Morphing%20Attacks&body=Title%3A%20MorphGuard%3A%20Morph%20Specific%20Margin%20Loss%20for%20Enhancing%20Robustness%20to%20Face%0A%20%20Morphing%20Attacks%0AAuthor%3A%20Iurii%20Medvedev%20and%20Nuno%20Goncalves%0AAbstract%3A%20%20%20Face%20recognition%20has%20evolved%20significantly%20with%20the%20advancement%20of%20deep%0Alearning%20techniques%2C%20enabling%20its%20widespread%20adoption%20in%20various%20applications%0Arequiring%20secure%20authentication.%20However%2C%20this%20progress%20has%20also%20increased%20its%0Aexposure%20to%20presentation%20attacks%2C%20including%20face%20morphing%2C%20which%20poses%20a%0Aserious%20security%20threat%20by%20allowing%20one%20identity%20to%20impersonate%20another.%0ATherefore%2C%20modern%20face%20recognition%20systems%20must%20be%20robust%20against%20such%20attacks.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20for%20training%20deep%20networks%20for%20face%0Arecognition%20with%20enhanced%20robustness%20to%20face%20morphing%20attacks.%20Our%20method%0Amodifies%20the%20classification%20task%20by%20introducing%20a%20dual-branch%20classification%0Astrategy%20that%20effectively%20handles%20the%20ambiguity%20in%20the%20labeling%20of%20face%20morphs.%0AThis%20adaptation%20allows%20the%20model%20to%20incorporate%20morph%20images%20into%20the%20training%0Aprocess%2C%20improving%20its%20ability%20to%20distinguish%20them%20from%20bona%20fide%20samples.%0A%20%20Our%20strategy%20has%20been%20validated%20on%20public%20benchmarks%2C%20demonstrating%20its%0Aeffectiveness%20in%20enhancing%20robustness%20against%20face%20morphing%20attacks.%0AFurthermore%2C%20our%20approach%20is%20universally%20applicable%20and%20can%20be%20integrated%20into%0Aexisting%20face%20recognition%20training%20pipelines%20to%20improve%20classification-based%0Arecognition%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphGuard%253A%2520Morph%2520Specific%2520Margin%2520Loss%2520for%2520Enhancing%2520Robustness%2520to%2520Face%250A%2520%2520Morphing%2520Attacks%26entry.906535625%3DIurii%2520Medvedev%2520and%2520Nuno%2520Goncalves%26entry.1292438233%3D%2520%2520Face%2520recognition%2520has%2520evolved%2520significantly%2520with%2520the%2520advancement%2520of%2520deep%250Alearning%2520techniques%252C%2520enabling%2520its%2520widespread%2520adoption%2520in%2520various%2520applications%250Arequiring%2520secure%2520authentication.%2520However%252C%2520this%2520progress%2520has%2520also%2520increased%2520its%250Aexposure%2520to%2520presentation%2520attacks%252C%2520including%2520face%2520morphing%252C%2520which%2520poses%2520a%250Aserious%2520security%2520threat%2520by%2520allowing%2520one%2520identity%2520to%2520impersonate%2520another.%250ATherefore%252C%2520modern%2520face%2520recognition%2520systems%2520must%2520be%2520robust%2520against%2520such%2520attacks.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520training%2520deep%2520networks%2520for%2520face%250Arecognition%2520with%2520enhanced%2520robustness%2520to%2520face%2520morphing%2520attacks.%2520Our%2520method%250Amodifies%2520the%2520classification%2520task%2520by%2520introducing%2520a%2520dual-branch%2520classification%250Astrategy%2520that%2520effectively%2520handles%2520the%2520ambiguity%2520in%2520the%2520labeling%2520of%2520face%2520morphs.%250AThis%2520adaptation%2520allows%2520the%2520model%2520to%2520incorporate%2520morph%2520images%2520into%2520the%2520training%250Aprocess%252C%2520improving%2520its%2520ability%2520to%2520distinguish%2520them%2520from%2520bona%2520fide%2520samples.%250A%2520%2520Our%2520strategy%2520has%2520been%2520validated%2520on%2520public%2520benchmarks%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520enhancing%2520robustness%2520against%2520face%2520morphing%2520attacks.%250AFurthermore%252C%2520our%2520approach%2520is%2520universally%2520applicable%2520and%2520can%2520be%2520integrated%2520into%250Aexisting%2520face%2520recognition%2520training%2520pipelines%2520to%2520improve%2520classification-based%250Arecognition%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphGuard%3A%20Morph%20Specific%20Margin%20Loss%20for%20Enhancing%20Robustness%20to%20Face%0A%20%20Morphing%20Attacks&entry.906535625=Iurii%20Medvedev%20and%20Nuno%20Goncalves&entry.1292438233=%20%20Face%20recognition%20has%20evolved%20significantly%20with%20the%20advancement%20of%20deep%0Alearning%20techniques%2C%20enabling%20its%20widespread%20adoption%20in%20various%20applications%0Arequiring%20secure%20authentication.%20However%2C%20this%20progress%20has%20also%20increased%20its%0Aexposure%20to%20presentation%20attacks%2C%20including%20face%20morphing%2C%20which%20poses%20a%0Aserious%20security%20threat%20by%20allowing%20one%20identity%20to%20impersonate%20another.%0ATherefore%2C%20modern%20face%20recognition%20systems%20must%20be%20robust%20against%20such%20attacks.%0A%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20for%20training%20deep%20networks%20for%20face%0Arecognition%20with%20enhanced%20robustness%20to%20face%20morphing%20attacks.%20Our%20method%0Amodifies%20the%20classification%20task%20by%20introducing%20a%20dual-branch%20classification%0Astrategy%20that%20effectively%20handles%20the%20ambiguity%20in%20the%20labeling%20of%20face%20morphs.%0AThis%20adaptation%20allows%20the%20model%20to%20incorporate%20morph%20images%20into%20the%20training%0Aprocess%2C%20improving%20its%20ability%20to%20distinguish%20them%20from%20bona%20fide%20samples.%0A%20%20Our%20strategy%20has%20been%20validated%20on%20public%20benchmarks%2C%20demonstrating%20its%0Aeffectiveness%20in%20enhancing%20robustness%20against%20face%20morphing%20attacks.%0AFurthermore%2C%20our%20approach%20is%20universally%20applicable%20and%20can%20be%20integrated%20into%0Aexisting%20face%20recognition%20training%20pipelines%20to%20improve%20classification-based%0Arecognition%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10497v1&entry.124074799=Read"},
{"title": "Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug\n  Design", "author": "Amira Alakhdar and Barnabas Poczos and Newell Washburn", "abstract": "  Developing bioactive molecules remains a central, time- and cost-heavy\nchallenge in drug discovery, particularly for novel targets lacking structural\nor functional data. Pharmacophore modeling presents an alternative for\ncapturing the key features required for molecular bioactivity against a\nbiological target. In this work, we present PharmaDiff, a\npharmacophore-conditioned diffusion model for 3D molecular generation.\nPharmaDiff employs a transformer-based architecture to integrate an atom-based\nrepresentation of the 3D pharmacophore into the generative process, enabling\nthe precise generation of 3D molecular graphs that align with predefined\npharmacophore hypotheses. Through comprehensive testing, PharmaDiff\ndemonstrates superior performance in matching 3D pharmacophore constraints\ncompared to ligand-based drug design methods. Additionally, it achieves higher\ndocking scores across a range of proteins in structure-based drug design,\nwithout the need for target protein structures. By integrating pharmacophore\nmodeling with 3D generative techniques, PharmaDiff offers a powerful and\nflexible framework for rational drug design.\n", "link": "http://arxiv.org/abs/2505.10545v1", "date": "2025-05-15", "relevancy": 2.0464, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5146}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5102}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pharmacophore-Conditioned%20Diffusion%20Model%20for%20Ligand-Based%20De%20Novo%20Drug%0A%20%20Design&body=Title%3A%20Pharmacophore-Conditioned%20Diffusion%20Model%20for%20Ligand-Based%20De%20Novo%20Drug%0A%20%20Design%0AAuthor%3A%20Amira%20Alakhdar%20and%20Barnabas%20Poczos%20and%20Newell%20Washburn%0AAbstract%3A%20%20%20Developing%20bioactive%20molecules%20remains%20a%20central%2C%20time-%20and%20cost-heavy%0Achallenge%20in%20drug%20discovery%2C%20particularly%20for%20novel%20targets%20lacking%20structural%0Aor%20functional%20data.%20Pharmacophore%20modeling%20presents%20an%20alternative%20for%0Acapturing%20the%20key%20features%20required%20for%20molecular%20bioactivity%20against%20a%0Abiological%20target.%20In%20this%20work%2C%20we%20present%20PharmaDiff%2C%20a%0Apharmacophore-conditioned%20diffusion%20model%20for%203D%20molecular%20generation.%0APharmaDiff%20employs%20a%20transformer-based%20architecture%20to%20integrate%20an%20atom-based%0Arepresentation%20of%20the%203D%20pharmacophore%20into%20the%20generative%20process%2C%20enabling%0Athe%20precise%20generation%20of%203D%20molecular%20graphs%20that%20align%20with%20predefined%0Apharmacophore%20hypotheses.%20Through%20comprehensive%20testing%2C%20PharmaDiff%0Ademonstrates%20superior%20performance%20in%20matching%203D%20pharmacophore%20constraints%0Acompared%20to%20ligand-based%20drug%20design%20methods.%20Additionally%2C%20it%20achieves%20higher%0Adocking%20scores%20across%20a%20range%20of%20proteins%20in%20structure-based%20drug%20design%2C%0Awithout%20the%20need%20for%20target%20protein%20structures.%20By%20integrating%20pharmacophore%0Amodeling%20with%203D%20generative%20techniques%2C%20PharmaDiff%20offers%20a%20powerful%20and%0Aflexible%20framework%20for%20rational%20drug%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPharmacophore-Conditioned%2520Diffusion%2520Model%2520for%2520Ligand-Based%2520De%2520Novo%2520Drug%250A%2520%2520Design%26entry.906535625%3DAmira%2520Alakhdar%2520and%2520Barnabas%2520Poczos%2520and%2520Newell%2520Washburn%26entry.1292438233%3D%2520%2520Developing%2520bioactive%2520molecules%2520remains%2520a%2520central%252C%2520time-%2520and%2520cost-heavy%250Achallenge%2520in%2520drug%2520discovery%252C%2520particularly%2520for%2520novel%2520targets%2520lacking%2520structural%250Aor%2520functional%2520data.%2520Pharmacophore%2520modeling%2520presents%2520an%2520alternative%2520for%250Acapturing%2520the%2520key%2520features%2520required%2520for%2520molecular%2520bioactivity%2520against%2520a%250Abiological%2520target.%2520In%2520this%2520work%252C%2520we%2520present%2520PharmaDiff%252C%2520a%250Apharmacophore-conditioned%2520diffusion%2520model%2520for%25203D%2520molecular%2520generation.%250APharmaDiff%2520employs%2520a%2520transformer-based%2520architecture%2520to%2520integrate%2520an%2520atom-based%250Arepresentation%2520of%2520the%25203D%2520pharmacophore%2520into%2520the%2520generative%2520process%252C%2520enabling%250Athe%2520precise%2520generation%2520of%25203D%2520molecular%2520graphs%2520that%2520align%2520with%2520predefined%250Apharmacophore%2520hypotheses.%2520Through%2520comprehensive%2520testing%252C%2520PharmaDiff%250Ademonstrates%2520superior%2520performance%2520in%2520matching%25203D%2520pharmacophore%2520constraints%250Acompared%2520to%2520ligand-based%2520drug%2520design%2520methods.%2520Additionally%252C%2520it%2520achieves%2520higher%250Adocking%2520scores%2520across%2520a%2520range%2520of%2520proteins%2520in%2520structure-based%2520drug%2520design%252C%250Awithout%2520the%2520need%2520for%2520target%2520protein%2520structures.%2520By%2520integrating%2520pharmacophore%250Amodeling%2520with%25203D%2520generative%2520techniques%252C%2520PharmaDiff%2520offers%2520a%2520powerful%2520and%250Aflexible%2520framework%2520for%2520rational%2520drug%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pharmacophore-Conditioned%20Diffusion%20Model%20for%20Ligand-Based%20De%20Novo%20Drug%0A%20%20Design&entry.906535625=Amira%20Alakhdar%20and%20Barnabas%20Poczos%20and%20Newell%20Washburn&entry.1292438233=%20%20Developing%20bioactive%20molecules%20remains%20a%20central%2C%20time-%20and%20cost-heavy%0Achallenge%20in%20drug%20discovery%2C%20particularly%20for%20novel%20targets%20lacking%20structural%0Aor%20functional%20data.%20Pharmacophore%20modeling%20presents%20an%20alternative%20for%0Acapturing%20the%20key%20features%20required%20for%20molecular%20bioactivity%20against%20a%0Abiological%20target.%20In%20this%20work%2C%20we%20present%20PharmaDiff%2C%20a%0Apharmacophore-conditioned%20diffusion%20model%20for%203D%20molecular%20generation.%0APharmaDiff%20employs%20a%20transformer-based%20architecture%20to%20integrate%20an%20atom-based%0Arepresentation%20of%20the%203D%20pharmacophore%20into%20the%20generative%20process%2C%20enabling%0Athe%20precise%20generation%20of%203D%20molecular%20graphs%20that%20align%20with%20predefined%0Apharmacophore%20hypotheses.%20Through%20comprehensive%20testing%2C%20PharmaDiff%0Ademonstrates%20superior%20performance%20in%20matching%203D%20pharmacophore%20constraints%0Acompared%20to%20ligand-based%20drug%20design%20methods.%20Additionally%2C%20it%20achieves%20higher%0Adocking%20scores%20across%20a%20range%20of%20proteins%20in%20structure-based%20drug%20design%2C%0Awithout%20the%20need%20for%20target%20protein%20structures.%20By%20integrating%20pharmacophore%0Amodeling%20with%203D%20generative%20techniques%2C%20PharmaDiff%20offers%20a%20powerful%20and%0Aflexible%20framework%20for%20rational%20drug%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10545v1&entry.124074799=Read"},
{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents", "author": "Petr Anokhin and Nikita Semenov and Artyom Sorokin and Dmitry Evseev and Andrey Kravchenko and Mikhail Burtsev and Evgeny Burnaev", "abstract": "  Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.\n", "link": "http://arxiv.org/abs/2407.04363v3", "date": "2025-05-15", "relevancy": 2.0446, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5801}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AriGraph%3A%20Learning%20Knowledge%20Graph%20World%20Models%20with%20Episodic%20Memory%20for%0A%20%20LLM%20Agents&body=Title%3A%20AriGraph%3A%20Learning%20Knowledge%20Graph%20World%20Models%20with%20Episodic%20Memory%20for%0A%20%20LLM%20Agents%0AAuthor%3A%20Petr%20Anokhin%20and%20Nikita%20Semenov%20and%20Artyom%20Sorokin%20and%20Dmitry%20Evseev%20and%20Andrey%20Kravchenko%20and%20Mikhail%20Burtsev%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20Advancements%20in%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20created%0Aa%20promising%20foundation%20for%20developing%20autonomous%20agents.%20With%20the%20right%20tools%2C%0Athese%20agents%20could%20learn%20to%20solve%20tasks%20in%20new%20environments%20by%20accumulating%20and%0Aupdating%20their%20knowledge.%20Current%20LLM-based%20agents%20process%20past%20experiences%0Ausing%20a%20full%20history%20of%20observations%2C%20summarization%2C%20retrieval%20augmentation.%0AHowever%2C%20these%20unstructured%20memory%20representations%20do%20not%20facilitate%20the%0Areasoning%20and%20planning%20essential%20for%20complex%20decision-making.%20In%20our%20study%2C%20we%0Aintroduce%20AriGraph%2C%20a%20novel%20method%20wherein%20the%20agent%20constructs%20and%20updates%20a%0Amemory%20graph%20that%20integrates%20semantic%20and%20episodic%20memories%20while%20exploring%20the%0Aenvironment.%20We%20demonstrate%20that%20our%20Ariadne%20LLM%20agent%2C%20consisting%20of%20the%0Aproposed%20memory%20architecture%20augmented%20with%20planning%20and%20decision-making%2C%0Aeffectively%20handles%20complex%20tasks%20within%20interactive%20text%20game%20environments%0Adifficult%20even%20for%20human%20players.%20Results%20show%20that%20our%20approach%20markedly%0Aoutperforms%20other%20established%20memory%20methods%20and%20strong%20RL%20baselines%20in%20a%20range%0Aof%20problems%20of%20varying%20complexity.%20Additionally%2C%20AriGraph%20demonstrates%0Acompetitive%20performance%20compared%20to%20dedicated%20knowledge%20graph-based%20methods%20in%0Astatic%20multi-hop%20question-answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04363v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAriGraph%253A%2520Learning%2520Knowledge%2520Graph%2520World%2520Models%2520with%2520Episodic%2520Memory%2520for%250A%2520%2520LLM%2520Agents%26entry.906535625%3DPetr%2520Anokhin%2520and%2520Nikita%2520Semenov%2520and%2520Artyom%2520Sorokin%2520and%2520Dmitry%2520Evseev%2520and%2520Andrey%2520Kravchenko%2520and%2520Mikhail%2520Burtsev%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520Advancements%2520in%2520the%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520created%250Aa%2520promising%2520foundation%2520for%2520developing%2520autonomous%2520agents.%2520With%2520the%2520right%2520tools%252C%250Athese%2520agents%2520could%2520learn%2520to%2520solve%2520tasks%2520in%2520new%2520environments%2520by%2520accumulating%2520and%250Aupdating%2520their%2520knowledge.%2520Current%2520LLM-based%2520agents%2520process%2520past%2520experiences%250Ausing%2520a%2520full%2520history%2520of%2520observations%252C%2520summarization%252C%2520retrieval%2520augmentation.%250AHowever%252C%2520these%2520unstructured%2520memory%2520representations%2520do%2520not%2520facilitate%2520the%250Areasoning%2520and%2520planning%2520essential%2520for%2520complex%2520decision-making.%2520In%2520our%2520study%252C%2520we%250Aintroduce%2520AriGraph%252C%2520a%2520novel%2520method%2520wherein%2520the%2520agent%2520constructs%2520and%2520updates%2520a%250Amemory%2520graph%2520that%2520integrates%2520semantic%2520and%2520episodic%2520memories%2520while%2520exploring%2520the%250Aenvironment.%2520We%2520demonstrate%2520that%2520our%2520Ariadne%2520LLM%2520agent%252C%2520consisting%2520of%2520the%250Aproposed%2520memory%2520architecture%2520augmented%2520with%2520planning%2520and%2520decision-making%252C%250Aeffectively%2520handles%2520complex%2520tasks%2520within%2520interactive%2520text%2520game%2520environments%250Adifficult%2520even%2520for%2520human%2520players.%2520Results%2520show%2520that%2520our%2520approach%2520markedly%250Aoutperforms%2520other%2520established%2520memory%2520methods%2520and%2520strong%2520RL%2520baselines%2520in%2520a%2520range%250Aof%2520problems%2520of%2520varying%2520complexity.%2520Additionally%252C%2520AriGraph%2520demonstrates%250Acompetitive%2520performance%2520compared%2520to%2520dedicated%2520knowledge%2520graph-based%2520methods%2520in%250Astatic%2520multi-hop%2520question-answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04363v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AriGraph%3A%20Learning%20Knowledge%20Graph%20World%20Models%20with%20Episodic%20Memory%20for%0A%20%20LLM%20Agents&entry.906535625=Petr%20Anokhin%20and%20Nikita%20Semenov%20and%20Artyom%20Sorokin%20and%20Dmitry%20Evseev%20and%20Andrey%20Kravchenko%20and%20Mikhail%20Burtsev%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20Advancements%20in%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20created%0Aa%20promising%20foundation%20for%20developing%20autonomous%20agents.%20With%20the%20right%20tools%2C%0Athese%20agents%20could%20learn%20to%20solve%20tasks%20in%20new%20environments%20by%20accumulating%20and%0Aupdating%20their%20knowledge.%20Current%20LLM-based%20agents%20process%20past%20experiences%0Ausing%20a%20full%20history%20of%20observations%2C%20summarization%2C%20retrieval%20augmentation.%0AHowever%2C%20these%20unstructured%20memory%20representations%20do%20not%20facilitate%20the%0Areasoning%20and%20planning%20essential%20for%20complex%20decision-making.%20In%20our%20study%2C%20we%0Aintroduce%20AriGraph%2C%20a%20novel%20method%20wherein%20the%20agent%20constructs%20and%20updates%20a%0Amemory%20graph%20that%20integrates%20semantic%20and%20episodic%20memories%20while%20exploring%20the%0Aenvironment.%20We%20demonstrate%20that%20our%20Ariadne%20LLM%20agent%2C%20consisting%20of%20the%0Aproposed%20memory%20architecture%20augmented%20with%20planning%20and%20decision-making%2C%0Aeffectively%20handles%20complex%20tasks%20within%20interactive%20text%20game%20environments%0Adifficult%20even%20for%20human%20players.%20Results%20show%20that%20our%20approach%20markedly%0Aoutperforms%20other%20established%20memory%20methods%20and%20strong%20RL%20baselines%20in%20a%20range%0Aof%20problems%20of%20varying%20complexity.%20Additionally%2C%20AriGraph%20demonstrates%0Acompetitive%20performance%20compared%20to%20dedicated%20knowledge%20graph-based%20methods%20in%0Astatic%20multi-hop%20question-answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04363v3&entry.124074799=Read"},
{"title": "Loop closure grasping: Topological transformations enable strong,\n  gentle, and versatile grasps", "author": "Kentaro Barhydt and O. Godson Osele and Sreela Kodali and Cosima du Pasquier and Chase M. Hartquist and H. Harry Asada and Allison M. Okamura", "abstract": "  Grasping mechanisms must both create and subsequently hold grasps that permit\nsafe and effective object manipulation. Existing mechanisms address the\ndifferent functional requirements of grasp creation and grasp holding using a\nsingle morphology, but have yet to achieve the simultaneous strength,\ngentleness, and versatility needed for many applications. We present \"loop\nclosure grasping\", a class of robotic grasping that addresses these different\nfunctional requirements through topological transformations between open-loop\nand closed-loop morphologies. We formalize these morphologies for grasping,\nformulate the loop closure grasping method, and present principles and a design\narchitecture that we implement using soft growing inflated beams, winches, and\nclamps. The mechanisms' initial open-loop topology enables versatile grasp\ncreation via unencumbered tip movement, and closing the loop enables strong and\ngentle holding with effectively infinite bending compliance. Loop closure\ngrasping circumvents the tradeoffs of single-morphology designs, enabling\ngrasps involving historically challenging objects, environments, and\nconfigurations.\n", "link": "http://arxiv.org/abs/2505.10552v1", "date": "2025-05-15", "relevancy": 2.0413, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5636}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loop%20closure%20grasping%3A%20Topological%20transformations%20enable%20strong%2C%0A%20%20gentle%2C%20and%20versatile%20grasps&body=Title%3A%20Loop%20closure%20grasping%3A%20Topological%20transformations%20enable%20strong%2C%0A%20%20gentle%2C%20and%20versatile%20grasps%0AAuthor%3A%20Kentaro%20Barhydt%20and%20O.%20Godson%20Osele%20and%20Sreela%20Kodali%20and%20Cosima%20du%20Pasquier%20and%20Chase%20M.%20Hartquist%20and%20H.%20Harry%20Asada%20and%20Allison%20M.%20Okamura%0AAbstract%3A%20%20%20Grasping%20mechanisms%20must%20both%20create%20and%20subsequently%20hold%20grasps%20that%20permit%0Asafe%20and%20effective%20object%20manipulation.%20Existing%20mechanisms%20address%20the%0Adifferent%20functional%20requirements%20of%20grasp%20creation%20and%20grasp%20holding%20using%20a%0Asingle%20morphology%2C%20but%20have%20yet%20to%20achieve%20the%20simultaneous%20strength%2C%0Agentleness%2C%20and%20versatility%20needed%20for%20many%20applications.%20We%20present%20%22loop%0Aclosure%20grasping%22%2C%20a%20class%20of%20robotic%20grasping%20that%20addresses%20these%20different%0Afunctional%20requirements%20through%20topological%20transformations%20between%20open-loop%0Aand%20closed-loop%20morphologies.%20We%20formalize%20these%20morphologies%20for%20grasping%2C%0Aformulate%20the%20loop%20closure%20grasping%20method%2C%20and%20present%20principles%20and%20a%20design%0Aarchitecture%20that%20we%20implement%20using%20soft%20growing%20inflated%20beams%2C%20winches%2C%20and%0Aclamps.%20The%20mechanisms%27%20initial%20open-loop%20topology%20enables%20versatile%20grasp%0Acreation%20via%20unencumbered%20tip%20movement%2C%20and%20closing%20the%20loop%20enables%20strong%20and%0Agentle%20holding%20with%20effectively%20infinite%20bending%20compliance.%20Loop%20closure%0Agrasping%20circumvents%20the%20tradeoffs%20of%20single-morphology%20designs%2C%20enabling%0Agrasps%20involving%20historically%20challenging%20objects%2C%20environments%2C%20and%0Aconfigurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoop%2520closure%2520grasping%253A%2520Topological%2520transformations%2520enable%2520strong%252C%250A%2520%2520gentle%252C%2520and%2520versatile%2520grasps%26entry.906535625%3DKentaro%2520Barhydt%2520and%2520O.%2520Godson%2520Osele%2520and%2520Sreela%2520Kodali%2520and%2520Cosima%2520du%2520Pasquier%2520and%2520Chase%2520M.%2520Hartquist%2520and%2520H.%2520Harry%2520Asada%2520and%2520Allison%2520M.%2520Okamura%26entry.1292438233%3D%2520%2520Grasping%2520mechanisms%2520must%2520both%2520create%2520and%2520subsequently%2520hold%2520grasps%2520that%2520permit%250Asafe%2520and%2520effective%2520object%2520manipulation.%2520Existing%2520mechanisms%2520address%2520the%250Adifferent%2520functional%2520requirements%2520of%2520grasp%2520creation%2520and%2520grasp%2520holding%2520using%2520a%250Asingle%2520morphology%252C%2520but%2520have%2520yet%2520to%2520achieve%2520the%2520simultaneous%2520strength%252C%250Agentleness%252C%2520and%2520versatility%2520needed%2520for%2520many%2520applications.%2520We%2520present%2520%2522loop%250Aclosure%2520grasping%2522%252C%2520a%2520class%2520of%2520robotic%2520grasping%2520that%2520addresses%2520these%2520different%250Afunctional%2520requirements%2520through%2520topological%2520transformations%2520between%2520open-loop%250Aand%2520closed-loop%2520morphologies.%2520We%2520formalize%2520these%2520morphologies%2520for%2520grasping%252C%250Aformulate%2520the%2520loop%2520closure%2520grasping%2520method%252C%2520and%2520present%2520principles%2520and%2520a%2520design%250Aarchitecture%2520that%2520we%2520implement%2520using%2520soft%2520growing%2520inflated%2520beams%252C%2520winches%252C%2520and%250Aclamps.%2520The%2520mechanisms%2527%2520initial%2520open-loop%2520topology%2520enables%2520versatile%2520grasp%250Acreation%2520via%2520unencumbered%2520tip%2520movement%252C%2520and%2520closing%2520the%2520loop%2520enables%2520strong%2520and%250Agentle%2520holding%2520with%2520effectively%2520infinite%2520bending%2520compliance.%2520Loop%2520closure%250Agrasping%2520circumvents%2520the%2520tradeoffs%2520of%2520single-morphology%2520designs%252C%2520enabling%250Agrasps%2520involving%2520historically%2520challenging%2520objects%252C%2520environments%252C%2520and%250Aconfigurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loop%20closure%20grasping%3A%20Topological%20transformations%20enable%20strong%2C%0A%20%20gentle%2C%20and%20versatile%20grasps&entry.906535625=Kentaro%20Barhydt%20and%20O.%20Godson%20Osele%20and%20Sreela%20Kodali%20and%20Cosima%20du%20Pasquier%20and%20Chase%20M.%20Hartquist%20and%20H.%20Harry%20Asada%20and%20Allison%20M.%20Okamura&entry.1292438233=%20%20Grasping%20mechanisms%20must%20both%20create%20and%20subsequently%20hold%20grasps%20that%20permit%0Asafe%20and%20effective%20object%20manipulation.%20Existing%20mechanisms%20address%20the%0Adifferent%20functional%20requirements%20of%20grasp%20creation%20and%20grasp%20holding%20using%20a%0Asingle%20morphology%2C%20but%20have%20yet%20to%20achieve%20the%20simultaneous%20strength%2C%0Agentleness%2C%20and%20versatility%20needed%20for%20many%20applications.%20We%20present%20%22loop%0Aclosure%20grasping%22%2C%20a%20class%20of%20robotic%20grasping%20that%20addresses%20these%20different%0Afunctional%20requirements%20through%20topological%20transformations%20between%20open-loop%0Aand%20closed-loop%20morphologies.%20We%20formalize%20these%20morphologies%20for%20grasping%2C%0Aformulate%20the%20loop%20closure%20grasping%20method%2C%20and%20present%20principles%20and%20a%20design%0Aarchitecture%20that%20we%20implement%20using%20soft%20growing%20inflated%20beams%2C%20winches%2C%20and%0Aclamps.%20The%20mechanisms%27%20initial%20open-loop%20topology%20enables%20versatile%20grasp%0Acreation%20via%20unencumbered%20tip%20movement%2C%20and%20closing%20the%20loop%20enables%20strong%20and%0Agentle%20holding%20with%20effectively%20infinite%20bending%20compliance.%20Loop%20closure%0Agrasping%20circumvents%20the%20tradeoffs%20of%20single-morphology%20designs%2C%20enabling%0Agrasps%20involving%20historically%20challenging%20objects%2C%20environments%2C%20and%0Aconfigurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10552v1&entry.124074799=Read"},
{"title": "Towards Graph Foundation Models: Training on Knowledge Graphs Enables\n  Transferability to General Graphs", "author": "Kai Wang and Siqiang Luo and Caihua Shan and Yifei Shen", "abstract": "  Inspired by the success of large language models, there is a trend toward\ndeveloping graph foundation models to conduct diverse downstream tasks in\nvarious domains. However, current models often require extra fine-tuning to\napply their learned structural and semantic representations to new graphs,\nwhich limits their versatility. Recent breakthroughs in zero-shot inductive\nreasoning on knowledge graphs (KGs), offer us a new perspective on extending KG\nreasoning to general graph applications. In this paper, we introduce SCR, a\nunified graph reasoning framework designed to train on knowledge graphs and\neffectively generalize across a wide range of graph tasks and domains. We begin\nby designing the task-specific KG structures to establish a unified topology\nfor different task formats. Then we propose semantic-conditioned message\npassing, a novel mechanism addressing the inherent semantic isolation in\ntraditional KG reasoning, by jointly modeling structural and semantic\ninvariance patterns in graph representations. To demonstrate the effectiveness,\nwe evaluate the inductive reasoning capability of SCR using 38 diverse graph\ndatasets, covering node-level, link-level, and graph-level tasks across\nmultiple domains. Our results show substantial performance gains over existing\nfoundation models and supervised baselines, highlighting the efficacy and\nadaptability of our approach.\n", "link": "http://arxiv.org/abs/2410.12609v2", "date": "2025-05-15", "relevancy": 2.0404, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Graph%20Foundation%20Models%3A%20Training%20on%20Knowledge%20Graphs%20Enables%0A%20%20Transferability%20to%20General%20Graphs&body=Title%3A%20Towards%20Graph%20Foundation%20Models%3A%20Training%20on%20Knowledge%20Graphs%20Enables%0A%20%20Transferability%20to%20General%20Graphs%0AAuthor%3A%20Kai%20Wang%20and%20Siqiang%20Luo%20and%20Caihua%20Shan%20and%20Yifei%20Shen%0AAbstract%3A%20%20%20Inspired%20by%20the%20success%20of%20large%20language%20models%2C%20there%20is%20a%20trend%20toward%0Adeveloping%20graph%20foundation%20models%20to%20conduct%20diverse%20downstream%20tasks%20in%0Avarious%20domains.%20However%2C%20current%20models%20often%20require%20extra%20fine-tuning%20to%0Aapply%20their%20learned%20structural%20and%20semantic%20representations%20to%20new%20graphs%2C%0Awhich%20limits%20their%20versatility.%20Recent%20breakthroughs%20in%20zero-shot%20inductive%0Areasoning%20on%20knowledge%20graphs%20%28KGs%29%2C%20offer%20us%20a%20new%20perspective%20on%20extending%20KG%0Areasoning%20to%20general%20graph%20applications.%20In%20this%20paper%2C%20we%20introduce%20SCR%2C%20a%0Aunified%20graph%20reasoning%20framework%20designed%20to%20train%20on%20knowledge%20graphs%20and%0Aeffectively%20generalize%20across%20a%20wide%20range%20of%20graph%20tasks%20and%20domains.%20We%20begin%0Aby%20designing%20the%20task-specific%20KG%20structures%20to%20establish%20a%20unified%20topology%0Afor%20different%20task%20formats.%20Then%20we%20propose%20semantic-conditioned%20message%0Apassing%2C%20a%20novel%20mechanism%20addressing%20the%20inherent%20semantic%20isolation%20in%0Atraditional%20KG%20reasoning%2C%20by%20jointly%20modeling%20structural%20and%20semantic%0Ainvariance%20patterns%20in%20graph%20representations.%20To%20demonstrate%20the%20effectiveness%2C%0Awe%20evaluate%20the%20inductive%20reasoning%20capability%20of%20SCR%20using%2038%20diverse%20graph%0Adatasets%2C%20covering%20node-level%2C%20link-level%2C%20and%20graph-level%20tasks%20across%0Amultiple%20domains.%20Our%20results%20show%20substantial%20performance%20gains%20over%20existing%0Afoundation%20models%20and%20supervised%20baselines%2C%20highlighting%20the%20efficacy%20and%0Aadaptability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Graph%2520Foundation%2520Models%253A%2520Training%2520on%2520Knowledge%2520Graphs%2520Enables%250A%2520%2520Transferability%2520to%2520General%2520Graphs%26entry.906535625%3DKai%2520Wang%2520and%2520Siqiang%2520Luo%2520and%2520Caihua%2520Shan%2520and%2520Yifei%2520Shen%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520success%2520of%2520large%2520language%2520models%252C%2520there%2520is%2520a%2520trend%2520toward%250Adeveloping%2520graph%2520foundation%2520models%2520to%2520conduct%2520diverse%2520downstream%2520tasks%2520in%250Avarious%2520domains.%2520However%252C%2520current%2520models%2520often%2520require%2520extra%2520fine-tuning%2520to%250Aapply%2520their%2520learned%2520structural%2520and%2520semantic%2520representations%2520to%2520new%2520graphs%252C%250Awhich%2520limits%2520their%2520versatility.%2520Recent%2520breakthroughs%2520in%2520zero-shot%2520inductive%250Areasoning%2520on%2520knowledge%2520graphs%2520%2528KGs%2529%252C%2520offer%2520us%2520a%2520new%2520perspective%2520on%2520extending%2520KG%250Areasoning%2520to%2520general%2520graph%2520applications.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SCR%252C%2520a%250Aunified%2520graph%2520reasoning%2520framework%2520designed%2520to%2520train%2520on%2520knowledge%2520graphs%2520and%250Aeffectively%2520generalize%2520across%2520a%2520wide%2520range%2520of%2520graph%2520tasks%2520and%2520domains.%2520We%2520begin%250Aby%2520designing%2520the%2520task-specific%2520KG%2520structures%2520to%2520establish%2520a%2520unified%2520topology%250Afor%2520different%2520task%2520formats.%2520Then%2520we%2520propose%2520semantic-conditioned%2520message%250Apassing%252C%2520a%2520novel%2520mechanism%2520addressing%2520the%2520inherent%2520semantic%2520isolation%2520in%250Atraditional%2520KG%2520reasoning%252C%2520by%2520jointly%2520modeling%2520structural%2520and%2520semantic%250Ainvariance%2520patterns%2520in%2520graph%2520representations.%2520To%2520demonstrate%2520the%2520effectiveness%252C%250Awe%2520evaluate%2520the%2520inductive%2520reasoning%2520capability%2520of%2520SCR%2520using%252038%2520diverse%2520graph%250Adatasets%252C%2520covering%2520node-level%252C%2520link-level%252C%2520and%2520graph-level%2520tasks%2520across%250Amultiple%2520domains.%2520Our%2520results%2520show%2520substantial%2520performance%2520gains%2520over%2520existing%250Afoundation%2520models%2520and%2520supervised%2520baselines%252C%2520highlighting%2520the%2520efficacy%2520and%250Aadaptability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Graph%20Foundation%20Models%3A%20Training%20on%20Knowledge%20Graphs%20Enables%0A%20%20Transferability%20to%20General%20Graphs&entry.906535625=Kai%20Wang%20and%20Siqiang%20Luo%20and%20Caihua%20Shan%20and%20Yifei%20Shen&entry.1292438233=%20%20Inspired%20by%20the%20success%20of%20large%20language%20models%2C%20there%20is%20a%20trend%20toward%0Adeveloping%20graph%20foundation%20models%20to%20conduct%20diverse%20downstream%20tasks%20in%0Avarious%20domains.%20However%2C%20current%20models%20often%20require%20extra%20fine-tuning%20to%0Aapply%20their%20learned%20structural%20and%20semantic%20representations%20to%20new%20graphs%2C%0Awhich%20limits%20their%20versatility.%20Recent%20breakthroughs%20in%20zero-shot%20inductive%0Areasoning%20on%20knowledge%20graphs%20%28KGs%29%2C%20offer%20us%20a%20new%20perspective%20on%20extending%20KG%0Areasoning%20to%20general%20graph%20applications.%20In%20this%20paper%2C%20we%20introduce%20SCR%2C%20a%0Aunified%20graph%20reasoning%20framework%20designed%20to%20train%20on%20knowledge%20graphs%20and%0Aeffectively%20generalize%20across%20a%20wide%20range%20of%20graph%20tasks%20and%20domains.%20We%20begin%0Aby%20designing%20the%20task-specific%20KG%20structures%20to%20establish%20a%20unified%20topology%0Afor%20different%20task%20formats.%20Then%20we%20propose%20semantic-conditioned%20message%0Apassing%2C%20a%20novel%20mechanism%20addressing%20the%20inherent%20semantic%20isolation%20in%0Atraditional%20KG%20reasoning%2C%20by%20jointly%20modeling%20structural%20and%20semantic%0Ainvariance%20patterns%20in%20graph%20representations.%20To%20demonstrate%20the%20effectiveness%2C%0Awe%20evaluate%20the%20inductive%20reasoning%20capability%20of%20SCR%20using%2038%20diverse%20graph%0Adatasets%2C%20covering%20node-level%2C%20link-level%2C%20and%20graph-level%20tasks%20across%0Amultiple%20domains.%20Our%20results%20show%20substantial%20performance%20gains%20over%20existing%0Afoundation%20models%20and%20supervised%20baselines%2C%20highlighting%20the%20efficacy%20and%0Aadaptability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12609v2&entry.124074799=Read"},
{"title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge", "author": "Ranjan Sapkota and Konstantinos I. Roumeliotis and Manoj Karkee", "abstract": "  This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications\n", "link": "http://arxiv.org/abs/2505.10468v1", "date": "2025-05-15", "relevancy": 2.0395, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.552}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5035}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Agents%20vs.%20Agentic%20AI%3A%20A%20Conceptual%20Taxonomy%2C%20Applications%20and%0A%20%20Challenge&body=Title%3A%20AI%20Agents%20vs.%20Agentic%20AI%3A%20A%20Conceptual%20Taxonomy%2C%20Applications%20and%0A%20%20Challenge%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Konstantinos%20I.%20Roumeliotis%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20study%20critically%20distinguishes%20between%20AI%20Agents%20and%20Agentic%20AI%2C%0Aoffering%20a%20structured%20conceptual%20taxonomy%2C%20application%20mapping%2C%20and%20challenge%0Aanalysis%20to%20clarify%20their%20divergent%20design%20philosophies%20and%20capabilities.%20We%0Abegin%20by%20outlining%20the%20search%20strategy%20and%20foundational%20definitions%2C%0Acharacterizing%20AI%20Agents%20as%20modular%20systems%20driven%20by%20Large%20Language%20Models%0A%28LLMs%29%20and%20Large%20Image%20Models%20%28LIMs%29%20for%20narrow%2C%20task-specific%20automation.%0AGenerative%20AI%20is%20positioned%20as%20a%20precursor%2C%20with%20AI%20Agents%20advancing%20through%0Atool%20integration%2C%20prompt%20engineering%2C%20and%20reasoning%20enhancements.%20In%20contrast%2C%0AAgentic%20AI%20systems%20represent%20a%20paradigmatic%20shift%20marked%20by%20multi-agent%0Acollaboration%2C%20dynamic%20task%20decomposition%2C%20persistent%20memory%2C%20and%20orchestrated%0Aautonomy.%20Through%20a%20sequential%20evaluation%20of%20architectural%20evolution%2C%0Aoperational%20mechanisms%2C%20interaction%20styles%2C%20and%20autonomy%20levels%2C%20we%20present%20a%0Acomparative%20analysis%20across%20both%20paradigms.%20Application%20domains%20such%20as%0Acustomer%20support%2C%20scheduling%2C%20and%20data%20summarization%20are%20contrasted%20with%0AAgentic%20AI%20deployments%20in%20research%20automation%2C%20robotic%20coordination%2C%20and%0Amedical%20decision%20support.%20We%20further%20examine%20unique%20challenges%20in%20each%20paradigm%0Aincluding%20hallucination%2C%20brittleness%2C%20emergent%20behavior%2C%20and%20coordination%0Afailure%20and%20propose%20targeted%20solutions%20such%20as%20ReAct%20loops%2C%20RAG%2C%20orchestration%0Alayers%2C%20and%20causal%20modeling.%20This%20work%20aims%20to%20provide%20a%20definitive%20roadmap%20for%0Adeveloping%20robust%2C%20scalable%2C%20and%20explainable%20AI%20agent%20and%20Agentic%20AI-driven%0Asystems.%20%3EAI%20Agents%2C%20Agent-driven%2C%20Vision-Language-Models%2C%20Agentic%20AI%20Decision%0ASupport%20System%2C%20Agentic-AI%20Applications%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Agents%2520vs.%2520Agentic%2520AI%253A%2520A%2520Conceptual%2520Taxonomy%252C%2520Applications%2520and%250A%2520%2520Challenge%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Konstantinos%2520I.%2520Roumeliotis%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520study%2520critically%2520distinguishes%2520between%2520AI%2520Agents%2520and%2520Agentic%2520AI%252C%250Aoffering%2520a%2520structured%2520conceptual%2520taxonomy%252C%2520application%2520mapping%252C%2520and%2520challenge%250Aanalysis%2520to%2520clarify%2520their%2520divergent%2520design%2520philosophies%2520and%2520capabilities.%2520We%250Abegin%2520by%2520outlining%2520the%2520search%2520strategy%2520and%2520foundational%2520definitions%252C%250Acharacterizing%2520AI%2520Agents%2520as%2520modular%2520systems%2520driven%2520by%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520Large%2520Image%2520Models%2520%2528LIMs%2529%2520for%2520narrow%252C%2520task-specific%2520automation.%250AGenerative%2520AI%2520is%2520positioned%2520as%2520a%2520precursor%252C%2520with%2520AI%2520Agents%2520advancing%2520through%250Atool%2520integration%252C%2520prompt%2520engineering%252C%2520and%2520reasoning%2520enhancements.%2520In%2520contrast%252C%250AAgentic%2520AI%2520systems%2520represent%2520a%2520paradigmatic%2520shift%2520marked%2520by%2520multi-agent%250Acollaboration%252C%2520dynamic%2520task%2520decomposition%252C%2520persistent%2520memory%252C%2520and%2520orchestrated%250Aautonomy.%2520Through%2520a%2520sequential%2520evaluation%2520of%2520architectural%2520evolution%252C%250Aoperational%2520mechanisms%252C%2520interaction%2520styles%252C%2520and%2520autonomy%2520levels%252C%2520we%2520present%2520a%250Acomparative%2520analysis%2520across%2520both%2520paradigms.%2520Application%2520domains%2520such%2520as%250Acustomer%2520support%252C%2520scheduling%252C%2520and%2520data%2520summarization%2520are%2520contrasted%2520with%250AAgentic%2520AI%2520deployments%2520in%2520research%2520automation%252C%2520robotic%2520coordination%252C%2520and%250Amedical%2520decision%2520support.%2520We%2520further%2520examine%2520unique%2520challenges%2520in%2520each%2520paradigm%250Aincluding%2520hallucination%252C%2520brittleness%252C%2520emergent%2520behavior%252C%2520and%2520coordination%250Afailure%2520and%2520propose%2520targeted%2520solutions%2520such%2520as%2520ReAct%2520loops%252C%2520RAG%252C%2520orchestration%250Alayers%252C%2520and%2520causal%2520modeling.%2520This%2520work%2520aims%2520to%2520provide%2520a%2520definitive%2520roadmap%2520for%250Adeveloping%2520robust%252C%2520scalable%252C%2520and%2520explainable%2520AI%2520agent%2520and%2520Agentic%2520AI-driven%250Asystems.%2520%253EAI%2520Agents%252C%2520Agent-driven%252C%2520Vision-Language-Models%252C%2520Agentic%2520AI%2520Decision%250ASupport%2520System%252C%2520Agentic-AI%2520Applications%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Agents%20vs.%20Agentic%20AI%3A%20A%20Conceptual%20Taxonomy%2C%20Applications%20and%0A%20%20Challenge&entry.906535625=Ranjan%20Sapkota%20and%20Konstantinos%20I.%20Roumeliotis%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20study%20critically%20distinguishes%20between%20AI%20Agents%20and%20Agentic%20AI%2C%0Aoffering%20a%20structured%20conceptual%20taxonomy%2C%20application%20mapping%2C%20and%20challenge%0Aanalysis%20to%20clarify%20their%20divergent%20design%20philosophies%20and%20capabilities.%20We%0Abegin%20by%20outlining%20the%20search%20strategy%20and%20foundational%20definitions%2C%0Acharacterizing%20AI%20Agents%20as%20modular%20systems%20driven%20by%20Large%20Language%20Models%0A%28LLMs%29%20and%20Large%20Image%20Models%20%28LIMs%29%20for%20narrow%2C%20task-specific%20automation.%0AGenerative%20AI%20is%20positioned%20as%20a%20precursor%2C%20with%20AI%20Agents%20advancing%20through%0Atool%20integration%2C%20prompt%20engineering%2C%20and%20reasoning%20enhancements.%20In%20contrast%2C%0AAgentic%20AI%20systems%20represent%20a%20paradigmatic%20shift%20marked%20by%20multi-agent%0Acollaboration%2C%20dynamic%20task%20decomposition%2C%20persistent%20memory%2C%20and%20orchestrated%0Aautonomy.%20Through%20a%20sequential%20evaluation%20of%20architectural%20evolution%2C%0Aoperational%20mechanisms%2C%20interaction%20styles%2C%20and%20autonomy%20levels%2C%20we%20present%20a%0Acomparative%20analysis%20across%20both%20paradigms.%20Application%20domains%20such%20as%0Acustomer%20support%2C%20scheduling%2C%20and%20data%20summarization%20are%20contrasted%20with%0AAgentic%20AI%20deployments%20in%20research%20automation%2C%20robotic%20coordination%2C%20and%0Amedical%20decision%20support.%20We%20further%20examine%20unique%20challenges%20in%20each%20paradigm%0Aincluding%20hallucination%2C%20brittleness%2C%20emergent%20behavior%2C%20and%20coordination%0Afailure%20and%20propose%20targeted%20solutions%20such%20as%20ReAct%20loops%2C%20RAG%2C%20orchestration%0Alayers%2C%20and%20causal%20modeling.%20This%20work%20aims%20to%20provide%20a%20definitive%20roadmap%20for%0Adeveloping%20robust%2C%20scalable%2C%20and%20explainable%20AI%20agent%20and%20Agentic%20AI-driven%0Asystems.%20%3EAI%20Agents%2C%20Agent-driven%2C%20Vision-Language-Models%2C%20Agentic%20AI%20Decision%0ASupport%20System%2C%20Agentic-AI%20Applications%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10468v1&entry.124074799=Read"},
{"title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression", "author": "Arianna Stropeni and Francesco Borsatti and Manuel Barusco and Davide Dalle Pezze and Marco Fabris and Gian Antonio Susto", "abstract": "  Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.\n", "link": "http://arxiv.org/abs/2505.07119v2", "date": "2025-05-15", "relevancy": 2.0374, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5156}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20IoT%20Deployment%20for%20Visual%20Anomaly%20Detection%20via%0A%20%20Efficient%20Compression&body=Title%3A%20Towards%20Scalable%20IoT%20Deployment%20for%20Visual%20Anomaly%20Detection%20via%0A%20%20Efficient%20Compression%0AAuthor%3A%20Arianna%20Stropeni%20and%20Francesco%20Borsatti%20and%20Manuel%20Barusco%20and%20Davide%20Dalle%20Pezze%20and%20Marco%20Fabris%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20Visual%20Anomaly%20Detection%20%28VAD%29%20is%20a%20key%20task%20in%20industrial%20settings%2C%20where%0Aminimizing%20operational%20costs%20is%20essential.%20Deploying%20deep%20learning%20models%0Awithin%20Internet%20of%20Things%20%28IoT%29%20environments%20introduces%20specific%20challenges%20due%0Ato%20limited%20computational%20power%20and%20bandwidth%20of%20edge%20devices.%20This%20study%0Ainvestigates%20how%20to%20perform%20VAD%20effectively%20under%20such%20constraints%20by%0Aleveraging%20compact%2C%20efficient%20processing%20strategies.%20We%20evaluate%20several%20data%0Acompression%20techniques%2C%20examining%20the%20tradeoff%20between%20system%20latency%20and%0Adetection%20accuracy.%20Experiments%20on%20the%20MVTec%20AD%20benchmark%20demonstrate%20that%0Asignificant%20compression%20can%20be%20achieved%20with%20minimal%20loss%20in%20anomaly%20detection%0Aperformance%20compared%20to%20uncompressed%20data.%20Current%20results%20show%20up%20to%2080%25%0Areduction%20in%20end-to-end%20inference%20time%2C%20including%20edge%20processing%2C%0Atransmission%2C%20and%20server%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520IoT%2520Deployment%2520for%2520Visual%2520Anomaly%2520Detection%2520via%250A%2520%2520Efficient%2520Compression%26entry.906535625%3DArianna%2520Stropeni%2520and%2520Francesco%2520Borsatti%2520and%2520Manuel%2520Barusco%2520and%2520Davide%2520Dalle%2520Pezze%2520and%2520Marco%2520Fabris%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520Visual%2520Anomaly%2520Detection%2520%2528VAD%2529%2520is%2520a%2520key%2520task%2520in%2520industrial%2520settings%252C%2520where%250Aminimizing%2520operational%2520costs%2520is%2520essential.%2520Deploying%2520deep%2520learning%2520models%250Awithin%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520environments%2520introduces%2520specific%2520challenges%2520due%250Ato%2520limited%2520computational%2520power%2520and%2520bandwidth%2520of%2520edge%2520devices.%2520This%2520study%250Ainvestigates%2520how%2520to%2520perform%2520VAD%2520effectively%2520under%2520such%2520constraints%2520by%250Aleveraging%2520compact%252C%2520efficient%2520processing%2520strategies.%2520We%2520evaluate%2520several%2520data%250Acompression%2520techniques%252C%2520examining%2520the%2520tradeoff%2520between%2520system%2520latency%2520and%250Adetection%2520accuracy.%2520Experiments%2520on%2520the%2520MVTec%2520AD%2520benchmark%2520demonstrate%2520that%250Asignificant%2520compression%2520can%2520be%2520achieved%2520with%2520minimal%2520loss%2520in%2520anomaly%2520detection%250Aperformance%2520compared%2520to%2520uncompressed%2520data.%2520Current%2520results%2520show%2520up%2520to%252080%2525%250Areduction%2520in%2520end-to-end%2520inference%2520time%252C%2520including%2520edge%2520processing%252C%250Atransmission%252C%2520and%2520server%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20IoT%20Deployment%20for%20Visual%20Anomaly%20Detection%20via%0A%20%20Efficient%20Compression&entry.906535625=Arianna%20Stropeni%20and%20Francesco%20Borsatti%20and%20Manuel%20Barusco%20and%20Davide%20Dalle%20Pezze%20and%20Marco%20Fabris%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20Visual%20Anomaly%20Detection%20%28VAD%29%20is%20a%20key%20task%20in%20industrial%20settings%2C%20where%0Aminimizing%20operational%20costs%20is%20essential.%20Deploying%20deep%20learning%20models%0Awithin%20Internet%20of%20Things%20%28IoT%29%20environments%20introduces%20specific%20challenges%20due%0Ato%20limited%20computational%20power%20and%20bandwidth%20of%20edge%20devices.%20This%20study%0Ainvestigates%20how%20to%20perform%20VAD%20effectively%20under%20such%20constraints%20by%0Aleveraging%20compact%2C%20efficient%20processing%20strategies.%20We%20evaluate%20several%20data%0Acompression%20techniques%2C%20examining%20the%20tradeoff%20between%20system%20latency%20and%0Adetection%20accuracy.%20Experiments%20on%20the%20MVTec%20AD%20benchmark%20demonstrate%20that%0Asignificant%20compression%20can%20be%20achieved%20with%20minimal%20loss%20in%20anomaly%20detection%0Aperformance%20compared%20to%20uncompressed%20data.%20Current%20results%20show%20up%20to%2080%25%0Areduction%20in%20end-to-end%20inference%20time%2C%20including%20edge%20processing%2C%0Atransmission%2C%20and%20server%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07119v2&entry.124074799=Read"},
{"title": "Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models", "author": "Averi Bates and Ryan Vavricka and Shane Carleton and Ruosi Shao and Chongle Pan", "abstract": "  The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows.\n", "link": "http://arxiv.org/abs/2503.12293v2", "date": "2025-05-15", "relevancy": 2.0351, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5156}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5044}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Modeling%20Language%20Code%20Generation%20from%20Diagram%20Images%20Using%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Unified%20Modeling%20Language%20Code%20Generation%20from%20Diagram%20Images%20Using%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Averi%20Bates%20and%20Ryan%20Vavricka%20and%20Shane%20Carleton%20and%20Ruosi%20Shao%20and%20Chongle%20Pan%0AAbstract%3A%20%20%20The%20Unified%20Modeling%20Language%20is%20a%20standardized%20visual%20language%20widely%20used%0Afor%20modeling%20and%20documenting%20the%20design%20of%20software%20systems.%20Although%20many%0Atools%20generate%20UML%20diagrams%20from%20UML%20code%2C%20generating%20executable%20UML%20code%20from%0Aimage-based%20UML%20diagrams%20remains%20challenging.%20This%20paper%20proposes%20a%20new%0Aapproach%20to%20generate%20UML%20code%20using%20a%20large%20multimodal%20language%20model%0Aautomatically.%20Synthetic%20UML%20activity%20and%20sequence%20diagram%20datasets%20were%0Acreated%20to%20train%20and%20test%20the%20model.%20We%20compared%20standard%20fine-tuning%20with%20LoRA%0Atechniques%20to%20optimize%20base%20models.%20The%20experiments%20measured%20code%20generation%0Aaccuracy%20across%20different%20model%20sizes%20and%20training%20strategies.%20These%20results%0Ademonstrated%20that%20domain-adapted%20MM-LLMs%20perform%20for%20UML%20code%20generation%0Aautomation%2C%20whereby%2C%20at%20the%20best%20model%2C%20it%20achieved%20BLEU%20and%20SSIM%20scores%20of%0A0.779%20and%200.942%20on%20sequence%20diagrams.%20This%20will%20enable%20the%20modernization%20of%0Alegacy%20systems%20and%20decrease%20the%20manual%20effort%20in%20software%20development%0Aworkflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Modeling%2520Language%2520Code%2520Generation%2520from%2520Diagram%2520Images%2520Using%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DAveri%2520Bates%2520and%2520Ryan%2520Vavricka%2520and%2520Shane%2520Carleton%2520and%2520Ruosi%2520Shao%2520and%2520Chongle%2520Pan%26entry.1292438233%3D%2520%2520The%2520Unified%2520Modeling%2520Language%2520is%2520a%2520standardized%2520visual%2520language%2520widely%2520used%250Afor%2520modeling%2520and%2520documenting%2520the%2520design%2520of%2520software%2520systems.%2520Although%2520many%250Atools%2520generate%2520UML%2520diagrams%2520from%2520UML%2520code%252C%2520generating%2520executable%2520UML%2520code%2520from%250Aimage-based%2520UML%2520diagrams%2520remains%2520challenging.%2520This%2520paper%2520proposes%2520a%2520new%250Aapproach%2520to%2520generate%2520UML%2520code%2520using%2520a%2520large%2520multimodal%2520language%2520model%250Aautomatically.%2520Synthetic%2520UML%2520activity%2520and%2520sequence%2520diagram%2520datasets%2520were%250Acreated%2520to%2520train%2520and%2520test%2520the%2520model.%2520We%2520compared%2520standard%2520fine-tuning%2520with%2520LoRA%250Atechniques%2520to%2520optimize%2520base%2520models.%2520The%2520experiments%2520measured%2520code%2520generation%250Aaccuracy%2520across%2520different%2520model%2520sizes%2520and%2520training%2520strategies.%2520These%2520results%250Ademonstrated%2520that%2520domain-adapted%2520MM-LLMs%2520perform%2520for%2520UML%2520code%2520generation%250Aautomation%252C%2520whereby%252C%2520at%2520the%2520best%2520model%252C%2520it%2520achieved%2520BLEU%2520and%2520SSIM%2520scores%2520of%250A0.779%2520and%25200.942%2520on%2520sequence%2520diagrams.%2520This%2520will%2520enable%2520the%2520modernization%2520of%250Alegacy%2520systems%2520and%2520decrease%2520the%2520manual%2520effort%2520in%2520software%2520development%250Aworkflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Modeling%20Language%20Code%20Generation%20from%20Diagram%20Images%20Using%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Averi%20Bates%20and%20Ryan%20Vavricka%20and%20Shane%20Carleton%20and%20Ruosi%20Shao%20and%20Chongle%20Pan&entry.1292438233=%20%20The%20Unified%20Modeling%20Language%20is%20a%20standardized%20visual%20language%20widely%20used%0Afor%20modeling%20and%20documenting%20the%20design%20of%20software%20systems.%20Although%20many%0Atools%20generate%20UML%20diagrams%20from%20UML%20code%2C%20generating%20executable%20UML%20code%20from%0Aimage-based%20UML%20diagrams%20remains%20challenging.%20This%20paper%20proposes%20a%20new%0Aapproach%20to%20generate%20UML%20code%20using%20a%20large%20multimodal%20language%20model%0Aautomatically.%20Synthetic%20UML%20activity%20and%20sequence%20diagram%20datasets%20were%0Acreated%20to%20train%20and%20test%20the%20model.%20We%20compared%20standard%20fine-tuning%20with%20LoRA%0Atechniques%20to%20optimize%20base%20models.%20The%20experiments%20measured%20code%20generation%0Aaccuracy%20across%20different%20model%20sizes%20and%20training%20strategies.%20These%20results%0Ademonstrated%20that%20domain-adapted%20MM-LLMs%20perform%20for%20UML%20code%20generation%0Aautomation%2C%20whereby%2C%20at%20the%20best%20model%2C%20it%20achieved%20BLEU%20and%20SSIM%20scores%20of%0A0.779%20and%200.942%20on%20sequence%20diagrams.%20This%20will%20enable%20the%20modernization%20of%0Alegacy%20systems%20and%20decrease%20the%20manual%20effort%20in%20software%20development%0Aworkflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12293v2&entry.124074799=Read"},
{"title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?", "author": "Pedro Orvalho and Marta Kwiatkowska", "abstract": "  Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.\n", "link": "http://arxiv.org/abs/2505.10443v1", "date": "2025-05-15", "relevancy": 2.034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F&body=Title%3A%20Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F%0AAuthor%3A%20Pedro%20Orvalho%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20Understanding%20the%20reasoning%20and%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Acritical%20for%20their%20reliable%20use%20in%20programming%20tasks.%20While%20recent%20studies%20have%0Aassessed%20LLMs%27%20ability%20to%20predict%20program%20outputs%2C%20most%20focus%20solely%20on%20the%0Aaccuracy%20of%20those%20predictions%2C%20without%20evaluating%20the%20reasoning%20behind%20them.%0AMoreover%2C%20it%20has%20been%20observed%20on%20mathematical%20reasoning%20tasks%20that%20LLMs%20can%0Aarrive%20at%20correct%20answers%20through%20flawed%20logic%2C%20raising%20concerns%20about%20similar%0Aissues%20in%20code%20understanding.%0A%20%20In%20this%20work%2C%20we%20evaluate%20whether%20state-of-the-art%20LLMs%20with%20up%20to%208B%0Aparameters%20can%20reason%20about%20Python%20programs%20or%20are%20simply%20guessing.%20We%20apply%0Afive%20semantics-preserving%20code%20mutations%3A%20renaming%20variables%2C%20mirroring%0Acomparison%20expressions%2C%20swapping%20if-else%20branches%2C%20converting%20for%20loops%20to%0Awhile%2C%20and%20loop%20unrolling.%20These%20mutations%20maintain%20program%20semantics%20while%0Aaltering%20its%20syntax.%20We%20evaluated%20six%20LLMs%20and%20performed%20a%20human%20expert%0Aanalysis%20using%20LiveCodeBench%20to%20assess%20whether%20the%20correct%20predictions%20are%0Abased%20on%20sound%20reasoning.%20We%20also%20evaluated%20prediction%20stability%20across%0Adifferent%20code%20mutations%20on%20LiveCodeBench%20and%20CruxEval.%20Our%20findings%20show%20that%0Asome%20LLMs%2C%20such%20as%20Llama3.2%2C%20produce%20correct%20predictions%20based%20on%20flawed%0Areasoning%20in%20up%20to%2061%25%20of%20cases.%20Furthermore%2C%20LLMs%20often%20change%20predictions%20in%0Aresponse%20to%20our%20code%20mutations%2C%20indicating%20limited%20robustness%20in%20their%20semantic%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Robust%2520in%2520Understanding%2520Code%2520Against%250A%2520%2520Semantics-Preserving%2520Mutations%253F%26entry.906535625%3DPedro%2520Orvalho%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520Understanding%2520the%2520reasoning%2520and%2520robustness%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%250Acritical%2520for%2520their%2520reliable%2520use%2520in%2520programming%2520tasks.%2520While%2520recent%2520studies%2520have%250Aassessed%2520LLMs%2527%2520ability%2520to%2520predict%2520program%2520outputs%252C%2520most%2520focus%2520solely%2520on%2520the%250Aaccuracy%2520of%2520those%2520predictions%252C%2520without%2520evaluating%2520the%2520reasoning%2520behind%2520them.%250AMoreover%252C%2520it%2520has%2520been%2520observed%2520on%2520mathematical%2520reasoning%2520tasks%2520that%2520LLMs%2520can%250Aarrive%2520at%2520correct%2520answers%2520through%2520flawed%2520logic%252C%2520raising%2520concerns%2520about%2520similar%250Aissues%2520in%2520code%2520understanding.%250A%2520%2520In%2520this%2520work%252C%2520we%2520evaluate%2520whether%2520state-of-the-art%2520LLMs%2520with%2520up%2520to%25208B%250Aparameters%2520can%2520reason%2520about%2520Python%2520programs%2520or%2520are%2520simply%2520guessing.%2520We%2520apply%250Afive%2520semantics-preserving%2520code%2520mutations%253A%2520renaming%2520variables%252C%2520mirroring%250Acomparison%2520expressions%252C%2520swapping%2520if-else%2520branches%252C%2520converting%2520for%2520loops%2520to%250Awhile%252C%2520and%2520loop%2520unrolling.%2520These%2520mutations%2520maintain%2520program%2520semantics%2520while%250Aaltering%2520its%2520syntax.%2520We%2520evaluated%2520six%2520LLMs%2520and%2520performed%2520a%2520human%2520expert%250Aanalysis%2520using%2520LiveCodeBench%2520to%2520assess%2520whether%2520the%2520correct%2520predictions%2520are%250Abased%2520on%2520sound%2520reasoning.%2520We%2520also%2520evaluated%2520prediction%2520stability%2520across%250Adifferent%2520code%2520mutations%2520on%2520LiveCodeBench%2520and%2520CruxEval.%2520Our%2520findings%2520show%2520that%250Asome%2520LLMs%252C%2520such%2520as%2520Llama3.2%252C%2520produce%2520correct%2520predictions%2520based%2520on%2520flawed%250Areasoning%2520in%2520up%2520to%252061%2525%2520of%2520cases.%2520Furthermore%252C%2520LLMs%2520often%2520change%2520predictions%2520in%250Aresponse%2520to%2520our%2520code%2520mutations%252C%2520indicating%2520limited%2520robustness%2520in%2520their%2520semantic%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Robust%20in%20Understanding%20Code%20Against%0A%20%20Semantics-Preserving%20Mutations%3F&entry.906535625=Pedro%20Orvalho%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20Understanding%20the%20reasoning%20and%20robustness%20of%20Large%20Language%20Models%20%28LLMs%29%20is%0Acritical%20for%20their%20reliable%20use%20in%20programming%20tasks.%20While%20recent%20studies%20have%0Aassessed%20LLMs%27%20ability%20to%20predict%20program%20outputs%2C%20most%20focus%20solely%20on%20the%0Aaccuracy%20of%20those%20predictions%2C%20without%20evaluating%20the%20reasoning%20behind%20them.%0AMoreover%2C%20it%20has%20been%20observed%20on%20mathematical%20reasoning%20tasks%20that%20LLMs%20can%0Aarrive%20at%20correct%20answers%20through%20flawed%20logic%2C%20raising%20concerns%20about%20similar%0Aissues%20in%20code%20understanding.%0A%20%20In%20this%20work%2C%20we%20evaluate%20whether%20state-of-the-art%20LLMs%20with%20up%20to%208B%0Aparameters%20can%20reason%20about%20Python%20programs%20or%20are%20simply%20guessing.%20We%20apply%0Afive%20semantics-preserving%20code%20mutations%3A%20renaming%20variables%2C%20mirroring%0Acomparison%20expressions%2C%20swapping%20if-else%20branches%2C%20converting%20for%20loops%20to%0Awhile%2C%20and%20loop%20unrolling.%20These%20mutations%20maintain%20program%20semantics%20while%0Aaltering%20its%20syntax.%20We%20evaluated%20six%20LLMs%20and%20performed%20a%20human%20expert%0Aanalysis%20using%20LiveCodeBench%20to%20assess%20whether%20the%20correct%20predictions%20are%0Abased%20on%20sound%20reasoning.%20We%20also%20evaluated%20prediction%20stability%20across%0Adifferent%20code%20mutations%20on%20LiveCodeBench%20and%20CruxEval.%20Our%20findings%20show%20that%0Asome%20LLMs%2C%20such%20as%20Llama3.2%2C%20produce%20correct%20predictions%20based%20on%20flawed%0Areasoning%20in%20up%20to%2061%25%20of%20cases.%20Furthermore%2C%20LLMs%20often%20change%20predictions%20in%0Aresponse%20to%20our%20code%20mutations%2C%20indicating%20limited%20robustness%20in%20their%20semantic%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10443v1&entry.124074799=Read"},
{"title": "Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient\n  Fine-Tuning of Language Models", "author": "Hyegang Son and Yonglak Son and Changhoon Kim and Young Geun Kim", "abstract": "  Transformer-based large-scale pre-trained models achieve great success.\nFine-tuning is the standard practice for leveraging these models in downstream\ntasks. Among the fine-tuning methods, adapter-tuning provides a\nparameter-efficient fine-tuning by introducing lightweight trainable modules\nwhile keeping most pre-trained parameters frozen. However, existing\nadapter-tuning methods still impose substantial resource usage. Through our\ninvestigation, we show that each adapter unequally contributes to both task\nperformance and resource usage. Motivated by this insight, we propose Selective\nAdapter FrEezing (SAFE), which gradually freezes less important adapters early\nto reduce unnecessary resource usage while maintaining performance. In our\nexperiments, SAFE reduces memory usage, computation amount, and training time\nby 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or\nbetter task performance compared to the baseline. We also demonstrate that SAFE\ninduces regularization effect, thereby smoothing the loss landscape, which\nenables the model to generalize better by avoiding sharp minima.\n", "link": "http://arxiv.org/abs/2412.03587v2", "date": "2025-05-15", "relevancy": 2.0335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5121}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Adapters%20Matter%3A%20Selective%20Adapter%20Freezing%20for%20Memory-Efficient%0A%20%20Fine-Tuning%20of%20Language%20Models&body=Title%3A%20Not%20All%20Adapters%20Matter%3A%20Selective%20Adapter%20Freezing%20for%20Memory-Efficient%0A%20%20Fine-Tuning%20of%20Language%20Models%0AAuthor%3A%20Hyegang%20Son%20and%20Yonglak%20Son%20and%20Changhoon%20Kim%20and%20Young%20Geun%20Kim%0AAbstract%3A%20%20%20Transformer-based%20large-scale%20pre-trained%20models%20achieve%20great%20success.%0AFine-tuning%20is%20the%20standard%20practice%20for%20leveraging%20these%20models%20in%20downstream%0Atasks.%20Among%20the%20fine-tuning%20methods%2C%20adapter-tuning%20provides%20a%0Aparameter-efficient%20fine-tuning%20by%20introducing%20lightweight%20trainable%20modules%0Awhile%20keeping%20most%20pre-trained%20parameters%20frozen.%20However%2C%20existing%0Aadapter-tuning%20methods%20still%20impose%20substantial%20resource%20usage.%20Through%20our%0Ainvestigation%2C%20we%20show%20that%20each%20adapter%20unequally%20contributes%20to%20both%20task%0Aperformance%20and%20resource%20usage.%20Motivated%20by%20this%20insight%2C%20we%20propose%20Selective%0AAdapter%20FrEezing%20%28SAFE%29%2C%20which%20gradually%20freezes%20less%20important%20adapters%20early%0Ato%20reduce%20unnecessary%20resource%20usage%20while%20maintaining%20performance.%20In%20our%0Aexperiments%2C%20SAFE%20reduces%20memory%20usage%2C%20computation%20amount%2C%20and%20training%20time%0Aby%2042.85%5C%25%2C%2034.59%5C%25%2C%20and%2011.82%5C%25%2C%20respectively%2C%20while%20achieving%20comparable%20or%0Abetter%20task%20performance%20compared%20to%20the%20baseline.%20We%20also%20demonstrate%20that%20SAFE%0Ainduces%20regularization%20effect%2C%20thereby%20smoothing%20the%20loss%20landscape%2C%20which%0Aenables%20the%20model%20to%20generalize%20better%20by%20avoiding%20sharp%20minima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Adapters%2520Matter%253A%2520Selective%2520Adapter%2520Freezing%2520for%2520Memory-Efficient%250A%2520%2520Fine-Tuning%2520of%2520Language%2520Models%26entry.906535625%3DHyegang%2520Son%2520and%2520Yonglak%2520Son%2520and%2520Changhoon%2520Kim%2520and%2520Young%2520Geun%2520Kim%26entry.1292438233%3D%2520%2520Transformer-based%2520large-scale%2520pre-trained%2520models%2520achieve%2520great%2520success.%250AFine-tuning%2520is%2520the%2520standard%2520practice%2520for%2520leveraging%2520these%2520models%2520in%2520downstream%250Atasks.%2520Among%2520the%2520fine-tuning%2520methods%252C%2520adapter-tuning%2520provides%2520a%250Aparameter-efficient%2520fine-tuning%2520by%2520introducing%2520lightweight%2520trainable%2520modules%250Awhile%2520keeping%2520most%2520pre-trained%2520parameters%2520frozen.%2520However%252C%2520existing%250Aadapter-tuning%2520methods%2520still%2520impose%2520substantial%2520resource%2520usage.%2520Through%2520our%250Ainvestigation%252C%2520we%2520show%2520that%2520each%2520adapter%2520unequally%2520contributes%2520to%2520both%2520task%250Aperformance%2520and%2520resource%2520usage.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520Selective%250AAdapter%2520FrEezing%2520%2528SAFE%2529%252C%2520which%2520gradually%2520freezes%2520less%2520important%2520adapters%2520early%250Ato%2520reduce%2520unnecessary%2520resource%2520usage%2520while%2520maintaining%2520performance.%2520In%2520our%250Aexperiments%252C%2520SAFE%2520reduces%2520memory%2520usage%252C%2520computation%2520amount%252C%2520and%2520training%2520time%250Aby%252042.85%255C%2525%252C%252034.59%255C%2525%252C%2520and%252011.82%255C%2525%252C%2520respectively%252C%2520while%2520achieving%2520comparable%2520or%250Abetter%2520task%2520performance%2520compared%2520to%2520the%2520baseline.%2520We%2520also%2520demonstrate%2520that%2520SAFE%250Ainduces%2520regularization%2520effect%252C%2520thereby%2520smoothing%2520the%2520loss%2520landscape%252C%2520which%250Aenables%2520the%2520model%2520to%2520generalize%2520better%2520by%2520avoiding%2520sharp%2520minima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Adapters%20Matter%3A%20Selective%20Adapter%20Freezing%20for%20Memory-Efficient%0A%20%20Fine-Tuning%20of%20Language%20Models&entry.906535625=Hyegang%20Son%20and%20Yonglak%20Son%20and%20Changhoon%20Kim%20and%20Young%20Geun%20Kim&entry.1292438233=%20%20Transformer-based%20large-scale%20pre-trained%20models%20achieve%20great%20success.%0AFine-tuning%20is%20the%20standard%20practice%20for%20leveraging%20these%20models%20in%20downstream%0Atasks.%20Among%20the%20fine-tuning%20methods%2C%20adapter-tuning%20provides%20a%0Aparameter-efficient%20fine-tuning%20by%20introducing%20lightweight%20trainable%20modules%0Awhile%20keeping%20most%20pre-trained%20parameters%20frozen.%20However%2C%20existing%0Aadapter-tuning%20methods%20still%20impose%20substantial%20resource%20usage.%20Through%20our%0Ainvestigation%2C%20we%20show%20that%20each%20adapter%20unequally%20contributes%20to%20both%20task%0Aperformance%20and%20resource%20usage.%20Motivated%20by%20this%20insight%2C%20we%20propose%20Selective%0AAdapter%20FrEezing%20%28SAFE%29%2C%20which%20gradually%20freezes%20less%20important%20adapters%20early%0Ato%20reduce%20unnecessary%20resource%20usage%20while%20maintaining%20performance.%20In%20our%0Aexperiments%2C%20SAFE%20reduces%20memory%20usage%2C%20computation%20amount%2C%20and%20training%20time%0Aby%2042.85%5C%25%2C%2034.59%5C%25%2C%20and%2011.82%5C%25%2C%20respectively%2C%20while%20achieving%20comparable%20or%0Abetter%20task%20performance%20compared%20to%20the%20baseline.%20We%20also%20demonstrate%20that%20SAFE%0Ainduces%20regularization%20effect%2C%20thereby%20smoothing%20the%20loss%20landscape%2C%20which%0Aenables%20the%20model%20to%20generalize%20better%20by%20avoiding%20sharp%20minima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03587v2&entry.124074799=Read"},
{"title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity", "author": "Shihao Zou and Qingfeng Li and Wei Ji and Jingjing Li and Yongkui Yang and Guoqi Li and Chao Dong", "abstract": "  Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer\n", "link": "http://arxiv.org/abs/2505.10352v1", "date": "2025-05-15", "relevancy": 2.0327, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5035}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeVideoFormer%3A%20An%20Efficient%20Spike-Driven%20Video%20Transformer%20with%0A%20%20Hamming%20Attention%20and%20%24%5Cmathcal%7BO%7D%28T%29%24%20Complexity&body=Title%3A%20SpikeVideoFormer%3A%20An%20Efficient%20Spike-Driven%20Video%20Transformer%20with%0A%20%20Hamming%20Attention%20and%20%24%5Cmathcal%7BO%7D%28T%29%24%20Complexity%0AAuthor%3A%20Shihao%20Zou%20and%20Qingfeng%20Li%20and%20Wei%20Ji%20and%20Jingjing%20Li%20and%20Yongkui%20Yang%20and%20Guoqi%20Li%20and%20Chao%20Dong%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20shown%20competitive%20performance%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29%20in%20various%20vision%20tasks%2C%20while%20offering%0Asuperior%20energy%20efficiency.%20However%2C%20existing%20SNN-based%20Transformers%20primarily%0Afocus%20on%20single-image%20tasks%2C%20emphasizing%20spatial%20features%20while%20not%20effectively%0Aleveraging%20SNNs%27%20efficiency%20in%20video-based%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aintroduce%20SpikeVideoFormer%2C%20an%20efficient%20spike-driven%20video%20Transformer%2C%0Afeaturing%20linear%20temporal%20complexity%20%24%5Cmathcal%7BO%7D%28T%29%24.%20Specifically%2C%20we%20design%0Aa%20spike-driven%20Hamming%20attention%20%28SDHA%29%20which%20provides%20a%20theoretically%20guided%0Aadaptation%20from%20traditional%20real-valued%20attention%20to%20spike-driven%20attention.%0ABuilding%20on%20SDHA%2C%20we%20further%20analyze%20various%20spike-driven%20space-time%20attention%0Adesigns%20and%20identify%20an%20optimal%20scheme%20that%20delivers%20appealing%20performance%20for%0Avideo%20tasks%2C%20while%20maintaining%20only%20linear%20temporal%20complexity.%20The%0Ageneralization%20ability%20and%20efficiency%20of%20our%20model%20are%20demonstrated%20across%0Adiverse%20downstream%20video%20tasks%2C%20including%20classification%2C%20human%20pose%20tracking%2C%0Aand%20semantic%20segmentation.%20Empirical%20results%20show%20our%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%20SNN%20approaches%2C%20with%0Aover%2015%5C%25%20improvement%20on%20the%20latter%20two%20tasks.%20Additionally%2C%20it%20matches%20the%0Aperformance%20of%20recent%20ANN-based%20methods%20while%20offering%20significant%20efficiency%0Agains%2C%20achieving%20%24%5Ctimes%2016%24%2C%20%24%5Ctimes%2010%24%20and%20%24%5Ctimes%205%24%20improvements%20on%20the%0Athree%20tasks.%20https%3A//github.com/JimmyZou/SpikeVideoFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeVideoFormer%253A%2520An%2520Efficient%2520Spike-Driven%2520Video%2520Transformer%2520with%250A%2520%2520Hamming%2520Attention%2520and%2520%2524%255Cmathcal%257BO%257D%2528T%2529%2524%2520Complexity%26entry.906535625%3DShihao%2520Zou%2520and%2520Qingfeng%2520Li%2520and%2520Wei%2520Ji%2520and%2520Jingjing%2520Li%2520and%2520Yongkui%2520Yang%2520and%2520Guoqi%2520Li%2520and%2520Chao%2520Dong%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520shown%2520competitive%2520performance%2520to%250AArtificial%2520Neural%2520Networks%2520%2528ANNs%2529%2520in%2520various%2520vision%2520tasks%252C%2520while%2520offering%250Asuperior%2520energy%2520efficiency.%2520However%252C%2520existing%2520SNN-based%2520Transformers%2520primarily%250Afocus%2520on%2520single-image%2520tasks%252C%2520emphasizing%2520spatial%2520features%2520while%2520not%2520effectively%250Aleveraging%2520SNNs%2527%2520efficiency%2520in%2520video-based%2520vision%2520tasks.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520SpikeVideoFormer%252C%2520an%2520efficient%2520spike-driven%2520video%2520Transformer%252C%250Afeaturing%2520linear%2520temporal%2520complexity%2520%2524%255Cmathcal%257BO%257D%2528T%2529%2524.%2520Specifically%252C%2520we%2520design%250Aa%2520spike-driven%2520Hamming%2520attention%2520%2528SDHA%2529%2520which%2520provides%2520a%2520theoretically%2520guided%250Aadaptation%2520from%2520traditional%2520real-valued%2520attention%2520to%2520spike-driven%2520attention.%250ABuilding%2520on%2520SDHA%252C%2520we%2520further%2520analyze%2520various%2520spike-driven%2520space-time%2520attention%250Adesigns%2520and%2520identify%2520an%2520optimal%2520scheme%2520that%2520delivers%2520appealing%2520performance%2520for%250Avideo%2520tasks%252C%2520while%2520maintaining%2520only%2520linear%2520temporal%2520complexity.%2520The%250Ageneralization%2520ability%2520and%2520efficiency%2520of%2520our%2520model%2520are%2520demonstrated%2520across%250Adiverse%2520downstream%2520video%2520tasks%252C%2520including%2520classification%252C%2520human%2520pose%2520tracking%252C%250Aand%2520semantic%2520segmentation.%2520Empirical%2520results%2520show%2520our%2520method%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520compared%2520to%2520existing%2520SNN%2520approaches%252C%2520with%250Aover%252015%255C%2525%2520improvement%2520on%2520the%2520latter%2520two%2520tasks.%2520Additionally%252C%2520it%2520matches%2520the%250Aperformance%2520of%2520recent%2520ANN-based%2520methods%2520while%2520offering%2520significant%2520efficiency%250Agains%252C%2520achieving%2520%2524%255Ctimes%252016%2524%252C%2520%2524%255Ctimes%252010%2524%2520and%2520%2524%255Ctimes%25205%2524%2520improvements%2520on%2520the%250Athree%2520tasks.%2520https%253A//github.com/JimmyZou/SpikeVideoFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeVideoFormer%3A%20An%20Efficient%20Spike-Driven%20Video%20Transformer%20with%0A%20%20Hamming%20Attention%20and%20%24%5Cmathcal%7BO%7D%28T%29%24%20Complexity&entry.906535625=Shihao%20Zou%20and%20Qingfeng%20Li%20and%20Wei%20Ji%20and%20Jingjing%20Li%20and%20Yongkui%20Yang%20and%20Guoqi%20Li%20and%20Chao%20Dong&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20shown%20competitive%20performance%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29%20in%20various%20vision%20tasks%2C%20while%20offering%0Asuperior%20energy%20efficiency.%20However%2C%20existing%20SNN-based%20Transformers%20primarily%0Afocus%20on%20single-image%20tasks%2C%20emphasizing%20spatial%20features%20while%20not%20effectively%0Aleveraging%20SNNs%27%20efficiency%20in%20video-based%20vision%20tasks.%20In%20this%20paper%2C%20we%0Aintroduce%20SpikeVideoFormer%2C%20an%20efficient%20spike-driven%20video%20Transformer%2C%0Afeaturing%20linear%20temporal%20complexity%20%24%5Cmathcal%7BO%7D%28T%29%24.%20Specifically%2C%20we%20design%0Aa%20spike-driven%20Hamming%20attention%20%28SDHA%29%20which%20provides%20a%20theoretically%20guided%0Aadaptation%20from%20traditional%20real-valued%20attention%20to%20spike-driven%20attention.%0ABuilding%20on%20SDHA%2C%20we%20further%20analyze%20various%20spike-driven%20space-time%20attention%0Adesigns%20and%20identify%20an%20optimal%20scheme%20that%20delivers%20appealing%20performance%20for%0Avideo%20tasks%2C%20while%20maintaining%20only%20linear%20temporal%20complexity.%20The%0Ageneralization%20ability%20and%20efficiency%20of%20our%20model%20are%20demonstrated%20across%0Adiverse%20downstream%20video%20tasks%2C%20including%20classification%2C%20human%20pose%20tracking%2C%0Aand%20semantic%20segmentation.%20Empirical%20results%20show%20our%20method%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%20SNN%20approaches%2C%20with%0Aover%2015%5C%25%20improvement%20on%20the%20latter%20two%20tasks.%20Additionally%2C%20it%20matches%20the%0Aperformance%20of%20recent%20ANN-based%20methods%20while%20offering%20significant%20efficiency%0Agains%2C%20achieving%20%24%5Ctimes%2016%24%2C%20%24%5Ctimes%2010%24%20and%20%24%5Ctimes%205%24%20improvements%20on%20the%0Athree%20tasks.%20https%3A//github.com/JimmyZou/SpikeVideoFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10352v1&entry.124074799=Read"},
{"title": "Superposition Yields Robust Neural Scaling", "author": "Yizhou liu and Ziming Liu and Jeff Gore", "abstract": "  The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.\n", "link": "http://arxiv.org/abs/2505.10465v1", "date": "2025-05-15", "relevancy": 2.0317, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superposition%20Yields%20Robust%20Neural%20Scaling&body=Title%3A%20Superposition%20Yields%20Robust%20Neural%20Scaling%0AAuthor%3A%20Yizhou%20liu%20and%20Ziming%20Liu%20and%20Jeff%20Gore%0AAbstract%3A%20%20%20The%20success%20of%20today%27s%20large%20language%20models%20%28LLMs%29%20depends%20on%20the%0Aobservation%20that%20larger%20models%20perform%20better.%20However%2C%20the%20origin%20of%20this%0Aneural%20scaling%20law%20--%20the%20finding%20that%20loss%20decreases%20as%20a%20power%20law%20with%20model%0Asize%20--%20remains%20unclear.%20Starting%20from%20two%20empirical%20principles%20--%20that%20LLMs%0Arepresent%20more%20things%20than%20the%20model%20dimensions%20%28widths%29%20they%20have%20%28i.e.%2C%0Arepresentations%20are%20superposed%29%2C%20and%20that%20words%20or%20concepts%20in%20language%20occur%0Awith%20varying%20frequencies%20--%20we%20constructed%20a%20toy%20model%20to%20study%20the%20loss%0Ascaling%20with%20model%20size.%20We%20found%20that%20when%20superposition%20is%20weak%2C%20meaning%20only%0Athe%20most%20frequent%20features%20are%20represented%20without%20interference%2C%20the%20scaling%20of%0Aloss%20with%20model%20size%20depends%20on%20the%20underlying%20feature%20frequency%3B%20if%20feature%0Afrequencies%20follow%20a%20power%20law%2C%20so%20does%20the%20loss.%20In%20contrast%2C%20under%20strong%0Asuperposition%2C%20where%20all%20features%20are%20represented%20but%20overlap%20with%20each%20other%2C%0Athe%20loss%20becomes%20inversely%20proportional%20to%20the%20model%20dimension%20across%20a%20wide%0Arange%20of%20feature%20frequency%20distributions.%20This%20robust%20scaling%20behavior%20is%0Aexplained%20geometrically%3A%20when%20many%20more%20vectors%20are%20packed%20into%20a%20lower%0Adimensional%20space%2C%20the%20interference%20%28squared%20overlaps%29%20between%20vectors%20scales%0Ainversely%20with%20that%20dimension.%20We%20then%20analyzed%20four%20families%20of%20open-sourced%0ALLMs%20and%20found%20that%20they%20exhibit%20strong%20superposition%20and%20quantitatively%20match%0Athe%20predictions%20of%20our%20toy%20model.%20The%20Chinchilla%20scaling%20law%20turned%20out%20to%20also%0Aagree%20with%20our%20results.%20We%20conclude%20that%20representation%20superposition%20is%20an%0Aimportant%20mechanism%20underlying%20the%20observed%20neural%20scaling%20laws.%20We%20anticipate%0Athat%20these%20insights%20will%20inspire%20new%20training%20strategies%20and%20model%0Aarchitectures%20to%20achieve%20better%20performance%20with%20less%20computation%20and%20fewer%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperposition%2520Yields%2520Robust%2520Neural%2520Scaling%26entry.906535625%3DYizhou%2520liu%2520and%2520Ziming%2520Liu%2520and%2520Jeff%2520Gore%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520today%2527s%2520large%2520language%2520models%2520%2528LLMs%2529%2520depends%2520on%2520the%250Aobservation%2520that%2520larger%2520models%2520perform%2520better.%2520However%252C%2520the%2520origin%2520of%2520this%250Aneural%2520scaling%2520law%2520--%2520the%2520finding%2520that%2520loss%2520decreases%2520as%2520a%2520power%2520law%2520with%2520model%250Asize%2520--%2520remains%2520unclear.%2520Starting%2520from%2520two%2520empirical%2520principles%2520--%2520that%2520LLMs%250Arepresent%2520more%2520things%2520than%2520the%2520model%2520dimensions%2520%2528widths%2529%2520they%2520have%2520%2528i.e.%252C%250Arepresentations%2520are%2520superposed%2529%252C%2520and%2520that%2520words%2520or%2520concepts%2520in%2520language%2520occur%250Awith%2520varying%2520frequencies%2520--%2520we%2520constructed%2520a%2520toy%2520model%2520to%2520study%2520the%2520loss%250Ascaling%2520with%2520model%2520size.%2520We%2520found%2520that%2520when%2520superposition%2520is%2520weak%252C%2520meaning%2520only%250Athe%2520most%2520frequent%2520features%2520are%2520represented%2520without%2520interference%252C%2520the%2520scaling%2520of%250Aloss%2520with%2520model%2520size%2520depends%2520on%2520the%2520underlying%2520feature%2520frequency%253B%2520if%2520feature%250Afrequencies%2520follow%2520a%2520power%2520law%252C%2520so%2520does%2520the%2520loss.%2520In%2520contrast%252C%2520under%2520strong%250Asuperposition%252C%2520where%2520all%2520features%2520are%2520represented%2520but%2520overlap%2520with%2520each%2520other%252C%250Athe%2520loss%2520becomes%2520inversely%2520proportional%2520to%2520the%2520model%2520dimension%2520across%2520a%2520wide%250Arange%2520of%2520feature%2520frequency%2520distributions.%2520This%2520robust%2520scaling%2520behavior%2520is%250Aexplained%2520geometrically%253A%2520when%2520many%2520more%2520vectors%2520are%2520packed%2520into%2520a%2520lower%250Adimensional%2520space%252C%2520the%2520interference%2520%2528squared%2520overlaps%2529%2520between%2520vectors%2520scales%250Ainversely%2520with%2520that%2520dimension.%2520We%2520then%2520analyzed%2520four%2520families%2520of%2520open-sourced%250ALLMs%2520and%2520found%2520that%2520they%2520exhibit%2520strong%2520superposition%2520and%2520quantitatively%2520match%250Athe%2520predictions%2520of%2520our%2520toy%2520model.%2520The%2520Chinchilla%2520scaling%2520law%2520turned%2520out%2520to%2520also%250Aagree%2520with%2520our%2520results.%2520We%2520conclude%2520that%2520representation%2520superposition%2520is%2520an%250Aimportant%2520mechanism%2520underlying%2520the%2520observed%2520neural%2520scaling%2520laws.%2520We%2520anticipate%250Athat%2520these%2520insights%2520will%2520inspire%2520new%2520training%2520strategies%2520and%2520model%250Aarchitectures%2520to%2520achieve%2520better%2520performance%2520with%2520less%2520computation%2520and%2520fewer%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superposition%20Yields%20Robust%20Neural%20Scaling&entry.906535625=Yizhou%20liu%20and%20Ziming%20Liu%20and%20Jeff%20Gore&entry.1292438233=%20%20The%20success%20of%20today%27s%20large%20language%20models%20%28LLMs%29%20depends%20on%20the%0Aobservation%20that%20larger%20models%20perform%20better.%20However%2C%20the%20origin%20of%20this%0Aneural%20scaling%20law%20--%20the%20finding%20that%20loss%20decreases%20as%20a%20power%20law%20with%20model%0Asize%20--%20remains%20unclear.%20Starting%20from%20two%20empirical%20principles%20--%20that%20LLMs%0Arepresent%20more%20things%20than%20the%20model%20dimensions%20%28widths%29%20they%20have%20%28i.e.%2C%0Arepresentations%20are%20superposed%29%2C%20and%20that%20words%20or%20concepts%20in%20language%20occur%0Awith%20varying%20frequencies%20--%20we%20constructed%20a%20toy%20model%20to%20study%20the%20loss%0Ascaling%20with%20model%20size.%20We%20found%20that%20when%20superposition%20is%20weak%2C%20meaning%20only%0Athe%20most%20frequent%20features%20are%20represented%20without%20interference%2C%20the%20scaling%20of%0Aloss%20with%20model%20size%20depends%20on%20the%20underlying%20feature%20frequency%3B%20if%20feature%0Afrequencies%20follow%20a%20power%20law%2C%20so%20does%20the%20loss.%20In%20contrast%2C%20under%20strong%0Asuperposition%2C%20where%20all%20features%20are%20represented%20but%20overlap%20with%20each%20other%2C%0Athe%20loss%20becomes%20inversely%20proportional%20to%20the%20model%20dimension%20across%20a%20wide%0Arange%20of%20feature%20frequency%20distributions.%20This%20robust%20scaling%20behavior%20is%0Aexplained%20geometrically%3A%20when%20many%20more%20vectors%20are%20packed%20into%20a%20lower%0Adimensional%20space%2C%20the%20interference%20%28squared%20overlaps%29%20between%20vectors%20scales%0Ainversely%20with%20that%20dimension.%20We%20then%20analyzed%20four%20families%20of%20open-sourced%0ALLMs%20and%20found%20that%20they%20exhibit%20strong%20superposition%20and%20quantitatively%20match%0Athe%20predictions%20of%20our%20toy%20model.%20The%20Chinchilla%20scaling%20law%20turned%20out%20to%20also%0Aagree%20with%20our%20results.%20We%20conclude%20that%20representation%20superposition%20is%20an%0Aimportant%20mechanism%20underlying%20the%20observed%20neural%20scaling%20laws.%20We%20anticipate%0Athat%20these%20insights%20will%20inspire%20new%20training%20strategies%20and%20model%0Aarchitectures%20to%20achieve%20better%20performance%20with%20less%20computation%20and%20fewer%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10465v1&entry.124074799=Read"},
{"title": "Learned Lightweight Smartphone ISP with Unpaired Data", "author": "Andrei Arhire and Radu Timofte", "abstract": "  The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .\n", "link": "http://arxiv.org/abs/2505.10420v1", "date": "2025-05-15", "relevancy": 2.0268, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5018}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Lightweight%20Smartphone%20ISP%20with%20Unpaired%20Data&body=Title%3A%20Learned%20Lightweight%20Smartphone%20ISP%20with%20Unpaired%20Data%0AAuthor%3A%20Andrei%20Arhire%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20The%20Image%20Signal%20Processor%20%28ISP%29%20is%20a%20fundamental%20component%20in%20modern%0Asmartphone%20cameras%20responsible%20for%20conversion%20of%20RAW%20sensor%20image%20data%20to%20RGB%0Aimages%20with%20a%20strong%20focus%20on%20perceptual%20quality.%20Recent%20work%20highlights%20the%0Apotential%20of%20deep%20learning%20approaches%20and%20their%20ability%20to%20capture%20details%20with%0Aa%20quality%20increasingly%20close%20to%20that%20of%20professional%20cameras.%20A%20difficult%20and%0Acostly%20step%20when%20developing%20a%20learned%20ISP%20is%20the%20acquisition%20of%20pixel-wise%0Aaligned%20paired%20data%20that%20maps%20the%20raw%20captured%20by%20a%20smartphone%20camera%20sensor%20to%0Ahigh-quality%20reference%20images.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Aproposing%20a%20novel%20training%20method%20for%20a%20learnable%20ISP%20that%20eliminates%20the%20need%0Afor%20direct%20correspondences%20between%20raw%20images%20and%20ground-truth%20data%20with%0Amatching%20content.%20Our%20unpaired%20approach%20employs%20a%20multi-term%20loss%20function%0Aguided%20by%20adversarial%20training%20with%20multiple%20discriminators%20processing%20feature%0Amaps%20from%20pre-trained%20networks%20to%20maintain%20content%20structure%20while%20learning%0Acolor%20and%20texture%20characteristics%20from%20the%20target%20RGB%20dataset.%20Using%0Alightweight%20neural%20network%20architectures%20suitable%20for%20mobile%20devices%20as%0Abackbones%2C%20we%20evaluated%20our%20method%20on%20the%20Zurich%20RAW%20to%20RGB%20and%20Fujifilm%0AUltraISP%20datasets.%20Compared%20to%20paired%20training%20methods%2C%20our%20unpaired%20learning%0Astrategy%20shows%20strong%20potential%20and%20achieves%20high%20fidelity%20across%20multiple%0Aevaluation%20metrics.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Lightweight%2520Smartphone%2520ISP%2520with%2520Unpaired%2520Data%26entry.906535625%3DAndrei%2520Arhire%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520The%2520Image%2520Signal%2520Processor%2520%2528ISP%2529%2520is%2520a%2520fundamental%2520component%2520in%2520modern%250Asmartphone%2520cameras%2520responsible%2520for%2520conversion%2520of%2520RAW%2520sensor%2520image%2520data%2520to%2520RGB%250Aimages%2520with%2520a%2520strong%2520focus%2520on%2520perceptual%2520quality.%2520Recent%2520work%2520highlights%2520the%250Apotential%2520of%2520deep%2520learning%2520approaches%2520and%2520their%2520ability%2520to%2520capture%2520details%2520with%250Aa%2520quality%2520increasingly%2520close%2520to%2520that%2520of%2520professional%2520cameras.%2520A%2520difficult%2520and%250Acostly%2520step%2520when%2520developing%2520a%2520learned%2520ISP%2520is%2520the%2520acquisition%2520of%2520pixel-wise%250Aaligned%2520paired%2520data%2520that%2520maps%2520the%2520raw%2520captured%2520by%2520a%2520smartphone%2520camera%2520sensor%2520to%250Ahigh-quality%2520reference%2520images.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%250Aproposing%2520a%2520novel%2520training%2520method%2520for%2520a%2520learnable%2520ISP%2520that%2520eliminates%2520the%2520need%250Afor%2520direct%2520correspondences%2520between%2520raw%2520images%2520and%2520ground-truth%2520data%2520with%250Amatching%2520content.%2520Our%2520unpaired%2520approach%2520employs%2520a%2520multi-term%2520loss%2520function%250Aguided%2520by%2520adversarial%2520training%2520with%2520multiple%2520discriminators%2520processing%2520feature%250Amaps%2520from%2520pre-trained%2520networks%2520to%2520maintain%2520content%2520structure%2520while%2520learning%250Acolor%2520and%2520texture%2520characteristics%2520from%2520the%2520target%2520RGB%2520dataset.%2520Using%250Alightweight%2520neural%2520network%2520architectures%2520suitable%2520for%2520mobile%2520devices%2520as%250Abackbones%252C%2520we%2520evaluated%2520our%2520method%2520on%2520the%2520Zurich%2520RAW%2520to%2520RGB%2520and%2520Fujifilm%250AUltraISP%2520datasets.%2520Compared%2520to%2520paired%2520training%2520methods%252C%2520our%2520unpaired%2520learning%250Astrategy%2520shows%2520strong%2520potential%2520and%2520achieves%2520high%2520fidelity%2520across%2520multiple%250Aevaluation%2520metrics.%2520The%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Lightweight%20Smartphone%20ISP%20with%20Unpaired%20Data&entry.906535625=Andrei%20Arhire%20and%20Radu%20Timofte&entry.1292438233=%20%20The%20Image%20Signal%20Processor%20%28ISP%29%20is%20a%20fundamental%20component%20in%20modern%0Asmartphone%20cameras%20responsible%20for%20conversion%20of%20RAW%20sensor%20image%20data%20to%20RGB%0Aimages%20with%20a%20strong%20focus%20on%20perceptual%20quality.%20Recent%20work%20highlights%20the%0Apotential%20of%20deep%20learning%20approaches%20and%20their%20ability%20to%20capture%20details%20with%0Aa%20quality%20increasingly%20close%20to%20that%20of%20professional%20cameras.%20A%20difficult%20and%0Acostly%20step%20when%20developing%20a%20learned%20ISP%20is%20the%20acquisition%20of%20pixel-wise%0Aaligned%20paired%20data%20that%20maps%20the%20raw%20captured%20by%20a%20smartphone%20camera%20sensor%20to%0Ahigh-quality%20reference%20images.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%0Aproposing%20a%20novel%20training%20method%20for%20a%20learnable%20ISP%20that%20eliminates%20the%20need%0Afor%20direct%20correspondences%20between%20raw%20images%20and%20ground-truth%20data%20with%0Amatching%20content.%20Our%20unpaired%20approach%20employs%20a%20multi-term%20loss%20function%0Aguided%20by%20adversarial%20training%20with%20multiple%20discriminators%20processing%20feature%0Amaps%20from%20pre-trained%20networks%20to%20maintain%20content%20structure%20while%20learning%0Acolor%20and%20texture%20characteristics%20from%20the%20target%20RGB%20dataset.%20Using%0Alightweight%20neural%20network%20architectures%20suitable%20for%20mobile%20devices%20as%0Abackbones%2C%20we%20evaluated%20our%20method%20on%20the%20Zurich%20RAW%20to%20RGB%20and%20Fujifilm%0AUltraISP%20datasets.%20Compared%20to%20paired%20training%20methods%2C%20our%20unpaired%20learning%0Astrategy%20shows%20strong%20potential%20and%20achieves%20high%20fidelity%20across%20multiple%0Aevaluation%20metrics.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10420v1&entry.124074799=Read"},
{"title": "Representation Convergence: Mutual Distillation is Secretly a Form of\n  Regularization", "author": "Zhengpeng Xie and Jiahang Cao and Qiang Zhang and Jianxiong Zhang and Changwei Wang and Renjing Xu", "abstract": "  In this paper, we argue that mutual distillation between reinforcement\nlearning policies serves as an implicit regularization, preventing them from\noverfitting to irrelevant features. We highlight two key contributions: (a)\nTheoretically, for the first time, we prove that enhancing the policy\nrobustness to irrelevant features leads to improved generalization performance.\n(b) Empirically, we demonstrate that mutual distillation between policies\ncontributes to such robustness, enabling the spontaneous emergence of invariant\nrepresentations over pixel inputs. Overall, our findings challenge the\nconventional view of distillation as merely a means of knowledge transfer,\noffering a novel perspective on the generalization in deep reinforcement\nlearning.\n", "link": "http://arxiv.org/abs/2501.02481v4", "date": "2025-05-15", "relevancy": 2.0116, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5203}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Convergence%3A%20Mutual%20Distillation%20is%20Secretly%20a%20Form%20of%0A%20%20Regularization&body=Title%3A%20Representation%20Convergence%3A%20Mutual%20Distillation%20is%20Secretly%20a%20Form%20of%0A%20%20Regularization%0AAuthor%3A%20Zhengpeng%20Xie%20and%20Jiahang%20Cao%20and%20Qiang%20Zhang%20and%20Jianxiong%20Zhang%20and%20Changwei%20Wang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20argue%20that%20mutual%20distillation%20between%20reinforcement%0Alearning%20policies%20serves%20as%20an%20implicit%20regularization%2C%20preventing%20them%20from%0Aoverfitting%20to%20irrelevant%20features.%20We%20highlight%20two%20key%20contributions%3A%20%28a%29%0ATheoretically%2C%20for%20the%20first%20time%2C%20we%20prove%20that%20enhancing%20the%20policy%0Arobustness%20to%20irrelevant%20features%20leads%20to%20improved%20generalization%20performance.%0A%28b%29%20Empirically%2C%20we%20demonstrate%20that%20mutual%20distillation%20between%20policies%0Acontributes%20to%20such%20robustness%2C%20enabling%20the%20spontaneous%20emergence%20of%20invariant%0Arepresentations%20over%20pixel%20inputs.%20Overall%2C%20our%20findings%20challenge%20the%0Aconventional%20view%20of%20distillation%20as%20merely%20a%20means%20of%20knowledge%20transfer%2C%0Aoffering%20a%20novel%20perspective%20on%20the%20generalization%20in%20deep%20reinforcement%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02481v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Convergence%253A%2520Mutual%2520Distillation%2520is%2520Secretly%2520a%2520Form%2520of%250A%2520%2520Regularization%26entry.906535625%3DZhengpeng%2520Xie%2520and%2520Jiahang%2520Cao%2520and%2520Qiang%2520Zhang%2520and%2520Jianxiong%2520Zhang%2520and%2520Changwei%2520Wang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520mutual%2520distillation%2520between%2520reinforcement%250Alearning%2520policies%2520serves%2520as%2520an%2520implicit%2520regularization%252C%2520preventing%2520them%2520from%250Aoverfitting%2520to%2520irrelevant%2520features.%2520We%2520highlight%2520two%2520key%2520contributions%253A%2520%2528a%2529%250ATheoretically%252C%2520for%2520the%2520first%2520time%252C%2520we%2520prove%2520that%2520enhancing%2520the%2520policy%250Arobustness%2520to%2520irrelevant%2520features%2520leads%2520to%2520improved%2520generalization%2520performance.%250A%2528b%2529%2520Empirically%252C%2520we%2520demonstrate%2520that%2520mutual%2520distillation%2520between%2520policies%250Acontributes%2520to%2520such%2520robustness%252C%2520enabling%2520the%2520spontaneous%2520emergence%2520of%2520invariant%250Arepresentations%2520over%2520pixel%2520inputs.%2520Overall%252C%2520our%2520findings%2520challenge%2520the%250Aconventional%2520view%2520of%2520distillation%2520as%2520merely%2520a%2520means%2520of%2520knowledge%2520transfer%252C%250Aoffering%2520a%2520novel%2520perspective%2520on%2520the%2520generalization%2520in%2520deep%2520reinforcement%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02481v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Convergence%3A%20Mutual%20Distillation%20is%20Secretly%20a%20Form%20of%0A%20%20Regularization&entry.906535625=Zhengpeng%20Xie%20and%20Jiahang%20Cao%20and%20Qiang%20Zhang%20and%20Jianxiong%20Zhang%20and%20Changwei%20Wang%20and%20Renjing%20Xu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20argue%20that%20mutual%20distillation%20between%20reinforcement%0Alearning%20policies%20serves%20as%20an%20implicit%20regularization%2C%20preventing%20them%20from%0Aoverfitting%20to%20irrelevant%20features.%20We%20highlight%20two%20key%20contributions%3A%20%28a%29%0ATheoretically%2C%20for%20the%20first%20time%2C%20we%20prove%20that%20enhancing%20the%20policy%0Arobustness%20to%20irrelevant%20features%20leads%20to%20improved%20generalization%20performance.%0A%28b%29%20Empirically%2C%20we%20demonstrate%20that%20mutual%20distillation%20between%20policies%0Acontributes%20to%20such%20robustness%2C%20enabling%20the%20spontaneous%20emergence%20of%20invariant%0Arepresentations%20over%20pixel%20inputs.%20Overall%2C%20our%20findings%20challenge%20the%0Aconventional%20view%20of%20distillation%20as%20merely%20a%20means%20of%20knowledge%20transfer%2C%0Aoffering%20a%20novel%20perspective%20on%20the%20generalization%20in%20deep%20reinforcement%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02481v4&entry.124074799=Read"},
{"title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with\n  Human-Like Data Efficiency", "author": "Daniel Weitekamp and Christopher MacLellan and Erik Harpstead and Kenneth Koedinger", "abstract": "  Human learning relies on specialization -- distinct cognitive mechanisms\nworking together to enable rapid learning. In contrast, most modern neural\nnetworks rely on a single mechanism: gradient descent over an objective\nfunction. This raises the question: might human learners' relatively rapid\nlearning from just tens of examples instead of tens of thousands in data-driven\ndeep learning arise from our ability to use multiple specialized mechanisms of\nlearning in combination? We investigate this question through an ablation\nanalysis of inductive human learning simulations in online tutoring\nenvironments. Comparing reinforcement learning to a more data-efficient\n3-mechanism symbolic rule induction approach, we find that decomposing learning\ninto multiple distinct mechanisms significantly improves data efficiency,\nbringing it in line with human learning. Furthermore, we show that this\ndecomposition has a greater impact on efficiency than the distinction between\nsymbolic and subsymbolic learning alone. Efforts to align data-driven machine\nlearning with human learning often overlook the stark difference in learning\nefficiency. Our findings suggest that integrating multiple specialized learning\nmechanisms may be key to bridging this gap.\n", "link": "http://arxiv.org/abs/2505.10422v1", "date": "2025-05-15", "relevancy": 2.0073, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5074}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposed%20Inductive%20Procedure%20Learning%3A%20Learning%20Academic%20Tasks%20with%0A%20%20Human-Like%20Data%20Efficiency&body=Title%3A%20Decomposed%20Inductive%20Procedure%20Learning%3A%20Learning%20Academic%20Tasks%20with%0A%20%20Human-Like%20Data%20Efficiency%0AAuthor%3A%20Daniel%20Weitekamp%20and%20Christopher%20MacLellan%20and%20Erik%20Harpstead%20and%20Kenneth%20Koedinger%0AAbstract%3A%20%20%20Human%20learning%20relies%20on%20specialization%20--%20distinct%20cognitive%20mechanisms%0Aworking%20together%20to%20enable%20rapid%20learning.%20In%20contrast%2C%20most%20modern%20neural%0Anetworks%20rely%20on%20a%20single%20mechanism%3A%20gradient%20descent%20over%20an%20objective%0Afunction.%20This%20raises%20the%20question%3A%20might%20human%20learners%27%20relatively%20rapid%0Alearning%20from%20just%20tens%20of%20examples%20instead%20of%20tens%20of%20thousands%20in%20data-driven%0Adeep%20learning%20arise%20from%20our%20ability%20to%20use%20multiple%20specialized%20mechanisms%20of%0Alearning%20in%20combination%3F%20We%20investigate%20this%20question%20through%20an%20ablation%0Aanalysis%20of%20inductive%20human%20learning%20simulations%20in%20online%20tutoring%0Aenvironments.%20Comparing%20reinforcement%20learning%20to%20a%20more%20data-efficient%0A3-mechanism%20symbolic%20rule%20induction%20approach%2C%20we%20find%20that%20decomposing%20learning%0Ainto%20multiple%20distinct%20mechanisms%20significantly%20improves%20data%20efficiency%2C%0Abringing%20it%20in%20line%20with%20human%20learning.%20Furthermore%2C%20we%20show%20that%20this%0Adecomposition%20has%20a%20greater%20impact%20on%20efficiency%20than%20the%20distinction%20between%0Asymbolic%20and%20subsymbolic%20learning%20alone.%20Efforts%20to%20align%20data-driven%20machine%0Alearning%20with%20human%20learning%20often%20overlook%20the%20stark%20difference%20in%20learning%0Aefficiency.%20Our%20findings%20suggest%20that%20integrating%20multiple%20specialized%20learning%0Amechanisms%20may%20be%20key%20to%20bridging%20this%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposed%2520Inductive%2520Procedure%2520Learning%253A%2520Learning%2520Academic%2520Tasks%2520with%250A%2520%2520Human-Like%2520Data%2520Efficiency%26entry.906535625%3DDaniel%2520Weitekamp%2520and%2520Christopher%2520MacLellan%2520and%2520Erik%2520Harpstead%2520and%2520Kenneth%2520Koedinger%26entry.1292438233%3D%2520%2520Human%2520learning%2520relies%2520on%2520specialization%2520--%2520distinct%2520cognitive%2520mechanisms%250Aworking%2520together%2520to%2520enable%2520rapid%2520learning.%2520In%2520contrast%252C%2520most%2520modern%2520neural%250Anetworks%2520rely%2520on%2520a%2520single%2520mechanism%253A%2520gradient%2520descent%2520over%2520an%2520objective%250Afunction.%2520This%2520raises%2520the%2520question%253A%2520might%2520human%2520learners%2527%2520relatively%2520rapid%250Alearning%2520from%2520just%2520tens%2520of%2520examples%2520instead%2520of%2520tens%2520of%2520thousands%2520in%2520data-driven%250Adeep%2520learning%2520arise%2520from%2520our%2520ability%2520to%2520use%2520multiple%2520specialized%2520mechanisms%2520of%250Alearning%2520in%2520combination%253F%2520We%2520investigate%2520this%2520question%2520through%2520an%2520ablation%250Aanalysis%2520of%2520inductive%2520human%2520learning%2520simulations%2520in%2520online%2520tutoring%250Aenvironments.%2520Comparing%2520reinforcement%2520learning%2520to%2520a%2520more%2520data-efficient%250A3-mechanism%2520symbolic%2520rule%2520induction%2520approach%252C%2520we%2520find%2520that%2520decomposing%2520learning%250Ainto%2520multiple%2520distinct%2520mechanisms%2520significantly%2520improves%2520data%2520efficiency%252C%250Abringing%2520it%2520in%2520line%2520with%2520human%2520learning.%2520Furthermore%252C%2520we%2520show%2520that%2520this%250Adecomposition%2520has%2520a%2520greater%2520impact%2520on%2520efficiency%2520than%2520the%2520distinction%2520between%250Asymbolic%2520and%2520subsymbolic%2520learning%2520alone.%2520Efforts%2520to%2520align%2520data-driven%2520machine%250Alearning%2520with%2520human%2520learning%2520often%2520overlook%2520the%2520stark%2520difference%2520in%2520learning%250Aefficiency.%2520Our%2520findings%2520suggest%2520that%2520integrating%2520multiple%2520specialized%2520learning%250Amechanisms%2520may%2520be%2520key%2520to%2520bridging%2520this%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposed%20Inductive%20Procedure%20Learning%3A%20Learning%20Academic%20Tasks%20with%0A%20%20Human-Like%20Data%20Efficiency&entry.906535625=Daniel%20Weitekamp%20and%20Christopher%20MacLellan%20and%20Erik%20Harpstead%20and%20Kenneth%20Koedinger&entry.1292438233=%20%20Human%20learning%20relies%20on%20specialization%20--%20distinct%20cognitive%20mechanisms%0Aworking%20together%20to%20enable%20rapid%20learning.%20In%20contrast%2C%20most%20modern%20neural%0Anetworks%20rely%20on%20a%20single%20mechanism%3A%20gradient%20descent%20over%20an%20objective%0Afunction.%20This%20raises%20the%20question%3A%20might%20human%20learners%27%20relatively%20rapid%0Alearning%20from%20just%20tens%20of%20examples%20instead%20of%20tens%20of%20thousands%20in%20data-driven%0Adeep%20learning%20arise%20from%20our%20ability%20to%20use%20multiple%20specialized%20mechanisms%20of%0Alearning%20in%20combination%3F%20We%20investigate%20this%20question%20through%20an%20ablation%0Aanalysis%20of%20inductive%20human%20learning%20simulations%20in%20online%20tutoring%0Aenvironments.%20Comparing%20reinforcement%20learning%20to%20a%20more%20data-efficient%0A3-mechanism%20symbolic%20rule%20induction%20approach%2C%20we%20find%20that%20decomposing%20learning%0Ainto%20multiple%20distinct%20mechanisms%20significantly%20improves%20data%20efficiency%2C%0Abringing%20it%20in%20line%20with%20human%20learning.%20Furthermore%2C%20we%20show%20that%20this%0Adecomposition%20has%20a%20greater%20impact%20on%20efficiency%20than%20the%20distinction%20between%0Asymbolic%20and%20subsymbolic%20learning%20alone.%20Efforts%20to%20align%20data-driven%20machine%0Alearning%20with%20human%20learning%20often%20overlook%20the%20stark%20difference%20in%20learning%0Aefficiency.%20Our%20findings%20suggest%20that%20integrating%20multiple%20specialized%20learning%0Amechanisms%20may%20be%20key%20to%20bridging%20this%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10422v1&entry.124074799=Read"},
{"title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations\n  Across Diverse Modalities and Models", "author": "Seongun Kim and Sol A Kim and Geonhyeong Kim and Enver Menadjiev and Chanwoo Lee and Seongwook Chung and Nari Kim and Jaesik Choi", "abstract": "  Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.\n", "link": "http://arxiv.org/abs/2505.10515v1", "date": "2025-05-15", "relevancy": 2.0052, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PnPXAI%3A%20A%20Universal%20XAI%20Framework%20Providing%20Automatic%20Explanations%0A%20%20Across%20Diverse%20Modalities%20and%20Models&body=Title%3A%20PnPXAI%3A%20A%20Universal%20XAI%20Framework%20Providing%20Automatic%20Explanations%0A%20%20Across%20Diverse%20Modalities%20and%20Models%0AAuthor%3A%20Seongun%20Kim%20and%20Sol%20A%20Kim%20and%20Geonhyeong%20Kim%20and%20Enver%20Menadjiev%20and%20Chanwoo%20Lee%20and%20Seongwook%20Chung%20and%20Nari%20Kim%20and%20Jaesik%20Choi%0AAbstract%3A%20%20%20Recently%2C%20post%20hoc%20explanation%20methods%20have%20emerged%20to%20enhance%20model%0Atransparency%20by%20attributing%20model%20outputs%20to%20input%20features.%20However%2C%20these%0Amethods%20face%20challenges%20due%20to%20their%20specificity%20to%20certain%20neural%20network%0Aarchitectures%20and%20data%20modalities.%20Existing%20explainable%20artificial%20intelligence%0A%28XAI%29%20frameworks%20have%20attempted%20to%20address%20these%20challenges%20but%20suffer%20from%0Aseveral%20limitations.%20These%20include%20limited%20flexibility%20to%20diverse%20model%0Aarchitectures%20and%20data%20modalities%20due%20to%20hard-coded%20implementations%2C%20a%0Arestricted%20number%20of%20supported%20XAI%20methods%20because%20of%20the%20requirements%20for%0Alayer-specific%20operations%20of%20attribution%20methods%2C%20and%20sub-optimal%0Arecommendations%20of%20explanations%20due%20to%20the%20lack%20of%20evaluation%20and%20optimization%0Aphases.%20Consequently%2C%20these%20limitations%20impede%20the%20adoption%20of%20XAI%20technology%0Ain%20real-world%20applications%2C%20making%20it%20difficult%20for%20practitioners%20to%20select%20the%0Aoptimal%20explanation%20method%20for%20their%20domain.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20%5Ctextbf%7BPnPXAI%7D%2C%20a%20universal%20XAI%20framework%20that%20supports%20diverse%20data%0Amodalities%20and%20neural%20network%20models%20in%20a%20Plug-and-Play%20%28PnP%29%20manner.%20PnPXAI%0Aautomatically%20detects%20model%20architectures%2C%20recommends%20applicable%20explanation%0Amethods%2C%20and%20optimizes%20hyperparameters%20for%20optimal%20explanations.%20We%20validate%0Athe%20framework%27s%20effectiveness%20through%20user%20surveys%20and%20showcase%20its%20versatility%0Aacross%20various%20domains%2C%20including%20medicine%20and%20finance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPnPXAI%253A%2520A%2520Universal%2520XAI%2520Framework%2520Providing%2520Automatic%2520Explanations%250A%2520%2520Across%2520Diverse%2520Modalities%2520and%2520Models%26entry.906535625%3DSeongun%2520Kim%2520and%2520Sol%2520A%2520Kim%2520and%2520Geonhyeong%2520Kim%2520and%2520Enver%2520Menadjiev%2520and%2520Chanwoo%2520Lee%2520and%2520Seongwook%2520Chung%2520and%2520Nari%2520Kim%2520and%2520Jaesik%2520Choi%26entry.1292438233%3D%2520%2520Recently%252C%2520post%2520hoc%2520explanation%2520methods%2520have%2520emerged%2520to%2520enhance%2520model%250Atransparency%2520by%2520attributing%2520model%2520outputs%2520to%2520input%2520features.%2520However%252C%2520these%250Amethods%2520face%2520challenges%2520due%2520to%2520their%2520specificity%2520to%2520certain%2520neural%2520network%250Aarchitectures%2520and%2520data%2520modalities.%2520Existing%2520explainable%2520artificial%2520intelligence%250A%2528XAI%2529%2520frameworks%2520have%2520attempted%2520to%2520address%2520these%2520challenges%2520but%2520suffer%2520from%250Aseveral%2520limitations.%2520These%2520include%2520limited%2520flexibility%2520to%2520diverse%2520model%250Aarchitectures%2520and%2520data%2520modalities%2520due%2520to%2520hard-coded%2520implementations%252C%2520a%250Arestricted%2520number%2520of%2520supported%2520XAI%2520methods%2520because%2520of%2520the%2520requirements%2520for%250Alayer-specific%2520operations%2520of%2520attribution%2520methods%252C%2520and%2520sub-optimal%250Arecommendations%2520of%2520explanations%2520due%2520to%2520the%2520lack%2520of%2520evaluation%2520and%2520optimization%250Aphases.%2520Consequently%252C%2520these%2520limitations%2520impede%2520the%2520adoption%2520of%2520XAI%2520technology%250Ain%2520real-world%2520applications%252C%2520making%2520it%2520difficult%2520for%2520practitioners%2520to%2520select%2520the%250Aoptimal%2520explanation%2520method%2520for%2520their%2520domain.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520%255Ctextbf%257BPnPXAI%257D%252C%2520a%2520universal%2520XAI%2520framework%2520that%2520supports%2520diverse%2520data%250Amodalities%2520and%2520neural%2520network%2520models%2520in%2520a%2520Plug-and-Play%2520%2528PnP%2529%2520manner.%2520PnPXAI%250Aautomatically%2520detects%2520model%2520architectures%252C%2520recommends%2520applicable%2520explanation%250Amethods%252C%2520and%2520optimizes%2520hyperparameters%2520for%2520optimal%2520explanations.%2520We%2520validate%250Athe%2520framework%2527s%2520effectiveness%2520through%2520user%2520surveys%2520and%2520showcase%2520its%2520versatility%250Aacross%2520various%2520domains%252C%2520including%2520medicine%2520and%2520finance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PnPXAI%3A%20A%20Universal%20XAI%20Framework%20Providing%20Automatic%20Explanations%0A%20%20Across%20Diverse%20Modalities%20and%20Models&entry.906535625=Seongun%20Kim%20and%20Sol%20A%20Kim%20and%20Geonhyeong%20Kim%20and%20Enver%20Menadjiev%20and%20Chanwoo%20Lee%20and%20Seongwook%20Chung%20and%20Nari%20Kim%20and%20Jaesik%20Choi&entry.1292438233=%20%20Recently%2C%20post%20hoc%20explanation%20methods%20have%20emerged%20to%20enhance%20model%0Atransparency%20by%20attributing%20model%20outputs%20to%20input%20features.%20However%2C%20these%0Amethods%20face%20challenges%20due%20to%20their%20specificity%20to%20certain%20neural%20network%0Aarchitectures%20and%20data%20modalities.%20Existing%20explainable%20artificial%20intelligence%0A%28XAI%29%20frameworks%20have%20attempted%20to%20address%20these%20challenges%20but%20suffer%20from%0Aseveral%20limitations.%20These%20include%20limited%20flexibility%20to%20diverse%20model%0Aarchitectures%20and%20data%20modalities%20due%20to%20hard-coded%20implementations%2C%20a%0Arestricted%20number%20of%20supported%20XAI%20methods%20because%20of%20the%20requirements%20for%0Alayer-specific%20operations%20of%20attribution%20methods%2C%20and%20sub-optimal%0Arecommendations%20of%20explanations%20due%20to%20the%20lack%20of%20evaluation%20and%20optimization%0Aphases.%20Consequently%2C%20these%20limitations%20impede%20the%20adoption%20of%20XAI%20technology%0Ain%20real-world%20applications%2C%20making%20it%20difficult%20for%20practitioners%20to%20select%20the%0Aoptimal%20explanation%20method%20for%20their%20domain.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20%5Ctextbf%7BPnPXAI%7D%2C%20a%20universal%20XAI%20framework%20that%20supports%20diverse%20data%0Amodalities%20and%20neural%20network%20models%20in%20a%20Plug-and-Play%20%28PnP%29%20manner.%20PnPXAI%0Aautomatically%20detects%20model%20architectures%2C%20recommends%20applicable%20explanation%0Amethods%2C%20and%20optimizes%20hyperparameters%20for%20optimal%20explanations.%20We%20validate%0Athe%20framework%27s%20effectiveness%20through%20user%20surveys%20and%20showcase%20its%20versatility%0Aacross%20various%20domains%2C%20including%20medicine%20and%20finance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10515v1&entry.124074799=Read"},
{"title": "Estimating the Diameter at Breast Height of Trees in a Forest With a\n  Single 360 Camera", "author": "Siming He and Zachary Osman and Fernando Cladera and Dexter Ong and Nitant Rai and Patrick Corey Green and Vijay Kumar and Pratik Chaudhari", "abstract": "  Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.\n", "link": "http://arxiv.org/abs/2505.03093v2", "date": "2025-05-15", "relevancy": 2.0024, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5206}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4998}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20Diameter%20at%20Breast%20Height%20of%20Trees%20in%20a%20Forest%20With%20a%0A%20%20Single%20360%20Camera&body=Title%3A%20Estimating%20the%20Diameter%20at%20Breast%20Height%20of%20Trees%20in%20a%20Forest%20With%20a%0A%20%20Single%20360%20Camera%0AAuthor%3A%20Siming%20He%20and%20Zachary%20Osman%20and%20Fernando%20Cladera%20and%20Dexter%20Ong%20and%20Nitant%20Rai%20and%20Patrick%20Corey%20Green%20and%20Vijay%20Kumar%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20Forest%20inventories%20rely%20on%20accurate%20measurements%20of%20the%20diameter%20at%20breast%0Aheight%20%28DBH%29%20for%20ecological%20monitoring%2C%20resource%20management%2C%20and%20carbon%0Aaccounting.%20While%20LiDAR-based%20techniques%20can%20achieve%20centimeter-level%0Aprecision%2C%20they%20are%20cost-prohibitive%20and%20operationally%20complex.%20We%20present%20a%0Alow-cost%20alternative%20that%20only%20needs%20a%20consumer-grade%20360%20video%20camera.%20Our%0Asemi-automated%20pipeline%20comprises%20of%20%28i%29%20a%20dense%20point%20cloud%20reconstruction%0Ausing%20Structure%20from%20Motion%20%28SfM%29%20photogrammetry%20software%20called%20Agisoft%0AMetashape%2C%20%28ii%29%20semantic%20trunk%20segmentation%20by%20projecting%20Grounded%20Segment%0AAnything%20%28SAM%29%20masks%20onto%20the%203D%20cloud%2C%20and%20%28iii%29%20a%20robust%20RANSAC-based%0Atechnique%20to%20estimate%20cross%20section%20shape%20and%20DBH.%20We%20introduce%20an%20interactive%0Avisualization%20tool%20for%20inspecting%20segmented%20trees%20and%20their%20estimated%20DBH.%20On%0A61%20acquisitions%20of%2043%20trees%20under%20a%20variety%20of%20conditions%2C%20our%20method%20attains%0Amedian%20absolute%20relative%20errors%20of%205-9%25%20with%20respect%20to%20%22ground-truth%22%20manual%0Ameasurements.%20This%20is%20only%202-4%25%20higher%20than%20LiDAR-based%20estimates%2C%20while%0Aemploying%20a%20single%20360%20camera%20that%20costs%20orders%20of%20magnitude%20less%2C%20requires%0Aminimal%20setup%2C%20and%20is%20widely%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520Diameter%2520at%2520Breast%2520Height%2520of%2520Trees%2520in%2520a%2520Forest%2520With%2520a%250A%2520%2520Single%2520360%2520Camera%26entry.906535625%3DSiming%2520He%2520and%2520Zachary%2520Osman%2520and%2520Fernando%2520Cladera%2520and%2520Dexter%2520Ong%2520and%2520Nitant%2520Rai%2520and%2520Patrick%2520Corey%2520Green%2520and%2520Vijay%2520Kumar%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520Forest%2520inventories%2520rely%2520on%2520accurate%2520measurements%2520of%2520the%2520diameter%2520at%2520breast%250Aheight%2520%2528DBH%2529%2520for%2520ecological%2520monitoring%252C%2520resource%2520management%252C%2520and%2520carbon%250Aaccounting.%2520While%2520LiDAR-based%2520techniques%2520can%2520achieve%2520centimeter-level%250Aprecision%252C%2520they%2520are%2520cost-prohibitive%2520and%2520operationally%2520complex.%2520We%2520present%2520a%250Alow-cost%2520alternative%2520that%2520only%2520needs%2520a%2520consumer-grade%2520360%2520video%2520camera.%2520Our%250Asemi-automated%2520pipeline%2520comprises%2520of%2520%2528i%2529%2520a%2520dense%2520point%2520cloud%2520reconstruction%250Ausing%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520photogrammetry%2520software%2520called%2520Agisoft%250AMetashape%252C%2520%2528ii%2529%2520semantic%2520trunk%2520segmentation%2520by%2520projecting%2520Grounded%2520Segment%250AAnything%2520%2528SAM%2529%2520masks%2520onto%2520the%25203D%2520cloud%252C%2520and%2520%2528iii%2529%2520a%2520robust%2520RANSAC-based%250Atechnique%2520to%2520estimate%2520cross%2520section%2520shape%2520and%2520DBH.%2520We%2520introduce%2520an%2520interactive%250Avisualization%2520tool%2520for%2520inspecting%2520segmented%2520trees%2520and%2520their%2520estimated%2520DBH.%2520On%250A61%2520acquisitions%2520of%252043%2520trees%2520under%2520a%2520variety%2520of%2520conditions%252C%2520our%2520method%2520attains%250Amedian%2520absolute%2520relative%2520errors%2520of%25205-9%2525%2520with%2520respect%2520to%2520%2522ground-truth%2522%2520manual%250Ameasurements.%2520This%2520is%2520only%25202-4%2525%2520higher%2520than%2520LiDAR-based%2520estimates%252C%2520while%250Aemploying%2520a%2520single%2520360%2520camera%2520that%2520costs%2520orders%2520of%2520magnitude%2520less%252C%2520requires%250Aminimal%2520setup%252C%2520and%2520is%2520widely%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20Diameter%20at%20Breast%20Height%20of%20Trees%20in%20a%20Forest%20With%20a%0A%20%20Single%20360%20Camera&entry.906535625=Siming%20He%20and%20Zachary%20Osman%20and%20Fernando%20Cladera%20and%20Dexter%20Ong%20and%20Nitant%20Rai%20and%20Patrick%20Corey%20Green%20and%20Vijay%20Kumar%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20Forest%20inventories%20rely%20on%20accurate%20measurements%20of%20the%20diameter%20at%20breast%0Aheight%20%28DBH%29%20for%20ecological%20monitoring%2C%20resource%20management%2C%20and%20carbon%0Aaccounting.%20While%20LiDAR-based%20techniques%20can%20achieve%20centimeter-level%0Aprecision%2C%20they%20are%20cost-prohibitive%20and%20operationally%20complex.%20We%20present%20a%0Alow-cost%20alternative%20that%20only%20needs%20a%20consumer-grade%20360%20video%20camera.%20Our%0Asemi-automated%20pipeline%20comprises%20of%20%28i%29%20a%20dense%20point%20cloud%20reconstruction%0Ausing%20Structure%20from%20Motion%20%28SfM%29%20photogrammetry%20software%20called%20Agisoft%0AMetashape%2C%20%28ii%29%20semantic%20trunk%20segmentation%20by%20projecting%20Grounded%20Segment%0AAnything%20%28SAM%29%20masks%20onto%20the%203D%20cloud%2C%20and%20%28iii%29%20a%20robust%20RANSAC-based%0Atechnique%20to%20estimate%20cross%20section%20shape%20and%20DBH.%20We%20introduce%20an%20interactive%0Avisualization%20tool%20for%20inspecting%20segmented%20trees%20and%20their%20estimated%20DBH.%20On%0A61%20acquisitions%20of%2043%20trees%20under%20a%20variety%20of%20conditions%2C%20our%20method%20attains%0Amedian%20absolute%20relative%20errors%20of%205-9%25%20with%20respect%20to%20%22ground-truth%22%20manual%0Ameasurements.%20This%20is%20only%202-4%25%20higher%20than%20LiDAR-based%20estimates%2C%20while%0Aemploying%20a%20single%20360%20camera%20that%20costs%20orders%20of%20magnitude%20less%2C%20requires%0Aminimal%20setup%2C%20and%20is%20widely%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03093v2&entry.124074799=Read"},
{"title": "Multi-Token Prediction Needs Registers", "author": "Anastasios Gerontopoulos and Spyros Gidaris and Nikos Komodakis", "abstract": "  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n", "link": "http://arxiv.org/abs/2505.10518v1", "date": "2025-05-15", "relevancy": 2.0008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5066}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Token%20Prediction%20Needs%20Registers&body=Title%3A%20Multi-Token%20Prediction%20Needs%20Registers%0AAuthor%3A%20Anastasios%20Gerontopoulos%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Multi-token%20prediction%20has%20emerged%20as%20a%20promising%20objective%20for%20improving%0Alanguage%20model%20pretraining%2C%20but%20its%20benefits%20have%20not%20consistently%20generalized%0Ato%20other%20settings%20such%20as%20fine-tuning.%20In%20this%20paper%2C%20we%20propose%20MuToR%2C%20a%0Asimple%20and%20effective%20approach%20to%20multi-token%20prediction%20that%20interleaves%0Alearnable%20register%20tokens%20into%20the%20input%20sequence%2C%20each%20tasked%20with%20predicting%0Afuture%20targets.%20Compared%20to%20existing%20methods%2C%20MuToR%20offers%20several%20key%0Aadvantages%3A%20it%20introduces%20only%20a%20negligible%20number%20of%20additional%20parameters%2C%0Arequires%20no%20architectural%20changes--ensuring%20compatibility%20with%20off-the-shelf%0Apretrained%20language%20models--and%20remains%20aligned%20with%20the%20next-token%20pretraining%0Aobjective%2C%20making%20it%20especially%20well-suited%20for%20supervised%20fine-tuning.%0AMoreover%2C%20it%20naturally%20supports%20scalable%20prediction%20horizons.%20We%20demonstrate%0Athe%20effectiveness%20and%20versatility%20of%20MuToR%20across%20a%20range%20of%20use%20cases%2C%0Aincluding%20supervised%20fine-tuning%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%2C%20and%0Apretraining%2C%20on%20challenging%20generative%20tasks%20in%20both%20language%20and%20vision%0Adomains.%20Our%20code%20will%20be%20available%20at%3A%20https%3A//github.com/nasosger/MuToR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Token%2520Prediction%2520Needs%2520Registers%26entry.906535625%3DAnastasios%2520Gerontopoulos%2520and%2520Spyros%2520Gidaris%2520and%2520Nikos%2520Komodakis%26entry.1292438233%3D%2520%2520Multi-token%2520prediction%2520has%2520emerged%2520as%2520a%2520promising%2520objective%2520for%2520improving%250Alanguage%2520model%2520pretraining%252C%2520but%2520its%2520benefits%2520have%2520not%2520consistently%2520generalized%250Ato%2520other%2520settings%2520such%2520as%2520fine-tuning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MuToR%252C%2520a%250Asimple%2520and%2520effective%2520approach%2520to%2520multi-token%2520prediction%2520that%2520interleaves%250Alearnable%2520register%2520tokens%2520into%2520the%2520input%2520sequence%252C%2520each%2520tasked%2520with%2520predicting%250Afuture%2520targets.%2520Compared%2520to%2520existing%2520methods%252C%2520MuToR%2520offers%2520several%2520key%250Aadvantages%253A%2520it%2520introduces%2520only%2520a%2520negligible%2520number%2520of%2520additional%2520parameters%252C%250Arequires%2520no%2520architectural%2520changes--ensuring%2520compatibility%2520with%2520off-the-shelf%250Apretrained%2520language%2520models--and%2520remains%2520aligned%2520with%2520the%2520next-token%2520pretraining%250Aobjective%252C%2520making%2520it%2520especially%2520well-suited%2520for%2520supervised%2520fine-tuning.%250AMoreover%252C%2520it%2520naturally%2520supports%2520scalable%2520prediction%2520horizons.%2520We%2520demonstrate%250Athe%2520effectiveness%2520and%2520versatility%2520of%2520MuToR%2520across%2520a%2520range%2520of%2520use%2520cases%252C%250Aincluding%2520supervised%2520fine-tuning%252C%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%252C%2520and%250Apretraining%252C%2520on%2520challenging%2520generative%2520tasks%2520in%2520both%2520language%2520and%2520vision%250Adomains.%2520Our%2520code%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/nasosger/MuToR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Token%20Prediction%20Needs%20Registers&entry.906535625=Anastasios%20Gerontopoulos%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Multi-token%20prediction%20has%20emerged%20as%20a%20promising%20objective%20for%20improving%0Alanguage%20model%20pretraining%2C%20but%20its%20benefits%20have%20not%20consistently%20generalized%0Ato%20other%20settings%20such%20as%20fine-tuning.%20In%20this%20paper%2C%20we%20propose%20MuToR%2C%20a%0Asimple%20and%20effective%20approach%20to%20multi-token%20prediction%20that%20interleaves%0Alearnable%20register%20tokens%20into%20the%20input%20sequence%2C%20each%20tasked%20with%20predicting%0Afuture%20targets.%20Compared%20to%20existing%20methods%2C%20MuToR%20offers%20several%20key%0Aadvantages%3A%20it%20introduces%20only%20a%20negligible%20number%20of%20additional%20parameters%2C%0Arequires%20no%20architectural%20changes--ensuring%20compatibility%20with%20off-the-shelf%0Apretrained%20language%20models--and%20remains%20aligned%20with%20the%20next-token%20pretraining%0Aobjective%2C%20making%20it%20especially%20well-suited%20for%20supervised%20fine-tuning.%0AMoreover%2C%20it%20naturally%20supports%20scalable%20prediction%20horizons.%20We%20demonstrate%0Athe%20effectiveness%20and%20versatility%20of%20MuToR%20across%20a%20range%20of%20use%20cases%2C%0Aincluding%20supervised%20fine-tuning%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%2C%20and%0Apretraining%2C%20on%20challenging%20generative%20tasks%20in%20both%20language%20and%20vision%0Adomains.%20Our%20code%20will%20be%20available%20at%3A%20https%3A//github.com/nasosger/MuToR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10518v1&entry.124074799=Read"},
{"title": "LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean\n  Forecasting", "author": "Qingyu Zheng and Qi Shao and Guijun Han and Wei Li and Hong Li and Xuan Wang", "abstract": "  Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.\n", "link": "http://arxiv.org/abs/2505.10191v1", "date": "2025-05-15", "relevancy": 1.9939, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LanTu%3A%20Dynamics-Enhanced%20Deep%20Learning%20for%20Eddy-Resolving%20Ocean%0A%20%20Forecasting&body=Title%3A%20LanTu%3A%20Dynamics-Enhanced%20Deep%20Learning%20for%20Eddy-Resolving%20Ocean%0A%20%20Forecasting%0AAuthor%3A%20Qingyu%20Zheng%20and%20Qi%20Shao%20and%20Guijun%20Han%20and%20Wei%20Li%20and%20Hong%20Li%20and%20Xuan%20Wang%0AAbstract%3A%20%20%20Mesoscale%20eddies%20dominate%20the%20spatiotemporal%20multiscale%20variability%20of%20the%0Aocean%2C%20and%20their%20impact%20on%20the%20energy%20cascade%20of%20the%20global%20ocean%20cannot%20be%0Aignored.%20Eddy-resolving%20ocean%20forecasting%20is%20providing%20more%20reliable%20protection%0Afor%20fisheries%20and%20navigational%20safety%2C%20but%20also%20presents%20significant%20scientific%0Achallenges%20and%20high%20computational%20costs%20for%20traditional%20numerical%20models.%0AArtificial%20intelligence%20%28AI%29-based%20weather%20and%20ocean%20forecasting%20systems%20are%0Abecoming%20powerful%20tools%20that%20balance%20forecast%20performance%20with%20computational%0Aefficiency.%20However%2C%20the%20complex%20multiscale%20features%20in%20the%20ocean%20dynamical%0Asystem%20make%20AI%20models%20still%20face%20many%20challenges%20in%20mesoscale%20eddy%20forecasting%0A%28especially%20regional%20modelling%29.%20Here%2C%20we%20develop%20LanTu%2C%20a%20regional%0Aeddy-resolving%20ocean%20forecasting%20system%20based%20on%20dynamics-enhanced%20deep%0Alearning.%20We%20incorporate%20cross-scale%20interactions%20into%20LanTu%20and%20construct%0Amultiscale%20physical%20constraint%20for%20optimising%20LanTu%20guided%20by%20knowledge%20of%20eddy%0Adynamics%20in%20order%20to%20improve%20the%20forecasting%20skill%20of%20LanTu%20for%20mesoscale%0Aevolution.%20The%20results%20show%20that%20LanTu%20outperforms%20the%20existing%20advanced%0Aoperational%20numerical%20ocean%20forecasting%20system%20%28NOFS%29%20and%20AI-based%20ocean%0Aforecasting%20system%20%28AI-OFS%29%20in%20temperature%2C%20salinity%2C%20sea%20level%20anomaly%20and%0Acurrent%20prediction%2C%20with%20a%20lead%20time%20of%20more%20than%2010%20days.%20Our%20study%20highlights%0Athat%20dynamics-enhanced%20deep%20learning%20%28LanTu%29%20can%20be%20a%20powerful%20paradigm%20for%0Aeddy-resolving%20ocean%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanTu%253A%2520Dynamics-Enhanced%2520Deep%2520Learning%2520for%2520Eddy-Resolving%2520Ocean%250A%2520%2520Forecasting%26entry.906535625%3DQingyu%2520Zheng%2520and%2520Qi%2520Shao%2520and%2520Guijun%2520Han%2520and%2520Wei%2520Li%2520and%2520Hong%2520Li%2520and%2520Xuan%2520Wang%26entry.1292438233%3D%2520%2520Mesoscale%2520eddies%2520dominate%2520the%2520spatiotemporal%2520multiscale%2520variability%2520of%2520the%250Aocean%252C%2520and%2520their%2520impact%2520on%2520the%2520energy%2520cascade%2520of%2520the%2520global%2520ocean%2520cannot%2520be%250Aignored.%2520Eddy-resolving%2520ocean%2520forecasting%2520is%2520providing%2520more%2520reliable%2520protection%250Afor%2520fisheries%2520and%2520navigational%2520safety%252C%2520but%2520also%2520presents%2520significant%2520scientific%250Achallenges%2520and%2520high%2520computational%2520costs%2520for%2520traditional%2520numerical%2520models.%250AArtificial%2520intelligence%2520%2528AI%2529-based%2520weather%2520and%2520ocean%2520forecasting%2520systems%2520are%250Abecoming%2520powerful%2520tools%2520that%2520balance%2520forecast%2520performance%2520with%2520computational%250Aefficiency.%2520However%252C%2520the%2520complex%2520multiscale%2520features%2520in%2520the%2520ocean%2520dynamical%250Asystem%2520make%2520AI%2520models%2520still%2520face%2520many%2520challenges%2520in%2520mesoscale%2520eddy%2520forecasting%250A%2528especially%2520regional%2520modelling%2529.%2520Here%252C%2520we%2520develop%2520LanTu%252C%2520a%2520regional%250Aeddy-resolving%2520ocean%2520forecasting%2520system%2520based%2520on%2520dynamics-enhanced%2520deep%250Alearning.%2520We%2520incorporate%2520cross-scale%2520interactions%2520into%2520LanTu%2520and%2520construct%250Amultiscale%2520physical%2520constraint%2520for%2520optimising%2520LanTu%2520guided%2520by%2520knowledge%2520of%2520eddy%250Adynamics%2520in%2520order%2520to%2520improve%2520the%2520forecasting%2520skill%2520of%2520LanTu%2520for%2520mesoscale%250Aevolution.%2520The%2520results%2520show%2520that%2520LanTu%2520outperforms%2520the%2520existing%2520advanced%250Aoperational%2520numerical%2520ocean%2520forecasting%2520system%2520%2528NOFS%2529%2520and%2520AI-based%2520ocean%250Aforecasting%2520system%2520%2528AI-OFS%2529%2520in%2520temperature%252C%2520salinity%252C%2520sea%2520level%2520anomaly%2520and%250Acurrent%2520prediction%252C%2520with%2520a%2520lead%2520time%2520of%2520more%2520than%252010%2520days.%2520Our%2520study%2520highlights%250Athat%2520dynamics-enhanced%2520deep%2520learning%2520%2528LanTu%2529%2520can%2520be%2520a%2520powerful%2520paradigm%2520for%250Aeddy-resolving%2520ocean%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LanTu%3A%20Dynamics-Enhanced%20Deep%20Learning%20for%20Eddy-Resolving%20Ocean%0A%20%20Forecasting&entry.906535625=Qingyu%20Zheng%20and%20Qi%20Shao%20and%20Guijun%20Han%20and%20Wei%20Li%20and%20Hong%20Li%20and%20Xuan%20Wang&entry.1292438233=%20%20Mesoscale%20eddies%20dominate%20the%20spatiotemporal%20multiscale%20variability%20of%20the%0Aocean%2C%20and%20their%20impact%20on%20the%20energy%20cascade%20of%20the%20global%20ocean%20cannot%20be%0Aignored.%20Eddy-resolving%20ocean%20forecasting%20is%20providing%20more%20reliable%20protection%0Afor%20fisheries%20and%20navigational%20safety%2C%20but%20also%20presents%20significant%20scientific%0Achallenges%20and%20high%20computational%20costs%20for%20traditional%20numerical%20models.%0AArtificial%20intelligence%20%28AI%29-based%20weather%20and%20ocean%20forecasting%20systems%20are%0Abecoming%20powerful%20tools%20that%20balance%20forecast%20performance%20with%20computational%0Aefficiency.%20However%2C%20the%20complex%20multiscale%20features%20in%20the%20ocean%20dynamical%0Asystem%20make%20AI%20models%20still%20face%20many%20challenges%20in%20mesoscale%20eddy%20forecasting%0A%28especially%20regional%20modelling%29.%20Here%2C%20we%20develop%20LanTu%2C%20a%20regional%0Aeddy-resolving%20ocean%20forecasting%20system%20based%20on%20dynamics-enhanced%20deep%0Alearning.%20We%20incorporate%20cross-scale%20interactions%20into%20LanTu%20and%20construct%0Amultiscale%20physical%20constraint%20for%20optimising%20LanTu%20guided%20by%20knowledge%20of%20eddy%0Adynamics%20in%20order%20to%20improve%20the%20forecasting%20skill%20of%20LanTu%20for%20mesoscale%0Aevolution.%20The%20results%20show%20that%20LanTu%20outperforms%20the%20existing%20advanced%0Aoperational%20numerical%20ocean%20forecasting%20system%20%28NOFS%29%20and%20AI-based%20ocean%0Aforecasting%20system%20%28AI-OFS%29%20in%20temperature%2C%20salinity%2C%20sea%20level%20anomaly%20and%0Acurrent%20prediction%2C%20with%20a%20lead%20time%20of%20more%20than%2010%20days.%20Our%20study%20highlights%0Athat%20dynamics-enhanced%20deep%20learning%20%28LanTu%29%20can%20be%20a%20powerful%20paradigm%20for%0Aeddy-resolving%20ocean%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10191v1&entry.124074799=Read"},
{"title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning", "author": "Yuwei Yin and Giuseppe Carenini", "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.\n", "link": "http://arxiv.org/abs/2502.04689v3", "date": "2025-05-15", "relevancy": 1.982, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning&body=Title%3A%20ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning%0AAuthor%3A%20Yuwei%20Yin%20and%20Giuseppe%20Carenini%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20on%0Acomplex%20evaluation%20benchmarks%2C%20many%20of%20which%20are%20formulated%20as%0Aquestion-answering%20%28QA%29%20tasks.%20Enhancing%20the%20performance%20of%20LLMs%20in%20QA%20contexts%0Ais%20becoming%20increasingly%20vital%20for%20advancing%20their%20development%20and%0Aapplicability.%20This%20paper%20introduces%20ARR%2C%20an%20intuitive%2C%20effective%2C%20and%20general%0AQA%20solving%20method%20that%20explicitly%20incorporates%20three%20key%20steps%3A%20analyzing%20the%0Aintent%20of%20the%20question%2C%20retrieving%20relevant%20information%2C%20and%20reasoning%20step%20by%0Astep.%20Notably%2C%20this%20paper%20is%20the%20first%20to%20introduce%20intent%20analysis%20in%20QA%2C%0Awhich%20plays%20a%20vital%20role%20in%20ARR.%20Comprehensive%20evaluations%20across%2010%20diverse%20QA%0Atasks%20demonstrate%20that%20ARR%20consistently%20outperforms%20the%20baseline%20methods.%0AAblation%20and%20case%20studies%20further%20validate%20the%20positive%20contributions%20of%20each%0AARR%20component.%20Furthermore%2C%20experiments%20involving%20variations%20in%20prompt%20design%0Aindicate%20that%20ARR%20maintains%20its%20effectiveness%20regardless%20of%20the%20specific%20prompt%0Aformulation.%20Additionally%2C%20extensive%20evaluations%20across%20various%20model%20sizes%2C%0ALLM%20series%2C%20and%20generation%20settings%20solidify%20the%20effectiveness%2C%20robustness%2C%20and%0Ageneralizability%20of%20ARR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04689v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARR%253A%2520Question%2520Answering%2520with%2520Large%2520Language%2520Models%2520via%2520Analyzing%252C%250A%2520%2520Retrieving%252C%2520and%2520Reasoning%26entry.906535625%3DYuwei%2520Yin%2520and%2520Giuseppe%2520Carenini%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520on%250Acomplex%2520evaluation%2520benchmarks%252C%2520many%2520of%2520which%2520are%2520formulated%2520as%250Aquestion-answering%2520%2528QA%2529%2520tasks.%2520Enhancing%2520the%2520performance%2520of%2520LLMs%2520in%2520QA%2520contexts%250Ais%2520becoming%2520increasingly%2520vital%2520for%2520advancing%2520their%2520development%2520and%250Aapplicability.%2520This%2520paper%2520introduces%2520ARR%252C%2520an%2520intuitive%252C%2520effective%252C%2520and%2520general%250AQA%2520solving%2520method%2520that%2520explicitly%2520incorporates%2520three%2520key%2520steps%253A%2520analyzing%2520the%250Aintent%2520of%2520the%2520question%252C%2520retrieving%2520relevant%2520information%252C%2520and%2520reasoning%2520step%2520by%250Astep.%2520Notably%252C%2520this%2520paper%2520is%2520the%2520first%2520to%2520introduce%2520intent%2520analysis%2520in%2520QA%252C%250Awhich%2520plays%2520a%2520vital%2520role%2520in%2520ARR.%2520Comprehensive%2520evaluations%2520across%252010%2520diverse%2520QA%250Atasks%2520demonstrate%2520that%2520ARR%2520consistently%2520outperforms%2520the%2520baseline%2520methods.%250AAblation%2520and%2520case%2520studies%2520further%2520validate%2520the%2520positive%2520contributions%2520of%2520each%250AARR%2520component.%2520Furthermore%252C%2520experiments%2520involving%2520variations%2520in%2520prompt%2520design%250Aindicate%2520that%2520ARR%2520maintains%2520its%2520effectiveness%2520regardless%2520of%2520the%2520specific%2520prompt%250Aformulation.%2520Additionally%252C%2520extensive%2520evaluations%2520across%2520various%2520model%2520sizes%252C%250ALLM%2520series%252C%2520and%2520generation%2520settings%2520solidify%2520the%2520effectiveness%252C%2520robustness%252C%2520and%250Ageneralizability%2520of%2520ARR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04689v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARR%3A%20Question%20Answering%20with%20Large%20Language%20Models%20via%20Analyzing%2C%0A%20%20Retrieving%2C%20and%20Reasoning&entry.906535625=Yuwei%20Yin%20and%20Giuseppe%20Carenini&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20on%0Acomplex%20evaluation%20benchmarks%2C%20many%20of%20which%20are%20formulated%20as%0Aquestion-answering%20%28QA%29%20tasks.%20Enhancing%20the%20performance%20of%20LLMs%20in%20QA%20contexts%0Ais%20becoming%20increasingly%20vital%20for%20advancing%20their%20development%20and%0Aapplicability.%20This%20paper%20introduces%20ARR%2C%20an%20intuitive%2C%20effective%2C%20and%20general%0AQA%20solving%20method%20that%20explicitly%20incorporates%20three%20key%20steps%3A%20analyzing%20the%0Aintent%20of%20the%20question%2C%20retrieving%20relevant%20information%2C%20and%20reasoning%20step%20by%0Astep.%20Notably%2C%20this%20paper%20is%20the%20first%20to%20introduce%20intent%20analysis%20in%20QA%2C%0Awhich%20plays%20a%20vital%20role%20in%20ARR.%20Comprehensive%20evaluations%20across%2010%20diverse%20QA%0Atasks%20demonstrate%20that%20ARR%20consistently%20outperforms%20the%20baseline%20methods.%0AAblation%20and%20case%20studies%20further%20validate%20the%20positive%20contributions%20of%20each%0AARR%20component.%20Furthermore%2C%20experiments%20involving%20variations%20in%20prompt%20design%0Aindicate%20that%20ARR%20maintains%20its%20effectiveness%20regardless%20of%20the%20specific%20prompt%0Aformulation.%20Additionally%2C%20extensive%20evaluations%20across%20various%20model%20sizes%2C%0ALLM%20series%2C%20and%20generation%20settings%20solidify%20the%20effectiveness%2C%20robustness%2C%20and%0Ageneralizability%20of%20ARR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04689v3&entry.124074799=Read"},
{"title": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular\n  Likelihoods", "author": "Conor Rosato and Harvinder Lehal and Simon Maskell and Lee Devlin and Malcolm Strens", "abstract": "  Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.\n", "link": "http://arxiv.org/abs/2505.10448v1", "date": "2025-05-15", "relevancy": 1.981, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5015}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20MCMC%20Sampling%20with%20Expensive-to-Compute%20and%20Irregular%0A%20%20Likelihoods&body=Title%3A%20Efficient%20MCMC%20Sampling%20with%20Expensive-to-Compute%20and%20Irregular%0A%20%20Likelihoods%0AAuthor%3A%20Conor%20Rosato%20and%20Harvinder%20Lehal%20and%20Simon%20Maskell%20and%20Lee%20Devlin%20and%20Malcolm%20Strens%0AAbstract%3A%20%20%20Bayesian%20inference%20with%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20is%20challenging%20when%0Athe%20likelihood%20function%20is%20irregular%20and%20expensive%20to%20compute.%20We%20explore%0Aseveral%20sampling%20algorithms%20that%20make%20use%20of%20subset%20evaluations%20to%20reduce%0Acomputational%20overhead.%20We%20adapt%20the%20subset%20samplers%20for%20this%20setting%20where%0Agradient%20information%20is%20not%20available%20or%20is%20unreliable.%20To%20achieve%20this%2C%20we%0Aintroduce%20data-driven%20proxies%20in%20place%20of%20Taylor%20expansions%20and%20define%20a%20novel%0Acomputation-cost%20aware%20adaptive%20controller.%20We%20undertake%20an%20extensive%0Aevaluation%20for%20a%20challenging%20disease%20modelling%20task%20and%20a%20configurable%20task%0Awith%20similar%20irregularity%20in%20the%20likelihood%20surface.%20We%20find%20our%20improved%0Aversion%20of%20Hierarchical%20Importance%20with%20Nested%20Training%20Samples%20%28HINTS%29%2C%20with%0Aadaptive%20proposals%20and%20a%20data-driven%20proxy%2C%20obtains%20the%20best%20sampling%20error%20in%0Aa%20fixed%20computational%20budget.%20We%20conclude%20that%20subset%20evaluations%20can%20provide%0Acheap%20and%20naturally-tempered%20exploration%2C%20while%20a%20data-driven%20proxy%20can%0Apre-screen%20proposals%20successfully%20in%20explored%20regions%20of%20the%20state%20space.%20These%0Atwo%20elements%20combine%20through%20hierarchical%20delayed%20acceptance%20to%20achieve%0Aefficient%2C%20exact%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520MCMC%2520Sampling%2520with%2520Expensive-to-Compute%2520and%2520Irregular%250A%2520%2520Likelihoods%26entry.906535625%3DConor%2520Rosato%2520and%2520Harvinder%2520Lehal%2520and%2520Simon%2520Maskell%2520and%2520Lee%2520Devlin%2520and%2520Malcolm%2520Strens%26entry.1292438233%3D%2520%2520Bayesian%2520inference%2520with%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520is%2520challenging%2520when%250Athe%2520likelihood%2520function%2520is%2520irregular%2520and%2520expensive%2520to%2520compute.%2520We%2520explore%250Aseveral%2520sampling%2520algorithms%2520that%2520make%2520use%2520of%2520subset%2520evaluations%2520to%2520reduce%250Acomputational%2520overhead.%2520We%2520adapt%2520the%2520subset%2520samplers%2520for%2520this%2520setting%2520where%250Agradient%2520information%2520is%2520not%2520available%2520or%2520is%2520unreliable.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520data-driven%2520proxies%2520in%2520place%2520of%2520Taylor%2520expansions%2520and%2520define%2520a%2520novel%250Acomputation-cost%2520aware%2520adaptive%2520controller.%2520We%2520undertake%2520an%2520extensive%250Aevaluation%2520for%2520a%2520challenging%2520disease%2520modelling%2520task%2520and%2520a%2520configurable%2520task%250Awith%2520similar%2520irregularity%2520in%2520the%2520likelihood%2520surface.%2520We%2520find%2520our%2520improved%250Aversion%2520of%2520Hierarchical%2520Importance%2520with%2520Nested%2520Training%2520Samples%2520%2528HINTS%2529%252C%2520with%250Aadaptive%2520proposals%2520and%2520a%2520data-driven%2520proxy%252C%2520obtains%2520the%2520best%2520sampling%2520error%2520in%250Aa%2520fixed%2520computational%2520budget.%2520We%2520conclude%2520that%2520subset%2520evaluations%2520can%2520provide%250Acheap%2520and%2520naturally-tempered%2520exploration%252C%2520while%2520a%2520data-driven%2520proxy%2520can%250Apre-screen%2520proposals%2520successfully%2520in%2520explored%2520regions%2520of%2520the%2520state%2520space.%2520These%250Atwo%2520elements%2520combine%2520through%2520hierarchical%2520delayed%2520acceptance%2520to%2520achieve%250Aefficient%252C%2520exact%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20MCMC%20Sampling%20with%20Expensive-to-Compute%20and%20Irregular%0A%20%20Likelihoods&entry.906535625=Conor%20Rosato%20and%20Harvinder%20Lehal%20and%20Simon%20Maskell%20and%20Lee%20Devlin%20and%20Malcolm%20Strens&entry.1292438233=%20%20Bayesian%20inference%20with%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%20is%20challenging%20when%0Athe%20likelihood%20function%20is%20irregular%20and%20expensive%20to%20compute.%20We%20explore%0Aseveral%20sampling%20algorithms%20that%20make%20use%20of%20subset%20evaluations%20to%20reduce%0Acomputational%20overhead.%20We%20adapt%20the%20subset%20samplers%20for%20this%20setting%20where%0Agradient%20information%20is%20not%20available%20or%20is%20unreliable.%20To%20achieve%20this%2C%20we%0Aintroduce%20data-driven%20proxies%20in%20place%20of%20Taylor%20expansions%20and%20define%20a%20novel%0Acomputation-cost%20aware%20adaptive%20controller.%20We%20undertake%20an%20extensive%0Aevaluation%20for%20a%20challenging%20disease%20modelling%20task%20and%20a%20configurable%20task%0Awith%20similar%20irregularity%20in%20the%20likelihood%20surface.%20We%20find%20our%20improved%0Aversion%20of%20Hierarchical%20Importance%20with%20Nested%20Training%20Samples%20%28HINTS%29%2C%20with%0Aadaptive%20proposals%20and%20a%20data-driven%20proxy%2C%20obtains%20the%20best%20sampling%20error%20in%0Aa%20fixed%20computational%20budget.%20We%20conclude%20that%20subset%20evaluations%20can%20provide%0Acheap%20and%20naturally-tempered%20exploration%2C%20while%20a%20data-driven%20proxy%20can%0Apre-screen%20proposals%20successfully%20in%20explored%20regions%20of%20the%20state%20space.%20These%0Atwo%20elements%20combine%20through%20hierarchical%20delayed%20acceptance%20to%20achieve%0Aefficient%2C%20exact%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10448v1&entry.124074799=Read"},
{"title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure\n  Vehicular Platoons", "author": "Hexu Li and Konstantinos Kalogiannis and Ahmed Mohamed Hussain and Panos Papadimitratos", "abstract": "  Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats.\n", "link": "http://arxiv.org/abs/2505.10273v1", "date": "2025-05-15", "relevancy": 1.9805, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5044}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionGuard%3A%20Transformer-based%20Misbehavior%20Detection%20for%20Secure%0A%20%20Vehicular%20Platoons&body=Title%3A%20AttentionGuard%3A%20Transformer-based%20Misbehavior%20Detection%20for%20Secure%0A%20%20Vehicular%20Platoons%0AAuthor%3A%20Hexu%20Li%20and%20Konstantinos%20Kalogiannis%20and%20Ahmed%20Mohamed%20Hussain%20and%20Panos%20Papadimitratos%0AAbstract%3A%20%20%20Vehicle%20platooning%2C%20with%20vehicles%20traveling%20in%20close%20formation%20coordinated%0Athrough%20Vehicle-to-Everything%20%28V2X%29%20communications%2C%20offers%20significant%20benefits%0Ain%20fuel%20efficiency%20and%20road%20utilization.%20However%2C%20it%20is%20vulnerable%20to%0Asophisticated%20falsification%20attacks%20by%20authenticated%20insiders%20that%20can%0Adestabilize%20the%20formation%20and%20potentially%20cause%20catastrophic%20collisions.%20This%0Apaper%20addresses%20this%20challenge%3A%20misbehavior%20detection%20in%20vehicle%20platooning%0Asystems.%20We%20present%20AttentionGuard%2C%20a%20transformer-based%20framework%20for%0Amisbehavior%20detection%20that%20leverages%20the%20self-attention%20mechanism%20to%20identify%0Aanomalous%20patterns%20in%20mobility%20data.%20Our%20proposal%20employs%20a%20multi-head%0Atransformer-encoder%20to%20process%20sequential%20kinematic%20information%2C%20enabling%0Aeffective%20differentiation%20between%20normal%20mobility%20patterns%20and%20falsification%0Aattacks%20across%20diverse%20platooning%20scenarios%2C%20including%20steady-state%0A%28no-maneuver%29%20operation%2C%20join%2C%20and%20exit%20maneuvers.%20Our%20evaluation%20uses%20an%0Aextensive%20simulation%20dataset%20featuring%20various%20attack%20vectors%20%28constant%2C%0Agradual%2C%20and%20combined%20falsifications%29%20and%20operational%20parameters%20%28controller%0Atypes%2C%20vehicle%20speeds%2C%20and%20attacker%20positions%29.%20Experimental%20results%0Ademonstrate%20that%20AttentionGuard%20achieves%20up%20to%200.95%20F1-score%20in%20attack%0Adetection%2C%20with%20robust%20performance%20maintained%20during%20complex%20maneuvers.%0ANotably%2C%20our%20system%20performs%20effectively%20with%20minimal%20latency%20%28100ms%20decision%0Aintervals%29%2C%20making%20it%20suitable%20for%20real-time%20transportation%20safety%0Aapplications.%20Comparative%20analysis%20reveals%20superior%20detection%20capabilities%20and%0Aestablishes%20the%20transformer-encoder%20as%20a%20promising%20approach%20for%20securing%0ACooperative%20Intelligent%20Transport%20Systems%20%28C-ITS%29%20against%20sophisticated%20insider%0Athreats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionGuard%253A%2520Transformer-based%2520Misbehavior%2520Detection%2520for%2520Secure%250A%2520%2520Vehicular%2520Platoons%26entry.906535625%3DHexu%2520Li%2520and%2520Konstantinos%2520Kalogiannis%2520and%2520Ahmed%2520Mohamed%2520Hussain%2520and%2520Panos%2520Papadimitratos%26entry.1292438233%3D%2520%2520Vehicle%2520platooning%252C%2520with%2520vehicles%2520traveling%2520in%2520close%2520formation%2520coordinated%250Athrough%2520Vehicle-to-Everything%2520%2528V2X%2529%2520communications%252C%2520offers%2520significant%2520benefits%250Ain%2520fuel%2520efficiency%2520and%2520road%2520utilization.%2520However%252C%2520it%2520is%2520vulnerable%2520to%250Asophisticated%2520falsification%2520attacks%2520by%2520authenticated%2520insiders%2520that%2520can%250Adestabilize%2520the%2520formation%2520and%2520potentially%2520cause%2520catastrophic%2520collisions.%2520This%250Apaper%2520addresses%2520this%2520challenge%253A%2520misbehavior%2520detection%2520in%2520vehicle%2520platooning%250Asystems.%2520We%2520present%2520AttentionGuard%252C%2520a%2520transformer-based%2520framework%2520for%250Amisbehavior%2520detection%2520that%2520leverages%2520the%2520self-attention%2520mechanism%2520to%2520identify%250Aanomalous%2520patterns%2520in%2520mobility%2520data.%2520Our%2520proposal%2520employs%2520a%2520multi-head%250Atransformer-encoder%2520to%2520process%2520sequential%2520kinematic%2520information%252C%2520enabling%250Aeffective%2520differentiation%2520between%2520normal%2520mobility%2520patterns%2520and%2520falsification%250Aattacks%2520across%2520diverse%2520platooning%2520scenarios%252C%2520including%2520steady-state%250A%2528no-maneuver%2529%2520operation%252C%2520join%252C%2520and%2520exit%2520maneuvers.%2520Our%2520evaluation%2520uses%2520an%250Aextensive%2520simulation%2520dataset%2520featuring%2520various%2520attack%2520vectors%2520%2528constant%252C%250Agradual%252C%2520and%2520combined%2520falsifications%2529%2520and%2520operational%2520parameters%2520%2528controller%250Atypes%252C%2520vehicle%2520speeds%252C%2520and%2520attacker%2520positions%2529.%2520Experimental%2520results%250Ademonstrate%2520that%2520AttentionGuard%2520achieves%2520up%2520to%25200.95%2520F1-score%2520in%2520attack%250Adetection%252C%2520with%2520robust%2520performance%2520maintained%2520during%2520complex%2520maneuvers.%250ANotably%252C%2520our%2520system%2520performs%2520effectively%2520with%2520minimal%2520latency%2520%2528100ms%2520decision%250Aintervals%2529%252C%2520making%2520it%2520suitable%2520for%2520real-time%2520transportation%2520safety%250Aapplications.%2520Comparative%2520analysis%2520reveals%2520superior%2520detection%2520capabilities%2520and%250Aestablishes%2520the%2520transformer-encoder%2520as%2520a%2520promising%2520approach%2520for%2520securing%250ACooperative%2520Intelligent%2520Transport%2520Systems%2520%2528C-ITS%2529%2520against%2520sophisticated%2520insider%250Athreats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionGuard%3A%20Transformer-based%20Misbehavior%20Detection%20for%20Secure%0A%20%20Vehicular%20Platoons&entry.906535625=Hexu%20Li%20and%20Konstantinos%20Kalogiannis%20and%20Ahmed%20Mohamed%20Hussain%20and%20Panos%20Papadimitratos&entry.1292438233=%20%20Vehicle%20platooning%2C%20with%20vehicles%20traveling%20in%20close%20formation%20coordinated%0Athrough%20Vehicle-to-Everything%20%28V2X%29%20communications%2C%20offers%20significant%20benefits%0Ain%20fuel%20efficiency%20and%20road%20utilization.%20However%2C%20it%20is%20vulnerable%20to%0Asophisticated%20falsification%20attacks%20by%20authenticated%20insiders%20that%20can%0Adestabilize%20the%20formation%20and%20potentially%20cause%20catastrophic%20collisions.%20This%0Apaper%20addresses%20this%20challenge%3A%20misbehavior%20detection%20in%20vehicle%20platooning%0Asystems.%20We%20present%20AttentionGuard%2C%20a%20transformer-based%20framework%20for%0Amisbehavior%20detection%20that%20leverages%20the%20self-attention%20mechanism%20to%20identify%0Aanomalous%20patterns%20in%20mobility%20data.%20Our%20proposal%20employs%20a%20multi-head%0Atransformer-encoder%20to%20process%20sequential%20kinematic%20information%2C%20enabling%0Aeffective%20differentiation%20between%20normal%20mobility%20patterns%20and%20falsification%0Aattacks%20across%20diverse%20platooning%20scenarios%2C%20including%20steady-state%0A%28no-maneuver%29%20operation%2C%20join%2C%20and%20exit%20maneuvers.%20Our%20evaluation%20uses%20an%0Aextensive%20simulation%20dataset%20featuring%20various%20attack%20vectors%20%28constant%2C%0Agradual%2C%20and%20combined%20falsifications%29%20and%20operational%20parameters%20%28controller%0Atypes%2C%20vehicle%20speeds%2C%20and%20attacker%20positions%29.%20Experimental%20results%0Ademonstrate%20that%20AttentionGuard%20achieves%20up%20to%200.95%20F1-score%20in%20attack%0Adetection%2C%20with%20robust%20performance%20maintained%20during%20complex%20maneuvers.%0ANotably%2C%20our%20system%20performs%20effectively%20with%20minimal%20latency%20%28100ms%20decision%0Aintervals%29%2C%20making%20it%20suitable%20for%20real-time%20transportation%20safety%0Aapplications.%20Comparative%20analysis%20reveals%20superior%20detection%20capabilities%20and%0Aestablishes%20the%20transformer-encoder%20as%20a%20promising%20approach%20for%20securing%0ACooperative%20Intelligent%20Transport%20Systems%20%28C-ITS%29%20against%20sophisticated%20insider%0Athreats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10273v1&entry.124074799=Read"},
{"title": "Evaluating Model Explanations without Ground Truth", "author": "Kaivalya Rawal and Zihao Fu and Eoin Delaney and Chris Russell", "abstract": "  There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.\n", "link": "http://arxiv.org/abs/2505.10399v1", "date": "2025-05-15", "relevancy": 1.9741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Model%20Explanations%20without%20Ground%20Truth&body=Title%3A%20Evaluating%20Model%20Explanations%20without%20Ground%20Truth%0AAuthor%3A%20Kaivalya%20Rawal%20and%20Zihao%20Fu%20and%20Eoin%20Delaney%20and%20Chris%20Russell%0AAbstract%3A%20%20%20There%20can%20be%20many%20competing%20and%20contradictory%20explanations%20for%20a%20single%20model%0Aprediction%2C%20making%20it%20difficult%20to%20select%20which%20one%20to%20use.%20Current%20explanation%0Aevaluation%20frameworks%20measure%20quality%20by%20comparing%20against%20ideal%20%22ground-truth%22%0Aexplanations%2C%20or%20by%20verifying%20model%20sensitivity%20to%20important%20inputs.%20We%20outline%0Athe%20limitations%20of%20these%20approaches%2C%20and%20propose%20three%20desirable%20principles%20to%0Aground%20the%20future%20development%20of%20explanation%20evaluation%20strategies%20for%20local%0Afeature%20importance%20explanations.%20We%20propose%20a%20ground-truth%20Agnostic%20eXplanation%0AEvaluation%20framework%20%28AXE%29%20for%20evaluating%20and%20comparing%20model%20explanations%20that%0Asatisfies%20these%20principles.%20Unlike%20prior%20approaches%2C%20AXE%20does%20not%20require%0Aaccess%20to%20ideal%20ground-truth%20explanations%20for%20comparison%2C%20or%20rely%20on%20model%0Asensitivity%20-%20providing%20an%20independent%20measure%20of%20explanation%20quality.%20We%0Averify%20AXE%20by%20comparing%20with%20baselines%2C%20and%20show%20how%20it%20can%20be%20used%20to%20detect%0Aexplanation%20fairwashing.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Model%2520Explanations%2520without%2520Ground%2520Truth%26entry.906535625%3DKaivalya%2520Rawal%2520and%2520Zihao%2520Fu%2520and%2520Eoin%2520Delaney%2520and%2520Chris%2520Russell%26entry.1292438233%3D%2520%2520There%2520can%2520be%2520many%2520competing%2520and%2520contradictory%2520explanations%2520for%2520a%2520single%2520model%250Aprediction%252C%2520making%2520it%2520difficult%2520to%2520select%2520which%2520one%2520to%2520use.%2520Current%2520explanation%250Aevaluation%2520frameworks%2520measure%2520quality%2520by%2520comparing%2520against%2520ideal%2520%2522ground-truth%2522%250Aexplanations%252C%2520or%2520by%2520verifying%2520model%2520sensitivity%2520to%2520important%2520inputs.%2520We%2520outline%250Athe%2520limitations%2520of%2520these%2520approaches%252C%2520and%2520propose%2520three%2520desirable%2520principles%2520to%250Aground%2520the%2520future%2520development%2520of%2520explanation%2520evaluation%2520strategies%2520for%2520local%250Afeature%2520importance%2520explanations.%2520We%2520propose%2520a%2520ground-truth%2520Agnostic%2520eXplanation%250AEvaluation%2520framework%2520%2528AXE%2529%2520for%2520evaluating%2520and%2520comparing%2520model%2520explanations%2520that%250Asatisfies%2520these%2520principles.%2520Unlike%2520prior%2520approaches%252C%2520AXE%2520does%2520not%2520require%250Aaccess%2520to%2520ideal%2520ground-truth%2520explanations%2520for%2520comparison%252C%2520or%2520rely%2520on%2520model%250Asensitivity%2520-%2520providing%2520an%2520independent%2520measure%2520of%2520explanation%2520quality.%2520We%250Averify%2520AXE%2520by%2520comparing%2520with%2520baselines%252C%2520and%2520show%2520how%2520it%2520can%2520be%2520used%2520to%2520detect%250Aexplanation%2520fairwashing.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Model%20Explanations%20without%20Ground%20Truth&entry.906535625=Kaivalya%20Rawal%20and%20Zihao%20Fu%20and%20Eoin%20Delaney%20and%20Chris%20Russell&entry.1292438233=%20%20There%20can%20be%20many%20competing%20and%20contradictory%20explanations%20for%20a%20single%20model%0Aprediction%2C%20making%20it%20difficult%20to%20select%20which%20one%20to%20use.%20Current%20explanation%0Aevaluation%20frameworks%20measure%20quality%20by%20comparing%20against%20ideal%20%22ground-truth%22%0Aexplanations%2C%20or%20by%20verifying%20model%20sensitivity%20to%20important%20inputs.%20We%20outline%0Athe%20limitations%20of%20these%20approaches%2C%20and%20propose%20three%20desirable%20principles%20to%0Aground%20the%20future%20development%20of%20explanation%20evaluation%20strategies%20for%20local%0Afeature%20importance%20explanations.%20We%20propose%20a%20ground-truth%20Agnostic%20eXplanation%0AEvaluation%20framework%20%28AXE%29%20for%20evaluating%20and%20comparing%20model%20explanations%20that%0Asatisfies%20these%20principles.%20Unlike%20prior%20approaches%2C%20AXE%20does%20not%20require%0Aaccess%20to%20ideal%20ground-truth%20explanations%20for%20comparison%2C%20or%20rely%20on%20model%0Asensitivity%20-%20providing%20an%20independent%20measure%20of%20explanation%20quality.%20We%0Averify%20AXE%20by%20comparing%20with%20baselines%2C%20and%20show%20how%20it%20can%20be%20used%20to%20detect%0Aexplanation%20fairwashing.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10399v1&entry.124074799=Read"},
{"title": "A multi-head deep fusion model for recognition of cattle foraging events\n  using sound and movement signals", "author": "Mariano Ferrero and Jos\u00e9 Omar Chelotti and Luciano Sebasti\u00e1n Martinez-Rau and Leandro Vignolo and Mart\u00edn Pires and Julio Ricardo Galli and Leonardo Luis Giovanini and Hugo Leonardo Rufiner", "abstract": "  Monitoring feeding behaviour is a relevant task for efficient herd management\nand the effective use of available resources in grazing cattle. The ability to\nautomatically recognise animals' feeding activities through the identification\nof specific jaw movements allows for the improvement of diet formulation, as\nwell as early detection of metabolic problems and symptoms of animal\ndiscomfort, among other benefits. The use of sensors to obtain signals for such\nmonitoring has become popular in the last two decades. The most frequently\nemployed sensors include accelerometers, microphones, and cameras, each with\nits own set of advantages and drawbacks. An unexplored aspect is the\nsimultaneous use of multiple sensors with the aim of combining signals in order\nto enhance the precision of the estimations. In this direction, this work\nintroduces a deep neural network based on the fusion of acoustic and inertial\nsignals, composed of convolutional, recurrent, and dense layers. The main\nadvantage of this model is the combination of signals through the automatic\nextraction of features independently from each of them. The model has emerged\nfrom an exploration and comparison of different neural network architectures\nproposed in this work, which carry out information fusion at different levels.\nFeature-level fusion has outperformed data and decision-level fusion by at\nleast a 0.14 based on the F1-score metric. Moreover, a comparison with\nstate-of-the-art machine learning methods is presented, including traditional\nand deep learning approaches. The proposed model yielded an F1-score value of\n0.802, representing a 14% increase compared to previous methods. Finally,\nresults from an ablation study and post-training quantization evaluation are\nalso reported.\n", "link": "http://arxiv.org/abs/2505.10198v1", "date": "2025-05-15", "relevancy": 1.9542, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20multi-head%20deep%20fusion%20model%20for%20recognition%20of%20cattle%20foraging%20events%0A%20%20using%20sound%20and%20movement%20signals&body=Title%3A%20A%20multi-head%20deep%20fusion%20model%20for%20recognition%20of%20cattle%20foraging%20events%0A%20%20using%20sound%20and%20movement%20signals%0AAuthor%3A%20Mariano%20Ferrero%20and%20Jos%C3%A9%20Omar%20Chelotti%20and%20Luciano%20Sebasti%C3%A1n%20Martinez-Rau%20and%20Leandro%20Vignolo%20and%20Mart%C3%ADn%20Pires%20and%20Julio%20Ricardo%20Galli%20and%20Leonardo%20Luis%20Giovanini%20and%20Hugo%20Leonardo%20Rufiner%0AAbstract%3A%20%20%20Monitoring%20feeding%20behaviour%20is%20a%20relevant%20task%20for%20efficient%20herd%20management%0Aand%20the%20effective%20use%20of%20available%20resources%20in%20grazing%20cattle.%20The%20ability%20to%0Aautomatically%20recognise%20animals%27%20feeding%20activities%20through%20the%20identification%0Aof%20specific%20jaw%20movements%20allows%20for%20the%20improvement%20of%20diet%20formulation%2C%20as%0Awell%20as%20early%20detection%20of%20metabolic%20problems%20and%20symptoms%20of%20animal%0Adiscomfort%2C%20among%20other%20benefits.%20The%20use%20of%20sensors%20to%20obtain%20signals%20for%20such%0Amonitoring%20has%20become%20popular%20in%20the%20last%20two%20decades.%20The%20most%20frequently%0Aemployed%20sensors%20include%20accelerometers%2C%20microphones%2C%20and%20cameras%2C%20each%20with%0Aits%20own%20set%20of%20advantages%20and%20drawbacks.%20An%20unexplored%20aspect%20is%20the%0Asimultaneous%20use%20of%20multiple%20sensors%20with%20the%20aim%20of%20combining%20signals%20in%20order%0Ato%20enhance%20the%20precision%20of%20the%20estimations.%20In%20this%20direction%2C%20this%20work%0Aintroduces%20a%20deep%20neural%20network%20based%20on%20the%20fusion%20of%20acoustic%20and%20inertial%0Asignals%2C%20composed%20of%20convolutional%2C%20recurrent%2C%20and%20dense%20layers.%20The%20main%0Aadvantage%20of%20this%20model%20is%20the%20combination%20of%20signals%20through%20the%20automatic%0Aextraction%20of%20features%20independently%20from%20each%20of%20them.%20The%20model%20has%20emerged%0Afrom%20an%20exploration%20and%20comparison%20of%20different%20neural%20network%20architectures%0Aproposed%20in%20this%20work%2C%20which%20carry%20out%20information%20fusion%20at%20different%20levels.%0AFeature-level%20fusion%20has%20outperformed%20data%20and%20decision-level%20fusion%20by%20at%0Aleast%20a%200.14%20based%20on%20the%20F1-score%20metric.%20Moreover%2C%20a%20comparison%20with%0Astate-of-the-art%20machine%20learning%20methods%20is%20presented%2C%20including%20traditional%0Aand%20deep%20learning%20approaches.%20The%20proposed%20model%20yielded%20an%20F1-score%20value%20of%0A0.802%2C%20representing%20a%2014%25%20increase%20compared%20to%20previous%20methods.%20Finally%2C%0Aresults%20from%20an%20ablation%20study%20and%20post-training%20quantization%20evaluation%20are%0Aalso%20reported.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520multi-head%2520deep%2520fusion%2520model%2520for%2520recognition%2520of%2520cattle%2520foraging%2520events%250A%2520%2520using%2520sound%2520and%2520movement%2520signals%26entry.906535625%3DMariano%2520Ferrero%2520and%2520Jos%25C3%25A9%2520Omar%2520Chelotti%2520and%2520Luciano%2520Sebasti%25C3%25A1n%2520Martinez-Rau%2520and%2520Leandro%2520Vignolo%2520and%2520Mart%25C3%25ADn%2520Pires%2520and%2520Julio%2520Ricardo%2520Galli%2520and%2520Leonardo%2520Luis%2520Giovanini%2520and%2520Hugo%2520Leonardo%2520Rufiner%26entry.1292438233%3D%2520%2520Monitoring%2520feeding%2520behaviour%2520is%2520a%2520relevant%2520task%2520for%2520efficient%2520herd%2520management%250Aand%2520the%2520effective%2520use%2520of%2520available%2520resources%2520in%2520grazing%2520cattle.%2520The%2520ability%2520to%250Aautomatically%2520recognise%2520animals%2527%2520feeding%2520activities%2520through%2520the%2520identification%250Aof%2520specific%2520jaw%2520movements%2520allows%2520for%2520the%2520improvement%2520of%2520diet%2520formulation%252C%2520as%250Awell%2520as%2520early%2520detection%2520of%2520metabolic%2520problems%2520and%2520symptoms%2520of%2520animal%250Adiscomfort%252C%2520among%2520other%2520benefits.%2520The%2520use%2520of%2520sensors%2520to%2520obtain%2520signals%2520for%2520such%250Amonitoring%2520has%2520become%2520popular%2520in%2520the%2520last%2520two%2520decades.%2520The%2520most%2520frequently%250Aemployed%2520sensors%2520include%2520accelerometers%252C%2520microphones%252C%2520and%2520cameras%252C%2520each%2520with%250Aits%2520own%2520set%2520of%2520advantages%2520and%2520drawbacks.%2520An%2520unexplored%2520aspect%2520is%2520the%250Asimultaneous%2520use%2520of%2520multiple%2520sensors%2520with%2520the%2520aim%2520of%2520combining%2520signals%2520in%2520order%250Ato%2520enhance%2520the%2520precision%2520of%2520the%2520estimations.%2520In%2520this%2520direction%252C%2520this%2520work%250Aintroduces%2520a%2520deep%2520neural%2520network%2520based%2520on%2520the%2520fusion%2520of%2520acoustic%2520and%2520inertial%250Asignals%252C%2520composed%2520of%2520convolutional%252C%2520recurrent%252C%2520and%2520dense%2520layers.%2520The%2520main%250Aadvantage%2520of%2520this%2520model%2520is%2520the%2520combination%2520of%2520signals%2520through%2520the%2520automatic%250Aextraction%2520of%2520features%2520independently%2520from%2520each%2520of%2520them.%2520The%2520model%2520has%2520emerged%250Afrom%2520an%2520exploration%2520and%2520comparison%2520of%2520different%2520neural%2520network%2520architectures%250Aproposed%2520in%2520this%2520work%252C%2520which%2520carry%2520out%2520information%2520fusion%2520at%2520different%2520levels.%250AFeature-level%2520fusion%2520has%2520outperformed%2520data%2520and%2520decision-level%2520fusion%2520by%2520at%250Aleast%2520a%25200.14%2520based%2520on%2520the%2520F1-score%2520metric.%2520Moreover%252C%2520a%2520comparison%2520with%250Astate-of-the-art%2520machine%2520learning%2520methods%2520is%2520presented%252C%2520including%2520traditional%250Aand%2520deep%2520learning%2520approaches.%2520The%2520proposed%2520model%2520yielded%2520an%2520F1-score%2520value%2520of%250A0.802%252C%2520representing%2520a%252014%2525%2520increase%2520compared%2520to%2520previous%2520methods.%2520Finally%252C%250Aresults%2520from%2520an%2520ablation%2520study%2520and%2520post-training%2520quantization%2520evaluation%2520are%250Aalso%2520reported.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20multi-head%20deep%20fusion%20model%20for%20recognition%20of%20cattle%20foraging%20events%0A%20%20using%20sound%20and%20movement%20signals&entry.906535625=Mariano%20Ferrero%20and%20Jos%C3%A9%20Omar%20Chelotti%20and%20Luciano%20Sebasti%C3%A1n%20Martinez-Rau%20and%20Leandro%20Vignolo%20and%20Mart%C3%ADn%20Pires%20and%20Julio%20Ricardo%20Galli%20and%20Leonardo%20Luis%20Giovanini%20and%20Hugo%20Leonardo%20Rufiner&entry.1292438233=%20%20Monitoring%20feeding%20behaviour%20is%20a%20relevant%20task%20for%20efficient%20herd%20management%0Aand%20the%20effective%20use%20of%20available%20resources%20in%20grazing%20cattle.%20The%20ability%20to%0Aautomatically%20recognise%20animals%27%20feeding%20activities%20through%20the%20identification%0Aof%20specific%20jaw%20movements%20allows%20for%20the%20improvement%20of%20diet%20formulation%2C%20as%0Awell%20as%20early%20detection%20of%20metabolic%20problems%20and%20symptoms%20of%20animal%0Adiscomfort%2C%20among%20other%20benefits.%20The%20use%20of%20sensors%20to%20obtain%20signals%20for%20such%0Amonitoring%20has%20become%20popular%20in%20the%20last%20two%20decades.%20The%20most%20frequently%0Aemployed%20sensors%20include%20accelerometers%2C%20microphones%2C%20and%20cameras%2C%20each%20with%0Aits%20own%20set%20of%20advantages%20and%20drawbacks.%20An%20unexplored%20aspect%20is%20the%0Asimultaneous%20use%20of%20multiple%20sensors%20with%20the%20aim%20of%20combining%20signals%20in%20order%0Ato%20enhance%20the%20precision%20of%20the%20estimations.%20In%20this%20direction%2C%20this%20work%0Aintroduces%20a%20deep%20neural%20network%20based%20on%20the%20fusion%20of%20acoustic%20and%20inertial%0Asignals%2C%20composed%20of%20convolutional%2C%20recurrent%2C%20and%20dense%20layers.%20The%20main%0Aadvantage%20of%20this%20model%20is%20the%20combination%20of%20signals%20through%20the%20automatic%0Aextraction%20of%20features%20independently%20from%20each%20of%20them.%20The%20model%20has%20emerged%0Afrom%20an%20exploration%20and%20comparison%20of%20different%20neural%20network%20architectures%0Aproposed%20in%20this%20work%2C%20which%20carry%20out%20information%20fusion%20at%20different%20levels.%0AFeature-level%20fusion%20has%20outperformed%20data%20and%20decision-level%20fusion%20by%20at%0Aleast%20a%200.14%20based%20on%20the%20F1-score%20metric.%20Moreover%2C%20a%20comparison%20with%0Astate-of-the-art%20machine%20learning%20methods%20is%20presented%2C%20including%20traditional%0Aand%20deep%20learning%20approaches.%20The%20proposed%20model%20yielded%20an%20F1-score%20value%20of%0A0.802%2C%20representing%20a%2014%25%20increase%20compared%20to%20previous%20methods.%20Finally%2C%0Aresults%20from%20an%20ablation%20study%20and%20post-training%20quantization%20evaluation%20are%0Aalso%20reported.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10198v1&entry.124074799=Read"},
{"title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs", "author": "Amirmohammad Farzaneh and Osvaldo Simeone", "abstract": "  The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space.\n", "link": "http://arxiv.org/abs/2501.13018v2", "date": "2025-05-15", "relevancy": 0.8928, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4285}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs&body=Title%3A%20Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs%0AAuthor%3A%20Amirmohammad%20Farzaneh%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20The%20selection%20of%20hyperparameters%2C%20such%20as%20prompt%20templates%20in%20large%20language%0Amodels%20%28LLMs%29%2C%20must%20often%20strike%20a%20balance%20between%20reliability%20and%20cost.%20In%0Amany%20cases%2C%20structural%20relationships%20between%20the%20expected%20reliability%20levels%20of%0Athe%20hyperparameters%20can%20be%20inferred%20from%20prior%20information%20and%20held-out%20data%20--%0Ae.g.%2C%20longer%20prompt%20templates%20may%20be%20more%20detailed%20and%20thus%20more%20reliable.%0AHowever%2C%20existing%20hyperparameter%20selection%20methods%20either%20do%20not%20provide%20formal%0Areliability%20guarantees%20or%20are%20unable%20to%20incorporate%20structured%20knowledge%20in%20the%0Ahyperparameter%20space.%20This%20paper%20introduces%20reliability%20graph-based%20Pareto%0Atesting%20%28RG-PT%29%2C%20a%20novel%20multi-objective%20hyperparameter%20selection%20framework%0Athat%20maintains%20formal%20reliability%20guarantees%20in%20terms%20of%20false%20discovery%20rate%0A%28FDR%29%2C%20while%20accounting%20for%20known%20relationships%20among%20hyperparameters%20via%20a%0Adirected%20acyclic%20graph.%20Edges%20in%20the%20graph%20reflect%20expected%20reliability%20and%0Acost%20trade-offs%20among%20hyperparameters%2C%20which%20are%20inferred%20via%20the%20Bradley-Terry%0A%28BT%29%20ranking%20model%20from%20prior%20information%20and%20held-out%20data.%20Experimental%0Aevaluations%20demonstrate%20that%20RG-PT%20significantly%20outperforms%20existing%20methods%0Asuch%20as%20learn-then-test%20%28LTT%29%20and%20Pareto%20testing%20%28PT%29%20through%20a%20more%20efficient%0Aexploration%20of%20the%20hyperparameter%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Objective%2520Hyperparameter%2520Selection%2520via%2520Hypothesis%2520Testing%2520on%250A%2520%2520Reliability%2520Graphs%26entry.906535625%3DAmirmohammad%2520Farzaneh%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520The%2520selection%2520of%2520hyperparameters%252C%2520such%2520as%2520prompt%2520templates%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520must%2520often%2520strike%2520a%2520balance%2520between%2520reliability%2520and%2520cost.%2520In%250Amany%2520cases%252C%2520structural%2520relationships%2520between%2520the%2520expected%2520reliability%2520levels%2520of%250Athe%2520hyperparameters%2520can%2520be%2520inferred%2520from%2520prior%2520information%2520and%2520held-out%2520data%2520--%250Ae.g.%252C%2520longer%2520prompt%2520templates%2520may%2520be%2520more%2520detailed%2520and%2520thus%2520more%2520reliable.%250AHowever%252C%2520existing%2520hyperparameter%2520selection%2520methods%2520either%2520do%2520not%2520provide%2520formal%250Areliability%2520guarantees%2520or%2520are%2520unable%2520to%2520incorporate%2520structured%2520knowledge%2520in%2520the%250Ahyperparameter%2520space.%2520This%2520paper%2520introduces%2520reliability%2520graph-based%2520Pareto%250Atesting%2520%2528RG-PT%2529%252C%2520a%2520novel%2520multi-objective%2520hyperparameter%2520selection%2520framework%250Athat%2520maintains%2520formal%2520reliability%2520guarantees%2520in%2520terms%2520of%2520false%2520discovery%2520rate%250A%2528FDR%2529%252C%2520while%2520accounting%2520for%2520known%2520relationships%2520among%2520hyperparameters%2520via%2520a%250Adirected%2520acyclic%2520graph.%2520Edges%2520in%2520the%2520graph%2520reflect%2520expected%2520reliability%2520and%250Acost%2520trade-offs%2520among%2520hyperparameters%252C%2520which%2520are%2520inferred%2520via%2520the%2520Bradley-Terry%250A%2528BT%2529%2520ranking%2520model%2520from%2520prior%2520information%2520and%2520held-out%2520data.%2520Experimental%250Aevaluations%2520demonstrate%2520that%2520RG-PT%2520significantly%2520outperforms%2520existing%2520methods%250Asuch%2520as%2520learn-then-test%2520%2528LTT%2529%2520and%2520Pareto%2520testing%2520%2528PT%2529%2520through%2520a%2520more%2520efficient%250Aexploration%2520of%2520the%2520hyperparameter%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs&entry.906535625=Amirmohammad%20Farzaneh%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20The%20selection%20of%20hyperparameters%2C%20such%20as%20prompt%20templates%20in%20large%20language%0Amodels%20%28LLMs%29%2C%20must%20often%20strike%20a%20balance%20between%20reliability%20and%20cost.%20In%0Amany%20cases%2C%20structural%20relationships%20between%20the%20expected%20reliability%20levels%20of%0Athe%20hyperparameters%20can%20be%20inferred%20from%20prior%20information%20and%20held-out%20data%20--%0Ae.g.%2C%20longer%20prompt%20templates%20may%20be%20more%20detailed%20and%20thus%20more%20reliable.%0AHowever%2C%20existing%20hyperparameter%20selection%20methods%20either%20do%20not%20provide%20formal%0Areliability%20guarantees%20or%20are%20unable%20to%20incorporate%20structured%20knowledge%20in%20the%0Ahyperparameter%20space.%20This%20paper%20introduces%20reliability%20graph-based%20Pareto%0Atesting%20%28RG-PT%29%2C%20a%20novel%20multi-objective%20hyperparameter%20selection%20framework%0Athat%20maintains%20formal%20reliability%20guarantees%20in%20terms%20of%20false%20discovery%20rate%0A%28FDR%29%2C%20while%20accounting%20for%20known%20relationships%20among%20hyperparameters%20via%20a%0Adirected%20acyclic%20graph.%20Edges%20in%20the%20graph%20reflect%20expected%20reliability%20and%0Acost%20trade-offs%20among%20hyperparameters%2C%20which%20are%20inferred%20via%20the%20Bradley-Terry%0A%28BT%29%20ranking%20model%20from%20prior%20information%20and%20held-out%20data.%20Experimental%0Aevaluations%20demonstrate%20that%20RG-PT%20significantly%20outperforms%20existing%20methods%0Asuch%20as%20learn-then-test%20%28LTT%29%20and%20Pareto%20testing%20%28PT%29%20through%20a%20more%20efficient%0Aexploration%20of%20the%20hyperparameter%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13018v2&entry.124074799=Read"},
{"title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with\n  Morphological Marker Conditioning", "author": "Wenhao Ding and Choon Hwai Yap and Kangjun Ji and Sim\u00e3o Castro", "abstract": "  A generative model for the mesh geometry of intracranial aneurysms (IA) is\ncrucial for training networks to predict blood flow forces in real time, which\nis a key factor affecting disease progression. This need is necessitated by the\nabsence of a large IA image datasets. Existing shape generation methods\nstruggle to capture realistic IA features and ignore the relationship between\nIA pouches and parent vessels, limiting physiological realism and their\ngeneration cannot be controlled to have specific morphological measurements. We\npropose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh\ngenerator. In the first stage, AneuG generates low-dimensional Graph Harmonic\nDeformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,\nconstrained to morphing energy statistics truths. GHD enables more accurate\nshape encoding than alternatives. In the second stage, AneuG generates parent\nvessels conditioned on GHD tokens, by generating vascular centreline and\npropagating the cross-section. AneuG's IA shape generation can further be\nconditioned to have specific clinically relevant morphological measurements.\nThis is useful for studies to understand shape variations represented by\nclinical measurements, and for flow simulation studies to understand effects of\nspecific clinical shape parameters on fluid dynamics. Source code and\nimplementation details are available at\nhttps://github.com/anonymousaneug/AneuG.\n", "link": "http://arxiv.org/abs/2505.10407v1", "date": "2025-05-15", "relevancy": 1.5346, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.53}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4923}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Stage%20Generative%20Model%20for%20Intracranial%20Aneurysm%20Meshes%20with%0A%20%20Morphological%20Marker%20Conditioning&body=Title%3A%20Two-Stage%20Generative%20Model%20for%20Intracranial%20Aneurysm%20Meshes%20with%0A%20%20Morphological%20Marker%20Conditioning%0AAuthor%3A%20Wenhao%20Ding%20and%20Choon%20Hwai%20Yap%20and%20Kangjun%20Ji%20and%20Sim%C3%A3o%20Castro%0AAbstract%3A%20%20%20A%20generative%20model%20for%20the%20mesh%20geometry%20of%20intracranial%20aneurysms%20%28IA%29%20is%0Acrucial%20for%20training%20networks%20to%20predict%20blood%20flow%20forces%20in%20real%20time%2C%20which%0Ais%20a%20key%20factor%20affecting%20disease%20progression.%20This%20need%20is%20necessitated%20by%20the%0Aabsence%20of%20a%20large%20IA%20image%20datasets.%20Existing%20shape%20generation%20methods%0Astruggle%20to%20capture%20realistic%20IA%20features%20and%20ignore%20the%20relationship%20between%0AIA%20pouches%20and%20parent%20vessels%2C%20limiting%20physiological%20realism%20and%20their%0Ageneration%20cannot%20be%20controlled%20to%20have%20specific%20morphological%20measurements.%20We%0Apropose%20AneuG%2C%20a%20two-stage%20Variational%20Autoencoder%20%28VAE%29-based%20IA%20mesh%0Agenerator.%20In%20the%20first%20stage%2C%20AneuG%20generates%20low-dimensional%20Graph%20Harmonic%0ADeformation%20%28GHD%29%20tokens%20to%20encode%20and%20reconstruct%20aneurysm%20pouch%20shapes%2C%0Aconstrained%20to%20morphing%20energy%20statistics%20truths.%20GHD%20enables%20more%20accurate%0Ashape%20encoding%20than%20alternatives.%20In%20the%20second%20stage%2C%20AneuG%20generates%20parent%0Avessels%20conditioned%20on%20GHD%20tokens%2C%20by%20generating%20vascular%20centreline%20and%0Apropagating%20the%20cross-section.%20AneuG%27s%20IA%20shape%20generation%20can%20further%20be%0Aconditioned%20to%20have%20specific%20clinically%20relevant%20morphological%20measurements.%0AThis%20is%20useful%20for%20studies%20to%20understand%20shape%20variations%20represented%20by%0Aclinical%20measurements%2C%20and%20for%20flow%20simulation%20studies%20to%20understand%20effects%20of%0Aspecific%20clinical%20shape%20parameters%20on%20fluid%20dynamics.%20Source%20code%20and%0Aimplementation%20details%20are%20available%20at%0Ahttps%3A//github.com/anonymousaneug/AneuG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Stage%2520Generative%2520Model%2520for%2520Intracranial%2520Aneurysm%2520Meshes%2520with%250A%2520%2520Morphological%2520Marker%2520Conditioning%26entry.906535625%3DWenhao%2520Ding%2520and%2520Choon%2520Hwai%2520Yap%2520and%2520Kangjun%2520Ji%2520and%2520Sim%25C3%25A3o%2520Castro%26entry.1292438233%3D%2520%2520A%2520generative%2520model%2520for%2520the%2520mesh%2520geometry%2520of%2520intracranial%2520aneurysms%2520%2528IA%2529%2520is%250Acrucial%2520for%2520training%2520networks%2520to%2520predict%2520blood%2520flow%2520forces%2520in%2520real%2520time%252C%2520which%250Ais%2520a%2520key%2520factor%2520affecting%2520disease%2520progression.%2520This%2520need%2520is%2520necessitated%2520by%2520the%250Aabsence%2520of%2520a%2520large%2520IA%2520image%2520datasets.%2520Existing%2520shape%2520generation%2520methods%250Astruggle%2520to%2520capture%2520realistic%2520IA%2520features%2520and%2520ignore%2520the%2520relationship%2520between%250AIA%2520pouches%2520and%2520parent%2520vessels%252C%2520limiting%2520physiological%2520realism%2520and%2520their%250Ageneration%2520cannot%2520be%2520controlled%2520to%2520have%2520specific%2520morphological%2520measurements.%2520We%250Apropose%2520AneuG%252C%2520a%2520two-stage%2520Variational%2520Autoencoder%2520%2528VAE%2529-based%2520IA%2520mesh%250Agenerator.%2520In%2520the%2520first%2520stage%252C%2520AneuG%2520generates%2520low-dimensional%2520Graph%2520Harmonic%250ADeformation%2520%2528GHD%2529%2520tokens%2520to%2520encode%2520and%2520reconstruct%2520aneurysm%2520pouch%2520shapes%252C%250Aconstrained%2520to%2520morphing%2520energy%2520statistics%2520truths.%2520GHD%2520enables%2520more%2520accurate%250Ashape%2520encoding%2520than%2520alternatives.%2520In%2520the%2520second%2520stage%252C%2520AneuG%2520generates%2520parent%250Avessels%2520conditioned%2520on%2520GHD%2520tokens%252C%2520by%2520generating%2520vascular%2520centreline%2520and%250Apropagating%2520the%2520cross-section.%2520AneuG%2527s%2520IA%2520shape%2520generation%2520can%2520further%2520be%250Aconditioned%2520to%2520have%2520specific%2520clinically%2520relevant%2520morphological%2520measurements.%250AThis%2520is%2520useful%2520for%2520studies%2520to%2520understand%2520shape%2520variations%2520represented%2520by%250Aclinical%2520measurements%252C%2520and%2520for%2520flow%2520simulation%2520studies%2520to%2520understand%2520effects%2520of%250Aspecific%2520clinical%2520shape%2520parameters%2520on%2520fluid%2520dynamics.%2520Source%2520code%2520and%250Aimplementation%2520details%2520are%2520available%2520at%250Ahttps%253A//github.com/anonymousaneug/AneuG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Stage%20Generative%20Model%20for%20Intracranial%20Aneurysm%20Meshes%20with%0A%20%20Morphological%20Marker%20Conditioning&entry.906535625=Wenhao%20Ding%20and%20Choon%20Hwai%20Yap%20and%20Kangjun%20Ji%20and%20Sim%C3%A3o%20Castro&entry.1292438233=%20%20A%20generative%20model%20for%20the%20mesh%20geometry%20of%20intracranial%20aneurysms%20%28IA%29%20is%0Acrucial%20for%20training%20networks%20to%20predict%20blood%20flow%20forces%20in%20real%20time%2C%20which%0Ais%20a%20key%20factor%20affecting%20disease%20progression.%20This%20need%20is%20necessitated%20by%20the%0Aabsence%20of%20a%20large%20IA%20image%20datasets.%20Existing%20shape%20generation%20methods%0Astruggle%20to%20capture%20realistic%20IA%20features%20and%20ignore%20the%20relationship%20between%0AIA%20pouches%20and%20parent%20vessels%2C%20limiting%20physiological%20realism%20and%20their%0Ageneration%20cannot%20be%20controlled%20to%20have%20specific%20morphological%20measurements.%20We%0Apropose%20AneuG%2C%20a%20two-stage%20Variational%20Autoencoder%20%28VAE%29-based%20IA%20mesh%0Agenerator.%20In%20the%20first%20stage%2C%20AneuG%20generates%20low-dimensional%20Graph%20Harmonic%0ADeformation%20%28GHD%29%20tokens%20to%20encode%20and%20reconstruct%20aneurysm%20pouch%20shapes%2C%0Aconstrained%20to%20morphing%20energy%20statistics%20truths.%20GHD%20enables%20more%20accurate%0Ashape%20encoding%20than%20alternatives.%20In%20the%20second%20stage%2C%20AneuG%20generates%20parent%0Avessels%20conditioned%20on%20GHD%20tokens%2C%20by%20generating%20vascular%20centreline%20and%0Apropagating%20the%20cross-section.%20AneuG%27s%20IA%20shape%20generation%20can%20further%20be%0Aconditioned%20to%20have%20specific%20clinically%20relevant%20morphological%20measurements.%0AThis%20is%20useful%20for%20studies%20to%20understand%20shape%20variations%20represented%20by%0Aclinical%20measurements%2C%20and%20for%20flow%20simulation%20studies%20to%20understand%20effects%20of%0Aspecific%20clinical%20shape%20parameters%20on%20fluid%20dynamics.%20Source%20code%20and%0Aimplementation%20details%20are%20available%20at%0Ahttps%3A//github.com/anonymousaneug/AneuG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10407v1&entry.124074799=Read"},
{"title": "A portable diagnosis model for Keratoconus using a smartphone", "author": "Yifan Li and Peter Ho and Jo Woon Chong", "abstract": "  Keratoconus (KC) is a corneal disorder that results in blurry and distorted\nvision. Traditional diagnostic tools, while effective, are often bulky, costly,\nand require professional operation. In this paper, we present a portable and\ninnovative methodology for diagnosing. Our proposed approach first captures the\nimage reflected on the eye's cornea when a smartphone screen-generated Placido\ndisc sheds its light on an eye, then utilizes a two-stage diagnosis for\nidentifying the KC cornea and pinpointing the location of the KC on the cornea.\nThe first stage estimates the height and width of the Placido disc extracted\nfrom the captured image to identify whether it has KC. In this KC\nidentification, k-means clustering is implemented to discern statistical\ncharacteristics, such as height and width values of extracted Placido discs,\nfrom non-KC (control) and KC-affected groups. The second stage involves the\ncreation of a distance matrix, providing a precise localization of KC on the\ncornea, which is critical for efficient treatment planning. The analysis of\nthese distance matrices, paired with a logistic regression model and robust\nstatistical analysis, reveals a clear distinction between control and KC\ngroups. The logistic regression model, which classifies small areas on the\ncornea as either control or KC-affected based on the corresponding inter-disc\ndistances in the distance matrix, reported a classification accuracy of 96.94%,\nwhich indicates that we can effectively pinpoint the protrusion caused by KC.\nThis comprehensive, smartphone-based method is expected to detect KC and\nstreamline timely treatment.\n", "link": "http://arxiv.org/abs/2505.08616v2", "date": "2025-05-15", "relevancy": 1.869, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4744}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.464}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20portable%20diagnosis%20model%20for%20Keratoconus%20using%20a%20smartphone&body=Title%3A%20A%20portable%20diagnosis%20model%20for%20Keratoconus%20using%20a%20smartphone%0AAuthor%3A%20Yifan%20Li%20and%20Peter%20Ho%20and%20Jo%20Woon%20Chong%0AAbstract%3A%20%20%20Keratoconus%20%28KC%29%20is%20a%20corneal%20disorder%20that%20results%20in%20blurry%20and%20distorted%0Avision.%20Traditional%20diagnostic%20tools%2C%20while%20effective%2C%20are%20often%20bulky%2C%20costly%2C%0Aand%20require%20professional%20operation.%20In%20this%20paper%2C%20we%20present%20a%20portable%20and%0Ainnovative%20methodology%20for%20diagnosing.%20Our%20proposed%20approach%20first%20captures%20the%0Aimage%20reflected%20on%20the%20eye%27s%20cornea%20when%20a%20smartphone%20screen-generated%20Placido%0Adisc%20sheds%20its%20light%20on%20an%20eye%2C%20then%20utilizes%20a%20two-stage%20diagnosis%20for%0Aidentifying%20the%20KC%20cornea%20and%20pinpointing%20the%20location%20of%20the%20KC%20on%20the%20cornea.%0AThe%20first%20stage%20estimates%20the%20height%20and%20width%20of%20the%20Placido%20disc%20extracted%0Afrom%20the%20captured%20image%20to%20identify%20whether%20it%20has%20KC.%20In%20this%20KC%0Aidentification%2C%20k-means%20clustering%20is%20implemented%20to%20discern%20statistical%0Acharacteristics%2C%20such%20as%20height%20and%20width%20values%20of%20extracted%20Placido%20discs%2C%0Afrom%20non-KC%20%28control%29%20and%20KC-affected%20groups.%20The%20second%20stage%20involves%20the%0Acreation%20of%20a%20distance%20matrix%2C%20providing%20a%20precise%20localization%20of%20KC%20on%20the%0Acornea%2C%20which%20is%20critical%20for%20efficient%20treatment%20planning.%20The%20analysis%20of%0Athese%20distance%20matrices%2C%20paired%20with%20a%20logistic%20regression%20model%20and%20robust%0Astatistical%20analysis%2C%20reveals%20a%20clear%20distinction%20between%20control%20and%20KC%0Agroups.%20The%20logistic%20regression%20model%2C%20which%20classifies%20small%20areas%20on%20the%0Acornea%20as%20either%20control%20or%20KC-affected%20based%20on%20the%20corresponding%20inter-disc%0Adistances%20in%20the%20distance%20matrix%2C%20reported%20a%20classification%20accuracy%20of%2096.94%25%2C%0Awhich%20indicates%20that%20we%20can%20effectively%20pinpoint%20the%20protrusion%20caused%20by%20KC.%0AThis%20comprehensive%2C%20smartphone-based%20method%20is%20expected%20to%20detect%20KC%20and%0Astreamline%20timely%20treatment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520portable%2520diagnosis%2520model%2520for%2520Keratoconus%2520using%2520a%2520smartphone%26entry.906535625%3DYifan%2520Li%2520and%2520Peter%2520Ho%2520and%2520Jo%2520Woon%2520Chong%26entry.1292438233%3D%2520%2520Keratoconus%2520%2528KC%2529%2520is%2520a%2520corneal%2520disorder%2520that%2520results%2520in%2520blurry%2520and%2520distorted%250Avision.%2520Traditional%2520diagnostic%2520tools%252C%2520while%2520effective%252C%2520are%2520often%2520bulky%252C%2520costly%252C%250Aand%2520require%2520professional%2520operation.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520portable%2520and%250Ainnovative%2520methodology%2520for%2520diagnosing.%2520Our%2520proposed%2520approach%2520first%2520captures%2520the%250Aimage%2520reflected%2520on%2520the%2520eye%2527s%2520cornea%2520when%2520a%2520smartphone%2520screen-generated%2520Placido%250Adisc%2520sheds%2520its%2520light%2520on%2520an%2520eye%252C%2520then%2520utilizes%2520a%2520two-stage%2520diagnosis%2520for%250Aidentifying%2520the%2520KC%2520cornea%2520and%2520pinpointing%2520the%2520location%2520of%2520the%2520KC%2520on%2520the%2520cornea.%250AThe%2520first%2520stage%2520estimates%2520the%2520height%2520and%2520width%2520of%2520the%2520Placido%2520disc%2520extracted%250Afrom%2520the%2520captured%2520image%2520to%2520identify%2520whether%2520it%2520has%2520KC.%2520In%2520this%2520KC%250Aidentification%252C%2520k-means%2520clustering%2520is%2520implemented%2520to%2520discern%2520statistical%250Acharacteristics%252C%2520such%2520as%2520height%2520and%2520width%2520values%2520of%2520extracted%2520Placido%2520discs%252C%250Afrom%2520non-KC%2520%2528control%2529%2520and%2520KC-affected%2520groups.%2520The%2520second%2520stage%2520involves%2520the%250Acreation%2520of%2520a%2520distance%2520matrix%252C%2520providing%2520a%2520precise%2520localization%2520of%2520KC%2520on%2520the%250Acornea%252C%2520which%2520is%2520critical%2520for%2520efficient%2520treatment%2520planning.%2520The%2520analysis%2520of%250Athese%2520distance%2520matrices%252C%2520paired%2520with%2520a%2520logistic%2520regression%2520model%2520and%2520robust%250Astatistical%2520analysis%252C%2520reveals%2520a%2520clear%2520distinction%2520between%2520control%2520and%2520KC%250Agroups.%2520The%2520logistic%2520regression%2520model%252C%2520which%2520classifies%2520small%2520areas%2520on%2520the%250Acornea%2520as%2520either%2520control%2520or%2520KC-affected%2520based%2520on%2520the%2520corresponding%2520inter-disc%250Adistances%2520in%2520the%2520distance%2520matrix%252C%2520reported%2520a%2520classification%2520accuracy%2520of%252096.94%2525%252C%250Awhich%2520indicates%2520that%2520we%2520can%2520effectively%2520pinpoint%2520the%2520protrusion%2520caused%2520by%2520KC.%250AThis%2520comprehensive%252C%2520smartphone-based%2520method%2520is%2520expected%2520to%2520detect%2520KC%2520and%250Astreamline%2520timely%2520treatment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20portable%20diagnosis%20model%20for%20Keratoconus%20using%20a%20smartphone&entry.906535625=Yifan%20Li%20and%20Peter%20Ho%20and%20Jo%20Woon%20Chong&entry.1292438233=%20%20Keratoconus%20%28KC%29%20is%20a%20corneal%20disorder%20that%20results%20in%20blurry%20and%20distorted%0Avision.%20Traditional%20diagnostic%20tools%2C%20while%20effective%2C%20are%20often%20bulky%2C%20costly%2C%0Aand%20require%20professional%20operation.%20In%20this%20paper%2C%20we%20present%20a%20portable%20and%0Ainnovative%20methodology%20for%20diagnosing.%20Our%20proposed%20approach%20first%20captures%20the%0Aimage%20reflected%20on%20the%20eye%27s%20cornea%20when%20a%20smartphone%20screen-generated%20Placido%0Adisc%20sheds%20its%20light%20on%20an%20eye%2C%20then%20utilizes%20a%20two-stage%20diagnosis%20for%0Aidentifying%20the%20KC%20cornea%20and%20pinpointing%20the%20location%20of%20the%20KC%20on%20the%20cornea.%0AThe%20first%20stage%20estimates%20the%20height%20and%20width%20of%20the%20Placido%20disc%20extracted%0Afrom%20the%20captured%20image%20to%20identify%20whether%20it%20has%20KC.%20In%20this%20KC%0Aidentification%2C%20k-means%20clustering%20is%20implemented%20to%20discern%20statistical%0Acharacteristics%2C%20such%20as%20height%20and%20width%20values%20of%20extracted%20Placido%20discs%2C%0Afrom%20non-KC%20%28control%29%20and%20KC-affected%20groups.%20The%20second%20stage%20involves%20the%0Acreation%20of%20a%20distance%20matrix%2C%20providing%20a%20precise%20localization%20of%20KC%20on%20the%0Acornea%2C%20which%20is%20critical%20for%20efficient%20treatment%20planning.%20The%20analysis%20of%0Athese%20distance%20matrices%2C%20paired%20with%20a%20logistic%20regression%20model%20and%20robust%0Astatistical%20analysis%2C%20reveals%20a%20clear%20distinction%20between%20control%20and%20KC%0Agroups.%20The%20logistic%20regression%20model%2C%20which%20classifies%20small%20areas%20on%20the%0Acornea%20as%20either%20control%20or%20KC-affected%20based%20on%20the%20corresponding%20inter-disc%0Adistances%20in%20the%20distance%20matrix%2C%20reported%20a%20classification%20accuracy%20of%2096.94%25%2C%0Awhich%20indicates%20that%20we%20can%20effectively%20pinpoint%20the%20protrusion%20caused%20by%20KC.%0AThis%20comprehensive%2C%20smartphone-based%20method%20is%20expected%20to%20detect%20KC%20and%0Astreamline%20timely%20treatment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08616v2&entry.124074799=Read"},
{"title": "An AI-driven framework for the prediction of personalised health\n  response to air pollution", "author": "Nazanin Zounemat Kermani and Sadjad Naderi and Claire H. Dilliway and Claire E. Heaney and Shrreya Behll and Boyang Chen and Hisham Abubakar-Waziri and Alexandra E. Porter and Marc Chadeau-Hyam and Fangxin Fang and Ian M. Adcock and Kian Fan Chung and Christopher C. Pain", "abstract": "  Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.\n", "link": "http://arxiv.org/abs/2505.10556v1", "date": "2025-05-15", "relevancy": 0.8947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20AI-driven%20framework%20for%20the%20prediction%20of%20personalised%20health%0A%20%20response%20to%20air%20pollution&body=Title%3A%20An%20AI-driven%20framework%20for%20the%20prediction%20of%20personalised%20health%0A%20%20response%20to%20air%20pollution%0AAuthor%3A%20Nazanin%20Zounemat%20Kermani%20and%20Sadjad%20Naderi%20and%20Claire%20H.%20Dilliway%20and%20Claire%20E.%20Heaney%20and%20Shrreya%20Behll%20and%20Boyang%20Chen%20and%20Hisham%20Abubakar-Waziri%20and%20Alexandra%20E.%20Porter%20and%20Marc%20Chadeau-Hyam%20and%20Fangxin%20Fang%20and%20Ian%20M.%20Adcock%20and%20Kian%20Fan%20Chung%20and%20Christopher%20C.%20Pain%0AAbstract%3A%20%20%20Air%20pollution%20poses%20a%20significant%20threat%20to%20public%20health%2C%20causing%20or%0Aexacerbating%20many%20respiratory%20and%20cardiovascular%20diseases.%20In%20addition%2C%20climate%0Achange%20is%20bringing%20about%20more%20extreme%20weather%20events%20such%20as%20wildfires%20and%0Aheatwaves%2C%20which%20can%20increase%20levels%20of%20pollution%20and%20worsen%20the%20effects%20of%0Apollution%20exposure.%20Recent%20advances%20in%20personal%20sensing%20have%20transformed%20the%0Acollection%20of%20behavioural%20and%20physiological%20data%2C%20leading%20to%20the%20potential%20for%0Anew%20improvements%20in%20healthcare.%20We%20wish%20to%20capitalise%20on%20this%20data%2C%20alongside%0Anew%20capabilities%20in%20AI%20for%20making%20time%20series%20predictions%2C%20in%20order%20to%20monitor%0Aand%20predict%20health%20outcomes%20for%20an%20individual.%20Thus%2C%20we%20present%20a%20novel%0Aworkflow%20for%20predicting%20personalised%20health%20responses%20to%20pollution%20by%0Aintegrating%20physiological%20data%20from%20wearable%20fitness%20devices%20with%20real-time%0Aenvironmental%20exposures.%20The%20data%20is%20collected%20from%20various%20sources%20in%20a%20secure%0Aand%20ethical%20manner%2C%20and%20is%20used%20to%20train%20an%20AI%20model%20to%20predict%20individual%0Ahealth%20responses%20to%20pollution%20exposure%20within%20a%20cloud-based%2C%20modular%20framework.%0AWe%20demonstrate%20that%20the%20AI%20model%20--%20an%20Adversarial%20Autoencoder%20neural%20network%0Ain%20this%20case%20--%20accurately%20reconstructs%20time-dependent%20health%20signals%20and%0Acaptures%20nonlinear%20responses%20to%20pollution.%20Transfer%20learning%20is%20applied%20using%0Adata%20from%20a%20personal%20smartwatch%2C%20which%20increases%20the%20generalisation%20abilities%0Aof%20the%20AI%20model%20and%20illustrates%20the%20adaptability%20of%20the%20approach%20to%20real-world%2C%0Auser-generated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520AI-driven%2520framework%2520for%2520the%2520prediction%2520of%2520personalised%2520health%250A%2520%2520response%2520to%2520air%2520pollution%26entry.906535625%3DNazanin%2520Zounemat%2520Kermani%2520and%2520Sadjad%2520Naderi%2520and%2520Claire%2520H.%2520Dilliway%2520and%2520Claire%2520E.%2520Heaney%2520and%2520Shrreya%2520Behll%2520and%2520Boyang%2520Chen%2520and%2520Hisham%2520Abubakar-Waziri%2520and%2520Alexandra%2520E.%2520Porter%2520and%2520Marc%2520Chadeau-Hyam%2520and%2520Fangxin%2520Fang%2520and%2520Ian%2520M.%2520Adcock%2520and%2520Kian%2520Fan%2520Chung%2520and%2520Christopher%2520C.%2520Pain%26entry.1292438233%3D%2520%2520Air%2520pollution%2520poses%2520a%2520significant%2520threat%2520to%2520public%2520health%252C%2520causing%2520or%250Aexacerbating%2520many%2520respiratory%2520and%2520cardiovascular%2520diseases.%2520In%2520addition%252C%2520climate%250Achange%2520is%2520bringing%2520about%2520more%2520extreme%2520weather%2520events%2520such%2520as%2520wildfires%2520and%250Aheatwaves%252C%2520which%2520can%2520increase%2520levels%2520of%2520pollution%2520and%2520worsen%2520the%2520effects%2520of%250Apollution%2520exposure.%2520Recent%2520advances%2520in%2520personal%2520sensing%2520have%2520transformed%2520the%250Acollection%2520of%2520behavioural%2520and%2520physiological%2520data%252C%2520leading%2520to%2520the%2520potential%2520for%250Anew%2520improvements%2520in%2520healthcare.%2520We%2520wish%2520to%2520capitalise%2520on%2520this%2520data%252C%2520alongside%250Anew%2520capabilities%2520in%2520AI%2520for%2520making%2520time%2520series%2520predictions%252C%2520in%2520order%2520to%2520monitor%250Aand%2520predict%2520health%2520outcomes%2520for%2520an%2520individual.%2520Thus%252C%2520we%2520present%2520a%2520novel%250Aworkflow%2520for%2520predicting%2520personalised%2520health%2520responses%2520to%2520pollution%2520by%250Aintegrating%2520physiological%2520data%2520from%2520wearable%2520fitness%2520devices%2520with%2520real-time%250Aenvironmental%2520exposures.%2520The%2520data%2520is%2520collected%2520from%2520various%2520sources%2520in%2520a%2520secure%250Aand%2520ethical%2520manner%252C%2520and%2520is%2520used%2520to%2520train%2520an%2520AI%2520model%2520to%2520predict%2520individual%250Ahealth%2520responses%2520to%2520pollution%2520exposure%2520within%2520a%2520cloud-based%252C%2520modular%2520framework.%250AWe%2520demonstrate%2520that%2520the%2520AI%2520model%2520--%2520an%2520Adversarial%2520Autoencoder%2520neural%2520network%250Ain%2520this%2520case%2520--%2520accurately%2520reconstructs%2520time-dependent%2520health%2520signals%2520and%250Acaptures%2520nonlinear%2520responses%2520to%2520pollution.%2520Transfer%2520learning%2520is%2520applied%2520using%250Adata%2520from%2520a%2520personal%2520smartwatch%252C%2520which%2520increases%2520the%2520generalisation%2520abilities%250Aof%2520the%2520AI%2520model%2520and%2520illustrates%2520the%2520adaptability%2520of%2520the%2520approach%2520to%2520real-world%252C%250Auser-generated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20AI-driven%20framework%20for%20the%20prediction%20of%20personalised%20health%0A%20%20response%20to%20air%20pollution&entry.906535625=Nazanin%20Zounemat%20Kermani%20and%20Sadjad%20Naderi%20and%20Claire%20H.%20Dilliway%20and%20Claire%20E.%20Heaney%20and%20Shrreya%20Behll%20and%20Boyang%20Chen%20and%20Hisham%20Abubakar-Waziri%20and%20Alexandra%20E.%20Porter%20and%20Marc%20Chadeau-Hyam%20and%20Fangxin%20Fang%20and%20Ian%20M.%20Adcock%20and%20Kian%20Fan%20Chung%20and%20Christopher%20C.%20Pain&entry.1292438233=%20%20Air%20pollution%20poses%20a%20significant%20threat%20to%20public%20health%2C%20causing%20or%0Aexacerbating%20many%20respiratory%20and%20cardiovascular%20diseases.%20In%20addition%2C%20climate%0Achange%20is%20bringing%20about%20more%20extreme%20weather%20events%20such%20as%20wildfires%20and%0Aheatwaves%2C%20which%20can%20increase%20levels%20of%20pollution%20and%20worsen%20the%20effects%20of%0Apollution%20exposure.%20Recent%20advances%20in%20personal%20sensing%20have%20transformed%20the%0Acollection%20of%20behavioural%20and%20physiological%20data%2C%20leading%20to%20the%20potential%20for%0Anew%20improvements%20in%20healthcare.%20We%20wish%20to%20capitalise%20on%20this%20data%2C%20alongside%0Anew%20capabilities%20in%20AI%20for%20making%20time%20series%20predictions%2C%20in%20order%20to%20monitor%0Aand%20predict%20health%20outcomes%20for%20an%20individual.%20Thus%2C%20we%20present%20a%20novel%0Aworkflow%20for%20predicting%20personalised%20health%20responses%20to%20pollution%20by%0Aintegrating%20physiological%20data%20from%20wearable%20fitness%20devices%20with%20real-time%0Aenvironmental%20exposures.%20The%20data%20is%20collected%20from%20various%20sources%20in%20a%20secure%0Aand%20ethical%20manner%2C%20and%20is%20used%20to%20train%20an%20AI%20model%20to%20predict%20individual%0Ahealth%20responses%20to%20pollution%20exposure%20within%20a%20cloud-based%2C%20modular%20framework.%0AWe%20demonstrate%20that%20the%20AI%20model%20--%20an%20Adversarial%20Autoencoder%20neural%20network%0Ain%20this%20case%20--%20accurately%20reconstructs%20time-dependent%20health%20signals%20and%0Acaptures%20nonlinear%20responses%20to%20pollution.%20Transfer%20learning%20is%20applied%20using%0Adata%20from%20a%20personal%20smartwatch%2C%20which%20increases%20the%20generalisation%20abilities%0Aof%20the%20AI%20model%20and%20illustrates%20the%20adaptability%20of%20the%20approach%20to%20real-world%2C%0Auser-generated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10556v1&entry.124074799=Read"},
{"title": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own", "author": "Gokul Puthumanaillam and Melkior Ornik", "abstract": "  This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.\n", "link": "http://arxiv.org/abs/2503.05760v3", "date": "2025-05-15", "relevancy": 1.8382, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4512}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Lazy%20Student%27s%20Dream%3A%20ChatGPT%20Passing%20an%20Engineering%20Course%20on%20Its%0A%20%20Own&body=Title%3A%20The%20Lazy%20Student%27s%20Dream%3A%20ChatGPT%20Passing%20an%20Engineering%20Course%20on%20Its%0A%20%20Own%0AAuthor%3A%20Gokul%20Puthumanaillam%20and%20Melkior%20Ornik%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20investigation%20into%20the%20capability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20successfully%20complete%20a%20semester-long%0Aundergraduate%20control%20systems%20course.%20Through%20evaluation%20of%20115%20course%0Adeliverables%2C%20we%20assess%20LLM%20performance%20using%20ChatGPT%20under%20a%20%22minimal%20effort%22%0Aprotocol%20that%20simulates%20realistic%20student%20usage%20patterns.%20The%20investigation%0Aemploys%20a%20rigorous%20testing%20methodology%20across%20multiple%20assessment%20formats%2C%20from%0Aauto-graded%20multiple%20choice%20questions%20to%20complex%20Python%20programming%20tasks%20and%0Along-form%20analytical%20writing.%20Our%20analysis%20provides%20quantitative%20insights%20into%0AAI%27s%20strengths%20and%20limitations%20in%20handling%20mathematical%20formulations%2C%20coding%0Achallenges%2C%20and%20theoretical%20concepts%20in%20control%20systems%20engineering.%20The%20LLM%0Aachieved%20a%20B-grade%20performance%20%2882.24%5C%25%29%2C%20approaching%20but%20not%20exceeding%20the%0Aclass%20average%20%2884.99%5C%25%29%2C%20with%20strongest%20results%20in%20structured%20assignments%20and%0Agreatest%20limitations%20in%20open-ended%20projects.%20The%20findings%20inform%20discussions%0Aabout%20course%20design%20adaptation%20in%20response%20to%20AI%20advancement%2C%20moving%20beyond%0Asimple%20prohibition%20towards%20thoughtful%20integration%20of%20these%20tools%20in%20engineering%0Aeducation.%20Additional%20materials%20including%20syllabus%2C%20examination%20papers%2C%20design%0Aprojects%2C%20and%20example%20responses%20can%20be%20found%20at%20the%20project%20website%3A%0Ahttps%3A//gradegpt.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05760v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Lazy%2520Student%2527s%2520Dream%253A%2520ChatGPT%2520Passing%2520an%2520Engineering%2520Course%2520on%2520Its%250A%2520%2520Own%26entry.906535625%3DGokul%2520Puthumanaillam%2520and%2520Melkior%2520Ornik%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520investigation%2520into%2520the%2520capability%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520successfully%2520complete%2520a%2520semester-long%250Aundergraduate%2520control%2520systems%2520course.%2520Through%2520evaluation%2520of%2520115%2520course%250Adeliverables%252C%2520we%2520assess%2520LLM%2520performance%2520using%2520ChatGPT%2520under%2520a%2520%2522minimal%2520effort%2522%250Aprotocol%2520that%2520simulates%2520realistic%2520student%2520usage%2520patterns.%2520The%2520investigation%250Aemploys%2520a%2520rigorous%2520testing%2520methodology%2520across%2520multiple%2520assessment%2520formats%252C%2520from%250Aauto-graded%2520multiple%2520choice%2520questions%2520to%2520complex%2520Python%2520programming%2520tasks%2520and%250Along-form%2520analytical%2520writing.%2520Our%2520analysis%2520provides%2520quantitative%2520insights%2520into%250AAI%2527s%2520strengths%2520and%2520limitations%2520in%2520handling%2520mathematical%2520formulations%252C%2520coding%250Achallenges%252C%2520and%2520theoretical%2520concepts%2520in%2520control%2520systems%2520engineering.%2520The%2520LLM%250Aachieved%2520a%2520B-grade%2520performance%2520%252882.24%255C%2525%2529%252C%2520approaching%2520but%2520not%2520exceeding%2520the%250Aclass%2520average%2520%252884.99%255C%2525%2529%252C%2520with%2520strongest%2520results%2520in%2520structured%2520assignments%2520and%250Agreatest%2520limitations%2520in%2520open-ended%2520projects.%2520The%2520findings%2520inform%2520discussions%250Aabout%2520course%2520design%2520adaptation%2520in%2520response%2520to%2520AI%2520advancement%252C%2520moving%2520beyond%250Asimple%2520prohibition%2520towards%2520thoughtful%2520integration%2520of%2520these%2520tools%2520in%2520engineering%250Aeducation.%2520Additional%2520materials%2520including%2520syllabus%252C%2520examination%2520papers%252C%2520design%250Aprojects%252C%2520and%2520example%2520responses%2520can%2520be%2520found%2520at%2520the%2520project%2520website%253A%250Ahttps%253A//gradegpt.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05760v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Lazy%20Student%27s%20Dream%3A%20ChatGPT%20Passing%20an%20Engineering%20Course%20on%20Its%0A%20%20Own&entry.906535625=Gokul%20Puthumanaillam%20and%20Melkior%20Ornik&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20investigation%20into%20the%20capability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20successfully%20complete%20a%20semester-long%0Aundergraduate%20control%20systems%20course.%20Through%20evaluation%20of%20115%20course%0Adeliverables%2C%20we%20assess%20LLM%20performance%20using%20ChatGPT%20under%20a%20%22minimal%20effort%22%0Aprotocol%20that%20simulates%20realistic%20student%20usage%20patterns.%20The%20investigation%0Aemploys%20a%20rigorous%20testing%20methodology%20across%20multiple%20assessment%20formats%2C%20from%0Aauto-graded%20multiple%20choice%20questions%20to%20complex%20Python%20programming%20tasks%20and%0Along-form%20analytical%20writing.%20Our%20analysis%20provides%20quantitative%20insights%20into%0AAI%27s%20strengths%20and%20limitations%20in%20handling%20mathematical%20formulations%2C%20coding%0Achallenges%2C%20and%20theoretical%20concepts%20in%20control%20systems%20engineering.%20The%20LLM%0Aachieved%20a%20B-grade%20performance%20%2882.24%5C%25%29%2C%20approaching%20but%20not%20exceeding%20the%0Aclass%20average%20%2884.99%5C%25%29%2C%20with%20strongest%20results%20in%20structured%20assignments%20and%0Agreatest%20limitations%20in%20open-ended%20projects.%20The%20findings%20inform%20discussions%0Aabout%20course%20design%20adaptation%20in%20response%20to%20AI%20advancement%2C%20moving%20beyond%0Asimple%20prohibition%20towards%20thoughtful%20integration%20of%20these%20tools%20in%20engineering%0Aeducation.%20Additional%20materials%20including%20syllabus%2C%20examination%20papers%2C%20design%0Aprojects%2C%20and%20example%20responses%20can%20be%20found%20at%20the%20project%20website%3A%0Ahttps%3A//gradegpt.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05760v3&entry.124074799=Read"},
{"title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents", "author": "Julius Henke", "abstract": "  A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.\n", "link": "http://arxiv.org/abs/2505.10321v1", "date": "2025-05-15", "relevancy": 1.8204, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4741}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPentest%3A%20Enhancing%20Vulnerability%20Management%20With%20Autonomous%20LLM%0A%20%20Agents&body=Title%3A%20AutoPentest%3A%20Enhancing%20Vulnerability%20Management%20With%20Autonomous%20LLM%0A%20%20Agents%0AAuthor%3A%20Julius%20Henke%0AAbstract%3A%20%20%20A%20recent%20area%20of%20increasing%20research%20is%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%20in%20penetration%20testing%2C%20which%20promises%20to%20reduce%20costs%20and%20thus%20allow%0Afor%20higher%20frequency.%20We%20conduct%20a%20review%20of%20related%20work%2C%20identifying%20best%0Apractices%20and%20common%20evaluation%20issues.%20We%20then%20present%20AutoPentest%2C%20an%0Aapplication%20for%20performing%20black-box%20penetration%20tests%20with%20a%20high%20degree%20of%0Aautonomy.%20AutoPentest%20is%20based%20on%20the%20LLM%20GPT-4o%20from%20OpenAI%20and%20the%20LLM%20agent%0Aframework%20LangChain.%20It%20can%20perform%20complex%20multi-step%20tasks%2C%20augmented%20by%0Aexternal%20tools%20and%20knowledge%20bases.%20We%20conduct%20a%20study%20on%20three%0Acapture-the-flag%20style%20Hack%20The%20Box%20%28HTB%29%20machines%2C%20comparing%20our%0Aimplementation%20AutoPentest%20with%20the%20baseline%20approach%20of%20manually%20using%20the%0AChatGPT-4o%20user%20interface.%20Both%20approaches%20are%20able%20to%20complete%2015-25%20%25%20of%20the%0Asubtasks%20on%20the%20HTB%20machines%2C%20with%20AutoPentest%20slightly%20outperforming%20ChatGPT.%0AWe%20measure%20a%20total%20cost%20of%20%5C%2496.20%20US%20when%20using%20AutoPentest%20across%20all%0Aexperiments%2C%20while%20a%20one-month%20subscription%20to%20ChatGPT%20Plus%20costs%20%5C%2420.%20The%0Aresults%20show%20that%20further%20implementation%20efforts%20and%20the%20use%20of%20more%20powerful%0ALLMs%20released%20in%20the%20future%20are%20likely%20to%20make%20this%20a%20viable%20part%20of%0Avulnerability%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPentest%253A%2520Enhancing%2520Vulnerability%2520Management%2520With%2520Autonomous%2520LLM%250A%2520%2520Agents%26entry.906535625%3DJulius%2520Henke%26entry.1292438233%3D%2520%2520A%2520recent%2520area%2520of%2520increasing%2520research%2520is%2520the%2520use%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520in%2520penetration%2520testing%252C%2520which%2520promises%2520to%2520reduce%2520costs%2520and%2520thus%2520allow%250Afor%2520higher%2520frequency.%2520We%2520conduct%2520a%2520review%2520of%2520related%2520work%252C%2520identifying%2520best%250Apractices%2520and%2520common%2520evaluation%2520issues.%2520We%2520then%2520present%2520AutoPentest%252C%2520an%250Aapplication%2520for%2520performing%2520black-box%2520penetration%2520tests%2520with%2520a%2520high%2520degree%2520of%250Aautonomy.%2520AutoPentest%2520is%2520based%2520on%2520the%2520LLM%2520GPT-4o%2520from%2520OpenAI%2520and%2520the%2520LLM%2520agent%250Aframework%2520LangChain.%2520It%2520can%2520perform%2520complex%2520multi-step%2520tasks%252C%2520augmented%2520by%250Aexternal%2520tools%2520and%2520knowledge%2520bases.%2520We%2520conduct%2520a%2520study%2520on%2520three%250Acapture-the-flag%2520style%2520Hack%2520The%2520Box%2520%2528HTB%2529%2520machines%252C%2520comparing%2520our%250Aimplementation%2520AutoPentest%2520with%2520the%2520baseline%2520approach%2520of%2520manually%2520using%2520the%250AChatGPT-4o%2520user%2520interface.%2520Both%2520approaches%2520are%2520able%2520to%2520complete%252015-25%2520%2525%2520of%2520the%250Asubtasks%2520on%2520the%2520HTB%2520machines%252C%2520with%2520AutoPentest%2520slightly%2520outperforming%2520ChatGPT.%250AWe%2520measure%2520a%2520total%2520cost%2520of%2520%255C%252496.20%2520US%2520when%2520using%2520AutoPentest%2520across%2520all%250Aexperiments%252C%2520while%2520a%2520one-month%2520subscription%2520to%2520ChatGPT%2520Plus%2520costs%2520%255C%252420.%2520The%250Aresults%2520show%2520that%2520further%2520implementation%2520efforts%2520and%2520the%2520use%2520of%2520more%2520powerful%250ALLMs%2520released%2520in%2520the%2520future%2520are%2520likely%2520to%2520make%2520this%2520a%2520viable%2520part%2520of%250Avulnerability%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPentest%3A%20Enhancing%20Vulnerability%20Management%20With%20Autonomous%20LLM%0A%20%20Agents&entry.906535625=Julius%20Henke&entry.1292438233=%20%20A%20recent%20area%20of%20increasing%20research%20is%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%20in%20penetration%20testing%2C%20which%20promises%20to%20reduce%20costs%20and%20thus%20allow%0Afor%20higher%20frequency.%20We%20conduct%20a%20review%20of%20related%20work%2C%20identifying%20best%0Apractices%20and%20common%20evaluation%20issues.%20We%20then%20present%20AutoPentest%2C%20an%0Aapplication%20for%20performing%20black-box%20penetration%20tests%20with%20a%20high%20degree%20of%0Aautonomy.%20AutoPentest%20is%20based%20on%20the%20LLM%20GPT-4o%20from%20OpenAI%20and%20the%20LLM%20agent%0Aframework%20LangChain.%20It%20can%20perform%20complex%20multi-step%20tasks%2C%20augmented%20by%0Aexternal%20tools%20and%20knowledge%20bases.%20We%20conduct%20a%20study%20on%20three%0Acapture-the-flag%20style%20Hack%20The%20Box%20%28HTB%29%20machines%2C%20comparing%20our%0Aimplementation%20AutoPentest%20with%20the%20baseline%20approach%20of%20manually%20using%20the%0AChatGPT-4o%20user%20interface.%20Both%20approaches%20are%20able%20to%20complete%2015-25%20%25%20of%20the%0Asubtasks%20on%20the%20HTB%20machines%2C%20with%20AutoPentest%20slightly%20outperforming%20ChatGPT.%0AWe%20measure%20a%20total%20cost%20of%20%5C%2496.20%20US%20when%20using%20AutoPentest%20across%20all%0Aexperiments%2C%20while%20a%20one-month%20subscription%20to%20ChatGPT%20Plus%20costs%20%5C%2420.%20The%0Aresults%20show%20that%20further%20implementation%20efforts%20and%20the%20use%20of%20more%20powerful%0ALLMs%20released%20in%20the%20future%20are%20likely%20to%20make%20this%20a%20viable%20part%20of%0Avulnerability%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10321v1&entry.124074799=Read"},
{"title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution\n  Generalisation in MRI Segmentation", "author": "Puru Vaish and Felix Meister and Tobias Heimann and Christoph Brune and Jelmer M. Wolterink", "abstract": "  Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.\n", "link": "http://arxiv.org/abs/2505.10223v1", "date": "2025-05-15", "relevancy": 1.647, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5672}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5508}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Agnostic%20Augmentations%20for%20Unknown%20Variations%3A%20Out-of-Distribution%0A%20%20Generalisation%20in%20MRI%20Segmentation&body=Title%3A%20Data-Agnostic%20Augmentations%20for%20Unknown%20Variations%3A%20Out-of-Distribution%0A%20%20Generalisation%20in%20MRI%20Segmentation%0AAuthor%3A%20Puru%20Vaish%20and%20Felix%20Meister%20and%20Tobias%20Heimann%20and%20Christoph%20Brune%20and%20Jelmer%20M.%20Wolterink%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20models%20are%20often%20trained%20on%20curated%20datasets%2C%0Aleading%20to%20performance%20degradation%20when%20deployed%20in%20real-world%20clinical%0Asettings%20due%20to%20mismatches%20between%20training%20and%20test%20distributions.%20While%20data%0Aaugmentation%20techniques%20are%20widely%20used%20to%20address%20these%20challenges%2C%0Atraditional%20visually%20consistent%20augmentation%20strategies%20lack%20the%20robustness%0Aneeded%20for%20diverse%20real-world%20scenarios.%20In%20this%20work%2C%20we%20systematically%0Aevaluate%20alternative%20augmentation%20strategies%2C%20focusing%20on%20MixUp%20and%20Auxiliary%0AFourier%20Augmentation.%20These%20methods%20mitigate%20the%20effects%20of%20multiple%20variations%0Awithout%20explicitly%20targeting%20specific%20sources%20of%20distribution%20shifts.%20We%0Ademonstrate%20how%20these%20techniques%20significantly%20improve%20out-of-distribution%0Ageneralization%20and%20robustness%20to%20imaging%20variations%20across%20a%20wide%20range%20of%0Atransformations%20in%20cardiac%20cine%20MRI%20and%20prostate%20MRI%20segmentation.%20We%0Aquantitatively%20find%20that%20these%20augmentation%20methods%20enhance%20learned%20feature%0Arepresentations%20by%20promoting%20separability%20and%20compactness.%20Additionally%2C%20we%0Ahighlight%20how%20their%20integration%20into%20nnU-Net%20training%20pipelines%20provides%20an%0Aeasy-to-implement%2C%20effective%20solution%20for%20enhancing%20the%20reliability%20of%20medical%0Asegmentation%20models%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Agnostic%2520Augmentations%2520for%2520Unknown%2520Variations%253A%2520Out-of-Distribution%250A%2520%2520Generalisation%2520in%2520MRI%2520Segmentation%26entry.906535625%3DPuru%2520Vaish%2520and%2520Felix%2520Meister%2520and%2520Tobias%2520Heimann%2520and%2520Christoph%2520Brune%2520and%2520Jelmer%2520M.%2520Wolterink%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520models%2520are%2520often%2520trained%2520on%2520curated%2520datasets%252C%250Aleading%2520to%2520performance%2520degradation%2520when%2520deployed%2520in%2520real-world%2520clinical%250Asettings%2520due%2520to%2520mismatches%2520between%2520training%2520and%2520test%2520distributions.%2520While%2520data%250Aaugmentation%2520techniques%2520are%2520widely%2520used%2520to%2520address%2520these%2520challenges%252C%250Atraditional%2520visually%2520consistent%2520augmentation%2520strategies%2520lack%2520the%2520robustness%250Aneeded%2520for%2520diverse%2520real-world%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520systematically%250Aevaluate%2520alternative%2520augmentation%2520strategies%252C%2520focusing%2520on%2520MixUp%2520and%2520Auxiliary%250AFourier%2520Augmentation.%2520These%2520methods%2520mitigate%2520the%2520effects%2520of%2520multiple%2520variations%250Awithout%2520explicitly%2520targeting%2520specific%2520sources%2520of%2520distribution%2520shifts.%2520We%250Ademonstrate%2520how%2520these%2520techniques%2520significantly%2520improve%2520out-of-distribution%250Ageneralization%2520and%2520robustness%2520to%2520imaging%2520variations%2520across%2520a%2520wide%2520range%2520of%250Atransformations%2520in%2520cardiac%2520cine%2520MRI%2520and%2520prostate%2520MRI%2520segmentation.%2520We%250Aquantitatively%2520find%2520that%2520these%2520augmentation%2520methods%2520enhance%2520learned%2520feature%250Arepresentations%2520by%2520promoting%2520separability%2520and%2520compactness.%2520Additionally%252C%2520we%250Ahighlight%2520how%2520their%2520integration%2520into%2520nnU-Net%2520training%2520pipelines%2520provides%2520an%250Aeasy-to-implement%252C%2520effective%2520solution%2520for%2520enhancing%2520the%2520reliability%2520of%2520medical%250Asegmentation%2520models%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Agnostic%20Augmentations%20for%20Unknown%20Variations%3A%20Out-of-Distribution%0A%20%20Generalisation%20in%20MRI%20Segmentation&entry.906535625=Puru%20Vaish%20and%20Felix%20Meister%20and%20Tobias%20Heimann%20and%20Christoph%20Brune%20and%20Jelmer%20M.%20Wolterink&entry.1292438233=%20%20Medical%20image%20segmentation%20models%20are%20often%20trained%20on%20curated%20datasets%2C%0Aleading%20to%20performance%20degradation%20when%20deployed%20in%20real-world%20clinical%0Asettings%20due%20to%20mismatches%20between%20training%20and%20test%20distributions.%20While%20data%0Aaugmentation%20techniques%20are%20widely%20used%20to%20address%20these%20challenges%2C%0Atraditional%20visually%20consistent%20augmentation%20strategies%20lack%20the%20robustness%0Aneeded%20for%20diverse%20real-world%20scenarios.%20In%20this%20work%2C%20we%20systematically%0Aevaluate%20alternative%20augmentation%20strategies%2C%20focusing%20on%20MixUp%20and%20Auxiliary%0AFourier%20Augmentation.%20These%20methods%20mitigate%20the%20effects%20of%20multiple%20variations%0Awithout%20explicitly%20targeting%20specific%20sources%20of%20distribution%20shifts.%20We%0Ademonstrate%20how%20these%20techniques%20significantly%20improve%20out-of-distribution%0Ageneralization%20and%20robustness%20to%20imaging%20variations%20across%20a%20wide%20range%20of%0Atransformations%20in%20cardiac%20cine%20MRI%20and%20prostate%20MRI%20segmentation.%20We%0Aquantitatively%20find%20that%20these%20augmentation%20methods%20enhance%20learned%20feature%0Arepresentations%20by%20promoting%20separability%20and%20compactness.%20Additionally%2C%20we%0Ahighlight%20how%20their%20integration%20into%20nnU-Net%20training%20pipelines%20provides%20an%0Aeasy-to-implement%2C%20effective%20solution%20for%20enhancing%20the%20reliability%20of%20medical%0Asegmentation%20models%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10223v1&entry.124074799=Read"},
{"title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data", "author": "Poli Apollinaire Nemkova and Solomon Ubani and Mark V. Albert", "abstract": "  In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.\n", "link": "http://arxiv.org/abs/2505.10260v1", "date": "2025-05-15", "relevancy": 1.9634, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20LLM%20Text%20Annotation%20Skills%3A%20A%20Study%20on%20Human%20Rights%20Violations%0A%20%20in%20Social%20Media%20Data&body=Title%3A%20Comparing%20LLM%20Text%20Annotation%20Skills%3A%20A%20Study%20on%20Human%20Rights%20Violations%0A%20%20in%20Social%20Media%20Data%0AAuthor%3A%20Poli%20Apollinaire%20Nemkova%20and%20Solomon%20Ubani%20and%20Mark%20V.%20Albert%0AAbstract%3A%20%20%20In%20the%20era%20of%20increasingly%20sophisticated%20natural%20language%20processing%20%28NLP%29%0Asystems%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%0Afor%20diverse%20applications%2C%20including%20tasks%20requiring%20nuanced%20textual%0Aunderstanding%20and%20contextual%20reasoning.%20This%20study%20investigates%20the%0Acapabilities%20of%20multiple%20state-of-the-art%20LLMs%20-%20GPT-3.5%2C%20GPT-4%2C%20LLAMA3%2C%0AMistral%207B%2C%20and%20Claude-2%20-%20for%20zero-shot%20and%20few-shot%20annotation%20of%20a%20complex%0Atextual%20dataset%20comprising%20social%20media%20posts%20in%20Russian%20and%20Ukrainian.%0ASpecifically%2C%20the%20focus%20is%20on%20the%20binary%20classification%20task%20of%20identifying%0Areferences%20to%20human%20rights%20violations%20within%20the%20dataset.%0A%20%20To%20evaluate%20the%20effectiveness%20of%20these%20models%2C%20their%20annotations%20are%20compared%0Aagainst%20a%20gold%20standard%20set%20of%20human%20double-annotated%20labels%20across%201000%0Asamples.%20The%20analysis%20includes%20assessing%20annotation%20performance%20under%20different%0Aprompting%20conditions%2C%20with%20prompts%20provided%20in%20both%20English%20and%20Russian.%0AAdditionally%2C%20the%20study%20explores%20the%20unique%20patterns%20of%20errors%20and%0Adisagreements%20exhibited%20by%20each%20model%2C%20offering%20insights%20into%20their%20strengths%2C%0Alimitations%2C%20and%20cross-linguistic%20adaptability.%0A%20%20By%20juxtaposing%20LLM%20outputs%20with%20human%20annotations%2C%20this%20research%20contributes%0Ato%20understanding%20the%20reliability%20and%20applicability%20of%20LLMs%20for%20sensitive%2C%0Adomain-specific%20tasks%20in%20multilingual%20contexts.%20It%20also%20sheds%20light%20on%20how%0Alanguage%20models%20handle%20inherently%20subjective%20and%20context-dependent%20judgments%2C%20a%0Acritical%20consideration%20for%20their%20deployment%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520LLM%2520Text%2520Annotation%2520Skills%253A%2520A%2520Study%2520on%2520Human%2520Rights%2520Violations%250A%2520%2520in%2520Social%2520Media%2520Data%26entry.906535625%3DPoli%2520Apollinaire%2520Nemkova%2520and%2520Solomon%2520Ubani%2520and%2520Mark%2520V.%2520Albert%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520increasingly%2520sophisticated%2520natural%2520language%2520processing%2520%2528NLP%2529%250Asystems%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%250Afor%2520diverse%2520applications%252C%2520including%2520tasks%2520requiring%2520nuanced%2520textual%250Aunderstanding%2520and%2520contextual%2520reasoning.%2520This%2520study%2520investigates%2520the%250Acapabilities%2520of%2520multiple%2520state-of-the-art%2520LLMs%2520-%2520GPT-3.5%252C%2520GPT-4%252C%2520LLAMA3%252C%250AMistral%25207B%252C%2520and%2520Claude-2%2520-%2520for%2520zero-shot%2520and%2520few-shot%2520annotation%2520of%2520a%2520complex%250Atextual%2520dataset%2520comprising%2520social%2520media%2520posts%2520in%2520Russian%2520and%2520Ukrainian.%250ASpecifically%252C%2520the%2520focus%2520is%2520on%2520the%2520binary%2520classification%2520task%2520of%2520identifying%250Areferences%2520to%2520human%2520rights%2520violations%2520within%2520the%2520dataset.%250A%2520%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520these%2520models%252C%2520their%2520annotations%2520are%2520compared%250Aagainst%2520a%2520gold%2520standard%2520set%2520of%2520human%2520double-annotated%2520labels%2520across%25201000%250Asamples.%2520The%2520analysis%2520includes%2520assessing%2520annotation%2520performance%2520under%2520different%250Aprompting%2520conditions%252C%2520with%2520prompts%2520provided%2520in%2520both%2520English%2520and%2520Russian.%250AAdditionally%252C%2520the%2520study%2520explores%2520the%2520unique%2520patterns%2520of%2520errors%2520and%250Adisagreements%2520exhibited%2520by%2520each%2520model%252C%2520offering%2520insights%2520into%2520their%2520strengths%252C%250Alimitations%252C%2520and%2520cross-linguistic%2520adaptability.%250A%2520%2520By%2520juxtaposing%2520LLM%2520outputs%2520with%2520human%2520annotations%252C%2520this%2520research%2520contributes%250Ato%2520understanding%2520the%2520reliability%2520and%2520applicability%2520of%2520LLMs%2520for%2520sensitive%252C%250Adomain-specific%2520tasks%2520in%2520multilingual%2520contexts.%2520It%2520also%2520sheds%2520light%2520on%2520how%250Alanguage%2520models%2520handle%2520inherently%2520subjective%2520and%2520context-dependent%2520judgments%252C%2520a%250Acritical%2520consideration%2520for%2520their%2520deployment%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20LLM%20Text%20Annotation%20Skills%3A%20A%20Study%20on%20Human%20Rights%20Violations%0A%20%20in%20Social%20Media%20Data&entry.906535625=Poli%20Apollinaire%20Nemkova%20and%20Solomon%20Ubani%20and%20Mark%20V.%20Albert&entry.1292438233=%20%20In%20the%20era%20of%20increasingly%20sophisticated%20natural%20language%20processing%20%28NLP%29%0Asystems%2C%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%0Afor%20diverse%20applications%2C%20including%20tasks%20requiring%20nuanced%20textual%0Aunderstanding%20and%20contextual%20reasoning.%20This%20study%20investigates%20the%0Acapabilities%20of%20multiple%20state-of-the-art%20LLMs%20-%20GPT-3.5%2C%20GPT-4%2C%20LLAMA3%2C%0AMistral%207B%2C%20and%20Claude-2%20-%20for%20zero-shot%20and%20few-shot%20annotation%20of%20a%20complex%0Atextual%20dataset%20comprising%20social%20media%20posts%20in%20Russian%20and%20Ukrainian.%0ASpecifically%2C%20the%20focus%20is%20on%20the%20binary%20classification%20task%20of%20identifying%0Areferences%20to%20human%20rights%20violations%20within%20the%20dataset.%0A%20%20To%20evaluate%20the%20effectiveness%20of%20these%20models%2C%20their%20annotations%20are%20compared%0Aagainst%20a%20gold%20standard%20set%20of%20human%20double-annotated%20labels%20across%201000%0Asamples.%20The%20analysis%20includes%20assessing%20annotation%20performance%20under%20different%0Aprompting%20conditions%2C%20with%20prompts%20provided%20in%20both%20English%20and%20Russian.%0AAdditionally%2C%20the%20study%20explores%20the%20unique%20patterns%20of%20errors%20and%0Adisagreements%20exhibited%20by%20each%20model%2C%20offering%20insights%20into%20their%20strengths%2C%0Alimitations%2C%20and%20cross-linguistic%20adaptability.%0A%20%20By%20juxtaposing%20LLM%20outputs%20with%20human%20annotations%2C%20this%20research%20contributes%0Ato%20understanding%20the%20reliability%20and%20applicability%20of%20LLMs%20for%20sensitive%2C%0Adomain-specific%20tasks%20in%20multilingual%20contexts.%20It%20also%20sheds%20light%20on%20how%0Alanguage%20models%20handle%20inherently%20subjective%20and%20context-dependent%20judgments%2C%20a%0Acritical%20consideration%20for%20their%20deployment%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10260v1&entry.124074799=Read"},
{"title": "Force-Driven Validation for Collaborative Robotics in Automated Avionics\n  Testing", "author": "Pietro Dardano and Paolo Rocco and David Frisini", "abstract": "  ARTO is a project combining collaborative robots (cobots) and Artificial\nIntelligence (AI) to automate functional test procedures for civilian and\nmilitary aircraft certification. This paper proposes a Deep Learning (DL) and\neXplainable AI (XAI) approach, equipping ARTO with interaction analysis\ncapabilities to verify and validate the operations on cockpit components.\nDuring these interactions, forces, torques, and end effector poses are recorded\nand preprocessed to filter disturbances caused by low performance force\ncontrollers and embedded Force Torque Sensors (FTS). Convolutional Neural\nNetworks (CNNs) then classify the cobot actions as Success or Fail, while also\nidentifying and reporting the causes of failure. To improve interpretability,\nGrad CAM, an XAI technique for visual explanations, is integrated to provide\ninsights into the models decision making process. This approach enhances the\nreliability and trustworthiness of the automated testing system, facilitating\nthe diagnosis and rectification of errors that may arise during testing.\n", "link": "http://arxiv.org/abs/2505.10224v1", "date": "2025-05-15", "relevancy": 1.5257, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5031}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Force-Driven%20Validation%20for%20Collaborative%20Robotics%20in%20Automated%20Avionics%0A%20%20Testing&body=Title%3A%20Force-Driven%20Validation%20for%20Collaborative%20Robotics%20in%20Automated%20Avionics%0A%20%20Testing%0AAuthor%3A%20Pietro%20Dardano%20and%20Paolo%20Rocco%20and%20David%20Frisini%0AAbstract%3A%20%20%20ARTO%20is%20a%20project%20combining%20collaborative%20robots%20%28cobots%29%20and%20Artificial%0AIntelligence%20%28AI%29%20to%20automate%20functional%20test%20procedures%20for%20civilian%20and%0Amilitary%20aircraft%20certification.%20This%20paper%20proposes%20a%20Deep%20Learning%20%28DL%29%20and%0AeXplainable%20AI%20%28XAI%29%20approach%2C%20equipping%20ARTO%20with%20interaction%20analysis%0Acapabilities%20to%20verify%20and%20validate%20the%20operations%20on%20cockpit%20components.%0ADuring%20these%20interactions%2C%20forces%2C%20torques%2C%20and%20end%20effector%20poses%20are%20recorded%0Aand%20preprocessed%20to%20filter%20disturbances%20caused%20by%20low%20performance%20force%0Acontrollers%20and%20embedded%20Force%20Torque%20Sensors%20%28FTS%29.%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20then%20classify%20the%20cobot%20actions%20as%20Success%20or%20Fail%2C%20while%20also%0Aidentifying%20and%20reporting%20the%20causes%20of%20failure.%20To%20improve%20interpretability%2C%0AGrad%20CAM%2C%20an%20XAI%20technique%20for%20visual%20explanations%2C%20is%20integrated%20to%20provide%0Ainsights%20into%20the%20models%20decision%20making%20process.%20This%20approach%20enhances%20the%0Areliability%20and%20trustworthiness%20of%20the%20automated%20testing%20system%2C%20facilitating%0Athe%20diagnosis%20and%20rectification%20of%20errors%20that%20may%20arise%20during%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForce-Driven%2520Validation%2520for%2520Collaborative%2520Robotics%2520in%2520Automated%2520Avionics%250A%2520%2520Testing%26entry.906535625%3DPietro%2520Dardano%2520and%2520Paolo%2520Rocco%2520and%2520David%2520Frisini%26entry.1292438233%3D%2520%2520ARTO%2520is%2520a%2520project%2520combining%2520collaborative%2520robots%2520%2528cobots%2529%2520and%2520Artificial%250AIntelligence%2520%2528AI%2529%2520to%2520automate%2520functional%2520test%2520procedures%2520for%2520civilian%2520and%250Amilitary%2520aircraft%2520certification.%2520This%2520paper%2520proposes%2520a%2520Deep%2520Learning%2520%2528DL%2529%2520and%250AeXplainable%2520AI%2520%2528XAI%2529%2520approach%252C%2520equipping%2520ARTO%2520with%2520interaction%2520analysis%250Acapabilities%2520to%2520verify%2520and%2520validate%2520the%2520operations%2520on%2520cockpit%2520components.%250ADuring%2520these%2520interactions%252C%2520forces%252C%2520torques%252C%2520and%2520end%2520effector%2520poses%2520are%2520recorded%250Aand%2520preprocessed%2520to%2520filter%2520disturbances%2520caused%2520by%2520low%2520performance%2520force%250Acontrollers%2520and%2520embedded%2520Force%2520Torque%2520Sensors%2520%2528FTS%2529.%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520then%2520classify%2520the%2520cobot%2520actions%2520as%2520Success%2520or%2520Fail%252C%2520while%2520also%250Aidentifying%2520and%2520reporting%2520the%2520causes%2520of%2520failure.%2520To%2520improve%2520interpretability%252C%250AGrad%2520CAM%252C%2520an%2520XAI%2520technique%2520for%2520visual%2520explanations%252C%2520is%2520integrated%2520to%2520provide%250Ainsights%2520into%2520the%2520models%2520decision%2520making%2520process.%2520This%2520approach%2520enhances%2520the%250Areliability%2520and%2520trustworthiness%2520of%2520the%2520automated%2520testing%2520system%252C%2520facilitating%250Athe%2520diagnosis%2520and%2520rectification%2520of%2520errors%2520that%2520may%2520arise%2520during%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Force-Driven%20Validation%20for%20Collaborative%20Robotics%20in%20Automated%20Avionics%0A%20%20Testing&entry.906535625=Pietro%20Dardano%20and%20Paolo%20Rocco%20and%20David%20Frisini&entry.1292438233=%20%20ARTO%20is%20a%20project%20combining%20collaborative%20robots%20%28cobots%29%20and%20Artificial%0AIntelligence%20%28AI%29%20to%20automate%20functional%20test%20procedures%20for%20civilian%20and%0Amilitary%20aircraft%20certification.%20This%20paper%20proposes%20a%20Deep%20Learning%20%28DL%29%20and%0AeXplainable%20AI%20%28XAI%29%20approach%2C%20equipping%20ARTO%20with%20interaction%20analysis%0Acapabilities%20to%20verify%20and%20validate%20the%20operations%20on%20cockpit%20components.%0ADuring%20these%20interactions%2C%20forces%2C%20torques%2C%20and%20end%20effector%20poses%20are%20recorded%0Aand%20preprocessed%20to%20filter%20disturbances%20caused%20by%20low%20performance%20force%0Acontrollers%20and%20embedded%20Force%20Torque%20Sensors%20%28FTS%29.%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20then%20classify%20the%20cobot%20actions%20as%20Success%20or%20Fail%2C%20while%20also%0Aidentifying%20and%20reporting%20the%20causes%20of%20failure.%20To%20improve%20interpretability%2C%0AGrad%20CAM%2C%20an%20XAI%20technique%20for%20visual%20explanations%2C%20is%20integrated%20to%20provide%0Ainsights%20into%20the%20models%20decision%20making%20process.%20This%20approach%20enhances%20the%0Areliability%20and%20trustworthiness%20of%20the%20automated%20testing%20system%2C%20facilitating%0Athe%20diagnosis%20and%20rectification%20of%20errors%20that%20may%20arise%20during%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10224v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


