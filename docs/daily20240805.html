<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240804.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes", "author": "Zhengyi Zhao and Chen Song and Xiaodong Gu and Yuan Dong and Qi Zuo and Weihao Yuan and Liefeng Bo and Zilong Dong and Qixing Huang", "abstract": "  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.\n", "link": "http://arxiv.org/abs/2403.15559v2", "date": "2024-08-02", "relevancy": 3.164, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6349}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6349}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimization%20Framework%20to%20Enforce%20Multi-View%20Consistency%20for%0A%20%20Texturing%203D%20Meshes&body=Title%3A%20An%20Optimization%20Framework%20to%20Enforce%20Multi-View%20Consistency%20for%0A%20%20Texturing%203D%20Meshes%0AAuthor%3A%20Zhengyi%20Zhao%20and%20Chen%20Song%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Qi%20Zuo%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo%20and%20Zilong%20Dong%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20A%20fundamental%20problem%20in%20the%20texturing%20of%203D%20meshes%20using%20pre-trained%0Atext-to-image%20models%20is%20to%20ensure%20multi-view%20consistency.%20State-of-the-art%0Aapproaches%20typically%20use%20diffusion%20models%20to%20aggregate%20multi-view%20inputs%2C%20where%0Acommon%20issues%20are%20the%20blurriness%20caused%20by%20the%20averaging%20operation%20in%20the%0Aaggregation%20step%20or%20inconsistencies%20in%20local%20features.%20This%20paper%20introduces%20an%0Aoptimization%20framework%20that%20proceeds%20in%20four%20stages%20to%20achieve%20multi-view%0Aconsistency.%20Specifically%2C%20the%20first%20stage%20generates%20an%20over-complete%20set%20of%202D%0Atextures%20from%20a%20predefined%20set%20of%20viewpoints%20using%20an%20MV-consistent%20diffusion%0Aprocess.%20The%20second%20stage%20selects%20a%20subset%20of%20views%20that%20are%20mutually%0Aconsistent%20while%20covering%20the%20underlying%203D%20model.%20We%20show%20how%20to%20achieve%20this%0Agoal%20by%20solving%20semi-definite%20programs.%20The%20third%20stage%20performs%20non-rigid%0Aalignment%20to%20align%20the%20selected%20views%20across%20overlapping%20regions.%20The%20fourth%0Astage%20solves%20an%20MRF%20problem%20to%20associate%20each%20mesh%20face%20with%20a%20selected%20view.%0AIn%20particular%2C%20the%20third%20and%20fourth%20stages%20are%20iterated%2C%20with%20the%20cuts%20obtained%0Ain%20the%20fourth%20stage%20encouraging%20non-rigid%20alignment%20in%20the%20third%20stage%20to%20focus%0Aon%20regions%20close%20to%20the%20cuts.%20Experimental%20results%20show%20that%20our%20approach%0Asignificantly%20outperforms%20baseline%20approaches%20both%20qualitatively%20and%0Aquantitatively.%20Project%20page%3A%20https%3A//aigc3d.github.io/ConsistenTex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimization%2520Framework%2520to%2520Enforce%2520Multi-View%2520Consistency%2520for%250A%2520%2520Texturing%25203D%2520Meshes%26entry.906535625%3DZhengyi%2520Zhao%2520and%2520Chen%2520Song%2520and%2520Xiaodong%2520Gu%2520and%2520Yuan%2520Dong%2520and%2520Qi%2520Zuo%2520and%2520Weihao%2520Yuan%2520and%2520Liefeng%2520Bo%2520and%2520Zilong%2520Dong%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520A%2520fundamental%2520problem%2520in%2520the%2520texturing%2520of%25203D%2520meshes%2520using%2520pre-trained%250Atext-to-image%2520models%2520is%2520to%2520ensure%2520multi-view%2520consistency.%2520State-of-the-art%250Aapproaches%2520typically%2520use%2520diffusion%2520models%2520to%2520aggregate%2520multi-view%2520inputs%252C%2520where%250Acommon%2520issues%2520are%2520the%2520blurriness%2520caused%2520by%2520the%2520averaging%2520operation%2520in%2520the%250Aaggregation%2520step%2520or%2520inconsistencies%2520in%2520local%2520features.%2520This%2520paper%2520introduces%2520an%250Aoptimization%2520framework%2520that%2520proceeds%2520in%2520four%2520stages%2520to%2520achieve%2520multi-view%250Aconsistency.%2520Specifically%252C%2520the%2520first%2520stage%2520generates%2520an%2520over-complete%2520set%2520of%25202D%250Atextures%2520from%2520a%2520predefined%2520set%2520of%2520viewpoints%2520using%2520an%2520MV-consistent%2520diffusion%250Aprocess.%2520The%2520second%2520stage%2520selects%2520a%2520subset%2520of%2520views%2520that%2520are%2520mutually%250Aconsistent%2520while%2520covering%2520the%2520underlying%25203D%2520model.%2520We%2520show%2520how%2520to%2520achieve%2520this%250Agoal%2520by%2520solving%2520semi-definite%2520programs.%2520The%2520third%2520stage%2520performs%2520non-rigid%250Aalignment%2520to%2520align%2520the%2520selected%2520views%2520across%2520overlapping%2520regions.%2520The%2520fourth%250Astage%2520solves%2520an%2520MRF%2520problem%2520to%2520associate%2520each%2520mesh%2520face%2520with%2520a%2520selected%2520view.%250AIn%2520particular%252C%2520the%2520third%2520and%2520fourth%2520stages%2520are%2520iterated%252C%2520with%2520the%2520cuts%2520obtained%250Ain%2520the%2520fourth%2520stage%2520encouraging%2520non-rigid%2520alignment%2520in%2520the%2520third%2520stage%2520to%2520focus%250Aon%2520regions%2520close%2520to%2520the%2520cuts.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520baseline%2520approaches%2520both%2520qualitatively%2520and%250Aquantitatively.%2520Project%2520page%253A%2520https%253A//aigc3d.github.io/ConsistenTex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimization%20Framework%20to%20Enforce%20Multi-View%20Consistency%20for%0A%20%20Texturing%203D%20Meshes&entry.906535625=Zhengyi%20Zhao%20and%20Chen%20Song%20and%20Xiaodong%20Gu%20and%20Yuan%20Dong%20and%20Qi%20Zuo%20and%20Weihao%20Yuan%20and%20Liefeng%20Bo%20and%20Zilong%20Dong%20and%20Qixing%20Huang&entry.1292438233=%20%20A%20fundamental%20problem%20in%20the%20texturing%20of%203D%20meshes%20using%20pre-trained%0Atext-to-image%20models%20is%20to%20ensure%20multi-view%20consistency.%20State-of-the-art%0Aapproaches%20typically%20use%20diffusion%20models%20to%20aggregate%20multi-view%20inputs%2C%20where%0Acommon%20issues%20are%20the%20blurriness%20caused%20by%20the%20averaging%20operation%20in%20the%0Aaggregation%20step%20or%20inconsistencies%20in%20local%20features.%20This%20paper%20introduces%20an%0Aoptimization%20framework%20that%20proceeds%20in%20four%20stages%20to%20achieve%20multi-view%0Aconsistency.%20Specifically%2C%20the%20first%20stage%20generates%20an%20over-complete%20set%20of%202D%0Atextures%20from%20a%20predefined%20set%20of%20viewpoints%20using%20an%20MV-consistent%20diffusion%0Aprocess.%20The%20second%20stage%20selects%20a%20subset%20of%20views%20that%20are%20mutually%0Aconsistent%20while%20covering%20the%20underlying%203D%20model.%20We%20show%20how%20to%20achieve%20this%0Agoal%20by%20solving%20semi-definite%20programs.%20The%20third%20stage%20performs%20non-rigid%0Aalignment%20to%20align%20the%20selected%20views%20across%20overlapping%20regions.%20The%20fourth%0Astage%20solves%20an%20MRF%20problem%20to%20associate%20each%20mesh%20face%20with%20a%20selected%20view.%0AIn%20particular%2C%20the%20third%20and%20fourth%20stages%20are%20iterated%2C%20with%20the%20cuts%20obtained%0Ain%20the%20fourth%20stage%20encouraging%20non-rigid%20alignment%20in%20the%20third%20stage%20to%20focus%0Aon%20regions%20close%20to%20the%20cuts.%20Experimental%20results%20show%20that%20our%20approach%0Asignificantly%20outperforms%20baseline%20approaches%20both%20qualitatively%20and%0Aquantitatively.%20Project%20page%3A%20https%3A//aigc3d.github.io/ConsistenTex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15559v2&entry.124074799=Read"},
{"title": "TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and\n  Resampling", "author": "Dong Huo and Zixin Guo and Xinxin Zuo and Zhihao Shi and Juwei Lu and Peng Dai and Songcen Xu and Li Cheng and Yee-Hong Yang", "abstract": "  Given a 3D mesh, we aim to synthesize 3D textures that correspond to\narbitrary textual descriptions. Current methods for generating and assembling\ntextures from sampled views often result in prominent seams or excessive\nsmoothing. To tackle these issues, we present TexGen, a novel multi-view\nsampling and resampling framework for texture generation leveraging a\npre-trained text-to-image diffusion model. For view consistent sampling, first\nof all we maintain a texture map in RGB space that is parameterized by the\ndenoising step and updated after each sampling step of the diffusion model to\nprogressively reduce the view discrepancy. An attention-guided multi-view\nsampling strategy is exploited to broadcast the appearance information across\nviews. To preserve texture details, we develop a noise resampling technique\nthat aids in the estimation of noise, generating inputs for subsequent\ndenoising steps, as directed by the text prompt and current texture map.\nThrough an extensive amount of qualitative and quantitative evaluations, we\ndemonstrate that our proposed method produces significantly better texture\nquality for diverse 3D objects with a high degree of view consistency and rich\nappearance details, outperforming current state-of-the-art methods.\nFurthermore, our proposed texture generation technique can also be applied to\ntexture editing while preserving the original identity. More experimental\nresults are available at https://dong-huo.github.io/TexGen/\n", "link": "http://arxiv.org/abs/2408.01291v1", "date": "2024-08-02", "relevancy": 3.1482, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6471}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6471}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexGen%3A%20Text-Guided%203D%20Texture%20Generation%20with%20Multi-view%20Sampling%20and%0A%20%20Resampling&body=Title%3A%20TexGen%3A%20Text-Guided%203D%20Texture%20Generation%20with%20Multi-view%20Sampling%20and%0A%20%20Resampling%0AAuthor%3A%20Dong%20Huo%20and%20Zixin%20Guo%20and%20Xinxin%20Zuo%20and%20Zhihao%20Shi%20and%20Juwei%20Lu%20and%20Peng%20Dai%20and%20Songcen%20Xu%20and%20Li%20Cheng%20and%20Yee-Hong%20Yang%0AAbstract%3A%20%20%20Given%20a%203D%20mesh%2C%20we%20aim%20to%20synthesize%203D%20textures%20that%20correspond%20to%0Aarbitrary%20textual%20descriptions.%20Current%20methods%20for%20generating%20and%20assembling%0Atextures%20from%20sampled%20views%20often%20result%20in%20prominent%20seams%20or%20excessive%0Asmoothing.%20To%20tackle%20these%20issues%2C%20we%20present%20TexGen%2C%20a%20novel%20multi-view%0Asampling%20and%20resampling%20framework%20for%20texture%20generation%20leveraging%20a%0Apre-trained%20text-to-image%20diffusion%20model.%20For%20view%20consistent%20sampling%2C%20first%0Aof%20all%20we%20maintain%20a%20texture%20map%20in%20RGB%20space%20that%20is%20parameterized%20by%20the%0Adenoising%20step%20and%20updated%20after%20each%20sampling%20step%20of%20the%20diffusion%20model%20to%0Aprogressively%20reduce%20the%20view%20discrepancy.%20An%20attention-guided%20multi-view%0Asampling%20strategy%20is%20exploited%20to%20broadcast%20the%20appearance%20information%20across%0Aviews.%20To%20preserve%20texture%20details%2C%20we%20develop%20a%20noise%20resampling%20technique%0Athat%20aids%20in%20the%20estimation%20of%20noise%2C%20generating%20inputs%20for%20subsequent%0Adenoising%20steps%2C%20as%20directed%20by%20the%20text%20prompt%20and%20current%20texture%20map.%0AThrough%20an%20extensive%20amount%20of%20qualitative%20and%20quantitative%20evaluations%2C%20we%0Ademonstrate%20that%20our%20proposed%20method%20produces%20significantly%20better%20texture%0Aquality%20for%20diverse%203D%20objects%20with%20a%20high%20degree%20of%20view%20consistency%20and%20rich%0Aappearance%20details%2C%20outperforming%20current%20state-of-the-art%20methods.%0AFurthermore%2C%20our%20proposed%20texture%20generation%20technique%20can%20also%20be%20applied%20to%0Atexture%20editing%20while%20preserving%20the%20original%20identity.%20More%20experimental%0Aresults%20are%20available%20at%20https%3A//dong-huo.github.io/TexGen/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexGen%253A%2520Text-Guided%25203D%2520Texture%2520Generation%2520with%2520Multi-view%2520Sampling%2520and%250A%2520%2520Resampling%26entry.906535625%3DDong%2520Huo%2520and%2520Zixin%2520Guo%2520and%2520Xinxin%2520Zuo%2520and%2520Zhihao%2520Shi%2520and%2520Juwei%2520Lu%2520and%2520Peng%2520Dai%2520and%2520Songcen%2520Xu%2520and%2520Li%2520Cheng%2520and%2520Yee-Hong%2520Yang%26entry.1292438233%3D%2520%2520Given%2520a%25203D%2520mesh%252C%2520we%2520aim%2520to%2520synthesize%25203D%2520textures%2520that%2520correspond%2520to%250Aarbitrary%2520textual%2520descriptions.%2520Current%2520methods%2520for%2520generating%2520and%2520assembling%250Atextures%2520from%2520sampled%2520views%2520often%2520result%2520in%2520prominent%2520seams%2520or%2520excessive%250Asmoothing.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520present%2520TexGen%252C%2520a%2520novel%2520multi-view%250Asampling%2520and%2520resampling%2520framework%2520for%2520texture%2520generation%2520leveraging%2520a%250Apre-trained%2520text-to-image%2520diffusion%2520model.%2520For%2520view%2520consistent%2520sampling%252C%2520first%250Aof%2520all%2520we%2520maintain%2520a%2520texture%2520map%2520in%2520RGB%2520space%2520that%2520is%2520parameterized%2520by%2520the%250Adenoising%2520step%2520and%2520updated%2520after%2520each%2520sampling%2520step%2520of%2520the%2520diffusion%2520model%2520to%250Aprogressively%2520reduce%2520the%2520view%2520discrepancy.%2520An%2520attention-guided%2520multi-view%250Asampling%2520strategy%2520is%2520exploited%2520to%2520broadcast%2520the%2520appearance%2520information%2520across%250Aviews.%2520To%2520preserve%2520texture%2520details%252C%2520we%2520develop%2520a%2520noise%2520resampling%2520technique%250Athat%2520aids%2520in%2520the%2520estimation%2520of%2520noise%252C%2520generating%2520inputs%2520for%2520subsequent%250Adenoising%2520steps%252C%2520as%2520directed%2520by%2520the%2520text%2520prompt%2520and%2520current%2520texture%2520map.%250AThrough%2520an%2520extensive%2520amount%2520of%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520our%2520proposed%2520method%2520produces%2520significantly%2520better%2520texture%250Aquality%2520for%2520diverse%25203D%2520objects%2520with%2520a%2520high%2520degree%2520of%2520view%2520consistency%2520and%2520rich%250Aappearance%2520details%252C%2520outperforming%2520current%2520state-of-the-art%2520methods.%250AFurthermore%252C%2520our%2520proposed%2520texture%2520generation%2520technique%2520can%2520also%2520be%2520applied%2520to%250Atexture%2520editing%2520while%2520preserving%2520the%2520original%2520identity.%2520More%2520experimental%250Aresults%2520are%2520available%2520at%2520https%253A//dong-huo.github.io/TexGen/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexGen%3A%20Text-Guided%203D%20Texture%20Generation%20with%20Multi-view%20Sampling%20and%0A%20%20Resampling&entry.906535625=Dong%20Huo%20and%20Zixin%20Guo%20and%20Xinxin%20Zuo%20and%20Zhihao%20Shi%20and%20Juwei%20Lu%20and%20Peng%20Dai%20and%20Songcen%20Xu%20and%20Li%20Cheng%20and%20Yee-Hong%20Yang&entry.1292438233=%20%20Given%20a%203D%20mesh%2C%20we%20aim%20to%20synthesize%203D%20textures%20that%20correspond%20to%0Aarbitrary%20textual%20descriptions.%20Current%20methods%20for%20generating%20and%20assembling%0Atextures%20from%20sampled%20views%20often%20result%20in%20prominent%20seams%20or%20excessive%0Asmoothing.%20To%20tackle%20these%20issues%2C%20we%20present%20TexGen%2C%20a%20novel%20multi-view%0Asampling%20and%20resampling%20framework%20for%20texture%20generation%20leveraging%20a%0Apre-trained%20text-to-image%20diffusion%20model.%20For%20view%20consistent%20sampling%2C%20first%0Aof%20all%20we%20maintain%20a%20texture%20map%20in%20RGB%20space%20that%20is%20parameterized%20by%20the%0Adenoising%20step%20and%20updated%20after%20each%20sampling%20step%20of%20the%20diffusion%20model%20to%0Aprogressively%20reduce%20the%20view%20discrepancy.%20An%20attention-guided%20multi-view%0Asampling%20strategy%20is%20exploited%20to%20broadcast%20the%20appearance%20information%20across%0Aviews.%20To%20preserve%20texture%20details%2C%20we%20develop%20a%20noise%20resampling%20technique%0Athat%20aids%20in%20the%20estimation%20of%20noise%2C%20generating%20inputs%20for%20subsequent%0Adenoising%20steps%2C%20as%20directed%20by%20the%20text%20prompt%20and%20current%20texture%20map.%0AThrough%20an%20extensive%20amount%20of%20qualitative%20and%20quantitative%20evaluations%2C%20we%0Ademonstrate%20that%20our%20proposed%20method%20produces%20significantly%20better%20texture%0Aquality%20for%20diverse%203D%20objects%20with%20a%20high%20degree%20of%20view%20consistency%20and%20rich%0Aappearance%20details%2C%20outperforming%20current%20state-of-the-art%20methods.%0AFurthermore%2C%20our%20proposed%20texture%20generation%20technique%20can%20also%20be%20applied%20to%0Atexture%20editing%20while%20preserving%20the%20original%20identity.%20More%20experimental%0Aresults%20are%20available%20at%20https%3A//dong-huo.github.io/TexGen/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01291v1&entry.124074799=Read"},
{"title": "A General Framework to Boost 3D GS Initialization for Text-to-3D\n  Generation by Lexical Richness", "author": "Lutao Jiang and Hangyu Li and Lin Wang", "abstract": "  Text-to-3D content creation has recently received much attention, especially\nwith the prevalence of 3D Gaussians Splatting. In general, GS-based methods\ncomprise two key stages: initialization and rendering optimization. To achieve\ninitialization, existing works directly apply random sphere initialization or\n3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such\nstrategies suffer from two critical yet challenging problems: 1) the final\nshapes are still similar to the initial ones even after training; 2) shapes can\nbe produced only from simple texts, e.g., \"a dog\", not for lexically richer\ntexts, e.g., \"a dog is sitting on the top of the airplane\". To address these\nproblems, this paper proposes a novel general framework to boost the 3D GS\nInitialization for text-to-3D generation upon the lexical richness. Our key\nidea is to aggregate 3D Gaussians into spatially uniform voxels to represent\ncomplex shapes while enabling the spatial interaction among the 3D Gaussians\nand semantic interaction between Gaussians and texts. Specifically, we first\nconstruct a voxelized representation, where each voxel holds a 3D Gaussian with\nits position, scale, and rotation fixed while setting opacity as the sole\nfactor to determine a position's occupancy. We then design an initialization\nnetwork mainly consisting of two novel components: 1) Global Information\nPerception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design\nenables each 3D Gaussian to assimilate the spatial information from other areas\nand semantic information from texts. Extensive experiments show the superiority\nof our framework of high-quality 3D GS initialization against the existing\nmethods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.\nAlso, our framework can be seamlessly plugged into SoTA training frameworks,\ne.g., LucidDreamer, for semantically consistent text-to-3D generation.\n", "link": "http://arxiv.org/abs/2408.01269v1", "date": "2024-08-02", "relevancy": 3.0151, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6169}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6071}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20to%20Boost%203D%20GS%20Initialization%20for%20Text-to-3D%0A%20%20Generation%20by%20Lexical%20Richness&body=Title%3A%20A%20General%20Framework%20to%20Boost%203D%20GS%20Initialization%20for%20Text-to-3D%0A%20%20Generation%20by%20Lexical%20Richness%0AAuthor%3A%20Lutao%20Jiang%20and%20Hangyu%20Li%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Text-to-3D%20content%20creation%20has%20recently%20received%20much%20attention%2C%20especially%0Awith%20the%20prevalence%20of%203D%20Gaussians%20Splatting.%20In%20general%2C%20GS-based%20methods%0Acomprise%20two%20key%20stages%3A%20initialization%20and%20rendering%20optimization.%20To%20achieve%0Ainitialization%2C%20existing%20works%20directly%20apply%20random%20sphere%20initialization%20or%0A3D%20diffusion%20models%2C%20e.g.%2C%20Point-E%2C%20to%20derive%20the%20initial%20shapes.%20However%2C%20such%0Astrategies%20suffer%20from%20two%20critical%20yet%20challenging%20problems%3A%201%29%20the%20final%0Ashapes%20are%20still%20similar%20to%20the%20initial%20ones%20even%20after%20training%3B%202%29%20shapes%20can%0Abe%20produced%20only%20from%20simple%20texts%2C%20e.g.%2C%20%22a%20dog%22%2C%20not%20for%20lexically%20richer%0Atexts%2C%20e.g.%2C%20%22a%20dog%20is%20sitting%20on%20the%20top%20of%20the%20airplane%22.%20To%20address%20these%0Aproblems%2C%20this%20paper%20proposes%20a%20novel%20general%20framework%20to%20boost%20the%203D%20GS%0AInitialization%20for%20text-to-3D%20generation%20upon%20the%20lexical%20richness.%20Our%20key%0Aidea%20is%20to%20aggregate%203D%20Gaussians%20into%20spatially%20uniform%20voxels%20to%20represent%0Acomplex%20shapes%20while%20enabling%20the%20spatial%20interaction%20among%20the%203D%20Gaussians%0Aand%20semantic%20interaction%20between%20Gaussians%20and%20texts.%20Specifically%2C%20we%20first%0Aconstruct%20a%20voxelized%20representation%2C%20where%20each%20voxel%20holds%20a%203D%20Gaussian%20with%0Aits%20position%2C%20scale%2C%20and%20rotation%20fixed%20while%20setting%20opacity%20as%20the%20sole%0Afactor%20to%20determine%20a%20position%27s%20occupancy.%20We%20then%20design%20an%20initialization%0Anetwork%20mainly%20consisting%20of%20two%20novel%20components%3A%201%29%20Global%20Information%0APerception%20%28GIP%29%20block%20and%202%29%20Gaussians-Text%20Fusion%20%28GTF%29%20block.%20Such%20a%20design%0Aenables%20each%203D%20Gaussian%20to%20assimilate%20the%20spatial%20information%20from%20other%20areas%0Aand%20semantic%20information%20from%20texts.%20Extensive%20experiments%20show%20the%20superiority%0Aof%20our%20framework%20of%20high-quality%203D%20GS%20initialization%20against%20the%20existing%0Amethods%2C%20e.g.%2C%20Shap-E%2C%20by%20taking%20lexically%20simple%2C%20medium%2C%20and%20hard%20texts.%0AAlso%2C%20our%20framework%20can%20be%20seamlessly%20plugged%20into%20SoTA%20training%20frameworks%2C%0Ae.g.%2C%20LucidDreamer%2C%20for%20semantically%20consistent%20text-to-3D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520to%2520Boost%25203D%2520GS%2520Initialization%2520for%2520Text-to-3D%250A%2520%2520Generation%2520by%2520Lexical%2520Richness%26entry.906535625%3DLutao%2520Jiang%2520and%2520Hangyu%2520Li%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Text-to-3D%2520content%2520creation%2520has%2520recently%2520received%2520much%2520attention%252C%2520especially%250Awith%2520the%2520prevalence%2520of%25203D%2520Gaussians%2520Splatting.%2520In%2520general%252C%2520GS-based%2520methods%250Acomprise%2520two%2520key%2520stages%253A%2520initialization%2520and%2520rendering%2520optimization.%2520To%2520achieve%250Ainitialization%252C%2520existing%2520works%2520directly%2520apply%2520random%2520sphere%2520initialization%2520or%250A3D%2520diffusion%2520models%252C%2520e.g.%252C%2520Point-E%252C%2520to%2520derive%2520the%2520initial%2520shapes.%2520However%252C%2520such%250Astrategies%2520suffer%2520from%2520two%2520critical%2520yet%2520challenging%2520problems%253A%25201%2529%2520the%2520final%250Ashapes%2520are%2520still%2520similar%2520to%2520the%2520initial%2520ones%2520even%2520after%2520training%253B%25202%2529%2520shapes%2520can%250Abe%2520produced%2520only%2520from%2520simple%2520texts%252C%2520e.g.%252C%2520%2522a%2520dog%2522%252C%2520not%2520for%2520lexically%2520richer%250Atexts%252C%2520e.g.%252C%2520%2522a%2520dog%2520is%2520sitting%2520on%2520the%2520top%2520of%2520the%2520airplane%2522.%2520To%2520address%2520these%250Aproblems%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520general%2520framework%2520to%2520boost%2520the%25203D%2520GS%250AInitialization%2520for%2520text-to-3D%2520generation%2520upon%2520the%2520lexical%2520richness.%2520Our%2520key%250Aidea%2520is%2520to%2520aggregate%25203D%2520Gaussians%2520into%2520spatially%2520uniform%2520voxels%2520to%2520represent%250Acomplex%2520shapes%2520while%2520enabling%2520the%2520spatial%2520interaction%2520among%2520the%25203D%2520Gaussians%250Aand%2520semantic%2520interaction%2520between%2520Gaussians%2520and%2520texts.%2520Specifically%252C%2520we%2520first%250Aconstruct%2520a%2520voxelized%2520representation%252C%2520where%2520each%2520voxel%2520holds%2520a%25203D%2520Gaussian%2520with%250Aits%2520position%252C%2520scale%252C%2520and%2520rotation%2520fixed%2520while%2520setting%2520opacity%2520as%2520the%2520sole%250Afactor%2520to%2520determine%2520a%2520position%2527s%2520occupancy.%2520We%2520then%2520design%2520an%2520initialization%250Anetwork%2520mainly%2520consisting%2520of%2520two%2520novel%2520components%253A%25201%2529%2520Global%2520Information%250APerception%2520%2528GIP%2529%2520block%2520and%25202%2529%2520Gaussians-Text%2520Fusion%2520%2528GTF%2529%2520block.%2520Such%2520a%2520design%250Aenables%2520each%25203D%2520Gaussian%2520to%2520assimilate%2520the%2520spatial%2520information%2520from%2520other%2520areas%250Aand%2520semantic%2520information%2520from%2520texts.%2520Extensive%2520experiments%2520show%2520the%2520superiority%250Aof%2520our%2520framework%2520of%2520high-quality%25203D%2520GS%2520initialization%2520against%2520the%2520existing%250Amethods%252C%2520e.g.%252C%2520Shap-E%252C%2520by%2520taking%2520lexically%2520simple%252C%2520medium%252C%2520and%2520hard%2520texts.%250AAlso%252C%2520our%2520framework%2520can%2520be%2520seamlessly%2520plugged%2520into%2520SoTA%2520training%2520frameworks%252C%250Ae.g.%252C%2520LucidDreamer%252C%2520for%2520semantically%2520consistent%2520text-to-3D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20to%20Boost%203D%20GS%20Initialization%20for%20Text-to-3D%0A%20%20Generation%20by%20Lexical%20Richness&entry.906535625=Lutao%20Jiang%20and%20Hangyu%20Li%20and%20Lin%20Wang&entry.1292438233=%20%20Text-to-3D%20content%20creation%20has%20recently%20received%20much%20attention%2C%20especially%0Awith%20the%20prevalence%20of%203D%20Gaussians%20Splatting.%20In%20general%2C%20GS-based%20methods%0Acomprise%20two%20key%20stages%3A%20initialization%20and%20rendering%20optimization.%20To%20achieve%0Ainitialization%2C%20existing%20works%20directly%20apply%20random%20sphere%20initialization%20or%0A3D%20diffusion%20models%2C%20e.g.%2C%20Point-E%2C%20to%20derive%20the%20initial%20shapes.%20However%2C%20such%0Astrategies%20suffer%20from%20two%20critical%20yet%20challenging%20problems%3A%201%29%20the%20final%0Ashapes%20are%20still%20similar%20to%20the%20initial%20ones%20even%20after%20training%3B%202%29%20shapes%20can%0Abe%20produced%20only%20from%20simple%20texts%2C%20e.g.%2C%20%22a%20dog%22%2C%20not%20for%20lexically%20richer%0Atexts%2C%20e.g.%2C%20%22a%20dog%20is%20sitting%20on%20the%20top%20of%20the%20airplane%22.%20To%20address%20these%0Aproblems%2C%20this%20paper%20proposes%20a%20novel%20general%20framework%20to%20boost%20the%203D%20GS%0AInitialization%20for%20text-to-3D%20generation%20upon%20the%20lexical%20richness.%20Our%20key%0Aidea%20is%20to%20aggregate%203D%20Gaussians%20into%20spatially%20uniform%20voxels%20to%20represent%0Acomplex%20shapes%20while%20enabling%20the%20spatial%20interaction%20among%20the%203D%20Gaussians%0Aand%20semantic%20interaction%20between%20Gaussians%20and%20texts.%20Specifically%2C%20we%20first%0Aconstruct%20a%20voxelized%20representation%2C%20where%20each%20voxel%20holds%20a%203D%20Gaussian%20with%0Aits%20position%2C%20scale%2C%20and%20rotation%20fixed%20while%20setting%20opacity%20as%20the%20sole%0Afactor%20to%20determine%20a%20position%27s%20occupancy.%20We%20then%20design%20an%20initialization%0Anetwork%20mainly%20consisting%20of%20two%20novel%20components%3A%201%29%20Global%20Information%0APerception%20%28GIP%29%20block%20and%202%29%20Gaussians-Text%20Fusion%20%28GTF%29%20block.%20Such%20a%20design%0Aenables%20each%203D%20Gaussian%20to%20assimilate%20the%20spatial%20information%20from%20other%20areas%0Aand%20semantic%20information%20from%20texts.%20Extensive%20experiments%20show%20the%20superiority%0Aof%20our%20framework%20of%20high-quality%203D%20GS%20initialization%20against%20the%20existing%0Amethods%2C%20e.g.%2C%20Shap-E%2C%20by%20taking%20lexically%20simple%2C%20medium%2C%20and%20hard%20texts.%0AAlso%2C%20our%20framework%20can%20be%20seamlessly%20plugged%20into%20SoTA%20training%20frameworks%2C%0Ae.g.%2C%20LucidDreamer%2C%20for%20semantically%20consistent%20text-to-3D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01269v1&entry.124074799=Read"},
{"title": "Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation\n  with Volumetric Visual Data Fusion", "author": "Ke Li and Reinhard Bacher and Susanne Schmidt and Wim Leemans and Frank Steinicke", "abstract": "  We introduce Reality Fusion, a novel robot teleoperation system that\nlocalizes, streams, projects, and merges a typical onboard depth sensor with a\nphotorealistic, high resolution, high framerate, and wide field of view (FoV)\nrendering of the complex remote environment represented as 3D Gaussian splats\n(3DGS). Our framework enables robust egocentric and exocentric robot\nteleoperation in immersive VR, with the 3DGS effectively extending spatial\ninformation of a depth sensor with limited FoV and balancing the trade-off\nbetween data streaming costs and data visual quality. We evaluated our\nframework through a user study with 24 participants, which revealed that\nReality Fusion leads to significantly better user performance, situation\nawareness, and user preferences. To support further research and development,\nwe provide an open-source implementation with an easy-to-replicate custom-made\ntelepresence robot, a high-performance virtual reality 3DGS renderer, and an\nimmersive robot control package. (Source code:\nhttps://github.com/uhhhci/RealityFusion)\n", "link": "http://arxiv.org/abs/2408.01225v1", "date": "2024-08-02", "relevancy": 2.9515, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion&body=Title%3A%20Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion%0AAuthor%3A%20Ke%20Li%20and%20Reinhard%20Bacher%20and%20Susanne%20Schmidt%20and%20Wim%20Leemans%20and%20Frank%20Steinicke%0AAbstract%3A%20%20%20We%20introduce%20Reality%20Fusion%2C%20a%20novel%20robot%20teleoperation%20system%20that%0Alocalizes%2C%20streams%2C%20projects%2C%20and%20merges%20a%20typical%20onboard%20depth%20sensor%20with%20a%0Aphotorealistic%2C%20high%20resolution%2C%20high%20framerate%2C%20and%20wide%20field%20of%20view%20%28FoV%29%0Arendering%20of%20the%20complex%20remote%20environment%20represented%20as%203D%20Gaussian%20splats%0A%283DGS%29.%20Our%20framework%20enables%20robust%20egocentric%20and%20exocentric%20robot%0Ateleoperation%20in%20immersive%20VR%2C%20with%20the%203DGS%20effectively%20extending%20spatial%0Ainformation%20of%20a%20depth%20sensor%20with%20limited%20FoV%20and%20balancing%20the%20trade-off%0Abetween%20data%20streaming%20costs%20and%20data%20visual%20quality.%20We%20evaluated%20our%0Aframework%20through%20a%20user%20study%20with%2024%20participants%2C%20which%20revealed%20that%0AReality%20Fusion%20leads%20to%20significantly%20better%20user%20performance%2C%20situation%0Aawareness%2C%20and%20user%20preferences.%20To%20support%20further%20research%20and%20development%2C%0Awe%20provide%20an%20open-source%20implementation%20with%20an%20easy-to-replicate%20custom-made%0Atelepresence%20robot%2C%20a%20high-performance%20virtual%20reality%203DGS%20renderer%2C%20and%20an%0Aimmersive%20robot%20control%20package.%20%28Source%20code%3A%0Ahttps%3A//github.com/uhhhci/RealityFusion%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReality%2520Fusion%253A%2520Robust%2520Real-time%2520Immersive%2520Mobile%2520Robot%2520Teleoperation%250A%2520%2520with%2520Volumetric%2520Visual%2520Data%2520Fusion%26entry.906535625%3DKe%2520Li%2520and%2520Reinhard%2520Bacher%2520and%2520Susanne%2520Schmidt%2520and%2520Wim%2520Leemans%2520and%2520Frank%2520Steinicke%26entry.1292438233%3D%2520%2520We%2520introduce%2520Reality%2520Fusion%252C%2520a%2520novel%2520robot%2520teleoperation%2520system%2520that%250Alocalizes%252C%2520streams%252C%2520projects%252C%2520and%2520merges%2520a%2520typical%2520onboard%2520depth%2520sensor%2520with%2520a%250Aphotorealistic%252C%2520high%2520resolution%252C%2520high%2520framerate%252C%2520and%2520wide%2520field%2520of%2520view%2520%2528FoV%2529%250Arendering%2520of%2520the%2520complex%2520remote%2520environment%2520represented%2520as%25203D%2520Gaussian%2520splats%250A%25283DGS%2529.%2520Our%2520framework%2520enables%2520robust%2520egocentric%2520and%2520exocentric%2520robot%250Ateleoperation%2520in%2520immersive%2520VR%252C%2520with%2520the%25203DGS%2520effectively%2520extending%2520spatial%250Ainformation%2520of%2520a%2520depth%2520sensor%2520with%2520limited%2520FoV%2520and%2520balancing%2520the%2520trade-off%250Abetween%2520data%2520streaming%2520costs%2520and%2520data%2520visual%2520quality.%2520We%2520evaluated%2520our%250Aframework%2520through%2520a%2520user%2520study%2520with%252024%2520participants%252C%2520which%2520revealed%2520that%250AReality%2520Fusion%2520leads%2520to%2520significantly%2520better%2520user%2520performance%252C%2520situation%250Aawareness%252C%2520and%2520user%2520preferences.%2520To%2520support%2520further%2520research%2520and%2520development%252C%250Awe%2520provide%2520an%2520open-source%2520implementation%2520with%2520an%2520easy-to-replicate%2520custom-made%250Atelepresence%2520robot%252C%2520a%2520high-performance%2520virtual%2520reality%25203DGS%2520renderer%252C%2520and%2520an%250Aimmersive%2520robot%2520control%2520package.%2520%2528Source%2520code%253A%250Ahttps%253A//github.com/uhhhci/RealityFusion%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reality%20Fusion%3A%20Robust%20Real-time%20Immersive%20Mobile%20Robot%20Teleoperation%0A%20%20with%20Volumetric%20Visual%20Data%20Fusion&entry.906535625=Ke%20Li%20and%20Reinhard%20Bacher%20and%20Susanne%20Schmidt%20and%20Wim%20Leemans%20and%20Frank%20Steinicke&entry.1292438233=%20%20We%20introduce%20Reality%20Fusion%2C%20a%20novel%20robot%20teleoperation%20system%20that%0Alocalizes%2C%20streams%2C%20projects%2C%20and%20merges%20a%20typical%20onboard%20depth%20sensor%20with%20a%0Aphotorealistic%2C%20high%20resolution%2C%20high%20framerate%2C%20and%20wide%20field%20of%20view%20%28FoV%29%0Arendering%20of%20the%20complex%20remote%20environment%20represented%20as%203D%20Gaussian%20splats%0A%283DGS%29.%20Our%20framework%20enables%20robust%20egocentric%20and%20exocentric%20robot%0Ateleoperation%20in%20immersive%20VR%2C%20with%20the%203DGS%20effectively%20extending%20spatial%0Ainformation%20of%20a%20depth%20sensor%20with%20limited%20FoV%20and%20balancing%20the%20trade-off%0Abetween%20data%20streaming%20costs%20and%20data%20visual%20quality.%20We%20evaluated%20our%0Aframework%20through%20a%20user%20study%20with%2024%20participants%2C%20which%20revealed%20that%0AReality%20Fusion%20leads%20to%20significantly%20better%20user%20performance%2C%20situation%0Aawareness%2C%20and%20user%20preferences.%20To%20support%20further%20research%20and%20development%2C%0Awe%20provide%20an%20open-source%20implementation%20with%20an%20easy-to-replicate%20custom-made%0Atelepresence%20robot%2C%20a%20high-performance%20virtual%20reality%203DGS%20renderer%2C%20and%20an%0Aimmersive%20robot%20control%20package.%20%28Source%20code%3A%0Ahttps%3A//github.com/uhhhci/RealityFusion%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01225v1&entry.124074799=Read"},
{"title": "SAM-guided Graph Cut for 3D Instance Segmentation", "author": "Haoyu Guo and He Zhu and Sida Peng and Yuang Wang and Yujun Shen and Ruizhen Hu and Xiaowei Zhou", "abstract": "  This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.\n", "link": "http://arxiv.org/abs/2312.08372v3", "date": "2024-08-02", "relevancy": 2.916, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5935}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-guided%20Graph%20Cut%20for%203D%20Instance%20Segmentation&body=Title%3A%20SAM-guided%20Graph%20Cut%20for%203D%20Instance%20Segmentation%0AAuthor%3A%20Haoyu%20Guo%20and%20He%20Zhu%20and%20Sida%20Peng%20and%20Yuang%20Wang%20and%20Yujun%20Shen%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%203D%20instance%20segmentation%20by%0Asimultaneously%20leveraging%203D%20geometric%20and%20multi-view%20image%20information.%20Many%0Aprevious%20works%20have%20applied%20deep%20learning%20techniques%20to%203D%20point%20clouds%20for%0Ainstance%20segmentation.%20However%2C%20these%20methods%20often%20failed%20to%20generalize%20to%0Avarious%20types%20of%20scenes%20due%20to%20the%20scarcity%20and%20low-diversity%20of%20labeled%203D%0Apoint%20cloud%20data.%20Some%20recent%20works%20have%20attempted%20to%20lift%202D%20instance%0Asegmentations%20to%203D%20within%20a%20bottom-up%20framework.%20The%20inconsistency%20in%202D%0Ainstance%20segmentations%20among%20views%20can%20substantially%20degrade%20the%20performance%20of%0A3D%20segmentation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%203D-to-2D%20query%20framework%20to%0Aeffectively%20exploit%202D%20segmentation%20models%20for%203D%20instance%20segmentation.%0ASpecifically%2C%20we%20pre-segment%20the%20scene%20into%20several%20superpoints%20in%203D%2C%0Aformulating%20the%20task%20into%20a%20graph%20cut%20problem.%20The%20superpoint%20graph%20is%0Aconstructed%20based%20on%202D%20segmentation%20models%2C%20where%20node%20features%20are%20obtained%0Afrom%20multi-view%20image%20features%20and%20edge%20weights%20are%20computed%20based%20on%0Amulti-view%20segmentation%20results%2C%20enabling%20the%20better%20generalization%20ability.%20To%0Aprocess%20the%20graph%2C%20we%20train%20a%20graph%20neural%20network%20using%20pseudo%203D%20labels%20from%0A2D%20segmentation%20models.%20Experimental%20results%20on%20the%20ScanNet%2C%20ScanNet%2B%2B%20and%0AKITTI-360%20datasets%20demonstrate%20that%20our%20method%20achieves%20robust%20segmentation%0Aperformance%20and%20can%20generalize%20across%20different%20types%20of%20scenes.%20Our%20project%0Apage%20is%20available%20at%20https%3A//zju3dv.github.io/sam_graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08372v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-guided%2520Graph%2520Cut%2520for%25203D%2520Instance%2520Segmentation%26entry.906535625%3DHaoyu%2520Guo%2520and%2520He%2520Zhu%2520and%2520Sida%2520Peng%2520and%2520Yuang%2520Wang%2520and%2520Yujun%2520Shen%2520and%2520Ruizhen%2520Hu%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%25203D%2520instance%2520segmentation%2520by%250Asimultaneously%2520leveraging%25203D%2520geometric%2520and%2520multi-view%2520image%2520information.%2520Many%250Aprevious%2520works%2520have%2520applied%2520deep%2520learning%2520techniques%2520to%25203D%2520point%2520clouds%2520for%250Ainstance%2520segmentation.%2520However%252C%2520these%2520methods%2520often%2520failed%2520to%2520generalize%2520to%250Avarious%2520types%2520of%2520scenes%2520due%2520to%2520the%2520scarcity%2520and%2520low-diversity%2520of%2520labeled%25203D%250Apoint%2520cloud%2520data.%2520Some%2520recent%2520works%2520have%2520attempted%2520to%2520lift%25202D%2520instance%250Asegmentations%2520to%25203D%2520within%2520a%2520bottom-up%2520framework.%2520The%2520inconsistency%2520in%25202D%250Ainstance%2520segmentations%2520among%2520views%2520can%2520substantially%2520degrade%2520the%2520performance%2520of%250A3D%2520segmentation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%25203D-to-2D%2520query%2520framework%2520to%250Aeffectively%2520exploit%25202D%2520segmentation%2520models%2520for%25203D%2520instance%2520segmentation.%250ASpecifically%252C%2520we%2520pre-segment%2520the%2520scene%2520into%2520several%2520superpoints%2520in%25203D%252C%250Aformulating%2520the%2520task%2520into%2520a%2520graph%2520cut%2520problem.%2520The%2520superpoint%2520graph%2520is%250Aconstructed%2520based%2520on%25202D%2520segmentation%2520models%252C%2520where%2520node%2520features%2520are%2520obtained%250Afrom%2520multi-view%2520image%2520features%2520and%2520edge%2520weights%2520are%2520computed%2520based%2520on%250Amulti-view%2520segmentation%2520results%252C%2520enabling%2520the%2520better%2520generalization%2520ability.%2520To%250Aprocess%2520the%2520graph%252C%2520we%2520train%2520a%2520graph%2520neural%2520network%2520using%2520pseudo%25203D%2520labels%2520from%250A2D%2520segmentation%2520models.%2520Experimental%2520results%2520on%2520the%2520ScanNet%252C%2520ScanNet%252B%252B%2520and%250AKITTI-360%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520robust%2520segmentation%250Aperformance%2520and%2520can%2520generalize%2520across%2520different%2520types%2520of%2520scenes.%2520Our%2520project%250Apage%2520is%2520available%2520at%2520https%253A//zju3dv.github.io/sam_graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08372v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-guided%20Graph%20Cut%20for%203D%20Instance%20Segmentation&entry.906535625=Haoyu%20Guo%20and%20He%20Zhu%20and%20Sida%20Peng%20and%20Yuang%20Wang%20and%20Yujun%20Shen%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%203D%20instance%20segmentation%20by%0Asimultaneously%20leveraging%203D%20geometric%20and%20multi-view%20image%20information.%20Many%0Aprevious%20works%20have%20applied%20deep%20learning%20techniques%20to%203D%20point%20clouds%20for%0Ainstance%20segmentation.%20However%2C%20these%20methods%20often%20failed%20to%20generalize%20to%0Avarious%20types%20of%20scenes%20due%20to%20the%20scarcity%20and%20low-diversity%20of%20labeled%203D%0Apoint%20cloud%20data.%20Some%20recent%20works%20have%20attempted%20to%20lift%202D%20instance%0Asegmentations%20to%203D%20within%20a%20bottom-up%20framework.%20The%20inconsistency%20in%202D%0Ainstance%20segmentations%20among%20views%20can%20substantially%20degrade%20the%20performance%20of%0A3D%20segmentation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%203D-to-2D%20query%20framework%20to%0Aeffectively%20exploit%202D%20segmentation%20models%20for%203D%20instance%20segmentation.%0ASpecifically%2C%20we%20pre-segment%20the%20scene%20into%20several%20superpoints%20in%203D%2C%0Aformulating%20the%20task%20into%20a%20graph%20cut%20problem.%20The%20superpoint%20graph%20is%0Aconstructed%20based%20on%202D%20segmentation%20models%2C%20where%20node%20features%20are%20obtained%0Afrom%20multi-view%20image%20features%20and%20edge%20weights%20are%20computed%20based%20on%0Amulti-view%20segmentation%20results%2C%20enabling%20the%20better%20generalization%20ability.%20To%0Aprocess%20the%20graph%2C%20we%20train%20a%20graph%20neural%20network%20using%20pseudo%203D%20labels%20from%0A2D%20segmentation%20models.%20Experimental%20results%20on%20the%20ScanNet%2C%20ScanNet%2B%2B%20and%0AKITTI-360%20datasets%20demonstrate%20that%20our%20method%20achieves%20robust%20segmentation%0Aperformance%20and%20can%20generalize%20across%20different%20types%20of%20scenes.%20Our%20project%0Apage%20is%20available%20at%20https%3A//zju3dv.github.io/sam_graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08372v3&entry.124074799=Read"},
{"title": "Balanced Residual Distillation Learning for 3D Point Cloud\n  Class-Incremental Semantic Segmentation", "author": "Yuanzhi Su and Siyuan Chen and Yuan-Gen Wang", "abstract": "  Class-incremental learning (CIL) thrives due to its success in processing the\ninflux of information by learning from continuously added new classes while\npreventing catastrophic forgetting about the old ones. It is essential for the\nperformance breakthrough of CIL to effectively refine past knowledge from the\nbase model and balance it with new learning. However, such an issue has not yet\nbeen considered in current research. In this work, we explore the potential of\nCIL from these perspectives and propose a novel balanced residual distillation\nframework (BRD-CIL) to push the performance bar of CIL to a new higher level.\nSpecifically, BRD-CIL designs a residual distillation learning strategy, which\ncan dynamically expand the network structure to capture the residuals between\nthe base and target models, effectively refining the past knowledge.\nFurthermore, BRD-CIL designs a balanced pseudo-label learning strategy by\ngenerating a guidance mask to reduce the preference for old classes, ensuring\nbalanced learning from new and old classes. We apply the proposed BRD-CIL to a\nchallenging 3D point cloud semantic segmentation task where the data are\nunordered and unstructured. Extensive experimental results demonstrate that\nBRD-CIL sets a new benchmark with an outstanding balance capability in\nclass-biased scenarios.\n", "link": "http://arxiv.org/abs/2408.01356v1", "date": "2024-08-02", "relevancy": 2.8399, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5745}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balanced%20Residual%20Distillation%20Learning%20for%203D%20Point%20Cloud%0A%20%20Class-Incremental%20Semantic%20Segmentation&body=Title%3A%20Balanced%20Residual%20Distillation%20Learning%20for%203D%20Point%20Cloud%0A%20%20Class-Incremental%20Semantic%20Segmentation%0AAuthor%3A%20Yuanzhi%20Su%20and%20Siyuan%20Chen%20and%20Yuan-Gen%20Wang%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20thrives%20due%20to%20its%20success%20in%20processing%20the%0Ainflux%20of%20information%20by%20learning%20from%20continuously%20added%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20about%20the%20old%20ones.%20It%20is%20essential%20for%20the%0Aperformance%20breakthrough%20of%20CIL%20to%20effectively%20refine%20past%20knowledge%20from%20the%0Abase%20model%20and%20balance%20it%20with%20new%20learning.%20However%2C%20such%20an%20issue%20has%20not%20yet%0Abeen%20considered%20in%20current%20research.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%0ACIL%20from%20these%20perspectives%20and%20propose%20a%20novel%20balanced%20residual%20distillation%0Aframework%20%28BRD-CIL%29%20to%20push%20the%20performance%20bar%20of%20CIL%20to%20a%20new%20higher%20level.%0ASpecifically%2C%20BRD-CIL%20designs%20a%20residual%20distillation%20learning%20strategy%2C%20which%0Acan%20dynamically%20expand%20the%20network%20structure%20to%20capture%20the%20residuals%20between%0Athe%20base%20and%20target%20models%2C%20effectively%20refining%20the%20past%20knowledge.%0AFurthermore%2C%20BRD-CIL%20designs%20a%20balanced%20pseudo-label%20learning%20strategy%20by%0Agenerating%20a%20guidance%20mask%20to%20reduce%20the%20preference%20for%20old%20classes%2C%20ensuring%0Abalanced%20learning%20from%20new%20and%20old%20classes.%20We%20apply%20the%20proposed%20BRD-CIL%20to%20a%0Achallenging%203D%20point%20cloud%20semantic%20segmentation%20task%20where%20the%20data%20are%0Aunordered%20and%20unstructured.%20Extensive%20experimental%20results%20demonstrate%20that%0ABRD-CIL%20sets%20a%20new%20benchmark%20with%20an%20outstanding%20balance%20capability%20in%0Aclass-biased%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalanced%2520Residual%2520Distillation%2520Learning%2520for%25203D%2520Point%2520Cloud%250A%2520%2520Class-Incremental%2520Semantic%2520Segmentation%26entry.906535625%3DYuanzhi%2520Su%2520and%2520Siyuan%2520Chen%2520and%2520Yuan-Gen%2520Wang%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520%2528CIL%2529%2520thrives%2520due%2520to%2520its%2520success%2520in%2520processing%2520the%250Ainflux%2520of%2520information%2520by%2520learning%2520from%2520continuously%2520added%2520new%2520classes%2520while%250Apreventing%2520catastrophic%2520forgetting%2520about%2520the%2520old%2520ones.%2520It%2520is%2520essential%2520for%2520the%250Aperformance%2520breakthrough%2520of%2520CIL%2520to%2520effectively%2520refine%2520past%2520knowledge%2520from%2520the%250Abase%2520model%2520and%2520balance%2520it%2520with%2520new%2520learning.%2520However%252C%2520such%2520an%2520issue%2520has%2520not%2520yet%250Abeen%2520considered%2520in%2520current%2520research.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520potential%2520of%250ACIL%2520from%2520these%2520perspectives%2520and%2520propose%2520a%2520novel%2520balanced%2520residual%2520distillation%250Aframework%2520%2528BRD-CIL%2529%2520to%2520push%2520the%2520performance%2520bar%2520of%2520CIL%2520to%2520a%2520new%2520higher%2520level.%250ASpecifically%252C%2520BRD-CIL%2520designs%2520a%2520residual%2520distillation%2520learning%2520strategy%252C%2520which%250Acan%2520dynamically%2520expand%2520the%2520network%2520structure%2520to%2520capture%2520the%2520residuals%2520between%250Athe%2520base%2520and%2520target%2520models%252C%2520effectively%2520refining%2520the%2520past%2520knowledge.%250AFurthermore%252C%2520BRD-CIL%2520designs%2520a%2520balanced%2520pseudo-label%2520learning%2520strategy%2520by%250Agenerating%2520a%2520guidance%2520mask%2520to%2520reduce%2520the%2520preference%2520for%2520old%2520classes%252C%2520ensuring%250Abalanced%2520learning%2520from%2520new%2520and%2520old%2520classes.%2520We%2520apply%2520the%2520proposed%2520BRD-CIL%2520to%2520a%250Achallenging%25203D%2520point%2520cloud%2520semantic%2520segmentation%2520task%2520where%2520the%2520data%2520are%250Aunordered%2520and%2520unstructured.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%250ABRD-CIL%2520sets%2520a%2520new%2520benchmark%2520with%2520an%2520outstanding%2520balance%2520capability%2520in%250Aclass-biased%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balanced%20Residual%20Distillation%20Learning%20for%203D%20Point%20Cloud%0A%20%20Class-Incremental%20Semantic%20Segmentation&entry.906535625=Yuanzhi%20Su%20and%20Siyuan%20Chen%20and%20Yuan-Gen%20Wang&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20thrives%20due%20to%20its%20success%20in%20processing%20the%0Ainflux%20of%20information%20by%20learning%20from%20continuously%20added%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20about%20the%20old%20ones.%20It%20is%20essential%20for%20the%0Aperformance%20breakthrough%20of%20CIL%20to%20effectively%20refine%20past%20knowledge%20from%20the%0Abase%20model%20and%20balance%20it%20with%20new%20learning.%20However%2C%20such%20an%20issue%20has%20not%20yet%0Abeen%20considered%20in%20current%20research.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%0ACIL%20from%20these%20perspectives%20and%20propose%20a%20novel%20balanced%20residual%20distillation%0Aframework%20%28BRD-CIL%29%20to%20push%20the%20performance%20bar%20of%20CIL%20to%20a%20new%20higher%20level.%0ASpecifically%2C%20BRD-CIL%20designs%20a%20residual%20distillation%20learning%20strategy%2C%20which%0Acan%20dynamically%20expand%20the%20network%20structure%20to%20capture%20the%20residuals%20between%0Athe%20base%20and%20target%20models%2C%20effectively%20refining%20the%20past%20knowledge.%0AFurthermore%2C%20BRD-CIL%20designs%20a%20balanced%20pseudo-label%20learning%20strategy%20by%0Agenerating%20a%20guidance%20mask%20to%20reduce%20the%20preference%20for%20old%20classes%2C%20ensuring%0Abalanced%20learning%20from%20new%20and%20old%20classes.%20We%20apply%20the%20proposed%20BRD-CIL%20to%20a%0Achallenging%203D%20point%20cloud%20semantic%20segmentation%20task%20where%20the%20data%20are%0Aunordered%20and%20unstructured.%20Extensive%20experimental%20results%20demonstrate%20that%0ABRD-CIL%20sets%20a%20new%20benchmark%20with%20an%20outstanding%20balance%20capability%20in%0Aclass-biased%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01356v1&entry.124074799=Read"},
{"title": "3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN\n  Networks", "author": "Xiaoshuang Li and Mingyuan Meng and Zimo Huang and Lei Bi and Eduardo Delamare and Dagan Feng and Bin Sheng and Jinman Kim", "abstract": "  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide\navailability and low cost. However, as a 2D projection image, PX does not\ncontain 3D anatomical information, and therefore has limited use in dental\napplications that can benefit from 3D information, e.g., tooth angular\nmisa-lignment detection and classification. Reconstructing 3D structures\ndirectly from 2D PX has recently been explored to address limitations with\nexisting methods primarily reliant on Convolutional Neural Networks (CNNs) for\ndirect 2D-to-3D mapping. These methods, however, are unable to correctly infer\ndepth-axis spatial information. In addition, they are limited by the in-trinsic\nlocality of convolution operations, as the convolution kernels only capture the\ninformation of immediate neighborhood pixels. In this study, we propose a\nprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for\n2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction\nstrategy, where 3D images are progressively re-constructed in the 3DPX with\nguidance imposed on the intermediate recon-struction result at each pyramid\nlevel. Further, motivated by the recent ad-vancement of MLPs that show promise\nin capturing fine-grained long-range dependency, our 3DPX integrates MLPs and\nCNNs to improve the semantic understanding during reconstruction. Extensive\nexperiments on two large datasets involving 464 studies demonstrate that our\n3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,\nincluding standalone MLP and transformers, in reconstruction quality, and also\nim-proves the performance of downstream angular misalignment classification\ntasks.\n", "link": "http://arxiv.org/abs/2408.01292v1", "date": "2024-08-02", "relevancy": 2.8132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DPX%3A%20Progressive%202D-to-3D%20Oral%20Image%20Reconstruction%20with%20Hybrid%20MLP-CNN%0A%20%20Networks&body=Title%3A%203DPX%3A%20Progressive%202D-to-3D%20Oral%20Image%20Reconstruction%20with%20Hybrid%20MLP-CNN%0A%20%20Networks%0AAuthor%3A%20Xiaoshuang%20Li%20and%20Mingyuan%20Meng%20and%20Zimo%20Huang%20and%20Lei%20Bi%20and%20Eduardo%20Delamare%20and%20Dagan%20Feng%20and%20Bin%20Sheng%20and%20Jinman%20Kim%0AAbstract%3A%20%20%20Panoramic%20X-ray%20%28PX%29%20is%20a%20prevalent%20modality%20in%20dental%20practice%20for%20its%20wide%0Aavailability%20and%20low%20cost.%20However%2C%20as%20a%202D%20projection%20image%2C%20PX%20does%20not%0Acontain%203D%20anatomical%20information%2C%20and%20therefore%20has%20limited%20use%20in%20dental%0Aapplications%20that%20can%20benefit%20from%203D%20information%2C%20e.g.%2C%20tooth%20angular%0Amisa-lignment%20detection%20and%20classification.%20Reconstructing%203D%20structures%0Adirectly%20from%202D%20PX%20has%20recently%20been%20explored%20to%20address%20limitations%20with%0Aexisting%20methods%20primarily%20reliant%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Adirect%202D-to-3D%20mapping.%20These%20methods%2C%20however%2C%20are%20unable%20to%20correctly%20infer%0Adepth-axis%20spatial%20information.%20In%20addition%2C%20they%20are%20limited%20by%20the%20in-trinsic%0Alocality%20of%20convolution%20operations%2C%20as%20the%20convolution%20kernels%20only%20capture%20the%0Ainformation%20of%20immediate%20neighborhood%20pixels.%20In%20this%20study%2C%20we%20propose%20a%0Aprogressive%20hybrid%20Multilayer%20Perceptron%20%28MLP%29-CNN%20pyra-mid%20network%20%283DPX%29%20for%0A2D-to-3D%20oral%20PX%20reconstruction.%20We%20introduce%20a%20progressive%20reconstruction%0Astrategy%2C%20where%203D%20images%20are%20progressively%20re-constructed%20in%20the%203DPX%20with%0Aguidance%20imposed%20on%20the%20intermediate%20recon-struction%20result%20at%20each%20pyramid%0Alevel.%20Further%2C%20motivated%20by%20the%20recent%20ad-vancement%20of%20MLPs%20that%20show%20promise%0Ain%20capturing%20fine-grained%20long-range%20dependency%2C%20our%203DPX%20integrates%20MLPs%20and%0ACNNs%20to%20improve%20the%20semantic%20understanding%20during%20reconstruction.%20Extensive%0Aexperiments%20on%20two%20large%20datasets%20involving%20464%20studies%20demonstrate%20that%20our%0A3DPX%20outperforms%20state-of-the-art%202D-to-3D%20oral%20reconstruction%20methods%2C%0Aincluding%20standalone%20MLP%20and%20transformers%2C%20in%20reconstruction%20quality%2C%20and%20also%0Aim-proves%20the%20performance%20of%20downstream%20angular%20misalignment%20classification%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DPX%253A%2520Progressive%25202D-to-3D%2520Oral%2520Image%2520Reconstruction%2520with%2520Hybrid%2520MLP-CNN%250A%2520%2520Networks%26entry.906535625%3DXiaoshuang%2520Li%2520and%2520Mingyuan%2520Meng%2520and%2520Zimo%2520Huang%2520and%2520Lei%2520Bi%2520and%2520Eduardo%2520Delamare%2520and%2520Dagan%2520Feng%2520and%2520Bin%2520Sheng%2520and%2520Jinman%2520Kim%26entry.1292438233%3D%2520%2520Panoramic%2520X-ray%2520%2528PX%2529%2520is%2520a%2520prevalent%2520modality%2520in%2520dental%2520practice%2520for%2520its%2520wide%250Aavailability%2520and%2520low%2520cost.%2520However%252C%2520as%2520a%25202D%2520projection%2520image%252C%2520PX%2520does%2520not%250Acontain%25203D%2520anatomical%2520information%252C%2520and%2520therefore%2520has%2520limited%2520use%2520in%2520dental%250Aapplications%2520that%2520can%2520benefit%2520from%25203D%2520information%252C%2520e.g.%252C%2520tooth%2520angular%250Amisa-lignment%2520detection%2520and%2520classification.%2520Reconstructing%25203D%2520structures%250Adirectly%2520from%25202D%2520PX%2520has%2520recently%2520been%2520explored%2520to%2520address%2520limitations%2520with%250Aexisting%2520methods%2520primarily%2520reliant%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520for%250Adirect%25202D-to-3D%2520mapping.%2520These%2520methods%252C%2520however%252C%2520are%2520unable%2520to%2520correctly%2520infer%250Adepth-axis%2520spatial%2520information.%2520In%2520addition%252C%2520they%2520are%2520limited%2520by%2520the%2520in-trinsic%250Alocality%2520of%2520convolution%2520operations%252C%2520as%2520the%2520convolution%2520kernels%2520only%2520capture%2520the%250Ainformation%2520of%2520immediate%2520neighborhood%2520pixels.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Aprogressive%2520hybrid%2520Multilayer%2520Perceptron%2520%2528MLP%2529-CNN%2520pyra-mid%2520network%2520%25283DPX%2529%2520for%250A2D-to-3D%2520oral%2520PX%2520reconstruction.%2520We%2520introduce%2520a%2520progressive%2520reconstruction%250Astrategy%252C%2520where%25203D%2520images%2520are%2520progressively%2520re-constructed%2520in%2520the%25203DPX%2520with%250Aguidance%2520imposed%2520on%2520the%2520intermediate%2520recon-struction%2520result%2520at%2520each%2520pyramid%250Alevel.%2520Further%252C%2520motivated%2520by%2520the%2520recent%2520ad-vancement%2520of%2520MLPs%2520that%2520show%2520promise%250Ain%2520capturing%2520fine-grained%2520long-range%2520dependency%252C%2520our%25203DPX%2520integrates%2520MLPs%2520and%250ACNNs%2520to%2520improve%2520the%2520semantic%2520understanding%2520during%2520reconstruction.%2520Extensive%250Aexperiments%2520on%2520two%2520large%2520datasets%2520involving%2520464%2520studies%2520demonstrate%2520that%2520our%250A3DPX%2520outperforms%2520state-of-the-art%25202D-to-3D%2520oral%2520reconstruction%2520methods%252C%250Aincluding%2520standalone%2520MLP%2520and%2520transformers%252C%2520in%2520reconstruction%2520quality%252C%2520and%2520also%250Aim-proves%2520the%2520performance%2520of%2520downstream%2520angular%2520misalignment%2520classification%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DPX%3A%20Progressive%202D-to-3D%20Oral%20Image%20Reconstruction%20with%20Hybrid%20MLP-CNN%0A%20%20Networks&entry.906535625=Xiaoshuang%20Li%20and%20Mingyuan%20Meng%20and%20Zimo%20Huang%20and%20Lei%20Bi%20and%20Eduardo%20Delamare%20and%20Dagan%20Feng%20and%20Bin%20Sheng%20and%20Jinman%20Kim&entry.1292438233=%20%20Panoramic%20X-ray%20%28PX%29%20is%20a%20prevalent%20modality%20in%20dental%20practice%20for%20its%20wide%0Aavailability%20and%20low%20cost.%20However%2C%20as%20a%202D%20projection%20image%2C%20PX%20does%20not%0Acontain%203D%20anatomical%20information%2C%20and%20therefore%20has%20limited%20use%20in%20dental%0Aapplications%20that%20can%20benefit%20from%203D%20information%2C%20e.g.%2C%20tooth%20angular%0Amisa-lignment%20detection%20and%20classification.%20Reconstructing%203D%20structures%0Adirectly%20from%202D%20PX%20has%20recently%20been%20explored%20to%20address%20limitations%20with%0Aexisting%20methods%20primarily%20reliant%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Adirect%202D-to-3D%20mapping.%20These%20methods%2C%20however%2C%20are%20unable%20to%20correctly%20infer%0Adepth-axis%20spatial%20information.%20In%20addition%2C%20they%20are%20limited%20by%20the%20in-trinsic%0Alocality%20of%20convolution%20operations%2C%20as%20the%20convolution%20kernels%20only%20capture%20the%0Ainformation%20of%20immediate%20neighborhood%20pixels.%20In%20this%20study%2C%20we%20propose%20a%0Aprogressive%20hybrid%20Multilayer%20Perceptron%20%28MLP%29-CNN%20pyra-mid%20network%20%283DPX%29%20for%0A2D-to-3D%20oral%20PX%20reconstruction.%20We%20introduce%20a%20progressive%20reconstruction%0Astrategy%2C%20where%203D%20images%20are%20progressively%20re-constructed%20in%20the%203DPX%20with%0Aguidance%20imposed%20on%20the%20intermediate%20recon-struction%20result%20at%20each%20pyramid%0Alevel.%20Further%2C%20motivated%20by%20the%20recent%20ad-vancement%20of%20MLPs%20that%20show%20promise%0Ain%20capturing%20fine-grained%20long-range%20dependency%2C%20our%203DPX%20integrates%20MLPs%20and%0ACNNs%20to%20improve%20the%20semantic%20understanding%20during%20reconstruction.%20Extensive%0Aexperiments%20on%20two%20large%20datasets%20involving%20464%20studies%20demonstrate%20that%20our%0A3DPX%20outperforms%20state-of-the-art%202D-to-3D%20oral%20reconstruction%20methods%2C%0Aincluding%20standalone%20MLP%20and%20transformers%2C%20in%20reconstruction%20quality%2C%20and%20also%0Aim-proves%20the%20performance%20of%20downstream%20angular%20misalignment%20classification%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01292v1&entry.124074799=Read"},
{"title": "S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from\n  a Single Sketch", "author": "Zidu Wang and Xiangyu Zhu and Jiang Yu and Tianshuo Zhang and Zhen Lei", "abstract": "  3D textured face reconstruction from sketches applicable in many scenarios\nsuch as animation, 3D avatars, artistic design, missing people search, etc., is\na highly promising but underdeveloped research topic. On the one hand, the\nstylistic diversity of sketches leads to existing sketch-to-3D-face methods\nonly being able to handle pose-limited and realistically shaded sketches. On\nthe other hand, texture plays a vital role in representing facial appearance,\nyet sketches lack this information, necessitating additional texture control in\nthe reconstruction process. This paper proposes a novel method for\nreconstructing controllable textured and detailed 3D faces from sketches, named\nS2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework\nthat directly reconstructs detailed geometry from the input sketch. To keep\ngeometry consistent with the delicate strokes of the sketch, we propose a novel\nsketch-to-geometry loss that ensures the reconstruction accurately fits the\ninput features like dimples and wrinkles. Our training strategies do not rely\non hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches.\nFurthermore, S2TD-Face introduces a texture control module utilizing text\nprompts to select the most suitable textures from a library and seamlessly\nintegrate them into the geometry, resulting in a 3D detailed face with\ncontrollable texture. S2TD-Face surpasses existing state-of-the-art methods in\nextensive quantitative and qualitative experiments. Our project is available at\nhttps://github.com/wang-zidu/S2TD-Face .\n", "link": "http://arxiv.org/abs/2408.01218v1", "date": "2024-08-02", "relevancy": 2.7477, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.561}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2TD-Face%3A%20Reconstruct%20a%20Detailed%203D%20Face%20with%20Controllable%20Texture%20from%0A%20%20a%20Single%20Sketch&body=Title%3A%20S2TD-Face%3A%20Reconstruct%20a%20Detailed%203D%20Face%20with%20Controllable%20Texture%20from%0A%20%20a%20Single%20Sketch%0AAuthor%3A%20Zidu%20Wang%20and%20Xiangyu%20Zhu%20and%20Jiang%20Yu%20and%20Tianshuo%20Zhang%20and%20Zhen%20Lei%0AAbstract%3A%20%20%203D%20textured%20face%20reconstruction%20from%20sketches%20applicable%20in%20many%20scenarios%0Asuch%20as%20animation%2C%203D%20avatars%2C%20artistic%20design%2C%20missing%20people%20search%2C%20etc.%2C%20is%0Aa%20highly%20promising%20but%20underdeveloped%20research%20topic.%20On%20the%20one%20hand%2C%20the%0Astylistic%20diversity%20of%20sketches%20leads%20to%20existing%20sketch-to-3D-face%20methods%0Aonly%20being%20able%20to%20handle%20pose-limited%20and%20realistically%20shaded%20sketches.%20On%0Athe%20other%20hand%2C%20texture%20plays%20a%20vital%20role%20in%20representing%20facial%20appearance%2C%0Ayet%20sketches%20lack%20this%20information%2C%20necessitating%20additional%20texture%20control%20in%0Athe%20reconstruction%20process.%20This%20paper%20proposes%20a%20novel%20method%20for%0Areconstructing%20controllable%20textured%20and%20detailed%203D%20faces%20from%20sketches%2C%20named%0AS2TD-Face.%20S2TD-Face%20introduces%20a%20two-stage%20geometry%20reconstruction%20framework%0Athat%20directly%20reconstructs%20detailed%20geometry%20from%20the%20input%20sketch.%20To%20keep%0Ageometry%20consistent%20with%20the%20delicate%20strokes%20of%20the%20sketch%2C%20we%20propose%20a%20novel%0Asketch-to-geometry%20loss%20that%20ensures%20the%20reconstruction%20accurately%20fits%20the%0Ainput%20features%20like%20dimples%20and%20wrinkles.%20Our%20training%20strategies%20do%20not%20rely%0Aon%20hard-to-obtain%203D%20face%20scanning%20data%20or%20labor-intensive%20hand-drawn%20sketches.%0AFurthermore%2C%20S2TD-Face%20introduces%20a%20texture%20control%20module%20utilizing%20text%0Aprompts%20to%20select%20the%20most%20suitable%20textures%20from%20a%20library%20and%20seamlessly%0Aintegrate%20them%20into%20the%20geometry%2C%20resulting%20in%20a%203D%20detailed%20face%20with%0Acontrollable%20texture.%20S2TD-Face%20surpasses%20existing%20state-of-the-art%20methods%20in%0Aextensive%20quantitative%20and%20qualitative%20experiments.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/wang-zidu/S2TD-Face%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2TD-Face%253A%2520Reconstruct%2520a%2520Detailed%25203D%2520Face%2520with%2520Controllable%2520Texture%2520from%250A%2520%2520a%2520Single%2520Sketch%26entry.906535625%3DZidu%2520Wang%2520and%2520Xiangyu%2520Zhu%2520and%2520Jiang%2520Yu%2520and%2520Tianshuo%2520Zhang%2520and%2520Zhen%2520Lei%26entry.1292438233%3D%2520%25203D%2520textured%2520face%2520reconstruction%2520from%2520sketches%2520applicable%2520in%2520many%2520scenarios%250Asuch%2520as%2520animation%252C%25203D%2520avatars%252C%2520artistic%2520design%252C%2520missing%2520people%2520search%252C%2520etc.%252C%2520is%250Aa%2520highly%2520promising%2520but%2520underdeveloped%2520research%2520topic.%2520On%2520the%2520one%2520hand%252C%2520the%250Astylistic%2520diversity%2520of%2520sketches%2520leads%2520to%2520existing%2520sketch-to-3D-face%2520methods%250Aonly%2520being%2520able%2520to%2520handle%2520pose-limited%2520and%2520realistically%2520shaded%2520sketches.%2520On%250Athe%2520other%2520hand%252C%2520texture%2520plays%2520a%2520vital%2520role%2520in%2520representing%2520facial%2520appearance%252C%250Ayet%2520sketches%2520lack%2520this%2520information%252C%2520necessitating%2520additional%2520texture%2520control%2520in%250Athe%2520reconstruction%2520process.%2520This%2520paper%2520proposes%2520a%2520novel%2520method%2520for%250Areconstructing%2520controllable%2520textured%2520and%2520detailed%25203D%2520faces%2520from%2520sketches%252C%2520named%250AS2TD-Face.%2520S2TD-Face%2520introduces%2520a%2520two-stage%2520geometry%2520reconstruction%2520framework%250Athat%2520directly%2520reconstructs%2520detailed%2520geometry%2520from%2520the%2520input%2520sketch.%2520To%2520keep%250Ageometry%2520consistent%2520with%2520the%2520delicate%2520strokes%2520of%2520the%2520sketch%252C%2520we%2520propose%2520a%2520novel%250Asketch-to-geometry%2520loss%2520that%2520ensures%2520the%2520reconstruction%2520accurately%2520fits%2520the%250Ainput%2520features%2520like%2520dimples%2520and%2520wrinkles.%2520Our%2520training%2520strategies%2520do%2520not%2520rely%250Aon%2520hard-to-obtain%25203D%2520face%2520scanning%2520data%2520or%2520labor-intensive%2520hand-drawn%2520sketches.%250AFurthermore%252C%2520S2TD-Face%2520introduces%2520a%2520texture%2520control%2520module%2520utilizing%2520text%250Aprompts%2520to%2520select%2520the%2520most%2520suitable%2520textures%2520from%2520a%2520library%2520and%2520seamlessly%250Aintegrate%2520them%2520into%2520the%2520geometry%252C%2520resulting%2520in%2520a%25203D%2520detailed%2520face%2520with%250Acontrollable%2520texture.%2520S2TD-Face%2520surpasses%2520existing%2520state-of-the-art%2520methods%2520in%250Aextensive%2520quantitative%2520and%2520qualitative%2520experiments.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/wang-zidu/S2TD-Face%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2TD-Face%3A%20Reconstruct%20a%20Detailed%203D%20Face%20with%20Controllable%20Texture%20from%0A%20%20a%20Single%20Sketch&entry.906535625=Zidu%20Wang%20and%20Xiangyu%20Zhu%20and%20Jiang%20Yu%20and%20Tianshuo%20Zhang%20and%20Zhen%20Lei&entry.1292438233=%20%203D%20textured%20face%20reconstruction%20from%20sketches%20applicable%20in%20many%20scenarios%0Asuch%20as%20animation%2C%203D%20avatars%2C%20artistic%20design%2C%20missing%20people%20search%2C%20etc.%2C%20is%0Aa%20highly%20promising%20but%20underdeveloped%20research%20topic.%20On%20the%20one%20hand%2C%20the%0Astylistic%20diversity%20of%20sketches%20leads%20to%20existing%20sketch-to-3D-face%20methods%0Aonly%20being%20able%20to%20handle%20pose-limited%20and%20realistically%20shaded%20sketches.%20On%0Athe%20other%20hand%2C%20texture%20plays%20a%20vital%20role%20in%20representing%20facial%20appearance%2C%0Ayet%20sketches%20lack%20this%20information%2C%20necessitating%20additional%20texture%20control%20in%0Athe%20reconstruction%20process.%20This%20paper%20proposes%20a%20novel%20method%20for%0Areconstructing%20controllable%20textured%20and%20detailed%203D%20faces%20from%20sketches%2C%20named%0AS2TD-Face.%20S2TD-Face%20introduces%20a%20two-stage%20geometry%20reconstruction%20framework%0Athat%20directly%20reconstructs%20detailed%20geometry%20from%20the%20input%20sketch.%20To%20keep%0Ageometry%20consistent%20with%20the%20delicate%20strokes%20of%20the%20sketch%2C%20we%20propose%20a%20novel%0Asketch-to-geometry%20loss%20that%20ensures%20the%20reconstruction%20accurately%20fits%20the%0Ainput%20features%20like%20dimples%20and%20wrinkles.%20Our%20training%20strategies%20do%20not%20rely%0Aon%20hard-to-obtain%203D%20face%20scanning%20data%20or%20labor-intensive%20hand-drawn%20sketches.%0AFurthermore%2C%20S2TD-Face%20introduces%20a%20texture%20control%20module%20utilizing%20text%0Aprompts%20to%20select%20the%20most%20suitable%20textures%20from%20a%20library%20and%20seamlessly%0Aintegrate%20them%20into%20the%20geometry%2C%20resulting%20in%20a%203D%20detailed%20face%20with%0Acontrollable%20texture.%20S2TD-Face%20surpasses%20existing%20state-of-the-art%20methods%20in%0Aextensive%20quantitative%20and%20qualitative%20experiments.%20Our%20project%20is%20available%20at%0Ahttps%3A//github.com/wang-zidu/S2TD-Face%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01218v1&entry.124074799=Read"},
{"title": "Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework", "author": "Liuyuan Wen", "abstract": "  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n", "link": "http://arxiv.org/abs/2408.01284v1", "date": "2024-08-02", "relevancy": 2.65, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5322}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5313}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-Of-Distribution%20Detection%20for%20Audio-visual%20Generalized%20Zero-Shot%0A%20%20Learning%3A%20A%20General%20Framework&body=Title%3A%20Out-Of-Distribution%20Detection%20for%20Audio-visual%20Generalized%20Zero-Shot%0A%20%20Learning%3A%20A%20General%20Framework%0AAuthor%3A%20Liuyuan%20Wen%0AAbstract%3A%20%20%20Generalized%20Zero-Shot%20Learning%20%28GZSL%29%20is%20a%20challenging%20task%20requiring%0Aaccurate%20classification%20of%20both%20seen%20and%20unseen%20classes.%20Within%20this%20domain%2C%0AAudio-visual%20GZSL%20emerges%20as%20an%20extremely%20exciting%20yet%20difficult%20task%2C%20given%0Athe%20inclusion%20of%20both%20visual%20and%20acoustic%20features%20as%20multi-modal%20inputs.%0AExisting%20efforts%20in%20this%20field%20mostly%20utilize%20either%20embedding-based%20or%0Agenerative-based%20methods.%20However%2C%20generative%20training%20is%20difficult%20and%0Aunstable%2C%20while%20embedding-based%20methods%20often%20encounter%20domain%20shift%20problem.%0AThus%2C%20we%20find%20it%20promising%20to%20integrate%20both%20methods%20into%20a%20unified%20framework%0Ato%20leverage%20their%20advantages%20while%20mitigating%20their%20respective%20disadvantages.%0AOur%20study%20introduces%20a%20general%20framework%20employing%20out-of-distribution%20%28OOD%29%0Adetection%2C%20aiming%20to%20harness%20the%20strengths%20of%20both%20approaches.%20We%20first%20employ%0Agenerative%20adversarial%20networks%20to%20synthesize%20unseen%20features%2C%20enabling%20the%0Atraining%20of%20an%20OOD%20detector%20alongside%20classifiers%20for%20seen%20and%20unseen%20classes.%0AThis%20detector%20determines%20whether%20a%20test%20feature%20belongs%20to%20seen%20or%20unseen%0Aclasses%2C%20followed%20by%20classification%20utilizing%20separate%20classifiers%20for%20each%0Afeature%20type.%20We%20test%20our%20framework%20on%20three%20popular%20audio-visual%20datasets%20and%0Aobserve%20a%20significant%20improvement%20comparing%20to%20existing%20state-of-the-art%20works.%0ACodes%20can%20be%20found%20in%20https%3A//github.com/liuyuan-wen/AV-OOD-GZSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-Of-Distribution%2520Detection%2520for%2520Audio-visual%2520Generalized%2520Zero-Shot%250A%2520%2520Learning%253A%2520A%2520General%2520Framework%26entry.906535625%3DLiuyuan%2520Wen%26entry.1292438233%3D%2520%2520Generalized%2520Zero-Shot%2520Learning%2520%2528GZSL%2529%2520is%2520a%2520challenging%2520task%2520requiring%250Aaccurate%2520classification%2520of%2520both%2520seen%2520and%2520unseen%2520classes.%2520Within%2520this%2520domain%252C%250AAudio-visual%2520GZSL%2520emerges%2520as%2520an%2520extremely%2520exciting%2520yet%2520difficult%2520task%252C%2520given%250Athe%2520inclusion%2520of%2520both%2520visual%2520and%2520acoustic%2520features%2520as%2520multi-modal%2520inputs.%250AExisting%2520efforts%2520in%2520this%2520field%2520mostly%2520utilize%2520either%2520embedding-based%2520or%250Agenerative-based%2520methods.%2520However%252C%2520generative%2520training%2520is%2520difficult%2520and%250Aunstable%252C%2520while%2520embedding-based%2520methods%2520often%2520encounter%2520domain%2520shift%2520problem.%250AThus%252C%2520we%2520find%2520it%2520promising%2520to%2520integrate%2520both%2520methods%2520into%2520a%2520unified%2520framework%250Ato%2520leverage%2520their%2520advantages%2520while%2520mitigating%2520their%2520respective%2520disadvantages.%250AOur%2520study%2520introduces%2520a%2520general%2520framework%2520employing%2520out-of-distribution%2520%2528OOD%2529%250Adetection%252C%2520aiming%2520to%2520harness%2520the%2520strengths%2520of%2520both%2520approaches.%2520We%2520first%2520employ%250Agenerative%2520adversarial%2520networks%2520to%2520synthesize%2520unseen%2520features%252C%2520enabling%2520the%250Atraining%2520of%2520an%2520OOD%2520detector%2520alongside%2520classifiers%2520for%2520seen%2520and%2520unseen%2520classes.%250AThis%2520detector%2520determines%2520whether%2520a%2520test%2520feature%2520belongs%2520to%2520seen%2520or%2520unseen%250Aclasses%252C%2520followed%2520by%2520classification%2520utilizing%2520separate%2520classifiers%2520for%2520each%250Afeature%2520type.%2520We%2520test%2520our%2520framework%2520on%2520three%2520popular%2520audio-visual%2520datasets%2520and%250Aobserve%2520a%2520significant%2520improvement%2520comparing%2520to%2520existing%2520state-of-the-art%2520works.%250ACodes%2520can%2520be%2520found%2520in%2520https%253A//github.com/liuyuan-wen/AV-OOD-GZSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-Of-Distribution%20Detection%20for%20Audio-visual%20Generalized%20Zero-Shot%0A%20%20Learning%3A%20A%20General%20Framework&entry.906535625=Liuyuan%20Wen&entry.1292438233=%20%20Generalized%20Zero-Shot%20Learning%20%28GZSL%29%20is%20a%20challenging%20task%20requiring%0Aaccurate%20classification%20of%20both%20seen%20and%20unseen%20classes.%20Within%20this%20domain%2C%0AAudio-visual%20GZSL%20emerges%20as%20an%20extremely%20exciting%20yet%20difficult%20task%2C%20given%0Athe%20inclusion%20of%20both%20visual%20and%20acoustic%20features%20as%20multi-modal%20inputs.%0AExisting%20efforts%20in%20this%20field%20mostly%20utilize%20either%20embedding-based%20or%0Agenerative-based%20methods.%20However%2C%20generative%20training%20is%20difficult%20and%0Aunstable%2C%20while%20embedding-based%20methods%20often%20encounter%20domain%20shift%20problem.%0AThus%2C%20we%20find%20it%20promising%20to%20integrate%20both%20methods%20into%20a%20unified%20framework%0Ato%20leverage%20their%20advantages%20while%20mitigating%20their%20respective%20disadvantages.%0AOur%20study%20introduces%20a%20general%20framework%20employing%20out-of-distribution%20%28OOD%29%0Adetection%2C%20aiming%20to%20harness%20the%20strengths%20of%20both%20approaches.%20We%20first%20employ%0Agenerative%20adversarial%20networks%20to%20synthesize%20unseen%20features%2C%20enabling%20the%0Atraining%20of%20an%20OOD%20detector%20alongside%20classifiers%20for%20seen%20and%20unseen%20classes.%0AThis%20detector%20determines%20whether%20a%20test%20feature%20belongs%20to%20seen%20or%20unseen%0Aclasses%2C%20followed%20by%20classification%20utilizing%20separate%20classifiers%20for%20each%0Afeature%20type.%20We%20test%20our%20framework%20on%20three%20popular%20audio-visual%20datasets%20and%0Aobserve%20a%20significant%20improvement%20comparing%20to%20existing%20state-of-the-art%20works.%0ACodes%20can%20be%20found%20in%20https%3A//github.com/liuyuan-wen/AV-OOD-GZSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01284v1&entry.124074799=Read"},
{"title": "A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity\n  Recognition", "author": "Hiroki Matsutani and Radu Marculescu", "abstract": "  In this paper, we introduce a low-cost and low-power tiny supervised\non-device learning (ODL) core that can address the distributional shift of\ninput data for human activity recognition. Although ODL for resource-limited\nedge devices has been studied recently, how exactly to provide the training\nlabels to these devices at runtime remains an open-issue. To address this\nproblem, we propose to combine an automatic data pruning with supervised ODL to\nreduce the number queries needed to acquire predicted labels from a nearby\nteacher device and thus save power consumption during model retraining. The\ndata pruning threshold is automatically tuned, eliminating a manual threshold\ntuning. As a tinyML solution at a few mW for the human activity recognition, we\ndesign a supervised ODL core that supports our automatic data pruning using a\n45nm CMOS process technology. We show that the required memory size for the\ncore is smaller than the same-shaped multilayer perceptron (MLP) and the power\nconsumption is only 3.39mW. Experiments using a human activity recognition\ndataset show that the proposed automatic data pruning reduces the communication\nvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.\n", "link": "http://arxiv.org/abs/2408.01283v1", "date": "2024-08-02", "relevancy": 2.6487, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5593}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5182}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tiny%20Supervised%20ODL%20Core%20with%20Auto%20Data%20Pruning%20for%20Human%20Activity%0A%20%20Recognition&body=Title%3A%20A%20Tiny%20Supervised%20ODL%20Core%20with%20Auto%20Data%20Pruning%20for%20Human%20Activity%0A%20%20Recognition%0AAuthor%3A%20Hiroki%20Matsutani%20and%20Radu%20Marculescu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20low-cost%20and%20low-power%20tiny%20supervised%0Aon-device%20learning%20%28ODL%29%20core%20that%20can%20address%20the%20distributional%20shift%20of%0Ainput%20data%20for%20human%20activity%20recognition.%20Although%20ODL%20for%20resource-limited%0Aedge%20devices%20has%20been%20studied%20recently%2C%20how%20exactly%20to%20provide%20the%20training%0Alabels%20to%20these%20devices%20at%20runtime%20remains%20an%20open-issue.%20To%20address%20this%0Aproblem%2C%20we%20propose%20to%20combine%20an%20automatic%20data%20pruning%20with%20supervised%20ODL%20to%0Areduce%20the%20number%20queries%20needed%20to%20acquire%20predicted%20labels%20from%20a%20nearby%0Ateacher%20device%20and%20thus%20save%20power%20consumption%20during%20model%20retraining.%20The%0Adata%20pruning%20threshold%20is%20automatically%20tuned%2C%20eliminating%20a%20manual%20threshold%0Atuning.%20As%20a%20tinyML%20solution%20at%20a%20few%20mW%20for%20the%20human%20activity%20recognition%2C%20we%0Adesign%20a%20supervised%20ODL%20core%20that%20supports%20our%20automatic%20data%20pruning%20using%20a%0A45nm%20CMOS%20process%20technology.%20We%20show%20that%20the%20required%20memory%20size%20for%20the%0Acore%20is%20smaller%20than%20the%20same-shaped%20multilayer%20perceptron%20%28MLP%29%20and%20the%20power%0Aconsumption%20is%20only%203.39mW.%20Experiments%20using%20a%20human%20activity%20recognition%0Adataset%20show%20that%20the%20proposed%20automatic%20data%20pruning%20reduces%20the%20communication%0Avolume%20by%2055.7%25%20and%20power%20consumption%20accordingly%20with%20only%200.9%25%20accuracy%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tiny%2520Supervised%2520ODL%2520Core%2520with%2520Auto%2520Data%2520Pruning%2520for%2520Human%2520Activity%250A%2520%2520Recognition%26entry.906535625%3DHiroki%2520Matsutani%2520and%2520Radu%2520Marculescu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520low-cost%2520and%2520low-power%2520tiny%2520supervised%250Aon-device%2520learning%2520%2528ODL%2529%2520core%2520that%2520can%2520address%2520the%2520distributional%2520shift%2520of%250Ainput%2520data%2520for%2520human%2520activity%2520recognition.%2520Although%2520ODL%2520for%2520resource-limited%250Aedge%2520devices%2520has%2520been%2520studied%2520recently%252C%2520how%2520exactly%2520to%2520provide%2520the%2520training%250Alabels%2520to%2520these%2520devices%2520at%2520runtime%2520remains%2520an%2520open-issue.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520to%2520combine%2520an%2520automatic%2520data%2520pruning%2520with%2520supervised%2520ODL%2520to%250Areduce%2520the%2520number%2520queries%2520needed%2520to%2520acquire%2520predicted%2520labels%2520from%2520a%2520nearby%250Ateacher%2520device%2520and%2520thus%2520save%2520power%2520consumption%2520during%2520model%2520retraining.%2520The%250Adata%2520pruning%2520threshold%2520is%2520automatically%2520tuned%252C%2520eliminating%2520a%2520manual%2520threshold%250Atuning.%2520As%2520a%2520tinyML%2520solution%2520at%2520a%2520few%2520mW%2520for%2520the%2520human%2520activity%2520recognition%252C%2520we%250Adesign%2520a%2520supervised%2520ODL%2520core%2520that%2520supports%2520our%2520automatic%2520data%2520pruning%2520using%2520a%250A45nm%2520CMOS%2520process%2520technology.%2520We%2520show%2520that%2520the%2520required%2520memory%2520size%2520for%2520the%250Acore%2520is%2520smaller%2520than%2520the%2520same-shaped%2520multilayer%2520perceptron%2520%2528MLP%2529%2520and%2520the%2520power%250Aconsumption%2520is%2520only%25203.39mW.%2520Experiments%2520using%2520a%2520human%2520activity%2520recognition%250Adataset%2520show%2520that%2520the%2520proposed%2520automatic%2520data%2520pruning%2520reduces%2520the%2520communication%250Avolume%2520by%252055.7%2525%2520and%2520power%2520consumption%2520accordingly%2520with%2520only%25200.9%2525%2520accuracy%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tiny%20Supervised%20ODL%20Core%20with%20Auto%20Data%20Pruning%20for%20Human%20Activity%0A%20%20Recognition&entry.906535625=Hiroki%20Matsutani%20and%20Radu%20Marculescu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20low-cost%20and%20low-power%20tiny%20supervised%0Aon-device%20learning%20%28ODL%29%20core%20that%20can%20address%20the%20distributional%20shift%20of%0Ainput%20data%20for%20human%20activity%20recognition.%20Although%20ODL%20for%20resource-limited%0Aedge%20devices%20has%20been%20studied%20recently%2C%20how%20exactly%20to%20provide%20the%20training%0Alabels%20to%20these%20devices%20at%20runtime%20remains%20an%20open-issue.%20To%20address%20this%0Aproblem%2C%20we%20propose%20to%20combine%20an%20automatic%20data%20pruning%20with%20supervised%20ODL%20to%0Areduce%20the%20number%20queries%20needed%20to%20acquire%20predicted%20labels%20from%20a%20nearby%0Ateacher%20device%20and%20thus%20save%20power%20consumption%20during%20model%20retraining.%20The%0Adata%20pruning%20threshold%20is%20automatically%20tuned%2C%20eliminating%20a%20manual%20threshold%0Atuning.%20As%20a%20tinyML%20solution%20at%20a%20few%20mW%20for%20the%20human%20activity%20recognition%2C%20we%0Adesign%20a%20supervised%20ODL%20core%20that%20supports%20our%20automatic%20data%20pruning%20using%20a%0A45nm%20CMOS%20process%20technology.%20We%20show%20that%20the%20required%20memory%20size%20for%20the%0Acore%20is%20smaller%20than%20the%20same-shaped%20multilayer%20perceptron%20%28MLP%29%20and%20the%20power%0Aconsumption%20is%20only%203.39mW.%20Experiments%20using%20a%20human%20activity%20recognition%0Adataset%20show%20that%20the%20proposed%20automatic%20data%20pruning%20reduces%20the%20communication%0Avolume%20by%2055.7%25%20and%20power%20consumption%20accordingly%20with%20only%200.9%25%20accuracy%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01283v1&entry.124074799=Read"},
{"title": "Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive\n  Learning for Cross-Subject EEG-based Emotion Recognition", "author": "Weishan Ye and Zhiguo Zhang and Fei Teng and Min Zhang and Jianhong Wang and Dong Ni and Fali Li and Peng Xu and Zhen Liang", "abstract": "  Electroencephalography (EEG) is an objective tool for emotion recognition\nwith promising applications. However, the scarcity of labeled data remains a\nmajor challenge in this field, limiting the widespread use of EEG-based emotion\nrecognition. In this paper, a semi-supervised Dual-stream Self-Attentive\nAdversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed\nto tackle the challenge of limited labeled data in cross-subject EEG-based\nemotion recognition. The DS-AGC framework includes two parallel streams for\nextracting non-structural and structural EEG features. The non-structural\nstream incorporates a semi-supervised multi-domain adaptation method to\nalleviate distribution discrepancy among labeled source domain, unlabeled\nsource domain, and unknown target domain. The structural stream develops a\ngraph contrastive learning method to extract effective graph-based feature\nrepresentation from multiple EEG channels in a semi-supervised manner. Further,\na self-attentive fusion module is developed for feature fusion, sample\nselection, and emotion recognition, which highlights EEG features more relevant\nto emotions and data samples in the labeled source domain that are closer to\nthe target domain. Extensive experiments conducted on two benchmark databases\n(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out\ncross-validation evaluation scheme show that the proposed model outperforms\nexisting methods under different incomplete label conditions (with an average\nimprovement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its\neffectiveness in addressing the label scarcity problem in cross-subject\nEEG-based emotion recognition.\n", "link": "http://arxiv.org/abs/2308.11635v2", "date": "2024-08-02", "relevancy": 2.6397, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5368}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5243}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Dual-Stream%20Self-Attentive%20Adversarial%20Graph%20Contrastive%0A%20%20Learning%20for%20Cross-Subject%20EEG-based%20Emotion%20Recognition&body=Title%3A%20Semi-Supervised%20Dual-Stream%20Self-Attentive%20Adversarial%20Graph%20Contrastive%0A%20%20Learning%20for%20Cross-Subject%20EEG-based%20Emotion%20Recognition%0AAuthor%3A%20Weishan%20Ye%20and%20Zhiguo%20Zhang%20and%20Fei%20Teng%20and%20Min%20Zhang%20and%20Jianhong%20Wang%20and%20Dong%20Ni%20and%20Fali%20Li%20and%20Peng%20Xu%20and%20Zhen%20Liang%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20is%20an%20objective%20tool%20for%20emotion%20recognition%0Awith%20promising%20applications.%20However%2C%20the%20scarcity%20of%20labeled%20data%20remains%20a%0Amajor%20challenge%20in%20this%20field%2C%20limiting%20the%20widespread%20use%20of%20EEG-based%20emotion%0Arecognition.%20In%20this%20paper%2C%20a%20semi-supervised%20Dual-stream%20Self-Attentive%0AAdversarial%20Graph%20Contrastive%20learning%20framework%20%28termed%20as%20DS-AGC%29%20is%20proposed%0Ato%20tackle%20the%20challenge%20of%20limited%20labeled%20data%20in%20cross-subject%20EEG-based%0Aemotion%20recognition.%20The%20DS-AGC%20framework%20includes%20two%20parallel%20streams%20for%0Aextracting%20non-structural%20and%20structural%20EEG%20features.%20The%20non-structural%0Astream%20incorporates%20a%20semi-supervised%20multi-domain%20adaptation%20method%20to%0Aalleviate%20distribution%20discrepancy%20among%20labeled%20source%20domain%2C%20unlabeled%0Asource%20domain%2C%20and%20unknown%20target%20domain.%20The%20structural%20stream%20develops%20a%0Agraph%20contrastive%20learning%20method%20to%20extract%20effective%20graph-based%20feature%0Arepresentation%20from%20multiple%20EEG%20channels%20in%20a%20semi-supervised%20manner.%20Further%2C%0Aa%20self-attentive%20fusion%20module%20is%20developed%20for%20feature%20fusion%2C%20sample%0Aselection%2C%20and%20emotion%20recognition%2C%20which%20highlights%20EEG%20features%20more%20relevant%0Ato%20emotions%20and%20data%20samples%20in%20the%20labeled%20source%20domain%20that%20are%20closer%20to%0Athe%20target%20domain.%20Extensive%20experiments%20conducted%20on%20two%20benchmark%20databases%0A%28SEED%20and%20SEED-IV%29%20using%20a%20semi-supervised%20cross-subject%20leave-one-subject-out%0Across-validation%20evaluation%20scheme%20show%20that%20the%20proposed%20model%20outperforms%0Aexisting%20methods%20under%20different%20incomplete%20label%20conditions%20%28with%20an%20average%0Aimprovement%20of%205.83%25%20on%20SEED%20and%206.99%25%20on%20SEED-IV%29%2C%20demonstrating%20its%0Aeffectiveness%20in%20addressing%20the%20label%20scarcity%20problem%20in%20cross-subject%0AEEG-based%20emotion%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Dual-Stream%2520Self-Attentive%2520Adversarial%2520Graph%2520Contrastive%250A%2520%2520Learning%2520for%2520Cross-Subject%2520EEG-based%2520Emotion%2520Recognition%26entry.906535625%3DWeishan%2520Ye%2520and%2520Zhiguo%2520Zhang%2520and%2520Fei%2520Teng%2520and%2520Min%2520Zhang%2520and%2520Jianhong%2520Wang%2520and%2520Dong%2520Ni%2520and%2520Fali%2520Li%2520and%2520Peng%2520Xu%2520and%2520Zhen%2520Liang%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520is%2520an%2520objective%2520tool%2520for%2520emotion%2520recognition%250Awith%2520promising%2520applications.%2520However%252C%2520the%2520scarcity%2520of%2520labeled%2520data%2520remains%2520a%250Amajor%2520challenge%2520in%2520this%2520field%252C%2520limiting%2520the%2520widespread%2520use%2520of%2520EEG-based%2520emotion%250Arecognition.%2520In%2520this%2520paper%252C%2520a%2520semi-supervised%2520Dual-stream%2520Self-Attentive%250AAdversarial%2520Graph%2520Contrastive%2520learning%2520framework%2520%2528termed%2520as%2520DS-AGC%2529%2520is%2520proposed%250Ato%2520tackle%2520the%2520challenge%2520of%2520limited%2520labeled%2520data%2520in%2520cross-subject%2520EEG-based%250Aemotion%2520recognition.%2520The%2520DS-AGC%2520framework%2520includes%2520two%2520parallel%2520streams%2520for%250Aextracting%2520non-structural%2520and%2520structural%2520EEG%2520features.%2520The%2520non-structural%250Astream%2520incorporates%2520a%2520semi-supervised%2520multi-domain%2520adaptation%2520method%2520to%250Aalleviate%2520distribution%2520discrepancy%2520among%2520labeled%2520source%2520domain%252C%2520unlabeled%250Asource%2520domain%252C%2520and%2520unknown%2520target%2520domain.%2520The%2520structural%2520stream%2520develops%2520a%250Agraph%2520contrastive%2520learning%2520method%2520to%2520extract%2520effective%2520graph-based%2520feature%250Arepresentation%2520from%2520multiple%2520EEG%2520channels%2520in%2520a%2520semi-supervised%2520manner.%2520Further%252C%250Aa%2520self-attentive%2520fusion%2520module%2520is%2520developed%2520for%2520feature%2520fusion%252C%2520sample%250Aselection%252C%2520and%2520emotion%2520recognition%252C%2520which%2520highlights%2520EEG%2520features%2520more%2520relevant%250Ato%2520emotions%2520and%2520data%2520samples%2520in%2520the%2520labeled%2520source%2520domain%2520that%2520are%2520closer%2520to%250Athe%2520target%2520domain.%2520Extensive%2520experiments%2520conducted%2520on%2520two%2520benchmark%2520databases%250A%2528SEED%2520and%2520SEED-IV%2529%2520using%2520a%2520semi-supervised%2520cross-subject%2520leave-one-subject-out%250Across-validation%2520evaluation%2520scheme%2520show%2520that%2520the%2520proposed%2520model%2520outperforms%250Aexisting%2520methods%2520under%2520different%2520incomplete%2520label%2520conditions%2520%2528with%2520an%2520average%250Aimprovement%2520of%25205.83%2525%2520on%2520SEED%2520and%25206.99%2525%2520on%2520SEED-IV%2529%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520addressing%2520the%2520label%2520scarcity%2520problem%2520in%2520cross-subject%250AEEG-based%2520emotion%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.11635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Dual-Stream%20Self-Attentive%20Adversarial%20Graph%20Contrastive%0A%20%20Learning%20for%20Cross-Subject%20EEG-based%20Emotion%20Recognition&entry.906535625=Weishan%20Ye%20and%20Zhiguo%20Zhang%20and%20Fei%20Teng%20and%20Min%20Zhang%20and%20Jianhong%20Wang%20and%20Dong%20Ni%20and%20Fali%20Li%20and%20Peng%20Xu%20and%20Zhen%20Liang&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20is%20an%20objective%20tool%20for%20emotion%20recognition%0Awith%20promising%20applications.%20However%2C%20the%20scarcity%20of%20labeled%20data%20remains%20a%0Amajor%20challenge%20in%20this%20field%2C%20limiting%20the%20widespread%20use%20of%20EEG-based%20emotion%0Arecognition.%20In%20this%20paper%2C%20a%20semi-supervised%20Dual-stream%20Self-Attentive%0AAdversarial%20Graph%20Contrastive%20learning%20framework%20%28termed%20as%20DS-AGC%29%20is%20proposed%0Ato%20tackle%20the%20challenge%20of%20limited%20labeled%20data%20in%20cross-subject%20EEG-based%0Aemotion%20recognition.%20The%20DS-AGC%20framework%20includes%20two%20parallel%20streams%20for%0Aextracting%20non-structural%20and%20structural%20EEG%20features.%20The%20non-structural%0Astream%20incorporates%20a%20semi-supervised%20multi-domain%20adaptation%20method%20to%0Aalleviate%20distribution%20discrepancy%20among%20labeled%20source%20domain%2C%20unlabeled%0Asource%20domain%2C%20and%20unknown%20target%20domain.%20The%20structural%20stream%20develops%20a%0Agraph%20contrastive%20learning%20method%20to%20extract%20effective%20graph-based%20feature%0Arepresentation%20from%20multiple%20EEG%20channels%20in%20a%20semi-supervised%20manner.%20Further%2C%0Aa%20self-attentive%20fusion%20module%20is%20developed%20for%20feature%20fusion%2C%20sample%0Aselection%2C%20and%20emotion%20recognition%2C%20which%20highlights%20EEG%20features%20more%20relevant%0Ato%20emotions%20and%20data%20samples%20in%20the%20labeled%20source%20domain%20that%20are%20closer%20to%0Athe%20target%20domain.%20Extensive%20experiments%20conducted%20on%20two%20benchmark%20databases%0A%28SEED%20and%20SEED-IV%29%20using%20a%20semi-supervised%20cross-subject%20leave-one-subject-out%0Across-validation%20evaluation%20scheme%20show%20that%20the%20proposed%20model%20outperforms%0Aexisting%20methods%20under%20different%20incomplete%20label%20conditions%20%28with%20an%20average%0Aimprovement%20of%205.83%25%20on%20SEED%20and%206.99%25%20on%20SEED-IV%29%2C%20demonstrating%20its%0Aeffectiveness%20in%20addressing%20the%20label%20scarcity%20problem%20in%20cross-subject%0AEEG-based%20emotion%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11635v2&entry.124074799=Read"},
{"title": "Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation", "author": "Se Jin Park and Chae Won Kim and Hyeongseop Rha and Minsu Kim and Joanna Hong and Jeong Hun Yeo and Yong Man Ro", "abstract": "  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n", "link": "http://arxiv.org/abs/2406.07867v2", "date": "2024-08-02", "relevancy": 2.5821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5241}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5194}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Go%20Real%20Talk%3A%20Spoken%20Dialogue%20Model%20for%20Face-to-Face%20Conversation&body=Title%3A%20Let%27s%20Go%20Real%20Talk%3A%20Spoken%20Dialogue%20Model%20for%20Face-to-Face%20Conversation%0AAuthor%3A%20Se%20Jin%20Park%20and%20Chae%20Won%20Kim%20and%20Hyeongseop%20Rha%20and%20Minsu%20Kim%20and%20Joanna%20Hong%20and%20Jeong%20Hun%20Yeo%20and%20Yong%20Man%20Ro%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20Face-to-Face%20spoken%20dialogue%20model.%20It%0Aprocesses%20audio-visual%20speech%20from%20user%20input%20and%20generates%20audio-visual%20speech%0Aas%20the%20response%2C%20marking%20the%20initial%20step%20towards%20creating%20an%20avatar%20chatbot%0Asystem%20without%20relying%20on%20intermediate%20text.%20To%20this%20end%2C%20we%20newly%20introduce%0AMultiDialog%2C%20the%20first%20large-scale%20multimodal%20%28i.e.%2C%20audio%20and%20visual%29%20spoken%0Adialogue%20corpus%20containing%20340%20hours%20of%20approximately%209%2C000%20dialogues%2C%20recorded%0Abased%20on%20the%20open%20domain%20dialogue%20dataset%2C%20TopicalChat.%20The%20MultiDialog%0Acontains%20parallel%20audio-visual%20recordings%20of%20conversation%20partners%20acting%0Aaccording%20to%20the%20given%20script%20with%20emotion%20annotations%2C%20which%20we%20expect%20to%20open%0Aup%20research%20opportunities%20in%20multimodal%20synthesis.%20Our%20Face-to-Face%20spoken%0Adialogue%20model%20incorporates%20a%20textually%20pretrained%20large%20language%20model%20and%0Aadapts%20it%20into%20the%20audio-visual%20spoken%20dialogue%20domain%20by%20incorporating%0Aspeech-text%20joint%20pretraining.%20Through%20extensive%20experiments%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20model%20in%20facilitating%20a%20face-to-face%20conversation.%20Demo%0Aand%20data%20are%20available%20at%20https%3A//multidialog.github.io%20and%0Ahttps%3A//huggingface.co/datasets/IVLLab/MultiDialog%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Go%2520Real%2520Talk%253A%2520Spoken%2520Dialogue%2520Model%2520for%2520Face-to-Face%2520Conversation%26entry.906535625%3DSe%2520Jin%2520Park%2520and%2520Chae%2520Won%2520Kim%2520and%2520Hyeongseop%2520Rha%2520and%2520Minsu%2520Kim%2520and%2520Joanna%2520Hong%2520and%2520Jeong%2520Hun%2520Yeo%2520and%2520Yong%2520Man%2520Ro%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520Face-to-Face%2520spoken%2520dialogue%2520model.%2520It%250Aprocesses%2520audio-visual%2520speech%2520from%2520user%2520input%2520and%2520generates%2520audio-visual%2520speech%250Aas%2520the%2520response%252C%2520marking%2520the%2520initial%2520step%2520towards%2520creating%2520an%2520avatar%2520chatbot%250Asystem%2520without%2520relying%2520on%2520intermediate%2520text.%2520To%2520this%2520end%252C%2520we%2520newly%2520introduce%250AMultiDialog%252C%2520the%2520first%2520large-scale%2520multimodal%2520%2528i.e.%252C%2520audio%2520and%2520visual%2529%2520spoken%250Adialogue%2520corpus%2520containing%2520340%2520hours%2520of%2520approximately%25209%252C000%2520dialogues%252C%2520recorded%250Abased%2520on%2520the%2520open%2520domain%2520dialogue%2520dataset%252C%2520TopicalChat.%2520The%2520MultiDialog%250Acontains%2520parallel%2520audio-visual%2520recordings%2520of%2520conversation%2520partners%2520acting%250Aaccording%2520to%2520the%2520given%2520script%2520with%2520emotion%2520annotations%252C%2520which%2520we%2520expect%2520to%2520open%250Aup%2520research%2520opportunities%2520in%2520multimodal%2520synthesis.%2520Our%2520Face-to-Face%2520spoken%250Adialogue%2520model%2520incorporates%2520a%2520textually%2520pretrained%2520large%2520language%2520model%2520and%250Aadapts%2520it%2520into%2520the%2520audio-visual%2520spoken%2520dialogue%2520domain%2520by%2520incorporating%250Aspeech-text%2520joint%2520pretraining.%2520Through%2520extensive%2520experiments%252C%2520we%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520model%2520in%2520facilitating%2520a%2520face-to-face%2520conversation.%2520Demo%250Aand%2520data%2520are%2520available%2520at%2520https%253A//multidialog.github.io%2520and%250Ahttps%253A//huggingface.co/datasets/IVLLab/MultiDialog%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Go%20Real%20Talk%3A%20Spoken%20Dialogue%20Model%20for%20Face-to-Face%20Conversation&entry.906535625=Se%20Jin%20Park%20and%20Chae%20Won%20Kim%20and%20Hyeongseop%20Rha%20and%20Minsu%20Kim%20and%20Joanna%20Hong%20and%20Jeong%20Hun%20Yeo%20and%20Yong%20Man%20Ro&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20Face-to-Face%20spoken%20dialogue%20model.%20It%0Aprocesses%20audio-visual%20speech%20from%20user%20input%20and%20generates%20audio-visual%20speech%0Aas%20the%20response%2C%20marking%20the%20initial%20step%20towards%20creating%20an%20avatar%20chatbot%0Asystem%20without%20relying%20on%20intermediate%20text.%20To%20this%20end%2C%20we%20newly%20introduce%0AMultiDialog%2C%20the%20first%20large-scale%20multimodal%20%28i.e.%2C%20audio%20and%20visual%29%20spoken%0Adialogue%20corpus%20containing%20340%20hours%20of%20approximately%209%2C000%20dialogues%2C%20recorded%0Abased%20on%20the%20open%20domain%20dialogue%20dataset%2C%20TopicalChat.%20The%20MultiDialog%0Acontains%20parallel%20audio-visual%20recordings%20of%20conversation%20partners%20acting%0Aaccording%20to%20the%20given%20script%20with%20emotion%20annotations%2C%20which%20we%20expect%20to%20open%0Aup%20research%20opportunities%20in%20multimodal%20synthesis.%20Our%20Face-to-Face%20spoken%0Adialogue%20model%20incorporates%20a%20textually%20pretrained%20large%20language%20model%20and%0Aadapts%20it%20into%20the%20audio-visual%20spoken%20dialogue%20domain%20by%20incorporating%0Aspeech-text%20joint%20pretraining.%20Through%20extensive%20experiments%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20model%20in%20facilitating%20a%20face-to-face%20conversation.%20Demo%0Aand%20data%20are%20available%20at%20https%3A//multidialog.github.io%20and%0Ahttps%3A//huggingface.co/datasets/IVLLab/MultiDialog%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07867v2&entry.124074799=Read"},
{"title": "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models", "author": "Simone Caldarella and Massimiliano Mancini and Elisa Ricci and Rahaf Aljundi", "abstract": "  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n", "link": "http://arxiv.org/abs/2408.01228v1", "date": "2024-08-02", "relevancy": 2.5766, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.532}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5118}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models&body=Title%3A%20The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models%0AAuthor%3A%20Simone%20Caldarella%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20combine%20visual%20and%20textual%20understanding%2C%0Arendering%20them%20well-suited%20for%20diverse%20tasks%20like%20generating%20image%20captions%20and%0Aanswering%20visual%20questions%20across%20various%20domains.%20However%2C%20these%20capabilities%0Aare%20built%20upon%20training%20on%20large%20amount%20of%20uncurated%20data%20crawled%20from%20the%20web.%0AThe%20latter%20may%20include%20sensitive%20information%20that%20VLMs%20could%20memorize%20and%20leak%2C%0Araising%20significant%20privacy%20concerns.%20In%20this%20paper%2C%20we%20assess%20whether%20these%0Avulnerabilities%20exist%2C%20focusing%20on%20identity%20leakage.%20Our%20study%20leads%20to%20three%0Akey%20findings%3A%20%28i%29%20VLMs%20leak%20identity%20information%2C%20even%20when%20the%20vision-language%0Aalignment%20and%20the%20fine-tuning%20use%20anonymized%20data%3B%20%28ii%29%20context%20has%20little%0Ainfluence%20on%20identity%20leakage%3B%20%28iii%29%20simple%2C%20widely%20used%20anonymization%0Atechniques%2C%20like%20blurring%2C%20are%20not%20sufficient%20to%20address%20the%20problem.%20These%0Afindings%20underscore%20the%20urgent%20need%20for%20robust%20privacy%20protection%20strategies%0Awhen%20deploying%20VLMs.%20Ethical%20awareness%20and%20responsible%20development%20practices%0Aare%20essential%20to%20mitigate%20these%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Phantom%2520Menace%253A%2520Unmasking%2520Privacy%2520Leakages%2520in%2520Vision-Language%2520Models%26entry.906535625%3DSimone%2520Caldarella%2520and%2520Massimiliano%2520Mancini%2520and%2520Elisa%2520Ricci%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520combine%2520visual%2520and%2520textual%2520understanding%252C%250Arendering%2520them%2520well-suited%2520for%2520diverse%2520tasks%2520like%2520generating%2520image%2520captions%2520and%250Aanswering%2520visual%2520questions%2520across%2520various%2520domains.%2520However%252C%2520these%2520capabilities%250Aare%2520built%2520upon%2520training%2520on%2520large%2520amount%2520of%2520uncurated%2520data%2520crawled%2520from%2520the%2520web.%250AThe%2520latter%2520may%2520include%2520sensitive%2520information%2520that%2520VLMs%2520could%2520memorize%2520and%2520leak%252C%250Araising%2520significant%2520privacy%2520concerns.%2520In%2520this%2520paper%252C%2520we%2520assess%2520whether%2520these%250Avulnerabilities%2520exist%252C%2520focusing%2520on%2520identity%2520leakage.%2520Our%2520study%2520leads%2520to%2520three%250Akey%2520findings%253A%2520%2528i%2529%2520VLMs%2520leak%2520identity%2520information%252C%2520even%2520when%2520the%2520vision-language%250Aalignment%2520and%2520the%2520fine-tuning%2520use%2520anonymized%2520data%253B%2520%2528ii%2529%2520context%2520has%2520little%250Ainfluence%2520on%2520identity%2520leakage%253B%2520%2528iii%2529%2520simple%252C%2520widely%2520used%2520anonymization%250Atechniques%252C%2520like%2520blurring%252C%2520are%2520not%2520sufficient%2520to%2520address%2520the%2520problem.%2520These%250Afindings%2520underscore%2520the%2520urgent%2520need%2520for%2520robust%2520privacy%2520protection%2520strategies%250Awhen%2520deploying%2520VLMs.%2520Ethical%2520awareness%2520and%2520responsible%2520development%2520practices%250Aare%2520essential%2520to%2520mitigate%2520these%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Phantom%20Menace%3A%20Unmasking%20Privacy%20Leakages%20in%20Vision-Language%20Models&entry.906535625=Simone%20Caldarella%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20combine%20visual%20and%20textual%20understanding%2C%0Arendering%20them%20well-suited%20for%20diverse%20tasks%20like%20generating%20image%20captions%20and%0Aanswering%20visual%20questions%20across%20various%20domains.%20However%2C%20these%20capabilities%0Aare%20built%20upon%20training%20on%20large%20amount%20of%20uncurated%20data%20crawled%20from%20the%20web.%0AThe%20latter%20may%20include%20sensitive%20information%20that%20VLMs%20could%20memorize%20and%20leak%2C%0Araising%20significant%20privacy%20concerns.%20In%20this%20paper%2C%20we%20assess%20whether%20these%0Avulnerabilities%20exist%2C%20focusing%20on%20identity%20leakage.%20Our%20study%20leads%20to%20three%0Akey%20findings%3A%20%28i%29%20VLMs%20leak%20identity%20information%2C%20even%20when%20the%20vision-language%0Aalignment%20and%20the%20fine-tuning%20use%20anonymized%20data%3B%20%28ii%29%20context%20has%20little%0Ainfluence%20on%20identity%20leakage%3B%20%28iii%29%20simple%2C%20widely%20used%20anonymization%0Atechniques%2C%20like%20blurring%2C%20are%20not%20sufficient%20to%20address%20the%20problem.%20These%0Afindings%20underscore%20the%20urgent%20need%20for%20robust%20privacy%20protection%20strategies%0Awhen%20deploying%20VLMs.%20Ethical%20awareness%20and%20responsible%20development%20practices%0Aare%20essential%20to%20mitigate%20these%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01228v1&entry.124074799=Read"},
{"title": "Incremental Object-Based Novelty Detection with Feedback Loop", "author": "Simone Caldarella and Elisa Ricci and Rahaf Aljundi", "abstract": "  Object-based Novelty Detection (ND) aims to identify unknown objects that do\nnot belong to classes seen during training by an object detection model. The\ntask is particularly crucial in real-world applications, as it allows to avoid\npotentially harmful behaviours, e.g. as in the case of object detection models\nadopted in a self-driving car or in an autonomous robot. Traditional approaches\nto ND focus on one time offline post processing of the pretrained object\ndetection output, leaving no possibility to improve the model robustness after\ntraining and discarding the abundant amount of out-of-distribution data\nencountered during deployment. In this work, we propose a novel framework for\nobject-based ND, assuming that human feedback can be requested on the predicted\noutput and later incorporated to refine the ND model without negatively\naffecting the main object detection performance. This refinement operation is\nrepeated whenever new feedback is available. To tackle this new formulation of\nthe problem for object detection, we propose a lightweight ND module attached\non top of a pre-trained object detection model, which is incrementally updated\nthrough a feedback loop. We also propose a new benchmark to evaluate methods on\nthis new setting and test extensively our ND approach against baselines,\nshowing increased robustness and a successful incorporation of the received\nfeedback.\n", "link": "http://arxiv.org/abs/2311.09004v2", "date": "2024-08-02", "relevancy": 2.5619, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5508}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.496}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Object-Based%20Novelty%20Detection%20with%20Feedback%20Loop&body=Title%3A%20Incremental%20Object-Based%20Novelty%20Detection%20with%20Feedback%20Loop%0AAuthor%3A%20Simone%20Caldarella%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi%0AAbstract%3A%20%20%20Object-based%20Novelty%20Detection%20%28ND%29%20aims%20to%20identify%20unknown%20objects%20that%20do%0Anot%20belong%20to%20classes%20seen%20during%20training%20by%20an%20object%20detection%20model.%20The%0Atask%20is%20particularly%20crucial%20in%20real-world%20applications%2C%20as%20it%20allows%20to%20avoid%0Apotentially%20harmful%20behaviours%2C%20e.g.%20as%20in%20the%20case%20of%20object%20detection%20models%0Aadopted%20in%20a%20self-driving%20car%20or%20in%20an%20autonomous%20robot.%20Traditional%20approaches%0Ato%20ND%20focus%20on%20one%20time%20offline%20post%20processing%20of%20the%20pretrained%20object%0Adetection%20output%2C%20leaving%20no%20possibility%20to%20improve%20the%20model%20robustness%20after%0Atraining%20and%20discarding%20the%20abundant%20amount%20of%20out-of-distribution%20data%0Aencountered%20during%20deployment.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20for%0Aobject-based%20ND%2C%20assuming%20that%20human%20feedback%20can%20be%20requested%20on%20the%20predicted%0Aoutput%20and%20later%20incorporated%20to%20refine%20the%20ND%20model%20without%20negatively%0Aaffecting%20the%20main%20object%20detection%20performance.%20This%20refinement%20operation%20is%0Arepeated%20whenever%20new%20feedback%20is%20available.%20To%20tackle%20this%20new%20formulation%20of%0Athe%20problem%20for%20object%20detection%2C%20we%20propose%20a%20lightweight%20ND%20module%20attached%0Aon%20top%20of%20a%20pre-trained%20object%20detection%20model%2C%20which%20is%20incrementally%20updated%0Athrough%20a%20feedback%20loop.%20We%20also%20propose%20a%20new%20benchmark%20to%20evaluate%20methods%20on%0Athis%20new%20setting%20and%20test%20extensively%20our%20ND%20approach%20against%20baselines%2C%0Ashowing%20increased%20robustness%20and%20a%20successful%20incorporation%20of%20the%20received%0Afeedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Object-Based%2520Novelty%2520Detection%2520with%2520Feedback%2520Loop%26entry.906535625%3DSimone%2520Caldarella%2520and%2520Elisa%2520Ricci%2520and%2520Rahaf%2520Aljundi%26entry.1292438233%3D%2520%2520Object-based%2520Novelty%2520Detection%2520%2528ND%2529%2520aims%2520to%2520identify%2520unknown%2520objects%2520that%2520do%250Anot%2520belong%2520to%2520classes%2520seen%2520during%2520training%2520by%2520an%2520object%2520detection%2520model.%2520The%250Atask%2520is%2520particularly%2520crucial%2520in%2520real-world%2520applications%252C%2520as%2520it%2520allows%2520to%2520avoid%250Apotentially%2520harmful%2520behaviours%252C%2520e.g.%2520as%2520in%2520the%2520case%2520of%2520object%2520detection%2520models%250Aadopted%2520in%2520a%2520self-driving%2520car%2520or%2520in%2520an%2520autonomous%2520robot.%2520Traditional%2520approaches%250Ato%2520ND%2520focus%2520on%2520one%2520time%2520offline%2520post%2520processing%2520of%2520the%2520pretrained%2520object%250Adetection%2520output%252C%2520leaving%2520no%2520possibility%2520to%2520improve%2520the%2520model%2520robustness%2520after%250Atraining%2520and%2520discarding%2520the%2520abundant%2520amount%2520of%2520out-of-distribution%2520data%250Aencountered%2520during%2520deployment.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%250Aobject-based%2520ND%252C%2520assuming%2520that%2520human%2520feedback%2520can%2520be%2520requested%2520on%2520the%2520predicted%250Aoutput%2520and%2520later%2520incorporated%2520to%2520refine%2520the%2520ND%2520model%2520without%2520negatively%250Aaffecting%2520the%2520main%2520object%2520detection%2520performance.%2520This%2520refinement%2520operation%2520is%250Arepeated%2520whenever%2520new%2520feedback%2520is%2520available.%2520To%2520tackle%2520this%2520new%2520formulation%2520of%250Athe%2520problem%2520for%2520object%2520detection%252C%2520we%2520propose%2520a%2520lightweight%2520ND%2520module%2520attached%250Aon%2520top%2520of%2520a%2520pre-trained%2520object%2520detection%2520model%252C%2520which%2520is%2520incrementally%2520updated%250Athrough%2520a%2520feedback%2520loop.%2520We%2520also%2520propose%2520a%2520new%2520benchmark%2520to%2520evaluate%2520methods%2520on%250Athis%2520new%2520setting%2520and%2520test%2520extensively%2520our%2520ND%2520approach%2520against%2520baselines%252C%250Ashowing%2520increased%2520robustness%2520and%2520a%2520successful%2520incorporation%2520of%2520the%2520received%250Afeedback.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Object-Based%20Novelty%20Detection%20with%20Feedback%20Loop&entry.906535625=Simone%20Caldarella%20and%20Elisa%20Ricci%20and%20Rahaf%20Aljundi&entry.1292438233=%20%20Object-based%20Novelty%20Detection%20%28ND%29%20aims%20to%20identify%20unknown%20objects%20that%20do%0Anot%20belong%20to%20classes%20seen%20during%20training%20by%20an%20object%20detection%20model.%20The%0Atask%20is%20particularly%20crucial%20in%20real-world%20applications%2C%20as%20it%20allows%20to%20avoid%0Apotentially%20harmful%20behaviours%2C%20e.g.%20as%20in%20the%20case%20of%20object%20detection%20models%0Aadopted%20in%20a%20self-driving%20car%20or%20in%20an%20autonomous%20robot.%20Traditional%20approaches%0Ato%20ND%20focus%20on%20one%20time%20offline%20post%20processing%20of%20the%20pretrained%20object%0Adetection%20output%2C%20leaving%20no%20possibility%20to%20improve%20the%20model%20robustness%20after%0Atraining%20and%20discarding%20the%20abundant%20amount%20of%20out-of-distribution%20data%0Aencountered%20during%20deployment.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20for%0Aobject-based%20ND%2C%20assuming%20that%20human%20feedback%20can%20be%20requested%20on%20the%20predicted%0Aoutput%20and%20later%20incorporated%20to%20refine%20the%20ND%20model%20without%20negatively%0Aaffecting%20the%20main%20object%20detection%20performance.%20This%20refinement%20operation%20is%0Arepeated%20whenever%20new%20feedback%20is%20available.%20To%20tackle%20this%20new%20formulation%20of%0Athe%20problem%20for%20object%20detection%2C%20we%20propose%20a%20lightweight%20ND%20module%20attached%0Aon%20top%20of%20a%20pre-trained%20object%20detection%20model%2C%20which%20is%20incrementally%20updated%0Athrough%20a%20feedback%20loop.%20We%20also%20propose%20a%20new%20benchmark%20to%20evaluate%20methods%20on%0Athis%20new%20setting%20and%20test%20extensively%20our%20ND%20approach%20against%20baselines%2C%0Ashowing%20increased%20robustness%20and%20a%20successful%20incorporation%20of%20the%20received%0Afeedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09004v2&entry.124074799=Read"},
{"title": "Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven\n  Digital Twins in Industrial Cyber-Physical Systems", "author": "Jinbo Wen and Jiawen Kang and Dusit Niyato and Yang Zhang and Shiwen Mao", "abstract": "  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern\nmanufacturing and industries. By digitizing data throughout the product life\ncycle, Digital Twins (DTs) in ICPSs enable a shift from current industrial\ninfrastructures to intelligent and adaptive infrastructures. Thanks to data\nprocess capability, Generative Artificial Intelligence (GAI) can drive the\nconstruction and update of DTs to improve predictive accuracy and prepare for\ndiverse smart manufacturing. However, mechanisms that leverage sensing\nIndustrial Internet of Things (IIoT) devices to share data for the construction\nof DTs are susceptible to adverse selection problems. In this paper, we first\ndevelop a GAI-driven DT architecture for ICPSs. To address the adverse\nselection problem caused by information asymmetry, we propose a contract theory\nmodel and develop the sustainable diffusion-based soft actor-critic algorithm\nto identify the optimal feasible contract. Specifically, we leverage the\ndynamic structured pruning technique to reduce parameter numbers of actor\nnetworks, allowing sustainability and efficient implementation of the proposed\nalgorithm. Finally, numerical results demonstrate the effectiveness of the\nproposed scheme.\n", "link": "http://arxiv.org/abs/2408.01173v1", "date": "2024-08-02", "relevancy": 2.5316, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.508}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.506}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems&body=Title%3A%20Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems%0AAuthor%3A%20Jinbo%20Wen%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato%20and%20Yang%20Zhang%20and%20Shiwen%20Mao%0AAbstract%3A%20%20%20Industrial%20Cyber-Physical%20Systems%20%28ICPSs%29%20are%20an%20integral%20component%20of%20modern%0Amanufacturing%20and%20industries.%20By%20digitizing%20data%20throughout%20the%20product%20life%0Acycle%2C%20Digital%20Twins%20%28DTs%29%20in%20ICPSs%20enable%20a%20shift%20from%20current%20industrial%0Ainfrastructures%20to%20intelligent%20and%20adaptive%20infrastructures.%20Thanks%20to%20data%0Aprocess%20capability%2C%20Generative%20Artificial%20Intelligence%20%28GAI%29%20can%20drive%20the%0Aconstruction%20and%20update%20of%20DTs%20to%20improve%20predictive%20accuracy%20and%20prepare%20for%0Adiverse%20smart%20manufacturing.%20However%2C%20mechanisms%20that%20leverage%20sensing%0AIndustrial%20Internet%20of%20Things%20%28IIoT%29%20devices%20to%20share%20data%20for%20the%20construction%0Aof%20DTs%20are%20susceptible%20to%20adverse%20selection%20problems.%20In%20this%20paper%2C%20we%20first%0Adevelop%20a%20GAI-driven%20DT%20architecture%20for%20ICPSs.%20To%20address%20the%20adverse%0Aselection%20problem%20caused%20by%20information%20asymmetry%2C%20we%20propose%20a%20contract%20theory%0Amodel%20and%20develop%20the%20sustainable%20diffusion-based%20soft%20actor-critic%20algorithm%0Ato%20identify%20the%20optimal%20feasible%20contract.%20Specifically%2C%20we%20leverage%20the%0Adynamic%20structured%20pruning%20technique%20to%20reduce%20parameter%20numbers%20of%20actor%0Anetworks%2C%20allowing%20sustainability%20and%20efficient%20implementation%20of%20the%20proposed%0Aalgorithm.%20Finally%2C%20numerical%20results%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSustainable%2520Diffusion-based%2520Incentive%2520Mechanism%2520for%2520Generative%2520AI-driven%250A%2520%2520Digital%2520Twins%2520in%2520Industrial%2520Cyber-Physical%2520Systems%26entry.906535625%3DJinbo%2520Wen%2520and%2520Jiawen%2520Kang%2520and%2520Dusit%2520Niyato%2520and%2520Yang%2520Zhang%2520and%2520Shiwen%2520Mao%26entry.1292438233%3D%2520%2520Industrial%2520Cyber-Physical%2520Systems%2520%2528ICPSs%2529%2520are%2520an%2520integral%2520component%2520of%2520modern%250Amanufacturing%2520and%2520industries.%2520By%2520digitizing%2520data%2520throughout%2520the%2520product%2520life%250Acycle%252C%2520Digital%2520Twins%2520%2528DTs%2529%2520in%2520ICPSs%2520enable%2520a%2520shift%2520from%2520current%2520industrial%250Ainfrastructures%2520to%2520intelligent%2520and%2520adaptive%2520infrastructures.%2520Thanks%2520to%2520data%250Aprocess%2520capability%252C%2520Generative%2520Artificial%2520Intelligence%2520%2528GAI%2529%2520can%2520drive%2520the%250Aconstruction%2520and%2520update%2520of%2520DTs%2520to%2520improve%2520predictive%2520accuracy%2520and%2520prepare%2520for%250Adiverse%2520smart%2520manufacturing.%2520However%252C%2520mechanisms%2520that%2520leverage%2520sensing%250AIndustrial%2520Internet%2520of%2520Things%2520%2528IIoT%2529%2520devices%2520to%2520share%2520data%2520for%2520the%2520construction%250Aof%2520DTs%2520are%2520susceptible%2520to%2520adverse%2520selection%2520problems.%2520In%2520this%2520paper%252C%2520we%2520first%250Adevelop%2520a%2520GAI-driven%2520DT%2520architecture%2520for%2520ICPSs.%2520To%2520address%2520the%2520adverse%250Aselection%2520problem%2520caused%2520by%2520information%2520asymmetry%252C%2520we%2520propose%2520a%2520contract%2520theory%250Amodel%2520and%2520develop%2520the%2520sustainable%2520diffusion-based%2520soft%2520actor-critic%2520algorithm%250Ato%2520identify%2520the%2520optimal%2520feasible%2520contract.%2520Specifically%252C%2520we%2520leverage%2520the%250Adynamic%2520structured%2520pruning%2520technique%2520to%2520reduce%2520parameter%2520numbers%2520of%2520actor%250Anetworks%252C%2520allowing%2520sustainability%2520and%2520efficient%2520implementation%2520of%2520the%2520proposed%250Aalgorithm.%2520Finally%252C%2520numerical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems&entry.906535625=Jinbo%20Wen%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato%20and%20Yang%20Zhang%20and%20Shiwen%20Mao&entry.1292438233=%20%20Industrial%20Cyber-Physical%20Systems%20%28ICPSs%29%20are%20an%20integral%20component%20of%20modern%0Amanufacturing%20and%20industries.%20By%20digitizing%20data%20throughout%20the%20product%20life%0Acycle%2C%20Digital%20Twins%20%28DTs%29%20in%20ICPSs%20enable%20a%20shift%20from%20current%20industrial%0Ainfrastructures%20to%20intelligent%20and%20adaptive%20infrastructures.%20Thanks%20to%20data%0Aprocess%20capability%2C%20Generative%20Artificial%20Intelligence%20%28GAI%29%20can%20drive%20the%0Aconstruction%20and%20update%20of%20DTs%20to%20improve%20predictive%20accuracy%20and%20prepare%20for%0Adiverse%20smart%20manufacturing.%20However%2C%20mechanisms%20that%20leverage%20sensing%0AIndustrial%20Internet%20of%20Things%20%28IIoT%29%20devices%20to%20share%20data%20for%20the%20construction%0Aof%20DTs%20are%20susceptible%20to%20adverse%20selection%20problems.%20In%20this%20paper%2C%20we%20first%0Adevelop%20a%20GAI-driven%20DT%20architecture%20for%20ICPSs.%20To%20address%20the%20adverse%0Aselection%20problem%20caused%20by%20information%20asymmetry%2C%20we%20propose%20a%20contract%20theory%0Amodel%20and%20develop%20the%20sustainable%20diffusion-based%20soft%20actor-critic%20algorithm%0Ato%20identify%20the%20optimal%20feasible%20contract.%20Specifically%2C%20we%20leverage%20the%0Adynamic%20structured%20pruning%20technique%20to%20reduce%20parameter%20numbers%20of%20actor%0Anetworks%2C%20allowing%20sustainability%20and%20efficient%20implementation%20of%20the%20proposed%0Aalgorithm.%20Finally%2C%20numerical%20results%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01173v1&entry.124074799=Read"},
{"title": "Hybrid Coordinate Descent for Efficient Neural Network Learning Using\n  Line Search and Gradient Descent", "author": "Yen-Che Hsiao and Abhishek Dutta", "abstract": "  This paper presents a novel coordinate descent algorithm leveraging a\ncombination of one-directional line search and gradient information for\nparameter updates for a squared error loss function. Each parameter undergoes\nupdates determined by either the line search or gradient method, contingent\nupon whether the modulus of the gradient of the loss with respect to that\nparameter surpasses a predefined threshold. Notably, a larger threshold value\nenhances algorithmic efficiency. Despite the potentially slower nature of the\nline search method relative to gradient descent, its parallelizability\nfacilitates computational time reduction. Experimental validation conducted on\na 2-layer Rectified Linear Unit network with synthetic data elucidates the\nimpact of hyperparameters on convergence rates and computational efficiency.\n", "link": "http://arxiv.org/abs/2408.01374v1", "date": "2024-08-02", "relevancy": 2.4815, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5159}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Coordinate%20Descent%20for%20Efficient%20Neural%20Network%20Learning%20Using%0A%20%20Line%20Search%20and%20Gradient%20Descent&body=Title%3A%20Hybrid%20Coordinate%20Descent%20for%20Efficient%20Neural%20Network%20Learning%20Using%0A%20%20Line%20Search%20and%20Gradient%20Descent%0AAuthor%3A%20Yen-Che%20Hsiao%20and%20Abhishek%20Dutta%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20coordinate%20descent%20algorithm%20leveraging%20a%0Acombination%20of%20one-directional%20line%20search%20and%20gradient%20information%20for%0Aparameter%20updates%20for%20a%20squared%20error%20loss%20function.%20Each%20parameter%20undergoes%0Aupdates%20determined%20by%20either%20the%20line%20search%20or%20gradient%20method%2C%20contingent%0Aupon%20whether%20the%20modulus%20of%20the%20gradient%20of%20the%20loss%20with%20respect%20to%20that%0Aparameter%20surpasses%20a%20predefined%20threshold.%20Notably%2C%20a%20larger%20threshold%20value%0Aenhances%20algorithmic%20efficiency.%20Despite%20the%20potentially%20slower%20nature%20of%20the%0Aline%20search%20method%20relative%20to%20gradient%20descent%2C%20its%20parallelizability%0Afacilitates%20computational%20time%20reduction.%20Experimental%20validation%20conducted%20on%0Aa%202-layer%20Rectified%20Linear%20Unit%20network%20with%20synthetic%20data%20elucidates%20the%0Aimpact%20of%20hyperparameters%20on%20convergence%20rates%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Coordinate%2520Descent%2520for%2520Efficient%2520Neural%2520Network%2520Learning%2520Using%250A%2520%2520Line%2520Search%2520and%2520Gradient%2520Descent%26entry.906535625%3DYen-Che%2520Hsiao%2520and%2520Abhishek%2520Dutta%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520coordinate%2520descent%2520algorithm%2520leveraging%2520a%250Acombination%2520of%2520one-directional%2520line%2520search%2520and%2520gradient%2520information%2520for%250Aparameter%2520updates%2520for%2520a%2520squared%2520error%2520loss%2520function.%2520Each%2520parameter%2520undergoes%250Aupdates%2520determined%2520by%2520either%2520the%2520line%2520search%2520or%2520gradient%2520method%252C%2520contingent%250Aupon%2520whether%2520the%2520modulus%2520of%2520the%2520gradient%2520of%2520the%2520loss%2520with%2520respect%2520to%2520that%250Aparameter%2520surpasses%2520a%2520predefined%2520threshold.%2520Notably%252C%2520a%2520larger%2520threshold%2520value%250Aenhances%2520algorithmic%2520efficiency.%2520Despite%2520the%2520potentially%2520slower%2520nature%2520of%2520the%250Aline%2520search%2520method%2520relative%2520to%2520gradient%2520descent%252C%2520its%2520parallelizability%250Afacilitates%2520computational%2520time%2520reduction.%2520Experimental%2520validation%2520conducted%2520on%250Aa%25202-layer%2520Rectified%2520Linear%2520Unit%2520network%2520with%2520synthetic%2520data%2520elucidates%2520the%250Aimpact%2520of%2520hyperparameters%2520on%2520convergence%2520rates%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Coordinate%20Descent%20for%20Efficient%20Neural%20Network%20Learning%20Using%0A%20%20Line%20Search%20and%20Gradient%20Descent&entry.906535625=Yen-Che%20Hsiao%20and%20Abhishek%20Dutta&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20coordinate%20descent%20algorithm%20leveraging%20a%0Acombination%20of%20one-directional%20line%20search%20and%20gradient%20information%20for%0Aparameter%20updates%20for%20a%20squared%20error%20loss%20function.%20Each%20parameter%20undergoes%0Aupdates%20determined%20by%20either%20the%20line%20search%20or%20gradient%20method%2C%20contingent%0Aupon%20whether%20the%20modulus%20of%20the%20gradient%20of%20the%20loss%20with%20respect%20to%20that%0Aparameter%20surpasses%20a%20predefined%20threshold.%20Notably%2C%20a%20larger%20threshold%20value%0Aenhances%20algorithmic%20efficiency.%20Despite%20the%20potentially%20slower%20nature%20of%20the%0Aline%20search%20method%20relative%20to%20gradient%20descent%2C%20its%20parallelizability%0Afacilitates%20computational%20time%20reduction.%20Experimental%20validation%20conducted%20on%0Aa%202-layer%20Rectified%20Linear%20Unit%20network%20with%20synthetic%20data%20elucidates%20the%0Aimpact%20of%20hyperparameters%20on%20convergence%20rates%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01374v1&entry.124074799=Read"},
{"title": "Autoencoders in Function Space", "author": "Justin Bunker and Mark Girolami and Hefin Lambley and Andrew M. Stuart and T. J. Sullivan", "abstract": "  Autoencoders have found widespread application, in both their original\ndeterministic form and in their variational formulation (VAEs). In scientific\napplications it is often of interest to consider data that are comprised of\nfunctions; the same perspective is useful in image processing. In practice,\ndiscretisation (of differential equations arising in the sciences) or\npixellation (of images) renders problems finite dimensional, but conceiving\nfirst of algorithms that operate on functions, and only then discretising or\npixellating, leads to better algorithms that smoothly operate between different\nlevels of discretisation or pixellation. In this paper function-space versions\nof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,\nanalysed, and deployed. Well-definedness of the objective function governing\nVAEs is a subtle issue, even in finite dimension, and more so on function\nspace. The FVAE objective is well defined whenever the data distribution is\ncompatible with the chosen generative model; this happens, for example, when\nthe data arise from a stochastic differential equation. The FAE objective is\nvalid much more broadly, and can be straightforwardly applied to data governed\nby differential equations. Pairing these objectives with neural operator\narchitectures, which can thus be evaluated on any mesh, enables new\napplications of autoencoders to inpainting, superresolution, and generative\nmodelling of scientific data.\n", "link": "http://arxiv.org/abs/2408.01362v1", "date": "2024-08-02", "relevancy": 2.4105, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5455}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoencoders%20in%20Function%20Space&body=Title%3A%20Autoencoders%20in%20Function%20Space%0AAuthor%3A%20Justin%20Bunker%20and%20Mark%20Girolami%20and%20Hefin%20Lambley%20and%20Andrew%20M.%20Stuart%20and%20T.%20J.%20Sullivan%0AAbstract%3A%20%20%20Autoencoders%20have%20found%20widespread%20application%2C%20in%20both%20their%20original%0Adeterministic%20form%20and%20in%20their%20variational%20formulation%20%28VAEs%29.%20In%20scientific%0Aapplications%20it%20is%20often%20of%20interest%20to%20consider%20data%20that%20are%20comprised%20of%0Afunctions%3B%20the%20same%20perspective%20is%20useful%20in%20image%20processing.%20In%20practice%2C%0Adiscretisation%20%28of%20differential%20equations%20arising%20in%20the%20sciences%29%20or%0Apixellation%20%28of%20images%29%20renders%20problems%20finite%20dimensional%2C%20but%20conceiving%0Afirst%20of%20algorithms%20that%20operate%20on%20functions%2C%20and%20only%20then%20discretising%20or%0Apixellating%2C%20leads%20to%20better%20algorithms%20that%20smoothly%20operate%20between%20different%0Alevels%20of%20discretisation%20or%20pixellation.%20In%20this%20paper%20function-space%20versions%0Aof%20the%20autoencoder%20%28FAE%29%20and%20variational%20autoencoder%20%28FVAE%29%20are%20introduced%2C%0Aanalysed%2C%20and%20deployed.%20Well-definedness%20of%20the%20objective%20function%20governing%0AVAEs%20is%20a%20subtle%20issue%2C%20even%20in%20finite%20dimension%2C%20and%20more%20so%20on%20function%0Aspace.%20The%20FVAE%20objective%20is%20well%20defined%20whenever%20the%20data%20distribution%20is%0Acompatible%20with%20the%20chosen%20generative%20model%3B%20this%20happens%2C%20for%20example%2C%20when%0Athe%20data%20arise%20from%20a%20stochastic%20differential%20equation.%20The%20FAE%20objective%20is%0Avalid%20much%20more%20broadly%2C%20and%20can%20be%20straightforwardly%20applied%20to%20data%20governed%0Aby%20differential%20equations.%20Pairing%20these%20objectives%20with%20neural%20operator%0Aarchitectures%2C%20which%20can%20thus%20be%20evaluated%20on%20any%20mesh%2C%20enables%20new%0Aapplications%20of%20autoencoders%20to%20inpainting%2C%20superresolution%2C%20and%20generative%0Amodelling%20of%20scientific%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoencoders%2520in%2520Function%2520Space%26entry.906535625%3DJustin%2520Bunker%2520and%2520Mark%2520Girolami%2520and%2520Hefin%2520Lambley%2520and%2520Andrew%2520M.%2520Stuart%2520and%2520T.%2520J.%2520Sullivan%26entry.1292438233%3D%2520%2520Autoencoders%2520have%2520found%2520widespread%2520application%252C%2520in%2520both%2520their%2520original%250Adeterministic%2520form%2520and%2520in%2520their%2520variational%2520formulation%2520%2528VAEs%2529.%2520In%2520scientific%250Aapplications%2520it%2520is%2520often%2520of%2520interest%2520to%2520consider%2520data%2520that%2520are%2520comprised%2520of%250Afunctions%253B%2520the%2520same%2520perspective%2520is%2520useful%2520in%2520image%2520processing.%2520In%2520practice%252C%250Adiscretisation%2520%2528of%2520differential%2520equations%2520arising%2520in%2520the%2520sciences%2529%2520or%250Apixellation%2520%2528of%2520images%2529%2520renders%2520problems%2520finite%2520dimensional%252C%2520but%2520conceiving%250Afirst%2520of%2520algorithms%2520that%2520operate%2520on%2520functions%252C%2520and%2520only%2520then%2520discretising%2520or%250Apixellating%252C%2520leads%2520to%2520better%2520algorithms%2520that%2520smoothly%2520operate%2520between%2520different%250Alevels%2520of%2520discretisation%2520or%2520pixellation.%2520In%2520this%2520paper%2520function-space%2520versions%250Aof%2520the%2520autoencoder%2520%2528FAE%2529%2520and%2520variational%2520autoencoder%2520%2528FVAE%2529%2520are%2520introduced%252C%250Aanalysed%252C%2520and%2520deployed.%2520Well-definedness%2520of%2520the%2520objective%2520function%2520governing%250AVAEs%2520is%2520a%2520subtle%2520issue%252C%2520even%2520in%2520finite%2520dimension%252C%2520and%2520more%2520so%2520on%2520function%250Aspace.%2520The%2520FVAE%2520objective%2520is%2520well%2520defined%2520whenever%2520the%2520data%2520distribution%2520is%250Acompatible%2520with%2520the%2520chosen%2520generative%2520model%253B%2520this%2520happens%252C%2520for%2520example%252C%2520when%250Athe%2520data%2520arise%2520from%2520a%2520stochastic%2520differential%2520equation.%2520The%2520FAE%2520objective%2520is%250Avalid%2520much%2520more%2520broadly%252C%2520and%2520can%2520be%2520straightforwardly%2520applied%2520to%2520data%2520governed%250Aby%2520differential%2520equations.%2520Pairing%2520these%2520objectives%2520with%2520neural%2520operator%250Aarchitectures%252C%2520which%2520can%2520thus%2520be%2520evaluated%2520on%2520any%2520mesh%252C%2520enables%2520new%250Aapplications%2520of%2520autoencoders%2520to%2520inpainting%252C%2520superresolution%252C%2520and%2520generative%250Amodelling%2520of%2520scientific%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoencoders%20in%20Function%20Space&entry.906535625=Justin%20Bunker%20and%20Mark%20Girolami%20and%20Hefin%20Lambley%20and%20Andrew%20M.%20Stuart%20and%20T.%20J.%20Sullivan&entry.1292438233=%20%20Autoencoders%20have%20found%20widespread%20application%2C%20in%20both%20their%20original%0Adeterministic%20form%20and%20in%20their%20variational%20formulation%20%28VAEs%29.%20In%20scientific%0Aapplications%20it%20is%20often%20of%20interest%20to%20consider%20data%20that%20are%20comprised%20of%0Afunctions%3B%20the%20same%20perspective%20is%20useful%20in%20image%20processing.%20In%20practice%2C%0Adiscretisation%20%28of%20differential%20equations%20arising%20in%20the%20sciences%29%20or%0Apixellation%20%28of%20images%29%20renders%20problems%20finite%20dimensional%2C%20but%20conceiving%0Afirst%20of%20algorithms%20that%20operate%20on%20functions%2C%20and%20only%20then%20discretising%20or%0Apixellating%2C%20leads%20to%20better%20algorithms%20that%20smoothly%20operate%20between%20different%0Alevels%20of%20discretisation%20or%20pixellation.%20In%20this%20paper%20function-space%20versions%0Aof%20the%20autoencoder%20%28FAE%29%20and%20variational%20autoencoder%20%28FVAE%29%20are%20introduced%2C%0Aanalysed%2C%20and%20deployed.%20Well-definedness%20of%20the%20objective%20function%20governing%0AVAEs%20is%20a%20subtle%20issue%2C%20even%20in%20finite%20dimension%2C%20and%20more%20so%20on%20function%0Aspace.%20The%20FVAE%20objective%20is%20well%20defined%20whenever%20the%20data%20distribution%20is%0Acompatible%20with%20the%20chosen%20generative%20model%3B%20this%20happens%2C%20for%20example%2C%20when%0Athe%20data%20arise%20from%20a%20stochastic%20differential%20equation.%20The%20FAE%20objective%20is%0Avalid%20much%20more%20broadly%2C%20and%20can%20be%20straightforwardly%20applied%20to%20data%20governed%0Aby%20differential%20equations.%20Pairing%20these%20objectives%20with%20neural%20operator%0Aarchitectures%2C%20which%20can%20thus%20be%20evaluated%20on%20any%20mesh%2C%20enables%20new%0Aapplications%20of%20autoencoders%20to%20inpainting%2C%20superresolution%2C%20and%20generative%0Amodelling%20of%20scientific%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01362v1&entry.124074799=Read"},
{"title": "Weakly Supervised Text-to-SQL Parsing through Question Decomposition", "author": "Tomer Wolfson and Daniel Deutch and Jonathan Berant", "abstract": "  Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query\nrelational data. Training such parsers, by contrast, generally requires\nexpertise in annotating natural language (NL) utterances with corresponding SQL\nqueries. In this work, we propose a weak supervision approach for training\ntext-to-SQL parsers. We take advantage of the recently proposed question\nmeaning representation called QDMR, an intermediate between NL and formal query\nlanguages. Given questions, their QDMR structures (annotated by non-experts or\nautomatically predicted), and the answers, we are able to automatically\nsynthesize SQL queries that are used to train text-to-SQL models. We test our\napproach by experimenting on five benchmark datasets. Our results show that the\nweakly supervised models perform competitively with those trained on annotated\nNL-SQL data. Overall, we effectively train text-to-SQL parsers, while using\nzero SQL annotations.\n", "link": "http://arxiv.org/abs/2112.06311v4", "date": "2024-08-02", "relevancy": 2.3341, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4933}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Text-to-SQL%20Parsing%20through%20Question%20Decomposition&body=Title%3A%20Weakly%20Supervised%20Text-to-SQL%20Parsing%20through%20Question%20Decomposition%0AAuthor%3A%20Tomer%20Wolfson%20and%20Daniel%20Deutch%20and%20Jonathan%20Berant%0AAbstract%3A%20%20%20Text-to-SQL%20parsers%20are%20crucial%20in%20enabling%20non-experts%20to%20effortlessly%20query%0Arelational%20data.%20Training%20such%20parsers%2C%20by%20contrast%2C%20generally%20requires%0Aexpertise%20in%20annotating%20natural%20language%20%28NL%29%20utterances%20with%20corresponding%20SQL%0Aqueries.%20In%20this%20work%2C%20we%20propose%20a%20weak%20supervision%20approach%20for%20training%0Atext-to-SQL%20parsers.%20We%20take%20advantage%20of%20the%20recently%20proposed%20question%0Ameaning%20representation%20called%20QDMR%2C%20an%20intermediate%20between%20NL%20and%20formal%20query%0Alanguages.%20Given%20questions%2C%20their%20QDMR%20structures%20%28annotated%20by%20non-experts%20or%0Aautomatically%20predicted%29%2C%20and%20the%20answers%2C%20we%20are%20able%20to%20automatically%0Asynthesize%20SQL%20queries%20that%20are%20used%20to%20train%20text-to-SQL%20models.%20We%20test%20our%0Aapproach%20by%20experimenting%20on%20five%20benchmark%20datasets.%20Our%20results%20show%20that%20the%0Aweakly%20supervised%20models%20perform%20competitively%20with%20those%20trained%20on%20annotated%0ANL-SQL%20data.%20Overall%2C%20we%20effectively%20train%20text-to-SQL%20parsers%2C%20while%20using%0Azero%20SQL%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.06311v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Text-to-SQL%2520Parsing%2520through%2520Question%2520Decomposition%26entry.906535625%3DTomer%2520Wolfson%2520and%2520Daniel%2520Deutch%2520and%2520Jonathan%2520Berant%26entry.1292438233%3D%2520%2520Text-to-SQL%2520parsers%2520are%2520crucial%2520in%2520enabling%2520non-experts%2520to%2520effortlessly%2520query%250Arelational%2520data.%2520Training%2520such%2520parsers%252C%2520by%2520contrast%252C%2520generally%2520requires%250Aexpertise%2520in%2520annotating%2520natural%2520language%2520%2528NL%2529%2520utterances%2520with%2520corresponding%2520SQL%250Aqueries.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520weak%2520supervision%2520approach%2520for%2520training%250Atext-to-SQL%2520parsers.%2520We%2520take%2520advantage%2520of%2520the%2520recently%2520proposed%2520question%250Ameaning%2520representation%2520called%2520QDMR%252C%2520an%2520intermediate%2520between%2520NL%2520and%2520formal%2520query%250Alanguages.%2520Given%2520questions%252C%2520their%2520QDMR%2520structures%2520%2528annotated%2520by%2520non-experts%2520or%250Aautomatically%2520predicted%2529%252C%2520and%2520the%2520answers%252C%2520we%2520are%2520able%2520to%2520automatically%250Asynthesize%2520SQL%2520queries%2520that%2520are%2520used%2520to%2520train%2520text-to-SQL%2520models.%2520We%2520test%2520our%250Aapproach%2520by%2520experimenting%2520on%2520five%2520benchmark%2520datasets.%2520Our%2520results%2520show%2520that%2520the%250Aweakly%2520supervised%2520models%2520perform%2520competitively%2520with%2520those%2520trained%2520on%2520annotated%250ANL-SQL%2520data.%2520Overall%252C%2520we%2520effectively%2520train%2520text-to-SQL%2520parsers%252C%2520while%2520using%250Azero%2520SQL%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.06311v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Text-to-SQL%20Parsing%20through%20Question%20Decomposition&entry.906535625=Tomer%20Wolfson%20and%20Daniel%20Deutch%20and%20Jonathan%20Berant&entry.1292438233=%20%20Text-to-SQL%20parsers%20are%20crucial%20in%20enabling%20non-experts%20to%20effortlessly%20query%0Arelational%20data.%20Training%20such%20parsers%2C%20by%20contrast%2C%20generally%20requires%0Aexpertise%20in%20annotating%20natural%20language%20%28NL%29%20utterances%20with%20corresponding%20SQL%0Aqueries.%20In%20this%20work%2C%20we%20propose%20a%20weak%20supervision%20approach%20for%20training%0Atext-to-SQL%20parsers.%20We%20take%20advantage%20of%20the%20recently%20proposed%20question%0Ameaning%20representation%20called%20QDMR%2C%20an%20intermediate%20between%20NL%20and%20formal%20query%0Alanguages.%20Given%20questions%2C%20their%20QDMR%20structures%20%28annotated%20by%20non-experts%20or%0Aautomatically%20predicted%29%2C%20and%20the%20answers%2C%20we%20are%20able%20to%20automatically%0Asynthesize%20SQL%20queries%20that%20are%20used%20to%20train%20text-to-SQL%20models.%20We%20test%20our%0Aapproach%20by%20experimenting%20on%20five%20benchmark%20datasets.%20Our%20results%20show%20that%20the%0Aweakly%20supervised%20models%20perform%20competitively%20with%20those%20trained%20on%20annotated%0ANL-SQL%20data.%20Overall%2C%20we%20effectively%20train%20text-to-SQL%20parsers%2C%20while%20using%0Azero%20SQL%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.06311v4&entry.124074799=Read"},
{"title": "VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling", "author": "Qian Zhang and Xiangzi Dai and Ninghua Yang and Xiang An and Ziyong Feng and Xingyu Ren", "abstract": "  VAR is a new generation paradigm that employs 'next-scale prediction' as\nopposed to 'next-token prediction'. This innovative transformation enables\nauto-regressive (AR) transformers to rapidly learn visual distributions and\nachieve robust generalization. However, the original VAR model is constrained\nto class-conditioned synthesis, relying solely on textual captions for\nguidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model\nthat integrates Visual Auto-Regressive techniques with the capabilities of\nCLIP. The VAR-CLIP framework encodes captions into text embeddings, which are\nthen utilized as textual conditions for image generation. To facilitate\ntraining on extensive datasets, such as ImageNet, we have constructed a\nsubstantial image-text dataset leveraging BLIP2. Furthermore, we delve into the\nsignificance of word positioning within CLIP for the purpose of caption\nguidance. Extensive experiments confirm VAR-CLIP's proficiency in generating\nfantasy images with high fidelity, textual congruence, and aesthetic\nexcellence. Our project page are https://github.com/daixiangzi/VAR-CLIP\n", "link": "http://arxiv.org/abs/2408.01181v1", "date": "2024-08-02", "relevancy": 2.3088, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5739}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAR-CLIP%3A%20Text-to-Image%20Generator%20with%20Visual%20Auto-Regressive%20Modeling&body=Title%3A%20VAR-CLIP%3A%20Text-to-Image%20Generator%20with%20Visual%20Auto-Regressive%20Modeling%0AAuthor%3A%20Qian%20Zhang%20and%20Xiangzi%20Dai%20and%20Ninghua%20Yang%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Xingyu%20Ren%0AAbstract%3A%20%20%20VAR%20is%20a%20new%20generation%20paradigm%20that%20employs%20%27next-scale%20prediction%27%20as%0Aopposed%20to%20%27next-token%20prediction%27.%20This%20innovative%20transformation%20enables%0Aauto-regressive%20%28AR%29%20transformers%20to%20rapidly%20learn%20visual%20distributions%20and%0Aachieve%20robust%20generalization.%20However%2C%20the%20original%20VAR%20model%20is%20constrained%0Ato%20class-conditioned%20synthesis%2C%20relying%20solely%20on%20textual%20captions%20for%0Aguidance.%20In%20this%20paper%2C%20we%20introduce%20VAR-CLIP%2C%20a%20novel%20text-to-image%20model%0Athat%20integrates%20Visual%20Auto-Regressive%20techniques%20with%20the%20capabilities%20of%0ACLIP.%20The%20VAR-CLIP%20framework%20encodes%20captions%20into%20text%20embeddings%2C%20which%20are%0Athen%20utilized%20as%20textual%20conditions%20for%20image%20generation.%20To%20facilitate%0Atraining%20on%20extensive%20datasets%2C%20such%20as%20ImageNet%2C%20we%20have%20constructed%20a%0Asubstantial%20image-text%20dataset%20leveraging%20BLIP2.%20Furthermore%2C%20we%20delve%20into%20the%0Asignificance%20of%20word%20positioning%20within%20CLIP%20for%20the%20purpose%20of%20caption%0Aguidance.%20Extensive%20experiments%20confirm%20VAR-CLIP%27s%20proficiency%20in%20generating%0Afantasy%20images%20with%20high%20fidelity%2C%20textual%20congruence%2C%20and%20aesthetic%0Aexcellence.%20Our%20project%20page%20are%20https%3A//github.com/daixiangzi/VAR-CLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAR-CLIP%253A%2520Text-to-Image%2520Generator%2520with%2520Visual%2520Auto-Regressive%2520Modeling%26entry.906535625%3DQian%2520Zhang%2520and%2520Xiangzi%2520Dai%2520and%2520Ninghua%2520Yang%2520and%2520Xiang%2520An%2520and%2520Ziyong%2520Feng%2520and%2520Xingyu%2520Ren%26entry.1292438233%3D%2520%2520VAR%2520is%2520a%2520new%2520generation%2520paradigm%2520that%2520employs%2520%2527next-scale%2520prediction%2527%2520as%250Aopposed%2520to%2520%2527next-token%2520prediction%2527.%2520This%2520innovative%2520transformation%2520enables%250Aauto-regressive%2520%2528AR%2529%2520transformers%2520to%2520rapidly%2520learn%2520visual%2520distributions%2520and%250Aachieve%2520robust%2520generalization.%2520However%252C%2520the%2520original%2520VAR%2520model%2520is%2520constrained%250Ato%2520class-conditioned%2520synthesis%252C%2520relying%2520solely%2520on%2520textual%2520captions%2520for%250Aguidance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VAR-CLIP%252C%2520a%2520novel%2520text-to-image%2520model%250Athat%2520integrates%2520Visual%2520Auto-Regressive%2520techniques%2520with%2520the%2520capabilities%2520of%250ACLIP.%2520The%2520VAR-CLIP%2520framework%2520encodes%2520captions%2520into%2520text%2520embeddings%252C%2520which%2520are%250Athen%2520utilized%2520as%2520textual%2520conditions%2520for%2520image%2520generation.%2520To%2520facilitate%250Atraining%2520on%2520extensive%2520datasets%252C%2520such%2520as%2520ImageNet%252C%2520we%2520have%2520constructed%2520a%250Asubstantial%2520image-text%2520dataset%2520leveraging%2520BLIP2.%2520Furthermore%252C%2520we%2520delve%2520into%2520the%250Asignificance%2520of%2520word%2520positioning%2520within%2520CLIP%2520for%2520the%2520purpose%2520of%2520caption%250Aguidance.%2520Extensive%2520experiments%2520confirm%2520VAR-CLIP%2527s%2520proficiency%2520in%2520generating%250Afantasy%2520images%2520with%2520high%2520fidelity%252C%2520textual%2520congruence%252C%2520and%2520aesthetic%250Aexcellence.%2520Our%2520project%2520page%2520are%2520https%253A//github.com/daixiangzi/VAR-CLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAR-CLIP%3A%20Text-to-Image%20Generator%20with%20Visual%20Auto-Regressive%20Modeling&entry.906535625=Qian%20Zhang%20and%20Xiangzi%20Dai%20and%20Ninghua%20Yang%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Xingyu%20Ren&entry.1292438233=%20%20VAR%20is%20a%20new%20generation%20paradigm%20that%20employs%20%27next-scale%20prediction%27%20as%0Aopposed%20to%20%27next-token%20prediction%27.%20This%20innovative%20transformation%20enables%0Aauto-regressive%20%28AR%29%20transformers%20to%20rapidly%20learn%20visual%20distributions%20and%0Aachieve%20robust%20generalization.%20However%2C%20the%20original%20VAR%20model%20is%20constrained%0Ato%20class-conditioned%20synthesis%2C%20relying%20solely%20on%20textual%20captions%20for%0Aguidance.%20In%20this%20paper%2C%20we%20introduce%20VAR-CLIP%2C%20a%20novel%20text-to-image%20model%0Athat%20integrates%20Visual%20Auto-Regressive%20techniques%20with%20the%20capabilities%20of%0ACLIP.%20The%20VAR-CLIP%20framework%20encodes%20captions%20into%20text%20embeddings%2C%20which%20are%0Athen%20utilized%20as%20textual%20conditions%20for%20image%20generation.%20To%20facilitate%0Atraining%20on%20extensive%20datasets%2C%20such%20as%20ImageNet%2C%20we%20have%20constructed%20a%0Asubstantial%20image-text%20dataset%20leveraging%20BLIP2.%20Furthermore%2C%20we%20delve%20into%20the%0Asignificance%20of%20word%20positioning%20within%20CLIP%20for%20the%20purpose%20of%20caption%0Aguidance.%20Extensive%20experiments%20confirm%20VAR-CLIP%27s%20proficiency%20in%20generating%0Afantasy%20images%20with%20high%20fidelity%2C%20textual%20congruence%2C%20and%20aesthetic%0Aexcellence.%20Our%20project%20page%20are%20https%3A//github.com/daixiangzi/VAR-CLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01181v1&entry.124074799=Read"},
{"title": "Optimal Mixed Integer Linear Optimization Trained Multivariate\n  Classification Trees", "author": "Brandon Alston and Illya V. Hicks", "abstract": "  Multivariate decision trees are powerful machine learning tools for\nclassification and regression that attract many researchers and industry\nprofessionals. An optimal binary tree has two types of vertices, (i) branching\nvertices which have exactly two children and where datapoints are assessed on a\nset of discrete features and (ii) leaf vertices at which datapoints are given a\nprediction, and can be obtained by solving a biobjective optimization problem\nthat seeks to (i) maximize the number of correctly classified datapoints and\n(ii) minimize the number of branching vertices. Branching vertices are linear\ncombinations of training features and therefore can be thought of as\nhyperplanes. In this paper, we propose two cut-based mixed integer linear\noptimization (MILO) formulations for designing optimal binary classification\ntrees (leaf vertices assign discrete classes). Our models leverage on-the-fly\nidentification of minimal infeasible subsystems (MISs) from which we derive\ncutting planes that hold the form of packing constraints. We show theoretical\nimprovements on the strongest flow-based MILO formulation currently in the\nliterature and conduct experiments on publicly available datasets to show our\nmodels' ability to scale, strength against traditional branch and bound\napproaches, and robustness in out-of-sample test performance. Our code and data\nare available on GitHub.\n", "link": "http://arxiv.org/abs/2408.01297v1", "date": "2024-08-02", "relevancy": 2.3087, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.482}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Mixed%20Integer%20Linear%20Optimization%20Trained%20Multivariate%0A%20%20Classification%20Trees&body=Title%3A%20Optimal%20Mixed%20Integer%20Linear%20Optimization%20Trained%20Multivariate%0A%20%20Classification%20Trees%0AAuthor%3A%20Brandon%20Alston%20and%20Illya%20V.%20Hicks%0AAbstract%3A%20%20%20Multivariate%20decision%20trees%20are%20powerful%20machine%20learning%20tools%20for%0Aclassification%20and%20regression%20that%20attract%20many%20researchers%20and%20industry%0Aprofessionals.%20An%20optimal%20binary%20tree%20has%20two%20types%20of%20vertices%2C%20%28i%29%20branching%0Avertices%20which%20have%20exactly%20two%20children%20and%20where%20datapoints%20are%20assessed%20on%20a%0Aset%20of%20discrete%20features%20and%20%28ii%29%20leaf%20vertices%20at%20which%20datapoints%20are%20given%20a%0Aprediction%2C%20and%20can%20be%20obtained%20by%20solving%20a%20biobjective%20optimization%20problem%0Athat%20seeks%20to%20%28i%29%20maximize%20the%20number%20of%20correctly%20classified%20datapoints%20and%0A%28ii%29%20minimize%20the%20number%20of%20branching%20vertices.%20Branching%20vertices%20are%20linear%0Acombinations%20of%20training%20features%20and%20therefore%20can%20be%20thought%20of%20as%0Ahyperplanes.%20In%20this%20paper%2C%20we%20propose%20two%20cut-based%20mixed%20integer%20linear%0Aoptimization%20%28MILO%29%20formulations%20for%20designing%20optimal%20binary%20classification%0Atrees%20%28leaf%20vertices%20assign%20discrete%20classes%29.%20Our%20models%20leverage%20on-the-fly%0Aidentification%20of%20minimal%20infeasible%20subsystems%20%28MISs%29%20from%20which%20we%20derive%0Acutting%20planes%20that%20hold%20the%20form%20of%20packing%20constraints.%20We%20show%20theoretical%0Aimprovements%20on%20the%20strongest%20flow-based%20MILO%20formulation%20currently%20in%20the%0Aliterature%20and%20conduct%20experiments%20on%20publicly%20available%20datasets%20to%20show%20our%0Amodels%27%20ability%20to%20scale%2C%20strength%20against%20traditional%20branch%20and%20bound%0Aapproaches%2C%20and%20robustness%20in%20out-of-sample%20test%20performance.%20Our%20code%20and%20data%0Aare%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Mixed%2520Integer%2520Linear%2520Optimization%2520Trained%2520Multivariate%250A%2520%2520Classification%2520Trees%26entry.906535625%3DBrandon%2520Alston%2520and%2520Illya%2520V.%2520Hicks%26entry.1292438233%3D%2520%2520Multivariate%2520decision%2520trees%2520are%2520powerful%2520machine%2520learning%2520tools%2520for%250Aclassification%2520and%2520regression%2520that%2520attract%2520many%2520researchers%2520and%2520industry%250Aprofessionals.%2520An%2520optimal%2520binary%2520tree%2520has%2520two%2520types%2520of%2520vertices%252C%2520%2528i%2529%2520branching%250Avertices%2520which%2520have%2520exactly%2520two%2520children%2520and%2520where%2520datapoints%2520are%2520assessed%2520on%2520a%250Aset%2520of%2520discrete%2520features%2520and%2520%2528ii%2529%2520leaf%2520vertices%2520at%2520which%2520datapoints%2520are%2520given%2520a%250Aprediction%252C%2520and%2520can%2520be%2520obtained%2520by%2520solving%2520a%2520biobjective%2520optimization%2520problem%250Athat%2520seeks%2520to%2520%2528i%2529%2520maximize%2520the%2520number%2520of%2520correctly%2520classified%2520datapoints%2520and%250A%2528ii%2529%2520minimize%2520the%2520number%2520of%2520branching%2520vertices.%2520Branching%2520vertices%2520are%2520linear%250Acombinations%2520of%2520training%2520features%2520and%2520therefore%2520can%2520be%2520thought%2520of%2520as%250Ahyperplanes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520cut-based%2520mixed%2520integer%2520linear%250Aoptimization%2520%2528MILO%2529%2520formulations%2520for%2520designing%2520optimal%2520binary%2520classification%250Atrees%2520%2528leaf%2520vertices%2520assign%2520discrete%2520classes%2529.%2520Our%2520models%2520leverage%2520on-the-fly%250Aidentification%2520of%2520minimal%2520infeasible%2520subsystems%2520%2528MISs%2529%2520from%2520which%2520we%2520derive%250Acutting%2520planes%2520that%2520hold%2520the%2520form%2520of%2520packing%2520constraints.%2520We%2520show%2520theoretical%250Aimprovements%2520on%2520the%2520strongest%2520flow-based%2520MILO%2520formulation%2520currently%2520in%2520the%250Aliterature%2520and%2520conduct%2520experiments%2520on%2520publicly%2520available%2520datasets%2520to%2520show%2520our%250Amodels%2527%2520ability%2520to%2520scale%252C%2520strength%2520against%2520traditional%2520branch%2520and%2520bound%250Aapproaches%252C%2520and%2520robustness%2520in%2520out-of-sample%2520test%2520performance.%2520Our%2520code%2520and%2520data%250Aare%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Mixed%20Integer%20Linear%20Optimization%20Trained%20Multivariate%0A%20%20Classification%20Trees&entry.906535625=Brandon%20Alston%20and%20Illya%20V.%20Hicks&entry.1292438233=%20%20Multivariate%20decision%20trees%20are%20powerful%20machine%20learning%20tools%20for%0Aclassification%20and%20regression%20that%20attract%20many%20researchers%20and%20industry%0Aprofessionals.%20An%20optimal%20binary%20tree%20has%20two%20types%20of%20vertices%2C%20%28i%29%20branching%0Avertices%20which%20have%20exactly%20two%20children%20and%20where%20datapoints%20are%20assessed%20on%20a%0Aset%20of%20discrete%20features%20and%20%28ii%29%20leaf%20vertices%20at%20which%20datapoints%20are%20given%20a%0Aprediction%2C%20and%20can%20be%20obtained%20by%20solving%20a%20biobjective%20optimization%20problem%0Athat%20seeks%20to%20%28i%29%20maximize%20the%20number%20of%20correctly%20classified%20datapoints%20and%0A%28ii%29%20minimize%20the%20number%20of%20branching%20vertices.%20Branching%20vertices%20are%20linear%0Acombinations%20of%20training%20features%20and%20therefore%20can%20be%20thought%20of%20as%0Ahyperplanes.%20In%20this%20paper%2C%20we%20propose%20two%20cut-based%20mixed%20integer%20linear%0Aoptimization%20%28MILO%29%20formulations%20for%20designing%20optimal%20binary%20classification%0Atrees%20%28leaf%20vertices%20assign%20discrete%20classes%29.%20Our%20models%20leverage%20on-the-fly%0Aidentification%20of%20minimal%20infeasible%20subsystems%20%28MISs%29%20from%20which%20we%20derive%0Acutting%20planes%20that%20hold%20the%20form%20of%20packing%20constraints.%20We%20show%20theoretical%0Aimprovements%20on%20the%20strongest%20flow-based%20MILO%20formulation%20currently%20in%20the%0Aliterature%20and%20conduct%20experiments%20on%20publicly%20available%20datasets%20to%20show%20our%0Amodels%27%20ability%20to%20scale%2C%20strength%20against%20traditional%20branch%20and%20bound%0Aapproaches%2C%20and%20robustness%20in%20out-of-sample%20test%20performance.%20Our%20code%20and%20data%0Aare%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01297v1&entry.124074799=Read"},
{"title": "Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance", "author": "Reyhane Askari Hemmat and Melissa Hall and Alicia Sun and Candace Ross and Michal Drozdzal and Adriana Romero-Soriano", "abstract": "  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n", "link": "http://arxiv.org/abs/2406.04551v2", "date": "2024-08-02", "relevancy": 2.2948, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5974}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5741}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Geo-diversity%20of%20Generated%20Images%20with%20Contextualized%20Vendi%0A%20%20Score%20Guidance&body=Title%3A%20Improving%20Geo-diversity%20of%20Generated%20Images%20with%20Contextualized%20Vendi%0A%20%20Score%20Guidance%0AAuthor%3A%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Alicia%20Sun%20and%20Candace%20Ross%20and%20Michal%20Drozdzal%20and%20Adriana%20Romero-Soriano%0AAbstract%3A%20%20%20With%20the%20growing%20popularity%20of%20text-to-image%20generative%20models%2C%20there%20has%0Abeen%20increasing%20focus%20on%20understanding%20their%20risks%20and%20biases.%20Recent%20work%20has%0Afound%20that%20state-of-the-art%20models%20struggle%20to%20depict%20everyday%20objects%20with%20the%0Atrue%20diversity%20of%20the%20real%20world%20and%20have%20notable%20gaps%20between%20geographic%0Aregions.%20In%20this%20work%2C%20we%20aim%20to%20increase%20the%20diversity%20of%20generated%20images%20of%0Acommon%20objects%20such%20that%20per-region%20variations%20are%20representative%20of%20the%20real%0Aworld.%20We%20introduce%20an%20inference%20time%20intervention%2C%20contextualized%20Vendi%20Score%0AGuidance%20%28c-VSG%29%2C%20that%20guides%20the%20backwards%20steps%20of%20latent%20diffusion%20models%20to%0Aincrease%20the%20diversity%20of%20a%20sample%20as%20compared%20to%20a%20%22memory%20bank%22%20of%20previously%0Agenerated%20images%20while%20constraining%20the%20amount%20of%20variation%20within%20that%20of%20an%0Aexemplar%20set%20of%20real-world%20contextualizing%20images.%20We%20evaluate%20c-VSG%20with%20two%0Ageographically%20representative%20datasets%20and%20find%20that%20it%20substantially%20increases%0Athe%20diversity%20of%20generated%20images%2C%20both%20for%20the%20worst%20performing%20regions%20and%20on%0Aaverage%2C%20while%20simultaneously%20maintaining%20or%20improving%20image%20quality%20and%0Aconsistency.%20Additionally%2C%20qualitative%20analyses%20reveal%20that%20diversity%20of%0Agenerated%20images%20is%20significantly%20improved%2C%20including%20along%20the%20lines%20of%0Areductive%20region%20portrayals%20present%20in%20the%20original%20model.%20We%20hope%20that%20this%0Awork%20is%20a%20step%20towards%20text-to-image%20generative%20models%20that%20reflect%20the%20true%0Ageographic%20diversity%20of%20the%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Geo-diversity%2520of%2520Generated%2520Images%2520with%2520Contextualized%2520Vendi%250A%2520%2520Score%2520Guidance%26entry.906535625%3DReyhane%2520Askari%2520Hemmat%2520and%2520Melissa%2520Hall%2520and%2520Alicia%2520Sun%2520and%2520Candace%2520Ross%2520and%2520Michal%2520Drozdzal%2520and%2520Adriana%2520Romero-Soriano%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520popularity%2520of%2520text-to-image%2520generative%2520models%252C%2520there%2520has%250Abeen%2520increasing%2520focus%2520on%2520understanding%2520their%2520risks%2520and%2520biases.%2520Recent%2520work%2520has%250Afound%2520that%2520state-of-the-art%2520models%2520struggle%2520to%2520depict%2520everyday%2520objects%2520with%2520the%250Atrue%2520diversity%2520of%2520the%2520real%2520world%2520and%2520have%2520notable%2520gaps%2520between%2520geographic%250Aregions.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520increase%2520the%2520diversity%2520of%2520generated%2520images%2520of%250Acommon%2520objects%2520such%2520that%2520per-region%2520variations%2520are%2520representative%2520of%2520the%2520real%250Aworld.%2520We%2520introduce%2520an%2520inference%2520time%2520intervention%252C%2520contextualized%2520Vendi%2520Score%250AGuidance%2520%2528c-VSG%2529%252C%2520that%2520guides%2520the%2520backwards%2520steps%2520of%2520latent%2520diffusion%2520models%2520to%250Aincrease%2520the%2520diversity%2520of%2520a%2520sample%2520as%2520compared%2520to%2520a%2520%2522memory%2520bank%2522%2520of%2520previously%250Agenerated%2520images%2520while%2520constraining%2520the%2520amount%2520of%2520variation%2520within%2520that%2520of%2520an%250Aexemplar%2520set%2520of%2520real-world%2520contextualizing%2520images.%2520We%2520evaluate%2520c-VSG%2520with%2520two%250Ageographically%2520representative%2520datasets%2520and%2520find%2520that%2520it%2520substantially%2520increases%250Athe%2520diversity%2520of%2520generated%2520images%252C%2520both%2520for%2520the%2520worst%2520performing%2520regions%2520and%2520on%250Aaverage%252C%2520while%2520simultaneously%2520maintaining%2520or%2520improving%2520image%2520quality%2520and%250Aconsistency.%2520Additionally%252C%2520qualitative%2520analyses%2520reveal%2520that%2520diversity%2520of%250Agenerated%2520images%2520is%2520significantly%2520improved%252C%2520including%2520along%2520the%2520lines%2520of%250Areductive%2520region%2520portrayals%2520present%2520in%2520the%2520original%2520model.%2520We%2520hope%2520that%2520this%250Awork%2520is%2520a%2520step%2520towards%2520text-to-image%2520generative%2520models%2520that%2520reflect%2520the%2520true%250Ageographic%2520diversity%2520of%2520the%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Geo-diversity%20of%20Generated%20Images%20with%20Contextualized%20Vendi%0A%20%20Score%20Guidance&entry.906535625=Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Alicia%20Sun%20and%20Candace%20Ross%20and%20Michal%20Drozdzal%20and%20Adriana%20Romero-Soriano&entry.1292438233=%20%20With%20the%20growing%20popularity%20of%20text-to-image%20generative%20models%2C%20there%20has%0Abeen%20increasing%20focus%20on%20understanding%20their%20risks%20and%20biases.%20Recent%20work%20has%0Afound%20that%20state-of-the-art%20models%20struggle%20to%20depict%20everyday%20objects%20with%20the%0Atrue%20diversity%20of%20the%20real%20world%20and%20have%20notable%20gaps%20between%20geographic%0Aregions.%20In%20this%20work%2C%20we%20aim%20to%20increase%20the%20diversity%20of%20generated%20images%20of%0Acommon%20objects%20such%20that%20per-region%20variations%20are%20representative%20of%20the%20real%0Aworld.%20We%20introduce%20an%20inference%20time%20intervention%2C%20contextualized%20Vendi%20Score%0AGuidance%20%28c-VSG%29%2C%20that%20guides%20the%20backwards%20steps%20of%20latent%20diffusion%20models%20to%0Aincrease%20the%20diversity%20of%20a%20sample%20as%20compared%20to%20a%20%22memory%20bank%22%20of%20previously%0Agenerated%20images%20while%20constraining%20the%20amount%20of%20variation%20within%20that%20of%20an%0Aexemplar%20set%20of%20real-world%20contextualizing%20images.%20We%20evaluate%20c-VSG%20with%20two%0Ageographically%20representative%20datasets%20and%20find%20that%20it%20substantially%20increases%0Athe%20diversity%20of%20generated%20images%2C%20both%20for%20the%20worst%20performing%20regions%20and%20on%0Aaverage%2C%20while%20simultaneously%20maintaining%20or%20improving%20image%20quality%20and%0Aconsistency.%20Additionally%2C%20qualitative%20analyses%20reveal%20that%20diversity%20of%0Agenerated%20images%20is%20significantly%20improved%2C%20including%20along%20the%20lines%20of%0Areductive%20region%20portrayals%20present%20in%20the%20original%20model.%20We%20hope%20that%20this%0Awork%20is%20a%20step%20towards%20text-to-image%20generative%20models%20that%20reflect%20the%20true%0Ageographic%20diversity%20of%20the%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04551v2&entry.124074799=Read"},
{"title": "Don't Waste Your Time: Early Stopping Cross-Validation", "author": "Edward Bergman and Lennart Purucker and Frank Hutter", "abstract": "  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n", "link": "http://arxiv.org/abs/2405.03389v2", "date": "2024-08-02", "relevancy": 2.2903, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4622}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation&body=Title%3A%20Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation%0AAuthor%3A%20Edward%20Bergman%20and%20Lennart%20Purucker%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20State-of-the-art%20automated%20machine%20learning%20systems%20for%20tabular%20data%20often%0Aemploy%20cross-validation%3B%20ensuring%20that%20measured%20performances%20generalize%20to%0Aunseen%20data%2C%20or%20that%20subsequent%20ensembling%20does%20not%20overfit.%20However%2C%20using%0Ak-fold%20cross-validation%20instead%20of%20holdout%20validation%20drastically%20increases%20the%0Acomputational%20cost%20of%20validating%20a%20single%20configuration.%20While%20ensuring%20better%0Ageneralization%20and%2C%20by%20extension%2C%20better%20performance%2C%20the%20additional%20cost%20is%0Aoften%20prohibitive%20for%20effective%20model%20selection%20within%20a%20time%20budget.%20We%20aim%20to%0Amake%20model%20selection%20with%20cross-validation%20more%20effective.%20Therefore%2C%20we%20study%0Aearly%20stopping%20the%20process%20of%20cross-validation%20during%20model%20selection.%20We%0Ainvestigate%20the%20impact%20of%20early%20stopping%20on%20random%20search%20for%20two%20algorithms%2C%0AMLP%20and%20random%20forest%2C%20across%2036%20classification%20datasets.%20We%20further%20analyze%0Athe%20impact%20of%20the%20number%20of%20folds%20by%20considering%203-%2C%205-%2C%20and%2010-folds.%20In%0Aaddition%2C%20we%20investigate%20the%20impact%20of%20early%20stopping%20with%20Bayesian%0Aoptimization%20instead%20of%20random%20search%20and%20also%20repeated%20cross-validation.%20Our%0Aexploratory%20study%20shows%20that%20even%20a%20simple-to-understand%20and%20easy-to-implement%0Amethod%20consistently%20allows%20model%20selection%20to%20converge%20faster%3B%20in%20~94%25%20of%20all%0Adatasets%2C%20on%20average%20by%20~214%25.%20Moreover%2C%20stopping%20cross-validation%20enables%0Amodel%20selection%20to%20explore%20the%20search%20space%20more%20exhaustively%20by%20considering%0A%2B167%25%20configurations%20on%20average%20within%20one%20hour%2C%20while%20also%20obtaining%20better%0Aoverall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Waste%2520Your%2520Time%253A%2520Early%2520Stopping%2520Cross-Validation%26entry.906535625%3DEdward%2520Bergman%2520and%2520Lennart%2520Purucker%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520State-of-the-art%2520automated%2520machine%2520learning%2520systems%2520for%2520tabular%2520data%2520often%250Aemploy%2520cross-validation%253B%2520ensuring%2520that%2520measured%2520performances%2520generalize%2520to%250Aunseen%2520data%252C%2520or%2520that%2520subsequent%2520ensembling%2520does%2520not%2520overfit.%2520However%252C%2520using%250Ak-fold%2520cross-validation%2520instead%2520of%2520holdout%2520validation%2520drastically%2520increases%2520the%250Acomputational%2520cost%2520of%2520validating%2520a%2520single%2520configuration.%2520While%2520ensuring%2520better%250Ageneralization%2520and%252C%2520by%2520extension%252C%2520better%2520performance%252C%2520the%2520additional%2520cost%2520is%250Aoften%2520prohibitive%2520for%2520effective%2520model%2520selection%2520within%2520a%2520time%2520budget.%2520We%2520aim%2520to%250Amake%2520model%2520selection%2520with%2520cross-validation%2520more%2520effective.%2520Therefore%252C%2520we%2520study%250Aearly%2520stopping%2520the%2520process%2520of%2520cross-validation%2520during%2520model%2520selection.%2520We%250Ainvestigate%2520the%2520impact%2520of%2520early%2520stopping%2520on%2520random%2520search%2520for%2520two%2520algorithms%252C%250AMLP%2520and%2520random%2520forest%252C%2520across%252036%2520classification%2520datasets.%2520We%2520further%2520analyze%250Athe%2520impact%2520of%2520the%2520number%2520of%2520folds%2520by%2520considering%25203-%252C%25205-%252C%2520and%252010-folds.%2520In%250Aaddition%252C%2520we%2520investigate%2520the%2520impact%2520of%2520early%2520stopping%2520with%2520Bayesian%250Aoptimization%2520instead%2520of%2520random%2520search%2520and%2520also%2520repeated%2520cross-validation.%2520Our%250Aexploratory%2520study%2520shows%2520that%2520even%2520a%2520simple-to-understand%2520and%2520easy-to-implement%250Amethod%2520consistently%2520allows%2520model%2520selection%2520to%2520converge%2520faster%253B%2520in%2520~94%2525%2520of%2520all%250Adatasets%252C%2520on%2520average%2520by%2520~214%2525.%2520Moreover%252C%2520stopping%2520cross-validation%2520enables%250Amodel%2520selection%2520to%2520explore%2520the%2520search%2520space%2520more%2520exhaustively%2520by%2520considering%250A%252B167%2525%2520configurations%2520on%2520average%2520within%2520one%2520hour%252C%2520while%2520also%2520obtaining%2520better%250Aoverall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Waste%20Your%20Time%3A%20Early%20Stopping%20Cross-Validation&entry.906535625=Edward%20Bergman%20and%20Lennart%20Purucker%20and%20Frank%20Hutter&entry.1292438233=%20%20State-of-the-art%20automated%20machine%20learning%20systems%20for%20tabular%20data%20often%0Aemploy%20cross-validation%3B%20ensuring%20that%20measured%20performances%20generalize%20to%0Aunseen%20data%2C%20or%20that%20subsequent%20ensembling%20does%20not%20overfit.%20However%2C%20using%0Ak-fold%20cross-validation%20instead%20of%20holdout%20validation%20drastically%20increases%20the%0Acomputational%20cost%20of%20validating%20a%20single%20configuration.%20While%20ensuring%20better%0Ageneralization%20and%2C%20by%20extension%2C%20better%20performance%2C%20the%20additional%20cost%20is%0Aoften%20prohibitive%20for%20effective%20model%20selection%20within%20a%20time%20budget.%20We%20aim%20to%0Amake%20model%20selection%20with%20cross-validation%20more%20effective.%20Therefore%2C%20we%20study%0Aearly%20stopping%20the%20process%20of%20cross-validation%20during%20model%20selection.%20We%0Ainvestigate%20the%20impact%20of%20early%20stopping%20on%20random%20search%20for%20two%20algorithms%2C%0AMLP%20and%20random%20forest%2C%20across%2036%20classification%20datasets.%20We%20further%20analyze%0Athe%20impact%20of%20the%20number%20of%20folds%20by%20considering%203-%2C%205-%2C%20and%2010-folds.%20In%0Aaddition%2C%20we%20investigate%20the%20impact%20of%20early%20stopping%20with%20Bayesian%0Aoptimization%20instead%20of%20random%20search%20and%20also%20repeated%20cross-validation.%20Our%0Aexploratory%20study%20shows%20that%20even%20a%20simple-to-understand%20and%20easy-to-implement%0Amethod%20consistently%20allows%20model%20selection%20to%20converge%20faster%3B%20in%20~94%25%20of%20all%0Adatasets%2C%20on%20average%20by%20~214%25.%20Moreover%2C%20stopping%20cross-validation%20enables%0Amodel%20selection%20to%20explore%20the%20search%20space%20more%20exhaustively%20by%20considering%0A%2B167%25%20configurations%20on%20average%20within%20one%20hour%2C%20while%20also%20obtaining%20better%0Aoverall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03389v2&entry.124074799=Read"},
{"title": "Resampling and averaging coordinates on data", "author": "Andrew J. Blumberg and Mathieu Carriere and Jun Hou Fung and Michael A. Mandell", "abstract": "  We introduce algorithms for robustly computing intrinsic coordinates on point\nclouds. Our approach relies on generating many candidate coordinates by\nsubsampling the data and varying hyperparameters of the embedding algorithm\n(e.g., manifold learning). We then identify a subset of representative\nembeddings by clustering the collection of candidate coordinates and using\nshape descriptors from topological data analysis. The final output is the\nembedding obtained as an average of the representative embeddings using\ngeneralized Procrustes analysis. We validate our algorithm on both synthetic\ndata and experimental measurements from genomics, demonstrating robustness to\nnoise and outliers.\n", "link": "http://arxiv.org/abs/2408.01379v1", "date": "2024-08-02", "relevancy": 2.2845, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.46}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4563}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resampling%20and%20averaging%20coordinates%20on%20data&body=Title%3A%20Resampling%20and%20averaging%20coordinates%20on%20data%0AAuthor%3A%20Andrew%20J.%20Blumberg%20and%20Mathieu%20Carriere%20and%20Jun%20Hou%20Fung%20and%20Michael%20A.%20Mandell%0AAbstract%3A%20%20%20We%20introduce%20algorithms%20for%20robustly%20computing%20intrinsic%20coordinates%20on%20point%0Aclouds.%20Our%20approach%20relies%20on%20generating%20many%20candidate%20coordinates%20by%0Asubsampling%20the%20data%20and%20varying%20hyperparameters%20of%20the%20embedding%20algorithm%0A%28e.g.%2C%20manifold%20learning%29.%20We%20then%20identify%20a%20subset%20of%20representative%0Aembeddings%20by%20clustering%20the%20collection%20of%20candidate%20coordinates%20and%20using%0Ashape%20descriptors%20from%20topological%20data%20analysis.%20The%20final%20output%20is%20the%0Aembedding%20obtained%20as%20an%20average%20of%20the%20representative%20embeddings%20using%0Ageneralized%20Procrustes%20analysis.%20We%20validate%20our%20algorithm%20on%20both%20synthetic%0Adata%20and%20experimental%20measurements%20from%20genomics%2C%20demonstrating%20robustness%20to%0Anoise%20and%20outliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResampling%2520and%2520averaging%2520coordinates%2520on%2520data%26entry.906535625%3DAndrew%2520J.%2520Blumberg%2520and%2520Mathieu%2520Carriere%2520and%2520Jun%2520Hou%2520Fung%2520and%2520Michael%2520A.%2520Mandell%26entry.1292438233%3D%2520%2520We%2520introduce%2520algorithms%2520for%2520robustly%2520computing%2520intrinsic%2520coordinates%2520on%2520point%250Aclouds.%2520Our%2520approach%2520relies%2520on%2520generating%2520many%2520candidate%2520coordinates%2520by%250Asubsampling%2520the%2520data%2520and%2520varying%2520hyperparameters%2520of%2520the%2520embedding%2520algorithm%250A%2528e.g.%252C%2520manifold%2520learning%2529.%2520We%2520then%2520identify%2520a%2520subset%2520of%2520representative%250Aembeddings%2520by%2520clustering%2520the%2520collection%2520of%2520candidate%2520coordinates%2520and%2520using%250Ashape%2520descriptors%2520from%2520topological%2520data%2520analysis.%2520The%2520final%2520output%2520is%2520the%250Aembedding%2520obtained%2520as%2520an%2520average%2520of%2520the%2520representative%2520embeddings%2520using%250Ageneralized%2520Procrustes%2520analysis.%2520We%2520validate%2520our%2520algorithm%2520on%2520both%2520synthetic%250Adata%2520and%2520experimental%2520measurements%2520from%2520genomics%252C%2520demonstrating%2520robustness%2520to%250Anoise%2520and%2520outliers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resampling%20and%20averaging%20coordinates%20on%20data&entry.906535625=Andrew%20J.%20Blumberg%20and%20Mathieu%20Carriere%20and%20Jun%20Hou%20Fung%20and%20Michael%20A.%20Mandell&entry.1292438233=%20%20We%20introduce%20algorithms%20for%20robustly%20computing%20intrinsic%20coordinates%20on%20point%0Aclouds.%20Our%20approach%20relies%20on%20generating%20many%20candidate%20coordinates%20by%0Asubsampling%20the%20data%20and%20varying%20hyperparameters%20of%20the%20embedding%20algorithm%0A%28e.g.%2C%20manifold%20learning%29.%20We%20then%20identify%20a%20subset%20of%20representative%0Aembeddings%20by%20clustering%20the%20collection%20of%20candidate%20coordinates%20and%20using%0Ashape%20descriptors%20from%20topological%20data%20analysis.%20The%20final%20output%20is%20the%0Aembedding%20obtained%20as%20an%20average%20of%20the%20representative%20embeddings%20using%0Ageneralized%20Procrustes%20analysis.%20We%20validate%20our%20algorithm%20on%20both%20synthetic%0Adata%20and%20experimental%20measurements%20from%20genomics%2C%20demonstrating%20robustness%20to%0Anoise%20and%20outliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01379v1&entry.124074799=Read"},
{"title": "MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection", "author": "Heitor R. Medeiros and David Latortue and Eric Granger and Marco Pedersoli", "abstract": "  In real-world scenarios, using multiple modalities like visible (RGB) and\ninfrared (IR) can greatly improve the performance of a predictive task such as\nobject detection (OD). Multimodal learning is a common way to leverage these\nmodalities, where multiple modality-specific encoders and a fusion module are\nused to improve performance. In this paper, we tackle a different way to employ\nRGB and IR modalities, where only one modality or the other is observed by a\nsingle shared vision encoder. This realistic setting requires a lower memory\nfootprint and is more suitable for applications such as autonomous driving and\nsurveillance, which commonly rely on RGB and IR data. However, when learning a\nsingle encoder on multiple modalities, one modality can dominate the other,\nproducing uneven recognition results. This work investigates how to efficiently\nleverage RGB and IR modalities to train a common transformer-based OD vision\nencoder, while countering the effects of modality imbalance. For this, we\nintroduce a novel training technique to Mix Patches (MiPa) from the two\nmodalities, in conjunction with a patch-wise modality agnostic module, for\nlearning a common representation of both modalities. Our experiments show that\nMiPa can learn a representation to reach competitive results on traditional\nRGB/IR benchmarks while only requiring a single modality during inference. Our\ncode is available at: https://github.com/heitorrapela/MiPa.\n", "link": "http://arxiv.org/abs/2404.18849v2", "date": "2024-08-02", "relevancy": 2.2729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5782}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection&body=Title%3A%20MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection%0AAuthor%3A%20Heitor%20R.%20Medeiros%20and%20David%20Latortue%20and%20Eric%20Granger%20and%20Marco%20Pedersoli%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20using%20multiple%20modalities%20like%20visible%20%28RGB%29%20and%0Ainfrared%20%28IR%29%20can%20greatly%20improve%20the%20performance%20of%20a%20predictive%20task%20such%20as%0Aobject%20detection%20%28OD%29.%20Multimodal%20learning%20is%20a%20common%20way%20to%20leverage%20these%0Amodalities%2C%20where%20multiple%20modality-specific%20encoders%20and%20a%20fusion%20module%20are%0Aused%20to%20improve%20performance.%20In%20this%20paper%2C%20we%20tackle%20a%20different%20way%20to%20employ%0ARGB%20and%20IR%20modalities%2C%20where%20only%20one%20modality%20or%20the%20other%20is%20observed%20by%20a%0Asingle%20shared%20vision%20encoder.%20This%20realistic%20setting%20requires%20a%20lower%20memory%0Afootprint%20and%20is%20more%20suitable%20for%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance%2C%20which%20commonly%20rely%20on%20RGB%20and%20IR%20data.%20However%2C%20when%20learning%20a%0Asingle%20encoder%20on%20multiple%20modalities%2C%20one%20modality%20can%20dominate%20the%20other%2C%0Aproducing%20uneven%20recognition%20results.%20This%20work%20investigates%20how%20to%20efficiently%0Aleverage%20RGB%20and%20IR%20modalities%20to%20train%20a%20common%20transformer-based%20OD%20vision%0Aencoder%2C%20while%20countering%20the%20effects%20of%20modality%20imbalance.%20For%20this%2C%20we%0Aintroduce%20a%20novel%20training%20technique%20to%20Mix%20Patches%20%28MiPa%29%20from%20the%20two%0Amodalities%2C%20in%20conjunction%20with%20a%20patch-wise%20modality%20agnostic%20module%2C%20for%0Alearning%20a%20common%20representation%20of%20both%20modalities.%20Our%20experiments%20show%20that%0AMiPa%20can%20learn%20a%20representation%20to%20reach%20competitive%20results%20on%20traditional%0ARGB/IR%20benchmarks%20while%20only%20requiring%20a%20single%20modality%20during%20inference.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/heitorrapela/MiPa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiPa%253A%2520Mixed%2520Patch%2520Infrared-Visible%2520Modality%2520Agnostic%2520Object%2520Detection%26entry.906535625%3DHeitor%2520R.%2520Medeiros%2520and%2520David%2520Latortue%2520and%2520Eric%2520Granger%2520and%2520Marco%2520Pedersoli%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520using%2520multiple%2520modalities%2520like%2520visible%2520%2528RGB%2529%2520and%250Ainfrared%2520%2528IR%2529%2520can%2520greatly%2520improve%2520the%2520performance%2520of%2520a%2520predictive%2520task%2520such%2520as%250Aobject%2520detection%2520%2528OD%2529.%2520Multimodal%2520learning%2520is%2520a%2520common%2520way%2520to%2520leverage%2520these%250Amodalities%252C%2520where%2520multiple%2520modality-specific%2520encoders%2520and%2520a%2520fusion%2520module%2520are%250Aused%2520to%2520improve%2520performance.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520a%2520different%2520way%2520to%2520employ%250ARGB%2520and%2520IR%2520modalities%252C%2520where%2520only%2520one%2520modality%2520or%2520the%2520other%2520is%2520observed%2520by%2520a%250Asingle%2520shared%2520vision%2520encoder.%2520This%2520realistic%2520setting%2520requires%2520a%2520lower%2520memory%250Afootprint%2520and%2520is%2520more%2520suitable%2520for%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%250Asurveillance%252C%2520which%2520commonly%2520rely%2520on%2520RGB%2520and%2520IR%2520data.%2520However%252C%2520when%2520learning%2520a%250Asingle%2520encoder%2520on%2520multiple%2520modalities%252C%2520one%2520modality%2520can%2520dominate%2520the%2520other%252C%250Aproducing%2520uneven%2520recognition%2520results.%2520This%2520work%2520investigates%2520how%2520to%2520efficiently%250Aleverage%2520RGB%2520and%2520IR%2520modalities%2520to%2520train%2520a%2520common%2520transformer-based%2520OD%2520vision%250Aencoder%252C%2520while%2520countering%2520the%2520effects%2520of%2520modality%2520imbalance.%2520For%2520this%252C%2520we%250Aintroduce%2520a%2520novel%2520training%2520technique%2520to%2520Mix%2520Patches%2520%2528MiPa%2529%2520from%2520the%2520two%250Amodalities%252C%2520in%2520conjunction%2520with%2520a%2520patch-wise%2520modality%2520agnostic%2520module%252C%2520for%250Alearning%2520a%2520common%2520representation%2520of%2520both%2520modalities.%2520Our%2520experiments%2520show%2520that%250AMiPa%2520can%2520learn%2520a%2520representation%2520to%2520reach%2520competitive%2520results%2520on%2520traditional%250ARGB/IR%2520benchmarks%2520while%2520only%2520requiring%2520a%2520single%2520modality%2520during%2520inference.%2520Our%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/heitorrapela/MiPa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection&entry.906535625=Heitor%20R.%20Medeiros%20and%20David%20Latortue%20and%20Eric%20Granger%20and%20Marco%20Pedersoli&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20using%20multiple%20modalities%20like%20visible%20%28RGB%29%20and%0Ainfrared%20%28IR%29%20can%20greatly%20improve%20the%20performance%20of%20a%20predictive%20task%20such%20as%0Aobject%20detection%20%28OD%29.%20Multimodal%20learning%20is%20a%20common%20way%20to%20leverage%20these%0Amodalities%2C%20where%20multiple%20modality-specific%20encoders%20and%20a%20fusion%20module%20are%0Aused%20to%20improve%20performance.%20In%20this%20paper%2C%20we%20tackle%20a%20different%20way%20to%20employ%0ARGB%20and%20IR%20modalities%2C%20where%20only%20one%20modality%20or%20the%20other%20is%20observed%20by%20a%0Asingle%20shared%20vision%20encoder.%20This%20realistic%20setting%20requires%20a%20lower%20memory%0Afootprint%20and%20is%20more%20suitable%20for%20applications%20such%20as%20autonomous%20driving%20and%0Asurveillance%2C%20which%20commonly%20rely%20on%20RGB%20and%20IR%20data.%20However%2C%20when%20learning%20a%0Asingle%20encoder%20on%20multiple%20modalities%2C%20one%20modality%20can%20dominate%20the%20other%2C%0Aproducing%20uneven%20recognition%20results.%20This%20work%20investigates%20how%20to%20efficiently%0Aleverage%20RGB%20and%20IR%20modalities%20to%20train%20a%20common%20transformer-based%20OD%20vision%0Aencoder%2C%20while%20countering%20the%20effects%20of%20modality%20imbalance.%20For%20this%2C%20we%0Aintroduce%20a%20novel%20training%20technique%20to%20Mix%20Patches%20%28MiPa%29%20from%20the%20two%0Amodalities%2C%20in%20conjunction%20with%20a%20patch-wise%20modality%20agnostic%20module%2C%20for%0Alearning%20a%20common%20representation%20of%20both%20modalities.%20Our%20experiments%20show%20that%0AMiPa%20can%20learn%20a%20representation%20to%20reach%20competitive%20results%20on%20traditional%0ARGB/IR%20benchmarks%20while%20only%20requiring%20a%20single%20modality%20during%20inference.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/heitorrapela/MiPa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18849v2&entry.124074799=Read"},
{"title": "EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using\n  Windowed Nonlinear Optimization", "author": "Runze Yuan and Tao Liu and Zijia Dai and Yi-Fan Zuo and Laurent Kneip", "abstract": "  Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.\n", "link": "http://arxiv.org/abs/2408.01370v1", "date": "2024-08-02", "relevancy": 2.2695, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVIT%3A%20Event-based%20Visual-Inertial%20Tracking%20in%20Semi-Dense%20Maps%20Using%0A%20%20Windowed%20Nonlinear%20Optimization&body=Title%3A%20EVIT%3A%20Event-based%20Visual-Inertial%20Tracking%20in%20Semi-Dense%20Maps%20Using%0A%20%20Windowed%20Nonlinear%20Optimization%0AAuthor%3A%20Runze%20Yuan%20and%20Tao%20Liu%20and%20Zijia%20Dai%20and%20Yi-Fan%20Zuo%20and%20Laurent%20Kneip%0AAbstract%3A%20%20%20Event%20cameras%20are%20an%20interesting%20visual%20exteroceptive%20sensor%20that%20reacts%20to%0Abrightness%20changes%20rather%20than%20integrating%20absolute%20image%20intensities.%20Owing%20to%0Athis%20design%2C%20the%20sensor%20exhibits%20strong%20performance%20in%20situations%20of%0Achallenging%20dynamics%20and%20illumination%20conditions.%20While%20event-based%0Asimultaneous%20tracking%20and%20mapping%20remains%20a%20challenging%20problem%2C%20a%20number%20of%0Arecent%20works%20have%20pointed%20out%20the%20sensor%27s%20suitability%20for%20prior%20map-based%0Atracking.%20By%20making%20use%20of%20cross-modal%20registration%20paradigms%2C%20the%20camera%27s%0Aego-motion%20can%20be%20tracked%20across%20a%20large%20spectrum%20of%20illumination%20and%20dynamics%0Aconditions%20on%20top%20of%20accurate%20maps%20that%20have%20been%20created%20a%20priori%20by%20more%0Atraditional%20sensors.%20The%20present%20paper%20follows%20up%20on%20a%20recently%20introduced%0Aevent-based%20geometric%20semi-dense%20tracking%20paradigm%2C%20and%20proposes%20the%20addition%0Aof%20inertial%20signals%20in%20order%20to%20robustify%20the%20estimation.%20More%20specifically%2C%0Athe%20added%20signals%20provide%20strong%20cues%20for%20pose%20initialization%20as%20well%20as%0Aregularization%20during%20windowed%2C%20multi-frame%20tracking.%20As%20a%20result%2C%20the%20proposed%0Aframework%20achieves%20increased%20performance%20under%20challenging%20illumination%0Aconditions%20as%20well%20as%20a%20reduction%20of%20the%20rate%20at%20which%20intermediate%20event%0Arepresentations%20need%20to%20be%20registered%20in%20order%20to%20maintain%20stable%20tracking%0Aacross%20highly%20dynamic%20sequences.%20Our%20evaluation%20focuses%20on%20a%20diverse%20set%20of%0Areal%20world%20sequences%20and%20comprises%20a%20comparison%20of%20our%20proposed%20method%20against%0Aa%20purely%20event-based%20alternative%20running%20at%20different%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVIT%253A%2520Event-based%2520Visual-Inertial%2520Tracking%2520in%2520Semi-Dense%2520Maps%2520Using%250A%2520%2520Windowed%2520Nonlinear%2520Optimization%26entry.906535625%3DRunze%2520Yuan%2520and%2520Tao%2520Liu%2520and%2520Zijia%2520Dai%2520and%2520Yi-Fan%2520Zuo%2520and%2520Laurent%2520Kneip%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520an%2520interesting%2520visual%2520exteroceptive%2520sensor%2520that%2520reacts%2520to%250Abrightness%2520changes%2520rather%2520than%2520integrating%2520absolute%2520image%2520intensities.%2520Owing%2520to%250Athis%2520design%252C%2520the%2520sensor%2520exhibits%2520strong%2520performance%2520in%2520situations%2520of%250Achallenging%2520dynamics%2520and%2520illumination%2520conditions.%2520While%2520event-based%250Asimultaneous%2520tracking%2520and%2520mapping%2520remains%2520a%2520challenging%2520problem%252C%2520a%2520number%2520of%250Arecent%2520works%2520have%2520pointed%2520out%2520the%2520sensor%2527s%2520suitability%2520for%2520prior%2520map-based%250Atracking.%2520By%2520making%2520use%2520of%2520cross-modal%2520registration%2520paradigms%252C%2520the%2520camera%2527s%250Aego-motion%2520can%2520be%2520tracked%2520across%2520a%2520large%2520spectrum%2520of%2520illumination%2520and%2520dynamics%250Aconditions%2520on%2520top%2520of%2520accurate%2520maps%2520that%2520have%2520been%2520created%2520a%2520priori%2520by%2520more%250Atraditional%2520sensors.%2520The%2520present%2520paper%2520follows%2520up%2520on%2520a%2520recently%2520introduced%250Aevent-based%2520geometric%2520semi-dense%2520tracking%2520paradigm%252C%2520and%2520proposes%2520the%2520addition%250Aof%2520inertial%2520signals%2520in%2520order%2520to%2520robustify%2520the%2520estimation.%2520More%2520specifically%252C%250Athe%2520added%2520signals%2520provide%2520strong%2520cues%2520for%2520pose%2520initialization%2520as%2520well%2520as%250Aregularization%2520during%2520windowed%252C%2520multi-frame%2520tracking.%2520As%2520a%2520result%252C%2520the%2520proposed%250Aframework%2520achieves%2520increased%2520performance%2520under%2520challenging%2520illumination%250Aconditions%2520as%2520well%2520as%2520a%2520reduction%2520of%2520the%2520rate%2520at%2520which%2520intermediate%2520event%250Arepresentations%2520need%2520to%2520be%2520registered%2520in%2520order%2520to%2520maintain%2520stable%2520tracking%250Aacross%2520highly%2520dynamic%2520sequences.%2520Our%2520evaluation%2520focuses%2520on%2520a%2520diverse%2520set%2520of%250Areal%2520world%2520sequences%2520and%2520comprises%2520a%2520comparison%2520of%2520our%2520proposed%2520method%2520against%250Aa%2520purely%2520event-based%2520alternative%2520running%2520at%2520different%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVIT%3A%20Event-based%20Visual-Inertial%20Tracking%20in%20Semi-Dense%20Maps%20Using%0A%20%20Windowed%20Nonlinear%20Optimization&entry.906535625=Runze%20Yuan%20and%20Tao%20Liu%20and%20Zijia%20Dai%20and%20Yi-Fan%20Zuo%20and%20Laurent%20Kneip&entry.1292438233=%20%20Event%20cameras%20are%20an%20interesting%20visual%20exteroceptive%20sensor%20that%20reacts%20to%0Abrightness%20changes%20rather%20than%20integrating%20absolute%20image%20intensities.%20Owing%20to%0Athis%20design%2C%20the%20sensor%20exhibits%20strong%20performance%20in%20situations%20of%0Achallenging%20dynamics%20and%20illumination%20conditions.%20While%20event-based%0Asimultaneous%20tracking%20and%20mapping%20remains%20a%20challenging%20problem%2C%20a%20number%20of%0Arecent%20works%20have%20pointed%20out%20the%20sensor%27s%20suitability%20for%20prior%20map-based%0Atracking.%20By%20making%20use%20of%20cross-modal%20registration%20paradigms%2C%20the%20camera%27s%0Aego-motion%20can%20be%20tracked%20across%20a%20large%20spectrum%20of%20illumination%20and%20dynamics%0Aconditions%20on%20top%20of%20accurate%20maps%20that%20have%20been%20created%20a%20priori%20by%20more%0Atraditional%20sensors.%20The%20present%20paper%20follows%20up%20on%20a%20recently%20introduced%0Aevent-based%20geometric%20semi-dense%20tracking%20paradigm%2C%20and%20proposes%20the%20addition%0Aof%20inertial%20signals%20in%20order%20to%20robustify%20the%20estimation.%20More%20specifically%2C%0Athe%20added%20signals%20provide%20strong%20cues%20for%20pose%20initialization%20as%20well%20as%0Aregularization%20during%20windowed%2C%20multi-frame%20tracking.%20As%20a%20result%2C%20the%20proposed%0Aframework%20achieves%20increased%20performance%20under%20challenging%20illumination%0Aconditions%20as%20well%20as%20a%20reduction%20of%20the%20rate%20at%20which%20intermediate%20event%0Arepresentations%20need%20to%20be%20registered%20in%20order%20to%20maintain%20stable%20tracking%0Aacross%20highly%20dynamic%20sequences.%20Our%20evaluation%20focuses%20on%20a%20diverse%20set%20of%0Areal%20world%20sequences%20and%20comprises%20a%20comparison%20of%20our%20proposed%20method%20against%0Aa%20purely%20event-based%20alternative%20running%20at%20different%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01370v1&entry.124074799=Read"},
{"title": "Feature Clock: High-Dimensional Effects in Two-Dimensional Plots", "author": "Olga Ovcharenko and Rita Sevastjanova and Valentina Boeva", "abstract": "  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n", "link": "http://arxiv.org/abs/2408.01294v1", "date": "2024-08-02", "relevancy": 2.239, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots&body=Title%3A%20Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots%0AAuthor%3A%20Olga%20Ovcharenko%20and%20Rita%20Sevastjanova%20and%20Valentina%20Boeva%0AAbstract%3A%20%20%20Humans%20struggle%20to%20perceive%20and%20interpret%20high-dimensional%20data.%20Therefore%2C%0Ahigh-dimensional%20data%20are%20often%20projected%20into%20two%20dimensions%20for%0Avisualization.%20Many%20applications%20benefit%20from%20complex%20nonlinear%20dimensionality%0Areduction%20techniques%2C%20but%20the%20effects%20of%20individual%20high-dimensional%20features%0Aare%20hard%20to%20explain%20in%20the%20two-dimensional%20space.%20Most%20visualization%20solutions%0Ause%20multiple%20two-dimensional%20plots%2C%20each%20showing%20the%20effect%20of%20one%0Ahigh-dimensional%20feature%20in%20two%20dimensions%3B%20this%20approach%20creates%20a%20need%20for%20a%0Avisual%20inspection%20of%20k%20plots%20for%20a%20k-dimensional%20input%20space.%20Our%20solution%2C%0AFeature%20Clock%2C%20provides%20a%20novel%20approach%20that%20eliminates%20the%20need%20to%20inspect%0Athese%20k%20plots%20to%20grasp%20the%20influence%20of%20original%20features%20on%20the%20data%20structure%0Adepicted%20in%20two%20dimensions.%20Feature%20Clock%20enhances%20the%20explainability%20and%0Acompactness%20of%20visualizations%20of%20embedded%20data%20and%20is%20available%20in%20an%0Aopen-source%20Python%20library.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Clock%253A%2520High-Dimensional%2520Effects%2520in%2520Two-Dimensional%2520Plots%26entry.906535625%3DOlga%2520Ovcharenko%2520and%2520Rita%2520Sevastjanova%2520and%2520Valentina%2520Boeva%26entry.1292438233%3D%2520%2520Humans%2520struggle%2520to%2520perceive%2520and%2520interpret%2520high-dimensional%2520data.%2520Therefore%252C%250Ahigh-dimensional%2520data%2520are%2520often%2520projected%2520into%2520two%2520dimensions%2520for%250Avisualization.%2520Many%2520applications%2520benefit%2520from%2520complex%2520nonlinear%2520dimensionality%250Areduction%2520techniques%252C%2520but%2520the%2520effects%2520of%2520individual%2520high-dimensional%2520features%250Aare%2520hard%2520to%2520explain%2520in%2520the%2520two-dimensional%2520space.%2520Most%2520visualization%2520solutions%250Ause%2520multiple%2520two-dimensional%2520plots%252C%2520each%2520showing%2520the%2520effect%2520of%2520one%250Ahigh-dimensional%2520feature%2520in%2520two%2520dimensions%253B%2520this%2520approach%2520creates%2520a%2520need%2520for%2520a%250Avisual%2520inspection%2520of%2520k%2520plots%2520for%2520a%2520k-dimensional%2520input%2520space.%2520Our%2520solution%252C%250AFeature%2520Clock%252C%2520provides%2520a%2520novel%2520approach%2520that%2520eliminates%2520the%2520need%2520to%2520inspect%250Athese%2520k%2520plots%2520to%2520grasp%2520the%2520influence%2520of%2520original%2520features%2520on%2520the%2520data%2520structure%250Adepicted%2520in%2520two%2520dimensions.%2520Feature%2520Clock%2520enhances%2520the%2520explainability%2520and%250Acompactness%2520of%2520visualizations%2520of%2520embedded%2520data%2520and%2520is%2520available%2520in%2520an%250Aopen-source%2520Python%2520library.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Clock%3A%20High-Dimensional%20Effects%20in%20Two-Dimensional%20Plots&entry.906535625=Olga%20Ovcharenko%20and%20Rita%20Sevastjanova%20and%20Valentina%20Boeva&entry.1292438233=%20%20Humans%20struggle%20to%20perceive%20and%20interpret%20high-dimensional%20data.%20Therefore%2C%0Ahigh-dimensional%20data%20are%20often%20projected%20into%20two%20dimensions%20for%0Avisualization.%20Many%20applications%20benefit%20from%20complex%20nonlinear%20dimensionality%0Areduction%20techniques%2C%20but%20the%20effects%20of%20individual%20high-dimensional%20features%0Aare%20hard%20to%20explain%20in%20the%20two-dimensional%20space.%20Most%20visualization%20solutions%0Ause%20multiple%20two-dimensional%20plots%2C%20each%20showing%20the%20effect%20of%20one%0Ahigh-dimensional%20feature%20in%20two%20dimensions%3B%20this%20approach%20creates%20a%20need%20for%20a%0Avisual%20inspection%20of%20k%20plots%20for%20a%20k-dimensional%20input%20space.%20Our%20solution%2C%0AFeature%20Clock%2C%20provides%20a%20novel%20approach%20that%20eliminates%20the%20need%20to%20inspect%0Athese%20k%20plots%20to%20grasp%20the%20influence%20of%20original%20features%20on%20the%20data%20structure%0Adepicted%20in%20two%20dimensions.%20Feature%20Clock%20enhances%20the%20explainability%20and%0Acompactness%20of%20visualizations%20of%20embedded%20data%20and%20is%20available%20in%20an%0Aopen-source%20Python%20library.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01294v1&entry.124074799=Read"},
{"title": "Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services", "author": "Ali Doosthosseini and Jonathan Decker and Hendrik Nolte and Julian M. Kunkel", "abstract": "  The widespread adoption of large language models (LLMs) has created a\npressing need for an efficient, secure and private serving infrastructure,\nwhich allows researchers to run open source or custom fine-tuned LLMs and\nensures users that their data remains private and is not stored without their\nconsent. While high-performance computing (HPC) systems equipped with\nstate-of-the-art GPUs are well-suited for training LLMs, their batch scheduling\nparadigm is not designed to support real-time serving of AI applications. Cloud\nsystems, on the other hand, are well suited for web services but commonly lack\naccess to the computational power of HPC clusters, especially expensive and\nscarce high-end GPUs, which are required for optimal inference speed. We\npropose an architecture with an implementation consisting of a web service that\nruns on a cloud VM with secure access to a scalable backend running a multitude\nof LLM models on HPC systems. By offering a web service using our HPC\ninfrastructure to host LLMs, we leverage the trusted environment of local\nuniversities and research centers to offer a private and secure alternative to\ncommercial LLM services. Our solution natively integrates with the HPC batch\nscheduler Slurm, enabling seamless deployment on HPC clusters, and is able to\nrun side by side with regular Slurm workloads, while utilizing gaps in the\nschedule created by Slurm. In order to ensure the security of the HPC system,\nwe use the SSH ForceCommand directive to construct a robust circuit breaker,\nwhich prevents successful attacks on the web-facing server from affecting the\ncluster. We have successfully deployed our system as a production service, and\nmade the source code available at \\url{https://github.com/gwdg/chat-ai}\n", "link": "http://arxiv.org/abs/2407.00110v2", "date": "2024-08-02", "relevancy": 2.2194, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4718}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4335}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chat%20AI%3A%20A%20Seamless%20Slurm-Native%20Solution%20for%20HPC-Based%20Services&body=Title%3A%20Chat%20AI%3A%20A%20Seamless%20Slurm-Native%20Solution%20for%20HPC-Based%20Services%0AAuthor%3A%20Ali%20Doosthosseini%20and%20Jonathan%20Decker%20and%20Hendrik%20Nolte%20and%20Julian%20M.%20Kunkel%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20has%20created%20a%0Apressing%20need%20for%20an%20efficient%2C%20secure%20and%20private%20serving%20infrastructure%2C%0Awhich%20allows%20researchers%20to%20run%20open%20source%20or%20custom%20fine-tuned%20LLMs%20and%0Aensures%20users%20that%20their%20data%20remains%20private%20and%20is%20not%20stored%20without%20their%0Aconsent.%20While%20high-performance%20computing%20%28HPC%29%20systems%20equipped%20with%0Astate-of-the-art%20GPUs%20are%20well-suited%20for%20training%20LLMs%2C%20their%20batch%20scheduling%0Aparadigm%20is%20not%20designed%20to%20support%20real-time%20serving%20of%20AI%20applications.%20Cloud%0Asystems%2C%20on%20the%20other%20hand%2C%20are%20well%20suited%20for%20web%20services%20but%20commonly%20lack%0Aaccess%20to%20the%20computational%20power%20of%20HPC%20clusters%2C%20especially%20expensive%20and%0Ascarce%20high-end%20GPUs%2C%20which%20are%20required%20for%20optimal%20inference%20speed.%20We%0Apropose%20an%20architecture%20with%20an%20implementation%20consisting%20of%20a%20web%20service%20that%0Aruns%20on%20a%20cloud%20VM%20with%20secure%20access%20to%20a%20scalable%20backend%20running%20a%20multitude%0Aof%20LLM%20models%20on%20HPC%20systems.%20By%20offering%20a%20web%20service%20using%20our%20HPC%0Ainfrastructure%20to%20host%20LLMs%2C%20we%20leverage%20the%20trusted%20environment%20of%20local%0Auniversities%20and%20research%20centers%20to%20offer%20a%20private%20and%20secure%20alternative%20to%0Acommercial%20LLM%20services.%20Our%20solution%20natively%20integrates%20with%20the%20HPC%20batch%0Ascheduler%20Slurm%2C%20enabling%20seamless%20deployment%20on%20HPC%20clusters%2C%20and%20is%20able%20to%0Arun%20side%20by%20side%20with%20regular%20Slurm%20workloads%2C%20while%20utilizing%20gaps%20in%20the%0Aschedule%20created%20by%20Slurm.%20In%20order%20to%20ensure%20the%20security%20of%20the%20HPC%20system%2C%0Awe%20use%20the%20SSH%20ForceCommand%20directive%20to%20construct%20a%20robust%20circuit%20breaker%2C%0Awhich%20prevents%20successful%20attacks%20on%20the%20web-facing%20server%20from%20affecting%20the%0Acluster.%20We%20have%20successfully%20deployed%20our%20system%20as%20a%20production%20service%2C%20and%0Amade%20the%20source%20code%20available%20at%20%5Curl%7Bhttps%3A//github.com/gwdg/chat-ai%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChat%2520AI%253A%2520A%2520Seamless%2520Slurm-Native%2520Solution%2520for%2520HPC-Based%2520Services%26entry.906535625%3DAli%2520Doosthosseini%2520and%2520Jonathan%2520Decker%2520and%2520Hendrik%2520Nolte%2520and%2520Julian%2520M.%2520Kunkel%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520created%2520a%250Apressing%2520need%2520for%2520an%2520efficient%252C%2520secure%2520and%2520private%2520serving%2520infrastructure%252C%250Awhich%2520allows%2520researchers%2520to%2520run%2520open%2520source%2520or%2520custom%2520fine-tuned%2520LLMs%2520and%250Aensures%2520users%2520that%2520their%2520data%2520remains%2520private%2520and%2520is%2520not%2520stored%2520without%2520their%250Aconsent.%2520While%2520high-performance%2520computing%2520%2528HPC%2529%2520systems%2520equipped%2520with%250Astate-of-the-art%2520GPUs%2520are%2520well-suited%2520for%2520training%2520LLMs%252C%2520their%2520batch%2520scheduling%250Aparadigm%2520is%2520not%2520designed%2520to%2520support%2520real-time%2520serving%2520of%2520AI%2520applications.%2520Cloud%250Asystems%252C%2520on%2520the%2520other%2520hand%252C%2520are%2520well%2520suited%2520for%2520web%2520services%2520but%2520commonly%2520lack%250Aaccess%2520to%2520the%2520computational%2520power%2520of%2520HPC%2520clusters%252C%2520especially%2520expensive%2520and%250Ascarce%2520high-end%2520GPUs%252C%2520which%2520are%2520required%2520for%2520optimal%2520inference%2520speed.%2520We%250Apropose%2520an%2520architecture%2520with%2520an%2520implementation%2520consisting%2520of%2520a%2520web%2520service%2520that%250Aruns%2520on%2520a%2520cloud%2520VM%2520with%2520secure%2520access%2520to%2520a%2520scalable%2520backend%2520running%2520a%2520multitude%250Aof%2520LLM%2520models%2520on%2520HPC%2520systems.%2520By%2520offering%2520a%2520web%2520service%2520using%2520our%2520HPC%250Ainfrastructure%2520to%2520host%2520LLMs%252C%2520we%2520leverage%2520the%2520trusted%2520environment%2520of%2520local%250Auniversities%2520and%2520research%2520centers%2520to%2520offer%2520a%2520private%2520and%2520secure%2520alternative%2520to%250Acommercial%2520LLM%2520services.%2520Our%2520solution%2520natively%2520integrates%2520with%2520the%2520HPC%2520batch%250Ascheduler%2520Slurm%252C%2520enabling%2520seamless%2520deployment%2520on%2520HPC%2520clusters%252C%2520and%2520is%2520able%2520to%250Arun%2520side%2520by%2520side%2520with%2520regular%2520Slurm%2520workloads%252C%2520while%2520utilizing%2520gaps%2520in%2520the%250Aschedule%2520created%2520by%2520Slurm.%2520In%2520order%2520to%2520ensure%2520the%2520security%2520of%2520the%2520HPC%2520system%252C%250Awe%2520use%2520the%2520SSH%2520ForceCommand%2520directive%2520to%2520construct%2520a%2520robust%2520circuit%2520breaker%252C%250Awhich%2520prevents%2520successful%2520attacks%2520on%2520the%2520web-facing%2520server%2520from%2520affecting%2520the%250Acluster.%2520We%2520have%2520successfully%2520deployed%2520our%2520system%2520as%2520a%2520production%2520service%252C%2520and%250Amade%2520the%2520source%2520code%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/gwdg/chat-ai%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chat%20AI%3A%20A%20Seamless%20Slurm-Native%20Solution%20for%20HPC-Based%20Services&entry.906535625=Ali%20Doosthosseini%20and%20Jonathan%20Decker%20and%20Hendrik%20Nolte%20and%20Julian%20M.%20Kunkel&entry.1292438233=%20%20The%20widespread%20adoption%20of%20large%20language%20models%20%28LLMs%29%20has%20created%20a%0Apressing%20need%20for%20an%20efficient%2C%20secure%20and%20private%20serving%20infrastructure%2C%0Awhich%20allows%20researchers%20to%20run%20open%20source%20or%20custom%20fine-tuned%20LLMs%20and%0Aensures%20users%20that%20their%20data%20remains%20private%20and%20is%20not%20stored%20without%20their%0Aconsent.%20While%20high-performance%20computing%20%28HPC%29%20systems%20equipped%20with%0Astate-of-the-art%20GPUs%20are%20well-suited%20for%20training%20LLMs%2C%20their%20batch%20scheduling%0Aparadigm%20is%20not%20designed%20to%20support%20real-time%20serving%20of%20AI%20applications.%20Cloud%0Asystems%2C%20on%20the%20other%20hand%2C%20are%20well%20suited%20for%20web%20services%20but%20commonly%20lack%0Aaccess%20to%20the%20computational%20power%20of%20HPC%20clusters%2C%20especially%20expensive%20and%0Ascarce%20high-end%20GPUs%2C%20which%20are%20required%20for%20optimal%20inference%20speed.%20We%0Apropose%20an%20architecture%20with%20an%20implementation%20consisting%20of%20a%20web%20service%20that%0Aruns%20on%20a%20cloud%20VM%20with%20secure%20access%20to%20a%20scalable%20backend%20running%20a%20multitude%0Aof%20LLM%20models%20on%20HPC%20systems.%20By%20offering%20a%20web%20service%20using%20our%20HPC%0Ainfrastructure%20to%20host%20LLMs%2C%20we%20leverage%20the%20trusted%20environment%20of%20local%0Auniversities%20and%20research%20centers%20to%20offer%20a%20private%20and%20secure%20alternative%20to%0Acommercial%20LLM%20services.%20Our%20solution%20natively%20integrates%20with%20the%20HPC%20batch%0Ascheduler%20Slurm%2C%20enabling%20seamless%20deployment%20on%20HPC%20clusters%2C%20and%20is%20able%20to%0Arun%20side%20by%20side%20with%20regular%20Slurm%20workloads%2C%20while%20utilizing%20gaps%20in%20the%0Aschedule%20created%20by%20Slurm.%20In%20order%20to%20ensure%20the%20security%20of%20the%20HPC%20system%2C%0Awe%20use%20the%20SSH%20ForceCommand%20directive%20to%20construct%20a%20robust%20circuit%20breaker%2C%0Awhich%20prevents%20successful%20attacks%20on%20the%20web-facing%20server%20from%20affecting%20the%0Acluster.%20We%20have%20successfully%20deployed%20our%20system%20as%20a%20production%20service%2C%20and%0Amade%20the%20source%20code%20available%20at%20%5Curl%7Bhttps%3A//github.com/gwdg/chat-ai%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00110v2&entry.124074799=Read"},
{"title": "MAC: Graph Sparsification by Maximizing Algebraic Connectivity", "author": "Kevin Doherty and Alan Papalia and Yewei Huang and David Rosen and Brendan Englot and John Leonard", "abstract": "  Simultaneous localization and mapping (SLAM) is a critical capability in\nautonomous navigation, but memory and computational limits make long-term\napplication of common SLAM techniques impractical; a robot must be able to\ndetermine what information should be retained and what can safely be forgotten.\nIn graph-based SLAM, the number of edges (measurements) in a pose graph\ndetermines both the memory requirements of storing a robot's observations and\nthe computational expense of algorithms deployed for performing state\nestimation using those observations, both of which can grow unbounded during\nlong-term navigation. Motivated by these challenges, we propose a new general\npurpose approach to sparsify graphs in a manner that maximizes algebraic\nconnectivity, a key spectral property of graphs which has been shown to control\nthe estimation error of pose graph SLAM solutions. Our algorithm, MAC (for\nmaximizing algebraic connectivity), is simple and computationally inexpensive,\nand admits formal post hoc performance guarantees on the quality of the\nsolution that it provides. In application to the problem of pose-graph SLAM, we\nshow on several benchmark datasets that our approach quickly produces\nhigh-quality sparsification results which retain the connectivity of the graph\nand, in turn, the quality of corresponding SLAM solutions.\n", "link": "http://arxiv.org/abs/2403.19879v2", "date": "2024-08-02", "relevancy": 2.2107, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5721}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAC%3A%20Graph%20Sparsification%20by%20Maximizing%20Algebraic%20Connectivity&body=Title%3A%20MAC%3A%20Graph%20Sparsification%20by%20Maximizing%20Algebraic%20Connectivity%0AAuthor%3A%20Kevin%20Doherty%20and%20Alan%20Papalia%20and%20Yewei%20Huang%20and%20David%20Rosen%20and%20Brendan%20Englot%20and%20John%20Leonard%0AAbstract%3A%20%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20in%0Aautonomous%20navigation%2C%20but%20memory%20and%20computational%20limits%20make%20long-term%0Aapplication%20of%20common%20SLAM%20techniques%20impractical%3B%20a%20robot%20must%20be%20able%20to%0Adetermine%20what%20information%20should%20be%20retained%20and%20what%20can%20safely%20be%20forgotten.%0AIn%20graph-based%20SLAM%2C%20the%20number%20of%20edges%20%28measurements%29%20in%20a%20pose%20graph%0Adetermines%20both%20the%20memory%20requirements%20of%20storing%20a%20robot%27s%20observations%20and%0Athe%20computational%20expense%20of%20algorithms%20deployed%20for%20performing%20state%0Aestimation%20using%20those%20observations%2C%20both%20of%20which%20can%20grow%20unbounded%20during%0Along-term%20navigation.%20Motivated%20by%20these%20challenges%2C%20we%20propose%20a%20new%20general%0Apurpose%20approach%20to%20sparsify%20graphs%20in%20a%20manner%20that%20maximizes%20algebraic%0Aconnectivity%2C%20a%20key%20spectral%20property%20of%20graphs%20which%20has%20been%20shown%20to%20control%0Athe%20estimation%20error%20of%20pose%20graph%20SLAM%20solutions.%20Our%20algorithm%2C%20MAC%20%28for%0Amaximizing%20algebraic%20connectivity%29%2C%20is%20simple%20and%20computationally%20inexpensive%2C%0Aand%20admits%20formal%20post%20hoc%20performance%20guarantees%20on%20the%20quality%20of%20the%0Asolution%20that%20it%20provides.%20In%20application%20to%20the%20problem%20of%20pose-graph%20SLAM%2C%20we%0Ashow%20on%20several%20benchmark%20datasets%20that%20our%20approach%20quickly%20produces%0Ahigh-quality%20sparsification%20results%20which%20retain%20the%20connectivity%20of%20the%20graph%0Aand%2C%20in%20turn%2C%20the%20quality%20of%20corresponding%20SLAM%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAC%253A%2520Graph%2520Sparsification%2520by%2520Maximizing%2520Algebraic%2520Connectivity%26entry.906535625%3DKevin%2520Doherty%2520and%2520Alan%2520Papalia%2520and%2520Yewei%2520Huang%2520and%2520David%2520Rosen%2520and%2520Brendan%2520Englot%2520and%2520John%2520Leonard%26entry.1292438233%3D%2520%2520Simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520is%2520a%2520critical%2520capability%2520in%250Aautonomous%2520navigation%252C%2520but%2520memory%2520and%2520computational%2520limits%2520make%2520long-term%250Aapplication%2520of%2520common%2520SLAM%2520techniques%2520impractical%253B%2520a%2520robot%2520must%2520be%2520able%2520to%250Adetermine%2520what%2520information%2520should%2520be%2520retained%2520and%2520what%2520can%2520safely%2520be%2520forgotten.%250AIn%2520graph-based%2520SLAM%252C%2520the%2520number%2520of%2520edges%2520%2528measurements%2529%2520in%2520a%2520pose%2520graph%250Adetermines%2520both%2520the%2520memory%2520requirements%2520of%2520storing%2520a%2520robot%2527s%2520observations%2520and%250Athe%2520computational%2520expense%2520of%2520algorithms%2520deployed%2520for%2520performing%2520state%250Aestimation%2520using%2520those%2520observations%252C%2520both%2520of%2520which%2520can%2520grow%2520unbounded%2520during%250Along-term%2520navigation.%2520Motivated%2520by%2520these%2520challenges%252C%2520we%2520propose%2520a%2520new%2520general%250Apurpose%2520approach%2520to%2520sparsify%2520graphs%2520in%2520a%2520manner%2520that%2520maximizes%2520algebraic%250Aconnectivity%252C%2520a%2520key%2520spectral%2520property%2520of%2520graphs%2520which%2520has%2520been%2520shown%2520to%2520control%250Athe%2520estimation%2520error%2520of%2520pose%2520graph%2520SLAM%2520solutions.%2520Our%2520algorithm%252C%2520MAC%2520%2528for%250Amaximizing%2520algebraic%2520connectivity%2529%252C%2520is%2520simple%2520and%2520computationally%2520inexpensive%252C%250Aand%2520admits%2520formal%2520post%2520hoc%2520performance%2520guarantees%2520on%2520the%2520quality%2520of%2520the%250Asolution%2520that%2520it%2520provides.%2520In%2520application%2520to%2520the%2520problem%2520of%2520pose-graph%2520SLAM%252C%2520we%250Ashow%2520on%2520several%2520benchmark%2520datasets%2520that%2520our%2520approach%2520quickly%2520produces%250Ahigh-quality%2520sparsification%2520results%2520which%2520retain%2520the%2520connectivity%2520of%2520the%2520graph%250Aand%252C%2520in%2520turn%252C%2520the%2520quality%2520of%2520corresponding%2520SLAM%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAC%3A%20Graph%20Sparsification%20by%20Maximizing%20Algebraic%20Connectivity&entry.906535625=Kevin%20Doherty%20and%20Alan%20Papalia%20and%20Yewei%20Huang%20and%20David%20Rosen%20and%20Brendan%20Englot%20and%20John%20Leonard&entry.1292438233=%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20in%0Aautonomous%20navigation%2C%20but%20memory%20and%20computational%20limits%20make%20long-term%0Aapplication%20of%20common%20SLAM%20techniques%20impractical%3B%20a%20robot%20must%20be%20able%20to%0Adetermine%20what%20information%20should%20be%20retained%20and%20what%20can%20safely%20be%20forgotten.%0AIn%20graph-based%20SLAM%2C%20the%20number%20of%20edges%20%28measurements%29%20in%20a%20pose%20graph%0Adetermines%20both%20the%20memory%20requirements%20of%20storing%20a%20robot%27s%20observations%20and%0Athe%20computational%20expense%20of%20algorithms%20deployed%20for%20performing%20state%0Aestimation%20using%20those%20observations%2C%20both%20of%20which%20can%20grow%20unbounded%20during%0Along-term%20navigation.%20Motivated%20by%20these%20challenges%2C%20we%20propose%20a%20new%20general%0Apurpose%20approach%20to%20sparsify%20graphs%20in%20a%20manner%20that%20maximizes%20algebraic%0Aconnectivity%2C%20a%20key%20spectral%20property%20of%20graphs%20which%20has%20been%20shown%20to%20control%0Athe%20estimation%20error%20of%20pose%20graph%20SLAM%20solutions.%20Our%20algorithm%2C%20MAC%20%28for%0Amaximizing%20algebraic%20connectivity%29%2C%20is%20simple%20and%20computationally%20inexpensive%2C%0Aand%20admits%20formal%20post%20hoc%20performance%20guarantees%20on%20the%20quality%20of%20the%0Asolution%20that%20it%20provides.%20In%20application%20to%20the%20problem%20of%20pose-graph%20SLAM%2C%20we%0Ashow%20on%20several%20benchmark%20datasets%20that%20our%20approach%20quickly%20produces%0Ahigh-quality%20sparsification%20results%20which%20retain%20the%20connectivity%20of%20the%20graph%0Aand%2C%20in%20turn%2C%20the%20quality%20of%20corresponding%20SLAM%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19879v2&entry.124074799=Read"},
{"title": "Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving", "author": "Xi Chen and Rahul Bhadani and Larry Head", "abstract": "  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n", "link": "http://arxiv.org/abs/2408.00374v2", "date": "2024-08-02", "relevancy": 2.1726, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6012}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Trajectory%20Prediction%20with%20Multi-View%20Data%20Integration%20in%0A%20%20Cooperative%20Driving&body=Title%3A%20Conformal%20Trajectory%20Prediction%20with%20Multi-View%20Data%20Integration%20in%0A%20%20Cooperative%20Driving%0AAuthor%3A%20Xi%20Chen%20and%20Rahul%20Bhadani%20and%20Larry%20Head%0AAbstract%3A%20%20%20Current%20research%20on%20trajectory%20prediction%20primarily%20relies%20on%20data%20collected%0Aby%20onboard%20sensors%20of%20an%20ego%20vehicle.%20With%20the%20rapid%20advancement%20in%20connected%0Atechnologies%2C%20such%20as%20vehicle-to-vehicle%20%28V2V%29%20and%20vehicle-to-infrastructure%0A%28V2I%29%20communication%2C%20valuable%20information%20from%20alternate%20views%20becomes%0Aaccessible%20via%20wireless%20networks.%20The%20integration%20of%20information%20from%0Aalternative%20views%20has%20the%20potential%20to%20overcome%20the%20inherent%20limitations%0Aassociated%20with%20a%20single%20viewpoint%2C%20such%20as%20occlusions%20and%20limited%20field%20of%0Aview.%20In%20this%20work%2C%20we%20introduce%20V2INet%2C%20a%20novel%20trajectory%20prediction%0Aframework%20designed%20to%20model%20multi-view%20data%20by%20extending%20existing%20single-view%0Amodels.%20Unlike%20previous%20approaches%20where%20the%20multi-view%20data%20is%20manually%20fused%0Aor%20formulated%20as%20a%20separate%20training%20stage%2C%20our%20model%20supports%20end-to-end%0Atraining%2C%20enhancing%20both%20flexibility%20and%20performance.%20Moreover%2C%20the%20predicted%0Amultimodal%20trajectories%20are%20calibrated%20by%20a%20post-hoc%20conformal%20prediction%0Amodule%20to%20get%20valid%20and%20efficient%20confidence%20regions.%20We%20evaluated%20the%20entire%0Aframework%20using%20the%20real-world%20V2I%20dataset%20V2X-Seq.%20Our%20results%20demonstrate%0Asuperior%20performance%20in%20terms%20of%20Final%20Displacement%20Error%20%28FDE%29%20and%20Miss%20Rate%0A%28MR%29%20using%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/xichennn/V2I_trajectory_prediction%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Trajectory%2520Prediction%2520with%2520Multi-View%2520Data%2520Integration%2520in%250A%2520%2520Cooperative%2520Driving%26entry.906535625%3DXi%2520Chen%2520and%2520Rahul%2520Bhadani%2520and%2520Larry%2520Head%26entry.1292438233%3D%2520%2520Current%2520research%2520on%2520trajectory%2520prediction%2520primarily%2520relies%2520on%2520data%2520collected%250Aby%2520onboard%2520sensors%2520of%2520an%2520ego%2520vehicle.%2520With%2520the%2520rapid%2520advancement%2520in%2520connected%250Atechnologies%252C%2520such%2520as%2520vehicle-to-vehicle%2520%2528V2V%2529%2520and%2520vehicle-to-infrastructure%250A%2528V2I%2529%2520communication%252C%2520valuable%2520information%2520from%2520alternate%2520views%2520becomes%250Aaccessible%2520via%2520wireless%2520networks.%2520The%2520integration%2520of%2520information%2520from%250Aalternative%2520views%2520has%2520the%2520potential%2520to%2520overcome%2520the%2520inherent%2520limitations%250Aassociated%2520with%2520a%2520single%2520viewpoint%252C%2520such%2520as%2520occlusions%2520and%2520limited%2520field%2520of%250Aview.%2520In%2520this%2520work%252C%2520we%2520introduce%2520V2INet%252C%2520a%2520novel%2520trajectory%2520prediction%250Aframework%2520designed%2520to%2520model%2520multi-view%2520data%2520by%2520extending%2520existing%2520single-view%250Amodels.%2520Unlike%2520previous%2520approaches%2520where%2520the%2520multi-view%2520data%2520is%2520manually%2520fused%250Aor%2520formulated%2520as%2520a%2520separate%2520training%2520stage%252C%2520our%2520model%2520supports%2520end-to-end%250Atraining%252C%2520enhancing%2520both%2520flexibility%2520and%2520performance.%2520Moreover%252C%2520the%2520predicted%250Amultimodal%2520trajectories%2520are%2520calibrated%2520by%2520a%2520post-hoc%2520conformal%2520prediction%250Amodule%2520to%2520get%2520valid%2520and%2520efficient%2520confidence%2520regions.%2520We%2520evaluated%2520the%2520entire%250Aframework%2520using%2520the%2520real-world%2520V2I%2520dataset%2520V2X-Seq.%2520Our%2520results%2520demonstrate%250Asuperior%2520performance%2520in%2520terms%2520of%2520Final%2520Displacement%2520Error%2520%2528FDE%2529%2520and%2520Miss%2520Rate%250A%2528MR%2529%2520using%2520a%2520single%2520GPU.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/xichennn/V2I_trajectory_prediction%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Trajectory%20Prediction%20with%20Multi-View%20Data%20Integration%20in%0A%20%20Cooperative%20Driving&entry.906535625=Xi%20Chen%20and%20Rahul%20Bhadani%20and%20Larry%20Head&entry.1292438233=%20%20Current%20research%20on%20trajectory%20prediction%20primarily%20relies%20on%20data%20collected%0Aby%20onboard%20sensors%20of%20an%20ego%20vehicle.%20With%20the%20rapid%20advancement%20in%20connected%0Atechnologies%2C%20such%20as%20vehicle-to-vehicle%20%28V2V%29%20and%20vehicle-to-infrastructure%0A%28V2I%29%20communication%2C%20valuable%20information%20from%20alternate%20views%20becomes%0Aaccessible%20via%20wireless%20networks.%20The%20integration%20of%20information%20from%0Aalternative%20views%20has%20the%20potential%20to%20overcome%20the%20inherent%20limitations%0Aassociated%20with%20a%20single%20viewpoint%2C%20such%20as%20occlusions%20and%20limited%20field%20of%0Aview.%20In%20this%20work%2C%20we%20introduce%20V2INet%2C%20a%20novel%20trajectory%20prediction%0Aframework%20designed%20to%20model%20multi-view%20data%20by%20extending%20existing%20single-view%0Amodels.%20Unlike%20previous%20approaches%20where%20the%20multi-view%20data%20is%20manually%20fused%0Aor%20formulated%20as%20a%20separate%20training%20stage%2C%20our%20model%20supports%20end-to-end%0Atraining%2C%20enhancing%20both%20flexibility%20and%20performance.%20Moreover%2C%20the%20predicted%0Amultimodal%20trajectories%20are%20calibrated%20by%20a%20post-hoc%20conformal%20prediction%0Amodule%20to%20get%20valid%20and%20efficient%20confidence%20regions.%20We%20evaluated%20the%20entire%0Aframework%20using%20the%20real-world%20V2I%20dataset%20V2X-Seq.%20Our%20results%20demonstrate%0Asuperior%20performance%20in%20terms%20of%20Final%20Displacement%20Error%20%28FDE%29%20and%20Miss%20Rate%0A%28MR%29%20using%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/xichennn/V2I_trajectory_prediction%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00374v2&entry.124074799=Read"},
{"title": "Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G\n  Subnetworks", "author": "Daniel Abode and Ramoni Adeogun and Lou Sala\u00fcn and Renato Abreu and Thomas Jacobsen and Gilberto Berardinelli", "abstract": "  In this paper, we present an unsupervised approach for frequency sub-band\nallocation in wireless networks using graph-based learning. We consider a dense\ndeployment of subnetworks in the factory environment with a limited number of\nsub-bands which must be optimally allocated to coordinate inter-subnetwork\ninterference. We model the subnetwork deployment as a conflict graph and\npropose an unsupervised learning approach inspired by the graph colouring\nheuristic and the Potts model to optimize the sub-band allocation using graph\nneural networks. The numerical evaluation shows that the proposed method\nachieves close performance to the centralized greedy colouring sub-band\nallocation heuristic with lower computational time complexity. In addition, it\nincurs reduced signalling overhead compared to iterative optimization\nheuristics that require all the mutual interfering channel information. We\nfurther demonstrate that the method is robust to different network settings.\n", "link": "http://arxiv.org/abs/2401.00950v2", "date": "2024-08-02", "relevancy": 2.1404, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4009}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Graph-based%20Learning%20Method%20for%20Sub-band%20Allocation%20in%206G%0A%20%20Subnetworks&body=Title%3A%20Unsupervised%20Graph-based%20Learning%20Method%20for%20Sub-band%20Allocation%20in%206G%0A%20%20Subnetworks%0AAuthor%3A%20Daniel%20Abode%20and%20Ramoni%20Adeogun%20and%20Lou%20Sala%C3%BCn%20and%20Renato%20Abreu%20and%20Thomas%20Jacobsen%20and%20Gilberto%20Berardinelli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20unsupervised%20approach%20for%20frequency%20sub-band%0Aallocation%20in%20wireless%20networks%20using%20graph-based%20learning.%20We%20consider%20a%20dense%0Adeployment%20of%20subnetworks%20in%20the%20factory%20environment%20with%20a%20limited%20number%20of%0Asub-bands%20which%20must%20be%20optimally%20allocated%20to%20coordinate%20inter-subnetwork%0Ainterference.%20We%20model%20the%20subnetwork%20deployment%20as%20a%20conflict%20graph%20and%0Apropose%20an%20unsupervised%20learning%20approach%20inspired%20by%20the%20graph%20colouring%0Aheuristic%20and%20the%20Potts%20model%20to%20optimize%20the%20sub-band%20allocation%20using%20graph%0Aneural%20networks.%20The%20numerical%20evaluation%20shows%20that%20the%20proposed%20method%0Aachieves%20close%20performance%20to%20the%20centralized%20greedy%20colouring%20sub-band%0Aallocation%20heuristic%20with%20lower%20computational%20time%20complexity.%20In%20addition%2C%20it%0Aincurs%20reduced%20signalling%20overhead%20compared%20to%20iterative%20optimization%0Aheuristics%20that%20require%20all%20the%20mutual%20interfering%20channel%20information.%20We%0Afurther%20demonstrate%20that%20the%20method%20is%20robust%20to%20different%20network%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Graph-based%2520Learning%2520Method%2520for%2520Sub-band%2520Allocation%2520in%25206G%250A%2520%2520Subnetworks%26entry.906535625%3DDaniel%2520Abode%2520and%2520Ramoni%2520Adeogun%2520and%2520Lou%2520Sala%25C3%25BCn%2520and%2520Renato%2520Abreu%2520and%2520Thomas%2520Jacobsen%2520and%2520Gilberto%2520Berardinelli%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520unsupervised%2520approach%2520for%2520frequency%2520sub-band%250Aallocation%2520in%2520wireless%2520networks%2520using%2520graph-based%2520learning.%2520We%2520consider%2520a%2520dense%250Adeployment%2520of%2520subnetworks%2520in%2520the%2520factory%2520environment%2520with%2520a%2520limited%2520number%2520of%250Asub-bands%2520which%2520must%2520be%2520optimally%2520allocated%2520to%2520coordinate%2520inter-subnetwork%250Ainterference.%2520We%2520model%2520the%2520subnetwork%2520deployment%2520as%2520a%2520conflict%2520graph%2520and%250Apropose%2520an%2520unsupervised%2520learning%2520approach%2520inspired%2520by%2520the%2520graph%2520colouring%250Aheuristic%2520and%2520the%2520Potts%2520model%2520to%2520optimize%2520the%2520sub-band%2520allocation%2520using%2520graph%250Aneural%2520networks.%2520The%2520numerical%2520evaluation%2520shows%2520that%2520the%2520proposed%2520method%250Aachieves%2520close%2520performance%2520to%2520the%2520centralized%2520greedy%2520colouring%2520sub-band%250Aallocation%2520heuristic%2520with%2520lower%2520computational%2520time%2520complexity.%2520In%2520addition%252C%2520it%250Aincurs%2520reduced%2520signalling%2520overhead%2520compared%2520to%2520iterative%2520optimization%250Aheuristics%2520that%2520require%2520all%2520the%2520mutual%2520interfering%2520channel%2520information.%2520We%250Afurther%2520demonstrate%2520that%2520the%2520method%2520is%2520robust%2520to%2520different%2520network%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Graph-based%20Learning%20Method%20for%20Sub-band%20Allocation%20in%206G%0A%20%20Subnetworks&entry.906535625=Daniel%20Abode%20and%20Ramoni%20Adeogun%20and%20Lou%20Sala%C3%BCn%20and%20Renato%20Abreu%20and%20Thomas%20Jacobsen%20and%20Gilberto%20Berardinelli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20unsupervised%20approach%20for%20frequency%20sub-band%0Aallocation%20in%20wireless%20networks%20using%20graph-based%20learning.%20We%20consider%20a%20dense%0Adeployment%20of%20subnetworks%20in%20the%20factory%20environment%20with%20a%20limited%20number%20of%0Asub-bands%20which%20must%20be%20optimally%20allocated%20to%20coordinate%20inter-subnetwork%0Ainterference.%20We%20model%20the%20subnetwork%20deployment%20as%20a%20conflict%20graph%20and%0Apropose%20an%20unsupervised%20learning%20approach%20inspired%20by%20the%20graph%20colouring%0Aheuristic%20and%20the%20Potts%20model%20to%20optimize%20the%20sub-band%20allocation%20using%20graph%0Aneural%20networks.%20The%20numerical%20evaluation%20shows%20that%20the%20proposed%20method%0Aachieves%20close%20performance%20to%20the%20centralized%20greedy%20colouring%20sub-band%0Aallocation%20heuristic%20with%20lower%20computational%20time%20complexity.%20In%20addition%2C%20it%0Aincurs%20reduced%20signalling%20overhead%20compared%20to%20iterative%20optimization%0Aheuristics%20that%20require%20all%20the%20mutual%20interfering%20channel%20information.%20We%0Afurther%20demonstrate%20that%20the%20method%20is%20robust%20to%20different%20network%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00950v2&entry.124074799=Read"},
{"title": "MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous\n  Vehicle Environment with Multi-source Data Integration", "author": "Xi Chen and Rahul Bhadani and Zhanbo Sun and Larry Head", "abstract": "  The prediction of surrounding vehicle trajectories is crucial for\ncollision-free path planning. In this study, we focus on a scenario where a\nconnected and autonomous vehicle (CAV) serves as the central agent, utilizing\nboth sensors and communication technologies to perceive its surrounding\ntraffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and\nhuman-driven vehicles (HDVs). Our trajectory prediction task is aimed at all\nthe detected surrounding vehicles. To effectively integrate the multi-source\ndata from both sensor and communication technologies, we propose a deep\nlearning framework called MSMA utilizing a cross-attention module for\nmulti-source data fusion. Vector map data is utilized to provide contextual\ninformation. The trajectory dataset is collected in CARLA simulator with\nsynthesized data errors introduced. Numerical experiments demonstrate that in a\nmixed traffic flow scenario, the integration of data from different sources\nenhances our understanding of the environment. This notably improves trajectory\nprediction accuracy, particularly in situations with a high CV market\npenetration rate. The code is available at: https://github.com/xichennn/MSMA.\n", "link": "http://arxiv.org/abs/2407.21310v2", "date": "2024-08-02", "relevancy": 2.1328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6071}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSMA%3A%20Multi-agent%20Trajectory%20Prediction%20in%20Connected%20and%20Autonomous%0A%20%20Vehicle%20Environment%20with%20Multi-source%20Data%20Integration&body=Title%3A%20MSMA%3A%20Multi-agent%20Trajectory%20Prediction%20in%20Connected%20and%20Autonomous%0A%20%20Vehicle%20Environment%20with%20Multi-source%20Data%20Integration%0AAuthor%3A%20Xi%20Chen%20and%20Rahul%20Bhadani%20and%20Zhanbo%20Sun%20and%20Larry%20Head%0AAbstract%3A%20%20%20The%20prediction%20of%20surrounding%20vehicle%20trajectories%20is%20crucial%20for%0Acollision-free%20path%20planning.%20In%20this%20study%2C%20we%20focus%20on%20a%20scenario%20where%20a%0Aconnected%20and%20autonomous%20vehicle%20%28CAV%29%20serves%20as%20the%20central%20agent%2C%20utilizing%0Aboth%20sensors%20and%20communication%20technologies%20to%20perceive%20its%20surrounding%0Atraffics%20consisting%20of%20autonomous%20vehicles%20%28AVs%29%2C%20connected%20vehicles%20%28CVs%29%2C%20and%0Ahuman-driven%20vehicles%20%28HDVs%29.%20Our%20trajectory%20prediction%20task%20is%20aimed%20at%20all%0Athe%20detected%20surrounding%20vehicles.%20To%20effectively%20integrate%20the%20multi-source%0Adata%20from%20both%20sensor%20and%20communication%20technologies%2C%20we%20propose%20a%20deep%0Alearning%20framework%20called%20MSMA%20utilizing%20a%20cross-attention%20module%20for%0Amulti-source%20data%20fusion.%20Vector%20map%20data%20is%20utilized%20to%20provide%20contextual%0Ainformation.%20The%20trajectory%20dataset%20is%20collected%20in%20CARLA%20simulator%20with%0Asynthesized%20data%20errors%20introduced.%20Numerical%20experiments%20demonstrate%20that%20in%20a%0Amixed%20traffic%20flow%20scenario%2C%20the%20integration%20of%20data%20from%20different%20sources%0Aenhances%20our%20understanding%20of%20the%20environment.%20This%20notably%20improves%20trajectory%0Aprediction%20accuracy%2C%20particularly%20in%20situations%20with%20a%20high%20CV%20market%0Apenetration%20rate.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/xichennn/MSMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21310v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSMA%253A%2520Multi-agent%2520Trajectory%2520Prediction%2520in%2520Connected%2520and%2520Autonomous%250A%2520%2520Vehicle%2520Environment%2520with%2520Multi-source%2520Data%2520Integration%26entry.906535625%3DXi%2520Chen%2520and%2520Rahul%2520Bhadani%2520and%2520Zhanbo%2520Sun%2520and%2520Larry%2520Head%26entry.1292438233%3D%2520%2520The%2520prediction%2520of%2520surrounding%2520vehicle%2520trajectories%2520is%2520crucial%2520for%250Acollision-free%2520path%2520planning.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520a%2520scenario%2520where%2520a%250Aconnected%2520and%2520autonomous%2520vehicle%2520%2528CAV%2529%2520serves%2520as%2520the%2520central%2520agent%252C%2520utilizing%250Aboth%2520sensors%2520and%2520communication%2520technologies%2520to%2520perceive%2520its%2520surrounding%250Atraffics%2520consisting%2520of%2520autonomous%2520vehicles%2520%2528AVs%2529%252C%2520connected%2520vehicles%2520%2528CVs%2529%252C%2520and%250Ahuman-driven%2520vehicles%2520%2528HDVs%2529.%2520Our%2520trajectory%2520prediction%2520task%2520is%2520aimed%2520at%2520all%250Athe%2520detected%2520surrounding%2520vehicles.%2520To%2520effectively%2520integrate%2520the%2520multi-source%250Adata%2520from%2520both%2520sensor%2520and%2520communication%2520technologies%252C%2520we%2520propose%2520a%2520deep%250Alearning%2520framework%2520called%2520MSMA%2520utilizing%2520a%2520cross-attention%2520module%2520for%250Amulti-source%2520data%2520fusion.%2520Vector%2520map%2520data%2520is%2520utilized%2520to%2520provide%2520contextual%250Ainformation.%2520The%2520trajectory%2520dataset%2520is%2520collected%2520in%2520CARLA%2520simulator%2520with%250Asynthesized%2520data%2520errors%2520introduced.%2520Numerical%2520experiments%2520demonstrate%2520that%2520in%2520a%250Amixed%2520traffic%2520flow%2520scenario%252C%2520the%2520integration%2520of%2520data%2520from%2520different%2520sources%250Aenhances%2520our%2520understanding%2520of%2520the%2520environment.%2520This%2520notably%2520improves%2520trajectory%250Aprediction%2520accuracy%252C%2520particularly%2520in%2520situations%2520with%2520a%2520high%2520CV%2520market%250Apenetration%2520rate.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/xichennn/MSMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21310v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSMA%3A%20Multi-agent%20Trajectory%20Prediction%20in%20Connected%20and%20Autonomous%0A%20%20Vehicle%20Environment%20with%20Multi-source%20Data%20Integration&entry.906535625=Xi%20Chen%20and%20Rahul%20Bhadani%20and%20Zhanbo%20Sun%20and%20Larry%20Head&entry.1292438233=%20%20The%20prediction%20of%20surrounding%20vehicle%20trajectories%20is%20crucial%20for%0Acollision-free%20path%20planning.%20In%20this%20study%2C%20we%20focus%20on%20a%20scenario%20where%20a%0Aconnected%20and%20autonomous%20vehicle%20%28CAV%29%20serves%20as%20the%20central%20agent%2C%20utilizing%0Aboth%20sensors%20and%20communication%20technologies%20to%20perceive%20its%20surrounding%0Atraffics%20consisting%20of%20autonomous%20vehicles%20%28AVs%29%2C%20connected%20vehicles%20%28CVs%29%2C%20and%0Ahuman-driven%20vehicles%20%28HDVs%29.%20Our%20trajectory%20prediction%20task%20is%20aimed%20at%20all%0Athe%20detected%20surrounding%20vehicles.%20To%20effectively%20integrate%20the%20multi-source%0Adata%20from%20both%20sensor%20and%20communication%20technologies%2C%20we%20propose%20a%20deep%0Alearning%20framework%20called%20MSMA%20utilizing%20a%20cross-attention%20module%20for%0Amulti-source%20data%20fusion.%20Vector%20map%20data%20is%20utilized%20to%20provide%20contextual%0Ainformation.%20The%20trajectory%20dataset%20is%20collected%20in%20CARLA%20simulator%20with%0Asynthesized%20data%20errors%20introduced.%20Numerical%20experiments%20demonstrate%20that%20in%20a%0Amixed%20traffic%20flow%20scenario%2C%20the%20integration%20of%20data%20from%20different%20sources%0Aenhances%20our%20understanding%20of%20the%20environment.%20This%20notably%20improves%20trajectory%0Aprediction%20accuracy%2C%20particularly%20in%20situations%20with%20a%20high%20CV%20market%0Apenetration%20rate.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/xichennn/MSMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21310v2&entry.124074799=Read"},
{"title": "Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition\n  Low-Light Image Enhancement", "author": "Wenbin Zou and Hongxia Gao and Weipeng Yang and Tongtong Liu", "abstract": "  Ultra-high-definition (UHD) technology has attracted widespread attention due\nto its exceptional visual quality, but it also poses new challenges for\nlow-light image enhancement (LLIE) techniques. UHD images inherently possess\nhigh computational complexity, leading existing UHD LLIE methods to employ\nhigh-magnification downsampling to reduce computational costs, which in turn\nresults in information loss. The wavelet transform not only allows downsampling\nwithout loss of information, but also separates the image content from the\nnoise. It enables state space models (SSMs) to avoid being affected by noise\nwhen modeling long sequences, thus making full use of the long-sequence\nmodeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel\napproach based on two pivotal insights derived from the wavelet domain: 1) most\nof the content information of an image exists in the low-frequency component,\nless in the high-frequency component. 2) The high-frequency component exerts a\nminimal influence on the outcomes of low-light enhancement. Specifically, to\nefficiently model global content information on UHD images, we proposed a\nlow-frequency state space block (LFSSBlock) by improving SSMs to focus on\nrestoring the information of low-frequency sub-bands. Moreover, we propose a\nhigh-frequency enhance block (HFEBlock) for high-frequency sub-band\ninformation, which uses the enhanced low-frequency information to correct the\nhigh-frequency information and effectively restore the correct high-frequency\ndetails. Through comprehensive evaluation, our method has demonstrated superior\nperformance, significantly outshining current leading techniques while\nmaintaining a more streamlined architecture. The code is available at\nhttps://github.com/AlexZou14/Wave-Mamba.\n", "link": "http://arxiv.org/abs/2408.01276v1", "date": "2024-08-02", "relevancy": 2.1263, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.58}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wave-Mamba%3A%20Wavelet%20State%20Space%20Model%20for%20Ultra-High-Definition%0A%20%20Low-Light%20Image%20Enhancement&body=Title%3A%20Wave-Mamba%3A%20Wavelet%20State%20Space%20Model%20for%20Ultra-High-Definition%0A%20%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Wenbin%20Zou%20and%20Hongxia%20Gao%20and%20Weipeng%20Yang%20and%20Tongtong%20Liu%0AAbstract%3A%20%20%20Ultra-high-definition%20%28UHD%29%20technology%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20exceptional%20visual%20quality%2C%20but%20it%20also%20poses%20new%20challenges%20for%0Alow-light%20image%20enhancement%20%28LLIE%29%20techniques.%20UHD%20images%20inherently%20possess%0Ahigh%20computational%20complexity%2C%20leading%20existing%20UHD%20LLIE%20methods%20to%20employ%0Ahigh-magnification%20downsampling%20to%20reduce%20computational%20costs%2C%20which%20in%20turn%0Aresults%20in%20information%20loss.%20The%20wavelet%20transform%20not%20only%20allows%20downsampling%0Awithout%20loss%20of%20information%2C%20but%20also%20separates%20the%20image%20content%20from%20the%0Anoise.%20It%20enables%20state%20space%20models%20%28SSMs%29%20to%20avoid%20being%20affected%20by%20noise%0Awhen%20modeling%20long%20sequences%2C%20thus%20making%20full%20use%20of%20the%20long-sequence%0Amodeling%20capability%20of%20SSMs.%20On%20this%20basis%2C%20we%20propose%20Wave-Mamba%2C%20a%20novel%0Aapproach%20based%20on%20two%20pivotal%20insights%20derived%20from%20the%20wavelet%20domain%3A%201%29%20most%0Aof%20the%20content%20information%20of%20an%20image%20exists%20in%20the%20low-frequency%20component%2C%0Aless%20in%20the%20high-frequency%20component.%202%29%20The%20high-frequency%20component%20exerts%20a%0Aminimal%20influence%20on%20the%20outcomes%20of%20low-light%20enhancement.%20Specifically%2C%20to%0Aefficiently%20model%20global%20content%20information%20on%20UHD%20images%2C%20we%20proposed%20a%0Alow-frequency%20state%20space%20block%20%28LFSSBlock%29%20by%20improving%20SSMs%20to%20focus%20on%0Arestoring%20the%20information%20of%20low-frequency%20sub-bands.%20Moreover%2C%20we%20propose%20a%0Ahigh-frequency%20enhance%20block%20%28HFEBlock%29%20for%20high-frequency%20sub-band%0Ainformation%2C%20which%20uses%20the%20enhanced%20low-frequency%20information%20to%20correct%20the%0Ahigh-frequency%20information%20and%20effectively%20restore%20the%20correct%20high-frequency%0Adetails.%20Through%20comprehensive%20evaluation%2C%20our%20method%20has%20demonstrated%20superior%0Aperformance%2C%20significantly%20outshining%20current%20leading%20techniques%20while%0Amaintaining%20a%20more%20streamlined%20architecture.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AlexZou14/Wave-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWave-Mamba%253A%2520Wavelet%2520State%2520Space%2520Model%2520for%2520Ultra-High-Definition%250A%2520%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DWenbin%2520Zou%2520and%2520Hongxia%2520Gao%2520and%2520Weipeng%2520Yang%2520and%2520Tongtong%2520Liu%26entry.1292438233%3D%2520%2520Ultra-high-definition%2520%2528UHD%2529%2520technology%2520has%2520attracted%2520widespread%2520attention%2520due%250Ato%2520its%2520exceptional%2520visual%2520quality%252C%2520but%2520it%2520also%2520poses%2520new%2520challenges%2520for%250Alow-light%2520image%2520enhancement%2520%2528LLIE%2529%2520techniques.%2520UHD%2520images%2520inherently%2520possess%250Ahigh%2520computational%2520complexity%252C%2520leading%2520existing%2520UHD%2520LLIE%2520methods%2520to%2520employ%250Ahigh-magnification%2520downsampling%2520to%2520reduce%2520computational%2520costs%252C%2520which%2520in%2520turn%250Aresults%2520in%2520information%2520loss.%2520The%2520wavelet%2520transform%2520not%2520only%2520allows%2520downsampling%250Awithout%2520loss%2520of%2520information%252C%2520but%2520also%2520separates%2520the%2520image%2520content%2520from%2520the%250Anoise.%2520It%2520enables%2520state%2520space%2520models%2520%2528SSMs%2529%2520to%2520avoid%2520being%2520affected%2520by%2520noise%250Awhen%2520modeling%2520long%2520sequences%252C%2520thus%2520making%2520full%2520use%2520of%2520the%2520long-sequence%250Amodeling%2520capability%2520of%2520SSMs.%2520On%2520this%2520basis%252C%2520we%2520propose%2520Wave-Mamba%252C%2520a%2520novel%250Aapproach%2520based%2520on%2520two%2520pivotal%2520insights%2520derived%2520from%2520the%2520wavelet%2520domain%253A%25201%2529%2520most%250Aof%2520the%2520content%2520information%2520of%2520an%2520image%2520exists%2520in%2520the%2520low-frequency%2520component%252C%250Aless%2520in%2520the%2520high-frequency%2520component.%25202%2529%2520The%2520high-frequency%2520component%2520exerts%2520a%250Aminimal%2520influence%2520on%2520the%2520outcomes%2520of%2520low-light%2520enhancement.%2520Specifically%252C%2520to%250Aefficiently%2520model%2520global%2520content%2520information%2520on%2520UHD%2520images%252C%2520we%2520proposed%2520a%250Alow-frequency%2520state%2520space%2520block%2520%2528LFSSBlock%2529%2520by%2520improving%2520SSMs%2520to%2520focus%2520on%250Arestoring%2520the%2520information%2520of%2520low-frequency%2520sub-bands.%2520Moreover%252C%2520we%2520propose%2520a%250Ahigh-frequency%2520enhance%2520block%2520%2528HFEBlock%2529%2520for%2520high-frequency%2520sub-band%250Ainformation%252C%2520which%2520uses%2520the%2520enhanced%2520low-frequency%2520information%2520to%2520correct%2520the%250Ahigh-frequency%2520information%2520and%2520effectively%2520restore%2520the%2520correct%2520high-frequency%250Adetails.%2520Through%2520comprehensive%2520evaluation%252C%2520our%2520method%2520has%2520demonstrated%2520superior%250Aperformance%252C%2520significantly%2520outshining%2520current%2520leading%2520techniques%2520while%250Amaintaining%2520a%2520more%2520streamlined%2520architecture.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AlexZou14/Wave-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wave-Mamba%3A%20Wavelet%20State%20Space%20Model%20for%20Ultra-High-Definition%0A%20%20Low-Light%20Image%20Enhancement&entry.906535625=Wenbin%20Zou%20and%20Hongxia%20Gao%20and%20Weipeng%20Yang%20and%20Tongtong%20Liu&entry.1292438233=%20%20Ultra-high-definition%20%28UHD%29%20technology%20has%20attracted%20widespread%20attention%20due%0Ato%20its%20exceptional%20visual%20quality%2C%20but%20it%20also%20poses%20new%20challenges%20for%0Alow-light%20image%20enhancement%20%28LLIE%29%20techniques.%20UHD%20images%20inherently%20possess%0Ahigh%20computational%20complexity%2C%20leading%20existing%20UHD%20LLIE%20methods%20to%20employ%0Ahigh-magnification%20downsampling%20to%20reduce%20computational%20costs%2C%20which%20in%20turn%0Aresults%20in%20information%20loss.%20The%20wavelet%20transform%20not%20only%20allows%20downsampling%0Awithout%20loss%20of%20information%2C%20but%20also%20separates%20the%20image%20content%20from%20the%0Anoise.%20It%20enables%20state%20space%20models%20%28SSMs%29%20to%20avoid%20being%20affected%20by%20noise%0Awhen%20modeling%20long%20sequences%2C%20thus%20making%20full%20use%20of%20the%20long-sequence%0Amodeling%20capability%20of%20SSMs.%20On%20this%20basis%2C%20we%20propose%20Wave-Mamba%2C%20a%20novel%0Aapproach%20based%20on%20two%20pivotal%20insights%20derived%20from%20the%20wavelet%20domain%3A%201%29%20most%0Aof%20the%20content%20information%20of%20an%20image%20exists%20in%20the%20low-frequency%20component%2C%0Aless%20in%20the%20high-frequency%20component.%202%29%20The%20high-frequency%20component%20exerts%20a%0Aminimal%20influence%20on%20the%20outcomes%20of%20low-light%20enhancement.%20Specifically%2C%20to%0Aefficiently%20model%20global%20content%20information%20on%20UHD%20images%2C%20we%20proposed%20a%0Alow-frequency%20state%20space%20block%20%28LFSSBlock%29%20by%20improving%20SSMs%20to%20focus%20on%0Arestoring%20the%20information%20of%20low-frequency%20sub-bands.%20Moreover%2C%20we%20propose%20a%0Ahigh-frequency%20enhance%20block%20%28HFEBlock%29%20for%20high-frequency%20sub-band%0Ainformation%2C%20which%20uses%20the%20enhanced%20low-frequency%20information%20to%20correct%20the%0Ahigh-frequency%20information%20and%20effectively%20restore%20the%20correct%20high-frequency%0Adetails.%20Through%20comprehensive%20evaluation%2C%20our%20method%20has%20demonstrated%20superior%0Aperformance%2C%20significantly%20outshining%20current%20leading%20techniques%20while%0Amaintaining%20a%20more%20streamlined%20architecture.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AlexZou14/Wave-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01276v1&entry.124074799=Read"},
{"title": "Incorporating Control Inputs in the Estimation of Continuous Mobile\n  Robot Trajectories and Continuum Robot Shapes", "author": "Sven Lilge and Timothy D. Barfoot", "abstract": "  Continuous-time batch state estimation using Gaussian processes is an\nefficient approach to estimate the trajectories of robots over time. In the\npast, relatively simple physics-motivated priors have been considered for such\napproaches, using assumptions such as constant velocity or acceleration. This\npaper presents an approach to incorporating exogenous control inputs, such as\nvelocity or acceleration commands, into the continuous Gaussian process\nstate-estimation framework. It is shown that this approach generalizes across\ndifferent domains in robotics, making it applicable to both the estimation of\ncontinuous-time trajectories for mobile robots and continuum-robot shapes.\nResults show that incorporating control inputs leads to more informed priors,\npotentially requiring less measurements and estimation nodes to obtain accurate\nestimates. This makes the approach particularly useful in situations in which\nlimited sensing is available.\n", "link": "http://arxiv.org/abs/2408.01333v1", "date": "2024-08-02", "relevancy": 2.119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6107}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5217}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Control%20Inputs%20in%20the%20Estimation%20of%20Continuous%20Mobile%0A%20%20Robot%20Trajectories%20and%20Continuum%20Robot%20Shapes&body=Title%3A%20Incorporating%20Control%20Inputs%20in%20the%20Estimation%20of%20Continuous%20Mobile%0A%20%20Robot%20Trajectories%20and%20Continuum%20Robot%20Shapes%0AAuthor%3A%20Sven%20Lilge%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Continuous-time%20batch%20state%20estimation%20using%20Gaussian%20processes%20is%20an%0Aefficient%20approach%20to%20estimate%20the%20trajectories%20of%20robots%20over%20time.%20In%20the%0Apast%2C%20relatively%20simple%20physics-motivated%20priors%20have%20been%20considered%20for%20such%0Aapproaches%2C%20using%20assumptions%20such%20as%20constant%20velocity%20or%20acceleration.%20This%0Apaper%20presents%20an%20approach%20to%20incorporating%20exogenous%20control%20inputs%2C%20such%20as%0Avelocity%20or%20acceleration%20commands%2C%20into%20the%20continuous%20Gaussian%20process%0Astate-estimation%20framework.%20It%20is%20shown%20that%20this%20approach%20generalizes%20across%0Adifferent%20domains%20in%20robotics%2C%20making%20it%20applicable%20to%20both%20the%20estimation%20of%0Acontinuous-time%20trajectories%20for%20mobile%20robots%20and%20continuum-robot%20shapes.%0AResults%20show%20that%20incorporating%20control%20inputs%20leads%20to%20more%20informed%20priors%2C%0Apotentially%20requiring%20less%20measurements%20and%20estimation%20nodes%20to%20obtain%20accurate%0Aestimates.%20This%20makes%20the%20approach%20particularly%20useful%20in%20situations%20in%20which%0Alimited%20sensing%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Control%2520Inputs%2520in%2520the%2520Estimation%2520of%2520Continuous%2520Mobile%250A%2520%2520Robot%2520Trajectories%2520and%2520Continuum%2520Robot%2520Shapes%26entry.906535625%3DSven%2520Lilge%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Continuous-time%2520batch%2520state%2520estimation%2520using%2520Gaussian%2520processes%2520is%2520an%250Aefficient%2520approach%2520to%2520estimate%2520the%2520trajectories%2520of%2520robots%2520over%2520time.%2520In%2520the%250Apast%252C%2520relatively%2520simple%2520physics-motivated%2520priors%2520have%2520been%2520considered%2520for%2520such%250Aapproaches%252C%2520using%2520assumptions%2520such%2520as%2520constant%2520velocity%2520or%2520acceleration.%2520This%250Apaper%2520presents%2520an%2520approach%2520to%2520incorporating%2520exogenous%2520control%2520inputs%252C%2520such%2520as%250Avelocity%2520or%2520acceleration%2520commands%252C%2520into%2520the%2520continuous%2520Gaussian%2520process%250Astate-estimation%2520framework.%2520It%2520is%2520shown%2520that%2520this%2520approach%2520generalizes%2520across%250Adifferent%2520domains%2520in%2520robotics%252C%2520making%2520it%2520applicable%2520to%2520both%2520the%2520estimation%2520of%250Acontinuous-time%2520trajectories%2520for%2520mobile%2520robots%2520and%2520continuum-robot%2520shapes.%250AResults%2520show%2520that%2520incorporating%2520control%2520inputs%2520leads%2520to%2520more%2520informed%2520priors%252C%250Apotentially%2520requiring%2520less%2520measurements%2520and%2520estimation%2520nodes%2520to%2520obtain%2520accurate%250Aestimates.%2520This%2520makes%2520the%2520approach%2520particularly%2520useful%2520in%2520situations%2520in%2520which%250Alimited%2520sensing%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Control%20Inputs%20in%20the%20Estimation%20of%20Continuous%20Mobile%0A%20%20Robot%20Trajectories%20and%20Continuum%20Robot%20Shapes&entry.906535625=Sven%20Lilge%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Continuous-time%20batch%20state%20estimation%20using%20Gaussian%20processes%20is%20an%0Aefficient%20approach%20to%20estimate%20the%20trajectories%20of%20robots%20over%20time.%20In%20the%0Apast%2C%20relatively%20simple%20physics-motivated%20priors%20have%20been%20considered%20for%20such%0Aapproaches%2C%20using%20assumptions%20such%20as%20constant%20velocity%20or%20acceleration.%20This%0Apaper%20presents%20an%20approach%20to%20incorporating%20exogenous%20control%20inputs%2C%20such%20as%0Avelocity%20or%20acceleration%20commands%2C%20into%20the%20continuous%20Gaussian%20process%0Astate-estimation%20framework.%20It%20is%20shown%20that%20this%20approach%20generalizes%20across%0Adifferent%20domains%20in%20robotics%2C%20making%20it%20applicable%20to%20both%20the%20estimation%20of%0Acontinuous-time%20trajectories%20for%20mobile%20robots%20and%20continuum-robot%20shapes.%0AResults%20show%20that%20incorporating%20control%20inputs%20leads%20to%20more%20informed%20priors%2C%0Apotentially%20requiring%20less%20measurements%20and%20estimation%20nodes%20to%20obtain%20accurate%0Aestimates.%20This%20makes%20the%20approach%20particularly%20useful%20in%20situations%20in%20which%0Alimited%20sensing%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01333v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification of Set-Membership Estimation in Control and\n  Perception: Revisiting the Minimum Enclosing Ellipsoid", "author": "Yukai Tang and Jean-Bernard Lasserre and Heng Yang", "abstract": "  Set-membership estimation (SME) outputs a set estimator that guarantees to\ncover the groundtruth. Such sets are, however, defined by (many) abstract (and\npotentially nonconvex) constraints and therefore difficult to manipulate. We\npresent tractable algorithms to compute simple and tight overapproximations of\nSME in the form of minimum enclosing ellipsoids (MEE). We first introduce the\nhierarchy of enclosing ellipsoids proposed by Nie and Demmel (2005), based on\nsums-of-squares relaxations, that asymptotically converge to the MEE of a basic\nsemialgebraic set. This framework, however, struggles in modern control and\nperception problems due to computational challenges. We contribute three\ncomputational enhancements to make this framework practical, namely constraints\npruning, generalized relaxed Chebyshev center, and handling non-Euclidean\ngeometry. We showcase numerical examples on system identification and object\npose estimation.\n", "link": "http://arxiv.org/abs/2311.15962v4", "date": "2024-08-02", "relevancy": 2.1095, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20of%20Set-Membership%20Estimation%20in%20Control%20and%0A%20%20Perception%3A%20Revisiting%20the%20Minimum%20Enclosing%20Ellipsoid&body=Title%3A%20Uncertainty%20Quantification%20of%20Set-Membership%20Estimation%20in%20Control%20and%0A%20%20Perception%3A%20Revisiting%20the%20Minimum%20Enclosing%20Ellipsoid%0AAuthor%3A%20Yukai%20Tang%20and%20Jean-Bernard%20Lasserre%20and%20Heng%20Yang%0AAbstract%3A%20%20%20Set-membership%20estimation%20%28SME%29%20outputs%20a%20set%20estimator%20that%20guarantees%20to%0Acover%20the%20groundtruth.%20Such%20sets%20are%2C%20however%2C%20defined%20by%20%28many%29%20abstract%20%28and%0Apotentially%20nonconvex%29%20constraints%20and%20therefore%20difficult%20to%20manipulate.%20We%0Apresent%20tractable%20algorithms%20to%20compute%20simple%20and%20tight%20overapproximations%20of%0ASME%20in%20the%20form%20of%20minimum%20enclosing%20ellipsoids%20%28MEE%29.%20We%20first%20introduce%20the%0Ahierarchy%20of%20enclosing%20ellipsoids%20proposed%20by%20Nie%20and%20Demmel%20%282005%29%2C%20based%20on%0Asums-of-squares%20relaxations%2C%20that%20asymptotically%20converge%20to%20the%20MEE%20of%20a%20basic%0Asemialgebraic%20set.%20This%20framework%2C%20however%2C%20struggles%20in%20modern%20control%20and%0Aperception%20problems%20due%20to%20computational%20challenges.%20We%20contribute%20three%0Acomputational%20enhancements%20to%20make%20this%20framework%20practical%2C%20namely%20constraints%0Apruning%2C%20generalized%20relaxed%20Chebyshev%20center%2C%20and%20handling%20non-Euclidean%0Ageometry.%20We%20showcase%20numerical%20examples%20on%20system%20identification%20and%20object%0Apose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15962v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520of%2520Set-Membership%2520Estimation%2520in%2520Control%2520and%250A%2520%2520Perception%253A%2520Revisiting%2520the%2520Minimum%2520Enclosing%2520Ellipsoid%26entry.906535625%3DYukai%2520Tang%2520and%2520Jean-Bernard%2520Lasserre%2520and%2520Heng%2520Yang%26entry.1292438233%3D%2520%2520Set-membership%2520estimation%2520%2528SME%2529%2520outputs%2520a%2520set%2520estimator%2520that%2520guarantees%2520to%250Acover%2520the%2520groundtruth.%2520Such%2520sets%2520are%252C%2520however%252C%2520defined%2520by%2520%2528many%2529%2520abstract%2520%2528and%250Apotentially%2520nonconvex%2529%2520constraints%2520and%2520therefore%2520difficult%2520to%2520manipulate.%2520We%250Apresent%2520tractable%2520algorithms%2520to%2520compute%2520simple%2520and%2520tight%2520overapproximations%2520of%250ASME%2520in%2520the%2520form%2520of%2520minimum%2520enclosing%2520ellipsoids%2520%2528MEE%2529.%2520We%2520first%2520introduce%2520the%250Ahierarchy%2520of%2520enclosing%2520ellipsoids%2520proposed%2520by%2520Nie%2520and%2520Demmel%2520%25282005%2529%252C%2520based%2520on%250Asums-of-squares%2520relaxations%252C%2520that%2520asymptotically%2520converge%2520to%2520the%2520MEE%2520of%2520a%2520basic%250Asemialgebraic%2520set.%2520This%2520framework%252C%2520however%252C%2520struggles%2520in%2520modern%2520control%2520and%250Aperception%2520problems%2520due%2520to%2520computational%2520challenges.%2520We%2520contribute%2520three%250Acomputational%2520enhancements%2520to%2520make%2520this%2520framework%2520practical%252C%2520namely%2520constraints%250Apruning%252C%2520generalized%2520relaxed%2520Chebyshev%2520center%252C%2520and%2520handling%2520non-Euclidean%250Ageometry.%2520We%2520showcase%2520numerical%2520examples%2520on%2520system%2520identification%2520and%2520object%250Apose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15962v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20of%20Set-Membership%20Estimation%20in%20Control%20and%0A%20%20Perception%3A%20Revisiting%20the%20Minimum%20Enclosing%20Ellipsoid&entry.906535625=Yukai%20Tang%20and%20Jean-Bernard%20Lasserre%20and%20Heng%20Yang&entry.1292438233=%20%20Set-membership%20estimation%20%28SME%29%20outputs%20a%20set%20estimator%20that%20guarantees%20to%0Acover%20the%20groundtruth.%20Such%20sets%20are%2C%20however%2C%20defined%20by%20%28many%29%20abstract%20%28and%0Apotentially%20nonconvex%29%20constraints%20and%20therefore%20difficult%20to%20manipulate.%20We%0Apresent%20tractable%20algorithms%20to%20compute%20simple%20and%20tight%20overapproximations%20of%0ASME%20in%20the%20form%20of%20minimum%20enclosing%20ellipsoids%20%28MEE%29.%20We%20first%20introduce%20the%0Ahierarchy%20of%20enclosing%20ellipsoids%20proposed%20by%20Nie%20and%20Demmel%20%282005%29%2C%20based%20on%0Asums-of-squares%20relaxations%2C%20that%20asymptotically%20converge%20to%20the%20MEE%20of%20a%20basic%0Asemialgebraic%20set.%20This%20framework%2C%20however%2C%20struggles%20in%20modern%20control%20and%0Aperception%20problems%20due%20to%20computational%20challenges.%20We%20contribute%20three%0Acomputational%20enhancements%20to%20make%20this%20framework%20practical%2C%20namely%20constraints%0Apruning%2C%20generalized%20relaxed%20Chebyshev%20center%2C%20and%20handling%20non-Euclidean%0Ageometry.%20We%20showcase%20numerical%20examples%20on%20system%20identification%20and%20object%0Apose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15962v4&entry.124074799=Read"},
{"title": "ZNorm: Z-Score Gradient Normalization for Accelerating Neural Network\n  Training", "author": "Juyoung Yun and Hoyoung Kim and Suin Cho and Hangil Kang", "abstract": "  The rapid advancements in deep learning necessitate efficient training\nmethods for deep neural networks (DNNs). As models grow in complexity,\nvanishing and exploding gradients impede convergence and performance. We\npropose Z-Score Normalization for Gradient Descent (ZNorm), an innovative\ntechnique that adjusts only the gradients to enhance training efficiency and\nimprove model performance. ZNorm normalizes the overall gradients, providing\nconsistent gradient scaling across layers, thereby reducing the risks of\nvanishing and exploding gradients. Our extensive experiments on CIFAR-10 and\nmedical datasets demonstrate that ZNorm not only accelerates convergence but\nalso enhances performance metrics. ZNorm consistently outperforms existing\nmethods, achieving superior results using the same computational settings. In\nmedical imaging applications, ZNorm improves tumor prediction and segmentation\nperformances, underscoring its practical utility. These findings highlight\nZNorm's potential as a robust and versatile tool for improving the efficiency\nand effectiveness of deep neural network training across a wide range of\narchitectures and applications.\n", "link": "http://arxiv.org/abs/2408.01215v1", "date": "2024-08-02", "relevancy": 2.0988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.548}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5247}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZNorm%3A%20Z-Score%20Gradient%20Normalization%20for%20Accelerating%20Neural%20Network%0A%20%20Training&body=Title%3A%20ZNorm%3A%20Z-Score%20Gradient%20Normalization%20for%20Accelerating%20Neural%20Network%0A%20%20Training%0AAuthor%3A%20Juyoung%20Yun%20and%20Hoyoung%20Kim%20and%20Suin%20Cho%20and%20Hangil%20Kang%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20deep%20learning%20necessitate%20efficient%20training%0Amethods%20for%20deep%20neural%20networks%20%28DNNs%29.%20As%20models%20grow%20in%20complexity%2C%0Avanishing%20and%20exploding%20gradients%20impede%20convergence%20and%20performance.%20We%0Apropose%20Z-Score%20Normalization%20for%20Gradient%20Descent%20%28ZNorm%29%2C%20an%20innovative%0Atechnique%20that%20adjusts%20only%20the%20gradients%20to%20enhance%20training%20efficiency%20and%0Aimprove%20model%20performance.%20ZNorm%20normalizes%20the%20overall%20gradients%2C%20providing%0Aconsistent%20gradient%20scaling%20across%20layers%2C%20thereby%20reducing%20the%20risks%20of%0Avanishing%20and%20exploding%20gradients.%20Our%20extensive%20experiments%20on%20CIFAR-10%20and%0Amedical%20datasets%20demonstrate%20that%20ZNorm%20not%20only%20accelerates%20convergence%20but%0Aalso%20enhances%20performance%20metrics.%20ZNorm%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20results%20using%20the%20same%20computational%20settings.%20In%0Amedical%20imaging%20applications%2C%20ZNorm%20improves%20tumor%20prediction%20and%20segmentation%0Aperformances%2C%20underscoring%20its%20practical%20utility.%20These%20findings%20highlight%0AZNorm%27s%20potential%20as%20a%20robust%20and%20versatile%20tool%20for%20improving%20the%20efficiency%0Aand%20effectiveness%20of%20deep%20neural%20network%20training%20across%20a%20wide%20range%20of%0Aarchitectures%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZNorm%253A%2520Z-Score%2520Gradient%2520Normalization%2520for%2520Accelerating%2520Neural%2520Network%250A%2520%2520Training%26entry.906535625%3DJuyoung%2520Yun%2520and%2520Hoyoung%2520Kim%2520and%2520Suin%2520Cho%2520and%2520Hangil%2520Kang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520deep%2520learning%2520necessitate%2520efficient%2520training%250Amethods%2520for%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520As%2520models%2520grow%2520in%2520complexity%252C%250Avanishing%2520and%2520exploding%2520gradients%2520impede%2520convergence%2520and%2520performance.%2520We%250Apropose%2520Z-Score%2520Normalization%2520for%2520Gradient%2520Descent%2520%2528ZNorm%2529%252C%2520an%2520innovative%250Atechnique%2520that%2520adjusts%2520only%2520the%2520gradients%2520to%2520enhance%2520training%2520efficiency%2520and%250Aimprove%2520model%2520performance.%2520ZNorm%2520normalizes%2520the%2520overall%2520gradients%252C%2520providing%250Aconsistent%2520gradient%2520scaling%2520across%2520layers%252C%2520thereby%2520reducing%2520the%2520risks%2520of%250Avanishing%2520and%2520exploding%2520gradients.%2520Our%2520extensive%2520experiments%2520on%2520CIFAR-10%2520and%250Amedical%2520datasets%2520demonstrate%2520that%2520ZNorm%2520not%2520only%2520accelerates%2520convergence%2520but%250Aalso%2520enhances%2520performance%2520metrics.%2520ZNorm%2520consistently%2520outperforms%2520existing%250Amethods%252C%2520achieving%2520superior%2520results%2520using%2520the%2520same%2520computational%2520settings.%2520In%250Amedical%2520imaging%2520applications%252C%2520ZNorm%2520improves%2520tumor%2520prediction%2520and%2520segmentation%250Aperformances%252C%2520underscoring%2520its%2520practical%2520utility.%2520These%2520findings%2520highlight%250AZNorm%2527s%2520potential%2520as%2520a%2520robust%2520and%2520versatile%2520tool%2520for%2520improving%2520the%2520efficiency%250Aand%2520effectiveness%2520of%2520deep%2520neural%2520network%2520training%2520across%2520a%2520wide%2520range%2520of%250Aarchitectures%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZNorm%3A%20Z-Score%20Gradient%20Normalization%20for%20Accelerating%20Neural%20Network%0A%20%20Training&entry.906535625=Juyoung%20Yun%20and%20Hoyoung%20Kim%20and%20Suin%20Cho%20and%20Hangil%20Kang&entry.1292438233=%20%20The%20rapid%20advancements%20in%20deep%20learning%20necessitate%20efficient%20training%0Amethods%20for%20deep%20neural%20networks%20%28DNNs%29.%20As%20models%20grow%20in%20complexity%2C%0Avanishing%20and%20exploding%20gradients%20impede%20convergence%20and%20performance.%20We%0Apropose%20Z-Score%20Normalization%20for%20Gradient%20Descent%20%28ZNorm%29%2C%20an%20innovative%0Atechnique%20that%20adjusts%20only%20the%20gradients%20to%20enhance%20training%20efficiency%20and%0Aimprove%20model%20performance.%20ZNorm%20normalizes%20the%20overall%20gradients%2C%20providing%0Aconsistent%20gradient%20scaling%20across%20layers%2C%20thereby%20reducing%20the%20risks%20of%0Avanishing%20and%20exploding%20gradients.%20Our%20extensive%20experiments%20on%20CIFAR-10%20and%0Amedical%20datasets%20demonstrate%20that%20ZNorm%20not%20only%20accelerates%20convergence%20but%0Aalso%20enhances%20performance%20metrics.%20ZNorm%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20superior%20results%20using%20the%20same%20computational%20settings.%20In%0Amedical%20imaging%20applications%2C%20ZNorm%20improves%20tumor%20prediction%20and%20segmentation%0Aperformances%2C%20underscoring%20its%20practical%20utility.%20These%20findings%20highlight%0AZNorm%27s%20potential%20as%20a%20robust%20and%20versatile%20tool%20for%20improving%20the%20efficiency%0Aand%20effectiveness%20of%20deep%20neural%20network%20training%20across%20a%20wide%20range%20of%0Aarchitectures%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01215v1&entry.124074799=Read"},
{"title": "Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation", "author": "Jheng-Hong Yang and Jimmy Lin", "abstract": "  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n", "link": "http://arxiv.org/abs/2408.01363v1", "date": "2024-08-02", "relevancy": 2.0901, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.508}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Automatic%20Relevance%20Judgment%20using%20Vision--Language%20Models%20for%0A%20%20Image--Text%20Retrieval%20Evaluation&body=Title%3A%20Toward%20Automatic%20Relevance%20Judgment%20using%20Vision--Language%20Models%20for%0A%20%20Image--Text%20Retrieval%20Evaluation%0AAuthor%3A%20Jheng-Hong%20Yang%20and%20Jimmy%20Lin%0AAbstract%3A%20%20%20Vision--Language%20Models%20%28VLMs%29%20have%20demonstrated%20success%20across%20diverse%0Aapplications%2C%20yet%20their%20potential%20to%20assist%20in%20relevance%20judgments%20remains%0Auncertain.%20This%20paper%20assesses%20the%20relevance%20estimation%20capabilities%20of%20VLMs%2C%0Aincluding%20CLIP%2C%20LLaVA%2C%20and%20GPT-4V%2C%20within%20a%20large-scale%20%5Ctextit%7Bad%20hoc%7D%0Aretrieval%20task%20tailored%20for%20multimedia%20content%20creation%20in%20a%20zero-shot%20fashion.%0APreliminary%20experiments%20reveal%20the%20following%3A%20%281%29%20Both%20LLaVA%20and%20GPT-4V%2C%0Aencompassing%20open-source%20and%20closed-source%20visual-instruction-tuned%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20achieve%20notable%20Kendall%27s%20%24%5Ctau%20%5Csim%200.4%24%20when%20compared%0Ato%20human%20relevance%20judgments%2C%20surpassing%20the%20CLIPScore%20metric.%20%282%29%20While%0ACLIPScore%20is%20strongly%20preferred%2C%20LLMs%20are%20less%20biased%20towards%20CLIP-based%0Aretrieval%20systems.%20%283%29%20GPT-4V%27s%20score%20distribution%20aligns%20more%20closely%20with%0Ahuman%20judgments%20than%20other%20models%2C%20achieving%20a%20Cohen%27s%20%24%5Ckappa%24%20value%20of%20around%0A0.08%2C%20which%20outperforms%20CLIPScore%20at%20approximately%20-0.096.%20These%20findings%0Aunderscore%20the%20potential%20of%20LLM-powered%20VLMs%20in%20enhancing%20relevance%20judgments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Automatic%2520Relevance%2520Judgment%2520using%2520Vision--Language%2520Models%2520for%250A%2520%2520Image--Text%2520Retrieval%2520Evaluation%26entry.906535625%3DJheng-Hong%2520Yang%2520and%2520Jimmy%2520Lin%26entry.1292438233%3D%2520%2520Vision--Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520success%2520across%2520diverse%250Aapplications%252C%2520yet%2520their%2520potential%2520to%2520assist%2520in%2520relevance%2520judgments%2520remains%250Auncertain.%2520This%2520paper%2520assesses%2520the%2520relevance%2520estimation%2520capabilities%2520of%2520VLMs%252C%250Aincluding%2520CLIP%252C%2520LLaVA%252C%2520and%2520GPT-4V%252C%2520within%2520a%2520large-scale%2520%255Ctextit%257Bad%2520hoc%257D%250Aretrieval%2520task%2520tailored%2520for%2520multimedia%2520content%2520creation%2520in%2520a%2520zero-shot%2520fashion.%250APreliminary%2520experiments%2520reveal%2520the%2520following%253A%2520%25281%2529%2520Both%2520LLaVA%2520and%2520GPT-4V%252C%250Aencompassing%2520open-source%2520and%2520closed-source%2520visual-instruction-tuned%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520achieve%2520notable%2520Kendall%2527s%2520%2524%255Ctau%2520%255Csim%25200.4%2524%2520when%2520compared%250Ato%2520human%2520relevance%2520judgments%252C%2520surpassing%2520the%2520CLIPScore%2520metric.%2520%25282%2529%2520While%250ACLIPScore%2520is%2520strongly%2520preferred%252C%2520LLMs%2520are%2520less%2520biased%2520towards%2520CLIP-based%250Aretrieval%2520systems.%2520%25283%2529%2520GPT-4V%2527s%2520score%2520distribution%2520aligns%2520more%2520closely%2520with%250Ahuman%2520judgments%2520than%2520other%2520models%252C%2520achieving%2520a%2520Cohen%2527s%2520%2524%255Ckappa%2524%2520value%2520of%2520around%250A0.08%252C%2520which%2520outperforms%2520CLIPScore%2520at%2520approximately%2520-0.096.%2520These%2520findings%250Aunderscore%2520the%2520potential%2520of%2520LLM-powered%2520VLMs%2520in%2520enhancing%2520relevance%2520judgments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Automatic%20Relevance%20Judgment%20using%20Vision--Language%20Models%20for%0A%20%20Image--Text%20Retrieval%20Evaluation&entry.906535625=Jheng-Hong%20Yang%20and%20Jimmy%20Lin&entry.1292438233=%20%20Vision--Language%20Models%20%28VLMs%29%20have%20demonstrated%20success%20across%20diverse%0Aapplications%2C%20yet%20their%20potential%20to%20assist%20in%20relevance%20judgments%20remains%0Auncertain.%20This%20paper%20assesses%20the%20relevance%20estimation%20capabilities%20of%20VLMs%2C%0Aincluding%20CLIP%2C%20LLaVA%2C%20and%20GPT-4V%2C%20within%20a%20large-scale%20%5Ctextit%7Bad%20hoc%7D%0Aretrieval%20task%20tailored%20for%20multimedia%20content%20creation%20in%20a%20zero-shot%20fashion.%0APreliminary%20experiments%20reveal%20the%20following%3A%20%281%29%20Both%20LLaVA%20and%20GPT-4V%2C%0Aencompassing%20open-source%20and%20closed-source%20visual-instruction-tuned%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20achieve%20notable%20Kendall%27s%20%24%5Ctau%20%5Csim%200.4%24%20when%20compared%0Ato%20human%20relevance%20judgments%2C%20surpassing%20the%20CLIPScore%20metric.%20%282%29%20While%0ACLIPScore%20is%20strongly%20preferred%2C%20LLMs%20are%20less%20biased%20towards%20CLIP-based%0Aretrieval%20systems.%20%283%29%20GPT-4V%27s%20score%20distribution%20aligns%20more%20closely%20with%0Ahuman%20judgments%20than%20other%20models%2C%20achieving%20a%20Cohen%27s%20%24%5Ckappa%24%20value%20of%20around%0A0.08%2C%20which%20outperforms%20CLIPScore%20at%20approximately%20-0.096.%20These%20findings%0Aunderscore%20the%20potential%20of%20LLM-powered%20VLMs%20in%20enhancing%20relevance%20judgments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01363v1&entry.124074799=Read"},
{"title": "NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing", "author": "Daoxin Zhong and Luke Robinson and Daniele De Martini", "abstract": "  This paper investigates the utility of Neural Radiance Fields (NeRF) models\nin extending the regions of operation of a mobile robot, controlled by\nImage-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a\n3D-representation prior, the robot's footprint may be extrapolated\ngeometrically and used to train a CNN-based network to extract it online from\nthe robot's appearance alone. The resulting footprint results in a tighter\nbound than a robot-wide bounding box, allowing the robot's controller to\nprescribe more optimal trajectories and expand its safe operational floor area.\n", "link": "http://arxiv.org/abs/2408.01251v1", "date": "2024-08-02", "relevancy": 2.0865, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5581}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRFoot%3A%20Robot-Footprint%20Estimation%20for%20Image-Based%20Visual%20Servoing&body=Title%3A%20NeRFoot%3A%20Robot-Footprint%20Estimation%20for%20Image-Based%20Visual%20Servoing%0AAuthor%3A%20Daoxin%20Zhong%20and%20Luke%20Robinson%20and%20Daniele%20De%20Martini%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20utility%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20models%0Ain%20extending%20the%20regions%20of%20operation%20of%20a%20mobile%20robot%2C%20controlled%20by%0AImage-Based%20Visual%20Servoing%20%28IBVS%29%20via%20static%20CCTV%20cameras.%20Using%20NeRF%20as%20a%0A3D-representation%20prior%2C%20the%20robot%27s%20footprint%20may%20be%20extrapolated%0Ageometrically%20and%20used%20to%20train%20a%20CNN-based%20network%20to%20extract%20it%20online%20from%0Athe%20robot%27s%20appearance%20alone.%20The%20resulting%20footprint%20results%20in%20a%20tighter%0Abound%20than%20a%20robot-wide%20bounding%20box%2C%20allowing%20the%20robot%27s%20controller%20to%0Aprescribe%20more%20optimal%20trajectories%20and%20expand%20its%20safe%20operational%20floor%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRFoot%253A%2520Robot-Footprint%2520Estimation%2520for%2520Image-Based%2520Visual%2520Servoing%26entry.906535625%3DDaoxin%2520Zhong%2520and%2520Luke%2520Robinson%2520and%2520Daniele%2520De%2520Martini%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520utility%2520of%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520models%250Ain%2520extending%2520the%2520regions%2520of%2520operation%2520of%2520a%2520mobile%2520robot%252C%2520controlled%2520by%250AImage-Based%2520Visual%2520Servoing%2520%2528IBVS%2529%2520via%2520static%2520CCTV%2520cameras.%2520Using%2520NeRF%2520as%2520a%250A3D-representation%2520prior%252C%2520the%2520robot%2527s%2520footprint%2520may%2520be%2520extrapolated%250Ageometrically%2520and%2520used%2520to%2520train%2520a%2520CNN-based%2520network%2520to%2520extract%2520it%2520online%2520from%250Athe%2520robot%2527s%2520appearance%2520alone.%2520The%2520resulting%2520footprint%2520results%2520in%2520a%2520tighter%250Abound%2520than%2520a%2520robot-wide%2520bounding%2520box%252C%2520allowing%2520the%2520robot%2527s%2520controller%2520to%250Aprescribe%2520more%2520optimal%2520trajectories%2520and%2520expand%2520its%2520safe%2520operational%2520floor%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRFoot%3A%20Robot-Footprint%20Estimation%20for%20Image-Based%20Visual%20Servoing&entry.906535625=Daoxin%20Zhong%20and%20Luke%20Robinson%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20This%20paper%20investigates%20the%20utility%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20models%0Ain%20extending%20the%20regions%20of%20operation%20of%20a%20mobile%20robot%2C%20controlled%20by%0AImage-Based%20Visual%20Servoing%20%28IBVS%29%20via%20static%20CCTV%20cameras.%20Using%20NeRF%20as%20a%0A3D-representation%20prior%2C%20the%20robot%27s%20footprint%20may%20be%20extrapolated%0Ageometrically%20and%20used%20to%20train%20a%20CNN-based%20network%20to%20extract%20it%20online%20from%0Athe%20robot%27s%20appearance%20alone.%20The%20resulting%20footprint%20results%20in%20a%20tighter%0Abound%20than%20a%20robot-wide%20bounding%20box%2C%20allowing%20the%20robot%27s%20controller%20to%0Aprescribe%20more%20optimal%20trajectories%20and%20expand%20its%20safe%20operational%20floor%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01251v1&entry.124074799=Read"},
{"title": "Deep Learning based Visually Rich Document Content Understanding: A\n  Survey", "author": "Yihao Ding and Jean Lee and Soyeon Caren Han", "abstract": "  Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.\n", "link": "http://arxiv.org/abs/2408.01287v1", "date": "2024-08-02", "relevancy": 2.0726, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20based%20Visually%20Rich%20Document%20Content%20Understanding%3A%20A%0A%20%20Survey&body=Title%3A%20Deep%20Learning%20based%20Visually%20Rich%20Document%20Content%20Understanding%3A%20A%0A%20%20Survey%0AAuthor%3A%20Yihao%20Ding%20and%20Jean%20Lee%20and%20Soyeon%20Caren%20Han%0AAbstract%3A%20%20%20Visually%20Rich%20Documents%20%28VRDs%29%20are%20essential%20in%20academia%2C%20finance%2C%20medical%0Afields%2C%20and%20marketing%20due%20to%20their%20multimodal%20information%20content.%20Traditional%0Amethods%20for%20extracting%20information%20from%20VRDs%20depend%20on%20expert%20knowledge%20and%0Amanual%20labor%2C%20making%20them%20costly%20and%20inefficient.%20The%20advent%20of%20deep%20learning%0Ahas%20revolutionized%20this%20process%2C%20introducing%20models%20that%20leverage%20multimodal%0Ainformation%20vision%2C%20text%2C%20and%20layout%20along%20with%20pretraining%20tasks%20to%20develop%0Acomprehensive%20document%20representations.%20These%20models%20have%20achieved%0Astate-of-the-art%20performance%20across%20various%20downstream%20tasks%2C%20significantly%0Aenhancing%20the%20efficiency%20and%20accuracy%20of%20information%20extraction%20from%20VRDs.%20In%0Aresponse%20to%20the%20growing%20demands%20and%20rapid%20developments%20in%20Visually%20Rich%0ADocument%20Understanding%20%28VRDU%29%2C%20this%20paper%20provides%20a%20comprehensive%20review%20of%0Adeep%20learning-based%20VRDU%20frameworks.%20We%20systematically%20survey%20and%20analyze%0Aexisting%20methods%20and%20benchmark%20datasets%2C%20categorizing%20them%20based%20on%20adopted%0Astrategies%20and%20downstream%20tasks.%20Furthermore%2C%20we%20compare%20different%20techniques%0Aused%20in%20VRDU%20models%2C%20focusing%20on%20feature%20representation%20and%20fusion%2C%20model%0Aarchitecture%2C%20and%20pretraining%20methods%2C%20while%20highlighting%20their%20strengths%2C%0Alimitations%2C%20and%20appropriate%20scenarios.%20Finally%2C%20we%20identify%20emerging%20trends%0Aand%20challenges%20in%20VRDU%2C%20offering%20insights%20into%20future%20research%20directions%20and%0Apractical%20applications.%20This%20survey%20aims%20to%20provide%20a%20thorough%20understanding%20of%0AVRDU%20advancements%2C%20benefiting%20both%20academic%20and%20industrial%20sectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520based%2520Visually%2520Rich%2520Document%2520Content%2520Understanding%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DYihao%2520Ding%2520and%2520Jean%2520Lee%2520and%2520Soyeon%2520Caren%2520Han%26entry.1292438233%3D%2520%2520Visually%2520Rich%2520Documents%2520%2528VRDs%2529%2520are%2520essential%2520in%2520academia%252C%2520finance%252C%2520medical%250Afields%252C%2520and%2520marketing%2520due%2520to%2520their%2520multimodal%2520information%2520content.%2520Traditional%250Amethods%2520for%2520extracting%2520information%2520from%2520VRDs%2520depend%2520on%2520expert%2520knowledge%2520and%250Amanual%2520labor%252C%2520making%2520them%2520costly%2520and%2520inefficient.%2520The%2520advent%2520of%2520deep%2520learning%250Ahas%2520revolutionized%2520this%2520process%252C%2520introducing%2520models%2520that%2520leverage%2520multimodal%250Ainformation%2520vision%252C%2520text%252C%2520and%2520layout%2520along%2520with%2520pretraining%2520tasks%2520to%2520develop%250Acomprehensive%2520document%2520representations.%2520These%2520models%2520have%2520achieved%250Astate-of-the-art%2520performance%2520across%2520various%2520downstream%2520tasks%252C%2520significantly%250Aenhancing%2520the%2520efficiency%2520and%2520accuracy%2520of%2520information%2520extraction%2520from%2520VRDs.%2520In%250Aresponse%2520to%2520the%2520growing%2520demands%2520and%2520rapid%2520developments%2520in%2520Visually%2520Rich%250ADocument%2520Understanding%2520%2528VRDU%2529%252C%2520this%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%250Adeep%2520learning-based%2520VRDU%2520frameworks.%2520We%2520systematically%2520survey%2520and%2520analyze%250Aexisting%2520methods%2520and%2520benchmark%2520datasets%252C%2520categorizing%2520them%2520based%2520on%2520adopted%250Astrategies%2520and%2520downstream%2520tasks.%2520Furthermore%252C%2520we%2520compare%2520different%2520techniques%250Aused%2520in%2520VRDU%2520models%252C%2520focusing%2520on%2520feature%2520representation%2520and%2520fusion%252C%2520model%250Aarchitecture%252C%2520and%2520pretraining%2520methods%252C%2520while%2520highlighting%2520their%2520strengths%252C%250Alimitations%252C%2520and%2520appropriate%2520scenarios.%2520Finally%252C%2520we%2520identify%2520emerging%2520trends%250Aand%2520challenges%2520in%2520VRDU%252C%2520offering%2520insights%2520into%2520future%2520research%2520directions%2520and%250Apractical%2520applications.%2520This%2520survey%2520aims%2520to%2520provide%2520a%2520thorough%2520understanding%2520of%250AVRDU%2520advancements%252C%2520benefiting%2520both%2520academic%2520and%2520industrial%2520sectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20based%20Visually%20Rich%20Document%20Content%20Understanding%3A%20A%0A%20%20Survey&entry.906535625=Yihao%20Ding%20and%20Jean%20Lee%20and%20Soyeon%20Caren%20Han&entry.1292438233=%20%20Visually%20Rich%20Documents%20%28VRDs%29%20are%20essential%20in%20academia%2C%20finance%2C%20medical%0Afields%2C%20and%20marketing%20due%20to%20their%20multimodal%20information%20content.%20Traditional%0Amethods%20for%20extracting%20information%20from%20VRDs%20depend%20on%20expert%20knowledge%20and%0Amanual%20labor%2C%20making%20them%20costly%20and%20inefficient.%20The%20advent%20of%20deep%20learning%0Ahas%20revolutionized%20this%20process%2C%20introducing%20models%20that%20leverage%20multimodal%0Ainformation%20vision%2C%20text%2C%20and%20layout%20along%20with%20pretraining%20tasks%20to%20develop%0Acomprehensive%20document%20representations.%20These%20models%20have%20achieved%0Astate-of-the-art%20performance%20across%20various%20downstream%20tasks%2C%20significantly%0Aenhancing%20the%20efficiency%20and%20accuracy%20of%20information%20extraction%20from%20VRDs.%20In%0Aresponse%20to%20the%20growing%20demands%20and%20rapid%20developments%20in%20Visually%20Rich%0ADocument%20Understanding%20%28VRDU%29%2C%20this%20paper%20provides%20a%20comprehensive%20review%20of%0Adeep%20learning-based%20VRDU%20frameworks.%20We%20systematically%20survey%20and%20analyze%0Aexisting%20methods%20and%20benchmark%20datasets%2C%20categorizing%20them%20based%20on%20adopted%0Astrategies%20and%20downstream%20tasks.%20Furthermore%2C%20we%20compare%20different%20techniques%0Aused%20in%20VRDU%20models%2C%20focusing%20on%20feature%20representation%20and%20fusion%2C%20model%0Aarchitecture%2C%20and%20pretraining%20methods%2C%20while%20highlighting%20their%20strengths%2C%0Alimitations%2C%20and%20appropriate%20scenarios.%20Finally%2C%20we%20identify%20emerging%20trends%0Aand%20challenges%20in%20VRDU%2C%20offering%20insights%20into%20future%20research%20directions%20and%0Apractical%20applications.%20This%20survey%20aims%20to%20provide%20a%20thorough%20understanding%20of%0AVRDU%20advancements%2C%20benefiting%20both%20academic%20and%20industrial%20sectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01287v1&entry.124074799=Read"},
{"title": "Underwater Object Detection Enhancement via Channel Stabilization", "author": "Muhammad Ali and Salman Khan", "abstract": "  The complex marine environment exacerbates the challenges of object detection\nmanifold. Marine trash endangers the aquatic ecosystem, presenting a persistent\nchallenge. Accurate detection of marine deposits is crucial for mitigating this\nharm. Our work addresses underwater object detection by enhancing image quality\nand evaluating detection methods. We use Detectron2's backbone with various\nbase models and configurations for this task.\n  We propose a novel channel stabilization technique alongside a simplified\nimage enhancement model to reduce haze and color cast in training images,\nimproving multi-scale object detection. Following image processing, we test\ndifferent Detectron2 backbones for optimal detection accuracy. Additionally, we\napply a sharpening filter with augmentation techniques to highlight object\nprofiles for easier recognition.\n  Results are demonstrated on the TrashCan Dataset, both instance and material\nversions. The best-performing backbone method incorporates our channel\nstabilization and augmentation techniques. We also compare our Detectron2\ndetection results with the Deformable Transformer. In the instance version of\nTrashCan 1.0, our method achieves a 9.53% absolute increase in average\nprecision for small objects and a 7% absolute gain in bounding box detection\ncompared to the baseline. The code will be available on Code:\nhttps://github.com/aliman80/Underwater-\nObject-Detection-via-Channel-Stablization\n", "link": "http://arxiv.org/abs/2408.01293v1", "date": "2024-08-02", "relevancy": 2.0536, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Underwater%20Object%20Detection%20Enhancement%20via%20Channel%20Stabilization&body=Title%3A%20Underwater%20Object%20Detection%20Enhancement%20via%20Channel%20Stabilization%0AAuthor%3A%20Muhammad%20Ali%20and%20Salman%20Khan%0AAbstract%3A%20%20%20The%20complex%20marine%20environment%20exacerbates%20the%20challenges%20of%20object%20detection%0Amanifold.%20Marine%20trash%20endangers%20the%20aquatic%20ecosystem%2C%20presenting%20a%20persistent%0Achallenge.%20Accurate%20detection%20of%20marine%20deposits%20is%20crucial%20for%20mitigating%20this%0Aharm.%20Our%20work%20addresses%20underwater%20object%20detection%20by%20enhancing%20image%20quality%0Aand%20evaluating%20detection%20methods.%20We%20use%20Detectron2%27s%20backbone%20with%20various%0Abase%20models%20and%20configurations%20for%20this%20task.%0A%20%20We%20propose%20a%20novel%20channel%20stabilization%20technique%20alongside%20a%20simplified%0Aimage%20enhancement%20model%20to%20reduce%20haze%20and%20color%20cast%20in%20training%20images%2C%0Aimproving%20multi-scale%20object%20detection.%20Following%20image%20processing%2C%20we%20test%0Adifferent%20Detectron2%20backbones%20for%20optimal%20detection%20accuracy.%20Additionally%2C%20we%0Aapply%20a%20sharpening%20filter%20with%20augmentation%20techniques%20to%20highlight%20object%0Aprofiles%20for%20easier%20recognition.%0A%20%20Results%20are%20demonstrated%20on%20the%20TrashCan%20Dataset%2C%20both%20instance%20and%20material%0Aversions.%20The%20best-performing%20backbone%20method%20incorporates%20our%20channel%0Astabilization%20and%20augmentation%20techniques.%20We%20also%20compare%20our%20Detectron2%0Adetection%20results%20with%20the%20Deformable%20Transformer.%20In%20the%20instance%20version%20of%0ATrashCan%201.0%2C%20our%20method%20achieves%20a%209.53%25%20absolute%20increase%20in%20average%0Aprecision%20for%20small%20objects%20and%20a%207%25%20absolute%20gain%20in%20bounding%20box%20detection%0Acompared%20to%20the%20baseline.%20The%20code%20will%20be%20available%20on%20Code%3A%0Ahttps%3A//github.com/aliman80/Underwater-%0AObject-Detection-via-Channel-Stablization%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderwater%2520Object%2520Detection%2520Enhancement%2520via%2520Channel%2520Stabilization%26entry.906535625%3DMuhammad%2520Ali%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520The%2520complex%2520marine%2520environment%2520exacerbates%2520the%2520challenges%2520of%2520object%2520detection%250Amanifold.%2520Marine%2520trash%2520endangers%2520the%2520aquatic%2520ecosystem%252C%2520presenting%2520a%2520persistent%250Achallenge.%2520Accurate%2520detection%2520of%2520marine%2520deposits%2520is%2520crucial%2520for%2520mitigating%2520this%250Aharm.%2520Our%2520work%2520addresses%2520underwater%2520object%2520detection%2520by%2520enhancing%2520image%2520quality%250Aand%2520evaluating%2520detection%2520methods.%2520We%2520use%2520Detectron2%2527s%2520backbone%2520with%2520various%250Abase%2520models%2520and%2520configurations%2520for%2520this%2520task.%250A%2520%2520We%2520propose%2520a%2520novel%2520channel%2520stabilization%2520technique%2520alongside%2520a%2520simplified%250Aimage%2520enhancement%2520model%2520to%2520reduce%2520haze%2520and%2520color%2520cast%2520in%2520training%2520images%252C%250Aimproving%2520multi-scale%2520object%2520detection.%2520Following%2520image%2520processing%252C%2520we%2520test%250Adifferent%2520Detectron2%2520backbones%2520for%2520optimal%2520detection%2520accuracy.%2520Additionally%252C%2520we%250Aapply%2520a%2520sharpening%2520filter%2520with%2520augmentation%2520techniques%2520to%2520highlight%2520object%250Aprofiles%2520for%2520easier%2520recognition.%250A%2520%2520Results%2520are%2520demonstrated%2520on%2520the%2520TrashCan%2520Dataset%252C%2520both%2520instance%2520and%2520material%250Aversions.%2520The%2520best-performing%2520backbone%2520method%2520incorporates%2520our%2520channel%250Astabilization%2520and%2520augmentation%2520techniques.%2520We%2520also%2520compare%2520our%2520Detectron2%250Adetection%2520results%2520with%2520the%2520Deformable%2520Transformer.%2520In%2520the%2520instance%2520version%2520of%250ATrashCan%25201.0%252C%2520our%2520method%2520achieves%2520a%25209.53%2525%2520absolute%2520increase%2520in%2520average%250Aprecision%2520for%2520small%2520objects%2520and%2520a%25207%2525%2520absolute%2520gain%2520in%2520bounding%2520box%2520detection%250Acompared%2520to%2520the%2520baseline.%2520The%2520code%2520will%2520be%2520available%2520on%2520Code%253A%250Ahttps%253A//github.com/aliman80/Underwater-%250AObject-Detection-via-Channel-Stablization%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underwater%20Object%20Detection%20Enhancement%20via%20Channel%20Stabilization&entry.906535625=Muhammad%20Ali%20and%20Salman%20Khan&entry.1292438233=%20%20The%20complex%20marine%20environment%20exacerbates%20the%20challenges%20of%20object%20detection%0Amanifold.%20Marine%20trash%20endangers%20the%20aquatic%20ecosystem%2C%20presenting%20a%20persistent%0Achallenge.%20Accurate%20detection%20of%20marine%20deposits%20is%20crucial%20for%20mitigating%20this%0Aharm.%20Our%20work%20addresses%20underwater%20object%20detection%20by%20enhancing%20image%20quality%0Aand%20evaluating%20detection%20methods.%20We%20use%20Detectron2%27s%20backbone%20with%20various%0Abase%20models%20and%20configurations%20for%20this%20task.%0A%20%20We%20propose%20a%20novel%20channel%20stabilization%20technique%20alongside%20a%20simplified%0Aimage%20enhancement%20model%20to%20reduce%20haze%20and%20color%20cast%20in%20training%20images%2C%0Aimproving%20multi-scale%20object%20detection.%20Following%20image%20processing%2C%20we%20test%0Adifferent%20Detectron2%20backbones%20for%20optimal%20detection%20accuracy.%20Additionally%2C%20we%0Aapply%20a%20sharpening%20filter%20with%20augmentation%20techniques%20to%20highlight%20object%0Aprofiles%20for%20easier%20recognition.%0A%20%20Results%20are%20demonstrated%20on%20the%20TrashCan%20Dataset%2C%20both%20instance%20and%20material%0Aversions.%20The%20best-performing%20backbone%20method%20incorporates%20our%20channel%0Astabilization%20and%20augmentation%20techniques.%20We%20also%20compare%20our%20Detectron2%0Adetection%20results%20with%20the%20Deformable%20Transformer.%20In%20the%20instance%20version%20of%0ATrashCan%201.0%2C%20our%20method%20achieves%20a%209.53%25%20absolute%20increase%20in%20average%0Aprecision%20for%20small%20objects%20and%20a%207%25%20absolute%20gain%20in%20bounding%20box%20detection%0Acompared%20to%20the%20baseline.%20The%20code%20will%20be%20available%20on%20Code%3A%0Ahttps%3A//github.com/aliman80/Underwater-%0AObject-Detection-via-Channel-Stablization%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01293v1&entry.124074799=Read"},
{"title": "Rethinking Pre-trained Feature Extractor Selection in Multiple Instance\n  Learning for Whole Slide Image Classification", "author": "Bryan Wong and Mun Yong Yi", "abstract": "  Multiple instance learning (MIL) has become a preferred method for\nclassifying gigapixel whole slide images (WSIs), without requiring patch label\nannotation. The focus of the current MIL research stream is on the\nembedding-based MIL approach, which involves extracting feature vectors from\npatches using a pre-trained feature extractor. These feature vectors are then\nfed into an MIL aggregator for slide-level prediction. Despite prior research\nsuggestions on enhancing the most commonly used ResNet50 supervised model\npre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting\nthe optimal feature extractor to maximize WSI performance. This study aims at\naddressing this gap by examining MIL feature extractors across three\ndimensions: pre-training dataset, backbone model, and pre-training method.\nExtensive experiments were carried out on the two public WSI datasets\n(TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings\nindicate the following: 1) Performance significantly improves with larger and\nmore varied pre-training datasets in both CNN and Transformer backbones. 2)\n`Modern and deeper' backbones greatly outperform `standard' backbones (ResNet\nand ViT), with performance improvements more guaranteed in Transformer-based\nbackbones. 3) The choice of self-supervised learning (SSL) method is crucial,\nwith the most significant benefits observed when applied to the Transformer\n(ViT) backbone. The study findings have practical implications, including\ndesigning more effective pathological foundation models. Our code is available\nat: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection\n", "link": "http://arxiv.org/abs/2408.01167v1", "date": "2024-08-02", "relevancy": 2.0454, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Pre-trained%20Feature%20Extractor%20Selection%20in%20Multiple%20Instance%0A%20%20Learning%20for%20Whole%20Slide%20Image%20Classification&body=Title%3A%20Rethinking%20Pre-trained%20Feature%20Extractor%20Selection%20in%20Multiple%20Instance%0A%20%20Learning%20for%20Whole%20Slide%20Image%20Classification%0AAuthor%3A%20Bryan%20Wong%20and%20Mun%20Yong%20Yi%0AAbstract%3A%20%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20become%20a%20preferred%20method%20for%0Aclassifying%20gigapixel%20whole%20slide%20images%20%28WSIs%29%2C%20without%20requiring%20patch%20label%0Aannotation.%20The%20focus%20of%20the%20current%20MIL%20research%20stream%20is%20on%20the%0Aembedding-based%20MIL%20approach%2C%20which%20involves%20extracting%20feature%20vectors%20from%0Apatches%20using%20a%20pre-trained%20feature%20extractor.%20These%20feature%20vectors%20are%20then%0Afed%20into%20an%20MIL%20aggregator%20for%20slide-level%20prediction.%20Despite%20prior%20research%0Asuggestions%20on%20enhancing%20the%20most%20commonly%20used%20ResNet50%20supervised%20model%0Apre-trained%20on%20ImageNet-1K%2C%20there%20remains%20a%20lack%20of%20clear%20guidance%20on%20selecting%0Athe%20optimal%20feature%20extractor%20to%20maximize%20WSI%20performance.%20This%20study%20aims%20at%0Aaddressing%20this%20gap%20by%20examining%20MIL%20feature%20extractors%20across%20three%0Adimensions%3A%20pre-training%20dataset%2C%20backbone%20model%2C%20and%20pre-training%20method.%0AExtensive%20experiments%20were%20carried%20out%20on%20the%20two%20public%20WSI%20datasets%0A%28TCGA-NSCLC%20and%20Camelyon16%29%20using%20four%20SOTA%20MIL%20models.%20The%20main%20findings%0Aindicate%20the%20following%3A%201%29%20Performance%20significantly%20improves%20with%20larger%20and%0Amore%20varied%20pre-training%20datasets%20in%20both%20CNN%20and%20Transformer%20backbones.%202%29%0A%60Modern%20and%20deeper%27%20backbones%20greatly%20outperform%20%60standard%27%20backbones%20%28ResNet%0Aand%20ViT%29%2C%20with%20performance%20improvements%20more%20guaranteed%20in%20Transformer-based%0Abackbones.%203%29%20The%20choice%20of%20self-supervised%20learning%20%28SSL%29%20method%20is%20crucial%2C%0Awith%20the%20most%20significant%20benefits%20observed%20when%20applied%20to%20the%20Transformer%0A%28ViT%29%20backbone.%20The%20study%20findings%20have%20practical%20implications%2C%20including%0Adesigning%20more%20effective%20pathological%20foundation%20models.%20Our%20code%20is%20available%0Aat%3A%20https%3A//anonymous.4open.science/r/MIL-Feature-Extractor-Selection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Pre-trained%2520Feature%2520Extractor%2520Selection%2520in%2520Multiple%2520Instance%250A%2520%2520Learning%2520for%2520Whole%2520Slide%2520Image%2520Classification%26entry.906535625%3DBryan%2520Wong%2520and%2520Mun%2520Yong%2520Yi%26entry.1292438233%3D%2520%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520has%2520become%2520a%2520preferred%2520method%2520for%250Aclassifying%2520gigapixel%2520whole%2520slide%2520images%2520%2528WSIs%2529%252C%2520without%2520requiring%2520patch%2520label%250Aannotation.%2520The%2520focus%2520of%2520the%2520current%2520MIL%2520research%2520stream%2520is%2520on%2520the%250Aembedding-based%2520MIL%2520approach%252C%2520which%2520involves%2520extracting%2520feature%2520vectors%2520from%250Apatches%2520using%2520a%2520pre-trained%2520feature%2520extractor.%2520These%2520feature%2520vectors%2520are%2520then%250Afed%2520into%2520an%2520MIL%2520aggregator%2520for%2520slide-level%2520prediction.%2520Despite%2520prior%2520research%250Asuggestions%2520on%2520enhancing%2520the%2520most%2520commonly%2520used%2520ResNet50%2520supervised%2520model%250Apre-trained%2520on%2520ImageNet-1K%252C%2520there%2520remains%2520a%2520lack%2520of%2520clear%2520guidance%2520on%2520selecting%250Athe%2520optimal%2520feature%2520extractor%2520to%2520maximize%2520WSI%2520performance.%2520This%2520study%2520aims%2520at%250Aaddressing%2520this%2520gap%2520by%2520examining%2520MIL%2520feature%2520extractors%2520across%2520three%250Adimensions%253A%2520pre-training%2520dataset%252C%2520backbone%2520model%252C%2520and%2520pre-training%2520method.%250AExtensive%2520experiments%2520were%2520carried%2520out%2520on%2520the%2520two%2520public%2520WSI%2520datasets%250A%2528TCGA-NSCLC%2520and%2520Camelyon16%2529%2520using%2520four%2520SOTA%2520MIL%2520models.%2520The%2520main%2520findings%250Aindicate%2520the%2520following%253A%25201%2529%2520Performance%2520significantly%2520improves%2520with%2520larger%2520and%250Amore%2520varied%2520pre-training%2520datasets%2520in%2520both%2520CNN%2520and%2520Transformer%2520backbones.%25202%2529%250A%2560Modern%2520and%2520deeper%2527%2520backbones%2520greatly%2520outperform%2520%2560standard%2527%2520backbones%2520%2528ResNet%250Aand%2520ViT%2529%252C%2520with%2520performance%2520improvements%2520more%2520guaranteed%2520in%2520Transformer-based%250Abackbones.%25203%2529%2520The%2520choice%2520of%2520self-supervised%2520learning%2520%2528SSL%2529%2520method%2520is%2520crucial%252C%250Awith%2520the%2520most%2520significant%2520benefits%2520observed%2520when%2520applied%2520to%2520the%2520Transformer%250A%2528ViT%2529%2520backbone.%2520The%2520study%2520findings%2520have%2520practical%2520implications%252C%2520including%250Adesigning%2520more%2520effective%2520pathological%2520foundation%2520models.%2520Our%2520code%2520is%2520available%250Aat%253A%2520https%253A//anonymous.4open.science/r/MIL-Feature-Extractor-Selection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Pre-trained%20Feature%20Extractor%20Selection%20in%20Multiple%20Instance%0A%20%20Learning%20for%20Whole%20Slide%20Image%20Classification&entry.906535625=Bryan%20Wong%20and%20Mun%20Yong%20Yi&entry.1292438233=%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20become%20a%20preferred%20method%20for%0Aclassifying%20gigapixel%20whole%20slide%20images%20%28WSIs%29%2C%20without%20requiring%20patch%20label%0Aannotation.%20The%20focus%20of%20the%20current%20MIL%20research%20stream%20is%20on%20the%0Aembedding-based%20MIL%20approach%2C%20which%20involves%20extracting%20feature%20vectors%20from%0Apatches%20using%20a%20pre-trained%20feature%20extractor.%20These%20feature%20vectors%20are%20then%0Afed%20into%20an%20MIL%20aggregator%20for%20slide-level%20prediction.%20Despite%20prior%20research%0Asuggestions%20on%20enhancing%20the%20most%20commonly%20used%20ResNet50%20supervised%20model%0Apre-trained%20on%20ImageNet-1K%2C%20there%20remains%20a%20lack%20of%20clear%20guidance%20on%20selecting%0Athe%20optimal%20feature%20extractor%20to%20maximize%20WSI%20performance.%20This%20study%20aims%20at%0Aaddressing%20this%20gap%20by%20examining%20MIL%20feature%20extractors%20across%20three%0Adimensions%3A%20pre-training%20dataset%2C%20backbone%20model%2C%20and%20pre-training%20method.%0AExtensive%20experiments%20were%20carried%20out%20on%20the%20two%20public%20WSI%20datasets%0A%28TCGA-NSCLC%20and%20Camelyon16%29%20using%20four%20SOTA%20MIL%20models.%20The%20main%20findings%0Aindicate%20the%20following%3A%201%29%20Performance%20significantly%20improves%20with%20larger%20and%0Amore%20varied%20pre-training%20datasets%20in%20both%20CNN%20and%20Transformer%20backbones.%202%29%0A%60Modern%20and%20deeper%27%20backbones%20greatly%20outperform%20%60standard%27%20backbones%20%28ResNet%0Aand%20ViT%29%2C%20with%20performance%20improvements%20more%20guaranteed%20in%20Transformer-based%0Abackbones.%203%29%20The%20choice%20of%20self-supervised%20learning%20%28SSL%29%20method%20is%20crucial%2C%0Awith%20the%20most%20significant%20benefits%20observed%20when%20applied%20to%20the%20Transformer%0A%28ViT%29%20backbone.%20The%20study%20findings%20have%20practical%20implications%2C%20including%0Adesigning%20more%20effective%20pathological%20foundation%20models.%20Our%20code%20is%20available%0Aat%3A%20https%3A//anonymous.4open.science/r/MIL-Feature-Extractor-Selection%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01167v1&entry.124074799=Read"},
{"title": "CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging", "author": "Sunny Gupta and Amit Sethi", "abstract": "  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n", "link": "http://arxiv.org/abs/2407.11652v3", "date": "2024-08-02", "relevancy": 2.0364, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5009}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging&body=Title%3A%20CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging%0AAuthor%3A%20Sunny%20Gupta%20and%20Amit%20Sethi%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20privacy-preserving%20approach%20to%20train%20models%0Aon%20decentralized%20data.%20Its%20potential%20in%20healthcare%20is%20significant%2C%20but%0Achallenges%20arise%20due%20to%20cross-client%20variations%20in%20medical%20image%20data%2C%0Aexacerbated%20by%20limited%20annotations.%20This%20paper%20introduces%20Cross-Client%0AVariations%20Adaptive%20Federated%20Learning%20%28CCVA-FL%29%20to%20address%20these%20issues.%0ACCVA-FL%20aims%20to%20minimize%20cross-client%20variations%20by%20transforming%20images%20into%20a%0Acommon%20feature%20space.%20It%20involves%20expert%20annotation%20of%20a%20subset%20of%20images%20from%0Aeach%20client%2C%20followed%20by%20the%20selection%20of%20a%20client%20with%20the%20least%20data%0Acomplexity%20as%20the%20target.%20Synthetic%20medical%20images%20are%20then%20generated%20using%0AScalable%20Diffusion%20Models%20with%20Transformers%20%28DiT%29%20based%20on%20the%20target%20client%27s%0Aannotated%20images.%20These%20synthetic%20images%2C%20capturing%20diversity%20and%20representing%0Athe%20original%20data%2C%20are%20shared%20with%20other%20clients.%20Each%20client%20then%20translates%0Aits%20local%20images%20into%20the%20target%20image%20space%20using%20image-to-image%20translation.%0AThe%20translated%20images%20are%20subsequently%20used%20in%20a%20federated%20learning%20setting%20to%0Adevelop%20a%20server%20model.%20Our%20results%20demonstrate%20that%20CCVA-FL%20outperforms%0AVanilla%20Federated%20Averaging%20by%20effectively%20addressing%20data%20distribution%0Adifferences%20across%20clients%20without%20compromising%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11652v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCVA-FL%253A%2520Cross-Client%2520Variations%2520Adaptive%2520Federated%2520Learning%2520for%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DSunny%2520Gupta%2520and%2520Amit%2520Sethi%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520privacy-preserving%2520approach%2520to%2520train%2520models%250Aon%2520decentralized%2520data.%2520Its%2520potential%2520in%2520healthcare%2520is%2520significant%252C%2520but%250Achallenges%2520arise%2520due%2520to%2520cross-client%2520variations%2520in%2520medical%2520image%2520data%252C%250Aexacerbated%2520by%2520limited%2520annotations.%2520This%2520paper%2520introduces%2520Cross-Client%250AVariations%2520Adaptive%2520Federated%2520Learning%2520%2528CCVA-FL%2529%2520to%2520address%2520these%2520issues.%250ACCVA-FL%2520aims%2520to%2520minimize%2520cross-client%2520variations%2520by%2520transforming%2520images%2520into%2520a%250Acommon%2520feature%2520space.%2520It%2520involves%2520expert%2520annotation%2520of%2520a%2520subset%2520of%2520images%2520from%250Aeach%2520client%252C%2520followed%2520by%2520the%2520selection%2520of%2520a%2520client%2520with%2520the%2520least%2520data%250Acomplexity%2520as%2520the%2520target.%2520Synthetic%2520medical%2520images%2520are%2520then%2520generated%2520using%250AScalable%2520Diffusion%2520Models%2520with%2520Transformers%2520%2528DiT%2529%2520based%2520on%2520the%2520target%2520client%2527s%250Aannotated%2520images.%2520These%2520synthetic%2520images%252C%2520capturing%2520diversity%2520and%2520representing%250Athe%2520original%2520data%252C%2520are%2520shared%2520with%2520other%2520clients.%2520Each%2520client%2520then%2520translates%250Aits%2520local%2520images%2520into%2520the%2520target%2520image%2520space%2520using%2520image-to-image%2520translation.%250AThe%2520translated%2520images%2520are%2520subsequently%2520used%2520in%2520a%2520federated%2520learning%2520setting%2520to%250Adevelop%2520a%2520server%2520model.%2520Our%2520results%2520demonstrate%2520that%2520CCVA-FL%2520outperforms%250AVanilla%2520Federated%2520Averaging%2520by%2520effectively%2520addressing%2520data%2520distribution%250Adifferences%2520across%2520clients%2520without%2520compromising%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11652v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCVA-FL%3A%20Cross-Client%20Variations%20Adaptive%20Federated%20Learning%20for%20Medical%0A%20%20Imaging&entry.906535625=Sunny%20Gupta%20and%20Amit%20Sethi&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20privacy-preserving%20approach%20to%20train%20models%0Aon%20decentralized%20data.%20Its%20potential%20in%20healthcare%20is%20significant%2C%20but%0Achallenges%20arise%20due%20to%20cross-client%20variations%20in%20medical%20image%20data%2C%0Aexacerbated%20by%20limited%20annotations.%20This%20paper%20introduces%20Cross-Client%0AVariations%20Adaptive%20Federated%20Learning%20%28CCVA-FL%29%20to%20address%20these%20issues.%0ACCVA-FL%20aims%20to%20minimize%20cross-client%20variations%20by%20transforming%20images%20into%20a%0Acommon%20feature%20space.%20It%20involves%20expert%20annotation%20of%20a%20subset%20of%20images%20from%0Aeach%20client%2C%20followed%20by%20the%20selection%20of%20a%20client%20with%20the%20least%20data%0Acomplexity%20as%20the%20target.%20Synthetic%20medical%20images%20are%20then%20generated%20using%0AScalable%20Diffusion%20Models%20with%20Transformers%20%28DiT%29%20based%20on%20the%20target%20client%27s%0Aannotated%20images.%20These%20synthetic%20images%2C%20capturing%20diversity%20and%20representing%0Athe%20original%20data%2C%20are%20shared%20with%20other%20clients.%20Each%20client%20then%20translates%0Aits%20local%20images%20into%20the%20target%20image%20space%20using%20image-to-image%20translation.%0AThe%20translated%20images%20are%20subsequently%20used%20in%20a%20federated%20learning%20setting%20to%0Adevelop%20a%20server%20model.%20Our%20results%20demonstrate%20that%20CCVA-FL%20outperforms%0AVanilla%20Federated%20Averaging%20by%20effectively%20addressing%20data%20distribution%0Adifferences%20across%20clients%20without%20compromising%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11652v3&entry.124074799=Read"},
{"title": "Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series", "author": "Raphael Azorin and Zied Ben Houidi and Massimo Gallo and Alessandro Finamore and Pietro Michiardi", "abstract": "  Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.\n", "link": "http://arxiv.org/abs/2406.15327v2", "date": "2024-08-02", "relevancy": 2.0246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5174}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5098}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series&body=Title%3A%20Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series%0AAuthor%3A%20Raphael%20Azorin%20and%20Zied%20Ben%20Houidi%20and%20Massimo%20Gallo%20and%20Alessandro%20Finamore%20and%20Pietro%20Michiardi%0AAbstract%3A%20%20%20Tabular%20data%20is%20ubiquitous%20in%20many%20real-life%20systems.%20In%20particular%2C%0Atime-dependent%20tabular%20data%2C%20where%20rows%20are%20chronologically%20related%2C%20is%0Atypically%20used%20for%20recording%20historical%20events%2C%20e.g.%2C%20financial%20transactions%2C%0Ahealthcare%20records%2C%20or%20stock%20history.%20Recently%2C%20hierarchical%20variants%20of%20the%0Aattention%20mechanism%20of%20transformer%20architectures%20have%20been%20used%20to%20model%0Atabular%20time-series%20data.%20At%20first%2C%20rows%20%28or%20columns%29%20are%20encoded%20separately%20by%0Acomputing%20attention%20between%20their%20fields.%20Subsequently%2C%20encoded%20rows%20%28or%0Acolumns%29%20are%20attended%20to%20one%20another%20to%20model%20the%20entire%20tabular%20time-series.%0AWhile%20efficient%2C%20this%20approach%20constrains%20the%20attention%20granularity%20and%20limits%0Aits%20ability%20to%20learn%20patterns%20at%20the%20field-level%20across%20separate%20rows%2C%20or%0Acolumns.%20We%20take%20a%20first%20step%20to%20address%20this%20gap%20by%20proposing%20Fieldy%2C%20a%0Afine-grained%20hierarchical%20model%20that%20contextualizes%20fields%20at%20both%20the%20row%20and%0Acolumn%20levels.%20We%20compare%20our%20proposal%20against%20state%20of%20the%20art%20models%20on%0Aregression%20and%20classification%20tasks%20using%20public%20tabular%20time-series%20datasets.%0AOur%20results%20show%20that%20combining%20row-wise%20and%20column-wise%20attention%20improves%0Aperformance%20without%20increasing%20model%20size.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/raphaaal/fieldy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Attention%2520in%2520Hierarchical%2520Transformers%2520for%2520Tabular%250A%2520%2520Time-series%26entry.906535625%3DRaphael%2520Azorin%2520and%2520Zied%2520Ben%2520Houidi%2520and%2520Massimo%2520Gallo%2520and%2520Alessandro%2520Finamore%2520and%2520Pietro%2520Michiardi%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520ubiquitous%2520in%2520many%2520real-life%2520systems.%2520In%2520particular%252C%250Atime-dependent%2520tabular%2520data%252C%2520where%2520rows%2520are%2520chronologically%2520related%252C%2520is%250Atypically%2520used%2520for%2520recording%2520historical%2520events%252C%2520e.g.%252C%2520financial%2520transactions%252C%250Ahealthcare%2520records%252C%2520or%2520stock%2520history.%2520Recently%252C%2520hierarchical%2520variants%2520of%2520the%250Aattention%2520mechanism%2520of%2520transformer%2520architectures%2520have%2520been%2520used%2520to%2520model%250Atabular%2520time-series%2520data.%2520At%2520first%252C%2520rows%2520%2528or%2520columns%2529%2520are%2520encoded%2520separately%2520by%250Acomputing%2520attention%2520between%2520their%2520fields.%2520Subsequently%252C%2520encoded%2520rows%2520%2528or%250Acolumns%2529%2520are%2520attended%2520to%2520one%2520another%2520to%2520model%2520the%2520entire%2520tabular%2520time-series.%250AWhile%2520efficient%252C%2520this%2520approach%2520constrains%2520the%2520attention%2520granularity%2520and%2520limits%250Aits%2520ability%2520to%2520learn%2520patterns%2520at%2520the%2520field-level%2520across%2520separate%2520rows%252C%2520or%250Acolumns.%2520We%2520take%2520a%2520first%2520step%2520to%2520address%2520this%2520gap%2520by%2520proposing%2520Fieldy%252C%2520a%250Afine-grained%2520hierarchical%2520model%2520that%2520contextualizes%2520fields%2520at%2520both%2520the%2520row%2520and%250Acolumn%2520levels.%2520We%2520compare%2520our%2520proposal%2520against%2520state%2520of%2520the%2520art%2520models%2520on%250Aregression%2520and%2520classification%2520tasks%2520using%2520public%2520tabular%2520time-series%2520datasets.%250AOur%2520results%2520show%2520that%2520combining%2520row-wise%2520and%2520column-wise%2520attention%2520improves%250Aperformance%2520without%2520increasing%2520model%2520size.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/raphaaal/fieldy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Attention%20in%20Hierarchical%20Transformers%20for%20Tabular%0A%20%20Time-series&entry.906535625=Raphael%20Azorin%20and%20Zied%20Ben%20Houidi%20and%20Massimo%20Gallo%20and%20Alessandro%20Finamore%20and%20Pietro%20Michiardi&entry.1292438233=%20%20Tabular%20data%20is%20ubiquitous%20in%20many%20real-life%20systems.%20In%20particular%2C%0Atime-dependent%20tabular%20data%2C%20where%20rows%20are%20chronologically%20related%2C%20is%0Atypically%20used%20for%20recording%20historical%20events%2C%20e.g.%2C%20financial%20transactions%2C%0Ahealthcare%20records%2C%20or%20stock%20history.%20Recently%2C%20hierarchical%20variants%20of%20the%0Aattention%20mechanism%20of%20transformer%20architectures%20have%20been%20used%20to%20model%0Atabular%20time-series%20data.%20At%20first%2C%20rows%20%28or%20columns%29%20are%20encoded%20separately%20by%0Acomputing%20attention%20between%20their%20fields.%20Subsequently%2C%20encoded%20rows%20%28or%0Acolumns%29%20are%20attended%20to%20one%20another%20to%20model%20the%20entire%20tabular%20time-series.%0AWhile%20efficient%2C%20this%20approach%20constrains%20the%20attention%20granularity%20and%20limits%0Aits%20ability%20to%20learn%20patterns%20at%20the%20field-level%20across%20separate%20rows%2C%20or%0Acolumns.%20We%20take%20a%20first%20step%20to%20address%20this%20gap%20by%20proposing%20Fieldy%2C%20a%0Afine-grained%20hierarchical%20model%20that%20contextualizes%20fields%20at%20both%20the%20row%20and%0Acolumn%20levels.%20We%20compare%20our%20proposal%20against%20state%20of%20the%20art%20models%20on%0Aregression%20and%20classification%20tasks%20using%20public%20tabular%20time-series%20datasets.%0AOur%20results%20show%20that%20combining%20row-wise%20and%20column-wise%20attention%20improves%0Aperformance%20without%20increasing%20model%20size.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/raphaaal/fieldy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15327v2&entry.124074799=Read"},
{"title": "Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Hamad Ahmed Altuwaijri and Manual Mazzara and Salvatore Distenano", "abstract": "  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets.\n", "link": "http://arxiv.org/abs/2408.01224v1", "date": "2024-08-02", "relevancy": 2.0139, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5221}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano%0AAbstract%3A%20%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-head%2520Spatial-Spectral%2520Mamba%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Hamad%2520Ahmed%2520Altuwaijri%2520and%2520Manual%2520Mazzara%2520and%2520Salvatore%2520Distenano%26entry.1292438233%3D%2520%2520Spatial-Spectral%2520Mamba%2520%2528SSM%2529%2520improves%2520computational%2520efficiency%2520and%2520captures%250Along-range%2520dependencies%252C%2520addressing%2520Transformer%2520limitations.%2520However%252C%250Atraditional%2520Mamba%2520models%2520overlook%2520rich%2520spectral%2520information%2520in%2520HSIs%2520and%250Astruggle%2520with%2520high%2520dimensionality%2520and%2520sequential%2520data.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520the%2520SSM%2520with%2520multi-head%2520self-attention%2520and%2520token%2520enhancement%250A%2528MHSSMamba%2529.%2520This%2520model%2520integrates%2520spectral%2520and%2520spatial%2520information%2520by%250Aenhancing%2520spectral%2520tokens%2520and%2520using%2520multi-head%2520attention%2520to%2520capture%2520complex%250Arelationships%2520between%2520spectral%2520bands%2520and%2520spatial%2520locations.%2520It%2520also%2520manages%250Along-range%2520dependencies%2520and%2520the%2520sequential%2520nature%2520of%2520HSI%2520data%252C%2520preserving%250Acontextual%2520information%2520across%2520spectral%2520bands.%2520MHSSMamba%2520achieved%2520remarkable%250Aclassification%2520accuracies%2520of%252097.62%255C%2525%2520on%2520Pavia%2520University%252C%252096.92%255C%2525%2520on%2520the%250AUniversity%2520of%2520Houston%252C%252096.85%255C%2525%2520on%2520Salinas%252C%2520and%252099.49%255C%2525%2520on%2520Wuhan-longKou%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano&entry.1292438233=%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01224v1&entry.124074799=Read"},
{"title": "A Weakly Supervised and Globally Explainable Learning Framework for\n  Brain Tumor Segmentation", "author": "Ruitao Xie and Limai Jiang and Xiaoxi He and Yi Pan and Yunpeng Cai", "abstract": "  Machine-based brain tumor segmentation can help doctors make better\ndiagnoses. However, the complex structure of brain tumors and expensive\npixel-level annotations present challenges for automatic tumor segmentation. In\nthis paper, we propose a counterfactual generation framework that not only\nachieves exceptional brain tumor segmentation performance without the need for\npixel-level annotations, but also provides explainability. Our framework\neffectively separates class-related features from class-unrelated features of\nthe samples, and generate new samples that preserve identity features while\naltering class attributes by embedding different class-related features. We\nperform topological data analysis on the extracted class-related features and\nobtain a globally explainable manifold, and for each abnormal sample to be\nsegmented, a meaningful normal sample could be effectively generated with the\nguidance of the rule-based paths designed within the manifold for comparison\nfor identifying the tumor regions. We evaluate our proposed method on two\ndatasets, which demonstrates superior performance of brain tumor segmentation.\nThe code is available at https://github.com/xrt11/tumor-segmentation.\n", "link": "http://arxiv.org/abs/2408.01191v1", "date": "2024-08-02", "relevancy": 2.0065, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5131}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Weakly%20Supervised%20and%20Globally%20Explainable%20Learning%20Framework%20for%0A%20%20Brain%20Tumor%20Segmentation&body=Title%3A%20A%20Weakly%20Supervised%20and%20Globally%20Explainable%20Learning%20Framework%20for%0A%20%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Ruitao%20Xie%20and%20Limai%20Jiang%20and%20Xiaoxi%20He%20and%20Yi%20Pan%20and%20Yunpeng%20Cai%0AAbstract%3A%20%20%20Machine-based%20brain%20tumor%20segmentation%20can%20help%20doctors%20make%20better%0Adiagnoses.%20However%2C%20the%20complex%20structure%20of%20brain%20tumors%20and%20expensive%0Apixel-level%20annotations%20present%20challenges%20for%20automatic%20tumor%20segmentation.%20In%0Athis%20paper%2C%20we%20propose%20a%20counterfactual%20generation%20framework%20that%20not%20only%0Aachieves%20exceptional%20brain%20tumor%20segmentation%20performance%20without%20the%20need%20for%0Apixel-level%20annotations%2C%20but%20also%20provides%20explainability.%20Our%20framework%0Aeffectively%20separates%20class-related%20features%20from%20class-unrelated%20features%20of%0Athe%20samples%2C%20and%20generate%20new%20samples%20that%20preserve%20identity%20features%20while%0Aaltering%20class%20attributes%20by%20embedding%20different%20class-related%20features.%20We%0Aperform%20topological%20data%20analysis%20on%20the%20extracted%20class-related%20features%20and%0Aobtain%20a%20globally%20explainable%20manifold%2C%20and%20for%20each%20abnormal%20sample%20to%20be%0Asegmented%2C%20a%20meaningful%20normal%20sample%20could%20be%20effectively%20generated%20with%20the%0Aguidance%20of%20the%20rule-based%20paths%20designed%20within%20the%20manifold%20for%20comparison%0Afor%20identifying%20the%20tumor%20regions.%20We%20evaluate%20our%20proposed%20method%20on%20two%0Adatasets%2C%20which%20demonstrates%20superior%20performance%20of%20brain%20tumor%20segmentation.%0AThe%20code%20is%20available%20at%20https%3A//github.com/xrt11/tumor-segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Weakly%2520Supervised%2520and%2520Globally%2520Explainable%2520Learning%2520Framework%2520for%250A%2520%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DRuitao%2520Xie%2520and%2520Limai%2520Jiang%2520and%2520Xiaoxi%2520He%2520and%2520Yi%2520Pan%2520and%2520Yunpeng%2520Cai%26entry.1292438233%3D%2520%2520Machine-based%2520brain%2520tumor%2520segmentation%2520can%2520help%2520doctors%2520make%2520better%250Adiagnoses.%2520However%252C%2520the%2520complex%2520structure%2520of%2520brain%2520tumors%2520and%2520expensive%250Apixel-level%2520annotations%2520present%2520challenges%2520for%2520automatic%2520tumor%2520segmentation.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520counterfactual%2520generation%2520framework%2520that%2520not%2520only%250Aachieves%2520exceptional%2520brain%2520tumor%2520segmentation%2520performance%2520without%2520the%2520need%2520for%250Apixel-level%2520annotations%252C%2520but%2520also%2520provides%2520explainability.%2520Our%2520framework%250Aeffectively%2520separates%2520class-related%2520features%2520from%2520class-unrelated%2520features%2520of%250Athe%2520samples%252C%2520and%2520generate%2520new%2520samples%2520that%2520preserve%2520identity%2520features%2520while%250Aaltering%2520class%2520attributes%2520by%2520embedding%2520different%2520class-related%2520features.%2520We%250Aperform%2520topological%2520data%2520analysis%2520on%2520the%2520extracted%2520class-related%2520features%2520and%250Aobtain%2520a%2520globally%2520explainable%2520manifold%252C%2520and%2520for%2520each%2520abnormal%2520sample%2520to%2520be%250Asegmented%252C%2520a%2520meaningful%2520normal%2520sample%2520could%2520be%2520effectively%2520generated%2520with%2520the%250Aguidance%2520of%2520the%2520rule-based%2520paths%2520designed%2520within%2520the%2520manifold%2520for%2520comparison%250Afor%2520identifying%2520the%2520tumor%2520regions.%2520We%2520evaluate%2520our%2520proposed%2520method%2520on%2520two%250Adatasets%252C%2520which%2520demonstrates%2520superior%2520performance%2520of%2520brain%2520tumor%2520segmentation.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/xrt11/tumor-segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Weakly%20Supervised%20and%20Globally%20Explainable%20Learning%20Framework%20for%0A%20%20Brain%20Tumor%20Segmentation&entry.906535625=Ruitao%20Xie%20and%20Limai%20Jiang%20and%20Xiaoxi%20He%20and%20Yi%20Pan%20and%20Yunpeng%20Cai&entry.1292438233=%20%20Machine-based%20brain%20tumor%20segmentation%20can%20help%20doctors%20make%20better%0Adiagnoses.%20However%2C%20the%20complex%20structure%20of%20brain%20tumors%20and%20expensive%0Apixel-level%20annotations%20present%20challenges%20for%20automatic%20tumor%20segmentation.%20In%0Athis%20paper%2C%20we%20propose%20a%20counterfactual%20generation%20framework%20that%20not%20only%0Aachieves%20exceptional%20brain%20tumor%20segmentation%20performance%20without%20the%20need%20for%0Apixel-level%20annotations%2C%20but%20also%20provides%20explainability.%20Our%20framework%0Aeffectively%20separates%20class-related%20features%20from%20class-unrelated%20features%20of%0Athe%20samples%2C%20and%20generate%20new%20samples%20that%20preserve%20identity%20features%20while%0Aaltering%20class%20attributes%20by%20embedding%20different%20class-related%20features.%20We%0Aperform%20topological%20data%20analysis%20on%20the%20extracted%20class-related%20features%20and%0Aobtain%20a%20globally%20explainable%20manifold%2C%20and%20for%20each%20abnormal%20sample%20to%20be%0Asegmented%2C%20a%20meaningful%20normal%20sample%20could%20be%20effectively%20generated%20with%20the%0Aguidance%20of%20the%20rule-based%20paths%20designed%20within%20the%20manifold%20for%20comparison%0Afor%20identifying%20the%20tumor%20regions.%20We%20evaluate%20our%20proposed%20method%20on%20two%0Adatasets%2C%20which%20demonstrates%20superior%20performance%20of%20brain%20tumor%20segmentation.%0AThe%20code%20is%20available%20at%20https%3A//github.com/xrt11/tumor-segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01191v1&entry.124074799=Read"},
{"title": "Certified Robust Invariant Polytope Training in Neural Controlled ODEs", "author": "Akash Harapanahalli and Samuel Coogan", "abstract": "  We consider a nonlinear control system modeled as an ordinary differential\nequation subject to disturbance, with a state feedback controller parameterized\nas a feedforward neural network. We propose a framework for training\ncontrollers with certified robust forward invariant polytopes, where any\ntrajectory initialized inside the polytope remains within the polytope,\nregardless of the disturbance. First, we parameterize a family of lifted\ncontrol systems in a higher dimensional space, where the original neural\ncontrolled system evolves on an invariant subspace of each lifted system. We\nuse interval analysis and neural network verifiers to further construct a\nfamily of lifted embedding systems, carefully capturing the knowledge of this\ninvariant subspace. If the vector field of any lifted embedding system\nsatisfies a sign constraint at a single point, then a certain convex polytope\nof the original system is robustly forward invariant. Treating the neural\nnetwork controller and the lifted system parameters as variables, we propose an\nalgorithm to train controllers with certified forward invariant polytopes in\nthe closed-loop control system. Through two examples, we demonstrate how the\nsimplicity of the sign constraint allows our approach to scale with system\ndimension to over $50$ states, and outperform state-of-the-art Lyapunov-based\nsampling approaches in runtime.\n", "link": "http://arxiv.org/abs/2408.01273v1", "date": "2024-08-02", "relevancy": 2.0059, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4897}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Robust%20Invariant%20Polytope%20Training%20in%20Neural%20Controlled%20ODEs&body=Title%3A%20Certified%20Robust%20Invariant%20Polytope%20Training%20in%20Neural%20Controlled%20ODEs%0AAuthor%3A%20Akash%20Harapanahalli%20and%20Samuel%20Coogan%0AAbstract%3A%20%20%20We%20consider%20a%20nonlinear%20control%20system%20modeled%20as%20an%20ordinary%20differential%0Aequation%20subject%20to%20disturbance%2C%20with%20a%20state%20feedback%20controller%20parameterized%0Aas%20a%20feedforward%20neural%20network.%20We%20propose%20a%20framework%20for%20training%0Acontrollers%20with%20certified%20robust%20forward%20invariant%20polytopes%2C%20where%20any%0Atrajectory%20initialized%20inside%20the%20polytope%20remains%20within%20the%20polytope%2C%0Aregardless%20of%20the%20disturbance.%20First%2C%20we%20parameterize%20a%20family%20of%20lifted%0Acontrol%20systems%20in%20a%20higher%20dimensional%20space%2C%20where%20the%20original%20neural%0Acontrolled%20system%20evolves%20on%20an%20invariant%20subspace%20of%20each%20lifted%20system.%20We%0Ause%20interval%20analysis%20and%20neural%20network%20verifiers%20to%20further%20construct%20a%0Afamily%20of%20lifted%20embedding%20systems%2C%20carefully%20capturing%20the%20knowledge%20of%20this%0Ainvariant%20subspace.%20If%20the%20vector%20field%20of%20any%20lifted%20embedding%20system%0Asatisfies%20a%20sign%20constraint%20at%20a%20single%20point%2C%20then%20a%20certain%20convex%20polytope%0Aof%20the%20original%20system%20is%20robustly%20forward%20invariant.%20Treating%20the%20neural%0Anetwork%20controller%20and%20the%20lifted%20system%20parameters%20as%20variables%2C%20we%20propose%20an%0Aalgorithm%20to%20train%20controllers%20with%20certified%20forward%20invariant%20polytopes%20in%0Athe%20closed-loop%20control%20system.%20Through%20two%20examples%2C%20we%20demonstrate%20how%20the%0Asimplicity%20of%20the%20sign%20constraint%20allows%20our%20approach%20to%20scale%20with%20system%0Adimension%20to%20over%20%2450%24%20states%2C%20and%20outperform%20state-of-the-art%20Lyapunov-based%0Asampling%20approaches%20in%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Robust%2520Invariant%2520Polytope%2520Training%2520in%2520Neural%2520Controlled%2520ODEs%26entry.906535625%3DAkash%2520Harapanahalli%2520and%2520Samuel%2520Coogan%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520nonlinear%2520control%2520system%2520modeled%2520as%2520an%2520ordinary%2520differential%250Aequation%2520subject%2520to%2520disturbance%252C%2520with%2520a%2520state%2520feedback%2520controller%2520parameterized%250Aas%2520a%2520feedforward%2520neural%2520network.%2520We%2520propose%2520a%2520framework%2520for%2520training%250Acontrollers%2520with%2520certified%2520robust%2520forward%2520invariant%2520polytopes%252C%2520where%2520any%250Atrajectory%2520initialized%2520inside%2520the%2520polytope%2520remains%2520within%2520the%2520polytope%252C%250Aregardless%2520of%2520the%2520disturbance.%2520First%252C%2520we%2520parameterize%2520a%2520family%2520of%2520lifted%250Acontrol%2520systems%2520in%2520a%2520higher%2520dimensional%2520space%252C%2520where%2520the%2520original%2520neural%250Acontrolled%2520system%2520evolves%2520on%2520an%2520invariant%2520subspace%2520of%2520each%2520lifted%2520system.%2520We%250Ause%2520interval%2520analysis%2520and%2520neural%2520network%2520verifiers%2520to%2520further%2520construct%2520a%250Afamily%2520of%2520lifted%2520embedding%2520systems%252C%2520carefully%2520capturing%2520the%2520knowledge%2520of%2520this%250Ainvariant%2520subspace.%2520If%2520the%2520vector%2520field%2520of%2520any%2520lifted%2520embedding%2520system%250Asatisfies%2520a%2520sign%2520constraint%2520at%2520a%2520single%2520point%252C%2520then%2520a%2520certain%2520convex%2520polytope%250Aof%2520the%2520original%2520system%2520is%2520robustly%2520forward%2520invariant.%2520Treating%2520the%2520neural%250Anetwork%2520controller%2520and%2520the%2520lifted%2520system%2520parameters%2520as%2520variables%252C%2520we%2520propose%2520an%250Aalgorithm%2520to%2520train%2520controllers%2520with%2520certified%2520forward%2520invariant%2520polytopes%2520in%250Athe%2520closed-loop%2520control%2520system.%2520Through%2520two%2520examples%252C%2520we%2520demonstrate%2520how%2520the%250Asimplicity%2520of%2520the%2520sign%2520constraint%2520allows%2520our%2520approach%2520to%2520scale%2520with%2520system%250Adimension%2520to%2520over%2520%252450%2524%2520states%252C%2520and%2520outperform%2520state-of-the-art%2520Lyapunov-based%250Asampling%2520approaches%2520in%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Robust%20Invariant%20Polytope%20Training%20in%20Neural%20Controlled%20ODEs&entry.906535625=Akash%20Harapanahalli%20and%20Samuel%20Coogan&entry.1292438233=%20%20We%20consider%20a%20nonlinear%20control%20system%20modeled%20as%20an%20ordinary%20differential%0Aequation%20subject%20to%20disturbance%2C%20with%20a%20state%20feedback%20controller%20parameterized%0Aas%20a%20feedforward%20neural%20network.%20We%20propose%20a%20framework%20for%20training%0Acontrollers%20with%20certified%20robust%20forward%20invariant%20polytopes%2C%20where%20any%0Atrajectory%20initialized%20inside%20the%20polytope%20remains%20within%20the%20polytope%2C%0Aregardless%20of%20the%20disturbance.%20First%2C%20we%20parameterize%20a%20family%20of%20lifted%0Acontrol%20systems%20in%20a%20higher%20dimensional%20space%2C%20where%20the%20original%20neural%0Acontrolled%20system%20evolves%20on%20an%20invariant%20subspace%20of%20each%20lifted%20system.%20We%0Ause%20interval%20analysis%20and%20neural%20network%20verifiers%20to%20further%20construct%20a%0Afamily%20of%20lifted%20embedding%20systems%2C%20carefully%20capturing%20the%20knowledge%20of%20this%0Ainvariant%20subspace.%20If%20the%20vector%20field%20of%20any%20lifted%20embedding%20system%0Asatisfies%20a%20sign%20constraint%20at%20a%20single%20point%2C%20then%20a%20certain%20convex%20polytope%0Aof%20the%20original%20system%20is%20robustly%20forward%20invariant.%20Treating%20the%20neural%0Anetwork%20controller%20and%20the%20lifted%20system%20parameters%20as%20variables%2C%20we%20propose%20an%0Aalgorithm%20to%20train%20controllers%20with%20certified%20forward%20invariant%20polytopes%20in%0Athe%20closed-loop%20control%20system.%20Through%20two%20examples%2C%20we%20demonstrate%20how%20the%0Asimplicity%20of%20the%20sign%20constraint%20allows%20our%20approach%20to%20scale%20with%20system%0Adimension%20to%20over%20%2450%24%20states%2C%20and%20outperform%20state-of-the-art%20Lyapunov-based%0Asampling%20approaches%20in%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01273v1&entry.124074799=Read"},
{"title": "Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities", "author": "Pablo Galv\u00e1n and Filip Lemic and Gerard Calvo Bartra and Sergi Abadal and Xavier Costa P\u00e9rez", "abstract": "  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n", "link": "http://arxiv.org/abs/2408.01239v1", "date": "2024-08-02", "relevancy": 2.0054, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities&body=Title%3A%20Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities%0AAuthor%3A%20Pablo%20Galv%C3%A1n%20and%20Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20P%C3%A9rez%0AAbstract%3A%20%20%20Flow-guided%20localization%20using%20in-body%20nanodevices%20in%20the%20bloodstream%20is%0Aexpected%20to%20be%20beneficial%20for%20early%20disease%20detection%2C%20continuous%20monitoring%20of%0Abiological%20conditions%2C%20and%20targeted%20treatment.%20The%20nanodevices%20face%20size%20and%0Apower%20constraints%20that%20produce%20erroneous%20raw%20data%20for%20localization%20purposes.%0AOn-body%20anchors%20receive%20this%20data%2C%20and%20use%20it%20to%20derive%20the%20locations%20of%0Adiagnostic%20events%20of%20interest.%20Different%20Machine%20Learning%20%28ML%29%20approaches%20have%0Abeen%20recently%20proposed%20for%20this%20task%2C%20yet%20they%20are%20currently%20restricted%20to%20a%0Areference%20bloodstream%20of%20a%20resting%20patient.%20As%20such%2C%20they%20are%20unable%20to%20deal%0Awith%20the%20physical%20diversity%20of%20patients%27%20bloodstreams%20and%20cannot%20provide%0Acontinuous%20monitoring%20due%20to%20changes%20in%20individual%20patient%27s%20activities.%20Toward%0Aaddressing%20these%20issues%20for%20the%20current%20State-of-the-Art%20%28SotA%29%20flow-guided%0Alocalization%20approach%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%0Apipeline%20for%20GNN%20adaptation%20based%20on%20individual%20physiological%20indicators%0Aincluding%20height%2C%20weight%2C%20and%20heart%20rate.%20Our%20results%20indicate%20that%20the%0Aproposed%20adaptions%20are%20beneficial%20in%20reconciling%20the%20individual%20differences%0Abetween%20bloodstreams%20and%20activities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Graph%2520Neural%2520Network-based%2520Flow-guided%2520Localization%2520to%250A%2520%2520Individual%2520Bloodstreams%2520and%2520Activities%26entry.906535625%3DPablo%2520Galv%25C3%25A1n%2520and%2520Filip%2520Lemic%2520and%2520Gerard%2520Calvo%2520Bartra%2520and%2520Sergi%2520Abadal%2520and%2520Xavier%2520Costa%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520Flow-guided%2520localization%2520using%2520in-body%2520nanodevices%2520in%2520the%2520bloodstream%2520is%250Aexpected%2520to%2520be%2520beneficial%2520for%2520early%2520disease%2520detection%252C%2520continuous%2520monitoring%2520of%250Abiological%2520conditions%252C%2520and%2520targeted%2520treatment.%2520The%2520nanodevices%2520face%2520size%2520and%250Apower%2520constraints%2520that%2520produce%2520erroneous%2520raw%2520data%2520for%2520localization%2520purposes.%250AOn-body%2520anchors%2520receive%2520this%2520data%252C%2520and%2520use%2520it%2520to%2520derive%2520the%2520locations%2520of%250Adiagnostic%2520events%2520of%2520interest.%2520Different%2520Machine%2520Learning%2520%2528ML%2529%2520approaches%2520have%250Abeen%2520recently%2520proposed%2520for%2520this%2520task%252C%2520yet%2520they%2520are%2520currently%2520restricted%2520to%2520a%250Areference%2520bloodstream%2520of%2520a%2520resting%2520patient.%2520As%2520such%252C%2520they%2520are%2520unable%2520to%2520deal%250Awith%2520the%2520physical%2520diversity%2520of%2520patients%2527%2520bloodstreams%2520and%2520cannot%2520provide%250Acontinuous%2520monitoring%2520due%2520to%2520changes%2520in%2520individual%2520patient%2527s%2520activities.%2520Toward%250Aaddressing%2520these%2520issues%2520for%2520the%2520current%2520State-of-the-Art%2520%2528SotA%2529%2520flow-guided%250Alocalization%2520approach%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520we%2520propose%2520a%250Apipeline%2520for%2520GNN%2520adaptation%2520based%2520on%2520individual%2520physiological%2520indicators%250Aincluding%2520height%252C%2520weight%252C%2520and%2520heart%2520rate.%2520Our%2520results%2520indicate%2520that%2520the%250Aproposed%2520adaptions%2520are%2520beneficial%2520in%2520reconciling%2520the%2520individual%2520differences%250Abetween%2520bloodstreams%2520and%2520activities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities&entry.906535625=Pablo%20Galv%C3%A1n%20and%20Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20P%C3%A9rez&entry.1292438233=%20%20Flow-guided%20localization%20using%20in-body%20nanodevices%20in%20the%20bloodstream%20is%0Aexpected%20to%20be%20beneficial%20for%20early%20disease%20detection%2C%20continuous%20monitoring%20of%0Abiological%20conditions%2C%20and%20targeted%20treatment.%20The%20nanodevices%20face%20size%20and%0Apower%20constraints%20that%20produce%20erroneous%20raw%20data%20for%20localization%20purposes.%0AOn-body%20anchors%20receive%20this%20data%2C%20and%20use%20it%20to%20derive%20the%20locations%20of%0Adiagnostic%20events%20of%20interest.%20Different%20Machine%20Learning%20%28ML%29%20approaches%20have%0Abeen%20recently%20proposed%20for%20this%20task%2C%20yet%20they%20are%20currently%20restricted%20to%20a%0Areference%20bloodstream%20of%20a%20resting%20patient.%20As%20such%2C%20they%20are%20unable%20to%20deal%0Awith%20the%20physical%20diversity%20of%20patients%27%20bloodstreams%20and%20cannot%20provide%0Acontinuous%20monitoring%20due%20to%20changes%20in%20individual%20patient%27s%20activities.%20Toward%0Aaddressing%20these%20issues%20for%20the%20current%20State-of-the-Art%20%28SotA%29%20flow-guided%0Alocalization%20approach%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%0Apipeline%20for%20GNN%20adaptation%20based%20on%20individual%20physiological%20indicators%0Aincluding%20height%2C%20weight%2C%20and%20heart%20rate.%20Our%20results%20indicate%20that%20the%0Aproposed%20adaptions%20are%20beneficial%20in%20reconciling%20the%20individual%20differences%0Abetween%20bloodstreams%20and%20activities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01239v1&entry.124074799=Read"},
{"title": "The virtual CAT: A tool for algorithmic thinking assessment in Swiss\n  compulsory education", "author": "Giorgia Adorni and Alberto Piatti", "abstract": "  In today's digital era, holding algorithmic thinking (AT) skills is crucial,\nnot only in computer science-related fields. These abilities enable individuals\nto break down complex problems into more manageable steps and create a sequence\nof actions to solve them. To address the increasing demand for AT assessments\nin educational settings and the limitations of current methods, this paper\nintroduces the virtual Cross Array Task (CAT), a digital adaptation of an\nunplugged assessment activity designed to evaluate algorithmic skills in Swiss\ncompulsory education. This tool offers scalable and automated assessment,\nreducing human involvement and mitigating potential data collection errors. The\nplatform features gesture-based and visual block-based programming interfaces,\nensuring its usability for diverse learners, further supported by multilingual\ncapabilities. To evaluate the virtual CAT platform, we conducted a pilot\nevaluation in Switzerland involving a heterogeneous group of students. The\nfindings show the platform's usability, proficiency and suitability for\nassessing AT skills among students of diverse ages, development stages, and\neducational backgrounds, as well as the feasibility of large-scale data\ncollection.\n", "link": "http://arxiv.org/abs/2408.01263v1", "date": "2024-08-02", "relevancy": 2.0053, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education&body=Title%3A%20The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education%0AAuthor%3A%20Giorgia%20Adorni%20and%20Alberto%20Piatti%0AAbstract%3A%20%20%20In%20today%27s%20digital%20era%2C%20holding%20algorithmic%20thinking%20%28AT%29%20skills%20is%20crucial%2C%0Anot%20only%20in%20computer%20science-related%20fields.%20These%20abilities%20enable%20individuals%0Ato%20break%20down%20complex%20problems%20into%20more%20manageable%20steps%20and%20create%20a%20sequence%0Aof%20actions%20to%20solve%20them.%20To%20address%20the%20increasing%20demand%20for%20AT%20assessments%0Ain%20educational%20settings%20and%20the%20limitations%20of%20current%20methods%2C%20this%20paper%0Aintroduces%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%20adaptation%20of%20an%0Aunplugged%20assessment%20activity%20designed%20to%20evaluate%20algorithmic%20skills%20in%20Swiss%0Acompulsory%20education.%20This%20tool%20offers%20scalable%20and%20automated%20assessment%2C%0Areducing%20human%20involvement%20and%20mitigating%20potential%20data%20collection%20errors.%20The%0Aplatform%20features%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Aensuring%20its%20usability%20for%20diverse%20learners%2C%20further%20supported%20by%20multilingual%0Acapabilities.%20To%20evaluate%20the%20virtual%20CAT%20platform%2C%20we%20conducted%20a%20pilot%0Aevaluation%20in%20Switzerland%20involving%20a%20heterogeneous%20group%20of%20students.%20The%0Afindings%20show%20the%20platform%27s%20usability%2C%20proficiency%20and%20suitability%20for%0Aassessing%20AT%20skills%20among%20students%20of%20diverse%20ages%2C%20development%20stages%2C%20and%0Aeducational%20backgrounds%2C%20as%20well%20as%20the%20feasibility%20of%20large-scale%20data%0Acollection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520virtual%2520CAT%253A%2520A%2520tool%2520for%2520algorithmic%2520thinking%2520assessment%2520in%2520Swiss%250A%2520%2520compulsory%2520education%26entry.906535625%3DGiorgia%2520Adorni%2520and%2520Alberto%2520Piatti%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520digital%2520era%252C%2520holding%2520algorithmic%2520thinking%2520%2528AT%2529%2520skills%2520is%2520crucial%252C%250Anot%2520only%2520in%2520computer%2520science-related%2520fields.%2520These%2520abilities%2520enable%2520individuals%250Ato%2520break%2520down%2520complex%2520problems%2520into%2520more%2520manageable%2520steps%2520and%2520create%2520a%2520sequence%250Aof%2520actions%2520to%2520solve%2520them.%2520To%2520address%2520the%2520increasing%2520demand%2520for%2520AT%2520assessments%250Ain%2520educational%2520settings%2520and%2520the%2520limitations%2520of%2520current%2520methods%252C%2520this%2520paper%250Aintroduces%2520the%2520virtual%2520Cross%2520Array%2520Task%2520%2528CAT%2529%252C%2520a%2520digital%2520adaptation%2520of%2520an%250Aunplugged%2520assessment%2520activity%2520designed%2520to%2520evaluate%2520algorithmic%2520skills%2520in%2520Swiss%250Acompulsory%2520education.%2520This%2520tool%2520offers%2520scalable%2520and%2520automated%2520assessment%252C%250Areducing%2520human%2520involvement%2520and%2520mitigating%2520potential%2520data%2520collection%2520errors.%2520The%250Aplatform%2520features%2520gesture-based%2520and%2520visual%2520block-based%2520programming%2520interfaces%252C%250Aensuring%2520its%2520usability%2520for%2520diverse%2520learners%252C%2520further%2520supported%2520by%2520multilingual%250Acapabilities.%2520To%2520evaluate%2520the%2520virtual%2520CAT%2520platform%252C%2520we%2520conducted%2520a%2520pilot%250Aevaluation%2520in%2520Switzerland%2520involving%2520a%2520heterogeneous%2520group%2520of%2520students.%2520The%250Afindings%2520show%2520the%2520platform%2527s%2520usability%252C%2520proficiency%2520and%2520suitability%2520for%250Aassessing%2520AT%2520skills%2520among%2520students%2520of%2520diverse%2520ages%252C%2520development%2520stages%252C%2520and%250Aeducational%2520backgrounds%252C%2520as%2520well%2520as%2520the%2520feasibility%2520of%2520large-scale%2520data%250Acollection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education&entry.906535625=Giorgia%20Adorni%20and%20Alberto%20Piatti&entry.1292438233=%20%20In%20today%27s%20digital%20era%2C%20holding%20algorithmic%20thinking%20%28AT%29%20skills%20is%20crucial%2C%0Anot%20only%20in%20computer%20science-related%20fields.%20These%20abilities%20enable%20individuals%0Ato%20break%20down%20complex%20problems%20into%20more%20manageable%20steps%20and%20create%20a%20sequence%0Aof%20actions%20to%20solve%20them.%20To%20address%20the%20increasing%20demand%20for%20AT%20assessments%0Ain%20educational%20settings%20and%20the%20limitations%20of%20current%20methods%2C%20this%20paper%0Aintroduces%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%20adaptation%20of%20an%0Aunplugged%20assessment%20activity%20designed%20to%20evaluate%20algorithmic%20skills%20in%20Swiss%0Acompulsory%20education.%20This%20tool%20offers%20scalable%20and%20automated%20assessment%2C%0Areducing%20human%20involvement%20and%20mitigating%20potential%20data%20collection%20errors.%20The%0Aplatform%20features%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Aensuring%20its%20usability%20for%20diverse%20learners%2C%20further%20supported%20by%20multilingual%0Acapabilities.%20To%20evaluate%20the%20virtual%20CAT%20platform%2C%20we%20conducted%20a%20pilot%0Aevaluation%20in%20Switzerland%20involving%20a%20heterogeneous%20group%20of%20students.%20The%0Afindings%20show%20the%20platform%27s%20usability%2C%20proficiency%20and%20suitability%20for%0Aassessing%20AT%20skills%20among%20students%20of%20diverse%20ages%2C%20development%20stages%2C%20and%0Aeducational%20backgrounds%2C%20as%20well%20as%20the%20feasibility%20of%20large-scale%20data%0Acollection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01263v1&entry.124074799=Read"},
{"title": "TopoNAS: Boosting Search Efficiency of Gradient-based NAS via\n  Topological Simplification", "author": "Danpei Zhao and Zhuoran Liu and Bo Yuan", "abstract": "  Improving search efficiency serves as one of the crucial objectives of Neural\nArchitecture Search (NAS). However, many current approaches ignore the\nuniversality of the search strategy and fail to reduce the computational\nredundancy during the search process, especially in one-shot NAS architectures.\nBesides, current NAS methods show invalid reparameterization in non-linear\nsearch space, leading to poor efficiency in common search spaces like DARTS. In\nthis paper, we propose TopoNAS, a model-agnostic approach for gradient-based\none-shot NAS that significantly reduces searching time and memory usage by\ntopological simplification of searchable paths. Firstly, we model the\nnon-linearity in search spaces to reveal the parameterization difficulties. To\nimprove the search efficiency, we present a topological simplification method\nand iteratively apply module-sharing strategies to simplify the topological\nstructure of searchable paths. In addition, a kernel normalization technique is\nalso proposed to preserve the search accuracy. Experimental results on the\nNASBench201 benchmark with various search spaces demonstrate the effectiveness\nof our method. It proves the proposed TopoNAS enhances the performance of\nvarious architectures in terms of search efficiency while maintaining a high\nlevel of accuracy. The project page is available at\nhttps://xdedss.github.io/topo_simplification.\n", "link": "http://arxiv.org/abs/2408.01311v1", "date": "2024-08-02", "relevancy": 1.9915, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5063}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoNAS%3A%20Boosting%20Search%20Efficiency%20of%20Gradient-based%20NAS%20via%0A%20%20Topological%20Simplification&body=Title%3A%20TopoNAS%3A%20Boosting%20Search%20Efficiency%20of%20Gradient-based%20NAS%20via%0A%20%20Topological%20Simplification%0AAuthor%3A%20Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Bo%20Yuan%0AAbstract%3A%20%20%20Improving%20search%20efficiency%20serves%20as%20one%20of%20the%20crucial%20objectives%20of%20Neural%0AArchitecture%20Search%20%28NAS%29.%20However%2C%20many%20current%20approaches%20ignore%20the%0Auniversality%20of%20the%20search%20strategy%20and%20fail%20to%20reduce%20the%20computational%0Aredundancy%20during%20the%20search%20process%2C%20especially%20in%20one-shot%20NAS%20architectures.%0ABesides%2C%20current%20NAS%20methods%20show%20invalid%20reparameterization%20in%20non-linear%0Asearch%20space%2C%20leading%20to%20poor%20efficiency%20in%20common%20search%20spaces%20like%20DARTS.%20In%0Athis%20paper%2C%20we%20propose%20TopoNAS%2C%20a%20model-agnostic%20approach%20for%20gradient-based%0Aone-shot%20NAS%20that%20significantly%20reduces%20searching%20time%20and%20memory%20usage%20by%0Atopological%20simplification%20of%20searchable%20paths.%20Firstly%2C%20we%20model%20the%0Anon-linearity%20in%20search%20spaces%20to%20reveal%20the%20parameterization%20difficulties.%20To%0Aimprove%20the%20search%20efficiency%2C%20we%20present%20a%20topological%20simplification%20method%0Aand%20iteratively%20apply%20module-sharing%20strategies%20to%20simplify%20the%20topological%0Astructure%20of%20searchable%20paths.%20In%20addition%2C%20a%20kernel%20normalization%20technique%20is%0Aalso%20proposed%20to%20preserve%20the%20search%20accuracy.%20Experimental%20results%20on%20the%0ANASBench201%20benchmark%20with%20various%20search%20spaces%20demonstrate%20the%20effectiveness%0Aof%20our%20method.%20It%20proves%20the%20proposed%20TopoNAS%20enhances%20the%20performance%20of%0Avarious%20architectures%20in%20terms%20of%20search%20efficiency%20while%20maintaining%20a%20high%0Alevel%20of%20accuracy.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//xdedss.github.io/topo_simplification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoNAS%253A%2520Boosting%2520Search%2520Efficiency%2520of%2520Gradient-based%2520NAS%2520via%250A%2520%2520Topological%2520Simplification%26entry.906535625%3DDanpei%2520Zhao%2520and%2520Zhuoran%2520Liu%2520and%2520Bo%2520Yuan%26entry.1292438233%3D%2520%2520Improving%2520search%2520efficiency%2520serves%2520as%2520one%2520of%2520the%2520crucial%2520objectives%2520of%2520Neural%250AArchitecture%2520Search%2520%2528NAS%2529.%2520However%252C%2520many%2520current%2520approaches%2520ignore%2520the%250Auniversality%2520of%2520the%2520search%2520strategy%2520and%2520fail%2520to%2520reduce%2520the%2520computational%250Aredundancy%2520during%2520the%2520search%2520process%252C%2520especially%2520in%2520one-shot%2520NAS%2520architectures.%250ABesides%252C%2520current%2520NAS%2520methods%2520show%2520invalid%2520reparameterization%2520in%2520non-linear%250Asearch%2520space%252C%2520leading%2520to%2520poor%2520efficiency%2520in%2520common%2520search%2520spaces%2520like%2520DARTS.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520TopoNAS%252C%2520a%2520model-agnostic%2520approach%2520for%2520gradient-based%250Aone-shot%2520NAS%2520that%2520significantly%2520reduces%2520searching%2520time%2520and%2520memory%2520usage%2520by%250Atopological%2520simplification%2520of%2520searchable%2520paths.%2520Firstly%252C%2520we%2520model%2520the%250Anon-linearity%2520in%2520search%2520spaces%2520to%2520reveal%2520the%2520parameterization%2520difficulties.%2520To%250Aimprove%2520the%2520search%2520efficiency%252C%2520we%2520present%2520a%2520topological%2520simplification%2520method%250Aand%2520iteratively%2520apply%2520module-sharing%2520strategies%2520to%2520simplify%2520the%2520topological%250Astructure%2520of%2520searchable%2520paths.%2520In%2520addition%252C%2520a%2520kernel%2520normalization%2520technique%2520is%250Aalso%2520proposed%2520to%2520preserve%2520the%2520search%2520accuracy.%2520Experimental%2520results%2520on%2520the%250ANASBench201%2520benchmark%2520with%2520various%2520search%2520spaces%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method.%2520It%2520proves%2520the%2520proposed%2520TopoNAS%2520enhances%2520the%2520performance%2520of%250Avarious%2520architectures%2520in%2520terms%2520of%2520search%2520efficiency%2520while%2520maintaining%2520a%2520high%250Alevel%2520of%2520accuracy.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//xdedss.github.io/topo_simplification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoNAS%3A%20Boosting%20Search%20Efficiency%20of%20Gradient-based%20NAS%20via%0A%20%20Topological%20Simplification&entry.906535625=Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Bo%20Yuan&entry.1292438233=%20%20Improving%20search%20efficiency%20serves%20as%20one%20of%20the%20crucial%20objectives%20of%20Neural%0AArchitecture%20Search%20%28NAS%29.%20However%2C%20many%20current%20approaches%20ignore%20the%0Auniversality%20of%20the%20search%20strategy%20and%20fail%20to%20reduce%20the%20computational%0Aredundancy%20during%20the%20search%20process%2C%20especially%20in%20one-shot%20NAS%20architectures.%0ABesides%2C%20current%20NAS%20methods%20show%20invalid%20reparameterization%20in%20non-linear%0Asearch%20space%2C%20leading%20to%20poor%20efficiency%20in%20common%20search%20spaces%20like%20DARTS.%20In%0Athis%20paper%2C%20we%20propose%20TopoNAS%2C%20a%20model-agnostic%20approach%20for%20gradient-based%0Aone-shot%20NAS%20that%20significantly%20reduces%20searching%20time%20and%20memory%20usage%20by%0Atopological%20simplification%20of%20searchable%20paths.%20Firstly%2C%20we%20model%20the%0Anon-linearity%20in%20search%20spaces%20to%20reveal%20the%20parameterization%20difficulties.%20To%0Aimprove%20the%20search%20efficiency%2C%20we%20present%20a%20topological%20simplification%20method%0Aand%20iteratively%20apply%20module-sharing%20strategies%20to%20simplify%20the%20topological%0Astructure%20of%20searchable%20paths.%20In%20addition%2C%20a%20kernel%20normalization%20technique%20is%0Aalso%20proposed%20to%20preserve%20the%20search%20accuracy.%20Experimental%20results%20on%20the%0ANASBench201%20benchmark%20with%20various%20search%20spaces%20demonstrate%20the%20effectiveness%0Aof%20our%20method.%20It%20proves%20the%20proposed%20TopoNAS%20enhances%20the%20performance%20of%0Avarious%20architectures%20in%20terms%20of%20search%20efficiency%20while%20maintaining%20a%20high%0Alevel%20of%20accuracy.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//xdedss.github.io/topo_simplification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01311v1&entry.124074799=Read"},
{"title": "LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task\n  Automation", "author": "Li Zhang and Shihe Wang and Xianqing Jia and Zhihan Zheng and Yunhe Yan and Longxi Gao and Yuanchun Li and Mengwei Xu", "abstract": "  The emergent large language/multimodal models facilitate the evolution of\nmobile agents, especially in mobile UI task automation. However, existing\nevaluation approaches, which rely on human validation or established datasets\nto compare agent-predicted actions with predefined action sequences, are\nunscalable and unfaithful. To overcome these limitations, this paper presents\nLlamaTouch, a testbed for on-device mobile UI task execution and faithful,\nscalable task evaluation. By observing that the task execution process only\ntransfers UI states, LlamaTouch employs a novel evaluation approach that only\nassesses whether an agent traverses all manually annotated, essential\napplication/system states. LlamaTouch comprises three key techniques: (1)\nOn-device task execution that enables mobile agents to interact with realistic\nmobile environments for task execution. (2) Fine-grained UI component\nannotation that merges pixel-level screenshots and textual screen hierarchies\nto explicitly identify and precisely annotate essential UI components with a\nrich set of designed annotation primitives. (3) A multi-level application state\nmatching algorithm that utilizes exact and fuzzy matching to accurately detect\ncritical information in each screen, even with unpredictable UI layout/content\ndynamics. LlamaTouch currently incorporates four mobile agents and 496 tasks,\nencompassing both tasks in the widely-used datasets and our self-constructed\nones to cover more diverse mobile applications. Evaluation results demonstrate\nLlamaTouch's high faithfulness of evaluation in real-world mobile environments\nand its better scalability than human validation. LlamaTouch also enables easy\ntask annotation and integration of new mobile agents. Code and dataset are\npublicly available at https://github.com/LlamaTouch/LlamaTouch.\n", "link": "http://arxiv.org/abs/2404.16054v2", "date": "2024-08-02", "relevancy": 1.9791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LlamaTouch%3A%20A%20Faithful%20and%20Scalable%20Testbed%20for%20Mobile%20UI%20Task%0A%20%20Automation&body=Title%3A%20LlamaTouch%3A%20A%20Faithful%20and%20Scalable%20Testbed%20for%20Mobile%20UI%20Task%0A%20%20Automation%0AAuthor%3A%20Li%20Zhang%20and%20Shihe%20Wang%20and%20Xianqing%20Jia%20and%20Zhihan%20Zheng%20and%20Yunhe%20Yan%20and%20Longxi%20Gao%20and%20Yuanchun%20Li%20and%20Mengwei%20Xu%0AAbstract%3A%20%20%20The%20emergent%20large%20language/multimodal%20models%20facilitate%20the%20evolution%20of%0Amobile%20agents%2C%20especially%20in%20mobile%20UI%20task%20automation.%20However%2C%20existing%0Aevaluation%20approaches%2C%20which%20rely%20on%20human%20validation%20or%20established%20datasets%0Ato%20compare%20agent-predicted%20actions%20with%20predefined%20action%20sequences%2C%20are%0Aunscalable%20and%20unfaithful.%20To%20overcome%20these%20limitations%2C%20this%20paper%20presents%0ALlamaTouch%2C%20a%20testbed%20for%20on-device%20mobile%20UI%20task%20execution%20and%20faithful%2C%0Ascalable%20task%20evaluation.%20By%20observing%20that%20the%20task%20execution%20process%20only%0Atransfers%20UI%20states%2C%20LlamaTouch%20employs%20a%20novel%20evaluation%20approach%20that%20only%0Aassesses%20whether%20an%20agent%20traverses%20all%20manually%20annotated%2C%20essential%0Aapplication/system%20states.%20LlamaTouch%20comprises%20three%20key%20techniques%3A%20%281%29%0AOn-device%20task%20execution%20that%20enables%20mobile%20agents%20to%20interact%20with%20realistic%0Amobile%20environments%20for%20task%20execution.%20%282%29%20Fine-grained%20UI%20component%0Aannotation%20that%20merges%20pixel-level%20screenshots%20and%20textual%20screen%20hierarchies%0Ato%20explicitly%20identify%20and%20precisely%20annotate%20essential%20UI%20components%20with%20a%0Arich%20set%20of%20designed%20annotation%20primitives.%20%283%29%20A%20multi-level%20application%20state%0Amatching%20algorithm%20that%20utilizes%20exact%20and%20fuzzy%20matching%20to%20accurately%20detect%0Acritical%20information%20in%20each%20screen%2C%20even%20with%20unpredictable%20UI%20layout/content%0Adynamics.%20LlamaTouch%20currently%20incorporates%20four%20mobile%20agents%20and%20496%20tasks%2C%0Aencompassing%20both%20tasks%20in%20the%20widely-used%20datasets%20and%20our%20self-constructed%0Aones%20to%20cover%20more%20diverse%20mobile%20applications.%20Evaluation%20results%20demonstrate%0ALlamaTouch%27s%20high%20faithfulness%20of%20evaluation%20in%20real-world%20mobile%20environments%0Aand%20its%20better%20scalability%20than%20human%20validation.%20LlamaTouch%20also%20enables%20easy%0Atask%20annotation%20and%20integration%20of%20new%20mobile%20agents.%20Code%20and%20dataset%20are%0Apublicly%20available%20at%20https%3A//github.com/LlamaTouch/LlamaTouch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlamaTouch%253A%2520A%2520Faithful%2520and%2520Scalable%2520Testbed%2520for%2520Mobile%2520UI%2520Task%250A%2520%2520Automation%26entry.906535625%3DLi%2520Zhang%2520and%2520Shihe%2520Wang%2520and%2520Xianqing%2520Jia%2520and%2520Zhihan%2520Zheng%2520and%2520Yunhe%2520Yan%2520and%2520Longxi%2520Gao%2520and%2520Yuanchun%2520Li%2520and%2520Mengwei%2520Xu%26entry.1292438233%3D%2520%2520The%2520emergent%2520large%2520language/multimodal%2520models%2520facilitate%2520the%2520evolution%2520of%250Amobile%2520agents%252C%2520especially%2520in%2520mobile%2520UI%2520task%2520automation.%2520However%252C%2520existing%250Aevaluation%2520approaches%252C%2520which%2520rely%2520on%2520human%2520validation%2520or%2520established%2520datasets%250Ato%2520compare%2520agent-predicted%2520actions%2520with%2520predefined%2520action%2520sequences%252C%2520are%250Aunscalable%2520and%2520unfaithful.%2520To%2520overcome%2520these%2520limitations%252C%2520this%2520paper%2520presents%250ALlamaTouch%252C%2520a%2520testbed%2520for%2520on-device%2520mobile%2520UI%2520task%2520execution%2520and%2520faithful%252C%250Ascalable%2520task%2520evaluation.%2520By%2520observing%2520that%2520the%2520task%2520execution%2520process%2520only%250Atransfers%2520UI%2520states%252C%2520LlamaTouch%2520employs%2520a%2520novel%2520evaluation%2520approach%2520that%2520only%250Aassesses%2520whether%2520an%2520agent%2520traverses%2520all%2520manually%2520annotated%252C%2520essential%250Aapplication/system%2520states.%2520LlamaTouch%2520comprises%2520three%2520key%2520techniques%253A%2520%25281%2529%250AOn-device%2520task%2520execution%2520that%2520enables%2520mobile%2520agents%2520to%2520interact%2520with%2520realistic%250Amobile%2520environments%2520for%2520task%2520execution.%2520%25282%2529%2520Fine-grained%2520UI%2520component%250Aannotation%2520that%2520merges%2520pixel-level%2520screenshots%2520and%2520textual%2520screen%2520hierarchies%250Ato%2520explicitly%2520identify%2520and%2520precisely%2520annotate%2520essential%2520UI%2520components%2520with%2520a%250Arich%2520set%2520of%2520designed%2520annotation%2520primitives.%2520%25283%2529%2520A%2520multi-level%2520application%2520state%250Amatching%2520algorithm%2520that%2520utilizes%2520exact%2520and%2520fuzzy%2520matching%2520to%2520accurately%2520detect%250Acritical%2520information%2520in%2520each%2520screen%252C%2520even%2520with%2520unpredictable%2520UI%2520layout/content%250Adynamics.%2520LlamaTouch%2520currently%2520incorporates%2520four%2520mobile%2520agents%2520and%2520496%2520tasks%252C%250Aencompassing%2520both%2520tasks%2520in%2520the%2520widely-used%2520datasets%2520and%2520our%2520self-constructed%250Aones%2520to%2520cover%2520more%2520diverse%2520mobile%2520applications.%2520Evaluation%2520results%2520demonstrate%250ALlamaTouch%2527s%2520high%2520faithfulness%2520of%2520evaluation%2520in%2520real-world%2520mobile%2520environments%250Aand%2520its%2520better%2520scalability%2520than%2520human%2520validation.%2520LlamaTouch%2520also%2520enables%2520easy%250Atask%2520annotation%2520and%2520integration%2520of%2520new%2520mobile%2520agents.%2520Code%2520and%2520dataset%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/LlamaTouch/LlamaTouch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LlamaTouch%3A%20A%20Faithful%20and%20Scalable%20Testbed%20for%20Mobile%20UI%20Task%0A%20%20Automation&entry.906535625=Li%20Zhang%20and%20Shihe%20Wang%20and%20Xianqing%20Jia%20and%20Zhihan%20Zheng%20and%20Yunhe%20Yan%20and%20Longxi%20Gao%20and%20Yuanchun%20Li%20and%20Mengwei%20Xu&entry.1292438233=%20%20The%20emergent%20large%20language/multimodal%20models%20facilitate%20the%20evolution%20of%0Amobile%20agents%2C%20especially%20in%20mobile%20UI%20task%20automation.%20However%2C%20existing%0Aevaluation%20approaches%2C%20which%20rely%20on%20human%20validation%20or%20established%20datasets%0Ato%20compare%20agent-predicted%20actions%20with%20predefined%20action%20sequences%2C%20are%0Aunscalable%20and%20unfaithful.%20To%20overcome%20these%20limitations%2C%20this%20paper%20presents%0ALlamaTouch%2C%20a%20testbed%20for%20on-device%20mobile%20UI%20task%20execution%20and%20faithful%2C%0Ascalable%20task%20evaluation.%20By%20observing%20that%20the%20task%20execution%20process%20only%0Atransfers%20UI%20states%2C%20LlamaTouch%20employs%20a%20novel%20evaluation%20approach%20that%20only%0Aassesses%20whether%20an%20agent%20traverses%20all%20manually%20annotated%2C%20essential%0Aapplication/system%20states.%20LlamaTouch%20comprises%20three%20key%20techniques%3A%20%281%29%0AOn-device%20task%20execution%20that%20enables%20mobile%20agents%20to%20interact%20with%20realistic%0Amobile%20environments%20for%20task%20execution.%20%282%29%20Fine-grained%20UI%20component%0Aannotation%20that%20merges%20pixel-level%20screenshots%20and%20textual%20screen%20hierarchies%0Ato%20explicitly%20identify%20and%20precisely%20annotate%20essential%20UI%20components%20with%20a%0Arich%20set%20of%20designed%20annotation%20primitives.%20%283%29%20A%20multi-level%20application%20state%0Amatching%20algorithm%20that%20utilizes%20exact%20and%20fuzzy%20matching%20to%20accurately%20detect%0Acritical%20information%20in%20each%20screen%2C%20even%20with%20unpredictable%20UI%20layout/content%0Adynamics.%20LlamaTouch%20currently%20incorporates%20four%20mobile%20agents%20and%20496%20tasks%2C%0Aencompassing%20both%20tasks%20in%20the%20widely-used%20datasets%20and%20our%20self-constructed%0Aones%20to%20cover%20more%20diverse%20mobile%20applications.%20Evaluation%20results%20demonstrate%0ALlamaTouch%27s%20high%20faithfulness%20of%20evaluation%20in%20real-world%20mobile%20environments%0Aand%20its%20better%20scalability%20than%20human%20validation.%20LlamaTouch%20also%20enables%20easy%0Atask%20annotation%20and%20integration%20of%20new%20mobile%20agents.%20Code%20and%20dataset%20are%0Apublicly%20available%20at%20https%3A//github.com/LlamaTouch/LlamaTouch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16054v2&entry.124074799=Read"},
{"title": "Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer", "author": "Yu Yang and Pan Xu", "abstract": "  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n", "link": "http://arxiv.org/abs/2408.01402v1", "date": "2024-08-02", "relevancy": 1.976, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4966}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4952}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Language%20Models%20Improve%20the%20Few-shot%20Prompt%20Ability%20of%0A%20%20Decision%20Transformer&body=Title%3A%20Pre-trained%20Language%20Models%20Improve%20the%20Few-shot%20Prompt%20Ability%20of%0A%20%20Decision%20Transformer%0AAuthor%3A%20Yu%20Yang%20and%20Pan%20Xu%0AAbstract%3A%20%20%20Decision%20Transformer%20%28DT%29%20has%20emerged%20as%20a%20promising%20class%20of%20algorithms%20in%0Aoffline%20reinforcement%20learning%20%28RL%29%20tasks%2C%20leveraging%20pre-collected%20datasets%0Aand%20Transformer%27s%20capability%20to%20model%20long%20sequences.%20Recent%20works%20have%0Ademonstrated%20that%20using%20parts%20of%20trajectories%20from%20training%20tasks%20as%20prompts%20in%0ADT%20enhances%20its%20performance%20on%20unseen%20tasks%2C%20giving%20rise%20to%20Prompt-DT%20methods.%0AHowever%2C%20collecting%20data%20from%20specific%20environments%20can%20be%20both%20costly%20and%0Aunsafe%20in%20many%20scenarios%2C%20leading%20to%20suboptimal%20performance%20and%20limited%0Afew-shot%20prompt%20abilities%20due%20to%20the%20data-hungry%20nature%20of%20Transformer-based%0Amodels.%20Additionally%2C%20the%20limited%20datasets%20used%20in%20pre-training%20make%20it%0Achallenging%20for%20Prompt-DT%20type%20of%20methods%20to%20distinguish%20between%20various%20RL%0Atasks%20through%20prompts%20alone.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%0ALanguage%20model-initialized%20Prompt%20Decision%20Transformer%20%28LPDT%29%2C%20which%20leverages%0Apre-trained%20language%20models%20for%20meta-RL%20tasks%20and%20fine-tunes%20the%20model%20using%0ALow-rank%20Adaptation%20%28LoRA%29.%20We%20further%20incorporate%20prompt%20regularization%20to%0Aeffectively%20differentiate%20between%20tasks%20based%20on%20prompt%20feature%0Arepresentations.%20Our%20approach%20integrates%20pre-trained%20language%20model%20and%20RL%0Atasks%20seamlessly.%20Extensive%20empirical%20studies%20demonstrate%20that%20initializing%0Awith%20a%20pre-trained%20language%20model%20significantly%20enhances%20the%20performance%20of%0APrompt-DT%20on%20unseen%20tasks%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-trained%2520Language%2520Models%2520Improve%2520the%2520Few-shot%2520Prompt%2520Ability%2520of%250A%2520%2520Decision%2520Transformer%26entry.906535625%3DYu%2520Yang%2520and%2520Pan%2520Xu%26entry.1292438233%3D%2520%2520Decision%2520Transformer%2520%2528DT%2529%2520has%2520emerged%2520as%2520a%2520promising%2520class%2520of%2520algorithms%2520in%250Aoffline%2520reinforcement%2520learning%2520%2528RL%2529%2520tasks%252C%2520leveraging%2520pre-collected%2520datasets%250Aand%2520Transformer%2527s%2520capability%2520to%2520model%2520long%2520sequences.%2520Recent%2520works%2520have%250Ademonstrated%2520that%2520using%2520parts%2520of%2520trajectories%2520from%2520training%2520tasks%2520as%2520prompts%2520in%250ADT%2520enhances%2520its%2520performance%2520on%2520unseen%2520tasks%252C%2520giving%2520rise%2520to%2520Prompt-DT%2520methods.%250AHowever%252C%2520collecting%2520data%2520from%2520specific%2520environments%2520can%2520be%2520both%2520costly%2520and%250Aunsafe%2520in%2520many%2520scenarios%252C%2520leading%2520to%2520suboptimal%2520performance%2520and%2520limited%250Afew-shot%2520prompt%2520abilities%2520due%2520to%2520the%2520data-hungry%2520nature%2520of%2520Transformer-based%250Amodels.%2520Additionally%252C%2520the%2520limited%2520datasets%2520used%2520in%2520pre-training%2520make%2520it%250Achallenging%2520for%2520Prompt-DT%2520type%2520of%2520methods%2520to%2520distinguish%2520between%2520various%2520RL%250Atasks%2520through%2520prompts%2520alone.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520the%250ALanguage%2520model-initialized%2520Prompt%2520Decision%2520Transformer%2520%2528LPDT%2529%252C%2520which%2520leverages%250Apre-trained%2520language%2520models%2520for%2520meta-RL%2520tasks%2520and%2520fine-tunes%2520the%2520model%2520using%250ALow-rank%2520Adaptation%2520%2528LoRA%2529.%2520We%2520further%2520incorporate%2520prompt%2520regularization%2520to%250Aeffectively%2520differentiate%2520between%2520tasks%2520based%2520on%2520prompt%2520feature%250Arepresentations.%2520Our%2520approach%2520integrates%2520pre-trained%2520language%2520model%2520and%2520RL%250Atasks%2520seamlessly.%2520Extensive%2520empirical%2520studies%2520demonstrate%2520that%2520initializing%250Awith%2520a%2520pre-trained%2520language%2520model%2520significantly%2520enhances%2520the%2520performance%2520of%250APrompt-DT%2520on%2520unseen%2520tasks%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Language%20Models%20Improve%20the%20Few-shot%20Prompt%20Ability%20of%0A%20%20Decision%20Transformer&entry.906535625=Yu%20Yang%20and%20Pan%20Xu&entry.1292438233=%20%20Decision%20Transformer%20%28DT%29%20has%20emerged%20as%20a%20promising%20class%20of%20algorithms%20in%0Aoffline%20reinforcement%20learning%20%28RL%29%20tasks%2C%20leveraging%20pre-collected%20datasets%0Aand%20Transformer%27s%20capability%20to%20model%20long%20sequences.%20Recent%20works%20have%0Ademonstrated%20that%20using%20parts%20of%20trajectories%20from%20training%20tasks%20as%20prompts%20in%0ADT%20enhances%20its%20performance%20on%20unseen%20tasks%2C%20giving%20rise%20to%20Prompt-DT%20methods.%0AHowever%2C%20collecting%20data%20from%20specific%20environments%20can%20be%20both%20costly%20and%0Aunsafe%20in%20many%20scenarios%2C%20leading%20to%20suboptimal%20performance%20and%20limited%0Afew-shot%20prompt%20abilities%20due%20to%20the%20data-hungry%20nature%20of%20Transformer-based%0Amodels.%20Additionally%2C%20the%20limited%20datasets%20used%20in%20pre-training%20make%20it%0Achallenging%20for%20Prompt-DT%20type%20of%20methods%20to%20distinguish%20between%20various%20RL%0Atasks%20through%20prompts%20alone.%20To%20address%20these%20challenges%2C%20we%20introduce%20the%0ALanguage%20model-initialized%20Prompt%20Decision%20Transformer%20%28LPDT%29%2C%20which%20leverages%0Apre-trained%20language%20models%20for%20meta-RL%20tasks%20and%20fine-tunes%20the%20model%20using%0ALow-rank%20Adaptation%20%28LoRA%29.%20We%20further%20incorporate%20prompt%20regularization%20to%0Aeffectively%20differentiate%20between%20tasks%20based%20on%20prompt%20feature%0Arepresentations.%20Our%20approach%20integrates%20pre-trained%20language%20model%20and%20RL%0Atasks%20seamlessly.%20Extensive%20empirical%20studies%20demonstrate%20that%20initializing%0Awith%20a%20pre-trained%20language%20model%20significantly%20enhances%20the%20performance%20of%0APrompt-DT%20on%20unseen%20tasks%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01402v1&entry.124074799=Read"},
{"title": "Decentralized Smoothing ADMM for Quantile Regression with Non-Convex\n  Sparse Penalties", "author": "Reza Mirzaeifard and Diyako Ghaderyan and Stefan Werner", "abstract": "  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data\nanalysis techniques are crucial for handling distributed data generated by\nsensors. Addressing the limitations of existing methods, such as the\nsub-gradient approach, which fails to distinguish between active and non-active\ncoefficients effectively, this paper introduces the decentralized smoothing\nalternating direction method of multipliers (DSAD) for penalized quantile\nregression. Our method leverages non-convex sparse penalties like the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving\nthe identification and retention of significant predictors. DSAD incorporates a\ntotal variation norm within a smoothing ADMM framework, achieving consensus\namong distributed nodes and ensuring uniform model performance across disparate\ndata sources. This approach overcomes traditional convergence challenges\nassociated with non-convex penalties in decentralized settings. We present\ntheoretical proofs and extensive simulation results to validate the\neffectiveness of the DSAD, demonstrating its superiority in achieving reliable\nconvergence and enhancing estimation accuracy compared with prior methods.\n", "link": "http://arxiv.org/abs/2408.01307v1", "date": "2024-08-02", "relevancy": 1.9728, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Smoothing%20ADMM%20for%20Quantile%20Regression%20with%20Non-Convex%0A%20%20Sparse%20Penalties&body=Title%3A%20Decentralized%20Smoothing%20ADMM%20for%20Quantile%20Regression%20with%20Non-Convex%0A%20%20Sparse%20Penalties%0AAuthor%3A%20Reza%20Mirzaeifard%20and%20Diyako%20Ghaderyan%20and%20Stefan%20Werner%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20internet-of-things%20%28IoT%29%20ecosystem%2C%20effective%20data%0Aanalysis%20techniques%20are%20crucial%20for%20handling%20distributed%20data%20generated%20by%0Asensors.%20Addressing%20the%20limitations%20of%20existing%20methods%2C%20such%20as%20the%0Asub-gradient%20approach%2C%20which%20fails%20to%20distinguish%20between%20active%20and%20non-active%0Acoefficients%20effectively%2C%20this%20paper%20introduces%20the%20decentralized%20smoothing%0Aalternating%20direction%20method%20of%20multipliers%20%28DSAD%29%20for%20penalized%20quantile%0Aregression.%20Our%20method%20leverages%20non-convex%20sparse%20penalties%20like%20the%20minimax%0Aconcave%20penalty%20%28MCP%29%20and%20smoothly%20clipped%20absolute%20deviation%20%28SCAD%29%2C%20improving%0Athe%20identification%20and%20retention%20of%20significant%20predictors.%20DSAD%20incorporates%20a%0Atotal%20variation%20norm%20within%20a%20smoothing%20ADMM%20framework%2C%20achieving%20consensus%0Aamong%20distributed%20nodes%20and%20ensuring%20uniform%20model%20performance%20across%20disparate%0Adata%20sources.%20This%20approach%20overcomes%20traditional%20convergence%20challenges%0Aassociated%20with%20non-convex%20penalties%20in%20decentralized%20settings.%20We%20present%0Atheoretical%20proofs%20and%20extensive%20simulation%20results%20to%20validate%20the%0Aeffectiveness%20of%20the%20DSAD%2C%20demonstrating%20its%20superiority%20in%20achieving%20reliable%0Aconvergence%20and%20enhancing%20estimation%20accuracy%20compared%20with%20prior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Smoothing%2520ADMM%2520for%2520Quantile%2520Regression%2520with%2520Non-Convex%250A%2520%2520Sparse%2520Penalties%26entry.906535625%3DReza%2520Mirzaeifard%2520and%2520Diyako%2520Ghaderyan%2520and%2520Stefan%2520Werner%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520internet-of-things%2520%2528IoT%2529%2520ecosystem%252C%2520effective%2520data%250Aanalysis%2520techniques%2520are%2520crucial%2520for%2520handling%2520distributed%2520data%2520generated%2520by%250Asensors.%2520Addressing%2520the%2520limitations%2520of%2520existing%2520methods%252C%2520such%2520as%2520the%250Asub-gradient%2520approach%252C%2520which%2520fails%2520to%2520distinguish%2520between%2520active%2520and%2520non-active%250Acoefficients%2520effectively%252C%2520this%2520paper%2520introduces%2520the%2520decentralized%2520smoothing%250Aalternating%2520direction%2520method%2520of%2520multipliers%2520%2528DSAD%2529%2520for%2520penalized%2520quantile%250Aregression.%2520Our%2520method%2520leverages%2520non-convex%2520sparse%2520penalties%2520like%2520the%2520minimax%250Aconcave%2520penalty%2520%2528MCP%2529%2520and%2520smoothly%2520clipped%2520absolute%2520deviation%2520%2528SCAD%2529%252C%2520improving%250Athe%2520identification%2520and%2520retention%2520of%2520significant%2520predictors.%2520DSAD%2520incorporates%2520a%250Atotal%2520variation%2520norm%2520within%2520a%2520smoothing%2520ADMM%2520framework%252C%2520achieving%2520consensus%250Aamong%2520distributed%2520nodes%2520and%2520ensuring%2520uniform%2520model%2520performance%2520across%2520disparate%250Adata%2520sources.%2520This%2520approach%2520overcomes%2520traditional%2520convergence%2520challenges%250Aassociated%2520with%2520non-convex%2520penalties%2520in%2520decentralized%2520settings.%2520We%2520present%250Atheoretical%2520proofs%2520and%2520extensive%2520simulation%2520results%2520to%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520DSAD%252C%2520demonstrating%2520its%2520superiority%2520in%2520achieving%2520reliable%250Aconvergence%2520and%2520enhancing%2520estimation%2520accuracy%2520compared%2520with%2520prior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Smoothing%20ADMM%20for%20Quantile%20Regression%20with%20Non-Convex%0A%20%20Sparse%20Penalties&entry.906535625=Reza%20Mirzaeifard%20and%20Diyako%20Ghaderyan%20and%20Stefan%20Werner&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20internet-of-things%20%28IoT%29%20ecosystem%2C%20effective%20data%0Aanalysis%20techniques%20are%20crucial%20for%20handling%20distributed%20data%20generated%20by%0Asensors.%20Addressing%20the%20limitations%20of%20existing%20methods%2C%20such%20as%20the%0Asub-gradient%20approach%2C%20which%20fails%20to%20distinguish%20between%20active%20and%20non-active%0Acoefficients%20effectively%2C%20this%20paper%20introduces%20the%20decentralized%20smoothing%0Aalternating%20direction%20method%20of%20multipliers%20%28DSAD%29%20for%20penalized%20quantile%0Aregression.%20Our%20method%20leverages%20non-convex%20sparse%20penalties%20like%20the%20minimax%0Aconcave%20penalty%20%28MCP%29%20and%20smoothly%20clipped%20absolute%20deviation%20%28SCAD%29%2C%20improving%0Athe%20identification%20and%20retention%20of%20significant%20predictors.%20DSAD%20incorporates%20a%0Atotal%20variation%20norm%20within%20a%20smoothing%20ADMM%20framework%2C%20achieving%20consensus%0Aamong%20distributed%20nodes%20and%20ensuring%20uniform%20model%20performance%20across%20disparate%0Adata%20sources.%20This%20approach%20overcomes%20traditional%20convergence%20challenges%0Aassociated%20with%20non-convex%20penalties%20in%20decentralized%20settings.%20We%20present%0Atheoretical%20proofs%20and%20extensive%20simulation%20results%20to%20validate%20the%0Aeffectiveness%20of%20the%20DSAD%2C%20demonstrating%20its%20superiority%20in%20achieving%20reliable%0Aconvergence%20and%20enhancing%20estimation%20accuracy%20compared%20with%20prior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01307v1&entry.124074799=Read"},
{"title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives", "author": "Desta Haileselassie Hagos and Rick Battle and Danda B. Rawat", "abstract": "  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n", "link": "http://arxiv.org/abs/2407.14962v3", "date": "2024-08-02", "relevancy": 1.971, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives&body=Title%3A%20Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives%0AAuthor%3A%20Desta%20Haileselassie%20Hagos%20and%20Rick%20Battle%20and%20Danda%20B.%20Rawat%0AAbstract%3A%20%20%20The%20emergence%20of%20Generative%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%0AModels%20%28LLMs%29%20has%20marked%20a%20new%20era%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%0Aintroducing%20unprecedented%20capabilities%20that%20are%20revolutionizing%20various%0Adomains.%20This%20paper%20explores%20the%20current%20state%20of%20these%20cutting-edge%0Atechnologies%2C%20demonstrating%20their%20remarkable%20advancements%20and%20wide-ranging%0Aapplications.%20Our%20paper%20contributes%20to%20providing%20a%20holistic%20perspective%20on%20the%0Atechnical%20foundations%2C%20practical%20applications%2C%20and%20emerging%20challenges%20within%0Athe%20evolving%20landscape%20of%20Generative%20AI%20and%20LLMs.%20We%20believe%20that%20understanding%0Athe%20generative%20capabilities%20of%20AI%20systems%20and%20the%20specific%20context%20of%20LLMs%20is%0Acrucial%20for%20researchers%2C%20practitioners%2C%20and%20policymakers%20to%20collaboratively%0Ashape%20the%20responsible%20and%20ethical%20integration%20of%20these%20technologies%20into%0Avarious%20domains.%20Furthermore%2C%20we%20identify%20and%20address%20main%20research%20gaps%2C%0Aproviding%20valuable%20insights%20to%20guide%20future%20research%20endeavors%20within%20the%20AI%0Aresearch%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14962v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Generative%2520AI%2520and%2520Large%2520Language%2520Models%253A%2520Current%250A%2520%2520Status%252C%2520Challenges%252C%2520and%2520Perspectives%26entry.906535625%3DDesta%2520Haileselassie%2520Hagos%2520and%2520Rick%2520Battle%2520and%2520Danda%2520B.%2520Rawat%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520has%2520marked%2520a%2520new%2520era%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%250Aintroducing%2520unprecedented%2520capabilities%2520that%2520are%2520revolutionizing%2520various%250Adomains.%2520This%2520paper%2520explores%2520the%2520current%2520state%2520of%2520these%2520cutting-edge%250Atechnologies%252C%2520demonstrating%2520their%2520remarkable%2520advancements%2520and%2520wide-ranging%250Aapplications.%2520Our%2520paper%2520contributes%2520to%2520providing%2520a%2520holistic%2520perspective%2520on%2520the%250Atechnical%2520foundations%252C%2520practical%2520applications%252C%2520and%2520emerging%2520challenges%2520within%250Athe%2520evolving%2520landscape%2520of%2520Generative%2520AI%2520and%2520LLMs.%2520We%2520believe%2520that%2520understanding%250Athe%2520generative%2520capabilities%2520of%2520AI%2520systems%2520and%2520the%2520specific%2520context%2520of%2520LLMs%2520is%250Acrucial%2520for%2520researchers%252C%2520practitioners%252C%2520and%2520policymakers%2520to%2520collaboratively%250Ashape%2520the%2520responsible%2520and%2520ethical%2520integration%2520of%2520these%2520technologies%2520into%250Avarious%2520domains.%2520Furthermore%252C%2520we%2520identify%2520and%2520address%2520main%2520research%2520gaps%252C%250Aproviding%2520valuable%2520insights%2520to%2520guide%2520future%2520research%2520endeavors%2520within%2520the%2520AI%250Aresearch%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14962v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Generative%20AI%20and%20Large%20Language%20Models%3A%20Current%0A%20%20Status%2C%20Challenges%2C%20and%20Perspectives&entry.906535625=Desta%20Haileselassie%20Hagos%20and%20Rick%20Battle%20and%20Danda%20B.%20Rawat&entry.1292438233=%20%20The%20emergence%20of%20Generative%20Artificial%20Intelligence%20%28AI%29%20and%20Large%20Language%0AModels%20%28LLMs%29%20has%20marked%20a%20new%20era%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%0Aintroducing%20unprecedented%20capabilities%20that%20are%20revolutionizing%20various%0Adomains.%20This%20paper%20explores%20the%20current%20state%20of%20these%20cutting-edge%0Atechnologies%2C%20demonstrating%20their%20remarkable%20advancements%20and%20wide-ranging%0Aapplications.%20Our%20paper%20contributes%20to%20providing%20a%20holistic%20perspective%20on%20the%0Atechnical%20foundations%2C%20practical%20applications%2C%20and%20emerging%20challenges%20within%0Athe%20evolving%20landscape%20of%20Generative%20AI%20and%20LLMs.%20We%20believe%20that%20understanding%0Athe%20generative%20capabilities%20of%20AI%20systems%20and%20the%20specific%20context%20of%20LLMs%20is%0Acrucial%20for%20researchers%2C%20practitioners%2C%20and%20policymakers%20to%20collaboratively%0Ashape%20the%20responsible%20and%20ethical%20integration%20of%20these%20technologies%20into%0Avarious%20domains.%20Furthermore%2C%20we%20identify%20and%20address%20main%20research%20gaps%2C%0Aproviding%20valuable%20insights%20to%20guide%20future%20research%20endeavors%20within%20the%20AI%0Aresearch%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14962v3&entry.124074799=Read"},
{"title": "Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Adil Mehmood Khan and Manual Mazzara and Salvatore Distenano", "abstract": "  In recent years, Transformers have garnered significant attention for\nHyperspectral Image Classification (HSIC) due to their self-attention\nmechanism, which provides strong classification performance. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a State Space Model, offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model. In the MorpMamba model, a token generation module first\nconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.\nThese tokens are then processed by a morphology block, which computes\nstructural and shape information using depthwise separable convolutional\noperations. The extracted information is enhanced in a feature enhancement\nmodule that adjusts the spatial and spectral tokens based on the center region\nof the HSI sample, allowing for effective information fusion within each block.\nSubsequently, the tokens are refined in a multi-head self-attention block to\nfurther improve the feature space. Finally, the combined information is fed\ninto the state space block for classification and the creation of the ground\ntruth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate\nthat the MorpMamba model outperforms (parametric efficiency) both CNN and\nTransformer models.\n", "link": "http://arxiv.org/abs/2408.01372v1", "date": "2024-08-02", "relevancy": 1.957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4854}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&body=Title%3A%20Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Adil%20Mehmood%20Khan%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Transformers%20have%20garnered%20significant%20attention%20for%0AHyperspectral%20Image%20Classification%20%28HSIC%29%20due%20to%20their%20self-attention%0Amechanism%2C%20which%20provides%20strong%20classification%20performance.%20However%2C%20these%0Amodels%20face%20major%20challenges%20in%20computational%20efficiency%2C%20as%20their%20complexity%0Aincreases%20quadratically%20with%20the%20sequence%20length.%20The%20Mamba%20architecture%2C%0Aleveraging%20a%20State%20Space%20Model%2C%20offers%20a%20more%20efficient%20alternative%20to%0ATransformers.%20This%20paper%20introduces%20the%20Spatial-Spectral%20Morphological%20Mamba%0A%28MorpMamba%29%20model.%20In%20the%20MorpMamba%20model%2C%20a%20token%20generation%20module%20first%0Aconverts%20the%20Hyperspectral%20Image%20%28HSI%29%20patch%20into%20spatial-spectral%20tokens.%0AThese%20tokens%20are%20then%20processed%20by%20a%20morphology%20block%2C%20which%20computes%0Astructural%20and%20shape%20information%20using%20depthwise%20separable%20convolutional%0Aoperations.%20The%20extracted%20information%20is%20enhanced%20in%20a%20feature%20enhancement%0Amodule%20that%20adjusts%20the%20spatial%20and%20spectral%20tokens%20based%20on%20the%20center%20region%0Aof%20the%20HSI%20sample%2C%20allowing%20for%20effective%20information%20fusion%20within%20each%20block.%0ASubsequently%2C%20the%20tokens%20are%20refined%20in%20a%20multi-head%20self-attention%20block%20to%0Afurther%20improve%20the%20feature%20space.%20Finally%2C%20the%20combined%20information%20is%20fed%0Ainto%20the%20state%20space%20block%20for%20classification%20and%20the%20creation%20of%20the%20ground%0Atruth%20map.%20Experiments%20on%20widely%20used%20Hyperspectral%20%28HS%29%20datasets%20demonstrate%0Athat%20the%20MorpMamba%20model%20outperforms%20%28parametric%20efficiency%29%20both%20CNN%20and%0ATransformer%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Spectral%2520Morphological%2520Mamba%2520for%2520Hyperspectral%2520Image%250A%2520%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Adil%2520Mehmood%2520Khan%2520and%2520Manual%2520Mazzara%2520and%2520Salvatore%2520Distenano%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Transformers%2520have%2520garnered%2520significant%2520attention%2520for%250AHyperspectral%2520Image%2520Classification%2520%2528HSIC%2529%2520due%2520to%2520their%2520self-attention%250Amechanism%252C%2520which%2520provides%2520strong%2520classification%2520performance.%2520However%252C%2520these%250Amodels%2520face%2520major%2520challenges%2520in%2520computational%2520efficiency%252C%2520as%2520their%2520complexity%250Aincreases%2520quadratically%2520with%2520the%2520sequence%2520length.%2520The%2520Mamba%2520architecture%252C%250Aleveraging%2520a%2520State%2520Space%2520Model%252C%2520offers%2520a%2520more%2520efficient%2520alternative%2520to%250ATransformers.%2520This%2520paper%2520introduces%2520the%2520Spatial-Spectral%2520Morphological%2520Mamba%250A%2528MorpMamba%2529%2520model.%2520In%2520the%2520MorpMamba%2520model%252C%2520a%2520token%2520generation%2520module%2520first%250Aconverts%2520the%2520Hyperspectral%2520Image%2520%2528HSI%2529%2520patch%2520into%2520spatial-spectral%2520tokens.%250AThese%2520tokens%2520are%2520then%2520processed%2520by%2520a%2520morphology%2520block%252C%2520which%2520computes%250Astructural%2520and%2520shape%2520information%2520using%2520depthwise%2520separable%2520convolutional%250Aoperations.%2520The%2520extracted%2520information%2520is%2520enhanced%2520in%2520a%2520feature%2520enhancement%250Amodule%2520that%2520adjusts%2520the%2520spatial%2520and%2520spectral%2520tokens%2520based%2520on%2520the%2520center%2520region%250Aof%2520the%2520HSI%2520sample%252C%2520allowing%2520for%2520effective%2520information%2520fusion%2520within%2520each%2520block.%250ASubsequently%252C%2520the%2520tokens%2520are%2520refined%2520in%2520a%2520multi-head%2520self-attention%2520block%2520to%250Afurther%2520improve%2520the%2520feature%2520space.%2520Finally%252C%2520the%2520combined%2520information%2520is%2520fed%250Ainto%2520the%2520state%2520space%2520block%2520for%2520classification%2520and%2520the%2520creation%2520of%2520the%2520ground%250Atruth%2520map.%2520Experiments%2520on%2520widely%2520used%2520Hyperspectral%2520%2528HS%2529%2520datasets%2520demonstrate%250Athat%2520the%2520MorpMamba%2520model%2520outperforms%2520%2528parametric%2520efficiency%2529%2520both%2520CNN%2520and%250ATransformer%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Spectral%20Morphological%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Adil%20Mehmood%20Khan%20and%20Manual%20Mazzara%20and%20Salvatore%20Distenano&entry.1292438233=%20%20In%20recent%20years%2C%20Transformers%20have%20garnered%20significant%20attention%20for%0AHyperspectral%20Image%20Classification%20%28HSIC%29%20due%20to%20their%20self-attention%0Amechanism%2C%20which%20provides%20strong%20classification%20performance.%20However%2C%20these%0Amodels%20face%20major%20challenges%20in%20computational%20efficiency%2C%20as%20their%20complexity%0Aincreases%20quadratically%20with%20the%20sequence%20length.%20The%20Mamba%20architecture%2C%0Aleveraging%20a%20State%20Space%20Model%2C%20offers%20a%20more%20efficient%20alternative%20to%0ATransformers.%20This%20paper%20introduces%20the%20Spatial-Spectral%20Morphological%20Mamba%0A%28MorpMamba%29%20model.%20In%20the%20MorpMamba%20model%2C%20a%20token%20generation%20module%20first%0Aconverts%20the%20Hyperspectral%20Image%20%28HSI%29%20patch%20into%20spatial-spectral%20tokens.%0AThese%20tokens%20are%20then%20processed%20by%20a%20morphology%20block%2C%20which%20computes%0Astructural%20and%20shape%20information%20using%20depthwise%20separable%20convolutional%0Aoperations.%20The%20extracted%20information%20is%20enhanced%20in%20a%20feature%20enhancement%0Amodule%20that%20adjusts%20the%20spatial%20and%20spectral%20tokens%20based%20on%20the%20center%20region%0Aof%20the%20HSI%20sample%2C%20allowing%20for%20effective%20information%20fusion%20within%20each%20block.%0ASubsequently%2C%20the%20tokens%20are%20refined%20in%20a%20multi-head%20self-attention%20block%20to%0Afurther%20improve%20the%20feature%20space.%20Finally%2C%20the%20combined%20information%20is%20fed%0Ainto%20the%20state%20space%20block%20for%20classification%20and%20the%20creation%20of%20the%20ground%0Atruth%20map.%20Experiments%20on%20widely%20used%20Hyperspectral%20%28HS%29%20datasets%20demonstrate%0Athat%20the%20MorpMamba%20model%20outperforms%20%28parametric%20efficiency%29%20both%20CNN%20and%0ATransformer%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01372v1&entry.124074799=Read"},
{"title": "DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs", "author": "Zhichun Wang and Xuan Chen", "abstract": "  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n", "link": "http://arxiv.org/abs/2408.01154v1", "date": "2024-08-02", "relevancy": 1.9471, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5231}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4927}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DERA%3A%20Dense%20Entity%20Retrieval%20for%20Entity%20Alignment%20in%20Knowledge%20Graphs&body=Title%3A%20DERA%3A%20Dense%20Entity%20Retrieval%20for%20Entity%20Alignment%20in%20Knowledge%20Graphs%0AAuthor%3A%20Zhichun%20Wang%20and%20Xuan%20Chen%0AAbstract%3A%20%20%20Entity%20Alignment%20%28EA%29%20aims%20to%20match%20equivalent%20entities%20in%20different%0AKnowledge%20Graphs%20%28KGs%29%2C%20which%20is%20essential%20for%20knowledge%20fusion%20and%0Aintegration.%20Recently%2C%20embedding-based%20EA%20has%20attracted%20significant%20attention%0Aand%20many%20approaches%20have%20been%20proposed.%20Early%20approaches%20primarily%20focus%20on%0Alearning%20entity%20embeddings%20from%20the%20structural%20features%20of%20KGs%2C%20defined%20by%0Arelation%20triples.%20Later%20methods%20incorporated%20entities%27%20names%20and%20attributes%20as%0Aauxiliary%20information%20to%20enhance%20embeddings%20for%20EA.%20However%2C%20these%20approaches%0Aoften%20used%20different%20techniques%20to%20encode%20structural%20and%20attribute%20information%2C%0Alimiting%20their%20interaction%20and%20mutual%20enhancement.%20In%20this%20work%2C%20we%20propose%20a%0Adense%20entity%20retrieval%20framework%20for%20EA%2C%20leveraging%20language%20models%20to%0Auniformly%20encode%20various%20features%20of%20entities%20and%20facilitate%20nearest%20entity%0Asearch%20across%20KGs.%20Alignment%20candidates%20are%20first%20generated%20through%20entity%0Aretrieval%2C%20which%20are%20subsequently%20reranked%20to%20determine%20the%20final%20alignments.%0AWe%20conduct%20comprehensive%20experiments%20on%20both%20cross-lingual%20and%20monolingual%20EA%0Adatasets%2C%20demonstrating%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Acompared%20to%20existing%20EA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDERA%253A%2520Dense%2520Entity%2520Retrieval%2520for%2520Entity%2520Alignment%2520in%2520Knowledge%2520Graphs%26entry.906535625%3DZhichun%2520Wang%2520and%2520Xuan%2520Chen%26entry.1292438233%3D%2520%2520Entity%2520Alignment%2520%2528EA%2529%2520aims%2520to%2520match%2520equivalent%2520entities%2520in%2520different%250AKnowledge%2520Graphs%2520%2528KGs%2529%252C%2520which%2520is%2520essential%2520for%2520knowledge%2520fusion%2520and%250Aintegration.%2520Recently%252C%2520embedding-based%2520EA%2520has%2520attracted%2520significant%2520attention%250Aand%2520many%2520approaches%2520have%2520been%2520proposed.%2520Early%2520approaches%2520primarily%2520focus%2520on%250Alearning%2520entity%2520embeddings%2520from%2520the%2520structural%2520features%2520of%2520KGs%252C%2520defined%2520by%250Arelation%2520triples.%2520Later%2520methods%2520incorporated%2520entities%2527%2520names%2520and%2520attributes%2520as%250Aauxiliary%2520information%2520to%2520enhance%2520embeddings%2520for%2520EA.%2520However%252C%2520these%2520approaches%250Aoften%2520used%2520different%2520techniques%2520to%2520encode%2520structural%2520and%2520attribute%2520information%252C%250Alimiting%2520their%2520interaction%2520and%2520mutual%2520enhancement.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Adense%2520entity%2520retrieval%2520framework%2520for%2520EA%252C%2520leveraging%2520language%2520models%2520to%250Auniformly%2520encode%2520various%2520features%2520of%2520entities%2520and%2520facilitate%2520nearest%2520entity%250Asearch%2520across%2520KGs.%2520Alignment%2520candidates%2520are%2520first%2520generated%2520through%2520entity%250Aretrieval%252C%2520which%2520are%2520subsequently%2520reranked%2520to%2520determine%2520the%2520final%2520alignments.%250AWe%2520conduct%2520comprehensive%2520experiments%2520on%2520both%2520cross-lingual%2520and%2520monolingual%2520EA%250Adatasets%252C%2520demonstrating%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%250Acompared%2520to%2520existing%2520EA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DERA%3A%20Dense%20Entity%20Retrieval%20for%20Entity%20Alignment%20in%20Knowledge%20Graphs&entry.906535625=Zhichun%20Wang%20and%20Xuan%20Chen&entry.1292438233=%20%20Entity%20Alignment%20%28EA%29%20aims%20to%20match%20equivalent%20entities%20in%20different%0AKnowledge%20Graphs%20%28KGs%29%2C%20which%20is%20essential%20for%20knowledge%20fusion%20and%0Aintegration.%20Recently%2C%20embedding-based%20EA%20has%20attracted%20significant%20attention%0Aand%20many%20approaches%20have%20been%20proposed.%20Early%20approaches%20primarily%20focus%20on%0Alearning%20entity%20embeddings%20from%20the%20structural%20features%20of%20KGs%2C%20defined%20by%0Arelation%20triples.%20Later%20methods%20incorporated%20entities%27%20names%20and%20attributes%20as%0Aauxiliary%20information%20to%20enhance%20embeddings%20for%20EA.%20However%2C%20these%20approaches%0Aoften%20used%20different%20techniques%20to%20encode%20structural%20and%20attribute%20information%2C%0Alimiting%20their%20interaction%20and%20mutual%20enhancement.%20In%20this%20work%2C%20we%20propose%20a%0Adense%20entity%20retrieval%20framework%20for%20EA%2C%20leveraging%20language%20models%20to%0Auniformly%20encode%20various%20features%20of%20entities%20and%20facilitate%20nearest%20entity%0Asearch%20across%20KGs.%20Alignment%20candidates%20are%20first%20generated%20through%20entity%0Aretrieval%2C%20which%20are%20subsequently%20reranked%20to%20determine%20the%20final%20alignments.%0AWe%20conduct%20comprehensive%20experiments%20on%20both%20cross-lingual%20and%20monolingual%20EA%0Adatasets%2C%20demonstrating%20that%20our%20approach%20achieves%20state-of-the-art%20performance%0Acompared%20to%20existing%20EA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01154v1&entry.124074799=Read"},
{"title": "UnifiedNN: Efficient Neural Network Training on the Cloud", "author": "Sifat Ut Taki and Spyridon Mastorakis and Arthi Padmanabhan", "abstract": "  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n", "link": "http://arxiv.org/abs/2408.01331v1", "date": "2024-08-02", "relevancy": 1.9436, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4833}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnifiedNN%3A%20Efficient%20Neural%20Network%20Training%20on%20the%20Cloud&body=Title%3A%20UnifiedNN%3A%20Efficient%20Neural%20Network%20Training%20on%20the%20Cloud%0AAuthor%3A%20Sifat%20Ut%20Taki%20and%20Spyridon%20Mastorakis%20and%20Arthi%20Padmanabhan%0AAbstract%3A%20%20%20Nowadays%2C%20cloud-based%20services%20are%20widely%20favored%20over%20the%20traditional%0Aapproach%20of%20locally%20training%20a%20Neural%20Network%20%28NN%29%20model.%20Oftentimes%2C%20a%20cloud%0Aservice%20processes%20multiple%20requests%20from%20users--thus%20training%20multiple%20NN%0Amodels%20concurrently.%20However%2C%20training%20NN%20models%20concurrently%20is%20a%20challenging%0Aprocess%2C%20which%20typically%20requires%20significant%20amounts%20of%20available%20computing%0Aresources%20and%20takes%20a%20long%20time%20to%20complete.%20In%20this%20paper%2C%20we%20present%0AUnifiedNN%20to%20effectively%20train%20multiple%20NN%20models%20concurrently%20on%20the%20cloud.%0AUnifiedNN%20effectively%20%22combines%22%20multiple%20NN%20models%20and%20features%20several%20memory%0Aand%20time%20conservation%20mechanisms%20to%20train%20multiple%20NN%20models%20simultaneously%0Awithout%20impacting%20the%20accuracy%20of%20the%20training%20process.%20Specifically%2C%20UnifiedNN%0Amerges%20multiple%20NN%20models%20and%20creates%20a%20large%20singular%20unified%20model%20in%20order%0Ato%20efficiently%20train%20all%20models%20at%20once.%20We%20have%20implemented%20a%20prototype%20of%0AUnifiedNN%20in%20PyTorch%20and%20we%20have%20compared%20its%20performance%20with%20relevant%0Astate-of-the-art%20frameworks.%20Our%20experimental%20results%20demonstrate%20that%0AUnifiedNN%20can%20reduce%20memory%20consumption%20by%20up%20to%2053%25%20and%20training%20time%20by%20up%20to%0A81%25%20when%20compared%20with%20vanilla%20PyTorch%20without%20impacting%20the%20model%20training%20and%0Atesting%20accuracy.%20Finally%2C%20our%20results%20indicate%20that%20UnifiedNN%20can%20reduce%0Amemory%20consumption%20by%20up%20to%2052%25%20and%20training%20time%20by%20up%20to%2041%25%20when%20compared%20to%0Astate-of-the-art%20frameworks%20when%20training%20multiple%20models%20concurrently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifiedNN%253A%2520Efficient%2520Neural%2520Network%2520Training%2520on%2520the%2520Cloud%26entry.906535625%3DSifat%2520Ut%2520Taki%2520and%2520Spyridon%2520Mastorakis%2520and%2520Arthi%2520Padmanabhan%26entry.1292438233%3D%2520%2520Nowadays%252C%2520cloud-based%2520services%2520are%2520widely%2520favored%2520over%2520the%2520traditional%250Aapproach%2520of%2520locally%2520training%2520a%2520Neural%2520Network%2520%2528NN%2529%2520model.%2520Oftentimes%252C%2520a%2520cloud%250Aservice%2520processes%2520multiple%2520requests%2520from%2520users--thus%2520training%2520multiple%2520NN%250Amodels%2520concurrently.%2520However%252C%2520training%2520NN%2520models%2520concurrently%2520is%2520a%2520challenging%250Aprocess%252C%2520which%2520typically%2520requires%2520significant%2520amounts%2520of%2520available%2520computing%250Aresources%2520and%2520takes%2520a%2520long%2520time%2520to%2520complete.%2520In%2520this%2520paper%252C%2520we%2520present%250AUnifiedNN%2520to%2520effectively%2520train%2520multiple%2520NN%2520models%2520concurrently%2520on%2520the%2520cloud.%250AUnifiedNN%2520effectively%2520%2522combines%2522%2520multiple%2520NN%2520models%2520and%2520features%2520several%2520memory%250Aand%2520time%2520conservation%2520mechanisms%2520to%2520train%2520multiple%2520NN%2520models%2520simultaneously%250Awithout%2520impacting%2520the%2520accuracy%2520of%2520the%2520training%2520process.%2520Specifically%252C%2520UnifiedNN%250Amerges%2520multiple%2520NN%2520models%2520and%2520creates%2520a%2520large%2520singular%2520unified%2520model%2520in%2520order%250Ato%2520efficiently%2520train%2520all%2520models%2520at%2520once.%2520We%2520have%2520implemented%2520a%2520prototype%2520of%250AUnifiedNN%2520in%2520PyTorch%2520and%2520we%2520have%2520compared%2520its%2520performance%2520with%2520relevant%250Astate-of-the-art%2520frameworks.%2520Our%2520experimental%2520results%2520demonstrate%2520that%250AUnifiedNN%2520can%2520reduce%2520memory%2520consumption%2520by%2520up%2520to%252053%2525%2520and%2520training%2520time%2520by%2520up%2520to%250A81%2525%2520when%2520compared%2520with%2520vanilla%2520PyTorch%2520without%2520impacting%2520the%2520model%2520training%2520and%250Atesting%2520accuracy.%2520Finally%252C%2520our%2520results%2520indicate%2520that%2520UnifiedNN%2520can%2520reduce%250Amemory%2520consumption%2520by%2520up%2520to%252052%2525%2520and%2520training%2520time%2520by%2520up%2520to%252041%2525%2520when%2520compared%2520to%250Astate-of-the-art%2520frameworks%2520when%2520training%2520multiple%2520models%2520concurrently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnifiedNN%3A%20Efficient%20Neural%20Network%20Training%20on%20the%20Cloud&entry.906535625=Sifat%20Ut%20Taki%20and%20Spyridon%20Mastorakis%20and%20Arthi%20Padmanabhan&entry.1292438233=%20%20Nowadays%2C%20cloud-based%20services%20are%20widely%20favored%20over%20the%20traditional%0Aapproach%20of%20locally%20training%20a%20Neural%20Network%20%28NN%29%20model.%20Oftentimes%2C%20a%20cloud%0Aservice%20processes%20multiple%20requests%20from%20users--thus%20training%20multiple%20NN%0Amodels%20concurrently.%20However%2C%20training%20NN%20models%20concurrently%20is%20a%20challenging%0Aprocess%2C%20which%20typically%20requires%20significant%20amounts%20of%20available%20computing%0Aresources%20and%20takes%20a%20long%20time%20to%20complete.%20In%20this%20paper%2C%20we%20present%0AUnifiedNN%20to%20effectively%20train%20multiple%20NN%20models%20concurrently%20on%20the%20cloud.%0AUnifiedNN%20effectively%20%22combines%22%20multiple%20NN%20models%20and%20features%20several%20memory%0Aand%20time%20conservation%20mechanisms%20to%20train%20multiple%20NN%20models%20simultaneously%0Awithout%20impacting%20the%20accuracy%20of%20the%20training%20process.%20Specifically%2C%20UnifiedNN%0Amerges%20multiple%20NN%20models%20and%20creates%20a%20large%20singular%20unified%20model%20in%20order%0Ato%20efficiently%20train%20all%20models%20at%20once.%20We%20have%20implemented%20a%20prototype%20of%0AUnifiedNN%20in%20PyTorch%20and%20we%20have%20compared%20its%20performance%20with%20relevant%0Astate-of-the-art%20frameworks.%20Our%20experimental%20results%20demonstrate%20that%0AUnifiedNN%20can%20reduce%20memory%20consumption%20by%20up%20to%2053%25%20and%20training%20time%20by%20up%20to%0A81%25%20when%20compared%20with%20vanilla%20PyTorch%20without%20impacting%20the%20model%20training%20and%0Atesting%20accuracy.%20Finally%2C%20our%20results%20indicate%20that%20UnifiedNN%20can%20reduce%0Amemory%20consumption%20by%20up%20to%2052%25%20and%20training%20time%20by%20up%20to%2041%25%20when%20compared%20to%0Astate-of-the-art%20frameworks%20when%20training%20multiple%20models%20concurrently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01331v1&entry.124074799=Read"},
{"title": "TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation", "author": "Yicheng Lin and Dandan Zhang and Yun Liu", "abstract": "  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n", "link": "http://arxiv.org/abs/2408.01156v1", "date": "2024-08-02", "relevancy": 1.9409, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4995}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCR-GPT%3A%20Integrating%20Autoregressive%20Model%20and%20Reinforcement%20Learning%20for%0A%20%20T-Cell%20Receptor%20Repertoires%20Generation&body=Title%3A%20TCR-GPT%3A%20Integrating%20Autoregressive%20Model%20and%20Reinforcement%20Learning%20for%0A%20%20T-Cell%20Receptor%20Repertoires%20Generation%0AAuthor%3A%20Yicheng%20Lin%20and%20Dandan%20Zhang%20and%20Yun%20Liu%0AAbstract%3A%20%20%20T-cell%20receptors%20%28TCRs%29%20play%20a%20crucial%20role%20in%20the%20immune%20system%20by%0Arecognizing%20and%20binding%20to%20specific%20antigens%20presented%20by%20infected%20or%20cancerous%0Acells.%20Understanding%20the%20sequence%20patterns%20of%20TCRs%20is%20essential%20for%20developing%0Atargeted%20immune%20therapies%20and%20designing%20effective%20vaccines.%20Language%20models%2C%0Asuch%20as%20auto-regressive%20transformers%2C%20offer%20a%20powerful%20solution%20to%20this%20problem%0Aby%20learning%20the%20probability%20distributions%20of%20TCR%20repertoires%2C%20enabling%20the%0Ageneration%20of%20new%20TCR%20sequences%20that%20inherit%20the%20underlying%20patterns%20of%20the%0Arepertoire.%20We%20introduce%20TCR-GPT%2C%20a%20probabilistic%20model%20built%20on%20a%20decoder-only%0Atransformer%20architecture%2C%20designed%20to%20uncover%20and%20replicate%20sequence%20patterns%0Ain%20TCR%20repertoires.%20TCR-GPT%20demonstrates%20an%20accuracy%20of%200.953%20in%20inferring%0Asequence%20probability%20distributions%20measured%20by%20Pearson%20correlation%20coefficient.%0AFurthermore%2C%20by%20leveraging%20Reinforcement%20Learning%28RL%29%2C%20we%20adapted%20the%0Adistribution%20of%20TCR%20sequences%20to%20generate%20TCRs%20capable%20of%20recognizing%20specific%0Apeptides%2C%20offering%20significant%20potential%20for%20advancing%20targeted%20immune%0Atherapies%20and%20vaccine%20development.%20With%20the%20efficacy%20of%20RL%2C%20fine-tuned%0Apretrained%20TCR-GPT%20models%20demonstrated%20the%20ability%20to%20produce%20TCR%20repertoires%0Alikely%20to%20bind%20specific%20peptides%2C%20illustrating%20RL%27s%20efficiency%20in%20enhancing%20the%0Amodel%27s%20adaptability%20to%20the%20probability%20distributions%20of%20biologically%20relevant%0ATCR%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCR-GPT%253A%2520Integrating%2520Autoregressive%2520Model%2520and%2520Reinforcement%2520Learning%2520for%250A%2520%2520T-Cell%2520Receptor%2520Repertoires%2520Generation%26entry.906535625%3DYicheng%2520Lin%2520and%2520Dandan%2520Zhang%2520and%2520Yun%2520Liu%26entry.1292438233%3D%2520%2520T-cell%2520receptors%2520%2528TCRs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520immune%2520system%2520by%250Arecognizing%2520and%2520binding%2520to%2520specific%2520antigens%2520presented%2520by%2520infected%2520or%2520cancerous%250Acells.%2520Understanding%2520the%2520sequence%2520patterns%2520of%2520TCRs%2520is%2520essential%2520for%2520developing%250Atargeted%2520immune%2520therapies%2520and%2520designing%2520effective%2520vaccines.%2520Language%2520models%252C%250Asuch%2520as%2520auto-regressive%2520transformers%252C%2520offer%2520a%2520powerful%2520solution%2520to%2520this%2520problem%250Aby%2520learning%2520the%2520probability%2520distributions%2520of%2520TCR%2520repertoires%252C%2520enabling%2520the%250Ageneration%2520of%2520new%2520TCR%2520sequences%2520that%2520inherit%2520the%2520underlying%2520patterns%2520of%2520the%250Arepertoire.%2520We%2520introduce%2520TCR-GPT%252C%2520a%2520probabilistic%2520model%2520built%2520on%2520a%2520decoder-only%250Atransformer%2520architecture%252C%2520designed%2520to%2520uncover%2520and%2520replicate%2520sequence%2520patterns%250Ain%2520TCR%2520repertoires.%2520TCR-GPT%2520demonstrates%2520an%2520accuracy%2520of%25200.953%2520in%2520inferring%250Asequence%2520probability%2520distributions%2520measured%2520by%2520Pearson%2520correlation%2520coefficient.%250AFurthermore%252C%2520by%2520leveraging%2520Reinforcement%2520Learning%2528RL%2529%252C%2520we%2520adapted%2520the%250Adistribution%2520of%2520TCR%2520sequences%2520to%2520generate%2520TCRs%2520capable%2520of%2520recognizing%2520specific%250Apeptides%252C%2520offering%2520significant%2520potential%2520for%2520advancing%2520targeted%2520immune%250Atherapies%2520and%2520vaccine%2520development.%2520With%2520the%2520efficacy%2520of%2520RL%252C%2520fine-tuned%250Apretrained%2520TCR-GPT%2520models%2520demonstrated%2520the%2520ability%2520to%2520produce%2520TCR%2520repertoires%250Alikely%2520to%2520bind%2520specific%2520peptides%252C%2520illustrating%2520RL%2527s%2520efficiency%2520in%2520enhancing%2520the%250Amodel%2527s%2520adaptability%2520to%2520the%2520probability%2520distributions%2520of%2520biologically%2520relevant%250ATCR%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCR-GPT%3A%20Integrating%20Autoregressive%20Model%20and%20Reinforcement%20Learning%20for%0A%20%20T-Cell%20Receptor%20Repertoires%20Generation&entry.906535625=Yicheng%20Lin%20and%20Dandan%20Zhang%20and%20Yun%20Liu&entry.1292438233=%20%20T-cell%20receptors%20%28TCRs%29%20play%20a%20crucial%20role%20in%20the%20immune%20system%20by%0Arecognizing%20and%20binding%20to%20specific%20antigens%20presented%20by%20infected%20or%20cancerous%0Acells.%20Understanding%20the%20sequence%20patterns%20of%20TCRs%20is%20essential%20for%20developing%0Atargeted%20immune%20therapies%20and%20designing%20effective%20vaccines.%20Language%20models%2C%0Asuch%20as%20auto-regressive%20transformers%2C%20offer%20a%20powerful%20solution%20to%20this%20problem%0Aby%20learning%20the%20probability%20distributions%20of%20TCR%20repertoires%2C%20enabling%20the%0Ageneration%20of%20new%20TCR%20sequences%20that%20inherit%20the%20underlying%20patterns%20of%20the%0Arepertoire.%20We%20introduce%20TCR-GPT%2C%20a%20probabilistic%20model%20built%20on%20a%20decoder-only%0Atransformer%20architecture%2C%20designed%20to%20uncover%20and%20replicate%20sequence%20patterns%0Ain%20TCR%20repertoires.%20TCR-GPT%20demonstrates%20an%20accuracy%20of%200.953%20in%20inferring%0Asequence%20probability%20distributions%20measured%20by%20Pearson%20correlation%20coefficient.%0AFurthermore%2C%20by%20leveraging%20Reinforcement%20Learning%28RL%29%2C%20we%20adapted%20the%0Adistribution%20of%20TCR%20sequences%20to%20generate%20TCRs%20capable%20of%20recognizing%20specific%0Apeptides%2C%20offering%20significant%20potential%20for%20advancing%20targeted%20immune%0Atherapies%20and%20vaccine%20development.%20With%20the%20efficacy%20of%20RL%2C%20fine-tuned%0Apretrained%20TCR-GPT%20models%20demonstrated%20the%20ability%20to%20produce%20TCR%20repertoires%0Alikely%20to%20bind%20specific%20peptides%2C%20illustrating%20RL%27s%20efficiency%20in%20enhancing%20the%0Amodel%27s%20adaptability%20to%20the%20probability%20distributions%20of%20biologically%20relevant%0ATCR%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01156v1&entry.124074799=Read"},
{"title": "CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning", "author": "Yu Feng and Zhen Tian and Yifan Zhu and Zongfu Han and Haoran Luo and Guangwei Zhang and Meina Song", "abstract": "  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n", "link": "http://arxiv.org/abs/2407.21043v2", "date": "2024-08-02", "relevancy": 1.9289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Prompt%3A%20Composition-Based%20Cross-modal%20Prompting%20for%0A%20%20Domain-Incremental%20Continual%20Learning&body=Title%3A%20CP-Prompt%3A%20Composition-Based%20Cross-modal%20Prompting%20for%0A%20%20Domain-Incremental%20Continual%20Learning%0AAuthor%3A%20Yu%20Feng%20and%20Zhen%20Tian%20and%20Yifan%20Zhu%20and%20Zongfu%20Han%20and%20Haoran%20Luo%20and%20Guangwei%20Zhang%20and%20Meina%20Song%0AAbstract%3A%20%20%20The%20key%20challenge%20of%20cross-modal%20domain-incremental%20learning%20%28DIL%29%20is%20to%0Aenable%20the%20learning%20model%20to%20continuously%20learn%20from%20novel%20data%20with%20different%0Afeature%20distributions%20under%20the%20same%20task%20without%20forgetting%20old%20ones.%20However%2C%0Aexisting%20top-performing%20methods%20still%20cause%20high%20forgetting%20rates%2C%20by%20lacking%0Aintra-domain%20knowledge%20extraction%20and%20inter-domain%20common%20prompting%20strategy.%0AIn%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20framework%2C%20CP-Prompt%2C%20by%0Atraining%20limited%20parameters%20to%20instruct%20a%20pre-trained%20model%20to%20learn%20new%0Adomains%20and%20avoid%20forgetting%20existing%20feature%20distributions.%20CP-Prompt%20captures%0Aintra-domain%20knowledge%20by%20compositionally%20inserting%20personalized%20prompts%20on%0Amulti-head%20self-attention%20layers%20and%20then%20learns%20the%20inter-domain%20knowledge%0Awith%20a%20common%20prompting%20strategy.%20CP-Prompt%20shows%20superiority%20compared%20with%0Astate-of-the-art%20baselines%20among%20three%20widely%20evaluated%20DIL%20tasks.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/dannis97500/CP_Prompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Prompt%253A%2520Composition-Based%2520Cross-modal%2520Prompting%2520for%250A%2520%2520Domain-Incremental%2520Continual%2520Learning%26entry.906535625%3DYu%2520Feng%2520and%2520Zhen%2520Tian%2520and%2520Yifan%2520Zhu%2520and%2520Zongfu%2520Han%2520and%2520Haoran%2520Luo%2520and%2520Guangwei%2520Zhang%2520and%2520Meina%2520Song%26entry.1292438233%3D%2520%2520The%2520key%2520challenge%2520of%2520cross-modal%2520domain-incremental%2520learning%2520%2528DIL%2529%2520is%2520to%250Aenable%2520the%2520learning%2520model%2520to%2520continuously%2520learn%2520from%2520novel%2520data%2520with%2520different%250Afeature%2520distributions%2520under%2520the%2520same%2520task%2520without%2520forgetting%2520old%2520ones.%2520However%252C%250Aexisting%2520top-performing%2520methods%2520still%2520cause%2520high%2520forgetting%2520rates%252C%2520by%2520lacking%250Aintra-domain%2520knowledge%2520extraction%2520and%2520inter-domain%2520common%2520prompting%2520strategy.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520framework%252C%2520CP-Prompt%252C%2520by%250Atraining%2520limited%2520parameters%2520to%2520instruct%2520a%2520pre-trained%2520model%2520to%2520learn%2520new%250Adomains%2520and%2520avoid%2520forgetting%2520existing%2520feature%2520distributions.%2520CP-Prompt%2520captures%250Aintra-domain%2520knowledge%2520by%2520compositionally%2520inserting%2520personalized%2520prompts%2520on%250Amulti-head%2520self-attention%2520layers%2520and%2520then%2520learns%2520the%2520inter-domain%2520knowledge%250Awith%2520a%2520common%2520prompting%2520strategy.%2520CP-Prompt%2520shows%2520superiority%2520compared%2520with%250Astate-of-the-art%2520baselines%2520among%2520three%2520widely%2520evaluated%2520DIL%2520tasks.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/dannis97500/CP_Prompt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Prompt%3A%20Composition-Based%20Cross-modal%20Prompting%20for%0A%20%20Domain-Incremental%20Continual%20Learning&entry.906535625=Yu%20Feng%20and%20Zhen%20Tian%20and%20Yifan%20Zhu%20and%20Zongfu%20Han%20and%20Haoran%20Luo%20and%20Guangwei%20Zhang%20and%20Meina%20Song&entry.1292438233=%20%20The%20key%20challenge%20of%20cross-modal%20domain-incremental%20learning%20%28DIL%29%20is%20to%0Aenable%20the%20learning%20model%20to%20continuously%20learn%20from%20novel%20data%20with%20different%0Afeature%20distributions%20under%20the%20same%20task%20without%20forgetting%20old%20ones.%20However%2C%0Aexisting%20top-performing%20methods%20still%20cause%20high%20forgetting%20rates%2C%20by%20lacking%0Aintra-domain%20knowledge%20extraction%20and%20inter-domain%20common%20prompting%20strategy.%0AIn%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20framework%2C%20CP-Prompt%2C%20by%0Atraining%20limited%20parameters%20to%20instruct%20a%20pre-trained%20model%20to%20learn%20new%0Adomains%20and%20avoid%20forgetting%20existing%20feature%20distributions.%20CP-Prompt%20captures%0Aintra-domain%20knowledge%20by%20compositionally%20inserting%20personalized%20prompts%20on%0Amulti-head%20self-attention%20layers%20and%20then%20learns%20the%20inter-domain%20knowledge%0Awith%20a%20common%20prompting%20strategy.%20CP-Prompt%20shows%20superiority%20compared%20with%0Astate-of-the-art%20baselines%20among%20three%20widely%20evaluated%20DIL%20tasks.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/dannis97500/CP_Prompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21043v2&entry.124074799=Read"},
{"title": "Insights from the Design Space Exploration of Flow-Guided Nanoscale\n  Localization", "author": "Filip Lemic and Gerard Calvo Bartra and Arnau Brosa L\u00f3pez and Jorge Torres G\u00f3mez and Jakob Struye and Falko Dressler and Sergi Abadal and Xavier Costa Perez", "abstract": "  Nanodevices with Terahertz (THz)-based wireless communication capabilities\nare providing a primer for flow-guided localization within the human\nbloodstreams. Such localization is allowing for assigning the locations of\nsensed events with the events themselves, providing benefits along the lines of\nearly and precise diagnostics, and reduced costs and invasiveness. Flow-guided\nlocalization is still in a rudimentary phase, with only a handful of works\ntargeting the problem. Nonetheless, the performance assessments of the proposed\nsolutions are already carried out in a non-standardized way, usually along a\nsingle performance metric, and ignoring various aspects that are relevant at\nsuch a scale (e.g., nanodevices' limited energy) and for such a challenging\nenvironment (e.g., extreme attenuation of in-body THz propagation). As such,\nthese assessments feature low levels of realism and cannot be compared in an\nobjective way. Toward addressing this issue, we account for the environmental\nand scale-related peculiarities of the scenario and assess the performance of\ntwo state-of-the-art flow-guided localization approaches along a set of\nheterogeneous performance metrics such as the accuracy and reliability of\nlocalization.\n", "link": "http://arxiv.org/abs/2305.18493v3", "date": "2024-08-02", "relevancy": 1.9266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4936}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4761}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20from%20the%20Design%20Space%20Exploration%20of%20Flow-Guided%20Nanoscale%0A%20%20Localization&body=Title%3A%20Insights%20from%20the%20Design%20Space%20Exploration%20of%20Flow-Guided%20Nanoscale%0A%20%20Localization%0AAuthor%3A%20Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Arnau%20Brosa%20L%C3%B3pez%20and%20Jorge%20Torres%20G%C3%B3mez%20and%20Jakob%20Struye%20and%20Falko%20Dressler%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20Perez%0AAbstract%3A%20%20%20Nanodevices%20with%20Terahertz%20%28THz%29-based%20wireless%20communication%20capabilities%0Aare%20providing%20a%20primer%20for%20flow-guided%20localization%20within%20the%20human%0Abloodstreams.%20Such%20localization%20is%20allowing%20for%20assigning%20the%20locations%20of%0Asensed%20events%20with%20the%20events%20themselves%2C%20providing%20benefits%20along%20the%20lines%20of%0Aearly%20and%20precise%20diagnostics%2C%20and%20reduced%20costs%20and%20invasiveness.%20Flow-guided%0Alocalization%20is%20still%20in%20a%20rudimentary%20phase%2C%20with%20only%20a%20handful%20of%20works%0Atargeting%20the%20problem.%20Nonetheless%2C%20the%20performance%20assessments%20of%20the%20proposed%0Asolutions%20are%20already%20carried%20out%20in%20a%20non-standardized%20way%2C%20usually%20along%20a%0Asingle%20performance%20metric%2C%20and%20ignoring%20various%20aspects%20that%20are%20relevant%20at%0Asuch%20a%20scale%20%28e.g.%2C%20nanodevices%27%20limited%20energy%29%20and%20for%20such%20a%20challenging%0Aenvironment%20%28e.g.%2C%20extreme%20attenuation%20of%20in-body%20THz%20propagation%29.%20As%20such%2C%0Athese%20assessments%20feature%20low%20levels%20of%20realism%20and%20cannot%20be%20compared%20in%20an%0Aobjective%20way.%20Toward%20addressing%20this%20issue%2C%20we%20account%20for%20the%20environmental%0Aand%20scale-related%20peculiarities%20of%20the%20scenario%20and%20assess%20the%20performance%20of%0Atwo%20state-of-the-art%20flow-guided%20localization%20approaches%20along%20a%20set%20of%0Aheterogeneous%20performance%20metrics%20such%20as%20the%20accuracy%20and%20reliability%20of%0Alocalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.18493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520from%2520the%2520Design%2520Space%2520Exploration%2520of%2520Flow-Guided%2520Nanoscale%250A%2520%2520Localization%26entry.906535625%3DFilip%2520Lemic%2520and%2520Gerard%2520Calvo%2520Bartra%2520and%2520Arnau%2520Brosa%2520L%25C3%25B3pez%2520and%2520Jorge%2520Torres%2520G%25C3%25B3mez%2520and%2520Jakob%2520Struye%2520and%2520Falko%2520Dressler%2520and%2520Sergi%2520Abadal%2520and%2520Xavier%2520Costa%2520Perez%26entry.1292438233%3D%2520%2520Nanodevices%2520with%2520Terahertz%2520%2528THz%2529-based%2520wireless%2520communication%2520capabilities%250Aare%2520providing%2520a%2520primer%2520for%2520flow-guided%2520localization%2520within%2520the%2520human%250Abloodstreams.%2520Such%2520localization%2520is%2520allowing%2520for%2520assigning%2520the%2520locations%2520of%250Asensed%2520events%2520with%2520the%2520events%2520themselves%252C%2520providing%2520benefits%2520along%2520the%2520lines%2520of%250Aearly%2520and%2520precise%2520diagnostics%252C%2520and%2520reduced%2520costs%2520and%2520invasiveness.%2520Flow-guided%250Alocalization%2520is%2520still%2520in%2520a%2520rudimentary%2520phase%252C%2520with%2520only%2520a%2520handful%2520of%2520works%250Atargeting%2520the%2520problem.%2520Nonetheless%252C%2520the%2520performance%2520assessments%2520of%2520the%2520proposed%250Asolutions%2520are%2520already%2520carried%2520out%2520in%2520a%2520non-standardized%2520way%252C%2520usually%2520along%2520a%250Asingle%2520performance%2520metric%252C%2520and%2520ignoring%2520various%2520aspects%2520that%2520are%2520relevant%2520at%250Asuch%2520a%2520scale%2520%2528e.g.%252C%2520nanodevices%2527%2520limited%2520energy%2529%2520and%2520for%2520such%2520a%2520challenging%250Aenvironment%2520%2528e.g.%252C%2520extreme%2520attenuation%2520of%2520in-body%2520THz%2520propagation%2529.%2520As%2520such%252C%250Athese%2520assessments%2520feature%2520low%2520levels%2520of%2520realism%2520and%2520cannot%2520be%2520compared%2520in%2520an%250Aobjective%2520way.%2520Toward%2520addressing%2520this%2520issue%252C%2520we%2520account%2520for%2520the%2520environmental%250Aand%2520scale-related%2520peculiarities%2520of%2520the%2520scenario%2520and%2520assess%2520the%2520performance%2520of%250Atwo%2520state-of-the-art%2520flow-guided%2520localization%2520approaches%2520along%2520a%2520set%2520of%250Aheterogeneous%2520performance%2520metrics%2520such%2520as%2520the%2520accuracy%2520and%2520reliability%2520of%250Alocalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.18493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20from%20the%20Design%20Space%20Exploration%20of%20Flow-Guided%20Nanoscale%0A%20%20Localization&entry.906535625=Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Arnau%20Brosa%20L%C3%B3pez%20and%20Jorge%20Torres%20G%C3%B3mez%20and%20Jakob%20Struye%20and%20Falko%20Dressler%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20Perez&entry.1292438233=%20%20Nanodevices%20with%20Terahertz%20%28THz%29-based%20wireless%20communication%20capabilities%0Aare%20providing%20a%20primer%20for%20flow-guided%20localization%20within%20the%20human%0Abloodstreams.%20Such%20localization%20is%20allowing%20for%20assigning%20the%20locations%20of%0Asensed%20events%20with%20the%20events%20themselves%2C%20providing%20benefits%20along%20the%20lines%20of%0Aearly%20and%20precise%20diagnostics%2C%20and%20reduced%20costs%20and%20invasiveness.%20Flow-guided%0Alocalization%20is%20still%20in%20a%20rudimentary%20phase%2C%20with%20only%20a%20handful%20of%20works%0Atargeting%20the%20problem.%20Nonetheless%2C%20the%20performance%20assessments%20of%20the%20proposed%0Asolutions%20are%20already%20carried%20out%20in%20a%20non-standardized%20way%2C%20usually%20along%20a%0Asingle%20performance%20metric%2C%20and%20ignoring%20various%20aspects%20that%20are%20relevant%20at%0Asuch%20a%20scale%20%28e.g.%2C%20nanodevices%27%20limited%20energy%29%20and%20for%20such%20a%20challenging%0Aenvironment%20%28e.g.%2C%20extreme%20attenuation%20of%20in-body%20THz%20propagation%29.%20As%20such%2C%0Athese%20assessments%20feature%20low%20levels%20of%20realism%20and%20cannot%20be%20compared%20in%20an%0Aobjective%20way.%20Toward%20addressing%20this%20issue%2C%20we%20account%20for%20the%20environmental%0Aand%20scale-related%20peculiarities%20of%20the%20scenario%20and%20assess%20the%20performance%20of%0Atwo%20state-of-the-art%20flow-guided%20localization%20approaches%20along%20a%20set%20of%0Aheterogeneous%20performance%20metrics%20such%20as%20the%20accuracy%20and%20reliability%20of%0Alocalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18493v3&entry.124074799=Read"},
{"title": "Gemma 2: Improving Open Language Models at a Practical Size", "author": " Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and L\u00e9onard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ram\u00e9 and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozi\u0144ska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Pluci\u0144ska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin G\u00f6rner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Perrin and S\u00e9bastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev", "abstract": "  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n", "link": "http://arxiv.org/abs/2408.00118v2", "date": "2024-08-02", "relevancy": 1.9166, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4829}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4782}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemma%202%3A%20Improving%20Open%20Language%20Models%20at%20a%20Practical%20Size&body=Title%3A%20Gemma%202%3A%20Improving%20Open%20Language%20Models%20at%20a%20Practical%20Size%0AAuthor%3A%20%20Gemma%20Team%20and%20Morgane%20Riviere%20and%20Shreya%20Pathak%20and%20Pier%20Giuseppe%20Sessa%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20L%C3%A9onard%20Hussenot%20and%20Thomas%20Mesnard%20and%20Bobak%20Shahriari%20and%20Alexandre%20Ram%C3%A9%20and%20Johan%20Ferret%20and%20Peter%20Liu%20and%20Pouya%20Tafti%20and%20Abe%20Friesen%20and%20Michelle%20Casbon%20and%20Sabela%20Ramos%20and%20Ravin%20Kumar%20and%20Charline%20Le%20Lan%20and%20Sammy%20Jerome%20and%20Anton%20Tsitsulin%20and%20Nino%20Vieillard%20and%20Piotr%20Stanczyk%20and%20Sertan%20Girgin%20and%20Nikola%20Momchev%20and%20Matt%20Hoffman%20and%20Shantanu%20Thakoor%20and%20Jean-Bastien%20Grill%20and%20Behnam%20Neyshabur%20and%20Olivier%20Bachem%20and%20Alanna%20Walton%20and%20Aliaksei%20Severyn%20and%20Alicia%20Parrish%20and%20Aliya%20Ahmad%20and%20Allen%20Hutchison%20and%20Alvin%20Abdagic%20and%20Amanda%20Carl%20and%20Amy%20Shen%20and%20Andy%20Brock%20and%20Andy%20Coenen%20and%20Anthony%20Laforge%20and%20Antonia%20Paterson%20and%20Ben%20Bastian%20and%20Bilal%20Piot%20and%20Bo%20Wu%20and%20Brandon%20Royal%20and%20Charlie%20Chen%20and%20Chintu%20Kumar%20and%20Chris%20Perry%20and%20Chris%20Welty%20and%20Christopher%20A.%20Choquette-Choo%20and%20Danila%20Sinopalnikov%20and%20David%20Weinberger%20and%20Dimple%20Vijaykumar%20and%20Dominika%20Rogozi%C5%84ska%20and%20Dustin%20Herbison%20and%20Elisa%20Bandy%20and%20Emma%20Wang%20and%20Eric%20Noland%20and%20Erica%20Moreira%20and%20Evan%20Senter%20and%20Evgenii%20Eltyshev%20and%20Francesco%20Visin%20and%20Gabriel%20Rasskin%20and%20Gary%20Wei%20and%20Glenn%20Cameron%20and%20Gus%20Martins%20and%20Hadi%20Hashemi%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Harleen%20Batra%20and%20Harsh%20Dhand%20and%20Ivan%20Nardini%20and%20Jacinda%20Mein%20and%20Jack%20Zhou%20and%20James%20Svensson%20and%20Jeff%20Stanway%20and%20Jetha%20Chan%20and%20Jin%20Peng%20Zhou%20and%20Joana%20Carrasqueira%20and%20Joana%20Iljazi%20and%20Jocelyn%20Becker%20and%20Joe%20Fernandez%20and%20Joost%20van%20Amersfoort%20and%20Josh%20Gordon%20and%20Josh%20Lipschultz%20and%20Josh%20Newlan%20and%20Ju-yeong%20Ji%20and%20Kareem%20Mohamed%20and%20Kartikeya%20Badola%20and%20Kat%20Black%20and%20Katie%20Millican%20and%20Keelin%20McDonell%20and%20Kelvin%20Nguyen%20and%20Kiranbir%20Sodhia%20and%20Kish%20Greene%20and%20Lars%20Lowe%20Sjoesund%20and%20Lauren%20Usui%20and%20Laurent%20Sifre%20and%20Lena%20Heuermann%20and%20Leticia%20Lago%20and%20Lilly%20McNealus%20and%20Livio%20Baldini%20Soares%20and%20Logan%20Kilpatrick%20and%20Lucas%20Dixon%20and%20Luciano%20Martins%20and%20Machel%20Reid%20and%20Manvinder%20Singh%20and%20Mark%20Iverson%20and%20Martin%20G%C3%B6rner%20and%20Mat%20Velloso%20and%20Mateo%20Wirth%20and%20Matt%20Davidow%20and%20Matt%20Miller%20and%20Matthew%20Rahtz%20and%20Matthew%20Watson%20and%20Meg%20Risdal%20and%20Mehran%20Kazemi%20and%20Michael%20Moynihan%20and%20Ming%20Zhang%20and%20Minsuk%20Kahng%20and%20Minwoo%20Park%20and%20Mofi%20Rahman%20and%20Mohit%20Khatwani%20and%20Natalie%20Dao%20and%20Nenshad%20Bardoliwalla%20and%20Nesh%20Devanathan%20and%20Neta%20Dumai%20and%20Nilay%20Chauhan%20and%20Oscar%20Wahltinez%20and%20Pankil%20Botarda%20and%20Parker%20Barnes%20and%20Paul%20Barham%20and%20Paul%20Michel%20and%20Pengchong%20Jin%20and%20Petko%20Georgiev%20and%20Phil%20Culliton%20and%20Pradeep%20Kuppala%20and%20Ramona%20Comanescu%20and%20Ramona%20Merhej%20and%20Reena%20Jana%20and%20Reza%20Ardeshir%20Rokni%20and%20Rishabh%20Agarwal%20and%20Ryan%20Mullins%20and%20Samaneh%20Saadat%20and%20Sara%20Mc%20Carthy%20and%20Sarah%20Perrin%20and%20S%C3%A9bastien%20M.%20R.%20Arnold%20and%20Sebastian%20Krause%20and%20Shengyang%20Dai%20and%20Shruti%20Garg%20and%20Shruti%20Sheth%20and%20Sue%20Ronstrom%20and%20Susan%20Chan%20and%20Timothy%20Jordan%20and%20Ting%20Yu%20and%20Tom%20Eccles%20and%20Tom%20Hennigan%20and%20Tomas%20Kocisky%20and%20Tulsee%20Doshi%20and%20Vihan%20Jain%20and%20Vikas%20Yadav%20and%20Vilobh%20Meshram%20and%20Vishal%20Dharmadhikari%20and%20Warren%20Barkley%20and%20Wei%20Wei%20and%20Wenming%20Ye%20and%20Woohyun%20Han%20and%20Woosuk%20Kwon%20and%20Xiang%20Xu%20and%20Zhe%20Shen%20and%20Zhitao%20Gong%20and%20Zichuan%20Wei%20and%20Victor%20Cotruta%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Minh%20Giang%20and%20Ludovic%20Peran%20and%20Tris%20Warkentin%20and%20Eli%20Collins%20and%20Joelle%20Barral%20and%20Zoubin%20Ghahramani%20and%20Raia%20Hadsell%20and%20D.%20Sculley%20and%20Jeanine%20Banks%20and%20Anca%20Dragan%20and%20Slav%20Petrov%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Clement%20Farabet%20and%20Elena%20Buchatskaya%20and%20Sebastian%20Borgeaud%20and%20Noah%20Fiedel%20and%20Armand%20Joulin%20and%20Kathleen%20Kenealy%20and%20Robert%20Dadashi%20and%20Alek%20Andreev%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Gemma%202%2C%20a%20new%20addition%20to%20the%20Gemma%20family%20of%0Alightweight%2C%20state-of-the-art%20open%20models%2C%20ranging%20in%20scale%20from%202%20billion%20to%0A27%20billion%20parameters.%20In%20this%20new%20version%2C%20we%20apply%20several%20known%20technical%0Amodifications%20to%20the%20Transformer%20architecture%2C%20such%20as%20interleaving%0Alocal-global%20attentions%20%28Beltagy%20et%20al.%2C%202020a%29%20and%20group-query%20attention%0A%28Ainslie%20et%20al.%2C%202023%29.%20We%20also%20train%20the%202B%20and%209B%20models%20with%20knowledge%0Adistillation%20%28Hinton%20et%20al.%2C%202015%29%20instead%20of%20next%20token%20prediction.%20The%0Aresulting%20models%20deliver%20the%20best%20performance%20for%20their%20size%2C%20and%20even%20offer%0Acompetitive%20alternatives%20to%20models%20that%20are%202-3%20times%20bigger.%20We%20release%20all%0Aour%20models%20to%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemma%25202%253A%2520Improving%2520Open%2520Language%2520Models%2520at%2520a%2520Practical%2520Size%26entry.906535625%3D%2520Gemma%2520Team%2520and%2520Morgane%2520Riviere%2520and%2520Shreya%2520Pathak%2520and%2520Pier%2520Giuseppe%2520Sessa%2520and%2520Cassidy%2520Hardin%2520and%2520Surya%2520Bhupatiraju%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Thomas%2520Mesnard%2520and%2520Bobak%2520Shahriari%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Johan%2520Ferret%2520and%2520Peter%2520Liu%2520and%2520Pouya%2520Tafti%2520and%2520Abe%2520Friesen%2520and%2520Michelle%2520Casbon%2520and%2520Sabela%2520Ramos%2520and%2520Ravin%2520Kumar%2520and%2520Charline%2520Le%2520Lan%2520and%2520Sammy%2520Jerome%2520and%2520Anton%2520Tsitsulin%2520and%2520Nino%2520Vieillard%2520and%2520Piotr%2520Stanczyk%2520and%2520Sertan%2520Girgin%2520and%2520Nikola%2520Momchev%2520and%2520Matt%2520Hoffman%2520and%2520Shantanu%2520Thakoor%2520and%2520Jean-Bastien%2520Grill%2520and%2520Behnam%2520Neyshabur%2520and%2520Olivier%2520Bachem%2520and%2520Alanna%2520Walton%2520and%2520Aliaksei%2520Severyn%2520and%2520Alicia%2520Parrish%2520and%2520Aliya%2520Ahmad%2520and%2520Allen%2520Hutchison%2520and%2520Alvin%2520Abdagic%2520and%2520Amanda%2520Carl%2520and%2520Amy%2520Shen%2520and%2520Andy%2520Brock%2520and%2520Andy%2520Coenen%2520and%2520Anthony%2520Laforge%2520and%2520Antonia%2520Paterson%2520and%2520Ben%2520Bastian%2520and%2520Bilal%2520Piot%2520and%2520Bo%2520Wu%2520and%2520Brandon%2520Royal%2520and%2520Charlie%2520Chen%2520and%2520Chintu%2520Kumar%2520and%2520Chris%2520Perry%2520and%2520Chris%2520Welty%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Danila%2520Sinopalnikov%2520and%2520David%2520Weinberger%2520and%2520Dimple%2520Vijaykumar%2520and%2520Dominika%2520Rogozi%25C5%2584ska%2520and%2520Dustin%2520Herbison%2520and%2520Elisa%2520Bandy%2520and%2520Emma%2520Wang%2520and%2520Eric%2520Noland%2520and%2520Erica%2520Moreira%2520and%2520Evan%2520Senter%2520and%2520Evgenii%2520Eltyshev%2520and%2520Francesco%2520Visin%2520and%2520Gabriel%2520Rasskin%2520and%2520Gary%2520Wei%2520and%2520Glenn%2520Cameron%2520and%2520Gus%2520Martins%2520and%2520Hadi%2520Hashemi%2520and%2520Hanna%2520Klimczak-Pluci%25C5%2584ska%2520and%2520Harleen%2520Batra%2520and%2520Harsh%2520Dhand%2520and%2520Ivan%2520Nardini%2520and%2520Jacinda%2520Mein%2520and%2520Jack%2520Zhou%2520and%2520James%2520Svensson%2520and%2520Jeff%2520Stanway%2520and%2520Jetha%2520Chan%2520and%2520Jin%2520Peng%2520Zhou%2520and%2520Joana%2520Carrasqueira%2520and%2520Joana%2520Iljazi%2520and%2520Jocelyn%2520Becker%2520and%2520Joe%2520Fernandez%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Josh%2520Gordon%2520and%2520Josh%2520Lipschultz%2520and%2520Josh%2520Newlan%2520and%2520Ju-yeong%2520Ji%2520and%2520Kareem%2520Mohamed%2520and%2520Kartikeya%2520Badola%2520and%2520Kat%2520Black%2520and%2520Katie%2520Millican%2520and%2520Keelin%2520McDonell%2520and%2520Kelvin%2520Nguyen%2520and%2520Kiranbir%2520Sodhia%2520and%2520Kish%2520Greene%2520and%2520Lars%2520Lowe%2520Sjoesund%2520and%2520Lauren%2520Usui%2520and%2520Laurent%2520Sifre%2520and%2520Lena%2520Heuermann%2520and%2520Leticia%2520Lago%2520and%2520Lilly%2520McNealus%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Logan%2520Kilpatrick%2520and%2520Lucas%2520Dixon%2520and%2520Luciano%2520Martins%2520and%2520Machel%2520Reid%2520and%2520Manvinder%2520Singh%2520and%2520Mark%2520Iverson%2520and%2520Martin%2520G%25C3%25B6rner%2520and%2520Mat%2520Velloso%2520and%2520Mateo%2520Wirth%2520and%2520Matt%2520Davidow%2520and%2520Matt%2520Miller%2520and%2520Matthew%2520Rahtz%2520and%2520Matthew%2520Watson%2520and%2520Meg%2520Risdal%2520and%2520Mehran%2520Kazemi%2520and%2520Michael%2520Moynihan%2520and%2520Ming%2520Zhang%2520and%2520Minsuk%2520Kahng%2520and%2520Minwoo%2520Park%2520and%2520Mofi%2520Rahman%2520and%2520Mohit%2520Khatwani%2520and%2520Natalie%2520Dao%2520and%2520Nenshad%2520Bardoliwalla%2520and%2520Nesh%2520Devanathan%2520and%2520Neta%2520Dumai%2520and%2520Nilay%2520Chauhan%2520and%2520Oscar%2520Wahltinez%2520and%2520Pankil%2520Botarda%2520and%2520Parker%2520Barnes%2520and%2520Paul%2520Barham%2520and%2520Paul%2520Michel%2520and%2520Pengchong%2520Jin%2520and%2520Petko%2520Georgiev%2520and%2520Phil%2520Culliton%2520and%2520Pradeep%2520Kuppala%2520and%2520Ramona%2520Comanescu%2520and%2520Ramona%2520Merhej%2520and%2520Reena%2520Jana%2520and%2520Reza%2520Ardeshir%2520Rokni%2520and%2520Rishabh%2520Agarwal%2520and%2520Ryan%2520Mullins%2520and%2520Samaneh%2520Saadat%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Sarah%2520Perrin%2520and%2520S%25C3%25A9bastien%2520M.%2520R.%2520Arnold%2520and%2520Sebastian%2520Krause%2520and%2520Shengyang%2520Dai%2520and%2520Shruti%2520Garg%2520and%2520Shruti%2520Sheth%2520and%2520Sue%2520Ronstrom%2520and%2520Susan%2520Chan%2520and%2520Timothy%2520Jordan%2520and%2520Ting%2520Yu%2520and%2520Tom%2520Eccles%2520and%2520Tom%2520Hennigan%2520and%2520Tomas%2520Kocisky%2520and%2520Tulsee%2520Doshi%2520and%2520Vihan%2520Jain%2520and%2520Vikas%2520Yadav%2520and%2520Vilobh%2520Meshram%2520and%2520Vishal%2520Dharmadhikari%2520and%2520Warren%2520Barkley%2520and%2520Wei%2520Wei%2520and%2520Wenming%2520Ye%2520and%2520Woohyun%2520Han%2520and%2520Woosuk%2520Kwon%2520and%2520Xiang%2520Xu%2520and%2520Zhe%2520Shen%2520and%2520Zhitao%2520Gong%2520and%2520Zichuan%2520Wei%2520and%2520Victor%2520Cotruta%2520and%2520Phoebe%2520Kirk%2520and%2520Anand%2520Rao%2520and%2520Minh%2520Giang%2520and%2520Ludovic%2520Peran%2520and%2520Tris%2520Warkentin%2520and%2520Eli%2520Collins%2520and%2520Joelle%2520Barral%2520and%2520Zoubin%2520Ghahramani%2520and%2520Raia%2520Hadsell%2520and%2520D.%2520Sculley%2520and%2520Jeanine%2520Banks%2520and%2520Anca%2520Dragan%2520and%2520Slav%2520Petrov%2520and%2520Oriol%2520Vinyals%2520and%2520Jeff%2520Dean%2520and%2520Demis%2520Hassabis%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Clement%2520Farabet%2520and%2520Elena%2520Buchatskaya%2520and%2520Sebastian%2520Borgeaud%2520and%2520Noah%2520Fiedel%2520and%2520Armand%2520Joulin%2520and%2520Kathleen%2520Kenealy%2520and%2520Robert%2520Dadashi%2520and%2520Alek%2520Andreev%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Gemma%25202%252C%2520a%2520new%2520addition%2520to%2520the%2520Gemma%2520family%2520of%250Alightweight%252C%2520state-of-the-art%2520open%2520models%252C%2520ranging%2520in%2520scale%2520from%25202%2520billion%2520to%250A27%2520billion%2520parameters.%2520In%2520this%2520new%2520version%252C%2520we%2520apply%2520several%2520known%2520technical%250Amodifications%2520to%2520the%2520Transformer%2520architecture%252C%2520such%2520as%2520interleaving%250Alocal-global%2520attentions%2520%2528Beltagy%2520et%2520al.%252C%25202020a%2529%2520and%2520group-query%2520attention%250A%2528Ainslie%2520et%2520al.%252C%25202023%2529.%2520We%2520also%2520train%2520the%25202B%2520and%25209B%2520models%2520with%2520knowledge%250Adistillation%2520%2528Hinton%2520et%2520al.%252C%25202015%2529%2520instead%2520of%2520next%2520token%2520prediction.%2520The%250Aresulting%2520models%2520deliver%2520the%2520best%2520performance%2520for%2520their%2520size%252C%2520and%2520even%2520offer%250Acompetitive%2520alternatives%2520to%2520models%2520that%2520are%25202-3%2520times%2520bigger.%2520We%2520release%2520all%250Aour%2520models%2520to%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemma%202%3A%20Improving%20Open%20Language%20Models%20at%20a%20Practical%20Size&entry.906535625=%20Gemma%20Team%20and%20Morgane%20Riviere%20and%20Shreya%20Pathak%20and%20Pier%20Giuseppe%20Sessa%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20L%C3%A9onard%20Hussenot%20and%20Thomas%20Mesnard%20and%20Bobak%20Shahriari%20and%20Alexandre%20Ram%C3%A9%20and%20Johan%20Ferret%20and%20Peter%20Liu%20and%20Pouya%20Tafti%20and%20Abe%20Friesen%20and%20Michelle%20Casbon%20and%20Sabela%20Ramos%20and%20Ravin%20Kumar%20and%20Charline%20Le%20Lan%20and%20Sammy%20Jerome%20and%20Anton%20Tsitsulin%20and%20Nino%20Vieillard%20and%20Piotr%20Stanczyk%20and%20Sertan%20Girgin%20and%20Nikola%20Momchev%20and%20Matt%20Hoffman%20and%20Shantanu%20Thakoor%20and%20Jean-Bastien%20Grill%20and%20Behnam%20Neyshabur%20and%20Olivier%20Bachem%20and%20Alanna%20Walton%20and%20Aliaksei%20Severyn%20and%20Alicia%20Parrish%20and%20Aliya%20Ahmad%20and%20Allen%20Hutchison%20and%20Alvin%20Abdagic%20and%20Amanda%20Carl%20and%20Amy%20Shen%20and%20Andy%20Brock%20and%20Andy%20Coenen%20and%20Anthony%20Laforge%20and%20Antonia%20Paterson%20and%20Ben%20Bastian%20and%20Bilal%20Piot%20and%20Bo%20Wu%20and%20Brandon%20Royal%20and%20Charlie%20Chen%20and%20Chintu%20Kumar%20and%20Chris%20Perry%20and%20Chris%20Welty%20and%20Christopher%20A.%20Choquette-Choo%20and%20Danila%20Sinopalnikov%20and%20David%20Weinberger%20and%20Dimple%20Vijaykumar%20and%20Dominika%20Rogozi%C5%84ska%20and%20Dustin%20Herbison%20and%20Elisa%20Bandy%20and%20Emma%20Wang%20and%20Eric%20Noland%20and%20Erica%20Moreira%20and%20Evan%20Senter%20and%20Evgenii%20Eltyshev%20and%20Francesco%20Visin%20and%20Gabriel%20Rasskin%20and%20Gary%20Wei%20and%20Glenn%20Cameron%20and%20Gus%20Martins%20and%20Hadi%20Hashemi%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Harleen%20Batra%20and%20Harsh%20Dhand%20and%20Ivan%20Nardini%20and%20Jacinda%20Mein%20and%20Jack%20Zhou%20and%20James%20Svensson%20and%20Jeff%20Stanway%20and%20Jetha%20Chan%20and%20Jin%20Peng%20Zhou%20and%20Joana%20Carrasqueira%20and%20Joana%20Iljazi%20and%20Jocelyn%20Becker%20and%20Joe%20Fernandez%20and%20Joost%20van%20Amersfoort%20and%20Josh%20Gordon%20and%20Josh%20Lipschultz%20and%20Josh%20Newlan%20and%20Ju-yeong%20Ji%20and%20Kareem%20Mohamed%20and%20Kartikeya%20Badola%20and%20Kat%20Black%20and%20Katie%20Millican%20and%20Keelin%20McDonell%20and%20Kelvin%20Nguyen%20and%20Kiranbir%20Sodhia%20and%20Kish%20Greene%20and%20Lars%20Lowe%20Sjoesund%20and%20Lauren%20Usui%20and%20Laurent%20Sifre%20and%20Lena%20Heuermann%20and%20Leticia%20Lago%20and%20Lilly%20McNealus%20and%20Livio%20Baldini%20Soares%20and%20Logan%20Kilpatrick%20and%20Lucas%20Dixon%20and%20Luciano%20Martins%20and%20Machel%20Reid%20and%20Manvinder%20Singh%20and%20Mark%20Iverson%20and%20Martin%20G%C3%B6rner%20and%20Mat%20Velloso%20and%20Mateo%20Wirth%20and%20Matt%20Davidow%20and%20Matt%20Miller%20and%20Matthew%20Rahtz%20and%20Matthew%20Watson%20and%20Meg%20Risdal%20and%20Mehran%20Kazemi%20and%20Michael%20Moynihan%20and%20Ming%20Zhang%20and%20Minsuk%20Kahng%20and%20Minwoo%20Park%20and%20Mofi%20Rahman%20and%20Mohit%20Khatwani%20and%20Natalie%20Dao%20and%20Nenshad%20Bardoliwalla%20and%20Nesh%20Devanathan%20and%20Neta%20Dumai%20and%20Nilay%20Chauhan%20and%20Oscar%20Wahltinez%20and%20Pankil%20Botarda%20and%20Parker%20Barnes%20and%20Paul%20Barham%20and%20Paul%20Michel%20and%20Pengchong%20Jin%20and%20Petko%20Georgiev%20and%20Phil%20Culliton%20and%20Pradeep%20Kuppala%20and%20Ramona%20Comanescu%20and%20Ramona%20Merhej%20and%20Reena%20Jana%20and%20Reza%20Ardeshir%20Rokni%20and%20Rishabh%20Agarwal%20and%20Ryan%20Mullins%20and%20Samaneh%20Saadat%20and%20Sara%20Mc%20Carthy%20and%20Sarah%20Perrin%20and%20S%C3%A9bastien%20M.%20R.%20Arnold%20and%20Sebastian%20Krause%20and%20Shengyang%20Dai%20and%20Shruti%20Garg%20and%20Shruti%20Sheth%20and%20Sue%20Ronstrom%20and%20Susan%20Chan%20and%20Timothy%20Jordan%20and%20Ting%20Yu%20and%20Tom%20Eccles%20and%20Tom%20Hennigan%20and%20Tomas%20Kocisky%20and%20Tulsee%20Doshi%20and%20Vihan%20Jain%20and%20Vikas%20Yadav%20and%20Vilobh%20Meshram%20and%20Vishal%20Dharmadhikari%20and%20Warren%20Barkley%20and%20Wei%20Wei%20and%20Wenming%20Ye%20and%20Woohyun%20Han%20and%20Woosuk%20Kwon%20and%20Xiang%20Xu%20and%20Zhe%20Shen%20and%20Zhitao%20Gong%20and%20Zichuan%20Wei%20and%20Victor%20Cotruta%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Minh%20Giang%20and%20Ludovic%20Peran%20and%20Tris%20Warkentin%20and%20Eli%20Collins%20and%20Joelle%20Barral%20and%20Zoubin%20Ghahramani%20and%20Raia%20Hadsell%20and%20D.%20Sculley%20and%20Jeanine%20Banks%20and%20Anca%20Dragan%20and%20Slav%20Petrov%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Clement%20Farabet%20and%20Elena%20Buchatskaya%20and%20Sebastian%20Borgeaud%20and%20Noah%20Fiedel%20and%20Armand%20Joulin%20and%20Kathleen%20Kenealy%20and%20Robert%20Dadashi%20and%20Alek%20Andreev&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Gemma%202%2C%20a%20new%20addition%20to%20the%20Gemma%20family%20of%0Alightweight%2C%20state-of-the-art%20open%20models%2C%20ranging%20in%20scale%20from%202%20billion%20to%0A27%20billion%20parameters.%20In%20this%20new%20version%2C%20we%20apply%20several%20known%20technical%0Amodifications%20to%20the%20Transformer%20architecture%2C%20such%20as%20interleaving%0Alocal-global%20attentions%20%28Beltagy%20et%20al.%2C%202020a%29%20and%20group-query%20attention%0A%28Ainslie%20et%20al.%2C%202023%29.%20We%20also%20train%20the%202B%20and%209B%20models%20with%20knowledge%0Adistillation%20%28Hinton%20et%20al.%2C%202015%29%20instead%20of%20next%20token%20prediction.%20The%0Aresulting%20models%20deliver%20the%20best%20performance%20for%20their%20size%2C%20and%20even%20offer%0Acompetitive%20alternatives%20to%20models%20that%20are%202-3%20times%20bigger.%20We%20release%20all%0Aour%20models%20to%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00118v2&entry.124074799=Read"},
{"title": "Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM\n  Auto-Prompting", "author": "Xiangyu Zhao and Chengqian Ma", "abstract": "  Large Language Models (LLMs) exhibit remarkable proficiency in addressing a\ndiverse array of tasks within the Natural Language Processing (NLP) domain,\nwith various prompt design strategies significantly augmenting their\ncapabilities. However, these prompts, while beneficial, each possess inherent\nlimitations. The primary prompt design methodologies are twofold: The first,\nexemplified by the Chain of Thought (CoT), involves manually crafting prompts\nspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).\nOnce these prompts are established, they are unalterable, and their\neffectiveness is capped by the expertise of the human designers. When applied\nto LLMs, the static nature of EDPs results in a uniform approach to both simple\nand complex problems within the same dataset, leading to the inefficient use of\ntokens for straightforward issues. The second method involves prompts\nautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which\nprovide tailored solutions to specific problems, mitigating the limitations of\nEDPs. However, LDPs may encounter a decline in performance when tackling\ncomplex problems due to the potential for error accumulation during the\nsolution planning process. To address these challenges, we have conceived a\nnovel Prompt Recursive Search (PRS) framework that leverages the LLM to\ngenerate solutions specific to the problem, thereby conserving tokens. The\nframework incorporates an assessment of problem complexity and an adjustable\nstructure, ensuring a reduction in the likelihood of errors. We have\nsubstantiated the efficacy of PRS framework through extensive experiments using\nLLMs with different numbers of parameters across a spectrum of datasets in\nvarious domains. Compared to the CoT method, the PRS method has increased the\naccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%\nimprovement.\n", "link": "http://arxiv.org/abs/2408.01423v1", "date": "2024-08-02", "relevancy": 1.9126, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5223}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4721}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Recursive%20Search%3A%20A%20Living%20Framework%20with%20Adaptive%20Growth%20in%20LLM%0A%20%20Auto-Prompting&body=Title%3A%20Prompt%20Recursive%20Search%3A%20A%20Living%20Framework%20with%20Adaptive%20Growth%20in%20LLM%0A%20%20Auto-Prompting%0AAuthor%3A%20Xiangyu%20Zhao%20and%20Chengqian%20Ma%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20proficiency%20in%20addressing%20a%0Adiverse%20array%20of%20tasks%20within%20the%20Natural%20Language%20Processing%20%28NLP%29%20domain%2C%0Awith%20various%20prompt%20design%20strategies%20significantly%20augmenting%20their%0Acapabilities.%20However%2C%20these%20prompts%2C%20while%20beneficial%2C%20each%20possess%20inherent%0Alimitations.%20The%20primary%20prompt%20design%20methodologies%20are%20twofold%3A%20The%20first%2C%0Aexemplified%20by%20the%20Chain%20of%20Thought%20%28CoT%29%2C%20involves%20manually%20crafting%20prompts%0Aspecific%20to%20individual%20datasets%2C%20hence%20termed%20Expert-Designed%20Prompts%20%28EDPs%29.%0AOnce%20these%20prompts%20are%20established%2C%20they%20are%20unalterable%2C%20and%20their%0Aeffectiveness%20is%20capped%20by%20the%20expertise%20of%20the%20human%20designers.%20When%20applied%0Ato%20LLMs%2C%20the%20static%20nature%20of%20EDPs%20results%20in%20a%20uniform%20approach%20to%20both%20simple%0Aand%20complex%20problems%20within%20the%20same%20dataset%2C%20leading%20to%20the%20inefficient%20use%20of%0Atokens%20for%20straightforward%20issues.%20The%20second%20method%20involves%20prompts%0Aautonomously%20generated%20by%20the%20LLM%2C%20known%20as%20LLM-Derived%20Prompts%20%28LDPs%29%2C%20which%0Aprovide%20tailored%20solutions%20to%20specific%20problems%2C%20mitigating%20the%20limitations%20of%0AEDPs.%20However%2C%20LDPs%20may%20encounter%20a%20decline%20in%20performance%20when%20tackling%0Acomplex%20problems%20due%20to%20the%20potential%20for%20error%20accumulation%20during%20the%0Asolution%20planning%20process.%20To%20address%20these%20challenges%2C%20we%20have%20conceived%20a%0Anovel%20Prompt%20Recursive%20Search%20%28PRS%29%20framework%20that%20leverages%20the%20LLM%20to%0Agenerate%20solutions%20specific%20to%20the%20problem%2C%20thereby%20conserving%20tokens.%20The%0Aframework%20incorporates%20an%20assessment%20of%20problem%20complexity%20and%20an%20adjustable%0Astructure%2C%20ensuring%20a%20reduction%20in%20the%20likelihood%20of%20errors.%20We%20have%0Asubstantiated%20the%20efficacy%20of%20PRS%20framework%20through%20extensive%20experiments%20using%0ALLMs%20with%20different%20numbers%20of%20parameters%20across%20a%20spectrum%20of%20datasets%20in%0Avarious%20domains.%20Compared%20to%20the%20CoT%20method%2C%20the%20PRS%20method%20has%20increased%20the%0Aaccuracy%20on%20the%20BBH%20dataset%20by%208%25%20using%20Llama3-7B%20model%2C%20achieving%20a%2022%25%0Aimprovement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Recursive%2520Search%253A%2520A%2520Living%2520Framework%2520with%2520Adaptive%2520Growth%2520in%2520LLM%250A%2520%2520Auto-Prompting%26entry.906535625%3DXiangyu%2520Zhao%2520and%2520Chengqian%2520Ma%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520proficiency%2520in%2520addressing%2520a%250Adiverse%2520array%2520of%2520tasks%2520within%2520the%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520domain%252C%250Awith%2520various%2520prompt%2520design%2520strategies%2520significantly%2520augmenting%2520their%250Acapabilities.%2520However%252C%2520these%2520prompts%252C%2520while%2520beneficial%252C%2520each%2520possess%2520inherent%250Alimitations.%2520The%2520primary%2520prompt%2520design%2520methodologies%2520are%2520twofold%253A%2520The%2520first%252C%250Aexemplified%2520by%2520the%2520Chain%2520of%2520Thought%2520%2528CoT%2529%252C%2520involves%2520manually%2520crafting%2520prompts%250Aspecific%2520to%2520individual%2520datasets%252C%2520hence%2520termed%2520Expert-Designed%2520Prompts%2520%2528EDPs%2529.%250AOnce%2520these%2520prompts%2520are%2520established%252C%2520they%2520are%2520unalterable%252C%2520and%2520their%250Aeffectiveness%2520is%2520capped%2520by%2520the%2520expertise%2520of%2520the%2520human%2520designers.%2520When%2520applied%250Ato%2520LLMs%252C%2520the%2520static%2520nature%2520of%2520EDPs%2520results%2520in%2520a%2520uniform%2520approach%2520to%2520both%2520simple%250Aand%2520complex%2520problems%2520within%2520the%2520same%2520dataset%252C%2520leading%2520to%2520the%2520inefficient%2520use%2520of%250Atokens%2520for%2520straightforward%2520issues.%2520The%2520second%2520method%2520involves%2520prompts%250Aautonomously%2520generated%2520by%2520the%2520LLM%252C%2520known%2520as%2520LLM-Derived%2520Prompts%2520%2528LDPs%2529%252C%2520which%250Aprovide%2520tailored%2520solutions%2520to%2520specific%2520problems%252C%2520mitigating%2520the%2520limitations%2520of%250AEDPs.%2520However%252C%2520LDPs%2520may%2520encounter%2520a%2520decline%2520in%2520performance%2520when%2520tackling%250Acomplex%2520problems%2520due%2520to%2520the%2520potential%2520for%2520error%2520accumulation%2520during%2520the%250Asolution%2520planning%2520process.%2520To%2520address%2520these%2520challenges%252C%2520we%2520have%2520conceived%2520a%250Anovel%2520Prompt%2520Recursive%2520Search%2520%2528PRS%2529%2520framework%2520that%2520leverages%2520the%2520LLM%2520to%250Agenerate%2520solutions%2520specific%2520to%2520the%2520problem%252C%2520thereby%2520conserving%2520tokens.%2520The%250Aframework%2520incorporates%2520an%2520assessment%2520of%2520problem%2520complexity%2520and%2520an%2520adjustable%250Astructure%252C%2520ensuring%2520a%2520reduction%2520in%2520the%2520likelihood%2520of%2520errors.%2520We%2520have%250Asubstantiated%2520the%2520efficacy%2520of%2520PRS%2520framework%2520through%2520extensive%2520experiments%2520using%250ALLMs%2520with%2520different%2520numbers%2520of%2520parameters%2520across%2520a%2520spectrum%2520of%2520datasets%2520in%250Avarious%2520domains.%2520Compared%2520to%2520the%2520CoT%2520method%252C%2520the%2520PRS%2520method%2520has%2520increased%2520the%250Aaccuracy%2520on%2520the%2520BBH%2520dataset%2520by%25208%2525%2520using%2520Llama3-7B%2520model%252C%2520achieving%2520a%252022%2525%250Aimprovement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Recursive%20Search%3A%20A%20Living%20Framework%20with%20Adaptive%20Growth%20in%20LLM%0A%20%20Auto-Prompting&entry.906535625=Xiangyu%20Zhao%20and%20Chengqian%20Ma&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20remarkable%20proficiency%20in%20addressing%20a%0Adiverse%20array%20of%20tasks%20within%20the%20Natural%20Language%20Processing%20%28NLP%29%20domain%2C%0Awith%20various%20prompt%20design%20strategies%20significantly%20augmenting%20their%0Acapabilities.%20However%2C%20these%20prompts%2C%20while%20beneficial%2C%20each%20possess%20inherent%0Alimitations.%20The%20primary%20prompt%20design%20methodologies%20are%20twofold%3A%20The%20first%2C%0Aexemplified%20by%20the%20Chain%20of%20Thought%20%28CoT%29%2C%20involves%20manually%20crafting%20prompts%0Aspecific%20to%20individual%20datasets%2C%20hence%20termed%20Expert-Designed%20Prompts%20%28EDPs%29.%0AOnce%20these%20prompts%20are%20established%2C%20they%20are%20unalterable%2C%20and%20their%0Aeffectiveness%20is%20capped%20by%20the%20expertise%20of%20the%20human%20designers.%20When%20applied%0Ato%20LLMs%2C%20the%20static%20nature%20of%20EDPs%20results%20in%20a%20uniform%20approach%20to%20both%20simple%0Aand%20complex%20problems%20within%20the%20same%20dataset%2C%20leading%20to%20the%20inefficient%20use%20of%0Atokens%20for%20straightforward%20issues.%20The%20second%20method%20involves%20prompts%0Aautonomously%20generated%20by%20the%20LLM%2C%20known%20as%20LLM-Derived%20Prompts%20%28LDPs%29%2C%20which%0Aprovide%20tailored%20solutions%20to%20specific%20problems%2C%20mitigating%20the%20limitations%20of%0AEDPs.%20However%2C%20LDPs%20may%20encounter%20a%20decline%20in%20performance%20when%20tackling%0Acomplex%20problems%20due%20to%20the%20potential%20for%20error%20accumulation%20during%20the%0Asolution%20planning%20process.%20To%20address%20these%20challenges%2C%20we%20have%20conceived%20a%0Anovel%20Prompt%20Recursive%20Search%20%28PRS%29%20framework%20that%20leverages%20the%20LLM%20to%0Agenerate%20solutions%20specific%20to%20the%20problem%2C%20thereby%20conserving%20tokens.%20The%0Aframework%20incorporates%20an%20assessment%20of%20problem%20complexity%20and%20an%20adjustable%0Astructure%2C%20ensuring%20a%20reduction%20in%20the%20likelihood%20of%20errors.%20We%20have%0Asubstantiated%20the%20efficacy%20of%20PRS%20framework%20through%20extensive%20experiments%20using%0ALLMs%20with%20different%20numbers%20of%20parameters%20across%20a%20spectrum%20of%20datasets%20in%0Avarious%20domains.%20Compared%20to%20the%20CoT%20method%2C%20the%20PRS%20method%20has%20increased%20the%0Aaccuracy%20on%20the%20BBH%20dataset%20by%208%25%20using%20Llama3-7B%20model%2C%20achieving%20a%2022%25%0Aimprovement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01423v1&entry.124074799=Read"},
{"title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation", "author": "Jiwoo Ryu and Hao-Wen Dong and Jongmin Jung and Dasaem Jeong", "abstract": "  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n", "link": "http://arxiv.org/abs/2408.01180v1", "date": "2024-08-02", "relevancy": 1.9117, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4981}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.484}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nested%20Music%20Transformer%3A%20Sequentially%20Decoding%20Compound%20Tokens%20in%0A%20%20Symbolic%20Music%20and%20Audio%20Generation&body=Title%3A%20Nested%20Music%20Transformer%3A%20Sequentially%20Decoding%20Compound%20Tokens%20in%0A%20%20Symbolic%20Music%20and%20Audio%20Generation%0AAuthor%3A%20Jiwoo%20Ryu%20and%20Hao-Wen%20Dong%20and%20Jongmin%20Jung%20and%20Dasaem%20Jeong%0AAbstract%3A%20%20%20Representing%20symbolic%20music%20with%20compound%20tokens%2C%20where%20each%20token%20consists%0Aof%20several%20different%20sub-tokens%20representing%20a%20distinct%20musical%20feature%20or%0Aattribute%2C%20offers%20the%20advantage%20of%20reducing%20sequence%20length.%20While%20previous%0Aresearch%20has%20validated%20the%20efficacy%20of%20compound%20tokens%20in%20music%20sequence%0Amodeling%2C%20predicting%20all%20sub-tokens%20simultaneously%20can%20lead%20to%20suboptimal%0Aresults%20as%20it%20may%20not%20fully%20capture%20the%20interdependencies%20between%20them.%20We%0Aintroduce%20the%20Nested%20Music%20Transformer%20%28NMT%29%2C%20an%20architecture%20tailored%20for%0Adecoding%20compound%20tokens%20autoregressively%2C%20similar%20to%20processing%20flattened%0Atokens%2C%20but%20with%20low%20memory%20usage.%20The%20NMT%20consists%20of%20two%20transformers%3A%20the%0Amain%20decoder%20that%20models%20a%20sequence%20of%20compound%20tokens%20and%20the%20sub-decoder%20for%0Amodeling%20sub-tokens%20of%20each%20compound%20token.%20The%20experiment%20results%20showed%20that%0Aapplying%20the%20NMT%20to%20compound%20tokens%20can%20enhance%20the%20performance%20in%20terms%20of%0Abetter%20perplexity%20in%20processing%20various%20symbolic%20music%20datasets%20and%20discrete%0Aaudio%20tokens%20from%20the%20MAESTRO%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNested%2520Music%2520Transformer%253A%2520Sequentially%2520Decoding%2520Compound%2520Tokens%2520in%250A%2520%2520Symbolic%2520Music%2520and%2520Audio%2520Generation%26entry.906535625%3DJiwoo%2520Ryu%2520and%2520Hao-Wen%2520Dong%2520and%2520Jongmin%2520Jung%2520and%2520Dasaem%2520Jeong%26entry.1292438233%3D%2520%2520Representing%2520symbolic%2520music%2520with%2520compound%2520tokens%252C%2520where%2520each%2520token%2520consists%250Aof%2520several%2520different%2520sub-tokens%2520representing%2520a%2520distinct%2520musical%2520feature%2520or%250Aattribute%252C%2520offers%2520the%2520advantage%2520of%2520reducing%2520sequence%2520length.%2520While%2520previous%250Aresearch%2520has%2520validated%2520the%2520efficacy%2520of%2520compound%2520tokens%2520in%2520music%2520sequence%250Amodeling%252C%2520predicting%2520all%2520sub-tokens%2520simultaneously%2520can%2520lead%2520to%2520suboptimal%250Aresults%2520as%2520it%2520may%2520not%2520fully%2520capture%2520the%2520interdependencies%2520between%2520them.%2520We%250Aintroduce%2520the%2520Nested%2520Music%2520Transformer%2520%2528NMT%2529%252C%2520an%2520architecture%2520tailored%2520for%250Adecoding%2520compound%2520tokens%2520autoregressively%252C%2520similar%2520to%2520processing%2520flattened%250Atokens%252C%2520but%2520with%2520low%2520memory%2520usage.%2520The%2520NMT%2520consists%2520of%2520two%2520transformers%253A%2520the%250Amain%2520decoder%2520that%2520models%2520a%2520sequence%2520of%2520compound%2520tokens%2520and%2520the%2520sub-decoder%2520for%250Amodeling%2520sub-tokens%2520of%2520each%2520compound%2520token.%2520The%2520experiment%2520results%2520showed%2520that%250Aapplying%2520the%2520NMT%2520to%2520compound%2520tokens%2520can%2520enhance%2520the%2520performance%2520in%2520terms%2520of%250Abetter%2520perplexity%2520in%2520processing%2520various%2520symbolic%2520music%2520datasets%2520and%2520discrete%250Aaudio%2520tokens%2520from%2520the%2520MAESTRO%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nested%20Music%20Transformer%3A%20Sequentially%20Decoding%20Compound%20Tokens%20in%0A%20%20Symbolic%20Music%20and%20Audio%20Generation&entry.906535625=Jiwoo%20Ryu%20and%20Hao-Wen%20Dong%20and%20Jongmin%20Jung%20and%20Dasaem%20Jeong&entry.1292438233=%20%20Representing%20symbolic%20music%20with%20compound%20tokens%2C%20where%20each%20token%20consists%0Aof%20several%20different%20sub-tokens%20representing%20a%20distinct%20musical%20feature%20or%0Aattribute%2C%20offers%20the%20advantage%20of%20reducing%20sequence%20length.%20While%20previous%0Aresearch%20has%20validated%20the%20efficacy%20of%20compound%20tokens%20in%20music%20sequence%0Amodeling%2C%20predicting%20all%20sub-tokens%20simultaneously%20can%20lead%20to%20suboptimal%0Aresults%20as%20it%20may%20not%20fully%20capture%20the%20interdependencies%20between%20them.%20We%0Aintroduce%20the%20Nested%20Music%20Transformer%20%28NMT%29%2C%20an%20architecture%20tailored%20for%0Adecoding%20compound%20tokens%20autoregressively%2C%20similar%20to%20processing%20flattened%0Atokens%2C%20but%20with%20low%20memory%20usage.%20The%20NMT%20consists%20of%20two%20transformers%3A%20the%0Amain%20decoder%20that%20models%20a%20sequence%20of%20compound%20tokens%20and%20the%20sub-decoder%20for%0Amodeling%20sub-tokens%20of%20each%20compound%20token.%20The%20experiment%20results%20showed%20that%0Aapplying%20the%20NMT%20to%20compound%20tokens%20can%20enhance%20the%20performance%20in%20terms%20of%0Abetter%20perplexity%20in%20processing%20various%20symbolic%20music%20datasets%20and%20discrete%0Aaudio%20tokens%20from%20the%20MAESTRO%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01180v1&entry.124074799=Read"},
{"title": "Point Prediction for Streaming Data", "author": "Aleena Chanda and N. V. Vinodchandran and Bertrand Clarke", "abstract": "  We present two new approaches for point prediction with streaming data. One\nis based on the Count-Min sketch (CMS) and the other is based on Gaussian\nprocess priors with a random bias. These methods are intended for the most\ngeneral predictive problems where no true model can be usefully formulated for\nthe data stream. In statistical contexts, this is often called the\n$\\mathcal{M}$-open problem class. Under the assumption that the data consists\nof i.i.d samples from a fixed distribution function $F$, we show that the\nCMS-based estimates of the distribution function are consistent.\n  We compare our new methods with two established predictors in terms of\ncumulative $L^1$ error. One is based on the Shtarkov solution (often called the\nnormalized maximum likelihood) in the normal experts setting and the other is\nbased on Dirichlet process priors. These comparisons are for two cases. The\nfirst is one-pass meaning that the updating of the predictors is done using the\nfact that the CMS is a sketch. For predictors that are not one-pass, we use\nstreaming $K$-means to give a representative subset of fixed size that can be\nupdated as data accumulate.\n  Preliminary computational work suggests that the one-pass median version of\nthe CMS method is rarely outperformed by the other methods for sufficiently\ncomplex data. We also find that predictors based on Gaussian process priors\nwith random biases perform well. The Shtarkov predictors we use here did not\nperform as well probably because we were only using the simplest example. The\nother predictors seemed to perform well mainly when the data did not look like\nthey came from an M-open data generator.\n", "link": "http://arxiv.org/abs/2408.01318v1", "date": "2024-08-02", "relevancy": 1.8873, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4824}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Prediction%20for%20Streaming%20Data&body=Title%3A%20Point%20Prediction%20for%20Streaming%20Data%0AAuthor%3A%20Aleena%20Chanda%20and%20N.%20V.%20Vinodchandran%20and%20Bertrand%20Clarke%0AAbstract%3A%20%20%20We%20present%20two%20new%20approaches%20for%20point%20prediction%20with%20streaming%20data.%20One%0Ais%20based%20on%20the%20Count-Min%20sketch%20%28CMS%29%20and%20the%20other%20is%20based%20on%20Gaussian%0Aprocess%20priors%20with%20a%20random%20bias.%20These%20methods%20are%20intended%20for%20the%20most%0Ageneral%20predictive%20problems%20where%20no%20true%20model%20can%20be%20usefully%20formulated%20for%0Athe%20data%20stream.%20In%20statistical%20contexts%2C%20this%20is%20often%20called%20the%0A%24%5Cmathcal%7BM%7D%24-open%20problem%20class.%20Under%20the%20assumption%20that%20the%20data%20consists%0Aof%20i.i.d%20samples%20from%20a%20fixed%20distribution%20function%20%24F%24%2C%20we%20show%20that%20the%0ACMS-based%20estimates%20of%20the%20distribution%20function%20are%20consistent.%0A%20%20We%20compare%20our%20new%20methods%20with%20two%20established%20predictors%20in%20terms%20of%0Acumulative%20%24L%5E1%24%20error.%20One%20is%20based%20on%20the%20Shtarkov%20solution%20%28often%20called%20the%0Anormalized%20maximum%20likelihood%29%20in%20the%20normal%20experts%20setting%20and%20the%20other%20is%0Abased%20on%20Dirichlet%20process%20priors.%20These%20comparisons%20are%20for%20two%20cases.%20The%0Afirst%20is%20one-pass%20meaning%20that%20the%20updating%20of%20the%20predictors%20is%20done%20using%20the%0Afact%20that%20the%20CMS%20is%20a%20sketch.%20For%20predictors%20that%20are%20not%20one-pass%2C%20we%20use%0Astreaming%20%24K%24-means%20to%20give%20a%20representative%20subset%20of%20fixed%20size%20that%20can%20be%0Aupdated%20as%20data%20accumulate.%0A%20%20Preliminary%20computational%20work%20suggests%20that%20the%20one-pass%20median%20version%20of%0Athe%20CMS%20method%20is%20rarely%20outperformed%20by%20the%20other%20methods%20for%20sufficiently%0Acomplex%20data.%20We%20also%20find%20that%20predictors%20based%20on%20Gaussian%20process%20priors%0Awith%20random%20biases%20perform%20well.%20The%20Shtarkov%20predictors%20we%20use%20here%20did%20not%0Aperform%20as%20well%20probably%20because%20we%20were%20only%20using%20the%20simplest%20example.%20The%0Aother%20predictors%20seemed%20to%20perform%20well%20mainly%20when%20the%20data%20did%20not%20look%20like%0Athey%20came%20from%20an%20M-open%20data%20generator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Prediction%2520for%2520Streaming%2520Data%26entry.906535625%3DAleena%2520Chanda%2520and%2520N.%2520V.%2520Vinodchandran%2520and%2520Bertrand%2520Clarke%26entry.1292438233%3D%2520%2520We%2520present%2520two%2520new%2520approaches%2520for%2520point%2520prediction%2520with%2520streaming%2520data.%2520One%250Ais%2520based%2520on%2520the%2520Count-Min%2520sketch%2520%2528CMS%2529%2520and%2520the%2520other%2520is%2520based%2520on%2520Gaussian%250Aprocess%2520priors%2520with%2520a%2520random%2520bias.%2520These%2520methods%2520are%2520intended%2520for%2520the%2520most%250Ageneral%2520predictive%2520problems%2520where%2520no%2520true%2520model%2520can%2520be%2520usefully%2520formulated%2520for%250Athe%2520data%2520stream.%2520In%2520statistical%2520contexts%252C%2520this%2520is%2520often%2520called%2520the%250A%2524%255Cmathcal%257BM%257D%2524-open%2520problem%2520class.%2520Under%2520the%2520assumption%2520that%2520the%2520data%2520consists%250Aof%2520i.i.d%2520samples%2520from%2520a%2520fixed%2520distribution%2520function%2520%2524F%2524%252C%2520we%2520show%2520that%2520the%250ACMS-based%2520estimates%2520of%2520the%2520distribution%2520function%2520are%2520consistent.%250A%2520%2520We%2520compare%2520our%2520new%2520methods%2520with%2520two%2520established%2520predictors%2520in%2520terms%2520of%250Acumulative%2520%2524L%255E1%2524%2520error.%2520One%2520is%2520based%2520on%2520the%2520Shtarkov%2520solution%2520%2528often%2520called%2520the%250Anormalized%2520maximum%2520likelihood%2529%2520in%2520the%2520normal%2520experts%2520setting%2520and%2520the%2520other%2520is%250Abased%2520on%2520Dirichlet%2520process%2520priors.%2520These%2520comparisons%2520are%2520for%2520two%2520cases.%2520The%250Afirst%2520is%2520one-pass%2520meaning%2520that%2520the%2520updating%2520of%2520the%2520predictors%2520is%2520done%2520using%2520the%250Afact%2520that%2520the%2520CMS%2520is%2520a%2520sketch.%2520For%2520predictors%2520that%2520are%2520not%2520one-pass%252C%2520we%2520use%250Astreaming%2520%2524K%2524-means%2520to%2520give%2520a%2520representative%2520subset%2520of%2520fixed%2520size%2520that%2520can%2520be%250Aupdated%2520as%2520data%2520accumulate.%250A%2520%2520Preliminary%2520computational%2520work%2520suggests%2520that%2520the%2520one-pass%2520median%2520version%2520of%250Athe%2520CMS%2520method%2520is%2520rarely%2520outperformed%2520by%2520the%2520other%2520methods%2520for%2520sufficiently%250Acomplex%2520data.%2520We%2520also%2520find%2520that%2520predictors%2520based%2520on%2520Gaussian%2520process%2520priors%250Awith%2520random%2520biases%2520perform%2520well.%2520The%2520Shtarkov%2520predictors%2520we%2520use%2520here%2520did%2520not%250Aperform%2520as%2520well%2520probably%2520because%2520we%2520were%2520only%2520using%2520the%2520simplest%2520example.%2520The%250Aother%2520predictors%2520seemed%2520to%2520perform%2520well%2520mainly%2520when%2520the%2520data%2520did%2520not%2520look%2520like%250Athey%2520came%2520from%2520an%2520M-open%2520data%2520generator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Prediction%20for%20Streaming%20Data&entry.906535625=Aleena%20Chanda%20and%20N.%20V.%20Vinodchandran%20and%20Bertrand%20Clarke&entry.1292438233=%20%20We%20present%20two%20new%20approaches%20for%20point%20prediction%20with%20streaming%20data.%20One%0Ais%20based%20on%20the%20Count-Min%20sketch%20%28CMS%29%20and%20the%20other%20is%20based%20on%20Gaussian%0Aprocess%20priors%20with%20a%20random%20bias.%20These%20methods%20are%20intended%20for%20the%20most%0Ageneral%20predictive%20problems%20where%20no%20true%20model%20can%20be%20usefully%20formulated%20for%0Athe%20data%20stream.%20In%20statistical%20contexts%2C%20this%20is%20often%20called%20the%0A%24%5Cmathcal%7BM%7D%24-open%20problem%20class.%20Under%20the%20assumption%20that%20the%20data%20consists%0Aof%20i.i.d%20samples%20from%20a%20fixed%20distribution%20function%20%24F%24%2C%20we%20show%20that%20the%0ACMS-based%20estimates%20of%20the%20distribution%20function%20are%20consistent.%0A%20%20We%20compare%20our%20new%20methods%20with%20two%20established%20predictors%20in%20terms%20of%0Acumulative%20%24L%5E1%24%20error.%20One%20is%20based%20on%20the%20Shtarkov%20solution%20%28often%20called%20the%0Anormalized%20maximum%20likelihood%29%20in%20the%20normal%20experts%20setting%20and%20the%20other%20is%0Abased%20on%20Dirichlet%20process%20priors.%20These%20comparisons%20are%20for%20two%20cases.%20The%0Afirst%20is%20one-pass%20meaning%20that%20the%20updating%20of%20the%20predictors%20is%20done%20using%20the%0Afact%20that%20the%20CMS%20is%20a%20sketch.%20For%20predictors%20that%20are%20not%20one-pass%2C%20we%20use%0Astreaming%20%24K%24-means%20to%20give%20a%20representative%20subset%20of%20fixed%20size%20that%20can%20be%0Aupdated%20as%20data%20accumulate.%0A%20%20Preliminary%20computational%20work%20suggests%20that%20the%20one-pass%20median%20version%20of%0Athe%20CMS%20method%20is%20rarely%20outperformed%20by%20the%20other%20methods%20for%20sufficiently%0Acomplex%20data.%20We%20also%20find%20that%20predictors%20based%20on%20Gaussian%20process%20priors%0Awith%20random%20biases%20perform%20well.%20The%20Shtarkov%20predictors%20we%20use%20here%20did%20not%0Aperform%20as%20well%20probably%20because%20we%20were%20only%20using%20the%20simplest%20example.%20The%0Aother%20predictors%20seemed%20to%20perform%20well%20mainly%20when%20the%20data%20did%20not%20look%20like%0Athey%20came%20from%20an%20M-open%20data%20generator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01318v1&entry.124074799=Read"},
{"title": "Reinforcement Learning applied to Insurance Portfolio Pursuit", "author": "Edward James Young and Alistair Rogers and Elliott Tong and James Jordon", "abstract": "  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. We term\nthe problem of modulating offers to achieve a desired target portfolio the\nportfolio pursuit problem. Having formulated the portfolio pursuit problem as a\nsequential decision making problem, we devise a novel reinforcement learning\nalgorithm for its solution. We test our method on a complex synthetic market\nenvironment, and demonstrate that it outperforms a baseline method which mimics\ncurrent industry approaches to portfolio pursuit.\n", "link": "http://arxiv.org/abs/2408.00713v2", "date": "2024-08-02", "relevancy": 1.8826, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.476}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20applied%20to%20Insurance%20Portfolio%20Pursuit&body=Title%3A%20Reinforcement%20Learning%20applied%20to%20Insurance%20Portfolio%20Pursuit%0AAuthor%3A%20Edward%20James%20Young%20and%20Alistair%20Rogers%20and%20Elliott%20Tong%20and%20James%20Jordon%0AAbstract%3A%20%20%20When%20faced%20with%20a%20new%20customer%2C%20many%20factors%20contribute%20to%20an%20insurance%0Afirm%27s%20decision%20of%20what%20offer%20to%20make%20to%20that%20customer.%20In%20addition%20to%20the%0Aexpected%20cost%20of%20providing%20the%20insurance%2C%20the%20firm%20must%20consider%20the%20other%0Aoffers%20likely%20to%20be%20made%20to%20the%20customer%2C%20and%20how%20sensitive%20the%20customer%20is%20to%0Adifferences%20in%20price.%20Moreover%2C%20firms%20often%20target%20a%20specific%20portfolio%20of%0Acustomers%20that%20could%20depend%20on%2C%20e.g.%2C%20age%2C%20location%2C%20and%20occupation.%20Given%20such%0Aa%20target%20portfolio%2C%20firms%20may%20choose%20to%20modulate%20an%20individual%20customer%27s%20offer%0Abased%20on%20whether%20the%20firm%20desires%20the%20customer%20within%20their%20portfolio.%20We%20term%0Athe%20problem%20of%20modulating%20offers%20to%20achieve%20a%20desired%20target%20portfolio%20the%0Aportfolio%20pursuit%20problem.%20Having%20formulated%20the%20portfolio%20pursuit%20problem%20as%20a%0Asequential%20decision%20making%20problem%2C%20we%20devise%20a%20novel%20reinforcement%20learning%0Aalgorithm%20for%20its%20solution.%20We%20test%20our%20method%20on%20a%20complex%20synthetic%20market%0Aenvironment%2C%20and%20demonstrate%20that%20it%20outperforms%20a%20baseline%20method%20which%20mimics%0Acurrent%20industry%20approaches%20to%20portfolio%20pursuit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520applied%2520to%2520Insurance%2520Portfolio%2520Pursuit%26entry.906535625%3DEdward%2520James%2520Young%2520and%2520Alistair%2520Rogers%2520and%2520Elliott%2520Tong%2520and%2520James%2520Jordon%26entry.1292438233%3D%2520%2520When%2520faced%2520with%2520a%2520new%2520customer%252C%2520many%2520factors%2520contribute%2520to%2520an%2520insurance%250Afirm%2527s%2520decision%2520of%2520what%2520offer%2520to%2520make%2520to%2520that%2520customer.%2520In%2520addition%2520to%2520the%250Aexpected%2520cost%2520of%2520providing%2520the%2520insurance%252C%2520the%2520firm%2520must%2520consider%2520the%2520other%250Aoffers%2520likely%2520to%2520be%2520made%2520to%2520the%2520customer%252C%2520and%2520how%2520sensitive%2520the%2520customer%2520is%2520to%250Adifferences%2520in%2520price.%2520Moreover%252C%2520firms%2520often%2520target%2520a%2520specific%2520portfolio%2520of%250Acustomers%2520that%2520could%2520depend%2520on%252C%2520e.g.%252C%2520age%252C%2520location%252C%2520and%2520occupation.%2520Given%2520such%250Aa%2520target%2520portfolio%252C%2520firms%2520may%2520choose%2520to%2520modulate%2520an%2520individual%2520customer%2527s%2520offer%250Abased%2520on%2520whether%2520the%2520firm%2520desires%2520the%2520customer%2520within%2520their%2520portfolio.%2520We%2520term%250Athe%2520problem%2520of%2520modulating%2520offers%2520to%2520achieve%2520a%2520desired%2520target%2520portfolio%2520the%250Aportfolio%2520pursuit%2520problem.%2520Having%2520formulated%2520the%2520portfolio%2520pursuit%2520problem%2520as%2520a%250Asequential%2520decision%2520making%2520problem%252C%2520we%2520devise%2520a%2520novel%2520reinforcement%2520learning%250Aalgorithm%2520for%2520its%2520solution.%2520We%2520test%2520our%2520method%2520on%2520a%2520complex%2520synthetic%2520market%250Aenvironment%252C%2520and%2520demonstrate%2520that%2520it%2520outperforms%2520a%2520baseline%2520method%2520which%2520mimics%250Acurrent%2520industry%2520approaches%2520to%2520portfolio%2520pursuit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20applied%20to%20Insurance%20Portfolio%20Pursuit&entry.906535625=Edward%20James%20Young%20and%20Alistair%20Rogers%20and%20Elliott%20Tong%20and%20James%20Jordon&entry.1292438233=%20%20When%20faced%20with%20a%20new%20customer%2C%20many%20factors%20contribute%20to%20an%20insurance%0Afirm%27s%20decision%20of%20what%20offer%20to%20make%20to%20that%20customer.%20In%20addition%20to%20the%0Aexpected%20cost%20of%20providing%20the%20insurance%2C%20the%20firm%20must%20consider%20the%20other%0Aoffers%20likely%20to%20be%20made%20to%20the%20customer%2C%20and%20how%20sensitive%20the%20customer%20is%20to%0Adifferences%20in%20price.%20Moreover%2C%20firms%20often%20target%20a%20specific%20portfolio%20of%0Acustomers%20that%20could%20depend%20on%2C%20e.g.%2C%20age%2C%20location%2C%20and%20occupation.%20Given%20such%0Aa%20target%20portfolio%2C%20firms%20may%20choose%20to%20modulate%20an%20individual%20customer%27s%20offer%0Abased%20on%20whether%20the%20firm%20desires%20the%20customer%20within%20their%20portfolio.%20We%20term%0Athe%20problem%20of%20modulating%20offers%20to%20achieve%20a%20desired%20target%20portfolio%20the%0Aportfolio%20pursuit%20problem.%20Having%20formulated%20the%20portfolio%20pursuit%20problem%20as%20a%0Asequential%20decision%20making%20problem%2C%20we%20devise%20a%20novel%20reinforcement%20learning%0Aalgorithm%20for%20its%20solution.%20We%20test%20our%20method%20on%20a%20complex%20synthetic%20market%0Aenvironment%2C%20and%20demonstrate%20that%20it%20outperforms%20a%20baseline%20method%20which%20mimics%0Acurrent%20industry%20approaches%20to%20portfolio%20pursuit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00713v2&entry.124074799=Read"},
{"title": "Certifiably Robust Encoding Schemes", "author": "Aman Saxena and Tom Wollschl\u00e4ger and Nicola Franco and Jeanette Miriam Lorenz and Stephan G\u00fcnnemann", "abstract": "  Quantum machine learning uses principles from quantum mechanics to process\ndata, offering potential advances in speed and performance. However, previous\nwork has shown that these models are susceptible to attacks that manipulate\ninput data or exploit noise in quantum circuits. Following this, various\nstudies have explored the robustness of these models. These works focus on the\nrobustness certification of manipulations of the quantum states. We extend this\nline of research by investigating the robustness against perturbations in the\nclassical data for a general class of data encoding schemes. We show that for\nsuch schemes, the addition of suitable noise channels is equivalent to\nevaluating the mean value of the noiseless classifier at the smoothed data,\nakin to Randomized Smoothing from classical machine learning. Using our general\nframework, we show that suitable additions of phase-damping noise channels\nimprove empirical and provable robustness for the considered class of encoding\nschemes.\n", "link": "http://arxiv.org/abs/2408.01200v1", "date": "2024-08-02", "relevancy": 1.8823, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4775}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certifiably%20Robust%20Encoding%20Schemes&body=Title%3A%20Certifiably%20Robust%20Encoding%20Schemes%0AAuthor%3A%20Aman%20Saxena%20and%20Tom%20Wollschl%C3%A4ger%20and%20Nicola%20Franco%20and%20Jeanette%20Miriam%20Lorenz%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Quantum%20machine%20learning%20uses%20principles%20from%20quantum%20mechanics%20to%20process%0Adata%2C%20offering%20potential%20advances%20in%20speed%20and%20performance.%20However%2C%20previous%0Awork%20has%20shown%20that%20these%20models%20are%20susceptible%20to%20attacks%20that%20manipulate%0Ainput%20data%20or%20exploit%20noise%20in%20quantum%20circuits.%20Following%20this%2C%20various%0Astudies%20have%20explored%20the%20robustness%20of%20these%20models.%20These%20works%20focus%20on%20the%0Arobustness%20certification%20of%20manipulations%20of%20the%20quantum%20states.%20We%20extend%20this%0Aline%20of%20research%20by%20investigating%20the%20robustness%20against%20perturbations%20in%20the%0Aclassical%20data%20for%20a%20general%20class%20of%20data%20encoding%20schemes.%20We%20show%20that%20for%0Asuch%20schemes%2C%20the%20addition%20of%20suitable%20noise%20channels%20is%20equivalent%20to%0Aevaluating%20the%20mean%20value%20of%20the%20noiseless%20classifier%20at%20the%20smoothed%20data%2C%0Aakin%20to%20Randomized%20Smoothing%20from%20classical%20machine%20learning.%20Using%20our%20general%0Aframework%2C%20we%20show%20that%20suitable%20additions%20of%20phase-damping%20noise%20channels%0Aimprove%20empirical%20and%20provable%20robustness%20for%20the%20considered%20class%20of%20encoding%0Aschemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertifiably%2520Robust%2520Encoding%2520Schemes%26entry.906535625%3DAman%2520Saxena%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520Nicola%2520Franco%2520and%2520Jeanette%2520Miriam%2520Lorenz%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Quantum%2520machine%2520learning%2520uses%2520principles%2520from%2520quantum%2520mechanics%2520to%2520process%250Adata%252C%2520offering%2520potential%2520advances%2520in%2520speed%2520and%2520performance.%2520However%252C%2520previous%250Awork%2520has%2520shown%2520that%2520these%2520models%2520are%2520susceptible%2520to%2520attacks%2520that%2520manipulate%250Ainput%2520data%2520or%2520exploit%2520noise%2520in%2520quantum%2520circuits.%2520Following%2520this%252C%2520various%250Astudies%2520have%2520explored%2520the%2520robustness%2520of%2520these%2520models.%2520These%2520works%2520focus%2520on%2520the%250Arobustness%2520certification%2520of%2520manipulations%2520of%2520the%2520quantum%2520states.%2520We%2520extend%2520this%250Aline%2520of%2520research%2520by%2520investigating%2520the%2520robustness%2520against%2520perturbations%2520in%2520the%250Aclassical%2520data%2520for%2520a%2520general%2520class%2520of%2520data%2520encoding%2520schemes.%2520We%2520show%2520that%2520for%250Asuch%2520schemes%252C%2520the%2520addition%2520of%2520suitable%2520noise%2520channels%2520is%2520equivalent%2520to%250Aevaluating%2520the%2520mean%2520value%2520of%2520the%2520noiseless%2520classifier%2520at%2520the%2520smoothed%2520data%252C%250Aakin%2520to%2520Randomized%2520Smoothing%2520from%2520classical%2520machine%2520learning.%2520Using%2520our%2520general%250Aframework%252C%2520we%2520show%2520that%2520suitable%2520additions%2520of%2520phase-damping%2520noise%2520channels%250Aimprove%2520empirical%2520and%2520provable%2520robustness%2520for%2520the%2520considered%2520class%2520of%2520encoding%250Aschemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certifiably%20Robust%20Encoding%20Schemes&entry.906535625=Aman%20Saxena%20and%20Tom%20Wollschl%C3%A4ger%20and%20Nicola%20Franco%20and%20Jeanette%20Miriam%20Lorenz%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Quantum%20machine%20learning%20uses%20principles%20from%20quantum%20mechanics%20to%20process%0Adata%2C%20offering%20potential%20advances%20in%20speed%20and%20performance.%20However%2C%20previous%0Awork%20has%20shown%20that%20these%20models%20are%20susceptible%20to%20attacks%20that%20manipulate%0Ainput%20data%20or%20exploit%20noise%20in%20quantum%20circuits.%20Following%20this%2C%20various%0Astudies%20have%20explored%20the%20robustness%20of%20these%20models.%20These%20works%20focus%20on%20the%0Arobustness%20certification%20of%20manipulations%20of%20the%20quantum%20states.%20We%20extend%20this%0Aline%20of%20research%20by%20investigating%20the%20robustness%20against%20perturbations%20in%20the%0Aclassical%20data%20for%20a%20general%20class%20of%20data%20encoding%20schemes.%20We%20show%20that%20for%0Asuch%20schemes%2C%20the%20addition%20of%20suitable%20noise%20channels%20is%20equivalent%20to%0Aevaluating%20the%20mean%20value%20of%20the%20noiseless%20classifier%20at%20the%20smoothed%20data%2C%0Aakin%20to%20Randomized%20Smoothing%20from%20classical%20machine%20learning.%20Using%20our%20general%0Aframework%2C%20we%20show%20that%20suitable%20additions%20of%20phase-damping%20noise%20channels%0Aimprove%20empirical%20and%20provable%20robustness%20for%20the%20considered%20class%20of%20encoding%0Aschemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01200v1&entry.124074799=Read"},
{"title": "High-Throughput Phenotyping of Clinical Text Using Large Language Models", "author": "Daniel B. Hier and S. Ilyas Munzir and Anne Stahlfeld and Tayo Obafemi-Ajayi and Michael D. Carrithers", "abstract": "  High-throughput phenotyping automates the mapping of patient signs to\nstandardized ontology concepts and is essential for precision medicine. This\nstudy evaluates the automation of phenotyping of clinical summaries from the\nOnline Mendelian Inheritance in Man (OMIM) database using large language\nmodels. Due to their rich phenotype data, these summaries can be surrogates for\nphysician notes. We conduct a performance comparison of GPT-4 and\nGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in\nidentifying, categorizing, and normalizing signs, achieving concordance with\nmanual annotators comparable to inter-rater agreement. Despite some limitations\nin sign normalization, the extensive pre-training of GPT-4 results in high\nperformance and generalizability across several phenotyping tasks while\nobviating the need for manually annotated training data. Large language models\nare expected to be the dominant method for automating high-throughput\nphenotyping of clinical text.\n", "link": "http://arxiv.org/abs/2408.01214v1", "date": "2024-08-02", "relevancy": 1.8737, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Throughput%20Phenotyping%20of%20Clinical%20Text%20Using%20Large%20Language%20Models&body=Title%3A%20High-Throughput%20Phenotyping%20of%20Clinical%20Text%20Using%20Large%20Language%20Models%0AAuthor%3A%20Daniel%20B.%20Hier%20and%20S.%20Ilyas%20Munzir%20and%20Anne%20Stahlfeld%20and%20Tayo%20Obafemi-Ajayi%20and%20Michael%20D.%20Carrithers%0AAbstract%3A%20%20%20High-throughput%20phenotyping%20automates%20the%20mapping%20of%20patient%20signs%20to%0Astandardized%20ontology%20concepts%20and%20is%20essential%20for%20precision%20medicine.%20This%0Astudy%20evaluates%20the%20automation%20of%20phenotyping%20of%20clinical%20summaries%20from%20the%0AOnline%20Mendelian%20Inheritance%20in%20Man%20%28OMIM%29%20database%20using%20large%20language%0Amodels.%20Due%20to%20their%20rich%20phenotype%20data%2C%20these%20summaries%20can%20be%20surrogates%20for%0Aphysician%20notes.%20We%20conduct%20a%20performance%20comparison%20of%20GPT-4%20and%0AGPT-3.5-Turbo.%20Our%20results%20indicate%20that%20GPT-4%20surpasses%20GPT-3.5-Turbo%20in%0Aidentifying%2C%20categorizing%2C%20and%20normalizing%20signs%2C%20achieving%20concordance%20with%0Amanual%20annotators%20comparable%20to%20inter-rater%20agreement.%20Despite%20some%20limitations%0Ain%20sign%20normalization%2C%20the%20extensive%20pre-training%20of%20GPT-4%20results%20in%20high%0Aperformance%20and%20generalizability%20across%20several%20phenotyping%20tasks%20while%0Aobviating%20the%20need%20for%20manually%20annotated%20training%20data.%20Large%20language%20models%0Aare%20expected%20to%20be%20the%20dominant%20method%20for%20automating%20high-throughput%0Aphenotyping%20of%20clinical%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Throughput%2520Phenotyping%2520of%2520Clinical%2520Text%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DDaniel%2520B.%2520Hier%2520and%2520S.%2520Ilyas%2520Munzir%2520and%2520Anne%2520Stahlfeld%2520and%2520Tayo%2520Obafemi-Ajayi%2520and%2520Michael%2520D.%2520Carrithers%26entry.1292438233%3D%2520%2520High-throughput%2520phenotyping%2520automates%2520the%2520mapping%2520of%2520patient%2520signs%2520to%250Astandardized%2520ontology%2520concepts%2520and%2520is%2520essential%2520for%2520precision%2520medicine.%2520This%250Astudy%2520evaluates%2520the%2520automation%2520of%2520phenotyping%2520of%2520clinical%2520summaries%2520from%2520the%250AOnline%2520Mendelian%2520Inheritance%2520in%2520Man%2520%2528OMIM%2529%2520database%2520using%2520large%2520language%250Amodels.%2520Due%2520to%2520their%2520rich%2520phenotype%2520data%252C%2520these%2520summaries%2520can%2520be%2520surrogates%2520for%250Aphysician%2520notes.%2520We%2520conduct%2520a%2520performance%2520comparison%2520of%2520GPT-4%2520and%250AGPT-3.5-Turbo.%2520Our%2520results%2520indicate%2520that%2520GPT-4%2520surpasses%2520GPT-3.5-Turbo%2520in%250Aidentifying%252C%2520categorizing%252C%2520and%2520normalizing%2520signs%252C%2520achieving%2520concordance%2520with%250Amanual%2520annotators%2520comparable%2520to%2520inter-rater%2520agreement.%2520Despite%2520some%2520limitations%250Ain%2520sign%2520normalization%252C%2520the%2520extensive%2520pre-training%2520of%2520GPT-4%2520results%2520in%2520high%250Aperformance%2520and%2520generalizability%2520across%2520several%2520phenotyping%2520tasks%2520while%250Aobviating%2520the%2520need%2520for%2520manually%2520annotated%2520training%2520data.%2520Large%2520language%2520models%250Aare%2520expected%2520to%2520be%2520the%2520dominant%2520method%2520for%2520automating%2520high-throughput%250Aphenotyping%2520of%2520clinical%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Throughput%20Phenotyping%20of%20Clinical%20Text%20Using%20Large%20Language%20Models&entry.906535625=Daniel%20B.%20Hier%20and%20S.%20Ilyas%20Munzir%20and%20Anne%20Stahlfeld%20and%20Tayo%20Obafemi-Ajayi%20and%20Michael%20D.%20Carrithers&entry.1292438233=%20%20High-throughput%20phenotyping%20automates%20the%20mapping%20of%20patient%20signs%20to%0Astandardized%20ontology%20concepts%20and%20is%20essential%20for%20precision%20medicine.%20This%0Astudy%20evaluates%20the%20automation%20of%20phenotyping%20of%20clinical%20summaries%20from%20the%0AOnline%20Mendelian%20Inheritance%20in%20Man%20%28OMIM%29%20database%20using%20large%20language%0Amodels.%20Due%20to%20their%20rich%20phenotype%20data%2C%20these%20summaries%20can%20be%20surrogates%20for%0Aphysician%20notes.%20We%20conduct%20a%20performance%20comparison%20of%20GPT-4%20and%0AGPT-3.5-Turbo.%20Our%20results%20indicate%20that%20GPT-4%20surpasses%20GPT-3.5-Turbo%20in%0Aidentifying%2C%20categorizing%2C%20and%20normalizing%20signs%2C%20achieving%20concordance%20with%0Amanual%20annotators%20comparable%20to%20inter-rater%20agreement.%20Despite%20some%20limitations%0Ain%20sign%20normalization%2C%20the%20extensive%20pre-training%20of%20GPT-4%20results%20in%20high%0Aperformance%20and%20generalizability%20across%20several%20phenotyping%20tasks%20while%0Aobviating%20the%20need%20for%20manually%20annotated%20training%20data.%20Large%20language%20models%0Aare%20expected%20to%20be%20the%20dominant%20method%20for%20automating%20high-throughput%0Aphenotyping%20of%20clinical%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01214v1&entry.124074799=Read"},
{"title": "Learning Visual Quadrupedal Loco-Manipulation from Demonstrations", "author": "Zhengmao He and Kun Lei and Yanjie Ze and Koushil Sreenath and Zhongyu Li and Huazhe Xu", "abstract": "  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n", "link": "http://arxiv.org/abs/2403.20328v2", "date": "2024-08-02", "relevancy": 1.849, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations&body=Title%3A%20Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations%0AAuthor%3A%20Zhengmao%20He%20and%20Kun%20Lei%20and%20Yanjie%20Ze%20and%20Koushil%20Sreenath%20and%20Zhongyu%20Li%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Quadruped%20robots%20are%20progressively%20being%20integrated%20into%20human%20environments.%0ADespite%20the%20growing%20locomotion%20capabilities%20of%20quadrupedal%20robots%2C%20their%0Ainteraction%20with%20objects%20in%20realistic%20scenes%20is%20still%20limited.%20While%20additional%0Arobotic%20arms%20on%20quadrupedal%20robots%20enable%20manipulating%20objects%2C%20they%20are%0Asometimes%20redundant%20given%20that%20a%20quadruped%20robot%20is%20essentially%20a%20mobile%20unit%0Aequipped%20with%20four%20limbs%2C%20each%20possessing%203%20degrees%20of%20freedom%20%28DoFs%29.%20Hence%2C%0Awe%20aim%20to%20empower%20a%20quadruped%20robot%20to%20execute%20real-world%20manipulation%20tasks%0Ausing%20only%20its%20legs.%20We%20decompose%20the%20loco-manipulation%20process%20into%20a%0Alow-level%20reinforcement%20learning%20%28RL%29-based%20controller%20and%20a%20high-level%0ABehavior%20Cloning%20%28BC%29-based%20planner.%20By%20parameterizing%20the%20manipulation%0Atrajectory%2C%20we%20synchronize%20the%20efforts%20of%20the%20upper%20and%20lower%20layers%2C%20thereby%0Aleveraging%20the%20advantages%20of%20both%20RL%20and%20BC.%20Our%20approach%20is%20validated%20through%0Asimulations%20and%20real-world%20experiments%2C%20demonstrating%20the%20robot%27s%20ability%20to%0Aperform%20tasks%20that%20demand%20mobility%20and%20high%20precision%2C%20such%20as%20lifting%20a%20basket%0Afrom%20the%20ground%20while%20moving%2C%20closing%20a%20dishwasher%2C%20pressing%20a%20button%2C%20and%0Apushing%20a%20door.%20Project%20website%3A%20https%3A//zhengmaohe.github.io/leg-manip%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visual%2520Quadrupedal%2520Loco-Manipulation%2520from%2520Demonstrations%26entry.906535625%3DZhengmao%2520He%2520and%2520Kun%2520Lei%2520and%2520Yanjie%2520Ze%2520and%2520Koushil%2520Sreenath%2520and%2520Zhongyu%2520Li%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Quadruped%2520robots%2520are%2520progressively%2520being%2520integrated%2520into%2520human%2520environments.%250ADespite%2520the%2520growing%2520locomotion%2520capabilities%2520of%2520quadrupedal%2520robots%252C%2520their%250Ainteraction%2520with%2520objects%2520in%2520realistic%2520scenes%2520is%2520still%2520limited.%2520While%2520additional%250Arobotic%2520arms%2520on%2520quadrupedal%2520robots%2520enable%2520manipulating%2520objects%252C%2520they%2520are%250Asometimes%2520redundant%2520given%2520that%2520a%2520quadruped%2520robot%2520is%2520essentially%2520a%2520mobile%2520unit%250Aequipped%2520with%2520four%2520limbs%252C%2520each%2520possessing%25203%2520degrees%2520of%2520freedom%2520%2528DoFs%2529.%2520Hence%252C%250Awe%2520aim%2520to%2520empower%2520a%2520quadruped%2520robot%2520to%2520execute%2520real-world%2520manipulation%2520tasks%250Ausing%2520only%2520its%2520legs.%2520We%2520decompose%2520the%2520loco-manipulation%2520process%2520into%2520a%250Alow-level%2520reinforcement%2520learning%2520%2528RL%2529-based%2520controller%2520and%2520a%2520high-level%250ABehavior%2520Cloning%2520%2528BC%2529-based%2520planner.%2520By%2520parameterizing%2520the%2520manipulation%250Atrajectory%252C%2520we%2520synchronize%2520the%2520efforts%2520of%2520the%2520upper%2520and%2520lower%2520layers%252C%2520thereby%250Aleveraging%2520the%2520advantages%2520of%2520both%2520RL%2520and%2520BC.%2520Our%2520approach%2520is%2520validated%2520through%250Asimulations%2520and%2520real-world%2520experiments%252C%2520demonstrating%2520the%2520robot%2527s%2520ability%2520to%250Aperform%2520tasks%2520that%2520demand%2520mobility%2520and%2520high%2520precision%252C%2520such%2520as%2520lifting%2520a%2520basket%250Afrom%2520the%2520ground%2520while%2520moving%252C%2520closing%2520a%2520dishwasher%252C%2520pressing%2520a%2520button%252C%2520and%250Apushing%2520a%2520door.%2520Project%2520website%253A%2520https%253A//zhengmaohe.github.io/leg-manip%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Quadrupedal%20Loco-Manipulation%20from%20Demonstrations&entry.906535625=Zhengmao%20He%20and%20Kun%20Lei%20and%20Yanjie%20Ze%20and%20Koushil%20Sreenath%20and%20Zhongyu%20Li%20and%20Huazhe%20Xu&entry.1292438233=%20%20Quadruped%20robots%20are%20progressively%20being%20integrated%20into%20human%20environments.%0ADespite%20the%20growing%20locomotion%20capabilities%20of%20quadrupedal%20robots%2C%20their%0Ainteraction%20with%20objects%20in%20realistic%20scenes%20is%20still%20limited.%20While%20additional%0Arobotic%20arms%20on%20quadrupedal%20robots%20enable%20manipulating%20objects%2C%20they%20are%0Asometimes%20redundant%20given%20that%20a%20quadruped%20robot%20is%20essentially%20a%20mobile%20unit%0Aequipped%20with%20four%20limbs%2C%20each%20possessing%203%20degrees%20of%20freedom%20%28DoFs%29.%20Hence%2C%0Awe%20aim%20to%20empower%20a%20quadruped%20robot%20to%20execute%20real-world%20manipulation%20tasks%0Ausing%20only%20its%20legs.%20We%20decompose%20the%20loco-manipulation%20process%20into%20a%0Alow-level%20reinforcement%20learning%20%28RL%29-based%20controller%20and%20a%20high-level%0ABehavior%20Cloning%20%28BC%29-based%20planner.%20By%20parameterizing%20the%20manipulation%0Atrajectory%2C%20we%20synchronize%20the%20efforts%20of%20the%20upper%20and%20lower%20layers%2C%20thereby%0Aleveraging%20the%20advantages%20of%20both%20RL%20and%20BC.%20Our%20approach%20is%20validated%20through%0Asimulations%20and%20real-world%20experiments%2C%20demonstrating%20the%20robot%27s%20ability%20to%0Aperform%20tasks%20that%20demand%20mobility%20and%20high%20precision%2C%20such%20as%20lifting%20a%20basket%0Afrom%20the%20ground%20while%20moving%2C%20closing%20a%20dishwasher%2C%20pressing%20a%20button%2C%20and%0Apushing%20a%20door.%20Project%20website%3A%20https%3A//zhengmaohe.github.io/leg-manip%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20328v2&entry.124074799=Read"},
{"title": "Motion-aware Latent Diffusion Models for Video Frame Interpolation", "author": "Zhilin Huang and Yijie Yu and Ling Yang and Chujun Qin and Bing Zheng and Xiawu Zheng and Zikun Zhou and Yaowei Wang and Wenming Yang", "abstract": "  With the advancement of AIGC, video frame interpolation (VFI) has become a\ncrucial component in existing video generation frameworks, attracting\nwidespread research interest. For the VFI task, the motion estimation between\nneighboring frames plays a crucial role in avoiding motion ambiguity. However,\nexisting VFI methods always struggle to accurately predict the motion\ninformation between consecutive frames, and this imprecise estimation leads to\nblurred and visually incoherent interpolated frames. In this paper, we propose\na novel diffusion framework, motion-aware latent diffusion models (MADiff),\nwhich is specifically designed for the VFI task. By incorporating motion priors\nbetween the conditional neighboring frames with the target interpolated frame\npredicted throughout the diffusion sampling procedure, MADiff progressively\nrefines the intermediate outcomes, culminating in generating both visually\nsmooth and realistic results. Extensive experiments conducted on benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance\nsignificantly outperforming existing approaches, especially under challenging\nscenarios involving dynamic textures with complex motion.\n", "link": "http://arxiv.org/abs/2404.13534v3", "date": "2024-08-02", "relevancy": 1.8466, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6335}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6145}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-aware%20Latent%20Diffusion%20Models%20for%20Video%20Frame%20Interpolation&body=Title%3A%20Motion-aware%20Latent%20Diffusion%20Models%20for%20Video%20Frame%20Interpolation%0AAuthor%3A%20Zhilin%20Huang%20and%20Yijie%20Yu%20and%20Ling%20Yang%20and%20Chujun%20Qin%20and%20Bing%20Zheng%20and%20Xiawu%20Zheng%20and%20Zikun%20Zhou%20and%20Yaowei%20Wang%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20AIGC%2C%20video%20frame%20interpolation%20%28VFI%29%20has%20become%20a%0Acrucial%20component%20in%20existing%20video%20generation%20frameworks%2C%20attracting%0Awidespread%20research%20interest.%20For%20the%20VFI%20task%2C%20the%20motion%20estimation%20between%0Aneighboring%20frames%20plays%20a%20crucial%20role%20in%20avoiding%20motion%20ambiguity.%20However%2C%0Aexisting%20VFI%20methods%20always%20struggle%20to%20accurately%20predict%20the%20motion%0Ainformation%20between%20consecutive%20frames%2C%20and%20this%20imprecise%20estimation%20leads%20to%0Ablurred%20and%20visually%20incoherent%20interpolated%20frames.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20diffusion%20framework%2C%20motion-aware%20latent%20diffusion%20models%20%28MADiff%29%2C%0Awhich%20is%20specifically%20designed%20for%20the%20VFI%20task.%20By%20incorporating%20motion%20priors%0Abetween%20the%20conditional%20neighboring%20frames%20with%20the%20target%20interpolated%20frame%0Apredicted%20throughout%20the%20diffusion%20sampling%20procedure%2C%20MADiff%20progressively%0Arefines%20the%20intermediate%20outcomes%2C%20culminating%20in%20generating%20both%20visually%0Asmooth%20and%20realistic%20results.%20Extensive%20experiments%20conducted%20on%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Asignificantly%20outperforming%20existing%20approaches%2C%20especially%20under%20challenging%0Ascenarios%20involving%20dynamic%20textures%20with%20complex%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13534v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-aware%2520Latent%2520Diffusion%2520Models%2520for%2520Video%2520Frame%2520Interpolation%26entry.906535625%3DZhilin%2520Huang%2520and%2520Yijie%2520Yu%2520and%2520Ling%2520Yang%2520and%2520Chujun%2520Qin%2520and%2520Bing%2520Zheng%2520and%2520Xiawu%2520Zheng%2520and%2520Zikun%2520Zhou%2520and%2520Yaowei%2520Wang%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520AIGC%252C%2520video%2520frame%2520interpolation%2520%2528VFI%2529%2520has%2520become%2520a%250Acrucial%2520component%2520in%2520existing%2520video%2520generation%2520frameworks%252C%2520attracting%250Awidespread%2520research%2520interest.%2520For%2520the%2520VFI%2520task%252C%2520the%2520motion%2520estimation%2520between%250Aneighboring%2520frames%2520plays%2520a%2520crucial%2520role%2520in%2520avoiding%2520motion%2520ambiguity.%2520However%252C%250Aexisting%2520VFI%2520methods%2520always%2520struggle%2520to%2520accurately%2520predict%2520the%2520motion%250Ainformation%2520between%2520consecutive%2520frames%252C%2520and%2520this%2520imprecise%2520estimation%2520leads%2520to%250Ablurred%2520and%2520visually%2520incoherent%2520interpolated%2520frames.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520novel%2520diffusion%2520framework%252C%2520motion-aware%2520latent%2520diffusion%2520models%2520%2528MADiff%2529%252C%250Awhich%2520is%2520specifically%2520designed%2520for%2520the%2520VFI%2520task.%2520By%2520incorporating%2520motion%2520priors%250Abetween%2520the%2520conditional%2520neighboring%2520frames%2520with%2520the%2520target%2520interpolated%2520frame%250Apredicted%2520throughout%2520the%2520diffusion%2520sampling%2520procedure%252C%2520MADiff%2520progressively%250Arefines%2520the%2520intermediate%2520outcomes%252C%2520culminating%2520in%2520generating%2520both%2520visually%250Asmooth%2520and%2520realistic%2520results.%2520Extensive%2520experiments%2520conducted%2520on%2520benchmark%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Asignificantly%2520outperforming%2520existing%2520approaches%252C%2520especially%2520under%2520challenging%250Ascenarios%2520involving%2520dynamic%2520textures%2520with%2520complex%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13534v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-aware%20Latent%20Diffusion%20Models%20for%20Video%20Frame%20Interpolation&entry.906535625=Zhilin%20Huang%20and%20Yijie%20Yu%20and%20Ling%20Yang%20and%20Chujun%20Qin%20and%20Bing%20Zheng%20and%20Xiawu%20Zheng%20and%20Zikun%20Zhou%20and%20Yaowei%20Wang%20and%20Wenming%20Yang&entry.1292438233=%20%20With%20the%20advancement%20of%20AIGC%2C%20video%20frame%20interpolation%20%28VFI%29%20has%20become%20a%0Acrucial%20component%20in%20existing%20video%20generation%20frameworks%2C%20attracting%0Awidespread%20research%20interest.%20For%20the%20VFI%20task%2C%20the%20motion%20estimation%20between%0Aneighboring%20frames%20plays%20a%20crucial%20role%20in%20avoiding%20motion%20ambiguity.%20However%2C%0Aexisting%20VFI%20methods%20always%20struggle%20to%20accurately%20predict%20the%20motion%0Ainformation%20between%20consecutive%20frames%2C%20and%20this%20imprecise%20estimation%20leads%20to%0Ablurred%20and%20visually%20incoherent%20interpolated%20frames.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20diffusion%20framework%2C%20motion-aware%20latent%20diffusion%20models%20%28MADiff%29%2C%0Awhich%20is%20specifically%20designed%20for%20the%20VFI%20task.%20By%20incorporating%20motion%20priors%0Abetween%20the%20conditional%20neighboring%20frames%20with%20the%20target%20interpolated%20frame%0Apredicted%20throughout%20the%20diffusion%20sampling%20procedure%2C%20MADiff%20progressively%0Arefines%20the%20intermediate%20outcomes%2C%20culminating%20in%20generating%20both%20visually%0Asmooth%20and%20realistic%20results.%20Extensive%20experiments%20conducted%20on%20benchmark%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Asignificantly%20outperforming%20existing%20approaches%2C%20especially%20under%20challenging%0Ascenarios%20involving%20dynamic%20textures%20with%20complex%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13534v3&entry.124074799=Read"},
{"title": "A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes", "author": "Vito Mengers and Nicolas Roth and Oliver Brock and Klaus Obermayer and Martin Rolfs", "abstract": "  How we perceive objects around us depends on what we actively attend to, yet\nour eye movements depend on the perceived objects. Still, object segmentation\nand gaze behavior are typically treated as two independent processes. Drawing\non an information processing pattern from robotics, we present a mechanistic\nmodel that simulates these processes for dynamic real-world scenes. Our\nimage-computable model uses the current scene segmentation for object-based\nsaccadic decision-making while using the foveated object to refine its scene\nsegmentation recursively. To model this refinement, we use a Bayesian filter,\nwhich also provides an uncertainty estimate for the segmentation that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior, measured by scanpath statistics,\nincluding foveation duration and saccade amplitude distributions used for\nparameter fitting and higher-level statistics not used for fitting. These\ninclude how object detections, inspections, and returns are balanced and a\ndelay of returning saccades without an explicit implementation of such temporal\ninhibition of return. Extensive simulations and ablation studies show that\nuncertainty promotes balanced exploration and that semantic object cues are\ncrucial to form the perceptual units used in object-based attention. Moreover,\nwe show how our model's modular design allows for extensions, such as\nincorporating saccadic momentum or pre-saccadic attention, to further align its\noutput with human scanpaths.\n", "link": "http://arxiv.org/abs/2408.01322v1", "date": "2024-08-02", "relevancy": 1.8366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.641}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6065}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robotics-Inspired%20Scanpath%20Model%20Reveals%20the%20Importance%20of%20Uncertainty%0A%20%20and%20Semantic%20Object%20Cues%20for%20Gaze%20Guidance%20in%20Dynamic%20Scenes&body=Title%3A%20A%20Robotics-Inspired%20Scanpath%20Model%20Reveals%20the%20Importance%20of%20Uncertainty%0A%20%20and%20Semantic%20Object%20Cues%20for%20Gaze%20Guidance%20in%20Dynamic%20Scenes%0AAuthor%3A%20Vito%20Mengers%20and%20Nicolas%20Roth%20and%20Oliver%20Brock%20and%20Klaus%20Obermayer%20and%20Martin%20Rolfs%0AAbstract%3A%20%20%20How%20we%20perceive%20objects%20around%20us%20depends%20on%20what%20we%20actively%20attend%20to%2C%20yet%0Aour%20eye%20movements%20depend%20on%20the%20perceived%20objects.%20Still%2C%20object%20segmentation%0Aand%20gaze%20behavior%20are%20typically%20treated%20as%20two%20independent%20processes.%20Drawing%0Aon%20an%20information%20processing%20pattern%20from%20robotics%2C%20we%20present%20a%20mechanistic%0Amodel%20that%20simulates%20these%20processes%20for%20dynamic%20real-world%20scenes.%20Our%0Aimage-computable%20model%20uses%20the%20current%20scene%20segmentation%20for%20object-based%0Asaccadic%20decision-making%20while%20using%20the%20foveated%20object%20to%20refine%20its%20scene%0Asegmentation%20recursively.%20To%20model%20this%20refinement%2C%20we%20use%20a%20Bayesian%20filter%2C%0Awhich%20also%20provides%20an%20uncertainty%20estimate%20for%20the%20segmentation%20that%20we%20use%20to%0Aguide%20active%20scene%20exploration.%20We%20demonstrate%20that%20this%20model%20closely%0Aresembles%20observers%27%20free%20viewing%20behavior%2C%20measured%20by%20scanpath%20statistics%2C%0Aincluding%20foveation%20duration%20and%20saccade%20amplitude%20distributions%20used%20for%0Aparameter%20fitting%20and%20higher-level%20statistics%20not%20used%20for%20fitting.%20These%0Ainclude%20how%20object%20detections%2C%20inspections%2C%20and%20returns%20are%20balanced%20and%20a%0Adelay%20of%20returning%20saccades%20without%20an%20explicit%20implementation%20of%20such%20temporal%0Ainhibition%20of%20return.%20Extensive%20simulations%20and%20ablation%20studies%20show%20that%0Auncertainty%20promotes%20balanced%20exploration%20and%20that%20semantic%20object%20cues%20are%0Acrucial%20to%20form%20the%20perceptual%20units%20used%20in%20object-based%20attention.%20Moreover%2C%0Awe%20show%20how%20our%20model%27s%20modular%20design%20allows%20for%20extensions%2C%20such%20as%0Aincorporating%20saccadic%20momentum%20or%20pre-saccadic%20attention%2C%20to%20further%20align%20its%0Aoutput%20with%20human%20scanpaths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robotics-Inspired%2520Scanpath%2520Model%2520Reveals%2520the%2520Importance%2520of%2520Uncertainty%250A%2520%2520and%2520Semantic%2520Object%2520Cues%2520for%2520Gaze%2520Guidance%2520in%2520Dynamic%2520Scenes%26entry.906535625%3DVito%2520Mengers%2520and%2520Nicolas%2520Roth%2520and%2520Oliver%2520Brock%2520and%2520Klaus%2520Obermayer%2520and%2520Martin%2520Rolfs%26entry.1292438233%3D%2520%2520How%2520we%2520perceive%2520objects%2520around%2520us%2520depends%2520on%2520what%2520we%2520actively%2520attend%2520to%252C%2520yet%250Aour%2520eye%2520movements%2520depend%2520on%2520the%2520perceived%2520objects.%2520Still%252C%2520object%2520segmentation%250Aand%2520gaze%2520behavior%2520are%2520typically%2520treated%2520as%2520two%2520independent%2520processes.%2520Drawing%250Aon%2520an%2520information%2520processing%2520pattern%2520from%2520robotics%252C%2520we%2520present%2520a%2520mechanistic%250Amodel%2520that%2520simulates%2520these%2520processes%2520for%2520dynamic%2520real-world%2520scenes.%2520Our%250Aimage-computable%2520model%2520uses%2520the%2520current%2520scene%2520segmentation%2520for%2520object-based%250Asaccadic%2520decision-making%2520while%2520using%2520the%2520foveated%2520object%2520to%2520refine%2520its%2520scene%250Asegmentation%2520recursively.%2520To%2520model%2520this%2520refinement%252C%2520we%2520use%2520a%2520Bayesian%2520filter%252C%250Awhich%2520also%2520provides%2520an%2520uncertainty%2520estimate%2520for%2520the%2520segmentation%2520that%2520we%2520use%2520to%250Aguide%2520active%2520scene%2520exploration.%2520We%2520demonstrate%2520that%2520this%2520model%2520closely%250Aresembles%2520observers%2527%2520free%2520viewing%2520behavior%252C%2520measured%2520by%2520scanpath%2520statistics%252C%250Aincluding%2520foveation%2520duration%2520and%2520saccade%2520amplitude%2520distributions%2520used%2520for%250Aparameter%2520fitting%2520and%2520higher-level%2520statistics%2520not%2520used%2520for%2520fitting.%2520These%250Ainclude%2520how%2520object%2520detections%252C%2520inspections%252C%2520and%2520returns%2520are%2520balanced%2520and%2520a%250Adelay%2520of%2520returning%2520saccades%2520without%2520an%2520explicit%2520implementation%2520of%2520such%2520temporal%250Ainhibition%2520of%2520return.%2520Extensive%2520simulations%2520and%2520ablation%2520studies%2520show%2520that%250Auncertainty%2520promotes%2520balanced%2520exploration%2520and%2520that%2520semantic%2520object%2520cues%2520are%250Acrucial%2520to%2520form%2520the%2520perceptual%2520units%2520used%2520in%2520object-based%2520attention.%2520Moreover%252C%250Awe%2520show%2520how%2520our%2520model%2527s%2520modular%2520design%2520allows%2520for%2520extensions%252C%2520such%2520as%250Aincorporating%2520saccadic%2520momentum%2520or%2520pre-saccadic%2520attention%252C%2520to%2520further%2520align%2520its%250Aoutput%2520with%2520human%2520scanpaths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robotics-Inspired%20Scanpath%20Model%20Reveals%20the%20Importance%20of%20Uncertainty%0A%20%20and%20Semantic%20Object%20Cues%20for%20Gaze%20Guidance%20in%20Dynamic%20Scenes&entry.906535625=Vito%20Mengers%20and%20Nicolas%20Roth%20and%20Oliver%20Brock%20and%20Klaus%20Obermayer%20and%20Martin%20Rolfs&entry.1292438233=%20%20How%20we%20perceive%20objects%20around%20us%20depends%20on%20what%20we%20actively%20attend%20to%2C%20yet%0Aour%20eye%20movements%20depend%20on%20the%20perceived%20objects.%20Still%2C%20object%20segmentation%0Aand%20gaze%20behavior%20are%20typically%20treated%20as%20two%20independent%20processes.%20Drawing%0Aon%20an%20information%20processing%20pattern%20from%20robotics%2C%20we%20present%20a%20mechanistic%0Amodel%20that%20simulates%20these%20processes%20for%20dynamic%20real-world%20scenes.%20Our%0Aimage-computable%20model%20uses%20the%20current%20scene%20segmentation%20for%20object-based%0Asaccadic%20decision-making%20while%20using%20the%20foveated%20object%20to%20refine%20its%20scene%0Asegmentation%20recursively.%20To%20model%20this%20refinement%2C%20we%20use%20a%20Bayesian%20filter%2C%0Awhich%20also%20provides%20an%20uncertainty%20estimate%20for%20the%20segmentation%20that%20we%20use%20to%0Aguide%20active%20scene%20exploration.%20We%20demonstrate%20that%20this%20model%20closely%0Aresembles%20observers%27%20free%20viewing%20behavior%2C%20measured%20by%20scanpath%20statistics%2C%0Aincluding%20foveation%20duration%20and%20saccade%20amplitude%20distributions%20used%20for%0Aparameter%20fitting%20and%20higher-level%20statistics%20not%20used%20for%20fitting.%20These%0Ainclude%20how%20object%20detections%2C%20inspections%2C%20and%20returns%20are%20balanced%20and%20a%0Adelay%20of%20returning%20saccades%20without%20an%20explicit%20implementation%20of%20such%20temporal%0Ainhibition%20of%20return.%20Extensive%20simulations%20and%20ablation%20studies%20show%20that%0Auncertainty%20promotes%20balanced%20exploration%20and%20that%20semantic%20object%20cues%20are%0Acrucial%20to%20form%20the%20perceptual%20units%20used%20in%20object-based%20attention.%20Moreover%2C%0Awe%20show%20how%20our%20model%27s%20modular%20design%20allows%20for%20extensions%2C%20such%20as%0Aincorporating%20saccadic%20momentum%20or%20pre-saccadic%20attention%2C%20to%20further%20align%20its%0Aoutput%20with%20human%20scanpaths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01322v1&entry.124074799=Read"},
{"title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs", "author": "Jingtong Su and Julia Kempe and Karen Ullrich", "abstract": "  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n", "link": "http://arxiv.org/abs/2408.01420v1", "date": "2024-08-02", "relevancy": 1.8273, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4702}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4639}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mission%20Impossible%3A%20A%20Statistical%20Perspective%20on%20Jailbreaking%20LLMs&body=Title%3A%20Mission%20Impossible%3A%20A%20Statistical%20Perspective%20on%20Jailbreaking%20LLMs%0AAuthor%3A%20Jingtong%20Su%20and%20Julia%20Kempe%20and%20Karen%20Ullrich%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20a%20deluge%20of%20text%20data%20with%0Alimited%20quality%20control.%20As%20a%20result%2C%20LLMs%20can%20exhibit%20unintended%20or%20even%0Aharmful%20behaviours%2C%20such%20as%20leaking%20information%2C%20fake%20news%20or%20hate%20speech.%0ACountermeasures%2C%20commonly%20referred%20to%20as%20preference%20alignment%2C%20include%0Afine-tuning%20the%20pretrained%20LLMs%20with%20carefully%20crafted%20text%20examples%20of%20desired%0Abehaviour.%20Even%20then%2C%20empirical%20evidence%20shows%20preference%20aligned%20LLMs%20can%20be%0Aenticed%20to%20harmful%20behaviour.%20This%20so%20called%20jailbreaking%20of%20LLMs%20is%20typically%0Aachieved%20by%20adversarially%20modifying%20the%20input%20prompt%20to%20the%20LLM.%20Our%20paper%0Aprovides%20theoretical%20insights%20into%20the%20phenomenon%20of%20preference%20alignment%20and%0Ajailbreaking%20from%20a%20statistical%20perspective.%20Under%20our%20framework%2C%20we%20first%20show%0Athat%20pretrained%20LLMs%20will%20mimic%20harmful%20behaviour%20if%20present%20in%20the%20training%0Acorpus.%20Under%20that%20same%20framework%2C%20we%20then%20introduce%20a%20statistical%20notion%20of%0Aalignment%2C%20and%20lower-bound%20the%20jailbreaking%20probability%2C%20showing%20that%20it%20is%0Aunpreventable%20under%20reasonable%20assumptions.%20Based%20on%20our%20insights%2C%20we%20propose%0Aan%20alteration%20to%20the%20currently%20prevalent%20alignment%20strategy%20RLHF.%20Specifically%2C%0Awe%20introduce%20a%20simple%20modification%20to%20the%20RLHF%20objective%2C%20we%20call%20E-RLHF%2C%20that%0Aaims%20to%20increase%20the%20likelihood%20of%20safe%20responses.%20E-RLHF%20brings%20no%20additional%0Atraining%20cost%2C%20and%20is%20compatible%20with%20other%20methods.%20Empirically%2C%20we%0Ademonstrate%20that%20E-RLHF%20outperforms%20RLHF%20on%20all%20alignment%20problems%20put%20forward%0Aby%20the%20AdvBench%20and%20HarmBench%20project%20without%20sacrificing%20model%20performance%20as%0Ameasured%20by%20the%20MT-Bench%20project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMission%2520Impossible%253A%2520A%2520Statistical%2520Perspective%2520on%2520Jailbreaking%2520LLMs%26entry.906535625%3DJingtong%2520Su%2520and%2520Julia%2520Kempe%2520and%2520Karen%2520Ullrich%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520a%2520deluge%2520of%2520text%2520data%2520with%250Alimited%2520quality%2520control.%2520As%2520a%2520result%252C%2520LLMs%2520can%2520exhibit%2520unintended%2520or%2520even%250Aharmful%2520behaviours%252C%2520such%2520as%2520leaking%2520information%252C%2520fake%2520news%2520or%2520hate%2520speech.%250ACountermeasures%252C%2520commonly%2520referred%2520to%2520as%2520preference%2520alignment%252C%2520include%250Afine-tuning%2520the%2520pretrained%2520LLMs%2520with%2520carefully%2520crafted%2520text%2520examples%2520of%2520desired%250Abehaviour.%2520Even%2520then%252C%2520empirical%2520evidence%2520shows%2520preference%2520aligned%2520LLMs%2520can%2520be%250Aenticed%2520to%2520harmful%2520behaviour.%2520This%2520so%2520called%2520jailbreaking%2520of%2520LLMs%2520is%2520typically%250Aachieved%2520by%2520adversarially%2520modifying%2520the%2520input%2520prompt%2520to%2520the%2520LLM.%2520Our%2520paper%250Aprovides%2520theoretical%2520insights%2520into%2520the%2520phenomenon%2520of%2520preference%2520alignment%2520and%250Ajailbreaking%2520from%2520a%2520statistical%2520perspective.%2520Under%2520our%2520framework%252C%2520we%2520first%2520show%250Athat%2520pretrained%2520LLMs%2520will%2520mimic%2520harmful%2520behaviour%2520if%2520present%2520in%2520the%2520training%250Acorpus.%2520Under%2520that%2520same%2520framework%252C%2520we%2520then%2520introduce%2520a%2520statistical%2520notion%2520of%250Aalignment%252C%2520and%2520lower-bound%2520the%2520jailbreaking%2520probability%252C%2520showing%2520that%2520it%2520is%250Aunpreventable%2520under%2520reasonable%2520assumptions.%2520Based%2520on%2520our%2520insights%252C%2520we%2520propose%250Aan%2520alteration%2520to%2520the%2520currently%2520prevalent%2520alignment%2520strategy%2520RLHF.%2520Specifically%252C%250Awe%2520introduce%2520a%2520simple%2520modification%2520to%2520the%2520RLHF%2520objective%252C%2520we%2520call%2520E-RLHF%252C%2520that%250Aaims%2520to%2520increase%2520the%2520likelihood%2520of%2520safe%2520responses.%2520E-RLHF%2520brings%2520no%2520additional%250Atraining%2520cost%252C%2520and%2520is%2520compatible%2520with%2520other%2520methods.%2520Empirically%252C%2520we%250Ademonstrate%2520that%2520E-RLHF%2520outperforms%2520RLHF%2520on%2520all%2520alignment%2520problems%2520put%2520forward%250Aby%2520the%2520AdvBench%2520and%2520HarmBench%2520project%2520without%2520sacrificing%2520model%2520performance%2520as%250Ameasured%2520by%2520the%2520MT-Bench%2520project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mission%20Impossible%3A%20A%20Statistical%20Perspective%20on%20Jailbreaking%20LLMs&entry.906535625=Jingtong%20Su%20and%20Julia%20Kempe%20and%20Karen%20Ullrich&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20a%20deluge%20of%20text%20data%20with%0Alimited%20quality%20control.%20As%20a%20result%2C%20LLMs%20can%20exhibit%20unintended%20or%20even%0Aharmful%20behaviours%2C%20such%20as%20leaking%20information%2C%20fake%20news%20or%20hate%20speech.%0ACountermeasures%2C%20commonly%20referred%20to%20as%20preference%20alignment%2C%20include%0Afine-tuning%20the%20pretrained%20LLMs%20with%20carefully%20crafted%20text%20examples%20of%20desired%0Abehaviour.%20Even%20then%2C%20empirical%20evidence%20shows%20preference%20aligned%20LLMs%20can%20be%0Aenticed%20to%20harmful%20behaviour.%20This%20so%20called%20jailbreaking%20of%20LLMs%20is%20typically%0Aachieved%20by%20adversarially%20modifying%20the%20input%20prompt%20to%20the%20LLM.%20Our%20paper%0Aprovides%20theoretical%20insights%20into%20the%20phenomenon%20of%20preference%20alignment%20and%0Ajailbreaking%20from%20a%20statistical%20perspective.%20Under%20our%20framework%2C%20we%20first%20show%0Athat%20pretrained%20LLMs%20will%20mimic%20harmful%20behaviour%20if%20present%20in%20the%20training%0Acorpus.%20Under%20that%20same%20framework%2C%20we%20then%20introduce%20a%20statistical%20notion%20of%0Aalignment%2C%20and%20lower-bound%20the%20jailbreaking%20probability%2C%20showing%20that%20it%20is%0Aunpreventable%20under%20reasonable%20assumptions.%20Based%20on%20our%20insights%2C%20we%20propose%0Aan%20alteration%20to%20the%20currently%20prevalent%20alignment%20strategy%20RLHF.%20Specifically%2C%0Awe%20introduce%20a%20simple%20modification%20to%20the%20RLHF%20objective%2C%20we%20call%20E-RLHF%2C%20that%0Aaims%20to%20increase%20the%20likelihood%20of%20safe%20responses.%20E-RLHF%20brings%20no%20additional%0Atraining%20cost%2C%20and%20is%20compatible%20with%20other%20methods.%20Empirically%2C%20we%0Ademonstrate%20that%20E-RLHF%20outperforms%20RLHF%20on%20all%20alignment%20problems%20put%20forward%0Aby%20the%20AdvBench%20and%20HarmBench%20project%20without%20sacrificing%20model%20performance%20as%0Ameasured%20by%20the%20MT-Bench%20project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01420v1&entry.124074799=Read"},
{"title": "LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison", "author": "Devam Mondal and Carlo Lipizzi", "abstract": "  In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.\n", "link": "http://arxiv.org/abs/2407.02659v2", "date": "2024-08-02", "relevancy": 1.8126, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4582}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Plagiarize%3A%20Ensuring%20Responsible%20Sourcing%20of%20Large%20Language%20Model%0A%20%20Training%20Data%20Through%20Knowledge%20Graph%20Comparison&body=Title%3A%20LLMs%20Plagiarize%3A%20Ensuring%20Responsible%20Sourcing%20of%20Large%20Language%20Model%0A%20%20Training%20Data%20Through%20Knowledge%20Graph%20Comparison%0AAuthor%3A%20Devam%20Mondal%20and%20Carlo%20Lipizzi%0AAbstract%3A%20%20%20In%20light%20of%20recent%20legal%20allegations%20brought%20by%20publishers%2C%20newspapers%2C%20and%0Aother%20creators%20of%20copyrighted%20corpora%20against%20large%20language%20model%20developers%0Awho%20use%20their%20copyrighted%20materials%20for%20training%20or%20fine-tuning%20purposes%2C%20we%0Apropose%20a%20novel%20system%2C%20a%20variant%20of%20a%20plagiarism%20detection%20system%2C%20that%0Aassesses%20whether%20a%20knowledge%20source%20has%20been%20used%20in%20the%20training%20or%0Afine-tuning%20of%20a%20large%20language%20model.%20Unlike%20current%20methods%2C%20we%20utilize%20an%0Aapproach%20that%20uses%20Resource%20Description%20Framework%20%28RDF%29%20triples%20to%20create%0Aknowledge%20graphs%20from%20both%20a%20source%20document%20and%20an%20LLM%20continuation%20of%20that%0Adocument.%20These%20graphs%20are%20then%20analyzed%20with%20respect%20to%20content%20using%20cosine%0Asimilarity%20and%20with%20respect%20to%20structure%20using%20a%20normalized%20version%20of%20graph%0Aedit%20distance%20that%20shows%20the%20degree%20of%20isomorphism.%20Unlike%20traditional%0Aplagiarism%20systems%20that%20focus%20on%20content%20matching%20and%20keyword%20identification%0Abetween%20a%20source%20and%20a%20target%20corpus%2C%20our%20approach%20enables%20a%20broader%20and%20more%0Aaccurate%20evaluation%20of%20similarity%20between%20a%20source%20document%20and%20LLM%0Acontinuation%20by%20focusing%20on%20relationships%20between%20ideas%20and%20their%20organization%0Awith%20regards%20to%20others.%20Additionally%2C%20our%20approach%20does%20not%20require%20access%20to%0ALLM%20metrics%20like%20perplexity%20that%20may%20be%20unavailable%20in%20closed%20large%20language%0Amodel%20%22black-box%22%20systems%2C%20as%20well%20as%20the%20training%20corpus.%20We%20thus%20assess%0Awhether%20an%20LLM%20has%20%22plagiarized%22%20a%20corpus%20in%20its%20continuation%20through%0Asimilarity%20measures.%20A%20prototype%20of%20our%20system%20will%20be%20found%20on%20a%20hyperlinked%0AGitHub%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Plagiarize%253A%2520Ensuring%2520Responsible%2520Sourcing%2520of%2520Large%2520Language%2520Model%250A%2520%2520Training%2520Data%2520Through%2520Knowledge%2520Graph%2520Comparison%26entry.906535625%3DDevam%2520Mondal%2520and%2520Carlo%2520Lipizzi%26entry.1292438233%3D%2520%2520In%2520light%2520of%2520recent%2520legal%2520allegations%2520brought%2520by%2520publishers%252C%2520newspapers%252C%2520and%250Aother%2520creators%2520of%2520copyrighted%2520corpora%2520against%2520large%2520language%2520model%2520developers%250Awho%2520use%2520their%2520copyrighted%2520materials%2520for%2520training%2520or%2520fine-tuning%2520purposes%252C%2520we%250Apropose%2520a%2520novel%2520system%252C%2520a%2520variant%2520of%2520a%2520plagiarism%2520detection%2520system%252C%2520that%250Aassesses%2520whether%2520a%2520knowledge%2520source%2520has%2520been%2520used%2520in%2520the%2520training%2520or%250Afine-tuning%2520of%2520a%2520large%2520language%2520model.%2520Unlike%2520current%2520methods%252C%2520we%2520utilize%2520an%250Aapproach%2520that%2520uses%2520Resource%2520Description%2520Framework%2520%2528RDF%2529%2520triples%2520to%2520create%250Aknowledge%2520graphs%2520from%2520both%2520a%2520source%2520document%2520and%2520an%2520LLM%2520continuation%2520of%2520that%250Adocument.%2520These%2520graphs%2520are%2520then%2520analyzed%2520with%2520respect%2520to%2520content%2520using%2520cosine%250Asimilarity%2520and%2520with%2520respect%2520to%2520structure%2520using%2520a%2520normalized%2520version%2520of%2520graph%250Aedit%2520distance%2520that%2520shows%2520the%2520degree%2520of%2520isomorphism.%2520Unlike%2520traditional%250Aplagiarism%2520systems%2520that%2520focus%2520on%2520content%2520matching%2520and%2520keyword%2520identification%250Abetween%2520a%2520source%2520and%2520a%2520target%2520corpus%252C%2520our%2520approach%2520enables%2520a%2520broader%2520and%2520more%250Aaccurate%2520evaluation%2520of%2520similarity%2520between%2520a%2520source%2520document%2520and%2520LLM%250Acontinuation%2520by%2520focusing%2520on%2520relationships%2520between%2520ideas%2520and%2520their%2520organization%250Awith%2520regards%2520to%2520others.%2520Additionally%252C%2520our%2520approach%2520does%2520not%2520require%2520access%2520to%250ALLM%2520metrics%2520like%2520perplexity%2520that%2520may%2520be%2520unavailable%2520in%2520closed%2520large%2520language%250Amodel%2520%2522black-box%2522%2520systems%252C%2520as%2520well%2520as%2520the%2520training%2520corpus.%2520We%2520thus%2520assess%250Awhether%2520an%2520LLM%2520has%2520%2522plagiarized%2522%2520a%2520corpus%2520in%2520its%2520continuation%2520through%250Asimilarity%2520measures.%2520A%2520prototype%2520of%2520our%2520system%2520will%2520be%2520found%2520on%2520a%2520hyperlinked%250AGitHub%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Plagiarize%3A%20Ensuring%20Responsible%20Sourcing%20of%20Large%20Language%20Model%0A%20%20Training%20Data%20Through%20Knowledge%20Graph%20Comparison&entry.906535625=Devam%20Mondal%20and%20Carlo%20Lipizzi&entry.1292438233=%20%20In%20light%20of%20recent%20legal%20allegations%20brought%20by%20publishers%2C%20newspapers%2C%20and%0Aother%20creators%20of%20copyrighted%20corpora%20against%20large%20language%20model%20developers%0Awho%20use%20their%20copyrighted%20materials%20for%20training%20or%20fine-tuning%20purposes%2C%20we%0Apropose%20a%20novel%20system%2C%20a%20variant%20of%20a%20plagiarism%20detection%20system%2C%20that%0Aassesses%20whether%20a%20knowledge%20source%20has%20been%20used%20in%20the%20training%20or%0Afine-tuning%20of%20a%20large%20language%20model.%20Unlike%20current%20methods%2C%20we%20utilize%20an%0Aapproach%20that%20uses%20Resource%20Description%20Framework%20%28RDF%29%20triples%20to%20create%0Aknowledge%20graphs%20from%20both%20a%20source%20document%20and%20an%20LLM%20continuation%20of%20that%0Adocument.%20These%20graphs%20are%20then%20analyzed%20with%20respect%20to%20content%20using%20cosine%0Asimilarity%20and%20with%20respect%20to%20structure%20using%20a%20normalized%20version%20of%20graph%0Aedit%20distance%20that%20shows%20the%20degree%20of%20isomorphism.%20Unlike%20traditional%0Aplagiarism%20systems%20that%20focus%20on%20content%20matching%20and%20keyword%20identification%0Abetween%20a%20source%20and%20a%20target%20corpus%2C%20our%20approach%20enables%20a%20broader%20and%20more%0Aaccurate%20evaluation%20of%20similarity%20between%20a%20source%20document%20and%20LLM%0Acontinuation%20by%20focusing%20on%20relationships%20between%20ideas%20and%20their%20organization%0Awith%20regards%20to%20others.%20Additionally%2C%20our%20approach%20does%20not%20require%20access%20to%0ALLM%20metrics%20like%20perplexity%20that%20may%20be%20unavailable%20in%20closed%20large%20language%0Amodel%20%22black-box%22%20systems%2C%20as%20well%20as%20the%20training%20corpus.%20We%20thus%20assess%0Awhether%20an%20LLM%20has%20%22plagiarized%22%20a%20corpus%20in%20its%20continuation%20through%0Asimilarity%20measures.%20A%20prototype%20of%20our%20system%20will%20be%20found%20on%20a%20hyperlinked%0AGitHub%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02659v2&entry.124074799=Read"},
{"title": "Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and\n  Outlook", "author": "Ziying Song and Lin Liu and Feiyang Jia and Yadan Luo and Guoxin Zhang and Lei Yang and Li Wang and Caiyan Jia", "abstract": "  In the realm of modern autonomous driving, the perception system is\nindispensable for accurately assessing the state of the surrounding\nenvironment, thereby enabling informed prediction and planning. The key step to\nthis system is related to 3D object detection that utilizes vehicle-mounted\nsensors such as LiDAR and cameras to identify the size, the category, and the\nlocation of nearby objects. Despite the surge in 3D object detection methods\naimed at enhancing detection precision and efficiency, there is a gap in the\nliterature that systematically examines their resilience against environmental\nvariations, noise, and weather changes. This study emphasizes the importance of\nrobustness, alongside accuracy and latency, in evaluating perception systems\nunder practical scenarios. Our work presents an extensive survey of\ncamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,\nthoroughly evaluating their trade-off between accuracy, latency, and\nrobustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair\ncomparisons. Among these, multi-modal 3D detection approaches exhibit superior\nrobustness, and a novel taxonomy is introduced to reorganize the literature for\nenhanced clarity. This survey aims to offer a more practical perspective on the\ncurrent capabilities and the constraints of 3D object detection algorithms in\nreal-world applications, thus steering future research towards\nrobustness-centric advancements.\n", "link": "http://arxiv.org/abs/2401.06542v2", "date": "2024-08-02", "relevancy": 1.8067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5925}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook&body=Title%3A%20Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook%0AAuthor%3A%20Ziying%20Song%20and%20Lin%20Liu%20and%20Feiyang%20Jia%20and%20Yadan%20Luo%20and%20Guoxin%20Zhang%20and%20Lei%20Yang%20and%20Li%20Wang%20and%20Caiyan%20Jia%0AAbstract%3A%20%20%20In%20the%20realm%20of%20modern%20autonomous%20driving%2C%20the%20perception%20system%20is%0Aindispensable%20for%20accurately%20assessing%20the%20state%20of%20the%20surrounding%0Aenvironment%2C%20thereby%20enabling%20informed%20prediction%20and%20planning.%20The%20key%20step%20to%0Athis%20system%20is%20related%20to%203D%20object%20detection%20that%20utilizes%20vehicle-mounted%0Asensors%20such%20as%20LiDAR%20and%20cameras%20to%20identify%20the%20size%2C%20the%20category%2C%20and%20the%0Alocation%20of%20nearby%20objects.%20Despite%20the%20surge%20in%203D%20object%20detection%20methods%0Aaimed%20at%20enhancing%20detection%20precision%20and%20efficiency%2C%20there%20is%20a%20gap%20in%20the%0Aliterature%20that%20systematically%20examines%20their%20resilience%20against%20environmental%0Avariations%2C%20noise%2C%20and%20weather%20changes.%20This%20study%20emphasizes%20the%20importance%20of%0Arobustness%2C%20alongside%20accuracy%20and%20latency%2C%20in%20evaluating%20perception%20systems%0Aunder%20practical%20scenarios.%20Our%20work%20presents%20an%20extensive%20survey%20of%0Acamera-only%2C%20LiDAR-only%2C%20and%20multi-modal%203D%20object%20detection%20algorithms%2C%0Athoroughly%20evaluating%20their%20trade-off%20between%20accuracy%2C%20latency%2C%20and%0Arobustness%2C%20particularly%20on%20datasets%20like%20KITTI-C%20and%20nuScenes-C%20to%20ensure%20fair%0Acomparisons.%20Among%20these%2C%20multi-modal%203D%20detection%20approaches%20exhibit%20superior%0Arobustness%2C%20and%20a%20novel%20taxonomy%20is%20introduced%20to%20reorganize%20the%20literature%20for%0Aenhanced%20clarity.%20This%20survey%20aims%20to%20offer%20a%20more%20practical%20perspective%20on%20the%0Acurrent%20capabilities%20and%20the%20constraints%20of%203D%20object%20detection%20algorithms%20in%0Areal-world%20applications%2C%20thus%20steering%20future%20research%20towards%0Arobustness-centric%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness-Aware%25203D%2520Object%2520Detection%2520in%2520Autonomous%2520Driving%253A%2520A%2520Review%2520and%250A%2520%2520Outlook%26entry.906535625%3DZiying%2520Song%2520and%2520Lin%2520Liu%2520and%2520Feiyang%2520Jia%2520and%2520Yadan%2520Luo%2520and%2520Guoxin%2520Zhang%2520and%2520Lei%2520Yang%2520and%2520Li%2520Wang%2520and%2520Caiyan%2520Jia%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520modern%2520autonomous%2520driving%252C%2520the%2520perception%2520system%2520is%250Aindispensable%2520for%2520accurately%2520assessing%2520the%2520state%2520of%2520the%2520surrounding%250Aenvironment%252C%2520thereby%2520enabling%2520informed%2520prediction%2520and%2520planning.%2520The%2520key%2520step%2520to%250Athis%2520system%2520is%2520related%2520to%25203D%2520object%2520detection%2520that%2520utilizes%2520vehicle-mounted%250Asensors%2520such%2520as%2520LiDAR%2520and%2520cameras%2520to%2520identify%2520the%2520size%252C%2520the%2520category%252C%2520and%2520the%250Alocation%2520of%2520nearby%2520objects.%2520Despite%2520the%2520surge%2520in%25203D%2520object%2520detection%2520methods%250Aaimed%2520at%2520enhancing%2520detection%2520precision%2520and%2520efficiency%252C%2520there%2520is%2520a%2520gap%2520in%2520the%250Aliterature%2520that%2520systematically%2520examines%2520their%2520resilience%2520against%2520environmental%250Avariations%252C%2520noise%252C%2520and%2520weather%2520changes.%2520This%2520study%2520emphasizes%2520the%2520importance%2520of%250Arobustness%252C%2520alongside%2520accuracy%2520and%2520latency%252C%2520in%2520evaluating%2520perception%2520systems%250Aunder%2520practical%2520scenarios.%2520Our%2520work%2520presents%2520an%2520extensive%2520survey%2520of%250Acamera-only%252C%2520LiDAR-only%252C%2520and%2520multi-modal%25203D%2520object%2520detection%2520algorithms%252C%250Athoroughly%2520evaluating%2520their%2520trade-off%2520between%2520accuracy%252C%2520latency%252C%2520and%250Arobustness%252C%2520particularly%2520on%2520datasets%2520like%2520KITTI-C%2520and%2520nuScenes-C%2520to%2520ensure%2520fair%250Acomparisons.%2520Among%2520these%252C%2520multi-modal%25203D%2520detection%2520approaches%2520exhibit%2520superior%250Arobustness%252C%2520and%2520a%2520novel%2520taxonomy%2520is%2520introduced%2520to%2520reorganize%2520the%2520literature%2520for%250Aenhanced%2520clarity.%2520This%2520survey%2520aims%2520to%2520offer%2520a%2520more%2520practical%2520perspective%2520on%2520the%250Acurrent%2520capabilities%2520and%2520the%2520constraints%2520of%25203D%2520object%2520detection%2520algorithms%2520in%250Areal-world%2520applications%252C%2520thus%2520steering%2520future%2520research%2520towards%250Arobustness-centric%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook&entry.906535625=Ziying%20Song%20and%20Lin%20Liu%20and%20Feiyang%20Jia%20and%20Yadan%20Luo%20and%20Guoxin%20Zhang%20and%20Lei%20Yang%20and%20Li%20Wang%20and%20Caiyan%20Jia&entry.1292438233=%20%20In%20the%20realm%20of%20modern%20autonomous%20driving%2C%20the%20perception%20system%20is%0Aindispensable%20for%20accurately%20assessing%20the%20state%20of%20the%20surrounding%0Aenvironment%2C%20thereby%20enabling%20informed%20prediction%20and%20planning.%20The%20key%20step%20to%0Athis%20system%20is%20related%20to%203D%20object%20detection%20that%20utilizes%20vehicle-mounted%0Asensors%20such%20as%20LiDAR%20and%20cameras%20to%20identify%20the%20size%2C%20the%20category%2C%20and%20the%0Alocation%20of%20nearby%20objects.%20Despite%20the%20surge%20in%203D%20object%20detection%20methods%0Aaimed%20at%20enhancing%20detection%20precision%20and%20efficiency%2C%20there%20is%20a%20gap%20in%20the%0Aliterature%20that%20systematically%20examines%20their%20resilience%20against%20environmental%0Avariations%2C%20noise%2C%20and%20weather%20changes.%20This%20study%20emphasizes%20the%20importance%20of%0Arobustness%2C%20alongside%20accuracy%20and%20latency%2C%20in%20evaluating%20perception%20systems%0Aunder%20practical%20scenarios.%20Our%20work%20presents%20an%20extensive%20survey%20of%0Acamera-only%2C%20LiDAR-only%2C%20and%20multi-modal%203D%20object%20detection%20algorithms%2C%0Athoroughly%20evaluating%20their%20trade-off%20between%20accuracy%2C%20latency%2C%20and%0Arobustness%2C%20particularly%20on%20datasets%20like%20KITTI-C%20and%20nuScenes-C%20to%20ensure%20fair%0Acomparisons.%20Among%20these%2C%20multi-modal%203D%20detection%20approaches%20exhibit%20superior%0Arobustness%2C%20and%20a%20novel%20taxonomy%20is%20introduced%20to%20reorganize%20the%20literature%20for%0Aenhanced%20clarity.%20This%20survey%20aims%20to%20offer%20a%20more%20practical%20perspective%20on%20the%0Acurrent%20capabilities%20and%20the%20constraints%20of%203D%20object%20detection%20algorithms%20in%0Areal-world%20applications%2C%20thus%20steering%20future%20research%20towards%0Arobustness-centric%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06542v2&entry.124074799=Read"},
{"title": "Automated System-level Testing of Unmanned Aerial Systems", "author": "Hassan Sartaj and Asmar Muqeet and Muhammad Zohaib Iqbal and Muhammad Uzair Khan", "abstract": "  Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.\n", "link": "http://arxiv.org/abs/2403.15857v2", "date": "2024-08-02", "relevancy": 1.8045, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4642}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20System-level%20Testing%20of%20Unmanned%20Aerial%20Systems&body=Title%3A%20Automated%20System-level%20Testing%20of%20Unmanned%20Aerial%20Systems%0AAuthor%3A%20Hassan%20Sartaj%20and%20Asmar%20Muqeet%20and%20Muhammad%20Zohaib%20Iqbal%20and%20Muhammad%20Uzair%20Khan%0AAbstract%3A%20%20%20Unmanned%20aerial%20systems%20%28UAS%29%20rely%20on%20various%20avionics%20systems%20that%20are%0Asafety-critical%20and%20mission-critical.%20A%20major%20requirement%20of%20international%0Asafety%20standards%20is%20to%20perform%20rigorous%20system-level%20testing%20of%20avionics%0Asoftware%20systems.%20The%20current%20industrial%20practice%20is%20to%20manually%20create%20test%0Ascenarios%2C%20manually/automatically%20execute%20these%20scenarios%20using%20simulators%2C%20and%0Amanually%20evaluate%20outcomes.%20The%20test%20scenarios%20typically%20consist%20of%20setting%0Acertain%20flight%20or%20environment%20conditions%20and%20testing%20the%20system%20under%20test%20in%0Athese%20settings.%20The%20state-of-the-art%20approaches%20for%20this%20purpose%20also%20require%0Amanual%20test%20scenario%20development%20and%20evaluation.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20approach%20to%20automate%20the%20system-level%20testing%20of%20the%20UAS.%20The%20proposed%0Aapproach%20%28AITester%29%20utilizes%20model-based%20testing%20and%20artificial%20intelligence%0A%28AI%29%20techniques%20to%20automatically%20generate%2C%20execute%2C%20and%20evaluate%20various%20test%0Ascenarios.%20The%20test%20scenarios%20are%20generated%20on%20the%20fly%2C%20i.e.%2C%20during%20test%0Aexecution%20based%20on%20the%20environmental%20context%20at%20runtime.%20The%20approach%20is%0Asupported%20by%20a%20toolset.%20We%20empirically%20evaluate%20the%20proposed%20approach%20on%20two%0Acore%20components%20of%20UAS%2C%20an%20autopilot%20system%20of%20an%20unmanned%20aerial%20vehicle%20%28UAV%29%0Aand%20cockpit%20display%20systems%20%28CDS%29%20of%20the%20ground%20control%20station%20%28GCS%29.%20The%0Aresults%20show%20that%20the%20AITester%20effectively%20generates%20test%20scenarios%20causing%0Adeviations%20from%20the%20expected%20behavior%20of%20the%20UAV%20autopilot%20and%20reveals%0Apotential%20flaws%20in%20the%20GCS-CDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520System-level%2520Testing%2520of%2520Unmanned%2520Aerial%2520Systems%26entry.906535625%3DHassan%2520Sartaj%2520and%2520Asmar%2520Muqeet%2520and%2520Muhammad%2520Zohaib%2520Iqbal%2520and%2520Muhammad%2520Uzair%2520Khan%26entry.1292438233%3D%2520%2520Unmanned%2520aerial%2520systems%2520%2528UAS%2529%2520rely%2520on%2520various%2520avionics%2520systems%2520that%2520are%250Asafety-critical%2520and%2520mission-critical.%2520A%2520major%2520requirement%2520of%2520international%250Asafety%2520standards%2520is%2520to%2520perform%2520rigorous%2520system-level%2520testing%2520of%2520avionics%250Asoftware%2520systems.%2520The%2520current%2520industrial%2520practice%2520is%2520to%2520manually%2520create%2520test%250Ascenarios%252C%2520manually/automatically%2520execute%2520these%2520scenarios%2520using%2520simulators%252C%2520and%250Amanually%2520evaluate%2520outcomes.%2520The%2520test%2520scenarios%2520typically%2520consist%2520of%2520setting%250Acertain%2520flight%2520or%2520environment%2520conditions%2520and%2520testing%2520the%2520system%2520under%2520test%2520in%250Athese%2520settings.%2520The%2520state-of-the-art%2520approaches%2520for%2520this%2520purpose%2520also%2520require%250Amanual%2520test%2520scenario%2520development%2520and%2520evaluation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520to%2520automate%2520the%2520system-level%2520testing%2520of%2520the%2520UAS.%2520The%2520proposed%250Aapproach%2520%2528AITester%2529%2520utilizes%2520model-based%2520testing%2520and%2520artificial%2520intelligence%250A%2528AI%2529%2520techniques%2520to%2520automatically%2520generate%252C%2520execute%252C%2520and%2520evaluate%2520various%2520test%250Ascenarios.%2520The%2520test%2520scenarios%2520are%2520generated%2520on%2520the%2520fly%252C%2520i.e.%252C%2520during%2520test%250Aexecution%2520based%2520on%2520the%2520environmental%2520context%2520at%2520runtime.%2520The%2520approach%2520is%250Asupported%2520by%2520a%2520toolset.%2520We%2520empirically%2520evaluate%2520the%2520proposed%2520approach%2520on%2520two%250Acore%2520components%2520of%2520UAS%252C%2520an%2520autopilot%2520system%2520of%2520an%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%250Aand%2520cockpit%2520display%2520systems%2520%2528CDS%2529%2520of%2520the%2520ground%2520control%2520station%2520%2528GCS%2529.%2520The%250Aresults%2520show%2520that%2520the%2520AITester%2520effectively%2520generates%2520test%2520scenarios%2520causing%250Adeviations%2520from%2520the%2520expected%2520behavior%2520of%2520the%2520UAV%2520autopilot%2520and%2520reveals%250Apotential%2520flaws%2520in%2520the%2520GCS-CDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20System-level%20Testing%20of%20Unmanned%20Aerial%20Systems&entry.906535625=Hassan%20Sartaj%20and%20Asmar%20Muqeet%20and%20Muhammad%20Zohaib%20Iqbal%20and%20Muhammad%20Uzair%20Khan&entry.1292438233=%20%20Unmanned%20aerial%20systems%20%28UAS%29%20rely%20on%20various%20avionics%20systems%20that%20are%0Asafety-critical%20and%20mission-critical.%20A%20major%20requirement%20of%20international%0Asafety%20standards%20is%20to%20perform%20rigorous%20system-level%20testing%20of%20avionics%0Asoftware%20systems.%20The%20current%20industrial%20practice%20is%20to%20manually%20create%20test%0Ascenarios%2C%20manually/automatically%20execute%20these%20scenarios%20using%20simulators%2C%20and%0Amanually%20evaluate%20outcomes.%20The%20test%20scenarios%20typically%20consist%20of%20setting%0Acertain%20flight%20or%20environment%20conditions%20and%20testing%20the%20system%20under%20test%20in%0Athese%20settings.%20The%20state-of-the-art%20approaches%20for%20this%20purpose%20also%20require%0Amanual%20test%20scenario%20development%20and%20evaluation.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20approach%20to%20automate%20the%20system-level%20testing%20of%20the%20UAS.%20The%20proposed%0Aapproach%20%28AITester%29%20utilizes%20model-based%20testing%20and%20artificial%20intelligence%0A%28AI%29%20techniques%20to%20automatically%20generate%2C%20execute%2C%20and%20evaluate%20various%20test%0Ascenarios.%20The%20test%20scenarios%20are%20generated%20on%20the%20fly%2C%20i.e.%2C%20during%20test%0Aexecution%20based%20on%20the%20environmental%20context%20at%20runtime.%20The%20approach%20is%0Asupported%20by%20a%20toolset.%20We%20empirically%20evaluate%20the%20proposed%20approach%20on%20two%0Acore%20components%20of%20UAS%2C%20an%20autopilot%20system%20of%20an%20unmanned%20aerial%20vehicle%20%28UAV%29%0Aand%20cockpit%20display%20systems%20%28CDS%29%20of%20the%20ground%20control%20station%20%28GCS%29.%20The%0Aresults%20show%20that%20the%20AITester%20effectively%20generates%20test%20scenarios%20causing%0Adeviations%20from%20the%20expected%20behavior%20of%20the%20UAV%20autopilot%20and%20reveals%0Apotential%20flaws%20in%20the%20GCS-CDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15857v2&entry.124074799=Read"},
{"title": "Adaptive Recruitment Resource Allocation to Improve Cohort\n  Representativeness in Participatory Biomedical Datasets", "author": "Victor Borza and Andrew Estornell and Ellen Wright Clayton and Chien-Ju Ho and Russell Rothman and Yevgeniy Vorobeychik and Bradley Malin", "abstract": "  Large participatory biomedical studies, studies that recruit individuals to\njoin a dataset, are gaining popularity and investment, especially for analysis\nby modern AI methods. Because they purposively recruit participants, these\nstudies are uniquely able to address a lack of historical representation, an\nissue that has affected many biomedical datasets. In this work, we define\nrepresentativeness as the similarity to a target population distribution of a\nset of attributes and our goal is to mirror the U.S. population across\ndistributions of age, gender, race, and ethnicity. Many participatory studies\nrecruit at several institutions, so we introduce a computational approach to\nadaptively allocate recruitment resources among sites to improve\nrepresentativeness. In simulated recruitment of 10,000-participant cohorts from\nmedical centers in the STAR Clinical Research Network, we show that our\napproach yields a more representative cohort than existing baselines. Thus, we\nhighlight the value of computational modeling in guiding recruitment efforts.\n", "link": "http://arxiv.org/abs/2408.01375v1", "date": "2024-08-02", "relevancy": 1.7863, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Recruitment%20Resource%20Allocation%20to%20Improve%20Cohort%0A%20%20Representativeness%20in%20Participatory%20Biomedical%20Datasets&body=Title%3A%20Adaptive%20Recruitment%20Resource%20Allocation%20to%20Improve%20Cohort%0A%20%20Representativeness%20in%20Participatory%20Biomedical%20Datasets%0AAuthor%3A%20Victor%20Borza%20and%20Andrew%20Estornell%20and%20Ellen%20Wright%20Clayton%20and%20Chien-Ju%20Ho%20and%20Russell%20Rothman%20and%20Yevgeniy%20Vorobeychik%20and%20Bradley%20Malin%0AAbstract%3A%20%20%20Large%20participatory%20biomedical%20studies%2C%20studies%20that%20recruit%20individuals%20to%0Ajoin%20a%20dataset%2C%20are%20gaining%20popularity%20and%20investment%2C%20especially%20for%20analysis%0Aby%20modern%20AI%20methods.%20Because%20they%20purposively%20recruit%20participants%2C%20these%0Astudies%20are%20uniquely%20able%20to%20address%20a%20lack%20of%20historical%20representation%2C%20an%0Aissue%20that%20has%20affected%20many%20biomedical%20datasets.%20In%20this%20work%2C%20we%20define%0Arepresentativeness%20as%20the%20similarity%20to%20a%20target%20population%20distribution%20of%20a%0Aset%20of%20attributes%20and%20our%20goal%20is%20to%20mirror%20the%20U.S.%20population%20across%0Adistributions%20of%20age%2C%20gender%2C%20race%2C%20and%20ethnicity.%20Many%20participatory%20studies%0Arecruit%20at%20several%20institutions%2C%20so%20we%20introduce%20a%20computational%20approach%20to%0Aadaptively%20allocate%20recruitment%20resources%20among%20sites%20to%20improve%0Arepresentativeness.%20In%20simulated%20recruitment%20of%2010%2C000-participant%20cohorts%20from%0Amedical%20centers%20in%20the%20STAR%20Clinical%20Research%20Network%2C%20we%20show%20that%20our%0Aapproach%20yields%20a%20more%20representative%20cohort%20than%20existing%20baselines.%20Thus%2C%20we%0Ahighlight%20the%20value%20of%20computational%20modeling%20in%20guiding%20recruitment%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Recruitment%2520Resource%2520Allocation%2520to%2520Improve%2520Cohort%250A%2520%2520Representativeness%2520in%2520Participatory%2520Biomedical%2520Datasets%26entry.906535625%3DVictor%2520Borza%2520and%2520Andrew%2520Estornell%2520and%2520Ellen%2520Wright%2520Clayton%2520and%2520Chien-Ju%2520Ho%2520and%2520Russell%2520Rothman%2520and%2520Yevgeniy%2520Vorobeychik%2520and%2520Bradley%2520Malin%26entry.1292438233%3D%2520%2520Large%2520participatory%2520biomedical%2520studies%252C%2520studies%2520that%2520recruit%2520individuals%2520to%250Ajoin%2520a%2520dataset%252C%2520are%2520gaining%2520popularity%2520and%2520investment%252C%2520especially%2520for%2520analysis%250Aby%2520modern%2520AI%2520methods.%2520Because%2520they%2520purposively%2520recruit%2520participants%252C%2520these%250Astudies%2520are%2520uniquely%2520able%2520to%2520address%2520a%2520lack%2520of%2520historical%2520representation%252C%2520an%250Aissue%2520that%2520has%2520affected%2520many%2520biomedical%2520datasets.%2520In%2520this%2520work%252C%2520we%2520define%250Arepresentativeness%2520as%2520the%2520similarity%2520to%2520a%2520target%2520population%2520distribution%2520of%2520a%250Aset%2520of%2520attributes%2520and%2520our%2520goal%2520is%2520to%2520mirror%2520the%2520U.S.%2520population%2520across%250Adistributions%2520of%2520age%252C%2520gender%252C%2520race%252C%2520and%2520ethnicity.%2520Many%2520participatory%2520studies%250Arecruit%2520at%2520several%2520institutions%252C%2520so%2520we%2520introduce%2520a%2520computational%2520approach%2520to%250Aadaptively%2520allocate%2520recruitment%2520resources%2520among%2520sites%2520to%2520improve%250Arepresentativeness.%2520In%2520simulated%2520recruitment%2520of%252010%252C000-participant%2520cohorts%2520from%250Amedical%2520centers%2520in%2520the%2520STAR%2520Clinical%2520Research%2520Network%252C%2520we%2520show%2520that%2520our%250Aapproach%2520yields%2520a%2520more%2520representative%2520cohort%2520than%2520existing%2520baselines.%2520Thus%252C%2520we%250Ahighlight%2520the%2520value%2520of%2520computational%2520modeling%2520in%2520guiding%2520recruitment%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Recruitment%20Resource%20Allocation%20to%20Improve%20Cohort%0A%20%20Representativeness%20in%20Participatory%20Biomedical%20Datasets&entry.906535625=Victor%20Borza%20and%20Andrew%20Estornell%20and%20Ellen%20Wright%20Clayton%20and%20Chien-Ju%20Ho%20and%20Russell%20Rothman%20and%20Yevgeniy%20Vorobeychik%20and%20Bradley%20Malin&entry.1292438233=%20%20Large%20participatory%20biomedical%20studies%2C%20studies%20that%20recruit%20individuals%20to%0Ajoin%20a%20dataset%2C%20are%20gaining%20popularity%20and%20investment%2C%20especially%20for%20analysis%0Aby%20modern%20AI%20methods.%20Because%20they%20purposively%20recruit%20participants%2C%20these%0Astudies%20are%20uniquely%20able%20to%20address%20a%20lack%20of%20historical%20representation%2C%20an%0Aissue%20that%20has%20affected%20many%20biomedical%20datasets.%20In%20this%20work%2C%20we%20define%0Arepresentativeness%20as%20the%20similarity%20to%20a%20target%20population%20distribution%20of%20a%0Aset%20of%20attributes%20and%20our%20goal%20is%20to%20mirror%20the%20U.S.%20population%20across%0Adistributions%20of%20age%2C%20gender%2C%20race%2C%20and%20ethnicity.%20Many%20participatory%20studies%0Arecruit%20at%20several%20institutions%2C%20so%20we%20introduce%20a%20computational%20approach%20to%0Aadaptively%20allocate%20recruitment%20resources%20among%20sites%20to%20improve%0Arepresentativeness.%20In%20simulated%20recruitment%20of%2010%2C000-participant%20cohorts%20from%0Amedical%20centers%20in%20the%20STAR%20Clinical%20Research%20Network%2C%20we%20show%20that%20our%0Aapproach%20yields%20a%20more%20representative%20cohort%20than%20existing%20baselines.%20Thus%2C%20we%0Ahighlight%20the%20value%20of%20computational%20modeling%20in%20guiding%20recruitment%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01375v1&entry.124074799=Read"},
{"title": "FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance", "author": "Shixun Wu and Yitong Ding and Yujia Zhai and Jinyang Liu and Jiajun Huang and Zizhe Jian and Huangliang Dai and Sheng Di and Bryan M. Wong and Zizhong Chen and Franck Cappello", "abstract": "  K-Means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-Means, a high-performance GPU-accelerated implementation of K-Means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-Means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-Means without fault tolerance outperforms cuML's\nK-Means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-Means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n", "link": "http://arxiv.org/abs/2408.01391v1", "date": "2024-08-02", "relevancy": 1.777, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4471}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4459}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FT%20K-Means%3A%20A%20High-Performance%20K-Means%20on%20GPU%20with%20Fault%20Tolerance&body=Title%3A%20FT%20K-Means%3A%20A%20High-Performance%20K-Means%20on%20GPU%20with%20Fault%20Tolerance%0AAuthor%3A%20Shixun%20Wu%20and%20Yitong%20Ding%20and%20Yujia%20Zhai%20and%20Jinyang%20Liu%20and%20Jiajun%20Huang%20and%20Zizhe%20Jian%20and%20Huangliang%20Dai%20and%20Sheng%20Di%20and%20Bryan%20M.%20Wong%20and%20Zizhong%20Chen%20and%20Franck%20Cappello%0AAbstract%3A%20%20%20K-Means%20is%20a%20widely%20used%20algorithm%20in%20clustering%2C%20however%2C%20its%20efficiency%20is%0Aprimarily%20constrained%20by%20the%20computational%20cost%20of%20distance%20computing.%20Existing%0Aimplementations%20suffer%20from%20suboptimal%20utilization%20of%20computational%20units%20and%0Alack%20resilience%20against%20soft%20errors.%20To%20address%20these%20challenges%2C%20we%20introduce%0AFT%20K-Means%2C%20a%20high-performance%20GPU-accelerated%20implementation%20of%20K-Means%20with%0Aonline%20fault%20tolerance.%20We%20first%20present%20a%20stepwise%20optimization%20strategy%20that%0Aachieves%20competitive%20performance%20compared%20to%20NVIDIA%27s%20cuML%20library.%20We%20further%0Aimprove%20FT%20K-Means%20with%20a%20template-based%20code%20generation%20framework%20that%0Asupports%20different%20data%20types%20and%20adapts%20to%20different%20input%20shapes.%20A%20novel%0Awarp-level%20tensor-core%20error%20correction%20scheme%20is%20proposed%20to%20address%20the%0Afailure%20of%20existing%20fault%20tolerance%20methods%20due%20to%20memory%20asynchronization%0Aduring%20copy%20operations.%20Our%20experimental%20evaluations%20on%20NVIDIA%20T4%20GPU%20and%20A100%0AGPU%20demonstrate%20that%20FT%20K-Means%20without%20fault%20tolerance%20outperforms%20cuML%27s%0AK-Means%20implementation%2C%20showing%20a%20performance%20increase%20of%2010%5C%25-300%5C%25%20in%0Ascenarios%20involving%20irregular%20data%20shapes.%20Moreover%2C%20the%20fault%20tolerance%0Afeature%20of%20FT%20K-Means%20introduces%20only%20an%20overhead%20of%2011%5C%25%2C%20maintaining%20robust%0Aperformance%20even%20with%20tens%20of%20errors%20injected%20per%20second.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFT%2520K-Means%253A%2520A%2520High-Performance%2520K-Means%2520on%2520GPU%2520with%2520Fault%2520Tolerance%26entry.906535625%3DShixun%2520Wu%2520and%2520Yitong%2520Ding%2520and%2520Yujia%2520Zhai%2520and%2520Jinyang%2520Liu%2520and%2520Jiajun%2520Huang%2520and%2520Zizhe%2520Jian%2520and%2520Huangliang%2520Dai%2520and%2520Sheng%2520Di%2520and%2520Bryan%2520M.%2520Wong%2520and%2520Zizhong%2520Chen%2520and%2520Franck%2520Cappello%26entry.1292438233%3D%2520%2520K-Means%2520is%2520a%2520widely%2520used%2520algorithm%2520in%2520clustering%252C%2520however%252C%2520its%2520efficiency%2520is%250Aprimarily%2520constrained%2520by%2520the%2520computational%2520cost%2520of%2520distance%2520computing.%2520Existing%250Aimplementations%2520suffer%2520from%2520suboptimal%2520utilization%2520of%2520computational%2520units%2520and%250Alack%2520resilience%2520against%2520soft%2520errors.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AFT%2520K-Means%252C%2520a%2520high-performance%2520GPU-accelerated%2520implementation%2520of%2520K-Means%2520with%250Aonline%2520fault%2520tolerance.%2520We%2520first%2520present%2520a%2520stepwise%2520optimization%2520strategy%2520that%250Aachieves%2520competitive%2520performance%2520compared%2520to%2520NVIDIA%2527s%2520cuML%2520library.%2520We%2520further%250Aimprove%2520FT%2520K-Means%2520with%2520a%2520template-based%2520code%2520generation%2520framework%2520that%250Asupports%2520different%2520data%2520types%2520and%2520adapts%2520to%2520different%2520input%2520shapes.%2520A%2520novel%250Awarp-level%2520tensor-core%2520error%2520correction%2520scheme%2520is%2520proposed%2520to%2520address%2520the%250Afailure%2520of%2520existing%2520fault%2520tolerance%2520methods%2520due%2520to%2520memory%2520asynchronization%250Aduring%2520copy%2520operations.%2520Our%2520experimental%2520evaluations%2520on%2520NVIDIA%2520T4%2520GPU%2520and%2520A100%250AGPU%2520demonstrate%2520that%2520FT%2520K-Means%2520without%2520fault%2520tolerance%2520outperforms%2520cuML%2527s%250AK-Means%2520implementation%252C%2520showing%2520a%2520performance%2520increase%2520of%252010%255C%2525-300%255C%2525%2520in%250Ascenarios%2520involving%2520irregular%2520data%2520shapes.%2520Moreover%252C%2520the%2520fault%2520tolerance%250Afeature%2520of%2520FT%2520K-Means%2520introduces%2520only%2520an%2520overhead%2520of%252011%255C%2525%252C%2520maintaining%2520robust%250Aperformance%2520even%2520with%2520tens%2520of%2520errors%2520injected%2520per%2520second.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FT%20K-Means%3A%20A%20High-Performance%20K-Means%20on%20GPU%20with%20Fault%20Tolerance&entry.906535625=Shixun%20Wu%20and%20Yitong%20Ding%20and%20Yujia%20Zhai%20and%20Jinyang%20Liu%20and%20Jiajun%20Huang%20and%20Zizhe%20Jian%20and%20Huangliang%20Dai%20and%20Sheng%20Di%20and%20Bryan%20M.%20Wong%20and%20Zizhong%20Chen%20and%20Franck%20Cappello&entry.1292438233=%20%20K-Means%20is%20a%20widely%20used%20algorithm%20in%20clustering%2C%20however%2C%20its%20efficiency%20is%0Aprimarily%20constrained%20by%20the%20computational%20cost%20of%20distance%20computing.%20Existing%0Aimplementations%20suffer%20from%20suboptimal%20utilization%20of%20computational%20units%20and%0Alack%20resilience%20against%20soft%20errors.%20To%20address%20these%20challenges%2C%20we%20introduce%0AFT%20K-Means%2C%20a%20high-performance%20GPU-accelerated%20implementation%20of%20K-Means%20with%0Aonline%20fault%20tolerance.%20We%20first%20present%20a%20stepwise%20optimization%20strategy%20that%0Aachieves%20competitive%20performance%20compared%20to%20NVIDIA%27s%20cuML%20library.%20We%20further%0Aimprove%20FT%20K-Means%20with%20a%20template-based%20code%20generation%20framework%20that%0Asupports%20different%20data%20types%20and%20adapts%20to%20different%20input%20shapes.%20A%20novel%0Awarp-level%20tensor-core%20error%20correction%20scheme%20is%20proposed%20to%20address%20the%0Afailure%20of%20existing%20fault%20tolerance%20methods%20due%20to%20memory%20asynchronization%0Aduring%20copy%20operations.%20Our%20experimental%20evaluations%20on%20NVIDIA%20T4%20GPU%20and%20A100%0AGPU%20demonstrate%20that%20FT%20K-Means%20without%20fault%20tolerance%20outperforms%20cuML%27s%0AK-Means%20implementation%2C%20showing%20a%20performance%20increase%20of%2010%5C%25-300%5C%25%20in%0Ascenarios%20involving%20irregular%20data%20shapes.%20Moreover%2C%20the%20fault%20tolerance%0Afeature%20of%20FT%20K-Means%20introduces%20only%20an%20overhead%20of%2011%5C%25%2C%20maintaining%20robust%0Aperformance%20even%20with%20tens%20of%20errors%20injected%20per%20second.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01391v1&entry.124074799=Read"},
{"title": "Efficient Test Data Generation for MC/DC with OCL and Search", "author": "Hassan Sartaj and Muhammad Zohaib Iqbal and Atif Aftab Ahmed Jilani and Muhammad Uzair Khan", "abstract": "  System-level testing of avionics software systems requires compliance with\ndifferent international safety standards such as DO-178C. An important\nconsideration of the avionics industry is automated test data generation\naccording to the criteria suggested by safety standards. One of the recommended\ncriteria by DO-178C is the modified condition/decision coverage (MC/DC)\ncriterion. The current model-based test data generation approaches use\nconstraints written in Object Constraint Language (OCL), and apply search\ntechniques to generate test data. These approaches either do not support MC/DC\ncriterion or suffer from performance issues while generating test data for\nlarge-scale avionics systems. In this paper, we propose an effective way to\nautomate MC/DC test data generation during model-based testing. We develop a\nstrategy that utilizes case-based reasoning (CBR) and range reduction\nheuristics designed to solve MC/DC-tailored OCL constraints. We performed an\nempirical study to compare our proposed strategy for MC/DC test data generation\nusing CBR, range reduction, both CBR and range reduction, with an original\nsearch algorithm, and random search. We also empirically compared our strategy\nwith existing constraint-solving approaches. The results show that both CBR and\nrange reduction for MC/DC test data generation outperform the baseline\napproach. Moreover, the combination of both CBR and range reduction for MC/DC\ntest data generation is an effective approach compared to existing constraint\nsolvers.\n", "link": "http://arxiv.org/abs/2401.03469v3", "date": "2024-08-02", "relevancy": 1.763, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4876}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4359}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Test%20Data%20Generation%20for%20MC/DC%20with%20OCL%20and%20Search&body=Title%3A%20Efficient%20Test%20Data%20Generation%20for%20MC/DC%20with%20OCL%20and%20Search%0AAuthor%3A%20Hassan%20Sartaj%20and%20Muhammad%20Zohaib%20Iqbal%20and%20Atif%20Aftab%20Ahmed%20Jilani%20and%20Muhammad%20Uzair%20Khan%0AAbstract%3A%20%20%20System-level%20testing%20of%20avionics%20software%20systems%20requires%20compliance%20with%0Adifferent%20international%20safety%20standards%20such%20as%20DO-178C.%20An%20important%0Aconsideration%20of%20the%20avionics%20industry%20is%20automated%20test%20data%20generation%0Aaccording%20to%20the%20criteria%20suggested%20by%20safety%20standards.%20One%20of%20the%20recommended%0Acriteria%20by%20DO-178C%20is%20the%20modified%20condition/decision%20coverage%20%28MC/DC%29%0Acriterion.%20The%20current%20model-based%20test%20data%20generation%20approaches%20use%0Aconstraints%20written%20in%20Object%20Constraint%20Language%20%28OCL%29%2C%20and%20apply%20search%0Atechniques%20to%20generate%20test%20data.%20These%20approaches%20either%20do%20not%20support%20MC/DC%0Acriterion%20or%20suffer%20from%20performance%20issues%20while%20generating%20test%20data%20for%0Alarge-scale%20avionics%20systems.%20In%20this%20paper%2C%20we%20propose%20an%20effective%20way%20to%0Aautomate%20MC/DC%20test%20data%20generation%20during%20model-based%20testing.%20We%20develop%20a%0Astrategy%20that%20utilizes%20case-based%20reasoning%20%28CBR%29%20and%20range%20reduction%0Aheuristics%20designed%20to%20solve%20MC/DC-tailored%20OCL%20constraints.%20We%20performed%20an%0Aempirical%20study%20to%20compare%20our%20proposed%20strategy%20for%20MC/DC%20test%20data%20generation%0Ausing%20CBR%2C%20range%20reduction%2C%20both%20CBR%20and%20range%20reduction%2C%20with%20an%20original%0Asearch%20algorithm%2C%20and%20random%20search.%20We%20also%20empirically%20compared%20our%20strategy%0Awith%20existing%20constraint-solving%20approaches.%20The%20results%20show%20that%20both%20CBR%20and%0Arange%20reduction%20for%20MC/DC%20test%20data%20generation%20outperform%20the%20baseline%0Aapproach.%20Moreover%2C%20the%20combination%20of%20both%20CBR%20and%20range%20reduction%20for%20MC/DC%0Atest%20data%20generation%20is%20an%20effective%20approach%20compared%20to%20existing%20constraint%0Asolvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03469v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Test%2520Data%2520Generation%2520for%2520MC/DC%2520with%2520OCL%2520and%2520Search%26entry.906535625%3DHassan%2520Sartaj%2520and%2520Muhammad%2520Zohaib%2520Iqbal%2520and%2520Atif%2520Aftab%2520Ahmed%2520Jilani%2520and%2520Muhammad%2520Uzair%2520Khan%26entry.1292438233%3D%2520%2520System-level%2520testing%2520of%2520avionics%2520software%2520systems%2520requires%2520compliance%2520with%250Adifferent%2520international%2520safety%2520standards%2520such%2520as%2520DO-178C.%2520An%2520important%250Aconsideration%2520of%2520the%2520avionics%2520industry%2520is%2520automated%2520test%2520data%2520generation%250Aaccording%2520to%2520the%2520criteria%2520suggested%2520by%2520safety%2520standards.%2520One%2520of%2520the%2520recommended%250Acriteria%2520by%2520DO-178C%2520is%2520the%2520modified%2520condition/decision%2520coverage%2520%2528MC/DC%2529%250Acriterion.%2520The%2520current%2520model-based%2520test%2520data%2520generation%2520approaches%2520use%250Aconstraints%2520written%2520in%2520Object%2520Constraint%2520Language%2520%2528OCL%2529%252C%2520and%2520apply%2520search%250Atechniques%2520to%2520generate%2520test%2520data.%2520These%2520approaches%2520either%2520do%2520not%2520support%2520MC/DC%250Acriterion%2520or%2520suffer%2520from%2520performance%2520issues%2520while%2520generating%2520test%2520data%2520for%250Alarge-scale%2520avionics%2520systems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520effective%2520way%2520to%250Aautomate%2520MC/DC%2520test%2520data%2520generation%2520during%2520model-based%2520testing.%2520We%2520develop%2520a%250Astrategy%2520that%2520utilizes%2520case-based%2520reasoning%2520%2528CBR%2529%2520and%2520range%2520reduction%250Aheuristics%2520designed%2520to%2520solve%2520MC/DC-tailored%2520OCL%2520constraints.%2520We%2520performed%2520an%250Aempirical%2520study%2520to%2520compare%2520our%2520proposed%2520strategy%2520for%2520MC/DC%2520test%2520data%2520generation%250Ausing%2520CBR%252C%2520range%2520reduction%252C%2520both%2520CBR%2520and%2520range%2520reduction%252C%2520with%2520an%2520original%250Asearch%2520algorithm%252C%2520and%2520random%2520search.%2520We%2520also%2520empirically%2520compared%2520our%2520strategy%250Awith%2520existing%2520constraint-solving%2520approaches.%2520The%2520results%2520show%2520that%2520both%2520CBR%2520and%250Arange%2520reduction%2520for%2520MC/DC%2520test%2520data%2520generation%2520outperform%2520the%2520baseline%250Aapproach.%2520Moreover%252C%2520the%2520combination%2520of%2520both%2520CBR%2520and%2520range%2520reduction%2520for%2520MC/DC%250Atest%2520data%2520generation%2520is%2520an%2520effective%2520approach%2520compared%2520to%2520existing%2520constraint%250Asolvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03469v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Test%20Data%20Generation%20for%20MC/DC%20with%20OCL%20and%20Search&entry.906535625=Hassan%20Sartaj%20and%20Muhammad%20Zohaib%20Iqbal%20and%20Atif%20Aftab%20Ahmed%20Jilani%20and%20Muhammad%20Uzair%20Khan&entry.1292438233=%20%20System-level%20testing%20of%20avionics%20software%20systems%20requires%20compliance%20with%0Adifferent%20international%20safety%20standards%20such%20as%20DO-178C.%20An%20important%0Aconsideration%20of%20the%20avionics%20industry%20is%20automated%20test%20data%20generation%0Aaccording%20to%20the%20criteria%20suggested%20by%20safety%20standards.%20One%20of%20the%20recommended%0Acriteria%20by%20DO-178C%20is%20the%20modified%20condition/decision%20coverage%20%28MC/DC%29%0Acriterion.%20The%20current%20model-based%20test%20data%20generation%20approaches%20use%0Aconstraints%20written%20in%20Object%20Constraint%20Language%20%28OCL%29%2C%20and%20apply%20search%0Atechniques%20to%20generate%20test%20data.%20These%20approaches%20either%20do%20not%20support%20MC/DC%0Acriterion%20or%20suffer%20from%20performance%20issues%20while%20generating%20test%20data%20for%0Alarge-scale%20avionics%20systems.%20In%20this%20paper%2C%20we%20propose%20an%20effective%20way%20to%0Aautomate%20MC/DC%20test%20data%20generation%20during%20model-based%20testing.%20We%20develop%20a%0Astrategy%20that%20utilizes%20case-based%20reasoning%20%28CBR%29%20and%20range%20reduction%0Aheuristics%20designed%20to%20solve%20MC/DC-tailored%20OCL%20constraints.%20We%20performed%20an%0Aempirical%20study%20to%20compare%20our%20proposed%20strategy%20for%20MC/DC%20test%20data%20generation%0Ausing%20CBR%2C%20range%20reduction%2C%20both%20CBR%20and%20range%20reduction%2C%20with%20an%20original%0Asearch%20algorithm%2C%20and%20random%20search.%20We%20also%20empirically%20compared%20our%20strategy%0Awith%20existing%20constraint-solving%20approaches.%20The%20results%20show%20that%20both%20CBR%20and%0Arange%20reduction%20for%20MC/DC%20test%20data%20generation%20outperform%20the%20baseline%0Aapproach.%20Moreover%2C%20the%20combination%20of%20both%20CBR%20and%20range%20reduction%20for%20MC/DC%0Atest%20data%20generation%20is%20an%20effective%20approach%20compared%20to%20existing%20constraint%0Asolvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03469v3&entry.124074799=Read"},
{"title": "A Reparameterized Discrete Diffusion Model for Text Generation", "author": "Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong", "abstract": "  This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n", "link": "http://arxiv.org/abs/2302.05737v3", "date": "2024-08-02", "relevancy": 1.7595, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6315}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5907}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Reparameterized%20Discrete%20Diffusion%20Model%20for%20Text%20Generation&body=Title%3A%20A%20Reparameterized%20Discrete%20Diffusion%20Model%20for%20Text%20Generation%0AAuthor%3A%20Lin%20Zheng%20and%20Jianbo%20Yuan%20and%20Lei%20Yu%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20This%20work%20studies%20discrete%20diffusion%20probabilistic%20models%20with%20applications%0Ato%20natural%20language%20generation.%20We%20derive%20an%20alternative%20yet%20equivalent%0Aformulation%20of%20the%20sampling%20from%20discrete%20diffusion%20processes%20and%20leverage%20this%0Ainsight%20to%20develop%20a%20family%20of%20reparameterized%20discrete%20diffusion%20models.%20The%0Aderived%20generic%20framework%20is%20highly%20flexible%2C%20offers%20a%20fresh%20perspective%20of%20the%0Ageneration%20process%20in%20discrete%20diffusion%20models%2C%20and%20features%20more%20effective%0Atraining%20and%20decoding%20techniques.%20We%20conduct%20extensive%20experiments%20to%20evaluate%0Athe%20text%20generation%20capability%20of%20our%20model%2C%20demonstrating%20significant%0Aimprovements%20over%20existing%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05737v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Reparameterized%2520Discrete%2520Diffusion%2520Model%2520for%2520Text%2520Generation%26entry.906535625%3DLin%2520Zheng%2520and%2520Jianbo%2520Yuan%2520and%2520Lei%2520Yu%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520This%2520work%2520studies%2520discrete%2520diffusion%2520probabilistic%2520models%2520with%2520applications%250Ato%2520natural%2520language%2520generation.%2520We%2520derive%2520an%2520alternative%2520yet%2520equivalent%250Aformulation%2520of%2520the%2520sampling%2520from%2520discrete%2520diffusion%2520processes%2520and%2520leverage%2520this%250Ainsight%2520to%2520develop%2520a%2520family%2520of%2520reparameterized%2520discrete%2520diffusion%2520models.%2520The%250Aderived%2520generic%2520framework%2520is%2520highly%2520flexible%252C%2520offers%2520a%2520fresh%2520perspective%2520of%2520the%250Ageneration%2520process%2520in%2520discrete%2520diffusion%2520models%252C%2520and%2520features%2520more%2520effective%250Atraining%2520and%2520decoding%2520techniques.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520evaluate%250Athe%2520text%2520generation%2520capability%2520of%2520our%2520model%252C%2520demonstrating%2520significant%250Aimprovements%2520over%2520existing%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05737v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Reparameterized%20Discrete%20Diffusion%20Model%20for%20Text%20Generation&entry.906535625=Lin%20Zheng%20and%20Jianbo%20Yuan%20and%20Lei%20Yu%20and%20Lingpeng%20Kong&entry.1292438233=%20%20This%20work%20studies%20discrete%20diffusion%20probabilistic%20models%20with%20applications%0Ato%20natural%20language%20generation.%20We%20derive%20an%20alternative%20yet%20equivalent%0Aformulation%20of%20the%20sampling%20from%20discrete%20diffusion%20processes%20and%20leverage%20this%0Ainsight%20to%20develop%20a%20family%20of%20reparameterized%20discrete%20diffusion%20models.%20The%0Aderived%20generic%20framework%20is%20highly%20flexible%2C%20offers%20a%20fresh%20perspective%20of%20the%0Ageneration%20process%20in%20discrete%20diffusion%20models%2C%20and%20features%20more%20effective%0Atraining%20and%20decoding%20techniques.%20We%20conduct%20extensive%20experiments%20to%20evaluate%0Athe%20text%20generation%20capability%20of%20our%20model%2C%20demonstrating%20significant%0Aimprovements%20over%20existing%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05737v3&entry.124074799=Read"},
{"title": "Real-time gravitational-wave inference for binary neutron stars using\n  machine learning", "author": "Maximilian Dax and Stephen R. Green and Jonathan Gair and Nihar Gupte and Michael P\u00fcrrer and Vivien Raymond and Jonas Wildberger and Jakob H. Macke and Alessandra Buonanno and Bernhard Sch\u00f6lkopf", "abstract": "  Mergers of binary neutron stars (BNSs) emit signals in both the\ngravitational-wave (GW) and electromagnetic (EM) spectra. Famously, the 2017\nmulti-messenger observation of GW170817 led to scientific discoveries across\ncosmology, nuclear physics, and gravity. Central to these results were the sky\nlocalization and distance obtained from GW data, which, in the case of\nGW170817, helped to identify the associated EM transient, AT 2017gfo, 11 hours\nafter the GW signal. Fast analysis of GW data is critical for directing\ntime-sensitive EM observations; however, due to challenges arising from the\nlength and complexity of signals, it is often necessary to make approximations\nthat sacrifice accuracy. Here, we present a machine learning framework that\nperforms complete BNS inference in just one second without making any such\napproximations. Our approach enhances multi-messenger observations by providing\n(i) accurate localization even before the merger; (ii) improved localization\nprecision by $\\sim30\\%$ compared to approximate low-latency methods; and (iii)\ndetailed information on luminosity distance, inclination, and masses, which can\nbe used to prioritize expensive telescope time. Additionally, the flexibility\nand reduced cost of our method open new opportunities for equation-of-state\nstudies. Finally, we demonstrate that our method scales to extremely long\nsignals, up to an hour in length, thus serving as a blueprint for data analysis\nfor next-generation ground- and space-based detectors.\n", "link": "http://arxiv.org/abs/2407.09602v2", "date": "2024-08-02", "relevancy": 1.7573, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4472}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4415}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20gravitational-wave%20inference%20for%20binary%20neutron%20stars%20using%0A%20%20machine%20learning&body=Title%3A%20Real-time%20gravitational-wave%20inference%20for%20binary%20neutron%20stars%20using%0A%20%20machine%20learning%0AAuthor%3A%20Maximilian%20Dax%20and%20Stephen%20R.%20Green%20and%20Jonathan%20Gair%20and%20Nihar%20Gupte%20and%20Michael%20P%C3%BCrrer%20and%20Vivien%20Raymond%20and%20Jonas%20Wildberger%20and%20Jakob%20H.%20Macke%20and%20Alessandra%20Buonanno%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Mergers%20of%20binary%20neutron%20stars%20%28BNSs%29%20emit%20signals%20in%20both%20the%0Agravitational-wave%20%28GW%29%20and%20electromagnetic%20%28EM%29%20spectra.%20Famously%2C%20the%202017%0Amulti-messenger%20observation%20of%20GW170817%20led%20to%20scientific%20discoveries%20across%0Acosmology%2C%20nuclear%20physics%2C%20and%20gravity.%20Central%20to%20these%20results%20were%20the%20sky%0Alocalization%20and%20distance%20obtained%20from%20GW%20data%2C%20which%2C%20in%20the%20case%20of%0AGW170817%2C%20helped%20to%20identify%20the%20associated%20EM%20transient%2C%20AT%202017gfo%2C%2011%20hours%0Aafter%20the%20GW%20signal.%20Fast%20analysis%20of%20GW%20data%20is%20critical%20for%20directing%0Atime-sensitive%20EM%20observations%3B%20however%2C%20due%20to%20challenges%20arising%20from%20the%0Alength%20and%20complexity%20of%20signals%2C%20it%20is%20often%20necessary%20to%20make%20approximations%0Athat%20sacrifice%20accuracy.%20Here%2C%20we%20present%20a%20machine%20learning%20framework%20that%0Aperforms%20complete%20BNS%20inference%20in%20just%20one%20second%20without%20making%20any%20such%0Aapproximations.%20Our%20approach%20enhances%20multi-messenger%20observations%20by%20providing%0A%28i%29%20accurate%20localization%20even%20before%20the%20merger%3B%20%28ii%29%20improved%20localization%0Aprecision%20by%20%24%5Csim30%5C%25%24%20compared%20to%20approximate%20low-latency%20methods%3B%20and%20%28iii%29%0Adetailed%20information%20on%20luminosity%20distance%2C%20inclination%2C%20and%20masses%2C%20which%20can%0Abe%20used%20to%20prioritize%20expensive%20telescope%20time.%20Additionally%2C%20the%20flexibility%0Aand%20reduced%20cost%20of%20our%20method%20open%20new%20opportunities%20for%20equation-of-state%0Astudies.%20Finally%2C%20we%20demonstrate%20that%20our%20method%20scales%20to%20extremely%20long%0Asignals%2C%20up%20to%20an%20hour%20in%20length%2C%20thus%20serving%20as%20a%20blueprint%20for%20data%20analysis%0Afor%20next-generation%20ground-%20and%20space-based%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520gravitational-wave%2520inference%2520for%2520binary%2520neutron%2520stars%2520using%250A%2520%2520machine%2520learning%26entry.906535625%3DMaximilian%2520Dax%2520and%2520Stephen%2520R.%2520Green%2520and%2520Jonathan%2520Gair%2520and%2520Nihar%2520Gupte%2520and%2520Michael%2520P%25C3%25BCrrer%2520and%2520Vivien%2520Raymond%2520and%2520Jonas%2520Wildberger%2520and%2520Jakob%2520H.%2520Macke%2520and%2520Alessandra%2520Buonanno%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520Mergers%2520of%2520binary%2520neutron%2520stars%2520%2528BNSs%2529%2520emit%2520signals%2520in%2520both%2520the%250Agravitational-wave%2520%2528GW%2529%2520and%2520electromagnetic%2520%2528EM%2529%2520spectra.%2520Famously%252C%2520the%25202017%250Amulti-messenger%2520observation%2520of%2520GW170817%2520led%2520to%2520scientific%2520discoveries%2520across%250Acosmology%252C%2520nuclear%2520physics%252C%2520and%2520gravity.%2520Central%2520to%2520these%2520results%2520were%2520the%2520sky%250Alocalization%2520and%2520distance%2520obtained%2520from%2520GW%2520data%252C%2520which%252C%2520in%2520the%2520case%2520of%250AGW170817%252C%2520helped%2520to%2520identify%2520the%2520associated%2520EM%2520transient%252C%2520AT%25202017gfo%252C%252011%2520hours%250Aafter%2520the%2520GW%2520signal.%2520Fast%2520analysis%2520of%2520GW%2520data%2520is%2520critical%2520for%2520directing%250Atime-sensitive%2520EM%2520observations%253B%2520however%252C%2520due%2520to%2520challenges%2520arising%2520from%2520the%250Alength%2520and%2520complexity%2520of%2520signals%252C%2520it%2520is%2520often%2520necessary%2520to%2520make%2520approximations%250Athat%2520sacrifice%2520accuracy.%2520Here%252C%2520we%2520present%2520a%2520machine%2520learning%2520framework%2520that%250Aperforms%2520complete%2520BNS%2520inference%2520in%2520just%2520one%2520second%2520without%2520making%2520any%2520such%250Aapproximations.%2520Our%2520approach%2520enhances%2520multi-messenger%2520observations%2520by%2520providing%250A%2528i%2529%2520accurate%2520localization%2520even%2520before%2520the%2520merger%253B%2520%2528ii%2529%2520improved%2520localization%250Aprecision%2520by%2520%2524%255Csim30%255C%2525%2524%2520compared%2520to%2520approximate%2520low-latency%2520methods%253B%2520and%2520%2528iii%2529%250Adetailed%2520information%2520on%2520luminosity%2520distance%252C%2520inclination%252C%2520and%2520masses%252C%2520which%2520can%250Abe%2520used%2520to%2520prioritize%2520expensive%2520telescope%2520time.%2520Additionally%252C%2520the%2520flexibility%250Aand%2520reduced%2520cost%2520of%2520our%2520method%2520open%2520new%2520opportunities%2520for%2520equation-of-state%250Astudies.%2520Finally%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520scales%2520to%2520extremely%2520long%250Asignals%252C%2520up%2520to%2520an%2520hour%2520in%2520length%252C%2520thus%2520serving%2520as%2520a%2520blueprint%2520for%2520data%2520analysis%250Afor%2520next-generation%2520ground-%2520and%2520space-based%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20gravitational-wave%20inference%20for%20binary%20neutron%20stars%20using%0A%20%20machine%20learning&entry.906535625=Maximilian%20Dax%20and%20Stephen%20R.%20Green%20and%20Jonathan%20Gair%20and%20Nihar%20Gupte%20and%20Michael%20P%C3%BCrrer%20and%20Vivien%20Raymond%20and%20Jonas%20Wildberger%20and%20Jakob%20H.%20Macke%20and%20Alessandra%20Buonanno%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Mergers%20of%20binary%20neutron%20stars%20%28BNSs%29%20emit%20signals%20in%20both%20the%0Agravitational-wave%20%28GW%29%20and%20electromagnetic%20%28EM%29%20spectra.%20Famously%2C%20the%202017%0Amulti-messenger%20observation%20of%20GW170817%20led%20to%20scientific%20discoveries%20across%0Acosmology%2C%20nuclear%20physics%2C%20and%20gravity.%20Central%20to%20these%20results%20were%20the%20sky%0Alocalization%20and%20distance%20obtained%20from%20GW%20data%2C%20which%2C%20in%20the%20case%20of%0AGW170817%2C%20helped%20to%20identify%20the%20associated%20EM%20transient%2C%20AT%202017gfo%2C%2011%20hours%0Aafter%20the%20GW%20signal.%20Fast%20analysis%20of%20GW%20data%20is%20critical%20for%20directing%0Atime-sensitive%20EM%20observations%3B%20however%2C%20due%20to%20challenges%20arising%20from%20the%0Alength%20and%20complexity%20of%20signals%2C%20it%20is%20often%20necessary%20to%20make%20approximations%0Athat%20sacrifice%20accuracy.%20Here%2C%20we%20present%20a%20machine%20learning%20framework%20that%0Aperforms%20complete%20BNS%20inference%20in%20just%20one%20second%20without%20making%20any%20such%0Aapproximations.%20Our%20approach%20enhances%20multi-messenger%20observations%20by%20providing%0A%28i%29%20accurate%20localization%20even%20before%20the%20merger%3B%20%28ii%29%20improved%20localization%0Aprecision%20by%20%24%5Csim30%5C%25%24%20compared%20to%20approximate%20low-latency%20methods%3B%20and%20%28iii%29%0Adetailed%20information%20on%20luminosity%20distance%2C%20inclination%2C%20and%20masses%2C%20which%20can%0Abe%20used%20to%20prioritize%20expensive%20telescope%20time.%20Additionally%2C%20the%20flexibility%0Aand%20reduced%20cost%20of%20our%20method%20open%20new%20opportunities%20for%20equation-of-state%0Astudies.%20Finally%2C%20we%20demonstrate%20that%20our%20method%20scales%20to%20extremely%20long%0Asignals%2C%20up%20to%20an%20hour%20in%20length%2C%20thus%20serving%20as%20a%20blueprint%20for%20data%20analysis%0Afor%20next-generation%20ground-%20and%20space-based%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09602v2&entry.124074799=Read"},
{"title": "Jacta: A Versatile Planner for Learning Dexterous and Whole-body\n  Manipulation", "author": "Jan Br\u00fcdigam and Ali-Adeeb Abbas and Maks Sorokin and Kuan Fang and Brandon Hung and Maya Guru and Stefan Sosnowski and Jiuguang Wang and Sandra Hirche and Simon Le Cleac'h", "abstract": "  Robotic manipulation is challenging due to discontinuous dynamics, as well as\nhigh-dimensional state and action spaces. Data-driven approaches that succeed\nin manipulation tasks require large amounts of data and expert demonstrations,\ntypically from humans. Existing manipulation planners are restricted to\nspecific systems and often depend on specialized algorithms for using\ndemonstration. Therefore, we introduce a flexible motion planner tailored to\ndexterous and whole-body manipulation tasks. Our planner creates readily usable\ndemonstrations for reinforcement learning algorithms, eliminating the need for\nadditional training pipeline complexities. With this approach, we can\nefficiently learn policies for complex manipulation tasks, where traditional\nreinforcement learning alone only makes little progress. Furthermore, we\ndemonstrate that learned policies are transferable to real robotic systems for\nsolving complex dexterous manipulation tasks.\n", "link": "http://arxiv.org/abs/2408.01258v1", "date": "2024-08-02", "relevancy": 1.7554, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6631}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jacta%3A%20A%20Versatile%20Planner%20for%20Learning%20Dexterous%20and%20Whole-body%0A%20%20Manipulation&body=Title%3A%20Jacta%3A%20A%20Versatile%20Planner%20for%20Learning%20Dexterous%20and%20Whole-body%0A%20%20Manipulation%0AAuthor%3A%20Jan%20Br%C3%BCdigam%20and%20Ali-Adeeb%20Abbas%20and%20Maks%20Sorokin%20and%20Kuan%20Fang%20and%20Brandon%20Hung%20and%20Maya%20Guru%20and%20Stefan%20Sosnowski%20and%20Jiuguang%20Wang%20and%20Sandra%20Hirche%20and%20Simon%20Le%20Cleac%27h%0AAbstract%3A%20%20%20Robotic%20manipulation%20is%20challenging%20due%20to%20discontinuous%20dynamics%2C%20as%20well%20as%0Ahigh-dimensional%20state%20and%20action%20spaces.%20Data-driven%20approaches%20that%20succeed%0Ain%20manipulation%20tasks%20require%20large%20amounts%20of%20data%20and%20expert%20demonstrations%2C%0Atypically%20from%20humans.%20Existing%20manipulation%20planners%20are%20restricted%20to%0Aspecific%20systems%20and%20often%20depend%20on%20specialized%20algorithms%20for%20using%0Ademonstration.%20Therefore%2C%20we%20introduce%20a%20flexible%20motion%20planner%20tailored%20to%0Adexterous%20and%20whole-body%20manipulation%20tasks.%20Our%20planner%20creates%20readily%20usable%0Ademonstrations%20for%20reinforcement%20learning%20algorithms%2C%20eliminating%20the%20need%20for%0Aadditional%20training%20pipeline%20complexities.%20With%20this%20approach%2C%20we%20can%0Aefficiently%20learn%20policies%20for%20complex%20manipulation%20tasks%2C%20where%20traditional%0Areinforcement%20learning%20alone%20only%20makes%20little%20progress.%20Furthermore%2C%20we%0Ademonstrate%20that%20learned%20policies%20are%20transferable%20to%20real%20robotic%20systems%20for%0Asolving%20complex%20dexterous%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJacta%253A%2520A%2520Versatile%2520Planner%2520for%2520Learning%2520Dexterous%2520and%2520Whole-body%250A%2520%2520Manipulation%26entry.906535625%3DJan%2520Br%25C3%25BCdigam%2520and%2520Ali-Adeeb%2520Abbas%2520and%2520Maks%2520Sorokin%2520and%2520Kuan%2520Fang%2520and%2520Brandon%2520Hung%2520and%2520Maya%2520Guru%2520and%2520Stefan%2520Sosnowski%2520and%2520Jiuguang%2520Wang%2520and%2520Sandra%2520Hirche%2520and%2520Simon%2520Le%2520Cleac%2527h%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520is%2520challenging%2520due%2520to%2520discontinuous%2520dynamics%252C%2520as%2520well%2520as%250Ahigh-dimensional%2520state%2520and%2520action%2520spaces.%2520Data-driven%2520approaches%2520that%2520succeed%250Ain%2520manipulation%2520tasks%2520require%2520large%2520amounts%2520of%2520data%2520and%2520expert%2520demonstrations%252C%250Atypically%2520from%2520humans.%2520Existing%2520manipulation%2520planners%2520are%2520restricted%2520to%250Aspecific%2520systems%2520and%2520often%2520depend%2520on%2520specialized%2520algorithms%2520for%2520using%250Ademonstration.%2520Therefore%252C%2520we%2520introduce%2520a%2520flexible%2520motion%2520planner%2520tailored%2520to%250Adexterous%2520and%2520whole-body%2520manipulation%2520tasks.%2520Our%2520planner%2520creates%2520readily%2520usable%250Ademonstrations%2520for%2520reinforcement%2520learning%2520algorithms%252C%2520eliminating%2520the%2520need%2520for%250Aadditional%2520training%2520pipeline%2520complexities.%2520With%2520this%2520approach%252C%2520we%2520can%250Aefficiently%2520learn%2520policies%2520for%2520complex%2520manipulation%2520tasks%252C%2520where%2520traditional%250Areinforcement%2520learning%2520alone%2520only%2520makes%2520little%2520progress.%2520Furthermore%252C%2520we%250Ademonstrate%2520that%2520learned%2520policies%2520are%2520transferable%2520to%2520real%2520robotic%2520systems%2520for%250Asolving%2520complex%2520dexterous%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jacta%3A%20A%20Versatile%20Planner%20for%20Learning%20Dexterous%20and%20Whole-body%0A%20%20Manipulation&entry.906535625=Jan%20Br%C3%BCdigam%20and%20Ali-Adeeb%20Abbas%20and%20Maks%20Sorokin%20and%20Kuan%20Fang%20and%20Brandon%20Hung%20and%20Maya%20Guru%20and%20Stefan%20Sosnowski%20and%20Jiuguang%20Wang%20and%20Sandra%20Hirche%20and%20Simon%20Le%20Cleac%27h&entry.1292438233=%20%20Robotic%20manipulation%20is%20challenging%20due%20to%20discontinuous%20dynamics%2C%20as%20well%20as%0Ahigh-dimensional%20state%20and%20action%20spaces.%20Data-driven%20approaches%20that%20succeed%0Ain%20manipulation%20tasks%20require%20large%20amounts%20of%20data%20and%20expert%20demonstrations%2C%0Atypically%20from%20humans.%20Existing%20manipulation%20planners%20are%20restricted%20to%0Aspecific%20systems%20and%20often%20depend%20on%20specialized%20algorithms%20for%20using%0Ademonstration.%20Therefore%2C%20we%20introduce%20a%20flexible%20motion%20planner%20tailored%20to%0Adexterous%20and%20whole-body%20manipulation%20tasks.%20Our%20planner%20creates%20readily%20usable%0Ademonstrations%20for%20reinforcement%20learning%20algorithms%2C%20eliminating%20the%20need%20for%0Aadditional%20training%20pipeline%20complexities.%20With%20this%20approach%2C%20we%20can%0Aefficiently%20learn%20policies%20for%20complex%20manipulation%20tasks%2C%20where%20traditional%0Areinforcement%20learning%20alone%20only%20makes%20little%20progress.%20Furthermore%2C%20we%0Ademonstrate%20that%20learned%20policies%20are%20transferable%20to%20real%20robotic%20systems%20for%0Asolving%20complex%20dexterous%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01258v1&entry.124074799=Read"},
{"title": "HeteroMorpheus: Universal Control Based on Morphological Heterogeneity\n  Modeling", "author": "YiFan Hao and Yang Yang and Junru Song and Wei Peng and Weien Zhou and Tingsong Jiang and Wen Yao", "abstract": "  In the field of robotic control, designing individual controllers for each\nrobot leads to high computational costs. Universal control policies, applicable\nacross diverse robot morphologies, promise to mitigate this challenge.\nPredominantly, models based on Graph Neural Networks (GNN) and Transformers are\nemployed, owing to their effectiveness in capturing relational dynamics across\na robot's limbs. However, these models typically employ homogeneous graph\nstructures that overlook the functional diversity of different limbs. To bridge\nthis gap, we introduce HeteroMorpheus, a novel method based on heterogeneous\ngraph Transformer. This method uniquely addresses limb heterogeneity, fostering\nbetter representation of robot dynamics of various morphologies. Through\nextensive experiments we demonstrate the superiority of HeteroMorpheus against\nstate-of-the-art methods in the capability of policy generalization, including\nzero-shot generalization and sample-efficient transfer to unfamiliar robot\nmorphologies.\n", "link": "http://arxiv.org/abs/2408.01230v1", "date": "2024-08-02", "relevancy": 1.7549, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5813}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeteroMorpheus%3A%20Universal%20Control%20Based%20on%20Morphological%20Heterogeneity%0A%20%20Modeling&body=Title%3A%20HeteroMorpheus%3A%20Universal%20Control%20Based%20on%20Morphological%20Heterogeneity%0A%20%20Modeling%0AAuthor%3A%20YiFan%20Hao%20and%20Yang%20Yang%20and%20Junru%20Song%20and%20Wei%20Peng%20and%20Weien%20Zhou%20and%20Tingsong%20Jiang%20and%20Wen%20Yao%0AAbstract%3A%20%20%20In%20the%20field%20of%20robotic%20control%2C%20designing%20individual%20controllers%20for%20each%0Arobot%20leads%20to%20high%20computational%20costs.%20Universal%20control%20policies%2C%20applicable%0Aacross%20diverse%20robot%20morphologies%2C%20promise%20to%20mitigate%20this%20challenge.%0APredominantly%2C%20models%20based%20on%20Graph%20Neural%20Networks%20%28GNN%29%20and%20Transformers%20are%0Aemployed%2C%20owing%20to%20their%20effectiveness%20in%20capturing%20relational%20dynamics%20across%0Aa%20robot%27s%20limbs.%20However%2C%20these%20models%20typically%20employ%20homogeneous%20graph%0Astructures%20that%20overlook%20the%20functional%20diversity%20of%20different%20limbs.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20HeteroMorpheus%2C%20a%20novel%20method%20based%20on%20heterogeneous%0Agraph%20Transformer.%20This%20method%20uniquely%20addresses%20limb%20heterogeneity%2C%20fostering%0Abetter%20representation%20of%20robot%20dynamics%20of%20various%20morphologies.%20Through%0Aextensive%20experiments%20we%20demonstrate%20the%20superiority%20of%20HeteroMorpheus%20against%0Astate-of-the-art%20methods%20in%20the%20capability%20of%20policy%20generalization%2C%20including%0Azero-shot%20generalization%20and%20sample-efficient%20transfer%20to%20unfamiliar%20robot%0Amorphologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeteroMorpheus%253A%2520Universal%2520Control%2520Based%2520on%2520Morphological%2520Heterogeneity%250A%2520%2520Modeling%26entry.906535625%3DYiFan%2520Hao%2520and%2520Yang%2520Yang%2520and%2520Junru%2520Song%2520and%2520Wei%2520Peng%2520and%2520Weien%2520Zhou%2520and%2520Tingsong%2520Jiang%2520and%2520Wen%2520Yao%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520robotic%2520control%252C%2520designing%2520individual%2520controllers%2520for%2520each%250Arobot%2520leads%2520to%2520high%2520computational%2520costs.%2520Universal%2520control%2520policies%252C%2520applicable%250Aacross%2520diverse%2520robot%2520morphologies%252C%2520promise%2520to%2520mitigate%2520this%2520challenge.%250APredominantly%252C%2520models%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520and%2520Transformers%2520are%250Aemployed%252C%2520owing%2520to%2520their%2520effectiveness%2520in%2520capturing%2520relational%2520dynamics%2520across%250Aa%2520robot%2527s%2520limbs.%2520However%252C%2520these%2520models%2520typically%2520employ%2520homogeneous%2520graph%250Astructures%2520that%2520overlook%2520the%2520functional%2520diversity%2520of%2520different%2520limbs.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520HeteroMorpheus%252C%2520a%2520novel%2520method%2520based%2520on%2520heterogeneous%250Agraph%2520Transformer.%2520This%2520method%2520uniquely%2520addresses%2520limb%2520heterogeneity%252C%2520fostering%250Abetter%2520representation%2520of%2520robot%2520dynamics%2520of%2520various%2520morphologies.%2520Through%250Aextensive%2520experiments%2520we%2520demonstrate%2520the%2520superiority%2520of%2520HeteroMorpheus%2520against%250Astate-of-the-art%2520methods%2520in%2520the%2520capability%2520of%2520policy%2520generalization%252C%2520including%250Azero-shot%2520generalization%2520and%2520sample-efficient%2520transfer%2520to%2520unfamiliar%2520robot%250Amorphologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeteroMorpheus%3A%20Universal%20Control%20Based%20on%20Morphological%20Heterogeneity%0A%20%20Modeling&entry.906535625=YiFan%20Hao%20and%20Yang%20Yang%20and%20Junru%20Song%20and%20Wei%20Peng%20and%20Weien%20Zhou%20and%20Tingsong%20Jiang%20and%20Wen%20Yao&entry.1292438233=%20%20In%20the%20field%20of%20robotic%20control%2C%20designing%20individual%20controllers%20for%20each%0Arobot%20leads%20to%20high%20computational%20costs.%20Universal%20control%20policies%2C%20applicable%0Aacross%20diverse%20robot%20morphologies%2C%20promise%20to%20mitigate%20this%20challenge.%0APredominantly%2C%20models%20based%20on%20Graph%20Neural%20Networks%20%28GNN%29%20and%20Transformers%20are%0Aemployed%2C%20owing%20to%20their%20effectiveness%20in%20capturing%20relational%20dynamics%20across%0Aa%20robot%27s%20limbs.%20However%2C%20these%20models%20typically%20employ%20homogeneous%20graph%0Astructures%20that%20overlook%20the%20functional%20diversity%20of%20different%20limbs.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20HeteroMorpheus%2C%20a%20novel%20method%20based%20on%20heterogeneous%0Agraph%20Transformer.%20This%20method%20uniquely%20addresses%20limb%20heterogeneity%2C%20fostering%0Abetter%20representation%20of%20robot%20dynamics%20of%20various%20morphologies.%20Through%0Aextensive%20experiments%20we%20demonstrate%20the%20superiority%20of%20HeteroMorpheus%20against%0Astate-of-the-art%20methods%20in%20the%20capability%20of%20policy%20generalization%2C%20including%0Azero-shot%20generalization%20and%20sample-efficient%20transfer%20to%20unfamiliar%20robot%0Amorphologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01230v1&entry.124074799=Read"},
{"title": "DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D\n  Diffusion Priors", "author": "Tianhao Xie and Eugene Belilovsky and Sudhir Mudur and Tiberiu Popa", "abstract": "  Direct mesh editing and deformation are key components in the geometric\nmodeling and animation pipeline. Mesh editing methods are typically framed as\noptimization problems combining user-specified vertex constraints with a\nregularizer that determines the position of the rest of the vertices. The\nchoice of the regularizer is key to the realism and authenticity of the final\nresult. Physics and geometry-based regularizers are not aware of the global\ncontext and semantics of the object, and the more recent deep learning priors\nare limited to a specific class of 3D object deformations. Our main\ncontribution is a vertex-based mesh editing method called DragD3D based on (1)\na novel optimization formulation that decouples the rotation and stretch\ncomponents of the deformation and combines a 3D geometric regularizer with (2)\nthe recently introduced DDS loss which scores the faithfulness of the rendered\n2D image to one from a diffusion model. Thus, our deformation method achieves\nglobally realistic shape deformation which is not restricted to any class of\nobjects. Our new formulation optimizes directly the transformation of the\nneural Jacobian field explicitly separating the rotational and stretching\ncomponents. The objective function of the optimization combines the approximate\ngradients of DDS and the gradients from the geometric loss to satisfy the\nvertex constraints. Additional user control over desired global shape\ndeformation is made possible by allowing explicit per-triangle deformation\ncontrol as well as explicit separation of rotational and stretching components\nof the deformation. We show that our deformations can be controlled to yield\nrealistic shape deformations that are aware of the global context of the\nobjects, and provide better results than just using geometric regularizers.\n", "link": "http://arxiv.org/abs/2310.04561v2", "date": "2024-08-02", "relevancy": 1.7543, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6304}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5304}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragD3D%3A%20Realistic%20Mesh%20Editing%20with%20Rigidity%20Control%20Driven%20by%202D%0A%20%20Diffusion%20Priors&body=Title%3A%20DragD3D%3A%20Realistic%20Mesh%20Editing%20with%20Rigidity%20Control%20Driven%20by%202D%0A%20%20Diffusion%20Priors%0AAuthor%3A%20Tianhao%20Xie%20and%20Eugene%20Belilovsky%20and%20Sudhir%20Mudur%20and%20Tiberiu%20Popa%0AAbstract%3A%20%20%20Direct%20mesh%20editing%20and%20deformation%20are%20key%20components%20in%20the%20geometric%0Amodeling%20and%20animation%20pipeline.%20Mesh%20editing%20methods%20are%20typically%20framed%20as%0Aoptimization%20problems%20combining%20user-specified%20vertex%20constraints%20with%20a%0Aregularizer%20that%20determines%20the%20position%20of%20the%20rest%20of%20the%20vertices.%20The%0Achoice%20of%20the%20regularizer%20is%20key%20to%20the%20realism%20and%20authenticity%20of%20the%20final%0Aresult.%20Physics%20and%20geometry-based%20regularizers%20are%20not%20aware%20of%20the%20global%0Acontext%20and%20semantics%20of%20the%20object%2C%20and%20the%20more%20recent%20deep%20learning%20priors%0Aare%20limited%20to%20a%20specific%20class%20of%203D%20object%20deformations.%20Our%20main%0Acontribution%20is%20a%20vertex-based%20mesh%20editing%20method%20called%20DragD3D%20based%20on%20%281%29%0Aa%20novel%20optimization%20formulation%20that%20decouples%20the%20rotation%20and%20stretch%0Acomponents%20of%20the%20deformation%20and%20combines%20a%203D%20geometric%20regularizer%20with%20%282%29%0Athe%20recently%20introduced%20DDS%20loss%20which%20scores%20the%20faithfulness%20of%20the%20rendered%0A2D%20image%20to%20one%20from%20a%20diffusion%20model.%20Thus%2C%20our%20deformation%20method%20achieves%0Aglobally%20realistic%20shape%20deformation%20which%20is%20not%20restricted%20to%20any%20class%20of%0Aobjects.%20Our%20new%20formulation%20optimizes%20directly%20the%20transformation%20of%20the%0Aneural%20Jacobian%20field%20explicitly%20separating%20the%20rotational%20and%20stretching%0Acomponents.%20The%20objective%20function%20of%20the%20optimization%20combines%20the%20approximate%0Agradients%20of%20DDS%20and%20the%20gradients%20from%20the%20geometric%20loss%20to%20satisfy%20the%0Avertex%20constraints.%20Additional%20user%20control%20over%20desired%20global%20shape%0Adeformation%20is%20made%20possible%20by%20allowing%20explicit%20per-triangle%20deformation%0Acontrol%20as%20well%20as%20explicit%20separation%20of%20rotational%20and%20stretching%20components%0Aof%20the%20deformation.%20We%20show%20that%20our%20deformations%20can%20be%20controlled%20to%20yield%0Arealistic%20shape%20deformations%20that%20are%20aware%20of%20the%20global%20context%20of%20the%0Aobjects%2C%20and%20provide%20better%20results%20than%20just%20using%20geometric%20regularizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragD3D%253A%2520Realistic%2520Mesh%2520Editing%2520with%2520Rigidity%2520Control%2520Driven%2520by%25202D%250A%2520%2520Diffusion%2520Priors%26entry.906535625%3DTianhao%2520Xie%2520and%2520Eugene%2520Belilovsky%2520and%2520Sudhir%2520Mudur%2520and%2520Tiberiu%2520Popa%26entry.1292438233%3D%2520%2520Direct%2520mesh%2520editing%2520and%2520deformation%2520are%2520key%2520components%2520in%2520the%2520geometric%250Amodeling%2520and%2520animation%2520pipeline.%2520Mesh%2520editing%2520methods%2520are%2520typically%2520framed%2520as%250Aoptimization%2520problems%2520combining%2520user-specified%2520vertex%2520constraints%2520with%2520a%250Aregularizer%2520that%2520determines%2520the%2520position%2520of%2520the%2520rest%2520of%2520the%2520vertices.%2520The%250Achoice%2520of%2520the%2520regularizer%2520is%2520key%2520to%2520the%2520realism%2520and%2520authenticity%2520of%2520the%2520final%250Aresult.%2520Physics%2520and%2520geometry-based%2520regularizers%2520are%2520not%2520aware%2520of%2520the%2520global%250Acontext%2520and%2520semantics%2520of%2520the%2520object%252C%2520and%2520the%2520more%2520recent%2520deep%2520learning%2520priors%250Aare%2520limited%2520to%2520a%2520specific%2520class%2520of%25203D%2520object%2520deformations.%2520Our%2520main%250Acontribution%2520is%2520a%2520vertex-based%2520mesh%2520editing%2520method%2520called%2520DragD3D%2520based%2520on%2520%25281%2529%250Aa%2520novel%2520optimization%2520formulation%2520that%2520decouples%2520the%2520rotation%2520and%2520stretch%250Acomponents%2520of%2520the%2520deformation%2520and%2520combines%2520a%25203D%2520geometric%2520regularizer%2520with%2520%25282%2529%250Athe%2520recently%2520introduced%2520DDS%2520loss%2520which%2520scores%2520the%2520faithfulness%2520of%2520the%2520rendered%250A2D%2520image%2520to%2520one%2520from%2520a%2520diffusion%2520model.%2520Thus%252C%2520our%2520deformation%2520method%2520achieves%250Aglobally%2520realistic%2520shape%2520deformation%2520which%2520is%2520not%2520restricted%2520to%2520any%2520class%2520of%250Aobjects.%2520Our%2520new%2520formulation%2520optimizes%2520directly%2520the%2520transformation%2520of%2520the%250Aneural%2520Jacobian%2520field%2520explicitly%2520separating%2520the%2520rotational%2520and%2520stretching%250Acomponents.%2520The%2520objective%2520function%2520of%2520the%2520optimization%2520combines%2520the%2520approximate%250Agradients%2520of%2520DDS%2520and%2520the%2520gradients%2520from%2520the%2520geometric%2520loss%2520to%2520satisfy%2520the%250Avertex%2520constraints.%2520Additional%2520user%2520control%2520over%2520desired%2520global%2520shape%250Adeformation%2520is%2520made%2520possible%2520by%2520allowing%2520explicit%2520per-triangle%2520deformation%250Acontrol%2520as%2520well%2520as%2520explicit%2520separation%2520of%2520rotational%2520and%2520stretching%2520components%250Aof%2520the%2520deformation.%2520We%2520show%2520that%2520our%2520deformations%2520can%2520be%2520controlled%2520to%2520yield%250Arealistic%2520shape%2520deformations%2520that%2520are%2520aware%2520of%2520the%2520global%2520context%2520of%2520the%250Aobjects%252C%2520and%2520provide%2520better%2520results%2520than%2520just%2520using%2520geometric%2520regularizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragD3D%3A%20Realistic%20Mesh%20Editing%20with%20Rigidity%20Control%20Driven%20by%202D%0A%20%20Diffusion%20Priors&entry.906535625=Tianhao%20Xie%20and%20Eugene%20Belilovsky%20and%20Sudhir%20Mudur%20and%20Tiberiu%20Popa&entry.1292438233=%20%20Direct%20mesh%20editing%20and%20deformation%20are%20key%20components%20in%20the%20geometric%0Amodeling%20and%20animation%20pipeline.%20Mesh%20editing%20methods%20are%20typically%20framed%20as%0Aoptimization%20problems%20combining%20user-specified%20vertex%20constraints%20with%20a%0Aregularizer%20that%20determines%20the%20position%20of%20the%20rest%20of%20the%20vertices.%20The%0Achoice%20of%20the%20regularizer%20is%20key%20to%20the%20realism%20and%20authenticity%20of%20the%20final%0Aresult.%20Physics%20and%20geometry-based%20regularizers%20are%20not%20aware%20of%20the%20global%0Acontext%20and%20semantics%20of%20the%20object%2C%20and%20the%20more%20recent%20deep%20learning%20priors%0Aare%20limited%20to%20a%20specific%20class%20of%203D%20object%20deformations.%20Our%20main%0Acontribution%20is%20a%20vertex-based%20mesh%20editing%20method%20called%20DragD3D%20based%20on%20%281%29%0Aa%20novel%20optimization%20formulation%20that%20decouples%20the%20rotation%20and%20stretch%0Acomponents%20of%20the%20deformation%20and%20combines%20a%203D%20geometric%20regularizer%20with%20%282%29%0Athe%20recently%20introduced%20DDS%20loss%20which%20scores%20the%20faithfulness%20of%20the%20rendered%0A2D%20image%20to%20one%20from%20a%20diffusion%20model.%20Thus%2C%20our%20deformation%20method%20achieves%0Aglobally%20realistic%20shape%20deformation%20which%20is%20not%20restricted%20to%20any%20class%20of%0Aobjects.%20Our%20new%20formulation%20optimizes%20directly%20the%20transformation%20of%20the%0Aneural%20Jacobian%20field%20explicitly%20separating%20the%20rotational%20and%20stretching%0Acomponents.%20The%20objective%20function%20of%20the%20optimization%20combines%20the%20approximate%0Agradients%20of%20DDS%20and%20the%20gradients%20from%20the%20geometric%20loss%20to%20satisfy%20the%0Avertex%20constraints.%20Additional%20user%20control%20over%20desired%20global%20shape%0Adeformation%20is%20made%20possible%20by%20allowing%20explicit%20per-triangle%20deformation%0Acontrol%20as%20well%20as%20explicit%20separation%20of%20rotational%20and%20stretching%20components%0Aof%20the%20deformation.%20We%20show%20that%20our%20deformations%20can%20be%20controlled%20to%20yield%0Arealistic%20shape%20deformations%20that%20are%20aware%20of%20the%20global%20context%20of%20the%0Aobjects%2C%20and%20provide%20better%20results%20than%20just%20using%20geometric%20regularizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04561v2&entry.124074799=Read"},
{"title": "Automated Classification of Dry Bean Varieties Using XGBoost and SVM\n  Models", "author": "Ramtin Ardeshirifar", "abstract": "  This paper presents a comparative study on the automated classification of\nseven different varieties of dry beans using machine learning models.\nLeveraging a dataset of 12,909 dry bean samples, reduced from an initial 13,611\nthrough outlier removal and feature extraction, we applied Principal Component\nAnalysis (PCA) for dimensionality reduction and trained two multiclass\nclassifiers: XGBoost and Support Vector Machine (SVM). The models were\nevaluated using nested cross-validation to ensure robust performance assessment\nand hyperparameter tuning. The XGBoost and SVM models achieved overall correct\nclassification rates of 94.00% and 94.39%, respectively. The results underscore\nthe efficacy of these machine learning approaches in agricultural applications,\nparticularly in enhancing the uniformity and efficiency of seed classification.\nThis study contributes to the growing body of work on precision agriculture,\ndemonstrating that automated systems can significantly support seed quality\ncontrol and crop yield optimization. Future work will explore incorporating\nmore diverse datasets and advanced algorithms to further improve classification\naccuracy.\n", "link": "http://arxiv.org/abs/2408.01244v1", "date": "2024-08-02", "relevancy": 1.7496, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4515}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4461}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Classification%20of%20Dry%20Bean%20Varieties%20Using%20XGBoost%20and%20SVM%0A%20%20Models&body=Title%3A%20Automated%20Classification%20of%20Dry%20Bean%20Varieties%20Using%20XGBoost%20and%20SVM%0A%20%20Models%0AAuthor%3A%20Ramtin%20Ardeshirifar%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comparative%20study%20on%20the%20automated%20classification%20of%0Aseven%20different%20varieties%20of%20dry%20beans%20using%20machine%20learning%20models.%0ALeveraging%20a%20dataset%20of%2012%2C909%20dry%20bean%20samples%2C%20reduced%20from%20an%20initial%2013%2C611%0Athrough%20outlier%20removal%20and%20feature%20extraction%2C%20we%20applied%20Principal%20Component%0AAnalysis%20%28PCA%29%20for%20dimensionality%20reduction%20and%20trained%20two%20multiclass%0Aclassifiers%3A%20XGBoost%20and%20Support%20Vector%20Machine%20%28SVM%29.%20The%20models%20were%0Aevaluated%20using%20nested%20cross-validation%20to%20ensure%20robust%20performance%20assessment%0Aand%20hyperparameter%20tuning.%20The%20XGBoost%20and%20SVM%20models%20achieved%20overall%20correct%0Aclassification%20rates%20of%2094.00%25%20and%2094.39%25%2C%20respectively.%20The%20results%20underscore%0Athe%20efficacy%20of%20these%20machine%20learning%20approaches%20in%20agricultural%20applications%2C%0Aparticularly%20in%20enhancing%20the%20uniformity%20and%20efficiency%20of%20seed%20classification.%0AThis%20study%20contributes%20to%20the%20growing%20body%20of%20work%20on%20precision%20agriculture%2C%0Ademonstrating%20that%20automated%20systems%20can%20significantly%20support%20seed%20quality%0Acontrol%20and%20crop%20yield%20optimization.%20Future%20work%20will%20explore%20incorporating%0Amore%20diverse%20datasets%20and%20advanced%20algorithms%20to%20further%20improve%20classification%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Classification%2520of%2520Dry%2520Bean%2520Varieties%2520Using%2520XGBoost%2520and%2520SVM%250A%2520%2520Models%26entry.906535625%3DRamtin%2520Ardeshirifar%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comparative%2520study%2520on%2520the%2520automated%2520classification%2520of%250Aseven%2520different%2520varieties%2520of%2520dry%2520beans%2520using%2520machine%2520learning%2520models.%250ALeveraging%2520a%2520dataset%2520of%252012%252C909%2520dry%2520bean%2520samples%252C%2520reduced%2520from%2520an%2520initial%252013%252C611%250Athrough%2520outlier%2520removal%2520and%2520feature%2520extraction%252C%2520we%2520applied%2520Principal%2520Component%250AAnalysis%2520%2528PCA%2529%2520for%2520dimensionality%2520reduction%2520and%2520trained%2520two%2520multiclass%250Aclassifiers%253A%2520XGBoost%2520and%2520Support%2520Vector%2520Machine%2520%2528SVM%2529.%2520The%2520models%2520were%250Aevaluated%2520using%2520nested%2520cross-validation%2520to%2520ensure%2520robust%2520performance%2520assessment%250Aand%2520hyperparameter%2520tuning.%2520The%2520XGBoost%2520and%2520SVM%2520models%2520achieved%2520overall%2520correct%250Aclassification%2520rates%2520of%252094.00%2525%2520and%252094.39%2525%252C%2520respectively.%2520The%2520results%2520underscore%250Athe%2520efficacy%2520of%2520these%2520machine%2520learning%2520approaches%2520in%2520agricultural%2520applications%252C%250Aparticularly%2520in%2520enhancing%2520the%2520uniformity%2520and%2520efficiency%2520of%2520seed%2520classification.%250AThis%2520study%2520contributes%2520to%2520the%2520growing%2520body%2520of%2520work%2520on%2520precision%2520agriculture%252C%250Ademonstrating%2520that%2520automated%2520systems%2520can%2520significantly%2520support%2520seed%2520quality%250Acontrol%2520and%2520crop%2520yield%2520optimization.%2520Future%2520work%2520will%2520explore%2520incorporating%250Amore%2520diverse%2520datasets%2520and%2520advanced%2520algorithms%2520to%2520further%2520improve%2520classification%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Classification%20of%20Dry%20Bean%20Varieties%20Using%20XGBoost%20and%20SVM%0A%20%20Models&entry.906535625=Ramtin%20Ardeshirifar&entry.1292438233=%20%20This%20paper%20presents%20a%20comparative%20study%20on%20the%20automated%20classification%20of%0Aseven%20different%20varieties%20of%20dry%20beans%20using%20machine%20learning%20models.%0ALeveraging%20a%20dataset%20of%2012%2C909%20dry%20bean%20samples%2C%20reduced%20from%20an%20initial%2013%2C611%0Athrough%20outlier%20removal%20and%20feature%20extraction%2C%20we%20applied%20Principal%20Component%0AAnalysis%20%28PCA%29%20for%20dimensionality%20reduction%20and%20trained%20two%20multiclass%0Aclassifiers%3A%20XGBoost%20and%20Support%20Vector%20Machine%20%28SVM%29.%20The%20models%20were%0Aevaluated%20using%20nested%20cross-validation%20to%20ensure%20robust%20performance%20assessment%0Aand%20hyperparameter%20tuning.%20The%20XGBoost%20and%20SVM%20models%20achieved%20overall%20correct%0Aclassification%20rates%20of%2094.00%25%20and%2094.39%25%2C%20respectively.%20The%20results%20underscore%0Athe%20efficacy%20of%20these%20machine%20learning%20approaches%20in%20agricultural%20applications%2C%0Aparticularly%20in%20enhancing%20the%20uniformity%20and%20efficiency%20of%20seed%20classification.%0AThis%20study%20contributes%20to%20the%20growing%20body%20of%20work%20on%20precision%20agriculture%2C%0Ademonstrating%20that%20automated%20systems%20can%20significantly%20support%20seed%20quality%0Acontrol%20and%20crop%20yield%20optimization.%20Future%20work%20will%20explore%20incorporating%0Amore%20diverse%20datasets%20and%20advanced%20algorithms%20to%20further%20improve%20classification%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01244v1&entry.124074799=Read"},
{"title": "Explaining a probabilistic prediction on the simplex with Shapley\n  compositions", "author": "Paul-Gauthier No\u00e9 and Miquel Perell\u00f3-Nieto and Jean-Fran\u00e7ois Bonastre and Peter Flach", "abstract": "  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n", "link": "http://arxiv.org/abs/2408.01382v1", "date": "2024-08-02", "relevancy": 1.7447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4249}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20a%20probabilistic%20prediction%20on%20the%20simplex%20with%20Shapley%0A%20%20compositions&body=Title%3A%20Explaining%20a%20probabilistic%20prediction%20on%20the%20simplex%20with%20Shapley%0A%20%20compositions%0AAuthor%3A%20Paul-Gauthier%20No%C3%A9%20and%20Miquel%20Perell%C3%B3-Nieto%20and%20Jean-Fran%C3%A7ois%20Bonastre%20and%20Peter%20Flach%0AAbstract%3A%20%20%20Originating%20in%20game%20theory%2C%20Shapley%20values%20are%20widely%20used%20for%20explaining%20a%0Amachine%20learning%20model%27s%20prediction%20by%20quantifying%20the%20contribution%20of%20each%0Afeature%27s%20value%20to%20the%20prediction.%20This%20requires%20a%20scalar%20prediction%20as%20in%0Abinary%20classification%2C%20whereas%20a%20multiclass%20probabilistic%20prediction%20is%20a%0Adiscrete%20probability%20distribution%2C%20living%20on%20a%20multidimensional%20simplex.%20In%0Asuch%20a%20multiclass%20setting%20the%20Shapley%20values%20are%20typically%20computed%20separately%0Aon%20each%20class%20in%20a%20one-vs-rest%20manner%2C%20ignoring%20the%20compositional%20nature%20of%20the%0Aoutput%20distribution.%20In%20this%20paper%2C%20we%20introduce%20Shapley%20compositions%20as%20a%0Awell-founded%20way%20to%20properly%20explain%20a%20multiclass%20probabilistic%20prediction%2C%0Ausing%20the%20Aitchison%20geometry%20from%20compositional%20data%20analysis.%20We%20prove%20that%0Athe%20Shapley%20composition%20is%20the%20unique%20quantity%20satisfying%20linearity%2C%20symmetry%0Aand%20efficiency%20on%20the%20Aitchison%20simplex%2C%20extending%20the%20corresponding%20axiomatic%0Aproperties%20of%20the%20standard%20Shapley%20value.%20We%20demonstrate%20this%20proper%20multiclass%0Atreatment%20in%20a%20range%20of%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520a%2520probabilistic%2520prediction%2520on%2520the%2520simplex%2520with%2520Shapley%250A%2520%2520compositions%26entry.906535625%3DPaul-Gauthier%2520No%25C3%25A9%2520and%2520Miquel%2520Perell%25C3%25B3-Nieto%2520and%2520Jean-Fran%25C3%25A7ois%2520Bonastre%2520and%2520Peter%2520Flach%26entry.1292438233%3D%2520%2520Originating%2520in%2520game%2520theory%252C%2520Shapley%2520values%2520are%2520widely%2520used%2520for%2520explaining%2520a%250Amachine%2520learning%2520model%2527s%2520prediction%2520by%2520quantifying%2520the%2520contribution%2520of%2520each%250Afeature%2527s%2520value%2520to%2520the%2520prediction.%2520This%2520requires%2520a%2520scalar%2520prediction%2520as%2520in%250Abinary%2520classification%252C%2520whereas%2520a%2520multiclass%2520probabilistic%2520prediction%2520is%2520a%250Adiscrete%2520probability%2520distribution%252C%2520living%2520on%2520a%2520multidimensional%2520simplex.%2520In%250Asuch%2520a%2520multiclass%2520setting%2520the%2520Shapley%2520values%2520are%2520typically%2520computed%2520separately%250Aon%2520each%2520class%2520in%2520a%2520one-vs-rest%2520manner%252C%2520ignoring%2520the%2520compositional%2520nature%2520of%2520the%250Aoutput%2520distribution.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Shapley%2520compositions%2520as%2520a%250Awell-founded%2520way%2520to%2520properly%2520explain%2520a%2520multiclass%2520probabilistic%2520prediction%252C%250Ausing%2520the%2520Aitchison%2520geometry%2520from%2520compositional%2520data%2520analysis.%2520We%2520prove%2520that%250Athe%2520Shapley%2520composition%2520is%2520the%2520unique%2520quantity%2520satisfying%2520linearity%252C%2520symmetry%250Aand%2520efficiency%2520on%2520the%2520Aitchison%2520simplex%252C%2520extending%2520the%2520corresponding%2520axiomatic%250Aproperties%2520of%2520the%2520standard%2520Shapley%2520value.%2520We%2520demonstrate%2520this%2520proper%2520multiclass%250Atreatment%2520in%2520a%2520range%2520of%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20a%20probabilistic%20prediction%20on%20the%20simplex%20with%20Shapley%0A%20%20compositions&entry.906535625=Paul-Gauthier%20No%C3%A9%20and%20Miquel%20Perell%C3%B3-Nieto%20and%20Jean-Fran%C3%A7ois%20Bonastre%20and%20Peter%20Flach&entry.1292438233=%20%20Originating%20in%20game%20theory%2C%20Shapley%20values%20are%20widely%20used%20for%20explaining%20a%0Amachine%20learning%20model%27s%20prediction%20by%20quantifying%20the%20contribution%20of%20each%0Afeature%27s%20value%20to%20the%20prediction.%20This%20requires%20a%20scalar%20prediction%20as%20in%0Abinary%20classification%2C%20whereas%20a%20multiclass%20probabilistic%20prediction%20is%20a%0Adiscrete%20probability%20distribution%2C%20living%20on%20a%20multidimensional%20simplex.%20In%0Asuch%20a%20multiclass%20setting%20the%20Shapley%20values%20are%20typically%20computed%20separately%0Aon%20each%20class%20in%20a%20one-vs-rest%20manner%2C%20ignoring%20the%20compositional%20nature%20of%20the%0Aoutput%20distribution.%20In%20this%20paper%2C%20we%20introduce%20Shapley%20compositions%20as%20a%0Awell-founded%20way%20to%20properly%20explain%20a%20multiclass%20probabilistic%20prediction%2C%0Ausing%20the%20Aitchison%20geometry%20from%20compositional%20data%20analysis.%20We%20prove%20that%0Athe%20Shapley%20composition%20is%20the%20unique%20quantity%20satisfying%20linearity%2C%20symmetry%0Aand%20efficiency%20on%20the%20Aitchison%20simplex%2C%20extending%20the%20corresponding%20axiomatic%0Aproperties%20of%20the%20standard%20Shapley%20value.%20We%20demonstrate%20this%20proper%20multiclass%0Atreatment%20in%20a%20range%20of%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01382v1&entry.124074799=Read"},
{"title": "CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset\n  Augmentation using Diffusion Models", "author": "Kushal Kumar Jain and Steve Grosz and Anoop M. Namboodiri and Anil K. Jain", "abstract": "  Forensic sketch-to-mugshot matching is a challenging task in face\nrecognition, primarily hindered by the scarcity of annotated forensic sketches\nand the modality gap between sketches and photographs. To address this, we\npropose CLIP4Sketch, a novel approach that leverages diffusion models to\ngenerate a large and diverse set of sketch images, which helps in enhancing the\nperformance of face recognition systems in sketch-to-mugshot matching. Our\nmethod utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate\nsketches with explicit control over identity and style. We combine CLIP and\nAdaface embeddings of a reference mugshot, along with textual descriptions of\nstyle, as the conditions to the diffusion model. We demonstrate the efficacy of\nour approach by generating a comprehensive dataset of sketches corresponding to\nmugshots and training a face recognition model on our synthetic data. Our\nresults show significant improvements in sketch-to-mugshot matching accuracy\nover training on an existing, limited amount of real face sketch data,\nvalidating the potential of diffusion models in enhancing the performance of\nface recognition systems across modalities. We also compare our dataset with\ndatasets generated using GAN-based methods to show its superiority.\n", "link": "http://arxiv.org/abs/2408.01233v1", "date": "2024-08-02", "relevancy": 1.7314, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.587}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models&body=Title%3A%20CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models%0AAuthor%3A%20Kushal%20Kumar%20Jain%20and%20Steve%20Grosz%20and%20Anoop%20M.%20Namboodiri%20and%20Anil%20K.%20Jain%0AAbstract%3A%20%20%20Forensic%20sketch-to-mugshot%20matching%20is%20a%20challenging%20task%20in%20face%0Arecognition%2C%20primarily%20hindered%20by%20the%20scarcity%20of%20annotated%20forensic%20sketches%0Aand%20the%20modality%20gap%20between%20sketches%20and%20photographs.%20To%20address%20this%2C%20we%0Apropose%20CLIP4Sketch%2C%20a%20novel%20approach%20that%20leverages%20diffusion%20models%20to%0Agenerate%20a%20large%20and%20diverse%20set%20of%20sketch%20images%2C%20which%20helps%20in%20enhancing%20the%0Aperformance%20of%20face%20recognition%20systems%20in%20sketch-to-mugshot%20matching.%20Our%0Amethod%20utilizes%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20to%20generate%0Asketches%20with%20explicit%20control%20over%20identity%20and%20style.%20We%20combine%20CLIP%20and%0AAdaface%20embeddings%20of%20a%20reference%20mugshot%2C%20along%20with%20textual%20descriptions%20of%0Astyle%2C%20as%20the%20conditions%20to%20the%20diffusion%20model.%20We%20demonstrate%20the%20efficacy%20of%0Aour%20approach%20by%20generating%20a%20comprehensive%20dataset%20of%20sketches%20corresponding%20to%0Amugshots%20and%20training%20a%20face%20recognition%20model%20on%20our%20synthetic%20data.%20Our%0Aresults%20show%20significant%20improvements%20in%20sketch-to-mugshot%20matching%20accuracy%0Aover%20training%20on%20an%20existing%2C%20limited%20amount%20of%20real%20face%20sketch%20data%2C%0Avalidating%20the%20potential%20of%20diffusion%20models%20in%20enhancing%20the%20performance%20of%0Aface%20recognition%20systems%20across%20modalities.%20We%20also%20compare%20our%20dataset%20with%0Adatasets%20generated%20using%20GAN-based%20methods%20to%20show%20its%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP4Sketch%253A%2520Enhancing%2520Sketch%2520to%2520Mugshot%2520Matching%2520through%2520Dataset%250A%2520%2520Augmentation%2520using%2520Diffusion%2520Models%26entry.906535625%3DKushal%2520Kumar%2520Jain%2520and%2520Steve%2520Grosz%2520and%2520Anoop%2520M.%2520Namboodiri%2520and%2520Anil%2520K.%2520Jain%26entry.1292438233%3D%2520%2520Forensic%2520sketch-to-mugshot%2520matching%2520is%2520a%2520challenging%2520task%2520in%2520face%250Arecognition%252C%2520primarily%2520hindered%2520by%2520the%2520scarcity%2520of%2520annotated%2520forensic%2520sketches%250Aand%2520the%2520modality%2520gap%2520between%2520sketches%2520and%2520photographs.%2520To%2520address%2520this%252C%2520we%250Apropose%2520CLIP4Sketch%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520diffusion%2520models%2520to%250Agenerate%2520a%2520large%2520and%2520diverse%2520set%2520of%2520sketch%2520images%252C%2520which%2520helps%2520in%2520enhancing%2520the%250Aperformance%2520of%2520face%2520recognition%2520systems%2520in%2520sketch-to-mugshot%2520matching.%2520Our%250Amethod%2520utilizes%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520to%2520generate%250Asketches%2520with%2520explicit%2520control%2520over%2520identity%2520and%2520style.%2520We%2520combine%2520CLIP%2520and%250AAdaface%2520embeddings%2520of%2520a%2520reference%2520mugshot%252C%2520along%2520with%2520textual%2520descriptions%2520of%250Astyle%252C%2520as%2520the%2520conditions%2520to%2520the%2520diffusion%2520model.%2520We%2520demonstrate%2520the%2520efficacy%2520of%250Aour%2520approach%2520by%2520generating%2520a%2520comprehensive%2520dataset%2520of%2520sketches%2520corresponding%2520to%250Amugshots%2520and%2520training%2520a%2520face%2520recognition%2520model%2520on%2520our%2520synthetic%2520data.%2520Our%250Aresults%2520show%2520significant%2520improvements%2520in%2520sketch-to-mugshot%2520matching%2520accuracy%250Aover%2520training%2520on%2520an%2520existing%252C%2520limited%2520amount%2520of%2520real%2520face%2520sketch%2520data%252C%250Avalidating%2520the%2520potential%2520of%2520diffusion%2520models%2520in%2520enhancing%2520the%2520performance%2520of%250Aface%2520recognition%2520systems%2520across%2520modalities.%2520We%2520also%2520compare%2520our%2520dataset%2520with%250Adatasets%2520generated%2520using%2520GAN-based%2520methods%2520to%2520show%2520its%2520superiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP4Sketch%3A%20Enhancing%20Sketch%20to%20Mugshot%20Matching%20through%20Dataset%0A%20%20Augmentation%20using%20Diffusion%20Models&entry.906535625=Kushal%20Kumar%20Jain%20and%20Steve%20Grosz%20and%20Anoop%20M.%20Namboodiri%20and%20Anil%20K.%20Jain&entry.1292438233=%20%20Forensic%20sketch-to-mugshot%20matching%20is%20a%20challenging%20task%20in%20face%0Arecognition%2C%20primarily%20hindered%20by%20the%20scarcity%20of%20annotated%20forensic%20sketches%0Aand%20the%20modality%20gap%20between%20sketches%20and%20photographs.%20To%20address%20this%2C%20we%0Apropose%20CLIP4Sketch%2C%20a%20novel%20approach%20that%20leverages%20diffusion%20models%20to%0Agenerate%20a%20large%20and%20diverse%20set%20of%20sketch%20images%2C%20which%20helps%20in%20enhancing%20the%0Aperformance%20of%20face%20recognition%20systems%20in%20sketch-to-mugshot%20matching.%20Our%0Amethod%20utilizes%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20to%20generate%0Asketches%20with%20explicit%20control%20over%20identity%20and%20style.%20We%20combine%20CLIP%20and%0AAdaface%20embeddings%20of%20a%20reference%20mugshot%2C%20along%20with%20textual%20descriptions%20of%0Astyle%2C%20as%20the%20conditions%20to%20the%20diffusion%20model.%20We%20demonstrate%20the%20efficacy%20of%0Aour%20approach%20by%20generating%20a%20comprehensive%20dataset%20of%20sketches%20corresponding%20to%0Amugshots%20and%20training%20a%20face%20recognition%20model%20on%20our%20synthetic%20data.%20Our%0Aresults%20show%20significant%20improvements%20in%20sketch-to-mugshot%20matching%20accuracy%0Aover%20training%20on%20an%20existing%2C%20limited%20amount%20of%20real%20face%20sketch%20data%2C%0Avalidating%20the%20potential%20of%20diffusion%20models%20in%20enhancing%20the%20performance%20of%0Aface%20recognition%20systems%20across%20modalities.%20We%20also%20compare%20our%20dataset%20with%0Adatasets%20generated%20using%20GAN-based%20methods%20to%20show%20its%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01233v1&entry.124074799=Read"},
{"title": "Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets", "author": "Tianxiao Zhang and Wenju Xu and Bo Luo and Guanghui Wang", "abstract": "  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n", "link": "http://arxiv.org/abs/2407.19394v3", "date": "2024-08-02", "relevancy": 1.727, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5822}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5693}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Wise%20Convolutions%20in%20Vision%20Transformers%20for%20Efficient%20Training%20on%0A%20%20Small%20Datasets&body=Title%3A%20Depth-Wise%20Convolutions%20in%20Vision%20Transformers%20for%20Efficient%20Training%20on%0A%20%20Small%20Datasets%0AAuthor%3A%20Tianxiao%20Zhang%20and%20Wenju%20Xu%20and%20Bo%20Luo%20and%20Guanghui%20Wang%0AAbstract%3A%20%20%20The%20Vision%20Transformer%20%28ViT%29%20leverages%20the%20Transformer%27s%20encoder%20to%20capture%0Aglobal%20information%20by%20dividing%20images%20into%20patches%20and%20achieves%20superior%0Aperformance%20across%20various%20computer%20vision%20tasks.%20However%2C%20the%20self-attention%0Amechanism%20of%20ViT%20captures%20the%20global%20context%20from%20the%20outset%2C%20overlooking%20the%0Ainherent%20relationships%20between%20neighboring%20pixels%20in%20images%20or%20videos.%0ATransformers%20mainly%20focus%20on%20global%20information%20while%20ignoring%20the%20fine-grained%0Alocal%20details.%20Consequently%2C%20ViT%20lacks%20inductive%20bias%20during%20image%20or%20video%0Adataset%20training.%20In%20contrast%2C%20convolutional%20neural%20networks%20%28CNNs%29%2C%20with%20their%0Areliance%20on%20local%20filters%2C%20possess%20an%20inherent%20inductive%20bias%2C%20making%20them%20more%0Aefficient%20and%20quicker%20to%20converge%20than%20ViT%20with%20less%20data.%20In%20this%20paper%2C%20we%0Apresent%20a%20lightweight%20Depth-Wise%20Convolution%20module%20as%20a%20shortcut%20in%20ViT%0Amodels%2C%20bypassing%20entire%20Transformer%20blocks%20to%20ensure%20the%20models%20capture%20both%0Alocal%20and%20global%20information%20with%20minimal%20overhead.%20Additionally%2C%20we%20introduce%0Atwo%20architecture%20variants%2C%20allowing%20the%20Depth-Wise%20Convolution%20modules%20to%20be%0Aapplied%20to%20multiple%20Transformer%20blocks%20for%20parameter%20savings%2C%20and%20incorporating%0Aindependent%20parallel%20Depth-Wise%20Convolution%20modules%20with%20different%20kernels%20to%0Aenhance%20the%20acquisition%20of%20local%20information.%20The%20proposed%20approach%0Asignificantly%20boosts%20the%20performance%20of%20ViT%20models%20on%20image%20classification%2C%0Aobject%20detection%20and%20instance%20segmentation%20by%20a%20large%20margin%2C%20especially%20on%0Asmall%20datasets%2C%20as%20evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20Tiny-ImageNet%20and%20ImageNet%0Afor%20image%20classification%2C%20and%20COCO%20for%20object%20detection%20and%20instance%0Asegmentation.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ZTX-100/Efficient_ViT_with_DW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19394v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Wise%2520Convolutions%2520in%2520Vision%2520Transformers%2520for%2520Efficient%2520Training%2520on%250A%2520%2520Small%2520Datasets%26entry.906535625%3DTianxiao%2520Zhang%2520and%2520Wenju%2520Xu%2520and%2520Bo%2520Luo%2520and%2520Guanghui%2520Wang%26entry.1292438233%3D%2520%2520The%2520Vision%2520Transformer%2520%2528ViT%2529%2520leverages%2520the%2520Transformer%2527s%2520encoder%2520to%2520capture%250Aglobal%2520information%2520by%2520dividing%2520images%2520into%2520patches%2520and%2520achieves%2520superior%250Aperformance%2520across%2520various%2520computer%2520vision%2520tasks.%2520However%252C%2520the%2520self-attention%250Amechanism%2520of%2520ViT%2520captures%2520the%2520global%2520context%2520from%2520the%2520outset%252C%2520overlooking%2520the%250Ainherent%2520relationships%2520between%2520neighboring%2520pixels%2520in%2520images%2520or%2520videos.%250ATransformers%2520mainly%2520focus%2520on%2520global%2520information%2520while%2520ignoring%2520the%2520fine-grained%250Alocal%2520details.%2520Consequently%252C%2520ViT%2520lacks%2520inductive%2520bias%2520during%2520image%2520or%2520video%250Adataset%2520training.%2520In%2520contrast%252C%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520with%2520their%250Areliance%2520on%2520local%2520filters%252C%2520possess%2520an%2520inherent%2520inductive%2520bias%252C%2520making%2520them%2520more%250Aefficient%2520and%2520quicker%2520to%2520converge%2520than%2520ViT%2520with%2520less%2520data.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520lightweight%2520Depth-Wise%2520Convolution%2520module%2520as%2520a%2520shortcut%2520in%2520ViT%250Amodels%252C%2520bypassing%2520entire%2520Transformer%2520blocks%2520to%2520ensure%2520the%2520models%2520capture%2520both%250Alocal%2520and%2520global%2520information%2520with%2520minimal%2520overhead.%2520Additionally%252C%2520we%2520introduce%250Atwo%2520architecture%2520variants%252C%2520allowing%2520the%2520Depth-Wise%2520Convolution%2520modules%2520to%2520be%250Aapplied%2520to%2520multiple%2520Transformer%2520blocks%2520for%2520parameter%2520savings%252C%2520and%2520incorporating%250Aindependent%2520parallel%2520Depth-Wise%2520Convolution%2520modules%2520with%2520different%2520kernels%2520to%250Aenhance%2520the%2520acquisition%2520of%2520local%2520information.%2520The%2520proposed%2520approach%250Asignificantly%2520boosts%2520the%2520performance%2520of%2520ViT%2520models%2520on%2520image%2520classification%252C%250Aobject%2520detection%2520and%2520instance%2520segmentation%2520by%2520a%2520large%2520margin%252C%2520especially%2520on%250Asmall%2520datasets%252C%2520as%2520evaluated%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520Tiny-ImageNet%2520and%2520ImageNet%250Afor%2520image%2520classification%252C%2520and%2520COCO%2520for%2520object%2520detection%2520and%2520instance%250Asegmentation.%2520The%2520source%2520code%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/ZTX-100/Efficient_ViT_with_DW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19394v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Wise%20Convolutions%20in%20Vision%20Transformers%20for%20Efficient%20Training%20on%0A%20%20Small%20Datasets&entry.906535625=Tianxiao%20Zhang%20and%20Wenju%20Xu%20and%20Bo%20Luo%20and%20Guanghui%20Wang&entry.1292438233=%20%20The%20Vision%20Transformer%20%28ViT%29%20leverages%20the%20Transformer%27s%20encoder%20to%20capture%0Aglobal%20information%20by%20dividing%20images%20into%20patches%20and%20achieves%20superior%0Aperformance%20across%20various%20computer%20vision%20tasks.%20However%2C%20the%20self-attention%0Amechanism%20of%20ViT%20captures%20the%20global%20context%20from%20the%20outset%2C%20overlooking%20the%0Ainherent%20relationships%20between%20neighboring%20pixels%20in%20images%20or%20videos.%0ATransformers%20mainly%20focus%20on%20global%20information%20while%20ignoring%20the%20fine-grained%0Alocal%20details.%20Consequently%2C%20ViT%20lacks%20inductive%20bias%20during%20image%20or%20video%0Adataset%20training.%20In%20contrast%2C%20convolutional%20neural%20networks%20%28CNNs%29%2C%20with%20their%0Areliance%20on%20local%20filters%2C%20possess%20an%20inherent%20inductive%20bias%2C%20making%20them%20more%0Aefficient%20and%20quicker%20to%20converge%20than%20ViT%20with%20less%20data.%20In%20this%20paper%2C%20we%0Apresent%20a%20lightweight%20Depth-Wise%20Convolution%20module%20as%20a%20shortcut%20in%20ViT%0Amodels%2C%20bypassing%20entire%20Transformer%20blocks%20to%20ensure%20the%20models%20capture%20both%0Alocal%20and%20global%20information%20with%20minimal%20overhead.%20Additionally%2C%20we%20introduce%0Atwo%20architecture%20variants%2C%20allowing%20the%20Depth-Wise%20Convolution%20modules%20to%20be%0Aapplied%20to%20multiple%20Transformer%20blocks%20for%20parameter%20savings%2C%20and%20incorporating%0Aindependent%20parallel%20Depth-Wise%20Convolution%20modules%20with%20different%20kernels%20to%0Aenhance%20the%20acquisition%20of%20local%20information.%20The%20proposed%20approach%0Asignificantly%20boosts%20the%20performance%20of%20ViT%20models%20on%20image%20classification%2C%0Aobject%20detection%20and%20instance%20segmentation%20by%20a%20large%20margin%2C%20especially%20on%0Asmall%20datasets%2C%20as%20evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20Tiny-ImageNet%20and%20ImageNet%0Afor%20image%20classification%2C%20and%20COCO%20for%20object%20detection%20and%20instance%0Asegmentation.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/ZTX-100/Efficient_ViT_with_DW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19394v3&entry.124074799=Read"},
{"title": "XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image\n  Pre-Training", "author": "Biao Wu and Yutong Xie and Zeyu Zhang and Minh Hieu Phan and Qi Chen and Ling Chen and Qi Wu", "abstract": "  Vision-and-language pretraining (VLP) in the medical field utilizes\ncontrastive learning on image-text pairs to achieve effective transfer across\ntasks. Yet, current VLP approaches with the masked modelling strategy face two\nchallenges when applied to the medical domain. First, current models struggle\nto accurately reconstruct key pathological features due to the scarcity of\nmedical data. Second, most methods only adopt either paired image-text or\nimage-only data, failing to exploit the combination of both paired and unpaired\ndata. To this end, this paper proposes a XLIP (Masked modelling for medical\nLanguage-Image Pre-training) framework to enhance pathological learning and\nfeature learning via unpaired data. First, we introduce the attention-masked\nimage modelling (AttMIM) and entity-driven masked language modelling module\n(EntMLM), which learns to reconstruct pathological visual and textual tokens\nvia multi-modal feature interaction, thus improving medical-enhanced features.\nThe AttMIM module masks a portion of the image features that are highly\nresponsive to textual features. This allows XLIP to improve the reconstruction\nof highly similar image data in medicine efficiency. Second, our XLIP\ncapitalizes unpaired data to enhance multimodal learning by introducing\ndisease-kind prompts. The experimental results show that XLIP achieves SOTA for\nzero-shot and fine-tuning classification performance on five datasets. Our code\nwill be available at https://github.com/White65534/XLIP\n", "link": "http://arxiv.org/abs/2407.19546v2", "date": "2024-08-02", "relevancy": 1.7206, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.533}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XLIP%3A%20Cross-modal%20Attention%20Masked%20Modelling%20for%20Medical%20Language-Image%0A%20%20Pre-Training&body=Title%3A%20XLIP%3A%20Cross-modal%20Attention%20Masked%20Modelling%20for%20Medical%20Language-Image%0A%20%20Pre-Training%0AAuthor%3A%20Biao%20Wu%20and%20Yutong%20Xie%20and%20Zeyu%20Zhang%20and%20Minh%20Hieu%20Phan%20and%20Qi%20Chen%20and%20Ling%20Chen%20and%20Qi%20Wu%0AAbstract%3A%20%20%20Vision-and-language%20pretraining%20%28VLP%29%20in%20the%20medical%20field%20utilizes%0Acontrastive%20learning%20on%20image-text%20pairs%20to%20achieve%20effective%20transfer%20across%0Atasks.%20Yet%2C%20current%20VLP%20approaches%20with%20the%20masked%20modelling%20strategy%20face%20two%0Achallenges%20when%20applied%20to%20the%20medical%20domain.%20First%2C%20current%20models%20struggle%0Ato%20accurately%20reconstruct%20key%20pathological%20features%20due%20to%20the%20scarcity%20of%0Amedical%20data.%20Second%2C%20most%20methods%20only%20adopt%20either%20paired%20image-text%20or%0Aimage-only%20data%2C%20failing%20to%20exploit%20the%20combination%20of%20both%20paired%20and%20unpaired%0Adata.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20XLIP%20%28Masked%20modelling%20for%20medical%0ALanguage-Image%20Pre-training%29%20framework%20to%20enhance%20pathological%20learning%20and%0Afeature%20learning%20via%20unpaired%20data.%20First%2C%20we%20introduce%20the%20attention-masked%0Aimage%20modelling%20%28AttMIM%29%20and%20entity-driven%20masked%20language%20modelling%20module%0A%28EntMLM%29%2C%20which%20learns%20to%20reconstruct%20pathological%20visual%20and%20textual%20tokens%0Avia%20multi-modal%20feature%20interaction%2C%20thus%20improving%20medical-enhanced%20features.%0AThe%20AttMIM%20module%20masks%20a%20portion%20of%20the%20image%20features%20that%20are%20highly%0Aresponsive%20to%20textual%20features.%20This%20allows%20XLIP%20to%20improve%20the%20reconstruction%0Aof%20highly%20similar%20image%20data%20in%20medicine%20efficiency.%20Second%2C%20our%20XLIP%0Acapitalizes%20unpaired%20data%20to%20enhance%20multimodal%20learning%20by%20introducing%0Adisease-kind%20prompts.%20The%20experimental%20results%20show%20that%20XLIP%20achieves%20SOTA%20for%0Azero-shot%20and%20fine-tuning%20classification%20performance%20on%20five%20datasets.%20Our%20code%0Awill%20be%20available%20at%20https%3A//github.com/White65534/XLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXLIP%253A%2520Cross-modal%2520Attention%2520Masked%2520Modelling%2520for%2520Medical%2520Language-Image%250A%2520%2520Pre-Training%26entry.906535625%3DBiao%2520Wu%2520and%2520Yutong%2520Xie%2520and%2520Zeyu%2520Zhang%2520and%2520Minh%2520Hieu%2520Phan%2520and%2520Qi%2520Chen%2520and%2520Ling%2520Chen%2520and%2520Qi%2520Wu%26entry.1292438233%3D%2520%2520Vision-and-language%2520pretraining%2520%2528VLP%2529%2520in%2520the%2520medical%2520field%2520utilizes%250Acontrastive%2520learning%2520on%2520image-text%2520pairs%2520to%2520achieve%2520effective%2520transfer%2520across%250Atasks.%2520Yet%252C%2520current%2520VLP%2520approaches%2520with%2520the%2520masked%2520modelling%2520strategy%2520face%2520two%250Achallenges%2520when%2520applied%2520to%2520the%2520medical%2520domain.%2520First%252C%2520current%2520models%2520struggle%250Ato%2520accurately%2520reconstruct%2520key%2520pathological%2520features%2520due%2520to%2520the%2520scarcity%2520of%250Amedical%2520data.%2520Second%252C%2520most%2520methods%2520only%2520adopt%2520either%2520paired%2520image-text%2520or%250Aimage-only%2520data%252C%2520failing%2520to%2520exploit%2520the%2520combination%2520of%2520both%2520paired%2520and%2520unpaired%250Adata.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520XLIP%2520%2528Masked%2520modelling%2520for%2520medical%250ALanguage-Image%2520Pre-training%2529%2520framework%2520to%2520enhance%2520pathological%2520learning%2520and%250Afeature%2520learning%2520via%2520unpaired%2520data.%2520First%252C%2520we%2520introduce%2520the%2520attention-masked%250Aimage%2520modelling%2520%2528AttMIM%2529%2520and%2520entity-driven%2520masked%2520language%2520modelling%2520module%250A%2528EntMLM%2529%252C%2520which%2520learns%2520to%2520reconstruct%2520pathological%2520visual%2520and%2520textual%2520tokens%250Avia%2520multi-modal%2520feature%2520interaction%252C%2520thus%2520improving%2520medical-enhanced%2520features.%250AThe%2520AttMIM%2520module%2520masks%2520a%2520portion%2520of%2520the%2520image%2520features%2520that%2520are%2520highly%250Aresponsive%2520to%2520textual%2520features.%2520This%2520allows%2520XLIP%2520to%2520improve%2520the%2520reconstruction%250Aof%2520highly%2520similar%2520image%2520data%2520in%2520medicine%2520efficiency.%2520Second%252C%2520our%2520XLIP%250Acapitalizes%2520unpaired%2520data%2520to%2520enhance%2520multimodal%2520learning%2520by%2520introducing%250Adisease-kind%2520prompts.%2520The%2520experimental%2520results%2520show%2520that%2520XLIP%2520achieves%2520SOTA%2520for%250Azero-shot%2520and%2520fine-tuning%2520classification%2520performance%2520on%2520five%2520datasets.%2520Our%2520code%250Awill%2520be%2520available%2520at%2520https%253A//github.com/White65534/XLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLIP%3A%20Cross-modal%20Attention%20Masked%20Modelling%20for%20Medical%20Language-Image%0A%20%20Pre-Training&entry.906535625=Biao%20Wu%20and%20Yutong%20Xie%20and%20Zeyu%20Zhang%20and%20Minh%20Hieu%20Phan%20and%20Qi%20Chen%20and%20Ling%20Chen%20and%20Qi%20Wu&entry.1292438233=%20%20Vision-and-language%20pretraining%20%28VLP%29%20in%20the%20medical%20field%20utilizes%0Acontrastive%20learning%20on%20image-text%20pairs%20to%20achieve%20effective%20transfer%20across%0Atasks.%20Yet%2C%20current%20VLP%20approaches%20with%20the%20masked%20modelling%20strategy%20face%20two%0Achallenges%20when%20applied%20to%20the%20medical%20domain.%20First%2C%20current%20models%20struggle%0Ato%20accurately%20reconstruct%20key%20pathological%20features%20due%20to%20the%20scarcity%20of%0Amedical%20data.%20Second%2C%20most%20methods%20only%20adopt%20either%20paired%20image-text%20or%0Aimage-only%20data%2C%20failing%20to%20exploit%20the%20combination%20of%20both%20paired%20and%20unpaired%0Adata.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20XLIP%20%28Masked%20modelling%20for%20medical%0ALanguage-Image%20Pre-training%29%20framework%20to%20enhance%20pathological%20learning%20and%0Afeature%20learning%20via%20unpaired%20data.%20First%2C%20we%20introduce%20the%20attention-masked%0Aimage%20modelling%20%28AttMIM%29%20and%20entity-driven%20masked%20language%20modelling%20module%0A%28EntMLM%29%2C%20which%20learns%20to%20reconstruct%20pathological%20visual%20and%20textual%20tokens%0Avia%20multi-modal%20feature%20interaction%2C%20thus%20improving%20medical-enhanced%20features.%0AThe%20AttMIM%20module%20masks%20a%20portion%20of%20the%20image%20features%20that%20are%20highly%0Aresponsive%20to%20textual%20features.%20This%20allows%20XLIP%20to%20improve%20the%20reconstruction%0Aof%20highly%20similar%20image%20data%20in%20medicine%20efficiency.%20Second%2C%20our%20XLIP%0Acapitalizes%20unpaired%20data%20to%20enhance%20multimodal%20learning%20by%20introducing%0Adisease-kind%20prompts.%20The%20experimental%20results%20show%20that%20XLIP%20achieves%20SOTA%20for%0Azero-shot%20and%20fine-tuning%20classification%20performance%20on%20five%20datasets.%20Our%20code%0Awill%20be%20available%20at%20https%3A//github.com/White65534/XLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19546v2&entry.124074799=Read"},
{"title": "Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic\n  Manipulation", "author": "Ruoxuan Feng and Di Hu and Wenke Ma and Xuelong Li", "abstract": "  Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.\n", "link": "http://arxiv.org/abs/2408.01366v1", "date": "2024-08-02", "relevancy": 1.7161, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6223}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Play%20to%20the%20Score%3A%20Stage-Guided%20Dynamic%20Multi-Sensory%20Fusion%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20Play%20to%20the%20Score%3A%20Stage-Guided%20Dynamic%20Multi-Sensory%20Fusion%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Ruoxuan%20Feng%20and%20Di%20Hu%20and%20Wenke%20Ma%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Humans%20possess%20a%20remarkable%20talent%20for%20flexibly%20alternating%20to%20different%0Asenses%20when%20interacting%20with%20the%20environment.%20Picture%20a%20chef%20skillfully%20gauging%0Athe%20timing%20of%20ingredient%20additions%20and%20controlling%20the%20heat%20according%20to%20the%0Acolors%2C%20sounds%2C%20and%20aromas%2C%20seamlessly%20navigating%20through%20every%20stage%20of%20the%0Acomplex%20cooking%20process.%20This%20ability%20is%20founded%20upon%20a%20thorough%20comprehension%0Aof%20task%20stages%2C%20as%20achieving%20the%20sub-goal%20within%20each%20stage%20can%20necessitate%20the%0Autilization%20of%20different%20senses.%20In%20order%20to%20endow%20robots%20with%20similar%20ability%2C%0Awe%20incorporate%20the%20task%20stages%20divided%20by%20sub-goals%20into%20the%20imitation%20learning%0Aprocess%20to%20accordingly%20guide%20dynamic%20multi-sensory%20fusion.%20We%20propose%20MS-Bot%2C%20a%0Astage-guided%20dynamic%20multi-sensory%20fusion%20method%20with%20coarse-to-fine%20stage%0Aunderstanding%2C%20which%20dynamically%20adjusts%20the%20priority%20of%20modalities%20based%20on%0Athe%20fine-grained%20state%20within%20the%20predicted%20current%20stage.%20We%20train%20a%20robot%0Asystem%20equipped%20with%20visual%2C%20auditory%2C%20and%20tactile%20sensors%20to%20accomplish%0Achallenging%20robotic%20manipulation%20tasks%3A%20pouring%20and%20peg%20insertion%20with%20keyway.%0AExperimental%20results%20indicate%20that%20our%20approach%20enables%20more%20effective%20and%0Aexplainable%20dynamic%20fusion%2C%20aligning%20more%20closely%20with%20the%20human%20fusion%20process%0Athan%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlay%2520to%2520the%2520Score%253A%2520Stage-Guided%2520Dynamic%2520Multi-Sensory%2520Fusion%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DRuoxuan%2520Feng%2520and%2520Di%2520Hu%2520and%2520Wenke%2520Ma%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Humans%2520possess%2520a%2520remarkable%2520talent%2520for%2520flexibly%2520alternating%2520to%2520different%250Asenses%2520when%2520interacting%2520with%2520the%2520environment.%2520Picture%2520a%2520chef%2520skillfully%2520gauging%250Athe%2520timing%2520of%2520ingredient%2520additions%2520and%2520controlling%2520the%2520heat%2520according%2520to%2520the%250Acolors%252C%2520sounds%252C%2520and%2520aromas%252C%2520seamlessly%2520navigating%2520through%2520every%2520stage%2520of%2520the%250Acomplex%2520cooking%2520process.%2520This%2520ability%2520is%2520founded%2520upon%2520a%2520thorough%2520comprehension%250Aof%2520task%2520stages%252C%2520as%2520achieving%2520the%2520sub-goal%2520within%2520each%2520stage%2520can%2520necessitate%2520the%250Autilization%2520of%2520different%2520senses.%2520In%2520order%2520to%2520endow%2520robots%2520with%2520similar%2520ability%252C%250Awe%2520incorporate%2520the%2520task%2520stages%2520divided%2520by%2520sub-goals%2520into%2520the%2520imitation%2520learning%250Aprocess%2520to%2520accordingly%2520guide%2520dynamic%2520multi-sensory%2520fusion.%2520We%2520propose%2520MS-Bot%252C%2520a%250Astage-guided%2520dynamic%2520multi-sensory%2520fusion%2520method%2520with%2520coarse-to-fine%2520stage%250Aunderstanding%252C%2520which%2520dynamically%2520adjusts%2520the%2520priority%2520of%2520modalities%2520based%2520on%250Athe%2520fine-grained%2520state%2520within%2520the%2520predicted%2520current%2520stage.%2520We%2520train%2520a%2520robot%250Asystem%2520equipped%2520with%2520visual%252C%2520auditory%252C%2520and%2520tactile%2520sensors%2520to%2520accomplish%250Achallenging%2520robotic%2520manipulation%2520tasks%253A%2520pouring%2520and%2520peg%2520insertion%2520with%2520keyway.%250AExperimental%2520results%2520indicate%2520that%2520our%2520approach%2520enables%2520more%2520effective%2520and%250Aexplainable%2520dynamic%2520fusion%252C%2520aligning%2520more%2520closely%2520with%2520the%2520human%2520fusion%2520process%250Athan%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Play%20to%20the%20Score%3A%20Stage-Guided%20Dynamic%20Multi-Sensory%20Fusion%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Ruoxuan%20Feng%20and%20Di%20Hu%20and%20Wenke%20Ma%20and%20Xuelong%20Li&entry.1292438233=%20%20Humans%20possess%20a%20remarkable%20talent%20for%20flexibly%20alternating%20to%20different%0Asenses%20when%20interacting%20with%20the%20environment.%20Picture%20a%20chef%20skillfully%20gauging%0Athe%20timing%20of%20ingredient%20additions%20and%20controlling%20the%20heat%20according%20to%20the%0Acolors%2C%20sounds%2C%20and%20aromas%2C%20seamlessly%20navigating%20through%20every%20stage%20of%20the%0Acomplex%20cooking%20process.%20This%20ability%20is%20founded%20upon%20a%20thorough%20comprehension%0Aof%20task%20stages%2C%20as%20achieving%20the%20sub-goal%20within%20each%20stage%20can%20necessitate%20the%0Autilization%20of%20different%20senses.%20In%20order%20to%20endow%20robots%20with%20similar%20ability%2C%0Awe%20incorporate%20the%20task%20stages%20divided%20by%20sub-goals%20into%20the%20imitation%20learning%0Aprocess%20to%20accordingly%20guide%20dynamic%20multi-sensory%20fusion.%20We%20propose%20MS-Bot%2C%20a%0Astage-guided%20dynamic%20multi-sensory%20fusion%20method%20with%20coarse-to-fine%20stage%0Aunderstanding%2C%20which%20dynamically%20adjusts%20the%20priority%20of%20modalities%20based%20on%0Athe%20fine-grained%20state%20within%20the%20predicted%20current%20stage.%20We%20train%20a%20robot%0Asystem%20equipped%20with%20visual%2C%20auditory%2C%20and%20tactile%20sensors%20to%20accomplish%0Achallenging%20robotic%20manipulation%20tasks%3A%20pouring%20and%20peg%20insertion%20with%20keyway.%0AExperimental%20results%20indicate%20that%20our%20approach%20enables%20more%20effective%20and%0Aexplainable%20dynamic%20fusion%2C%20aligning%20more%20closely%20with%20the%20human%20fusion%20process%0Athan%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01366v1&entry.124074799=Read"},
{"title": "Actra: Optimized Transformer Architecture for Vision-Language-Action\n  Models in Robot Learning", "author": "Yueen Ma and Dafeng Chi and Shiguang Wu and Yuecheng Liu and Yuzheng Zhuang and Jianye Hao and Irwin King", "abstract": "  Vision-language-action models have gained significant attention for their\nability to model trajectories in robot learning. However, most existing models\nrely on Transformer models with vanilla causal attention, which we find\nsuboptimal for processing segmented multi-modal sequences. Additionally, the\nautoregressive generation approach falls short in generating multi-dimensional\nactions. In this paper, we introduce Actra, an optimized Transformer\narchitecture featuring trajectory attention and learnable action queries,\ndesigned for effective encoding and decoding of segmented\nvision-language-action trajectories in robot imitation learning. Furthermore,\nwe devise a multi-modal contrastive learning objective to explicitly align\ndifferent modalities, complementing the primary behavior cloning objective.\nThrough extensive experiments conducted across various environments, Actra\nexhibits substantial performance improvement when compared to state-of-the-art\nmodels in terms of generalizability, dexterity, and precision.\n", "link": "http://arxiv.org/abs/2408.01147v1", "date": "2024-08-02", "relevancy": 1.7114, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.585}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5723}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Actra%3A%20Optimized%20Transformer%20Architecture%20for%20Vision-Language-Action%0A%20%20Models%20in%20Robot%20Learning&body=Title%3A%20Actra%3A%20Optimized%20Transformer%20Architecture%20for%20Vision-Language-Action%0A%20%20Models%20in%20Robot%20Learning%0AAuthor%3A%20Yueen%20Ma%20and%20Dafeng%20Chi%20and%20Shiguang%20Wu%20and%20Yuecheng%20Liu%20and%20Yuzheng%20Zhuang%20and%20Jianye%20Hao%20and%20Irwin%20King%0AAbstract%3A%20%20%20Vision-language-action%20models%20have%20gained%20significant%20attention%20for%20their%0Aability%20to%20model%20trajectories%20in%20robot%20learning.%20However%2C%20most%20existing%20models%0Arely%20on%20Transformer%20models%20with%20vanilla%20causal%20attention%2C%20which%20we%20find%0Asuboptimal%20for%20processing%20segmented%20multi-modal%20sequences.%20Additionally%2C%20the%0Aautoregressive%20generation%20approach%20falls%20short%20in%20generating%20multi-dimensional%0Aactions.%20In%20this%20paper%2C%20we%20introduce%20Actra%2C%20an%20optimized%20Transformer%0Aarchitecture%20featuring%20trajectory%20attention%20and%20learnable%20action%20queries%2C%0Adesigned%20for%20effective%20encoding%20and%20decoding%20of%20segmented%0Avision-language-action%20trajectories%20in%20robot%20imitation%20learning.%20Furthermore%2C%0Awe%20devise%20a%20multi-modal%20contrastive%20learning%20objective%20to%20explicitly%20align%0Adifferent%20modalities%2C%20complementing%20the%20primary%20behavior%20cloning%20objective.%0AThrough%20extensive%20experiments%20conducted%20across%20various%20environments%2C%20Actra%0Aexhibits%20substantial%20performance%20improvement%20when%20compared%20to%20state-of-the-art%0Amodels%20in%20terms%20of%20generalizability%2C%20dexterity%2C%20and%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActra%253A%2520Optimized%2520Transformer%2520Architecture%2520for%2520Vision-Language-Action%250A%2520%2520Models%2520in%2520Robot%2520Learning%26entry.906535625%3DYueen%2520Ma%2520and%2520Dafeng%2520Chi%2520and%2520Shiguang%2520Wu%2520and%2520Yuecheng%2520Liu%2520and%2520Yuzheng%2520Zhuang%2520and%2520Jianye%2520Hao%2520and%2520Irwin%2520King%26entry.1292438233%3D%2520%2520Vision-language-action%2520models%2520have%2520gained%2520significant%2520attention%2520for%2520their%250Aability%2520to%2520model%2520trajectories%2520in%2520robot%2520learning.%2520However%252C%2520most%2520existing%2520models%250Arely%2520on%2520Transformer%2520models%2520with%2520vanilla%2520causal%2520attention%252C%2520which%2520we%2520find%250Asuboptimal%2520for%2520processing%2520segmented%2520multi-modal%2520sequences.%2520Additionally%252C%2520the%250Aautoregressive%2520generation%2520approach%2520falls%2520short%2520in%2520generating%2520multi-dimensional%250Aactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Actra%252C%2520an%2520optimized%2520Transformer%250Aarchitecture%2520featuring%2520trajectory%2520attention%2520and%2520learnable%2520action%2520queries%252C%250Adesigned%2520for%2520effective%2520encoding%2520and%2520decoding%2520of%2520segmented%250Avision-language-action%2520trajectories%2520in%2520robot%2520imitation%2520learning.%2520Furthermore%252C%250Awe%2520devise%2520a%2520multi-modal%2520contrastive%2520learning%2520objective%2520to%2520explicitly%2520align%250Adifferent%2520modalities%252C%2520complementing%2520the%2520primary%2520behavior%2520cloning%2520objective.%250AThrough%2520extensive%2520experiments%2520conducted%2520across%2520various%2520environments%252C%2520Actra%250Aexhibits%2520substantial%2520performance%2520improvement%2520when%2520compared%2520to%2520state-of-the-art%250Amodels%2520in%2520terms%2520of%2520generalizability%252C%2520dexterity%252C%2520and%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actra%3A%20Optimized%20Transformer%20Architecture%20for%20Vision-Language-Action%0A%20%20Models%20in%20Robot%20Learning&entry.906535625=Yueen%20Ma%20and%20Dafeng%20Chi%20and%20Shiguang%20Wu%20and%20Yuecheng%20Liu%20and%20Yuzheng%20Zhuang%20and%20Jianye%20Hao%20and%20Irwin%20King&entry.1292438233=%20%20Vision-language-action%20models%20have%20gained%20significant%20attention%20for%20their%0Aability%20to%20model%20trajectories%20in%20robot%20learning.%20However%2C%20most%20existing%20models%0Arely%20on%20Transformer%20models%20with%20vanilla%20causal%20attention%2C%20which%20we%20find%0Asuboptimal%20for%20processing%20segmented%20multi-modal%20sequences.%20Additionally%2C%20the%0Aautoregressive%20generation%20approach%20falls%20short%20in%20generating%20multi-dimensional%0Aactions.%20In%20this%20paper%2C%20we%20introduce%20Actra%2C%20an%20optimized%20Transformer%0Aarchitecture%20featuring%20trajectory%20attention%20and%20learnable%20action%20queries%2C%0Adesigned%20for%20effective%20encoding%20and%20decoding%20of%20segmented%0Avision-language-action%20trajectories%20in%20robot%20imitation%20learning.%20Furthermore%2C%0Awe%20devise%20a%20multi-modal%20contrastive%20learning%20objective%20to%20explicitly%20align%0Adifferent%20modalities%2C%20complementing%20the%20primary%20behavior%20cloning%20objective.%0AThrough%20extensive%20experiments%20conducted%20across%20various%20environments%2C%20Actra%0Aexhibits%20substantial%20performance%20improvement%20when%20compared%20to%20state-of-the-art%0Amodels%20in%20terms%20of%20generalizability%2C%20dexterity%2C%20and%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01147v1&entry.124074799=Read"},
{"title": "StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation", "author": "Bingyu Li and Da Zhang and Zhiyuan Zhao and Junyu Gao and Xuelong Li", "abstract": "  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n", "link": "http://arxiv.org/abs/2408.01343v1", "date": "2024-08-02", "relevancy": 1.707, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5961}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5616}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StitchFusion%3A%20Weaving%20Any%20Visual%20Modalities%20to%20Enhance%20Multimodal%0A%20%20Semantic%20Segmentation&body=Title%3A%20StitchFusion%3A%20Weaving%20Any%20Visual%20Modalities%20to%20Enhance%20Multimodal%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Bingyu%20Li%20and%20Da%20Zhang%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Multimodal%20semantic%20segmentation%20shows%20significant%20potential%20for%20enhancing%0Asegmentation%20accuracy%20in%20complex%20scenes.%20However%2C%20current%20methods%20often%0Aincorporate%20specialized%20feature%20fusion%20modules%20tailored%20to%20specific%20modalities%2C%0Athereby%20restricting%20input%20flexibility%20and%20increasing%20the%20number%20of%20training%0Aparameters.%20To%20address%20these%20challenges%2C%20we%20propose%20StitchFusion%2C%20a%0Astraightforward%20yet%20effective%20modal%20fusion%20framework%20that%20integrates%0Alarge-scale%20pre-trained%20models%20directly%20as%20encoders%20and%20feature%20fusers.%20This%0Aapproach%20facilitates%20comprehensive%20multi-modal%20and%20multi-scale%20feature%20fusion%2C%0Aaccommodating%20any%20visual%20modal%20inputs.%20Specifically%2C%20Our%20framework%20achieves%0Amodal%20integration%20during%20encoding%20by%20sharing%20multi-modal%20visual%20information.%20To%0Aenhance%20information%20exchange%20across%20modalities%2C%20we%20introduce%20a%0Amulti-directional%20adapter%20module%20%28MultiAdapter%29%20to%20enable%20cross-modal%0Ainformation%20transfer%20during%20encoding.%20By%20leveraging%20MultiAdapter%20to%20propagate%0Amulti-scale%20information%20across%20pre-trained%20encoders%20during%20the%20encoding%0Aprocess%2C%20StitchFusion%20achieves%20multi-modal%20visual%20information%20integration%0Aduring%20encoding.%20Extensive%20comparative%20experiments%20demonstrate%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20on%20four%20multi-modal%20segmentation%20datasets%0Awith%20minimal%20additional%20parameters.%20Furthermore%2C%20the%20experimental%20integration%0Aof%20MultiAdapter%20with%20existing%20Feature%20Fusion%20Modules%20%28FFMs%29%20highlights%20their%0Acomplementary%20nature.%20Our%20code%20is%20available%20at%20StitchFusion_repo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStitchFusion%253A%2520Weaving%2520Any%2520Visual%2520Modalities%2520to%2520Enhance%2520Multimodal%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DBingyu%2520Li%2520and%2520Da%2520Zhang%2520and%2520Zhiyuan%2520Zhao%2520and%2520Junyu%2520Gao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520semantic%2520segmentation%2520shows%2520significant%2520potential%2520for%2520enhancing%250Asegmentation%2520accuracy%2520in%2520complex%2520scenes.%2520However%252C%2520current%2520methods%2520often%250Aincorporate%2520specialized%2520feature%2520fusion%2520modules%2520tailored%2520to%2520specific%2520modalities%252C%250Athereby%2520restricting%2520input%2520flexibility%2520and%2520increasing%2520the%2520number%2520of%2520training%250Aparameters.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520StitchFusion%252C%2520a%250Astraightforward%2520yet%2520effective%2520modal%2520fusion%2520framework%2520that%2520integrates%250Alarge-scale%2520pre-trained%2520models%2520directly%2520as%2520encoders%2520and%2520feature%2520fusers.%2520This%250Aapproach%2520facilitates%2520comprehensive%2520multi-modal%2520and%2520multi-scale%2520feature%2520fusion%252C%250Aaccommodating%2520any%2520visual%2520modal%2520inputs.%2520Specifically%252C%2520Our%2520framework%2520achieves%250Amodal%2520integration%2520during%2520encoding%2520by%2520sharing%2520multi-modal%2520visual%2520information.%2520To%250Aenhance%2520information%2520exchange%2520across%2520modalities%252C%2520we%2520introduce%2520a%250Amulti-directional%2520adapter%2520module%2520%2528MultiAdapter%2529%2520to%2520enable%2520cross-modal%250Ainformation%2520transfer%2520during%2520encoding.%2520By%2520leveraging%2520MultiAdapter%2520to%2520propagate%250Amulti-scale%2520information%2520across%2520pre-trained%2520encoders%2520during%2520the%2520encoding%250Aprocess%252C%2520StitchFusion%2520achieves%2520multi-modal%2520visual%2520information%2520integration%250Aduring%2520encoding.%2520Extensive%2520comparative%2520experiments%2520demonstrate%2520that%2520our%2520model%250Aachieves%2520state-of-the-art%2520performance%2520on%2520four%2520multi-modal%2520segmentation%2520datasets%250Awith%2520minimal%2520additional%2520parameters.%2520Furthermore%252C%2520the%2520experimental%2520integration%250Aof%2520MultiAdapter%2520with%2520existing%2520Feature%2520Fusion%2520Modules%2520%2528FFMs%2529%2520highlights%2520their%250Acomplementary%2520nature.%2520Our%2520code%2520is%2520available%2520at%2520StitchFusion_repo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StitchFusion%3A%20Weaving%20Any%20Visual%20Modalities%20to%20Enhance%20Multimodal%0A%20%20Semantic%20Segmentation&entry.906535625=Bingyu%20Li%20and%20Da%20Zhang%20and%20Zhiyuan%20Zhao%20and%20Junyu%20Gao%20and%20Xuelong%20Li&entry.1292438233=%20%20Multimodal%20semantic%20segmentation%20shows%20significant%20potential%20for%20enhancing%0Asegmentation%20accuracy%20in%20complex%20scenes.%20However%2C%20current%20methods%20often%0Aincorporate%20specialized%20feature%20fusion%20modules%20tailored%20to%20specific%20modalities%2C%0Athereby%20restricting%20input%20flexibility%20and%20increasing%20the%20number%20of%20training%0Aparameters.%20To%20address%20these%20challenges%2C%20we%20propose%20StitchFusion%2C%20a%0Astraightforward%20yet%20effective%20modal%20fusion%20framework%20that%20integrates%0Alarge-scale%20pre-trained%20models%20directly%20as%20encoders%20and%20feature%20fusers.%20This%0Aapproach%20facilitates%20comprehensive%20multi-modal%20and%20multi-scale%20feature%20fusion%2C%0Aaccommodating%20any%20visual%20modal%20inputs.%20Specifically%2C%20Our%20framework%20achieves%0Amodal%20integration%20during%20encoding%20by%20sharing%20multi-modal%20visual%20information.%20To%0Aenhance%20information%20exchange%20across%20modalities%2C%20we%20introduce%20a%0Amulti-directional%20adapter%20module%20%28MultiAdapter%29%20to%20enable%20cross-modal%0Ainformation%20transfer%20during%20encoding.%20By%20leveraging%20MultiAdapter%20to%20propagate%0Amulti-scale%20information%20across%20pre-trained%20encoders%20during%20the%20encoding%0Aprocess%2C%20StitchFusion%20achieves%20multi-modal%20visual%20information%20integration%0Aduring%20encoding.%20Extensive%20comparative%20experiments%20demonstrate%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20on%20four%20multi-modal%20segmentation%20datasets%0Awith%20minimal%20additional%20parameters.%20Furthermore%2C%20the%20experimental%20integration%0Aof%20MultiAdapter%20with%20existing%20Feature%20Fusion%20Modules%20%28FFMs%29%20highlights%20their%0Acomplementary%20nature.%20Our%20code%20is%20available%20at%20StitchFusion_repo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01343v1&entry.124074799=Read"},
{"title": "Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and\n  Contaminated by Outliers", "author": "Takeyuki Sasai and Hironori Fujisawa", "abstract": "  We investigate a problem estimating coefficients of linear regression under\nsparsity assumption when covariates and noises are sampled from heavy tailed\ndistributions. Additionally, we consider the situation where not only\ncovariates and noises are sampled from heavy tailed distributions but also\ncontaminated by outliers. Our estimators can be computed efficiently, and\nexhibit sharp error bounds.\n", "link": "http://arxiv.org/abs/2408.01336v1", "date": "2024-08-02", "relevancy": 1.6923, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4471}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4199}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Linear%20Regression%20when%20Noises%20and%20Covariates%20are%20Heavy-Tailed%20and%0A%20%20Contaminated%20by%20Outliers&body=Title%3A%20Sparse%20Linear%20Regression%20when%20Noises%20and%20Covariates%20are%20Heavy-Tailed%20and%0A%20%20Contaminated%20by%20Outliers%0AAuthor%3A%20Takeyuki%20Sasai%20and%20Hironori%20Fujisawa%0AAbstract%3A%20%20%20We%20investigate%20a%20problem%20estimating%20coefficients%20of%20linear%20regression%20under%0Asparsity%20assumption%20when%20covariates%20and%20noises%20are%20sampled%20from%20heavy%20tailed%0Adistributions.%20Additionally%2C%20we%20consider%20the%20situation%20where%20not%20only%0Acovariates%20and%20noises%20are%20sampled%20from%20heavy%20tailed%20distributions%20but%20also%0Acontaminated%20by%20outliers.%20Our%20estimators%20can%20be%20computed%20efficiently%2C%20and%0Aexhibit%20sharp%20error%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Linear%2520Regression%2520when%2520Noises%2520and%2520Covariates%2520are%2520Heavy-Tailed%2520and%250A%2520%2520Contaminated%2520by%2520Outliers%26entry.906535625%3DTakeyuki%2520Sasai%2520and%2520Hironori%2520Fujisawa%26entry.1292438233%3D%2520%2520We%2520investigate%2520a%2520problem%2520estimating%2520coefficients%2520of%2520linear%2520regression%2520under%250Asparsity%2520assumption%2520when%2520covariates%2520and%2520noises%2520are%2520sampled%2520from%2520heavy%2520tailed%250Adistributions.%2520Additionally%252C%2520we%2520consider%2520the%2520situation%2520where%2520not%2520only%250Acovariates%2520and%2520noises%2520are%2520sampled%2520from%2520heavy%2520tailed%2520distributions%2520but%2520also%250Acontaminated%2520by%2520outliers.%2520Our%2520estimators%2520can%2520be%2520computed%2520efficiently%252C%2520and%250Aexhibit%2520sharp%2520error%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Linear%20Regression%20when%20Noises%20and%20Covariates%20are%20Heavy-Tailed%20and%0A%20%20Contaminated%20by%20Outliers&entry.906535625=Takeyuki%20Sasai%20and%20Hironori%20Fujisawa&entry.1292438233=%20%20We%20investigate%20a%20problem%20estimating%20coefficients%20of%20linear%20regression%20under%0Asparsity%20assumption%20when%20covariates%20and%20noises%20are%20sampled%20from%20heavy%20tailed%0Adistributions.%20Additionally%2C%20we%20consider%20the%20situation%20where%20not%20only%0Acovariates%20and%20noises%20are%20sampled%20from%20heavy%20tailed%20distributions%20but%20also%0Acontaminated%20by%20outliers.%20Our%20estimators%20can%20be%20computed%20efficiently%2C%20and%0Aexhibit%20sharp%20error%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01336v1&entry.124074799=Read"},
{"title": "Accurate and Efficient Event-based Semantic Segmentation Using Adaptive\n  Spiking Encoder-Decoder Network", "author": "Rui Zhang and Luziwei Leng and Kaiwei Che and Hu Zhang and Jie Cheng and Qinghai Guo and Jiangxing Liao and Ran Cheng", "abstract": "  Spiking neural networks (SNNs), known for their low-power, event-driven\ncomputation and intrinsic temporal dynamics, are emerging as promising\nsolutions for processing dynamic, asynchronous signals from event-based\nsensors. Despite their potential, SNNs face challenges in training and\narchitectural design, resulting in limited performance in challenging\nevent-based dense prediction tasks compared to artificial neural networks\n(ANNs). In this work, we develop an efficient spiking encoder-decoder network\n(SpikingEDN) for large-scale event-based semantic segmentation tasks. To\nenhance the learning efficiency from dynamic event streams, we harness the\nadaptive threshold which improves network accuracy, sparsity and robustness in\nstreaming inference. Moreover, we develop a dual-path Spiking\nSpatially-Adaptive Modulation module, which is specifically tailored to enhance\nthe representation of sparse events and multi-modal inputs, thereby\nconsiderably improving network performance. Our SpikingEDN attains a mean\nintersection over union (MIoU) of 72.57\\% on the DDD17 dataset and 58.32\\% on\nthe larger DSEC-Semantic dataset, showing competitive results to the\nstate-of-the-art ANNs while requiring substantially fewer computational\nresources. Our results shed light on the untapped potential of SNNs in\nevent-based vision applications. The source code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2304.11857v3", "date": "2024-08-02", "relevancy": 1.679, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5871}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20and%20Efficient%20Event-based%20Semantic%20Segmentation%20Using%20Adaptive%0A%20%20Spiking%20Encoder-Decoder%20Network&body=Title%3A%20Accurate%20and%20Efficient%20Event-based%20Semantic%20Segmentation%20Using%20Adaptive%0A%20%20Spiking%20Encoder-Decoder%20Network%0AAuthor%3A%20Rui%20Zhang%20and%20Luziwei%20Leng%20and%20Kaiwei%20Che%20and%20Hu%20Zhang%20and%20Jie%20Cheng%20and%20Qinghai%20Guo%20and%20Jiangxing%20Liao%20and%20Ran%20Cheng%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%2C%20known%20for%20their%20low-power%2C%20event-driven%0Acomputation%20and%20intrinsic%20temporal%20dynamics%2C%20are%20emerging%20as%20promising%0Asolutions%20for%20processing%20dynamic%2C%20asynchronous%20signals%20from%20event-based%0Asensors.%20Despite%20their%20potential%2C%20SNNs%20face%20challenges%20in%20training%20and%0Aarchitectural%20design%2C%20resulting%20in%20limited%20performance%20in%20challenging%0Aevent-based%20dense%20prediction%20tasks%20compared%20to%20artificial%20neural%20networks%0A%28ANNs%29.%20In%20this%20work%2C%20we%20develop%20an%20efficient%20spiking%20encoder-decoder%20network%0A%28SpikingEDN%29%20for%20large-scale%20event-based%20semantic%20segmentation%20tasks.%20To%0Aenhance%20the%20learning%20efficiency%20from%20dynamic%20event%20streams%2C%20we%20harness%20the%0Aadaptive%20threshold%20which%20improves%20network%20accuracy%2C%20sparsity%20and%20robustness%20in%0Astreaming%20inference.%20Moreover%2C%20we%20develop%20a%20dual-path%20Spiking%0ASpatially-Adaptive%20Modulation%20module%2C%20which%20is%20specifically%20tailored%20to%20enhance%0Athe%20representation%20of%20sparse%20events%20and%20multi-modal%20inputs%2C%20thereby%0Aconsiderably%20improving%20network%20performance.%20Our%20SpikingEDN%20attains%20a%20mean%0Aintersection%20over%20union%20%28MIoU%29%20of%2072.57%5C%25%20on%20the%20DDD17%20dataset%20and%2058.32%5C%25%20on%0Athe%20larger%20DSEC-Semantic%20dataset%2C%20showing%20competitive%20results%20to%20the%0Astate-of-the-art%20ANNs%20while%20requiring%20substantially%20fewer%20computational%0Aresources.%20Our%20results%20shed%20light%20on%20the%20untapped%20potential%20of%20SNNs%20in%0Aevent-based%20vision%20applications.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.11857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520and%2520Efficient%2520Event-based%2520Semantic%2520Segmentation%2520Using%2520Adaptive%250A%2520%2520Spiking%2520Encoder-Decoder%2520Network%26entry.906535625%3DRui%2520Zhang%2520and%2520Luziwei%2520Leng%2520and%2520Kaiwei%2520Che%2520and%2520Hu%2520Zhang%2520and%2520Jie%2520Cheng%2520and%2520Qinghai%2520Guo%2520and%2520Jiangxing%2520Liao%2520and%2520Ran%2520Cheng%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520known%2520for%2520their%2520low-power%252C%2520event-driven%250Acomputation%2520and%2520intrinsic%2520temporal%2520dynamics%252C%2520are%2520emerging%2520as%2520promising%250Asolutions%2520for%2520processing%2520dynamic%252C%2520asynchronous%2520signals%2520from%2520event-based%250Asensors.%2520Despite%2520their%2520potential%252C%2520SNNs%2520face%2520challenges%2520in%2520training%2520and%250Aarchitectural%2520design%252C%2520resulting%2520in%2520limited%2520performance%2520in%2520challenging%250Aevent-based%2520dense%2520prediction%2520tasks%2520compared%2520to%2520artificial%2520neural%2520networks%250A%2528ANNs%2529.%2520In%2520this%2520work%252C%2520we%2520develop%2520an%2520efficient%2520spiking%2520encoder-decoder%2520network%250A%2528SpikingEDN%2529%2520for%2520large-scale%2520event-based%2520semantic%2520segmentation%2520tasks.%2520To%250Aenhance%2520the%2520learning%2520efficiency%2520from%2520dynamic%2520event%2520streams%252C%2520we%2520harness%2520the%250Aadaptive%2520threshold%2520which%2520improves%2520network%2520accuracy%252C%2520sparsity%2520and%2520robustness%2520in%250Astreaming%2520inference.%2520Moreover%252C%2520we%2520develop%2520a%2520dual-path%2520Spiking%250ASpatially-Adaptive%2520Modulation%2520module%252C%2520which%2520is%2520specifically%2520tailored%2520to%2520enhance%250Athe%2520representation%2520of%2520sparse%2520events%2520and%2520multi-modal%2520inputs%252C%2520thereby%250Aconsiderably%2520improving%2520network%2520performance.%2520Our%2520SpikingEDN%2520attains%2520a%2520mean%250Aintersection%2520over%2520union%2520%2528MIoU%2529%2520of%252072.57%255C%2525%2520on%2520the%2520DDD17%2520dataset%2520and%252058.32%255C%2525%2520on%250Athe%2520larger%2520DSEC-Semantic%2520dataset%252C%2520showing%2520competitive%2520results%2520to%2520the%250Astate-of-the-art%2520ANNs%2520while%2520requiring%2520substantially%2520fewer%2520computational%250Aresources.%2520Our%2520results%2520shed%2520light%2520on%2520the%2520untapped%2520potential%2520of%2520SNNs%2520in%250Aevent-based%2520vision%2520applications.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.11857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20and%20Efficient%20Event-based%20Semantic%20Segmentation%20Using%20Adaptive%0A%20%20Spiking%20Encoder-Decoder%20Network&entry.906535625=Rui%20Zhang%20and%20Luziwei%20Leng%20and%20Kaiwei%20Che%20and%20Hu%20Zhang%20and%20Jie%20Cheng%20and%20Qinghai%20Guo%20and%20Jiangxing%20Liao%20and%20Ran%20Cheng&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%2C%20known%20for%20their%20low-power%2C%20event-driven%0Acomputation%20and%20intrinsic%20temporal%20dynamics%2C%20are%20emerging%20as%20promising%0Asolutions%20for%20processing%20dynamic%2C%20asynchronous%20signals%20from%20event-based%0Asensors.%20Despite%20their%20potential%2C%20SNNs%20face%20challenges%20in%20training%20and%0Aarchitectural%20design%2C%20resulting%20in%20limited%20performance%20in%20challenging%0Aevent-based%20dense%20prediction%20tasks%20compared%20to%20artificial%20neural%20networks%0A%28ANNs%29.%20In%20this%20work%2C%20we%20develop%20an%20efficient%20spiking%20encoder-decoder%20network%0A%28SpikingEDN%29%20for%20large-scale%20event-based%20semantic%20segmentation%20tasks.%20To%0Aenhance%20the%20learning%20efficiency%20from%20dynamic%20event%20streams%2C%20we%20harness%20the%0Aadaptive%20threshold%20which%20improves%20network%20accuracy%2C%20sparsity%20and%20robustness%20in%0Astreaming%20inference.%20Moreover%2C%20we%20develop%20a%20dual-path%20Spiking%0ASpatially-Adaptive%20Modulation%20module%2C%20which%20is%20specifically%20tailored%20to%20enhance%0Athe%20representation%20of%20sparse%20events%20and%20multi-modal%20inputs%2C%20thereby%0Aconsiderably%20improving%20network%20performance.%20Our%20SpikingEDN%20attains%20a%20mean%0Aintersection%20over%20union%20%28MIoU%29%20of%2072.57%5C%25%20on%20the%20DDD17%20dataset%20and%2058.32%5C%25%20on%0Athe%20larger%20DSEC-Semantic%20dataset%2C%20showing%20competitive%20results%20to%20the%0Astate-of-the-art%20ANNs%20while%20requiring%20substantially%20fewer%20computational%0Aresources.%20Our%20results%20shed%20light%20on%20the%20untapped%20potential%20of%20SNNs%20in%0Aevent-based%20vision%20applications.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.11857v3&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation on Event Reasoning of Large Language Models", "author": "Zhengwei Tao and Zhi Jin and Yifan Zhang and Xiancai Chen and Haiyan Zhao and Jia Li and Bing Liang and Chongyang Tao and Qun Liu and Kam-Fai Wong", "abstract": "  Event reasoning is a fundamental ability that underlies many applications. It\nrequires event schema knowledge to perform global reasoning and needs to deal\nwith the diversity of the inter-event relations and the reasoning paradigms.\nHow well LLMs accomplish event reasoning on various relations and reasoning\nparadigms remains unknown. To mitigate this disparity, we comprehensively\nevaluate the abilities of event reasoning of LLMs. We introduce a novel\nbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of\nevaluation of schema and instance and is comprehensive in relations and\nreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs\nhave abilities to accomplish event reasoning but their performances are far\nfrom satisfactory. We also notice the imbalance of event reasoning abilities in\nLLMs. Besides, LLMs have event schema knowledge, however, they're not aligned\nwith humans on how to utilize the knowledge. Based on these findings, we guide\nthe LLMs in utilizing the event schema knowledge as memory leading to\nimprovements on event reasoning.\n", "link": "http://arxiv.org/abs/2404.17513v2", "date": "2024-08-02", "relevancy": 0.9038, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.461}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4474}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20on%20Event%20Reasoning%20of%20Large%20Language%20Models&body=Title%3A%20A%20Comprehensive%20Evaluation%20on%20Event%20Reasoning%20of%20Large%20Language%20Models%0AAuthor%3A%20Zhengwei%20Tao%20and%20Zhi%20Jin%20and%20Yifan%20Zhang%20and%20Xiancai%20Chen%20and%20Haiyan%20Zhao%20and%20Jia%20Li%20and%20Bing%20Liang%20and%20Chongyang%20Tao%20and%20Qun%20Liu%20and%20Kam-Fai%20Wong%0AAbstract%3A%20%20%20Event%20reasoning%20is%20a%20fundamental%20ability%20that%20underlies%20many%20applications.%20It%0Arequires%20event%20schema%20knowledge%20to%20perform%20global%20reasoning%20and%20needs%20to%20deal%0Awith%20the%20diversity%20of%20the%20inter-event%20relations%20and%20the%20reasoning%20paradigms.%0AHow%20well%20LLMs%20accomplish%20event%20reasoning%20on%20various%20relations%20and%20reasoning%0Aparadigms%20remains%20unknown.%20To%20mitigate%20this%20disparity%2C%20we%20comprehensively%0Aevaluate%20the%20abilities%20of%20event%20reasoning%20of%20LLMs.%20We%20introduce%20a%20novel%0Abenchmark%20EV2%20for%20EValuation%20of%20EVent%20reasoning.%20EV2%20consists%20of%20two%20levels%20of%0Aevaluation%20of%20schema%20and%20instance%20and%20is%20comprehensive%20in%20relations%20and%0Areasoning%20paradigms.%20We%20conduct%20extensive%20experiments%20on%20EV2.%20We%20find%20that%20LLMs%0Ahave%20abilities%20to%20accomplish%20event%20reasoning%20but%20their%20performances%20are%20far%0Afrom%20satisfactory.%20We%20also%20notice%20the%20imbalance%20of%20event%20reasoning%20abilities%20in%0ALLMs.%20Besides%2C%20LLMs%20have%20event%20schema%20knowledge%2C%20however%2C%20they%27re%20not%20aligned%0Awith%20humans%20on%20how%20to%20utilize%20the%20knowledge.%20Based%20on%20these%20findings%2C%20we%20guide%0Athe%20LLMs%20in%20utilizing%20the%20event%20schema%20knowledge%20as%20memory%20leading%20to%0Aimprovements%20on%20event%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520on%2520Event%2520Reasoning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DZhengwei%2520Tao%2520and%2520Zhi%2520Jin%2520and%2520Yifan%2520Zhang%2520and%2520Xiancai%2520Chen%2520and%2520Haiyan%2520Zhao%2520and%2520Jia%2520Li%2520and%2520Bing%2520Liang%2520and%2520Chongyang%2520Tao%2520and%2520Qun%2520Liu%2520and%2520Kam-Fai%2520Wong%26entry.1292438233%3D%2520%2520Event%2520reasoning%2520is%2520a%2520fundamental%2520ability%2520that%2520underlies%2520many%2520applications.%2520It%250Arequires%2520event%2520schema%2520knowledge%2520to%2520perform%2520global%2520reasoning%2520and%2520needs%2520to%2520deal%250Awith%2520the%2520diversity%2520of%2520the%2520inter-event%2520relations%2520and%2520the%2520reasoning%2520paradigms.%250AHow%2520well%2520LLMs%2520accomplish%2520event%2520reasoning%2520on%2520various%2520relations%2520and%2520reasoning%250Aparadigms%2520remains%2520unknown.%2520To%2520mitigate%2520this%2520disparity%252C%2520we%2520comprehensively%250Aevaluate%2520the%2520abilities%2520of%2520event%2520reasoning%2520of%2520LLMs.%2520We%2520introduce%2520a%2520novel%250Abenchmark%2520EV2%2520for%2520EValuation%2520of%2520EVent%2520reasoning.%2520EV2%2520consists%2520of%2520two%2520levels%2520of%250Aevaluation%2520of%2520schema%2520and%2520instance%2520and%2520is%2520comprehensive%2520in%2520relations%2520and%250Areasoning%2520paradigms.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520EV2.%2520We%2520find%2520that%2520LLMs%250Ahave%2520abilities%2520to%2520accomplish%2520event%2520reasoning%2520but%2520their%2520performances%2520are%2520far%250Afrom%2520satisfactory.%2520We%2520also%2520notice%2520the%2520imbalance%2520of%2520event%2520reasoning%2520abilities%2520in%250ALLMs.%2520Besides%252C%2520LLMs%2520have%2520event%2520schema%2520knowledge%252C%2520however%252C%2520they%2527re%2520not%2520aligned%250Awith%2520humans%2520on%2520how%2520to%2520utilize%2520the%2520knowledge.%2520Based%2520on%2520these%2520findings%252C%2520we%2520guide%250Athe%2520LLMs%2520in%2520utilizing%2520the%2520event%2520schema%2520knowledge%2520as%2520memory%2520leading%2520to%250Aimprovements%2520on%2520event%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20on%20Event%20Reasoning%20of%20Large%20Language%20Models&entry.906535625=Zhengwei%20Tao%20and%20Zhi%20Jin%20and%20Yifan%20Zhang%20and%20Xiancai%20Chen%20and%20Haiyan%20Zhao%20and%20Jia%20Li%20and%20Bing%20Liang%20and%20Chongyang%20Tao%20and%20Qun%20Liu%20and%20Kam-Fai%20Wong&entry.1292438233=%20%20Event%20reasoning%20is%20a%20fundamental%20ability%20that%20underlies%20many%20applications.%20It%0Arequires%20event%20schema%20knowledge%20to%20perform%20global%20reasoning%20and%20needs%20to%20deal%0Awith%20the%20diversity%20of%20the%20inter-event%20relations%20and%20the%20reasoning%20paradigms.%0AHow%20well%20LLMs%20accomplish%20event%20reasoning%20on%20various%20relations%20and%20reasoning%0Aparadigms%20remains%20unknown.%20To%20mitigate%20this%20disparity%2C%20we%20comprehensively%0Aevaluate%20the%20abilities%20of%20event%20reasoning%20of%20LLMs.%20We%20introduce%20a%20novel%0Abenchmark%20EV2%20for%20EValuation%20of%20EVent%20reasoning.%20EV2%20consists%20of%20two%20levels%20of%0Aevaluation%20of%20schema%20and%20instance%20and%20is%20comprehensive%20in%20relations%20and%0Areasoning%20paradigms.%20We%20conduct%20extensive%20experiments%20on%20EV2.%20We%20find%20that%20LLMs%0Ahave%20abilities%20to%20accomplish%20event%20reasoning%20but%20their%20performances%20are%20far%0Afrom%20satisfactory.%20We%20also%20notice%20the%20imbalance%20of%20event%20reasoning%20abilities%20in%0ALLMs.%20Besides%2C%20LLMs%20have%20event%20schema%20knowledge%2C%20however%2C%20they%27re%20not%20aligned%0Awith%20humans%20on%20how%20to%20utilize%20the%20knowledge.%20Based%20on%20these%20findings%2C%20we%20guide%0Athe%20LLMs%20in%20utilizing%20the%20event%20schema%20knowledge%20as%20memory%20leading%20to%0Aimprovements%20on%20event%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17513v2&entry.124074799=Read"},
{"title": "Assessing Robustness of Machine Learning Models using Covariate\n  Perturbations", "author": "Arun Prakash R and Anwesha Bhattacharyya and Joel Vaughan and Vijayan N. Nair", "abstract": "  As machine learning models become increasingly prevalent in critical\ndecision-making models and systems in fields like finance, healthcare, etc.,\nensuring their robustness against adversarial attacks and changes in the input\ndata is paramount, especially in cases where models potentially overfit. This\npaper proposes a comprehensive framework for assessing the robustness of\nmachine learning models through covariate perturbation techniques. We explore\nvarious perturbation strategies to assess robustness and examine their impact\non model predictions, including separate strategies for numeric and non-numeric\nvariables, summaries of perturbations to assess and compare model robustness\nacross different scenarios, and local robustness diagnosis to identify any\nregions in the data where a model is particularly unstable. Through empirical\nstudies on real world dataset, we demonstrate the effectiveness of our approach\nin comparing robustness across models, identifying the instabilities in the\nmodel, and enhancing model robustness.\n", "link": "http://arxiv.org/abs/2408.01300v1", "date": "2024-08-02", "relevancy": 1.3461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4622}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4491}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Robustness%20of%20Machine%20Learning%20Models%20using%20Covariate%0A%20%20Perturbations&body=Title%3A%20Assessing%20Robustness%20of%20Machine%20Learning%20Models%20using%20Covariate%0A%20%20Perturbations%0AAuthor%3A%20Arun%20Prakash%20R%20and%20Anwesha%20Bhattacharyya%20and%20Joel%20Vaughan%20and%20Vijayan%20N.%20Nair%0AAbstract%3A%20%20%20As%20machine%20learning%20models%20become%20increasingly%20prevalent%20in%20critical%0Adecision-making%20models%20and%20systems%20in%20fields%20like%20finance%2C%20healthcare%2C%20etc.%2C%0Aensuring%20their%20robustness%20against%20adversarial%20attacks%20and%20changes%20in%20the%20input%0Adata%20is%20paramount%2C%20especially%20in%20cases%20where%20models%20potentially%20overfit.%20This%0Apaper%20proposes%20a%20comprehensive%20framework%20for%20assessing%20the%20robustness%20of%0Amachine%20learning%20models%20through%20covariate%20perturbation%20techniques.%20We%20explore%0Avarious%20perturbation%20strategies%20to%20assess%20robustness%20and%20examine%20their%20impact%0Aon%20model%20predictions%2C%20including%20separate%20strategies%20for%20numeric%20and%20non-numeric%0Avariables%2C%20summaries%20of%20perturbations%20to%20assess%20and%20compare%20model%20robustness%0Aacross%20different%20scenarios%2C%20and%20local%20robustness%20diagnosis%20to%20identify%20any%0Aregions%20in%20the%20data%20where%20a%20model%20is%20particularly%20unstable.%20Through%20empirical%0Astudies%20on%20real%20world%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Ain%20comparing%20robustness%20across%20models%2C%20identifying%20the%20instabilities%20in%20the%0Amodel%2C%20and%20enhancing%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Robustness%2520of%2520Machine%2520Learning%2520Models%2520using%2520Covariate%250A%2520%2520Perturbations%26entry.906535625%3DArun%2520Prakash%2520R%2520and%2520Anwesha%2520Bhattacharyya%2520and%2520Joel%2520Vaughan%2520and%2520Vijayan%2520N.%2520Nair%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520models%2520become%2520increasingly%2520prevalent%2520in%2520critical%250Adecision-making%2520models%2520and%2520systems%2520in%2520fields%2520like%2520finance%252C%2520healthcare%252C%2520etc.%252C%250Aensuring%2520their%2520robustness%2520against%2520adversarial%2520attacks%2520and%2520changes%2520in%2520the%2520input%250Adata%2520is%2520paramount%252C%2520especially%2520in%2520cases%2520where%2520models%2520potentially%2520overfit.%2520This%250Apaper%2520proposes%2520a%2520comprehensive%2520framework%2520for%2520assessing%2520the%2520robustness%2520of%250Amachine%2520learning%2520models%2520through%2520covariate%2520perturbation%2520techniques.%2520We%2520explore%250Avarious%2520perturbation%2520strategies%2520to%2520assess%2520robustness%2520and%2520examine%2520their%2520impact%250Aon%2520model%2520predictions%252C%2520including%2520separate%2520strategies%2520for%2520numeric%2520and%2520non-numeric%250Avariables%252C%2520summaries%2520of%2520perturbations%2520to%2520assess%2520and%2520compare%2520model%2520robustness%250Aacross%2520different%2520scenarios%252C%2520and%2520local%2520robustness%2520diagnosis%2520to%2520identify%2520any%250Aregions%2520in%2520the%2520data%2520where%2520a%2520model%2520is%2520particularly%2520unstable.%2520Through%2520empirical%250Astudies%2520on%2520real%2520world%2520dataset%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Ain%2520comparing%2520robustness%2520across%2520models%252C%2520identifying%2520the%2520instabilities%2520in%2520the%250Amodel%252C%2520and%2520enhancing%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Robustness%20of%20Machine%20Learning%20Models%20using%20Covariate%0A%20%20Perturbations&entry.906535625=Arun%20Prakash%20R%20and%20Anwesha%20Bhattacharyya%20and%20Joel%20Vaughan%20and%20Vijayan%20N.%20Nair&entry.1292438233=%20%20As%20machine%20learning%20models%20become%20increasingly%20prevalent%20in%20critical%0Adecision-making%20models%20and%20systems%20in%20fields%20like%20finance%2C%20healthcare%2C%20etc.%2C%0Aensuring%20their%20robustness%20against%20adversarial%20attacks%20and%20changes%20in%20the%20input%0Adata%20is%20paramount%2C%20especially%20in%20cases%20where%20models%20potentially%20overfit.%20This%0Apaper%20proposes%20a%20comprehensive%20framework%20for%20assessing%20the%20robustness%20of%0Amachine%20learning%20models%20through%20covariate%20perturbation%20techniques.%20We%20explore%0Avarious%20perturbation%20strategies%20to%20assess%20robustness%20and%20examine%20their%20impact%0Aon%20model%20predictions%2C%20including%20separate%20strategies%20for%20numeric%20and%20non-numeric%0Avariables%2C%20summaries%20of%20perturbations%20to%20assess%20and%20compare%20model%20robustness%0Aacross%20different%20scenarios%2C%20and%20local%20robustness%20diagnosis%20to%20identify%20any%0Aregions%20in%20the%20data%20where%20a%20model%20is%20particularly%20unstable.%20Through%20empirical%0Astudies%20on%20real%20world%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Ain%20comparing%20robustness%20across%20models%2C%20identifying%20the%20instabilities%20in%20the%0Amodel%2C%20and%20enhancing%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01300v1&entry.124074799=Read"},
{"title": "A Decision-driven Methodology for Designing Uncertainty-aware AI\n  Self-Assessment", "author": "Gregory Canal and Vladimir Leung and Philip Sage and Eric Heim and I-Jeng Wang", "abstract": "  Artificial intelligence (AI) has revolutionized decision-making processes and\nsystems throughout society and, in particular, has emerged as a significant\ntechnology in high-impact scenarios of national interest. Yet, despite AI's\nimpressive predictive capabilities in controlled settings, it still suffers\nfrom a range of practical setbacks preventing its widespread use in various\ncritical scenarios. In particular, it is generally unclear if a given AI\nsystem's predictions can be trusted by decision-makers in downstream\napplications. To address the need for more transparent, robust, and trustworthy\nAI systems, a suite of tools has been developed to quantify the uncertainty of\nAI predictions and, more generally, enable AI to \"self-assess\" the reliability\nof its predictions. In this manuscript, we categorize methods for AI\nself-assessment along several key dimensions and provide guidelines for\nselecting and designing the appropriate method for a practitioner's needs. In\nparticular, we focus on uncertainty estimation techniques that consider the\nimpact of self-assessment on the choices made by downstream decision-makers and\non the resulting costs and benefits of decision outcomes. To demonstrate the\nutility of our methodology for self-assessment design, we illustrate its use\nfor two realistic national-interest scenarios. This manuscript is a practical\nguide for machine learning engineers and AI system users to select the ideal\nself-assessment techniques for each problem.\n", "link": "http://arxiv.org/abs/2408.01301v1", "date": "2024-08-02", "relevancy": 1.4895, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5342}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Decision-driven%20Methodology%20for%20Designing%20Uncertainty-aware%20AI%0A%20%20Self-Assessment&body=Title%3A%20A%20Decision-driven%20Methodology%20for%20Designing%20Uncertainty-aware%20AI%0A%20%20Self-Assessment%0AAuthor%3A%20Gregory%20Canal%20and%20Vladimir%20Leung%20and%20Philip%20Sage%20and%20Eric%20Heim%20and%20I-Jeng%20Wang%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20revolutionized%20decision-making%20processes%20and%0Asystems%20throughout%20society%20and%2C%20in%20particular%2C%20has%20emerged%20as%20a%20significant%0Atechnology%20in%20high-impact%20scenarios%20of%20national%20interest.%20Yet%2C%20despite%20AI%27s%0Aimpressive%20predictive%20capabilities%20in%20controlled%20settings%2C%20it%20still%20suffers%0Afrom%20a%20range%20of%20practical%20setbacks%20preventing%20its%20widespread%20use%20in%20various%0Acritical%20scenarios.%20In%20particular%2C%20it%20is%20generally%20unclear%20if%20a%20given%20AI%0Asystem%27s%20predictions%20can%20be%20trusted%20by%20decision-makers%20in%20downstream%0Aapplications.%20To%20address%20the%20need%20for%20more%20transparent%2C%20robust%2C%20and%20trustworthy%0AAI%20systems%2C%20a%20suite%20of%20tools%20has%20been%20developed%20to%20quantify%20the%20uncertainty%20of%0AAI%20predictions%20and%2C%20more%20generally%2C%20enable%20AI%20to%20%22self-assess%22%20the%20reliability%0Aof%20its%20predictions.%20In%20this%20manuscript%2C%20we%20categorize%20methods%20for%20AI%0Aself-assessment%20along%20several%20key%20dimensions%20and%20provide%20guidelines%20for%0Aselecting%20and%20designing%20the%20appropriate%20method%20for%20a%20practitioner%27s%20needs.%20In%0Aparticular%2C%20we%20focus%20on%20uncertainty%20estimation%20techniques%20that%20consider%20the%0Aimpact%20of%20self-assessment%20on%20the%20choices%20made%20by%20downstream%20decision-makers%20and%0Aon%20the%20resulting%20costs%20and%20benefits%20of%20decision%20outcomes.%20To%20demonstrate%20the%0Autility%20of%20our%20methodology%20for%20self-assessment%20design%2C%20we%20illustrate%20its%20use%0Afor%20two%20realistic%20national-interest%20scenarios.%20This%20manuscript%20is%20a%20practical%0Aguide%20for%20machine%20learning%20engineers%20and%20AI%20system%20users%20to%20select%20the%20ideal%0Aself-assessment%20techniques%20for%20each%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Decision-driven%2520Methodology%2520for%2520Designing%2520Uncertainty-aware%2520AI%250A%2520%2520Self-Assessment%26entry.906535625%3DGregory%2520Canal%2520and%2520Vladimir%2520Leung%2520and%2520Philip%2520Sage%2520and%2520Eric%2520Heim%2520and%2520I-Jeng%2520Wang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520revolutionized%2520decision-making%2520processes%2520and%250Asystems%2520throughout%2520society%2520and%252C%2520in%2520particular%252C%2520has%2520emerged%2520as%2520a%2520significant%250Atechnology%2520in%2520high-impact%2520scenarios%2520of%2520national%2520interest.%2520Yet%252C%2520despite%2520AI%2527s%250Aimpressive%2520predictive%2520capabilities%2520in%2520controlled%2520settings%252C%2520it%2520still%2520suffers%250Afrom%2520a%2520range%2520of%2520practical%2520setbacks%2520preventing%2520its%2520widespread%2520use%2520in%2520various%250Acritical%2520scenarios.%2520In%2520particular%252C%2520it%2520is%2520generally%2520unclear%2520if%2520a%2520given%2520AI%250Asystem%2527s%2520predictions%2520can%2520be%2520trusted%2520by%2520decision-makers%2520in%2520downstream%250Aapplications.%2520To%2520address%2520the%2520need%2520for%2520more%2520transparent%252C%2520robust%252C%2520and%2520trustworthy%250AAI%2520systems%252C%2520a%2520suite%2520of%2520tools%2520has%2520been%2520developed%2520to%2520quantify%2520the%2520uncertainty%2520of%250AAI%2520predictions%2520and%252C%2520more%2520generally%252C%2520enable%2520AI%2520to%2520%2522self-assess%2522%2520the%2520reliability%250Aof%2520its%2520predictions.%2520In%2520this%2520manuscript%252C%2520we%2520categorize%2520methods%2520for%2520AI%250Aself-assessment%2520along%2520several%2520key%2520dimensions%2520and%2520provide%2520guidelines%2520for%250Aselecting%2520and%2520designing%2520the%2520appropriate%2520method%2520for%2520a%2520practitioner%2527s%2520needs.%2520In%250Aparticular%252C%2520we%2520focus%2520on%2520uncertainty%2520estimation%2520techniques%2520that%2520consider%2520the%250Aimpact%2520of%2520self-assessment%2520on%2520the%2520choices%2520made%2520by%2520downstream%2520decision-makers%2520and%250Aon%2520the%2520resulting%2520costs%2520and%2520benefits%2520of%2520decision%2520outcomes.%2520To%2520demonstrate%2520the%250Autility%2520of%2520our%2520methodology%2520for%2520self-assessment%2520design%252C%2520we%2520illustrate%2520its%2520use%250Afor%2520two%2520realistic%2520national-interest%2520scenarios.%2520This%2520manuscript%2520is%2520a%2520practical%250Aguide%2520for%2520machine%2520learning%2520engineers%2520and%2520AI%2520system%2520users%2520to%2520select%2520the%2520ideal%250Aself-assessment%2520techniques%2520for%2520each%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Decision-driven%20Methodology%20for%20Designing%20Uncertainty-aware%20AI%0A%20%20Self-Assessment&entry.906535625=Gregory%20Canal%20and%20Vladimir%20Leung%20and%20Philip%20Sage%20and%20Eric%20Heim%20and%20I-Jeng%20Wang&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20revolutionized%20decision-making%20processes%20and%0Asystems%20throughout%20society%20and%2C%20in%20particular%2C%20has%20emerged%20as%20a%20significant%0Atechnology%20in%20high-impact%20scenarios%20of%20national%20interest.%20Yet%2C%20despite%20AI%27s%0Aimpressive%20predictive%20capabilities%20in%20controlled%20settings%2C%20it%20still%20suffers%0Afrom%20a%20range%20of%20practical%20setbacks%20preventing%20its%20widespread%20use%20in%20various%0Acritical%20scenarios.%20In%20particular%2C%20it%20is%20generally%20unclear%20if%20a%20given%20AI%0Asystem%27s%20predictions%20can%20be%20trusted%20by%20decision-makers%20in%20downstream%0Aapplications.%20To%20address%20the%20need%20for%20more%20transparent%2C%20robust%2C%20and%20trustworthy%0AAI%20systems%2C%20a%20suite%20of%20tools%20has%20been%20developed%20to%20quantify%20the%20uncertainty%20of%0AAI%20predictions%20and%2C%20more%20generally%2C%20enable%20AI%20to%20%22self-assess%22%20the%20reliability%0Aof%20its%20predictions.%20In%20this%20manuscript%2C%20we%20categorize%20methods%20for%20AI%0Aself-assessment%20along%20several%20key%20dimensions%20and%20provide%20guidelines%20for%0Aselecting%20and%20designing%20the%20appropriate%20method%20for%20a%20practitioner%27s%20needs.%20In%0Aparticular%2C%20we%20focus%20on%20uncertainty%20estimation%20techniques%20that%20consider%20the%0Aimpact%20of%20self-assessment%20on%20the%20choices%20made%20by%20downstream%20decision-makers%20and%0Aon%20the%20resulting%20costs%20and%20benefits%20of%20decision%20outcomes.%20To%20demonstrate%20the%0Autility%20of%20our%20methodology%20for%20self-assessment%20design%2C%20we%20illustrate%20its%20use%0Afor%20two%20realistic%20national-interest%20scenarios.%20This%20manuscript%20is%20a%20practical%0Aguide%20for%20machine%20learning%20engineers%20and%20AI%20system%20users%20to%20select%20the%20ideal%0Aself-assessment%20techniques%20for%20each%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01301v1&entry.124074799=Read"},
{"title": "Canonical Decision Diagrams Modulo Theories", "author": "Massimo Michelutti and Gabriele Masina and Giuseppe Spallitta and Roberto Sebastiani", "abstract": "  Decision diagrams (DDs) are powerful tools to represent effectively\npropositional formulas, which are largely used in many domains, in particular\nin formal verification and in knowledge compilation. Some forms of DDs (e.g.,\nOBDDs, SDDs) are canonical, that is, (under given conditions on the atom list)\nthey univocally represent equivalence classes of formulas. Given the limited\nexpressiveness of propositional logic, a few attempts to leverage DDs to SMT\nlevel have been presented in the literature. Unfortunately, these techniques\nstill suffer from some limitations: most procedures are theory-specific; some\nproduce theory DDs (T-DDs) which do not univocally represent T-valid formulas\nor T-inconsistent formulas; none of these techniques provably produces\ntheory-canonical T-DDs, which (under given conditions on the T-atom list)\nunivocally represent T-equivalence classes of formulas. Also, these procedures\nare not easy to implement, and very few implementations are actually available.\nIn this paper, we present a novel very-general technique to leverage DDs to SMT\nlevel, which has several advantages: it is very easy to implement on top of an\nAllSMT solver and a DD package, which are used as blackboxes; it works for\nevery form of DDs and every theory, or combination thereof, supported by the\nAllSMT solver; it produces theory-canonical T-DDs if the propositional DD is\ncanonical. We have implemented a prototype tool for both T-OBDDs and T-SDDs on\ntop of OBDD and SDD packages and the MathSAT SMT solver. Some preliminary\nempirical evaluation supports the effectiveness of the approach.\n", "link": "http://arxiv.org/abs/2404.16455v3", "date": "2024-08-02", "relevancy": 1.5657, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3972}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Canonical%20Decision%20Diagrams%20Modulo%20Theories&body=Title%3A%20Canonical%20Decision%20Diagrams%20Modulo%20Theories%0AAuthor%3A%20Massimo%20Michelutti%20and%20Gabriele%20Masina%20and%20Giuseppe%20Spallitta%20and%20Roberto%20Sebastiani%0AAbstract%3A%20%20%20Decision%20diagrams%20%28DDs%29%20are%20powerful%20tools%20to%20represent%20effectively%0Apropositional%20formulas%2C%20which%20are%20largely%20used%20in%20many%20domains%2C%20in%20particular%0Ain%20formal%20verification%20and%20in%20knowledge%20compilation.%20Some%20forms%20of%20DDs%20%28e.g.%2C%0AOBDDs%2C%20SDDs%29%20are%20canonical%2C%20that%20is%2C%20%28under%20given%20conditions%20on%20the%20atom%20list%29%0Athey%20univocally%20represent%20equivalence%20classes%20of%20formulas.%20Given%20the%20limited%0Aexpressiveness%20of%20propositional%20logic%2C%20a%20few%20attempts%20to%20leverage%20DDs%20to%20SMT%0Alevel%20have%20been%20presented%20in%20the%20literature.%20Unfortunately%2C%20these%20techniques%0Astill%20suffer%20from%20some%20limitations%3A%20most%20procedures%20are%20theory-specific%3B%20some%0Aproduce%20theory%20DDs%20%28T-DDs%29%20which%20do%20not%20univocally%20represent%20T-valid%20formulas%0Aor%20T-inconsistent%20formulas%3B%20none%20of%20these%20techniques%20provably%20produces%0Atheory-canonical%20T-DDs%2C%20which%20%28under%20given%20conditions%20on%20the%20T-atom%20list%29%0Aunivocally%20represent%20T-equivalence%20classes%20of%20formulas.%20Also%2C%20these%20procedures%0Aare%20not%20easy%20to%20implement%2C%20and%20very%20few%20implementations%20are%20actually%20available.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20very-general%20technique%20to%20leverage%20DDs%20to%20SMT%0Alevel%2C%20which%20has%20several%20advantages%3A%20it%20is%20very%20easy%20to%20implement%20on%20top%20of%20an%0AAllSMT%20solver%20and%20a%20DD%20package%2C%20which%20are%20used%20as%20blackboxes%3B%20it%20works%20for%0Aevery%20form%20of%20DDs%20and%20every%20theory%2C%20or%20combination%20thereof%2C%20supported%20by%20the%0AAllSMT%20solver%3B%20it%20produces%20theory-canonical%20T-DDs%20if%20the%20propositional%20DD%20is%0Acanonical.%20We%20have%20implemented%20a%20prototype%20tool%20for%20both%20T-OBDDs%20and%20T-SDDs%20on%0Atop%20of%20OBDD%20and%20SDD%20packages%20and%20the%20MathSAT%20SMT%20solver.%20Some%20preliminary%0Aempirical%20evaluation%20supports%20the%20effectiveness%20of%20the%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16455v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonical%2520Decision%2520Diagrams%2520Modulo%2520Theories%26entry.906535625%3DMassimo%2520Michelutti%2520and%2520Gabriele%2520Masina%2520and%2520Giuseppe%2520Spallitta%2520and%2520Roberto%2520Sebastiani%26entry.1292438233%3D%2520%2520Decision%2520diagrams%2520%2528DDs%2529%2520are%2520powerful%2520tools%2520to%2520represent%2520effectively%250Apropositional%2520formulas%252C%2520which%2520are%2520largely%2520used%2520in%2520many%2520domains%252C%2520in%2520particular%250Ain%2520formal%2520verification%2520and%2520in%2520knowledge%2520compilation.%2520Some%2520forms%2520of%2520DDs%2520%2528e.g.%252C%250AOBDDs%252C%2520SDDs%2529%2520are%2520canonical%252C%2520that%2520is%252C%2520%2528under%2520given%2520conditions%2520on%2520the%2520atom%2520list%2529%250Athey%2520univocally%2520represent%2520equivalence%2520classes%2520of%2520formulas.%2520Given%2520the%2520limited%250Aexpressiveness%2520of%2520propositional%2520logic%252C%2520a%2520few%2520attempts%2520to%2520leverage%2520DDs%2520to%2520SMT%250Alevel%2520have%2520been%2520presented%2520in%2520the%2520literature.%2520Unfortunately%252C%2520these%2520techniques%250Astill%2520suffer%2520from%2520some%2520limitations%253A%2520most%2520procedures%2520are%2520theory-specific%253B%2520some%250Aproduce%2520theory%2520DDs%2520%2528T-DDs%2529%2520which%2520do%2520not%2520univocally%2520represent%2520T-valid%2520formulas%250Aor%2520T-inconsistent%2520formulas%253B%2520none%2520of%2520these%2520techniques%2520provably%2520produces%250Atheory-canonical%2520T-DDs%252C%2520which%2520%2528under%2520given%2520conditions%2520on%2520the%2520T-atom%2520list%2529%250Aunivocally%2520represent%2520T-equivalence%2520classes%2520of%2520formulas.%2520Also%252C%2520these%2520procedures%250Aare%2520not%2520easy%2520to%2520implement%252C%2520and%2520very%2520few%2520implementations%2520are%2520actually%2520available.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520very-general%2520technique%2520to%2520leverage%2520DDs%2520to%2520SMT%250Alevel%252C%2520which%2520has%2520several%2520advantages%253A%2520it%2520is%2520very%2520easy%2520to%2520implement%2520on%2520top%2520of%2520an%250AAllSMT%2520solver%2520and%2520a%2520DD%2520package%252C%2520which%2520are%2520used%2520as%2520blackboxes%253B%2520it%2520works%2520for%250Aevery%2520form%2520of%2520DDs%2520and%2520every%2520theory%252C%2520or%2520combination%2520thereof%252C%2520supported%2520by%2520the%250AAllSMT%2520solver%253B%2520it%2520produces%2520theory-canonical%2520T-DDs%2520if%2520the%2520propositional%2520DD%2520is%250Acanonical.%2520We%2520have%2520implemented%2520a%2520prototype%2520tool%2520for%2520both%2520T-OBDDs%2520and%2520T-SDDs%2520on%250Atop%2520of%2520OBDD%2520and%2520SDD%2520packages%2520and%2520the%2520MathSAT%2520SMT%2520solver.%2520Some%2520preliminary%250Aempirical%2520evaluation%2520supports%2520the%2520effectiveness%2520of%2520the%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16455v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Canonical%20Decision%20Diagrams%20Modulo%20Theories&entry.906535625=Massimo%20Michelutti%20and%20Gabriele%20Masina%20and%20Giuseppe%20Spallitta%20and%20Roberto%20Sebastiani&entry.1292438233=%20%20Decision%20diagrams%20%28DDs%29%20are%20powerful%20tools%20to%20represent%20effectively%0Apropositional%20formulas%2C%20which%20are%20largely%20used%20in%20many%20domains%2C%20in%20particular%0Ain%20formal%20verification%20and%20in%20knowledge%20compilation.%20Some%20forms%20of%20DDs%20%28e.g.%2C%0AOBDDs%2C%20SDDs%29%20are%20canonical%2C%20that%20is%2C%20%28under%20given%20conditions%20on%20the%20atom%20list%29%0Athey%20univocally%20represent%20equivalence%20classes%20of%20formulas.%20Given%20the%20limited%0Aexpressiveness%20of%20propositional%20logic%2C%20a%20few%20attempts%20to%20leverage%20DDs%20to%20SMT%0Alevel%20have%20been%20presented%20in%20the%20literature.%20Unfortunately%2C%20these%20techniques%0Astill%20suffer%20from%20some%20limitations%3A%20most%20procedures%20are%20theory-specific%3B%20some%0Aproduce%20theory%20DDs%20%28T-DDs%29%20which%20do%20not%20univocally%20represent%20T-valid%20formulas%0Aor%20T-inconsistent%20formulas%3B%20none%20of%20these%20techniques%20provably%20produces%0Atheory-canonical%20T-DDs%2C%20which%20%28under%20given%20conditions%20on%20the%20T-atom%20list%29%0Aunivocally%20represent%20T-equivalence%20classes%20of%20formulas.%20Also%2C%20these%20procedures%0Aare%20not%20easy%20to%20implement%2C%20and%20very%20few%20implementations%20are%20actually%20available.%0AIn%20this%20paper%2C%20we%20present%20a%20novel%20very-general%20technique%20to%20leverage%20DDs%20to%20SMT%0Alevel%2C%20which%20has%20several%20advantages%3A%20it%20is%20very%20easy%20to%20implement%20on%20top%20of%20an%0AAllSMT%20solver%20and%20a%20DD%20package%2C%20which%20are%20used%20as%20blackboxes%3B%20it%20works%20for%0Aevery%20form%20of%20DDs%20and%20every%20theory%2C%20or%20combination%20thereof%2C%20supported%20by%20the%0AAllSMT%20solver%3B%20it%20produces%20theory-canonical%20T-DDs%20if%20the%20propositional%20DD%20is%0Acanonical.%20We%20have%20implemented%20a%20prototype%20tool%20for%20both%20T-OBDDs%20and%20T-SDDs%20on%0Atop%20of%20OBDD%20and%20SDD%20packages%20and%20the%20MathSAT%20SMT%20solver.%20Some%20preliminary%0Aempirical%20evaluation%20supports%20the%20effectiveness%20of%20the%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16455v3&entry.124074799=Read"},
{"title": "The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online\n  Recommender Systems", "author": "Guy Aridor and Duarte Goncalves and Ruoyan Kong and Daniel Kluver and Joseph Konstan", "abstract": "  An increasingly important aspect of designing recommender systems involves\nconsidering how recommendations will influence consumer choices. This paper\naddresses this issue by introducing a method for collecting user beliefs about\nun-experienced items - a critical predictor of choice behavior. We implemented\nthis method on the MovieLens platform, resulting in a rich dataset that\ncombines user ratings, beliefs, and observed recommendations. We document\nchallenges to such data collection, including selection bias in response and\nlimited coverage of the product space. This unique resource empowers\nresearchers to delve deeper into user behavior and analyze user choices absent\nrecommendations, measure the effectiveness of recommendations, and prototype\nalgorithms that leverage user belief data, ultimately leading to more impactful\nrecommender systems. The dataset can be found at\nhttps://grouplens.org/datasets/movielens/ml_belief_2024/.\n", "link": "http://arxiv.org/abs/2405.11053v3", "date": "2024-08-02", "relevancy": 1.352, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20MovieLens%20Beliefs%20Dataset%3A%20Collecting%20Pre-Choice%20Data%20for%20Online%0A%20%20Recommender%20Systems&body=Title%3A%20The%20MovieLens%20Beliefs%20Dataset%3A%20Collecting%20Pre-Choice%20Data%20for%20Online%0A%20%20Recommender%20Systems%0AAuthor%3A%20Guy%20Aridor%20and%20Duarte%20Goncalves%20and%20Ruoyan%20Kong%20and%20Daniel%20Kluver%20and%20Joseph%20Konstan%0AAbstract%3A%20%20%20An%20increasingly%20important%20aspect%20of%20designing%20recommender%20systems%20involves%0Aconsidering%20how%20recommendations%20will%20influence%20consumer%20choices.%20This%20paper%0Aaddresses%20this%20issue%20by%20introducing%20a%20method%20for%20collecting%20user%20beliefs%20about%0Aun-experienced%20items%20-%20a%20critical%20predictor%20of%20choice%20behavior.%20We%20implemented%0Athis%20method%20on%20the%20MovieLens%20platform%2C%20resulting%20in%20a%20rich%20dataset%20that%0Acombines%20user%20ratings%2C%20beliefs%2C%20and%20observed%20recommendations.%20We%20document%0Achallenges%20to%20such%20data%20collection%2C%20including%20selection%20bias%20in%20response%20and%0Alimited%20coverage%20of%20the%20product%20space.%20This%20unique%20resource%20empowers%0Aresearchers%20to%20delve%20deeper%20into%20user%20behavior%20and%20analyze%20user%20choices%20absent%0Arecommendations%2C%20measure%20the%20effectiveness%20of%20recommendations%2C%20and%20prototype%0Aalgorithms%20that%20leverage%20user%20belief%20data%2C%20ultimately%20leading%20to%20more%20impactful%0Arecommender%20systems.%20The%20dataset%20can%20be%20found%20at%0Ahttps%3A//grouplens.org/datasets/movielens/ml_belief_2024/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520MovieLens%2520Beliefs%2520Dataset%253A%2520Collecting%2520Pre-Choice%2520Data%2520for%2520Online%250A%2520%2520Recommender%2520Systems%26entry.906535625%3DGuy%2520Aridor%2520and%2520Duarte%2520Goncalves%2520and%2520Ruoyan%2520Kong%2520and%2520Daniel%2520Kluver%2520and%2520Joseph%2520Konstan%26entry.1292438233%3D%2520%2520An%2520increasingly%2520important%2520aspect%2520of%2520designing%2520recommender%2520systems%2520involves%250Aconsidering%2520how%2520recommendations%2520will%2520influence%2520consumer%2520choices.%2520This%2520paper%250Aaddresses%2520this%2520issue%2520by%2520introducing%2520a%2520method%2520for%2520collecting%2520user%2520beliefs%2520about%250Aun-experienced%2520items%2520-%2520a%2520critical%2520predictor%2520of%2520choice%2520behavior.%2520We%2520implemented%250Athis%2520method%2520on%2520the%2520MovieLens%2520platform%252C%2520resulting%2520in%2520a%2520rich%2520dataset%2520that%250Acombines%2520user%2520ratings%252C%2520beliefs%252C%2520and%2520observed%2520recommendations.%2520We%2520document%250Achallenges%2520to%2520such%2520data%2520collection%252C%2520including%2520selection%2520bias%2520in%2520response%2520and%250Alimited%2520coverage%2520of%2520the%2520product%2520space.%2520This%2520unique%2520resource%2520empowers%250Aresearchers%2520to%2520delve%2520deeper%2520into%2520user%2520behavior%2520and%2520analyze%2520user%2520choices%2520absent%250Arecommendations%252C%2520measure%2520the%2520effectiveness%2520of%2520recommendations%252C%2520and%2520prototype%250Aalgorithms%2520that%2520leverage%2520user%2520belief%2520data%252C%2520ultimately%2520leading%2520to%2520more%2520impactful%250Arecommender%2520systems.%2520The%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//grouplens.org/datasets/movielens/ml_belief_2024/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20MovieLens%20Beliefs%20Dataset%3A%20Collecting%20Pre-Choice%20Data%20for%20Online%0A%20%20Recommender%20Systems&entry.906535625=Guy%20Aridor%20and%20Duarte%20Goncalves%20and%20Ruoyan%20Kong%20and%20Daniel%20Kluver%20and%20Joseph%20Konstan&entry.1292438233=%20%20An%20increasingly%20important%20aspect%20of%20designing%20recommender%20systems%20involves%0Aconsidering%20how%20recommendations%20will%20influence%20consumer%20choices.%20This%20paper%0Aaddresses%20this%20issue%20by%20introducing%20a%20method%20for%20collecting%20user%20beliefs%20about%0Aun-experienced%20items%20-%20a%20critical%20predictor%20of%20choice%20behavior.%20We%20implemented%0Athis%20method%20on%20the%20MovieLens%20platform%2C%20resulting%20in%20a%20rich%20dataset%20that%0Acombines%20user%20ratings%2C%20beliefs%2C%20and%20observed%20recommendations.%20We%20document%0Achallenges%20to%20such%20data%20collection%2C%20including%20selection%20bias%20in%20response%20and%0Alimited%20coverage%20of%20the%20product%20space.%20This%20unique%20resource%20empowers%0Aresearchers%20to%20delve%20deeper%20into%20user%20behavior%20and%20analyze%20user%20choices%20absent%0Arecommendations%2C%20measure%20the%20effectiveness%20of%20recommendations%2C%20and%20prototype%0Aalgorithms%20that%20leverage%20user%20belief%20data%2C%20ultimately%20leading%20to%20more%20impactful%0Arecommender%20systems.%20The%20dataset%20can%20be%20found%20at%0Ahttps%3A//grouplens.org/datasets/movielens/ml_belief_2024/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11053v3&entry.124074799=Read"},
{"title": "Comprehensive Library of Variational LSE Solvers", "author": "Nico Meyer and Martin R\u00f6hn and Jakob Murauer and Axel Plinge and Christopher Mutschler and Daniel D. Scherer", "abstract": "  Linear systems of equations can be found in various mathematical domains, as\nwell as in the field of machine learning. By employing noisy intermediate-scale\nquantum devices, variational solvers promise to accelerate finding solutions\nfor large systems. Although there is a wealth of theoretical research on these\nalgorithms, only fragmentary implementations exist. To fill this gap, we have\ndeveloped the variational-lse-solver framework, which realizes existing\napproaches in literature, and introduces several enhancements. The\nuser-friendly interface is designed for researchers that work at the\nabstraction level of identifying and developing end-to-end applications.\n", "link": "http://arxiv.org/abs/2404.09916v2", "date": "2024-08-02", "relevancy": 1.6707, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4245}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Library%20of%20Variational%20LSE%20Solvers&body=Title%3A%20Comprehensive%20Library%20of%20Variational%20LSE%20Solvers%0AAuthor%3A%20Nico%20Meyer%20and%20Martin%20R%C3%B6hn%20and%20Jakob%20Murauer%20and%20Axel%20Plinge%20and%20Christopher%20Mutschler%20and%20Daniel%20D.%20Scherer%0AAbstract%3A%20%20%20Linear%20systems%20of%20equations%20can%20be%20found%20in%20various%20mathematical%20domains%2C%20as%0Awell%20as%20in%20the%20field%20of%20machine%20learning.%20By%20employing%20noisy%20intermediate-scale%0Aquantum%20devices%2C%20variational%20solvers%20promise%20to%20accelerate%20finding%20solutions%0Afor%20large%20systems.%20Although%20there%20is%20a%20wealth%20of%20theoretical%20research%20on%20these%0Aalgorithms%2C%20only%20fragmentary%20implementations%20exist.%20To%20fill%20this%20gap%2C%20we%20have%0Adeveloped%20the%20variational-lse-solver%20framework%2C%20which%20realizes%20existing%0Aapproaches%20in%20literature%2C%20and%20introduces%20several%20enhancements.%20The%0Auser-friendly%20interface%20is%20designed%20for%20researchers%20that%20work%20at%20the%0Aabstraction%20level%20of%20identifying%20and%20developing%20end-to-end%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Library%2520of%2520Variational%2520LSE%2520Solvers%26entry.906535625%3DNico%2520Meyer%2520and%2520Martin%2520R%25C3%25B6hn%2520and%2520Jakob%2520Murauer%2520and%2520Axel%2520Plinge%2520and%2520Christopher%2520Mutschler%2520and%2520Daniel%2520D.%2520Scherer%26entry.1292438233%3D%2520%2520Linear%2520systems%2520of%2520equations%2520can%2520be%2520found%2520in%2520various%2520mathematical%2520domains%252C%2520as%250Awell%2520as%2520in%2520the%2520field%2520of%2520machine%2520learning.%2520By%2520employing%2520noisy%2520intermediate-scale%250Aquantum%2520devices%252C%2520variational%2520solvers%2520promise%2520to%2520accelerate%2520finding%2520solutions%250Afor%2520large%2520systems.%2520Although%2520there%2520is%2520a%2520wealth%2520of%2520theoretical%2520research%2520on%2520these%250Aalgorithms%252C%2520only%2520fragmentary%2520implementations%2520exist.%2520To%2520fill%2520this%2520gap%252C%2520we%2520have%250Adeveloped%2520the%2520variational-lse-solver%2520framework%252C%2520which%2520realizes%2520existing%250Aapproaches%2520in%2520literature%252C%2520and%2520introduces%2520several%2520enhancements.%2520The%250Auser-friendly%2520interface%2520is%2520designed%2520for%2520researchers%2520that%2520work%2520at%2520the%250Aabstraction%2520level%2520of%2520identifying%2520and%2520developing%2520end-to-end%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Library%20of%20Variational%20LSE%20Solvers&entry.906535625=Nico%20Meyer%20and%20Martin%20R%C3%B6hn%20and%20Jakob%20Murauer%20and%20Axel%20Plinge%20and%20Christopher%20Mutschler%20and%20Daniel%20D.%20Scherer&entry.1292438233=%20%20Linear%20systems%20of%20equations%20can%20be%20found%20in%20various%20mathematical%20domains%2C%20as%0Awell%20as%20in%20the%20field%20of%20machine%20learning.%20By%20employing%20noisy%20intermediate-scale%0Aquantum%20devices%2C%20variational%20solvers%20promise%20to%20accelerate%20finding%20solutions%0Afor%20large%20systems.%20Although%20there%20is%20a%20wealth%20of%20theoretical%20research%20on%20these%0Aalgorithms%2C%20only%20fragmentary%20implementations%20exist.%20To%20fill%20this%20gap%2C%20we%20have%0Adeveloped%20the%20variational-lse-solver%20framework%2C%20which%20realizes%20existing%0Aapproaches%20in%20literature%2C%20and%20introduces%20several%20enhancements.%20The%0Auser-friendly%20interface%20is%20designed%20for%20researchers%20that%20work%20at%20the%0Aabstraction%20level%20of%20identifying%20and%20developing%20end-to-end%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09916v2&entry.124074799=Read"},
{"title": "Routoo: Learning to Route to Large Language Models Effectively", "author": "Alireza Mohammadshahi and Arshad Rafiq Shaikh and Majid Yazdani", "abstract": "  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n", "link": "http://arxiv.org/abs/2401.13979v2", "date": "2024-08-02", "relevancy": 0.9893, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.499}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4953}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Routoo%3A%20Learning%20to%20Route%20to%20Large%20Language%20Models%20Effectively&body=Title%3A%20Routoo%3A%20Learning%20to%20Route%20to%20Large%20Language%20Models%20Effectively%0AAuthor%3A%20Alireza%20Mohammadshahi%20and%20Arshad%20Rafiq%20Shaikh%20and%20Majid%20Yazdani%0AAbstract%3A%20%20%20Developing%20foundational%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%0Acostly%20and%20inefficient.%20Also%2C%20closed-source%20and%20larger%20open-source%20models%0Agenerally%20offer%20better%20response%20quality%20but%20come%20with%20higher%20inference%20costs%0Athan%20smaller%20models.%20In%20this%20paper%2C%20we%20introduce%20Routoo%2C%20an%20architecture%0Adesigned%20to%20optimize%20the%20selection%20of%20LLMs%20for%20specific%20prompts%20based%20on%0Aperformance%2C%20cost%2C%20and%20efficiency.%20Routoo%20consists%20of%20two%20key%20components%3A%20a%0Aperformance%20predictor%20and%20a%20cost-aware%20decoding.%20The%20performance%20predictor%20is%20a%0Alightweight%20LLM%20that%20estimates%20the%20performance%20of%20various%20underlying%20LLMs%0Awithout%20needing%20to%20execute%20and%20evaluate%20them.%20The%20cost-aware%20decoding%20then%0Aselects%20the%20most%20suitable%20model%20based%20on%20these%20predictions%20and%20other%0Aconstraints%20like%20cost%20and%20latency.%20We%20evaluated%20Routoo%20using%20the%20MMLU%20benchmark%0Aacross%2057%20domains%20employing%20open-source%20models.%20Our%20results%20show%20that%20Routoo%0Amatches%20the%20performance%20of%20the%20Mixtral%208x7b%20model%20while%20reducing%20inference%0Acosts%20by%20one-third.%20Additionally%2C%20by%20allowing%20increased%20costs%2C%20Routoo%20surpasses%0AMixtral%27s%20accuracy%20by%20over%205%25%20at%20equivalent%20costs%2C%20achieving%20an%20accuracy%20of%0A75.9%25.%20When%20integrating%20GPT4%20into%20our%20model%20pool%2C%20Routoo%20nearly%20matches%20GPT4%27s%0Aperformance%20at%20half%20the%20cost%20and%20exceeds%20it%20with%20a%2025%25%20cost%20reduction.%20These%0Aoutcomes%20highlight%20Routoo%27s%20potential%20to%20create%20new%20SOTA%20in%20a%20cost-effective%0Amanner%20by%20leveraging%20the%20collective%20knowledge%20of%20multiple%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoutoo%253A%2520Learning%2520to%2520Route%2520to%2520Large%2520Language%2520Models%2520Effectively%26entry.906535625%3DAlireza%2520Mohammadshahi%2520and%2520Arshad%2520Rafiq%2520Shaikh%2520and%2520Majid%2520Yazdani%26entry.1292438233%3D%2520%2520Developing%2520foundational%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520becoming%2520increasingly%250Acostly%2520and%2520inefficient.%2520Also%252C%2520closed-source%2520and%2520larger%2520open-source%2520models%250Agenerally%2520offer%2520better%2520response%2520quality%2520but%2520come%2520with%2520higher%2520inference%2520costs%250Athan%2520smaller%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Routoo%252C%2520an%2520architecture%250Adesigned%2520to%2520optimize%2520the%2520selection%2520of%2520LLMs%2520for%2520specific%2520prompts%2520based%2520on%250Aperformance%252C%2520cost%252C%2520and%2520efficiency.%2520Routoo%2520consists%2520of%2520two%2520key%2520components%253A%2520a%250Aperformance%2520predictor%2520and%2520a%2520cost-aware%2520decoding.%2520The%2520performance%2520predictor%2520is%2520a%250Alightweight%2520LLM%2520that%2520estimates%2520the%2520performance%2520of%2520various%2520underlying%2520LLMs%250Awithout%2520needing%2520to%2520execute%2520and%2520evaluate%2520them.%2520The%2520cost-aware%2520decoding%2520then%250Aselects%2520the%2520most%2520suitable%2520model%2520based%2520on%2520these%2520predictions%2520and%2520other%250Aconstraints%2520like%2520cost%2520and%2520latency.%2520We%2520evaluated%2520Routoo%2520using%2520the%2520MMLU%2520benchmark%250Aacross%252057%2520domains%2520employing%2520open-source%2520models.%2520Our%2520results%2520show%2520that%2520Routoo%250Amatches%2520the%2520performance%2520of%2520the%2520Mixtral%25208x7b%2520model%2520while%2520reducing%2520inference%250Acosts%2520by%2520one-third.%2520Additionally%252C%2520by%2520allowing%2520increased%2520costs%252C%2520Routoo%2520surpasses%250AMixtral%2527s%2520accuracy%2520by%2520over%25205%2525%2520at%2520equivalent%2520costs%252C%2520achieving%2520an%2520accuracy%2520of%250A75.9%2525.%2520When%2520integrating%2520GPT4%2520into%2520our%2520model%2520pool%252C%2520Routoo%2520nearly%2520matches%2520GPT4%2527s%250Aperformance%2520at%2520half%2520the%2520cost%2520and%2520exceeds%2520it%2520with%2520a%252025%2525%2520cost%2520reduction.%2520These%250Aoutcomes%2520highlight%2520Routoo%2527s%2520potential%2520to%2520create%2520new%2520SOTA%2520in%2520a%2520cost-effective%250Amanner%2520by%2520leveraging%2520the%2520collective%2520knowledge%2520of%2520multiple%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Routoo%3A%20Learning%20to%20Route%20to%20Large%20Language%20Models%20Effectively&entry.906535625=Alireza%20Mohammadshahi%20and%20Arshad%20Rafiq%20Shaikh%20and%20Majid%20Yazdani&entry.1292438233=%20%20Developing%20foundational%20large%20language%20models%20%28LLMs%29%20is%20becoming%20increasingly%0Acostly%20and%20inefficient.%20Also%2C%20closed-source%20and%20larger%20open-source%20models%0Agenerally%20offer%20better%20response%20quality%20but%20come%20with%20higher%20inference%20costs%0Athan%20smaller%20models.%20In%20this%20paper%2C%20we%20introduce%20Routoo%2C%20an%20architecture%0Adesigned%20to%20optimize%20the%20selection%20of%20LLMs%20for%20specific%20prompts%20based%20on%0Aperformance%2C%20cost%2C%20and%20efficiency.%20Routoo%20consists%20of%20two%20key%20components%3A%20a%0Aperformance%20predictor%20and%20a%20cost-aware%20decoding.%20The%20performance%20predictor%20is%20a%0Alightweight%20LLM%20that%20estimates%20the%20performance%20of%20various%20underlying%20LLMs%0Awithout%20needing%20to%20execute%20and%20evaluate%20them.%20The%20cost-aware%20decoding%20then%0Aselects%20the%20most%20suitable%20model%20based%20on%20these%20predictions%20and%20other%0Aconstraints%20like%20cost%20and%20latency.%20We%20evaluated%20Routoo%20using%20the%20MMLU%20benchmark%0Aacross%2057%20domains%20employing%20open-source%20models.%20Our%20results%20show%20that%20Routoo%0Amatches%20the%20performance%20of%20the%20Mixtral%208x7b%20model%20while%20reducing%20inference%0Acosts%20by%20one-third.%20Additionally%2C%20by%20allowing%20increased%20costs%2C%20Routoo%20surpasses%0AMixtral%27s%20accuracy%20by%20over%205%25%20at%20equivalent%20costs%2C%20achieving%20an%20accuracy%20of%0A75.9%25.%20When%20integrating%20GPT4%20into%20our%20model%20pool%2C%20Routoo%20nearly%20matches%20GPT4%27s%0Aperformance%20at%20half%20the%20cost%20and%20exceeds%20it%20with%20a%2025%25%20cost%20reduction.%20These%0Aoutcomes%20highlight%20Routoo%27s%20potential%20to%20create%20new%20SOTA%20in%20a%20cost-effective%0Amanner%20by%20leveraging%20the%20collective%20knowledge%20of%20multiple%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13979v2&entry.124074799=Read"},
{"title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought", "author": "Ori Yoran and Tomer Wolfson and Ben Bogin and Uri Katz and Daniel Deutch and Jonathan Berant", "abstract": "  Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.\n", "link": "http://arxiv.org/abs/2304.13007v4", "date": "2024-08-02", "relevancy": 1.3341, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4713}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Answering%20Questions%20by%20Meta-Reasoning%20over%20Multiple%20Chains%20of%20Thought&body=Title%3A%20Answering%20Questions%20by%20Meta-Reasoning%20over%20Multiple%20Chains%20of%20Thought%0AAuthor%3A%20Ori%20Yoran%20and%20Tomer%20Wolfson%20and%20Ben%20Bogin%20and%20Uri%20Katz%20and%20Daniel%20Deutch%20and%20Jonathan%20Berant%0AAbstract%3A%20%20%20Modern%20systems%20for%20multi-hop%20question%20answering%20%28QA%29%20typically%20break%0Aquestions%20into%20a%20sequence%20of%20reasoning%20steps%2C%20termed%20chain-of-thought%20%28CoT%29%2C%0Abefore%20arriving%20at%20a%20final%20answer.%20Often%2C%20multiple%20chains%20are%20sampled%20and%0Aaggregated%20through%20a%20voting%20mechanism%20over%20the%20final%20answers%2C%20but%20the%0Aintermediate%20steps%20themselves%20are%20discarded.%20While%20such%20approaches%20improve%0Aperformance%2C%20they%20do%20not%20consider%20the%20relations%20between%20intermediate%20steps%0Aacross%20chains%20and%20do%20not%20provide%20a%20unified%20explanation%20for%20the%20predicted%0Aanswer.%20We%20introduce%20Multi-Chain%20Reasoning%20%28MCR%29%2C%20an%20approach%20which%20prompts%0Alarge%20language%20models%20to%20meta-reason%20over%20multiple%20chains%20of%20thought%2C%20rather%0Athan%20aggregating%20their%20answers.%20MCR%20examines%20different%20reasoning%20chains%2C%20mixes%0Ainformation%20between%20them%20and%20selects%20the%20most%20relevant%20facts%20in%20generating%20an%0Aexplanation%20and%20predicting%20the%20answer.%20MCR%20outperforms%20strong%20baselines%20on%207%0Amulti-hop%20QA%20datasets.%20Moreover%2C%20our%20analysis%20reveals%20that%20MCR%20explanations%0Aexhibit%20high%20quality%2C%20enabling%20humans%20to%20verify%20its%20answers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.13007v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnswering%2520Questions%2520by%2520Meta-Reasoning%2520over%2520Multiple%2520Chains%2520of%2520Thought%26entry.906535625%3DOri%2520Yoran%2520and%2520Tomer%2520Wolfson%2520and%2520Ben%2520Bogin%2520and%2520Uri%2520Katz%2520and%2520Daniel%2520Deutch%2520and%2520Jonathan%2520Berant%26entry.1292438233%3D%2520%2520Modern%2520systems%2520for%2520multi-hop%2520question%2520answering%2520%2528QA%2529%2520typically%2520break%250Aquestions%2520into%2520a%2520sequence%2520of%2520reasoning%2520steps%252C%2520termed%2520chain-of-thought%2520%2528CoT%2529%252C%250Abefore%2520arriving%2520at%2520a%2520final%2520answer.%2520Often%252C%2520multiple%2520chains%2520are%2520sampled%2520and%250Aaggregated%2520through%2520a%2520voting%2520mechanism%2520over%2520the%2520final%2520answers%252C%2520but%2520the%250Aintermediate%2520steps%2520themselves%2520are%2520discarded.%2520While%2520such%2520approaches%2520improve%250Aperformance%252C%2520they%2520do%2520not%2520consider%2520the%2520relations%2520between%2520intermediate%2520steps%250Aacross%2520chains%2520and%2520do%2520not%2520provide%2520a%2520unified%2520explanation%2520for%2520the%2520predicted%250Aanswer.%2520We%2520introduce%2520Multi-Chain%2520Reasoning%2520%2528MCR%2529%252C%2520an%2520approach%2520which%2520prompts%250Alarge%2520language%2520models%2520to%2520meta-reason%2520over%2520multiple%2520chains%2520of%2520thought%252C%2520rather%250Athan%2520aggregating%2520their%2520answers.%2520MCR%2520examines%2520different%2520reasoning%2520chains%252C%2520mixes%250Ainformation%2520between%2520them%2520and%2520selects%2520the%2520most%2520relevant%2520facts%2520in%2520generating%2520an%250Aexplanation%2520and%2520predicting%2520the%2520answer.%2520MCR%2520outperforms%2520strong%2520baselines%2520on%25207%250Amulti-hop%2520QA%2520datasets.%2520Moreover%252C%2520our%2520analysis%2520reveals%2520that%2520MCR%2520explanations%250Aexhibit%2520high%2520quality%252C%2520enabling%2520humans%2520to%2520verify%2520its%2520answers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.13007v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Answering%20Questions%20by%20Meta-Reasoning%20over%20Multiple%20Chains%20of%20Thought&entry.906535625=Ori%20Yoran%20and%20Tomer%20Wolfson%20and%20Ben%20Bogin%20and%20Uri%20Katz%20and%20Daniel%20Deutch%20and%20Jonathan%20Berant&entry.1292438233=%20%20Modern%20systems%20for%20multi-hop%20question%20answering%20%28QA%29%20typically%20break%0Aquestions%20into%20a%20sequence%20of%20reasoning%20steps%2C%20termed%20chain-of-thought%20%28CoT%29%2C%0Abefore%20arriving%20at%20a%20final%20answer.%20Often%2C%20multiple%20chains%20are%20sampled%20and%0Aaggregated%20through%20a%20voting%20mechanism%20over%20the%20final%20answers%2C%20but%20the%0Aintermediate%20steps%20themselves%20are%20discarded.%20While%20such%20approaches%20improve%0Aperformance%2C%20they%20do%20not%20consider%20the%20relations%20between%20intermediate%20steps%0Aacross%20chains%20and%20do%20not%20provide%20a%20unified%20explanation%20for%20the%20predicted%0Aanswer.%20We%20introduce%20Multi-Chain%20Reasoning%20%28MCR%29%2C%20an%20approach%20which%20prompts%0Alarge%20language%20models%20to%20meta-reason%20over%20multiple%20chains%20of%20thought%2C%20rather%0Athan%20aggregating%20their%20answers.%20MCR%20examines%20different%20reasoning%20chains%2C%20mixes%0Ainformation%20between%20them%20and%20selects%20the%20most%20relevant%20facts%20in%20generating%20an%0Aexplanation%20and%20predicting%20the%20answer.%20MCR%20outperforms%20strong%20baselines%20on%207%0Amulti-hop%20QA%20datasets.%20Moreover%2C%20our%20analysis%20reveals%20that%20MCR%20explanations%0Aexhibit%20high%20quality%2C%20enabling%20humans%20to%20verify%20its%20answers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.13007v4&entry.124074799=Read"},
{"title": "Bond Graphs for multi-physics informed Neural Networks for multi-variate\n  time series", "author": "Alexis-Raja Brachet and Pierre-Yves Richard and C\u00e9line Hudelot", "abstract": "  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed\nMachine Learning has seen a growing interest. It operates mainly by imposing\ndata, learning, or architecture bias with simulation data, Partial Differential\nEquations, or equivariance and invariance properties. While it has shown great\nsuccess on tasks involving one physical domain, such as fluid dynamics,\nexisting methods are not adapted to tasks with complex multi-physical and\nmulti-domain phenomena. In addition, it is mainly formulated as an end-to-end\nlearning scheme. To address these challenges, we propose to leverage Bond\nGraphs, a multi-physics modeling approach, together with Message Passing Graph\nNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producing\nmulti-physics-informed representations that can be fed into any task-specific\nmodel. It provides a unified way to integrate both data and architecture biases\nin deep learning. Our experiments on two challenging multi-domain physical\nsystems - a Direct Current Motor and the Respiratory System - demonstrate the\neffectiveness of our approach on a multivariate time-series forecasting task.\n", "link": "http://arxiv.org/abs/2405.13586v2", "date": "2024-08-02", "relevancy": 1.6044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5928}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5286}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bond%20Graphs%20for%20multi-physics%20informed%20Neural%20Networks%20for%20multi-variate%0A%20%20time%20series&body=Title%3A%20Bond%20Graphs%20for%20multi-physics%20informed%20Neural%20Networks%20for%20multi-variate%0A%20%20time%20series%0AAuthor%3A%20Alexis-Raja%20Brachet%20and%20Pierre-Yves%20Richard%20and%20C%C3%A9line%20Hudelot%0AAbstract%3A%20%20%20In%20the%20trend%20of%20hybrid%20Artificial%20Intelligence%20techniques%2C%20Physical-Informed%0AMachine%20Learning%20has%20seen%20a%20growing%20interest.%20It%20operates%20mainly%20by%20imposing%0Adata%2C%20learning%2C%20or%20architecture%20bias%20with%20simulation%20data%2C%20Partial%20Differential%0AEquations%2C%20or%20equivariance%20and%20invariance%20properties.%20While%20it%20has%20shown%20great%0Asuccess%20on%20tasks%20involving%20one%20physical%20domain%2C%20such%20as%20fluid%20dynamics%2C%0Aexisting%20methods%20are%20not%20adapted%20to%20tasks%20with%20complex%20multi-physical%20and%0Amulti-domain%20phenomena.%20In%20addition%2C%20it%20is%20mainly%20formulated%20as%20an%20end-to-end%0Alearning%20scheme.%20To%20address%20these%20challenges%2C%20we%20propose%20to%20leverage%20Bond%0AGraphs%2C%20a%20multi-physics%20modeling%20approach%2C%20together%20with%20Message%20Passing%20Graph%0ANeural%20Networks.%20We%20propose%20a%20Neural%20Bond%20graph%20Encoder%20%28NBgE%29%20producing%0Amulti-physics-informed%20representations%20that%20can%20be%20fed%20into%20any%20task-specific%0Amodel.%20It%20provides%20a%20unified%20way%20to%20integrate%20both%20data%20and%20architecture%20biases%0Ain%20deep%20learning.%20Our%20experiments%20on%20two%20challenging%20multi-domain%20physical%0Asystems%20-%20a%20Direct%20Current%20Motor%20and%20the%20Respiratory%20System%20-%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20a%20multivariate%20time-series%20forecasting%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBond%2520Graphs%2520for%2520multi-physics%2520informed%2520Neural%2520Networks%2520for%2520multi-variate%250A%2520%2520time%2520series%26entry.906535625%3DAlexis-Raja%2520Brachet%2520and%2520Pierre-Yves%2520Richard%2520and%2520C%25C3%25A9line%2520Hudelot%26entry.1292438233%3D%2520%2520In%2520the%2520trend%2520of%2520hybrid%2520Artificial%2520Intelligence%2520techniques%252C%2520Physical-Informed%250AMachine%2520Learning%2520has%2520seen%2520a%2520growing%2520interest.%2520It%2520operates%2520mainly%2520by%2520imposing%250Adata%252C%2520learning%252C%2520or%2520architecture%2520bias%2520with%2520simulation%2520data%252C%2520Partial%2520Differential%250AEquations%252C%2520or%2520equivariance%2520and%2520invariance%2520properties.%2520While%2520it%2520has%2520shown%2520great%250Asuccess%2520on%2520tasks%2520involving%2520one%2520physical%2520domain%252C%2520such%2520as%2520fluid%2520dynamics%252C%250Aexisting%2520methods%2520are%2520not%2520adapted%2520to%2520tasks%2520with%2520complex%2520multi-physical%2520and%250Amulti-domain%2520phenomena.%2520In%2520addition%252C%2520it%2520is%2520mainly%2520formulated%2520as%2520an%2520end-to-end%250Alearning%2520scheme.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520to%2520leverage%2520Bond%250AGraphs%252C%2520a%2520multi-physics%2520modeling%2520approach%252C%2520together%2520with%2520Message%2520Passing%2520Graph%250ANeural%2520Networks.%2520We%2520propose%2520a%2520Neural%2520Bond%2520graph%2520Encoder%2520%2528NBgE%2529%2520producing%250Amulti-physics-informed%2520representations%2520that%2520can%2520be%2520fed%2520into%2520any%2520task-specific%250Amodel.%2520It%2520provides%2520a%2520unified%2520way%2520to%2520integrate%2520both%2520data%2520and%2520architecture%2520biases%250Ain%2520deep%2520learning.%2520Our%2520experiments%2520on%2520two%2520challenging%2520multi-domain%2520physical%250Asystems%2520-%2520a%2520Direct%2520Current%2520Motor%2520and%2520the%2520Respiratory%2520System%2520-%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520a%2520multivariate%2520time-series%2520forecasting%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bond%20Graphs%20for%20multi-physics%20informed%20Neural%20Networks%20for%20multi-variate%0A%20%20time%20series&entry.906535625=Alexis-Raja%20Brachet%20and%20Pierre-Yves%20Richard%20and%20C%C3%A9line%20Hudelot&entry.1292438233=%20%20In%20the%20trend%20of%20hybrid%20Artificial%20Intelligence%20techniques%2C%20Physical-Informed%0AMachine%20Learning%20has%20seen%20a%20growing%20interest.%20It%20operates%20mainly%20by%20imposing%0Adata%2C%20learning%2C%20or%20architecture%20bias%20with%20simulation%20data%2C%20Partial%20Differential%0AEquations%2C%20or%20equivariance%20and%20invariance%20properties.%20While%20it%20has%20shown%20great%0Asuccess%20on%20tasks%20involving%20one%20physical%20domain%2C%20such%20as%20fluid%20dynamics%2C%0Aexisting%20methods%20are%20not%20adapted%20to%20tasks%20with%20complex%20multi-physical%20and%0Amulti-domain%20phenomena.%20In%20addition%2C%20it%20is%20mainly%20formulated%20as%20an%20end-to-end%0Alearning%20scheme.%20To%20address%20these%20challenges%2C%20we%20propose%20to%20leverage%20Bond%0AGraphs%2C%20a%20multi-physics%20modeling%20approach%2C%20together%20with%20Message%20Passing%20Graph%0ANeural%20Networks.%20We%20propose%20a%20Neural%20Bond%20graph%20Encoder%20%28NBgE%29%20producing%0Amulti-physics-informed%20representations%20that%20can%20be%20fed%20into%20any%20task-specific%0Amodel.%20It%20provides%20a%20unified%20way%20to%20integrate%20both%20data%20and%20architecture%20biases%0Ain%20deep%20learning.%20Our%20experiments%20on%20two%20challenging%20multi-domain%20physical%0Asystems%20-%20a%20Direct%20Current%20Motor%20and%20the%20Respiratory%20System%20-%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20a%20multivariate%20time-series%20forecasting%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13586v2&entry.124074799=Read"},
{"title": "LLMs' Understanding of Natural Language Revealed", "author": "Walid S. Saba", "abstract": "  Large language models (LLMs) are the result of a massive experiment in\nbottom-up, data-driven reverse engineering of language at scale. Despite their\nutility in a number of downstream NLP tasks, ample research has shown that LLMs\nare incapable of performing reasoning in tasks that require quantification over\nand the manipulation of symbolic variables (e.g., planning and problem\nsolving); see for example [25][26]. In this document, however, we will focus on\ntesting LLMs for their language understanding capabilities, their supposed\nforte. As we will show here, the language understanding capabilities of LLMs\nhave been widely exaggerated. While LLMs have proven to generate human-like\ncoherent language (since that's how they were designed), their language\nunderstanding capabilities have not been properly tested. In particular, we\nbelieve that the language understanding capabilities of LLMs should be tested\nby performing an operation that is the opposite of 'text generation' and\nspecifically by giving the LLM snippets of text as input and then querying what\nthe LLM \"understood\". As we show here, when doing so it will become apparent\nthat LLMs do not truly understand language, beyond very superficial inferences\nthat are essentially the byproduct of the memorization of massive amounts of\ningested text.\n", "link": "http://arxiv.org/abs/2407.19630v2", "date": "2024-08-02", "relevancy": 1.361, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%27%20Understanding%20of%20Natural%20Language%20Revealed&body=Title%3A%20LLMs%27%20Understanding%20of%20Natural%20Language%20Revealed%0AAuthor%3A%20Walid%20S.%20Saba%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20the%20result%20of%20a%20massive%20experiment%20in%0Abottom-up%2C%20data-driven%20reverse%20engineering%20of%20language%20at%20scale.%20Despite%20their%0Autility%20in%20a%20number%20of%20downstream%20NLP%20tasks%2C%20ample%20research%20has%20shown%20that%20LLMs%0Aare%20incapable%20of%20performing%20reasoning%20in%20tasks%20that%20require%20quantification%20over%0Aand%20the%20manipulation%20of%20symbolic%20variables%20%28e.g.%2C%20planning%20and%20problem%0Asolving%29%3B%20see%20for%20example%20%5B25%5D%5B26%5D.%20In%20this%20document%2C%20however%2C%20we%20will%20focus%20on%0Atesting%20LLMs%20for%20their%20language%20understanding%20capabilities%2C%20their%20supposed%0Aforte.%20As%20we%20will%20show%20here%2C%20the%20language%20understanding%20capabilities%20of%20LLMs%0Ahave%20been%20widely%20exaggerated.%20While%20LLMs%20have%20proven%20to%20generate%20human-like%0Acoherent%20language%20%28since%20that%27s%20how%20they%20were%20designed%29%2C%20their%20language%0Aunderstanding%20capabilities%20have%20not%20been%20properly%20tested.%20In%20particular%2C%20we%0Abelieve%20that%20the%20language%20understanding%20capabilities%20of%20LLMs%20should%20be%20tested%0Aby%20performing%20an%20operation%20that%20is%20the%20opposite%20of%20%27text%20generation%27%20and%0Aspecifically%20by%20giving%20the%20LLM%20snippets%20of%20text%20as%20input%20and%20then%20querying%20what%0Athe%20LLM%20%22understood%22.%20As%20we%20show%20here%2C%20when%20doing%20so%20it%20will%20become%20apparent%0Athat%20LLMs%20do%20not%20truly%20understand%20language%2C%20beyond%20very%20superficial%20inferences%0Athat%20are%20essentially%20the%20byproduct%20of%20the%20memorization%20of%20massive%20amounts%20of%0Aingested%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2527%2520Understanding%2520of%2520Natural%2520Language%2520Revealed%26entry.906535625%3DWalid%2520S.%2520Saba%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520the%2520result%2520of%2520a%2520massive%2520experiment%2520in%250Abottom-up%252C%2520data-driven%2520reverse%2520engineering%2520of%2520language%2520at%2520scale.%2520Despite%2520their%250Autility%2520in%2520a%2520number%2520of%2520downstream%2520NLP%2520tasks%252C%2520ample%2520research%2520has%2520shown%2520that%2520LLMs%250Aare%2520incapable%2520of%2520performing%2520reasoning%2520in%2520tasks%2520that%2520require%2520quantification%2520over%250Aand%2520the%2520manipulation%2520of%2520symbolic%2520variables%2520%2528e.g.%252C%2520planning%2520and%2520problem%250Asolving%2529%253B%2520see%2520for%2520example%2520%255B25%255D%255B26%255D.%2520In%2520this%2520document%252C%2520however%252C%2520we%2520will%2520focus%2520on%250Atesting%2520LLMs%2520for%2520their%2520language%2520understanding%2520capabilities%252C%2520their%2520supposed%250Aforte.%2520As%2520we%2520will%2520show%2520here%252C%2520the%2520language%2520understanding%2520capabilities%2520of%2520LLMs%250Ahave%2520been%2520widely%2520exaggerated.%2520While%2520LLMs%2520have%2520proven%2520to%2520generate%2520human-like%250Acoherent%2520language%2520%2528since%2520that%2527s%2520how%2520they%2520were%2520designed%2529%252C%2520their%2520language%250Aunderstanding%2520capabilities%2520have%2520not%2520been%2520properly%2520tested.%2520In%2520particular%252C%2520we%250Abelieve%2520that%2520the%2520language%2520understanding%2520capabilities%2520of%2520LLMs%2520should%2520be%2520tested%250Aby%2520performing%2520an%2520operation%2520that%2520is%2520the%2520opposite%2520of%2520%2527text%2520generation%2527%2520and%250Aspecifically%2520by%2520giving%2520the%2520LLM%2520snippets%2520of%2520text%2520as%2520input%2520and%2520then%2520querying%2520what%250Athe%2520LLM%2520%2522understood%2522.%2520As%2520we%2520show%2520here%252C%2520when%2520doing%2520so%2520it%2520will%2520become%2520apparent%250Athat%2520LLMs%2520do%2520not%2520truly%2520understand%2520language%252C%2520beyond%2520very%2520superficial%2520inferences%250Athat%2520are%2520essentially%2520the%2520byproduct%2520of%2520the%2520memorization%2520of%2520massive%2520amounts%2520of%250Aingested%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%27%20Understanding%20of%20Natural%20Language%20Revealed&entry.906535625=Walid%20S.%20Saba&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20the%20result%20of%20a%20massive%20experiment%20in%0Abottom-up%2C%20data-driven%20reverse%20engineering%20of%20language%20at%20scale.%20Despite%20their%0Autility%20in%20a%20number%20of%20downstream%20NLP%20tasks%2C%20ample%20research%20has%20shown%20that%20LLMs%0Aare%20incapable%20of%20performing%20reasoning%20in%20tasks%20that%20require%20quantification%20over%0Aand%20the%20manipulation%20of%20symbolic%20variables%20%28e.g.%2C%20planning%20and%20problem%0Asolving%29%3B%20see%20for%20example%20%5B25%5D%5B26%5D.%20In%20this%20document%2C%20however%2C%20we%20will%20focus%20on%0Atesting%20LLMs%20for%20their%20language%20understanding%20capabilities%2C%20their%20supposed%0Aforte.%20As%20we%20will%20show%20here%2C%20the%20language%20understanding%20capabilities%20of%20LLMs%0Ahave%20been%20widely%20exaggerated.%20While%20LLMs%20have%20proven%20to%20generate%20human-like%0Acoherent%20language%20%28since%20that%27s%20how%20they%20were%20designed%29%2C%20their%20language%0Aunderstanding%20capabilities%20have%20not%20been%20properly%20tested.%20In%20particular%2C%20we%0Abelieve%20that%20the%20language%20understanding%20capabilities%20of%20LLMs%20should%20be%20tested%0Aby%20performing%20an%20operation%20that%20is%20the%20opposite%20of%20%27text%20generation%27%20and%0Aspecifically%20by%20giving%20the%20LLM%20snippets%20of%20text%20as%20input%20and%20then%20querying%20what%0Athe%20LLM%20%22understood%22.%20As%20we%20show%20here%2C%20when%20doing%20so%20it%20will%20become%20apparent%0Athat%20LLMs%20do%20not%20truly%20understand%20language%2C%20beyond%20very%20superficial%20inferences%0Athat%20are%20essentially%20the%20byproduct%20of%20the%20memorization%20of%20massive%20amounts%20of%0Aingested%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19630v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


