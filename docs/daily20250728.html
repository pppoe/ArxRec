<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250727.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars", "author": "Byungjun Kim and Shunsuke Saito and Giljoo Nam and Tomas Simon and Jason Saragih and Hanbyul Joo and Junxuan Li", "abstract": "  We present a universal prior model for 3D head avatars with explicit hair\ncompositionality. Existing approaches to build generalizable priors for 3D head\navatars often adopt a holistic modeling approach, treating the face and hair as\nan inseparable entity. This overlooks the inherent compositionality of the\nhuman head, making it difficult for the model to naturally disentangle face and\nhair representations, especially when the dataset is limited. Furthermore, such\nholistic models struggle to support applications like 3D face and hairstyle\nswapping in a flexible and controllable manner. To address these challenges, we\nintroduce a prior model that explicitly accounts for the compositionality of\nface and hair, learning their latent spaces separately. A key enabler of this\napproach is our synthetic hairless data creation pipeline, which removes hair\nfrom studio-captured datasets using estimated hairless geometry and texture\nderived from a diffusion prior. By leveraging a paired dataset of hair and\nhairless captures, we train disentangled prior models for face and hair,\nincorporating compositionality as an inductive bias to facilitate effective\nseparation. Our model's inherent compositionality enables seamless transfer of\nface and hair components between avatars while preserving identity.\nAdditionally, we demonstrate that our model can be fine-tuned in a few-shot\nmanner using monocular captures to create high-fidelity, hair-compositional 3D\nhead avatars for unseen subjects. These capabilities highlight the practical\napplicability of our approach in real-world scenarios, paving the way for\nflexible and expressive 3D avatar generation.\n", "link": "http://arxiv.org/abs/2507.19481v1", "date": "2025-07-25", "relevancy": 3.3518, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6922}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6922}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HairCUP%3A%20Hair%20Compositional%20Universal%20Prior%20for%203D%20Gaussian%20Avatars&body=Title%3A%20HairCUP%3A%20Hair%20Compositional%20Universal%20Prior%20for%203D%20Gaussian%20Avatars%0AAuthor%3A%20Byungjun%20Kim%20and%20Shunsuke%20Saito%20and%20Giljoo%20Nam%20and%20Tomas%20Simon%20and%20Jason%20Saragih%20and%20Hanbyul%20Joo%20and%20Junxuan%20Li%0AAbstract%3A%20%20%20We%20present%20a%20universal%20prior%20model%20for%203D%20head%20avatars%20with%20explicit%20hair%0Acompositionality.%20Existing%20approaches%20to%20build%20generalizable%20priors%20for%203D%20head%0Aavatars%20often%20adopt%20a%20holistic%20modeling%20approach%2C%20treating%20the%20face%20and%20hair%20as%0Aan%20inseparable%20entity.%20This%20overlooks%20the%20inherent%20compositionality%20of%20the%0Ahuman%20head%2C%20making%20it%20difficult%20for%20the%20model%20to%20naturally%20disentangle%20face%20and%0Ahair%20representations%2C%20especially%20when%20the%20dataset%20is%20limited.%20Furthermore%2C%20such%0Aholistic%20models%20struggle%20to%20support%20applications%20like%203D%20face%20and%20hairstyle%0Aswapping%20in%20a%20flexible%20and%20controllable%20manner.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20prior%20model%20that%20explicitly%20accounts%20for%20the%20compositionality%20of%0Aface%20and%20hair%2C%20learning%20their%20latent%20spaces%20separately.%20A%20key%20enabler%20of%20this%0Aapproach%20is%20our%20synthetic%20hairless%20data%20creation%20pipeline%2C%20which%20removes%20hair%0Afrom%20studio-captured%20datasets%20using%20estimated%20hairless%20geometry%20and%20texture%0Aderived%20from%20a%20diffusion%20prior.%20By%20leveraging%20a%20paired%20dataset%20of%20hair%20and%0Ahairless%20captures%2C%20we%20train%20disentangled%20prior%20models%20for%20face%20and%20hair%2C%0Aincorporating%20compositionality%20as%20an%20inductive%20bias%20to%20facilitate%20effective%0Aseparation.%20Our%20model%27s%20inherent%20compositionality%20enables%20seamless%20transfer%20of%0Aface%20and%20hair%20components%20between%20avatars%20while%20preserving%20identity.%0AAdditionally%2C%20we%20demonstrate%20that%20our%20model%20can%20be%20fine-tuned%20in%20a%20few-shot%0Amanner%20using%20monocular%20captures%20to%20create%20high-fidelity%2C%20hair-compositional%203D%0Ahead%20avatars%20for%20unseen%20subjects.%20These%20capabilities%20highlight%20the%20practical%0Aapplicability%20of%20our%20approach%20in%20real-world%20scenarios%2C%20paving%20the%20way%20for%0Aflexible%20and%20expressive%203D%20avatar%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHairCUP%253A%2520Hair%2520Compositional%2520Universal%2520Prior%2520for%25203D%2520Gaussian%2520Avatars%26entry.906535625%3DByungjun%2520Kim%2520and%2520Shunsuke%2520Saito%2520and%2520Giljoo%2520Nam%2520and%2520Tomas%2520Simon%2520and%2520Jason%2520Saragih%2520and%2520Hanbyul%2520Joo%2520and%2520Junxuan%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520universal%2520prior%2520model%2520for%25203D%2520head%2520avatars%2520with%2520explicit%2520hair%250Acompositionality.%2520Existing%2520approaches%2520to%2520build%2520generalizable%2520priors%2520for%25203D%2520head%250Aavatars%2520often%2520adopt%2520a%2520holistic%2520modeling%2520approach%252C%2520treating%2520the%2520face%2520and%2520hair%2520as%250Aan%2520inseparable%2520entity.%2520This%2520overlooks%2520the%2520inherent%2520compositionality%2520of%2520the%250Ahuman%2520head%252C%2520making%2520it%2520difficult%2520for%2520the%2520model%2520to%2520naturally%2520disentangle%2520face%2520and%250Ahair%2520representations%252C%2520especially%2520when%2520the%2520dataset%2520is%2520limited.%2520Furthermore%252C%2520such%250Aholistic%2520models%2520struggle%2520to%2520support%2520applications%2520like%25203D%2520face%2520and%2520hairstyle%250Aswapping%2520in%2520a%2520flexible%2520and%2520controllable%2520manner.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520prior%2520model%2520that%2520explicitly%2520accounts%2520for%2520the%2520compositionality%2520of%250Aface%2520and%2520hair%252C%2520learning%2520their%2520latent%2520spaces%2520separately.%2520A%2520key%2520enabler%2520of%2520this%250Aapproach%2520is%2520our%2520synthetic%2520hairless%2520data%2520creation%2520pipeline%252C%2520which%2520removes%2520hair%250Afrom%2520studio-captured%2520datasets%2520using%2520estimated%2520hairless%2520geometry%2520and%2520texture%250Aderived%2520from%2520a%2520diffusion%2520prior.%2520By%2520leveraging%2520a%2520paired%2520dataset%2520of%2520hair%2520and%250Ahairless%2520captures%252C%2520we%2520train%2520disentangled%2520prior%2520models%2520for%2520face%2520and%2520hair%252C%250Aincorporating%2520compositionality%2520as%2520an%2520inductive%2520bias%2520to%2520facilitate%2520effective%250Aseparation.%2520Our%2520model%2527s%2520inherent%2520compositionality%2520enables%2520seamless%2520transfer%2520of%250Aface%2520and%2520hair%2520components%2520between%2520avatars%2520while%2520preserving%2520identity.%250AAdditionally%252C%2520we%2520demonstrate%2520that%2520our%2520model%2520can%2520be%2520fine-tuned%2520in%2520a%2520few-shot%250Amanner%2520using%2520monocular%2520captures%2520to%2520create%2520high-fidelity%252C%2520hair-compositional%25203D%250Ahead%2520avatars%2520for%2520unseen%2520subjects.%2520These%2520capabilities%2520highlight%2520the%2520practical%250Aapplicability%2520of%2520our%2520approach%2520in%2520real-world%2520scenarios%252C%2520paving%2520the%2520way%2520for%250Aflexible%2520and%2520expressive%25203D%2520avatar%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HairCUP%3A%20Hair%20Compositional%20Universal%20Prior%20for%203D%20Gaussian%20Avatars&entry.906535625=Byungjun%20Kim%20and%20Shunsuke%20Saito%20and%20Giljoo%20Nam%20and%20Tomas%20Simon%20and%20Jason%20Saragih%20and%20Hanbyul%20Joo%20and%20Junxuan%20Li&entry.1292438233=%20%20We%20present%20a%20universal%20prior%20model%20for%203D%20head%20avatars%20with%20explicit%20hair%0Acompositionality.%20Existing%20approaches%20to%20build%20generalizable%20priors%20for%203D%20head%0Aavatars%20often%20adopt%20a%20holistic%20modeling%20approach%2C%20treating%20the%20face%20and%20hair%20as%0Aan%20inseparable%20entity.%20This%20overlooks%20the%20inherent%20compositionality%20of%20the%0Ahuman%20head%2C%20making%20it%20difficult%20for%20the%20model%20to%20naturally%20disentangle%20face%20and%0Ahair%20representations%2C%20especially%20when%20the%20dataset%20is%20limited.%20Furthermore%2C%20such%0Aholistic%20models%20struggle%20to%20support%20applications%20like%203D%20face%20and%20hairstyle%0Aswapping%20in%20a%20flexible%20and%20controllable%20manner.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20prior%20model%20that%20explicitly%20accounts%20for%20the%20compositionality%20of%0Aface%20and%20hair%2C%20learning%20their%20latent%20spaces%20separately.%20A%20key%20enabler%20of%20this%0Aapproach%20is%20our%20synthetic%20hairless%20data%20creation%20pipeline%2C%20which%20removes%20hair%0Afrom%20studio-captured%20datasets%20using%20estimated%20hairless%20geometry%20and%20texture%0Aderived%20from%20a%20diffusion%20prior.%20By%20leveraging%20a%20paired%20dataset%20of%20hair%20and%0Ahairless%20captures%2C%20we%20train%20disentangled%20prior%20models%20for%20face%20and%20hair%2C%0Aincorporating%20compositionality%20as%20an%20inductive%20bias%20to%20facilitate%20effective%0Aseparation.%20Our%20model%27s%20inherent%20compositionality%20enables%20seamless%20transfer%20of%0Aface%20and%20hair%20components%20between%20avatars%20while%20preserving%20identity.%0AAdditionally%2C%20we%20demonstrate%20that%20our%20model%20can%20be%20fine-tuned%20in%20a%20few-shot%0Amanner%20using%20monocular%20captures%20to%20create%20high-fidelity%2C%20hair-compositional%203D%0Ahead%20avatars%20for%20unseen%20subjects.%20These%20capabilities%20highlight%20the%20practical%0Aapplicability%20of%20our%20approach%20in%20real-world%20scenarios%2C%20paving%20the%20way%20for%0Aflexible%20and%20expressive%203D%20avatar%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19481v1&entry.124074799=Read"},
{"title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous\n  Driving with Gaussian Splatting", "author": "Baijun Ye and Minghui Qin and Saining Zhang and Moonjun Gong and Shaoting Zhu and Zebang Shen and Luan Zhang and Lu Zhang and Hao Zhao and Hang Zhao", "abstract": "  Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for autonomous driving perception. Project\nPage: https://gs-occ3d.github.io/\n", "link": "http://arxiv.org/abs/2507.19451v1", "date": "2025-07-25", "relevancy": 3.2517, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6807}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6364}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting&body=Title%3A%20GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting%0AAuthor%3A%20Baijun%20Ye%20and%20Minghui%20Qin%20and%20Saining%20Zhang%20and%20Moonjun%20Gong%20and%20Shaoting%20Zhu%20and%20Zebang%20Shen%20and%20Luan%20Zhang%20and%20Lu%20Zhang%20and%20Hao%20Zhao%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Occupancy%20is%20crucial%20for%20autonomous%20driving%2C%20providing%20essential%20geometric%0Apriors%20for%20perception%20and%20planning.%20However%2C%20existing%20methods%20predominantly%0Arely%20on%20LiDAR-based%20occupancy%20annotations%2C%20which%20limits%20scalability%20and%0Aprevents%20leveraging%20vast%20amounts%20of%20potential%20crowdsourced%20data%20for%0Aauto-labeling.%20To%20address%20this%2C%20we%20propose%20GS-Occ3D%2C%20a%20scalable%20vision-only%0Aframework%20that%20directly%20reconstructs%20occupancy.%20Vision-only%20occupancy%0Areconstruction%20poses%20significant%20challenges%20due%20to%20sparse%20viewpoints%2C%20dynamic%0Ascene%20elements%2C%20severe%20occlusions%2C%20and%20long-horizon%20motion.%20Existing%0Avision-based%20methods%20primarily%20rely%20on%20mesh%20representation%2C%20which%20suffer%20from%0Aincomplete%20geometry%20and%20additional%20post-processing%2C%20limiting%20scalability.%20To%0Aovercome%20these%20issues%2C%20GS-Occ3D%20optimizes%20an%20explicit%20occupancy%20representation%0Ausing%20an%20Octree-based%20Gaussian%20Surfel%20formulation%2C%20ensuring%20efficiency%20and%0Ascalability.%20Additionally%2C%20we%20decompose%20scenes%20into%20static%20background%2C%20ground%2C%0Aand%20dynamic%20objects%2C%20enabling%20tailored%20modeling%20strategies%3A%20%281%29%20Ground%20is%0Aexplicitly%20reconstructed%20as%20a%20dominant%20structural%20element%2C%20significantly%0Aimproving%20large-area%20consistency%3B%20%282%29%20Dynamic%20vehicles%20are%20separately%20modeled%0Ato%20better%20capture%20motion-related%20occupancy%20patterns.%20Extensive%20experiments%20on%0Athe%20Waymo%20dataset%20demonstrate%20that%20GS-Occ3D%20achieves%20state-of-the-art%20geometry%0Areconstruction%20results.%20By%20curating%20vision-only%20binary%20occupancy%20labels%20from%0Adiverse%20urban%20scenes%2C%20we%20show%20their%20effectiveness%20for%20downstream%20occupancy%0Amodels%20on%20Occ3D-Waymo%20and%20superior%20zero-shot%20generalization%20on%20Occ3D-nuScenes.%0AIt%20highlights%20the%20potential%20of%20large-scale%20vision-based%20occupancy%0Areconstruction%20as%20a%20new%20paradigm%20for%20autonomous%20driving%20perception.%20Project%0APage%3A%20https%3A//gs-occ3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Occ3D%253A%2520Scaling%2520Vision-only%2520Occupancy%2520Reconstruction%2520for%2520Autonomous%250A%2520%2520Driving%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DBaijun%2520Ye%2520and%2520Minghui%2520Qin%2520and%2520Saining%2520Zhang%2520and%2520Moonjun%2520Gong%2520and%2520Shaoting%2520Zhu%2520and%2520Zebang%2520Shen%2520and%2520Luan%2520Zhang%2520and%2520Lu%2520Zhang%2520and%2520Hao%2520Zhao%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Occupancy%2520is%2520crucial%2520for%2520autonomous%2520driving%252C%2520providing%2520essential%2520geometric%250Apriors%2520for%2520perception%2520and%2520planning.%2520However%252C%2520existing%2520methods%2520predominantly%250Arely%2520on%2520LiDAR-based%2520occupancy%2520annotations%252C%2520which%2520limits%2520scalability%2520and%250Aprevents%2520leveraging%2520vast%2520amounts%2520of%2520potential%2520crowdsourced%2520data%2520for%250Aauto-labeling.%2520To%2520address%2520this%252C%2520we%2520propose%2520GS-Occ3D%252C%2520a%2520scalable%2520vision-only%250Aframework%2520that%2520directly%2520reconstructs%2520occupancy.%2520Vision-only%2520occupancy%250Areconstruction%2520poses%2520significant%2520challenges%2520due%2520to%2520sparse%2520viewpoints%252C%2520dynamic%250Ascene%2520elements%252C%2520severe%2520occlusions%252C%2520and%2520long-horizon%2520motion.%2520Existing%250Avision-based%2520methods%2520primarily%2520rely%2520on%2520mesh%2520representation%252C%2520which%2520suffer%2520from%250Aincomplete%2520geometry%2520and%2520additional%2520post-processing%252C%2520limiting%2520scalability.%2520To%250Aovercome%2520these%2520issues%252C%2520GS-Occ3D%2520optimizes%2520an%2520explicit%2520occupancy%2520representation%250Ausing%2520an%2520Octree-based%2520Gaussian%2520Surfel%2520formulation%252C%2520ensuring%2520efficiency%2520and%250Ascalability.%2520Additionally%252C%2520we%2520decompose%2520scenes%2520into%2520static%2520background%252C%2520ground%252C%250Aand%2520dynamic%2520objects%252C%2520enabling%2520tailored%2520modeling%2520strategies%253A%2520%25281%2529%2520Ground%2520is%250Aexplicitly%2520reconstructed%2520as%2520a%2520dominant%2520structural%2520element%252C%2520significantly%250Aimproving%2520large-area%2520consistency%253B%2520%25282%2529%2520Dynamic%2520vehicles%2520are%2520separately%2520modeled%250Ato%2520better%2520capture%2520motion-related%2520occupancy%2520patterns.%2520Extensive%2520experiments%2520on%250Athe%2520Waymo%2520dataset%2520demonstrate%2520that%2520GS-Occ3D%2520achieves%2520state-of-the-art%2520geometry%250Areconstruction%2520results.%2520By%2520curating%2520vision-only%2520binary%2520occupancy%2520labels%2520from%250Adiverse%2520urban%2520scenes%252C%2520we%2520show%2520their%2520effectiveness%2520for%2520downstream%2520occupancy%250Amodels%2520on%2520Occ3D-Waymo%2520and%2520superior%2520zero-shot%2520generalization%2520on%2520Occ3D-nuScenes.%250AIt%2520highlights%2520the%2520potential%2520of%2520large-scale%2520vision-based%2520occupancy%250Areconstruction%2520as%2520a%2520new%2520paradigm%2520for%2520autonomous%2520driving%2520perception.%2520Project%250APage%253A%2520https%253A//gs-occ3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Occ3D%3A%20Scaling%20Vision-only%20Occupancy%20Reconstruction%20for%20Autonomous%0A%20%20Driving%20with%20Gaussian%20Splatting&entry.906535625=Baijun%20Ye%20and%20Minghui%20Qin%20and%20Saining%20Zhang%20and%20Moonjun%20Gong%20and%20Shaoting%20Zhu%20and%20Zebang%20Shen%20and%20Luan%20Zhang%20and%20Lu%20Zhang%20and%20Hao%20Zhao%20and%20Hang%20Zhao&entry.1292438233=%20%20Occupancy%20is%20crucial%20for%20autonomous%20driving%2C%20providing%20essential%20geometric%0Apriors%20for%20perception%20and%20planning.%20However%2C%20existing%20methods%20predominantly%0Arely%20on%20LiDAR-based%20occupancy%20annotations%2C%20which%20limits%20scalability%20and%0Aprevents%20leveraging%20vast%20amounts%20of%20potential%20crowdsourced%20data%20for%0Aauto-labeling.%20To%20address%20this%2C%20we%20propose%20GS-Occ3D%2C%20a%20scalable%20vision-only%0Aframework%20that%20directly%20reconstructs%20occupancy.%20Vision-only%20occupancy%0Areconstruction%20poses%20significant%20challenges%20due%20to%20sparse%20viewpoints%2C%20dynamic%0Ascene%20elements%2C%20severe%20occlusions%2C%20and%20long-horizon%20motion.%20Existing%0Avision-based%20methods%20primarily%20rely%20on%20mesh%20representation%2C%20which%20suffer%20from%0Aincomplete%20geometry%20and%20additional%20post-processing%2C%20limiting%20scalability.%20To%0Aovercome%20these%20issues%2C%20GS-Occ3D%20optimizes%20an%20explicit%20occupancy%20representation%0Ausing%20an%20Octree-based%20Gaussian%20Surfel%20formulation%2C%20ensuring%20efficiency%20and%0Ascalability.%20Additionally%2C%20we%20decompose%20scenes%20into%20static%20background%2C%20ground%2C%0Aand%20dynamic%20objects%2C%20enabling%20tailored%20modeling%20strategies%3A%20%281%29%20Ground%20is%0Aexplicitly%20reconstructed%20as%20a%20dominant%20structural%20element%2C%20significantly%0Aimproving%20large-area%20consistency%3B%20%282%29%20Dynamic%20vehicles%20are%20separately%20modeled%0Ato%20better%20capture%20motion-related%20occupancy%20patterns.%20Extensive%20experiments%20on%0Athe%20Waymo%20dataset%20demonstrate%20that%20GS-Occ3D%20achieves%20state-of-the-art%20geometry%0Areconstruction%20results.%20By%20curating%20vision-only%20binary%20occupancy%20labels%20from%0Adiverse%20urban%20scenes%2C%20we%20show%20their%20effectiveness%20for%20downstream%20occupancy%0Amodels%20on%20Occ3D-Waymo%20and%20superior%20zero-shot%20generalization%20on%20Occ3D-nuScenes.%0AIt%20highlights%20the%20potential%20of%20large-scale%20vision-based%20occupancy%0Areconstruction%20as%20a%20new%20paradigm%20for%20autonomous%20driving%20perception.%20Project%0APage%3A%20https%3A//gs-occ3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19451v1&entry.124074799=Read"},
{"title": "Back to the Features: DINO as a Foundation for Video World Models", "author": "Federico Baldassarre and Marc Szafraniec and Basile Terver and Vasil Khalidov and Francisco Massa and Yann LeCun and Patrick Labatut and Maximilian Seitzer and Piotr Bojanowski", "abstract": "  We present DINO-world, a powerful generalist video world model trained to\npredict future frames in the latent space of DINOv2. By leveraging a\npre-trained image encoder and training a future predictor on a large-scale\nuncurated video dataset, DINO-world learns the temporal dynamics of diverse\nscenes, from driving and indoor scenes to simulated environments. We show that\nDINO-world outperforms previous models on a variety of video prediction\nbenchmarks, e.g. segmentation and depth forecasting, and demonstrates strong\nunderstanding of intuitive physics. Furthermore, we show that it is possible to\nfine-tune the predictor on observation-action trajectories. The resulting\naction-conditioned world model can be used for planning by simulating candidate\ntrajectories in latent space.\n", "link": "http://arxiv.org/abs/2507.19468v1", "date": "2025-07-25", "relevancy": 3.1, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6304}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20to%20the%20Features%3A%20DINO%20as%20a%20Foundation%20for%20Video%20World%20Models&body=Title%3A%20Back%20to%20the%20Features%3A%20DINO%20as%20a%20Foundation%20for%20Video%20World%20Models%0AAuthor%3A%20Federico%20Baldassarre%20and%20Marc%20Szafraniec%20and%20Basile%20Terver%20and%20Vasil%20Khalidov%20and%20Francisco%20Massa%20and%20Yann%20LeCun%20and%20Patrick%20Labatut%20and%20Maximilian%20Seitzer%20and%20Piotr%20Bojanowski%0AAbstract%3A%20%20%20We%20present%20DINO-world%2C%20a%20powerful%20generalist%20video%20world%20model%20trained%20to%0Apredict%20future%20frames%20in%20the%20latent%20space%20of%20DINOv2.%20By%20leveraging%20a%0Apre-trained%20image%20encoder%20and%20training%20a%20future%20predictor%20on%20a%20large-scale%0Auncurated%20video%20dataset%2C%20DINO-world%20learns%20the%20temporal%20dynamics%20of%20diverse%0Ascenes%2C%20from%20driving%20and%20indoor%20scenes%20to%20simulated%20environments.%20We%20show%20that%0ADINO-world%20outperforms%20previous%20models%20on%20a%20variety%20of%20video%20prediction%0Abenchmarks%2C%20e.g.%20segmentation%20and%20depth%20forecasting%2C%20and%20demonstrates%20strong%0Aunderstanding%20of%20intuitive%20physics.%20Furthermore%2C%20we%20show%20that%20it%20is%20possible%20to%0Afine-tune%20the%20predictor%20on%20observation-action%20trajectories.%20The%20resulting%0Aaction-conditioned%20world%20model%20can%20be%20used%20for%20planning%20by%20simulating%20candidate%0Atrajectories%20in%20latent%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520to%2520the%2520Features%253A%2520DINO%2520as%2520a%2520Foundation%2520for%2520Video%2520World%2520Models%26entry.906535625%3DFederico%2520Baldassarre%2520and%2520Marc%2520Szafraniec%2520and%2520Basile%2520Terver%2520and%2520Vasil%2520Khalidov%2520and%2520Francisco%2520Massa%2520and%2520Yann%2520LeCun%2520and%2520Patrick%2520Labatut%2520and%2520Maximilian%2520Seitzer%2520and%2520Piotr%2520Bojanowski%26entry.1292438233%3D%2520%2520We%2520present%2520DINO-world%252C%2520a%2520powerful%2520generalist%2520video%2520world%2520model%2520trained%2520to%250Apredict%2520future%2520frames%2520in%2520the%2520latent%2520space%2520of%2520DINOv2.%2520By%2520leveraging%2520a%250Apre-trained%2520image%2520encoder%2520and%2520training%2520a%2520future%2520predictor%2520on%2520a%2520large-scale%250Auncurated%2520video%2520dataset%252C%2520DINO-world%2520learns%2520the%2520temporal%2520dynamics%2520of%2520diverse%250Ascenes%252C%2520from%2520driving%2520and%2520indoor%2520scenes%2520to%2520simulated%2520environments.%2520We%2520show%2520that%250ADINO-world%2520outperforms%2520previous%2520models%2520on%2520a%2520variety%2520of%2520video%2520prediction%250Abenchmarks%252C%2520e.g.%2520segmentation%2520and%2520depth%2520forecasting%252C%2520and%2520demonstrates%2520strong%250Aunderstanding%2520of%2520intuitive%2520physics.%2520Furthermore%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%250Afine-tune%2520the%2520predictor%2520on%2520observation-action%2520trajectories.%2520The%2520resulting%250Aaction-conditioned%2520world%2520model%2520can%2520be%2520used%2520for%2520planning%2520by%2520simulating%2520candidate%250Atrajectories%2520in%2520latent%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%20the%20Features%3A%20DINO%20as%20a%20Foundation%20for%20Video%20World%20Models&entry.906535625=Federico%20Baldassarre%20and%20Marc%20Szafraniec%20and%20Basile%20Terver%20and%20Vasil%20Khalidov%20and%20Francisco%20Massa%20and%20Yann%20LeCun%20and%20Patrick%20Labatut%20and%20Maximilian%20Seitzer%20and%20Piotr%20Bojanowski&entry.1292438233=%20%20We%20present%20DINO-world%2C%20a%20powerful%20generalist%20video%20world%20model%20trained%20to%0Apredict%20future%20frames%20in%20the%20latent%20space%20of%20DINOv2.%20By%20leveraging%20a%0Apre-trained%20image%20encoder%20and%20training%20a%20future%20predictor%20on%20a%20large-scale%0Auncurated%20video%20dataset%2C%20DINO-world%20learns%20the%20temporal%20dynamics%20of%20diverse%0Ascenes%2C%20from%20driving%20and%20indoor%20scenes%20to%20simulated%20environments.%20We%20show%20that%0ADINO-world%20outperforms%20previous%20models%20on%20a%20variety%20of%20video%20prediction%0Abenchmarks%2C%20e.g.%20segmentation%20and%20depth%20forecasting%2C%20and%20demonstrates%20strong%0Aunderstanding%20of%20intuitive%20physics.%20Furthermore%2C%20we%20show%20that%20it%20is%20possible%20to%0Afine-tune%20the%20predictor%20on%20observation-action%20trajectories.%20The%20resulting%0Aaction-conditioned%20world%20model%20can%20be%20used%20for%20planning%20by%20simulating%20candidate%0Atrajectories%20in%20latent%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19468v1&entry.124074799=Read"},
{"title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit\n  Tracing", "author": "Yiming Zhang and Chengzhang Yu and Zhuokai Zhao and Kun Wang and Qiankun Li and Zihan Chen and Yang Liu and Zenghui Ding and Yining Sun", "abstract": "  The processing mechanisms underlying language and image understanding in\nlarge vision-language models (LVLMs) have been extensively studied. However,\nthe internal reasoning mechanisms of LVLMs for spatiotemporal understanding\nremain poorly understood. In this work, we introduce a systematic,\ncircuit-based framework designed to investigate how spatiotemporal visual\nsemantics are represented and processed within these LVLMs. Specifically, our\nframework comprises three circuits: visual auditing circuit, semantic tracing\ncircuit, and attention flow circuit. Through the lens of these circuits, we\ndiscover that visual semantics are highly localized to specific object\ntokens--removing these tokens can degrade model performance by up to 92.6%.\nFurthermore, we identify that interpretable concepts of objects and actions\nemerge and become progressively refined in the middle-to-late layers of LVLMs.\nIn contrary to the current works that solely focus on objects in one image, we\nreveal that the middle-to-late layers of LVLMs exhibit specialized functional\nlocalization for spatiotemporal semantics. Our findings offer significant\nmechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a\nfoundation for designing more robust and interpretable models.\n", "link": "http://arxiv.org/abs/2507.19420v1", "date": "2025-07-25", "relevancy": 3.0858, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CircuitProbe%3A%20Dissecting%20Spatiotemporal%20Visual%20Semantics%20with%20Circuit%0A%20%20Tracing&body=Title%3A%20CircuitProbe%3A%20Dissecting%20Spatiotemporal%20Visual%20Semantics%20with%20Circuit%0A%20%20Tracing%0AAuthor%3A%20Yiming%20Zhang%20and%20Chengzhang%20Yu%20and%20Zhuokai%20Zhao%20and%20Kun%20Wang%20and%20Qiankun%20Li%20and%20Zihan%20Chen%20and%20Yang%20Liu%20and%20Zenghui%20Ding%20and%20Yining%20Sun%0AAbstract%3A%20%20%20The%20processing%20mechanisms%20underlying%20language%20and%20image%20understanding%20in%0Alarge%20vision-language%20models%20%28LVLMs%29%20have%20been%20extensively%20studied.%20However%2C%0Athe%20internal%20reasoning%20mechanisms%20of%20LVLMs%20for%20spatiotemporal%20understanding%0Aremain%20poorly%20understood.%20In%20this%20work%2C%20we%20introduce%20a%20systematic%2C%0Acircuit-based%20framework%20designed%20to%20investigate%20how%20spatiotemporal%20visual%0Asemantics%20are%20represented%20and%20processed%20within%20these%20LVLMs.%20Specifically%2C%20our%0Aframework%20comprises%20three%20circuits%3A%20visual%20auditing%20circuit%2C%20semantic%20tracing%0Acircuit%2C%20and%20attention%20flow%20circuit.%20Through%20the%20lens%20of%20these%20circuits%2C%20we%0Adiscover%20that%20visual%20semantics%20are%20highly%20localized%20to%20specific%20object%0Atokens--removing%20these%20tokens%20can%20degrade%20model%20performance%20by%20up%20to%2092.6%25.%0AFurthermore%2C%20we%20identify%20that%20interpretable%20concepts%20of%20objects%20and%20actions%0Aemerge%20and%20become%20progressively%20refined%20in%20the%20middle-to-late%20layers%20of%20LVLMs.%0AIn%20contrary%20to%20the%20current%20works%20that%20solely%20focus%20on%20objects%20in%20one%20image%2C%20we%0Areveal%20that%20the%20middle-to-late%20layers%20of%20LVLMs%20exhibit%20specialized%20functional%0Alocalization%20for%20spatiotemporal%20semantics.%20Our%20findings%20offer%20significant%0Amechanistic%20insights%20into%20spatiotemporal%20semantics%20analysis%20of%20LVLMs%2C%20laying%20a%0Afoundation%20for%20designing%20more%20robust%20and%20interpretable%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCircuitProbe%253A%2520Dissecting%2520Spatiotemporal%2520Visual%2520Semantics%2520with%2520Circuit%250A%2520%2520Tracing%26entry.906535625%3DYiming%2520Zhang%2520and%2520Chengzhang%2520Yu%2520and%2520Zhuokai%2520Zhao%2520and%2520Kun%2520Wang%2520and%2520Qiankun%2520Li%2520and%2520Zihan%2520Chen%2520and%2520Yang%2520Liu%2520and%2520Zenghui%2520Ding%2520and%2520Yining%2520Sun%26entry.1292438233%3D%2520%2520The%2520processing%2520mechanisms%2520underlying%2520language%2520and%2520image%2520understanding%2520in%250Alarge%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520been%2520extensively%2520studied.%2520However%252C%250Athe%2520internal%2520reasoning%2520mechanisms%2520of%2520LVLMs%2520for%2520spatiotemporal%2520understanding%250Aremain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520systematic%252C%250Acircuit-based%2520framework%2520designed%2520to%2520investigate%2520how%2520spatiotemporal%2520visual%250Asemantics%2520are%2520represented%2520and%2520processed%2520within%2520these%2520LVLMs.%2520Specifically%252C%2520our%250Aframework%2520comprises%2520three%2520circuits%253A%2520visual%2520auditing%2520circuit%252C%2520semantic%2520tracing%250Acircuit%252C%2520and%2520attention%2520flow%2520circuit.%2520Through%2520the%2520lens%2520of%2520these%2520circuits%252C%2520we%250Adiscover%2520that%2520visual%2520semantics%2520are%2520highly%2520localized%2520to%2520specific%2520object%250Atokens--removing%2520these%2520tokens%2520can%2520degrade%2520model%2520performance%2520by%2520up%2520to%252092.6%2525.%250AFurthermore%252C%2520we%2520identify%2520that%2520interpretable%2520concepts%2520of%2520objects%2520and%2520actions%250Aemerge%2520and%2520become%2520progressively%2520refined%2520in%2520the%2520middle-to-late%2520layers%2520of%2520LVLMs.%250AIn%2520contrary%2520to%2520the%2520current%2520works%2520that%2520solely%2520focus%2520on%2520objects%2520in%2520one%2520image%252C%2520we%250Areveal%2520that%2520the%2520middle-to-late%2520layers%2520of%2520LVLMs%2520exhibit%2520specialized%2520functional%250Alocalization%2520for%2520spatiotemporal%2520semantics.%2520Our%2520findings%2520offer%2520significant%250Amechanistic%2520insights%2520into%2520spatiotemporal%2520semantics%2520analysis%2520of%2520LVLMs%252C%2520laying%2520a%250Afoundation%2520for%2520designing%2520more%2520robust%2520and%2520interpretable%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CircuitProbe%3A%20Dissecting%20Spatiotemporal%20Visual%20Semantics%20with%20Circuit%0A%20%20Tracing&entry.906535625=Yiming%20Zhang%20and%20Chengzhang%20Yu%20and%20Zhuokai%20Zhao%20and%20Kun%20Wang%20and%20Qiankun%20Li%20and%20Zihan%20Chen%20and%20Yang%20Liu%20and%20Zenghui%20Ding%20and%20Yining%20Sun&entry.1292438233=%20%20The%20processing%20mechanisms%20underlying%20language%20and%20image%20understanding%20in%0Alarge%20vision-language%20models%20%28LVLMs%29%20have%20been%20extensively%20studied.%20However%2C%0Athe%20internal%20reasoning%20mechanisms%20of%20LVLMs%20for%20spatiotemporal%20understanding%0Aremain%20poorly%20understood.%20In%20this%20work%2C%20we%20introduce%20a%20systematic%2C%0Acircuit-based%20framework%20designed%20to%20investigate%20how%20spatiotemporal%20visual%0Asemantics%20are%20represented%20and%20processed%20within%20these%20LVLMs.%20Specifically%2C%20our%0Aframework%20comprises%20three%20circuits%3A%20visual%20auditing%20circuit%2C%20semantic%20tracing%0Acircuit%2C%20and%20attention%20flow%20circuit.%20Through%20the%20lens%20of%20these%20circuits%2C%20we%0Adiscover%20that%20visual%20semantics%20are%20highly%20localized%20to%20specific%20object%0Atokens--removing%20these%20tokens%20can%20degrade%20model%20performance%20by%20up%20to%2092.6%25.%0AFurthermore%2C%20we%20identify%20that%20interpretable%20concepts%20of%20objects%20and%20actions%0Aemerge%20and%20become%20progressively%20refined%20in%20the%20middle-to-late%20layers%20of%20LVLMs.%0AIn%20contrary%20to%20the%20current%20works%20that%20solely%20focus%20on%20objects%20in%20one%20image%2C%20we%0Areveal%20that%20the%20middle-to-late%20layers%20of%20LVLMs%20exhibit%20specialized%20functional%0Alocalization%20for%20spatiotemporal%20semantics.%20Our%20findings%20offer%20significant%0Amechanistic%20insights%20into%20spatiotemporal%20semantics%20analysis%20of%20LVLMs%2C%20laying%20a%0Afoundation%20for%20designing%20more%20robust%20and%20interpretable%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19420v1&entry.124074799=Read"},
{"title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit\n  Representations", "author": "Ziren Gong and Xiaohan Li and Fabio Tosi and Youmin Zhang and Stefano Mattoccia and Jun Wu and Matteo Poggi", "abstract": "  This paper presents DINO-SLAM, a DINO-informed design strategy to enhance\nneural implicit (Neural Radiance Field -- NeRF) and explicit representations\n(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive\nscene representations. Purposely, we rely on a Scene Structure Encoder (SSE)\nthat enriches DINO features into Enhanced DINO ones (EDINO) to capture\nhierarchical scene elements and their structural relationships. Building upon\nit, we propose two foundational paradigms for NeRF and 3DGS SLAM systems\nintegrating EDINO features. Our DINO-informed pipelines achieve superior\nperformance on the Replica, ScanNet, and TUM compared to state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2507.19474v1", "date": "2025-07-25", "relevancy": 3.0843, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6351}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-SLAM%3A%20DINO-informed%20RGB-D%20SLAM%20for%20Neural%20Implicit%20and%20Explicit%0A%20%20Representations&body=Title%3A%20DINO-SLAM%3A%20DINO-informed%20RGB-D%20SLAM%20for%20Neural%20Implicit%20and%20Explicit%0A%20%20Representations%0AAuthor%3A%20Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Youmin%20Zhang%20and%20Stefano%20Mattoccia%20and%20Jun%20Wu%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%20This%20paper%20presents%20DINO-SLAM%2C%20a%20DINO-informed%20design%20strategy%20to%20enhance%0Aneural%20implicit%20%28Neural%20Radiance%20Field%20--%20NeRF%29%20and%20explicit%20representations%0A%283D%20Gaussian%20Splatting%20--%203DGS%29%20in%20SLAM%20systems%20through%20more%20comprehensive%0Ascene%20representations.%20Purposely%2C%20we%20rely%20on%20a%20Scene%20Structure%20Encoder%20%28SSE%29%0Athat%20enriches%20DINO%20features%20into%20Enhanced%20DINO%20ones%20%28EDINO%29%20to%20capture%0Ahierarchical%20scene%20elements%20and%20their%20structural%20relationships.%20Building%20upon%0Ait%2C%20we%20propose%20two%20foundational%20paradigms%20for%20NeRF%20and%203DGS%20SLAM%20systems%0Aintegrating%20EDINO%20features.%20Our%20DINO-informed%20pipelines%20achieve%20superior%0Aperformance%20on%20the%20Replica%2C%20ScanNet%2C%20and%20TUM%20compared%20to%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-SLAM%253A%2520DINO-informed%2520RGB-D%2520SLAM%2520for%2520Neural%2520Implicit%2520and%2520Explicit%250A%2520%2520Representations%26entry.906535625%3DZiren%2520Gong%2520and%2520Xiaohan%2520Li%2520and%2520Fabio%2520Tosi%2520and%2520Youmin%2520Zhang%2520and%2520Stefano%2520Mattoccia%2520and%2520Jun%2520Wu%2520and%2520Matteo%2520Poggi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DINO-SLAM%252C%2520a%2520DINO-informed%2520design%2520strategy%2520to%2520enhance%250Aneural%2520implicit%2520%2528Neural%2520Radiance%2520Field%2520--%2520NeRF%2529%2520and%2520explicit%2520representations%250A%25283D%2520Gaussian%2520Splatting%2520--%25203DGS%2529%2520in%2520SLAM%2520systems%2520through%2520more%2520comprehensive%250Ascene%2520representations.%2520Purposely%252C%2520we%2520rely%2520on%2520a%2520Scene%2520Structure%2520Encoder%2520%2528SSE%2529%250Athat%2520enriches%2520DINO%2520features%2520into%2520Enhanced%2520DINO%2520ones%2520%2528EDINO%2529%2520to%2520capture%250Ahierarchical%2520scene%2520elements%2520and%2520their%2520structural%2520relationships.%2520Building%2520upon%250Ait%252C%2520we%2520propose%2520two%2520foundational%2520paradigms%2520for%2520NeRF%2520and%25203DGS%2520SLAM%2520systems%250Aintegrating%2520EDINO%2520features.%2520Our%2520DINO-informed%2520pipelines%2520achieve%2520superior%250Aperformance%2520on%2520the%2520Replica%252C%2520ScanNet%252C%2520and%2520TUM%2520compared%2520to%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-SLAM%3A%20DINO-informed%20RGB-D%20SLAM%20for%20Neural%20Implicit%20and%20Explicit%0A%20%20Representations&entry.906535625=Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Youmin%20Zhang%20and%20Stefano%20Mattoccia%20and%20Jun%20Wu%20and%20Matteo%20Poggi&entry.1292438233=%20%20This%20paper%20presents%20DINO-SLAM%2C%20a%20DINO-informed%20design%20strategy%20to%20enhance%0Aneural%20implicit%20%28Neural%20Radiance%20Field%20--%20NeRF%29%20and%20explicit%20representations%0A%283D%20Gaussian%20Splatting%20--%203DGS%29%20in%20SLAM%20systems%20through%20more%20comprehensive%0Ascene%20representations.%20Purposely%2C%20we%20rely%20on%20a%20Scene%20Structure%20Encoder%20%28SSE%29%0Athat%20enriches%20DINO%20features%20into%20Enhanced%20DINO%20ones%20%28EDINO%29%20to%20capture%0Ahierarchical%20scene%20elements%20and%20their%20structural%20relationships.%20Building%20upon%0Ait%2C%20we%20propose%20two%20foundational%20paradigms%20for%20NeRF%20and%203DGS%20SLAM%20systems%0Aintegrating%20EDINO%20features.%20Our%20DINO-informed%20pipelines%20achieve%20superior%0Aperformance%20on%20the%20Replica%2C%20ScanNet%2C%20and%20TUM%20compared%20to%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19474v1&entry.124074799=Read"},
{"title": "ObjectRelator: Enabling Cross-View Object Relation Understanding Across\n  Ego-Centric and Exo-Centric Perspectives", "author": "Yuqian Fu and Runze Wang and Bin Ren and Guolei Sun and Biao Gong and Yanwei Fu and Danda Pani Paudel and Xuanjing Huang and Luc Van Gool", "abstract": "  Bridging the gap between ego-centric and exo-centric views has been a\nlong-standing question in computer vision. In this paper, we focus on the\nemerging Ego-Exo object correspondence task, which aims to understand object\nrelations across ego-exo perspectives through segmentation. While numerous\nsegmentation models have been proposed, most operate on a single image (view),\nmaking them impractical for cross-view scenarios. PSALM, a recently proposed\nsegmentation method, stands out as a notable exception with its demonstrated\nzero-shot ability on this task. However, due to the drastic viewpoint change\nbetween ego and exo, PSALM fails to accurately locate and segment objects,\nespecially in complex backgrounds or when object appearances change\nsignificantly. To address these issues, we propose ObjectRelator, a novel\napproach featuring two key modules: Multimodal Condition Fusion (MCFuse) and\nSSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language\nas an additional cue, integrating both visual masks and textual descriptions to\nimprove object localization and prevent incorrect associations. XObjAlign\nenforces cross-view consistency through self-supervised alignment, enhancing\nrobustness to object appearance variations. Extensive experiments demonstrate\nObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and\nHANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art\nperformance. Code is made available at: http://yuqianfu.com/ObjectRelator.\n", "link": "http://arxiv.org/abs/2411.19083v2", "date": "2025-07-25", "relevancy": 2.9509, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjectRelator%3A%20Enabling%20Cross-View%20Object%20Relation%20Understanding%20Across%0A%20%20Ego-Centric%20and%20Exo-Centric%20Perspectives&body=Title%3A%20ObjectRelator%3A%20Enabling%20Cross-View%20Object%20Relation%20Understanding%20Across%0A%20%20Ego-Centric%20and%20Exo-Centric%20Perspectives%0AAuthor%3A%20Yuqian%20Fu%20and%20Runze%20Wang%20and%20Bin%20Ren%20and%20Guolei%20Sun%20and%20Biao%20Gong%20and%20Yanwei%20Fu%20and%20Danda%20Pani%20Paudel%20and%20Xuanjing%20Huang%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Bridging%20the%20gap%20between%20ego-centric%20and%20exo-centric%20views%20has%20been%20a%0Along-standing%20question%20in%20computer%20vision.%20In%20this%20paper%2C%20we%20focus%20on%20the%0Aemerging%20Ego-Exo%20object%20correspondence%20task%2C%20which%20aims%20to%20understand%20object%0Arelations%20across%20ego-exo%20perspectives%20through%20segmentation.%20While%20numerous%0Asegmentation%20models%20have%20been%20proposed%2C%20most%20operate%20on%20a%20single%20image%20%28view%29%2C%0Amaking%20them%20impractical%20for%20cross-view%20scenarios.%20PSALM%2C%20a%20recently%20proposed%0Asegmentation%20method%2C%20stands%20out%20as%20a%20notable%20exception%20with%20its%20demonstrated%0Azero-shot%20ability%20on%20this%20task.%20However%2C%20due%20to%20the%20drastic%20viewpoint%20change%0Abetween%20ego%20and%20exo%2C%20PSALM%20fails%20to%20accurately%20locate%20and%20segment%20objects%2C%0Aespecially%20in%20complex%20backgrounds%20or%20when%20object%20appearances%20change%0Asignificantly.%20To%20address%20these%20issues%2C%20we%20propose%20ObjectRelator%2C%20a%20novel%0Aapproach%20featuring%20two%20key%20modules%3A%20Multimodal%20Condition%20Fusion%20%28MCFuse%29%20and%0ASSL-based%20Cross-View%20Object%20Alignment%20%28XObjAlign%29.%20MCFuse%20introduces%20language%0Aas%20an%20additional%20cue%2C%20integrating%20both%20visual%20masks%20and%20textual%20descriptions%20to%0Aimprove%20object%20localization%20and%20prevent%20incorrect%20associations.%20XObjAlign%0Aenforces%20cross-view%20consistency%20through%20self-supervised%20alignment%2C%20enhancing%0Arobustness%20to%20object%20appearance%20variations.%20Extensive%20experiments%20demonstrate%0AObjectRelator%27s%20effectiveness%20on%20the%20large-scale%20Ego-Exo4D%20benchmark%20and%0AHANDAL-X%20%28an%20adapted%20dataset%20for%20cross-view%20segmentation%29%20with%20state-of-the-art%0Aperformance.%20Code%20is%20made%20available%20at%3A%20http%3A//yuqianfu.com/ObjectRelator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjectRelator%253A%2520Enabling%2520Cross-View%2520Object%2520Relation%2520Understanding%2520Across%250A%2520%2520Ego-Centric%2520and%2520Exo-Centric%2520Perspectives%26entry.906535625%3DYuqian%2520Fu%2520and%2520Runze%2520Wang%2520and%2520Bin%2520Ren%2520and%2520Guolei%2520Sun%2520and%2520Biao%2520Gong%2520and%2520Yanwei%2520Fu%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Xuanjing%2520Huang%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Bridging%2520the%2520gap%2520between%2520ego-centric%2520and%2520exo-centric%2520views%2520has%2520been%2520a%250Along-standing%2520question%2520in%2520computer%2520vision.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%250Aemerging%2520Ego-Exo%2520object%2520correspondence%2520task%252C%2520which%2520aims%2520to%2520understand%2520object%250Arelations%2520across%2520ego-exo%2520perspectives%2520through%2520segmentation.%2520While%2520numerous%250Asegmentation%2520models%2520have%2520been%2520proposed%252C%2520most%2520operate%2520on%2520a%2520single%2520image%2520%2528view%2529%252C%250Amaking%2520them%2520impractical%2520for%2520cross-view%2520scenarios.%2520PSALM%252C%2520a%2520recently%2520proposed%250Asegmentation%2520method%252C%2520stands%2520out%2520as%2520a%2520notable%2520exception%2520with%2520its%2520demonstrated%250Azero-shot%2520ability%2520on%2520this%2520task.%2520However%252C%2520due%2520to%2520the%2520drastic%2520viewpoint%2520change%250Abetween%2520ego%2520and%2520exo%252C%2520PSALM%2520fails%2520to%2520accurately%2520locate%2520and%2520segment%2520objects%252C%250Aespecially%2520in%2520complex%2520backgrounds%2520or%2520when%2520object%2520appearances%2520change%250Asignificantly.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520ObjectRelator%252C%2520a%2520novel%250Aapproach%2520featuring%2520two%2520key%2520modules%253A%2520Multimodal%2520Condition%2520Fusion%2520%2528MCFuse%2529%2520and%250ASSL-based%2520Cross-View%2520Object%2520Alignment%2520%2528XObjAlign%2529.%2520MCFuse%2520introduces%2520language%250Aas%2520an%2520additional%2520cue%252C%2520integrating%2520both%2520visual%2520masks%2520and%2520textual%2520descriptions%2520to%250Aimprove%2520object%2520localization%2520and%2520prevent%2520incorrect%2520associations.%2520XObjAlign%250Aenforces%2520cross-view%2520consistency%2520through%2520self-supervised%2520alignment%252C%2520enhancing%250Arobustness%2520to%2520object%2520appearance%2520variations.%2520Extensive%2520experiments%2520demonstrate%250AObjectRelator%2527s%2520effectiveness%2520on%2520the%2520large-scale%2520Ego-Exo4D%2520benchmark%2520and%250AHANDAL-X%2520%2528an%2520adapted%2520dataset%2520for%2520cross-view%2520segmentation%2529%2520with%2520state-of-the-art%250Aperformance.%2520Code%2520is%2520made%2520available%2520at%253A%2520http%253A//yuqianfu.com/ObjectRelator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjectRelator%3A%20Enabling%20Cross-View%20Object%20Relation%20Understanding%20Across%0A%20%20Ego-Centric%20and%20Exo-Centric%20Perspectives&entry.906535625=Yuqian%20Fu%20and%20Runze%20Wang%20and%20Bin%20Ren%20and%20Guolei%20Sun%20and%20Biao%20Gong%20and%20Yanwei%20Fu%20and%20Danda%20Pani%20Paudel%20and%20Xuanjing%20Huang%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Bridging%20the%20gap%20between%20ego-centric%20and%20exo-centric%20views%20has%20been%20a%0Along-standing%20question%20in%20computer%20vision.%20In%20this%20paper%2C%20we%20focus%20on%20the%0Aemerging%20Ego-Exo%20object%20correspondence%20task%2C%20which%20aims%20to%20understand%20object%0Arelations%20across%20ego-exo%20perspectives%20through%20segmentation.%20While%20numerous%0Asegmentation%20models%20have%20been%20proposed%2C%20most%20operate%20on%20a%20single%20image%20%28view%29%2C%0Amaking%20them%20impractical%20for%20cross-view%20scenarios.%20PSALM%2C%20a%20recently%20proposed%0Asegmentation%20method%2C%20stands%20out%20as%20a%20notable%20exception%20with%20its%20demonstrated%0Azero-shot%20ability%20on%20this%20task.%20However%2C%20due%20to%20the%20drastic%20viewpoint%20change%0Abetween%20ego%20and%20exo%2C%20PSALM%20fails%20to%20accurately%20locate%20and%20segment%20objects%2C%0Aespecially%20in%20complex%20backgrounds%20or%20when%20object%20appearances%20change%0Asignificantly.%20To%20address%20these%20issues%2C%20we%20propose%20ObjectRelator%2C%20a%20novel%0Aapproach%20featuring%20two%20key%20modules%3A%20Multimodal%20Condition%20Fusion%20%28MCFuse%29%20and%0ASSL-based%20Cross-View%20Object%20Alignment%20%28XObjAlign%29.%20MCFuse%20introduces%20language%0Aas%20an%20additional%20cue%2C%20integrating%20both%20visual%20masks%20and%20textual%20descriptions%20to%0Aimprove%20object%20localization%20and%20prevent%20incorrect%20associations.%20XObjAlign%0Aenforces%20cross-view%20consistency%20through%20self-supervised%20alignment%2C%20enhancing%0Arobustness%20to%20object%20appearance%20variations.%20Extensive%20experiments%20demonstrate%0AObjectRelator%27s%20effectiveness%20on%20the%20large-scale%20Ego-Exo4D%20benchmark%20and%0AHANDAL-X%20%28an%20adapted%20dataset%20for%20cross-view%20segmentation%29%20with%20state-of-the-art%0Aperformance.%20Code%20is%20made%20available%20at%3A%20http%3A//yuqianfu.com/ObjectRelator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19083v2&entry.124074799=Read"},
{"title": "Unraveling the geometry of visual relational reasoning", "author": "Jiaqi Shang and Gabriel Kreiman and Haim Sompolinsky", "abstract": "  Humans readily generalize abstract relations, such as recognizing \"constant\"\nin shape or color, whereas neural networks struggle, limiting their flexible\nreasoning. To investigate mechanisms underlying such generalization, we\nintroduce SimplifiedRPM, a novel benchmark for systematically evaluating\nabstract relational reasoning, addressing limitations in prior datasets. In\nparallel, we conduct human experiments to quantify relational difficulty,\nenabling direct model-human comparisons. Testing four models, ResNet-50, Vision\nTransformer, Wild Relation Network, and Scattering Compositional Learner (SCL),\nwe find that SCL generalizes best and most closely aligns with human behavior.\nUsing a geometric approach, we identify key representation properties that\naccurately predict generalization and uncover a fundamental trade-off between\nsignal and dimensionality: novel relations compress into training-induced\nsubspaces. Layer-wise analysis reveals where relational structure emerges,\nhighlights bottlenecks, and generates concrete hypotheses about abstract\nreasoning in the brain. Motivated by these insights, we propose SNRloss, a\nnovel objective explicitly balancing representation geometry. Our results\nestablish a geometric foundation for relational reasoning, paving the way for\nmore human-like visual reasoning in AI and opening promising avenues for\nextending geometric analysis to broader cognitive tasks.\n", "link": "http://arxiv.org/abs/2502.17382v2", "date": "2025-07-25", "relevancy": 2.9071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning&body=Title%3A%20Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning%0AAuthor%3A%20Jiaqi%20Shang%20and%20Gabriel%20Kreiman%20and%20Haim%20Sompolinsky%0AAbstract%3A%20%20%20Humans%20readily%20generalize%20abstract%20relations%2C%20such%20as%20recognizing%20%22constant%22%0Ain%20shape%20or%20color%2C%20whereas%20neural%20networks%20struggle%2C%20limiting%20their%20flexible%0Areasoning.%20To%20investigate%20mechanisms%20underlying%20such%20generalization%2C%20we%0Aintroduce%20SimplifiedRPM%2C%20a%20novel%20benchmark%20for%20systematically%20evaluating%0Aabstract%20relational%20reasoning%2C%20addressing%20limitations%20in%20prior%20datasets.%20In%0Aparallel%2C%20we%20conduct%20human%20experiments%20to%20quantify%20relational%20difficulty%2C%0Aenabling%20direct%20model-human%20comparisons.%20Testing%20four%20models%2C%20ResNet-50%2C%20Vision%0ATransformer%2C%20Wild%20Relation%20Network%2C%20and%20Scattering%20Compositional%20Learner%20%28SCL%29%2C%0Awe%20find%20that%20SCL%20generalizes%20best%20and%20most%20closely%20aligns%20with%20human%20behavior.%0AUsing%20a%20geometric%20approach%2C%20we%20identify%20key%20representation%20properties%20that%0Aaccurately%20predict%20generalization%20and%20uncover%20a%20fundamental%20trade-off%20between%0Asignal%20and%20dimensionality%3A%20novel%20relations%20compress%20into%20training-induced%0Asubspaces.%20Layer-wise%20analysis%20reveals%20where%20relational%20structure%20emerges%2C%0Ahighlights%20bottlenecks%2C%20and%20generates%20concrete%20hypotheses%20about%20abstract%0Areasoning%20in%20the%20brain.%20Motivated%20by%20these%20insights%2C%20we%20propose%20SNRloss%2C%20a%0Anovel%20objective%20explicitly%20balancing%20representation%20geometry.%20Our%20results%0Aestablish%20a%20geometric%20foundation%20for%20relational%20reasoning%2C%20paving%20the%20way%20for%0Amore%20human-like%20visual%20reasoning%20in%20AI%20and%20opening%20promising%20avenues%20for%0Aextending%20geometric%20analysis%20to%20broader%20cognitive%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520geometry%2520of%2520visual%2520relational%2520reasoning%26entry.906535625%3DJiaqi%2520Shang%2520and%2520Gabriel%2520Kreiman%2520and%2520Haim%2520Sompolinsky%26entry.1292438233%3D%2520%2520Humans%2520readily%2520generalize%2520abstract%2520relations%252C%2520such%2520as%2520recognizing%2520%2522constant%2522%250Ain%2520shape%2520or%2520color%252C%2520whereas%2520neural%2520networks%2520struggle%252C%2520limiting%2520their%2520flexible%250Areasoning.%2520To%2520investigate%2520mechanisms%2520underlying%2520such%2520generalization%252C%2520we%250Aintroduce%2520SimplifiedRPM%252C%2520a%2520novel%2520benchmark%2520for%2520systematically%2520evaluating%250Aabstract%2520relational%2520reasoning%252C%2520addressing%2520limitations%2520in%2520prior%2520datasets.%2520In%250Aparallel%252C%2520we%2520conduct%2520human%2520experiments%2520to%2520quantify%2520relational%2520difficulty%252C%250Aenabling%2520direct%2520model-human%2520comparisons.%2520Testing%2520four%2520models%252C%2520ResNet-50%252C%2520Vision%250ATransformer%252C%2520Wild%2520Relation%2520Network%252C%2520and%2520Scattering%2520Compositional%2520Learner%2520%2528SCL%2529%252C%250Awe%2520find%2520that%2520SCL%2520generalizes%2520best%2520and%2520most%2520closely%2520aligns%2520with%2520human%2520behavior.%250AUsing%2520a%2520geometric%2520approach%252C%2520we%2520identify%2520key%2520representation%2520properties%2520that%250Aaccurately%2520predict%2520generalization%2520and%2520uncover%2520a%2520fundamental%2520trade-off%2520between%250Asignal%2520and%2520dimensionality%253A%2520novel%2520relations%2520compress%2520into%2520training-induced%250Asubspaces.%2520Layer-wise%2520analysis%2520reveals%2520where%2520relational%2520structure%2520emerges%252C%250Ahighlights%2520bottlenecks%252C%2520and%2520generates%2520concrete%2520hypotheses%2520about%2520abstract%250Areasoning%2520in%2520the%2520brain.%2520Motivated%2520by%2520these%2520insights%252C%2520we%2520propose%2520SNRloss%252C%2520a%250Anovel%2520objective%2520explicitly%2520balancing%2520representation%2520geometry.%2520Our%2520results%250Aestablish%2520a%2520geometric%2520foundation%2520for%2520relational%2520reasoning%252C%2520paving%2520the%2520way%2520for%250Amore%2520human-like%2520visual%2520reasoning%2520in%2520AI%2520and%2520opening%2520promising%2520avenues%2520for%250Aextending%2520geometric%2520analysis%2520to%2520broader%2520cognitive%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning&entry.906535625=Jiaqi%20Shang%20and%20Gabriel%20Kreiman%20and%20Haim%20Sompolinsky&entry.1292438233=%20%20Humans%20readily%20generalize%20abstract%20relations%2C%20such%20as%20recognizing%20%22constant%22%0Ain%20shape%20or%20color%2C%20whereas%20neural%20networks%20struggle%2C%20limiting%20their%20flexible%0Areasoning.%20To%20investigate%20mechanisms%20underlying%20such%20generalization%2C%20we%0Aintroduce%20SimplifiedRPM%2C%20a%20novel%20benchmark%20for%20systematically%20evaluating%0Aabstract%20relational%20reasoning%2C%20addressing%20limitations%20in%20prior%20datasets.%20In%0Aparallel%2C%20we%20conduct%20human%20experiments%20to%20quantify%20relational%20difficulty%2C%0Aenabling%20direct%20model-human%20comparisons.%20Testing%20four%20models%2C%20ResNet-50%2C%20Vision%0ATransformer%2C%20Wild%20Relation%20Network%2C%20and%20Scattering%20Compositional%20Learner%20%28SCL%29%2C%0Awe%20find%20that%20SCL%20generalizes%20best%20and%20most%20closely%20aligns%20with%20human%20behavior.%0AUsing%20a%20geometric%20approach%2C%20we%20identify%20key%20representation%20properties%20that%0Aaccurately%20predict%20generalization%20and%20uncover%20a%20fundamental%20trade-off%20between%0Asignal%20and%20dimensionality%3A%20novel%20relations%20compress%20into%20training-induced%0Asubspaces.%20Layer-wise%20analysis%20reveals%20where%20relational%20structure%20emerges%2C%0Ahighlights%20bottlenecks%2C%20and%20generates%20concrete%20hypotheses%20about%20abstract%0Areasoning%20in%20the%20brain.%20Motivated%20by%20these%20insights%2C%20we%20propose%20SNRloss%2C%20a%0Anovel%20objective%20explicitly%20balancing%20representation%20geometry.%20Our%20results%0Aestablish%20a%20geometric%20foundation%20for%20relational%20reasoning%2C%20paving%20the%20way%20for%0Amore%20human-like%20visual%20reasoning%20in%20AI%20and%20opening%20promising%20avenues%20for%0Aextending%20geometric%20analysis%20to%20broader%20cognitive%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17382v2&entry.124074799=Read"},
{"title": "Modality Agnostic Efficient Long Range Encoder", "author": "Toufiq Parag and Ahmed Elgammal", "abstract": "  The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.\n", "link": "http://arxiv.org/abs/2507.19409v1", "date": "2025-07-25", "relevancy": 2.7178, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality%20Agnostic%20Efficient%20Long%20Range%20Encoder&body=Title%3A%20Modality%20Agnostic%20Efficient%20Long%20Range%20Encoder%0AAuthor%3A%20Toufiq%20Parag%20and%20Ahmed%20Elgammal%0AAbstract%3A%20%20%20The%20long-context%20capability%20of%20recent%20large%20transformer%20models%20can%20be%0Asurmised%20to%20rely%20on%20techniques%20such%20as%20attention/model%20parallelism%2C%20as%20well%20as%0Ahardware-level%20optimizations.%20While%20these%20strategies%20allow%20input%20lengths%20to%0Ascale%20to%20millions%20of%20tokens%2C%20they%20do%20not%20fundamentally%20mitigate%20the%20quadratic%0Acomputational%20and%20memory%20complexity%20of%20the%20core%20attention%20mechanism.%20In%20this%0Apaper%2C%20we%20address%20the%20challenge%20of%20long-context%20processing%20on%20a%20single%20device%0Ausing%20generic%20implementations%20by%20reducing%20the%20quadratic%20memory%20footprint%20and%0Ainference%20cost.%20Existing%20approaches%20to%20extend%20the%20context%20length%20for%20generic%0Asingle%20device%20implementations%20--%20such%20as%20token%20merging%20and%20modified%20attentions%0A--%20are%20often%20modality%20specific%20and%20attain%20a%20suboptimal%20tradeoff%20between%0Aaccuracy%20and%20efficiency.%20To%20overcome%20these%20limitations%2C%20we%20propose%20MAELRE%0A%28Modality%20Agnostic%20Efficient%20Long%20Range%20Encoder%29%2C%20a%20unified%20and%20efficient%0Atransformer%20architecture%20designed%20for%20long-range%20encoding%20across%20diverse%0Amodalities.%20MAELRE%20integrates%20token%20merging%20with%20attention%20approximation%2C%0Aprogressively%20merging%20tokens%20at%20different%20stages%20of%20internal%20computational%0Ablocks.%20It%20employs%20a%20lightweight%20attention%20approximation%20when%20the%20number%20of%0Atokens%20is%20large%2C%20and%20switches%20to%20standard%20dot-product%20attention%20as%20the%20sequence%0Abecomes%20shorter%20through%20successive%20aggregation.%20We%20demonstrate%20that%20MAELRE%0Aachieves%20superior%20accuracy%20while%20reducing%20computational%20cost%20compared%20to%0Aexisting%20long-context%20models%20on%20classification%20tasks%20spanning%20multiple%0Amodalities%2C%20including%20text%2C%20time%20series%2C%20audio%2C%20and%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality%2520Agnostic%2520Efficient%2520Long%2520Range%2520Encoder%26entry.906535625%3DToufiq%2520Parag%2520and%2520Ahmed%2520Elgammal%26entry.1292438233%3D%2520%2520The%2520long-context%2520capability%2520of%2520recent%2520large%2520transformer%2520models%2520can%2520be%250Asurmised%2520to%2520rely%2520on%2520techniques%2520such%2520as%2520attention/model%2520parallelism%252C%2520as%2520well%2520as%250Ahardware-level%2520optimizations.%2520While%2520these%2520strategies%2520allow%2520input%2520lengths%2520to%250Ascale%2520to%2520millions%2520of%2520tokens%252C%2520they%2520do%2520not%2520fundamentally%2520mitigate%2520the%2520quadratic%250Acomputational%2520and%2520memory%2520complexity%2520of%2520the%2520core%2520attention%2520mechanism.%2520In%2520this%250Apaper%252C%2520we%2520address%2520the%2520challenge%2520of%2520long-context%2520processing%2520on%2520a%2520single%2520device%250Ausing%2520generic%2520implementations%2520by%2520reducing%2520the%2520quadratic%2520memory%2520footprint%2520and%250Ainference%2520cost.%2520Existing%2520approaches%2520to%2520extend%2520the%2520context%2520length%2520for%2520generic%250Asingle%2520device%2520implementations%2520--%2520such%2520as%2520token%2520merging%2520and%2520modified%2520attentions%250A--%2520are%2520often%2520modality%2520specific%2520and%2520attain%2520a%2520suboptimal%2520tradeoff%2520between%250Aaccuracy%2520and%2520efficiency.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520MAELRE%250A%2528Modality%2520Agnostic%2520Efficient%2520Long%2520Range%2520Encoder%2529%252C%2520a%2520unified%2520and%2520efficient%250Atransformer%2520architecture%2520designed%2520for%2520long-range%2520encoding%2520across%2520diverse%250Amodalities.%2520MAELRE%2520integrates%2520token%2520merging%2520with%2520attention%2520approximation%252C%250Aprogressively%2520merging%2520tokens%2520at%2520different%2520stages%2520of%2520internal%2520computational%250Ablocks.%2520It%2520employs%2520a%2520lightweight%2520attention%2520approximation%2520when%2520the%2520number%2520of%250Atokens%2520is%2520large%252C%2520and%2520switches%2520to%2520standard%2520dot-product%2520attention%2520as%2520the%2520sequence%250Abecomes%2520shorter%2520through%2520successive%2520aggregation.%2520We%2520demonstrate%2520that%2520MAELRE%250Aachieves%2520superior%2520accuracy%2520while%2520reducing%2520computational%2520cost%2520compared%2520to%250Aexisting%2520long-context%2520models%2520on%2520classification%2520tasks%2520spanning%2520multiple%250Amodalities%252C%2520including%2520text%252C%2520time%2520series%252C%2520audio%252C%2520and%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality%20Agnostic%20Efficient%20Long%20Range%20Encoder&entry.906535625=Toufiq%20Parag%20and%20Ahmed%20Elgammal&entry.1292438233=%20%20The%20long-context%20capability%20of%20recent%20large%20transformer%20models%20can%20be%0Asurmised%20to%20rely%20on%20techniques%20such%20as%20attention/model%20parallelism%2C%20as%20well%20as%0Ahardware-level%20optimizations.%20While%20these%20strategies%20allow%20input%20lengths%20to%0Ascale%20to%20millions%20of%20tokens%2C%20they%20do%20not%20fundamentally%20mitigate%20the%20quadratic%0Acomputational%20and%20memory%20complexity%20of%20the%20core%20attention%20mechanism.%20In%20this%0Apaper%2C%20we%20address%20the%20challenge%20of%20long-context%20processing%20on%20a%20single%20device%0Ausing%20generic%20implementations%20by%20reducing%20the%20quadratic%20memory%20footprint%20and%0Ainference%20cost.%20Existing%20approaches%20to%20extend%20the%20context%20length%20for%20generic%0Asingle%20device%20implementations%20--%20such%20as%20token%20merging%20and%20modified%20attentions%0A--%20are%20often%20modality%20specific%20and%20attain%20a%20suboptimal%20tradeoff%20between%0Aaccuracy%20and%20efficiency.%20To%20overcome%20these%20limitations%2C%20we%20propose%20MAELRE%0A%28Modality%20Agnostic%20Efficient%20Long%20Range%20Encoder%29%2C%20a%20unified%20and%20efficient%0Atransformer%20architecture%20designed%20for%20long-range%20encoding%20across%20diverse%0Amodalities.%20MAELRE%20integrates%20token%20merging%20with%20attention%20approximation%2C%0Aprogressively%20merging%20tokens%20at%20different%20stages%20of%20internal%20computational%0Ablocks.%20It%20employs%20a%20lightweight%20attention%20approximation%20when%20the%20number%20of%0Atokens%20is%20large%2C%20and%20switches%20to%20standard%20dot-product%20attention%20as%20the%20sequence%0Abecomes%20shorter%20through%20successive%20aggregation.%20We%20demonstrate%20that%20MAELRE%0Aachieves%20superior%20accuracy%20while%20reducing%20computational%20cost%20compared%20to%0Aexisting%20long-context%20models%20on%20classification%20tasks%20spanning%20multiple%0Amodalities%2C%20including%20text%2C%20time%20series%2C%20audio%2C%20and%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19409v1&entry.124074799=Read"},
{"title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "author": "Mina Huh and Fangyuan Xu and Yi-Hao Peng and Chongyan Chen and Hansika Murugu and Danna Gurari and Eunsol Choi and Amy Pavel", "abstract": "  Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.\n", "link": "http://arxiv.org/abs/2408.06303v2", "date": "2025-07-25", "relevancy": 2.6447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People&body=Title%3A%20Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People%0AAuthor%3A%20Mina%20Huh%20and%20Fangyuan%20Xu%20and%20Yi-Hao%20Peng%20and%20Chongyan%20Chen%20and%20Hansika%20Murugu%20and%20Danna%20Gurari%20and%20Eunsol%20Choi%20and%20Amy%20Pavel%0AAbstract%3A%20%20%20Vision%20language%20models%20can%20now%20generate%20long-form%20answers%20to%20questions%20about%0Aimages%20-%20long-form%20visual%20question%20answers%20%28LFVQA%29.%20We%20contribute%20VizWiz-LF%2C%20a%0Adataset%20of%20long-form%20answers%20to%20visual%20questions%20posed%20by%20blind%20and%20low%20vision%0A%28BLV%29%20users.%20VizWiz-LF%20contains%204.2k%20long-form%20answers%20to%20600%20visual%20questions%2C%0Acollected%20from%20human%20expert%20describers%20and%20six%20VQA%20models.%20We%20develop%20and%0Aannotate%20functional%20roles%20of%20sentences%20of%20LFVQA%20and%20demonstrate%20that%20long-form%0Aanswers%20contain%20information%20beyond%20the%20question%20answer%20such%20as%20explanations%20and%0Asuggestions.%20We%20further%20conduct%20automatic%20and%20human%20evaluations%20with%20BLV%20and%0Asighted%20people%20to%20evaluate%20long-form%20answers.%20BLV%20people%20perceive%20both%0Ahuman-written%20and%20generated%20long-form%20answers%20to%20be%20plausible%2C%20but%20generated%0Aanswers%20often%20hallucinate%20incorrect%20visual%20details%2C%20especially%20for%20unanswerable%0Avisual%20questions%20%28e.g.%2C%20blurry%20or%20irrelevant%20images%29.%20To%20reduce%20hallucinations%2C%0Awe%20evaluate%20the%20ability%20of%20VQA%20models%20to%20abstain%20from%20answering%20unanswerable%0Aquestions%20across%20multiple%20prompting%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Form%2520Answers%2520to%2520Visual%2520Questions%2520from%2520Blind%2520and%2520Low%2520Vision%2520People%26entry.906535625%3DMina%2520Huh%2520and%2520Fangyuan%2520Xu%2520and%2520Yi-Hao%2520Peng%2520and%2520Chongyan%2520Chen%2520and%2520Hansika%2520Murugu%2520and%2520Danna%2520Gurari%2520and%2520Eunsol%2520Choi%2520and%2520Amy%2520Pavel%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520can%2520now%2520generate%2520long-form%2520answers%2520to%2520questions%2520about%250Aimages%2520-%2520long-form%2520visual%2520question%2520answers%2520%2528LFVQA%2529.%2520We%2520contribute%2520VizWiz-LF%252C%2520a%250Adataset%2520of%2520long-form%2520answers%2520to%2520visual%2520questions%2520posed%2520by%2520blind%2520and%2520low%2520vision%250A%2528BLV%2529%2520users.%2520VizWiz-LF%2520contains%25204.2k%2520long-form%2520answers%2520to%2520600%2520visual%2520questions%252C%250Acollected%2520from%2520human%2520expert%2520describers%2520and%2520six%2520VQA%2520models.%2520We%2520develop%2520and%250Aannotate%2520functional%2520roles%2520of%2520sentences%2520of%2520LFVQA%2520and%2520demonstrate%2520that%2520long-form%250Aanswers%2520contain%2520information%2520beyond%2520the%2520question%2520answer%2520such%2520as%2520explanations%2520and%250Asuggestions.%2520We%2520further%2520conduct%2520automatic%2520and%2520human%2520evaluations%2520with%2520BLV%2520and%250Asighted%2520people%2520to%2520evaluate%2520long-form%2520answers.%2520BLV%2520people%2520perceive%2520both%250Ahuman-written%2520and%2520generated%2520long-form%2520answers%2520to%2520be%2520plausible%252C%2520but%2520generated%250Aanswers%2520often%2520hallucinate%2520incorrect%2520visual%2520details%252C%2520especially%2520for%2520unanswerable%250Avisual%2520questions%2520%2528e.g.%252C%2520blurry%2520or%2520irrelevant%2520images%2529.%2520To%2520reduce%2520hallucinations%252C%250Awe%2520evaluate%2520the%2520ability%2520of%2520VQA%2520models%2520to%2520abstain%2520from%2520answering%2520unanswerable%250Aquestions%2520across%2520multiple%2520prompting%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Form%20Answers%20to%20Visual%20Questions%20from%20Blind%20and%20Low%20Vision%20People&entry.906535625=Mina%20Huh%20and%20Fangyuan%20Xu%20and%20Yi-Hao%20Peng%20and%20Chongyan%20Chen%20and%20Hansika%20Murugu%20and%20Danna%20Gurari%20and%20Eunsol%20Choi%20and%20Amy%20Pavel&entry.1292438233=%20%20Vision%20language%20models%20can%20now%20generate%20long-form%20answers%20to%20questions%20about%0Aimages%20-%20long-form%20visual%20question%20answers%20%28LFVQA%29.%20We%20contribute%20VizWiz-LF%2C%20a%0Adataset%20of%20long-form%20answers%20to%20visual%20questions%20posed%20by%20blind%20and%20low%20vision%0A%28BLV%29%20users.%20VizWiz-LF%20contains%204.2k%20long-form%20answers%20to%20600%20visual%20questions%2C%0Acollected%20from%20human%20expert%20describers%20and%20six%20VQA%20models.%20We%20develop%20and%0Aannotate%20functional%20roles%20of%20sentences%20of%20LFVQA%20and%20demonstrate%20that%20long-form%0Aanswers%20contain%20information%20beyond%20the%20question%20answer%20such%20as%20explanations%20and%0Asuggestions.%20We%20further%20conduct%20automatic%20and%20human%20evaluations%20with%20BLV%20and%0Asighted%20people%20to%20evaluate%20long-form%20answers.%20BLV%20people%20perceive%20both%0Ahuman-written%20and%20generated%20long-form%20answers%20to%20be%20plausible%2C%20but%20generated%0Aanswers%20often%20hallucinate%20incorrect%20visual%20details%2C%20especially%20for%20unanswerable%0Avisual%20questions%20%28e.g.%2C%20blurry%20or%20irrelevant%20images%29.%20To%20reduce%20hallucinations%2C%0Awe%20evaluate%20the%20ability%20of%20VQA%20models%20to%20abstain%20from%20answering%20unanswerable%0Aquestions%20across%20multiple%20prompting%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06303v2&entry.124074799=Read"},
{"title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation\n  and Analysis", "author": "Zhengpeng Feng and Clement Atzberger and Sadiq Jaffer and Jovana Knezevic and Silja Sormunen and Robin Young and Madeline C Lisaius and Markus Immitzer and David A. Coomes and Anil Madhavapeddy and Andrew Blake and Srinivasan Keshav", "abstract": "  Satellite remote sensing from repeated observations and multiple sensors\nenables a wide range of downstream applications, including climate modeling,\ncarbon accounting, and strategies for conservation and sustainable land use.\nHowever, satellite time series are voluminous, often corrupted by sensor noise,\nclouds, and atmospheric conditions, and unevenly spaced in time, making them\nchallenging to use. We present TESSERA, an open, global, land-oriented remote\nsensing foundation model that uses self-supervised learning to generate\n`ready-to-use' embeddings at 10~m scale from pixel-level satellite time series\ndata. TESSERA uses two parallel Transformer-based encoders to combine optical\ndata from ten Sentinel-2 spectral bands at 10-60~m spatial resolution and two\nSentinel-1 synthetic aperture radar backscatter coefficients at 10~m resolution\nto create embeddings that are subsequently fused with a multilayer perceptron\nto create annual global embedding maps. We compare our work with\nstate-of-the-art task-specific models and other foundation models in five\ndiverse downstream tasks and find that TESSERA closely matches or outperforms\nthese baselines. We believe that TESSERA's ease of use, openness, computation-,\nlabel-, and data-efficiency, and high performance will prove transformative in\na wide range of vegetation-oriented ecological and agricultural applications.\n", "link": "http://arxiv.org/abs/2506.20380v2", "date": "2025-07-25", "relevancy": 2.483, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis&body=Title%3A%20TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis%0AAuthor%3A%20Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav%0AAbstract%3A%20%20%20Satellite%20remote%20sensing%20from%20repeated%20observations%20and%20multiple%20sensors%0Aenables%20a%20wide%20range%20of%20downstream%20applications%2C%20including%20climate%20modeling%2C%0Acarbon%20accounting%2C%20and%20strategies%20for%20conservation%20and%20sustainable%20land%20use.%0AHowever%2C%20satellite%20time%20series%20are%20voluminous%2C%20often%20corrupted%20by%20sensor%20noise%2C%0Aclouds%2C%20and%20atmospheric%20conditions%2C%20and%20unevenly%20spaced%20in%20time%2C%20making%20them%0Achallenging%20to%20use.%20We%20present%20TESSERA%2C%20an%20open%2C%20global%2C%20land-oriented%20remote%0Asensing%20foundation%20model%20that%20uses%20self-supervised%20learning%20to%20generate%0A%60ready-to-use%27%20embeddings%20at%2010~m%20scale%20from%20pixel-level%20satellite%20time%20series%0Adata.%20TESSERA%20uses%20two%20parallel%20Transformer-based%20encoders%20to%20combine%20optical%0Adata%20from%20ten%20Sentinel-2%20spectral%20bands%20at%2010-60~m%20spatial%20resolution%20and%20two%0ASentinel-1%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010~m%20resolution%0Ato%20create%20embeddings%20that%20are%20subsequently%20fused%20with%20a%20multilayer%20perceptron%0Ato%20create%20annual%20global%20embedding%20maps.%20We%20compare%20our%20work%20with%0Astate-of-the-art%20task-specific%20models%20and%20other%20foundation%20models%20in%20five%0Adiverse%20downstream%20tasks%20and%20find%20that%20TESSERA%20closely%20matches%20or%20outperforms%0Athese%20baselines.%20We%20believe%20that%20TESSERA%27s%20ease%20of%20use%2C%20openness%2C%20computation-%2C%0Alabel-%2C%20and%20data-efficiency%2C%20and%20high%20performance%20will%20prove%20transformative%20in%0Aa%20wide%20range%20of%20vegetation-oriented%20ecological%20and%20agricultural%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTESSERA%253A%2520Temporal%2520Embeddings%2520of%2520Surface%2520Spectra%2520for%2520Earth%2520Representation%250A%2520%2520and%2520Analysis%26entry.906535625%3DZhengpeng%2520Feng%2520and%2520Clement%2520Atzberger%2520and%2520Sadiq%2520Jaffer%2520and%2520Jovana%2520Knezevic%2520and%2520Silja%2520Sormunen%2520and%2520Robin%2520Young%2520and%2520Madeline%2520C%2520Lisaius%2520and%2520Markus%2520Immitzer%2520and%2520David%2520A.%2520Coomes%2520and%2520Anil%2520Madhavapeddy%2520and%2520Andrew%2520Blake%2520and%2520Srinivasan%2520Keshav%26entry.1292438233%3D%2520%2520Satellite%2520remote%2520sensing%2520from%2520repeated%2520observations%2520and%2520multiple%2520sensors%250Aenables%2520a%2520wide%2520range%2520of%2520downstream%2520applications%252C%2520including%2520climate%2520modeling%252C%250Acarbon%2520accounting%252C%2520and%2520strategies%2520for%2520conservation%2520and%2520sustainable%2520land%2520use.%250AHowever%252C%2520satellite%2520time%2520series%2520are%2520voluminous%252C%2520often%2520corrupted%2520by%2520sensor%2520noise%252C%250Aclouds%252C%2520and%2520atmospheric%2520conditions%252C%2520and%2520unevenly%2520spaced%2520in%2520time%252C%2520making%2520them%250Achallenging%2520to%2520use.%2520We%2520present%2520TESSERA%252C%2520an%2520open%252C%2520global%252C%2520land-oriented%2520remote%250Asensing%2520foundation%2520model%2520that%2520uses%2520self-supervised%2520learning%2520to%2520generate%250A%2560ready-to-use%2527%2520embeddings%2520at%252010~m%2520scale%2520from%2520pixel-level%2520satellite%2520time%2520series%250Adata.%2520TESSERA%2520uses%2520two%2520parallel%2520Transformer-based%2520encoders%2520to%2520combine%2520optical%250Adata%2520from%2520ten%2520Sentinel-2%2520spectral%2520bands%2520at%252010-60~m%2520spatial%2520resolution%2520and%2520two%250ASentinel-1%2520synthetic%2520aperture%2520radar%2520backscatter%2520coefficients%2520at%252010~m%2520resolution%250Ato%2520create%2520embeddings%2520that%2520are%2520subsequently%2520fused%2520with%2520a%2520multilayer%2520perceptron%250Ato%2520create%2520annual%2520global%2520embedding%2520maps.%2520We%2520compare%2520our%2520work%2520with%250Astate-of-the-art%2520task-specific%2520models%2520and%2520other%2520foundation%2520models%2520in%2520five%250Adiverse%2520downstream%2520tasks%2520and%2520find%2520that%2520TESSERA%2520closely%2520matches%2520or%2520outperforms%250Athese%2520baselines.%2520We%2520believe%2520that%2520TESSERA%2527s%2520ease%2520of%2520use%252C%2520openness%252C%2520computation-%252C%250Alabel-%252C%2520and%2520data-efficiency%252C%2520and%2520high%2520performance%2520will%2520prove%2520transformative%2520in%250Aa%2520wide%2520range%2520of%2520vegetation-oriented%2520ecological%2520and%2520agricultural%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TESSERA%3A%20Temporal%20Embeddings%20of%20Surface%20Spectra%20for%20Earth%20Representation%0A%20%20and%20Analysis&entry.906535625=Zhengpeng%20Feng%20and%20Clement%20Atzberger%20and%20Sadiq%20Jaffer%20and%20Jovana%20Knezevic%20and%20Silja%20Sormunen%20and%20Robin%20Young%20and%20Madeline%20C%20Lisaius%20and%20Markus%20Immitzer%20and%20David%20A.%20Coomes%20and%20Anil%20Madhavapeddy%20and%20Andrew%20Blake%20and%20Srinivasan%20Keshav&entry.1292438233=%20%20Satellite%20remote%20sensing%20from%20repeated%20observations%20and%20multiple%20sensors%0Aenables%20a%20wide%20range%20of%20downstream%20applications%2C%20including%20climate%20modeling%2C%0Acarbon%20accounting%2C%20and%20strategies%20for%20conservation%20and%20sustainable%20land%20use.%0AHowever%2C%20satellite%20time%20series%20are%20voluminous%2C%20often%20corrupted%20by%20sensor%20noise%2C%0Aclouds%2C%20and%20atmospheric%20conditions%2C%20and%20unevenly%20spaced%20in%20time%2C%20making%20them%0Achallenging%20to%20use.%20We%20present%20TESSERA%2C%20an%20open%2C%20global%2C%20land-oriented%20remote%0Asensing%20foundation%20model%20that%20uses%20self-supervised%20learning%20to%20generate%0A%60ready-to-use%27%20embeddings%20at%2010~m%20scale%20from%20pixel-level%20satellite%20time%20series%0Adata.%20TESSERA%20uses%20two%20parallel%20Transformer-based%20encoders%20to%20combine%20optical%0Adata%20from%20ten%20Sentinel-2%20spectral%20bands%20at%2010-60~m%20spatial%20resolution%20and%20two%0ASentinel-1%20synthetic%20aperture%20radar%20backscatter%20coefficients%20at%2010~m%20resolution%0Ato%20create%20embeddings%20that%20are%20subsequently%20fused%20with%20a%20multilayer%20perceptron%0Ato%20create%20annual%20global%20embedding%20maps.%20We%20compare%20our%20work%20with%0Astate-of-the-art%20task-specific%20models%20and%20other%20foundation%20models%20in%20five%0Adiverse%20downstream%20tasks%20and%20find%20that%20TESSERA%20closely%20matches%20or%20outperforms%0Athese%20baselines.%20We%20believe%20that%20TESSERA%27s%20ease%20of%20use%2C%20openness%2C%20computation-%2C%0Alabel-%2C%20and%20data-efficiency%2C%20and%20high%20performance%20will%20prove%20transformative%20in%0Aa%20wide%20range%20of%20vegetation-oriented%20ecological%20and%20agricultural%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20380v2&entry.124074799=Read"},
{"title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive\n  Initialization", "author": "Pol Francesch Huc and Emily Bates and Simone D'Amico", "abstract": "  The advent of novel view synthesis techniques such as NeRF and 3D Gaussian\nSplatting (3DGS) has enabled learning precise 3D models only from posed\nmonocular images. Although these methods are attractive, they hold two major\nlimitations that prevent their use in space applications: they require poses\nduring training, and have high computational cost at training and inference. To\naddress these limitations, this work contributes: (1) a Convolutional Neural\nNetwork (CNN) based primitive initializer for 3DGS using monocular images; (2)\na pipeline capable of training with noisy or implicit pose estimates; and (3)\nand analysis of initialization variants that reduce the training cost of\nprecise 3D models. A CNN takes a single image as input and outputs a coarse 3D\nmodel represented as an assembly of primitives, along with the target's pose\nrelative to the camera. This assembly of primitives is then used to initialize\n3DGS, significantly reducing the number of training iterations and input images\nneeded -- by at least an order of magnitude. For additional flexibility, the\nCNN component has multiple variants with different pose estimation techniques.\nThis work performs a comparison between these variants, evaluating their\neffectiveness for downstream 3DGS training under noisy or implicit pose\nestimates. The results demonstrate that even with imperfect pose supervision,\nthe pipeline is able to learn high-fidelity 3D representations, opening the\ndoor for the use of novel view synthesis in space applications.\n", "link": "http://arxiv.org/abs/2507.19459v1", "date": "2025-07-25", "relevancy": 2.4248, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6069}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Learning%20of%20Non-Cooperative%20Spacecraft%203D%20Models%20through%20Primitive%0A%20%20Initialization&body=Title%3A%20Fast%20Learning%20of%20Non-Cooperative%20Spacecraft%203D%20Models%20through%20Primitive%0A%20%20Initialization%0AAuthor%3A%20Pol%20Francesch%20Huc%20and%20Emily%20Bates%20and%20Simone%20D%27Amico%0AAbstract%3A%20%20%20The%20advent%20of%20novel%20view%20synthesis%20techniques%20such%20as%20NeRF%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20has%20enabled%20learning%20precise%203D%20models%20only%20from%20posed%0Amonocular%20images.%20Although%20these%20methods%20are%20attractive%2C%20they%20hold%20two%20major%0Alimitations%20that%20prevent%20their%20use%20in%20space%20applications%3A%20they%20require%20poses%0Aduring%20training%2C%20and%20have%20high%20computational%20cost%20at%20training%20and%20inference.%20To%0Aaddress%20these%20limitations%2C%20this%20work%20contributes%3A%20%281%29%20a%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20based%20primitive%20initializer%20for%203DGS%20using%20monocular%20images%3B%20%282%29%0Aa%20pipeline%20capable%20of%20training%20with%20noisy%20or%20implicit%20pose%20estimates%3B%20and%20%283%29%0Aand%20analysis%20of%20initialization%20variants%20that%20reduce%20the%20training%20cost%20of%0Aprecise%203D%20models.%20A%20CNN%20takes%20a%20single%20image%20as%20input%20and%20outputs%20a%20coarse%203D%0Amodel%20represented%20as%20an%20assembly%20of%20primitives%2C%20along%20with%20the%20target%27s%20pose%0Arelative%20to%20the%20camera.%20This%20assembly%20of%20primitives%20is%20then%20used%20to%20initialize%0A3DGS%2C%20significantly%20reducing%20the%20number%20of%20training%20iterations%20and%20input%20images%0Aneeded%20--%20by%20at%20least%20an%20order%20of%20magnitude.%20For%20additional%20flexibility%2C%20the%0ACNN%20component%20has%20multiple%20variants%20with%20different%20pose%20estimation%20techniques.%0AThis%20work%20performs%20a%20comparison%20between%20these%20variants%2C%20evaluating%20their%0Aeffectiveness%20for%20downstream%203DGS%20training%20under%20noisy%20or%20implicit%20pose%0Aestimates.%20The%20results%20demonstrate%20that%20even%20with%20imperfect%20pose%20supervision%2C%0Athe%20pipeline%20is%20able%20to%20learn%20high-fidelity%203D%20representations%2C%20opening%20the%0Adoor%20for%20the%20use%20of%20novel%20view%20synthesis%20in%20space%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Learning%2520of%2520Non-Cooperative%2520Spacecraft%25203D%2520Models%2520through%2520Primitive%250A%2520%2520Initialization%26entry.906535625%3DPol%2520Francesch%2520Huc%2520and%2520Emily%2520Bates%2520and%2520Simone%2520D%2527Amico%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520novel%2520view%2520synthesis%2520techniques%2520such%2520as%2520NeRF%2520and%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520has%2520enabled%2520learning%2520precise%25203D%2520models%2520only%2520from%2520posed%250Amonocular%2520images.%2520Although%2520these%2520methods%2520are%2520attractive%252C%2520they%2520hold%2520two%2520major%250Alimitations%2520that%2520prevent%2520their%2520use%2520in%2520space%2520applications%253A%2520they%2520require%2520poses%250Aduring%2520training%252C%2520and%2520have%2520high%2520computational%2520cost%2520at%2520training%2520and%2520inference.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520work%2520contributes%253A%2520%25281%2529%2520a%2520Convolutional%2520Neural%250ANetwork%2520%2528CNN%2529%2520based%2520primitive%2520initializer%2520for%25203DGS%2520using%2520monocular%2520images%253B%2520%25282%2529%250Aa%2520pipeline%2520capable%2520of%2520training%2520with%2520noisy%2520or%2520implicit%2520pose%2520estimates%253B%2520and%2520%25283%2529%250Aand%2520analysis%2520of%2520initialization%2520variants%2520that%2520reduce%2520the%2520training%2520cost%2520of%250Aprecise%25203D%2520models.%2520A%2520CNN%2520takes%2520a%2520single%2520image%2520as%2520input%2520and%2520outputs%2520a%2520coarse%25203D%250Amodel%2520represented%2520as%2520an%2520assembly%2520of%2520primitives%252C%2520along%2520with%2520the%2520target%2527s%2520pose%250Arelative%2520to%2520the%2520camera.%2520This%2520assembly%2520of%2520primitives%2520is%2520then%2520used%2520to%2520initialize%250A3DGS%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520training%2520iterations%2520and%2520input%2520images%250Aneeded%2520--%2520by%2520at%2520least%2520an%2520order%2520of%2520magnitude.%2520For%2520additional%2520flexibility%252C%2520the%250ACNN%2520component%2520has%2520multiple%2520variants%2520with%2520different%2520pose%2520estimation%2520techniques.%250AThis%2520work%2520performs%2520a%2520comparison%2520between%2520these%2520variants%252C%2520evaluating%2520their%250Aeffectiveness%2520for%2520downstream%25203DGS%2520training%2520under%2520noisy%2520or%2520implicit%2520pose%250Aestimates.%2520The%2520results%2520demonstrate%2520that%2520even%2520with%2520imperfect%2520pose%2520supervision%252C%250Athe%2520pipeline%2520is%2520able%2520to%2520learn%2520high-fidelity%25203D%2520representations%252C%2520opening%2520the%250Adoor%2520for%2520the%2520use%2520of%2520novel%2520view%2520synthesis%2520in%2520space%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Learning%20of%20Non-Cooperative%20Spacecraft%203D%20Models%20through%20Primitive%0A%20%20Initialization&entry.906535625=Pol%20Francesch%20Huc%20and%20Emily%20Bates%20and%20Simone%20D%27Amico&entry.1292438233=%20%20The%20advent%20of%20novel%20view%20synthesis%20techniques%20such%20as%20NeRF%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20has%20enabled%20learning%20precise%203D%20models%20only%20from%20posed%0Amonocular%20images.%20Although%20these%20methods%20are%20attractive%2C%20they%20hold%20two%20major%0Alimitations%20that%20prevent%20their%20use%20in%20space%20applications%3A%20they%20require%20poses%0Aduring%20training%2C%20and%20have%20high%20computational%20cost%20at%20training%20and%20inference.%20To%0Aaddress%20these%20limitations%2C%20this%20work%20contributes%3A%20%281%29%20a%20Convolutional%20Neural%0ANetwork%20%28CNN%29%20based%20primitive%20initializer%20for%203DGS%20using%20monocular%20images%3B%20%282%29%0Aa%20pipeline%20capable%20of%20training%20with%20noisy%20or%20implicit%20pose%20estimates%3B%20and%20%283%29%0Aand%20analysis%20of%20initialization%20variants%20that%20reduce%20the%20training%20cost%20of%0Aprecise%203D%20models.%20A%20CNN%20takes%20a%20single%20image%20as%20input%20and%20outputs%20a%20coarse%203D%0Amodel%20represented%20as%20an%20assembly%20of%20primitives%2C%20along%20with%20the%20target%27s%20pose%0Arelative%20to%20the%20camera.%20This%20assembly%20of%20primitives%20is%20then%20used%20to%20initialize%0A3DGS%2C%20significantly%20reducing%20the%20number%20of%20training%20iterations%20and%20input%20images%0Aneeded%20--%20by%20at%20least%20an%20order%20of%20magnitude.%20For%20additional%20flexibility%2C%20the%0ACNN%20component%20has%20multiple%20variants%20with%20different%20pose%20estimation%20techniques.%0AThis%20work%20performs%20a%20comparison%20between%20these%20variants%2C%20evaluating%20their%0Aeffectiveness%20for%20downstream%203DGS%20training%20under%20noisy%20or%20implicit%20pose%0Aestimates.%20The%20results%20demonstrate%20that%20even%20with%20imperfect%20pose%20supervision%2C%0Athe%20pipeline%20is%20able%20to%20learn%20high-fidelity%203D%20representations%2C%20opening%20the%0Adoor%20for%20the%20use%20of%20novel%20view%20synthesis%20in%20space%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19459v1&entry.124074799=Read"},
{"title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding", "author": " StepFun and  : and Bin Wang and Bojun Wang and Changyi Wan and Guanzhe Huang and Hanpeng Hu and Haonan Jia and Hao Nie and Mingliang Li and Nuo Chen and Siyu Chen and Song Yuan and Wuxun Xie and Xiaoniu Song and Xing Chen and Xingping Yang and Xuelin Zhang and Yanbo Yu and Yaoyu Wang and Yibo Zhu and Yimin Jiang and Yu Zhou and Yuanwei Lu and Houyi Li and Jingcheng Hu and Ka Man Lo and Ailin Huang and Binxing Jiao and Bo Li and Boyu Chen and Changxin Miao and Chang Lou and Chen Hu and Chen Xu and Chenfeng Yu and Chengyuan Yao and Daokuan Lv and Dapeng Shi and Deshan Sun and Ding Huang and Dingyuan Hu and Dongqing Pang and Enle Liu and Fajie Zhang and Fanqi Wan and Gulin Yan and Han Zhang and Han Zhou and Hanghao Wu and Hangyu Guo and Hanqi Chen and Hanshan Zhang and Hao Wu and Haocheng Zhang and Haolong Yan and Haoran Lv and Haoran Wei and Hebin Zhou and Heng Wang and Heng Wang and Hongxin Li and Hongyu Zhou and Hongyuan Wang and Huiyong Guo and Jia Wang and Jiahao Gong and Jialing Xie and Jian Zhou and Jianjian Sun and Jiaoren Wu and Jiaran Zhang and Jiayu Liu and Jie Cheng and Jie Luo and Jie Yan and Jie Yang and Jieyi Hou and Jinguang Zhang and Jinlan Cao and Jisheng Yin and Junfeng Liu and Junhao Huang and Junzhe Lin and Kaijun Tan and Kaixiang Li and Kang An and Kangheng Lin and Kenkun Liu and Lei Yang and Liang Zhao and Liangyu Chen and Lieyu Shi and Liguo Tan and Lin Lin and Lin Zhang and Lina Chen and Liwen Huang and Liying Shi and Longlong Gu and Mei Chen and Mengqiang Ren and Ming Li and Mingzhe Chen and Na Wang and Nan Wu and Qi Han and Qian Zhao and Qiang Zhang and Qianni Liu and Qiaohui Chen and Qiling Wu and Qinglin He and Qinyuan Tan and Qiufeng Wang and Qiuping Wu and Qiuyan Liang and Quan Sun and Rui Li and Ruihang Miao and Ruosi Wan and Ruyan Guo and Shangwu Zhong and Shaoliang Pang and Shengjie Fan and Shijie Shang and Shilei Jiang and Shiliang Yang and Shiming Hao and Shuli Gao and Siming Huang and Siqi Liu and Tiancheng Cao and Tianhao Cheng and Tianhao Peng and Wang You and Wei Ji and Wen Sun and Wenjin Deng and Wenqing He and Wenzhen Zheng and Xi Chen and Xiangwen Kong and Xianzhen Luo and Xiaobo Yang and Xiaojia Liu and Xiaoxiao Ren and Xin Han and Xin Li and Xin Wu and Xu Zhao and Yanan Wei and Yang Li and Yangguang Li and Yangshijie Xu and Yanming Xu and Yaqiang Shi and Yeqing Shen and Yi Yang and Yifei Yang and Yifeng Gong and Yihan Chen and Yijing Yang and Yinmin Zhang and Yizhuang Zhou and Yuanhao Ding and Yuantao Fan and Yuanzhen Yang and Yuchu Luo and Yue Peng and Yufan Lu and Yuhang Deng and Yuhe Yin and Yujie Liu and Yukun Chen and Yuling Zhao and Yun Mou and Yunlong Li and Yunzhou Ju and Yusheng Li and Yuxiang Yang and Yuxiang Zhang and Yuyang Chen and Zejia Weng and Zhe Xie and Zheng Ge and Zheng Gong and Zhenyi Lu and Zhewei Huang and Zhichao Chang and Zhiguo Huang and Zhirui Wang and Zidong Yang and Zili Wang and Ziqi Wang and Zixin Zhang and Binxing Jiao and Daxin Jiang and Heung-Yeung Shum and Xiangyu Zhang", "abstract": "  Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.\n", "link": "http://arxiv.org/abs/2507.19427v1", "date": "2025-07-25", "relevancy": 2.2951, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.577}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-3%20is%20Large%20yet%20Affordable%3A%20Model-system%20Co-design%20for%0A%20%20Cost-effective%20Decoding&body=Title%3A%20Step-3%20is%20Large%20yet%20Affordable%3A%20Model-system%20Co-design%20for%0A%20%20Cost-effective%20Decoding%0AAuthor%3A%20%20StepFun%20and%20%20%3A%20and%20Bin%20Wang%20and%20Bojun%20Wang%20and%20Changyi%20Wan%20and%20Guanzhe%20Huang%20and%20Hanpeng%20Hu%20and%20Haonan%20Jia%20and%20Hao%20Nie%20and%20Mingliang%20Li%20and%20Nuo%20Chen%20and%20Siyu%20Chen%20and%20Song%20Yuan%20and%20Wuxun%20Xie%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Xingping%20Yang%20and%20Xuelin%20Zhang%20and%20Yanbo%20Yu%20and%20Yaoyu%20Wang%20and%20Yibo%20Zhu%20and%20Yimin%20Jiang%20and%20Yu%20Zhou%20and%20Yuanwei%20Lu%20and%20Houyi%20Li%20and%20Jingcheng%20Hu%20and%20Ka%20Man%20Lo%20and%20Ailin%20Huang%20and%20Binxing%20Jiao%20and%20Bo%20Li%20and%20Boyu%20Chen%20and%20Changxin%20Miao%20and%20Chang%20Lou%20and%20Chen%20Hu%20and%20Chen%20Xu%20and%20Chenfeng%20Yu%20and%20Chengyuan%20Yao%20and%20Daokuan%20Lv%20and%20Dapeng%20Shi%20and%20Deshan%20Sun%20and%20Ding%20Huang%20and%20Dingyuan%20Hu%20and%20Dongqing%20Pang%20and%20Enle%20Liu%20and%20Fajie%20Zhang%20and%20Fanqi%20Wan%20and%20Gulin%20Yan%20and%20Han%20Zhang%20and%20Han%20Zhou%20and%20Hanghao%20Wu%20and%20Hangyu%20Guo%20and%20Hanqi%20Chen%20and%20Hanshan%20Zhang%20and%20Hao%20Wu%20and%20Haocheng%20Zhang%20and%20Haolong%20Yan%20and%20Haoran%20Lv%20and%20Haoran%20Wei%20and%20Hebin%20Zhou%20and%20Heng%20Wang%20and%20Heng%20Wang%20and%20Hongxin%20Li%20and%20Hongyu%20Zhou%20and%20Hongyuan%20Wang%20and%20Huiyong%20Guo%20and%20Jia%20Wang%20and%20Jiahao%20Gong%20and%20Jialing%20Xie%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jiaoren%20Wu%20and%20Jiaran%20Zhang%20and%20Jiayu%20Liu%20and%20Jie%20Cheng%20and%20Jie%20Luo%20and%20Jie%20Yan%20and%20Jie%20Yang%20and%20Jieyi%20Hou%20and%20Jinguang%20Zhang%20and%20Jinlan%20Cao%20and%20Jisheng%20Yin%20and%20Junfeng%20Liu%20and%20Junhao%20Huang%20and%20Junzhe%20Lin%20and%20Kaijun%20Tan%20and%20Kaixiang%20Li%20and%20Kang%20An%20and%20Kangheng%20Lin%20and%20Kenkun%20Liu%20and%20Lei%20Yang%20and%20Liang%20Zhao%20and%20Liangyu%20Chen%20and%20Lieyu%20Shi%20and%20Liguo%20Tan%20and%20Lin%20Lin%20and%20Lin%20Zhang%20and%20Lina%20Chen%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Longlong%20Gu%20and%20Mei%20Chen%20and%20Mengqiang%20Ren%20and%20Ming%20Li%20and%20Mingzhe%20Chen%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Qi%20Han%20and%20Qian%20Zhao%20and%20Qiang%20Zhang%20and%20Qianni%20Liu%20and%20Qiaohui%20Chen%20and%20Qiling%20Wu%20and%20Qinglin%20He%20and%20Qinyuan%20Tan%20and%20Qiufeng%20Wang%20and%20Qiuping%20Wu%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Rui%20Li%20and%20Ruihang%20Miao%20and%20Ruosi%20Wan%20and%20Ruyan%20Guo%20and%20Shangwu%20Zhong%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Shilei%20Jiang%20and%20Shiliang%20Yang%20and%20Shiming%20Hao%20and%20Shuli%20Gao%20and%20Siming%20Huang%20and%20Siqi%20Liu%20and%20Tiancheng%20Cao%20and%20Tianhao%20Cheng%20and%20Tianhao%20Peng%20and%20Wang%20You%20and%20Wei%20Ji%20and%20Wen%20Sun%20and%20Wenjin%20Deng%20and%20Wenqing%20He%20and%20Wenzhen%20Zheng%20and%20Xi%20Chen%20and%20Xiangwen%20Kong%20and%20Xianzhen%20Luo%20and%20Xiaobo%20Yang%20and%20Xiaojia%20Liu%20and%20Xiaoxiao%20Ren%20and%20Xin%20Han%20and%20Xin%20Li%20and%20Xin%20Wu%20and%20Xu%20Zhao%20and%20Yanan%20Wei%20and%20Yang%20Li%20and%20Yangguang%20Li%20and%20Yangshijie%20Xu%20and%20Yanming%20Xu%20and%20Yaqiang%20Shi%20and%20Yeqing%20Shen%20and%20Yi%20Yang%20and%20Yifei%20Yang%20and%20Yifeng%20Gong%20and%20Yihan%20Chen%20and%20Yijing%20Yang%20and%20Yinmin%20Zhang%20and%20Yizhuang%20Zhou%20and%20Yuanhao%20Ding%20and%20Yuantao%20Fan%20and%20Yuanzhen%20Yang%20and%20Yuchu%20Luo%20and%20Yue%20Peng%20and%20Yufan%20Lu%20and%20Yuhang%20Deng%20and%20Yuhe%20Yin%20and%20Yujie%20Liu%20and%20Yukun%20Chen%20and%20Yuling%20Zhao%20and%20Yun%20Mou%20and%20Yunlong%20Li%20and%20Yunzhou%20Ju%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuxiang%20Zhang%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Zhe%20Xie%20and%20Zheng%20Ge%20and%20Zheng%20Gong%20and%20Zhenyi%20Lu%20and%20Zhewei%20Huang%20and%20Zhichao%20Chang%20and%20Zhiguo%20Huang%20and%20Zhirui%20Wang%20and%20Zidong%20Yang%20and%20Zili%20Wang%20and%20Ziqi%20Wang%20and%20Zixin%20Zhang%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Heung-Yeung%20Shum%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20face%20low%20hardware%20efficiency%20during%20decoding%2C%0Aespecially%20for%20long-context%20reasoning%20tasks.%20This%20paper%20introduces%20Step-3%2C%20a%0A321B-parameter%20VLM%20with%20hardware-aware%20model-system%20co-design%20optimized%20for%0Aminimizing%20decoding%20costs.%20Step-3%20innovates%20in%20two%20key%20dimensions%3A%20%281%29%20A%20novel%0AMulti-Matrix%20Factorization%20Attention%20%28MFA%29%20mechanism%20that%20significantly%20reduces%0Aboth%20KV%20cache%20size%20and%20computation%20while%20maintaining%20high%20attention%0Aexpressiveness%2C%20and%20%282%29%20Attention-FFN%20Disaggregation%20%28AFD%29%2C%20a%20distributed%0Ainference%20system%20that%20decouples%20attention%20and%20Feed-Forward%20Network%20%28FFN%29%20layers%0Ainto%20specialized%20subsystems.%20This%20co-design%20achieves%20unprecedented%20cost%0Aefficiency%3A%20Step-3%20significantly%20reduces%20theoretical%20decoding%20costs%20compared%0Awith%20models%20like%20DeepSeek-V3%20and%20Qwen3%20MoE%20235B%2C%20with%20the%20gains%20widening%20at%0Alonger%20context.%20Step-3%20achieves%20low%20cost%20while%20activating%2038B%20parameters%20per%0Atoken%20%28more%20than%20DeepSeek-V3%20and%20Qwen3%20MoE%20235B%29%2C%20demonstrating%20that%0Ahardware-aligned%20attention%20arithmetic%20intensity%2C%20MoE%20sparsity%2C%20and%20AFD%20are%0Acritical%20to%20cost-effectiveness.%20We%20perform%20a%20head-to-head%20comparison%20with%0ADeepSeek-V3%20in%20its%20favorable%20scenarios.%20Our%20implementation%20on%20Hopper%20GPUs%0Aachieves%20a%20decoding%20throughput%20of%20up%20to%204%2C039%20tokens%20per%20second%20per%20GPU%20under%0A50ms%20TPOT%20SLA%20%284K%20context%2C%20FP8%2C%20no%20MTP%29.%20It%20is%20higher%20than%20DeepSeek-V3%27s%202%2C324%0Ain%20the%20same%20setup%20and%20sets%20a%20new%20Pareto%20frontier%20for%20LLM%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-3%2520is%2520Large%2520yet%2520Affordable%253A%2520Model-system%2520Co-design%2520for%250A%2520%2520Cost-effective%2520Decoding%26entry.906535625%3D%2520StepFun%2520and%2520%2520%253A%2520and%2520Bin%2520Wang%2520and%2520Bojun%2520Wang%2520and%2520Changyi%2520Wan%2520and%2520Guanzhe%2520Huang%2520and%2520Hanpeng%2520Hu%2520and%2520Haonan%2520Jia%2520and%2520Hao%2520Nie%2520and%2520Mingliang%2520Li%2520and%2520Nuo%2520Chen%2520and%2520Siyu%2520Chen%2520and%2520Song%2520Yuan%2520and%2520Wuxun%2520Xie%2520and%2520Xiaoniu%2520Song%2520and%2520Xing%2520Chen%2520and%2520Xingping%2520Yang%2520and%2520Xuelin%2520Zhang%2520and%2520Yanbo%2520Yu%2520and%2520Yaoyu%2520Wang%2520and%2520Yibo%2520Zhu%2520and%2520Yimin%2520Jiang%2520and%2520Yu%2520Zhou%2520and%2520Yuanwei%2520Lu%2520and%2520Houyi%2520Li%2520and%2520Jingcheng%2520Hu%2520and%2520Ka%2520Man%2520Lo%2520and%2520Ailin%2520Huang%2520and%2520Binxing%2520Jiao%2520and%2520Bo%2520Li%2520and%2520Boyu%2520Chen%2520and%2520Changxin%2520Miao%2520and%2520Chang%2520Lou%2520and%2520Chen%2520Hu%2520and%2520Chen%2520Xu%2520and%2520Chenfeng%2520Yu%2520and%2520Chengyuan%2520Yao%2520and%2520Daokuan%2520Lv%2520and%2520Dapeng%2520Shi%2520and%2520Deshan%2520Sun%2520and%2520Ding%2520Huang%2520and%2520Dingyuan%2520Hu%2520and%2520Dongqing%2520Pang%2520and%2520Enle%2520Liu%2520and%2520Fajie%2520Zhang%2520and%2520Fanqi%2520Wan%2520and%2520Gulin%2520Yan%2520and%2520Han%2520Zhang%2520and%2520Han%2520Zhou%2520and%2520Hanghao%2520Wu%2520and%2520Hangyu%2520Guo%2520and%2520Hanqi%2520Chen%2520and%2520Hanshan%2520Zhang%2520and%2520Hao%2520Wu%2520and%2520Haocheng%2520Zhang%2520and%2520Haolong%2520Yan%2520and%2520Haoran%2520Lv%2520and%2520Haoran%2520Wei%2520and%2520Hebin%2520Zhou%2520and%2520Heng%2520Wang%2520and%2520Heng%2520Wang%2520and%2520Hongxin%2520Li%2520and%2520Hongyu%2520Zhou%2520and%2520Hongyuan%2520Wang%2520and%2520Huiyong%2520Guo%2520and%2520Jia%2520Wang%2520and%2520Jiahao%2520Gong%2520and%2520Jialing%2520Xie%2520and%2520Jian%2520Zhou%2520and%2520Jianjian%2520Sun%2520and%2520Jiaoren%2520Wu%2520and%2520Jiaran%2520Zhang%2520and%2520Jiayu%2520Liu%2520and%2520Jie%2520Cheng%2520and%2520Jie%2520Luo%2520and%2520Jie%2520Yan%2520and%2520Jie%2520Yang%2520and%2520Jieyi%2520Hou%2520and%2520Jinguang%2520Zhang%2520and%2520Jinlan%2520Cao%2520and%2520Jisheng%2520Yin%2520and%2520Junfeng%2520Liu%2520and%2520Junhao%2520Huang%2520and%2520Junzhe%2520Lin%2520and%2520Kaijun%2520Tan%2520and%2520Kaixiang%2520Li%2520and%2520Kang%2520An%2520and%2520Kangheng%2520Lin%2520and%2520Kenkun%2520Liu%2520and%2520Lei%2520Yang%2520and%2520Liang%2520Zhao%2520and%2520Liangyu%2520Chen%2520and%2520Lieyu%2520Shi%2520and%2520Liguo%2520Tan%2520and%2520Lin%2520Lin%2520and%2520Lin%2520Zhang%2520and%2520Lina%2520Chen%2520and%2520Liwen%2520Huang%2520and%2520Liying%2520Shi%2520and%2520Longlong%2520Gu%2520and%2520Mei%2520Chen%2520and%2520Mengqiang%2520Ren%2520and%2520Ming%2520Li%2520and%2520Mingzhe%2520Chen%2520and%2520Na%2520Wang%2520and%2520Nan%2520Wu%2520and%2520Qi%2520Han%2520and%2520Qian%2520Zhao%2520and%2520Qiang%2520Zhang%2520and%2520Qianni%2520Liu%2520and%2520Qiaohui%2520Chen%2520and%2520Qiling%2520Wu%2520and%2520Qinglin%2520He%2520and%2520Qinyuan%2520Tan%2520and%2520Qiufeng%2520Wang%2520and%2520Qiuping%2520Wu%2520and%2520Qiuyan%2520Liang%2520and%2520Quan%2520Sun%2520and%2520Rui%2520Li%2520and%2520Ruihang%2520Miao%2520and%2520Ruosi%2520Wan%2520and%2520Ruyan%2520Guo%2520and%2520Shangwu%2520Zhong%2520and%2520Shaoliang%2520Pang%2520and%2520Shengjie%2520Fan%2520and%2520Shijie%2520Shang%2520and%2520Shilei%2520Jiang%2520and%2520Shiliang%2520Yang%2520and%2520Shiming%2520Hao%2520and%2520Shuli%2520Gao%2520and%2520Siming%2520Huang%2520and%2520Siqi%2520Liu%2520and%2520Tiancheng%2520Cao%2520and%2520Tianhao%2520Cheng%2520and%2520Tianhao%2520Peng%2520and%2520Wang%2520You%2520and%2520Wei%2520Ji%2520and%2520Wen%2520Sun%2520and%2520Wenjin%2520Deng%2520and%2520Wenqing%2520He%2520and%2520Wenzhen%2520Zheng%2520and%2520Xi%2520Chen%2520and%2520Xiangwen%2520Kong%2520and%2520Xianzhen%2520Luo%2520and%2520Xiaobo%2520Yang%2520and%2520Xiaojia%2520Liu%2520and%2520Xiaoxiao%2520Ren%2520and%2520Xin%2520Han%2520and%2520Xin%2520Li%2520and%2520Xin%2520Wu%2520and%2520Xu%2520Zhao%2520and%2520Yanan%2520Wei%2520and%2520Yang%2520Li%2520and%2520Yangguang%2520Li%2520and%2520Yangshijie%2520Xu%2520and%2520Yanming%2520Xu%2520and%2520Yaqiang%2520Shi%2520and%2520Yeqing%2520Shen%2520and%2520Yi%2520Yang%2520and%2520Yifei%2520Yang%2520and%2520Yifeng%2520Gong%2520and%2520Yihan%2520Chen%2520and%2520Yijing%2520Yang%2520and%2520Yinmin%2520Zhang%2520and%2520Yizhuang%2520Zhou%2520and%2520Yuanhao%2520Ding%2520and%2520Yuantao%2520Fan%2520and%2520Yuanzhen%2520Yang%2520and%2520Yuchu%2520Luo%2520and%2520Yue%2520Peng%2520and%2520Yufan%2520Lu%2520and%2520Yuhang%2520Deng%2520and%2520Yuhe%2520Yin%2520and%2520Yujie%2520Liu%2520and%2520Yukun%2520Chen%2520and%2520Yuling%2520Zhao%2520and%2520Yun%2520Mou%2520and%2520Yunlong%2520Li%2520and%2520Yunzhou%2520Ju%2520and%2520Yusheng%2520Li%2520and%2520Yuxiang%2520Yang%2520and%2520Yuxiang%2520Zhang%2520and%2520Yuyang%2520Chen%2520and%2520Zejia%2520Weng%2520and%2520Zhe%2520Xie%2520and%2520Zheng%2520Ge%2520and%2520Zheng%2520Gong%2520and%2520Zhenyi%2520Lu%2520and%2520Zhewei%2520Huang%2520and%2520Zhichao%2520Chang%2520and%2520Zhiguo%2520Huang%2520and%2520Zhirui%2520Wang%2520and%2520Zidong%2520Yang%2520and%2520Zili%2520Wang%2520and%2520Ziqi%2520Wang%2520and%2520Zixin%2520Zhang%2520and%2520Binxing%2520Jiao%2520and%2520Daxin%2520Jiang%2520and%2520Heung-Yeung%2520Shum%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520face%2520low%2520hardware%2520efficiency%2520during%2520decoding%252C%250Aespecially%2520for%2520long-context%2520reasoning%2520tasks.%2520This%2520paper%2520introduces%2520Step-3%252C%2520a%250A321B-parameter%2520VLM%2520with%2520hardware-aware%2520model-system%2520co-design%2520optimized%2520for%250Aminimizing%2520decoding%2520costs.%2520Step-3%2520innovates%2520in%2520two%2520key%2520dimensions%253A%2520%25281%2529%2520A%2520novel%250AMulti-Matrix%2520Factorization%2520Attention%2520%2528MFA%2529%2520mechanism%2520that%2520significantly%2520reduces%250Aboth%2520KV%2520cache%2520size%2520and%2520computation%2520while%2520maintaining%2520high%2520attention%250Aexpressiveness%252C%2520and%2520%25282%2529%2520Attention-FFN%2520Disaggregation%2520%2528AFD%2529%252C%2520a%2520distributed%250Ainference%2520system%2520that%2520decouples%2520attention%2520and%2520Feed-Forward%2520Network%2520%2528FFN%2529%2520layers%250Ainto%2520specialized%2520subsystems.%2520This%2520co-design%2520achieves%2520unprecedented%2520cost%250Aefficiency%253A%2520Step-3%2520significantly%2520reduces%2520theoretical%2520decoding%2520costs%2520compared%250Awith%2520models%2520like%2520DeepSeek-V3%2520and%2520Qwen3%2520MoE%2520235B%252C%2520with%2520the%2520gains%2520widening%2520at%250Alonger%2520context.%2520Step-3%2520achieves%2520low%2520cost%2520while%2520activating%252038B%2520parameters%2520per%250Atoken%2520%2528more%2520than%2520DeepSeek-V3%2520and%2520Qwen3%2520MoE%2520235B%2529%252C%2520demonstrating%2520that%250Ahardware-aligned%2520attention%2520arithmetic%2520intensity%252C%2520MoE%2520sparsity%252C%2520and%2520AFD%2520are%250Acritical%2520to%2520cost-effectiveness.%2520We%2520perform%2520a%2520head-to-head%2520comparison%2520with%250ADeepSeek-V3%2520in%2520its%2520favorable%2520scenarios.%2520Our%2520implementation%2520on%2520Hopper%2520GPUs%250Aachieves%2520a%2520decoding%2520throughput%2520of%2520up%2520to%25204%252C039%2520tokens%2520per%2520second%2520per%2520GPU%2520under%250A50ms%2520TPOT%2520SLA%2520%25284K%2520context%252C%2520FP8%252C%2520no%2520MTP%2529.%2520It%2520is%2520higher%2520than%2520DeepSeek-V3%2527s%25202%252C324%250Ain%2520the%2520same%2520setup%2520and%2520sets%2520a%2520new%2520Pareto%2520frontier%2520for%2520LLM%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-3%20is%20Large%20yet%20Affordable%3A%20Model-system%20Co-design%20for%0A%20%20Cost-effective%20Decoding&entry.906535625=%20StepFun%20and%20%20%3A%20and%20Bin%20Wang%20and%20Bojun%20Wang%20and%20Changyi%20Wan%20and%20Guanzhe%20Huang%20and%20Hanpeng%20Hu%20and%20Haonan%20Jia%20and%20Hao%20Nie%20and%20Mingliang%20Li%20and%20Nuo%20Chen%20and%20Siyu%20Chen%20and%20Song%20Yuan%20and%20Wuxun%20Xie%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Xingping%20Yang%20and%20Xuelin%20Zhang%20and%20Yanbo%20Yu%20and%20Yaoyu%20Wang%20and%20Yibo%20Zhu%20and%20Yimin%20Jiang%20and%20Yu%20Zhou%20and%20Yuanwei%20Lu%20and%20Houyi%20Li%20and%20Jingcheng%20Hu%20and%20Ka%20Man%20Lo%20and%20Ailin%20Huang%20and%20Binxing%20Jiao%20and%20Bo%20Li%20and%20Boyu%20Chen%20and%20Changxin%20Miao%20and%20Chang%20Lou%20and%20Chen%20Hu%20and%20Chen%20Xu%20and%20Chenfeng%20Yu%20and%20Chengyuan%20Yao%20and%20Daokuan%20Lv%20and%20Dapeng%20Shi%20and%20Deshan%20Sun%20and%20Ding%20Huang%20and%20Dingyuan%20Hu%20and%20Dongqing%20Pang%20and%20Enle%20Liu%20and%20Fajie%20Zhang%20and%20Fanqi%20Wan%20and%20Gulin%20Yan%20and%20Han%20Zhang%20and%20Han%20Zhou%20and%20Hanghao%20Wu%20and%20Hangyu%20Guo%20and%20Hanqi%20Chen%20and%20Hanshan%20Zhang%20and%20Hao%20Wu%20and%20Haocheng%20Zhang%20and%20Haolong%20Yan%20and%20Haoran%20Lv%20and%20Haoran%20Wei%20and%20Hebin%20Zhou%20and%20Heng%20Wang%20and%20Heng%20Wang%20and%20Hongxin%20Li%20and%20Hongyu%20Zhou%20and%20Hongyuan%20Wang%20and%20Huiyong%20Guo%20and%20Jia%20Wang%20and%20Jiahao%20Gong%20and%20Jialing%20Xie%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jiaoren%20Wu%20and%20Jiaran%20Zhang%20and%20Jiayu%20Liu%20and%20Jie%20Cheng%20and%20Jie%20Luo%20and%20Jie%20Yan%20and%20Jie%20Yang%20and%20Jieyi%20Hou%20and%20Jinguang%20Zhang%20and%20Jinlan%20Cao%20and%20Jisheng%20Yin%20and%20Junfeng%20Liu%20and%20Junhao%20Huang%20and%20Junzhe%20Lin%20and%20Kaijun%20Tan%20and%20Kaixiang%20Li%20and%20Kang%20An%20and%20Kangheng%20Lin%20and%20Kenkun%20Liu%20and%20Lei%20Yang%20and%20Liang%20Zhao%20and%20Liangyu%20Chen%20and%20Lieyu%20Shi%20and%20Liguo%20Tan%20and%20Lin%20Lin%20and%20Lin%20Zhang%20and%20Lina%20Chen%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Longlong%20Gu%20and%20Mei%20Chen%20and%20Mengqiang%20Ren%20and%20Ming%20Li%20and%20Mingzhe%20Chen%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Qi%20Han%20and%20Qian%20Zhao%20and%20Qiang%20Zhang%20and%20Qianni%20Liu%20and%20Qiaohui%20Chen%20and%20Qiling%20Wu%20and%20Qinglin%20He%20and%20Qinyuan%20Tan%20and%20Qiufeng%20Wang%20and%20Qiuping%20Wu%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Rui%20Li%20and%20Ruihang%20Miao%20and%20Ruosi%20Wan%20and%20Ruyan%20Guo%20and%20Shangwu%20Zhong%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Shilei%20Jiang%20and%20Shiliang%20Yang%20and%20Shiming%20Hao%20and%20Shuli%20Gao%20and%20Siming%20Huang%20and%20Siqi%20Liu%20and%20Tiancheng%20Cao%20and%20Tianhao%20Cheng%20and%20Tianhao%20Peng%20and%20Wang%20You%20and%20Wei%20Ji%20and%20Wen%20Sun%20and%20Wenjin%20Deng%20and%20Wenqing%20He%20and%20Wenzhen%20Zheng%20and%20Xi%20Chen%20and%20Xiangwen%20Kong%20and%20Xianzhen%20Luo%20and%20Xiaobo%20Yang%20and%20Xiaojia%20Liu%20and%20Xiaoxiao%20Ren%20and%20Xin%20Han%20and%20Xin%20Li%20and%20Xin%20Wu%20and%20Xu%20Zhao%20and%20Yanan%20Wei%20and%20Yang%20Li%20and%20Yangguang%20Li%20and%20Yangshijie%20Xu%20and%20Yanming%20Xu%20and%20Yaqiang%20Shi%20and%20Yeqing%20Shen%20and%20Yi%20Yang%20and%20Yifei%20Yang%20and%20Yifeng%20Gong%20and%20Yihan%20Chen%20and%20Yijing%20Yang%20and%20Yinmin%20Zhang%20and%20Yizhuang%20Zhou%20and%20Yuanhao%20Ding%20and%20Yuantao%20Fan%20and%20Yuanzhen%20Yang%20and%20Yuchu%20Luo%20and%20Yue%20Peng%20and%20Yufan%20Lu%20and%20Yuhang%20Deng%20and%20Yuhe%20Yin%20and%20Yujie%20Liu%20and%20Yukun%20Chen%20and%20Yuling%20Zhao%20and%20Yun%20Mou%20and%20Yunlong%20Li%20and%20Yunzhou%20Ju%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuxiang%20Zhang%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Zhe%20Xie%20and%20Zheng%20Ge%20and%20Zheng%20Gong%20and%20Zhenyi%20Lu%20and%20Zhewei%20Huang%20and%20Zhichao%20Chang%20and%20Zhiguo%20Huang%20and%20Zhirui%20Wang%20and%20Zidong%20Yang%20and%20Zili%20Wang%20and%20Ziqi%20Wang%20and%20Zixin%20Zhang%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Heung-Yeung%20Shum%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20face%20low%20hardware%20efficiency%20during%20decoding%2C%0Aespecially%20for%20long-context%20reasoning%20tasks.%20This%20paper%20introduces%20Step-3%2C%20a%0A321B-parameter%20VLM%20with%20hardware-aware%20model-system%20co-design%20optimized%20for%0Aminimizing%20decoding%20costs.%20Step-3%20innovates%20in%20two%20key%20dimensions%3A%20%281%29%20A%20novel%0AMulti-Matrix%20Factorization%20Attention%20%28MFA%29%20mechanism%20that%20significantly%20reduces%0Aboth%20KV%20cache%20size%20and%20computation%20while%20maintaining%20high%20attention%0Aexpressiveness%2C%20and%20%282%29%20Attention-FFN%20Disaggregation%20%28AFD%29%2C%20a%20distributed%0Ainference%20system%20that%20decouples%20attention%20and%20Feed-Forward%20Network%20%28FFN%29%20layers%0Ainto%20specialized%20subsystems.%20This%20co-design%20achieves%20unprecedented%20cost%0Aefficiency%3A%20Step-3%20significantly%20reduces%20theoretical%20decoding%20costs%20compared%0Awith%20models%20like%20DeepSeek-V3%20and%20Qwen3%20MoE%20235B%2C%20with%20the%20gains%20widening%20at%0Alonger%20context.%20Step-3%20achieves%20low%20cost%20while%20activating%2038B%20parameters%20per%0Atoken%20%28more%20than%20DeepSeek-V3%20and%20Qwen3%20MoE%20235B%29%2C%20demonstrating%20that%0Ahardware-aligned%20attention%20arithmetic%20intensity%2C%20MoE%20sparsity%2C%20and%20AFD%20are%0Acritical%20to%20cost-effectiveness.%20We%20perform%20a%20head-to-head%20comparison%20with%0ADeepSeek-V3%20in%20its%20favorable%20scenarios.%20Our%20implementation%20on%20Hopper%20GPUs%0Aachieves%20a%20decoding%20throughput%20of%20up%20to%204%2C039%20tokens%20per%20second%20per%20GPU%20under%0A50ms%20TPOT%20SLA%20%284K%20context%2C%20FP8%2C%20no%20MTP%29.%20It%20is%20higher%20than%20DeepSeek-V3%27s%202%2C324%0Ain%20the%20same%20setup%20and%20sets%20a%20new%20Pareto%20frontier%20for%20LLM%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19427v1&entry.124074799=Read"},
{"title": "ReCatcher: Towards LLMs Regression Testing for Code Generation", "author": "Altaf Allah Abbassi and Leuson Da Silva and Amin Nikanjam and Foutse Khomh", "abstract": "  Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions.\n", "link": "http://arxiv.org/abs/2507.19390v1", "date": "2025-07-25", "relevancy": 2.277, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCatcher%3A%20Towards%20LLMs%20Regression%20Testing%20for%20Code%20Generation&body=Title%3A%20ReCatcher%3A%20Towards%20LLMs%20Regression%20Testing%20for%20Code%20Generation%0AAuthor%3A%20Altaf%20Allah%20Abbassi%20and%20Leuson%20Da%20Silva%20and%20Amin%20Nikanjam%20and%20Foutse%20Khomh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20generation%20evolve%20rapidly%20through%0Afine-tuning%2C%20merging%2C%20or%20new%20model%20releases.%20However%2C%20such%20updates%20can%0Aintroduce%20regressions%2C%20not%20only%20in%20correctness%20but%20also%20in%20code%20quality%20and%0Aperformance.%20To%20address%20this%2C%20we%20present%20ReCatcher%2C%20a%20regression%20testing%0Aframework%20for%20Python%20code%20generation.%20ReCatcher%20systematically%20compares%20two%0ALLMs%2C%20typically%20a%20current%20model%20and%20a%20candidate%20update%2C%20across%20three%0Adimensions%3A%20logical%20correctness%2C%20static%20code%20quality%2C%20and%20execution%0Aperformance.%20We%20apply%20ReCatcher%20to%20assess%20regressions%20across%20three%20update%0Ascenarios%2C%20fine-tuning%2C%20merging%2C%20and%20model%20release%2C%20using%20CodeLlama%2C%0ADeepSeek-Coder%2C%20and%20GPT-4o.%20Our%20evaluation%20shows%20that%20fine-tuning%20with%0Across-language%20datasets%20increases%20syntax%20errors%20by%20up%20to%2012%25.%20Merging%20with%0Ageneral-purpose%20models%20like%20Llama2%20leads%20to%20regressions%20in%20correctness%20by%20up%20to%0A18%25.%20GPT-4o%20introduces%20regressions%20of%20up%20to%2050%25%20in%20handling%20missing%20imports%0Acompared%20to%20GPT-3.5-turbo%2C%20while%20GPT-4o-mini%20suffers%20up%20to%2080%25%20performance%0Adegradation%20in%20execution%20time%20versus%20GPT-4o.%20Overall%2C%20logical%20correctness%2C%0Aperformance%2C%20and%20error%20handling%20%28e.g.%2C%20syntax%20errors%20and%20missing%20imports%29%20are%0Athe%20most%20regression-prone%20areas.%20Comparing%20ReCatcher%20with%20baseline%20solutions%2C%0Ait%20presents%20better%20and%20consistent%20accuracy%20across%20logical%20and%20performance%0Aaspects.%20ReCatcher%20highlights%20the%20importance%20of%20systematic%20regression%0Aevaluation%20before%20adopting%20new%20models%2C%20while%20assisting%20researchers%20and%0Apractitioners%20in%20making%20more%20informed%20update%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCatcher%253A%2520Towards%2520LLMs%2520Regression%2520Testing%2520for%2520Code%2520Generation%26entry.906535625%3DAltaf%2520Allah%2520Abbassi%2520and%2520Leuson%2520Da%2520Silva%2520and%2520Amin%2520Nikanjam%2520and%2520Foutse%2520Khomh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520code%2520generation%2520evolve%2520rapidly%2520through%250Afine-tuning%252C%2520merging%252C%2520or%2520new%2520model%2520releases.%2520However%252C%2520such%2520updates%2520can%250Aintroduce%2520regressions%252C%2520not%2520only%2520in%2520correctness%2520but%2520also%2520in%2520code%2520quality%2520and%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520present%2520ReCatcher%252C%2520a%2520regression%2520testing%250Aframework%2520for%2520Python%2520code%2520generation.%2520ReCatcher%2520systematically%2520compares%2520two%250ALLMs%252C%2520typically%2520a%2520current%2520model%2520and%2520a%2520candidate%2520update%252C%2520across%2520three%250Adimensions%253A%2520logical%2520correctness%252C%2520static%2520code%2520quality%252C%2520and%2520execution%250Aperformance.%2520We%2520apply%2520ReCatcher%2520to%2520assess%2520regressions%2520across%2520three%2520update%250Ascenarios%252C%2520fine-tuning%252C%2520merging%252C%2520and%2520model%2520release%252C%2520using%2520CodeLlama%252C%250ADeepSeek-Coder%252C%2520and%2520GPT-4o.%2520Our%2520evaluation%2520shows%2520that%2520fine-tuning%2520with%250Across-language%2520datasets%2520increases%2520syntax%2520errors%2520by%2520up%2520to%252012%2525.%2520Merging%2520with%250Ageneral-purpose%2520models%2520like%2520Llama2%2520leads%2520to%2520regressions%2520in%2520correctness%2520by%2520up%2520to%250A18%2525.%2520GPT-4o%2520introduces%2520regressions%2520of%2520up%2520to%252050%2525%2520in%2520handling%2520missing%2520imports%250Acompared%2520to%2520GPT-3.5-turbo%252C%2520while%2520GPT-4o-mini%2520suffers%2520up%2520to%252080%2525%2520performance%250Adegradation%2520in%2520execution%2520time%2520versus%2520GPT-4o.%2520Overall%252C%2520logical%2520correctness%252C%250Aperformance%252C%2520and%2520error%2520handling%2520%2528e.g.%252C%2520syntax%2520errors%2520and%2520missing%2520imports%2529%2520are%250Athe%2520most%2520regression-prone%2520areas.%2520Comparing%2520ReCatcher%2520with%2520baseline%2520solutions%252C%250Ait%2520presents%2520better%2520and%2520consistent%2520accuracy%2520across%2520logical%2520and%2520performance%250Aaspects.%2520ReCatcher%2520highlights%2520the%2520importance%2520of%2520systematic%2520regression%250Aevaluation%2520before%2520adopting%2520new%2520models%252C%2520while%2520assisting%2520researchers%2520and%250Apractitioners%2520in%2520making%2520more%2520informed%2520update%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCatcher%3A%20Towards%20LLMs%20Regression%20Testing%20for%20Code%20Generation&entry.906535625=Altaf%20Allah%20Abbassi%20and%20Leuson%20Da%20Silva%20and%20Amin%20Nikanjam%20and%20Foutse%20Khomh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20for%20code%20generation%20evolve%20rapidly%20through%0Afine-tuning%2C%20merging%2C%20or%20new%20model%20releases.%20However%2C%20such%20updates%20can%0Aintroduce%20regressions%2C%20not%20only%20in%20correctness%20but%20also%20in%20code%20quality%20and%0Aperformance.%20To%20address%20this%2C%20we%20present%20ReCatcher%2C%20a%20regression%20testing%0Aframework%20for%20Python%20code%20generation.%20ReCatcher%20systematically%20compares%20two%0ALLMs%2C%20typically%20a%20current%20model%20and%20a%20candidate%20update%2C%20across%20three%0Adimensions%3A%20logical%20correctness%2C%20static%20code%20quality%2C%20and%20execution%0Aperformance.%20We%20apply%20ReCatcher%20to%20assess%20regressions%20across%20three%20update%0Ascenarios%2C%20fine-tuning%2C%20merging%2C%20and%20model%20release%2C%20using%20CodeLlama%2C%0ADeepSeek-Coder%2C%20and%20GPT-4o.%20Our%20evaluation%20shows%20that%20fine-tuning%20with%0Across-language%20datasets%20increases%20syntax%20errors%20by%20up%20to%2012%25.%20Merging%20with%0Ageneral-purpose%20models%20like%20Llama2%20leads%20to%20regressions%20in%20correctness%20by%20up%20to%0A18%25.%20GPT-4o%20introduces%20regressions%20of%20up%20to%2050%25%20in%20handling%20missing%20imports%0Acompared%20to%20GPT-3.5-turbo%2C%20while%20GPT-4o-mini%20suffers%20up%20to%2080%25%20performance%0Adegradation%20in%20execution%20time%20versus%20GPT-4o.%20Overall%2C%20logical%20correctness%2C%0Aperformance%2C%20and%20error%20handling%20%28e.g.%2C%20syntax%20errors%20and%20missing%20imports%29%20are%0Athe%20most%20regression-prone%20areas.%20Comparing%20ReCatcher%20with%20baseline%20solutions%2C%0Ait%20presents%20better%20and%20consistent%20accuracy%20across%20logical%20and%20performance%0Aaspects.%20ReCatcher%20highlights%20the%20importance%20of%20systematic%20regression%0Aevaluation%20before%20adopting%20new%20models%2C%20while%20assisting%20researchers%20and%0Apractitioners%20in%20making%20more%20informed%20update%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19390v1&entry.124074799=Read"},
{"title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for\n  Efficient 3D Medical Image Segmentation", "author": "Nand Kumar Yadav and Rodrigue Rizk and William CW Chen and KC Santosh", "abstract": "  Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP\n", "link": "http://arxiv.org/abs/2507.16122v3", "date": "2025-07-25", "relevancy": 2.2633, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6155}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLRU%2B%2B%3A%20Multiscale%20Lightweight%20Residual%20UNETR%2B%2B%20with%20Attention%20for%0A%20%20Efficient%203D%20Medical%20Image%20Segmentation&body=Title%3A%20MLRU%2B%2B%3A%20Multiscale%20Lightweight%20Residual%20UNETR%2B%2B%20with%20Attention%20for%0A%20%20Efficient%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Nand%20Kumar%20Yadav%20and%20Rodrigue%20Rizk%20and%20William%20CW%20Chen%20and%20KC%20Santosh%0AAbstract%3A%20%20%20Accurate%20and%20efficient%20medical%20image%20segmentation%20is%20crucial%20but%20challenging%0Adue%20to%20anatomical%20variability%20and%20high%20computational%20demands%20on%20volumetric%0Adata.%20Recent%20hybrid%20CNN-Transformer%20architectures%20achieve%20state-of-the-art%0Aresults%20but%20add%20significant%20complexity.%20In%20this%20paper%2C%20we%20propose%20MLRU%2B%2B%2C%20a%0AMultiscale%20Lightweight%20Residual%20UNETR%2B%2B%20architecture%20designed%20to%20balance%0Asegmentation%20accuracy%20and%20computational%20efficiency.%20It%20introduces%20two%20key%0Ainnovations%3A%20a%20Lightweight%20Channel%20and%20Bottleneck%20Attention%20Module%20%28LCBAM%29%20that%0Aenhances%20contextual%20feature%20encoding%20with%20minimal%20overhead%2C%20and%20a%20Multiscale%0ABottleneck%20Block%20%28M2B%29%20in%20the%20decoder%20that%20captures%20fine-grained%20details%20via%0Amulti-resolution%20feature%20aggregation.%20Experiments%20on%20four%20publicly%20available%0Abenchmark%20datasets%20%28Synapse%2C%20BTCV%2C%20ACDC%2C%20and%20Decathlon%20Lung%29%20demonstrate%20that%0AMLRU%2B%2B%20achieves%20state-of-the-art%20performance%2C%20with%20average%20Dice%20scores%20of%0A87.57%25%20%28Synapse%29%2C%2093.00%25%20%28ACDC%29%2C%20and%2081.12%25%20%28Lung%29.%20Compared%20to%20existing%0Aleading%20models%2C%20MLRU%2B%2B%20improves%20Dice%20scores%20by%205.38%25%20and%202.12%25%20on%20Synapse%20and%0AACDC%2C%20respectively%2C%20while%20significantly%20reducing%20parameter%20count%20and%0Acomputational%20cost.%20Ablation%20studies%20evaluating%20LCBAM%20and%20M2B%20further%20confirm%0Athe%20effectiveness%20of%20the%20proposed%20architectural%20components.%20Results%20suggest%0Athat%20MLRU%2B%2B%20offers%20a%20practical%20and%20high-performing%20solution%20for%203D%20medical%0Aimage%20segmentation%20tasks.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/1027865/MLRUPP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16122v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLRU%252B%252B%253A%2520Multiscale%2520Lightweight%2520Residual%2520UNETR%252B%252B%2520with%2520Attention%2520for%250A%2520%2520Efficient%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DNand%2520Kumar%2520Yadav%2520and%2520Rodrigue%2520Rizk%2520and%2520William%2520CW%2520Chen%2520and%2520KC%2520Santosh%26entry.1292438233%3D%2520%2520Accurate%2520and%2520efficient%2520medical%2520image%2520segmentation%2520is%2520crucial%2520but%2520challenging%250Adue%2520to%2520anatomical%2520variability%2520and%2520high%2520computational%2520demands%2520on%2520volumetric%250Adata.%2520Recent%2520hybrid%2520CNN-Transformer%2520architectures%2520achieve%2520state-of-the-art%250Aresults%2520but%2520add%2520significant%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MLRU%252B%252B%252C%2520a%250AMultiscale%2520Lightweight%2520Residual%2520UNETR%252B%252B%2520architecture%2520designed%2520to%2520balance%250Asegmentation%2520accuracy%2520and%2520computational%2520efficiency.%2520It%2520introduces%2520two%2520key%250Ainnovations%253A%2520a%2520Lightweight%2520Channel%2520and%2520Bottleneck%2520Attention%2520Module%2520%2528LCBAM%2529%2520that%250Aenhances%2520contextual%2520feature%2520encoding%2520with%2520minimal%2520overhead%252C%2520and%2520a%2520Multiscale%250ABottleneck%2520Block%2520%2528M2B%2529%2520in%2520the%2520decoder%2520that%2520captures%2520fine-grained%2520details%2520via%250Amulti-resolution%2520feature%2520aggregation.%2520Experiments%2520on%2520four%2520publicly%2520available%250Abenchmark%2520datasets%2520%2528Synapse%252C%2520BTCV%252C%2520ACDC%252C%2520and%2520Decathlon%2520Lung%2529%2520demonstrate%2520that%250AMLRU%252B%252B%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520average%2520Dice%2520scores%2520of%250A87.57%2525%2520%2528Synapse%2529%252C%252093.00%2525%2520%2528ACDC%2529%252C%2520and%252081.12%2525%2520%2528Lung%2529.%2520Compared%2520to%2520existing%250Aleading%2520models%252C%2520MLRU%252B%252B%2520improves%2520Dice%2520scores%2520by%25205.38%2525%2520and%25202.12%2525%2520on%2520Synapse%2520and%250AACDC%252C%2520respectively%252C%2520while%2520significantly%2520reducing%2520parameter%2520count%2520and%250Acomputational%2520cost.%2520Ablation%2520studies%2520evaluating%2520LCBAM%2520and%2520M2B%2520further%2520confirm%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520architectural%2520components.%2520Results%2520suggest%250Athat%2520MLRU%252B%252B%2520offers%2520a%2520practical%2520and%2520high-performing%2520solution%2520for%25203D%2520medical%250Aimage%2520segmentation%2520tasks.%2520Source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/1027865/MLRUPP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16122v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLRU%2B%2B%3A%20Multiscale%20Lightweight%20Residual%20UNETR%2B%2B%20with%20Attention%20for%0A%20%20Efficient%203D%20Medical%20Image%20Segmentation&entry.906535625=Nand%20Kumar%20Yadav%20and%20Rodrigue%20Rizk%20and%20William%20CW%20Chen%20and%20KC%20Santosh&entry.1292438233=%20%20Accurate%20and%20efficient%20medical%20image%20segmentation%20is%20crucial%20but%20challenging%0Adue%20to%20anatomical%20variability%20and%20high%20computational%20demands%20on%20volumetric%0Adata.%20Recent%20hybrid%20CNN-Transformer%20architectures%20achieve%20state-of-the-art%0Aresults%20but%20add%20significant%20complexity.%20In%20this%20paper%2C%20we%20propose%20MLRU%2B%2B%2C%20a%0AMultiscale%20Lightweight%20Residual%20UNETR%2B%2B%20architecture%20designed%20to%20balance%0Asegmentation%20accuracy%20and%20computational%20efficiency.%20It%20introduces%20two%20key%0Ainnovations%3A%20a%20Lightweight%20Channel%20and%20Bottleneck%20Attention%20Module%20%28LCBAM%29%20that%0Aenhances%20contextual%20feature%20encoding%20with%20minimal%20overhead%2C%20and%20a%20Multiscale%0ABottleneck%20Block%20%28M2B%29%20in%20the%20decoder%20that%20captures%20fine-grained%20details%20via%0Amulti-resolution%20feature%20aggregation.%20Experiments%20on%20four%20publicly%20available%0Abenchmark%20datasets%20%28Synapse%2C%20BTCV%2C%20ACDC%2C%20and%20Decathlon%20Lung%29%20demonstrate%20that%0AMLRU%2B%2B%20achieves%20state-of-the-art%20performance%2C%20with%20average%20Dice%20scores%20of%0A87.57%25%20%28Synapse%29%2C%2093.00%25%20%28ACDC%29%2C%20and%2081.12%25%20%28Lung%29.%20Compared%20to%20existing%0Aleading%20models%2C%20MLRU%2B%2B%20improves%20Dice%20scores%20by%205.38%25%20and%202.12%25%20on%20Synapse%20and%0AACDC%2C%20respectively%2C%20while%20significantly%20reducing%20parameter%20count%20and%0Acomputational%20cost.%20Ablation%20studies%20evaluating%20LCBAM%20and%20M2B%20further%20confirm%0Athe%20effectiveness%20of%20the%20proposed%20architectural%20components.%20Results%20suggest%0Athat%20MLRU%2B%2B%20offers%20a%20practical%20and%20high-performing%20solution%20for%203D%20medical%0Aimage%20segmentation%20tasks.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/1027865/MLRUPP%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16122v3&entry.124074799=Read"},
{"title": "Let It Go? Not Quite: Addressing Item Cold Start in Sequential\n  Recommendations with Content-Based Initialization", "author": "Anton Pembek and Artem Fatkulin and Anton Klenitskiy and Alexey Vasilev", "abstract": "  Many sequential recommender systems suffer from the cold start problem, where\nitems with few or no interactions cannot be effectively used by the model due\nto the absence of a trained embedding. Content-based approaches, which leverage\nitem metadata, are commonly used in such scenarios. One possible way is to use\nembeddings derived from content features such as textual descriptions as\ninitialization for the model embeddings. However, directly using frozen content\nembeddings often results in suboptimal performance, as they may not fully adapt\nto the recommendation task. On the other hand, fine-tuning these embeddings can\ndegrade performance for cold-start items, as item representations may drift far\nfrom their original structure after training. We propose a novel approach to\naddress this limitation. Instead of entirely freezing the content embeddings or\nfine-tuning them extensively, we introduce a small trainable delta to frozen\nembeddings that enables the model to adapt item representations without letting\nthem go too far from their original semantic structure. This approach\ndemonstrates consistent improvements across multiple datasets and modalities,\nincluding e-commerce datasets with textual descriptions and a music dataset\nwith audio-based representation.\n", "link": "http://arxiv.org/abs/2507.19473v1", "date": "2025-07-25", "relevancy": 2.248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20It%20Go%3F%20Not%20Quite%3A%20Addressing%20Item%20Cold%20Start%20in%20Sequential%0A%20%20Recommendations%20with%20Content-Based%20Initialization&body=Title%3A%20Let%20It%20Go%3F%20Not%20Quite%3A%20Addressing%20Item%20Cold%20Start%20in%20Sequential%0A%20%20Recommendations%20with%20Content-Based%20Initialization%0AAuthor%3A%20Anton%20Pembek%20and%20Artem%20Fatkulin%20and%20Anton%20Klenitskiy%20and%20Alexey%20Vasilev%0AAbstract%3A%20%20%20Many%20sequential%20recommender%20systems%20suffer%20from%20the%20cold%20start%20problem%2C%20where%0Aitems%20with%20few%20or%20no%20interactions%20cannot%20be%20effectively%20used%20by%20the%20model%20due%0Ato%20the%20absence%20of%20a%20trained%20embedding.%20Content-based%20approaches%2C%20which%20leverage%0Aitem%20metadata%2C%20are%20commonly%20used%20in%20such%20scenarios.%20One%20possible%20way%20is%20to%20use%0Aembeddings%20derived%20from%20content%20features%20such%20as%20textual%20descriptions%20as%0Ainitialization%20for%20the%20model%20embeddings.%20However%2C%20directly%20using%20frozen%20content%0Aembeddings%20often%20results%20in%20suboptimal%20performance%2C%20as%20they%20may%20not%20fully%20adapt%0Ato%20the%20recommendation%20task.%20On%20the%20other%20hand%2C%20fine-tuning%20these%20embeddings%20can%0Adegrade%20performance%20for%20cold-start%20items%2C%20as%20item%20representations%20may%20drift%20far%0Afrom%20their%20original%20structure%20after%20training.%20We%20propose%20a%20novel%20approach%20to%0Aaddress%20this%20limitation.%20Instead%20of%20entirely%20freezing%20the%20content%20embeddings%20or%0Afine-tuning%20them%20extensively%2C%20we%20introduce%20a%20small%20trainable%20delta%20to%20frozen%0Aembeddings%20that%20enables%20the%20model%20to%20adapt%20item%20representations%20without%20letting%0Athem%20go%20too%20far%20from%20their%20original%20semantic%20structure.%20This%20approach%0Ademonstrates%20consistent%20improvements%20across%20multiple%20datasets%20and%20modalities%2C%0Aincluding%20e-commerce%20datasets%20with%20textual%20descriptions%20and%20a%20music%20dataset%0Awith%20audio-based%20representation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520It%2520Go%253F%2520Not%2520Quite%253A%2520Addressing%2520Item%2520Cold%2520Start%2520in%2520Sequential%250A%2520%2520Recommendations%2520with%2520Content-Based%2520Initialization%26entry.906535625%3DAnton%2520Pembek%2520and%2520Artem%2520Fatkulin%2520and%2520Anton%2520Klenitskiy%2520and%2520Alexey%2520Vasilev%26entry.1292438233%3D%2520%2520Many%2520sequential%2520recommender%2520systems%2520suffer%2520from%2520the%2520cold%2520start%2520problem%252C%2520where%250Aitems%2520with%2520few%2520or%2520no%2520interactions%2520cannot%2520be%2520effectively%2520used%2520by%2520the%2520model%2520due%250Ato%2520the%2520absence%2520of%2520a%2520trained%2520embedding.%2520Content-based%2520approaches%252C%2520which%2520leverage%250Aitem%2520metadata%252C%2520are%2520commonly%2520used%2520in%2520such%2520scenarios.%2520One%2520possible%2520way%2520is%2520to%2520use%250Aembeddings%2520derived%2520from%2520content%2520features%2520such%2520as%2520textual%2520descriptions%2520as%250Ainitialization%2520for%2520the%2520model%2520embeddings.%2520However%252C%2520directly%2520using%2520frozen%2520content%250Aembeddings%2520often%2520results%2520in%2520suboptimal%2520performance%252C%2520as%2520they%2520may%2520not%2520fully%2520adapt%250Ato%2520the%2520recommendation%2520task.%2520On%2520the%2520other%2520hand%252C%2520fine-tuning%2520these%2520embeddings%2520can%250Adegrade%2520performance%2520for%2520cold-start%2520items%252C%2520as%2520item%2520representations%2520may%2520drift%2520far%250Afrom%2520their%2520original%2520structure%2520after%2520training.%2520We%2520propose%2520a%2520novel%2520approach%2520to%250Aaddress%2520this%2520limitation.%2520Instead%2520of%2520entirely%2520freezing%2520the%2520content%2520embeddings%2520or%250Afine-tuning%2520them%2520extensively%252C%2520we%2520introduce%2520a%2520small%2520trainable%2520delta%2520to%2520frozen%250Aembeddings%2520that%2520enables%2520the%2520model%2520to%2520adapt%2520item%2520representations%2520without%2520letting%250Athem%2520go%2520too%2520far%2520from%2520their%2520original%2520semantic%2520structure.%2520This%2520approach%250Ademonstrates%2520consistent%2520improvements%2520across%2520multiple%2520datasets%2520and%2520modalities%252C%250Aincluding%2520e-commerce%2520datasets%2520with%2520textual%2520descriptions%2520and%2520a%2520music%2520dataset%250Awith%2520audio-based%2520representation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20It%20Go%3F%20Not%20Quite%3A%20Addressing%20Item%20Cold%20Start%20in%20Sequential%0A%20%20Recommendations%20with%20Content-Based%20Initialization&entry.906535625=Anton%20Pembek%20and%20Artem%20Fatkulin%20and%20Anton%20Klenitskiy%20and%20Alexey%20Vasilev&entry.1292438233=%20%20Many%20sequential%20recommender%20systems%20suffer%20from%20the%20cold%20start%20problem%2C%20where%0Aitems%20with%20few%20or%20no%20interactions%20cannot%20be%20effectively%20used%20by%20the%20model%20due%0Ato%20the%20absence%20of%20a%20trained%20embedding.%20Content-based%20approaches%2C%20which%20leverage%0Aitem%20metadata%2C%20are%20commonly%20used%20in%20such%20scenarios.%20One%20possible%20way%20is%20to%20use%0Aembeddings%20derived%20from%20content%20features%20such%20as%20textual%20descriptions%20as%0Ainitialization%20for%20the%20model%20embeddings.%20However%2C%20directly%20using%20frozen%20content%0Aembeddings%20often%20results%20in%20suboptimal%20performance%2C%20as%20they%20may%20not%20fully%20adapt%0Ato%20the%20recommendation%20task.%20On%20the%20other%20hand%2C%20fine-tuning%20these%20embeddings%20can%0Adegrade%20performance%20for%20cold-start%20items%2C%20as%20item%20representations%20may%20drift%20far%0Afrom%20their%20original%20structure%20after%20training.%20We%20propose%20a%20novel%20approach%20to%0Aaddress%20this%20limitation.%20Instead%20of%20entirely%20freezing%20the%20content%20embeddings%20or%0Afine-tuning%20them%20extensively%2C%20we%20introduce%20a%20small%20trainable%20delta%20to%20frozen%0Aembeddings%20that%20enables%20the%20model%20to%20adapt%20item%20representations%20without%20letting%0Athem%20go%20too%20far%20from%20their%20original%20semantic%20structure.%20This%20approach%0Ademonstrates%20consistent%20improvements%20across%20multiple%20datasets%20and%20modalities%2C%0Aincluding%20e-commerce%20datasets%20with%20textual%20descriptions%20and%20a%20music%20dataset%0Awith%20audio-based%20representation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19473v1&entry.124074799=Read"},
{"title": "CXR-CML: Improved zero-shot classification of long-tailed multi-label\n  diseases in Chest X-Rays", "author": "Rajesh Madhipati and Sheethal Bhat and Lukas Buess and Andreas Maier", "abstract": "  Chest radiography (CXR) plays a crucial role in the diagnosis of various\ndiseases. However, the inherent class imbalance in the distribution of clinical\nfindings presents a significant challenge for current self-supervised deep\nlearning models. These models often fail to accurately classify long-tailed\nclasses. Current Vision-Language models such as Contrastive Language Image\nPre-training (CLIP) models effectively model the manifold distribution of the\nlatent space, enabling high zero-shot classification accuracies. Although CLIP\nperforms well on most of the primary classes in the dataset, our work reveals\nthat its effectiveness decreases significantly for classes with a long-tailed\ndistribution. Our approach employs a class-weighting mechanism that directly\naligns with the distribution of classes within the latent space. This method\nensures a substantial improvement in overall classification performance, with\nparticular emphasis on enhancing the recognition and accuracy of rarely\nobserved classes. We accomplish this by applying Gaussian Mixture Model (GMM)\nclustering to the latent space. The subsequent clusters are further refined by\nStudent t-distribution, followed by a metric loss that utilizes the altered\nembeddings. Our approach facilitates stable and adaptive clustering of the\nfeatures. This results in a notable average improvement of 7\\% points in\nzero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from\nprevious SOTA models.\n", "link": "http://arxiv.org/abs/2507.19398v1", "date": "2025-07-25", "relevancy": 2.2319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5771}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5594}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CXR-CML%3A%20Improved%20zero-shot%20classification%20of%20long-tailed%20multi-label%0A%20%20diseases%20in%20Chest%20X-Rays&body=Title%3A%20CXR-CML%3A%20Improved%20zero-shot%20classification%20of%20long-tailed%20multi-label%0A%20%20diseases%20in%20Chest%20X-Rays%0AAuthor%3A%20Rajesh%20Madhipati%20and%20Sheethal%20Bhat%20and%20Lukas%20Buess%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Chest%20radiography%20%28CXR%29%20plays%20a%20crucial%20role%20in%20the%20diagnosis%20of%20various%0Adiseases.%20However%2C%20the%20inherent%20class%20imbalance%20in%20the%20distribution%20of%20clinical%0Afindings%20presents%20a%20significant%20challenge%20for%20current%20self-supervised%20deep%0Alearning%20models.%20These%20models%20often%20fail%20to%20accurately%20classify%20long-tailed%0Aclasses.%20Current%20Vision-Language%20models%20such%20as%20Contrastive%20Language%20Image%0APre-training%20%28CLIP%29%20models%20effectively%20model%20the%20manifold%20distribution%20of%20the%0Alatent%20space%2C%20enabling%20high%20zero-shot%20classification%20accuracies.%20Although%20CLIP%0Aperforms%20well%20on%20most%20of%20the%20primary%20classes%20in%20the%20dataset%2C%20our%20work%20reveals%0Athat%20its%20effectiveness%20decreases%20significantly%20for%20classes%20with%20a%20long-tailed%0Adistribution.%20Our%20approach%20employs%20a%20class-weighting%20mechanism%20that%20directly%0Aaligns%20with%20the%20distribution%20of%20classes%20within%20the%20latent%20space.%20This%20method%0Aensures%20a%20substantial%20improvement%20in%20overall%20classification%20performance%2C%20with%0Aparticular%20emphasis%20on%20enhancing%20the%20recognition%20and%20accuracy%20of%20rarely%0Aobserved%20classes.%20We%20accomplish%20this%20by%20applying%20Gaussian%20Mixture%20Model%20%28GMM%29%0Aclustering%20to%20the%20latent%20space.%20The%20subsequent%20clusters%20are%20further%20refined%20by%0AStudent%20t-distribution%2C%20followed%20by%20a%20metric%20loss%20that%20utilizes%20the%20altered%0Aembeddings.%20Our%20approach%20facilitates%20stable%20and%20adaptive%20clustering%20of%20the%0Afeatures.%20This%20results%20in%20a%20notable%20average%20improvement%20of%207%5C%25%20points%20in%0Azero-shot%20AUC%20scores%20across%2040%20classes%20in%20the%20MIMIC-CXR-JPG%20dataset%20from%0Aprevious%20SOTA%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCXR-CML%253A%2520Improved%2520zero-shot%2520classification%2520of%2520long-tailed%2520multi-label%250A%2520%2520diseases%2520in%2520Chest%2520X-Rays%26entry.906535625%3DRajesh%2520Madhipati%2520and%2520Sheethal%2520Bhat%2520and%2520Lukas%2520Buess%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Chest%2520radiography%2520%2528CXR%2529%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520diagnosis%2520of%2520various%250Adiseases.%2520However%252C%2520the%2520inherent%2520class%2520imbalance%2520in%2520the%2520distribution%2520of%2520clinical%250Afindings%2520presents%2520a%2520significant%2520challenge%2520for%2520current%2520self-supervised%2520deep%250Alearning%2520models.%2520These%2520models%2520often%2520fail%2520to%2520accurately%2520classify%2520long-tailed%250Aclasses.%2520Current%2520Vision-Language%2520models%2520such%2520as%2520Contrastive%2520Language%2520Image%250APre-training%2520%2528CLIP%2529%2520models%2520effectively%2520model%2520the%2520manifold%2520distribution%2520of%2520the%250Alatent%2520space%252C%2520enabling%2520high%2520zero-shot%2520classification%2520accuracies.%2520Although%2520CLIP%250Aperforms%2520well%2520on%2520most%2520of%2520the%2520primary%2520classes%2520in%2520the%2520dataset%252C%2520our%2520work%2520reveals%250Athat%2520its%2520effectiveness%2520decreases%2520significantly%2520for%2520classes%2520with%2520a%2520long-tailed%250Adistribution.%2520Our%2520approach%2520employs%2520a%2520class-weighting%2520mechanism%2520that%2520directly%250Aaligns%2520with%2520the%2520distribution%2520of%2520classes%2520within%2520the%2520latent%2520space.%2520This%2520method%250Aensures%2520a%2520substantial%2520improvement%2520in%2520overall%2520classification%2520performance%252C%2520with%250Aparticular%2520emphasis%2520on%2520enhancing%2520the%2520recognition%2520and%2520accuracy%2520of%2520rarely%250Aobserved%2520classes.%2520We%2520accomplish%2520this%2520by%2520applying%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%250Aclustering%2520to%2520the%2520latent%2520space.%2520The%2520subsequent%2520clusters%2520are%2520further%2520refined%2520by%250AStudent%2520t-distribution%252C%2520followed%2520by%2520a%2520metric%2520loss%2520that%2520utilizes%2520the%2520altered%250Aembeddings.%2520Our%2520approach%2520facilitates%2520stable%2520and%2520adaptive%2520clustering%2520of%2520the%250Afeatures.%2520This%2520results%2520in%2520a%2520notable%2520average%2520improvement%2520of%25207%255C%2525%2520points%2520in%250Azero-shot%2520AUC%2520scores%2520across%252040%2520classes%2520in%2520the%2520MIMIC-CXR-JPG%2520dataset%2520from%250Aprevious%2520SOTA%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CXR-CML%3A%20Improved%20zero-shot%20classification%20of%20long-tailed%20multi-label%0A%20%20diseases%20in%20Chest%20X-Rays&entry.906535625=Rajesh%20Madhipati%20and%20Sheethal%20Bhat%20and%20Lukas%20Buess%20and%20Andreas%20Maier&entry.1292438233=%20%20Chest%20radiography%20%28CXR%29%20plays%20a%20crucial%20role%20in%20the%20diagnosis%20of%20various%0Adiseases.%20However%2C%20the%20inherent%20class%20imbalance%20in%20the%20distribution%20of%20clinical%0Afindings%20presents%20a%20significant%20challenge%20for%20current%20self-supervised%20deep%0Alearning%20models.%20These%20models%20often%20fail%20to%20accurately%20classify%20long-tailed%0Aclasses.%20Current%20Vision-Language%20models%20such%20as%20Contrastive%20Language%20Image%0APre-training%20%28CLIP%29%20models%20effectively%20model%20the%20manifold%20distribution%20of%20the%0Alatent%20space%2C%20enabling%20high%20zero-shot%20classification%20accuracies.%20Although%20CLIP%0Aperforms%20well%20on%20most%20of%20the%20primary%20classes%20in%20the%20dataset%2C%20our%20work%20reveals%0Athat%20its%20effectiveness%20decreases%20significantly%20for%20classes%20with%20a%20long-tailed%0Adistribution.%20Our%20approach%20employs%20a%20class-weighting%20mechanism%20that%20directly%0Aaligns%20with%20the%20distribution%20of%20classes%20within%20the%20latent%20space.%20This%20method%0Aensures%20a%20substantial%20improvement%20in%20overall%20classification%20performance%2C%20with%0Aparticular%20emphasis%20on%20enhancing%20the%20recognition%20and%20accuracy%20of%20rarely%0Aobserved%20classes.%20We%20accomplish%20this%20by%20applying%20Gaussian%20Mixture%20Model%20%28GMM%29%0Aclustering%20to%20the%20latent%20space.%20The%20subsequent%20clusters%20are%20further%20refined%20by%0AStudent%20t-distribution%2C%20followed%20by%20a%20metric%20loss%20that%20utilizes%20the%20altered%0Aembeddings.%20Our%20approach%20facilitates%20stable%20and%20adaptive%20clustering%20of%20the%0Afeatures.%20This%20results%20in%20a%20notable%20average%20improvement%20of%207%5C%25%20points%20in%0Azero-shot%20AUC%20scores%20across%2040%20classes%20in%20the%20MIMIC-CXR-JPG%20dataset%20from%0Aprevious%20SOTA%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19398v1&entry.124074799=Read"},
{"title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)", "author": "Semih Eren and Deniz Kucukahmetler and Nico Scherf", "abstract": "  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n", "link": "http://arxiv.org/abs/2507.17897v2", "date": "2025-07-25", "relevancy": 2.2212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&body=Title%3A%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29%0AAuthor%3A%20Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf%0AAbstract%3A%20%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Recurrent%2520Ensembles%2520for%2520Predicting%2520Brain%2520Responses%2520to%250A%2520%2520Naturalistic%2520Movies%2520%2528Algonauts%25202025%2529%26entry.906535625%3DSemih%2520Eren%2520and%2520Deniz%2520Kucukahmetler%2520and%2520Nico%2520Scherf%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520distributed%2520cortical%2520responses%2520to%2520naturalistic%2520stimuli%250Arequires%2520models%2520that%2520integrate%2520visual%252C%2520auditory%2520and%2520semantic%2520information%2520over%250Atime.%2520We%2520present%2520a%2520hierarchical%2520multimodal%2520recurrent%2520ensemble%2520that%2520maps%250Apretrained%2520video%252C%2520audio%252C%2520and%2520language%2520embeddings%2520to%2520fMRI%2520time%2520series%2520recorded%250Awhile%2520four%2520subjects%2520watched%2520almost%252080%2520hours%2520of%2520movies%2520provided%2520by%2520the%2520Algonauts%250A2025%2520challenge.%2520Modality-specific%2520bidirectional%2520RNNs%2520encode%2520temporal%2520dynamics%253B%250Atheir%2520hidden%2520states%2520are%2520fused%2520and%2520passed%2520to%2520a%2520second%2520recurrent%2520layer%252C%2520and%250Alightweight%2520subject-specific%2520heads%2520output%2520responses%2520for%25201000%2520cortical%2520parcels.%250ATraining%2520relies%2520on%2520a%2520composite%2520MSE-correlation%2520loss%2520and%2520a%2520curriculum%2520that%250Agradually%2520shifts%2520emphasis%2520from%2520early%2520sensory%2520to%2520late%2520association%2520regions.%250AAveraging%2520100%2520model%2520variants%2520further%2520boosts%2520robustness.%2520The%2520resulting%2520system%250Aranked%2520third%2520on%2520the%2520competition%2520leaderboard%252C%2520achieving%2520an%2520overall%2520Pearson%2520r%2520%253D%250A0.2094%2520and%2520the%2520highest%2520single-parcel%2520peak%2520score%2520%2528mean%2520r%2520%253D%25200.63%2529%2520among%2520all%250Aparticipants%252C%2520with%2520particularly%2520strong%2520gains%2520for%2520the%2520most%2520challenging%2520subject%250A%2528Subject%25205%2529.%2520The%2520approach%2520establishes%2520a%2520simple%252C%2520extensible%2520baseline%2520for%2520future%250Amultimodal%2520brain-encoding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&entry.906535625=Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf&entry.1292438233=%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17897v2&entry.124074799=Read"},
{"title": "DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image\n  Quality Assessment", "author": "Yiwei Lou and Yuanpeng He and Rongchao Zhang and Yongzhi Cao and Hanpin Wang and Yu Huang", "abstract": "  Blind image quality assessment (BIQA) methods often incorporate auxiliary\ntasks to improve performance. However, existing approaches face limitations due\nto insufficient integration and a lack of flexible uncertainty estimation,\nleading to suboptimal performance. To address these challenges, we propose a\nmultitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which\nperforms multitask optimization with the assistance of scene and distortion\ntype classification tasks. To achieve a more robust and reliable\nrepresentation, we design a novel trustworthy information fusion strategy. It\nfirst combines diverse features and patterns across sub-regions to enhance\ninformation richness, and then performs local-global information fusion by\nbalancing fine-grained details with coarse-grained context. Moreover, DEFNet\nexploits advanced uncertainty estimation technique inspired by evidential\nlearning with the help of normal-inverse gamma distribution mixture. Extensive\nexperiments on both synthetic and authentic distortion datasets demonstrate the\neffectiveness and robustness of the proposed framework. Additional evaluation\nand analysis are carried out to highlight its strong generalization capability\nand adaptability to previously unseen scenarios.\n", "link": "http://arxiv.org/abs/2507.19418v1", "date": "2025-07-25", "relevancy": 2.2021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5811}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5445}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEFNet%3A%20Multitasks-based%20Deep%20Evidential%20Fusion%20Network%20for%20Blind%20Image%0A%20%20Quality%20Assessment&body=Title%3A%20DEFNet%3A%20Multitasks-based%20Deep%20Evidential%20Fusion%20Network%20for%20Blind%20Image%0A%20%20Quality%20Assessment%0AAuthor%3A%20Yiwei%20Lou%20and%20Yuanpeng%20He%20and%20Rongchao%20Zhang%20and%20Yongzhi%20Cao%20and%20Hanpin%20Wang%20and%20Yu%20Huang%0AAbstract%3A%20%20%20Blind%20image%20quality%20assessment%20%28BIQA%29%20methods%20often%20incorporate%20auxiliary%0Atasks%20to%20improve%20performance.%20However%2C%20existing%20approaches%20face%20limitations%20due%0Ato%20insufficient%20integration%20and%20a%20lack%20of%20flexible%20uncertainty%20estimation%2C%0Aleading%20to%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Amultitasks-based%20Deep%20Evidential%20Fusion%20Network%20%28DEFNet%29%20for%20BIQA%2C%20which%0Aperforms%20multitask%20optimization%20with%20the%20assistance%20of%20scene%20and%20distortion%0Atype%20classification%20tasks.%20To%20achieve%20a%20more%20robust%20and%20reliable%0Arepresentation%2C%20we%20design%20a%20novel%20trustworthy%20information%20fusion%20strategy.%20It%0Afirst%20combines%20diverse%20features%20and%20patterns%20across%20sub-regions%20to%20enhance%0Ainformation%20richness%2C%20and%20then%20performs%20local-global%20information%20fusion%20by%0Abalancing%20fine-grained%20details%20with%20coarse-grained%20context.%20Moreover%2C%20DEFNet%0Aexploits%20advanced%20uncertainty%20estimation%20technique%20inspired%20by%20evidential%0Alearning%20with%20the%20help%20of%20normal-inverse%20gamma%20distribution%20mixture.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20authentic%20distortion%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20framework.%20Additional%20evaluation%0Aand%20analysis%20are%20carried%20out%20to%20highlight%20its%20strong%20generalization%20capability%0Aand%20adaptability%20to%20previously%20unseen%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEFNet%253A%2520Multitasks-based%2520Deep%2520Evidential%2520Fusion%2520Network%2520for%2520Blind%2520Image%250A%2520%2520Quality%2520Assessment%26entry.906535625%3DYiwei%2520Lou%2520and%2520Yuanpeng%2520He%2520and%2520Rongchao%2520Zhang%2520and%2520Yongzhi%2520Cao%2520and%2520Hanpin%2520Wang%2520and%2520Yu%2520Huang%26entry.1292438233%3D%2520%2520Blind%2520image%2520quality%2520assessment%2520%2528BIQA%2529%2520methods%2520often%2520incorporate%2520auxiliary%250Atasks%2520to%2520improve%2520performance.%2520However%252C%2520existing%2520approaches%2520face%2520limitations%2520due%250Ato%2520insufficient%2520integration%2520and%2520a%2520lack%2520of%2520flexible%2520uncertainty%2520estimation%252C%250Aleading%2520to%2520suboptimal%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Amultitasks-based%2520Deep%2520Evidential%2520Fusion%2520Network%2520%2528DEFNet%2529%2520for%2520BIQA%252C%2520which%250Aperforms%2520multitask%2520optimization%2520with%2520the%2520assistance%2520of%2520scene%2520and%2520distortion%250Atype%2520classification%2520tasks.%2520To%2520achieve%2520a%2520more%2520robust%2520and%2520reliable%250Arepresentation%252C%2520we%2520design%2520a%2520novel%2520trustworthy%2520information%2520fusion%2520strategy.%2520It%250Afirst%2520combines%2520diverse%2520features%2520and%2520patterns%2520across%2520sub-regions%2520to%2520enhance%250Ainformation%2520richness%252C%2520and%2520then%2520performs%2520local-global%2520information%2520fusion%2520by%250Abalancing%2520fine-grained%2520details%2520with%2520coarse-grained%2520context.%2520Moreover%252C%2520DEFNet%250Aexploits%2520advanced%2520uncertainty%2520estimation%2520technique%2520inspired%2520by%2520evidential%250Alearning%2520with%2520the%2520help%2520of%2520normal-inverse%2520gamma%2520distribution%2520mixture.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520authentic%2520distortion%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520framework.%2520Additional%2520evaluation%250Aand%2520analysis%2520are%2520carried%2520out%2520to%2520highlight%2520its%2520strong%2520generalization%2520capability%250Aand%2520adaptability%2520to%2520previously%2520unseen%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEFNet%3A%20Multitasks-based%20Deep%20Evidential%20Fusion%20Network%20for%20Blind%20Image%0A%20%20Quality%20Assessment&entry.906535625=Yiwei%20Lou%20and%20Yuanpeng%20He%20and%20Rongchao%20Zhang%20and%20Yongzhi%20Cao%20and%20Hanpin%20Wang%20and%20Yu%20Huang&entry.1292438233=%20%20Blind%20image%20quality%20assessment%20%28BIQA%29%20methods%20often%20incorporate%20auxiliary%0Atasks%20to%20improve%20performance.%20However%2C%20existing%20approaches%20face%20limitations%20due%0Ato%20insufficient%20integration%20and%20a%20lack%20of%20flexible%20uncertainty%20estimation%2C%0Aleading%20to%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Amultitasks-based%20Deep%20Evidential%20Fusion%20Network%20%28DEFNet%29%20for%20BIQA%2C%20which%0Aperforms%20multitask%20optimization%20with%20the%20assistance%20of%20scene%20and%20distortion%0Atype%20classification%20tasks.%20To%20achieve%20a%20more%20robust%20and%20reliable%0Arepresentation%2C%20we%20design%20a%20novel%20trustworthy%20information%20fusion%20strategy.%20It%0Afirst%20combines%20diverse%20features%20and%20patterns%20across%20sub-regions%20to%20enhance%0Ainformation%20richness%2C%20and%20then%20performs%20local-global%20information%20fusion%20by%0Abalancing%20fine-grained%20details%20with%20coarse-grained%20context.%20Moreover%2C%20DEFNet%0Aexploits%20advanced%20uncertainty%20estimation%20technique%20inspired%20by%20evidential%0Alearning%20with%20the%20help%20of%20normal-inverse%20gamma%20distribution%20mixture.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20authentic%20distortion%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20framework.%20Additional%20evaluation%0Aand%20analysis%20are%20carried%20out%20to%20highlight%20its%20strong%20generalization%20capability%0Aand%20adaptability%20to%20previously%20unseen%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19418v1&entry.124074799=Read"},
{"title": "Integrating Physics and Topology in Neural Networks for Learning Rigid\n  Body Dynamics", "author": "Amaury Wei and Olga Fink", "abstract": "  Rigid body interactions are fundamental to numerous scientific disciplines,\nbut remain challenging to simulate due to their abrupt nonlinear nature and\nsensitivity to complex, often unknown environmental factors. These challenges\ncall for adaptable learning-based methods capable of capturing complex\ninteractions beyond explicit physical models and simulations. While graph\nneural networks can handle simple scenarios, they struggle with complex scenes\nand long-term predictions. We introduce a novel framework for modeling rigid\nbody dynamics and learning collision interactions, addressing key limitations\nof existing graph-based methods. Our approach extends the traditional\nrepresentation of meshes by incorporating higher-order topology complexes,\noffering a physically consistent representation. Additionally, we propose a\nphysics-informed message-passing neural architecture, embedding physical laws\ndirectly in the model. Our method demonstrates superior accuracy, even during\nlong rollouts, and exhibits strong generalization to unseen scenarios.\nImportantly, this work addresses the challenge of multi-entity dynamic\ninteractions, with applications spanning diverse scientific and engineering\ndomains.\n", "link": "http://arxiv.org/abs/2411.11467v3", "date": "2025-07-25", "relevancy": 2.1864, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5491}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics&body=Title%3A%20Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics%0AAuthor%3A%20Amaury%20Wei%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11467v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Physics%2520and%2520Topology%2520in%2520Neural%2520Networks%2520for%2520Learning%2520Rigid%250A%2520%2520Body%2520Dynamics%26entry.906535625%3DAmaury%2520Wei%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Rigid%2520body%2520interactions%2520are%2520fundamental%2520to%2520numerous%2520scientific%2520disciplines%252C%250Abut%2520remain%2520challenging%2520to%2520simulate%2520due%2520to%2520their%2520abrupt%2520nonlinear%2520nature%2520and%250Asensitivity%2520to%2520complex%252C%2520often%2520unknown%2520environmental%2520factors.%2520These%2520challenges%250Acall%2520for%2520adaptable%2520learning-based%2520methods%2520capable%2520of%2520capturing%2520complex%250Ainteractions%2520beyond%2520explicit%2520physical%2520models%2520and%2520simulations.%2520While%2520graph%250Aneural%2520networks%2520can%2520handle%2520simple%2520scenarios%252C%2520they%2520struggle%2520with%2520complex%2520scenes%250Aand%2520long-term%2520predictions.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520modeling%2520rigid%250Abody%2520dynamics%2520and%2520learning%2520collision%2520interactions%252C%2520addressing%2520key%2520limitations%250Aof%2520existing%2520graph-based%2520methods.%2520Our%2520approach%2520extends%2520the%2520traditional%250Arepresentation%2520of%2520meshes%2520by%2520incorporating%2520higher-order%2520topology%2520complexes%252C%250Aoffering%2520a%2520physically%2520consistent%2520representation.%2520Additionally%252C%2520we%2520propose%2520a%250Aphysics-informed%2520message-passing%2520neural%2520architecture%252C%2520embedding%2520physical%2520laws%250Adirectly%2520in%2520the%2520model.%2520Our%2520method%2520demonstrates%2520superior%2520accuracy%252C%2520even%2520during%250Along%2520rollouts%252C%2520and%2520exhibits%2520strong%2520generalization%2520to%2520unseen%2520scenarios.%250AImportantly%252C%2520this%2520work%2520addresses%2520the%2520challenge%2520of%2520multi-entity%2520dynamic%250Ainteractions%252C%2520with%2520applications%2520spanning%2520diverse%2520scientific%2520and%2520engineering%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11467v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Physics%20and%20Topology%20in%20Neural%20Networks%20for%20Learning%20Rigid%0A%20%20Body%20Dynamics&entry.906535625=Amaury%20Wei%20and%20Olga%20Fink&entry.1292438233=%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11467v3&entry.124074799=Read"},
{"title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "author": "Mina Huh and Zihui Xue and Ujjaini Das and Kumar Ashutosh and Kristen Grauman and Amy Pavel", "abstract": "  People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise.\n", "link": "http://arxiv.org/abs/2506.00717v2", "date": "2025-07-25", "relevancy": 2.1568, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5705}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5359}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vid2Coach%3A%20Transforming%20How-To%20Videos%20into%20Task%20Assistants&body=Title%3A%20Vid2Coach%3A%20Transforming%20How-To%20Videos%20into%20Task%20Assistants%0AAuthor%3A%20Mina%20Huh%20and%20Zihui%20Xue%20and%20Ujjaini%20Das%20and%20Kumar%20Ashutosh%20and%20Kristen%20Grauman%20and%20Amy%20Pavel%0AAbstract%3A%20%20%20People%20use%20videos%20to%20learn%20new%20recipes%2C%20exercises%2C%20and%20crafts.%20Such%20videos%0Aremain%20difficult%20for%20blind%20and%20low%20vision%20%28BLV%29%20people%20to%20follow%20as%20they%20rely%0Aon%20visual%20comparison.%20Our%20observations%20of%20visual%20rehabilitation%20therapists%0A%28VRTs%29%20guiding%20BLV%20people%20to%20follow%20how-to%20videos%20revealed%20that%20VRTs%20provide%0Aboth%20proactive%20and%20responsive%20support%20including%20detailed%20descriptions%2C%0Anon-visual%20workarounds%2C%20and%20progress%20feedback.%20We%20propose%20Vid2Coach%2C%20a%20system%0Athat%20transforms%20how-to%20videos%20into%20wearable%20camera-based%20assistants%20that%0Aprovide%20accessible%20instructions%20and%20mixed-initiative%20feedback.%20From%20the%20video%2C%0AVid2Coach%20generates%20accessible%20instructions%20by%20augmenting%20narrated%20instructions%0Awith%20demonstration%20details%20and%20completion%20criteria%20for%20each%20step.%20It%20then%20uses%0Aretrieval-augmented-generation%20to%20extract%20relevant%20non-visual%20workarounds%20from%0ABLV-specific%20resources.%20Vid2Coach%20then%20monitors%20user%20progress%20with%20a%20camera%0Aembedded%20in%20commercial%20smart%20glasses%20to%20provide%20context-aware%20instructions%2C%0Aproactive%20feedback%2C%20and%20answers%20to%20user%20questions.%20BLV%20participants%20%28N%3D8%29%20using%0AVid2Coach%20completed%20cooking%20tasks%20with%2058.5%5C%25%20fewer%20errors%20than%20when%20using%0Atheir%20typical%20workflow%20and%20wanted%20to%20use%20Vid2Coach%20in%20their%20daily%20lives.%0AVid2Coach%20demonstrates%20an%20opportunity%20for%20AI%20visual%20assistance%20that%20strengthens%0Arather%20than%20replaces%20non-visual%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVid2Coach%253A%2520Transforming%2520How-To%2520Videos%2520into%2520Task%2520Assistants%26entry.906535625%3DMina%2520Huh%2520and%2520Zihui%2520Xue%2520and%2520Ujjaini%2520Das%2520and%2520Kumar%2520Ashutosh%2520and%2520Kristen%2520Grauman%2520and%2520Amy%2520Pavel%26entry.1292438233%3D%2520%2520People%2520use%2520videos%2520to%2520learn%2520new%2520recipes%252C%2520exercises%252C%2520and%2520crafts.%2520Such%2520videos%250Aremain%2520difficult%2520for%2520blind%2520and%2520low%2520vision%2520%2528BLV%2529%2520people%2520to%2520follow%2520as%2520they%2520rely%250Aon%2520visual%2520comparison.%2520Our%2520observations%2520of%2520visual%2520rehabilitation%2520therapists%250A%2528VRTs%2529%2520guiding%2520BLV%2520people%2520to%2520follow%2520how-to%2520videos%2520revealed%2520that%2520VRTs%2520provide%250Aboth%2520proactive%2520and%2520responsive%2520support%2520including%2520detailed%2520descriptions%252C%250Anon-visual%2520workarounds%252C%2520and%2520progress%2520feedback.%2520We%2520propose%2520Vid2Coach%252C%2520a%2520system%250Athat%2520transforms%2520how-to%2520videos%2520into%2520wearable%2520camera-based%2520assistants%2520that%250Aprovide%2520accessible%2520instructions%2520and%2520mixed-initiative%2520feedback.%2520From%2520the%2520video%252C%250AVid2Coach%2520generates%2520accessible%2520instructions%2520by%2520augmenting%2520narrated%2520instructions%250Awith%2520demonstration%2520details%2520and%2520completion%2520criteria%2520for%2520each%2520step.%2520It%2520then%2520uses%250Aretrieval-augmented-generation%2520to%2520extract%2520relevant%2520non-visual%2520workarounds%2520from%250ABLV-specific%2520resources.%2520Vid2Coach%2520then%2520monitors%2520user%2520progress%2520with%2520a%2520camera%250Aembedded%2520in%2520commercial%2520smart%2520glasses%2520to%2520provide%2520context-aware%2520instructions%252C%250Aproactive%2520feedback%252C%2520and%2520answers%2520to%2520user%2520questions.%2520BLV%2520participants%2520%2528N%253D8%2529%2520using%250AVid2Coach%2520completed%2520cooking%2520tasks%2520with%252058.5%255C%2525%2520fewer%2520errors%2520than%2520when%2520using%250Atheir%2520typical%2520workflow%2520and%2520wanted%2520to%2520use%2520Vid2Coach%2520in%2520their%2520daily%2520lives.%250AVid2Coach%2520demonstrates%2520an%2520opportunity%2520for%2520AI%2520visual%2520assistance%2520that%2520strengthens%250Arather%2520than%2520replaces%2520non-visual%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vid2Coach%3A%20Transforming%20How-To%20Videos%20into%20Task%20Assistants&entry.906535625=Mina%20Huh%20and%20Zihui%20Xue%20and%20Ujjaini%20Das%20and%20Kumar%20Ashutosh%20and%20Kristen%20Grauman%20and%20Amy%20Pavel&entry.1292438233=%20%20People%20use%20videos%20to%20learn%20new%20recipes%2C%20exercises%2C%20and%20crafts.%20Such%20videos%0Aremain%20difficult%20for%20blind%20and%20low%20vision%20%28BLV%29%20people%20to%20follow%20as%20they%20rely%0Aon%20visual%20comparison.%20Our%20observations%20of%20visual%20rehabilitation%20therapists%0A%28VRTs%29%20guiding%20BLV%20people%20to%20follow%20how-to%20videos%20revealed%20that%20VRTs%20provide%0Aboth%20proactive%20and%20responsive%20support%20including%20detailed%20descriptions%2C%0Anon-visual%20workarounds%2C%20and%20progress%20feedback.%20We%20propose%20Vid2Coach%2C%20a%20system%0Athat%20transforms%20how-to%20videos%20into%20wearable%20camera-based%20assistants%20that%0Aprovide%20accessible%20instructions%20and%20mixed-initiative%20feedback.%20From%20the%20video%2C%0AVid2Coach%20generates%20accessible%20instructions%20by%20augmenting%20narrated%20instructions%0Awith%20demonstration%20details%20and%20completion%20criteria%20for%20each%20step.%20It%20then%20uses%0Aretrieval-augmented-generation%20to%20extract%20relevant%20non-visual%20workarounds%20from%0ABLV-specific%20resources.%20Vid2Coach%20then%20monitors%20user%20progress%20with%20a%20camera%0Aembedded%20in%20commercial%20smart%20glasses%20to%20provide%20context-aware%20instructions%2C%0Aproactive%20feedback%2C%20and%20answers%20to%20user%20questions.%20BLV%20participants%20%28N%3D8%29%20using%0AVid2Coach%20completed%20cooking%20tasks%20with%2058.5%5C%25%20fewer%20errors%20than%20when%20using%0Atheir%20typical%20workflow%20and%20wanted%20to%20use%20Vid2Coach%20in%20their%20daily%20lives.%0AVid2Coach%20demonstrates%20an%20opportunity%20for%20AI%20visual%20assistance%20that%20strengthens%0Arather%20than%20replaces%20non-visual%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00717v2&entry.124074799=Read"},
{"title": "Forest-Guided Clustering -- Shedding Light into the Random Forest Black\n  Box", "author": "Lisa Barros de Andrade e Sousa and Gregor Miller and Ronan Le Gleut and Dominik Thalmeier and Helena Pelin and Marie Piraud", "abstract": "  As machine learning models are increasingly deployed in sensitive application\nareas, the demand for interpretable and trustworthy decision-making has\nincreased. Random Forests (RF), despite their widespread use and strong\nperformance on tabular data, remain difficult to interpret due to their\nensemble nature. We present Forest-Guided Clustering (FGC), a model-specific\nexplainability method that reveals both local and global structure in RFs by\ngrouping instances according to shared decision paths. FGC produces\nhuman-interpretable clusters aligned with the model's internal logic and\ncomputes cluster-specific and global feature importance scores to derive\ndecision rules underlying RF predictions. FGC accurately recovered latent\nsubclass structure on a benchmark dataset and outperformed classical clustering\nand post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC\nuncovered biologically coherent subpopulations, disentangled disease-relevant\nsignals from confounders, and recovered known and novel gene expression\npatterns. FGC bridges the gap between performance and interpretability by\nproviding structure-aware insights that go beyond feature-level attribution.\n", "link": "http://arxiv.org/abs/2507.19455v1", "date": "2025-07-25", "relevancy": 2.1522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forest-Guided%20Clustering%20--%20Shedding%20Light%20into%20the%20Random%20Forest%20Black%0A%20%20Box&body=Title%3A%20Forest-Guided%20Clustering%20--%20Shedding%20Light%20into%20the%20Random%20Forest%20Black%0A%20%20Box%0AAuthor%3A%20Lisa%20Barros%20de%20Andrade%20e%20Sousa%20and%20Gregor%20Miller%20and%20Ronan%20Le%20Gleut%20and%20Dominik%20Thalmeier%20and%20Helena%20Pelin%20and%20Marie%20Piraud%0AAbstract%3A%20%20%20As%20machine%20learning%20models%20are%20increasingly%20deployed%20in%20sensitive%20application%0Aareas%2C%20the%20demand%20for%20interpretable%20and%20trustworthy%20decision-making%20has%0Aincreased.%20Random%20Forests%20%28RF%29%2C%20despite%20their%20widespread%20use%20and%20strong%0Aperformance%20on%20tabular%20data%2C%20remain%20difficult%20to%20interpret%20due%20to%20their%0Aensemble%20nature.%20We%20present%20Forest-Guided%20Clustering%20%28FGC%29%2C%20a%20model-specific%0Aexplainability%20method%20that%20reveals%20both%20local%20and%20global%20structure%20in%20RFs%20by%0Agrouping%20instances%20according%20to%20shared%20decision%20paths.%20FGC%20produces%0Ahuman-interpretable%20clusters%20aligned%20with%20the%20model%27s%20internal%20logic%20and%0Acomputes%20cluster-specific%20and%20global%20feature%20importance%20scores%20to%20derive%0Adecision%20rules%20underlying%20RF%20predictions.%20FGC%20accurately%20recovered%20latent%0Asubclass%20structure%20on%20a%20benchmark%20dataset%20and%20outperformed%20classical%20clustering%0Aand%20post-hoc%20explanation%20methods.%20Applied%20to%20an%20AML%20transcriptomic%20dataset%2C%20FGC%0Auncovered%20biologically%20coherent%20subpopulations%2C%20disentangled%20disease-relevant%0Asignals%20from%20confounders%2C%20and%20recovered%20known%20and%20novel%20gene%20expression%0Apatterns.%20FGC%20bridges%20the%20gap%20between%20performance%20and%20interpretability%20by%0Aproviding%20structure-aware%20insights%20that%20go%20beyond%20feature-level%20attribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForest-Guided%2520Clustering%2520--%2520Shedding%2520Light%2520into%2520the%2520Random%2520Forest%2520Black%250A%2520%2520Box%26entry.906535625%3DLisa%2520Barros%2520de%2520Andrade%2520e%2520Sousa%2520and%2520Gregor%2520Miller%2520and%2520Ronan%2520Le%2520Gleut%2520and%2520Dominik%2520Thalmeier%2520and%2520Helena%2520Pelin%2520and%2520Marie%2520Piraud%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520models%2520are%2520increasingly%2520deployed%2520in%2520sensitive%2520application%250Aareas%252C%2520the%2520demand%2520for%2520interpretable%2520and%2520trustworthy%2520decision-making%2520has%250Aincreased.%2520Random%2520Forests%2520%2528RF%2529%252C%2520despite%2520their%2520widespread%2520use%2520and%2520strong%250Aperformance%2520on%2520tabular%2520data%252C%2520remain%2520difficult%2520to%2520interpret%2520due%2520to%2520their%250Aensemble%2520nature.%2520We%2520present%2520Forest-Guided%2520Clustering%2520%2528FGC%2529%252C%2520a%2520model-specific%250Aexplainability%2520method%2520that%2520reveals%2520both%2520local%2520and%2520global%2520structure%2520in%2520RFs%2520by%250Agrouping%2520instances%2520according%2520to%2520shared%2520decision%2520paths.%2520FGC%2520produces%250Ahuman-interpretable%2520clusters%2520aligned%2520with%2520the%2520model%2527s%2520internal%2520logic%2520and%250Acomputes%2520cluster-specific%2520and%2520global%2520feature%2520importance%2520scores%2520to%2520derive%250Adecision%2520rules%2520underlying%2520RF%2520predictions.%2520FGC%2520accurately%2520recovered%2520latent%250Asubclass%2520structure%2520on%2520a%2520benchmark%2520dataset%2520and%2520outperformed%2520classical%2520clustering%250Aand%2520post-hoc%2520explanation%2520methods.%2520Applied%2520to%2520an%2520AML%2520transcriptomic%2520dataset%252C%2520FGC%250Auncovered%2520biologically%2520coherent%2520subpopulations%252C%2520disentangled%2520disease-relevant%250Asignals%2520from%2520confounders%252C%2520and%2520recovered%2520known%2520and%2520novel%2520gene%2520expression%250Apatterns.%2520FGC%2520bridges%2520the%2520gap%2520between%2520performance%2520and%2520interpretability%2520by%250Aproviding%2520structure-aware%2520insights%2520that%2520go%2520beyond%2520feature-level%2520attribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forest-Guided%20Clustering%20--%20Shedding%20Light%20into%20the%20Random%20Forest%20Black%0A%20%20Box&entry.906535625=Lisa%20Barros%20de%20Andrade%20e%20Sousa%20and%20Gregor%20Miller%20and%20Ronan%20Le%20Gleut%20and%20Dominik%20Thalmeier%20and%20Helena%20Pelin%20and%20Marie%20Piraud&entry.1292438233=%20%20As%20machine%20learning%20models%20are%20increasingly%20deployed%20in%20sensitive%20application%0Aareas%2C%20the%20demand%20for%20interpretable%20and%20trustworthy%20decision-making%20has%0Aincreased.%20Random%20Forests%20%28RF%29%2C%20despite%20their%20widespread%20use%20and%20strong%0Aperformance%20on%20tabular%20data%2C%20remain%20difficult%20to%20interpret%20due%20to%20their%0Aensemble%20nature.%20We%20present%20Forest-Guided%20Clustering%20%28FGC%29%2C%20a%20model-specific%0Aexplainability%20method%20that%20reveals%20both%20local%20and%20global%20structure%20in%20RFs%20by%0Agrouping%20instances%20according%20to%20shared%20decision%20paths.%20FGC%20produces%0Ahuman-interpretable%20clusters%20aligned%20with%20the%20model%27s%20internal%20logic%20and%0Acomputes%20cluster-specific%20and%20global%20feature%20importance%20scores%20to%20derive%0Adecision%20rules%20underlying%20RF%20predictions.%20FGC%20accurately%20recovered%20latent%0Asubclass%20structure%20on%20a%20benchmark%20dataset%20and%20outperformed%20classical%20clustering%0Aand%20post-hoc%20explanation%20methods.%20Applied%20to%20an%20AML%20transcriptomic%20dataset%2C%20FGC%0Auncovered%20biologically%20coherent%20subpopulations%2C%20disentangled%20disease-relevant%0Asignals%20from%20confounders%2C%20and%20recovered%20known%20and%20novel%20gene%20expression%0Apatterns.%20FGC%20bridges%20the%20gap%20between%20performance%20and%20interpretability%20by%0Aproviding%20structure-aware%20insights%20that%20go%20beyond%20feature-level%20attribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19455v1&entry.124074799=Read"},
{"title": "Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning", "author": "Geri Skenderi and Luigi Capogrosso and Andrea Toaiari and Matteo Denitto and Franco Fummi and Simone Melzi", "abstract": "  Auxiliary tasks facilitate learning in situations where data is scarce or the\nprincipal task of interest is extremely complex. This idea is primarily\ninspired by the improved generalization capability induced by solving multiple\ntasks simultaneously, which leads to a more robust shared representation.\nNevertheless, finding optimal auxiliary tasks is a crucial problem that often\nrequires hand-crafted solutions or expensive meta-learning approaches. In this\npaper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised\ndisentanglement procedure is used to discover a new unrelated auxiliary\nclassification task, which allows us to go from a Single-Task Learning (STL) to\na Multi-Task Learning (MTL) problem. The disentanglement procedure works at the\nrepresentation level, isolating the variation related to the principal task\ninto an isolated subspace and additionally producing an arbitrary number of\northogonal subspaces, each of which encourages high separability among\nprojections. We generate the auxiliary classification task through a clustering\nprocedure on the most disentangled subspace, obtaining a discrete set of\nlabels. Subsequently, the original data, the labels associated with the\nprincipal task, and the newly discovered ones can be fed into any MTL\nframework. Experimental validation on both synthetic and real data, along with\nvarious ablation studies, demonstrates promising results, revealing the\npotential in what has been, so far, an unexplored connection between learning\ndisentangled representations and MTL. The source code is available at\nhttps://github.com/intelligolabs/Detaux.\n", "link": "http://arxiv.org/abs/2310.09278v3", "date": "2025-07-25", "relevancy": 2.142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning&body=Title%3A%20Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning%0AAuthor%3A%20Geri%20Skenderi%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Matteo%20Denitto%20and%20Franco%20Fummi%20and%20Simone%20Melzi%0AAbstract%3A%20%20%20Auxiliary%20tasks%20facilitate%20learning%20in%20situations%20where%20data%20is%20scarce%20or%20the%0Aprincipal%20task%20of%20interest%20is%20extremely%20complex.%20This%20idea%20is%20primarily%0Ainspired%20by%20the%20improved%20generalization%20capability%20induced%20by%20solving%20multiple%0Atasks%20simultaneously%2C%20which%20leads%20to%20a%20more%20robust%20shared%20representation.%0ANevertheless%2C%20finding%20optimal%20auxiliary%20tasks%20is%20a%20crucial%20problem%20that%20often%0Arequires%20hand-crafted%20solutions%20or%20expensive%20meta-learning%20approaches.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%2C%20dubbed%20Detaux%2C%20whereby%20a%20weakly%20supervised%0Adisentanglement%20procedure%20is%20used%20to%20discover%20a%20new%20unrelated%20auxiliary%0Aclassification%20task%2C%20which%20allows%20us%20to%20go%20from%20a%20Single-Task%20Learning%20%28STL%29%20to%0Aa%20Multi-Task%20Learning%20%28MTL%29%20problem.%20The%20disentanglement%20procedure%20works%20at%20the%0Arepresentation%20level%2C%20isolating%20the%20variation%20related%20to%20the%20principal%20task%0Ainto%20an%20isolated%20subspace%20and%20additionally%20producing%20an%20arbitrary%20number%20of%0Aorthogonal%20subspaces%2C%20each%20of%20which%20encourages%20high%20separability%20among%0Aprojections.%20We%20generate%20the%20auxiliary%20classification%20task%20through%20a%20clustering%0Aprocedure%20on%20the%20most%20disentangled%20subspace%2C%20obtaining%20a%20discrete%20set%20of%0Alabels.%20Subsequently%2C%20the%20original%20data%2C%20the%20labels%20associated%20with%20the%0Aprincipal%20task%2C%20and%20the%20newly%20discovered%20ones%20can%20be%20fed%20into%20any%20MTL%0Aframework.%20Experimental%20validation%20on%20both%20synthetic%20and%20real%20data%2C%20along%20with%0Avarious%20ablation%20studies%2C%20demonstrates%20promising%20results%2C%20revealing%20the%0Apotential%20in%20what%20has%20been%2C%20so%20far%2C%20an%20unexplored%20connection%20between%20learning%0Adisentangled%20representations%20and%20MTL.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/Detaux.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09278v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Latent%2520Spaces%2520Facilitate%2520Data-Driven%2520Auxiliary%2520Learning%26entry.906535625%3DGeri%2520Skenderi%2520and%2520Luigi%2520Capogrosso%2520and%2520Andrea%2520Toaiari%2520and%2520Matteo%2520Denitto%2520and%2520Franco%2520Fummi%2520and%2520Simone%2520Melzi%26entry.1292438233%3D%2520%2520Auxiliary%2520tasks%2520facilitate%2520learning%2520in%2520situations%2520where%2520data%2520is%2520scarce%2520or%2520the%250Aprincipal%2520task%2520of%2520interest%2520is%2520extremely%2520complex.%2520This%2520idea%2520is%2520primarily%250Ainspired%2520by%2520the%2520improved%2520generalization%2520capability%2520induced%2520by%2520solving%2520multiple%250Atasks%2520simultaneously%252C%2520which%2520leads%2520to%2520a%2520more%2520robust%2520shared%2520representation.%250ANevertheless%252C%2520finding%2520optimal%2520auxiliary%2520tasks%2520is%2520a%2520crucial%2520problem%2520that%2520often%250Arequires%2520hand-crafted%2520solutions%2520or%2520expensive%2520meta-learning%2520approaches.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520dubbed%2520Detaux%252C%2520whereby%2520a%2520weakly%2520supervised%250Adisentanglement%2520procedure%2520is%2520used%2520to%2520discover%2520a%2520new%2520unrelated%2520auxiliary%250Aclassification%2520task%252C%2520which%2520allows%2520us%2520to%2520go%2520from%2520a%2520Single-Task%2520Learning%2520%2528STL%2529%2520to%250Aa%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520problem.%2520The%2520disentanglement%2520procedure%2520works%2520at%2520the%250Arepresentation%2520level%252C%2520isolating%2520the%2520variation%2520related%2520to%2520the%2520principal%2520task%250Ainto%2520an%2520isolated%2520subspace%2520and%2520additionally%2520producing%2520an%2520arbitrary%2520number%2520of%250Aorthogonal%2520subspaces%252C%2520each%2520of%2520which%2520encourages%2520high%2520separability%2520among%250Aprojections.%2520We%2520generate%2520the%2520auxiliary%2520classification%2520task%2520through%2520a%2520clustering%250Aprocedure%2520on%2520the%2520most%2520disentangled%2520subspace%252C%2520obtaining%2520a%2520discrete%2520set%2520of%250Alabels.%2520Subsequently%252C%2520the%2520original%2520data%252C%2520the%2520labels%2520associated%2520with%2520the%250Aprincipal%2520task%252C%2520and%2520the%2520newly%2520discovered%2520ones%2520can%2520be%2520fed%2520into%2520any%2520MTL%250Aframework.%2520Experimental%2520validation%2520on%2520both%2520synthetic%2520and%2520real%2520data%252C%2520along%2520with%250Avarious%2520ablation%2520studies%252C%2520demonstrates%2520promising%2520results%252C%2520revealing%2520the%250Apotential%2520in%2520what%2520has%2520been%252C%2520so%2520far%252C%2520an%2520unexplored%2520connection%2520between%2520learning%250Adisentangled%2520representations%2520and%2520MTL.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/intelligolabs/Detaux.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09278v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning&entry.906535625=Geri%20Skenderi%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Matteo%20Denitto%20and%20Franco%20Fummi%20and%20Simone%20Melzi&entry.1292438233=%20%20Auxiliary%20tasks%20facilitate%20learning%20in%20situations%20where%20data%20is%20scarce%20or%20the%0Aprincipal%20task%20of%20interest%20is%20extremely%20complex.%20This%20idea%20is%20primarily%0Ainspired%20by%20the%20improved%20generalization%20capability%20induced%20by%20solving%20multiple%0Atasks%20simultaneously%2C%20which%20leads%20to%20a%20more%20robust%20shared%20representation.%0ANevertheless%2C%20finding%20optimal%20auxiliary%20tasks%20is%20a%20crucial%20problem%20that%20often%0Arequires%20hand-crafted%20solutions%20or%20expensive%20meta-learning%20approaches.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%2C%20dubbed%20Detaux%2C%20whereby%20a%20weakly%20supervised%0Adisentanglement%20procedure%20is%20used%20to%20discover%20a%20new%20unrelated%20auxiliary%0Aclassification%20task%2C%20which%20allows%20us%20to%20go%20from%20a%20Single-Task%20Learning%20%28STL%29%20to%0Aa%20Multi-Task%20Learning%20%28MTL%29%20problem.%20The%20disentanglement%20procedure%20works%20at%20the%0Arepresentation%20level%2C%20isolating%20the%20variation%20related%20to%20the%20principal%20task%0Ainto%20an%20isolated%20subspace%20and%20additionally%20producing%20an%20arbitrary%20number%20of%0Aorthogonal%20subspaces%2C%20each%20of%20which%20encourages%20high%20separability%20among%0Aprojections.%20We%20generate%20the%20auxiliary%20classification%20task%20through%20a%20clustering%0Aprocedure%20on%20the%20most%20disentangled%20subspace%2C%20obtaining%20a%20discrete%20set%20of%0Alabels.%20Subsequently%2C%20the%20original%20data%2C%20the%20labels%20associated%20with%20the%0Aprincipal%20task%2C%20and%20the%20newly%20discovered%20ones%20can%20be%20fed%20into%20any%20MTL%0Aframework.%20Experimental%20validation%20on%20both%20synthetic%20and%20real%20data%2C%20along%20with%0Avarious%20ablation%20studies%2C%20demonstrates%20promising%20results%2C%20revealing%20the%0Apotential%20in%20what%20has%20been%2C%20so%20far%2C%20an%20unexplored%20connection%20between%20learning%0Adisentangled%20representations%20and%20MTL.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/intelligolabs/Detaux.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09278v3&entry.124074799=Read"},
{"title": "Understanding LLM Scientific Reasoning through Promptings and Model's\n  Explanation on the Answers", "author": "Alice Rueda and Mohammed S. Hassan and Argyrios Perivolaris and Bazen G. Teferra and Reza Samavi and Sirisha Rambhatla and Yuqi Wu and Yanbo Zhang and Bo Cao and Divya Sharma and Sridhar Krishnan and Venkat Bhat", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems.\n", "link": "http://arxiv.org/abs/2505.01482v2", "date": "2025-07-25", "relevancy": 2.0927, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Scientific%20Reasoning%20through%20Promptings%20and%20Model%27s%0A%20%20Explanation%20on%20the%20Answers&body=Title%3A%20Understanding%20LLM%20Scientific%20Reasoning%20through%20Promptings%20and%20Model%27s%0A%20%20Explanation%20on%20the%20Answers%0AAuthor%3A%20Alice%20Rueda%20and%20Mohammed%20S.%20Hassan%20and%20Argyrios%20Perivolaris%20and%20Bazen%20G.%20Teferra%20and%20Reza%20Samavi%20and%20Sirisha%20Rambhatla%20and%20Yuqi%20Wu%20and%20Yanbo%20Zhang%20and%20Bo%20Cao%20and%20Divya%20Sharma%20and%20Sridhar%20Krishnan%20and%20Venkat%20Bhat%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Anatural%20language%20understanding%2C%20reasoning%2C%20and%20problem-solving%20across%20various%0Adomains.%20However%2C%20their%20ability%20to%20perform%20complex%2C%20multi-step%20reasoning%0Atask-essential%20for%20applications%20in%20science%2C%20medicine%2C%20and%20law-remains%20an%20area%0Aof%20active%20investigation.%20This%20paper%20examines%20the%20reasoning%20capabilities%20of%0Acontemporary%20LLMs%2C%20analyzing%20their%20strengths%2C%20limitations%2C%20and%20potential%20for%0Aimprovement.%20The%20study%20uses%20prompt%20engineering%20techniques%20on%20the%20Graduate-Level%0AGoogleProof%20Q%26A%20%28GPQA%29%20dataset%20to%20assess%20the%20scientific%20reasoning%20of%20GPT-4o.%0AFive%20popular%20prompt%20engineering%20techniques%20and%20two%20tailored%20promptings%20were%0Atested%3A%20baseline%20direct%20answer%20%28zero-shot%29%2C%20chain-of-thought%20%28CoT%29%2C%20zero-shot%0ACoT%2C%20self-ask%2C%20self-consistency%2C%20decomposition%2C%20and%20multipath%20promptings.%20Our%0Afindings%20indicate%20that%20while%20LLMs%20exhibit%20emergent%20reasoning%20abilities%2C%20they%0Aoften%20rely%20on%20pattern%20recognition%20rather%20than%20true%20logical%20inference%2C%20leading%0Ato%20inconsistencies%20in%20complex%20problem-solving.%20The%20results%20indicated%20that%0Aself-consistency%20outperformed%20the%20other%20prompt%20engineering%20technique%20with%20an%0Aaccuracy%20of%2052.99%25%2C%20followed%20by%20direct%20answer%20%2852.23%25%29.%20Zero-shot%20CoT%20%2850%25%29%0Aoutperformed%20multipath%20%2848.44%25%29%2C%20decomposition%20%2847.77%25%29%2C%20self-ask%20%2846.88%25%29%2C%20and%0ACoT%20%2843.75%25%29.%20Self-consistency%20performed%20the%20second%20worst%20in%20explaining%20the%0Aanswers.%20Simple%20techniques%20such%20as%20direct%20answer%2C%20CoT%2C%20and%20zero-shot%20CoT%20have%0Athe%20best%20scientific%20reasoning.%20We%20propose%20a%20research%20agenda%20aimed%20at%20bridging%0Athese%20gaps%20by%20integrating%20structured%20reasoning%20frameworks%2C%20hybrid%20AI%0Aapproaches%2C%20and%20human-in-the-loop%20methodologies.%20By%20critically%20evaluating%20the%0Areasoning%20mechanisms%20of%20LLMs%2C%20this%20paper%20contributes%20to%20the%20ongoing%20discourse%0Aon%20the%20future%20of%20artificial%20general%20intelligence%20and%20the%20development%20of%20more%0Arobust%2C%20trustworthy%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Scientific%2520Reasoning%2520through%2520Promptings%2520and%2520Model%2527s%250A%2520%2520Explanation%2520on%2520the%2520Answers%26entry.906535625%3DAlice%2520Rueda%2520and%2520Mohammed%2520S.%2520Hassan%2520and%2520Argyrios%2520Perivolaris%2520and%2520Bazen%2520G.%2520Teferra%2520and%2520Reza%2520Samavi%2520and%2520Sirisha%2520Rambhatla%2520and%2520Yuqi%2520Wu%2520and%2520Yanbo%2520Zhang%2520and%2520Bo%2520Cao%2520and%2520Divya%2520Sharma%2520and%2520Sridhar%2520Krishnan%2520and%2520Venkat%2520Bhat%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Anatural%2520language%2520understanding%252C%2520reasoning%252C%2520and%2520problem-solving%2520across%2520various%250Adomains.%2520However%252C%2520their%2520ability%2520to%2520perform%2520complex%252C%2520multi-step%2520reasoning%250Atask-essential%2520for%2520applications%2520in%2520science%252C%2520medicine%252C%2520and%2520law-remains%2520an%2520area%250Aof%2520active%2520investigation.%2520This%2520paper%2520examines%2520the%2520reasoning%2520capabilities%2520of%250Acontemporary%2520LLMs%252C%2520analyzing%2520their%2520strengths%252C%2520limitations%252C%2520and%2520potential%2520for%250Aimprovement.%2520The%2520study%2520uses%2520prompt%2520engineering%2520techniques%2520on%2520the%2520Graduate-Level%250AGoogleProof%2520Q%2526A%2520%2528GPQA%2529%2520dataset%2520to%2520assess%2520the%2520scientific%2520reasoning%2520of%2520GPT-4o.%250AFive%2520popular%2520prompt%2520engineering%2520techniques%2520and%2520two%2520tailored%2520promptings%2520were%250Atested%253A%2520baseline%2520direct%2520answer%2520%2528zero-shot%2529%252C%2520chain-of-thought%2520%2528CoT%2529%252C%2520zero-shot%250ACoT%252C%2520self-ask%252C%2520self-consistency%252C%2520decomposition%252C%2520and%2520multipath%2520promptings.%2520Our%250Afindings%2520indicate%2520that%2520while%2520LLMs%2520exhibit%2520emergent%2520reasoning%2520abilities%252C%2520they%250Aoften%2520rely%2520on%2520pattern%2520recognition%2520rather%2520than%2520true%2520logical%2520inference%252C%2520leading%250Ato%2520inconsistencies%2520in%2520complex%2520problem-solving.%2520The%2520results%2520indicated%2520that%250Aself-consistency%2520outperformed%2520the%2520other%2520prompt%2520engineering%2520technique%2520with%2520an%250Aaccuracy%2520of%252052.99%2525%252C%2520followed%2520by%2520direct%2520answer%2520%252852.23%2525%2529.%2520Zero-shot%2520CoT%2520%252850%2525%2529%250Aoutperformed%2520multipath%2520%252848.44%2525%2529%252C%2520decomposition%2520%252847.77%2525%2529%252C%2520self-ask%2520%252846.88%2525%2529%252C%2520and%250ACoT%2520%252843.75%2525%2529.%2520Self-consistency%2520performed%2520the%2520second%2520worst%2520in%2520explaining%2520the%250Aanswers.%2520Simple%2520techniques%2520such%2520as%2520direct%2520answer%252C%2520CoT%252C%2520and%2520zero-shot%2520CoT%2520have%250Athe%2520best%2520scientific%2520reasoning.%2520We%2520propose%2520a%2520research%2520agenda%2520aimed%2520at%2520bridging%250Athese%2520gaps%2520by%2520integrating%2520structured%2520reasoning%2520frameworks%252C%2520hybrid%2520AI%250Aapproaches%252C%2520and%2520human-in-the-loop%2520methodologies.%2520By%2520critically%2520evaluating%2520the%250Areasoning%2520mechanisms%2520of%2520LLMs%252C%2520this%2520paper%2520contributes%2520to%2520the%2520ongoing%2520discourse%250Aon%2520the%2520future%2520of%2520artificial%2520general%2520intelligence%2520and%2520the%2520development%2520of%2520more%250Arobust%252C%2520trustworthy%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Scientific%20Reasoning%20through%20Promptings%20and%20Model%27s%0A%20%20Explanation%20on%20the%20Answers&entry.906535625=Alice%20Rueda%20and%20Mohammed%20S.%20Hassan%20and%20Argyrios%20Perivolaris%20and%20Bazen%20G.%20Teferra%20and%20Reza%20Samavi%20and%20Sirisha%20Rambhatla%20and%20Yuqi%20Wu%20and%20Yanbo%20Zhang%20and%20Bo%20Cao%20and%20Divya%20Sharma%20and%20Sridhar%20Krishnan%20and%20Venkat%20Bhat&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Anatural%20language%20understanding%2C%20reasoning%2C%20and%20problem-solving%20across%20various%0Adomains.%20However%2C%20their%20ability%20to%20perform%20complex%2C%20multi-step%20reasoning%0Atask-essential%20for%20applications%20in%20science%2C%20medicine%2C%20and%20law-remains%20an%20area%0Aof%20active%20investigation.%20This%20paper%20examines%20the%20reasoning%20capabilities%20of%0Acontemporary%20LLMs%2C%20analyzing%20their%20strengths%2C%20limitations%2C%20and%20potential%20for%0Aimprovement.%20The%20study%20uses%20prompt%20engineering%20techniques%20on%20the%20Graduate-Level%0AGoogleProof%20Q%26A%20%28GPQA%29%20dataset%20to%20assess%20the%20scientific%20reasoning%20of%20GPT-4o.%0AFive%20popular%20prompt%20engineering%20techniques%20and%20two%20tailored%20promptings%20were%0Atested%3A%20baseline%20direct%20answer%20%28zero-shot%29%2C%20chain-of-thought%20%28CoT%29%2C%20zero-shot%0ACoT%2C%20self-ask%2C%20self-consistency%2C%20decomposition%2C%20and%20multipath%20promptings.%20Our%0Afindings%20indicate%20that%20while%20LLMs%20exhibit%20emergent%20reasoning%20abilities%2C%20they%0Aoften%20rely%20on%20pattern%20recognition%20rather%20than%20true%20logical%20inference%2C%20leading%0Ato%20inconsistencies%20in%20complex%20problem-solving.%20The%20results%20indicated%20that%0Aself-consistency%20outperformed%20the%20other%20prompt%20engineering%20technique%20with%20an%0Aaccuracy%20of%2052.99%25%2C%20followed%20by%20direct%20answer%20%2852.23%25%29.%20Zero-shot%20CoT%20%2850%25%29%0Aoutperformed%20multipath%20%2848.44%25%29%2C%20decomposition%20%2847.77%25%29%2C%20self-ask%20%2846.88%25%29%2C%20and%0ACoT%20%2843.75%25%29.%20Self-consistency%20performed%20the%20second%20worst%20in%20explaining%20the%0Aanswers.%20Simple%20techniques%20such%20as%20direct%20answer%2C%20CoT%2C%20and%20zero-shot%20CoT%20have%0Athe%20best%20scientific%20reasoning.%20We%20propose%20a%20research%20agenda%20aimed%20at%20bridging%0Athese%20gaps%20by%20integrating%20structured%20reasoning%20frameworks%2C%20hybrid%20AI%0Aapproaches%2C%20and%20human-in-the-loop%20methodologies.%20By%20critically%20evaluating%20the%0Areasoning%20mechanisms%20of%20LLMs%2C%20this%20paper%20contributes%20to%20the%20ongoing%20discourse%0Aon%20the%20future%20of%20artificial%20general%20intelligence%20and%20the%20development%20of%20more%0Arobust%2C%20trustworthy%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01482v2&entry.124074799=Read"},
{"title": "Advancing Event Forecasting through Massive Training of Large Language\n  Models: Challenges, Solutions, and Broader Impacts", "author": "Sang-Woo Lee and Sohee Yang and Donghyun Kwak and Noah Y. Siegel", "abstract": "  Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions.\n", "link": "http://arxiv.org/abs/2507.19477v1", "date": "2025-07-25", "relevancy": 1.993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Event%20Forecasting%20through%20Massive%20Training%20of%20Large%20Language%0A%20%20Models%3A%20Challenges%2C%20Solutions%2C%20and%20Broader%20Impacts&body=Title%3A%20Advancing%20Event%20Forecasting%20through%20Massive%20Training%20of%20Large%20Language%0A%20%20Models%3A%20Challenges%2C%20Solutions%2C%20and%20Broader%20Impacts%0AAuthor%3A%20Sang-Woo%20Lee%20and%20Sohee%20Yang%20and%20Donghyun%20Kwak%20and%20Noah%20Y.%20Siegel%0AAbstract%3A%20%20%20Many%20recent%20papers%20have%20studied%20the%20development%20of%20superforecaster-level%0Aevent%20forecasting%20LLMs.%20While%20methodological%20problems%20with%20early%20studies%20cast%0Adoubt%20on%20the%20use%20of%20LLMs%20for%20event%20forecasting%2C%20recent%20studies%20with%20improved%0Aevaluation%20methods%20have%20shown%20that%20state-of-the-art%20LLMs%20are%20gradually%20reaching%0Asuperforecaster-level%20performance%2C%20and%20reinforcement%20learning%20has%20also%20been%0Areported%20to%20improve%20future%20forecasting.%20Additionally%2C%20the%20unprecedented%20success%0Aof%20recent%20reasoning%20models%20and%20Deep%20Research-style%20models%20suggests%20that%0Atechnology%20capable%20of%20greatly%20improving%20forecasting%20performance%20has%20been%0Adeveloped.%20Therefore%2C%20based%20on%20these%20positive%20recent%20trends%2C%20we%20argue%20that%20the%0Atime%20is%20ripe%20for%20research%20on%20large-scale%20training%20of%20superforecaster-level%0Aevent%20forecasting%20LLMs.%20We%20discuss%20two%20key%20research%20directions%3A%20training%0Amethods%20and%20data%20acquisition.%20For%20training%2C%20we%20first%20introduce%20three%0Adifficulties%20of%20LLM-based%20event%20forecasting%20training%3A%20noisiness-sparsity%2C%0Aknowledge%20cut-off%2C%20and%20simple%20reward%20structure%20problems.%20Then%2C%20we%20present%0Arelated%20ideas%20to%20mitigate%20these%20problems%3A%20hypothetical%20event%20Bayesian%20networks%2C%0Autilizing%20poorly-recalled%20and%20counterfactual%20events%2C%20and%20auxiliary%20reward%0Asignals.%20For%20data%2C%20we%20propose%20aggressive%20use%20of%20market%2C%20public%2C%20and%20crawling%0Adatasets%20to%20enable%20large-scale%20training%20and%20evaluation.%20Finally%2C%20we%20explain%20how%0Athese%20technical%20advances%20could%20enable%20AI%20to%20provide%20predictive%20intelligence%20to%0Asociety%20in%20broader%20areas.%20This%20position%20paper%20presents%20promising%20specific%20paths%0Aand%20considerations%20for%20getting%20closer%20to%20superforecaster-level%20AI%20technology%2C%0Aaiming%20to%20call%20for%20researchers%27%20interest%20in%20these%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Event%2520Forecasting%2520through%2520Massive%2520Training%2520of%2520Large%2520Language%250A%2520%2520Models%253A%2520Challenges%252C%2520Solutions%252C%2520and%2520Broader%2520Impacts%26entry.906535625%3DSang-Woo%2520Lee%2520and%2520Sohee%2520Yang%2520and%2520Donghyun%2520Kwak%2520and%2520Noah%2520Y.%2520Siegel%26entry.1292438233%3D%2520%2520Many%2520recent%2520papers%2520have%2520studied%2520the%2520development%2520of%2520superforecaster-level%250Aevent%2520forecasting%2520LLMs.%2520While%2520methodological%2520problems%2520with%2520early%2520studies%2520cast%250Adoubt%2520on%2520the%2520use%2520of%2520LLMs%2520for%2520event%2520forecasting%252C%2520recent%2520studies%2520with%2520improved%250Aevaluation%2520methods%2520have%2520shown%2520that%2520state-of-the-art%2520LLMs%2520are%2520gradually%2520reaching%250Asuperforecaster-level%2520performance%252C%2520and%2520reinforcement%2520learning%2520has%2520also%2520been%250Areported%2520to%2520improve%2520future%2520forecasting.%2520Additionally%252C%2520the%2520unprecedented%2520success%250Aof%2520recent%2520reasoning%2520models%2520and%2520Deep%2520Research-style%2520models%2520suggests%2520that%250Atechnology%2520capable%2520of%2520greatly%2520improving%2520forecasting%2520performance%2520has%2520been%250Adeveloped.%2520Therefore%252C%2520based%2520on%2520these%2520positive%2520recent%2520trends%252C%2520we%2520argue%2520that%2520the%250Atime%2520is%2520ripe%2520for%2520research%2520on%2520large-scale%2520training%2520of%2520superforecaster-level%250Aevent%2520forecasting%2520LLMs.%2520We%2520discuss%2520two%2520key%2520research%2520directions%253A%2520training%250Amethods%2520and%2520data%2520acquisition.%2520For%2520training%252C%2520we%2520first%2520introduce%2520three%250Adifficulties%2520of%2520LLM-based%2520event%2520forecasting%2520training%253A%2520noisiness-sparsity%252C%250Aknowledge%2520cut-off%252C%2520and%2520simple%2520reward%2520structure%2520problems.%2520Then%252C%2520we%2520present%250Arelated%2520ideas%2520to%2520mitigate%2520these%2520problems%253A%2520hypothetical%2520event%2520Bayesian%2520networks%252C%250Autilizing%2520poorly-recalled%2520and%2520counterfactual%2520events%252C%2520and%2520auxiliary%2520reward%250Asignals.%2520For%2520data%252C%2520we%2520propose%2520aggressive%2520use%2520of%2520market%252C%2520public%252C%2520and%2520crawling%250Adatasets%2520to%2520enable%2520large-scale%2520training%2520and%2520evaluation.%2520Finally%252C%2520we%2520explain%2520how%250Athese%2520technical%2520advances%2520could%2520enable%2520AI%2520to%2520provide%2520predictive%2520intelligence%2520to%250Asociety%2520in%2520broader%2520areas.%2520This%2520position%2520paper%2520presents%2520promising%2520specific%2520paths%250Aand%2520considerations%2520for%2520getting%2520closer%2520to%2520superforecaster-level%2520AI%2520technology%252C%250Aaiming%2520to%2520call%2520for%2520researchers%2527%2520interest%2520in%2520these%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Event%20Forecasting%20through%20Massive%20Training%20of%20Large%20Language%0A%20%20Models%3A%20Challenges%2C%20Solutions%2C%20and%20Broader%20Impacts&entry.906535625=Sang-Woo%20Lee%20and%20Sohee%20Yang%20and%20Donghyun%20Kwak%20and%20Noah%20Y.%20Siegel&entry.1292438233=%20%20Many%20recent%20papers%20have%20studied%20the%20development%20of%20superforecaster-level%0Aevent%20forecasting%20LLMs.%20While%20methodological%20problems%20with%20early%20studies%20cast%0Adoubt%20on%20the%20use%20of%20LLMs%20for%20event%20forecasting%2C%20recent%20studies%20with%20improved%0Aevaluation%20methods%20have%20shown%20that%20state-of-the-art%20LLMs%20are%20gradually%20reaching%0Asuperforecaster-level%20performance%2C%20and%20reinforcement%20learning%20has%20also%20been%0Areported%20to%20improve%20future%20forecasting.%20Additionally%2C%20the%20unprecedented%20success%0Aof%20recent%20reasoning%20models%20and%20Deep%20Research-style%20models%20suggests%20that%0Atechnology%20capable%20of%20greatly%20improving%20forecasting%20performance%20has%20been%0Adeveloped.%20Therefore%2C%20based%20on%20these%20positive%20recent%20trends%2C%20we%20argue%20that%20the%0Atime%20is%20ripe%20for%20research%20on%20large-scale%20training%20of%20superforecaster-level%0Aevent%20forecasting%20LLMs.%20We%20discuss%20two%20key%20research%20directions%3A%20training%0Amethods%20and%20data%20acquisition.%20For%20training%2C%20we%20first%20introduce%20three%0Adifficulties%20of%20LLM-based%20event%20forecasting%20training%3A%20noisiness-sparsity%2C%0Aknowledge%20cut-off%2C%20and%20simple%20reward%20structure%20problems.%20Then%2C%20we%20present%0Arelated%20ideas%20to%20mitigate%20these%20problems%3A%20hypothetical%20event%20Bayesian%20networks%2C%0Autilizing%20poorly-recalled%20and%20counterfactual%20events%2C%20and%20auxiliary%20reward%0Asignals.%20For%20data%2C%20we%20propose%20aggressive%20use%20of%20market%2C%20public%2C%20and%20crawling%0Adatasets%20to%20enable%20large-scale%20training%20and%20evaluation.%20Finally%2C%20we%20explain%20how%0Athese%20technical%20advances%20could%20enable%20AI%20to%20provide%20predictive%20intelligence%20to%0Asociety%20in%20broader%20areas.%20This%20position%20paper%20presents%20promising%20specific%20paths%0Aand%20considerations%20for%20getting%20closer%20to%20superforecaster-level%20AI%20technology%2C%0Aaiming%20to%20call%20for%20researchers%27%20interest%20in%20these%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19477v1&entry.124074799=Read"},
{"title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc\n  Attributions for Opaque Models", "author": "Yuchi Tang and I\u00f1aki Esnaola and George Panoutsos", "abstract": "  Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding.\n", "link": "http://arxiv.org/abs/2507.10643v2", "date": "2025-07-25", "relevancy": 1.9485, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaylorPODA%3A%20A%20Taylor%20Expansion-Based%20Method%20to%20Improve%20Post-Hoc%0A%20%20Attributions%20for%20Opaque%20Models&body=Title%3A%20TaylorPODA%3A%20A%20Taylor%20Expansion-Based%20Method%20to%20Improve%20Post-Hoc%0A%20%20Attributions%20for%20Opaque%20Models%0AAuthor%3A%20Yuchi%20Tang%20and%20I%C3%B1aki%20Esnaola%20and%20George%20Panoutsos%0AAbstract%3A%20%20%20Existing%20post-hoc%20model-agnostic%20methods%20generate%20external%20explanations%20for%0Aopaque%20models%2C%20primarily%20by%20locally%20attributing%20the%20model%20output%20to%20its%20input%0Afeatures.%20However%2C%20they%20often%20lack%20an%20explicit%20and%20systematic%20framework%20for%0Aquantifying%20the%20contribution%20of%20individual%20features.%20Building%20on%20the%20Taylor%0Aexpansion%20framework%20introduced%20by%20Deng%20et%20al.%20%282024%29%20to%20unify%20existing%20local%0Aattribution%20methods%2C%20we%20propose%20a%20rigorous%20set%20of%20postulates%20--%20%22precision%22%2C%0A%22federation%22%2C%20and%20%22zero-discrepancy%22%20--%20to%20govern%20Taylor%20term-specific%0Aattribution.%20Guided%20by%20these%20postulates%2C%20we%20introduce%20TaylorPODA%20%28Taylor%0Aexpansion-derived%20imPortance-Order%20aDapted%20Attribution%29%2C%20which%20incorporates%20an%0Aadditional%20%22adaptation%22%20property.%20This%20property%20enables%20alignment%20with%0Atask-specific%20goals%2C%20especially%20in%20post-hoc%20settings%20lacking%20ground-truth%0Aexplanations.%20Empirical%20evaluations%20demonstrate%20that%20TaylorPODA%20achieves%0Acompetitive%20results%20against%20baseline%20methods%2C%20providing%20principled%20and%0Avisualization-friendly%20explanations.%20This%20work%20represents%20a%20step%20toward%20the%0Atrustworthy%20deployment%20of%20opaque%20models%20by%20offering%20explanations%20with%20stronger%0Atheoretical%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaylorPODA%253A%2520A%2520Taylor%2520Expansion-Based%2520Method%2520to%2520Improve%2520Post-Hoc%250A%2520%2520Attributions%2520for%2520Opaque%2520Models%26entry.906535625%3DYuchi%2520Tang%2520and%2520I%25C3%25B1aki%2520Esnaola%2520and%2520George%2520Panoutsos%26entry.1292438233%3D%2520%2520Existing%2520post-hoc%2520model-agnostic%2520methods%2520generate%2520external%2520explanations%2520for%250Aopaque%2520models%252C%2520primarily%2520by%2520locally%2520attributing%2520the%2520model%2520output%2520to%2520its%2520input%250Afeatures.%2520However%252C%2520they%2520often%2520lack%2520an%2520explicit%2520and%2520systematic%2520framework%2520for%250Aquantifying%2520the%2520contribution%2520of%2520individual%2520features.%2520Building%2520on%2520the%2520Taylor%250Aexpansion%2520framework%2520introduced%2520by%2520Deng%2520et%2520al.%2520%25282024%2529%2520to%2520unify%2520existing%2520local%250Aattribution%2520methods%252C%2520we%2520propose%2520a%2520rigorous%2520set%2520of%2520postulates%2520--%2520%2522precision%2522%252C%250A%2522federation%2522%252C%2520and%2520%2522zero-discrepancy%2522%2520--%2520to%2520govern%2520Taylor%2520term-specific%250Aattribution.%2520Guided%2520by%2520these%2520postulates%252C%2520we%2520introduce%2520TaylorPODA%2520%2528Taylor%250Aexpansion-derived%2520imPortance-Order%2520aDapted%2520Attribution%2529%252C%2520which%2520incorporates%2520an%250Aadditional%2520%2522adaptation%2522%2520property.%2520This%2520property%2520enables%2520alignment%2520with%250Atask-specific%2520goals%252C%2520especially%2520in%2520post-hoc%2520settings%2520lacking%2520ground-truth%250Aexplanations.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520TaylorPODA%2520achieves%250Acompetitive%2520results%2520against%2520baseline%2520methods%252C%2520providing%2520principled%2520and%250Avisualization-friendly%2520explanations.%2520This%2520work%2520represents%2520a%2520step%2520toward%2520the%250Atrustworthy%2520deployment%2520of%2520opaque%2520models%2520by%2520offering%2520explanations%2520with%2520stronger%250Atheoretical%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaylorPODA%3A%20A%20Taylor%20Expansion-Based%20Method%20to%20Improve%20Post-Hoc%0A%20%20Attributions%20for%20Opaque%20Models&entry.906535625=Yuchi%20Tang%20and%20I%C3%B1aki%20Esnaola%20and%20George%20Panoutsos&entry.1292438233=%20%20Existing%20post-hoc%20model-agnostic%20methods%20generate%20external%20explanations%20for%0Aopaque%20models%2C%20primarily%20by%20locally%20attributing%20the%20model%20output%20to%20its%20input%0Afeatures.%20However%2C%20they%20often%20lack%20an%20explicit%20and%20systematic%20framework%20for%0Aquantifying%20the%20contribution%20of%20individual%20features.%20Building%20on%20the%20Taylor%0Aexpansion%20framework%20introduced%20by%20Deng%20et%20al.%20%282024%29%20to%20unify%20existing%20local%0Aattribution%20methods%2C%20we%20propose%20a%20rigorous%20set%20of%20postulates%20--%20%22precision%22%2C%0A%22federation%22%2C%20and%20%22zero-discrepancy%22%20--%20to%20govern%20Taylor%20term-specific%0Aattribution.%20Guided%20by%20these%20postulates%2C%20we%20introduce%20TaylorPODA%20%28Taylor%0Aexpansion-derived%20imPortance-Order%20aDapted%20Attribution%29%2C%20which%20incorporates%20an%0Aadditional%20%22adaptation%22%20property.%20This%20property%20enables%20alignment%20with%0Atask-specific%20goals%2C%20especially%20in%20post-hoc%20settings%20lacking%20ground-truth%0Aexplanations.%20Empirical%20evaluations%20demonstrate%20that%20TaylorPODA%20achieves%0Acompetitive%20results%20against%20baseline%20methods%2C%20providing%20principled%20and%0Avisualization-friendly%20explanations.%20This%20work%20represents%20a%20step%20toward%20the%0Atrustworthy%20deployment%20of%20opaque%20models%20by%20offering%20explanations%20with%20stronger%0Atheoretical%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10643v2&entry.124074799=Read"},
{"title": "Efficient Lines Detection for Robot Soccer", "author": "Jo\u00e3o G. Melo and Jo\u00e3o P. Mafaldo and Edna Barros", "abstract": "  Self-localization is essential in robot soccer, where accurate detection of\nvisual field features, such as lines and boundaries, is critical for reliable\npose estimation. This paper presents a lightweight and efficient method for\ndetecting soccer field lines using the ELSED algorithm, extended with a\nclassification step that analyzes RGB color transitions to identify lines\nbelonging to the field. We introduce a pipeline based on Particle Swarm\nOptimization (PSO) for threshold calibration to optimize detection performance,\nrequiring only a small number of annotated samples. Our approach achieves\naccuracy comparable to a state-of-the-art deep learning model while offering\nhigher processing speed, making it well-suited for real-time applications on\nlow-power robotic platforms.\n", "link": "http://arxiv.org/abs/2507.19469v1", "date": "2025-07-25", "relevancy": 1.9398, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4833}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Lines%20Detection%20for%20Robot%20Soccer&body=Title%3A%20Efficient%20Lines%20Detection%20for%20Robot%20Soccer%0AAuthor%3A%20Jo%C3%A3o%20G.%20Melo%20and%20Jo%C3%A3o%20P.%20Mafaldo%20and%20Edna%20Barros%0AAbstract%3A%20%20%20Self-localization%20is%20essential%20in%20robot%20soccer%2C%20where%20accurate%20detection%20of%0Avisual%20field%20features%2C%20such%20as%20lines%20and%20boundaries%2C%20is%20critical%20for%20reliable%0Apose%20estimation.%20This%20paper%20presents%20a%20lightweight%20and%20efficient%20method%20for%0Adetecting%20soccer%20field%20lines%20using%20the%20ELSED%20algorithm%2C%20extended%20with%20a%0Aclassification%20step%20that%20analyzes%20RGB%20color%20transitions%20to%20identify%20lines%0Abelonging%20to%20the%20field.%20We%20introduce%20a%20pipeline%20based%20on%20Particle%20Swarm%0AOptimization%20%28PSO%29%20for%20threshold%20calibration%20to%20optimize%20detection%20performance%2C%0Arequiring%20only%20a%20small%20number%20of%20annotated%20samples.%20Our%20approach%20achieves%0Aaccuracy%20comparable%20to%20a%20state-of-the-art%20deep%20learning%20model%20while%20offering%0Ahigher%20processing%20speed%2C%20making%20it%20well-suited%20for%20real-time%20applications%20on%0Alow-power%20robotic%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Lines%2520Detection%2520for%2520Robot%2520Soccer%26entry.906535625%3DJo%25C3%25A3o%2520G.%2520Melo%2520and%2520Jo%25C3%25A3o%2520P.%2520Mafaldo%2520and%2520Edna%2520Barros%26entry.1292438233%3D%2520%2520Self-localization%2520is%2520essential%2520in%2520robot%2520soccer%252C%2520where%2520accurate%2520detection%2520of%250Avisual%2520field%2520features%252C%2520such%2520as%2520lines%2520and%2520boundaries%252C%2520is%2520critical%2520for%2520reliable%250Apose%2520estimation.%2520This%2520paper%2520presents%2520a%2520lightweight%2520and%2520efficient%2520method%2520for%250Adetecting%2520soccer%2520field%2520lines%2520using%2520the%2520ELSED%2520algorithm%252C%2520extended%2520with%2520a%250Aclassification%2520step%2520that%2520analyzes%2520RGB%2520color%2520transitions%2520to%2520identify%2520lines%250Abelonging%2520to%2520the%2520field.%2520We%2520introduce%2520a%2520pipeline%2520based%2520on%2520Particle%2520Swarm%250AOptimization%2520%2528PSO%2529%2520for%2520threshold%2520calibration%2520to%2520optimize%2520detection%2520performance%252C%250Arequiring%2520only%2520a%2520small%2520number%2520of%2520annotated%2520samples.%2520Our%2520approach%2520achieves%250Aaccuracy%2520comparable%2520to%2520a%2520state-of-the-art%2520deep%2520learning%2520model%2520while%2520offering%250Ahigher%2520processing%2520speed%252C%2520making%2520it%2520well-suited%2520for%2520real-time%2520applications%2520on%250Alow-power%2520robotic%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Lines%20Detection%20for%20Robot%20Soccer&entry.906535625=Jo%C3%A3o%20G.%20Melo%20and%20Jo%C3%A3o%20P.%20Mafaldo%20and%20Edna%20Barros&entry.1292438233=%20%20Self-localization%20is%20essential%20in%20robot%20soccer%2C%20where%20accurate%20detection%20of%0Avisual%20field%20features%2C%20such%20as%20lines%20and%20boundaries%2C%20is%20critical%20for%20reliable%0Apose%20estimation.%20This%20paper%20presents%20a%20lightweight%20and%20efficient%20method%20for%0Adetecting%20soccer%20field%20lines%20using%20the%20ELSED%20algorithm%2C%20extended%20with%20a%0Aclassification%20step%20that%20analyzes%20RGB%20color%20transitions%20to%20identify%20lines%0Abelonging%20to%20the%20field.%20We%20introduce%20a%20pipeline%20based%20on%20Particle%20Swarm%0AOptimization%20%28PSO%29%20for%20threshold%20calibration%20to%20optimize%20detection%20performance%2C%0Arequiring%20only%20a%20small%20number%20of%20annotated%20samples.%20Our%20approach%20achieves%0Aaccuracy%20comparable%20to%20a%20state-of-the-art%20deep%20learning%20model%20while%20offering%0Ahigher%20processing%20speed%2C%20making%20it%20well-suited%20for%20real-time%20applications%20on%0Alow-power%20robotic%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19469v1&entry.124074799=Read"},
{"title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic\n  Grounding for Generalizable Robotic Manipulation", "author": "Chenyu Su and Weiwei Shang and Chen Qian and Fei Zhang and Shuang Cong", "abstract": "  Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos are available at\nhttps://github.com/scy-v/ReSem3D and https://resem3d.github.io.\n", "link": "http://arxiv.org/abs/2507.18262v2", "date": "2025-07-25", "relevancy": 1.8594, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReSem3D%3A%20Refinable%203D%20Spatial%20Constraints%20via%20Fine-Grained%20Semantic%0A%20%20Grounding%20for%20Generalizable%20Robotic%20Manipulation&body=Title%3A%20ReSem3D%3A%20Refinable%203D%20Spatial%20Constraints%20via%20Fine-Grained%20Semantic%0A%20%20Grounding%20for%20Generalizable%20Robotic%20Manipulation%0AAuthor%3A%20Chenyu%20Su%20and%20Weiwei%20Shang%20and%20Chen%20Qian%20and%20Fei%20Zhang%20and%20Shuang%20Cong%0AAbstract%3A%20%20%20Semantics-driven%203D%20spatial%20constraints%20align%20highlevel%20semantic%0Arepresentations%20with%20low-level%20action%20spaces%2C%20facilitating%20the%20unification%20of%0Atask%20understanding%20and%20execution%20in%20robotic%20manipulation.%20The%20synergistic%0Areasoning%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Foundation%0AModels%20%28VFMs%29%20enables%20cross-modal%203D%20spatial%20constraint%20construction.%0ANevertheless%2C%20existing%20methods%20have%20three%20key%20limitations%3A%20%281%29%20coarse%20semantic%0Agranularity%20in%20constraint%20modeling%2C%20%282%29%20lack%20of%20real-time%20closed-loop%20planning%2C%0A%283%29%20compromised%20robustness%20in%20semantically%20diverse%20environments.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ReSem3D%2C%20a%20unified%20manipulation%20framework%20for%0Asemantically%20diverse%20environments%2C%20leveraging%20the%20synergy%20between%20VFMs%20and%0AMLLMs%20to%20achieve%20fine-grained%20visual%20grounding%20and%20dynamically%20constructs%0Ahierarchical%203D%20spatial%20constraints%20for%20real-time%20manipulation.%20Specifically%2C%0Athe%20framework%20is%20driven%20by%20hierarchical%20recursive%20reasoning%20in%20MLLMs%2C%20which%0Ainteract%20with%20VFMs%20to%20automatically%20construct%203D%20spatial%20constraints%20from%0Anatural%20language%20instructions%20and%20RGB-D%20observations%20in%20two%20stages%3A%20part-level%0Aextraction%20and%20region-level%20refinement.%20Subsequently%2C%20these%20constraints%20are%0Aencoded%20as%20real-time%20optimization%20objectives%20in%20joint%20space%2C%20enabling%20reactive%0Abehavior%20to%20dynamic%20disturbances.%20Extensive%20simulation%20and%20real-world%0Aexperiments%20are%20conducted%20in%20semantically%20rich%20household%20and%20sparse%20chemical%0Alab%20environments.%20The%20results%20demonstrate%20that%20ReSem3D%20performs%20diverse%0Amanipulation%20tasks%20under%20zero-shot%20conditions%2C%20exhibiting%20strong%20adaptability%0Aand%20generalization.%20Code%20and%20videos%20are%20available%20at%0Ahttps%3A//github.com/scy-v/ReSem3D%20and%20https%3A//resem3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReSem3D%253A%2520Refinable%25203D%2520Spatial%2520Constraints%2520via%2520Fine-Grained%2520Semantic%250A%2520%2520Grounding%2520for%2520Generalizable%2520Robotic%2520Manipulation%26entry.906535625%3DChenyu%2520Su%2520and%2520Weiwei%2520Shang%2520and%2520Chen%2520Qian%2520and%2520Fei%2520Zhang%2520and%2520Shuang%2520Cong%26entry.1292438233%3D%2520%2520Semantics-driven%25203D%2520spatial%2520constraints%2520align%2520highlevel%2520semantic%250Arepresentations%2520with%2520low-level%2520action%2520spaces%252C%2520facilitating%2520the%2520unification%2520of%250Atask%2520understanding%2520and%2520execution%2520in%2520robotic%2520manipulation.%2520The%2520synergistic%250Areasoning%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520and%2520Vision%2520Foundation%250AModels%2520%2528VFMs%2529%2520enables%2520cross-modal%25203D%2520spatial%2520constraint%2520construction.%250ANevertheless%252C%2520existing%2520methods%2520have%2520three%2520key%2520limitations%253A%2520%25281%2529%2520coarse%2520semantic%250Agranularity%2520in%2520constraint%2520modeling%252C%2520%25282%2529%2520lack%2520of%2520real-time%2520closed-loop%2520planning%252C%250A%25283%2529%2520compromised%2520robustness%2520in%2520semantically%2520diverse%2520environments.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520ReSem3D%252C%2520a%2520unified%2520manipulation%2520framework%2520for%250Asemantically%2520diverse%2520environments%252C%2520leveraging%2520the%2520synergy%2520between%2520VFMs%2520and%250AMLLMs%2520to%2520achieve%2520fine-grained%2520visual%2520grounding%2520and%2520dynamically%2520constructs%250Ahierarchical%25203D%2520spatial%2520constraints%2520for%2520real-time%2520manipulation.%2520Specifically%252C%250Athe%2520framework%2520is%2520driven%2520by%2520hierarchical%2520recursive%2520reasoning%2520in%2520MLLMs%252C%2520which%250Ainteract%2520with%2520VFMs%2520to%2520automatically%2520construct%25203D%2520spatial%2520constraints%2520from%250Anatural%2520language%2520instructions%2520and%2520RGB-D%2520observations%2520in%2520two%2520stages%253A%2520part-level%250Aextraction%2520and%2520region-level%2520refinement.%2520Subsequently%252C%2520these%2520constraints%2520are%250Aencoded%2520as%2520real-time%2520optimization%2520objectives%2520in%2520joint%2520space%252C%2520enabling%2520reactive%250Abehavior%2520to%2520dynamic%2520disturbances.%2520Extensive%2520simulation%2520and%2520real-world%250Aexperiments%2520are%2520conducted%2520in%2520semantically%2520rich%2520household%2520and%2520sparse%2520chemical%250Alab%2520environments.%2520The%2520results%2520demonstrate%2520that%2520ReSem3D%2520performs%2520diverse%250Amanipulation%2520tasks%2520under%2520zero-shot%2520conditions%252C%2520exhibiting%2520strong%2520adaptability%250Aand%2520generalization.%2520Code%2520and%2520videos%2520are%2520available%2520at%250Ahttps%253A//github.com/scy-v/ReSem3D%2520and%2520https%253A//resem3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReSem3D%3A%20Refinable%203D%20Spatial%20Constraints%20via%20Fine-Grained%20Semantic%0A%20%20Grounding%20for%20Generalizable%20Robotic%20Manipulation&entry.906535625=Chenyu%20Su%20and%20Weiwei%20Shang%20and%20Chen%20Qian%20and%20Fei%20Zhang%20and%20Shuang%20Cong&entry.1292438233=%20%20Semantics-driven%203D%20spatial%20constraints%20align%20highlevel%20semantic%0Arepresentations%20with%20low-level%20action%20spaces%2C%20facilitating%20the%20unification%20of%0Atask%20understanding%20and%20execution%20in%20robotic%20manipulation.%20The%20synergistic%0Areasoning%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Foundation%0AModels%20%28VFMs%29%20enables%20cross-modal%203D%20spatial%20constraint%20construction.%0ANevertheless%2C%20existing%20methods%20have%20three%20key%20limitations%3A%20%281%29%20coarse%20semantic%0Agranularity%20in%20constraint%20modeling%2C%20%282%29%20lack%20of%20real-time%20closed-loop%20planning%2C%0A%283%29%20compromised%20robustness%20in%20semantically%20diverse%20environments.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ReSem3D%2C%20a%20unified%20manipulation%20framework%20for%0Asemantically%20diverse%20environments%2C%20leveraging%20the%20synergy%20between%20VFMs%20and%0AMLLMs%20to%20achieve%20fine-grained%20visual%20grounding%20and%20dynamically%20constructs%0Ahierarchical%203D%20spatial%20constraints%20for%20real-time%20manipulation.%20Specifically%2C%0Athe%20framework%20is%20driven%20by%20hierarchical%20recursive%20reasoning%20in%20MLLMs%2C%20which%0Ainteract%20with%20VFMs%20to%20automatically%20construct%203D%20spatial%20constraints%20from%0Anatural%20language%20instructions%20and%20RGB-D%20observations%20in%20two%20stages%3A%20part-level%0Aextraction%20and%20region-level%20refinement.%20Subsequently%2C%20these%20constraints%20are%0Aencoded%20as%20real-time%20optimization%20objectives%20in%20joint%20space%2C%20enabling%20reactive%0Abehavior%20to%20dynamic%20disturbances.%20Extensive%20simulation%20and%20real-world%0Aexperiments%20are%20conducted%20in%20semantically%20rich%20household%20and%20sparse%20chemical%0Alab%20environments.%20The%20results%20demonstrate%20that%20ReSem3D%20performs%20diverse%0Amanipulation%20tasks%20under%20zero-shot%20conditions%2C%20exhibiting%20strong%20adaptability%0Aand%20generalization.%20Code%20and%20videos%20are%20available%20at%0Ahttps%3A//github.com/scy-v/ReSem3D%20and%20https%3A//resem3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18262v2&entry.124074799=Read"},
{"title": "Perfect Clustering in Very Sparse Diverse Multiplex Networks", "author": "Marianna Pensky", "abstract": "  The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product\nGraph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the\nnetwork have the same collection of nodes. In addition, all layers can be\npartitioned into groups such that the layers in the same group are embedded in\nthe same ambient subspace but otherwise matrices of connection probabilities\ncan be all different. This setting includes majority of multilayer network\nmodels as its particular cases. The key task in this model is to recover the\ngroups of layers with unique subspace structures, since the case where all\nlayers of the network are embedded in the same subspace has been fairly well\nstudied. Until now, clustering of layers in such networks was based on the\nlayer-per-layer analysis, which required the multilayer network to be\nsufficiently dense. Nevertheless, in this paper we succeeded in pooling\ninformation in all layers together and providing a tensor-based methodology\nthat ensures perfect clustering for a much sparser network. Our theoretical\nresults, established under intuitive non-restrictive assumptions, assert that\nthe new technique achieves perfect clustering under sparsity conditions that,\nup to logarithmic factors, coincide with the computational lower bound derived\nfor a much simpler model.\n", "link": "http://arxiv.org/abs/2507.19423v1", "date": "2025-07-25", "relevancy": 1.8332, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4678}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perfect%20Clustering%20in%20Very%20Sparse%20Diverse%20Multiplex%20Networks&body=Title%3A%20Perfect%20Clustering%20in%20Very%20Sparse%20Diverse%20Multiplex%20Networks%0AAuthor%3A%20Marianna%20Pensky%0AAbstract%3A%20%20%20The%20paper%20studies%20the%20DIverse%20MultiPLEx%20Signed%20Generalized%20Random%20Dot%20Product%0AGraph%20%28DIMPLE-SGRDPG%29%20network%20model%20%28Pensky%20%282024%29%29%2C%20where%20all%20layers%20of%20the%0Anetwork%20have%20the%20same%20collection%20of%20nodes.%20In%20addition%2C%20all%20layers%20can%20be%0Apartitioned%20into%20groups%20such%20that%20the%20layers%20in%20the%20same%20group%20are%20embedded%20in%0Athe%20same%20ambient%20subspace%20but%20otherwise%20matrices%20of%20connection%20probabilities%0Acan%20be%20all%20different.%20This%20setting%20includes%20majority%20of%20multilayer%20network%0Amodels%20as%20its%20particular%20cases.%20The%20key%20task%20in%20this%20model%20is%20to%20recover%20the%0Agroups%20of%20layers%20with%20unique%20subspace%20structures%2C%20since%20the%20case%20where%20all%0Alayers%20of%20the%20network%20are%20embedded%20in%20the%20same%20subspace%20has%20been%20fairly%20well%0Astudied.%20Until%20now%2C%20clustering%20of%20layers%20in%20such%20networks%20was%20based%20on%20the%0Alayer-per-layer%20analysis%2C%20which%20required%20the%20multilayer%20network%20to%20be%0Asufficiently%20dense.%20Nevertheless%2C%20in%20this%20paper%20we%20succeeded%20in%20pooling%0Ainformation%20in%20all%20layers%20together%20and%20providing%20a%20tensor-based%20methodology%0Athat%20ensures%20perfect%20clustering%20for%20a%20much%20sparser%20network.%20Our%20theoretical%0Aresults%2C%20established%20under%20intuitive%20non-restrictive%20assumptions%2C%20assert%20that%0Athe%20new%20technique%20achieves%20perfect%20clustering%20under%20sparsity%20conditions%20that%2C%0Aup%20to%20logarithmic%20factors%2C%20coincide%20with%20the%20computational%20lower%20bound%20derived%0Afor%20a%20much%20simpler%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerfect%2520Clustering%2520in%2520Very%2520Sparse%2520Diverse%2520Multiplex%2520Networks%26entry.906535625%3DMarianna%2520Pensky%26entry.1292438233%3D%2520%2520The%2520paper%2520studies%2520the%2520DIverse%2520MultiPLEx%2520Signed%2520Generalized%2520Random%2520Dot%2520Product%250AGraph%2520%2528DIMPLE-SGRDPG%2529%2520network%2520model%2520%2528Pensky%2520%25282024%2529%2529%252C%2520where%2520all%2520layers%2520of%2520the%250Anetwork%2520have%2520the%2520same%2520collection%2520of%2520nodes.%2520In%2520addition%252C%2520all%2520layers%2520can%2520be%250Apartitioned%2520into%2520groups%2520such%2520that%2520the%2520layers%2520in%2520the%2520same%2520group%2520are%2520embedded%2520in%250Athe%2520same%2520ambient%2520subspace%2520but%2520otherwise%2520matrices%2520of%2520connection%2520probabilities%250Acan%2520be%2520all%2520different.%2520This%2520setting%2520includes%2520majority%2520of%2520multilayer%2520network%250Amodels%2520as%2520its%2520particular%2520cases.%2520The%2520key%2520task%2520in%2520this%2520model%2520is%2520to%2520recover%2520the%250Agroups%2520of%2520layers%2520with%2520unique%2520subspace%2520structures%252C%2520since%2520the%2520case%2520where%2520all%250Alayers%2520of%2520the%2520network%2520are%2520embedded%2520in%2520the%2520same%2520subspace%2520has%2520been%2520fairly%2520well%250Astudied.%2520Until%2520now%252C%2520clustering%2520of%2520layers%2520in%2520such%2520networks%2520was%2520based%2520on%2520the%250Alayer-per-layer%2520analysis%252C%2520which%2520required%2520the%2520multilayer%2520network%2520to%2520be%250Asufficiently%2520dense.%2520Nevertheless%252C%2520in%2520this%2520paper%2520we%2520succeeded%2520in%2520pooling%250Ainformation%2520in%2520all%2520layers%2520together%2520and%2520providing%2520a%2520tensor-based%2520methodology%250Athat%2520ensures%2520perfect%2520clustering%2520for%2520a%2520much%2520sparser%2520network.%2520Our%2520theoretical%250Aresults%252C%2520established%2520under%2520intuitive%2520non-restrictive%2520assumptions%252C%2520assert%2520that%250Athe%2520new%2520technique%2520achieves%2520perfect%2520clustering%2520under%2520sparsity%2520conditions%2520that%252C%250Aup%2520to%2520logarithmic%2520factors%252C%2520coincide%2520with%2520the%2520computational%2520lower%2520bound%2520derived%250Afor%2520a%2520much%2520simpler%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perfect%20Clustering%20in%20Very%20Sparse%20Diverse%20Multiplex%20Networks&entry.906535625=Marianna%20Pensky&entry.1292438233=%20%20The%20paper%20studies%20the%20DIverse%20MultiPLEx%20Signed%20Generalized%20Random%20Dot%20Product%0AGraph%20%28DIMPLE-SGRDPG%29%20network%20model%20%28Pensky%20%282024%29%29%2C%20where%20all%20layers%20of%20the%0Anetwork%20have%20the%20same%20collection%20of%20nodes.%20In%20addition%2C%20all%20layers%20can%20be%0Apartitioned%20into%20groups%20such%20that%20the%20layers%20in%20the%20same%20group%20are%20embedded%20in%0Athe%20same%20ambient%20subspace%20but%20otherwise%20matrices%20of%20connection%20probabilities%0Acan%20be%20all%20different.%20This%20setting%20includes%20majority%20of%20multilayer%20network%0Amodels%20as%20its%20particular%20cases.%20The%20key%20task%20in%20this%20model%20is%20to%20recover%20the%0Agroups%20of%20layers%20with%20unique%20subspace%20structures%2C%20since%20the%20case%20where%20all%0Alayers%20of%20the%20network%20are%20embedded%20in%20the%20same%20subspace%20has%20been%20fairly%20well%0Astudied.%20Until%20now%2C%20clustering%20of%20layers%20in%20such%20networks%20was%20based%20on%20the%0Alayer-per-layer%20analysis%2C%20which%20required%20the%20multilayer%20network%20to%20be%0Asufficiently%20dense.%20Nevertheless%2C%20in%20this%20paper%20we%20succeeded%20in%20pooling%0Ainformation%20in%20all%20layers%20together%20and%20providing%20a%20tensor-based%20methodology%0Athat%20ensures%20perfect%20clustering%20for%20a%20much%20sparser%20network.%20Our%20theoretical%0Aresults%2C%20established%20under%20intuitive%20non-restrictive%20assumptions%2C%20assert%20that%0Athe%20new%20technique%20achieves%20perfect%20clustering%20under%20sparsity%20conditions%20that%2C%0Aup%20to%20logarithmic%20factors%2C%20coincide%20with%20the%20computational%20lower%20bound%20derived%0Afor%20a%20much%20simpler%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19423v1&entry.124074799=Read"},
{"title": "ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding", "author": "Arindam Ghosh and Mark Fuhs and Bongjun Kim and Anurag Chowdhury and Monika Woszczyna", "abstract": "  From an application standpoint, speaker-role diarization (RD), such as doctor\nvs. patient, host vs. guest, etc. is often more useful than traditional speaker\ndiarization (SD), which assigns generic labels like speaker-1, speaker-2 etc.\nIn the context of joint automatic speech recognition (ASR) + SD (who spoke\nwhat?), recent end-to-end models employ an auxiliary SD transducer,\nsynchronized with the ASR transducer, to predict speakers per word. In this\npaper, we extend this framework to RD with three key contributions: (1) we\nsimplify the training via forced alignment and cross-entropy loss instead of\nRNNT loss, (2) we show that word prediction and role prediction require\ndifferent amounts of predictor's context, leading to separate task-specific\npredictors, unlike existing shared-predictor models, and (3) we propose a way\nto leverage RD posterior activity to influence ASR decoding and reduce\nsmall-word deletion errors.\n", "link": "http://arxiv.org/abs/2507.17765v2", "date": "2025-07-25", "relevancy": 1.8135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASR-Guided%20Speaker-Role%20Diarization%20and%20Diarization-Guided%20ASR%20Decoding&body=Title%3A%20ASR-Guided%20Speaker-Role%20Diarization%20and%20Diarization-Guided%20ASR%20Decoding%0AAuthor%3A%20Arindam%20Ghosh%20and%20Mark%20Fuhs%20and%20Bongjun%20Kim%20and%20Anurag%20Chowdhury%20and%20Monika%20Woszczyna%0AAbstract%3A%20%20%20From%20an%20application%20standpoint%2C%20speaker-role%20diarization%20%28RD%29%2C%20such%20as%20doctor%0Avs.%20patient%2C%20host%20vs.%20guest%2C%20etc.%20is%20often%20more%20useful%20than%20traditional%20speaker%0Adiarization%20%28SD%29%2C%20which%20assigns%20generic%20labels%20like%20speaker-1%2C%20speaker-2%20etc.%0AIn%20the%20context%20of%20joint%20automatic%20speech%20recognition%20%28ASR%29%20%2B%20SD%20%28who%20spoke%0Awhat%3F%29%2C%20recent%20end-to-end%20models%20employ%20an%20auxiliary%20SD%20transducer%2C%0Asynchronized%20with%20the%20ASR%20transducer%2C%20to%20predict%20speakers%20per%20word.%20In%20this%0Apaper%2C%20we%20extend%20this%20framework%20to%20RD%20with%20three%20key%20contributions%3A%20%281%29%20we%0Asimplify%20the%20training%20via%20forced%20alignment%20and%20cross-entropy%20loss%20instead%20of%0ARNNT%20loss%2C%20%282%29%20we%20show%20that%20word%20prediction%20and%20role%20prediction%20require%0Adifferent%20amounts%20of%20predictor%27s%20context%2C%20leading%20to%20separate%20task-specific%0Apredictors%2C%20unlike%20existing%20shared-predictor%20models%2C%20and%20%283%29%20we%20propose%20a%20way%0Ato%20leverage%20RD%20posterior%20activity%20to%20influence%20ASR%20decoding%20and%20reduce%0Asmall-word%20deletion%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASR-Guided%2520Speaker-Role%2520Diarization%2520and%2520Diarization-Guided%2520ASR%2520Decoding%26entry.906535625%3DArindam%2520Ghosh%2520and%2520Mark%2520Fuhs%2520and%2520Bongjun%2520Kim%2520and%2520Anurag%2520Chowdhury%2520and%2520Monika%2520Woszczyna%26entry.1292438233%3D%2520%2520From%2520an%2520application%2520standpoint%252C%2520speaker-role%2520diarization%2520%2528RD%2529%252C%2520such%2520as%2520doctor%250Avs.%2520patient%252C%2520host%2520vs.%2520guest%252C%2520etc.%2520is%2520often%2520more%2520useful%2520than%2520traditional%2520speaker%250Adiarization%2520%2528SD%2529%252C%2520which%2520assigns%2520generic%2520labels%2520like%2520speaker-1%252C%2520speaker-2%2520etc.%250AIn%2520the%2520context%2520of%2520joint%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520%252B%2520SD%2520%2528who%2520spoke%250Awhat%253F%2529%252C%2520recent%2520end-to-end%2520models%2520employ%2520an%2520auxiliary%2520SD%2520transducer%252C%250Asynchronized%2520with%2520the%2520ASR%2520transducer%252C%2520to%2520predict%2520speakers%2520per%2520word.%2520In%2520this%250Apaper%252C%2520we%2520extend%2520this%2520framework%2520to%2520RD%2520with%2520three%2520key%2520contributions%253A%2520%25281%2529%2520we%250Asimplify%2520the%2520training%2520via%2520forced%2520alignment%2520and%2520cross-entropy%2520loss%2520instead%2520of%250ARNNT%2520loss%252C%2520%25282%2529%2520we%2520show%2520that%2520word%2520prediction%2520and%2520role%2520prediction%2520require%250Adifferent%2520amounts%2520of%2520predictor%2527s%2520context%252C%2520leading%2520to%2520separate%2520task-specific%250Apredictors%252C%2520unlike%2520existing%2520shared-predictor%2520models%252C%2520and%2520%25283%2529%2520we%2520propose%2520a%2520way%250Ato%2520leverage%2520RD%2520posterior%2520activity%2520to%2520influence%2520ASR%2520decoding%2520and%2520reduce%250Asmall-word%2520deletion%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASR-Guided%20Speaker-Role%20Diarization%20and%20Diarization-Guided%20ASR%20Decoding&entry.906535625=Arindam%20Ghosh%20and%20Mark%20Fuhs%20and%20Bongjun%20Kim%20and%20Anurag%20Chowdhury%20and%20Monika%20Woszczyna&entry.1292438233=%20%20From%20an%20application%20standpoint%2C%20speaker-role%20diarization%20%28RD%29%2C%20such%20as%20doctor%0Avs.%20patient%2C%20host%20vs.%20guest%2C%20etc.%20is%20often%20more%20useful%20than%20traditional%20speaker%0Adiarization%20%28SD%29%2C%20which%20assigns%20generic%20labels%20like%20speaker-1%2C%20speaker-2%20etc.%0AIn%20the%20context%20of%20joint%20automatic%20speech%20recognition%20%28ASR%29%20%2B%20SD%20%28who%20spoke%0Awhat%3F%29%2C%20recent%20end-to-end%20models%20employ%20an%20auxiliary%20SD%20transducer%2C%0Asynchronized%20with%20the%20ASR%20transducer%2C%20to%20predict%20speakers%20per%20word.%20In%20this%0Apaper%2C%20we%20extend%20this%20framework%20to%20RD%20with%20three%20key%20contributions%3A%20%281%29%20we%0Asimplify%20the%20training%20via%20forced%20alignment%20and%20cross-entropy%20loss%20instead%20of%0ARNNT%20loss%2C%20%282%29%20we%20show%20that%20word%20prediction%20and%20role%20prediction%20require%0Adifferent%20amounts%20of%20predictor%27s%20context%2C%20leading%20to%20separate%20task-specific%0Apredictors%2C%20unlike%20existing%20shared-predictor%20models%2C%20and%20%283%29%20we%20propose%20a%20way%0Ato%20leverage%20RD%20posterior%20activity%20to%20influence%20ASR%20decoding%20and%20reduce%0Asmall-word%20deletion%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17765v2&entry.124074799=Read"},
{"title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale", "author": "Daniel Goldstein and Eric Alcaide and Janna Lu and Eugene Cheah", "abstract": "  We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper\n", "link": "http://arxiv.org/abs/2505.03005v3", "date": "2025-07-25", "relevancy": 1.7537, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5964}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5701}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RADLADS%3A%20Rapid%20Attention%20Distillation%20to%20Linear%20Attention%20Decoders%20at%0A%20%20Scale&body=Title%3A%20RADLADS%3A%20Rapid%20Attention%20Distillation%20to%20Linear%20Attention%20Decoders%20at%0A%20%20Scale%0AAuthor%3A%20Daniel%20Goldstein%20and%20Eric%20Alcaide%20and%20Janna%20Lu%20and%20Eugene%20Cheah%0AAbstract%3A%20%20%20We%20present%20Rapid%20Attention%20Distillation%20to%20Linear%20Attention%20Decoders%20at%20Scale%0A%28RADLADS%29%2C%20a%20protocol%20for%20rapidly%20converting%20softmax%20attention%20transformers%0Ainto%20linear%20attention%20decoder%20models%2C%20along%20with%20two%20new%20RWKV-variant%0Aarchitectures%2C%20and%20models%20converted%20from%20popular%20Qwen2.5%20open%20source%20models%20in%0A7B%2C%2032B%2C%20and%2072B%20sizes.%20Our%20conversion%20process%20requires%20only%20350-700M%20tokens%2C%0Aless%20than%200.005%25%20of%20the%20token%20count%20used%20to%20train%20the%20original%20teacher%20models.%0AConverting%20to%20our%2072B%20linear%20attention%20model%20costs%20less%20than%20%5C%242%2C000%20USD%20at%0Atoday%27s%20prices%2C%20yet%20quality%20at%20inference%20remains%20close%20to%20the%20original%0Atransformer.%20These%20models%20achieve%20state-of-the-art%20downstream%20performance%0Aacross%20a%20set%20of%20standard%20benchmarks%20for%20linear%20attention%20models%20of%20their%20size.%0AWe%20release%20all%20our%20models%20on%20HuggingFace%20under%20the%20Apache%202.0%20license%2C%20with%20the%0Aexception%20of%20our%2072B%20models%20which%20are%20also%20governed%20by%20the%20Qwen%20License%0AAgreement.%0A%20%20Models%20at%0Ahttps%3A//huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102%0ATraining%20Code%20at%20https%3A//github.com/recursal/RADLADS-paper%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03005v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRADLADS%253A%2520Rapid%2520Attention%2520Distillation%2520to%2520Linear%2520Attention%2520Decoders%2520at%250A%2520%2520Scale%26entry.906535625%3DDaniel%2520Goldstein%2520and%2520Eric%2520Alcaide%2520and%2520Janna%2520Lu%2520and%2520Eugene%2520Cheah%26entry.1292438233%3D%2520%2520We%2520present%2520Rapid%2520Attention%2520Distillation%2520to%2520Linear%2520Attention%2520Decoders%2520at%2520Scale%250A%2528RADLADS%2529%252C%2520a%2520protocol%2520for%2520rapidly%2520converting%2520softmax%2520attention%2520transformers%250Ainto%2520linear%2520attention%2520decoder%2520models%252C%2520along%2520with%2520two%2520new%2520RWKV-variant%250Aarchitectures%252C%2520and%2520models%2520converted%2520from%2520popular%2520Qwen2.5%2520open%2520source%2520models%2520in%250A7B%252C%252032B%252C%2520and%252072B%2520sizes.%2520Our%2520conversion%2520process%2520requires%2520only%2520350-700M%2520tokens%252C%250Aless%2520than%25200.005%2525%2520of%2520the%2520token%2520count%2520used%2520to%2520train%2520the%2520original%2520teacher%2520models.%250AConverting%2520to%2520our%252072B%2520linear%2520attention%2520model%2520costs%2520less%2520than%2520%255C%25242%252C000%2520USD%2520at%250Atoday%2527s%2520prices%252C%2520yet%2520quality%2520at%2520inference%2520remains%2520close%2520to%2520the%2520original%250Atransformer.%2520These%2520models%2520achieve%2520state-of-the-art%2520downstream%2520performance%250Aacross%2520a%2520set%2520of%2520standard%2520benchmarks%2520for%2520linear%2520attention%2520models%2520of%2520their%2520size.%250AWe%2520release%2520all%2520our%2520models%2520on%2520HuggingFace%2520under%2520the%2520Apache%25202.0%2520license%252C%2520with%2520the%250Aexception%2520of%2520our%252072B%2520models%2520which%2520are%2520also%2520governed%2520by%2520the%2520Qwen%2520License%250AAgreement.%250A%2520%2520Models%2520at%250Ahttps%253A//huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102%250ATraining%2520Code%2520at%2520https%253A//github.com/recursal/RADLADS-paper%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03005v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RADLADS%3A%20Rapid%20Attention%20Distillation%20to%20Linear%20Attention%20Decoders%20at%0A%20%20Scale&entry.906535625=Daniel%20Goldstein%20and%20Eric%20Alcaide%20and%20Janna%20Lu%20and%20Eugene%20Cheah&entry.1292438233=%20%20We%20present%20Rapid%20Attention%20Distillation%20to%20Linear%20Attention%20Decoders%20at%20Scale%0A%28RADLADS%29%2C%20a%20protocol%20for%20rapidly%20converting%20softmax%20attention%20transformers%0Ainto%20linear%20attention%20decoder%20models%2C%20along%20with%20two%20new%20RWKV-variant%0Aarchitectures%2C%20and%20models%20converted%20from%20popular%20Qwen2.5%20open%20source%20models%20in%0A7B%2C%2032B%2C%20and%2072B%20sizes.%20Our%20conversion%20process%20requires%20only%20350-700M%20tokens%2C%0Aless%20than%200.005%25%20of%20the%20token%20count%20used%20to%20train%20the%20original%20teacher%20models.%0AConverting%20to%20our%2072B%20linear%20attention%20model%20costs%20less%20than%20%5C%242%2C000%20USD%20at%0Atoday%27s%20prices%2C%20yet%20quality%20at%20inference%20remains%20close%20to%20the%20original%0Atransformer.%20These%20models%20achieve%20state-of-the-art%20downstream%20performance%0Aacross%20a%20set%20of%20standard%20benchmarks%20for%20linear%20attention%20models%20of%20their%20size.%0AWe%20release%20all%20our%20models%20on%20HuggingFace%20under%20the%20Apache%202.0%20license%2C%20with%20the%0Aexception%20of%20our%2072B%20models%20which%20are%20also%20governed%20by%20the%20Qwen%20License%0AAgreement.%0A%20%20Models%20at%0Ahttps%3A//huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102%0ATraining%20Code%20at%20https%3A//github.com/recursal/RADLADS-paper%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03005v3&entry.124074799=Read"},
{"title": "Data Augmentation for Spoken Grammatical Error Correction", "author": "Penny Karanasou and Mengjie Qian and Stefano Bann\u00f2 and Mark J. F. Gales and Kate M. Knill", "abstract": "  While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations.\n", "link": "http://arxiv.org/abs/2507.19374v1", "date": "2025-07-25", "relevancy": 1.7412, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4322}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Spoken%20Grammatical%20Error%20Correction&body=Title%3A%20Data%20Augmentation%20for%20Spoken%20Grammatical%20Error%20Correction%0AAuthor%3A%20Penny%20Karanasou%20and%20Mengjie%20Qian%20and%20Stefano%20Bann%C3%B2%20and%20Mark%20J.%20F.%20Gales%20and%20Kate%20M.%20Knill%0AAbstract%3A%20%20%20While%20there%20exist%20strong%20benchmark%20datasets%20for%20grammatical%20error%20correction%0A%28GEC%29%2C%20high-quality%20annotated%20spoken%20datasets%20for%20Spoken%20GEC%20%28SGEC%29%20are%20still%0Aunder-resourced.%20In%20this%20paper%2C%20we%20propose%20a%20fully%20automated%20method%20to%20generate%0Aaudio-text%20pairs%20with%20grammatical%20errors%20and%20disfluencies.%20Moreover%2C%20we%20propose%0Aa%20series%20of%20objective%20metrics%20that%20can%20be%20used%20to%20evaluate%20the%20generated%20data%0Aand%20choose%20the%20more%20suitable%20dataset%20for%20SGEC.%20The%20goal%20is%20to%20generate%20an%0Aaugmented%20dataset%20that%20maintains%20the%20textual%20and%20acoustic%20characteristics%20of%0Athe%20original%20data%20while%20providing%20new%20types%20of%20errors.%20This%20augmented%20dataset%0Ashould%20augment%20and%20enrich%20the%20original%20corpus%20without%20altering%20the%20language%0Aassessment%20scores%20of%20the%20second%20language%20%28L2%29%20learners.%20We%20evaluate%20the%20use%20of%0Athe%20augmented%20corpus%20both%20for%20written%20GEC%20%28the%20text%20part%29%20and%20for%20SGEC%20%28the%0Aaudio-text%20pairs%29.%20Our%20experiments%20are%20conducted%20on%20the%20S%5C%26I%20Corpus%2C%20the%20first%0Apublicly%20available%20speech%20dataset%20with%20grammar%20error%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520for%2520Spoken%2520Grammatical%2520Error%2520Correction%26entry.906535625%3DPenny%2520Karanasou%2520and%2520Mengjie%2520Qian%2520and%2520Stefano%2520Bann%25C3%25B2%2520and%2520Mark%2520J.%2520F.%2520Gales%2520and%2520Kate%2520M.%2520Knill%26entry.1292438233%3D%2520%2520While%2520there%2520exist%2520strong%2520benchmark%2520datasets%2520for%2520grammatical%2520error%2520correction%250A%2528GEC%2529%252C%2520high-quality%2520annotated%2520spoken%2520datasets%2520for%2520Spoken%2520GEC%2520%2528SGEC%2529%2520are%2520still%250Aunder-resourced.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520fully%2520automated%2520method%2520to%2520generate%250Aaudio-text%2520pairs%2520with%2520grammatical%2520errors%2520and%2520disfluencies.%2520Moreover%252C%2520we%2520propose%250Aa%2520series%2520of%2520objective%2520metrics%2520that%2520can%2520be%2520used%2520to%2520evaluate%2520the%2520generated%2520data%250Aand%2520choose%2520the%2520more%2520suitable%2520dataset%2520for%2520SGEC.%2520The%2520goal%2520is%2520to%2520generate%2520an%250Aaugmented%2520dataset%2520that%2520maintains%2520the%2520textual%2520and%2520acoustic%2520characteristics%2520of%250Athe%2520original%2520data%2520while%2520providing%2520new%2520types%2520of%2520errors.%2520This%2520augmented%2520dataset%250Ashould%2520augment%2520and%2520enrich%2520the%2520original%2520corpus%2520without%2520altering%2520the%2520language%250Aassessment%2520scores%2520of%2520the%2520second%2520language%2520%2528L2%2529%2520learners.%2520We%2520evaluate%2520the%2520use%2520of%250Athe%2520augmented%2520corpus%2520both%2520for%2520written%2520GEC%2520%2528the%2520text%2520part%2529%2520and%2520for%2520SGEC%2520%2528the%250Aaudio-text%2520pairs%2529.%2520Our%2520experiments%2520are%2520conducted%2520on%2520the%2520S%255C%2526I%2520Corpus%252C%2520the%2520first%250Apublicly%2520available%2520speech%2520dataset%2520with%2520grammar%2520error%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Spoken%20Grammatical%20Error%20Correction&entry.906535625=Penny%20Karanasou%20and%20Mengjie%20Qian%20and%20Stefano%20Bann%C3%B2%20and%20Mark%20J.%20F.%20Gales%20and%20Kate%20M.%20Knill&entry.1292438233=%20%20While%20there%20exist%20strong%20benchmark%20datasets%20for%20grammatical%20error%20correction%0A%28GEC%29%2C%20high-quality%20annotated%20spoken%20datasets%20for%20Spoken%20GEC%20%28SGEC%29%20are%20still%0Aunder-resourced.%20In%20this%20paper%2C%20we%20propose%20a%20fully%20automated%20method%20to%20generate%0Aaudio-text%20pairs%20with%20grammatical%20errors%20and%20disfluencies.%20Moreover%2C%20we%20propose%0Aa%20series%20of%20objective%20metrics%20that%20can%20be%20used%20to%20evaluate%20the%20generated%20data%0Aand%20choose%20the%20more%20suitable%20dataset%20for%20SGEC.%20The%20goal%20is%20to%20generate%20an%0Aaugmented%20dataset%20that%20maintains%20the%20textual%20and%20acoustic%20characteristics%20of%0Athe%20original%20data%20while%20providing%20new%20types%20of%20errors.%20This%20augmented%20dataset%0Ashould%20augment%20and%20enrich%20the%20original%20corpus%20without%20altering%20the%20language%0Aassessment%20scores%20of%20the%20second%20language%20%28L2%29%20learners.%20We%20evaluate%20the%20use%20of%0Athe%20augmented%20corpus%20both%20for%20written%20GEC%20%28the%20text%20part%29%20and%20for%20SGEC%20%28the%0Aaudio-text%20pairs%29.%20Our%20experiments%20are%20conducted%20on%20the%20S%5C%26I%20Corpus%2C%20the%20first%0Apublicly%20available%20speech%20dataset%20with%20grammar%20error%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19374v1&entry.124074799=Read"},
{"title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security", "author": "Gabriel Chua", "abstract": "  As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.\n", "link": "http://arxiv.org/abs/2507.19399v1", "date": "2025-07-25", "relevancy": 1.7321, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Running%20in%20CIRCLE%3F%20A%20Simple%20Benchmark%20for%20LLM%20Code%20Interpreter%20Security&body=Title%3A%20Running%20in%20CIRCLE%3F%20A%20Simple%20Benchmark%20for%20LLM%20Code%20Interpreter%20Security%0AAuthor%3A%20Gabriel%20Chua%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20integrate%20native%20code%0Ainterpreters%2C%20they%20enable%20powerful%20real-time%20execution%20capabilities%2C%0Asubstantially%20expanding%20their%20utility.%20However%2C%20such%20integrations%20introduce%0Apotential%20system-level%20cybersecurity%20threats%2C%20fundamentally%20different%20from%0Aprompt-based%20vulnerabilities.%20To%20systematically%20evaluate%20these%0Ainterpreter-specific%20risks%2C%20we%20propose%20CIRCLE%20%28Code-Interpreter%20Resilience%0ACheck%20for%20LLM%20Exploits%29%2C%20a%20simple%20benchmark%20comprising%201%2C260%20prompts%20targeting%0ACPU%2C%20memory%2C%20and%20disk%20resource%20exhaustion.%20Each%20risk%20category%20includes%0Aexplicitly%20malicious%20%28%22direct%22%29%20and%20plausibly%20benign%20%28%22indirect%22%29%20prompt%0Avariants.%20Our%20automated%20evaluation%20framework%20assesses%20not%20only%20whether%20LLMs%0Arefuse%20or%20generates%20risky%20code%2C%20but%20also%20executes%20the%20generated%20code%20within%20the%0Ainterpreter%20environment%20to%20evaluate%20code%20correctness%2C%20simplifications%20made%20by%0Athe%20LLM%20to%20make%20the%20code%20safe%2C%20or%20execution%20timeouts.%20Evaluating%207%20commercially%0Aavailable%20models%20from%20OpenAI%20and%20Google%2C%20we%20uncover%20significant%20and%0Ainconsistent%20vulnerabilities.%20For%20instance%2C%20evaluations%20show%20substantial%0Adisparities%20even%20within%20providers%20-%20OpenAI%27s%20o4-mini%20correctly%20refuses%20risky%0Arequests%20at%207.1%25%2C%20notably%20higher%20rates%20compared%20to%20GPT-4.1%20at%200.5%25.%20Results%0Aparticularly%20underscore%20that%20indirect%2C%20socially-engineered%20prompts%0Asubstantially%20weaken%20model%20defenses.%20This%20highlights%20an%20urgent%20need%20for%0Ainterpreter-specific%20cybersecurity%20benchmarks%2C%20dedicated%20mitigation%20tools%0A%28e.g.%2C%20guardrails%29%2C%20and%20clear%20industry%20standards%20to%20guide%20safe%20and%20responsible%0Adeployment%20of%20LLM%20interpreter%20integrations.%20The%20benchmark%20dataset%20and%0Aevaluation%20code%20are%20publicly%20released%20to%20foster%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRunning%2520in%2520CIRCLE%253F%2520A%2520Simple%2520Benchmark%2520for%2520LLM%2520Code%2520Interpreter%2520Security%26entry.906535625%3DGabriel%2520Chua%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520integrate%2520native%2520code%250Ainterpreters%252C%2520they%2520enable%2520powerful%2520real-time%2520execution%2520capabilities%252C%250Asubstantially%2520expanding%2520their%2520utility.%2520However%252C%2520such%2520integrations%2520introduce%250Apotential%2520system-level%2520cybersecurity%2520threats%252C%2520fundamentally%2520different%2520from%250Aprompt-based%2520vulnerabilities.%2520To%2520systematically%2520evaluate%2520these%250Ainterpreter-specific%2520risks%252C%2520we%2520propose%2520CIRCLE%2520%2528Code-Interpreter%2520Resilience%250ACheck%2520for%2520LLM%2520Exploits%2529%252C%2520a%2520simple%2520benchmark%2520comprising%25201%252C260%2520prompts%2520targeting%250ACPU%252C%2520memory%252C%2520and%2520disk%2520resource%2520exhaustion.%2520Each%2520risk%2520category%2520includes%250Aexplicitly%2520malicious%2520%2528%2522direct%2522%2529%2520and%2520plausibly%2520benign%2520%2528%2522indirect%2522%2529%2520prompt%250Avariants.%2520Our%2520automated%2520evaluation%2520framework%2520assesses%2520not%2520only%2520whether%2520LLMs%250Arefuse%2520or%2520generates%2520risky%2520code%252C%2520but%2520also%2520executes%2520the%2520generated%2520code%2520within%2520the%250Ainterpreter%2520environment%2520to%2520evaluate%2520code%2520correctness%252C%2520simplifications%2520made%2520by%250Athe%2520LLM%2520to%2520make%2520the%2520code%2520safe%252C%2520or%2520execution%2520timeouts.%2520Evaluating%25207%2520commercially%250Aavailable%2520models%2520from%2520OpenAI%2520and%2520Google%252C%2520we%2520uncover%2520significant%2520and%250Ainconsistent%2520vulnerabilities.%2520For%2520instance%252C%2520evaluations%2520show%2520substantial%250Adisparities%2520even%2520within%2520providers%2520-%2520OpenAI%2527s%2520o4-mini%2520correctly%2520refuses%2520risky%250Arequests%2520at%25207.1%2525%252C%2520notably%2520higher%2520rates%2520compared%2520to%2520GPT-4.1%2520at%25200.5%2525.%2520Results%250Aparticularly%2520underscore%2520that%2520indirect%252C%2520socially-engineered%2520prompts%250Asubstantially%2520weaken%2520model%2520defenses.%2520This%2520highlights%2520an%2520urgent%2520need%2520for%250Ainterpreter-specific%2520cybersecurity%2520benchmarks%252C%2520dedicated%2520mitigation%2520tools%250A%2528e.g.%252C%2520guardrails%2529%252C%2520and%2520clear%2520industry%2520standards%2520to%2520guide%2520safe%2520and%2520responsible%250Adeployment%2520of%2520LLM%2520interpreter%2520integrations.%2520The%2520benchmark%2520dataset%2520and%250Aevaluation%2520code%2520are%2520publicly%2520released%2520to%2520foster%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Running%20in%20CIRCLE%3F%20A%20Simple%20Benchmark%20for%20LLM%20Code%20Interpreter%20Security&entry.906535625=Gabriel%20Chua&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20increasingly%20integrate%20native%20code%0Ainterpreters%2C%20they%20enable%20powerful%20real-time%20execution%20capabilities%2C%0Asubstantially%20expanding%20their%20utility.%20However%2C%20such%20integrations%20introduce%0Apotential%20system-level%20cybersecurity%20threats%2C%20fundamentally%20different%20from%0Aprompt-based%20vulnerabilities.%20To%20systematically%20evaluate%20these%0Ainterpreter-specific%20risks%2C%20we%20propose%20CIRCLE%20%28Code-Interpreter%20Resilience%0ACheck%20for%20LLM%20Exploits%29%2C%20a%20simple%20benchmark%20comprising%201%2C260%20prompts%20targeting%0ACPU%2C%20memory%2C%20and%20disk%20resource%20exhaustion.%20Each%20risk%20category%20includes%0Aexplicitly%20malicious%20%28%22direct%22%29%20and%20plausibly%20benign%20%28%22indirect%22%29%20prompt%0Avariants.%20Our%20automated%20evaluation%20framework%20assesses%20not%20only%20whether%20LLMs%0Arefuse%20or%20generates%20risky%20code%2C%20but%20also%20executes%20the%20generated%20code%20within%20the%0Ainterpreter%20environment%20to%20evaluate%20code%20correctness%2C%20simplifications%20made%20by%0Athe%20LLM%20to%20make%20the%20code%20safe%2C%20or%20execution%20timeouts.%20Evaluating%207%20commercially%0Aavailable%20models%20from%20OpenAI%20and%20Google%2C%20we%20uncover%20significant%20and%0Ainconsistent%20vulnerabilities.%20For%20instance%2C%20evaluations%20show%20substantial%0Adisparities%20even%20within%20providers%20-%20OpenAI%27s%20o4-mini%20correctly%20refuses%20risky%0Arequests%20at%207.1%25%2C%20notably%20higher%20rates%20compared%20to%20GPT-4.1%20at%200.5%25.%20Results%0Aparticularly%20underscore%20that%20indirect%2C%20socially-engineered%20prompts%0Asubstantially%20weaken%20model%20defenses.%20This%20highlights%20an%20urgent%20need%20for%0Ainterpreter-specific%20cybersecurity%20benchmarks%2C%20dedicated%20mitigation%20tools%0A%28e.g.%2C%20guardrails%29%2C%20and%20clear%20industry%20standards%20to%20guide%20safe%20and%20responsible%0Adeployment%20of%20LLM%20interpreter%20integrations.%20The%20benchmark%20dataset%20and%0Aevaluation%20code%20are%20publicly%20released%20to%20foster%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19399v1&entry.124074799=Read"},
{"title": "Linearly Convergent Algorithms for Nonsmooth Problems with Unknown\n  Smooth Pieces", "author": "Zhe Zhang and Suvrit Sra", "abstract": "  We develop efficient algorithms for optimizing piecewise smooth (PWS)\nfunctions where the underlying partition of the domain into smooth pieces is\n\\emph{unknown}. For PWS functions satisfying a quadratic growth (QG) condition,\nwe propose a bundle-level (BL) type method that achieves global linear\nconvergence -- to our knowledge, the first such result for any algorithm for\nthis problem class. We extend this method to handle approximately PWS functions\nand to solve weakly-convex PWS problems, improving the state-of-the-art\ncomplexity to match the benchmark for smooth non-convex optimization.\nFurthermore, we introduce the first verifiable and accurate termination\ncriterion for PWS optimization. Similar to the gradient norm in smooth\noptimization, this certificate tightly characterizes the optimality gap under\nthe QG condition, and can moreover be evaluated without knowledge of any\nproblem parameters. We develop a search subroutine for this certificate and\nembed it within a guess-and-check framework, resulting in an almost\nparameter-free algorithm for both the convex QG and weakly-convex settings.\n", "link": "http://arxiv.org/abs/2507.19465v1", "date": "2025-07-25", "relevancy": 1.7139, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4154}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linearly%20Convergent%20Algorithms%20for%20Nonsmooth%20Problems%20with%20Unknown%0A%20%20Smooth%20Pieces&body=Title%3A%20Linearly%20Convergent%20Algorithms%20for%20Nonsmooth%20Problems%20with%20Unknown%0A%20%20Smooth%20Pieces%0AAuthor%3A%20Zhe%20Zhang%20and%20Suvrit%20Sra%0AAbstract%3A%20%20%20We%20develop%20efficient%20algorithms%20for%20optimizing%20piecewise%20smooth%20%28PWS%29%0Afunctions%20where%20the%20underlying%20partition%20of%20the%20domain%20into%20smooth%20pieces%20is%0A%5Cemph%7Bunknown%7D.%20For%20PWS%20functions%20satisfying%20a%20quadratic%20growth%20%28QG%29%20condition%2C%0Awe%20propose%20a%20bundle-level%20%28BL%29%20type%20method%20that%20achieves%20global%20linear%0Aconvergence%20--%20to%20our%20knowledge%2C%20the%20first%20such%20result%20for%20any%20algorithm%20for%0Athis%20problem%20class.%20We%20extend%20this%20method%20to%20handle%20approximately%20PWS%20functions%0Aand%20to%20solve%20weakly-convex%20PWS%20problems%2C%20improving%20the%20state-of-the-art%0Acomplexity%20to%20match%20the%20benchmark%20for%20smooth%20non-convex%20optimization.%0AFurthermore%2C%20we%20introduce%20the%20first%20verifiable%20and%20accurate%20termination%0Acriterion%20for%20PWS%20optimization.%20Similar%20to%20the%20gradient%20norm%20in%20smooth%0Aoptimization%2C%20this%20certificate%20tightly%20characterizes%20the%20optimality%20gap%20under%0Athe%20QG%20condition%2C%20and%20can%20moreover%20be%20evaluated%20without%20knowledge%20of%20any%0Aproblem%20parameters.%20We%20develop%20a%20search%20subroutine%20for%20this%20certificate%20and%0Aembed%20it%20within%20a%20guess-and-check%20framework%2C%20resulting%20in%20an%20almost%0Aparameter-free%20algorithm%20for%20both%20the%20convex%20QG%20and%20weakly-convex%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinearly%2520Convergent%2520Algorithms%2520for%2520Nonsmooth%2520Problems%2520with%2520Unknown%250A%2520%2520Smooth%2520Pieces%26entry.906535625%3DZhe%2520Zhang%2520and%2520Suvrit%2520Sra%26entry.1292438233%3D%2520%2520We%2520develop%2520efficient%2520algorithms%2520for%2520optimizing%2520piecewise%2520smooth%2520%2528PWS%2529%250Afunctions%2520where%2520the%2520underlying%2520partition%2520of%2520the%2520domain%2520into%2520smooth%2520pieces%2520is%250A%255Cemph%257Bunknown%257D.%2520For%2520PWS%2520functions%2520satisfying%2520a%2520quadratic%2520growth%2520%2528QG%2529%2520condition%252C%250Awe%2520propose%2520a%2520bundle-level%2520%2528BL%2529%2520type%2520method%2520that%2520achieves%2520global%2520linear%250Aconvergence%2520--%2520to%2520our%2520knowledge%252C%2520the%2520first%2520such%2520result%2520for%2520any%2520algorithm%2520for%250Athis%2520problem%2520class.%2520We%2520extend%2520this%2520method%2520to%2520handle%2520approximately%2520PWS%2520functions%250Aand%2520to%2520solve%2520weakly-convex%2520PWS%2520problems%252C%2520improving%2520the%2520state-of-the-art%250Acomplexity%2520to%2520match%2520the%2520benchmark%2520for%2520smooth%2520non-convex%2520optimization.%250AFurthermore%252C%2520we%2520introduce%2520the%2520first%2520verifiable%2520and%2520accurate%2520termination%250Acriterion%2520for%2520PWS%2520optimization.%2520Similar%2520to%2520the%2520gradient%2520norm%2520in%2520smooth%250Aoptimization%252C%2520this%2520certificate%2520tightly%2520characterizes%2520the%2520optimality%2520gap%2520under%250Athe%2520QG%2520condition%252C%2520and%2520can%2520moreover%2520be%2520evaluated%2520without%2520knowledge%2520of%2520any%250Aproblem%2520parameters.%2520We%2520develop%2520a%2520search%2520subroutine%2520for%2520this%2520certificate%2520and%250Aembed%2520it%2520within%2520a%2520guess-and-check%2520framework%252C%2520resulting%2520in%2520an%2520almost%250Aparameter-free%2520algorithm%2520for%2520both%2520the%2520convex%2520QG%2520and%2520weakly-convex%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linearly%20Convergent%20Algorithms%20for%20Nonsmooth%20Problems%20with%20Unknown%0A%20%20Smooth%20Pieces&entry.906535625=Zhe%20Zhang%20and%20Suvrit%20Sra&entry.1292438233=%20%20We%20develop%20efficient%20algorithms%20for%20optimizing%20piecewise%20smooth%20%28PWS%29%0Afunctions%20where%20the%20underlying%20partition%20of%20the%20domain%20into%20smooth%20pieces%20is%0A%5Cemph%7Bunknown%7D.%20For%20PWS%20functions%20satisfying%20a%20quadratic%20growth%20%28QG%29%20condition%2C%0Awe%20propose%20a%20bundle-level%20%28BL%29%20type%20method%20that%20achieves%20global%20linear%0Aconvergence%20--%20to%20our%20knowledge%2C%20the%20first%20such%20result%20for%20any%20algorithm%20for%0Athis%20problem%20class.%20We%20extend%20this%20method%20to%20handle%20approximately%20PWS%20functions%0Aand%20to%20solve%20weakly-convex%20PWS%20problems%2C%20improving%20the%20state-of-the-art%0Acomplexity%20to%20match%20the%20benchmark%20for%20smooth%20non-convex%20optimization.%0AFurthermore%2C%20we%20introduce%20the%20first%20verifiable%20and%20accurate%20termination%0Acriterion%20for%20PWS%20optimization.%20Similar%20to%20the%20gradient%20norm%20in%20smooth%0Aoptimization%2C%20this%20certificate%20tightly%20characterizes%20the%20optimality%20gap%20under%0Athe%20QG%20condition%2C%20and%20can%20moreover%20be%20evaluated%20without%20knowledge%20of%20any%0Aproblem%20parameters.%20We%20develop%20a%20search%20subroutine%20for%20this%20certificate%20and%0Aembed%20it%20within%20a%20guess-and-check%20framework%2C%20resulting%20in%20an%20almost%0Aparameter-free%20algorithm%20for%20both%20the%20convex%20QG%20and%20weakly-convex%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19465v1&entry.124074799=Read"},
{"title": "Integration of a Graph-Based Path Planner and Mixed-Integer MPC for\n  Robot Navigation in Cluttered Environments", "author": "Joshua A. Robbins and Stephen J. Harnett and Andrew F. Thompson and Sean Brennan and Herschel C. Pangborn", "abstract": "  The ability to update a path plan is a required capability for autonomous\nmobile robots navigating through uncertain environments. This paper proposes a\nre-planning strategy using a multilayer planning and control framework for\ncases where the robot's environment is partially known. A medial axis\ngraph-based planner defines a global path plan based on known obstacles, where\neach edge in the graph corresponds to a unique corridor. A mixed-integer model\npredictive control (MPC) method detects if a terminal constraint derived from\nthe global plan is infeasible, subject to a non-convex description of the local\nenvironment. Infeasibility detection is used to trigger efficient global\nre-planning via medial axis graph edge deletion. The proposed re-planning\nstrategy is demonstrated experimentally.\n", "link": "http://arxiv.org/abs/2504.13372v2", "date": "2025-07-25", "relevancy": 1.6102, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5437}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20a%20Graph-Based%20Path%20Planner%20and%20Mixed-Integer%20MPC%20for%0A%20%20Robot%20Navigation%20in%20Cluttered%20Environments&body=Title%3A%20Integration%20of%20a%20Graph-Based%20Path%20Planner%20and%20Mixed-Integer%20MPC%20for%0A%20%20Robot%20Navigation%20in%20Cluttered%20Environments%0AAuthor%3A%20Joshua%20A.%20Robbins%20and%20Stephen%20J.%20Harnett%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn%0AAbstract%3A%20%20%20The%20ability%20to%20update%20a%20path%20plan%20is%20a%20required%20capability%20for%20autonomous%0Amobile%20robots%20navigating%20through%20uncertain%20environments.%20This%20paper%20proposes%20a%0Are-planning%20strategy%20using%20a%20multilayer%20planning%20and%20control%20framework%20for%0Acases%20where%20the%20robot%27s%20environment%20is%20partially%20known.%20A%20medial%20axis%0Agraph-based%20planner%20defines%20a%20global%20path%20plan%20based%20on%20known%20obstacles%2C%20where%0Aeach%20edge%20in%20the%20graph%20corresponds%20to%20a%20unique%20corridor.%20A%20mixed-integer%20model%0Apredictive%20control%20%28MPC%29%20method%20detects%20if%20a%20terminal%20constraint%20derived%20from%0Athe%20global%20plan%20is%20infeasible%2C%20subject%20to%20a%20non-convex%20description%20of%20the%20local%0Aenvironment.%20Infeasibility%20detection%20is%20used%20to%20trigger%20efficient%20global%0Are-planning%20via%20medial%20axis%20graph%20edge%20deletion.%20The%20proposed%20re-planning%0Astrategy%20is%20demonstrated%20experimentally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520a%2520Graph-Based%2520Path%2520Planner%2520and%2520Mixed-Integer%2520MPC%2520for%250A%2520%2520Robot%2520Navigation%2520in%2520Cluttered%2520Environments%26entry.906535625%3DJoshua%2520A.%2520Robbins%2520and%2520Stephen%2520J.%2520Harnett%2520and%2520Andrew%2520F.%2520Thompson%2520and%2520Sean%2520Brennan%2520and%2520Herschel%2520C.%2520Pangborn%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520update%2520a%2520path%2520plan%2520is%2520a%2520required%2520capability%2520for%2520autonomous%250Amobile%2520robots%2520navigating%2520through%2520uncertain%2520environments.%2520This%2520paper%2520proposes%2520a%250Are-planning%2520strategy%2520using%2520a%2520multilayer%2520planning%2520and%2520control%2520framework%2520for%250Acases%2520where%2520the%2520robot%2527s%2520environment%2520is%2520partially%2520known.%2520A%2520medial%2520axis%250Agraph-based%2520planner%2520defines%2520a%2520global%2520path%2520plan%2520based%2520on%2520known%2520obstacles%252C%2520where%250Aeach%2520edge%2520in%2520the%2520graph%2520corresponds%2520to%2520a%2520unique%2520corridor.%2520A%2520mixed-integer%2520model%250Apredictive%2520control%2520%2528MPC%2529%2520method%2520detects%2520if%2520a%2520terminal%2520constraint%2520derived%2520from%250Athe%2520global%2520plan%2520is%2520infeasible%252C%2520subject%2520to%2520a%2520non-convex%2520description%2520of%2520the%2520local%250Aenvironment.%2520Infeasibility%2520detection%2520is%2520used%2520to%2520trigger%2520efficient%2520global%250Are-planning%2520via%2520medial%2520axis%2520graph%2520edge%2520deletion.%2520The%2520proposed%2520re-planning%250Astrategy%2520is%2520demonstrated%2520experimentally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20a%20Graph-Based%20Path%20Planner%20and%20Mixed-Integer%20MPC%20for%0A%20%20Robot%20Navigation%20in%20Cluttered%20Environments&entry.906535625=Joshua%20A.%20Robbins%20and%20Stephen%20J.%20Harnett%20and%20Andrew%20F.%20Thompson%20and%20Sean%20Brennan%20and%20Herschel%20C.%20Pangborn&entry.1292438233=%20%20The%20ability%20to%20update%20a%20path%20plan%20is%20a%20required%20capability%20for%20autonomous%0Amobile%20robots%20navigating%20through%20uncertain%20environments.%20This%20paper%20proposes%20a%0Are-planning%20strategy%20using%20a%20multilayer%20planning%20and%20control%20framework%20for%0Acases%20where%20the%20robot%27s%20environment%20is%20partially%20known.%20A%20medial%20axis%0Agraph-based%20planner%20defines%20a%20global%20path%20plan%20based%20on%20known%20obstacles%2C%20where%0Aeach%20edge%20in%20the%20graph%20corresponds%20to%20a%20unique%20corridor.%20A%20mixed-integer%20model%0Apredictive%20control%20%28MPC%29%20method%20detects%20if%20a%20terminal%20constraint%20derived%20from%0Athe%20global%20plan%20is%20infeasible%2C%20subject%20to%20a%20non-convex%20description%20of%20the%20local%0Aenvironment.%20Infeasibility%20detection%20is%20used%20to%20trigger%20efficient%20global%0Are-planning%20via%20medial%20axis%20graph%20edge%20deletion.%20The%20proposed%20re-planning%0Astrategy%20is%20demonstrated%20experimentally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13372v2&entry.124074799=Read"},
{"title": "Observations Meet Actions: Learning Control-Sufficient Representations\n  for Robust Policy Generalization", "author": "Yuliang Gu and Hongpeng Cao and Marco Caccamo and Naira Hovakimyan", "abstract": "  Capturing latent variations (\"contexts\") is key to deploying\nreinforcement-learning (RL) agents beyond their training regime. We recast\ncontext-based RL as a dual inference-control problem and formally characterize\ntwo properties and their hierarchy: observation sufficiency (preserving all\npredictive information) and control sufficiency (retaining decision-making\nrelevant information). Exploiting this dichotomy, we derive a contextual\nevidence lower bound(ELBO)-style objective that cleanly separates\nrepresentation learning from policy learning and optimizes it with Bottlenecked\nContextual Policy Optimization (BCPO), an algorithm that places a variational\ninformation-bottleneck encoder in front of any off-policy policy learner. On\nstandard continuous-control benchmarks with shifting physical parameters, BCPO\nmatches or surpasses other baselines while using fewer samples and retaining\nperformance far outside the training regime. The framework unifies theory,\ndiagnostics, and practice for context-based RL.\n", "link": "http://arxiv.org/abs/2507.19437v1", "date": "2025-07-25", "relevancy": 1.6062, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Observations%20Meet%20Actions%3A%20Learning%20Control-Sufficient%20Representations%0A%20%20for%20Robust%20Policy%20Generalization&body=Title%3A%20Observations%20Meet%20Actions%3A%20Learning%20Control-Sufficient%20Representations%0A%20%20for%20Robust%20Policy%20Generalization%0AAuthor%3A%20Yuliang%20Gu%20and%20Hongpeng%20Cao%20and%20Marco%20Caccamo%20and%20Naira%20Hovakimyan%0AAbstract%3A%20%20%20Capturing%20latent%20variations%20%28%22contexts%22%29%20is%20key%20to%20deploying%0Areinforcement-learning%20%28RL%29%20agents%20beyond%20their%20training%20regime.%20We%20recast%0Acontext-based%20RL%20as%20a%20dual%20inference-control%20problem%20and%20formally%20characterize%0Atwo%20properties%20and%20their%20hierarchy%3A%20observation%20sufficiency%20%28preserving%20all%0Apredictive%20information%29%20and%20control%20sufficiency%20%28retaining%20decision-making%0Arelevant%20information%29.%20Exploiting%20this%20dichotomy%2C%20we%20derive%20a%20contextual%0Aevidence%20lower%20bound%28ELBO%29-style%20objective%20that%20cleanly%20separates%0Arepresentation%20learning%20from%20policy%20learning%20and%20optimizes%20it%20with%20Bottlenecked%0AContextual%20Policy%20Optimization%20%28BCPO%29%2C%20an%20algorithm%20that%20places%20a%20variational%0Ainformation-bottleneck%20encoder%20in%20front%20of%20any%20off-policy%20policy%20learner.%20On%0Astandard%20continuous-control%20benchmarks%20with%20shifting%20physical%20parameters%2C%20BCPO%0Amatches%20or%20surpasses%20other%20baselines%20while%20using%20fewer%20samples%20and%20retaining%0Aperformance%20far%20outside%20the%20training%20regime.%20The%20framework%20unifies%20theory%2C%0Adiagnostics%2C%20and%20practice%20for%20context-based%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObservations%2520Meet%2520Actions%253A%2520Learning%2520Control-Sufficient%2520Representations%250A%2520%2520for%2520Robust%2520Policy%2520Generalization%26entry.906535625%3DYuliang%2520Gu%2520and%2520Hongpeng%2520Cao%2520and%2520Marco%2520Caccamo%2520and%2520Naira%2520Hovakimyan%26entry.1292438233%3D%2520%2520Capturing%2520latent%2520variations%2520%2528%2522contexts%2522%2529%2520is%2520key%2520to%2520deploying%250Areinforcement-learning%2520%2528RL%2529%2520agents%2520beyond%2520their%2520training%2520regime.%2520We%2520recast%250Acontext-based%2520RL%2520as%2520a%2520dual%2520inference-control%2520problem%2520and%2520formally%2520characterize%250Atwo%2520properties%2520and%2520their%2520hierarchy%253A%2520observation%2520sufficiency%2520%2528preserving%2520all%250Apredictive%2520information%2529%2520and%2520control%2520sufficiency%2520%2528retaining%2520decision-making%250Arelevant%2520information%2529.%2520Exploiting%2520this%2520dichotomy%252C%2520we%2520derive%2520a%2520contextual%250Aevidence%2520lower%2520bound%2528ELBO%2529-style%2520objective%2520that%2520cleanly%2520separates%250Arepresentation%2520learning%2520from%2520policy%2520learning%2520and%2520optimizes%2520it%2520with%2520Bottlenecked%250AContextual%2520Policy%2520Optimization%2520%2528BCPO%2529%252C%2520an%2520algorithm%2520that%2520places%2520a%2520variational%250Ainformation-bottleneck%2520encoder%2520in%2520front%2520of%2520any%2520off-policy%2520policy%2520learner.%2520On%250Astandard%2520continuous-control%2520benchmarks%2520with%2520shifting%2520physical%2520parameters%252C%2520BCPO%250Amatches%2520or%2520surpasses%2520other%2520baselines%2520while%2520using%2520fewer%2520samples%2520and%2520retaining%250Aperformance%2520far%2520outside%2520the%2520training%2520regime.%2520The%2520framework%2520unifies%2520theory%252C%250Adiagnostics%252C%2520and%2520practice%2520for%2520context-based%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Observations%20Meet%20Actions%3A%20Learning%20Control-Sufficient%20Representations%0A%20%20for%20Robust%20Policy%20Generalization&entry.906535625=Yuliang%20Gu%20and%20Hongpeng%20Cao%20and%20Marco%20Caccamo%20and%20Naira%20Hovakimyan&entry.1292438233=%20%20Capturing%20latent%20variations%20%28%22contexts%22%29%20is%20key%20to%20deploying%0Areinforcement-learning%20%28RL%29%20agents%20beyond%20their%20training%20regime.%20We%20recast%0Acontext-based%20RL%20as%20a%20dual%20inference-control%20problem%20and%20formally%20characterize%0Atwo%20properties%20and%20their%20hierarchy%3A%20observation%20sufficiency%20%28preserving%20all%0Apredictive%20information%29%20and%20control%20sufficiency%20%28retaining%20decision-making%0Arelevant%20information%29.%20Exploiting%20this%20dichotomy%2C%20we%20derive%20a%20contextual%0Aevidence%20lower%20bound%28ELBO%29-style%20objective%20that%20cleanly%20separates%0Arepresentation%20learning%20from%20policy%20learning%20and%20optimizes%20it%20with%20Bottlenecked%0AContextual%20Policy%20Optimization%20%28BCPO%29%2C%20an%20algorithm%20that%20places%20a%20variational%0Ainformation-bottleneck%20encoder%20in%20front%20of%20any%20off-policy%20policy%20learner.%20On%0Astandard%20continuous-control%20benchmarks%20with%20shifting%20physical%20parameters%2C%20BCPO%0Amatches%20or%20surpasses%20other%20baselines%20while%20using%20fewer%20samples%20and%20retaining%0Aperformance%20far%20outside%20the%20training%20regime.%20The%20framework%20unifies%20theory%2C%0Adiagnostics%2C%20and%20practice%20for%20context-based%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19437v1&entry.124074799=Read"},
{"title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "author": "Yichen Huang and Lin F. Yang", "abstract": "  The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks.\n", "link": "http://arxiv.org/abs/2507.15855v3", "date": "2025-07-25", "relevancy": 1.5652, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025&body=Title%3A%20Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025%0AAuthor%3A%20Yichen%20Huang%20and%20Lin%20F.%20Yang%0AAbstract%3A%20%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20poses%20uniquely%20challenging%0Aproblems%20requiring%20deep%20insight%2C%20creativity%2C%20and%20formal%20reasoning.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20perform%20well%20on%20mathematical%20benchmarks%20like%20AIME%2C%20they%0Astruggle%20with%20Olympiad-level%20tasks.%20We%20use%20Google%27s%20Gemini%202.5%20Pro%20on%20the%20newly%0Areleased%20IMO%202025%20problems%2C%20avoiding%20data%20contamination.%20Using%20a%0Aself-verification%20pipeline%20with%20careful%20prompt%20design%2C%205%20%28out%20of%206%29%20problems%0Aare%20solved%20correctly.%20This%20result%20underscores%20the%20importance%20of%20developing%0Aoptimal%20strategies%20to%20harness%20the%20full%20potential%20of%20powerful%20LLMs%20for%20complex%0Areasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15855v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25202.5%2520Pro%2520Capable%2520of%2520Winning%2520Gold%2520at%2520IMO%25202025%26entry.906535625%3DYichen%2520Huang%2520and%2520Lin%2520F.%2520Yang%26entry.1292438233%3D%2520%2520The%2520International%2520Mathematical%2520Olympiad%2520%2528IMO%2529%2520poses%2520uniquely%2520challenging%250Aproblems%2520requiring%2520deep%2520insight%252C%2520creativity%252C%2520and%2520formal%2520reasoning.%2520While%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520perform%2520well%2520on%2520mathematical%2520benchmarks%2520like%2520AIME%252C%2520they%250Astruggle%2520with%2520Olympiad-level%2520tasks.%2520We%2520use%2520Google%2527s%2520Gemini%25202.5%2520Pro%2520on%2520the%2520newly%250Areleased%2520IMO%25202025%2520problems%252C%2520avoiding%2520data%2520contamination.%2520Using%2520a%250Aself-verification%2520pipeline%2520with%2520careful%2520prompt%2520design%252C%25205%2520%2528out%2520of%25206%2529%2520problems%250Aare%2520solved%2520correctly.%2520This%2520result%2520underscores%2520the%2520importance%2520of%2520developing%250Aoptimal%2520strategies%2520to%2520harness%2520the%2520full%2520potential%2520of%2520powerful%2520LLMs%2520for%2520complex%250Areasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15855v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025&entry.906535625=Yichen%20Huang%20and%20Lin%20F.%20Yang&entry.1292438233=%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20poses%20uniquely%20challenging%0Aproblems%20requiring%20deep%20insight%2C%20creativity%2C%20and%20formal%20reasoning.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20perform%20well%20on%20mathematical%20benchmarks%20like%20AIME%2C%20they%0Astruggle%20with%20Olympiad-level%20tasks.%20We%20use%20Google%27s%20Gemini%202.5%20Pro%20on%20the%20newly%0Areleased%20IMO%202025%20problems%2C%20avoiding%20data%20contamination.%20Using%20a%0Aself-verification%20pipeline%20with%20careful%20prompt%20design%2C%205%20%28out%20of%206%29%20problems%0Aare%20solved%20correctly.%20This%20result%20underscores%20the%20importance%20of%20developing%0Aoptimal%20strategies%20to%20harness%20the%20full%20potential%20of%20powerful%20LLMs%20for%20complex%0Areasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15855v3&entry.124074799=Read"},
{"title": "SILS: Strategic Influence on Liquidity Stability and Whale Detection in\n  Concentrated-Liquidity DEXs", "author": "Ali RajabiNekoo and Laleh Rasoul and Amirfarhad Farhadi and Azadeh Zamanifar", "abstract": "  Traditional methods for identifying impactful liquidity providers (LPs) in\nConcentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as\nnominal capital size or surface-level activity, which often lead to inaccurate\nrisk analysis. The SILS framework offers a significantly more detailed\napproach, characterizing LPs not just as capital holders but as dynamic\nsystemic agents whose actions directly impact market stability. This represents\na fundamental paradigm shift from the static, volume-based analysis to a\ndynamic, impact-focused understanding. This advanced approach uses on-chain\nevent logs and smart contract execution traces to compute Exponential\nTime-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly\ndetection. Most importantly, it defines an LP's functional importance through\nthe Liquidity Stability Impact Score (LSIS), a counterfactual metric that\nmeasures the potential degradation of the market if the LP withdraws. This\ncombined approach provides a more detailed and realistic characterization of an\nLP's impact, moving beyond the binary and often misleading classifications used\nby existing methods. This impact-focused and comprehensive approach enables\nSILS to accurately identify high-impact LPs-including those missed by\ntraditional methods and supports essential applications like a protective\noracle layer and actionable trader signals, thereby significantly enhancing\nDeFi ecosystem. The framework provides unprecedented transparency into the\nunderlying liquidity structure and associated risks, effectively reducing the\ncommon false positives and uncovering critical false negatives found in\ntraditional models. Therefore, SILS provides an effective mechanism for\nproactive risk management, transforming how DeFi protocols safeguard their\necosystems against asymmetric liquidity behavior.\n", "link": "http://arxiv.org/abs/2507.19411v1", "date": "2025-07-25", "relevancy": 1.5333, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SILS%3A%20Strategic%20Influence%20on%20Liquidity%20Stability%20and%20Whale%20Detection%20in%0A%20%20Concentrated-Liquidity%20DEXs&body=Title%3A%20SILS%3A%20Strategic%20Influence%20on%20Liquidity%20Stability%20and%20Whale%20Detection%20in%0A%20%20Concentrated-Liquidity%20DEXs%0AAuthor%3A%20Ali%20RajabiNekoo%20and%20Laleh%20Rasoul%20and%20Amirfarhad%20Farhadi%20and%20Azadeh%20Zamanifar%0AAbstract%3A%20%20%20Traditional%20methods%20for%20identifying%20impactful%20liquidity%20providers%20%28LPs%29%20in%0AConcentrated%20Liquidity%20Market%20Makers%20%28CLMMs%29%20rely%20on%20broad%20measures%2C%20such%20as%0Anominal%20capital%20size%20or%20surface-level%20activity%2C%20which%20often%20lead%20to%20inaccurate%0Arisk%20analysis.%20The%20SILS%20framework%20offers%20a%20significantly%20more%20detailed%0Aapproach%2C%20characterizing%20LPs%20not%20just%20as%20capital%20holders%20but%20as%20dynamic%0Asystemic%20agents%20whose%20actions%20directly%20impact%20market%20stability.%20This%20represents%0Aa%20fundamental%20paradigm%20shift%20from%20the%20static%2C%20volume-based%20analysis%20to%20a%0Adynamic%2C%20impact-focused%20understanding.%20This%20advanced%20approach%20uses%20on-chain%0Aevent%20logs%20and%20smart%20contract%20execution%20traces%20to%20compute%20Exponential%0ATime-Weighted%20Liquidity%20%28ETWL%29%20profiles%20and%20apply%20unsupervised%20anomaly%0Adetection.%20Most%20importantly%2C%20it%20defines%20an%20LP%27s%20functional%20importance%20through%0Athe%20Liquidity%20Stability%20Impact%20Score%20%28LSIS%29%2C%20a%20counterfactual%20metric%20that%0Ameasures%20the%20potential%20degradation%20of%20the%20market%20if%20the%20LP%20withdraws.%20This%0Acombined%20approach%20provides%20a%20more%20detailed%20and%20realistic%20characterization%20of%20an%0ALP%27s%20impact%2C%20moving%20beyond%20the%20binary%20and%20often%20misleading%20classifications%20used%0Aby%20existing%20methods.%20This%20impact-focused%20and%20comprehensive%20approach%20enables%0ASILS%20to%20accurately%20identify%20high-impact%20LPs-including%20those%20missed%20by%0Atraditional%20methods%20and%20supports%20essential%20applications%20like%20a%20protective%0Aoracle%20layer%20and%20actionable%20trader%20signals%2C%20thereby%20significantly%20enhancing%0ADeFi%20ecosystem.%20The%20framework%20provides%20unprecedented%20transparency%20into%20the%0Aunderlying%20liquidity%20structure%20and%20associated%20risks%2C%20effectively%20reducing%20the%0Acommon%20false%20positives%20and%20uncovering%20critical%20false%20negatives%20found%20in%0Atraditional%20models.%20Therefore%2C%20SILS%20provides%20an%20effective%20mechanism%20for%0Aproactive%20risk%20management%2C%20transforming%20how%20DeFi%20protocols%20safeguard%20their%0Aecosystems%20against%20asymmetric%20liquidity%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSILS%253A%2520Strategic%2520Influence%2520on%2520Liquidity%2520Stability%2520and%2520Whale%2520Detection%2520in%250A%2520%2520Concentrated-Liquidity%2520DEXs%26entry.906535625%3DAli%2520RajabiNekoo%2520and%2520Laleh%2520Rasoul%2520and%2520Amirfarhad%2520Farhadi%2520and%2520Azadeh%2520Zamanifar%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%2520identifying%2520impactful%2520liquidity%2520providers%2520%2528LPs%2529%2520in%250AConcentrated%2520Liquidity%2520Market%2520Makers%2520%2528CLMMs%2529%2520rely%2520on%2520broad%2520measures%252C%2520such%2520as%250Anominal%2520capital%2520size%2520or%2520surface-level%2520activity%252C%2520which%2520often%2520lead%2520to%2520inaccurate%250Arisk%2520analysis.%2520The%2520SILS%2520framework%2520offers%2520a%2520significantly%2520more%2520detailed%250Aapproach%252C%2520characterizing%2520LPs%2520not%2520just%2520as%2520capital%2520holders%2520but%2520as%2520dynamic%250Asystemic%2520agents%2520whose%2520actions%2520directly%2520impact%2520market%2520stability.%2520This%2520represents%250Aa%2520fundamental%2520paradigm%2520shift%2520from%2520the%2520static%252C%2520volume-based%2520analysis%2520to%2520a%250Adynamic%252C%2520impact-focused%2520understanding.%2520This%2520advanced%2520approach%2520uses%2520on-chain%250Aevent%2520logs%2520and%2520smart%2520contract%2520execution%2520traces%2520to%2520compute%2520Exponential%250ATime-Weighted%2520Liquidity%2520%2528ETWL%2529%2520profiles%2520and%2520apply%2520unsupervised%2520anomaly%250Adetection.%2520Most%2520importantly%252C%2520it%2520defines%2520an%2520LP%2527s%2520functional%2520importance%2520through%250Athe%2520Liquidity%2520Stability%2520Impact%2520Score%2520%2528LSIS%2529%252C%2520a%2520counterfactual%2520metric%2520that%250Ameasures%2520the%2520potential%2520degradation%2520of%2520the%2520market%2520if%2520the%2520LP%2520withdraws.%2520This%250Acombined%2520approach%2520provides%2520a%2520more%2520detailed%2520and%2520realistic%2520characterization%2520of%2520an%250ALP%2527s%2520impact%252C%2520moving%2520beyond%2520the%2520binary%2520and%2520often%2520misleading%2520classifications%2520used%250Aby%2520existing%2520methods.%2520This%2520impact-focused%2520and%2520comprehensive%2520approach%2520enables%250ASILS%2520to%2520accurately%2520identify%2520high-impact%2520LPs-including%2520those%2520missed%2520by%250Atraditional%2520methods%2520and%2520supports%2520essential%2520applications%2520like%2520a%2520protective%250Aoracle%2520layer%2520and%2520actionable%2520trader%2520signals%252C%2520thereby%2520significantly%2520enhancing%250ADeFi%2520ecosystem.%2520The%2520framework%2520provides%2520unprecedented%2520transparency%2520into%2520the%250Aunderlying%2520liquidity%2520structure%2520and%2520associated%2520risks%252C%2520effectively%2520reducing%2520the%250Acommon%2520false%2520positives%2520and%2520uncovering%2520critical%2520false%2520negatives%2520found%2520in%250Atraditional%2520models.%2520Therefore%252C%2520SILS%2520provides%2520an%2520effective%2520mechanism%2520for%250Aproactive%2520risk%2520management%252C%2520transforming%2520how%2520DeFi%2520protocols%2520safeguard%2520their%250Aecosystems%2520against%2520asymmetric%2520liquidity%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SILS%3A%20Strategic%20Influence%20on%20Liquidity%20Stability%20and%20Whale%20Detection%20in%0A%20%20Concentrated-Liquidity%20DEXs&entry.906535625=Ali%20RajabiNekoo%20and%20Laleh%20Rasoul%20and%20Amirfarhad%20Farhadi%20and%20Azadeh%20Zamanifar&entry.1292438233=%20%20Traditional%20methods%20for%20identifying%20impactful%20liquidity%20providers%20%28LPs%29%20in%0AConcentrated%20Liquidity%20Market%20Makers%20%28CLMMs%29%20rely%20on%20broad%20measures%2C%20such%20as%0Anominal%20capital%20size%20or%20surface-level%20activity%2C%20which%20often%20lead%20to%20inaccurate%0Arisk%20analysis.%20The%20SILS%20framework%20offers%20a%20significantly%20more%20detailed%0Aapproach%2C%20characterizing%20LPs%20not%20just%20as%20capital%20holders%20but%20as%20dynamic%0Asystemic%20agents%20whose%20actions%20directly%20impact%20market%20stability.%20This%20represents%0Aa%20fundamental%20paradigm%20shift%20from%20the%20static%2C%20volume-based%20analysis%20to%20a%0Adynamic%2C%20impact-focused%20understanding.%20This%20advanced%20approach%20uses%20on-chain%0Aevent%20logs%20and%20smart%20contract%20execution%20traces%20to%20compute%20Exponential%0ATime-Weighted%20Liquidity%20%28ETWL%29%20profiles%20and%20apply%20unsupervised%20anomaly%0Adetection.%20Most%20importantly%2C%20it%20defines%20an%20LP%27s%20functional%20importance%20through%0Athe%20Liquidity%20Stability%20Impact%20Score%20%28LSIS%29%2C%20a%20counterfactual%20metric%20that%0Ameasures%20the%20potential%20degradation%20of%20the%20market%20if%20the%20LP%20withdraws.%20This%0Acombined%20approach%20provides%20a%20more%20detailed%20and%20realistic%20characterization%20of%20an%0ALP%27s%20impact%2C%20moving%20beyond%20the%20binary%20and%20often%20misleading%20classifications%20used%0Aby%20existing%20methods.%20This%20impact-focused%20and%20comprehensive%20approach%20enables%0ASILS%20to%20accurately%20identify%20high-impact%20LPs-including%20those%20missed%20by%0Atraditional%20methods%20and%20supports%20essential%20applications%20like%20a%20protective%0Aoracle%20layer%20and%20actionable%20trader%20signals%2C%20thereby%20significantly%20enhancing%0ADeFi%20ecosystem.%20The%20framework%20provides%20unprecedented%20transparency%20into%20the%0Aunderlying%20liquidity%20structure%20and%20associated%20risks%2C%20effectively%20reducing%20the%0Acommon%20false%20positives%20and%20uncovering%20critical%20false%20negatives%20found%20in%0Atraditional%20models.%20Therefore%2C%20SILS%20provides%20an%20effective%20mechanism%20for%0Aproactive%20risk%20management%2C%20transforming%20how%20DeFi%20protocols%20safeguard%20their%0Aecosystems%20against%20asymmetric%20liquidity%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19411v1&entry.124074799=Read"},
{"title": "Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for\n  Physical Interactions", "author": "Mingyang Xu and Jiayi Shao and Yulan Ju and Ximing Shen and Qingyuan Gao and Weijen Chen and Qing Zhang and Yun Suen Pai and Giulia Barbareschi and Matthias Hoppe and Kouta Minamizawa and Kai Kunze", "abstract": "  Flying robots, such as quadrotor drones, offer new possibilities for\nhuman-robot interaction but often pose safety risks due to fast-spinning\npropellers, rigid structures, and noise. In contrast, lighter-than-air\nflapping-wing robots, inspired by animal movement, offer a soft, quiet, and\ntouch-safe alternative. Building on these advantages, we present Cuddle-Fish, a\nsoft flapping-wing floating robot designed for close-proximity interactions in\nindoor spaces. Through a user study with 24 participants, we explored their\nperceptions of the robot and experiences during a series of co-located\ndemonstrations in which the robot moved near them. Results showed that\nparticipants felt safe, willingly engaged in touch-based interactions with the\nrobot, and exhibited spontaneous affective behaviours, such as patting,\nstroking, hugging, and cheek-touching, without external prompting. They also\nreported positive emotional responses towards the robot. These findings suggest\nthat the soft floating robot with flapping wings can serve as a novel and\nsocially acceptable alternative to traditional rigid flying robots, opening new\npotential for applications in companionship, affective interaction, and play in\neveryday indoor environments.\n", "link": "http://arxiv.org/abs/2504.01293v2", "date": "2025-07-25", "relevancy": 1.5019, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5307}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cuddle-Fish%3A%20Exploring%20a%20Soft%20Floating%20Robot%20with%20Flapping%20Wings%20for%0A%20%20Physical%20Interactions&body=Title%3A%20Cuddle-Fish%3A%20Exploring%20a%20Soft%20Floating%20Robot%20with%20Flapping%20Wings%20for%0A%20%20Physical%20Interactions%0AAuthor%3A%20Mingyang%20Xu%20and%20Jiayi%20Shao%20and%20Yulan%20Ju%20and%20Ximing%20Shen%20and%20Qingyuan%20Gao%20and%20Weijen%20Chen%20and%20Qing%20Zhang%20and%20Yun%20Suen%20Pai%20and%20Giulia%20Barbareschi%20and%20Matthias%20Hoppe%20and%20Kouta%20Minamizawa%20and%20Kai%20Kunze%0AAbstract%3A%20%20%20Flying%20robots%2C%20such%20as%20quadrotor%20drones%2C%20offer%20new%20possibilities%20for%0Ahuman-robot%20interaction%20but%20often%20pose%20safety%20risks%20due%20to%20fast-spinning%0Apropellers%2C%20rigid%20structures%2C%20and%20noise.%20In%20contrast%2C%20lighter-than-air%0Aflapping-wing%20robots%2C%20inspired%20by%20animal%20movement%2C%20offer%20a%20soft%2C%20quiet%2C%20and%0Atouch-safe%20alternative.%20Building%20on%20these%20advantages%2C%20we%20present%20Cuddle-Fish%2C%20a%0Asoft%20flapping-wing%20floating%20robot%20designed%20for%20close-proximity%20interactions%20in%0Aindoor%20spaces.%20Through%20a%20user%20study%20with%2024%20participants%2C%20we%20explored%20their%0Aperceptions%20of%20the%20robot%20and%20experiences%20during%20a%20series%20of%20co-located%0Ademonstrations%20in%20which%20the%20robot%20moved%20near%20them.%20Results%20showed%20that%0Aparticipants%20felt%20safe%2C%20willingly%20engaged%20in%20touch-based%20interactions%20with%20the%0Arobot%2C%20and%20exhibited%20spontaneous%20affective%20behaviours%2C%20such%20as%20patting%2C%0Astroking%2C%20hugging%2C%20and%20cheek-touching%2C%20without%20external%20prompting.%20They%20also%0Areported%20positive%20emotional%20responses%20towards%20the%20robot.%20These%20findings%20suggest%0Athat%20the%20soft%20floating%20robot%20with%20flapping%20wings%20can%20serve%20as%20a%20novel%20and%0Asocially%20acceptable%20alternative%20to%20traditional%20rigid%20flying%20robots%2C%20opening%20new%0Apotential%20for%20applications%20in%20companionship%2C%20affective%20interaction%2C%20and%20play%20in%0Aeveryday%20indoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuddle-Fish%253A%2520Exploring%2520a%2520Soft%2520Floating%2520Robot%2520with%2520Flapping%2520Wings%2520for%250A%2520%2520Physical%2520Interactions%26entry.906535625%3DMingyang%2520Xu%2520and%2520Jiayi%2520Shao%2520and%2520Yulan%2520Ju%2520and%2520Ximing%2520Shen%2520and%2520Qingyuan%2520Gao%2520and%2520Weijen%2520Chen%2520and%2520Qing%2520Zhang%2520and%2520Yun%2520Suen%2520Pai%2520and%2520Giulia%2520Barbareschi%2520and%2520Matthias%2520Hoppe%2520and%2520Kouta%2520Minamizawa%2520and%2520Kai%2520Kunze%26entry.1292438233%3D%2520%2520Flying%2520robots%252C%2520such%2520as%2520quadrotor%2520drones%252C%2520offer%2520new%2520possibilities%2520for%250Ahuman-robot%2520interaction%2520but%2520often%2520pose%2520safety%2520risks%2520due%2520to%2520fast-spinning%250Apropellers%252C%2520rigid%2520structures%252C%2520and%2520noise.%2520In%2520contrast%252C%2520lighter-than-air%250Aflapping-wing%2520robots%252C%2520inspired%2520by%2520animal%2520movement%252C%2520offer%2520a%2520soft%252C%2520quiet%252C%2520and%250Atouch-safe%2520alternative.%2520Building%2520on%2520these%2520advantages%252C%2520we%2520present%2520Cuddle-Fish%252C%2520a%250Asoft%2520flapping-wing%2520floating%2520robot%2520designed%2520for%2520close-proximity%2520interactions%2520in%250Aindoor%2520spaces.%2520Through%2520a%2520user%2520study%2520with%252024%2520participants%252C%2520we%2520explored%2520their%250Aperceptions%2520of%2520the%2520robot%2520and%2520experiences%2520during%2520a%2520series%2520of%2520co-located%250Ademonstrations%2520in%2520which%2520the%2520robot%2520moved%2520near%2520them.%2520Results%2520showed%2520that%250Aparticipants%2520felt%2520safe%252C%2520willingly%2520engaged%2520in%2520touch-based%2520interactions%2520with%2520the%250Arobot%252C%2520and%2520exhibited%2520spontaneous%2520affective%2520behaviours%252C%2520such%2520as%2520patting%252C%250Astroking%252C%2520hugging%252C%2520and%2520cheek-touching%252C%2520without%2520external%2520prompting.%2520They%2520also%250Areported%2520positive%2520emotional%2520responses%2520towards%2520the%2520robot.%2520These%2520findings%2520suggest%250Athat%2520the%2520soft%2520floating%2520robot%2520with%2520flapping%2520wings%2520can%2520serve%2520as%2520a%2520novel%2520and%250Asocially%2520acceptable%2520alternative%2520to%2520traditional%2520rigid%2520flying%2520robots%252C%2520opening%2520new%250Apotential%2520for%2520applications%2520in%2520companionship%252C%2520affective%2520interaction%252C%2520and%2520play%2520in%250Aeveryday%2520indoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cuddle-Fish%3A%20Exploring%20a%20Soft%20Floating%20Robot%20with%20Flapping%20Wings%20for%0A%20%20Physical%20Interactions&entry.906535625=Mingyang%20Xu%20and%20Jiayi%20Shao%20and%20Yulan%20Ju%20and%20Ximing%20Shen%20and%20Qingyuan%20Gao%20and%20Weijen%20Chen%20and%20Qing%20Zhang%20and%20Yun%20Suen%20Pai%20and%20Giulia%20Barbareschi%20and%20Matthias%20Hoppe%20and%20Kouta%20Minamizawa%20and%20Kai%20Kunze&entry.1292438233=%20%20Flying%20robots%2C%20such%20as%20quadrotor%20drones%2C%20offer%20new%20possibilities%20for%0Ahuman-robot%20interaction%20but%20often%20pose%20safety%20risks%20due%20to%20fast-spinning%0Apropellers%2C%20rigid%20structures%2C%20and%20noise.%20In%20contrast%2C%20lighter-than-air%0Aflapping-wing%20robots%2C%20inspired%20by%20animal%20movement%2C%20offer%20a%20soft%2C%20quiet%2C%20and%0Atouch-safe%20alternative.%20Building%20on%20these%20advantages%2C%20we%20present%20Cuddle-Fish%2C%20a%0Asoft%20flapping-wing%20floating%20robot%20designed%20for%20close-proximity%20interactions%20in%0Aindoor%20spaces.%20Through%20a%20user%20study%20with%2024%20participants%2C%20we%20explored%20their%0Aperceptions%20of%20the%20robot%20and%20experiences%20during%20a%20series%20of%20co-located%0Ademonstrations%20in%20which%20the%20robot%20moved%20near%20them.%20Results%20showed%20that%0Aparticipants%20felt%20safe%2C%20willingly%20engaged%20in%20touch-based%20interactions%20with%20the%0Arobot%2C%20and%20exhibited%20spontaneous%20affective%20behaviours%2C%20such%20as%20patting%2C%0Astroking%2C%20hugging%2C%20and%20cheek-touching%2C%20without%20external%20prompting.%20They%20also%0Areported%20positive%20emotional%20responses%20towards%20the%20robot.%20These%20findings%20suggest%0Athat%20the%20soft%20floating%20robot%20with%20flapping%20wings%20can%20serve%20as%20a%20novel%20and%0Asocially%20acceptable%20alternative%20to%20traditional%20rigid%20flying%20robots%2C%20opening%20new%0Apotential%20for%20applications%20in%20companionship%2C%20affective%20interaction%2C%20and%20play%20in%0Aeveryday%20indoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01293v2&entry.124074799=Read"},
{"title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "author": "Lakshya A Agrawal and Shangyin Tan and Dilara Soylu and Noah Ziems and Rishi Khare and Krista Opsahl-Ong and Arnav Singhvi and Herumb Shandilya and Michael J Ryan and Meng Jiang and Christopher Potts and Koushik Sen and Alexandros G. Dimakis and Ion Stoica and Dan Klein and Matei Zaharia and Omar Khattab", "abstract": "  Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.\n", "link": "http://arxiv.org/abs/2507.19457v1", "date": "2025-07-25", "relevancy": 1.4755, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4989}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4988}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEPA%3A%20Reflective%20Prompt%20Evolution%20Can%20Outperform%20Reinforcement%20Learning&body=Title%3A%20GEPA%3A%20Reflective%20Prompt%20Evolution%20Can%20Outperform%20Reinforcement%20Learning%0AAuthor%3A%20Lakshya%20A%20Agrawal%20and%20Shangyin%20Tan%20and%20Dilara%20Soylu%20and%20Noah%20Ziems%20and%20Rishi%20Khare%20and%20Krista%20Opsahl-Ong%20and%20Arnav%20Singhvi%20and%20Herumb%20Shandilya%20and%20Michael%20J%20Ryan%20and%20Meng%20Jiang%20and%20Christopher%20Potts%20and%20Koushik%20Sen%20and%20Alexandros%20G.%20Dimakis%20and%20Ion%20Stoica%20and%20Dan%20Klein%20and%20Matei%20Zaharia%20and%20Omar%20Khattab%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20adapted%20to%20downstream%20tasks%20via%0Areinforcement%20learning%20%28RL%29%20methods%20like%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20which%20often%20require%20thousands%20of%20rollouts%20to%20learn%20new%20tasks.%20We%20argue%0Athat%20the%20interpretable%20nature%20of%20language%20can%20often%20provide%20a%20much%20richer%0Alearning%20medium%20for%20LLMs%2C%20compared%20with%20policy%20gradients%20derived%20from%20sparse%2C%0Ascalar%20rewards.%20To%20test%20this%2C%20we%20introduce%20GEPA%20%28Genetic-Pareto%29%2C%20a%20prompt%0Aoptimizer%20that%20thoroughly%20incorporates%20natural%20language%20reflection%20to%20learn%0Ahigh-level%20rules%20from%20trial%20and%20error.%20Given%20any%20AI%20system%20containing%20one%20or%0Amore%20LLM%20prompts%2C%20GEPA%20samples%20system-level%20trajectories%20%28e.g.%2C%20reasoning%2C%20tool%0Acalls%2C%20and%20tool%20outputs%29%20and%20reflects%20on%20them%20in%20natural%20language%20to%20diagnose%0Aproblems%2C%20propose%20and%20test%20prompt%20updates%2C%20and%20combine%20complementary%20lessons%0Afrom%20the%20Pareto%20frontier%20of%20its%20own%20attempts.%20As%20a%20result%20of%20GEPA%27s%20design%2C%20it%0Acan%20often%20turn%20even%20just%20a%20few%20rollouts%20into%20a%20large%20quality%20gain.%20Across%20four%0Atasks%2C%20GEPA%20outperforms%20GRPO%20by%2010%25%20on%20average%20and%20by%20up%20to%2020%25%2C%20while%20using%20up%0Ato%2035x%20fewer%20rollouts.%20GEPA%20also%20outperforms%20the%20leading%20prompt%20optimizer%2C%0AMIPROv2%2C%20by%20over%2010%25%20across%20two%20LLMs%2C%20and%20demonstrates%20promising%20results%20as%20an%0Ainference-time%20search%20strategy%20for%20code%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEPA%253A%2520Reflective%2520Prompt%2520Evolution%2520Can%2520Outperform%2520Reinforcement%2520Learning%26entry.906535625%3DLakshya%2520A%2520Agrawal%2520and%2520Shangyin%2520Tan%2520and%2520Dilara%2520Soylu%2520and%2520Noah%2520Ziems%2520and%2520Rishi%2520Khare%2520and%2520Krista%2520Opsahl-Ong%2520and%2520Arnav%2520Singhvi%2520and%2520Herumb%2520Shandilya%2520and%2520Michael%2520J%2520Ryan%2520and%2520Meng%2520Jiang%2520and%2520Christopher%2520Potts%2520and%2520Koushik%2520Sen%2520and%2520Alexandros%2520G.%2520Dimakis%2520and%2520Ion%2520Stoica%2520and%2520Dan%2520Klein%2520and%2520Matei%2520Zaharia%2520and%2520Omar%2520Khattab%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520adapted%2520to%2520downstream%2520tasks%2520via%250Areinforcement%2520learning%2520%2528RL%2529%2520methods%2520like%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520which%2520often%2520require%2520thousands%2520of%2520rollouts%2520to%2520learn%2520new%2520tasks.%2520We%2520argue%250Athat%2520the%2520interpretable%2520nature%2520of%2520language%2520can%2520often%2520provide%2520a%2520much%2520richer%250Alearning%2520medium%2520for%2520LLMs%252C%2520compared%2520with%2520policy%2520gradients%2520derived%2520from%2520sparse%252C%250Ascalar%2520rewards.%2520To%2520test%2520this%252C%2520we%2520introduce%2520GEPA%2520%2528Genetic-Pareto%2529%252C%2520a%2520prompt%250Aoptimizer%2520that%2520thoroughly%2520incorporates%2520natural%2520language%2520reflection%2520to%2520learn%250Ahigh-level%2520rules%2520from%2520trial%2520and%2520error.%2520Given%2520any%2520AI%2520system%2520containing%2520one%2520or%250Amore%2520LLM%2520prompts%252C%2520GEPA%2520samples%2520system-level%2520trajectories%2520%2528e.g.%252C%2520reasoning%252C%2520tool%250Acalls%252C%2520and%2520tool%2520outputs%2529%2520and%2520reflects%2520on%2520them%2520in%2520natural%2520language%2520to%2520diagnose%250Aproblems%252C%2520propose%2520and%2520test%2520prompt%2520updates%252C%2520and%2520combine%2520complementary%2520lessons%250Afrom%2520the%2520Pareto%2520frontier%2520of%2520its%2520own%2520attempts.%2520As%2520a%2520result%2520of%2520GEPA%2527s%2520design%252C%2520it%250Acan%2520often%2520turn%2520even%2520just%2520a%2520few%2520rollouts%2520into%2520a%2520large%2520quality%2520gain.%2520Across%2520four%250Atasks%252C%2520GEPA%2520outperforms%2520GRPO%2520by%252010%2525%2520on%2520average%2520and%2520by%2520up%2520to%252020%2525%252C%2520while%2520using%2520up%250Ato%252035x%2520fewer%2520rollouts.%2520GEPA%2520also%2520outperforms%2520the%2520leading%2520prompt%2520optimizer%252C%250AMIPROv2%252C%2520by%2520over%252010%2525%2520across%2520two%2520LLMs%252C%2520and%2520demonstrates%2520promising%2520results%2520as%2520an%250Ainference-time%2520search%2520strategy%2520for%2520code%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEPA%3A%20Reflective%20Prompt%20Evolution%20Can%20Outperform%20Reinforcement%20Learning&entry.906535625=Lakshya%20A%20Agrawal%20and%20Shangyin%20Tan%20and%20Dilara%20Soylu%20and%20Noah%20Ziems%20and%20Rishi%20Khare%20and%20Krista%20Opsahl-Ong%20and%20Arnav%20Singhvi%20and%20Herumb%20Shandilya%20and%20Michael%20J%20Ryan%20and%20Meng%20Jiang%20and%20Christopher%20Potts%20and%20Koushik%20Sen%20and%20Alexandros%20G.%20Dimakis%20and%20Ion%20Stoica%20and%20Dan%20Klein%20and%20Matei%20Zaharia%20and%20Omar%20Khattab&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20adapted%20to%20downstream%20tasks%20via%0Areinforcement%20learning%20%28RL%29%20methods%20like%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20which%20often%20require%20thousands%20of%20rollouts%20to%20learn%20new%20tasks.%20We%20argue%0Athat%20the%20interpretable%20nature%20of%20language%20can%20often%20provide%20a%20much%20richer%0Alearning%20medium%20for%20LLMs%2C%20compared%20with%20policy%20gradients%20derived%20from%20sparse%2C%0Ascalar%20rewards.%20To%20test%20this%2C%20we%20introduce%20GEPA%20%28Genetic-Pareto%29%2C%20a%20prompt%0Aoptimizer%20that%20thoroughly%20incorporates%20natural%20language%20reflection%20to%20learn%0Ahigh-level%20rules%20from%20trial%20and%20error.%20Given%20any%20AI%20system%20containing%20one%20or%0Amore%20LLM%20prompts%2C%20GEPA%20samples%20system-level%20trajectories%20%28e.g.%2C%20reasoning%2C%20tool%0Acalls%2C%20and%20tool%20outputs%29%20and%20reflects%20on%20them%20in%20natural%20language%20to%20diagnose%0Aproblems%2C%20propose%20and%20test%20prompt%20updates%2C%20and%20combine%20complementary%20lessons%0Afrom%20the%20Pareto%20frontier%20of%20its%20own%20attempts.%20As%20a%20result%20of%20GEPA%27s%20design%2C%20it%0Acan%20often%20turn%20even%20just%20a%20few%20rollouts%20into%20a%20large%20quality%20gain.%20Across%20four%0Atasks%2C%20GEPA%20outperforms%20GRPO%20by%2010%25%20on%20average%20and%20by%20up%20to%2020%25%2C%20while%20using%20up%0Ato%2035x%20fewer%20rollouts.%20GEPA%20also%20outperforms%20the%20leading%20prompt%20optimizer%2C%0AMIPROv2%2C%20by%20over%2010%25%20across%20two%20LLMs%2C%20and%20demonstrates%20promising%20results%20as%20an%0Ainference-time%20search%20strategy%20for%20code%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19457v1&entry.124074799=Read"},
{"title": "On Arbitrary Predictions from Equally Valid Models", "author": "Sarah Lockfisch and Kristian Schwethelm and Martin Menten and Rickmer Braren and Daniel Rueckert and Alexander Ziller and Georgios Kaissis", "abstract": "  Model multiplicity refers to the existence of multiple machine learning\nmodels that describe the data equally well but may produce different\npredictions on individual samples. In medicine, these models can admit\nconflicting predictions for the same patient -- a risk that is poorly\nunderstood and insufficiently addressed.\n  In this study, we empirically analyze the extent, drivers, and ramifications\nof predictive multiplicity across diverse medical tasks and model\narchitectures, and show that even small ensembles can mitigate/eliminate\npredictive multiplicity in practice. Our analysis reveals that (1) standard\nvalidation metrics fail to identify a uniquely optimal model and (2) a\nsubstantial amount of predictions hinges on arbitrary choices made during model\ndevelopment. Using multiple models instead of a single model reveals instances\nwhere predictions differ across equally plausible models -- highlighting\npatients that would receive arbitrary diagnoses if any single model were used.\nIn contrast, (3) a small ensemble paired with an abstention strategy can\neffectively mitigate measurable predictive multiplicity in practice;\npredictions with high inter-model consensus may thus be amenable to automated\nclassification. While accuracy is not a principled antidote to predictive\nmultiplicity, we find that (4) higher accuracy achieved through increased model\ncapacity reduces predictive multiplicity.\n  Our findings underscore the clinical importance of accounting for model\nmultiplicity and advocate for ensemble-based strategies to improve diagnostic\nreliability. In cases where models fail to reach sufficient consensus, we\nrecommend deferring decisions to expert review.\n", "link": "http://arxiv.org/abs/2507.19408v1", "date": "2025-07-25", "relevancy": 1.4308, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4724}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Arbitrary%20Predictions%20from%20Equally%20Valid%20Models&body=Title%3A%20On%20Arbitrary%20Predictions%20from%20Equally%20Valid%20Models%0AAuthor%3A%20Sarah%20Lockfisch%20and%20Kristian%20Schwethelm%20and%20Martin%20Menten%20and%20Rickmer%20Braren%20and%20Daniel%20Rueckert%20and%20Alexander%20Ziller%20and%20Georgios%20Kaissis%0AAbstract%3A%20%20%20Model%20multiplicity%20refers%20to%20the%20existence%20of%20multiple%20machine%20learning%0Amodels%20that%20describe%20the%20data%20equally%20well%20but%20may%20produce%20different%0Apredictions%20on%20individual%20samples.%20In%20medicine%2C%20these%20models%20can%20admit%0Aconflicting%20predictions%20for%20the%20same%20patient%20--%20a%20risk%20that%20is%20poorly%0Aunderstood%20and%20insufficiently%20addressed.%0A%20%20In%20this%20study%2C%20we%20empirically%20analyze%20the%20extent%2C%20drivers%2C%20and%20ramifications%0Aof%20predictive%20multiplicity%20across%20diverse%20medical%20tasks%20and%20model%0Aarchitectures%2C%20and%20show%20that%20even%20small%20ensembles%20can%20mitigate/eliminate%0Apredictive%20multiplicity%20in%20practice.%20Our%20analysis%20reveals%20that%20%281%29%20standard%0Avalidation%20metrics%20fail%20to%20identify%20a%20uniquely%20optimal%20model%20and%20%282%29%20a%0Asubstantial%20amount%20of%20predictions%20hinges%20on%20arbitrary%20choices%20made%20during%20model%0Adevelopment.%20Using%20multiple%20models%20instead%20of%20a%20single%20model%20reveals%20instances%0Awhere%20predictions%20differ%20across%20equally%20plausible%20models%20--%20highlighting%0Apatients%20that%20would%20receive%20arbitrary%20diagnoses%20if%20any%20single%20model%20were%20used.%0AIn%20contrast%2C%20%283%29%20a%20small%20ensemble%20paired%20with%20an%20abstention%20strategy%20can%0Aeffectively%20mitigate%20measurable%20predictive%20multiplicity%20in%20practice%3B%0Apredictions%20with%20high%20inter-model%20consensus%20may%20thus%20be%20amenable%20to%20automated%0Aclassification.%20While%20accuracy%20is%20not%20a%20principled%20antidote%20to%20predictive%0Amultiplicity%2C%20we%20find%20that%20%284%29%20higher%20accuracy%20achieved%20through%20increased%20model%0Acapacity%20reduces%20predictive%20multiplicity.%0A%20%20Our%20findings%20underscore%20the%20clinical%20importance%20of%20accounting%20for%20model%0Amultiplicity%20and%20advocate%20for%20ensemble-based%20strategies%20to%20improve%20diagnostic%0Areliability.%20In%20cases%20where%20models%20fail%20to%20reach%20sufficient%20consensus%2C%20we%0Arecommend%20deferring%20decisions%20to%20expert%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Arbitrary%2520Predictions%2520from%2520Equally%2520Valid%2520Models%26entry.906535625%3DSarah%2520Lockfisch%2520and%2520Kristian%2520Schwethelm%2520and%2520Martin%2520Menten%2520and%2520Rickmer%2520Braren%2520and%2520Daniel%2520Rueckert%2520and%2520Alexander%2520Ziller%2520and%2520Georgios%2520Kaissis%26entry.1292438233%3D%2520%2520Model%2520multiplicity%2520refers%2520to%2520the%2520existence%2520of%2520multiple%2520machine%2520learning%250Amodels%2520that%2520describe%2520the%2520data%2520equally%2520well%2520but%2520may%2520produce%2520different%250Apredictions%2520on%2520individual%2520samples.%2520In%2520medicine%252C%2520these%2520models%2520can%2520admit%250Aconflicting%2520predictions%2520for%2520the%2520same%2520patient%2520--%2520a%2520risk%2520that%2520is%2520poorly%250Aunderstood%2520and%2520insufficiently%2520addressed.%250A%2520%2520In%2520this%2520study%252C%2520we%2520empirically%2520analyze%2520the%2520extent%252C%2520drivers%252C%2520and%2520ramifications%250Aof%2520predictive%2520multiplicity%2520across%2520diverse%2520medical%2520tasks%2520and%2520model%250Aarchitectures%252C%2520and%2520show%2520that%2520even%2520small%2520ensembles%2520can%2520mitigate/eliminate%250Apredictive%2520multiplicity%2520in%2520practice.%2520Our%2520analysis%2520reveals%2520that%2520%25281%2529%2520standard%250Avalidation%2520metrics%2520fail%2520to%2520identify%2520a%2520uniquely%2520optimal%2520model%2520and%2520%25282%2529%2520a%250Asubstantial%2520amount%2520of%2520predictions%2520hinges%2520on%2520arbitrary%2520choices%2520made%2520during%2520model%250Adevelopment.%2520Using%2520multiple%2520models%2520instead%2520of%2520a%2520single%2520model%2520reveals%2520instances%250Awhere%2520predictions%2520differ%2520across%2520equally%2520plausible%2520models%2520--%2520highlighting%250Apatients%2520that%2520would%2520receive%2520arbitrary%2520diagnoses%2520if%2520any%2520single%2520model%2520were%2520used.%250AIn%2520contrast%252C%2520%25283%2529%2520a%2520small%2520ensemble%2520paired%2520with%2520an%2520abstention%2520strategy%2520can%250Aeffectively%2520mitigate%2520measurable%2520predictive%2520multiplicity%2520in%2520practice%253B%250Apredictions%2520with%2520high%2520inter-model%2520consensus%2520may%2520thus%2520be%2520amenable%2520to%2520automated%250Aclassification.%2520While%2520accuracy%2520is%2520not%2520a%2520principled%2520antidote%2520to%2520predictive%250Amultiplicity%252C%2520we%2520find%2520that%2520%25284%2529%2520higher%2520accuracy%2520achieved%2520through%2520increased%2520model%250Acapacity%2520reduces%2520predictive%2520multiplicity.%250A%2520%2520Our%2520findings%2520underscore%2520the%2520clinical%2520importance%2520of%2520accounting%2520for%2520model%250Amultiplicity%2520and%2520advocate%2520for%2520ensemble-based%2520strategies%2520to%2520improve%2520diagnostic%250Areliability.%2520In%2520cases%2520where%2520models%2520fail%2520to%2520reach%2520sufficient%2520consensus%252C%2520we%250Arecommend%2520deferring%2520decisions%2520to%2520expert%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Arbitrary%20Predictions%20from%20Equally%20Valid%20Models&entry.906535625=Sarah%20Lockfisch%20and%20Kristian%20Schwethelm%20and%20Martin%20Menten%20and%20Rickmer%20Braren%20and%20Daniel%20Rueckert%20and%20Alexander%20Ziller%20and%20Georgios%20Kaissis&entry.1292438233=%20%20Model%20multiplicity%20refers%20to%20the%20existence%20of%20multiple%20machine%20learning%0Amodels%20that%20describe%20the%20data%20equally%20well%20but%20may%20produce%20different%0Apredictions%20on%20individual%20samples.%20In%20medicine%2C%20these%20models%20can%20admit%0Aconflicting%20predictions%20for%20the%20same%20patient%20--%20a%20risk%20that%20is%20poorly%0Aunderstood%20and%20insufficiently%20addressed.%0A%20%20In%20this%20study%2C%20we%20empirically%20analyze%20the%20extent%2C%20drivers%2C%20and%20ramifications%0Aof%20predictive%20multiplicity%20across%20diverse%20medical%20tasks%20and%20model%0Aarchitectures%2C%20and%20show%20that%20even%20small%20ensembles%20can%20mitigate/eliminate%0Apredictive%20multiplicity%20in%20practice.%20Our%20analysis%20reveals%20that%20%281%29%20standard%0Avalidation%20metrics%20fail%20to%20identify%20a%20uniquely%20optimal%20model%20and%20%282%29%20a%0Asubstantial%20amount%20of%20predictions%20hinges%20on%20arbitrary%20choices%20made%20during%20model%0Adevelopment.%20Using%20multiple%20models%20instead%20of%20a%20single%20model%20reveals%20instances%0Awhere%20predictions%20differ%20across%20equally%20plausible%20models%20--%20highlighting%0Apatients%20that%20would%20receive%20arbitrary%20diagnoses%20if%20any%20single%20model%20were%20used.%0AIn%20contrast%2C%20%283%29%20a%20small%20ensemble%20paired%20with%20an%20abstention%20strategy%20can%0Aeffectively%20mitigate%20measurable%20predictive%20multiplicity%20in%20practice%3B%0Apredictions%20with%20high%20inter-model%20consensus%20may%20thus%20be%20amenable%20to%20automated%0Aclassification.%20While%20accuracy%20is%20not%20a%20principled%20antidote%20to%20predictive%0Amultiplicity%2C%20we%20find%20that%20%284%29%20higher%20accuracy%20achieved%20through%20increased%20model%0Acapacity%20reduces%20predictive%20multiplicity.%0A%20%20Our%20findings%20underscore%20the%20clinical%20importance%20of%20accounting%20for%20model%0Amultiplicity%20and%20advocate%20for%20ensemble-based%20strategies%20to%20improve%20diagnostic%0Areliability.%20In%20cases%20where%20models%20fail%20to%20reach%20sufficient%20consensus%2C%20we%0Arecommend%20deferring%20decisions%20to%20expert%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19408v1&entry.124074799=Read"},
{"title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible\n  Whole Sky Camera Sequences", "author": "Gabriel Jarry and Ramon Dalmau and Philippe Very and Franck Ballerini and Stefania-Denisa Bocu", "abstract": "  Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.\n", "link": "http://arxiv.org/abs/2507.18330v2", "date": "2025-07-25", "relevancy": 1.428, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5484}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4746}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GVCCS%3A%20A%20Dataset%20for%20Contrail%20Identification%20and%20Tracking%20on%20Visible%0A%20%20Whole%20Sky%20Camera%20Sequences&body=Title%3A%20GVCCS%3A%20A%20Dataset%20for%20Contrail%20Identification%20and%20Tracking%20on%20Visible%0A%20%20Whole%20Sky%20Camera%20Sequences%0AAuthor%3A%20Gabriel%20Jarry%20and%20Ramon%20Dalmau%20and%20Philippe%20Very%20and%20Franck%20Ballerini%20and%20Stefania-Denisa%20Bocu%0AAbstract%3A%20%20%20Aviation%27s%20climate%20impact%20includes%20not%20only%20CO2%20emissions%20but%20also%0Asignificant%20non-CO2%20effects%2C%20especially%20from%20contrails.%20These%20ice%20clouds%20can%0Aalter%20Earth%27s%20radiative%20balance%2C%20potentially%20rivaling%20the%20warming%20effect%20of%0Aaviation%20CO2.%20Physics-based%20models%20provide%20useful%20estimates%20of%20contrail%0Aformation%20and%20climate%20impact%2C%20but%20their%20accuracy%20depends%20heavily%20on%20the%20quality%0Aof%20atmospheric%20input%20data%20and%20on%20assumptions%20used%20to%20represent%20complex%0Aprocesses%20like%20ice%20particle%20formation%20and%20humidity-driven%20persistence.%0AObservational%20data%20from%20remote%20sensors%2C%20such%20as%20satellites%20and%20ground%20cameras%2C%0Acould%20be%20used%20to%20validate%20and%20calibrate%20these%20models.%20However%2C%20existing%0Adatasets%20don%27t%20explore%20all%20aspect%20of%20contrail%20dynamics%20and%20formation%3A%20they%0Atypically%20lack%20temporal%20tracking%2C%20and%20do%20not%20attribute%20contrails%20to%20their%0Asource%20flights.%20To%20address%20these%20limitations%2C%20we%20present%20the%20Ground%20Visible%0ACamera%20Contrail%20Sequences%20%28GVCCS%29%2C%20a%20new%20open%20data%20set%20of%20contrails%20recorded%0Awith%20a%20ground-based%20all-sky%20camera%20in%20the%20visible%20range.%20Each%20contrail%20is%0Aindividually%20labeled%20and%20tracked%20over%20time%2C%20allowing%20a%20detailed%20analysis%20of%20its%0Alifecycle.%20The%20dataset%20contains%20122%20video%20sequences%20%2824%2C228%20frames%29%20and%0Aincludes%20flight%20identifiers%20for%20contrails%20that%20form%20above%20the%20camera.%20As%0Areference%2C%20we%20also%20propose%20a%20unified%20deep%20learning%20framework%20for%20contrail%0Aanalysis%20using%20a%20panoptic%20segmentation%20model%20that%20performs%20semantic%0Asegmentation%20%28contrail%20pixel%20identification%29%2C%20instance%20segmentation%20%28individual%0Acontrail%20separation%29%2C%20and%20temporal%20tracking%20in%20a%20single%20architecture.%20By%0Aproviding%20high-quality%2C%20temporally%20resolved%20annotations%20and%20a%20benchmark%20for%0Amodel%20evaluation%2C%20our%20work%20supports%20improved%20contrail%20monitoring%20and%20will%0Afacilitate%20better%20calibration%20of%20physical%20models.%20This%20sets%20the%20groundwork%20for%0Amore%20accurate%20climate%20impact%20understanding%20and%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGVCCS%253A%2520A%2520Dataset%2520for%2520Contrail%2520Identification%2520and%2520Tracking%2520on%2520Visible%250A%2520%2520Whole%2520Sky%2520Camera%2520Sequences%26entry.906535625%3DGabriel%2520Jarry%2520and%2520Ramon%2520Dalmau%2520and%2520Philippe%2520Very%2520and%2520Franck%2520Ballerini%2520and%2520Stefania-Denisa%2520Bocu%26entry.1292438233%3D%2520%2520Aviation%2527s%2520climate%2520impact%2520includes%2520not%2520only%2520CO2%2520emissions%2520but%2520also%250Asignificant%2520non-CO2%2520effects%252C%2520especially%2520from%2520contrails.%2520These%2520ice%2520clouds%2520can%250Aalter%2520Earth%2527s%2520radiative%2520balance%252C%2520potentially%2520rivaling%2520the%2520warming%2520effect%2520of%250Aaviation%2520CO2.%2520Physics-based%2520models%2520provide%2520useful%2520estimates%2520of%2520contrail%250Aformation%2520and%2520climate%2520impact%252C%2520but%2520their%2520accuracy%2520depends%2520heavily%2520on%2520the%2520quality%250Aof%2520atmospheric%2520input%2520data%2520and%2520on%2520assumptions%2520used%2520to%2520represent%2520complex%250Aprocesses%2520like%2520ice%2520particle%2520formation%2520and%2520humidity-driven%2520persistence.%250AObservational%2520data%2520from%2520remote%2520sensors%252C%2520such%2520as%2520satellites%2520and%2520ground%2520cameras%252C%250Acould%2520be%2520used%2520to%2520validate%2520and%2520calibrate%2520these%2520models.%2520However%252C%2520existing%250Adatasets%2520don%2527t%2520explore%2520all%2520aspect%2520of%2520contrail%2520dynamics%2520and%2520formation%253A%2520they%250Atypically%2520lack%2520temporal%2520tracking%252C%2520and%2520do%2520not%2520attribute%2520contrails%2520to%2520their%250Asource%2520flights.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520the%2520Ground%2520Visible%250ACamera%2520Contrail%2520Sequences%2520%2528GVCCS%2529%252C%2520a%2520new%2520open%2520data%2520set%2520of%2520contrails%2520recorded%250Awith%2520a%2520ground-based%2520all-sky%2520camera%2520in%2520the%2520visible%2520range.%2520Each%2520contrail%2520is%250Aindividually%2520labeled%2520and%2520tracked%2520over%2520time%252C%2520allowing%2520a%2520detailed%2520analysis%2520of%2520its%250Alifecycle.%2520The%2520dataset%2520contains%2520122%2520video%2520sequences%2520%252824%252C228%2520frames%2529%2520and%250Aincludes%2520flight%2520identifiers%2520for%2520contrails%2520that%2520form%2520above%2520the%2520camera.%2520As%250Areference%252C%2520we%2520also%2520propose%2520a%2520unified%2520deep%2520learning%2520framework%2520for%2520contrail%250Aanalysis%2520using%2520a%2520panoptic%2520segmentation%2520model%2520that%2520performs%2520semantic%250Asegmentation%2520%2528contrail%2520pixel%2520identification%2529%252C%2520instance%2520segmentation%2520%2528individual%250Acontrail%2520separation%2529%252C%2520and%2520temporal%2520tracking%2520in%2520a%2520single%2520architecture.%2520By%250Aproviding%2520high-quality%252C%2520temporally%2520resolved%2520annotations%2520and%2520a%2520benchmark%2520for%250Amodel%2520evaluation%252C%2520our%2520work%2520supports%2520improved%2520contrail%2520monitoring%2520and%2520will%250Afacilitate%2520better%2520calibration%2520of%2520physical%2520models.%2520This%2520sets%2520the%2520groundwork%2520for%250Amore%2520accurate%2520climate%2520impact%2520understanding%2520and%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GVCCS%3A%20A%20Dataset%20for%20Contrail%20Identification%20and%20Tracking%20on%20Visible%0A%20%20Whole%20Sky%20Camera%20Sequences&entry.906535625=Gabriel%20Jarry%20and%20Ramon%20Dalmau%20and%20Philippe%20Very%20and%20Franck%20Ballerini%20and%20Stefania-Denisa%20Bocu&entry.1292438233=%20%20Aviation%27s%20climate%20impact%20includes%20not%20only%20CO2%20emissions%20but%20also%0Asignificant%20non-CO2%20effects%2C%20especially%20from%20contrails.%20These%20ice%20clouds%20can%0Aalter%20Earth%27s%20radiative%20balance%2C%20potentially%20rivaling%20the%20warming%20effect%20of%0Aaviation%20CO2.%20Physics-based%20models%20provide%20useful%20estimates%20of%20contrail%0Aformation%20and%20climate%20impact%2C%20but%20their%20accuracy%20depends%20heavily%20on%20the%20quality%0Aof%20atmospheric%20input%20data%20and%20on%20assumptions%20used%20to%20represent%20complex%0Aprocesses%20like%20ice%20particle%20formation%20and%20humidity-driven%20persistence.%0AObservational%20data%20from%20remote%20sensors%2C%20such%20as%20satellites%20and%20ground%20cameras%2C%0Acould%20be%20used%20to%20validate%20and%20calibrate%20these%20models.%20However%2C%20existing%0Adatasets%20don%27t%20explore%20all%20aspect%20of%20contrail%20dynamics%20and%20formation%3A%20they%0Atypically%20lack%20temporal%20tracking%2C%20and%20do%20not%20attribute%20contrails%20to%20their%0Asource%20flights.%20To%20address%20these%20limitations%2C%20we%20present%20the%20Ground%20Visible%0ACamera%20Contrail%20Sequences%20%28GVCCS%29%2C%20a%20new%20open%20data%20set%20of%20contrails%20recorded%0Awith%20a%20ground-based%20all-sky%20camera%20in%20the%20visible%20range.%20Each%20contrail%20is%0Aindividually%20labeled%20and%20tracked%20over%20time%2C%20allowing%20a%20detailed%20analysis%20of%20its%0Alifecycle.%20The%20dataset%20contains%20122%20video%20sequences%20%2824%2C228%20frames%29%20and%0Aincludes%20flight%20identifiers%20for%20contrails%20that%20form%20above%20the%20camera.%20As%0Areference%2C%20we%20also%20propose%20a%20unified%20deep%20learning%20framework%20for%20contrail%0Aanalysis%20using%20a%20panoptic%20segmentation%20model%20that%20performs%20semantic%0Asegmentation%20%28contrail%20pixel%20identification%29%2C%20instance%20segmentation%20%28individual%0Acontrail%20separation%29%2C%20and%20temporal%20tracking%20in%20a%20single%20architecture.%20By%0Aproviding%20high-quality%2C%20temporally%20resolved%20annotations%20and%20a%20benchmark%20for%0Amodel%20evaluation%2C%20our%20work%20supports%20improved%20contrail%20monitoring%20and%20will%0Afacilitate%20better%20calibration%20of%20physical%20models.%20This%20sets%20the%20groundwork%20for%0Amore%20accurate%20climate%20impact%20understanding%20and%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18330v2&entry.124074799=Read"},
{"title": "Gradient-based grand canonical optimization enabled by graph neural\n  networks with fractional atomic existence", "author": "Mads-Peter Verner Christiansen and Bj\u00f8rk Hammer", "abstract": "  Machine learning interatomic potentials have become an indispensable tool for\nmaterials science, enabling the study of larger systems and longer timescales.\nState-of-the-art models are generally graph neural networks that employ message\npassing to iteratively update atomic embeddings that are ultimately used for\npredicting properties. In this work we extend the message passing formalism\nwith the inclusion of a continuous variable that accounts for fractional atomic\nexistence. This allows us to calculate the gradient of the Gibbs free energy\nwith respect to both the Cartesian coordinates of atoms and their existence.\nUsing this we propose a gradient-based grand canonical optimization method and\ndocument its capabilities for a Cu(110) surface oxide.\n", "link": "http://arxiv.org/abs/2507.19438v1", "date": "2025-07-25", "relevancy": 1.4267, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4888}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4648}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-based%20grand%20canonical%20optimization%20enabled%20by%20graph%20neural%0A%20%20networks%20with%20fractional%20atomic%20existence&body=Title%3A%20Gradient-based%20grand%20canonical%20optimization%20enabled%20by%20graph%20neural%0A%20%20networks%20with%20fractional%20atomic%20existence%0AAuthor%3A%20Mads-Peter%20Verner%20Christiansen%20and%20Bj%C3%B8rk%20Hammer%0AAbstract%3A%20%20%20Machine%20learning%20interatomic%20potentials%20have%20become%20an%20indispensable%20tool%20for%0Amaterials%20science%2C%20enabling%20the%20study%20of%20larger%20systems%20and%20longer%20timescales.%0AState-of-the-art%20models%20are%20generally%20graph%20neural%20networks%20that%20employ%20message%0Apassing%20to%20iteratively%20update%20atomic%20embeddings%20that%20are%20ultimately%20used%20for%0Apredicting%20properties.%20In%20this%20work%20we%20extend%20the%20message%20passing%20formalism%0Awith%20the%20inclusion%20of%20a%20continuous%20variable%20that%20accounts%20for%20fractional%20atomic%0Aexistence.%20This%20allows%20us%20to%20calculate%20the%20gradient%20of%20the%20Gibbs%20free%20energy%0Awith%20respect%20to%20both%20the%20Cartesian%20coordinates%20of%20atoms%20and%20their%20existence.%0AUsing%20this%20we%20propose%20a%20gradient-based%20grand%20canonical%20optimization%20method%20and%0Adocument%20its%20capabilities%20for%20a%20Cu%28110%29%20surface%20oxide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-based%2520grand%2520canonical%2520optimization%2520enabled%2520by%2520graph%2520neural%250A%2520%2520networks%2520with%2520fractional%2520atomic%2520existence%26entry.906535625%3DMads-Peter%2520Verner%2520Christiansen%2520and%2520Bj%25C3%25B8rk%2520Hammer%26entry.1292438233%3D%2520%2520Machine%2520learning%2520interatomic%2520potentials%2520have%2520become%2520an%2520indispensable%2520tool%2520for%250Amaterials%2520science%252C%2520enabling%2520the%2520study%2520of%2520larger%2520systems%2520and%2520longer%2520timescales.%250AState-of-the-art%2520models%2520are%2520generally%2520graph%2520neural%2520networks%2520that%2520employ%2520message%250Apassing%2520to%2520iteratively%2520update%2520atomic%2520embeddings%2520that%2520are%2520ultimately%2520used%2520for%250Apredicting%2520properties.%2520In%2520this%2520work%2520we%2520extend%2520the%2520message%2520passing%2520formalism%250Awith%2520the%2520inclusion%2520of%2520a%2520continuous%2520variable%2520that%2520accounts%2520for%2520fractional%2520atomic%250Aexistence.%2520This%2520allows%2520us%2520to%2520calculate%2520the%2520gradient%2520of%2520the%2520Gibbs%2520free%2520energy%250Awith%2520respect%2520to%2520both%2520the%2520Cartesian%2520coordinates%2520of%2520atoms%2520and%2520their%2520existence.%250AUsing%2520this%2520we%2520propose%2520a%2520gradient-based%2520grand%2520canonical%2520optimization%2520method%2520and%250Adocument%2520its%2520capabilities%2520for%2520a%2520Cu%2528110%2529%2520surface%2520oxide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20grand%20canonical%20optimization%20enabled%20by%20graph%20neural%0A%20%20networks%20with%20fractional%20atomic%20existence&entry.906535625=Mads-Peter%20Verner%20Christiansen%20and%20Bj%C3%B8rk%20Hammer&entry.1292438233=%20%20Machine%20learning%20interatomic%20potentials%20have%20become%20an%20indispensable%20tool%20for%0Amaterials%20science%2C%20enabling%20the%20study%20of%20larger%20systems%20and%20longer%20timescales.%0AState-of-the-art%20models%20are%20generally%20graph%20neural%20networks%20that%20employ%20message%0Apassing%20to%20iteratively%20update%20atomic%20embeddings%20that%20are%20ultimately%20used%20for%0Apredicting%20properties.%20In%20this%20work%20we%20extend%20the%20message%20passing%20formalism%0Awith%20the%20inclusion%20of%20a%20continuous%20variable%20that%20accounts%20for%20fractional%20atomic%0Aexistence.%20This%20allows%20us%20to%20calculate%20the%20gradient%20of%20the%20Gibbs%20free%20energy%0Awith%20respect%20to%20both%20the%20Cartesian%20coordinates%20of%20atoms%20and%20their%20existence.%0AUsing%20this%20we%20propose%20a%20gradient-based%20grand%20canonical%20optimization%20method%20and%0Adocument%20its%20capabilities%20for%20a%20Cu%28110%29%20surface%20oxide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19438v1&entry.124074799=Read"},
{"title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset\n  Management Under Budget Constraints", "author": "Amir Fard and Arnold X. -X. Yuan", "abstract": "  Budget planning and maintenance optimization are crucial for infrastructure\nasset management, ensuring cost-effectiveness and sustainability. However, the\ncomplexity arising from combinatorial action spaces, diverse asset\ndeterioration, stringent budget constraints, and environmental uncertainty\nsignificantly limits existing methods' scalability. This paper proposes a\nHierarchical Deep Reinforcement Learning methodology specifically tailored to\nmulti-year infrastructure planning. Our approach decomposes the problem into\ntwo hierarchical levels: a high-level Budget Planner allocating annual budgets\nwithin explicit feasibility bounds, and a low-level Maintenance Planner\nprioritizing assets within the allocated budget. By structurally separating\nmacro-budget decisions from asset-level prioritization and integrating linear\nprogramming projection within a hierarchical Soft Actor-Critic framework, the\nmethod efficiently addresses exponential growth in the action space and ensures\nrigorous budget compliance. A case study evaluating sewer networks of varying\nsizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed\napproach. Compared to conventional Deep Q-Learning and enhanced genetic\nalgorithms, our methodology converges more rapidly, scales effectively, and\nconsistently delivers near-optimal solutions even as network size grows.\n", "link": "http://arxiv.org/abs/2507.19458v1", "date": "2025-07-25", "relevancy": 1.4119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4865}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4768}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Deep%20Reinforcement%20Learning%20Framework%20for%20Multi-Year%20Asset%0A%20%20Management%20Under%20Budget%20Constraints&body=Title%3A%20Hierarchical%20Deep%20Reinforcement%20Learning%20Framework%20for%20Multi-Year%20Asset%0A%20%20Management%20Under%20Budget%20Constraints%0AAuthor%3A%20Amir%20Fard%20and%20Arnold%20X.%20-X.%20Yuan%0AAbstract%3A%20%20%20Budget%20planning%20and%20maintenance%20optimization%20are%20crucial%20for%20infrastructure%0Aasset%20management%2C%20ensuring%20cost-effectiveness%20and%20sustainability.%20However%2C%20the%0Acomplexity%20arising%20from%20combinatorial%20action%20spaces%2C%20diverse%20asset%0Adeterioration%2C%20stringent%20budget%20constraints%2C%20and%20environmental%20uncertainty%0Asignificantly%20limits%20existing%20methods%27%20scalability.%20This%20paper%20proposes%20a%0AHierarchical%20Deep%20Reinforcement%20Learning%20methodology%20specifically%20tailored%20to%0Amulti-year%20infrastructure%20planning.%20Our%20approach%20decomposes%20the%20problem%20into%0Atwo%20hierarchical%20levels%3A%20a%20high-level%20Budget%20Planner%20allocating%20annual%20budgets%0Awithin%20explicit%20feasibility%20bounds%2C%20and%20a%20low-level%20Maintenance%20Planner%0Aprioritizing%20assets%20within%20the%20allocated%20budget.%20By%20structurally%20separating%0Amacro-budget%20decisions%20from%20asset-level%20prioritization%20and%20integrating%20linear%0Aprogramming%20projection%20within%20a%20hierarchical%20Soft%20Actor-Critic%20framework%2C%20the%0Amethod%20efficiently%20addresses%20exponential%20growth%20in%20the%20action%20space%20and%20ensures%0Arigorous%20budget%20compliance.%20A%20case%20study%20evaluating%20sewer%20networks%20of%20varying%0Asizes%20%2810%2C%2015%2C%20and%2020%20sewersheds%29%20illustrates%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20Compared%20to%20conventional%20Deep%20Q-Learning%20and%20enhanced%20genetic%0Aalgorithms%2C%20our%20methodology%20converges%20more%20rapidly%2C%20scales%20effectively%2C%20and%0Aconsistently%20delivers%20near-optimal%20solutions%20even%20as%20network%20size%20grows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Deep%2520Reinforcement%2520Learning%2520Framework%2520for%2520Multi-Year%2520Asset%250A%2520%2520Management%2520Under%2520Budget%2520Constraints%26entry.906535625%3DAmir%2520Fard%2520and%2520Arnold%2520X.%2520-X.%2520Yuan%26entry.1292438233%3D%2520%2520Budget%2520planning%2520and%2520maintenance%2520optimization%2520are%2520crucial%2520for%2520infrastructure%250Aasset%2520management%252C%2520ensuring%2520cost-effectiveness%2520and%2520sustainability.%2520However%252C%2520the%250Acomplexity%2520arising%2520from%2520combinatorial%2520action%2520spaces%252C%2520diverse%2520asset%250Adeterioration%252C%2520stringent%2520budget%2520constraints%252C%2520and%2520environmental%2520uncertainty%250Asignificantly%2520limits%2520existing%2520methods%2527%2520scalability.%2520This%2520paper%2520proposes%2520a%250AHierarchical%2520Deep%2520Reinforcement%2520Learning%2520methodology%2520specifically%2520tailored%2520to%250Amulti-year%2520infrastructure%2520planning.%2520Our%2520approach%2520decomposes%2520the%2520problem%2520into%250Atwo%2520hierarchical%2520levels%253A%2520a%2520high-level%2520Budget%2520Planner%2520allocating%2520annual%2520budgets%250Awithin%2520explicit%2520feasibility%2520bounds%252C%2520and%2520a%2520low-level%2520Maintenance%2520Planner%250Aprioritizing%2520assets%2520within%2520the%2520allocated%2520budget.%2520By%2520structurally%2520separating%250Amacro-budget%2520decisions%2520from%2520asset-level%2520prioritization%2520and%2520integrating%2520linear%250Aprogramming%2520projection%2520within%2520a%2520hierarchical%2520Soft%2520Actor-Critic%2520framework%252C%2520the%250Amethod%2520efficiently%2520addresses%2520exponential%2520growth%2520in%2520the%2520action%2520space%2520and%2520ensures%250Arigorous%2520budget%2520compliance.%2520A%2520case%2520study%2520evaluating%2520sewer%2520networks%2520of%2520varying%250Asizes%2520%252810%252C%252015%252C%2520and%252020%2520sewersheds%2529%2520illustrates%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aapproach.%2520Compared%2520to%2520conventional%2520Deep%2520Q-Learning%2520and%2520enhanced%2520genetic%250Aalgorithms%252C%2520our%2520methodology%2520converges%2520more%2520rapidly%252C%2520scales%2520effectively%252C%2520and%250Aconsistently%2520delivers%2520near-optimal%2520solutions%2520even%2520as%2520network%2520size%2520grows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Deep%20Reinforcement%20Learning%20Framework%20for%20Multi-Year%20Asset%0A%20%20Management%20Under%20Budget%20Constraints&entry.906535625=Amir%20Fard%20and%20Arnold%20X.%20-X.%20Yuan&entry.1292438233=%20%20Budget%20planning%20and%20maintenance%20optimization%20are%20crucial%20for%20infrastructure%0Aasset%20management%2C%20ensuring%20cost-effectiveness%20and%20sustainability.%20However%2C%20the%0Acomplexity%20arising%20from%20combinatorial%20action%20spaces%2C%20diverse%20asset%0Adeterioration%2C%20stringent%20budget%20constraints%2C%20and%20environmental%20uncertainty%0Asignificantly%20limits%20existing%20methods%27%20scalability.%20This%20paper%20proposes%20a%0AHierarchical%20Deep%20Reinforcement%20Learning%20methodology%20specifically%20tailored%20to%0Amulti-year%20infrastructure%20planning.%20Our%20approach%20decomposes%20the%20problem%20into%0Atwo%20hierarchical%20levels%3A%20a%20high-level%20Budget%20Planner%20allocating%20annual%20budgets%0Awithin%20explicit%20feasibility%20bounds%2C%20and%20a%20low-level%20Maintenance%20Planner%0Aprioritizing%20assets%20within%20the%20allocated%20budget.%20By%20structurally%20separating%0Amacro-budget%20decisions%20from%20asset-level%20prioritization%20and%20integrating%20linear%0Aprogramming%20projection%20within%20a%20hierarchical%20Soft%20Actor-Critic%20framework%2C%20the%0Amethod%20efficiently%20addresses%20exponential%20growth%20in%20the%20action%20space%20and%20ensures%0Arigorous%20budget%20compliance.%20A%20case%20study%20evaluating%20sewer%20networks%20of%20varying%0Asizes%20%2810%2C%2015%2C%20and%2020%20sewersheds%29%20illustrates%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20Compared%20to%20conventional%20Deep%20Q-Learning%20and%20enhanced%20genetic%0Aalgorithms%2C%20our%20methodology%20converges%20more%20rapidly%2C%20scales%20effectively%2C%20and%0Aconsistently%20delivers%20near-optimal%20solutions%20even%20as%20network%20size%20grows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19458v1&entry.124074799=Read"},
{"title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  Agents", "author": "Xuehui Wang and Zhenyu Wu and JingJing Xie and Zichen Ding and Bowen Yang and Zehao Li and Zhaoyang Liu and Qingyun Li and Xuan Dong and Zhe Chen and Weiyun Wang and Xiangyu Zhao and Jixuan Chen and Haodong Duan and Tianbao Xie and Chenyu Yang and Shiqian Su and Yue Yu and Yuan Huang and Yiqian Liu and Xiao Zhang and Yanting Zhang and Xiangyu Yue and Weijie Su and Xizhou Zhu and Wei Shen and Jifeng Dai and Wenhai Wang", "abstract": "  We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.\n", "link": "http://arxiv.org/abs/2507.19478v1", "date": "2025-07-25", "relevancy": 1.3976, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMBench-GUI%3A%20Hierarchical%20Multi-Platform%20Evaluation%20Framework%20for%20GUI%0A%20%20Agents&body=Title%3A%20MMBench-GUI%3A%20Hierarchical%20Multi-Platform%20Evaluation%20Framework%20for%20GUI%0A%20%20Agents%0AAuthor%3A%20Xuehui%20Wang%20and%20Zhenyu%20Wu%20and%20JingJing%20Xie%20and%20Zichen%20Ding%20and%20Bowen%20Yang%20and%20Zehao%20Li%20and%20Zhaoyang%20Liu%20and%20Qingyun%20Li%20and%20Xuan%20Dong%20and%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jixuan%20Chen%20and%20Haodong%20Duan%20and%20Tianbao%20Xie%20and%20Chenyu%20Yang%20and%20Shiqian%20Su%20and%20Yue%20Yu%20and%20Yuan%20Huang%20and%20Yiqian%20Liu%20and%20Xiao%20Zhang%20and%20Yanting%20Zhang%20and%20Xiangyu%20Yue%20and%20Weijie%20Su%20and%20Xizhou%20Zhu%20and%20Wei%20Shen%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20We%20introduce%20MMBench-GUI%2C%20a%20hierarchical%20benchmark%20for%20evaluating%20GUI%0Aautomation%20agents%20across%20Windows%2C%20macOS%2C%20Linux%2C%20iOS%2C%20Android%2C%20and%20Web%0Aplatforms.%20It%20comprises%20four%20levels%3A%20GUI%20Content%20Understanding%2C%20Element%0AGrounding%2C%20Task%20Automation%2C%20and%20Task%20Collaboration%2C%20covering%20essential%20skills%0Afor%20GUI%20agents.%20In%20addition%2C%20we%20propose%20a%20novel%20Efficiency-Quality%20Area%20%28EQA%29%0Ametric%20to%20assess%20GUI%20agent%20execution%20efficiency%20in%20online%20automation%20scenarios.%0AThrough%20MMBench-GUI%2C%20we%20identify%20accurate%20visual%20grounding%20as%20a%20critical%0Adeterminant%20of%20overall%20task%20success%2C%20emphasizing%20the%20substantial%20benefits%20of%0Amodular%20frameworks%20that%20integrate%20specialized%20grounding%20modules.%20Furthermore%2C%0Ato%20achieve%20reliable%20GUI%20automation%2C%20an%20agent%20requires%20strong%20task%20planning%20and%0Across-platform%20generalization%20abilities%2C%20with%20long-context%20memory%2C%20a%20broad%0Aaction%20space%2C%20and%20long-term%20reasoning%20playing%20a%20critical%20role.%20More%20important%2C%0Atask%20efficiency%20remains%20a%20critically%20underexplored%20dimension%2C%20and%20all%20models%0Asuffer%20from%20substantial%20inefficiencies%2C%20with%20excessive%20redundant%20steps%20even%0Awhen%20tasks%20are%20ultimately%20completed.%20The%20integration%20of%20precise%20localization%2C%0Aeffective%20planning%2C%20and%20early%20stopping%20strategies%20is%20indispensable%20to%20enable%0Atruly%20efficient%20and%20scalable%20GUI%20automation.%20Our%20benchmark%20code%2C%20evaluation%0Adata%2C%20and%20running%20environment%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/open-compass/MMBench-GUI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMBench-GUI%253A%2520Hierarchical%2520Multi-Platform%2520Evaluation%2520Framework%2520for%2520GUI%250A%2520%2520Agents%26entry.906535625%3DXuehui%2520Wang%2520and%2520Zhenyu%2520Wu%2520and%2520JingJing%2520Xie%2520and%2520Zichen%2520Ding%2520and%2520Bowen%2520Yang%2520and%2520Zehao%2520Li%2520and%2520Zhaoyang%2520Liu%2520and%2520Qingyun%2520Li%2520and%2520Xuan%2520Dong%2520and%2520Zhe%2520Chen%2520and%2520Weiyun%2520Wang%2520and%2520Xiangyu%2520Zhao%2520and%2520Jixuan%2520Chen%2520and%2520Haodong%2520Duan%2520and%2520Tianbao%2520Xie%2520and%2520Chenyu%2520Yang%2520and%2520Shiqian%2520Su%2520and%2520Yue%2520Yu%2520and%2520Yuan%2520Huang%2520and%2520Yiqian%2520Liu%2520and%2520Xiao%2520Zhang%2520and%2520Yanting%2520Zhang%2520and%2520Xiangyu%2520Yue%2520and%2520Weijie%2520Su%2520and%2520Xizhou%2520Zhu%2520and%2520Wei%2520Shen%2520and%2520Jifeng%2520Dai%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520MMBench-GUI%252C%2520a%2520hierarchical%2520benchmark%2520for%2520evaluating%2520GUI%250Aautomation%2520agents%2520across%2520Windows%252C%2520macOS%252C%2520Linux%252C%2520iOS%252C%2520Android%252C%2520and%2520Web%250Aplatforms.%2520It%2520comprises%2520four%2520levels%253A%2520GUI%2520Content%2520Understanding%252C%2520Element%250AGrounding%252C%2520Task%2520Automation%252C%2520and%2520Task%2520Collaboration%252C%2520covering%2520essential%2520skills%250Afor%2520GUI%2520agents.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520Efficiency-Quality%2520Area%2520%2528EQA%2529%250Ametric%2520to%2520assess%2520GUI%2520agent%2520execution%2520efficiency%2520in%2520online%2520automation%2520scenarios.%250AThrough%2520MMBench-GUI%252C%2520we%2520identify%2520accurate%2520visual%2520grounding%2520as%2520a%2520critical%250Adeterminant%2520of%2520overall%2520task%2520success%252C%2520emphasizing%2520the%2520substantial%2520benefits%2520of%250Amodular%2520frameworks%2520that%2520integrate%2520specialized%2520grounding%2520modules.%2520Furthermore%252C%250Ato%2520achieve%2520reliable%2520GUI%2520automation%252C%2520an%2520agent%2520requires%2520strong%2520task%2520planning%2520and%250Across-platform%2520generalization%2520abilities%252C%2520with%2520long-context%2520memory%252C%2520a%2520broad%250Aaction%2520space%252C%2520and%2520long-term%2520reasoning%2520playing%2520a%2520critical%2520role.%2520More%2520important%252C%250Atask%2520efficiency%2520remains%2520a%2520critically%2520underexplored%2520dimension%252C%2520and%2520all%2520models%250Asuffer%2520from%2520substantial%2520inefficiencies%252C%2520with%2520excessive%2520redundant%2520steps%2520even%250Awhen%2520tasks%2520are%2520ultimately%2520completed.%2520The%2520integration%2520of%2520precise%2520localization%252C%250Aeffective%2520planning%252C%2520and%2520early%2520stopping%2520strategies%2520is%2520indispensable%2520to%2520enable%250Atruly%2520efficient%2520and%2520scalable%2520GUI%2520automation.%2520Our%2520benchmark%2520code%252C%2520evaluation%250Adata%252C%2520and%2520running%2520environment%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/open-compass/MMBench-GUI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMBench-GUI%3A%20Hierarchical%20Multi-Platform%20Evaluation%20Framework%20for%20GUI%0A%20%20Agents&entry.906535625=Xuehui%20Wang%20and%20Zhenyu%20Wu%20and%20JingJing%20Xie%20and%20Zichen%20Ding%20and%20Bowen%20Yang%20and%20Zehao%20Li%20and%20Zhaoyang%20Liu%20and%20Qingyun%20Li%20and%20Xuan%20Dong%20and%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Xiangyu%20Zhao%20and%20Jixuan%20Chen%20and%20Haodong%20Duan%20and%20Tianbao%20Xie%20and%20Chenyu%20Yang%20and%20Shiqian%20Su%20and%20Yue%20Yu%20and%20Yuan%20Huang%20and%20Yiqian%20Liu%20and%20Xiao%20Zhang%20and%20Yanting%20Zhang%20and%20Xiangyu%20Yue%20and%20Weijie%20Su%20and%20Xizhou%20Zhu%20and%20Wei%20Shen%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang&entry.1292438233=%20%20We%20introduce%20MMBench-GUI%2C%20a%20hierarchical%20benchmark%20for%20evaluating%20GUI%0Aautomation%20agents%20across%20Windows%2C%20macOS%2C%20Linux%2C%20iOS%2C%20Android%2C%20and%20Web%0Aplatforms.%20It%20comprises%20four%20levels%3A%20GUI%20Content%20Understanding%2C%20Element%0AGrounding%2C%20Task%20Automation%2C%20and%20Task%20Collaboration%2C%20covering%20essential%20skills%0Afor%20GUI%20agents.%20In%20addition%2C%20we%20propose%20a%20novel%20Efficiency-Quality%20Area%20%28EQA%29%0Ametric%20to%20assess%20GUI%20agent%20execution%20efficiency%20in%20online%20automation%20scenarios.%0AThrough%20MMBench-GUI%2C%20we%20identify%20accurate%20visual%20grounding%20as%20a%20critical%0Adeterminant%20of%20overall%20task%20success%2C%20emphasizing%20the%20substantial%20benefits%20of%0Amodular%20frameworks%20that%20integrate%20specialized%20grounding%20modules.%20Furthermore%2C%0Ato%20achieve%20reliable%20GUI%20automation%2C%20an%20agent%20requires%20strong%20task%20planning%20and%0Across-platform%20generalization%20abilities%2C%20with%20long-context%20memory%2C%20a%20broad%0Aaction%20space%2C%20and%20long-term%20reasoning%20playing%20a%20critical%20role.%20More%20important%2C%0Atask%20efficiency%20remains%20a%20critically%20underexplored%20dimension%2C%20and%20all%20models%0Asuffer%20from%20substantial%20inefficiencies%2C%20with%20excessive%20redundant%20steps%20even%0Awhen%20tasks%20are%20ultimately%20completed.%20The%20integration%20of%20precise%20localization%2C%0Aeffective%20planning%2C%20and%20early%20stopping%20strategies%20is%20indispensable%20to%20enable%0Atruly%20efficient%20and%20scalable%20GUI%20automation.%20Our%20benchmark%20code%2C%20evaluation%0Adata%2C%20and%20running%20environment%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/open-compass/MMBench-GUI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19478v1&entry.124074799=Read"},
{"title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal\n  Data", "author": "Eric V. Strobl", "abstract": "  Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia.\n", "link": "http://arxiv.org/abs/2506.16629v4", "date": "2025-07-25", "relevancy": 1.3964, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4673}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Causally%20Predictable%20Outcomes%20from%20Psychiatric%20Longitudinal%0A%20%20Data&body=Title%3A%20Learning%20Causally%20Predictable%20Outcomes%20from%20Psychiatric%20Longitudinal%0A%20%20Data%0AAuthor%3A%20Eric%20V.%20Strobl%0AAbstract%3A%20%20%20Causal%20inference%20in%20longitudinal%20biomedical%20data%20remains%20a%20central%20challenge%2C%0Aespecially%20in%20psychiatry%2C%20where%20symptom%20heterogeneity%20and%20latent%20confounding%0Afrequently%20undermine%20classical%20estimators.%20Most%20existing%20methods%20for%20treatment%0Aeffect%20estimation%20presuppose%20a%20fixed%20outcome%20variable%20and%20address%20confounding%0Athrough%20observed%20covariate%20adjustment.%20However%2C%20the%20assumption%20of%0Aunconfoundedness%20may%20not%20hold%20for%20a%20fixed%20outcome%20in%20practice.%20To%20address%20this%0Afoundational%20limitation%2C%20we%20directly%20optimize%20the%20outcome%20definition%20to%0Amaximize%20causal%20identifiability.%20Our%20DEBIAS%20%28Durable%20Effects%20with%0ABackdoor-Invariant%20Aggregated%20Symptoms%29%20algorithm%20learns%20non-negative%2C%0Aclinically%20interpretable%20weights%20for%20outcome%20aggregation%2C%20maximizing%20durable%0Atreatment%20effects%20and%20empirically%20minimizing%20both%20observed%20and%20latent%0Aconfounding%20by%20leveraging%20the%20time-limited%20direct%20effects%20of%20prior%20treatments%0Ain%20psychiatric%20longitudinal%20data.%20The%20algorithm%20also%20furnishes%20an%20empirically%0Averifiable%20test%20for%20outcome%20unconfoundedness.%20DEBIAS%20consistently%20outperforms%0Astate-of-the-art%20methods%20in%20recovering%20causal%20effects%20for%20clinically%0Ainterpretable%20composite%20outcomes%20across%20comprehensive%20experiments%20in%20depression%0Aand%20schizophrenia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16629v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Causally%2520Predictable%2520Outcomes%2520from%2520Psychiatric%2520Longitudinal%250A%2520%2520Data%26entry.906535625%3DEric%2520V.%2520Strobl%26entry.1292438233%3D%2520%2520Causal%2520inference%2520in%2520longitudinal%2520biomedical%2520data%2520remains%2520a%2520central%2520challenge%252C%250Aespecially%2520in%2520psychiatry%252C%2520where%2520symptom%2520heterogeneity%2520and%2520latent%2520confounding%250Afrequently%2520undermine%2520classical%2520estimators.%2520Most%2520existing%2520methods%2520for%2520treatment%250Aeffect%2520estimation%2520presuppose%2520a%2520fixed%2520outcome%2520variable%2520and%2520address%2520confounding%250Athrough%2520observed%2520covariate%2520adjustment.%2520However%252C%2520the%2520assumption%2520of%250Aunconfoundedness%2520may%2520not%2520hold%2520for%2520a%2520fixed%2520outcome%2520in%2520practice.%2520To%2520address%2520this%250Afoundational%2520limitation%252C%2520we%2520directly%2520optimize%2520the%2520outcome%2520definition%2520to%250Amaximize%2520causal%2520identifiability.%2520Our%2520DEBIAS%2520%2528Durable%2520Effects%2520with%250ABackdoor-Invariant%2520Aggregated%2520Symptoms%2529%2520algorithm%2520learns%2520non-negative%252C%250Aclinically%2520interpretable%2520weights%2520for%2520outcome%2520aggregation%252C%2520maximizing%2520durable%250Atreatment%2520effects%2520and%2520empirically%2520minimizing%2520both%2520observed%2520and%2520latent%250Aconfounding%2520by%2520leveraging%2520the%2520time-limited%2520direct%2520effects%2520of%2520prior%2520treatments%250Ain%2520psychiatric%2520longitudinal%2520data.%2520The%2520algorithm%2520also%2520furnishes%2520an%2520empirically%250Averifiable%2520test%2520for%2520outcome%2520unconfoundedness.%2520DEBIAS%2520consistently%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520recovering%2520causal%2520effects%2520for%2520clinically%250Ainterpretable%2520composite%2520outcomes%2520across%2520comprehensive%2520experiments%2520in%2520depression%250Aand%2520schizophrenia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16629v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Causally%20Predictable%20Outcomes%20from%20Psychiatric%20Longitudinal%0A%20%20Data&entry.906535625=Eric%20V.%20Strobl&entry.1292438233=%20%20Causal%20inference%20in%20longitudinal%20biomedical%20data%20remains%20a%20central%20challenge%2C%0Aespecially%20in%20psychiatry%2C%20where%20symptom%20heterogeneity%20and%20latent%20confounding%0Afrequently%20undermine%20classical%20estimators.%20Most%20existing%20methods%20for%20treatment%0Aeffect%20estimation%20presuppose%20a%20fixed%20outcome%20variable%20and%20address%20confounding%0Athrough%20observed%20covariate%20adjustment.%20However%2C%20the%20assumption%20of%0Aunconfoundedness%20may%20not%20hold%20for%20a%20fixed%20outcome%20in%20practice.%20To%20address%20this%0Afoundational%20limitation%2C%20we%20directly%20optimize%20the%20outcome%20definition%20to%0Amaximize%20causal%20identifiability.%20Our%20DEBIAS%20%28Durable%20Effects%20with%0ABackdoor-Invariant%20Aggregated%20Symptoms%29%20algorithm%20learns%20non-negative%2C%0Aclinically%20interpretable%20weights%20for%20outcome%20aggregation%2C%20maximizing%20durable%0Atreatment%20effects%20and%20empirically%20minimizing%20both%20observed%20and%20latent%0Aconfounding%20by%20leveraging%20the%20time-limited%20direct%20effects%20of%20prior%20treatments%0Ain%20psychiatric%20longitudinal%20data.%20The%20algorithm%20also%20furnishes%20an%20empirically%0Averifiable%20test%20for%20outcome%20unconfoundedness.%20DEBIAS%20consistently%20outperforms%0Astate-of-the-art%20methods%20in%20recovering%20causal%20effects%20for%20clinically%0Ainterpretable%20composite%20outcomes%20across%20comprehensive%20experiments%20in%20depression%0Aand%20schizophrenia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16629v4&entry.124074799=Read"},
{"title": "Bounded KRnet and its applications to density estimation and\n  approximation", "author": "Li Zeng and Xiaoliang Wan and Tao Zhou", "abstract": "  In this paper, we develop an invertible mapping, called B-KRnet, on a bounded\ndomain and apply it to density estimation/approximation for data or the\nsolutions of PDEs such as the Fokker-Planck equation and the Keller-Segel\nequation. Similar to KRnet, B-KRnet consists of a series of coupling layers\nwith progressively fewer active transformation dimensions, inspired by the\ntriangular structure of the Knothe-Rosenblatt (KR) rearrangement. The main\ndifference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube\nwhile KRnet is defined on the whole space, in other words, a new mechanism is\nintroduced in B-KRnet to maintain the exact invertibility. Using B-KRnet as a\ntransport map, we obtain an explicit probability density function (PDF) model\nthat corresponds to the pushforward of a base (uniform) distribution on the\nhypercube. It can be directly applied to density estimation when only data are\navailable. By coupling KRnet and B-KRnet, we define a deep generative model on\na high-dimensional domain where some dimensions are bounded and other\ndimensions are unbounded. A typical case is the solution of the stationary\nkinetic Fokker-Planck equation, which is a PDF of position and momentum. Based\non B-KRnet, we develop an adaptive learning approach to approximate partial\ndifferential equations whose solutions are PDFs or can be treated as PDFs. A\nvariety of numerical experiments is presented to demonstrate the effectiveness\nof B-KRnet.\n", "link": "http://arxiv.org/abs/2305.09063v4", "date": "2025-07-25", "relevancy": 1.3664, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4789}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bounded%20KRnet%20and%20its%20applications%20to%20density%20estimation%20and%0A%20%20approximation&body=Title%3A%20Bounded%20KRnet%20and%20its%20applications%20to%20density%20estimation%20and%0A%20%20approximation%0AAuthor%3A%20Li%20Zeng%20and%20Xiaoliang%20Wan%20and%20Tao%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20an%20invertible%20mapping%2C%20called%20B-KRnet%2C%20on%20a%20bounded%0Adomain%20and%20apply%20it%20to%20density%20estimation/approximation%20for%20data%20or%20the%0Asolutions%20of%20PDEs%20such%20as%20the%20Fokker-Planck%20equation%20and%20the%20Keller-Segel%0Aequation.%20Similar%20to%20KRnet%2C%20B-KRnet%20consists%20of%20a%20series%20of%20coupling%20layers%0Awith%20progressively%20fewer%20active%20transformation%20dimensions%2C%20inspired%20by%20the%0Atriangular%20structure%20of%20the%20Knothe-Rosenblatt%20%28KR%29%20rearrangement.%20The%20main%0Adifference%20between%20B-KRnet%20and%20KRnet%20is%20that%20B-KRnet%20is%20defined%20on%20a%20hypercube%0Awhile%20KRnet%20is%20defined%20on%20the%20whole%20space%2C%20in%20other%20words%2C%20a%20new%20mechanism%20is%0Aintroduced%20in%20B-KRnet%20to%20maintain%20the%20exact%20invertibility.%20Using%20B-KRnet%20as%20a%0Atransport%20map%2C%20we%20obtain%20an%20explicit%20probability%20density%20function%20%28PDF%29%20model%0Athat%20corresponds%20to%20the%20pushforward%20of%20a%20base%20%28uniform%29%20distribution%20on%20the%0Ahypercube.%20It%20can%20be%20directly%20applied%20to%20density%20estimation%20when%20only%20data%20are%0Aavailable.%20By%20coupling%20KRnet%20and%20B-KRnet%2C%20we%20define%20a%20deep%20generative%20model%20on%0Aa%20high-dimensional%20domain%20where%20some%20dimensions%20are%20bounded%20and%20other%0Adimensions%20are%20unbounded.%20A%20typical%20case%20is%20the%20solution%20of%20the%20stationary%0Akinetic%20Fokker-Planck%20equation%2C%20which%20is%20a%20PDF%20of%20position%20and%20momentum.%20Based%0Aon%20B-KRnet%2C%20we%20develop%20an%20adaptive%20learning%20approach%20to%20approximate%20partial%0Adifferential%20equations%20whose%20solutions%20are%20PDFs%20or%20can%20be%20treated%20as%20PDFs.%20A%0Avariety%20of%20numerical%20experiments%20is%20presented%20to%20demonstrate%20the%20effectiveness%0Aof%20B-KRnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.09063v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBounded%2520KRnet%2520and%2520its%2520applications%2520to%2520density%2520estimation%2520and%250A%2520%2520approximation%26entry.906535625%3DLi%2520Zeng%2520and%2520Xiaoliang%2520Wan%2520and%2520Tao%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520an%2520invertible%2520mapping%252C%2520called%2520B-KRnet%252C%2520on%2520a%2520bounded%250Adomain%2520and%2520apply%2520it%2520to%2520density%2520estimation/approximation%2520for%2520data%2520or%2520the%250Asolutions%2520of%2520PDEs%2520such%2520as%2520the%2520Fokker-Planck%2520equation%2520and%2520the%2520Keller-Segel%250Aequation.%2520Similar%2520to%2520KRnet%252C%2520B-KRnet%2520consists%2520of%2520a%2520series%2520of%2520coupling%2520layers%250Awith%2520progressively%2520fewer%2520active%2520transformation%2520dimensions%252C%2520inspired%2520by%2520the%250Atriangular%2520structure%2520of%2520the%2520Knothe-Rosenblatt%2520%2528KR%2529%2520rearrangement.%2520The%2520main%250Adifference%2520between%2520B-KRnet%2520and%2520KRnet%2520is%2520that%2520B-KRnet%2520is%2520defined%2520on%2520a%2520hypercube%250Awhile%2520KRnet%2520is%2520defined%2520on%2520the%2520whole%2520space%252C%2520in%2520other%2520words%252C%2520a%2520new%2520mechanism%2520is%250Aintroduced%2520in%2520B-KRnet%2520to%2520maintain%2520the%2520exact%2520invertibility.%2520Using%2520B-KRnet%2520as%2520a%250Atransport%2520map%252C%2520we%2520obtain%2520an%2520explicit%2520probability%2520density%2520function%2520%2528PDF%2529%2520model%250Athat%2520corresponds%2520to%2520the%2520pushforward%2520of%2520a%2520base%2520%2528uniform%2529%2520distribution%2520on%2520the%250Ahypercube.%2520It%2520can%2520be%2520directly%2520applied%2520to%2520density%2520estimation%2520when%2520only%2520data%2520are%250Aavailable.%2520By%2520coupling%2520KRnet%2520and%2520B-KRnet%252C%2520we%2520define%2520a%2520deep%2520generative%2520model%2520on%250Aa%2520high-dimensional%2520domain%2520where%2520some%2520dimensions%2520are%2520bounded%2520and%2520other%250Adimensions%2520are%2520unbounded.%2520A%2520typical%2520case%2520is%2520the%2520solution%2520of%2520the%2520stationary%250Akinetic%2520Fokker-Planck%2520equation%252C%2520which%2520is%2520a%2520PDF%2520of%2520position%2520and%2520momentum.%2520Based%250Aon%2520B-KRnet%252C%2520we%2520develop%2520an%2520adaptive%2520learning%2520approach%2520to%2520approximate%2520partial%250Adifferential%2520equations%2520whose%2520solutions%2520are%2520PDFs%2520or%2520can%2520be%2520treated%2520as%2520PDFs.%2520A%250Avariety%2520of%2520numerical%2520experiments%2520is%2520presented%2520to%2520demonstrate%2520the%2520effectiveness%250Aof%2520B-KRnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.09063v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounded%20KRnet%20and%20its%20applications%20to%20density%20estimation%20and%0A%20%20approximation&entry.906535625=Li%20Zeng%20and%20Xiaoliang%20Wan%20and%20Tao%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20an%20invertible%20mapping%2C%20called%20B-KRnet%2C%20on%20a%20bounded%0Adomain%20and%20apply%20it%20to%20density%20estimation/approximation%20for%20data%20or%20the%0Asolutions%20of%20PDEs%20such%20as%20the%20Fokker-Planck%20equation%20and%20the%20Keller-Segel%0Aequation.%20Similar%20to%20KRnet%2C%20B-KRnet%20consists%20of%20a%20series%20of%20coupling%20layers%0Awith%20progressively%20fewer%20active%20transformation%20dimensions%2C%20inspired%20by%20the%0Atriangular%20structure%20of%20the%20Knothe-Rosenblatt%20%28KR%29%20rearrangement.%20The%20main%0Adifference%20between%20B-KRnet%20and%20KRnet%20is%20that%20B-KRnet%20is%20defined%20on%20a%20hypercube%0Awhile%20KRnet%20is%20defined%20on%20the%20whole%20space%2C%20in%20other%20words%2C%20a%20new%20mechanism%20is%0Aintroduced%20in%20B-KRnet%20to%20maintain%20the%20exact%20invertibility.%20Using%20B-KRnet%20as%20a%0Atransport%20map%2C%20we%20obtain%20an%20explicit%20probability%20density%20function%20%28PDF%29%20model%0Athat%20corresponds%20to%20the%20pushforward%20of%20a%20base%20%28uniform%29%20distribution%20on%20the%0Ahypercube.%20It%20can%20be%20directly%20applied%20to%20density%20estimation%20when%20only%20data%20are%0Aavailable.%20By%20coupling%20KRnet%20and%20B-KRnet%2C%20we%20define%20a%20deep%20generative%20model%20on%0Aa%20high-dimensional%20domain%20where%20some%20dimensions%20are%20bounded%20and%20other%0Adimensions%20are%20unbounded.%20A%20typical%20case%20is%20the%20solution%20of%20the%20stationary%0Akinetic%20Fokker-Planck%20equation%2C%20which%20is%20a%20PDF%20of%20position%20and%20momentum.%20Based%0Aon%20B-KRnet%2C%20we%20develop%20an%20adaptive%20learning%20approach%20to%20approximate%20partial%0Adifferential%20equations%20whose%20solutions%20are%20PDFs%20or%20can%20be%20treated%20as%20PDFs.%20A%0Avariety%20of%20numerical%20experiments%20is%20presented%20to%20demonstrate%20the%20effectiveness%0Aof%20B-KRnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.09063v4&entry.124074799=Read"},
{"title": "Agreement-Based Cascading for Efficient Inference", "author": "Steven Kolawole and Don Dennis and Ameet Talwalkar and Virginia Smith", "abstract": "  Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades.\n", "link": "http://arxiv.org/abs/2407.02348v3", "date": "2025-07-25", "relevancy": 1.3569, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4602}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agreement-Based%20Cascading%20for%20Efficient%20Inference&body=Title%3A%20Agreement-Based%20Cascading%20for%20Efficient%20Inference%0AAuthor%3A%20Steven%20Kolawole%20and%20Don%20Dennis%20and%20Ameet%20Talwalkar%20and%20Virginia%20Smith%0AAbstract%3A%20%20%20Adaptive%20inference%20schemes%20reduce%20the%20cost%20of%20machine%20learning%20inference%20by%0Aassigning%20smaller%20models%20to%20easier%20examples%2C%20attempting%20to%20avoid%20invocation%20of%0Alarger%20models%20when%20possible.%20In%20this%20work%20we%20explore%20a%20simple%2C%20effective%0Aadaptive%20inference%20technique%20we%20term%20Agreement-Based%20Cascading%20%28ABC%29.%20ABC%0Abuilds%20a%20cascade%20of%20models%20of%20increasing%20size/complexity%2C%20and%20uses%20agreement%0Abetween%20ensembles%20of%20models%20at%20each%20level%20of%20the%20cascade%20as%20a%20basis%20for%0Adata-dependent%20routing.%20Although%20ensemble%20execution%20introduces%20additional%0Aexpense%2C%20we%20show%20that%20these%20costs%20can%20be%20easily%20offset%20in%20practice%20due%20to%20large%0Aexpected%20differences%20in%20model%20sizes%2C%20parallel%20inference%20execution%20capabilities%2C%0Aand%20accuracy%20benefits%20of%20ensembling.%20We%20examine%20ABC%20theoretically%20and%0Aempirically%20in%20terms%20of%20these%20parameters%2C%20showing%20that%20the%20approach%20can%0Areliably%20act%20as%20a%20drop-in%20replacement%20for%20existing%20models%20and%20surpass%20the%20best%0Asingle%20model%20it%20aims%20to%20replace%20in%20terms%20of%20both%20efficiency%20and%20accuracy.%0AAdditionally%2C%20we%20explore%20the%20performance%20of%20ABC%20relative%20to%20existing%20cascading%0Amethods%20in%20three%20common%20scenarios%3A%20%281%29%20edge-to-cloud%20inference%2C%20where%20ABC%0Areduces%20communication%20costs%20by%20up%20to%2014x%3B%20%282%29%20cloud-based%20model%20serving%2C%20where%0Ait%20achieves%20a%203x%20reduction%20in%20rental%20costs%3B%20and%20%283%29%20inference%20via%20model%20API%0Aservices%2C%20where%20ABC%20achieves%20a%202-25x%20reduction%20in%20average%20price%20per%0Atoken/request%20relative%20to%20state-of-the-art%20LLM%20cascades.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02348v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgreement-Based%2520Cascading%2520for%2520Efficient%2520Inference%26entry.906535625%3DSteven%2520Kolawole%2520and%2520Don%2520Dennis%2520and%2520Ameet%2520Talwalkar%2520and%2520Virginia%2520Smith%26entry.1292438233%3D%2520%2520Adaptive%2520inference%2520schemes%2520reduce%2520the%2520cost%2520of%2520machine%2520learning%2520inference%2520by%250Aassigning%2520smaller%2520models%2520to%2520easier%2520examples%252C%2520attempting%2520to%2520avoid%2520invocation%2520of%250Alarger%2520models%2520when%2520possible.%2520In%2520this%2520work%2520we%2520explore%2520a%2520simple%252C%2520effective%250Aadaptive%2520inference%2520technique%2520we%2520term%2520Agreement-Based%2520Cascading%2520%2528ABC%2529.%2520ABC%250Abuilds%2520a%2520cascade%2520of%2520models%2520of%2520increasing%2520size/complexity%252C%2520and%2520uses%2520agreement%250Abetween%2520ensembles%2520of%2520models%2520at%2520each%2520level%2520of%2520the%2520cascade%2520as%2520a%2520basis%2520for%250Adata-dependent%2520routing.%2520Although%2520ensemble%2520execution%2520introduces%2520additional%250Aexpense%252C%2520we%2520show%2520that%2520these%2520costs%2520can%2520be%2520easily%2520offset%2520in%2520practice%2520due%2520to%2520large%250Aexpected%2520differences%2520in%2520model%2520sizes%252C%2520parallel%2520inference%2520execution%2520capabilities%252C%250Aand%2520accuracy%2520benefits%2520of%2520ensembling.%2520We%2520examine%2520ABC%2520theoretically%2520and%250Aempirically%2520in%2520terms%2520of%2520these%2520parameters%252C%2520showing%2520that%2520the%2520approach%2520can%250Areliably%2520act%2520as%2520a%2520drop-in%2520replacement%2520for%2520existing%2520models%2520and%2520surpass%2520the%2520best%250Asingle%2520model%2520it%2520aims%2520to%2520replace%2520in%2520terms%2520of%2520both%2520efficiency%2520and%2520accuracy.%250AAdditionally%252C%2520we%2520explore%2520the%2520performance%2520of%2520ABC%2520relative%2520to%2520existing%2520cascading%250Amethods%2520in%2520three%2520common%2520scenarios%253A%2520%25281%2529%2520edge-to-cloud%2520inference%252C%2520where%2520ABC%250Areduces%2520communication%2520costs%2520by%2520up%2520to%252014x%253B%2520%25282%2529%2520cloud-based%2520model%2520serving%252C%2520where%250Ait%2520achieves%2520a%25203x%2520reduction%2520in%2520rental%2520costs%253B%2520and%2520%25283%2529%2520inference%2520via%2520model%2520API%250Aservices%252C%2520where%2520ABC%2520achieves%2520a%25202-25x%2520reduction%2520in%2520average%2520price%2520per%250Atoken/request%2520relative%2520to%2520state-of-the-art%2520LLM%2520cascades.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02348v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agreement-Based%20Cascading%20for%20Efficient%20Inference&entry.906535625=Steven%20Kolawole%20and%20Don%20Dennis%20and%20Ameet%20Talwalkar%20and%20Virginia%20Smith&entry.1292438233=%20%20Adaptive%20inference%20schemes%20reduce%20the%20cost%20of%20machine%20learning%20inference%20by%0Aassigning%20smaller%20models%20to%20easier%20examples%2C%20attempting%20to%20avoid%20invocation%20of%0Alarger%20models%20when%20possible.%20In%20this%20work%20we%20explore%20a%20simple%2C%20effective%0Aadaptive%20inference%20technique%20we%20term%20Agreement-Based%20Cascading%20%28ABC%29.%20ABC%0Abuilds%20a%20cascade%20of%20models%20of%20increasing%20size/complexity%2C%20and%20uses%20agreement%0Abetween%20ensembles%20of%20models%20at%20each%20level%20of%20the%20cascade%20as%20a%20basis%20for%0Adata-dependent%20routing.%20Although%20ensemble%20execution%20introduces%20additional%0Aexpense%2C%20we%20show%20that%20these%20costs%20can%20be%20easily%20offset%20in%20practice%20due%20to%20large%0Aexpected%20differences%20in%20model%20sizes%2C%20parallel%20inference%20execution%20capabilities%2C%0Aand%20accuracy%20benefits%20of%20ensembling.%20We%20examine%20ABC%20theoretically%20and%0Aempirically%20in%20terms%20of%20these%20parameters%2C%20showing%20that%20the%20approach%20can%0Areliably%20act%20as%20a%20drop-in%20replacement%20for%20existing%20models%20and%20surpass%20the%20best%0Asingle%20model%20it%20aims%20to%20replace%20in%20terms%20of%20both%20efficiency%20and%20accuracy.%0AAdditionally%2C%20we%20explore%20the%20performance%20of%20ABC%20relative%20to%20existing%20cascading%0Amethods%20in%20three%20common%20scenarios%3A%20%281%29%20edge-to-cloud%20inference%2C%20where%20ABC%0Areduces%20communication%20costs%20by%20up%20to%2014x%3B%20%282%29%20cloud-based%20model%20serving%2C%20where%0Ait%20achieves%20a%203x%20reduction%20in%20rental%20costs%3B%20and%20%283%29%20inference%20via%20model%20API%0Aservices%2C%20where%20ABC%20achieves%20a%202-25x%20reduction%20in%20average%20price%20per%0Atoken/request%20relative%20to%20state-of-the-art%20LLM%20cascades.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02348v3&entry.124074799=Read"},
{"title": "Distillation Scaling Laws", "author": "Dan Busbridge and Amitis Shidani and Floris Weers and Jason Ramapuram and Etai Littwin and Russ Webb", "abstract": "  We propose a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings mitigate the risks associated with large-scale\ndistillation by enabling compute-optimal allocation for both the teacher and\nstudent to maximize student performance. We provide compute-optimal\ndistillation recipes for two key scenarios: when a teacher already exists, and\nwhen a teacher needs training. In settings involving many students or an\nexisting teacher, distillation outperforms supervised learning up to a compute\nlevel that scales predictably with student size. Conversely, if only one\nstudent is to be distilled and a teacher also requires training, supervised\nlearning is generally preferable. Additionally, our large-scale study of\ndistillation increases our understanding of the process and helps inform\nexperimental design.\n", "link": "http://arxiv.org/abs/2502.08606v2", "date": "2025-07-25", "relevancy": 1.3295, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5109}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4285}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distillation%20Scaling%20Laws&body=Title%3A%20Distillation%20Scaling%20Laws%0AAuthor%3A%20Dan%20Busbridge%20and%20Amitis%20Shidani%20and%20Floris%20Weers%20and%20Jason%20Ramapuram%20and%20Etai%20Littwin%20and%20Russ%20Webb%0AAbstract%3A%20%20%20We%20propose%20a%20distillation%20scaling%20law%20that%20estimates%20distilled%20model%0Aperformance%20based%20on%20a%20compute%20budget%20and%20its%20allocation%20between%20the%20student%0Aand%20teacher.%20Our%20findings%20mitigate%20the%20risks%20associated%20with%20large-scale%0Adistillation%20by%20enabling%20compute-optimal%20allocation%20for%20both%20the%20teacher%20and%0Astudent%20to%20maximize%20student%20performance.%20We%20provide%20compute-optimal%0Adistillation%20recipes%20for%20two%20key%20scenarios%3A%20when%20a%20teacher%20already%20exists%2C%20and%0Awhen%20a%20teacher%20needs%20training.%20In%20settings%20involving%20many%20students%20or%20an%0Aexisting%20teacher%2C%20distillation%20outperforms%20supervised%20learning%20up%20to%20a%20compute%0Alevel%20that%20scales%20predictably%20with%20student%20size.%20Conversely%2C%20if%20only%20one%0Astudent%20is%20to%20be%20distilled%20and%20a%20teacher%20also%20requires%20training%2C%20supervised%0Alearning%20is%20generally%20preferable.%20Additionally%2C%20our%20large-scale%20study%20of%0Adistillation%20increases%20our%20understanding%20of%20the%20process%20and%20helps%20inform%0Aexperimental%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistillation%2520Scaling%2520Laws%26entry.906535625%3DDan%2520Busbridge%2520and%2520Amitis%2520Shidani%2520and%2520Floris%2520Weers%2520and%2520Jason%2520Ramapuram%2520and%2520Etai%2520Littwin%2520and%2520Russ%2520Webb%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520distillation%2520scaling%2520law%2520that%2520estimates%2520distilled%2520model%250Aperformance%2520based%2520on%2520a%2520compute%2520budget%2520and%2520its%2520allocation%2520between%2520the%2520student%250Aand%2520teacher.%2520Our%2520findings%2520mitigate%2520the%2520risks%2520associated%2520with%2520large-scale%250Adistillation%2520by%2520enabling%2520compute-optimal%2520allocation%2520for%2520both%2520the%2520teacher%2520and%250Astudent%2520to%2520maximize%2520student%2520performance.%2520We%2520provide%2520compute-optimal%250Adistillation%2520recipes%2520for%2520two%2520key%2520scenarios%253A%2520when%2520a%2520teacher%2520already%2520exists%252C%2520and%250Awhen%2520a%2520teacher%2520needs%2520training.%2520In%2520settings%2520involving%2520many%2520students%2520or%2520an%250Aexisting%2520teacher%252C%2520distillation%2520outperforms%2520supervised%2520learning%2520up%2520to%2520a%2520compute%250Alevel%2520that%2520scales%2520predictably%2520with%2520student%2520size.%2520Conversely%252C%2520if%2520only%2520one%250Astudent%2520is%2520to%2520be%2520distilled%2520and%2520a%2520teacher%2520also%2520requires%2520training%252C%2520supervised%250Alearning%2520is%2520generally%2520preferable.%2520Additionally%252C%2520our%2520large-scale%2520study%2520of%250Adistillation%2520increases%2520our%2520understanding%2520of%2520the%2520process%2520and%2520helps%2520inform%250Aexperimental%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distillation%20Scaling%20Laws&entry.906535625=Dan%20Busbridge%20and%20Amitis%20Shidani%20and%20Floris%20Weers%20and%20Jason%20Ramapuram%20and%20Etai%20Littwin%20and%20Russ%20Webb&entry.1292438233=%20%20We%20propose%20a%20distillation%20scaling%20law%20that%20estimates%20distilled%20model%0Aperformance%20based%20on%20a%20compute%20budget%20and%20its%20allocation%20between%20the%20student%0Aand%20teacher.%20Our%20findings%20mitigate%20the%20risks%20associated%20with%20large-scale%0Adistillation%20by%20enabling%20compute-optimal%20allocation%20for%20both%20the%20teacher%20and%0Astudent%20to%20maximize%20student%20performance.%20We%20provide%20compute-optimal%0Adistillation%20recipes%20for%20two%20key%20scenarios%3A%20when%20a%20teacher%20already%20exists%2C%20and%0Awhen%20a%20teacher%20needs%20training.%20In%20settings%20involving%20many%20students%20or%20an%0Aexisting%20teacher%2C%20distillation%20outperforms%20supervised%20learning%20up%20to%20a%20compute%0Alevel%20that%20scales%20predictably%20with%20student%20size.%20Conversely%2C%20if%20only%20one%0Astudent%20is%20to%20be%20distilled%20and%20a%20teacher%20also%20requires%20training%2C%20supervised%0Alearning%20is%20generally%20preferable.%20Additionally%2C%20our%20large-scale%20study%20of%0Adistillation%20increases%20our%20understanding%20of%20the%20process%20and%20helps%20inform%0Aexperimental%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08606v2&entry.124074799=Read"},
{"title": "FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for\n  Financial Fraud Detection A Technical Report", "author": "Matteo Cardaioli and Luca Marangoni and Giada Martini and Francesco Mazzolin and Luca Pajola and Andrea Ferretto Parodi and Alessandra Saitta and Maria Chiara Vernillo", "abstract": "  The increasing complexity and volume of financial transactions pose\nsignificant challenges to traditional fraud detection systems. This technical\nreport investigates and compares the efficacy of classical, quantum, and\nquantum-hybrid machine learning models for the binary classification of\nfraudulent financial activities.\n  As of our methodology, first, we develop a comprehensive behavioural feature\nengineering framework to transform raw transactional data into a rich,\ndescriptive feature set. Second, we implement and evaluate a range of models on\nthe IBM Anti-Money Laundering (AML) dataset. The classical baseline models\ninclude Logistic Regression, Decision Tree, Random Forest, and XGBoost. These\nare compared against three hybrid classic quantum algorithms architectures: a\nQuantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),\nand a Hybrid Quantum Neural Network (HQNN).\n  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a\npractical, API-driven system architecture designed for real-world deployment,\nfeaturing a classical-first, quantum-enhanced philosophy with robust fallback\nmechanisms.\n  Our results demonstrate that classical tree-based models, particularly\n\\textit{Random Forest}, significantly outperform the quantum counterparts in\nthe current setup, achieving high accuracy (\\(97.34\\%\\)) and F-measure\n(\\(86.95\\%\\)). Among the quantum models, \\textbf{QSVM} shows the most promise,\ndelivering high precision (\\(77.15\\%\\)) and a low false-positive rate\n(\\(1.36\\%\\)), albeit with lower recall and significant computational overhead.\n  This report provides a benchmark for a real-world financial application,\nhighlights the current limitations of quantum machine learning in this domain,\nand outlines promising directions for future research.\n", "link": "http://arxiv.org/abs/2507.19402v1", "date": "2025-07-25", "relevancy": 1.3029, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4293}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FD4QC%3A%20Application%20of%20Classical%20and%20Quantum-Hybrid%20Machine%20Learning%20for%0A%20%20Financial%20Fraud%20Detection%20A%20Technical%20Report&body=Title%3A%20FD4QC%3A%20Application%20of%20Classical%20and%20Quantum-Hybrid%20Machine%20Learning%20for%0A%20%20Financial%20Fraud%20Detection%20A%20Technical%20Report%0AAuthor%3A%20Matteo%20Cardaioli%20and%20Luca%20Marangoni%20and%20Giada%20Martini%20and%20Francesco%20Mazzolin%20and%20Luca%20Pajola%20and%20Andrea%20Ferretto%20Parodi%20and%20Alessandra%20Saitta%20and%20Maria%20Chiara%20Vernillo%0AAbstract%3A%20%20%20The%20increasing%20complexity%20and%20volume%20of%20financial%20transactions%20pose%0Asignificant%20challenges%20to%20traditional%20fraud%20detection%20systems.%20This%20technical%0Areport%20investigates%20and%20compares%20the%20efficacy%20of%20classical%2C%20quantum%2C%20and%0Aquantum-hybrid%20machine%20learning%20models%20for%20the%20binary%20classification%20of%0Afraudulent%20financial%20activities.%0A%20%20As%20of%20our%20methodology%2C%20first%2C%20we%20develop%20a%20comprehensive%20behavioural%20feature%0Aengineering%20framework%20to%20transform%20raw%20transactional%20data%20into%20a%20rich%2C%0Adescriptive%20feature%20set.%20Second%2C%20we%20implement%20and%20evaluate%20a%20range%20of%20models%20on%0Athe%20IBM%20Anti-Money%20Laundering%20%28AML%29%20dataset.%20The%20classical%20baseline%20models%0Ainclude%20Logistic%20Regression%2C%20Decision%20Tree%2C%20Random%20Forest%2C%20and%20XGBoost.%20These%0Aare%20compared%20against%20three%20hybrid%20classic%20quantum%20algorithms%20architectures%3A%20a%0AQuantum%20Support%20Vector%20Machine%20%28QSVM%29%2C%20a%20Variational%20Quantum%20Classifier%20%28VQC%29%2C%0Aand%20a%20Hybrid%20Quantum%20Neural%20Network%20%28HQNN%29.%0A%20%20Furthermore%2C%20we%20propose%20Fraud%20Detection%20for%20Quantum%20Computing%20%28FD4QC%29%2C%20a%0Apractical%2C%20API-driven%20system%20architecture%20designed%20for%20real-world%20deployment%2C%0Afeaturing%20a%20classical-first%2C%20quantum-enhanced%20philosophy%20with%20robust%20fallback%0Amechanisms.%0A%20%20Our%20results%20demonstrate%20that%20classical%20tree-based%20models%2C%20particularly%0A%5Ctextit%7BRandom%20Forest%7D%2C%20significantly%20outperform%20the%20quantum%20counterparts%20in%0Athe%20current%20setup%2C%20achieving%20high%20accuracy%20%28%5C%2897.34%5C%25%5C%29%29%20and%20F-measure%0A%28%5C%2886.95%5C%25%5C%29%29.%20Among%20the%20quantum%20models%2C%20%5Ctextbf%7BQSVM%7D%20shows%20the%20most%20promise%2C%0Adelivering%20high%20precision%20%28%5C%2877.15%5C%25%5C%29%29%20and%20a%20low%20false-positive%20rate%0A%28%5C%281.36%5C%25%5C%29%29%2C%20albeit%20with%20lower%20recall%20and%20significant%20computational%20overhead.%0A%20%20This%20report%20provides%20a%20benchmark%20for%20a%20real-world%20financial%20application%2C%0Ahighlights%20the%20current%20limitations%20of%20quantum%20machine%20learning%20in%20this%20domain%2C%0Aand%20outlines%20promising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFD4QC%253A%2520Application%2520of%2520Classical%2520and%2520Quantum-Hybrid%2520Machine%2520Learning%2520for%250A%2520%2520Financial%2520Fraud%2520Detection%2520A%2520Technical%2520Report%26entry.906535625%3DMatteo%2520Cardaioli%2520and%2520Luca%2520Marangoni%2520and%2520Giada%2520Martini%2520and%2520Francesco%2520Mazzolin%2520and%2520Luca%2520Pajola%2520and%2520Andrea%2520Ferretto%2520Parodi%2520and%2520Alessandra%2520Saitta%2520and%2520Maria%2520Chiara%2520Vernillo%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520and%2520volume%2520of%2520financial%2520transactions%2520pose%250Asignificant%2520challenges%2520to%2520traditional%2520fraud%2520detection%2520systems.%2520This%2520technical%250Areport%2520investigates%2520and%2520compares%2520the%2520efficacy%2520of%2520classical%252C%2520quantum%252C%2520and%250Aquantum-hybrid%2520machine%2520learning%2520models%2520for%2520the%2520binary%2520classification%2520of%250Afraudulent%2520financial%2520activities.%250A%2520%2520As%2520of%2520our%2520methodology%252C%2520first%252C%2520we%2520develop%2520a%2520comprehensive%2520behavioural%2520feature%250Aengineering%2520framework%2520to%2520transform%2520raw%2520transactional%2520data%2520into%2520a%2520rich%252C%250Adescriptive%2520feature%2520set.%2520Second%252C%2520we%2520implement%2520and%2520evaluate%2520a%2520range%2520of%2520models%2520on%250Athe%2520IBM%2520Anti-Money%2520Laundering%2520%2528AML%2529%2520dataset.%2520The%2520classical%2520baseline%2520models%250Ainclude%2520Logistic%2520Regression%252C%2520Decision%2520Tree%252C%2520Random%2520Forest%252C%2520and%2520XGBoost.%2520These%250Aare%2520compared%2520against%2520three%2520hybrid%2520classic%2520quantum%2520algorithms%2520architectures%253A%2520a%250AQuantum%2520Support%2520Vector%2520Machine%2520%2528QSVM%2529%252C%2520a%2520Variational%2520Quantum%2520Classifier%2520%2528VQC%2529%252C%250Aand%2520a%2520Hybrid%2520Quantum%2520Neural%2520Network%2520%2528HQNN%2529.%250A%2520%2520Furthermore%252C%2520we%2520propose%2520Fraud%2520Detection%2520for%2520Quantum%2520Computing%2520%2528FD4QC%2529%252C%2520a%250Apractical%252C%2520API-driven%2520system%2520architecture%2520designed%2520for%2520real-world%2520deployment%252C%250Afeaturing%2520a%2520classical-first%252C%2520quantum-enhanced%2520philosophy%2520with%2520robust%2520fallback%250Amechanisms.%250A%2520%2520Our%2520results%2520demonstrate%2520that%2520classical%2520tree-based%2520models%252C%2520particularly%250A%255Ctextit%257BRandom%2520Forest%257D%252C%2520significantly%2520outperform%2520the%2520quantum%2520counterparts%2520in%250Athe%2520current%2520setup%252C%2520achieving%2520high%2520accuracy%2520%2528%255C%252897.34%255C%2525%255C%2529%2529%2520and%2520F-measure%250A%2528%255C%252886.95%255C%2525%255C%2529%2529.%2520Among%2520the%2520quantum%2520models%252C%2520%255Ctextbf%257BQSVM%257D%2520shows%2520the%2520most%2520promise%252C%250Adelivering%2520high%2520precision%2520%2528%255C%252877.15%255C%2525%255C%2529%2529%2520and%2520a%2520low%2520false-positive%2520rate%250A%2528%255C%25281.36%255C%2525%255C%2529%2529%252C%2520albeit%2520with%2520lower%2520recall%2520and%2520significant%2520computational%2520overhead.%250A%2520%2520This%2520report%2520provides%2520a%2520benchmark%2520for%2520a%2520real-world%2520financial%2520application%252C%250Ahighlights%2520the%2520current%2520limitations%2520of%2520quantum%2520machine%2520learning%2520in%2520this%2520domain%252C%250Aand%2520outlines%2520promising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FD4QC%3A%20Application%20of%20Classical%20and%20Quantum-Hybrid%20Machine%20Learning%20for%0A%20%20Financial%20Fraud%20Detection%20A%20Technical%20Report&entry.906535625=Matteo%20Cardaioli%20and%20Luca%20Marangoni%20and%20Giada%20Martini%20and%20Francesco%20Mazzolin%20and%20Luca%20Pajola%20and%20Andrea%20Ferretto%20Parodi%20and%20Alessandra%20Saitta%20and%20Maria%20Chiara%20Vernillo&entry.1292438233=%20%20The%20increasing%20complexity%20and%20volume%20of%20financial%20transactions%20pose%0Asignificant%20challenges%20to%20traditional%20fraud%20detection%20systems.%20This%20technical%0Areport%20investigates%20and%20compares%20the%20efficacy%20of%20classical%2C%20quantum%2C%20and%0Aquantum-hybrid%20machine%20learning%20models%20for%20the%20binary%20classification%20of%0Afraudulent%20financial%20activities.%0A%20%20As%20of%20our%20methodology%2C%20first%2C%20we%20develop%20a%20comprehensive%20behavioural%20feature%0Aengineering%20framework%20to%20transform%20raw%20transactional%20data%20into%20a%20rich%2C%0Adescriptive%20feature%20set.%20Second%2C%20we%20implement%20and%20evaluate%20a%20range%20of%20models%20on%0Athe%20IBM%20Anti-Money%20Laundering%20%28AML%29%20dataset.%20The%20classical%20baseline%20models%0Ainclude%20Logistic%20Regression%2C%20Decision%20Tree%2C%20Random%20Forest%2C%20and%20XGBoost.%20These%0Aare%20compared%20against%20three%20hybrid%20classic%20quantum%20algorithms%20architectures%3A%20a%0AQuantum%20Support%20Vector%20Machine%20%28QSVM%29%2C%20a%20Variational%20Quantum%20Classifier%20%28VQC%29%2C%0Aand%20a%20Hybrid%20Quantum%20Neural%20Network%20%28HQNN%29.%0A%20%20Furthermore%2C%20we%20propose%20Fraud%20Detection%20for%20Quantum%20Computing%20%28FD4QC%29%2C%20a%0Apractical%2C%20API-driven%20system%20architecture%20designed%20for%20real-world%20deployment%2C%0Afeaturing%20a%20classical-first%2C%20quantum-enhanced%20philosophy%20with%20robust%20fallback%0Amechanisms.%0A%20%20Our%20results%20demonstrate%20that%20classical%20tree-based%20models%2C%20particularly%0A%5Ctextit%7BRandom%20Forest%7D%2C%20significantly%20outperform%20the%20quantum%20counterparts%20in%0Athe%20current%20setup%2C%20achieving%20high%20accuracy%20%28%5C%2897.34%5C%25%5C%29%29%20and%20F-measure%0A%28%5C%2886.95%5C%25%5C%29%29.%20Among%20the%20quantum%20models%2C%20%5Ctextbf%7BQSVM%7D%20shows%20the%20most%20promise%2C%0Adelivering%20high%20precision%20%28%5C%2877.15%5C%25%5C%29%29%20and%20a%20low%20false-positive%20rate%0A%28%5C%281.36%5C%25%5C%29%29%2C%20albeit%20with%20lower%20recall%20and%20significant%20computational%20overhead.%0A%20%20This%20report%20provides%20a%20benchmark%20for%20a%20real-world%20financial%20application%2C%0Ahighlights%20the%20current%20limitations%20of%20quantum%20machine%20learning%20in%20this%20domain%2C%0Aand%20outlines%20promising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19402v1&entry.124074799=Read"},
{"title": "SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle\n  Functions", "author": "Matthias Wei\u00df and Falk Dettinger and Michael Weyrich", "abstract": "  Connected and software-defined vehicles promise to offer a broad range of\nservices and advanced functions to customers, aiming to increase passenger\ncomfort and support autonomous driving capabilities. Due to the high\nreliability and availability requirements of connected vehicles, it is crucial\nto resolve any occurring failures quickly. To achieve this however, a complex\ncloud/edge architecture with a mesh of dependencies must be navigated to\ndiagnose the responsible root cause. As such, manual analyses become unfeasible\nsince they would significantly delay the troubleshooting.\n  To address this challenge, this paper presents SDVDiag, an extensible\nplatform for the automated diagnosis of connected vehicle functions. The\nplatform enables the creation of pipelines that cover all steps from initial\ndata collection to the tracing of potential root causes. In addition, SDVDiag\nsupports self-adaptive behavior by the ability to exchange modules at runtime.\nDependencies between functions are detected and continuously updated, resulting\nin a dynamic graph view of the system. In addition, vital system metrics are\nmonitored for anomalies. Whenever an incident is investigated, a snapshot of\nthe graph is taken and augmented by relevant anomalies. Finally, the analysis\nis performed by traversing the graph and creating a ranking of the most likely\ncauses.\n  To evaluate the platform, it is deployed inside an 5G test fleet environment\nfor connected vehicle functions. The results show that injected faults can be\ndetected reliably. As such, the platform offers the potential to gain new\ninsights and reduce downtime by identifying problems and their causes at an\nearly stage.\n", "link": "http://arxiv.org/abs/2507.19403v1", "date": "2025-07-25", "relevancy": 1.3017, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4386}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4298}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDVDiag%3A%20A%20Modular%20Platform%20for%20the%20Diagnosis%20of%20Connected%20Vehicle%0A%20%20Functions&body=Title%3A%20SDVDiag%3A%20A%20Modular%20Platform%20for%20the%20Diagnosis%20of%20Connected%20Vehicle%0A%20%20Functions%0AAuthor%3A%20Matthias%20Wei%C3%9F%20and%20Falk%20Dettinger%20and%20Michael%20Weyrich%0AAbstract%3A%20%20%20Connected%20and%20software-defined%20vehicles%20promise%20to%20offer%20a%20broad%20range%20of%0Aservices%20and%20advanced%20functions%20to%20customers%2C%20aiming%20to%20increase%20passenger%0Acomfort%20and%20support%20autonomous%20driving%20capabilities.%20Due%20to%20the%20high%0Areliability%20and%20availability%20requirements%20of%20connected%20vehicles%2C%20it%20is%20crucial%0Ato%20resolve%20any%20occurring%20failures%20quickly.%20To%20achieve%20this%20however%2C%20a%20complex%0Acloud/edge%20architecture%20with%20a%20mesh%20of%20dependencies%20must%20be%20navigated%20to%0Adiagnose%20the%20responsible%20root%20cause.%20As%20such%2C%20manual%20analyses%20become%20unfeasible%0Asince%20they%20would%20significantly%20delay%20the%20troubleshooting.%0A%20%20To%20address%20this%20challenge%2C%20this%20paper%20presents%20SDVDiag%2C%20an%20extensible%0Aplatform%20for%20the%20automated%20diagnosis%20of%20connected%20vehicle%20functions.%20The%0Aplatform%20enables%20the%20creation%20of%20pipelines%20that%20cover%20all%20steps%20from%20initial%0Adata%20collection%20to%20the%20tracing%20of%20potential%20root%20causes.%20In%20addition%2C%20SDVDiag%0Asupports%20self-adaptive%20behavior%20by%20the%20ability%20to%20exchange%20modules%20at%20runtime.%0ADependencies%20between%20functions%20are%20detected%20and%20continuously%20updated%2C%20resulting%0Ain%20a%20dynamic%20graph%20view%20of%20the%20system.%20In%20addition%2C%20vital%20system%20metrics%20are%0Amonitored%20for%20anomalies.%20Whenever%20an%20incident%20is%20investigated%2C%20a%20snapshot%20of%0Athe%20graph%20is%20taken%20and%20augmented%20by%20relevant%20anomalies.%20Finally%2C%20the%20analysis%0Ais%20performed%20by%20traversing%20the%20graph%20and%20creating%20a%20ranking%20of%20the%20most%20likely%0Acauses.%0A%20%20To%20evaluate%20the%20platform%2C%20it%20is%20deployed%20inside%20an%205G%20test%20fleet%20environment%0Afor%20connected%20vehicle%20functions.%20The%20results%20show%20that%20injected%20faults%20can%20be%0Adetected%20reliably.%20As%20such%2C%20the%20platform%20offers%20the%20potential%20to%20gain%20new%0Ainsights%20and%20reduce%20downtime%20by%20identifying%20problems%20and%20their%20causes%20at%20an%0Aearly%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDVDiag%253A%2520A%2520Modular%2520Platform%2520for%2520the%2520Diagnosis%2520of%2520Connected%2520Vehicle%250A%2520%2520Functions%26entry.906535625%3DMatthias%2520Wei%25C3%259F%2520and%2520Falk%2520Dettinger%2520and%2520Michael%2520Weyrich%26entry.1292438233%3D%2520%2520Connected%2520and%2520software-defined%2520vehicles%2520promise%2520to%2520offer%2520a%2520broad%2520range%2520of%250Aservices%2520and%2520advanced%2520functions%2520to%2520customers%252C%2520aiming%2520to%2520increase%2520passenger%250Acomfort%2520and%2520support%2520autonomous%2520driving%2520capabilities.%2520Due%2520to%2520the%2520high%250Areliability%2520and%2520availability%2520requirements%2520of%2520connected%2520vehicles%252C%2520it%2520is%2520crucial%250Ato%2520resolve%2520any%2520occurring%2520failures%2520quickly.%2520To%2520achieve%2520this%2520however%252C%2520a%2520complex%250Acloud/edge%2520architecture%2520with%2520a%2520mesh%2520of%2520dependencies%2520must%2520be%2520navigated%2520to%250Adiagnose%2520the%2520responsible%2520root%2520cause.%2520As%2520such%252C%2520manual%2520analyses%2520become%2520unfeasible%250Asince%2520they%2520would%2520significantly%2520delay%2520the%2520troubleshooting.%250A%2520%2520To%2520address%2520this%2520challenge%252C%2520this%2520paper%2520presents%2520SDVDiag%252C%2520an%2520extensible%250Aplatform%2520for%2520the%2520automated%2520diagnosis%2520of%2520connected%2520vehicle%2520functions.%2520The%250Aplatform%2520enables%2520the%2520creation%2520of%2520pipelines%2520that%2520cover%2520all%2520steps%2520from%2520initial%250Adata%2520collection%2520to%2520the%2520tracing%2520of%2520potential%2520root%2520causes.%2520In%2520addition%252C%2520SDVDiag%250Asupports%2520self-adaptive%2520behavior%2520by%2520the%2520ability%2520to%2520exchange%2520modules%2520at%2520runtime.%250ADependencies%2520between%2520functions%2520are%2520detected%2520and%2520continuously%2520updated%252C%2520resulting%250Ain%2520a%2520dynamic%2520graph%2520view%2520of%2520the%2520system.%2520In%2520addition%252C%2520vital%2520system%2520metrics%2520are%250Amonitored%2520for%2520anomalies.%2520Whenever%2520an%2520incident%2520is%2520investigated%252C%2520a%2520snapshot%2520of%250Athe%2520graph%2520is%2520taken%2520and%2520augmented%2520by%2520relevant%2520anomalies.%2520Finally%252C%2520the%2520analysis%250Ais%2520performed%2520by%2520traversing%2520the%2520graph%2520and%2520creating%2520a%2520ranking%2520of%2520the%2520most%2520likely%250Acauses.%250A%2520%2520To%2520evaluate%2520the%2520platform%252C%2520it%2520is%2520deployed%2520inside%2520an%25205G%2520test%2520fleet%2520environment%250Afor%2520connected%2520vehicle%2520functions.%2520The%2520results%2520show%2520that%2520injected%2520faults%2520can%2520be%250Adetected%2520reliably.%2520As%2520such%252C%2520the%2520platform%2520offers%2520the%2520potential%2520to%2520gain%2520new%250Ainsights%2520and%2520reduce%2520downtime%2520by%2520identifying%2520problems%2520and%2520their%2520causes%2520at%2520an%250Aearly%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDVDiag%3A%20A%20Modular%20Platform%20for%20the%20Diagnosis%20of%20Connected%20Vehicle%0A%20%20Functions&entry.906535625=Matthias%20Wei%C3%9F%20and%20Falk%20Dettinger%20and%20Michael%20Weyrich&entry.1292438233=%20%20Connected%20and%20software-defined%20vehicles%20promise%20to%20offer%20a%20broad%20range%20of%0Aservices%20and%20advanced%20functions%20to%20customers%2C%20aiming%20to%20increase%20passenger%0Acomfort%20and%20support%20autonomous%20driving%20capabilities.%20Due%20to%20the%20high%0Areliability%20and%20availability%20requirements%20of%20connected%20vehicles%2C%20it%20is%20crucial%0Ato%20resolve%20any%20occurring%20failures%20quickly.%20To%20achieve%20this%20however%2C%20a%20complex%0Acloud/edge%20architecture%20with%20a%20mesh%20of%20dependencies%20must%20be%20navigated%20to%0Adiagnose%20the%20responsible%20root%20cause.%20As%20such%2C%20manual%20analyses%20become%20unfeasible%0Asince%20they%20would%20significantly%20delay%20the%20troubleshooting.%0A%20%20To%20address%20this%20challenge%2C%20this%20paper%20presents%20SDVDiag%2C%20an%20extensible%0Aplatform%20for%20the%20automated%20diagnosis%20of%20connected%20vehicle%20functions.%20The%0Aplatform%20enables%20the%20creation%20of%20pipelines%20that%20cover%20all%20steps%20from%20initial%0Adata%20collection%20to%20the%20tracing%20of%20potential%20root%20causes.%20In%20addition%2C%20SDVDiag%0Asupports%20self-adaptive%20behavior%20by%20the%20ability%20to%20exchange%20modules%20at%20runtime.%0ADependencies%20between%20functions%20are%20detected%20and%20continuously%20updated%2C%20resulting%0Ain%20a%20dynamic%20graph%20view%20of%20the%20system.%20In%20addition%2C%20vital%20system%20metrics%20are%0Amonitored%20for%20anomalies.%20Whenever%20an%20incident%20is%20investigated%2C%20a%20snapshot%20of%0Athe%20graph%20is%20taken%20and%20augmented%20by%20relevant%20anomalies.%20Finally%2C%20the%20analysis%0Ais%20performed%20by%20traversing%20the%20graph%20and%20creating%20a%20ranking%20of%20the%20most%20likely%0Acauses.%0A%20%20To%20evaluate%20the%20platform%2C%20it%20is%20deployed%20inside%20an%205G%20test%20fleet%20environment%0Afor%20connected%20vehicle%20functions.%20The%20results%20show%20that%20injected%20faults%20can%20be%0Adetected%20reliably.%20As%20such%2C%20the%20platform%20offers%20the%20potential%20to%20gain%20new%0Ainsights%20and%20reduce%20downtime%20by%20identifying%20problems%20and%20their%20causes%20at%20an%0Aearly%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19403v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


