<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241103.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GeoSplatting: Towards Geometry Guided Gaussian Splatting for\n  Physically-based Inverse Rendering", "author": "Kai Ye and Chong Gao and Guanbin Li and Wenzheng Chen and Baoquan Chen", "abstract": "  We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2410.24204v2", "date": "2024-11-01", "relevancy": 3.3398, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6791}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6713}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering&body=Title%3A%20GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering%0AAuthor%3A%20Kai%20Ye%20and%20Chong%20Gao%20and%20Guanbin%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20physically-based%20inverse%20rendering%20using%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20While%20recent%203DGS%20methods%20have%0Aachieved%20remarkable%20results%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20accurately%20capturing%0Ahigh-fidelity%20geometry%2C%20physically%20interpretable%20materials%20and%20lighting%20remains%0Achallenging%2C%20as%20it%20requires%20precise%20geometry%20modeling%20to%20provide%20accurate%0Asurface%20normals%2C%20along%20with%20physically-based%20rendering%20%28PBR%29%20techniques%20to%0Aensure%20correct%20material%20and%20lighting%20disentanglement.%20Previous%203DGS%20methods%0Aresort%20to%20approximating%20surface%20normals%2C%20but%20often%20struggle%20with%20noisy%20local%0Ageometry%2C%20leading%20to%20inaccurate%20normal%20estimation%20and%20suboptimal%0Amaterial-lighting%20decomposition.%20In%20this%20paper%2C%20we%20introduce%20GeoSplatting%2C%20a%0Anovel%20hybrid%20representation%20that%20augments%203DGS%20with%20explicit%20geometric%20guidance%0Aand%20differentiable%20PBR%20equations.%20Specifically%2C%20we%20bridge%20isosurface%20and%203DGS%0Atogether%2C%20where%20we%20first%20extract%20isosurface%20mesh%20from%20a%20scalar%20field%2C%20then%0Aconvert%20it%20into%203DGS%20points%20and%20formulate%20PBR%20equations%20for%20them%20in%20a%20fully%0Adifferentiable%20manner.%20In%20GeoSplatting%2C%203DGS%20is%20grounded%20on%20the%20mesh%20geometry%2C%0Aenabling%20precise%20surface%20normal%20modeling%2C%20which%20facilitates%20the%20use%20of%20PBR%0Aframeworks%20for%20material%20decomposition.%20This%20approach%20further%20maintains%20the%0Aefficiency%20and%20quality%20of%20NVS%20from%203DGS%20while%20ensuring%20accurate%20geometry%20from%0Athe%20isosurface.%20Comprehensive%20evaluations%20across%20diverse%20datasets%20demonstrate%0Athe%20superiority%20of%20GeoSplatting%2C%20consistently%20outperforming%20existing%20methods%0Aboth%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSplatting%253A%2520Towards%2520Geometry%2520Guided%2520Gaussian%2520Splatting%2520for%250A%2520%2520Physically-based%2520Inverse%2520Rendering%26entry.906535625%3DKai%2520Ye%2520and%2520Chong%2520Gao%2520and%2520Guanbin%2520Li%2520and%2520Wenzheng%2520Chen%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520physically-based%2520inverse%2520rendering%2520using%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520representations.%2520While%2520recent%25203DGS%2520methods%2520have%250Aachieved%2520remarkable%2520results%2520in%2520novel%2520view%2520synthesis%2520%2528NVS%2529%252C%2520accurately%2520capturing%250Ahigh-fidelity%2520geometry%252C%2520physically%2520interpretable%2520materials%2520and%2520lighting%2520remains%250Achallenging%252C%2520as%2520it%2520requires%2520precise%2520geometry%2520modeling%2520to%2520provide%2520accurate%250Asurface%2520normals%252C%2520along%2520with%2520physically-based%2520rendering%2520%2528PBR%2529%2520techniques%2520to%250Aensure%2520correct%2520material%2520and%2520lighting%2520disentanglement.%2520Previous%25203DGS%2520methods%250Aresort%2520to%2520approximating%2520surface%2520normals%252C%2520but%2520often%2520struggle%2520with%2520noisy%2520local%250Ageometry%252C%2520leading%2520to%2520inaccurate%2520normal%2520estimation%2520and%2520suboptimal%250Amaterial-lighting%2520decomposition.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GeoSplatting%252C%2520a%250Anovel%2520hybrid%2520representation%2520that%2520augments%25203DGS%2520with%2520explicit%2520geometric%2520guidance%250Aand%2520differentiable%2520PBR%2520equations.%2520Specifically%252C%2520we%2520bridge%2520isosurface%2520and%25203DGS%250Atogether%252C%2520where%2520we%2520first%2520extract%2520isosurface%2520mesh%2520from%2520a%2520scalar%2520field%252C%2520then%250Aconvert%2520it%2520into%25203DGS%2520points%2520and%2520formulate%2520PBR%2520equations%2520for%2520them%2520in%2520a%2520fully%250Adifferentiable%2520manner.%2520In%2520GeoSplatting%252C%25203DGS%2520is%2520grounded%2520on%2520the%2520mesh%2520geometry%252C%250Aenabling%2520precise%2520surface%2520normal%2520modeling%252C%2520which%2520facilitates%2520the%2520use%2520of%2520PBR%250Aframeworks%2520for%2520material%2520decomposition.%2520This%2520approach%2520further%2520maintains%2520the%250Aefficiency%2520and%2520quality%2520of%2520NVS%2520from%25203DGS%2520while%2520ensuring%2520accurate%2520geometry%2520from%250Athe%2520isosurface.%2520Comprehensive%2520evaluations%2520across%2520diverse%2520datasets%2520demonstrate%250Athe%2520superiority%2520of%2520GeoSplatting%252C%2520consistently%2520outperforming%2520existing%2520methods%250Aboth%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering&entry.906535625=Kai%20Ye%20and%20Chong%20Gao%20and%20Guanbin%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20physically-based%20inverse%20rendering%20using%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20While%20recent%203DGS%20methods%20have%0Aachieved%20remarkable%20results%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20accurately%20capturing%0Ahigh-fidelity%20geometry%2C%20physically%20interpretable%20materials%20and%20lighting%20remains%0Achallenging%2C%20as%20it%20requires%20precise%20geometry%20modeling%20to%20provide%20accurate%0Asurface%20normals%2C%20along%20with%20physically-based%20rendering%20%28PBR%29%20techniques%20to%0Aensure%20correct%20material%20and%20lighting%20disentanglement.%20Previous%203DGS%20methods%0Aresort%20to%20approximating%20surface%20normals%2C%20but%20often%20struggle%20with%20noisy%20local%0Ageometry%2C%20leading%20to%20inaccurate%20normal%20estimation%20and%20suboptimal%0Amaterial-lighting%20decomposition.%20In%20this%20paper%2C%20we%20introduce%20GeoSplatting%2C%20a%0Anovel%20hybrid%20representation%20that%20augments%203DGS%20with%20explicit%20geometric%20guidance%0Aand%20differentiable%20PBR%20equations.%20Specifically%2C%20we%20bridge%20isosurface%20and%203DGS%0Atogether%2C%20where%20we%20first%20extract%20isosurface%20mesh%20from%20a%20scalar%20field%2C%20then%0Aconvert%20it%20into%203DGS%20points%20and%20formulate%20PBR%20equations%20for%20them%20in%20a%20fully%0Adifferentiable%20manner.%20In%20GeoSplatting%2C%203DGS%20is%20grounded%20on%20the%20mesh%20geometry%2C%0Aenabling%20precise%20surface%20normal%20modeling%2C%20which%20facilitates%20the%20use%20of%20PBR%0Aframeworks%20for%20material%20decomposition.%20This%20approach%20further%20maintains%20the%0Aefficiency%20and%20quality%20of%20NVS%20from%203DGS%20while%20ensuring%20accurate%20geometry%20from%0Athe%20isosurface.%20Comprehensive%20evaluations%20across%20diverse%20datasets%20demonstrate%0Athe%20superiority%20of%20GeoSplatting%2C%20consistently%20outperforming%20existing%20methods%0Aboth%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24204v2&entry.124074799=Read"},
{"title": "Improving Generalization in Visual Reasoning via Self-Ensemble", "author": "Tien-Huy Nguyen and Quang-Khai Tran and Anh-Tuan Quang-Hoang", "abstract": "  The cognitive faculty of visual reasoning necessitates the integration of\nmultimodal perceptual processing and commonsense and external knowledge of the\nworld. In recent years, a plethora of large vision-language models (LVLMs) have\nbeen proposed, demonstrating outstanding power and exceptional proficiency in\ncommonsense reasoning across diverse domains and tasks. Nevertheless, training\nsuch LVLMs requires a lot of costly resources. Recent approaches, instead of\ntraining LVLMs from scratch on various large datasets, focus on exploring ways\nto take advantage of the capabilities of many different LVLMs, such as ensemble\nmethods. In this work, we propose self-ensemble, a novel method that improves\nthe generalization and visual reasoning of the model without updating any\nparameters, a training-free method. Our key insight is that we realized that\nLVLM itself can ensemble without the need for any other LVLMs, which helps to\nunlock their internal capabilities. Extensive experiments on various benchmarks\ndemonstrate the effectiveness of our method in achieving state-of-the-art\n(SOTA) performance on SketchyVQA, Outside Knowledge VQA, and\nout-of-distribution VQA tasks.\n", "link": "http://arxiv.org/abs/2410.20883v2", "date": "2024-11-01", "relevancy": 2.9016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5898}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generalization%20in%20Visual%20Reasoning%20via%20Self-Ensemble&body=Title%3A%20Improving%20Generalization%20in%20Visual%20Reasoning%20via%20Self-Ensemble%0AAuthor%3A%20Tien-Huy%20Nguyen%20and%20Quang-Khai%20Tran%20and%20Anh-Tuan%20Quang-Hoang%0AAbstract%3A%20%20%20The%20cognitive%20faculty%20of%20visual%20reasoning%20necessitates%20the%20integration%20of%0Amultimodal%20perceptual%20processing%20and%20commonsense%20and%20external%20knowledge%20of%20the%0Aworld.%20In%20recent%20years%2C%20a%20plethora%20of%20large%20vision-language%20models%20%28LVLMs%29%20have%0Abeen%20proposed%2C%20demonstrating%20outstanding%20power%20and%20exceptional%20proficiency%20in%0Acommonsense%20reasoning%20across%20diverse%20domains%20and%20tasks.%20Nevertheless%2C%20training%0Asuch%20LVLMs%20requires%20a%20lot%20of%20costly%20resources.%20Recent%20approaches%2C%20instead%20of%0Atraining%20LVLMs%20from%20scratch%20on%20various%20large%20datasets%2C%20focus%20on%20exploring%20ways%0Ato%20take%20advantage%20of%20the%20capabilities%20of%20many%20different%20LVLMs%2C%20such%20as%20ensemble%0Amethods.%20In%20this%20work%2C%20we%20propose%20self-ensemble%2C%20a%20novel%20method%20that%20improves%0Athe%20generalization%20and%20visual%20reasoning%20of%20the%20model%20without%20updating%20any%0Aparameters%2C%20a%20training-free%20method.%20Our%20key%20insight%20is%20that%20we%20realized%20that%0ALVLM%20itself%20can%20ensemble%20without%20the%20need%20for%20any%20other%20LVLMs%2C%20which%20helps%20to%0Aunlock%20their%20internal%20capabilities.%20Extensive%20experiments%20on%20various%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20in%20achieving%20state-of-the-art%0A%28SOTA%29%20performance%20on%20SketchyVQA%2C%20Outside%20Knowledge%20VQA%2C%20and%0Aout-of-distribution%20VQA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generalization%2520in%2520Visual%2520Reasoning%2520via%2520Self-Ensemble%26entry.906535625%3DTien-Huy%2520Nguyen%2520and%2520Quang-Khai%2520Tran%2520and%2520Anh-Tuan%2520Quang-Hoang%26entry.1292438233%3D%2520%2520The%2520cognitive%2520faculty%2520of%2520visual%2520reasoning%2520necessitates%2520the%2520integration%2520of%250Amultimodal%2520perceptual%2520processing%2520and%2520commonsense%2520and%2520external%2520knowledge%2520of%2520the%250Aworld.%2520In%2520recent%2520years%252C%2520a%2520plethora%2520of%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%250Abeen%2520proposed%252C%2520demonstrating%2520outstanding%2520power%2520and%2520exceptional%2520proficiency%2520in%250Acommonsense%2520reasoning%2520across%2520diverse%2520domains%2520and%2520tasks.%2520Nevertheless%252C%2520training%250Asuch%2520LVLMs%2520requires%2520a%2520lot%2520of%2520costly%2520resources.%2520Recent%2520approaches%252C%2520instead%2520of%250Atraining%2520LVLMs%2520from%2520scratch%2520on%2520various%2520large%2520datasets%252C%2520focus%2520on%2520exploring%2520ways%250Ato%2520take%2520advantage%2520of%2520the%2520capabilities%2520of%2520many%2520different%2520LVLMs%252C%2520such%2520as%2520ensemble%250Amethods.%2520In%2520this%2520work%252C%2520we%2520propose%2520self-ensemble%252C%2520a%2520novel%2520method%2520that%2520improves%250Athe%2520generalization%2520and%2520visual%2520reasoning%2520of%2520the%2520model%2520without%2520updating%2520any%250Aparameters%252C%2520a%2520training-free%2520method.%2520Our%2520key%2520insight%2520is%2520that%2520we%2520realized%2520that%250ALVLM%2520itself%2520can%2520ensemble%2520without%2520the%2520need%2520for%2520any%2520other%2520LVLMs%252C%2520which%2520helps%2520to%250Aunlock%2520their%2520internal%2520capabilities.%2520Extensive%2520experiments%2520on%2520various%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520achieving%2520state-of-the-art%250A%2528SOTA%2529%2520performance%2520on%2520SketchyVQA%252C%2520Outside%2520Knowledge%2520VQA%252C%2520and%250Aout-of-distribution%2520VQA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generalization%20in%20Visual%20Reasoning%20via%20Self-Ensemble&entry.906535625=Tien-Huy%20Nguyen%20and%20Quang-Khai%20Tran%20and%20Anh-Tuan%20Quang-Hoang&entry.1292438233=%20%20The%20cognitive%20faculty%20of%20visual%20reasoning%20necessitates%20the%20integration%20of%0Amultimodal%20perceptual%20processing%20and%20commonsense%20and%20external%20knowledge%20of%20the%0Aworld.%20In%20recent%20years%2C%20a%20plethora%20of%20large%20vision-language%20models%20%28LVLMs%29%20have%0Abeen%20proposed%2C%20demonstrating%20outstanding%20power%20and%20exceptional%20proficiency%20in%0Acommonsense%20reasoning%20across%20diverse%20domains%20and%20tasks.%20Nevertheless%2C%20training%0Asuch%20LVLMs%20requires%20a%20lot%20of%20costly%20resources.%20Recent%20approaches%2C%20instead%20of%0Atraining%20LVLMs%20from%20scratch%20on%20various%20large%20datasets%2C%20focus%20on%20exploring%20ways%0Ato%20take%20advantage%20of%20the%20capabilities%20of%20many%20different%20LVLMs%2C%20such%20as%20ensemble%0Amethods.%20In%20this%20work%2C%20we%20propose%20self-ensemble%2C%20a%20novel%20method%20that%20improves%0Athe%20generalization%20and%20visual%20reasoning%20of%20the%20model%20without%20updating%20any%0Aparameters%2C%20a%20training-free%20method.%20Our%20key%20insight%20is%20that%20we%20realized%20that%0ALVLM%20itself%20can%20ensemble%20without%20the%20need%20for%20any%20other%20LVLMs%2C%20which%20helps%20to%0Aunlock%20their%20internal%20capabilities.%20Extensive%20experiments%20on%20various%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20in%20achieving%20state-of-the-art%0A%28SOTA%29%20performance%20on%20SketchyVQA%2C%20Outside%20Knowledge%20VQA%2C%20and%0Aout-of-distribution%20VQA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20883v2&entry.124074799=Read"},
{"title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos", "author": "Fuzhao Xue and Yukang Chen and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Ethan He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han", "abstract": "  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, improving the long video\ncaptioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n", "link": "http://arxiv.org/abs/2408.10188v5", "date": "2024-11-01", "relevancy": 2.8525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5754}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&body=Title%3A%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos%0AAuthor%3A%20Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models%2C%0Aespecially%20for%20long%20video%20understanding.%20We%20introduce%20LongVILA%2C%20a%20full-stack%0Asolution%20for%20long-context%20visual-language%20models%20by%20co-designing%20the%20algorithm%0Aand%20system.%20For%20model%20training%2C%20we%20upgrade%20existing%20VLMs%20to%20support%20long%20video%0Aunderstanding%20by%20incorporating%20two%20additional%20stages%2C%20i.e.%2C%20long%20context%0Aextension%20and%20long%20video%20supervised%20fine-tuning.%20However%2C%20training%20on%20long%0Avideo%20is%20computationally%20and%20memory%20intensive.%20We%20introduce%20the%20long-context%0AMulti-Modal%20Sequence%20Parallelism%20%28MM-SP%29%20system%20that%20efficiently%20parallelizes%0Along%20video%20training%20and%20inference%2C%20enabling%202M%20context%20length%20training%20on%20256%0AGPUs%20without%20any%20gradient%20checkpointing.%20LongVILA%20efficiently%20extends%20the%0Anumber%20of%20video%20frames%20of%20VILA%20from%208%20to%202048%2C%20improving%20the%20long%20video%0Acaptioning%20score%20from%202.00%20to%203.26%20%28out%20of%205%29%2C%20achieving%2099.8%25%20accuracy%20in%0A6%2C000-frame%20%28more%20than%201%20million%20tokens%29%20video%20needle-in-a-haystack.%0ALongVILA-7B%20demonstrates%20strong%20accuracy%20on%20the%20VideoMME%20benchmark%2C%20i.e.%2C%2061.8%25%0Awith%20subtitle.%20Besides%2C%20MM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20style%20sequence%0Aparallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron%20with%20a%20hybrid%20context%20and%0Atensor%20parallelism.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10188v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVILA%253A%2520Scaling%2520Long-Context%2520Visual%2520Language%2520Models%2520for%2520Long%2520Videos%26entry.906535625%3DFuzhao%2520Xue%2520and%2520Yukang%2520Chen%2520and%2520Dacheng%2520Li%2520and%2520Qinghao%2520Hu%2520and%2520Ligeng%2520Zhu%2520and%2520Xiuyu%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhijian%2520Liu%2520and%2520Ethan%2520He%2520and%2520Hongxu%2520Yin%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Long-context%2520capability%2520is%2520critical%2520for%2520multi-modal%2520foundation%2520models%252C%250Aespecially%2520for%2520long%2520video%2520understanding.%2520We%2520introduce%2520LongVILA%252C%2520a%2520full-stack%250Asolution%2520for%2520long-context%2520visual-language%2520models%2520by%2520co-designing%2520the%2520algorithm%250Aand%2520system.%2520For%2520model%2520training%252C%2520we%2520upgrade%2520existing%2520VLMs%2520to%2520support%2520long%2520video%250Aunderstanding%2520by%2520incorporating%2520two%2520additional%2520stages%252C%2520i.e.%252C%2520long%2520context%250Aextension%2520and%2520long%2520video%2520supervised%2520fine-tuning.%2520However%252C%2520training%2520on%2520long%250Avideo%2520is%2520computationally%2520and%2520memory%2520intensive.%2520We%2520introduce%2520the%2520long-context%250AMulti-Modal%2520Sequence%2520Parallelism%2520%2528MM-SP%2529%2520system%2520that%2520efficiently%2520parallelizes%250Along%2520video%2520training%2520and%2520inference%252C%2520enabling%25202M%2520context%2520length%2520training%2520on%2520256%250AGPUs%2520without%2520any%2520gradient%2520checkpointing.%2520LongVILA%2520efficiently%2520extends%2520the%250Anumber%2520of%2520video%2520frames%2520of%2520VILA%2520from%25208%2520to%25202048%252C%2520improving%2520the%2520long%2520video%250Acaptioning%2520score%2520from%25202.00%2520to%25203.26%2520%2528out%2520of%25205%2529%252C%2520achieving%252099.8%2525%2520accuracy%2520in%250A6%252C000-frame%2520%2528more%2520than%25201%2520million%2520tokens%2529%2520video%2520needle-in-a-haystack.%250ALongVILA-7B%2520demonstrates%2520strong%2520accuracy%2520on%2520the%2520VideoMME%2520benchmark%252C%2520i.e.%252C%252061.8%2525%250Awith%2520subtitle.%2520Besides%252C%2520MM-SP%2520is%25202.1x%2520-%25205.7x%2520faster%2520than%2520ring%2520style%2520sequence%250Aparallelism%2520and%25201.1x%2520-%25201.4x%2520faster%2520than%2520Megatron%2520with%2520a%2520hybrid%2520context%2520and%250Atensor%2520parallelism.%2520Moreover%252C%2520it%2520seamlessly%2520integrates%2520with%2520Hugging%2520Face%250ATransformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10188v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&entry.906535625=Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models%2C%0Aespecially%20for%20long%20video%20understanding.%20We%20introduce%20LongVILA%2C%20a%20full-stack%0Asolution%20for%20long-context%20visual-language%20models%20by%20co-designing%20the%20algorithm%0Aand%20system.%20For%20model%20training%2C%20we%20upgrade%20existing%20VLMs%20to%20support%20long%20video%0Aunderstanding%20by%20incorporating%20two%20additional%20stages%2C%20i.e.%2C%20long%20context%0Aextension%20and%20long%20video%20supervised%20fine-tuning.%20However%2C%20training%20on%20long%0Avideo%20is%20computationally%20and%20memory%20intensive.%20We%20introduce%20the%20long-context%0AMulti-Modal%20Sequence%20Parallelism%20%28MM-SP%29%20system%20that%20efficiently%20parallelizes%0Along%20video%20training%20and%20inference%2C%20enabling%202M%20context%20length%20training%20on%20256%0AGPUs%20without%20any%20gradient%20checkpointing.%20LongVILA%20efficiently%20extends%20the%0Anumber%20of%20video%20frames%20of%20VILA%20from%208%20to%202048%2C%20improving%20the%20long%20video%0Acaptioning%20score%20from%202.00%20to%203.26%20%28out%20of%205%29%2C%20achieving%2099.8%25%20accuracy%20in%0A6%2C000-frame%20%28more%20than%201%20million%20tokens%29%20video%20needle-in-a-haystack.%0ALongVILA-7B%20demonstrates%20strong%20accuracy%20on%20the%20VideoMME%20benchmark%2C%20i.e.%2C%2061.8%25%0Awith%20subtitle.%20Besides%2C%20MM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20style%20sequence%0Aparallelism%20and%201.1x%20-%201.4x%20faster%20than%20Megatron%20with%20a%20hybrid%20context%20and%0Atensor%20parallelism.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%0ATransformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10188v5&entry.124074799=Read"},
{"title": "Improving Node Representation by Boosting Target-Aware Contrastive Loss", "author": "Ying-Chun Lin and Jennifer Neville", "abstract": "  Graphs model complex relationships between entities, with nodes and edges\ncapturing intricate connections. Node representation learning involves\ntransforming nodes into low-dimensional embeddings. These embeddings are\ntypically used as features for downstream tasks. Therefore, their quality has a\nsignificant impact on task performance. Existing approaches for node\nrepresentation learning span (semi-)supervised, unsupervised, and\nself-supervised paradigms. In graph domains, (semi-)supervised learning often\nonly optimizes models based on class labels, neglecting other abundant graph\nsignals, which limits generalization. While self-supervised or unsupervised\nlearning produces representations that better capture underlying graph signals,\nthe usefulness of these captured signals for downstream target tasks can vary.\nTo bridge this gap, we introduce Target-Aware Contrastive Learning\n(Target-aware CL) which aims to enhance target task performance by maximizing\nthe mutual information between the target task and node representations with a\nself-supervised learning process. This is achieved through a sampling function,\nXGBoost Sampler (XGSampler), to sample proper positive examples for the\nproposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware\nCL increases the mutual information between the target task and node\nrepresentations, such that model generalization is improved. Additionally,\nXGSampler enhances the interpretability of each signal by showing the weights\nfor sampling the proper positive examples. We show experimentally that XTCL\nsignificantly improves the performance on two target tasks: node classification\nand link prediction tasks, compared to state-of-the-art models.\n", "link": "http://arxiv.org/abs/2410.03901v2", "date": "2024-11-01", "relevancy": 2.6994, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Node%20Representation%20by%20Boosting%20Target-Aware%20Contrastive%20Loss&body=Title%3A%20Improving%20Node%20Representation%20by%20Boosting%20Target-Aware%20Contrastive%20Loss%0AAuthor%3A%20Ying-Chun%20Lin%20and%20Jennifer%20Neville%0AAbstract%3A%20%20%20Graphs%20model%20complex%20relationships%20between%20entities%2C%20with%20nodes%20and%20edges%0Acapturing%20intricate%20connections.%20Node%20representation%20learning%20involves%0Atransforming%20nodes%20into%20low-dimensional%20embeddings.%20These%20embeddings%20are%0Atypically%20used%20as%20features%20for%20downstream%20tasks.%20Therefore%2C%20their%20quality%20has%20a%0Asignificant%20impact%20on%20task%20performance.%20Existing%20approaches%20for%20node%0Arepresentation%20learning%20span%20%28semi-%29supervised%2C%20unsupervised%2C%20and%0Aself-supervised%20paradigms.%20In%20graph%20domains%2C%20%28semi-%29supervised%20learning%20often%0Aonly%20optimizes%20models%20based%20on%20class%20labels%2C%20neglecting%20other%20abundant%20graph%0Asignals%2C%20which%20limits%20generalization.%20While%20self-supervised%20or%20unsupervised%0Alearning%20produces%20representations%20that%20better%20capture%20underlying%20graph%20signals%2C%0Athe%20usefulness%20of%20these%20captured%20signals%20for%20downstream%20target%20tasks%20can%20vary.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20Target-Aware%20Contrastive%20Learning%0A%28Target-aware%20CL%29%20which%20aims%20to%20enhance%20target%20task%20performance%20by%20maximizing%0Athe%20mutual%20information%20between%20the%20target%20task%20and%20node%20representations%20with%20a%0Aself-supervised%20learning%20process.%20This%20is%20achieved%20through%20a%20sampling%20function%2C%0AXGBoost%20Sampler%20%28XGSampler%29%2C%20to%20sample%20proper%20positive%20examples%20for%20the%0Aproposed%20Target-Aware%20Contrastive%20Loss%20%28XTCL%29.%20By%20minimizing%20XTCL%2C%20Target-aware%0ACL%20increases%20the%20mutual%20information%20between%20the%20target%20task%20and%20node%0Arepresentations%2C%20such%20that%20model%20generalization%20is%20improved.%20Additionally%2C%0AXGSampler%20enhances%20the%20interpretability%20of%20each%20signal%20by%20showing%20the%20weights%0Afor%20sampling%20the%20proper%20positive%20examples.%20We%20show%20experimentally%20that%20XTCL%0Asignificantly%20improves%20the%20performance%20on%20two%20target%20tasks%3A%20node%20classification%0Aand%20link%20prediction%20tasks%2C%20compared%20to%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Node%2520Representation%2520by%2520Boosting%2520Target-Aware%2520Contrastive%2520Loss%26entry.906535625%3DYing-Chun%2520Lin%2520and%2520Jennifer%2520Neville%26entry.1292438233%3D%2520%2520Graphs%2520model%2520complex%2520relationships%2520between%2520entities%252C%2520with%2520nodes%2520and%2520edges%250Acapturing%2520intricate%2520connections.%2520Node%2520representation%2520learning%2520involves%250Atransforming%2520nodes%2520into%2520low-dimensional%2520embeddings.%2520These%2520embeddings%2520are%250Atypically%2520used%2520as%2520features%2520for%2520downstream%2520tasks.%2520Therefore%252C%2520their%2520quality%2520has%2520a%250Asignificant%2520impact%2520on%2520task%2520performance.%2520Existing%2520approaches%2520for%2520node%250Arepresentation%2520learning%2520span%2520%2528semi-%2529supervised%252C%2520unsupervised%252C%2520and%250Aself-supervised%2520paradigms.%2520In%2520graph%2520domains%252C%2520%2528semi-%2529supervised%2520learning%2520often%250Aonly%2520optimizes%2520models%2520based%2520on%2520class%2520labels%252C%2520neglecting%2520other%2520abundant%2520graph%250Asignals%252C%2520which%2520limits%2520generalization.%2520While%2520self-supervised%2520or%2520unsupervised%250Alearning%2520produces%2520representations%2520that%2520better%2520capture%2520underlying%2520graph%2520signals%252C%250Athe%2520usefulness%2520of%2520these%2520captured%2520signals%2520for%2520downstream%2520target%2520tasks%2520can%2520vary.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Target-Aware%2520Contrastive%2520Learning%250A%2528Target-aware%2520CL%2529%2520which%2520aims%2520to%2520enhance%2520target%2520task%2520performance%2520by%2520maximizing%250Athe%2520mutual%2520information%2520between%2520the%2520target%2520task%2520and%2520node%2520representations%2520with%2520a%250Aself-supervised%2520learning%2520process.%2520This%2520is%2520achieved%2520through%2520a%2520sampling%2520function%252C%250AXGBoost%2520Sampler%2520%2528XGSampler%2529%252C%2520to%2520sample%2520proper%2520positive%2520examples%2520for%2520the%250Aproposed%2520Target-Aware%2520Contrastive%2520Loss%2520%2528XTCL%2529.%2520By%2520minimizing%2520XTCL%252C%2520Target-aware%250ACL%2520increases%2520the%2520mutual%2520information%2520between%2520the%2520target%2520task%2520and%2520node%250Arepresentations%252C%2520such%2520that%2520model%2520generalization%2520is%2520improved.%2520Additionally%252C%250AXGSampler%2520enhances%2520the%2520interpretability%2520of%2520each%2520signal%2520by%2520showing%2520the%2520weights%250Afor%2520sampling%2520the%2520proper%2520positive%2520examples.%2520We%2520show%2520experimentally%2520that%2520XTCL%250Asignificantly%2520improves%2520the%2520performance%2520on%2520two%2520target%2520tasks%253A%2520node%2520classification%250Aand%2520link%2520prediction%2520tasks%252C%2520compared%2520to%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Node%20Representation%20by%20Boosting%20Target-Aware%20Contrastive%20Loss&entry.906535625=Ying-Chun%20Lin%20and%20Jennifer%20Neville&entry.1292438233=%20%20Graphs%20model%20complex%20relationships%20between%20entities%2C%20with%20nodes%20and%20edges%0Acapturing%20intricate%20connections.%20Node%20representation%20learning%20involves%0Atransforming%20nodes%20into%20low-dimensional%20embeddings.%20These%20embeddings%20are%0Atypically%20used%20as%20features%20for%20downstream%20tasks.%20Therefore%2C%20their%20quality%20has%20a%0Asignificant%20impact%20on%20task%20performance.%20Existing%20approaches%20for%20node%0Arepresentation%20learning%20span%20%28semi-%29supervised%2C%20unsupervised%2C%20and%0Aself-supervised%20paradigms.%20In%20graph%20domains%2C%20%28semi-%29supervised%20learning%20often%0Aonly%20optimizes%20models%20based%20on%20class%20labels%2C%20neglecting%20other%20abundant%20graph%0Asignals%2C%20which%20limits%20generalization.%20While%20self-supervised%20or%20unsupervised%0Alearning%20produces%20representations%20that%20better%20capture%20underlying%20graph%20signals%2C%0Athe%20usefulness%20of%20these%20captured%20signals%20for%20downstream%20target%20tasks%20can%20vary.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20Target-Aware%20Contrastive%20Learning%0A%28Target-aware%20CL%29%20which%20aims%20to%20enhance%20target%20task%20performance%20by%20maximizing%0Athe%20mutual%20information%20between%20the%20target%20task%20and%20node%20representations%20with%20a%0Aself-supervised%20learning%20process.%20This%20is%20achieved%20through%20a%20sampling%20function%2C%0AXGBoost%20Sampler%20%28XGSampler%29%2C%20to%20sample%20proper%20positive%20examples%20for%20the%0Aproposed%20Target-Aware%20Contrastive%20Loss%20%28XTCL%29.%20By%20minimizing%20XTCL%2C%20Target-aware%0ACL%20increases%20the%20mutual%20information%20between%20the%20target%20task%20and%20node%0Arepresentations%2C%20such%20that%20model%20generalization%20is%20improved.%20Additionally%2C%0AXGSampler%20enhances%20the%20interpretability%20of%20each%20signal%20by%20showing%20the%20weights%0Afor%20sampling%20the%20proper%20positive%20examples.%20We%20show%20experimentally%20that%20XTCL%0Asignificantly%20improves%20the%20performance%20on%20two%20target%20tasks%3A%20node%20classification%0Aand%20link%20prediction%20tasks%2C%20compared%20to%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03901v2&entry.124074799=Read"},
{"title": "VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images", "author": "Jose Vargas Quiros and Bart Liefers and Karin van Garderen and Jeroen Vermeulen and Eyened Reading Center and Sinergia Consortium and Caroline Klaver", "abstract": "  We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets . Additional CFIs, mainly from the\npopulation-based Rotterdam Study were annotated by graders for arteries and\nveins at pixel level, resulting in a dataset diverse in patient demographics\nand imaging conditions. VascX models demonstrated superior segmentation\nperformance across datasets, image quality levels, and anatomic regions when\ncompared to existing, publicly available models, likely due to the increased\nsize and variety of our training set. Important improvements were observed in\nartery-vein and disc segmentation performance, particularly in segmentations of\nthese structures on CFIs of intermediate quality, common in large cohorts and\nclinical datasets. Importantly, these improvements translated into\nsignificantly more accurate vascular features when we compared features\nextracted from VascX segmentation masks with features extracted from\nsegmentation masks generated by previous models. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye.\n", "link": "http://arxiv.org/abs/2409.16016v2", "date": "2024-11-01", "relevancy": 2.6784, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VascX%20Models%3A%20Model%20Ensembles%20for%20Retinal%20Vascular%20Analysis%20from%20Color%0A%20%20Fundus%20Images&body=Title%3A%20VascX%20Models%3A%20Model%20Ensembles%20for%20Retinal%20Vascular%20Analysis%20from%20Color%0A%20%20Fundus%20Images%0AAuthor%3A%20Jose%20Vargas%20Quiros%20and%20Bart%20Liefers%20and%20Karin%20van%20Garderen%20and%20Jeroen%20Vermeulen%20and%20Eyened%20Reading%20Center%20and%20Sinergia%20Consortium%20and%20Caroline%20Klaver%0AAbstract%3A%20%20%20We%20introduce%20VascX%20models%2C%20a%20comprehensive%20set%20of%20model%20ensembles%20for%0Aanalyzing%20retinal%20vasculature%20from%20color%20fundus%20images%20%28CFIs%29.%20Annotated%20CFIs%0Awere%20aggregated%20from%20public%20datasets%20.%20Additional%20CFIs%2C%20mainly%20from%20the%0Apopulation-based%20Rotterdam%20Study%20were%20annotated%20by%20graders%20for%20arteries%20and%0Aveins%20at%20pixel%20level%2C%20resulting%20in%20a%20dataset%20diverse%20in%20patient%20demographics%0Aand%20imaging%20conditions.%20VascX%20models%20demonstrated%20superior%20segmentation%0Aperformance%20across%20datasets%2C%20image%20quality%20levels%2C%20and%20anatomic%20regions%20when%0Acompared%20to%20existing%2C%20publicly%20available%20models%2C%20likely%20due%20to%20the%20increased%0Asize%20and%20variety%20of%20our%20training%20set.%20Important%20improvements%20were%20observed%20in%0Aartery-vein%20and%20disc%20segmentation%20performance%2C%20particularly%20in%20segmentations%20of%0Athese%20structures%20on%20CFIs%20of%20intermediate%20quality%2C%20common%20in%20large%20cohorts%20and%0Aclinical%20datasets.%20Importantly%2C%20these%20improvements%20translated%20into%0Asignificantly%20more%20accurate%20vascular%20features%20when%20we%20compared%20features%0Aextracted%20from%20VascX%20segmentation%20masks%20with%20features%20extracted%20from%0Asegmentation%20masks%20generated%20by%20previous%20models.%20With%20VascX%20models%20we%20provide%20a%0Arobust%2C%20ready-to-use%20set%20of%20model%20ensembles%20and%20inference%20code%20aimed%20at%0Asimplifying%20the%20implementation%20and%20enhancing%20the%20quality%20of%20automated%20retinal%0Avasculature%20analyses.%20The%20precise%20vessel%20parameters%20generated%20by%20the%20model%20can%0Aserve%20as%20starting%20points%20for%20the%20identification%20of%20disease%20patterns%20in%20and%0Aoutside%20of%20the%20eye.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16016v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVascX%2520Models%253A%2520Model%2520Ensembles%2520for%2520Retinal%2520Vascular%2520Analysis%2520from%2520Color%250A%2520%2520Fundus%2520Images%26entry.906535625%3DJose%2520Vargas%2520Quiros%2520and%2520Bart%2520Liefers%2520and%2520Karin%2520van%2520Garderen%2520and%2520Jeroen%2520Vermeulen%2520and%2520Eyened%2520Reading%2520Center%2520and%2520Sinergia%2520Consortium%2520and%2520Caroline%2520Klaver%26entry.1292438233%3D%2520%2520We%2520introduce%2520VascX%2520models%252C%2520a%2520comprehensive%2520set%2520of%2520model%2520ensembles%2520for%250Aanalyzing%2520retinal%2520vasculature%2520from%2520color%2520fundus%2520images%2520%2528CFIs%2529.%2520Annotated%2520CFIs%250Awere%2520aggregated%2520from%2520public%2520datasets%2520.%2520Additional%2520CFIs%252C%2520mainly%2520from%2520the%250Apopulation-based%2520Rotterdam%2520Study%2520were%2520annotated%2520by%2520graders%2520for%2520arteries%2520and%250Aveins%2520at%2520pixel%2520level%252C%2520resulting%2520in%2520a%2520dataset%2520diverse%2520in%2520patient%2520demographics%250Aand%2520imaging%2520conditions.%2520VascX%2520models%2520demonstrated%2520superior%2520segmentation%250Aperformance%2520across%2520datasets%252C%2520image%2520quality%2520levels%252C%2520and%2520anatomic%2520regions%2520when%250Acompared%2520to%2520existing%252C%2520publicly%2520available%2520models%252C%2520likely%2520due%2520to%2520the%2520increased%250Asize%2520and%2520variety%2520of%2520our%2520training%2520set.%2520Important%2520improvements%2520were%2520observed%2520in%250Aartery-vein%2520and%2520disc%2520segmentation%2520performance%252C%2520particularly%2520in%2520segmentations%2520of%250Athese%2520structures%2520on%2520CFIs%2520of%2520intermediate%2520quality%252C%2520common%2520in%2520large%2520cohorts%2520and%250Aclinical%2520datasets.%2520Importantly%252C%2520these%2520improvements%2520translated%2520into%250Asignificantly%2520more%2520accurate%2520vascular%2520features%2520when%2520we%2520compared%2520features%250Aextracted%2520from%2520VascX%2520segmentation%2520masks%2520with%2520features%2520extracted%2520from%250Asegmentation%2520masks%2520generated%2520by%2520previous%2520models.%2520With%2520VascX%2520models%2520we%2520provide%2520a%250Arobust%252C%2520ready-to-use%2520set%2520of%2520model%2520ensembles%2520and%2520inference%2520code%2520aimed%2520at%250Asimplifying%2520the%2520implementation%2520and%2520enhancing%2520the%2520quality%2520of%2520automated%2520retinal%250Avasculature%2520analyses.%2520The%2520precise%2520vessel%2520parameters%2520generated%2520by%2520the%2520model%2520can%250Aserve%2520as%2520starting%2520points%2520for%2520the%2520identification%2520of%2520disease%2520patterns%2520in%2520and%250Aoutside%2520of%2520the%2520eye.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16016v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VascX%20Models%3A%20Model%20Ensembles%20for%20Retinal%20Vascular%20Analysis%20from%20Color%0A%20%20Fundus%20Images&entry.906535625=Jose%20Vargas%20Quiros%20and%20Bart%20Liefers%20and%20Karin%20van%20Garderen%20and%20Jeroen%20Vermeulen%20and%20Eyened%20Reading%20Center%20and%20Sinergia%20Consortium%20and%20Caroline%20Klaver&entry.1292438233=%20%20We%20introduce%20VascX%20models%2C%20a%20comprehensive%20set%20of%20model%20ensembles%20for%0Aanalyzing%20retinal%20vasculature%20from%20color%20fundus%20images%20%28CFIs%29.%20Annotated%20CFIs%0Awere%20aggregated%20from%20public%20datasets%20.%20Additional%20CFIs%2C%20mainly%20from%20the%0Apopulation-based%20Rotterdam%20Study%20were%20annotated%20by%20graders%20for%20arteries%20and%0Aveins%20at%20pixel%20level%2C%20resulting%20in%20a%20dataset%20diverse%20in%20patient%20demographics%0Aand%20imaging%20conditions.%20VascX%20models%20demonstrated%20superior%20segmentation%0Aperformance%20across%20datasets%2C%20image%20quality%20levels%2C%20and%20anatomic%20regions%20when%0Acompared%20to%20existing%2C%20publicly%20available%20models%2C%20likely%20due%20to%20the%20increased%0Asize%20and%20variety%20of%20our%20training%20set.%20Important%20improvements%20were%20observed%20in%0Aartery-vein%20and%20disc%20segmentation%20performance%2C%20particularly%20in%20segmentations%20of%0Athese%20structures%20on%20CFIs%20of%20intermediate%20quality%2C%20common%20in%20large%20cohorts%20and%0Aclinical%20datasets.%20Importantly%2C%20these%20improvements%20translated%20into%0Asignificantly%20more%20accurate%20vascular%20features%20when%20we%20compared%20features%0Aextracted%20from%20VascX%20segmentation%20masks%20with%20features%20extracted%20from%0Asegmentation%20masks%20generated%20by%20previous%20models.%20With%20VascX%20models%20we%20provide%20a%0Arobust%2C%20ready-to-use%20set%20of%20model%20ensembles%20and%20inference%20code%20aimed%20at%0Asimplifying%20the%20implementation%20and%20enhancing%20the%20quality%20of%20automated%20retinal%0Avasculature%20analyses.%20The%20precise%20vessel%20parameters%20generated%20by%20the%20model%20can%0Aserve%20as%20starting%20points%20for%20the%20identification%20of%20disease%20patterns%20in%20and%0Aoutside%20of%20the%20eye.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16016v2&entry.124074799=Read"},
{"title": "Efficiency for Free: Ideal Data Are Transportable Representations", "author": "Peng Sun and Yi Jiang and Tao Lin", "abstract": "  Data, the seminal opportunity and challenge in modern machine learning,\ncurrently constrains the scalability of representation learning and impedes the\npace of model evolution. In this work, we investigate the efficiency properties\nof data from both optimization and generalization perspectives. Our theoretical\nand empirical analysis reveals an unexpected finding: for a given task,\nutilizing a publicly available, task- and architecture-agnostic model (referred\nto as the `prior model' in this paper) can effectively produce efficient data.\nBuilding on this insight, we propose the Representation Learning Accelerator\n(\\algopt), which promotes the formation and utilization of efficient data,\nthereby accelerating representation learning. Utilizing a ResNet-18 pre-trained\non CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K\nreduces computational costs by 50% while maintaining the same accuracy as the\nmodel trained with the original BYOL, which requires 100% cost. Our code is\navailable at: \\url{https://github.com/LINs-lab/ReLA}.\n", "link": "http://arxiv.org/abs/2405.14669v2", "date": "2024-11-01", "relevancy": 2.671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5436}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5295}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations&body=Title%3A%20Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations%0AAuthor%3A%20Peng%20Sun%20and%20Yi%20Jiang%20and%20Tao%20Lin%0AAbstract%3A%20%20%20Data%2C%20the%20seminal%20opportunity%20and%20challenge%20in%20modern%20machine%20learning%2C%0Acurrently%20constrains%20the%20scalability%20of%20representation%20learning%20and%20impedes%20the%0Apace%20of%20model%20evolution.%20In%20this%20work%2C%20we%20investigate%20the%20efficiency%20properties%0Aof%20data%20from%20both%20optimization%20and%20generalization%20perspectives.%20Our%20theoretical%0Aand%20empirical%20analysis%20reveals%20an%20unexpected%20finding%3A%20for%20a%20given%20task%2C%0Autilizing%20a%20publicly%20available%2C%20task-%20and%20architecture-agnostic%20model%20%28referred%0Ato%20as%20the%20%60prior%20model%27%20in%20this%20paper%29%20can%20effectively%20produce%20efficient%20data.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20the%20Representation%20Learning%20Accelerator%0A%28%5Calgopt%29%2C%20which%20promotes%20the%20formation%20and%20utilization%20of%20efficient%20data%2C%0Athereby%20accelerating%20representation%20learning.%20Utilizing%20a%20ResNet-18%20pre-trained%0Aon%20CIFAR-10%20as%20a%20prior%20model%20to%20inform%20ResNet-50%20training%20on%20ImageNet-1K%0Areduces%20computational%20costs%20by%2050%25%20while%20maintaining%20the%20same%20accuracy%20as%20the%0Amodel%20trained%20with%20the%20original%20BYOL%2C%20which%20requires%20100%25%20cost.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/LINs-lab/ReLA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiency%2520for%2520Free%253A%2520Ideal%2520Data%2520Are%2520Transportable%2520Representations%26entry.906535625%3DPeng%2520Sun%2520and%2520Yi%2520Jiang%2520and%2520Tao%2520Lin%26entry.1292438233%3D%2520%2520Data%252C%2520the%2520seminal%2520opportunity%2520and%2520challenge%2520in%2520modern%2520machine%2520learning%252C%250Acurrently%2520constrains%2520the%2520scalability%2520of%2520representation%2520learning%2520and%2520impedes%2520the%250Apace%2520of%2520model%2520evolution.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520efficiency%2520properties%250Aof%2520data%2520from%2520both%2520optimization%2520and%2520generalization%2520perspectives.%2520Our%2520theoretical%250Aand%2520empirical%2520analysis%2520reveals%2520an%2520unexpected%2520finding%253A%2520for%2520a%2520given%2520task%252C%250Autilizing%2520a%2520publicly%2520available%252C%2520task-%2520and%2520architecture-agnostic%2520model%2520%2528referred%250Ato%2520as%2520the%2520%2560prior%2520model%2527%2520in%2520this%2520paper%2529%2520can%2520effectively%2520produce%2520efficient%2520data.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520propose%2520the%2520Representation%2520Learning%2520Accelerator%250A%2528%255Calgopt%2529%252C%2520which%2520promotes%2520the%2520formation%2520and%2520utilization%2520of%2520efficient%2520data%252C%250Athereby%2520accelerating%2520representation%2520learning.%2520Utilizing%2520a%2520ResNet-18%2520pre-trained%250Aon%2520CIFAR-10%2520as%2520a%2520prior%2520model%2520to%2520inform%2520ResNet-50%2520training%2520on%2520ImageNet-1K%250Areduces%2520computational%2520costs%2520by%252050%2525%2520while%2520maintaining%2520the%2520same%2520accuracy%2520as%2520the%250Amodel%2520trained%2520with%2520the%2520original%2520BYOL%252C%2520which%2520requires%2520100%2525%2520cost.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/LINs-lab/ReLA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations&entry.906535625=Peng%20Sun%20and%20Yi%20Jiang%20and%20Tao%20Lin&entry.1292438233=%20%20Data%2C%20the%20seminal%20opportunity%20and%20challenge%20in%20modern%20machine%20learning%2C%0Acurrently%20constrains%20the%20scalability%20of%20representation%20learning%20and%20impedes%20the%0Apace%20of%20model%20evolution.%20In%20this%20work%2C%20we%20investigate%20the%20efficiency%20properties%0Aof%20data%20from%20both%20optimization%20and%20generalization%20perspectives.%20Our%20theoretical%0Aand%20empirical%20analysis%20reveals%20an%20unexpected%20finding%3A%20for%20a%20given%20task%2C%0Autilizing%20a%20publicly%20available%2C%20task-%20and%20architecture-agnostic%20model%20%28referred%0Ato%20as%20the%20%60prior%20model%27%20in%20this%20paper%29%20can%20effectively%20produce%20efficient%20data.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20the%20Representation%20Learning%20Accelerator%0A%28%5Calgopt%29%2C%20which%20promotes%20the%20formation%20and%20utilization%20of%20efficient%20data%2C%0Athereby%20accelerating%20representation%20learning.%20Utilizing%20a%20ResNet-18%20pre-trained%0Aon%20CIFAR-10%20as%20a%20prior%20model%20to%20inform%20ResNet-50%20training%20on%20ImageNet-1K%0Areduces%20computational%20costs%20by%2050%25%20while%20maintaining%20the%20same%20accuracy%20as%20the%0Amodel%20trained%20with%20the%20original%20BYOL%2C%20which%20requires%20100%25%20cost.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/LINs-lab/ReLA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14669v2&entry.124074799=Read"},
{"title": "FRoundation: Are Foundation Models Ready for Face Recognition?", "author": "Tahar Chettaoui and Naser Damer and Fadi Boutros", "abstract": "  Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models.\n", "link": "http://arxiv.org/abs/2410.23831v2", "date": "2024-11-01", "relevancy": 2.6085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F&body=Title%3A%20FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F%0AAuthor%3A%20Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros%0AAbstract%3A%20%20%20Foundation%20models%20are%20predominantly%20trained%20in%20an%20unsupervised%20or%0Aself-supervised%20manner%20on%20highly%20diverse%20and%20large-scale%20datasets%2C%20making%20them%0Abroadly%20applicable%20to%20various%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%0Afor%20the%20first%20time%20whether%20such%20models%20are%20suitable%20for%20the%20specific%20domain%20of%0Aface%20recognition.%20We%20further%20propose%20and%20demonstrate%20the%20adaptation%20of%20these%0Amodels%20for%20face%20recognition%20across%20different%20levels%20of%20data%20availability.%0AExtensive%20experiments%20are%20conducted%20on%20multiple%20foundation%20models%20and%20datasets%0Aof%20varying%20scales%20for%20training%20and%20fine-tuning%2C%20with%20evaluation%20on%20a%20wide%20range%0Aof%20benchmarks.%20Our%20results%20indicate%20that%2C%20despite%20their%20versatility%2C%0Apre-trained%20foundation%20models%20underperform%20in%20face%20recognition%20compared%20to%0Asimilar%20architectures%20trained%20specifically%20for%20this%20task.%20However%2C%20fine-tuning%0Afoundation%20models%20yields%20promising%20results%2C%20often%20surpassing%20models%20trained%0Afrom%20scratch%20when%20training%20data%20is%20limited.%20Even%20with%20access%20to%20large-scale%0Aface%20recognition%20training%20datasets%2C%20fine-tuned%20foundation%20models%20perform%0Acomparably%20to%20models%20trained%20from%20scratch%2C%20but%20with%20lower%20training%0Acomputational%20costs%20and%20without%20relying%20on%20the%20assumption%20of%20extensive%20data%0Aavailability.%20Our%20analysis%20also%20explores%20bias%20in%20face%20recognition%2C%20with%0Aslightly%20higher%20bias%20observed%20in%20some%20settings%20when%20using%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRoundation%253A%2520Are%2520Foundation%2520Models%2520Ready%2520for%2520Face%2520Recognition%253F%26entry.906535625%3DTahar%2520Chettaoui%2520and%2520Naser%2520Damer%2520and%2520Fadi%2520Boutros%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520predominantly%2520trained%2520in%2520an%2520unsupervised%2520or%250Aself-supervised%2520manner%2520on%2520highly%2520diverse%2520and%2520large-scale%2520datasets%252C%2520making%2520them%250Abroadly%2520applicable%2520to%2520various%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520investigate%250Afor%2520the%2520first%2520time%2520whether%2520such%2520models%2520are%2520suitable%2520for%2520the%2520specific%2520domain%2520of%250Aface%2520recognition.%2520We%2520further%2520propose%2520and%2520demonstrate%2520the%2520adaptation%2520of%2520these%250Amodels%2520for%2520face%2520recognition%2520across%2520different%2520levels%2520of%2520data%2520availability.%250AExtensive%2520experiments%2520are%2520conducted%2520on%2520multiple%2520foundation%2520models%2520and%2520datasets%250Aof%2520varying%2520scales%2520for%2520training%2520and%2520fine-tuning%252C%2520with%2520evaluation%2520on%2520a%2520wide%2520range%250Aof%2520benchmarks.%2520Our%2520results%2520indicate%2520that%252C%2520despite%2520their%2520versatility%252C%250Apre-trained%2520foundation%2520models%2520underperform%2520in%2520face%2520recognition%2520compared%2520to%250Asimilar%2520architectures%2520trained%2520specifically%2520for%2520this%2520task.%2520However%252C%2520fine-tuning%250Afoundation%2520models%2520yields%2520promising%2520results%252C%2520often%2520surpassing%2520models%2520trained%250Afrom%2520scratch%2520when%2520training%2520data%2520is%2520limited.%2520Even%2520with%2520access%2520to%2520large-scale%250Aface%2520recognition%2520training%2520datasets%252C%2520fine-tuned%2520foundation%2520models%2520perform%250Acomparably%2520to%2520models%2520trained%2520from%2520scratch%252C%2520but%2520with%2520lower%2520training%250Acomputational%2520costs%2520and%2520without%2520relying%2520on%2520the%2520assumption%2520of%2520extensive%2520data%250Aavailability.%2520Our%2520analysis%2520also%2520explores%2520bias%2520in%2520face%2520recognition%252C%2520with%250Aslightly%2520higher%2520bias%2520observed%2520in%2520some%2520settings%2520when%2520using%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F&entry.906535625=Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros&entry.1292438233=%20%20Foundation%20models%20are%20predominantly%20trained%20in%20an%20unsupervised%20or%0Aself-supervised%20manner%20on%20highly%20diverse%20and%20large-scale%20datasets%2C%20making%20them%0Abroadly%20applicable%20to%20various%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%0Afor%20the%20first%20time%20whether%20such%20models%20are%20suitable%20for%20the%20specific%20domain%20of%0Aface%20recognition.%20We%20further%20propose%20and%20demonstrate%20the%20adaptation%20of%20these%0Amodels%20for%20face%20recognition%20across%20different%20levels%20of%20data%20availability.%0AExtensive%20experiments%20are%20conducted%20on%20multiple%20foundation%20models%20and%20datasets%0Aof%20varying%20scales%20for%20training%20and%20fine-tuning%2C%20with%20evaluation%20on%20a%20wide%20range%0Aof%20benchmarks.%20Our%20results%20indicate%20that%2C%20despite%20their%20versatility%2C%0Apre-trained%20foundation%20models%20underperform%20in%20face%20recognition%20compared%20to%0Asimilar%20architectures%20trained%20specifically%20for%20this%20task.%20However%2C%20fine-tuning%0Afoundation%20models%20yields%20promising%20results%2C%20often%20surpassing%20models%20trained%0Afrom%20scratch%20when%20training%20data%20is%20limited.%20Even%20with%20access%20to%20large-scale%0Aface%20recognition%20training%20datasets%2C%20fine-tuned%20foundation%20models%20perform%0Acomparably%20to%20models%20trained%20from%20scratch%2C%20but%20with%20lower%20training%0Acomputational%20costs%20and%20without%20relying%20on%20the%20assumption%20of%20extensive%20data%0Aavailability.%20Our%20analysis%20also%20explores%20bias%20in%20face%20recognition%2C%20with%0Aslightly%20higher%20bias%20observed%20in%20some%20settings%20when%20using%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23831v2&entry.124074799=Read"},
{"title": "Video Diffusion Models are Training-free Motion Interpreter and\n  Controller", "author": "Zeqi Xiao and Yifan Zhou and Shuai Yang and Xingang Pan", "abstract": "  Video generation primarily aims to model authentic and customized motion\nacross frames, making understanding and controlling the motion a crucial topic.\nMost diffusion-based studies on video motion focus on motion customization with\ntraining-based paradigms, which, however, demands substantial training\nresources and necessitates retraining for diverse models. Crucially, these\napproaches do not explore how video diffusion models encode cross-frame motion\ninformation in their features, lacking interpretability and transparency in\ntheir effectiveness. To answer this question, this paper introduces a novel\nperspective to understand, localize, and manipulate motion-aware features in\nvideo diffusion models. Through analysis using Principal Component Analysis\n(PCA), our work discloses that robust motion-aware feature already exists in\nvideo diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating\ncontent correlation information and filtering motion channels. MOFT provides a\ndistinct set of benefits, including the ability to encode comprehensive motion\ninformation with clear interpretability, extraction without the need for\ntraining, and generalizability across diverse architectures. Leveraging MOFT,\nwe propose a novel training-free video motion control framework. Our method\ndemonstrates competitive performance in generating natural and faithful motion,\nproviding architecture-agnostic insights and applicability in a variety of\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2405.14864v2", "date": "2024-11-01", "relevancy": 2.545, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6304}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller&body=Title%3A%20Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller%0AAuthor%3A%20Zeqi%20Xiao%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20Video%20generation%20primarily%20aims%20to%20model%20authentic%20and%20customized%20motion%0Aacross%20frames%2C%20making%20understanding%20and%20controlling%20the%20motion%20a%20crucial%20topic.%0AMost%20diffusion-based%20studies%20on%20video%20motion%20focus%20on%20motion%20customization%20with%0Atraining-based%20paradigms%2C%20which%2C%20however%2C%20demands%20substantial%20training%0Aresources%20and%20necessitates%20retraining%20for%20diverse%20models.%20Crucially%2C%20these%0Aapproaches%20do%20not%20explore%20how%20video%20diffusion%20models%20encode%20cross-frame%20motion%0Ainformation%20in%20their%20features%2C%20lacking%20interpretability%20and%20transparency%20in%0Atheir%20effectiveness.%20To%20answer%20this%20question%2C%20this%20paper%20introduces%20a%20novel%0Aperspective%20to%20understand%2C%20localize%2C%20and%20manipulate%20motion-aware%20features%20in%0Avideo%20diffusion%20models.%20Through%20analysis%20using%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20our%20work%20discloses%20that%20robust%20motion-aware%20feature%20already%20exists%20in%0Avideo%20diffusion%20models.%20We%20present%20a%20new%20MOtion%20FeaTure%20%28MOFT%29%20by%20eliminating%0Acontent%20correlation%20information%20and%20filtering%20motion%20channels.%20MOFT%20provides%20a%0Adistinct%20set%20of%20benefits%2C%20including%20the%20ability%20to%20encode%20comprehensive%20motion%0Ainformation%20with%20clear%20interpretability%2C%20extraction%20without%20the%20need%20for%0Atraining%2C%20and%20generalizability%20across%20diverse%20architectures.%20Leveraging%20MOFT%2C%0Awe%20propose%20a%20novel%20training-free%20video%20motion%20control%20framework.%20Our%20method%0Ademonstrates%20competitive%20performance%20in%20generating%20natural%20and%20faithful%20motion%2C%0Aproviding%20architecture-agnostic%20insights%20and%20applicability%20in%20a%20variety%20of%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Diffusion%2520Models%2520are%2520Training-free%2520Motion%2520Interpreter%2520and%250A%2520%2520Controller%26entry.906535625%3DZeqi%2520Xiao%2520and%2520Yifan%2520Zhou%2520and%2520Shuai%2520Yang%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520Video%2520generation%2520primarily%2520aims%2520to%2520model%2520authentic%2520and%2520customized%2520motion%250Aacross%2520frames%252C%2520making%2520understanding%2520and%2520controlling%2520the%2520motion%2520a%2520crucial%2520topic.%250AMost%2520diffusion-based%2520studies%2520on%2520video%2520motion%2520focus%2520on%2520motion%2520customization%2520with%250Atraining-based%2520paradigms%252C%2520which%252C%2520however%252C%2520demands%2520substantial%2520training%250Aresources%2520and%2520necessitates%2520retraining%2520for%2520diverse%2520models.%2520Crucially%252C%2520these%250Aapproaches%2520do%2520not%2520explore%2520how%2520video%2520diffusion%2520models%2520encode%2520cross-frame%2520motion%250Ainformation%2520in%2520their%2520features%252C%2520lacking%2520interpretability%2520and%2520transparency%2520in%250Atheir%2520effectiveness.%2520To%2520answer%2520this%2520question%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Aperspective%2520to%2520understand%252C%2520localize%252C%2520and%2520manipulate%2520motion-aware%2520features%2520in%250Avideo%2520diffusion%2520models.%2520Through%2520analysis%2520using%2520Principal%2520Component%2520Analysis%250A%2528PCA%2529%252C%2520our%2520work%2520discloses%2520that%2520robust%2520motion-aware%2520feature%2520already%2520exists%2520in%250Avideo%2520diffusion%2520models.%2520We%2520present%2520a%2520new%2520MOtion%2520FeaTure%2520%2528MOFT%2529%2520by%2520eliminating%250Acontent%2520correlation%2520information%2520and%2520filtering%2520motion%2520channels.%2520MOFT%2520provides%2520a%250Adistinct%2520set%2520of%2520benefits%252C%2520including%2520the%2520ability%2520to%2520encode%2520comprehensive%2520motion%250Ainformation%2520with%2520clear%2520interpretability%252C%2520extraction%2520without%2520the%2520need%2520for%250Atraining%252C%2520and%2520generalizability%2520across%2520diverse%2520architectures.%2520Leveraging%2520MOFT%252C%250Awe%2520propose%2520a%2520novel%2520training-free%2520video%2520motion%2520control%2520framework.%2520Our%2520method%250Ademonstrates%2520competitive%2520performance%2520in%2520generating%2520natural%2520and%2520faithful%2520motion%252C%250Aproviding%2520architecture-agnostic%2520insights%2520and%2520applicability%2520in%2520a%2520variety%2520of%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller&entry.906535625=Zeqi%20Xiao%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Xingang%20Pan&entry.1292438233=%20%20Video%20generation%20primarily%20aims%20to%20model%20authentic%20and%20customized%20motion%0Aacross%20frames%2C%20making%20understanding%20and%20controlling%20the%20motion%20a%20crucial%20topic.%0AMost%20diffusion-based%20studies%20on%20video%20motion%20focus%20on%20motion%20customization%20with%0Atraining-based%20paradigms%2C%20which%2C%20however%2C%20demands%20substantial%20training%0Aresources%20and%20necessitates%20retraining%20for%20diverse%20models.%20Crucially%2C%20these%0Aapproaches%20do%20not%20explore%20how%20video%20diffusion%20models%20encode%20cross-frame%20motion%0Ainformation%20in%20their%20features%2C%20lacking%20interpretability%20and%20transparency%20in%0Atheir%20effectiveness.%20To%20answer%20this%20question%2C%20this%20paper%20introduces%20a%20novel%0Aperspective%20to%20understand%2C%20localize%2C%20and%20manipulate%20motion-aware%20features%20in%0Avideo%20diffusion%20models.%20Through%20analysis%20using%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20our%20work%20discloses%20that%20robust%20motion-aware%20feature%20already%20exists%20in%0Avideo%20diffusion%20models.%20We%20present%20a%20new%20MOtion%20FeaTure%20%28MOFT%29%20by%20eliminating%0Acontent%20correlation%20information%20and%20filtering%20motion%20channels.%20MOFT%20provides%20a%0Adistinct%20set%20of%20benefits%2C%20including%20the%20ability%20to%20encode%20comprehensive%20motion%0Ainformation%20with%20clear%20interpretability%2C%20extraction%20without%20the%20need%20for%0Atraining%2C%20and%20generalizability%20across%20diverse%20architectures.%20Leveraging%20MOFT%2C%0Awe%20propose%20a%20novel%20training-free%20video%20motion%20control%20framework.%20Our%20method%0Ademonstrates%20competitive%20performance%20in%20generating%20natural%20and%20faithful%20motion%2C%0Aproviding%20architecture-agnostic%20insights%20and%20applicability%20in%20a%20variety%20of%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14864v2&entry.124074799=Read"},
{"title": "Autoregressive Image Generation without Vector Quantization", "author": "Tianhong Li and Yonglong Tian and He Li and Mingyang Deng and Kaiming He", "abstract": "  Conventional wisdom holds that autoregressive models for image generation are\ntypically accompanied by vector-quantized tokens. We observe that while a\ndiscrete-valued space can facilitate representing a categorical distribution,\nit is not a necessity for autoregressive modeling. In this work, we propose to\nmodel the per-token probability distribution using a diffusion procedure, which\nallows us to apply autoregressive models in a continuous-valued space. Rather\nthan using categorical cross-entropy loss, we define a Diffusion Loss function\nto model the per-token probability. This approach eliminates the need for\ndiscrete-valued tokenizers. We evaluate its effectiveness across a wide range\nof cases, including standard autoregressive models and generalized masked\nautoregressive (MAR) variants. By removing vector quantization, our image\ngenerator achieves strong results while enjoying the speed advantage of\nsequence modeling. We hope this work will motivate the use of autoregressive\ngeneration in other continuous-valued domains and applications. Code is\navailable at: https://github.com/LTH14/mar.\n", "link": "http://arxiv.org/abs/2406.11838v3", "date": "2024-11-01", "relevancy": 2.4433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6208}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6038}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Image%20Generation%20without%20Vector%20Quantization&body=Title%3A%20Autoregressive%20Image%20Generation%20without%20Vector%20Quantization%0AAuthor%3A%20Tianhong%20Li%20and%20Yonglong%20Tian%20and%20He%20Li%20and%20Mingyang%20Deng%20and%20Kaiming%20He%0AAbstract%3A%20%20%20Conventional%20wisdom%20holds%20that%20autoregressive%20models%20for%20image%20generation%20are%0Atypically%20accompanied%20by%20vector-quantized%20tokens.%20We%20observe%20that%20while%20a%0Adiscrete-valued%20space%20can%20facilitate%20representing%20a%20categorical%20distribution%2C%0Ait%20is%20not%20a%20necessity%20for%20autoregressive%20modeling.%20In%20this%20work%2C%20we%20propose%20to%0Amodel%20the%20per-token%20probability%20distribution%20using%20a%20diffusion%20procedure%2C%20which%0Aallows%20us%20to%20apply%20autoregressive%20models%20in%20a%20continuous-valued%20space.%20Rather%0Athan%20using%20categorical%20cross-entropy%20loss%2C%20we%20define%20a%20Diffusion%20Loss%20function%0Ato%20model%20the%20per-token%20probability.%20This%20approach%20eliminates%20the%20need%20for%0Adiscrete-valued%20tokenizers.%20We%20evaluate%20its%20effectiveness%20across%20a%20wide%20range%0Aof%20cases%2C%20including%20standard%20autoregressive%20models%20and%20generalized%20masked%0Aautoregressive%20%28MAR%29%20variants.%20By%20removing%20vector%20quantization%2C%20our%20image%0Agenerator%20achieves%20strong%20results%20while%20enjoying%20the%20speed%20advantage%20of%0Asequence%20modeling.%20We%20hope%20this%20work%20will%20motivate%20the%20use%20of%20autoregressive%0Ageneration%20in%20other%20continuous-valued%20domains%20and%20applications.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/LTH14/mar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11838v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Image%2520Generation%2520without%2520Vector%2520Quantization%26entry.906535625%3DTianhong%2520Li%2520and%2520Yonglong%2520Tian%2520and%2520He%2520Li%2520and%2520Mingyang%2520Deng%2520and%2520Kaiming%2520He%26entry.1292438233%3D%2520%2520Conventional%2520wisdom%2520holds%2520that%2520autoregressive%2520models%2520for%2520image%2520generation%2520are%250Atypically%2520accompanied%2520by%2520vector-quantized%2520tokens.%2520We%2520observe%2520that%2520while%2520a%250Adiscrete-valued%2520space%2520can%2520facilitate%2520representing%2520a%2520categorical%2520distribution%252C%250Ait%2520is%2520not%2520a%2520necessity%2520for%2520autoregressive%2520modeling.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%250Amodel%2520the%2520per-token%2520probability%2520distribution%2520using%2520a%2520diffusion%2520procedure%252C%2520which%250Aallows%2520us%2520to%2520apply%2520autoregressive%2520models%2520in%2520a%2520continuous-valued%2520space.%2520Rather%250Athan%2520using%2520categorical%2520cross-entropy%2520loss%252C%2520we%2520define%2520a%2520Diffusion%2520Loss%2520function%250Ato%2520model%2520the%2520per-token%2520probability.%2520This%2520approach%2520eliminates%2520the%2520need%2520for%250Adiscrete-valued%2520tokenizers.%2520We%2520evaluate%2520its%2520effectiveness%2520across%2520a%2520wide%2520range%250Aof%2520cases%252C%2520including%2520standard%2520autoregressive%2520models%2520and%2520generalized%2520masked%250Aautoregressive%2520%2528MAR%2529%2520variants.%2520By%2520removing%2520vector%2520quantization%252C%2520our%2520image%250Agenerator%2520achieves%2520strong%2520results%2520while%2520enjoying%2520the%2520speed%2520advantage%2520of%250Asequence%2520modeling.%2520We%2520hope%2520this%2520work%2520will%2520motivate%2520the%2520use%2520of%2520autoregressive%250Ageneration%2520in%2520other%2520continuous-valued%2520domains%2520and%2520applications.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/LTH14/mar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11838v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Image%20Generation%20without%20Vector%20Quantization&entry.906535625=Tianhong%20Li%20and%20Yonglong%20Tian%20and%20He%20Li%20and%20Mingyang%20Deng%20and%20Kaiming%20He&entry.1292438233=%20%20Conventional%20wisdom%20holds%20that%20autoregressive%20models%20for%20image%20generation%20are%0Atypically%20accompanied%20by%20vector-quantized%20tokens.%20We%20observe%20that%20while%20a%0Adiscrete-valued%20space%20can%20facilitate%20representing%20a%20categorical%20distribution%2C%0Ait%20is%20not%20a%20necessity%20for%20autoregressive%20modeling.%20In%20this%20work%2C%20we%20propose%20to%0Amodel%20the%20per-token%20probability%20distribution%20using%20a%20diffusion%20procedure%2C%20which%0Aallows%20us%20to%20apply%20autoregressive%20models%20in%20a%20continuous-valued%20space.%20Rather%0Athan%20using%20categorical%20cross-entropy%20loss%2C%20we%20define%20a%20Diffusion%20Loss%20function%0Ato%20model%20the%20per-token%20probability.%20This%20approach%20eliminates%20the%20need%20for%0Adiscrete-valued%20tokenizers.%20We%20evaluate%20its%20effectiveness%20across%20a%20wide%20range%0Aof%20cases%2C%20including%20standard%20autoregressive%20models%20and%20generalized%20masked%0Aautoregressive%20%28MAR%29%20variants.%20By%20removing%20vector%20quantization%2C%20our%20image%0Agenerator%20achieves%20strong%20results%20while%20enjoying%20the%20speed%20advantage%20of%0Asequence%20modeling.%20We%20hope%20this%20work%20will%20motivate%20the%20use%20of%20autoregressive%0Ageneration%20in%20other%20continuous-valued%20domains%20and%20applications.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/LTH14/mar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11838v3&entry.124074799=Read"},
{"title": "HENASY: Learning to Assemble Scene-Entities for Egocentric\n  Video-Language Model", "author": "Khoa Vo and Thinh Phan and Kashu Yamazaki and Minh Tran and Ngan Le", "abstract": "  Current video-language models (VLMs) rely extensively on instance-level\nalignment between video and language modalities, which presents two major\nlimitations: (1) visual reasoning disobeys the natural perception that humans\ndo in first-person perspective, leading to a lack of reasoning interpretation;\nand (2) learning is limited in capturing inherent fine-grained relationships\nbetween two modalities.\n  In this paper, we take an inspiration from human perception and explore a\ncompositional approach for egocentric video representation. We introduce HENASY\n(Hierarchical ENtities ASsemblY), which includes a spatiotemporal token\ngrouping mechanism to explicitly assemble dynamically evolving scene entities\nthrough time and model their relationship for video representation. By\nleveraging compositional structure understanding, HENASY possesses strong\ninterpretability via visual grounding with free-form text queries. We further\nexplore a suite of multi-grained contrastive losses to facilitate\nentity-centric understandings. This comprises three alignment types:\nvideo-narration, noun-entity, verb-entities alignments.\n  Our method demonstrates strong interpretability in both quantitative and\nqualitative experiments; while maintaining competitive performances on five\ndownstream tasks via zero-shot transfer or as video/text representation,\nincluding video/text retrieval, action recognition, multi-choice query, natural\nlanguage query, and moments query.\n", "link": "http://arxiv.org/abs/2406.00307v4", "date": "2024-11-01", "relevancy": 2.4385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6231}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HENASY%3A%20Learning%20to%20Assemble%20Scene-Entities%20for%20Egocentric%0A%20%20Video-Language%20Model&body=Title%3A%20HENASY%3A%20Learning%20to%20Assemble%20Scene-Entities%20for%20Egocentric%0A%20%20Video-Language%20Model%0AAuthor%3A%20Khoa%20Vo%20and%20Thinh%20Phan%20and%20Kashu%20Yamazaki%20and%20Minh%20Tran%20and%20Ngan%20Le%0AAbstract%3A%20%20%20Current%20video-language%20models%20%28VLMs%29%20rely%20extensively%20on%20instance-level%0Aalignment%20between%20video%20and%20language%20modalities%2C%20which%20presents%20two%20major%0Alimitations%3A%20%281%29%20visual%20reasoning%20disobeys%20the%20natural%20perception%20that%20humans%0Ado%20in%20first-person%20perspective%2C%20leading%20to%20a%20lack%20of%20reasoning%20interpretation%3B%0Aand%20%282%29%20learning%20is%20limited%20in%20capturing%20inherent%20fine-grained%20relationships%0Abetween%20two%20modalities.%0A%20%20In%20this%20paper%2C%20we%20take%20an%20inspiration%20from%20human%20perception%20and%20explore%20a%0Acompositional%20approach%20for%20egocentric%20video%20representation.%20We%20introduce%20HENASY%0A%28Hierarchical%20ENtities%20ASsemblY%29%2C%20which%20includes%20a%20spatiotemporal%20token%0Agrouping%20mechanism%20to%20explicitly%20assemble%20dynamically%20evolving%20scene%20entities%0Athrough%20time%20and%20model%20their%20relationship%20for%20video%20representation.%20By%0Aleveraging%20compositional%20structure%20understanding%2C%20HENASY%20possesses%20strong%0Ainterpretability%20via%20visual%20grounding%20with%20free-form%20text%20queries.%20We%20further%0Aexplore%20a%20suite%20of%20multi-grained%20contrastive%20losses%20to%20facilitate%0Aentity-centric%20understandings.%20This%20comprises%20three%20alignment%20types%3A%0Avideo-narration%2C%20noun-entity%2C%20verb-entities%20alignments.%0A%20%20Our%20method%20demonstrates%20strong%20interpretability%20in%20both%20quantitative%20and%0Aqualitative%20experiments%3B%20while%20maintaining%20competitive%20performances%20on%20five%0Adownstream%20tasks%20via%20zero-shot%20transfer%20or%20as%20video/text%20representation%2C%0Aincluding%20video/text%20retrieval%2C%20action%20recognition%2C%20multi-choice%20query%2C%20natural%0Alanguage%20query%2C%20and%20moments%20query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00307v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHENASY%253A%2520Learning%2520to%2520Assemble%2520Scene-Entities%2520for%2520Egocentric%250A%2520%2520Video-Language%2520Model%26entry.906535625%3DKhoa%2520Vo%2520and%2520Thinh%2520Phan%2520and%2520Kashu%2520Yamazaki%2520and%2520Minh%2520Tran%2520and%2520Ngan%2520Le%26entry.1292438233%3D%2520%2520Current%2520video-language%2520models%2520%2528VLMs%2529%2520rely%2520extensively%2520on%2520instance-level%250Aalignment%2520between%2520video%2520and%2520language%2520modalities%252C%2520which%2520presents%2520two%2520major%250Alimitations%253A%2520%25281%2529%2520visual%2520reasoning%2520disobeys%2520the%2520natural%2520perception%2520that%2520humans%250Ado%2520in%2520first-person%2520perspective%252C%2520leading%2520to%2520a%2520lack%2520of%2520reasoning%2520interpretation%253B%250Aand%2520%25282%2529%2520learning%2520is%2520limited%2520in%2520capturing%2520inherent%2520fine-grained%2520relationships%250Abetween%2520two%2520modalities.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520take%2520an%2520inspiration%2520from%2520human%2520perception%2520and%2520explore%2520a%250Acompositional%2520approach%2520for%2520egocentric%2520video%2520representation.%2520We%2520introduce%2520HENASY%250A%2528Hierarchical%2520ENtities%2520ASsemblY%2529%252C%2520which%2520includes%2520a%2520spatiotemporal%2520token%250Agrouping%2520mechanism%2520to%2520explicitly%2520assemble%2520dynamically%2520evolving%2520scene%2520entities%250Athrough%2520time%2520and%2520model%2520their%2520relationship%2520for%2520video%2520representation.%2520By%250Aleveraging%2520compositional%2520structure%2520understanding%252C%2520HENASY%2520possesses%2520strong%250Ainterpretability%2520via%2520visual%2520grounding%2520with%2520free-form%2520text%2520queries.%2520We%2520further%250Aexplore%2520a%2520suite%2520of%2520multi-grained%2520contrastive%2520losses%2520to%2520facilitate%250Aentity-centric%2520understandings.%2520This%2520comprises%2520three%2520alignment%2520types%253A%250Avideo-narration%252C%2520noun-entity%252C%2520verb-entities%2520alignments.%250A%2520%2520Our%2520method%2520demonstrates%2520strong%2520interpretability%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520experiments%253B%2520while%2520maintaining%2520competitive%2520performances%2520on%2520five%250Adownstream%2520tasks%2520via%2520zero-shot%2520transfer%2520or%2520as%2520video/text%2520representation%252C%250Aincluding%2520video/text%2520retrieval%252C%2520action%2520recognition%252C%2520multi-choice%2520query%252C%2520natural%250Alanguage%2520query%252C%2520and%2520moments%2520query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00307v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HENASY%3A%20Learning%20to%20Assemble%20Scene-Entities%20for%20Egocentric%0A%20%20Video-Language%20Model&entry.906535625=Khoa%20Vo%20and%20Thinh%20Phan%20and%20Kashu%20Yamazaki%20and%20Minh%20Tran%20and%20Ngan%20Le&entry.1292438233=%20%20Current%20video-language%20models%20%28VLMs%29%20rely%20extensively%20on%20instance-level%0Aalignment%20between%20video%20and%20language%20modalities%2C%20which%20presents%20two%20major%0Alimitations%3A%20%281%29%20visual%20reasoning%20disobeys%20the%20natural%20perception%20that%20humans%0Ado%20in%20first-person%20perspective%2C%20leading%20to%20a%20lack%20of%20reasoning%20interpretation%3B%0Aand%20%282%29%20learning%20is%20limited%20in%20capturing%20inherent%20fine-grained%20relationships%0Abetween%20two%20modalities.%0A%20%20In%20this%20paper%2C%20we%20take%20an%20inspiration%20from%20human%20perception%20and%20explore%20a%0Acompositional%20approach%20for%20egocentric%20video%20representation.%20We%20introduce%20HENASY%0A%28Hierarchical%20ENtities%20ASsemblY%29%2C%20which%20includes%20a%20spatiotemporal%20token%0Agrouping%20mechanism%20to%20explicitly%20assemble%20dynamically%20evolving%20scene%20entities%0Athrough%20time%20and%20model%20their%20relationship%20for%20video%20representation.%20By%0Aleveraging%20compositional%20structure%20understanding%2C%20HENASY%20possesses%20strong%0Ainterpretability%20via%20visual%20grounding%20with%20free-form%20text%20queries.%20We%20further%0Aexplore%20a%20suite%20of%20multi-grained%20contrastive%20losses%20to%20facilitate%0Aentity-centric%20understandings.%20This%20comprises%20three%20alignment%20types%3A%0Avideo-narration%2C%20noun-entity%2C%20verb-entities%20alignments.%0A%20%20Our%20method%20demonstrates%20strong%20interpretability%20in%20both%20quantitative%20and%0Aqualitative%20experiments%3B%20while%20maintaining%20competitive%20performances%20on%20five%0Adownstream%20tasks%20via%20zero-shot%20transfer%20or%20as%20video/text%20representation%2C%0Aincluding%20video/text%20retrieval%2C%20action%20recognition%2C%20multi-choice%20query%2C%20natural%0Alanguage%20query%2C%20and%20moments%20query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00307v4&entry.124074799=Read"},
{"title": "Nova: A Practical and Advanced Alignment", "author": "Mingan Lin and Fan Yang and Yanjun Shen and Haoze Sun and Tianpeng Li and Tao Zhang and Chenzheng Zhu and Tao Zhang and Miao Zheng and Xu Li and Yijie Zhou and Mingyang Chen and Yanzhao Qin and Youquan Li and Hao Liang and Fei Li and Yadong Li and Mang Wang and Guosheng Dong and Kun Fang and Jianhua Xu and Bin Cui and Wentao Zhang and Zenan Zhou and Weipeng Chen", "abstract": "  We introduce Nova, a suite of practical alignment techniques employed in a\nseries of empirically validated high-performing models. This represents the\nfirst comprehensive account of alignment methodologies, offering valuable\ninsights for advancing AI research. We investigate the critical components that\nenhance model performance during the alignment process, including optimization\nmethods, data strategies, capability enhancements, and evaluation processes.\nThe process spans three key stages: Prompt Augmentation System(PAS), Supervised\nFine-Tuning(SFT), and Preference Alignment. The problems encountered, the\nsolutions applied, and the improvements made are thoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Nova Alignment. Importantly,\nQwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B\nand Llama-3-70B base models, optimized through Nova. The Nova models show\nsignificant core improvements, with user experience gains of 17% to 28%, and\nexcels on specialized benchmarks. In open-source benchmark evaluations, both\nQwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective\nofficial instruct versions across nearly all datasets. This report aims to\nclarify the key technologies behind the alignment process, fostering a deeper\nunderstanding within the community. Llama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.\n", "link": "http://arxiv.org/abs/2410.14940v3", "date": "2024-11-01", "relevancy": 2.4281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nova%3A%20A%20Practical%20and%20Advanced%20Alignment&body=Title%3A%20Nova%3A%20A%20Practical%20and%20Advanced%20Alignment%0AAuthor%3A%20Mingan%20Lin%20and%20Fan%20Yang%20and%20Yanjun%20Shen%20and%20Haoze%20Sun%20and%20Tianpeng%20Li%20and%20Tao%20Zhang%20and%20Chenzheng%20Zhu%20and%20Tao%20Zhang%20and%20Miao%20Zheng%20and%20Xu%20Li%20and%20Yijie%20Zhou%20and%20Mingyang%20Chen%20and%20Yanzhao%20Qin%20and%20Youquan%20Li%20and%20Hao%20Liang%20and%20Fei%20Li%20and%20Yadong%20Li%20and%20Mang%20Wang%20and%20Guosheng%20Dong%20and%20Kun%20Fang%20and%20Jianhua%20Xu%20and%20Bin%20Cui%20and%20Wentao%20Zhang%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen%0AAbstract%3A%20%20%20We%20introduce%20Nova%2C%20a%20suite%20of%20practical%20alignment%20techniques%20employed%20in%20a%0Aseries%20of%20empirically%20validated%20high-performing%20models.%20This%20represents%20the%0Afirst%20comprehensive%20account%20of%20alignment%20methodologies%2C%20offering%20valuable%0Ainsights%20for%20advancing%20AI%20research.%20We%20investigate%20the%20critical%20components%20that%0Aenhance%20model%20performance%20during%20the%20alignment%20process%2C%20including%20optimization%0Amethods%2C%20data%20strategies%2C%20capability%20enhancements%2C%20and%20evaluation%20processes.%0AThe%20process%20spans%20three%20key%20stages%3A%20Prompt%20Augmentation%20System%28PAS%29%2C%20Supervised%0AFine-Tuning%28SFT%29%2C%20and%20Preference%20Alignment.%20The%20problems%20encountered%2C%20the%0Asolutions%20applied%2C%20and%20the%20improvements%20made%20are%20thoroughly%20recorded.%0A%20%20Through%20comparisons%20across%20well-established%20benchmarks%2C%20we%20highlight%20the%0Atechnological%20advancements%20enabled%20by%20Nova%20Alignment.%20Importantly%2C%0AQwen2-Nova-72B%20and%20Llama3-PBM-Nova-70B%20are%20instruct%20versions%20of%20the%20Qwen2-72B%0Aand%20Llama-3-70B%20base%20models%2C%20optimized%20through%20Nova.%20The%20Nova%20models%20show%0Asignificant%20core%20improvements%2C%20with%20user%20experience%20gains%20of%2017%25%20to%2028%25%2C%20and%0Aexcels%20on%20specialized%20benchmarks.%20In%20open-source%20benchmark%20evaluations%2C%20both%0AQwen2-Nova-72B%20and%20Llama3-PBM-Nova-70B%20consistently%20outperform%20their%20respective%0Aofficial%20instruct%20versions%20across%20nearly%20all%20datasets.%20This%20report%20aims%20to%0Aclarify%20the%20key%20technologies%20behind%20the%20alignment%20process%2C%20fostering%20a%20deeper%0Aunderstanding%20within%20the%20community.%20Llama3-PBM-Nova-70B%20model%20is%20available%20at%0Ahttps%3A//huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNova%253A%2520A%2520Practical%2520and%2520Advanced%2520Alignment%26entry.906535625%3DMingan%2520Lin%2520and%2520Fan%2520Yang%2520and%2520Yanjun%2520Shen%2520and%2520Haoze%2520Sun%2520and%2520Tianpeng%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Chenzheng%2520Zhu%2520and%2520Tao%2520Zhang%2520and%2520Miao%2520Zheng%2520and%2520Xu%2520Li%2520and%2520Yijie%2520Zhou%2520and%2520Mingyang%2520Chen%2520and%2520Yanzhao%2520Qin%2520and%2520Youquan%2520Li%2520and%2520Hao%2520Liang%2520and%2520Fei%2520Li%2520and%2520Yadong%2520Li%2520and%2520Mang%2520Wang%2520and%2520Guosheng%2520Dong%2520and%2520Kun%2520Fang%2520and%2520Jianhua%2520Xu%2520and%2520Bin%2520Cui%2520and%2520Wentao%2520Zhang%2520and%2520Zenan%2520Zhou%2520and%2520Weipeng%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Nova%252C%2520a%2520suite%2520of%2520practical%2520alignment%2520techniques%2520employed%2520in%2520a%250Aseries%2520of%2520empirically%2520validated%2520high-performing%2520models.%2520This%2520represents%2520the%250Afirst%2520comprehensive%2520account%2520of%2520alignment%2520methodologies%252C%2520offering%2520valuable%250Ainsights%2520for%2520advancing%2520AI%2520research.%2520We%2520investigate%2520the%2520critical%2520components%2520that%250Aenhance%2520model%2520performance%2520during%2520the%2520alignment%2520process%252C%2520including%2520optimization%250Amethods%252C%2520data%2520strategies%252C%2520capability%2520enhancements%252C%2520and%2520evaluation%2520processes.%250AThe%2520process%2520spans%2520three%2520key%2520stages%253A%2520Prompt%2520Augmentation%2520System%2528PAS%2529%252C%2520Supervised%250AFine-Tuning%2528SFT%2529%252C%2520and%2520Preference%2520Alignment.%2520The%2520problems%2520encountered%252C%2520the%250Asolutions%2520applied%252C%2520and%2520the%2520improvements%2520made%2520are%2520thoroughly%2520recorded.%250A%2520%2520Through%2520comparisons%2520across%2520well-established%2520benchmarks%252C%2520we%2520highlight%2520the%250Atechnological%2520advancements%2520enabled%2520by%2520Nova%2520Alignment.%2520Importantly%252C%250AQwen2-Nova-72B%2520and%2520Llama3-PBM-Nova-70B%2520are%2520instruct%2520versions%2520of%2520the%2520Qwen2-72B%250Aand%2520Llama-3-70B%2520base%2520models%252C%2520optimized%2520through%2520Nova.%2520The%2520Nova%2520models%2520show%250Asignificant%2520core%2520improvements%252C%2520with%2520user%2520experience%2520gains%2520of%252017%2525%2520to%252028%2525%252C%2520and%250Aexcels%2520on%2520specialized%2520benchmarks.%2520In%2520open-source%2520benchmark%2520evaluations%252C%2520both%250AQwen2-Nova-72B%2520and%2520Llama3-PBM-Nova-70B%2520consistently%2520outperform%2520their%2520respective%250Aofficial%2520instruct%2520versions%2520across%2520nearly%2520all%2520datasets.%2520This%2520report%2520aims%2520to%250Aclarify%2520the%2520key%2520technologies%2520behind%2520the%2520alignment%2520process%252C%2520fostering%2520a%2520deeper%250Aunderstanding%2520within%2520the%2520community.%2520Llama3-PBM-Nova-70B%2520model%2520is%2520available%2520at%250Ahttps%253A//huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nova%3A%20A%20Practical%20and%20Advanced%20Alignment&entry.906535625=Mingan%20Lin%20and%20Fan%20Yang%20and%20Yanjun%20Shen%20and%20Haoze%20Sun%20and%20Tianpeng%20Li%20and%20Tao%20Zhang%20and%20Chenzheng%20Zhu%20and%20Tao%20Zhang%20and%20Miao%20Zheng%20and%20Xu%20Li%20and%20Yijie%20Zhou%20and%20Mingyang%20Chen%20and%20Yanzhao%20Qin%20and%20Youquan%20Li%20and%20Hao%20Liang%20and%20Fei%20Li%20and%20Yadong%20Li%20and%20Mang%20Wang%20and%20Guosheng%20Dong%20and%20Kun%20Fang%20and%20Jianhua%20Xu%20and%20Bin%20Cui%20and%20Wentao%20Zhang%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen&entry.1292438233=%20%20We%20introduce%20Nova%2C%20a%20suite%20of%20practical%20alignment%20techniques%20employed%20in%20a%0Aseries%20of%20empirically%20validated%20high-performing%20models.%20This%20represents%20the%0Afirst%20comprehensive%20account%20of%20alignment%20methodologies%2C%20offering%20valuable%0Ainsights%20for%20advancing%20AI%20research.%20We%20investigate%20the%20critical%20components%20that%0Aenhance%20model%20performance%20during%20the%20alignment%20process%2C%20including%20optimization%0Amethods%2C%20data%20strategies%2C%20capability%20enhancements%2C%20and%20evaluation%20processes.%0AThe%20process%20spans%20three%20key%20stages%3A%20Prompt%20Augmentation%20System%28PAS%29%2C%20Supervised%0AFine-Tuning%28SFT%29%2C%20and%20Preference%20Alignment.%20The%20problems%20encountered%2C%20the%0Asolutions%20applied%2C%20and%20the%20improvements%20made%20are%20thoroughly%20recorded.%0A%20%20Through%20comparisons%20across%20well-established%20benchmarks%2C%20we%20highlight%20the%0Atechnological%20advancements%20enabled%20by%20Nova%20Alignment.%20Importantly%2C%0AQwen2-Nova-72B%20and%20Llama3-PBM-Nova-70B%20are%20instruct%20versions%20of%20the%20Qwen2-72B%0Aand%20Llama-3-70B%20base%20models%2C%20optimized%20through%20Nova.%20The%20Nova%20models%20show%0Asignificant%20core%20improvements%2C%20with%20user%20experience%20gains%20of%2017%25%20to%2028%25%2C%20and%0Aexcels%20on%20specialized%20benchmarks.%20In%20open-source%20benchmark%20evaluations%2C%20both%0AQwen2-Nova-72B%20and%20Llama3-PBM-Nova-70B%20consistently%20outperform%20their%20respective%0Aofficial%20instruct%20versions%20across%20nearly%20all%20datasets.%20This%20report%20aims%20to%0Aclarify%20the%20key%20technologies%20behind%20the%20alignment%20process%2C%20fostering%20a%20deeper%0Aunderstanding%20within%20the%20community.%20Llama3-PBM-Nova-70B%20model%20is%20available%20at%0Ahttps%3A//huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14940v3&entry.124074799=Read"},
{"title": "Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking", "author": "Shengsheng Qian and Dizhan Xue and Yifei Wang and Shengjie Zhang and Huaiwen Zhang and Changsheng Xu", "abstract": "  Self-Supervised Learning (SSL) is an effective paradigm for learning\nrepresentations from unlabeled data, such as text, images, and videos. However,\nresearchers have recently found that SSL is vulnerable to backdoor attacks. The\nattacker can embed hidden SSL backdoors via a few poisoned examples in the\ntraining dataset and maliciously manipulate the behavior of downstream models.\nTo defend against SSL backdoor attacks, a feasible route is to detect and\nremove the poisonous samples in the training set. However, the existing SSL\nbackdoor defense method fails to detect the poisonous samples precisely. In\nthis paper, we propose to erase the SSL backdoor by cluster activation masking\nand propose a novel PoisonCAM method. After obtaining the threat model trained\non the poisoned dataset, our method can precisely detect poisonous samples\nbased on the assumption that masking the backdoor trigger can effectively\nchange the activation of a downstream clustering model. In experiments, our\nPoisonCAM achieves 96\\% accuracy for backdoor trigger detection compared to 3\\%\nof the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed\nPoisonCAM significantly improves the performance of the trained SSL model under\nbackdoor attacks compared to the state-of-the-art method. Our code, data, and\ntrained models will be open once this paper is accepted.\n", "link": "http://arxiv.org/abs/2312.07955v2", "date": "2024-11-01", "relevancy": 2.4069, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4867}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4792}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Erasing%20Self-Supervised%20Learning%20Backdoor%20by%20Cluster%20Activation%20Masking&body=Title%3A%20Erasing%20Self-Supervised%20Learning%20Backdoor%20by%20Cluster%20Activation%20Masking%0AAuthor%3A%20Shengsheng%20Qian%20and%20Dizhan%20Xue%20and%20Yifei%20Wang%20and%20Shengjie%20Zhang%20and%20Huaiwen%20Zhang%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20is%20an%20effective%20paradigm%20for%20learning%0Arepresentations%20from%20unlabeled%20data%2C%20such%20as%20text%2C%20images%2C%20and%20videos.%20However%2C%0Aresearchers%20have%20recently%20found%20that%20SSL%20is%20vulnerable%20to%20backdoor%20attacks.%20The%0Aattacker%20can%20embed%20hidden%20SSL%20backdoors%20via%20a%20few%20poisoned%20examples%20in%20the%0Atraining%20dataset%20and%20maliciously%20manipulate%20the%20behavior%20of%20downstream%20models.%0ATo%20defend%20against%20SSL%20backdoor%20attacks%2C%20a%20feasible%20route%20is%20to%20detect%20and%0Aremove%20the%20poisonous%20samples%20in%20the%20training%20set.%20However%2C%20the%20existing%20SSL%0Abackdoor%20defense%20method%20fails%20to%20detect%20the%20poisonous%20samples%20precisely.%20In%0Athis%20paper%2C%20we%20propose%20to%20erase%20the%20SSL%20backdoor%20by%20cluster%20activation%20masking%0Aand%20propose%20a%20novel%20PoisonCAM%20method.%20After%20obtaining%20the%20threat%20model%20trained%0Aon%20the%20poisoned%20dataset%2C%20our%20method%20can%20precisely%20detect%20poisonous%20samples%0Abased%20on%20the%20assumption%20that%20masking%20the%20backdoor%20trigger%20can%20effectively%0Achange%20the%20activation%20of%20a%20downstream%20clustering%20model.%20In%20experiments%2C%20our%0APoisonCAM%20achieves%2096%5C%25%20accuracy%20for%20backdoor%20trigger%20detection%20compared%20to%203%5C%25%0Aof%20the%20state-of-the-art%20method%20on%20poisoned%20ImageNet-100.%20Moreover%2C%20our%20proposed%0APoisonCAM%20significantly%20improves%20the%20performance%20of%20the%20trained%20SSL%20model%20under%0Abackdoor%20attacks%20compared%20to%20the%20state-of-the-art%20method.%20Our%20code%2C%20data%2C%20and%0Atrained%20models%20will%20be%20open%20once%20this%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErasing%2520Self-Supervised%2520Learning%2520Backdoor%2520by%2520Cluster%2520Activation%2520Masking%26entry.906535625%3DShengsheng%2520Qian%2520and%2520Dizhan%2520Xue%2520and%2520Yifei%2520Wang%2520and%2520Shengjie%2520Zhang%2520and%2520Huaiwen%2520Zhang%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520is%2520an%2520effective%2520paradigm%2520for%2520learning%250Arepresentations%2520from%2520unlabeled%2520data%252C%2520such%2520as%2520text%252C%2520images%252C%2520and%2520videos.%2520However%252C%250Aresearchers%2520have%2520recently%2520found%2520that%2520SSL%2520is%2520vulnerable%2520to%2520backdoor%2520attacks.%2520The%250Aattacker%2520can%2520embed%2520hidden%2520SSL%2520backdoors%2520via%2520a%2520few%2520poisoned%2520examples%2520in%2520the%250Atraining%2520dataset%2520and%2520maliciously%2520manipulate%2520the%2520behavior%2520of%2520downstream%2520models.%250ATo%2520defend%2520against%2520SSL%2520backdoor%2520attacks%252C%2520a%2520feasible%2520route%2520is%2520to%2520detect%2520and%250Aremove%2520the%2520poisonous%2520samples%2520in%2520the%2520training%2520set.%2520However%252C%2520the%2520existing%2520SSL%250Abackdoor%2520defense%2520method%2520fails%2520to%2520detect%2520the%2520poisonous%2520samples%2520precisely.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520to%2520erase%2520the%2520SSL%2520backdoor%2520by%2520cluster%2520activation%2520masking%250Aand%2520propose%2520a%2520novel%2520PoisonCAM%2520method.%2520After%2520obtaining%2520the%2520threat%2520model%2520trained%250Aon%2520the%2520poisoned%2520dataset%252C%2520our%2520method%2520can%2520precisely%2520detect%2520poisonous%2520samples%250Abased%2520on%2520the%2520assumption%2520that%2520masking%2520the%2520backdoor%2520trigger%2520can%2520effectively%250Achange%2520the%2520activation%2520of%2520a%2520downstream%2520clustering%2520model.%2520In%2520experiments%252C%2520our%250APoisonCAM%2520achieves%252096%255C%2525%2520accuracy%2520for%2520backdoor%2520trigger%2520detection%2520compared%2520to%25203%255C%2525%250Aof%2520the%2520state-of-the-art%2520method%2520on%2520poisoned%2520ImageNet-100.%2520Moreover%252C%2520our%2520proposed%250APoisonCAM%2520significantly%2520improves%2520the%2520performance%2520of%2520the%2520trained%2520SSL%2520model%2520under%250Abackdoor%2520attacks%2520compared%2520to%2520the%2520state-of-the-art%2520method.%2520Our%2520code%252C%2520data%252C%2520and%250Atrained%2520models%2520will%2520be%2520open%2520once%2520this%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Erasing%20Self-Supervised%20Learning%20Backdoor%20by%20Cluster%20Activation%20Masking&entry.906535625=Shengsheng%20Qian%20and%20Dizhan%20Xue%20and%20Yifei%20Wang%20and%20Shengjie%20Zhang%20and%20Huaiwen%20Zhang%20and%20Changsheng%20Xu&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20is%20an%20effective%20paradigm%20for%20learning%0Arepresentations%20from%20unlabeled%20data%2C%20such%20as%20text%2C%20images%2C%20and%20videos.%20However%2C%0Aresearchers%20have%20recently%20found%20that%20SSL%20is%20vulnerable%20to%20backdoor%20attacks.%20The%0Aattacker%20can%20embed%20hidden%20SSL%20backdoors%20via%20a%20few%20poisoned%20examples%20in%20the%0Atraining%20dataset%20and%20maliciously%20manipulate%20the%20behavior%20of%20downstream%20models.%0ATo%20defend%20against%20SSL%20backdoor%20attacks%2C%20a%20feasible%20route%20is%20to%20detect%20and%0Aremove%20the%20poisonous%20samples%20in%20the%20training%20set.%20However%2C%20the%20existing%20SSL%0Abackdoor%20defense%20method%20fails%20to%20detect%20the%20poisonous%20samples%20precisely.%20In%0Athis%20paper%2C%20we%20propose%20to%20erase%20the%20SSL%20backdoor%20by%20cluster%20activation%20masking%0Aand%20propose%20a%20novel%20PoisonCAM%20method.%20After%20obtaining%20the%20threat%20model%20trained%0Aon%20the%20poisoned%20dataset%2C%20our%20method%20can%20precisely%20detect%20poisonous%20samples%0Abased%20on%20the%20assumption%20that%20masking%20the%20backdoor%20trigger%20can%20effectively%0Achange%20the%20activation%20of%20a%20downstream%20clustering%20model.%20In%20experiments%2C%20our%0APoisonCAM%20achieves%2096%5C%25%20accuracy%20for%20backdoor%20trigger%20detection%20compared%20to%203%5C%25%0Aof%20the%20state-of-the-art%20method%20on%20poisoned%20ImageNet-100.%20Moreover%2C%20our%20proposed%0APoisonCAM%20significantly%20improves%20the%20performance%20of%20the%20trained%20SSL%20model%20under%0Abackdoor%20attacks%20compared%20to%20the%20state-of-the-art%20method.%20Our%20code%2C%20data%2C%20and%0Atrained%20models%20will%20be%20open%20once%20this%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07955v2&entry.124074799=Read"},
{"title": "Digital Twins in Additive Manufacturing: A Systematic Review", "author": "Md Manjurul Ahsan and Yingtao Liu and Shivakumar Raman and Zahed Siddique", "abstract": "  Digital Twins (DTs) are becoming popular in Additive Manufacturing (AM) due\nto their ability to create virtual replicas of physical components of AM\nmachines, which helps in real-time production monitoring. Advanced techniques\nsuch as Machine Learning (ML), Augmented Reality (AR), and simulation-based\nmodels play key roles in developing intelligent and adaptable DTs in\nmanufacturing processes. However, questions remain regarding scalability, the\nintegration of high-quality data, and the computational power required for\nreal-time applications in developing DTs. Understanding the current state of\nDTs in AM is essential to address these challenges and fully utilize their\npotential in advancing AM processes. Considering this opportunity, this work\naims to provide a comprehensive overview of DTs in AM by addressing the\nfollowing four research questions: (1) What are the key types of DTs used in AM\nand their specific applications? (2) What are the recent developments and\nimplementations of DTs? (3) How are DTs employed in process improvement and\nhybrid manufacturing? (4) How are DTs integrated with Industry 4.0\ntechnologies? By discussing current applications and techniques, we aim to\noffer a better understanding and potential future research directions for\nresearchers and practitioners in AM and DTs.\n", "link": "http://arxiv.org/abs/2409.00877v2", "date": "2024-11-01", "relevancy": 2.3934, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4914}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4914}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twins%20in%20Additive%20Manufacturing%3A%20A%20Systematic%20Review&body=Title%3A%20Digital%20Twins%20in%20Additive%20Manufacturing%3A%20A%20Systematic%20Review%0AAuthor%3A%20Md%20Manjurul%20Ahsan%20and%20Yingtao%20Liu%20and%20Shivakumar%20Raman%20and%20Zahed%20Siddique%0AAbstract%3A%20%20%20Digital%20Twins%20%28DTs%29%20are%20becoming%20popular%20in%20Additive%20Manufacturing%20%28AM%29%20due%0Ato%20their%20ability%20to%20create%20virtual%20replicas%20of%20physical%20components%20of%20AM%0Amachines%2C%20which%20helps%20in%20real-time%20production%20monitoring.%20Advanced%20techniques%0Asuch%20as%20Machine%20Learning%20%28ML%29%2C%20Augmented%20Reality%20%28AR%29%2C%20and%20simulation-based%0Amodels%20play%20key%20roles%20in%20developing%20intelligent%20and%20adaptable%20DTs%20in%0Amanufacturing%20processes.%20However%2C%20questions%20remain%20regarding%20scalability%2C%20the%0Aintegration%20of%20high-quality%20data%2C%20and%20the%20computational%20power%20required%20for%0Areal-time%20applications%20in%20developing%20DTs.%20Understanding%20the%20current%20state%20of%0ADTs%20in%20AM%20is%20essential%20to%20address%20these%20challenges%20and%20fully%20utilize%20their%0Apotential%20in%20advancing%20AM%20processes.%20Considering%20this%20opportunity%2C%20this%20work%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20DTs%20in%20AM%20by%20addressing%20the%0Afollowing%20four%20research%20questions%3A%20%281%29%20What%20are%20the%20key%20types%20of%20DTs%20used%20in%20AM%0Aand%20their%20specific%20applications%3F%20%282%29%20What%20are%20the%20recent%20developments%20and%0Aimplementations%20of%20DTs%3F%20%283%29%20How%20are%20DTs%20employed%20in%20process%20improvement%20and%0Ahybrid%20manufacturing%3F%20%284%29%20How%20are%20DTs%20integrated%20with%20Industry%204.0%0Atechnologies%3F%20By%20discussing%20current%20applications%20and%20techniques%2C%20we%20aim%20to%0Aoffer%20a%20better%20understanding%20and%20potential%20future%20research%20directions%20for%0Aresearchers%20and%20practitioners%20in%20AM%20and%20DTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twins%2520in%2520Additive%2520Manufacturing%253A%2520A%2520Systematic%2520Review%26entry.906535625%3DMd%2520Manjurul%2520Ahsan%2520and%2520Yingtao%2520Liu%2520and%2520Shivakumar%2520Raman%2520and%2520Zahed%2520Siddique%26entry.1292438233%3D%2520%2520Digital%2520Twins%2520%2528DTs%2529%2520are%2520becoming%2520popular%2520in%2520Additive%2520Manufacturing%2520%2528AM%2529%2520due%250Ato%2520their%2520ability%2520to%2520create%2520virtual%2520replicas%2520of%2520physical%2520components%2520of%2520AM%250Amachines%252C%2520which%2520helps%2520in%2520real-time%2520production%2520monitoring.%2520Advanced%2520techniques%250Asuch%2520as%2520Machine%2520Learning%2520%2528ML%2529%252C%2520Augmented%2520Reality%2520%2528AR%2529%252C%2520and%2520simulation-based%250Amodels%2520play%2520key%2520roles%2520in%2520developing%2520intelligent%2520and%2520adaptable%2520DTs%2520in%250Amanufacturing%2520processes.%2520However%252C%2520questions%2520remain%2520regarding%2520scalability%252C%2520the%250Aintegration%2520of%2520high-quality%2520data%252C%2520and%2520the%2520computational%2520power%2520required%2520for%250Areal-time%2520applications%2520in%2520developing%2520DTs.%2520Understanding%2520the%2520current%2520state%2520of%250ADTs%2520in%2520AM%2520is%2520essential%2520to%2520address%2520these%2520challenges%2520and%2520fully%2520utilize%2520their%250Apotential%2520in%2520advancing%2520AM%2520processes.%2520Considering%2520this%2520opportunity%252C%2520this%2520work%250Aaims%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520DTs%2520in%2520AM%2520by%2520addressing%2520the%250Afollowing%2520four%2520research%2520questions%253A%2520%25281%2529%2520What%2520are%2520the%2520key%2520types%2520of%2520DTs%2520used%2520in%2520AM%250Aand%2520their%2520specific%2520applications%253F%2520%25282%2529%2520What%2520are%2520the%2520recent%2520developments%2520and%250Aimplementations%2520of%2520DTs%253F%2520%25283%2529%2520How%2520are%2520DTs%2520employed%2520in%2520process%2520improvement%2520and%250Ahybrid%2520manufacturing%253F%2520%25284%2529%2520How%2520are%2520DTs%2520integrated%2520with%2520Industry%25204.0%250Atechnologies%253F%2520By%2520discussing%2520current%2520applications%2520and%2520techniques%252C%2520we%2520aim%2520to%250Aoffer%2520a%2520better%2520understanding%2520and%2520potential%2520future%2520research%2520directions%2520for%250Aresearchers%2520and%2520practitioners%2520in%2520AM%2520and%2520DTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twins%20in%20Additive%20Manufacturing%3A%20A%20Systematic%20Review&entry.906535625=Md%20Manjurul%20Ahsan%20and%20Yingtao%20Liu%20and%20Shivakumar%20Raman%20and%20Zahed%20Siddique&entry.1292438233=%20%20Digital%20Twins%20%28DTs%29%20are%20becoming%20popular%20in%20Additive%20Manufacturing%20%28AM%29%20due%0Ato%20their%20ability%20to%20create%20virtual%20replicas%20of%20physical%20components%20of%20AM%0Amachines%2C%20which%20helps%20in%20real-time%20production%20monitoring.%20Advanced%20techniques%0Asuch%20as%20Machine%20Learning%20%28ML%29%2C%20Augmented%20Reality%20%28AR%29%2C%20and%20simulation-based%0Amodels%20play%20key%20roles%20in%20developing%20intelligent%20and%20adaptable%20DTs%20in%0Amanufacturing%20processes.%20However%2C%20questions%20remain%20regarding%20scalability%2C%20the%0Aintegration%20of%20high-quality%20data%2C%20and%20the%20computational%20power%20required%20for%0Areal-time%20applications%20in%20developing%20DTs.%20Understanding%20the%20current%20state%20of%0ADTs%20in%20AM%20is%20essential%20to%20address%20these%20challenges%20and%20fully%20utilize%20their%0Apotential%20in%20advancing%20AM%20processes.%20Considering%20this%20opportunity%2C%20this%20work%0Aaims%20to%20provide%20a%20comprehensive%20overview%20of%20DTs%20in%20AM%20by%20addressing%20the%0Afollowing%20four%20research%20questions%3A%20%281%29%20What%20are%20the%20key%20types%20of%20DTs%20used%20in%20AM%0Aand%20their%20specific%20applications%3F%20%282%29%20What%20are%20the%20recent%20developments%20and%0Aimplementations%20of%20DTs%3F%20%283%29%20How%20are%20DTs%20employed%20in%20process%20improvement%20and%0Ahybrid%20manufacturing%3F%20%284%29%20How%20are%20DTs%20integrated%20with%20Industry%204.0%0Atechnologies%3F%20By%20discussing%20current%20applications%20and%20techniques%2C%20we%20aim%20to%0Aoffer%20a%20better%20understanding%20and%20potential%20future%20research%20directions%20for%0Aresearchers%20and%20practitioners%20in%20AM%20and%20DTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00877v2&entry.124074799=Read"},
{"title": "SelfCodeAlign: Self-Alignment for Code Generation", "author": "Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro von Werra and Arjun Guha and Lingming Zhang", "abstract": "  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n", "link": "http://arxiv.org/abs/2410.24198v2", "date": "2024-11-01", "relevancy": 2.3805, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4813}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation&body=Title%3A%20SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation%0AAuthor%3A%20Yuxiang%20Wei%20and%20Federico%20Cassano%20and%20Jiawei%20Liu%20and%20Yifeng%20Ding%20and%20Naman%20Jain%20and%20Zachary%20Mueller%20and%20Harm%20de%20Vries%20and%20Leandro%20von%20Werra%20and%20Arjun%20Guha%20and%20Lingming%20Zhang%0AAbstract%3A%20%20%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20approach%20that%20significantly%0Aimproves%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions.%20We%20propose%20SelfCodeAlign%2C%20the%20first%20fully%20transparent%20and%0Apermissive%20pipeline%20for%20self-aligning%20code%20LLMs%20without%20extensive%20human%0Aannotations%20or%20distillation.%20SelfCodeAlign%20employs%20the%20same%20base%20model%20for%0Ainference%20throughout%20the%20data%20generation%20process.%20It%20first%20extracts%20diverse%0Acoding%20concepts%20from%20high-quality%20seed%20snippets%20to%20generate%20new%20tasks.%20It%20then%0Asamples%20multiple%20responses%20per%20task%2C%20pairs%20each%20with%20test%20cases%2C%20and%20validates%0Athem%20in%20a%20sandbox%20environment.%20Finally%2C%20passing%20examples%20are%20selected%20for%0Ainstruction%20tuning.%20In%20our%20primary%20experiments%2C%20we%20use%20SelfCodeAlign%20with%0ACodeQwen1.5-7B%20to%20generate%20a%20dataset%20of%2074k%20instruction-response%20pairs.%0AFinetuning%20on%20this%20dataset%20leads%20to%20a%20model%20that%20achieves%20a%2067.1%20pass%401%20on%0AHumanEval%2B%2C%20surpassing%20CodeLlama-70B-Instruct%20despite%20being%20ten%20times%20smaller.%0AAcross%20all%20benchmarks%2C%20this%20finetuned%20model%20consistently%20outperforms%20the%0Aoriginal%20version%20trained%20with%20OctoPack%2C%20the%20previous%20state-of-the-art%20method%0Afor%20instruction%20tuning%20without%20human%20annotations%20or%20distillation.%20Additionally%2C%0Awe%20show%20that%20SelfCodeAlign%20is%20effective%20across%20LLMs%20of%20various%20sizes%2C%20from%203B%0Ato%2033B%2C%20and%20that%20the%20base%20models%20can%20benefit%20more%20from%20alignment%20with%20their%20own%0Adata%20distribution.%20We%20further%20validate%20each%20component%27s%20effectiveness%20in%20our%0Apipeline%2C%20showing%20that%20SelfCodeAlign%20outperforms%20both%20direct%20distillation%20from%0AGPT-4o%20and%20leading%20GPT-3.5-based%20distillation%20methods%2C%20such%20as%20OSS-Instruct%20and%0AEvol-Instruct.%20SelfCodeAlign%20has%20also%20led%20to%20the%20creation%20of%0AStarCoder2-Instruct%2C%20the%20first%20fully%20transparent%2C%20permissively%20licensed%2C%20and%0Aself-aligned%20code%20LLM%20that%20achieves%20state-of-the-art%20coding%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfCodeAlign%253A%2520Self-Alignment%2520for%2520Code%2520Generation%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Federico%2520Cassano%2520and%2520Jiawei%2520Liu%2520and%2520Yifeng%2520Ding%2520and%2520Naman%2520Jain%2520and%2520Zachary%2520Mueller%2520and%2520Harm%2520de%2520Vries%2520and%2520Leandro%2520von%2520Werra%2520and%2520Arjun%2520Guha%2520and%2520Lingming%2520Zhang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520is%2520a%2520supervised%2520fine-tuning%2520approach%2520that%2520significantly%250Aimproves%2520the%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520follow%2520human%250Ainstructions.%2520We%2520propose%2520SelfCodeAlign%252C%2520the%2520first%2520fully%2520transparent%2520and%250Apermissive%2520pipeline%2520for%2520self-aligning%2520code%2520LLMs%2520without%2520extensive%2520human%250Aannotations%2520or%2520distillation.%2520SelfCodeAlign%2520employs%2520the%2520same%2520base%2520model%2520for%250Ainference%2520throughout%2520the%2520data%2520generation%2520process.%2520It%2520first%2520extracts%2520diverse%250Acoding%2520concepts%2520from%2520high-quality%2520seed%2520snippets%2520to%2520generate%2520new%2520tasks.%2520It%2520then%250Asamples%2520multiple%2520responses%2520per%2520task%252C%2520pairs%2520each%2520with%2520test%2520cases%252C%2520and%2520validates%250Athem%2520in%2520a%2520sandbox%2520environment.%2520Finally%252C%2520passing%2520examples%2520are%2520selected%2520for%250Ainstruction%2520tuning.%2520In%2520our%2520primary%2520experiments%252C%2520we%2520use%2520SelfCodeAlign%2520with%250ACodeQwen1.5-7B%2520to%2520generate%2520a%2520dataset%2520of%252074k%2520instruction-response%2520pairs.%250AFinetuning%2520on%2520this%2520dataset%2520leads%2520to%2520a%2520model%2520that%2520achieves%2520a%252067.1%2520pass%25401%2520on%250AHumanEval%252B%252C%2520surpassing%2520CodeLlama-70B-Instruct%2520despite%2520being%2520ten%2520times%2520smaller.%250AAcross%2520all%2520benchmarks%252C%2520this%2520finetuned%2520model%2520consistently%2520outperforms%2520the%250Aoriginal%2520version%2520trained%2520with%2520OctoPack%252C%2520the%2520previous%2520state-of-the-art%2520method%250Afor%2520instruction%2520tuning%2520without%2520human%2520annotations%2520or%2520distillation.%2520Additionally%252C%250Awe%2520show%2520that%2520SelfCodeAlign%2520is%2520effective%2520across%2520LLMs%2520of%2520various%2520sizes%252C%2520from%25203B%250Ato%252033B%252C%2520and%2520that%2520the%2520base%2520models%2520can%2520benefit%2520more%2520from%2520alignment%2520with%2520their%2520own%250Adata%2520distribution.%2520We%2520further%2520validate%2520each%2520component%2527s%2520effectiveness%2520in%2520our%250Apipeline%252C%2520showing%2520that%2520SelfCodeAlign%2520outperforms%2520both%2520direct%2520distillation%2520from%250AGPT-4o%2520and%2520leading%2520GPT-3.5-based%2520distillation%2520methods%252C%2520such%2520as%2520OSS-Instruct%2520and%250AEvol-Instruct.%2520SelfCodeAlign%2520has%2520also%2520led%2520to%2520the%2520creation%2520of%250AStarCoder2-Instruct%252C%2520the%2520first%2520fully%2520transparent%252C%2520permissively%2520licensed%252C%2520and%250Aself-aligned%2520code%2520LLM%2520that%2520achieves%2520state-of-the-art%2520coding%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation&entry.906535625=Yuxiang%20Wei%20and%20Federico%20Cassano%20and%20Jiawei%20Liu%20and%20Yifeng%20Ding%20and%20Naman%20Jain%20and%20Zachary%20Mueller%20and%20Harm%20de%20Vries%20and%20Leandro%20von%20Werra%20and%20Arjun%20Guha%20and%20Lingming%20Zhang&entry.1292438233=%20%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20approach%20that%20significantly%0Aimproves%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions.%20We%20propose%20SelfCodeAlign%2C%20the%20first%20fully%20transparent%20and%0Apermissive%20pipeline%20for%20self-aligning%20code%20LLMs%20without%20extensive%20human%0Aannotations%20or%20distillation.%20SelfCodeAlign%20employs%20the%20same%20base%20model%20for%0Ainference%20throughout%20the%20data%20generation%20process.%20It%20first%20extracts%20diverse%0Acoding%20concepts%20from%20high-quality%20seed%20snippets%20to%20generate%20new%20tasks.%20It%20then%0Asamples%20multiple%20responses%20per%20task%2C%20pairs%20each%20with%20test%20cases%2C%20and%20validates%0Athem%20in%20a%20sandbox%20environment.%20Finally%2C%20passing%20examples%20are%20selected%20for%0Ainstruction%20tuning.%20In%20our%20primary%20experiments%2C%20we%20use%20SelfCodeAlign%20with%0ACodeQwen1.5-7B%20to%20generate%20a%20dataset%20of%2074k%20instruction-response%20pairs.%0AFinetuning%20on%20this%20dataset%20leads%20to%20a%20model%20that%20achieves%20a%2067.1%20pass%401%20on%0AHumanEval%2B%2C%20surpassing%20CodeLlama-70B-Instruct%20despite%20being%20ten%20times%20smaller.%0AAcross%20all%20benchmarks%2C%20this%20finetuned%20model%20consistently%20outperforms%20the%0Aoriginal%20version%20trained%20with%20OctoPack%2C%20the%20previous%20state-of-the-art%20method%0Afor%20instruction%20tuning%20without%20human%20annotations%20or%20distillation.%20Additionally%2C%0Awe%20show%20that%20SelfCodeAlign%20is%20effective%20across%20LLMs%20of%20various%20sizes%2C%20from%203B%0Ato%2033B%2C%20and%20that%20the%20base%20models%20can%20benefit%20more%20from%20alignment%20with%20their%20own%0Adata%20distribution.%20We%20further%20validate%20each%20component%27s%20effectiveness%20in%20our%0Apipeline%2C%20showing%20that%20SelfCodeAlign%20outperforms%20both%20direct%20distillation%20from%0AGPT-4o%20and%20leading%20GPT-3.5-based%20distillation%20methods%2C%20such%20as%20OSS-Instruct%20and%0AEvol-Instruct.%20SelfCodeAlign%20has%20also%20led%20to%20the%20creation%20of%0AStarCoder2-Instruct%2C%20the%20first%20fully%20transparent%2C%20permissively%20licensed%2C%20and%0Aself-aligned%20code%20LLM%20that%20achieves%20state-of-the-art%20coding%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24198v2&entry.124074799=Read"},
{"title": "DELTA: Dense Efficient Long-range 3D Tracking for any video", "author": "Tuan Duc Ngo and Peiye Zhuang and Chuang Gan and Evangelos Kalogerakis and Sergey Tulyakov and Hsin-Ying Lee and Chaoyang Wang", "abstract": "  Tracking dense 3D motion from monocular videos remains challenging,\nparticularly when aiming for pixel-level precision over long sequences. We\nintroduce DELTA, a novel method that efficiently tracks every pixel in 3D\nspace, enabling accurate motion estimation across entire videos. Our approach\nleverages a joint global-local attention mechanism for reduced-resolution\ntracking, followed by a transformer-based upsampler to achieve high-resolution\npredictions. Unlike existing methods, which are limited by computational\ninefficiency or sparse tracking, DELTA delivers dense 3D tracking at scale,\nrunning over 8x faster than previous methods while achieving state-of-the-art\naccuracy. Furthermore, we explore the impact of depth representation on\ntracking performance and identify log-depth as the optimal choice. Extensive\nexperiments demonstrate the superiority of DELTA on multiple benchmarks,\nachieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\nOur method provides a robust solution for applications requiring fine-grained,\nlong-term motion tracking in 3D space.\n", "link": "http://arxiv.org/abs/2410.24211v2", "date": "2024-11-01", "relevancy": 2.3537, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6013}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5814}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video&body=Title%3A%20DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video%0AAuthor%3A%20Tuan%20Duc%20Ngo%20and%20Peiye%20Zhuang%20and%20Chuang%20Gan%20and%20Evangelos%20Kalogerakis%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%0AAbstract%3A%20%20%20Tracking%20dense%203D%20motion%20from%20monocular%20videos%20remains%20challenging%2C%0Aparticularly%20when%20aiming%20for%20pixel-level%20precision%20over%20long%20sequences.%20We%0Aintroduce%20DELTA%2C%20a%20novel%20method%20that%20efficiently%20tracks%20every%20pixel%20in%203D%0Aspace%2C%20enabling%20accurate%20motion%20estimation%20across%20entire%20videos.%20Our%20approach%0Aleverages%20a%20joint%20global-local%20attention%20mechanism%20for%20reduced-resolution%0Atracking%2C%20followed%20by%20a%20transformer-based%20upsampler%20to%20achieve%20high-resolution%0Apredictions.%20Unlike%20existing%20methods%2C%20which%20are%20limited%20by%20computational%0Ainefficiency%20or%20sparse%20tracking%2C%20DELTA%20delivers%20dense%203D%20tracking%20at%20scale%2C%0Arunning%20over%208x%20faster%20than%20previous%20methods%20while%20achieving%20state-of-the-art%0Aaccuracy.%20Furthermore%2C%20we%20explore%20the%20impact%20of%20depth%20representation%20on%0Atracking%20performance%20and%20identify%20log-depth%20as%20the%20optimal%20choice.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20DELTA%20on%20multiple%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20results%20in%20both%202D%20and%203D%20dense%20tracking%20tasks.%0AOur%20method%20provides%20a%20robust%20solution%20for%20applications%20requiring%20fine-grained%2C%0Along-term%20motion%20tracking%20in%203D%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELTA%253A%2520Dense%2520Efficient%2520Long-range%25203D%2520Tracking%2520for%2520any%2520video%26entry.906535625%3DTuan%2520Duc%2520Ngo%2520and%2520Peiye%2520Zhuang%2520and%2520Chuang%2520Gan%2520and%2520Evangelos%2520Kalogerakis%2520and%2520Sergey%2520Tulyakov%2520and%2520Hsin-Ying%2520Lee%2520and%2520Chaoyang%2520Wang%26entry.1292438233%3D%2520%2520Tracking%2520dense%25203D%2520motion%2520from%2520monocular%2520videos%2520remains%2520challenging%252C%250Aparticularly%2520when%2520aiming%2520for%2520pixel-level%2520precision%2520over%2520long%2520sequences.%2520We%250Aintroduce%2520DELTA%252C%2520a%2520novel%2520method%2520that%2520efficiently%2520tracks%2520every%2520pixel%2520in%25203D%250Aspace%252C%2520enabling%2520accurate%2520motion%2520estimation%2520across%2520entire%2520videos.%2520Our%2520approach%250Aleverages%2520a%2520joint%2520global-local%2520attention%2520mechanism%2520for%2520reduced-resolution%250Atracking%252C%2520followed%2520by%2520a%2520transformer-based%2520upsampler%2520to%2520achieve%2520high-resolution%250Apredictions.%2520Unlike%2520existing%2520methods%252C%2520which%2520are%2520limited%2520by%2520computational%250Ainefficiency%2520or%2520sparse%2520tracking%252C%2520DELTA%2520delivers%2520dense%25203D%2520tracking%2520at%2520scale%252C%250Arunning%2520over%25208x%2520faster%2520than%2520previous%2520methods%2520while%2520achieving%2520state-of-the-art%250Aaccuracy.%2520Furthermore%252C%2520we%2520explore%2520the%2520impact%2520of%2520depth%2520representation%2520on%250Atracking%2520performance%2520and%2520identify%2520log-depth%2520as%2520the%2520optimal%2520choice.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520DELTA%2520on%2520multiple%2520benchmarks%252C%250Aachieving%2520new%2520state-of-the-art%2520results%2520in%2520both%25202D%2520and%25203D%2520dense%2520tracking%2520tasks.%250AOur%2520method%2520provides%2520a%2520robust%2520solution%2520for%2520applications%2520requiring%2520fine-grained%252C%250Along-term%2520motion%2520tracking%2520in%25203D%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video&entry.906535625=Tuan%20Duc%20Ngo%20and%20Peiye%20Zhuang%20and%20Chuang%20Gan%20and%20Evangelos%20Kalogerakis%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang&entry.1292438233=%20%20Tracking%20dense%203D%20motion%20from%20monocular%20videos%20remains%20challenging%2C%0Aparticularly%20when%20aiming%20for%20pixel-level%20precision%20over%20long%20sequences.%20We%0Aintroduce%20DELTA%2C%20a%20novel%20method%20that%20efficiently%20tracks%20every%20pixel%20in%203D%0Aspace%2C%20enabling%20accurate%20motion%20estimation%20across%20entire%20videos.%20Our%20approach%0Aleverages%20a%20joint%20global-local%20attention%20mechanism%20for%20reduced-resolution%0Atracking%2C%20followed%20by%20a%20transformer-based%20upsampler%20to%20achieve%20high-resolution%0Apredictions.%20Unlike%20existing%20methods%2C%20which%20are%20limited%20by%20computational%0Ainefficiency%20or%20sparse%20tracking%2C%20DELTA%20delivers%20dense%203D%20tracking%20at%20scale%2C%0Arunning%20over%208x%20faster%20than%20previous%20methods%20while%20achieving%20state-of-the-art%0Aaccuracy.%20Furthermore%2C%20we%20explore%20the%20impact%20of%20depth%20representation%20on%0Atracking%20performance%20and%20identify%20log-depth%20as%20the%20optimal%20choice.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20DELTA%20on%20multiple%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20results%20in%20both%202D%20and%203D%20dense%20tracking%20tasks.%0AOur%20method%20provides%20a%20robust%20solution%20for%20applications%20requiring%20fine-grained%2C%0Along-term%20motion%20tracking%20in%203D%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24211v2&entry.124074799=Read"},
{"title": "Language Imbalance Driven Rewarding for Multilingual Self-improving", "author": "Wen Yang and Junhong Wu and Chen Wang and Chengqing Zong and Jiajun Zhang", "abstract": "  Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.\n", "link": "http://arxiv.org/abs/2410.08964v2", "date": "2024-11-01", "relevancy": 2.3493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving&body=Title%3A%20Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving%0AAuthor%3A%20Wen%20Yang%20and%20Junhong%20Wu%20and%20Chen%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20state-of-the-art%20performance%0Aacross%20numerous%20tasks.%20However%2C%20these%20advancements%20have%20predominantly%20benefited%0A%22first-class%22%20languages%20such%20as%20English%20and%20Chinese%2C%20leaving%20many%20other%0Alanguages%20underrepresented.%20This%20imbalance%2C%20while%20limiting%20broader%0Aapplications%2C%20generates%20a%20natural%20preference%20ranking%20between%20languages%2C%0Aoffering%20an%20opportunity%20to%20bootstrap%20the%20multilingual%20capabilities%20of%20LLM%20in%20a%0Aself-improving%20manner.%20Thus%2C%20we%20propose%20%24%5Ctextit%7BLanguage%20Imbalance%20Driven%0ARewarding%7D%24%2C%20where%20the%20inherent%20imbalance%20between%20dominant%20and%20non-dominant%0Alanguages%20within%20LLMs%20is%20leveraged%20as%20a%20reward%20signal.%20Iterative%20DPO%20training%0Ademonstrates%20that%20this%20approach%20not%20only%20enhances%20LLM%20performance%20in%0Anon-dominant%20languages%20but%20also%20improves%20the%20dominant%20language%27s%20capacity%2C%0Athereby%20yielding%20an%20iterative%20reward%20signal.%20Fine-tuning%0AMeta-Llama-3-8B-Instruct%20over%20two%20iterations%20of%20this%20approach%20results%20in%0Acontinuous%20improvements%20in%20multilingual%20performance%20across%0Ainstruction-following%20and%20arithmetic%20reasoning%20tasks%2C%20evidenced%20by%20an%20average%0Aimprovement%20of%207.46%25%20win%20rate%20on%20the%20X-AlpacaEval%20leaderboard%20and%2013.9%25%0Aaccuracy%20on%20the%20MGSM%20benchmark.%20This%20work%20serves%20as%20an%20initial%20exploration%2C%0Apaving%20the%20way%20for%20multilingual%20self-improvement%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Imbalance%2520Driven%2520Rewarding%2520for%2520Multilingual%2520Self-improving%26entry.906535625%3DWen%2520Yang%2520and%2520Junhong%2520Wu%2520and%2520Chen%2520Wang%2520and%2520Chengqing%2520Zong%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520state-of-the-art%2520performance%250Aacross%2520numerous%2520tasks.%2520However%252C%2520these%2520advancements%2520have%2520predominantly%2520benefited%250A%2522first-class%2522%2520languages%2520such%2520as%2520English%2520and%2520Chinese%252C%2520leaving%2520many%2520other%250Alanguages%2520underrepresented.%2520This%2520imbalance%252C%2520while%2520limiting%2520broader%250Aapplications%252C%2520generates%2520a%2520natural%2520preference%2520ranking%2520between%2520languages%252C%250Aoffering%2520an%2520opportunity%2520to%2520bootstrap%2520the%2520multilingual%2520capabilities%2520of%2520LLM%2520in%2520a%250Aself-improving%2520manner.%2520Thus%252C%2520we%2520propose%2520%2524%255Ctextit%257BLanguage%2520Imbalance%2520Driven%250ARewarding%257D%2524%252C%2520where%2520the%2520inherent%2520imbalance%2520between%2520dominant%2520and%2520non-dominant%250Alanguages%2520within%2520LLMs%2520is%2520leveraged%2520as%2520a%2520reward%2520signal.%2520Iterative%2520DPO%2520training%250Ademonstrates%2520that%2520this%2520approach%2520not%2520only%2520enhances%2520LLM%2520performance%2520in%250Anon-dominant%2520languages%2520but%2520also%2520improves%2520the%2520dominant%2520language%2527s%2520capacity%252C%250Athereby%2520yielding%2520an%2520iterative%2520reward%2520signal.%2520Fine-tuning%250AMeta-Llama-3-8B-Instruct%2520over%2520two%2520iterations%2520of%2520this%2520approach%2520results%2520in%250Acontinuous%2520improvements%2520in%2520multilingual%2520performance%2520across%250Ainstruction-following%2520and%2520arithmetic%2520reasoning%2520tasks%252C%2520evidenced%2520by%2520an%2520average%250Aimprovement%2520of%25207.46%2525%2520win%2520rate%2520on%2520the%2520X-AlpacaEval%2520leaderboard%2520and%252013.9%2525%250Aaccuracy%2520on%2520the%2520MGSM%2520benchmark.%2520This%2520work%2520serves%2520as%2520an%2520initial%2520exploration%252C%250Apaving%2520the%2520way%2520for%2520multilingual%2520self-improvement%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Imbalance%20Driven%20Rewarding%20for%20Multilingual%20Self-improving&entry.906535625=Wen%20Yang%20and%20Junhong%20Wu%20and%20Chen%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20state-of-the-art%20performance%0Aacross%20numerous%20tasks.%20However%2C%20these%20advancements%20have%20predominantly%20benefited%0A%22first-class%22%20languages%20such%20as%20English%20and%20Chinese%2C%20leaving%20many%20other%0Alanguages%20underrepresented.%20This%20imbalance%2C%20while%20limiting%20broader%0Aapplications%2C%20generates%20a%20natural%20preference%20ranking%20between%20languages%2C%0Aoffering%20an%20opportunity%20to%20bootstrap%20the%20multilingual%20capabilities%20of%20LLM%20in%20a%0Aself-improving%20manner.%20Thus%2C%20we%20propose%20%24%5Ctextit%7BLanguage%20Imbalance%20Driven%0ARewarding%7D%24%2C%20where%20the%20inherent%20imbalance%20between%20dominant%20and%20non-dominant%0Alanguages%20within%20LLMs%20is%20leveraged%20as%20a%20reward%20signal.%20Iterative%20DPO%20training%0Ademonstrates%20that%20this%20approach%20not%20only%20enhances%20LLM%20performance%20in%0Anon-dominant%20languages%20but%20also%20improves%20the%20dominant%20language%27s%20capacity%2C%0Athereby%20yielding%20an%20iterative%20reward%20signal.%20Fine-tuning%0AMeta-Llama-3-8B-Instruct%20over%20two%20iterations%20of%20this%20approach%20results%20in%0Acontinuous%20improvements%20in%20multilingual%20performance%20across%0Ainstruction-following%20and%20arithmetic%20reasoning%20tasks%2C%20evidenced%20by%20an%20average%0Aimprovement%20of%207.46%25%20win%20rate%20on%20the%20X-AlpacaEval%20leaderboard%20and%2013.9%25%0Aaccuracy%20on%20the%20MGSM%20benchmark.%20This%20work%20serves%20as%20an%20initial%20exploration%2C%0Apaving%20the%20way%20for%20multilingual%20self-improvement%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08964v2&entry.124074799=Read"},
{"title": "Return of Unconditional Generation: A Self-supervised Representation\n  Generation Method", "author": "Tianhong Li and Dina Katabi and Kaiming He", "abstract": "  Unconditional generation -- the problem of modeling data distribution without\nrelying on human-annotated labels -- is a long-standing and fundamental\nchallenge in generative models, creating a potential of learning from\nlarge-scale unlabeled data. In the literature, the generation quality of an\nunconditional method has been much worse than that of its conditional\ncounterpart. This gap can be attributed to the lack of semantic information\nprovided by labels. In this work, we show that one can close this gap by\ngenerating semantic representations in the representation space produced by a\nself-supervised encoder. These representations can be used to condition the\nimage generator. This framework, called Representation-Conditioned Generation\n(RCG), provides an effective solution to the unconditional generation problem\nwithout using labels. Through comprehensive experiments, we observe that RCG\nsignificantly improves unconditional generation quality: e.g., it achieves a\nnew state-of-the-art FID of 2.15 on ImageNet 256x256, largely reducing the\nprevious best of 5.91 by a relative 64%. Our unconditional results are situated\nin the same tier as the leading class-conditional ones. We hope these\nencouraging observations will attract the community's attention to the\nfundamental problem of unconditional generation. Code is available at\nhttps://github.com/LTH14/rcg.\n", "link": "http://arxiv.org/abs/2312.03701v4", "date": "2024-11-01", "relevancy": 2.3078, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5872}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5697}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Return%20of%20Unconditional%20Generation%3A%20A%20Self-supervised%20Representation%0A%20%20Generation%20Method&body=Title%3A%20Return%20of%20Unconditional%20Generation%3A%20A%20Self-supervised%20Representation%0A%20%20Generation%20Method%0AAuthor%3A%20Tianhong%20Li%20and%20Dina%20Katabi%20and%20Kaiming%20He%0AAbstract%3A%20%20%20Unconditional%20generation%20--%20the%20problem%20of%20modeling%20data%20distribution%20without%0Arelying%20on%20human-annotated%20labels%20--%20is%20a%20long-standing%20and%20fundamental%0Achallenge%20in%20generative%20models%2C%20creating%20a%20potential%20of%20learning%20from%0Alarge-scale%20unlabeled%20data.%20In%20the%20literature%2C%20the%20generation%20quality%20of%20an%0Aunconditional%20method%20has%20been%20much%20worse%20than%20that%20of%20its%20conditional%0Acounterpart.%20This%20gap%20can%20be%20attributed%20to%20the%20lack%20of%20semantic%20information%0Aprovided%20by%20labels.%20In%20this%20work%2C%20we%20show%20that%20one%20can%20close%20this%20gap%20by%0Agenerating%20semantic%20representations%20in%20the%20representation%20space%20produced%20by%20a%0Aself-supervised%20encoder.%20These%20representations%20can%20be%20used%20to%20condition%20the%0Aimage%20generator.%20This%20framework%2C%20called%20Representation-Conditioned%20Generation%0A%28RCG%29%2C%20provides%20an%20effective%20solution%20to%20the%20unconditional%20generation%20problem%0Awithout%20using%20labels.%20Through%20comprehensive%20experiments%2C%20we%20observe%20that%20RCG%0Asignificantly%20improves%20unconditional%20generation%20quality%3A%20e.g.%2C%20it%20achieves%20a%0Anew%20state-of-the-art%20FID%20of%202.15%20on%20ImageNet%20256x256%2C%20largely%20reducing%20the%0Aprevious%20best%20of%205.91%20by%20a%20relative%2064%25.%20Our%20unconditional%20results%20are%20situated%0Ain%20the%20same%20tier%20as%20the%20leading%20class-conditional%20ones.%20We%20hope%20these%0Aencouraging%20observations%20will%20attract%20the%20community%27s%20attention%20to%20the%0Afundamental%20problem%20of%20unconditional%20generation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LTH14/rcg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03701v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReturn%2520of%2520Unconditional%2520Generation%253A%2520A%2520Self-supervised%2520Representation%250A%2520%2520Generation%2520Method%26entry.906535625%3DTianhong%2520Li%2520and%2520Dina%2520Katabi%2520and%2520Kaiming%2520He%26entry.1292438233%3D%2520%2520Unconditional%2520generation%2520--%2520the%2520problem%2520of%2520modeling%2520data%2520distribution%2520without%250Arelying%2520on%2520human-annotated%2520labels%2520--%2520is%2520a%2520long-standing%2520and%2520fundamental%250Achallenge%2520in%2520generative%2520models%252C%2520creating%2520a%2520potential%2520of%2520learning%2520from%250Alarge-scale%2520unlabeled%2520data.%2520In%2520the%2520literature%252C%2520the%2520generation%2520quality%2520of%2520an%250Aunconditional%2520method%2520has%2520been%2520much%2520worse%2520than%2520that%2520of%2520its%2520conditional%250Acounterpart.%2520This%2520gap%2520can%2520be%2520attributed%2520to%2520the%2520lack%2520of%2520semantic%2520information%250Aprovided%2520by%2520labels.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520one%2520can%2520close%2520this%2520gap%2520by%250Agenerating%2520semantic%2520representations%2520in%2520the%2520representation%2520space%2520produced%2520by%2520a%250Aself-supervised%2520encoder.%2520These%2520representations%2520can%2520be%2520used%2520to%2520condition%2520the%250Aimage%2520generator.%2520This%2520framework%252C%2520called%2520Representation-Conditioned%2520Generation%250A%2528RCG%2529%252C%2520provides%2520an%2520effective%2520solution%2520to%2520the%2520unconditional%2520generation%2520problem%250Awithout%2520using%2520labels.%2520Through%2520comprehensive%2520experiments%252C%2520we%2520observe%2520that%2520RCG%250Asignificantly%2520improves%2520unconditional%2520generation%2520quality%253A%2520e.g.%252C%2520it%2520achieves%2520a%250Anew%2520state-of-the-art%2520FID%2520of%25202.15%2520on%2520ImageNet%2520256x256%252C%2520largely%2520reducing%2520the%250Aprevious%2520best%2520of%25205.91%2520by%2520a%2520relative%252064%2525.%2520Our%2520unconditional%2520results%2520are%2520situated%250Ain%2520the%2520same%2520tier%2520as%2520the%2520leading%2520class-conditional%2520ones.%2520We%2520hope%2520these%250Aencouraging%2520observations%2520will%2520attract%2520the%2520community%2527s%2520attention%2520to%2520the%250Afundamental%2520problem%2520of%2520unconditional%2520generation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/LTH14/rcg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03701v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Return%20of%20Unconditional%20Generation%3A%20A%20Self-supervised%20Representation%0A%20%20Generation%20Method&entry.906535625=Tianhong%20Li%20and%20Dina%20Katabi%20and%20Kaiming%20He&entry.1292438233=%20%20Unconditional%20generation%20--%20the%20problem%20of%20modeling%20data%20distribution%20without%0Arelying%20on%20human-annotated%20labels%20--%20is%20a%20long-standing%20and%20fundamental%0Achallenge%20in%20generative%20models%2C%20creating%20a%20potential%20of%20learning%20from%0Alarge-scale%20unlabeled%20data.%20In%20the%20literature%2C%20the%20generation%20quality%20of%20an%0Aunconditional%20method%20has%20been%20much%20worse%20than%20that%20of%20its%20conditional%0Acounterpart.%20This%20gap%20can%20be%20attributed%20to%20the%20lack%20of%20semantic%20information%0Aprovided%20by%20labels.%20In%20this%20work%2C%20we%20show%20that%20one%20can%20close%20this%20gap%20by%0Agenerating%20semantic%20representations%20in%20the%20representation%20space%20produced%20by%20a%0Aself-supervised%20encoder.%20These%20representations%20can%20be%20used%20to%20condition%20the%0Aimage%20generator.%20This%20framework%2C%20called%20Representation-Conditioned%20Generation%0A%28RCG%29%2C%20provides%20an%20effective%20solution%20to%20the%20unconditional%20generation%20problem%0Awithout%20using%20labels.%20Through%20comprehensive%20experiments%2C%20we%20observe%20that%20RCG%0Asignificantly%20improves%20unconditional%20generation%20quality%3A%20e.g.%2C%20it%20achieves%20a%0Anew%20state-of-the-art%20FID%20of%202.15%20on%20ImageNet%20256x256%2C%20largely%20reducing%20the%0Aprevious%20best%20of%205.91%20by%20a%20relative%2064%25.%20Our%20unconditional%20results%20are%20situated%0Ain%20the%20same%20tier%20as%20the%20leading%20class-conditional%20ones.%20We%20hope%20these%0Aencouraging%20observations%20will%20attract%20the%20community%27s%20attention%20to%20the%0Afundamental%20problem%20of%20unconditional%20generation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/LTH14/rcg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03701v4&entry.124074799=Read"},
{"title": "Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video", "author": "Guiqiu Liao and Matjaz Jogan and Sai Koushik and Eric Eaton and Daniel A. Hashimoto", "abstract": "  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n", "link": "http://arxiv.org/abs/2407.15794v4", "date": "2024-11-01", "relevancy": 2.2852, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&body=Title%3A%20Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video%0AAuthor%3A%20Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto%0AAbstract%3A%20%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15794v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520spatio-temporal%2520knowledge%2520for%2520weakly%2520supervised%2520object%250A%2520%2520detection%2520and%2520segmentation%2520in%2520surgical%2520video%26entry.906535625%3DGuiqiu%2520Liao%2520and%2520Matjaz%2520Jogan%2520and%2520Sai%2520Koushik%2520and%2520Eric%2520Eaton%2520and%2520Daniel%2520A.%2520Hashimoto%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520video%2520object%2520segmentation%2520%2528WSVOS%2529%2520enables%2520the%250Aidentification%2520of%2520segmentation%2520maps%2520without%2520requiring%2520an%2520extensive%2520training%250Adataset%2520of%2520object%2520masks%252C%2520relying%2520instead%2520on%2520coarse%2520video%2520labels%2520indicating%250Aobject%2520presence.%2520Current%2520state-of-the-art%2520methods%2520either%2520require%2520multiple%250Aindependent%2520stages%2520of%2520processing%2520that%2520employ%2520motion%2520cues%2520or%252C%2520in%2520the%2520case%2520of%250Aend-to-end%2520trainable%2520networks%252C%2520lack%2520in%2520segmentation%2520accuracy%252C%2520in%2520part%2520due%2520to%250Athe%2520difficulty%2520of%2520learning%2520segmentation%2520maps%2520from%2520videos%2520with%2520transient%2520object%250Apresence.%2520This%2520limits%2520the%2520application%2520of%2520WSVOS%2520for%2520semantic%2520annotation%2520of%250Asurgical%2520videos%2520where%2520multiple%2520surgical%2520tools%2520frequently%2520move%2520in%2520and%2520out%2520of%2520the%250Afield%2520of%2520view%252C%2520a%2520problem%2520that%2520is%2520more%2520difficult%2520than%2520typically%2520encountered%2520in%250AWSVOS.%2520This%2520paper%2520introduces%2520Video%2520Spatio-Temporal%2520Disentanglement%2520Networks%250A%2528VDST-Net%2529%252C%2520a%2520framework%2520to%2520disentangle%2520spatiotemporal%2520information%2520using%250Asemi-decoupled%2520knowledge%2520distillation%2520to%2520predict%2520high-quality%2520class%2520activation%250Amaps%2520%2528CAMs%2529.%2520A%2520teacher%2520network%2520designed%2520to%2520resolve%2520temporal%2520conflicts%2520when%250Aspecifics%2520about%2520object%2520location%2520and%2520timing%2520in%2520the%2520video%2520are%2520not%2520provided%2520works%250Awith%2520a%2520student%2520network%2520that%2520integrates%2520information%2520over%2520time%2520by%2520leveraging%250Atemporal%2520dependencies.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520framework%2520on%2520a%2520public%250Areference%2520dataset%2520and%2520on%2520a%2520more%2520challenging%2520surgical%2520video%2520dataset%2520where%250Aobjects%2520are%252C%2520on%2520average%252C%2520present%2520in%2520less%2520than%252060%255C%2525%2520of%2520annotated%2520frames.%2520Our%250Amethod%2520outperforms%2520state-of-the-art%2520techniques%2520and%2520generates%2520superior%250Asegmentation%2520masks%2520under%2520video-level%2520weak%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15794v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20spatio-temporal%20knowledge%20for%20weakly%20supervised%20object%0A%20%20detection%20and%20segmentation%20in%20surgical%20video&entry.906535625=Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Sai%20Koushik%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto&entry.1292438233=%20%20Weakly%20supervised%20video%20object%20segmentation%20%28WSVOS%29%20enables%20the%0Aidentification%20of%20segmentation%20maps%20without%20requiring%20an%20extensive%20training%0Adataset%20of%20object%20masks%2C%20relying%20instead%20on%20coarse%20video%20labels%20indicating%0Aobject%20presence.%20Current%20state-of-the-art%20methods%20either%20require%20multiple%0Aindependent%20stages%20of%20processing%20that%20employ%20motion%20cues%20or%2C%20in%20the%20case%20of%0Aend-to-end%20trainable%20networks%2C%20lack%20in%20segmentation%20accuracy%2C%20in%20part%20due%20to%0Athe%20difficulty%20of%20learning%20segmentation%20maps%20from%20videos%20with%20transient%20object%0Apresence.%20This%20limits%20the%20application%20of%20WSVOS%20for%20semantic%20annotation%20of%0Asurgical%20videos%20where%20multiple%20surgical%20tools%20frequently%20move%20in%20and%20out%20of%20the%0Afield%20of%20view%2C%20a%20problem%20that%20is%20more%20difficult%20than%20typically%20encountered%20in%0AWSVOS.%20This%20paper%20introduces%20Video%20Spatio-Temporal%20Disentanglement%20Networks%0A%28VDST-Net%29%2C%20a%20framework%20to%20disentangle%20spatiotemporal%20information%20using%0Asemi-decoupled%20knowledge%20distillation%20to%20predict%20high-quality%20class%20activation%0Amaps%20%28CAMs%29.%20A%20teacher%20network%20designed%20to%20resolve%20temporal%20conflicts%20when%0Aspecifics%20about%20object%20location%20and%20timing%20in%20the%20video%20are%20not%20provided%20works%0Awith%20a%20student%20network%20that%20integrates%20information%20over%20time%20by%20leveraging%0Atemporal%20dependencies.%20We%20demonstrate%20the%20efficacy%20of%20our%20framework%20on%20a%20public%0Areference%20dataset%20and%20on%20a%20more%20challenging%20surgical%20video%20dataset%20where%0Aobjects%20are%2C%20on%20average%2C%20present%20in%20less%20than%2060%5C%25%20of%20annotated%20frames.%20Our%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20generates%20superior%0Asegmentation%20masks%20under%20video-level%20weak%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15794v4&entry.124074799=Read"},
{"title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks", "author": "Hong-fu Chou and Vu Nguyen Ha and Prabhu Thiruvasagam and Thanh-Dung Le and Geoffrey Eappen and Ti Ti Nguyen and Luis M. Garces-Socarras and Jorge L. Gonzalez-Rios and Juan Carlos Merlano-Duncan and Symeon Chatzinotas", "abstract": "  Earth Observation (EO) systems are crucial for cartography, disaster\nsurveillance, and resource administration. Nonetheless, they encounter\nconsiderable obstacles in the processing and transmission of extensive data,\nespecially in specialized domains such as precision agriculture and real-time\ndisaster response. Earth observation satellites, outfitted with remote sensing\ntechnology, gather data from onboard sensors and IoT-enabled terrestrial\nobjects, delivering important information remotely. Domain-adapted Large\nLanguage Models (LLMs) provide a solution by enabling the integration of raw\nand processed EO data. Through domain adaptation, LLMs improve the assimilation\nand analysis of many data sources, tackling the intricacies of specialized\ndatasets in agriculture and disaster response. This data synthesis, directed by\nLLMs, enhances the precision and pertinence of conveyed information. This study\nprovides a thorough examination of using semantic inference and deep learning\nfor sophisticated EO systems. It presents an innovative architecture for\nsemantic communication in EO satellite networks, designed to improve data\ntransmission efficiency using semantic processing methodologies. Recent\nadvancements in onboard processing technologies enable dependable, adaptable,\nand energy-efficient data management in orbit. These improvements guarantee\nreliable performance in adverse space circumstances using radiation-hardened\nand reconfigurable technology. Collectively, these advancements enable\nnext-generation satellite missions with improved processing capabilities,\ncrucial for operational flexibility and real-time decision-making in 6G\nsatellite communication.\n", "link": "http://arxiv.org/abs/2409.15246v3", "date": "2024-11-01", "relevancy": 2.181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.55}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-Air%20Deep%20Learning%20Integrated%20Semantic%20Inference%20Models%20for%20Enhanced%0A%20%20Earth%20Observation%20Satellite%20Networks&body=Title%3A%20On-Air%20Deep%20Learning%20Integrated%20Semantic%20Inference%20Models%20for%20Enhanced%0A%20%20Earth%20Observation%20Satellite%20Networks%0AAuthor%3A%20Hong-fu%20Chou%20and%20Vu%20Nguyen%20Ha%20and%20Prabhu%20Thiruvasagam%20and%20Thanh-Dung%20Le%20and%20Geoffrey%20Eappen%20and%20Ti%20Ti%20Nguyen%20and%20Luis%20M.%20Garces-Socarras%20and%20Jorge%20L.%20Gonzalez-Rios%20and%20Juan%20Carlos%20Merlano-Duncan%20and%20Symeon%20Chatzinotas%0AAbstract%3A%20%20%20Earth%20Observation%20%28EO%29%20systems%20are%20crucial%20for%20cartography%2C%20disaster%0Asurveillance%2C%20and%20resource%20administration.%20Nonetheless%2C%20they%20encounter%0Aconsiderable%20obstacles%20in%20the%20processing%20and%20transmission%20of%20extensive%20data%2C%0Aespecially%20in%20specialized%20domains%20such%20as%20precision%20agriculture%20and%20real-time%0Adisaster%20response.%20Earth%20observation%20satellites%2C%20outfitted%20with%20remote%20sensing%0Atechnology%2C%20gather%20data%20from%20onboard%20sensors%20and%20IoT-enabled%20terrestrial%0Aobjects%2C%20delivering%20important%20information%20remotely.%20Domain-adapted%20Large%0ALanguage%20Models%20%28LLMs%29%20provide%20a%20solution%20by%20enabling%20the%20integration%20of%20raw%0Aand%20processed%20EO%20data.%20Through%20domain%20adaptation%2C%20LLMs%20improve%20the%20assimilation%0Aand%20analysis%20of%20many%20data%20sources%2C%20tackling%20the%20intricacies%20of%20specialized%0Adatasets%20in%20agriculture%20and%20disaster%20response.%20This%20data%20synthesis%2C%20directed%20by%0ALLMs%2C%20enhances%20the%20precision%20and%20pertinence%20of%20conveyed%20information.%20This%20study%0Aprovides%20a%20thorough%20examination%20of%20using%20semantic%20inference%20and%20deep%20learning%0Afor%20sophisticated%20EO%20systems.%20It%20presents%20an%20innovative%20architecture%20for%0Asemantic%20communication%20in%20EO%20satellite%20networks%2C%20designed%20to%20improve%20data%0Atransmission%20efficiency%20using%20semantic%20processing%20methodologies.%20Recent%0Aadvancements%20in%20onboard%20processing%20technologies%20enable%20dependable%2C%20adaptable%2C%0Aand%20energy-efficient%20data%20management%20in%20orbit.%20These%20improvements%20guarantee%0Areliable%20performance%20in%20adverse%20space%20circumstances%20using%20radiation-hardened%0Aand%20reconfigurable%20technology.%20Collectively%2C%20these%20advancements%20enable%0Anext-generation%20satellite%20missions%20with%20improved%20processing%20capabilities%2C%0Acrucial%20for%20operational%20flexibility%20and%20real-time%20decision-making%20in%206G%0Asatellite%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15246v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-Air%2520Deep%2520Learning%2520Integrated%2520Semantic%2520Inference%2520Models%2520for%2520Enhanced%250A%2520%2520Earth%2520Observation%2520Satellite%2520Networks%26entry.906535625%3DHong-fu%2520Chou%2520and%2520Vu%2520Nguyen%2520Ha%2520and%2520Prabhu%2520Thiruvasagam%2520and%2520Thanh-Dung%2520Le%2520and%2520Geoffrey%2520Eappen%2520and%2520Ti%2520Ti%2520Nguyen%2520and%2520Luis%2520M.%2520Garces-Socarras%2520and%2520Jorge%2520L.%2520Gonzalez-Rios%2520and%2520Juan%2520Carlos%2520Merlano-Duncan%2520and%2520Symeon%2520Chatzinotas%26entry.1292438233%3D%2520%2520Earth%2520Observation%2520%2528EO%2529%2520systems%2520are%2520crucial%2520for%2520cartography%252C%2520disaster%250Asurveillance%252C%2520and%2520resource%2520administration.%2520Nonetheless%252C%2520they%2520encounter%250Aconsiderable%2520obstacles%2520in%2520the%2520processing%2520and%2520transmission%2520of%2520extensive%2520data%252C%250Aespecially%2520in%2520specialized%2520domains%2520such%2520as%2520precision%2520agriculture%2520and%2520real-time%250Adisaster%2520response.%2520Earth%2520observation%2520satellites%252C%2520outfitted%2520with%2520remote%2520sensing%250Atechnology%252C%2520gather%2520data%2520from%2520onboard%2520sensors%2520and%2520IoT-enabled%2520terrestrial%250Aobjects%252C%2520delivering%2520important%2520information%2520remotely.%2520Domain-adapted%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520provide%2520a%2520solution%2520by%2520enabling%2520the%2520integration%2520of%2520raw%250Aand%2520processed%2520EO%2520data.%2520Through%2520domain%2520adaptation%252C%2520LLMs%2520improve%2520the%2520assimilation%250Aand%2520analysis%2520of%2520many%2520data%2520sources%252C%2520tackling%2520the%2520intricacies%2520of%2520specialized%250Adatasets%2520in%2520agriculture%2520and%2520disaster%2520response.%2520This%2520data%2520synthesis%252C%2520directed%2520by%250ALLMs%252C%2520enhances%2520the%2520precision%2520and%2520pertinence%2520of%2520conveyed%2520information.%2520This%2520study%250Aprovides%2520a%2520thorough%2520examination%2520of%2520using%2520semantic%2520inference%2520and%2520deep%2520learning%250Afor%2520sophisticated%2520EO%2520systems.%2520It%2520presents%2520an%2520innovative%2520architecture%2520for%250Asemantic%2520communication%2520in%2520EO%2520satellite%2520networks%252C%2520designed%2520to%2520improve%2520data%250Atransmission%2520efficiency%2520using%2520semantic%2520processing%2520methodologies.%2520Recent%250Aadvancements%2520in%2520onboard%2520processing%2520technologies%2520enable%2520dependable%252C%2520adaptable%252C%250Aand%2520energy-efficient%2520data%2520management%2520in%2520orbit.%2520These%2520improvements%2520guarantee%250Areliable%2520performance%2520in%2520adverse%2520space%2520circumstances%2520using%2520radiation-hardened%250Aand%2520reconfigurable%2520technology.%2520Collectively%252C%2520these%2520advancements%2520enable%250Anext-generation%2520satellite%2520missions%2520with%2520improved%2520processing%2520capabilities%252C%250Acrucial%2520for%2520operational%2520flexibility%2520and%2520real-time%2520decision-making%2520in%25206G%250Asatellite%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15246v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-Air%20Deep%20Learning%20Integrated%20Semantic%20Inference%20Models%20for%20Enhanced%0A%20%20Earth%20Observation%20Satellite%20Networks&entry.906535625=Hong-fu%20Chou%20and%20Vu%20Nguyen%20Ha%20and%20Prabhu%20Thiruvasagam%20and%20Thanh-Dung%20Le%20and%20Geoffrey%20Eappen%20and%20Ti%20Ti%20Nguyen%20and%20Luis%20M.%20Garces-Socarras%20and%20Jorge%20L.%20Gonzalez-Rios%20and%20Juan%20Carlos%20Merlano-Duncan%20and%20Symeon%20Chatzinotas&entry.1292438233=%20%20Earth%20Observation%20%28EO%29%20systems%20are%20crucial%20for%20cartography%2C%20disaster%0Asurveillance%2C%20and%20resource%20administration.%20Nonetheless%2C%20they%20encounter%0Aconsiderable%20obstacles%20in%20the%20processing%20and%20transmission%20of%20extensive%20data%2C%0Aespecially%20in%20specialized%20domains%20such%20as%20precision%20agriculture%20and%20real-time%0Adisaster%20response.%20Earth%20observation%20satellites%2C%20outfitted%20with%20remote%20sensing%0Atechnology%2C%20gather%20data%20from%20onboard%20sensors%20and%20IoT-enabled%20terrestrial%0Aobjects%2C%20delivering%20important%20information%20remotely.%20Domain-adapted%20Large%0ALanguage%20Models%20%28LLMs%29%20provide%20a%20solution%20by%20enabling%20the%20integration%20of%20raw%0Aand%20processed%20EO%20data.%20Through%20domain%20adaptation%2C%20LLMs%20improve%20the%20assimilation%0Aand%20analysis%20of%20many%20data%20sources%2C%20tackling%20the%20intricacies%20of%20specialized%0Adatasets%20in%20agriculture%20and%20disaster%20response.%20This%20data%20synthesis%2C%20directed%20by%0ALLMs%2C%20enhances%20the%20precision%20and%20pertinence%20of%20conveyed%20information.%20This%20study%0Aprovides%20a%20thorough%20examination%20of%20using%20semantic%20inference%20and%20deep%20learning%0Afor%20sophisticated%20EO%20systems.%20It%20presents%20an%20innovative%20architecture%20for%0Asemantic%20communication%20in%20EO%20satellite%20networks%2C%20designed%20to%20improve%20data%0Atransmission%20efficiency%20using%20semantic%20processing%20methodologies.%20Recent%0Aadvancements%20in%20onboard%20processing%20technologies%20enable%20dependable%2C%20adaptable%2C%0Aand%20energy-efficient%20data%20management%20in%20orbit.%20These%20improvements%20guarantee%0Areliable%20performance%20in%20adverse%20space%20circumstances%20using%20radiation-hardened%0Aand%20reconfigurable%20technology.%20Collectively%2C%20these%20advancements%20enable%0Anext-generation%20satellite%20missions%20with%20improved%20processing%20capabilities%2C%0Acrucial%20for%20operational%20flexibility%20and%20real-time%20decision-making%20in%206G%0Asatellite%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15246v3&entry.124074799=Read"},
{"title": "DenoiseRep: Denoising Model for Representation Learning", "author": "Zhengrui Xu and Guan'an Wang and Xiaowen Huang and Jitao Sang", "abstract": "  The denoising model has been proven a powerful generative model but has\nlittle exploration of discriminative tasks. Representation learning is\nimportant in discriminative tasks, which is defined as \"learning\nrepresentations (or features) of the data that make it easier to extract useful\ninformation when building classifiers or other predictors\". In this paper, we\npropose a novel Denoising Model for Representation Learning (DenoiseRep) to\nimprove feature discrimination with joint feature extraction and denoising.\nDenoiseRep views each embedding layer in a backbone as a denoising layer,\nprocessing the cascaded embedding layers as if we are recursively denoise\nfeatures step-by-step. This unifies the frameworks of feature extraction and\ndenoising, where the former progressively embeds features from low-level to\nhigh-level, and the latter recursively denoises features step-by-step. After\nthat, DenoiseRep fuses the parameters of feature extraction and denoising\nlayers, and theoretically demonstrates its equivalence before and after the\nfusion, thus making feature denoising computation-free. DenoiseRep is a\nlabel-free algorithm that incrementally improves features but also\ncomplementary to the label if available. Experimental results on various\ndiscriminative vision tasks, including re-identification (Market-1501,\nDukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet,\nUB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation\n(ADE20K) show stability and impressive improvements. We also validate its\neffectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda)\narchitectures.\n", "link": "http://arxiv.org/abs/2406.08773v3", "date": "2024-11-01", "relevancy": 2.1781, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5622}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenoiseRep%3A%20Denoising%20Model%20for%20Representation%20Learning&body=Title%3A%20DenoiseRep%3A%20Denoising%20Model%20for%20Representation%20Learning%0AAuthor%3A%20Zhengrui%20Xu%20and%20Guan%27an%20Wang%20and%20Xiaowen%20Huang%20and%20Jitao%20Sang%0AAbstract%3A%20%20%20The%20denoising%20model%20has%20been%20proven%20a%20powerful%20generative%20model%20but%20has%0Alittle%20exploration%20of%20discriminative%20tasks.%20Representation%20learning%20is%0Aimportant%20in%20discriminative%20tasks%2C%20which%20is%20defined%20as%20%22learning%0Arepresentations%20%28or%20features%29%20of%20the%20data%20that%20make%20it%20easier%20to%20extract%20useful%0Ainformation%20when%20building%20classifiers%20or%20other%20predictors%22.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Denoising%20Model%20for%20Representation%20Learning%20%28DenoiseRep%29%20to%0Aimprove%20feature%20discrimination%20with%20joint%20feature%20extraction%20and%20denoising.%0ADenoiseRep%20views%20each%20embedding%20layer%20in%20a%20backbone%20as%20a%20denoising%20layer%2C%0Aprocessing%20the%20cascaded%20embedding%20layers%20as%20if%20we%20are%20recursively%20denoise%0Afeatures%20step-by-step.%20This%20unifies%20the%20frameworks%20of%20feature%20extraction%20and%0Adenoising%2C%20where%20the%20former%20progressively%20embeds%20features%20from%20low-level%20to%0Ahigh-level%2C%20and%20the%20latter%20recursively%20denoises%20features%20step-by-step.%20After%0Athat%2C%20DenoiseRep%20fuses%20the%20parameters%20of%20feature%20extraction%20and%20denoising%0Alayers%2C%20and%20theoretically%20demonstrates%20its%20equivalence%20before%20and%20after%20the%0Afusion%2C%20thus%20making%20feature%20denoising%20computation-free.%20DenoiseRep%20is%20a%0Alabel-free%20algorithm%20that%20incrementally%20improves%20features%20but%20also%0Acomplementary%20to%20the%20label%20if%20available.%20Experimental%20results%20on%20various%0Adiscriminative%20vision%20tasks%2C%20including%20re-identification%20%28Market-1501%2C%0ADukeMTMC-reID%2C%20MSMT17%2C%20CUHK-03%2C%20vehicleID%29%2C%20image%20classification%20%28ImageNet%2C%0AUB200%2C%20Oxford-Pet%2C%20Flowers%29%2C%20object%20detection%20%28COCO%29%2C%20image%20segmentation%0A%28ADE20K%29%20show%20stability%20and%20impressive%20improvements.%20We%20also%20validate%20its%0Aeffectiveness%20on%20the%20CNN%20%28ResNet%29%20and%20Transformer%20%28ViT%2C%20Swin%2C%20Vmamda%29%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08773v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoiseRep%253A%2520Denoising%2520Model%2520for%2520Representation%2520Learning%26entry.906535625%3DZhengrui%2520Xu%2520and%2520Guan%2527an%2520Wang%2520and%2520Xiaowen%2520Huang%2520and%2520Jitao%2520Sang%26entry.1292438233%3D%2520%2520The%2520denoising%2520model%2520has%2520been%2520proven%2520a%2520powerful%2520generative%2520model%2520but%2520has%250Alittle%2520exploration%2520of%2520discriminative%2520tasks.%2520Representation%2520learning%2520is%250Aimportant%2520in%2520discriminative%2520tasks%252C%2520which%2520is%2520defined%2520as%2520%2522learning%250Arepresentations%2520%2528or%2520features%2529%2520of%2520the%2520data%2520that%2520make%2520it%2520easier%2520to%2520extract%2520useful%250Ainformation%2520when%2520building%2520classifiers%2520or%2520other%2520predictors%2522.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520Denoising%2520Model%2520for%2520Representation%2520Learning%2520%2528DenoiseRep%2529%2520to%250Aimprove%2520feature%2520discrimination%2520with%2520joint%2520feature%2520extraction%2520and%2520denoising.%250ADenoiseRep%2520views%2520each%2520embedding%2520layer%2520in%2520a%2520backbone%2520as%2520a%2520denoising%2520layer%252C%250Aprocessing%2520the%2520cascaded%2520embedding%2520layers%2520as%2520if%2520we%2520are%2520recursively%2520denoise%250Afeatures%2520step-by-step.%2520This%2520unifies%2520the%2520frameworks%2520of%2520feature%2520extraction%2520and%250Adenoising%252C%2520where%2520the%2520former%2520progressively%2520embeds%2520features%2520from%2520low-level%2520to%250Ahigh-level%252C%2520and%2520the%2520latter%2520recursively%2520denoises%2520features%2520step-by-step.%2520After%250Athat%252C%2520DenoiseRep%2520fuses%2520the%2520parameters%2520of%2520feature%2520extraction%2520and%2520denoising%250Alayers%252C%2520and%2520theoretically%2520demonstrates%2520its%2520equivalence%2520before%2520and%2520after%2520the%250Afusion%252C%2520thus%2520making%2520feature%2520denoising%2520computation-free.%2520DenoiseRep%2520is%2520a%250Alabel-free%2520algorithm%2520that%2520incrementally%2520improves%2520features%2520but%2520also%250Acomplementary%2520to%2520the%2520label%2520if%2520available.%2520Experimental%2520results%2520on%2520various%250Adiscriminative%2520vision%2520tasks%252C%2520including%2520re-identification%2520%2528Market-1501%252C%250ADukeMTMC-reID%252C%2520MSMT17%252C%2520CUHK-03%252C%2520vehicleID%2529%252C%2520image%2520classification%2520%2528ImageNet%252C%250AUB200%252C%2520Oxford-Pet%252C%2520Flowers%2529%252C%2520object%2520detection%2520%2528COCO%2529%252C%2520image%2520segmentation%250A%2528ADE20K%2529%2520show%2520stability%2520and%2520impressive%2520improvements.%2520We%2520also%2520validate%2520its%250Aeffectiveness%2520on%2520the%2520CNN%2520%2528ResNet%2529%2520and%2520Transformer%2520%2528ViT%252C%2520Swin%252C%2520Vmamda%2529%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08773v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenoiseRep%3A%20Denoising%20Model%20for%20Representation%20Learning&entry.906535625=Zhengrui%20Xu%20and%20Guan%27an%20Wang%20and%20Xiaowen%20Huang%20and%20Jitao%20Sang&entry.1292438233=%20%20The%20denoising%20model%20has%20been%20proven%20a%20powerful%20generative%20model%20but%20has%0Alittle%20exploration%20of%20discriminative%20tasks.%20Representation%20learning%20is%0Aimportant%20in%20discriminative%20tasks%2C%20which%20is%20defined%20as%20%22learning%0Arepresentations%20%28or%20features%29%20of%20the%20data%20that%20make%20it%20easier%20to%20extract%20useful%0Ainformation%20when%20building%20classifiers%20or%20other%20predictors%22.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Denoising%20Model%20for%20Representation%20Learning%20%28DenoiseRep%29%20to%0Aimprove%20feature%20discrimination%20with%20joint%20feature%20extraction%20and%20denoising.%0ADenoiseRep%20views%20each%20embedding%20layer%20in%20a%20backbone%20as%20a%20denoising%20layer%2C%0Aprocessing%20the%20cascaded%20embedding%20layers%20as%20if%20we%20are%20recursively%20denoise%0Afeatures%20step-by-step.%20This%20unifies%20the%20frameworks%20of%20feature%20extraction%20and%0Adenoising%2C%20where%20the%20former%20progressively%20embeds%20features%20from%20low-level%20to%0Ahigh-level%2C%20and%20the%20latter%20recursively%20denoises%20features%20step-by-step.%20After%0Athat%2C%20DenoiseRep%20fuses%20the%20parameters%20of%20feature%20extraction%20and%20denoising%0Alayers%2C%20and%20theoretically%20demonstrates%20its%20equivalence%20before%20and%20after%20the%0Afusion%2C%20thus%20making%20feature%20denoising%20computation-free.%20DenoiseRep%20is%20a%0Alabel-free%20algorithm%20that%20incrementally%20improves%20features%20but%20also%0Acomplementary%20to%20the%20label%20if%20available.%20Experimental%20results%20on%20various%0Adiscriminative%20vision%20tasks%2C%20including%20re-identification%20%28Market-1501%2C%0ADukeMTMC-reID%2C%20MSMT17%2C%20CUHK-03%2C%20vehicleID%29%2C%20image%20classification%20%28ImageNet%2C%0AUB200%2C%20Oxford-Pet%2C%20Flowers%29%2C%20object%20detection%20%28COCO%29%2C%20image%20segmentation%0A%28ADE20K%29%20show%20stability%20and%20impressive%20improvements.%20We%20also%20validate%20its%0Aeffectiveness%20on%20the%20CNN%20%28ResNet%29%20and%20Transformer%20%28ViT%2C%20Swin%2C%20Vmamda%29%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08773v3&entry.124074799=Read"},
{"title": "A survey on deep learning in medical image registration: new\n  technologies, uncertainty, evaluation metrics, and beyond", "author": "Junyu Chen and Yihao Liu and Shuwen Wei and Zhangxing Bian and Shalini Subramanian and Aaron Carass and Jerry L. Prince and Yong Du", "abstract": "  Deep learning technologies have dramatically reshaped the field of medical\nimage registration over the past decade. The initial developments, such as\nregression-based and U-Net-based networks, established the foundation for deep\nlearning in image registration. Subsequent progress has been made in various\naspects of deep learning-based registration, including similarity measures,\ndeformation regularizations, network architectures, and uncertainty estimation.\nThese advancements have not only enriched the field of image registration but\nhave also facilitated its application in a wide range of tasks, including atlas\nconstruction, multi-atlas segmentation, motion estimation, and 2D-3D\nregistration. In this paper, we present a comprehensive overview of the most\nrecent advancements in deep learning-based image registration. We begin with a\nconcise introduction to the core concepts of deep learning-based image\nregistration. Then, we delve into innovative network architectures, loss\nfunctions specific to registration, and methods for estimating registration\nuncertainty. Additionally, this paper explores appropriate evaluation metrics\nfor assessing the performance of deep learning models in registration tasks.\nFinally, we highlight the practical applications of these novel techniques in\nmedical imaging and discuss the future prospects of deep learning-based image\nregistration.\n", "link": "http://arxiv.org/abs/2307.15615v4", "date": "2024-11-01", "relevancy": 2.1321, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5447}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20registration%3A%20new%0A%20%20technologies%2C%20uncertainty%2C%20evaluation%20metrics%2C%20and%20beyond&body=Title%3A%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20registration%3A%20new%0A%20%20technologies%2C%20uncertainty%2C%20evaluation%20metrics%2C%20and%20beyond%0AAuthor%3A%20Junyu%20Chen%20and%20Yihao%20Liu%20and%20Shuwen%20Wei%20and%20Zhangxing%20Bian%20and%20Shalini%20Subramanian%20and%20Aaron%20Carass%20and%20Jerry%20L.%20Prince%20and%20Yong%20Du%0AAbstract%3A%20%20%20Deep%20learning%20technologies%20have%20dramatically%20reshaped%20the%20field%20of%20medical%0Aimage%20registration%20over%20the%20past%20decade.%20The%20initial%20developments%2C%20such%20as%0Aregression-based%20and%20U-Net-based%20networks%2C%20established%20the%20foundation%20for%20deep%0Alearning%20in%20image%20registration.%20Subsequent%20progress%20has%20been%20made%20in%20various%0Aaspects%20of%20deep%20learning-based%20registration%2C%20including%20similarity%20measures%2C%0Adeformation%20regularizations%2C%20network%20architectures%2C%20and%20uncertainty%20estimation.%0AThese%20advancements%20have%20not%20only%20enriched%20the%20field%20of%20image%20registration%20but%0Ahave%20also%20facilitated%20its%20application%20in%20a%20wide%20range%20of%20tasks%2C%20including%20atlas%0Aconstruction%2C%20multi-atlas%20segmentation%2C%20motion%20estimation%2C%20and%202D-3D%0Aregistration.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20overview%20of%20the%20most%0Arecent%20advancements%20in%20deep%20learning-based%20image%20registration.%20We%20begin%20with%20a%0Aconcise%20introduction%20to%20the%20core%20concepts%20of%20deep%20learning-based%20image%0Aregistration.%20Then%2C%20we%20delve%20into%20innovative%20network%20architectures%2C%20loss%0Afunctions%20specific%20to%20registration%2C%20and%20methods%20for%20estimating%20registration%0Auncertainty.%20Additionally%2C%20this%20paper%20explores%20appropriate%20evaluation%20metrics%0Afor%20assessing%20the%20performance%20of%20deep%20learning%20models%20in%20registration%20tasks.%0AFinally%2C%20we%20highlight%20the%20practical%20applications%20of%20these%20novel%20techniques%20in%0Amedical%20imaging%20and%20discuss%20the%20future%20prospects%20of%20deep%20learning-based%20image%0Aregistration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.15615v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520survey%2520on%2520deep%2520learning%2520in%2520medical%2520image%2520registration%253A%2520new%250A%2520%2520technologies%252C%2520uncertainty%252C%2520evaluation%2520metrics%252C%2520and%2520beyond%26entry.906535625%3DJunyu%2520Chen%2520and%2520Yihao%2520Liu%2520and%2520Shuwen%2520Wei%2520and%2520Zhangxing%2520Bian%2520and%2520Shalini%2520Subramanian%2520and%2520Aaron%2520Carass%2520and%2520Jerry%2520L.%2520Prince%2520and%2520Yong%2520Du%26entry.1292438233%3D%2520%2520Deep%2520learning%2520technologies%2520have%2520dramatically%2520reshaped%2520the%2520field%2520of%2520medical%250Aimage%2520registration%2520over%2520the%2520past%2520decade.%2520The%2520initial%2520developments%252C%2520such%2520as%250Aregression-based%2520and%2520U-Net-based%2520networks%252C%2520established%2520the%2520foundation%2520for%2520deep%250Alearning%2520in%2520image%2520registration.%2520Subsequent%2520progress%2520has%2520been%2520made%2520in%2520various%250Aaspects%2520of%2520deep%2520learning-based%2520registration%252C%2520including%2520similarity%2520measures%252C%250Adeformation%2520regularizations%252C%2520network%2520architectures%252C%2520and%2520uncertainty%2520estimation.%250AThese%2520advancements%2520have%2520not%2520only%2520enriched%2520the%2520field%2520of%2520image%2520registration%2520but%250Ahave%2520also%2520facilitated%2520its%2520application%2520in%2520a%2520wide%2520range%2520of%2520tasks%252C%2520including%2520atlas%250Aconstruction%252C%2520multi-atlas%2520segmentation%252C%2520motion%2520estimation%252C%2520and%25202D-3D%250Aregistration.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520overview%2520of%2520the%2520most%250Arecent%2520advancements%2520in%2520deep%2520learning-based%2520image%2520registration.%2520We%2520begin%2520with%2520a%250Aconcise%2520introduction%2520to%2520the%2520core%2520concepts%2520of%2520deep%2520learning-based%2520image%250Aregistration.%2520Then%252C%2520we%2520delve%2520into%2520innovative%2520network%2520architectures%252C%2520loss%250Afunctions%2520specific%2520to%2520registration%252C%2520and%2520methods%2520for%2520estimating%2520registration%250Auncertainty.%2520Additionally%252C%2520this%2520paper%2520explores%2520appropriate%2520evaluation%2520metrics%250Afor%2520assessing%2520the%2520performance%2520of%2520deep%2520learning%2520models%2520in%2520registration%2520tasks.%250AFinally%252C%2520we%2520highlight%2520the%2520practical%2520applications%2520of%2520these%2520novel%2520techniques%2520in%250Amedical%2520imaging%2520and%2520discuss%2520the%2520future%2520prospects%2520of%2520deep%2520learning-based%2520image%250Aregistration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.15615v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20survey%20on%20deep%20learning%20in%20medical%20image%20registration%3A%20new%0A%20%20technologies%2C%20uncertainty%2C%20evaluation%20metrics%2C%20and%20beyond&entry.906535625=Junyu%20Chen%20and%20Yihao%20Liu%20and%20Shuwen%20Wei%20and%20Zhangxing%20Bian%20and%20Shalini%20Subramanian%20and%20Aaron%20Carass%20and%20Jerry%20L.%20Prince%20and%20Yong%20Du&entry.1292438233=%20%20Deep%20learning%20technologies%20have%20dramatically%20reshaped%20the%20field%20of%20medical%0Aimage%20registration%20over%20the%20past%20decade.%20The%20initial%20developments%2C%20such%20as%0Aregression-based%20and%20U-Net-based%20networks%2C%20established%20the%20foundation%20for%20deep%0Alearning%20in%20image%20registration.%20Subsequent%20progress%20has%20been%20made%20in%20various%0Aaspects%20of%20deep%20learning-based%20registration%2C%20including%20similarity%20measures%2C%0Adeformation%20regularizations%2C%20network%20architectures%2C%20and%20uncertainty%20estimation.%0AThese%20advancements%20have%20not%20only%20enriched%20the%20field%20of%20image%20registration%20but%0Ahave%20also%20facilitated%20its%20application%20in%20a%20wide%20range%20of%20tasks%2C%20including%20atlas%0Aconstruction%2C%20multi-atlas%20segmentation%2C%20motion%20estimation%2C%20and%202D-3D%0Aregistration.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20overview%20of%20the%20most%0Arecent%20advancements%20in%20deep%20learning-based%20image%20registration.%20We%20begin%20with%20a%0Aconcise%20introduction%20to%20the%20core%20concepts%20of%20deep%20learning-based%20image%0Aregistration.%20Then%2C%20we%20delve%20into%20innovative%20network%20architectures%2C%20loss%0Afunctions%20specific%20to%20registration%2C%20and%20methods%20for%20estimating%20registration%0Auncertainty.%20Additionally%2C%20this%20paper%20explores%20appropriate%20evaluation%20metrics%0Afor%20assessing%20the%20performance%20of%20deep%20learning%20models%20in%20registration%20tasks.%0AFinally%2C%20we%20highlight%20the%20practical%20applications%20of%20these%20novel%20techniques%20in%0Amedical%20imaging%20and%20discuss%20the%20future%20prospects%20of%20deep%20learning-based%20image%0Aregistration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.15615v4&entry.124074799=Read"},
{"title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference", "author": "Jo\u00e3o Monteiro and \u00c9tienne Marcotte and Pierre-Andr\u00e9 No\u00ebl and Valentina Zantedeschi and David V\u00e1zquez and Nicolas Chapados and Christopher Pal and Perouz Taslakian", "abstract": "  In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.\n", "link": "http://arxiv.org/abs/2404.15420v3", "date": "2024-11-01", "relevancy": 2.1288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference&body=Title%3A%20XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference%0AAuthor%3A%20Jo%C3%A3o%20Monteiro%20and%20%C3%89tienne%20Marcotte%20and%20Pierre-Andr%C3%A9%20No%C3%ABl%20and%20Valentina%20Zantedeschi%20and%20David%20V%C3%A1zquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20approaches%20typically%20leverage%20prompting%20to%0Acondition%20decoder-only%20language%20model%20generation%20on%20reference%20information.%0AJust-in-time%20processing%20of%20a%20context%20is%20inefficient%20due%20to%20the%20quadratic%20cost%0Aof%20self-attention%20operations%2C%20and%20caching%20is%20desirable.%20However%2C%20caching%0Atransformer%20states%20can%20easily%20require%20almost%20as%20much%20space%20as%20the%20model%0Aparameters.%20When%20the%20right%20context%20isn%27t%20known%20in%20advance%2C%20caching%20ICL%20can%20be%0Achallenging.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20models%20that%2C%0Ainspired%20by%20the%20encoder-decoder%20architecture%2C%20use%20cross-attention%20to%20condition%0Ageneration%20on%20reference%20text%20without%20the%20prompt.%20More%20precisely%2C%20we%20leverage%0Apre-trained%20decoder-only%20models%20and%20only%20train%20a%20small%20number%20of%20added%20layers.%0AWe%20use%20Question-Answering%20%28QA%29%20as%20a%20testbed%20to%20evaluate%20the%20ability%20of%20our%0Amodels%20to%20perform%20conditional%20generation%20and%20observe%20that%20they%20outperform%20ICL%2C%0Aare%20comparable%20to%20fine-tuned%20prompted%20LLMs%2C%20and%20drastically%20reduce%20the%20space%0Afootprint%20relative%20to%20standard%20KV%20caching%20by%20two%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15420v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXC-Cache%253A%2520Cross-Attending%2520to%2520Cached%2520Context%2520for%2520Efficient%2520LLM%2520Inference%26entry.906535625%3DJo%25C3%25A3o%2520Monteiro%2520and%2520%25C3%2589tienne%2520Marcotte%2520and%2520Pierre-Andr%25C3%25A9%2520No%25C3%25ABl%2520and%2520Valentina%2520Zantedeschi%2520and%2520David%2520V%25C3%25A1zquez%2520and%2520Nicolas%2520Chapados%2520and%2520Christopher%2520Pal%2520and%2520Perouz%2520Taslakian%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520approaches%2520typically%2520leverage%2520prompting%2520to%250Acondition%2520decoder-only%2520language%2520model%2520generation%2520on%2520reference%2520information.%250AJust-in-time%2520processing%2520of%2520a%2520context%2520is%2520inefficient%2520due%2520to%2520the%2520quadratic%2520cost%250Aof%2520self-attention%2520operations%252C%2520and%2520caching%2520is%2520desirable.%2520However%252C%2520caching%250Atransformer%2520states%2520can%2520easily%2520require%2520almost%2520as%2520much%2520space%2520as%2520the%2520model%250Aparameters.%2520When%2520the%2520right%2520context%2520isn%2527t%2520known%2520in%2520advance%252C%2520caching%2520ICL%2520can%2520be%250Achallenging.%2520This%2520work%2520addresses%2520these%2520limitations%2520by%2520introducing%2520models%2520that%252C%250Ainspired%2520by%2520the%2520encoder-decoder%2520architecture%252C%2520use%2520cross-attention%2520to%2520condition%250Ageneration%2520on%2520reference%2520text%2520without%2520the%2520prompt.%2520More%2520precisely%252C%2520we%2520leverage%250Apre-trained%2520decoder-only%2520models%2520and%2520only%2520train%2520a%2520small%2520number%2520of%2520added%2520layers.%250AWe%2520use%2520Question-Answering%2520%2528QA%2529%2520as%2520a%2520testbed%2520to%2520evaluate%2520the%2520ability%2520of%2520our%250Amodels%2520to%2520perform%2520conditional%2520generation%2520and%2520observe%2520that%2520they%2520outperform%2520ICL%252C%250Aare%2520comparable%2520to%2520fine-tuned%2520prompted%2520LLMs%252C%2520and%2520drastically%2520reduce%2520the%2520space%250Afootprint%2520relative%2520to%2520standard%2520KV%2520caching%2520by%2520two%2520orders%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15420v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference&entry.906535625=Jo%C3%A3o%20Monteiro%20and%20%C3%89tienne%20Marcotte%20and%20Pierre-Andr%C3%A9%20No%C3%ABl%20and%20Valentina%20Zantedeschi%20and%20David%20V%C3%A1zquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20approaches%20typically%20leverage%20prompting%20to%0Acondition%20decoder-only%20language%20model%20generation%20on%20reference%20information.%0AJust-in-time%20processing%20of%20a%20context%20is%20inefficient%20due%20to%20the%20quadratic%20cost%0Aof%20self-attention%20operations%2C%20and%20caching%20is%20desirable.%20However%2C%20caching%0Atransformer%20states%20can%20easily%20require%20almost%20as%20much%20space%20as%20the%20model%0Aparameters.%20When%20the%20right%20context%20isn%27t%20known%20in%20advance%2C%20caching%20ICL%20can%20be%0Achallenging.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20models%20that%2C%0Ainspired%20by%20the%20encoder-decoder%20architecture%2C%20use%20cross-attention%20to%20condition%0Ageneration%20on%20reference%20text%20without%20the%20prompt.%20More%20precisely%2C%20we%20leverage%0Apre-trained%20decoder-only%20models%20and%20only%20train%20a%20small%20number%20of%20added%20layers.%0AWe%20use%20Question-Answering%20%28QA%29%20as%20a%20testbed%20to%20evaluate%20the%20ability%20of%20our%0Amodels%20to%20perform%20conditional%20generation%20and%20observe%20that%20they%20outperform%20ICL%2C%0Aare%20comparable%20to%20fine-tuned%20prompted%20LLMs%2C%20and%20drastically%20reduce%20the%20space%0Afootprint%20relative%20to%20standard%20KV%20caching%20by%20two%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15420v3&entry.124074799=Read"},
{"title": "Efficient Adversarial Training in LLMs with Continuous Attacks", "author": "Sophie Xhonneux and Alessandro Sordoni and Stephan G\u00fcnnemann and Gauthier Gidel and Leo Schwinn", "abstract": "  Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs.\n", "link": "http://arxiv.org/abs/2405.15589v3", "date": "2024-11-01", "relevancy": 2.1033, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks&body=Title%3A%20Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks%0AAuthor%3A%20Sophie%20Xhonneux%20and%20Alessandro%20Sordoni%20and%20Stephan%20G%C3%BCnnemann%20and%20Gauthier%20Gidel%20and%20Leo%20Schwinn%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20vulnerable%20to%20adversarial%20attacks%20that%20can%0Abypass%20their%20safety%20guardrails.%20In%20many%20domains%2C%20adversarial%20training%20has%0Aproven%20to%20be%20one%20of%20the%20most%20promising%20methods%20to%20reliably%20improve%20robustness%0Aagainst%20such%20attacks.%20Yet%2C%20in%20the%20context%20of%20LLMs%2C%20current%20methods%20for%0Aadversarial%20training%20are%20hindered%20by%20the%20high%20computational%20costs%20required%20to%0Aperform%20discrete%20adversarial%20attacks%20at%20each%20training%20iteration.%20We%20address%0Athis%20problem%20by%20instead%20calculating%20adversarial%20attacks%20in%20the%20continuous%0Aembedding%20space%20of%20the%20LLM%2C%20which%20is%20orders%20of%20magnitudes%20more%20efficient.%20We%0Apropose%20a%20fast%20adversarial%20training%20algorithm%20%28C-AdvUL%29%20composed%20of%20two%20losses%3A%0Athe%20first%20makes%20the%20model%20robust%20on%20continuous%20embedding%20attacks%20computed%20on%20an%0Aadversarial%20behaviour%20dataset%3B%20the%20second%20ensures%20the%20usefulness%20of%20the%20final%0Amodel%20by%20fine-tuning%20on%20utility%20data.%20Moreover%2C%20we%20introduce%20C-AdvIPO%2C%20an%0Aadversarial%20variant%20of%20IPO%20that%20does%20not%20require%20utility%20data%20for%20adversarially%0Arobust%20alignment.%20Our%20empirical%20evaluation%20on%20five%20models%20from%20different%0Afamilies%20%28Gemma%2C%20Phi3%2C%20Mistral%2C%20Zephyr%2C%20Llama2%29%20and%20at%20different%20scales%20%282B%2C%0A3.8B%2C%207B%29%20shows%20that%20both%20algorithms%20substantially%20enhance%20LLM%20robustness%0Aagainst%20discrete%20attacks%20%28GCG%2C%20AutoDAN%2C%20PAIR%29%2C%20while%20maintaining%20utility.%20Our%0Aresults%20demonstrate%20that%20robustness%20to%20continuous%20perturbations%20can%20extrapolate%0Ato%20discrete%20threat%20models.%20Thereby%2C%20we%20present%20a%20path%20toward%20scalable%0Aadversarial%20training%20algorithms%20for%20robustly%20aligning%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15589v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Adversarial%2520Training%2520in%2520LLMs%2520with%2520Continuous%2520Attacks%26entry.906535625%3DSophie%2520Xhonneux%2520and%2520Alessandro%2520Sordoni%2520and%2520Stephan%2520G%25C3%25BCnnemann%2520and%2520Gauthier%2520Gidel%2520and%2520Leo%2520Schwinn%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520vulnerable%2520to%2520adversarial%2520attacks%2520that%2520can%250Abypass%2520their%2520safety%2520guardrails.%2520In%2520many%2520domains%252C%2520adversarial%2520training%2520has%250Aproven%2520to%2520be%2520one%2520of%2520the%2520most%2520promising%2520methods%2520to%2520reliably%2520improve%2520robustness%250Aagainst%2520such%2520attacks.%2520Yet%252C%2520in%2520the%2520context%2520of%2520LLMs%252C%2520current%2520methods%2520for%250Aadversarial%2520training%2520are%2520hindered%2520by%2520the%2520high%2520computational%2520costs%2520required%2520to%250Aperform%2520discrete%2520adversarial%2520attacks%2520at%2520each%2520training%2520iteration.%2520We%2520address%250Athis%2520problem%2520by%2520instead%2520calculating%2520adversarial%2520attacks%2520in%2520the%2520continuous%250Aembedding%2520space%2520of%2520the%2520LLM%252C%2520which%2520is%2520orders%2520of%2520magnitudes%2520more%2520efficient.%2520We%250Apropose%2520a%2520fast%2520adversarial%2520training%2520algorithm%2520%2528C-AdvUL%2529%2520composed%2520of%2520two%2520losses%253A%250Athe%2520first%2520makes%2520the%2520model%2520robust%2520on%2520continuous%2520embedding%2520attacks%2520computed%2520on%2520an%250Aadversarial%2520behaviour%2520dataset%253B%2520the%2520second%2520ensures%2520the%2520usefulness%2520of%2520the%2520final%250Amodel%2520by%2520fine-tuning%2520on%2520utility%2520data.%2520Moreover%252C%2520we%2520introduce%2520C-AdvIPO%252C%2520an%250Aadversarial%2520variant%2520of%2520IPO%2520that%2520does%2520not%2520require%2520utility%2520data%2520for%2520adversarially%250Arobust%2520alignment.%2520Our%2520empirical%2520evaluation%2520on%2520five%2520models%2520from%2520different%250Afamilies%2520%2528Gemma%252C%2520Phi3%252C%2520Mistral%252C%2520Zephyr%252C%2520Llama2%2529%2520and%2520at%2520different%2520scales%2520%25282B%252C%250A3.8B%252C%25207B%2529%2520shows%2520that%2520both%2520algorithms%2520substantially%2520enhance%2520LLM%2520robustness%250Aagainst%2520discrete%2520attacks%2520%2528GCG%252C%2520AutoDAN%252C%2520PAIR%2529%252C%2520while%2520maintaining%2520utility.%2520Our%250Aresults%2520demonstrate%2520that%2520robustness%2520to%2520continuous%2520perturbations%2520can%2520extrapolate%250Ato%2520discrete%2520threat%2520models.%2520Thereby%252C%2520we%2520present%2520a%2520path%2520toward%2520scalable%250Aadversarial%2520training%2520algorithms%2520for%2520robustly%2520aligning%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15589v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Adversarial%20Training%20in%20LLMs%20with%20Continuous%20Attacks&entry.906535625=Sophie%20Xhonneux%20and%20Alessandro%20Sordoni%20and%20Stephan%20G%C3%BCnnemann%20and%20Gauthier%20Gidel%20and%20Leo%20Schwinn&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20vulnerable%20to%20adversarial%20attacks%20that%20can%0Abypass%20their%20safety%20guardrails.%20In%20many%20domains%2C%20adversarial%20training%20has%0Aproven%20to%20be%20one%20of%20the%20most%20promising%20methods%20to%20reliably%20improve%20robustness%0Aagainst%20such%20attacks.%20Yet%2C%20in%20the%20context%20of%20LLMs%2C%20current%20methods%20for%0Aadversarial%20training%20are%20hindered%20by%20the%20high%20computational%20costs%20required%20to%0Aperform%20discrete%20adversarial%20attacks%20at%20each%20training%20iteration.%20We%20address%0Athis%20problem%20by%20instead%20calculating%20adversarial%20attacks%20in%20the%20continuous%0Aembedding%20space%20of%20the%20LLM%2C%20which%20is%20orders%20of%20magnitudes%20more%20efficient.%20We%0Apropose%20a%20fast%20adversarial%20training%20algorithm%20%28C-AdvUL%29%20composed%20of%20two%20losses%3A%0Athe%20first%20makes%20the%20model%20robust%20on%20continuous%20embedding%20attacks%20computed%20on%20an%0Aadversarial%20behaviour%20dataset%3B%20the%20second%20ensures%20the%20usefulness%20of%20the%20final%0Amodel%20by%20fine-tuning%20on%20utility%20data.%20Moreover%2C%20we%20introduce%20C-AdvIPO%2C%20an%0Aadversarial%20variant%20of%20IPO%20that%20does%20not%20require%20utility%20data%20for%20adversarially%0Arobust%20alignment.%20Our%20empirical%20evaluation%20on%20five%20models%20from%20different%0Afamilies%20%28Gemma%2C%20Phi3%2C%20Mistral%2C%20Zephyr%2C%20Llama2%29%20and%20at%20different%20scales%20%282B%2C%0A3.8B%2C%207B%29%20shows%20that%20both%20algorithms%20substantially%20enhance%20LLM%20robustness%0Aagainst%20discrete%20attacks%20%28GCG%2C%20AutoDAN%2C%20PAIR%29%2C%20while%20maintaining%20utility.%20Our%0Aresults%20demonstrate%20that%20robustness%20to%20continuous%20perturbations%20can%20extrapolate%0Ato%20discrete%20threat%20models.%20Thereby%2C%20we%20present%20a%20path%20toward%20scalable%0Aadversarial%20training%20algorithms%20for%20robustly%20aligning%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15589v3&entry.124074799=Read"},
{"title": "Provable optimal transport with transformers: The essence of depth and\n  prompt engineering", "author": "Hadi Daneshmand", "abstract": "  Can we establish provable performance guarantees for transformers?\nEstablishing such theoretical guarantees is a milestone in developing\ntrustworthy generative AI. In this paper, we take a step toward addressing this\nquestion by focusing on optimal transport, a fundamental problem at the\nintersection of combinatorial and continuous optimization. Leveraging the\ncomputational power of attention layers, we prove that a transformer with fixed\nparameters can effectively solve the optimal transport problem in Wasserstein-2\nwith entropic regularization for an arbitrary number of points. Consequently,\nthe transformer can sort lists of arbitrary sizes up to an approximation\nfactor. Our results rely on an engineered prompt that enables the transformer\nto implement gradient descent with adaptive stepsizes on the dual optimal\ntransport. Combining the convergence analysis of gradient descent with Sinkhorn\ndynamics, we establish an explicit approximation bound for optimal transport\nwith transformers, which improves as depth increases. Our findings provide\nnovel insights into the essence of prompt engineering and depth for solving\noptimal transport. In particular, prompt engineering boosts the algorithmic\nexpressivity of transformers, allowing them implement an optimization method.\nWith increasing depth, transformers can simulate several iterations of gradient\ndescent.\n", "link": "http://arxiv.org/abs/2410.19931v2", "date": "2024-11-01", "relevancy": 2.0511, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5845}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5035}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20optimal%20transport%20with%20transformers%3A%20The%20essence%20of%20depth%20and%0A%20%20prompt%20engineering&body=Title%3A%20Provable%20optimal%20transport%20with%20transformers%3A%20The%20essence%20of%20depth%20and%0A%20%20prompt%20engineering%0AAuthor%3A%20Hadi%20Daneshmand%0AAbstract%3A%20%20%20Can%20we%20establish%20provable%20performance%20guarantees%20for%20transformers%3F%0AEstablishing%20such%20theoretical%20guarantees%20is%20a%20milestone%20in%20developing%0Atrustworthy%20generative%20AI.%20In%20this%20paper%2C%20we%20take%20a%20step%20toward%20addressing%20this%0Aquestion%20by%20focusing%20on%20optimal%20transport%2C%20a%20fundamental%20problem%20at%20the%0Aintersection%20of%20combinatorial%20and%20continuous%20optimization.%20Leveraging%20the%0Acomputational%20power%20of%20attention%20layers%2C%20we%20prove%20that%20a%20transformer%20with%20fixed%0Aparameters%20can%20effectively%20solve%20the%20optimal%20transport%20problem%20in%20Wasserstein-2%0Awith%20entropic%20regularization%20for%20an%20arbitrary%20number%20of%20points.%20Consequently%2C%0Athe%20transformer%20can%20sort%20lists%20of%20arbitrary%20sizes%20up%20to%20an%20approximation%0Afactor.%20Our%20results%20rely%20on%20an%20engineered%20prompt%20that%20enables%20the%20transformer%0Ato%20implement%20gradient%20descent%20with%20adaptive%20stepsizes%20on%20the%20dual%20optimal%0Atransport.%20Combining%20the%20convergence%20analysis%20of%20gradient%20descent%20with%20Sinkhorn%0Adynamics%2C%20we%20establish%20an%20explicit%20approximation%20bound%20for%20optimal%20transport%0Awith%20transformers%2C%20which%20improves%20as%20depth%20increases.%20Our%20findings%20provide%0Anovel%20insights%20into%20the%20essence%20of%20prompt%20engineering%20and%20depth%20for%20solving%0Aoptimal%20transport.%20In%20particular%2C%20prompt%20engineering%20boosts%20the%20algorithmic%0Aexpressivity%20of%20transformers%2C%20allowing%20them%20implement%20an%20optimization%20method.%0AWith%20increasing%20depth%2C%20transformers%20can%20simulate%20several%20iterations%20of%20gradient%0Adescent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520optimal%2520transport%2520with%2520transformers%253A%2520The%2520essence%2520of%2520depth%2520and%250A%2520%2520prompt%2520engineering%26entry.906535625%3DHadi%2520Daneshmand%26entry.1292438233%3D%2520%2520Can%2520we%2520establish%2520provable%2520performance%2520guarantees%2520for%2520transformers%253F%250AEstablishing%2520such%2520theoretical%2520guarantees%2520is%2520a%2520milestone%2520in%2520developing%250Atrustworthy%2520generative%2520AI.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520step%2520toward%2520addressing%2520this%250Aquestion%2520by%2520focusing%2520on%2520optimal%2520transport%252C%2520a%2520fundamental%2520problem%2520at%2520the%250Aintersection%2520of%2520combinatorial%2520and%2520continuous%2520optimization.%2520Leveraging%2520the%250Acomputational%2520power%2520of%2520attention%2520layers%252C%2520we%2520prove%2520that%2520a%2520transformer%2520with%2520fixed%250Aparameters%2520can%2520effectively%2520solve%2520the%2520optimal%2520transport%2520problem%2520in%2520Wasserstein-2%250Awith%2520entropic%2520regularization%2520for%2520an%2520arbitrary%2520number%2520of%2520points.%2520Consequently%252C%250Athe%2520transformer%2520can%2520sort%2520lists%2520of%2520arbitrary%2520sizes%2520up%2520to%2520an%2520approximation%250Afactor.%2520Our%2520results%2520rely%2520on%2520an%2520engineered%2520prompt%2520that%2520enables%2520the%2520transformer%250Ato%2520implement%2520gradient%2520descent%2520with%2520adaptive%2520stepsizes%2520on%2520the%2520dual%2520optimal%250Atransport.%2520Combining%2520the%2520convergence%2520analysis%2520of%2520gradient%2520descent%2520with%2520Sinkhorn%250Adynamics%252C%2520we%2520establish%2520an%2520explicit%2520approximation%2520bound%2520for%2520optimal%2520transport%250Awith%2520transformers%252C%2520which%2520improves%2520as%2520depth%2520increases.%2520Our%2520findings%2520provide%250Anovel%2520insights%2520into%2520the%2520essence%2520of%2520prompt%2520engineering%2520and%2520depth%2520for%2520solving%250Aoptimal%2520transport.%2520In%2520particular%252C%2520prompt%2520engineering%2520boosts%2520the%2520algorithmic%250Aexpressivity%2520of%2520transformers%252C%2520allowing%2520them%2520implement%2520an%2520optimization%2520method.%250AWith%2520increasing%2520depth%252C%2520transformers%2520can%2520simulate%2520several%2520iterations%2520of%2520gradient%250Adescent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20optimal%20transport%20with%20transformers%3A%20The%20essence%20of%20depth%20and%0A%20%20prompt%20engineering&entry.906535625=Hadi%20Daneshmand&entry.1292438233=%20%20Can%20we%20establish%20provable%20performance%20guarantees%20for%20transformers%3F%0AEstablishing%20such%20theoretical%20guarantees%20is%20a%20milestone%20in%20developing%0Atrustworthy%20generative%20AI.%20In%20this%20paper%2C%20we%20take%20a%20step%20toward%20addressing%20this%0Aquestion%20by%20focusing%20on%20optimal%20transport%2C%20a%20fundamental%20problem%20at%20the%0Aintersection%20of%20combinatorial%20and%20continuous%20optimization.%20Leveraging%20the%0Acomputational%20power%20of%20attention%20layers%2C%20we%20prove%20that%20a%20transformer%20with%20fixed%0Aparameters%20can%20effectively%20solve%20the%20optimal%20transport%20problem%20in%20Wasserstein-2%0Awith%20entropic%20regularization%20for%20an%20arbitrary%20number%20of%20points.%20Consequently%2C%0Athe%20transformer%20can%20sort%20lists%20of%20arbitrary%20sizes%20up%20to%20an%20approximation%0Afactor.%20Our%20results%20rely%20on%20an%20engineered%20prompt%20that%20enables%20the%20transformer%0Ato%20implement%20gradient%20descent%20with%20adaptive%20stepsizes%20on%20the%20dual%20optimal%0Atransport.%20Combining%20the%20convergence%20analysis%20of%20gradient%20descent%20with%20Sinkhorn%0Adynamics%2C%20we%20establish%20an%20explicit%20approximation%20bound%20for%20optimal%20transport%0Awith%20transformers%2C%20which%20improves%20as%20depth%20increases.%20Our%20findings%20provide%0Anovel%20insights%20into%20the%20essence%20of%20prompt%20engineering%20and%20depth%20for%20solving%0Aoptimal%20transport.%20In%20particular%2C%20prompt%20engineering%20boosts%20the%20algorithmic%0Aexpressivity%20of%20transformers%2C%20allowing%20them%20implement%20an%20optimization%20method.%0AWith%20increasing%20depth%2C%20transformers%20can%20simulate%20several%20iterations%20of%20gradient%0Adescent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19931v2&entry.124074799=Read"},
{"title": "CaptainCook4D: A Dataset for Understanding Errors in Procedural\n  Activities", "author": "Rohith Peddi and Shivvrat Arya and Bharath Challa and Likhitha Pallapothula and Akshay Vyas and Bhavya Gouripeddi and Jikai Wang and Qifan Zhang and Vasundhara Komaragiri and Eric Ragan and Nicholas Ruozzi and Yu Xiang and Vibhav Gogate", "abstract": "  Following step-by-step procedures is an essential component of various\nactivities carried out by individuals in their daily lives. These procedures\nserve as a guiding framework that helps to achieve goals efficiently, whether\nit is assembling furniture or preparing a recipe. However, the complexity and\nduration of procedural activities inherently increase the likelihood of making\nerrors. Understanding such procedural activities from a sequence of frames is a\nchallenging task that demands an accurate interpretation of visual information\nand the ability to reason about the structure of the activity. To this end, we\ncollect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings\n(94.5 hours) of people performing recipes in real kitchen environments. This\ndataset consists of two distinct types of activity: one in which participants\nadhere to the provided recipe instructions and another in which they deviate\nand induce errors. We provide 5.3K step annotations and 10K fine-grained action\nannotations and benchmark the dataset for the following tasks: supervised error\nrecognition, multistep localization, and procedure learning\n", "link": "http://arxiv.org/abs/2312.14556v3", "date": "2024-11-01", "relevancy": 2.0461, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaptainCook4D%3A%20A%20Dataset%20for%20Understanding%20Errors%20in%20Procedural%0A%20%20Activities&body=Title%3A%20CaptainCook4D%3A%20A%20Dataset%20for%20Understanding%20Errors%20in%20Procedural%0A%20%20Activities%0AAuthor%3A%20Rohith%20Peddi%20and%20Shivvrat%20Arya%20and%20Bharath%20Challa%20and%20Likhitha%20Pallapothula%20and%20Akshay%20Vyas%20and%20Bhavya%20Gouripeddi%20and%20Jikai%20Wang%20and%20Qifan%20Zhang%20and%20Vasundhara%20Komaragiri%20and%20Eric%20Ragan%20and%20Nicholas%20Ruozzi%20and%20Yu%20Xiang%20and%20Vibhav%20Gogate%0AAbstract%3A%20%20%20Following%20step-by-step%20procedures%20is%20an%20essential%20component%20of%20various%0Aactivities%20carried%20out%20by%20individuals%20in%20their%20daily%20lives.%20These%20procedures%0Aserve%20as%20a%20guiding%20framework%20that%20helps%20to%20achieve%20goals%20efficiently%2C%20whether%0Ait%20is%20assembling%20furniture%20or%20preparing%20a%20recipe.%20However%2C%20the%20complexity%20and%0Aduration%20of%20procedural%20activities%20inherently%20increase%20the%20likelihood%20of%20making%0Aerrors.%20Understanding%20such%20procedural%20activities%20from%20a%20sequence%20of%20frames%20is%20a%0Achallenging%20task%20that%20demands%20an%20accurate%20interpretation%20of%20visual%20information%0Aand%20the%20ability%20to%20reason%20about%20the%20structure%20of%20the%20activity.%20To%20this%20end%2C%20we%0Acollect%20a%20new%20egocentric%204D%20dataset%2C%20CaptainCook4D%2C%20comprising%20384%20recordings%0A%2894.5%20hours%29%20of%20people%20performing%20recipes%20in%20real%20kitchen%20environments.%20This%0Adataset%20consists%20of%20two%20distinct%20types%20of%20activity%3A%20one%20in%20which%20participants%0Aadhere%20to%20the%20provided%20recipe%20instructions%20and%20another%20in%20which%20they%20deviate%0Aand%20induce%20errors.%20We%20provide%205.3K%20step%20annotations%20and%2010K%20fine-grained%20action%0Aannotations%20and%20benchmark%20the%20dataset%20for%20the%20following%20tasks%3A%20supervised%20error%0Arecognition%2C%20multistep%20localization%2C%20and%20procedure%20learning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14556v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaptainCook4D%253A%2520A%2520Dataset%2520for%2520Understanding%2520Errors%2520in%2520Procedural%250A%2520%2520Activities%26entry.906535625%3DRohith%2520Peddi%2520and%2520Shivvrat%2520Arya%2520and%2520Bharath%2520Challa%2520and%2520Likhitha%2520Pallapothula%2520and%2520Akshay%2520Vyas%2520and%2520Bhavya%2520Gouripeddi%2520and%2520Jikai%2520Wang%2520and%2520Qifan%2520Zhang%2520and%2520Vasundhara%2520Komaragiri%2520and%2520Eric%2520Ragan%2520and%2520Nicholas%2520Ruozzi%2520and%2520Yu%2520Xiang%2520and%2520Vibhav%2520Gogate%26entry.1292438233%3D%2520%2520Following%2520step-by-step%2520procedures%2520is%2520an%2520essential%2520component%2520of%2520various%250Aactivities%2520carried%2520out%2520by%2520individuals%2520in%2520their%2520daily%2520lives.%2520These%2520procedures%250Aserve%2520as%2520a%2520guiding%2520framework%2520that%2520helps%2520to%2520achieve%2520goals%2520efficiently%252C%2520whether%250Ait%2520is%2520assembling%2520furniture%2520or%2520preparing%2520a%2520recipe.%2520However%252C%2520the%2520complexity%2520and%250Aduration%2520of%2520procedural%2520activities%2520inherently%2520increase%2520the%2520likelihood%2520of%2520making%250Aerrors.%2520Understanding%2520such%2520procedural%2520activities%2520from%2520a%2520sequence%2520of%2520frames%2520is%2520a%250Achallenging%2520task%2520that%2520demands%2520an%2520accurate%2520interpretation%2520of%2520visual%2520information%250Aand%2520the%2520ability%2520to%2520reason%2520about%2520the%2520structure%2520of%2520the%2520activity.%2520To%2520this%2520end%252C%2520we%250Acollect%2520a%2520new%2520egocentric%25204D%2520dataset%252C%2520CaptainCook4D%252C%2520comprising%2520384%2520recordings%250A%252894.5%2520hours%2529%2520of%2520people%2520performing%2520recipes%2520in%2520real%2520kitchen%2520environments.%2520This%250Adataset%2520consists%2520of%2520two%2520distinct%2520types%2520of%2520activity%253A%2520one%2520in%2520which%2520participants%250Aadhere%2520to%2520the%2520provided%2520recipe%2520instructions%2520and%2520another%2520in%2520which%2520they%2520deviate%250Aand%2520induce%2520errors.%2520We%2520provide%25205.3K%2520step%2520annotations%2520and%252010K%2520fine-grained%2520action%250Aannotations%2520and%2520benchmark%2520the%2520dataset%2520for%2520the%2520following%2520tasks%253A%2520supervised%2520error%250Arecognition%252C%2520multistep%2520localization%252C%2520and%2520procedure%2520learning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14556v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaptainCook4D%3A%20A%20Dataset%20for%20Understanding%20Errors%20in%20Procedural%0A%20%20Activities&entry.906535625=Rohith%20Peddi%20and%20Shivvrat%20Arya%20and%20Bharath%20Challa%20and%20Likhitha%20Pallapothula%20and%20Akshay%20Vyas%20and%20Bhavya%20Gouripeddi%20and%20Jikai%20Wang%20and%20Qifan%20Zhang%20and%20Vasundhara%20Komaragiri%20and%20Eric%20Ragan%20and%20Nicholas%20Ruozzi%20and%20Yu%20Xiang%20and%20Vibhav%20Gogate&entry.1292438233=%20%20Following%20step-by-step%20procedures%20is%20an%20essential%20component%20of%20various%0Aactivities%20carried%20out%20by%20individuals%20in%20their%20daily%20lives.%20These%20procedures%0Aserve%20as%20a%20guiding%20framework%20that%20helps%20to%20achieve%20goals%20efficiently%2C%20whether%0Ait%20is%20assembling%20furniture%20or%20preparing%20a%20recipe.%20However%2C%20the%20complexity%20and%0Aduration%20of%20procedural%20activities%20inherently%20increase%20the%20likelihood%20of%20making%0Aerrors.%20Understanding%20such%20procedural%20activities%20from%20a%20sequence%20of%20frames%20is%20a%0Achallenging%20task%20that%20demands%20an%20accurate%20interpretation%20of%20visual%20information%0Aand%20the%20ability%20to%20reason%20about%20the%20structure%20of%20the%20activity.%20To%20this%20end%2C%20we%0Acollect%20a%20new%20egocentric%204D%20dataset%2C%20CaptainCook4D%2C%20comprising%20384%20recordings%0A%2894.5%20hours%29%20of%20people%20performing%20recipes%20in%20real%20kitchen%20environments.%20This%0Adataset%20consists%20of%20two%20distinct%20types%20of%20activity%3A%20one%20in%20which%20participants%0Aadhere%20to%20the%20provided%20recipe%20instructions%20and%20another%20in%20which%20they%20deviate%0Aand%20induce%20errors.%20We%20provide%205.3K%20step%20annotations%20and%2010K%20fine-grained%20action%0Aannotations%20and%20benchmark%20the%20dataset%20for%20the%20following%20tasks%3A%20supervised%20error%0Arecognition%2C%20multistep%20localization%2C%20and%20procedure%20learning%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14556v3&entry.124074799=Read"},
{"title": "Tiny Learning-Based MPC for Multirotors: Solver-Aware Learning for\n  Efficient Embedded Predictive Control", "author": "Babak Akbari and Justin Frank and Melissa Greeff", "abstract": "  Tiny aerial robots show promise for applications like environmental\nmonitoring and search-and-rescue but face challenges in control due to their\nlimited computing power and complex dynamics. Model Predictive Control (MPC)\ncan achieve agile trajectory tracking and handle constraints. Although current\nlearning-based MPC methods, such as Gaussian Process (GP) MPC, improve control\nperformance by learning residual dynamics, they are computationally demanding,\nlimiting their onboard application on tiny robots. This paper introduces Tiny\nLearning-Based Model Predictive Control (LB MPC), a novel framework for\nresource-constrained micro multirotor platforms. By exploiting multirotor\ndynamics' structure and developing an efficient solver, our approach enables\nhigh-rate control at 100 Hz on a Crazyflie 2.1 with a Teensy 4.0\nmicrocontroller. We demonstrate a 23% average improvement in tracking\nperformance over existing embedded MPC methods, achieving the first onboard\nimplementation of learning-based MPC on a tiny multirotor (53 g).\n", "link": "http://arxiv.org/abs/2410.23634v2", "date": "2024-11-01", "relevancy": 2.0173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5381}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5172}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20Learning-Based%20MPC%20for%20Multirotors%3A%20Solver-Aware%20Learning%20for%0A%20%20Efficient%20Embedded%20Predictive%20Control&body=Title%3A%20Tiny%20Learning-Based%20MPC%20for%20Multirotors%3A%20Solver-Aware%20Learning%20for%0A%20%20Efficient%20Embedded%20Predictive%20Control%0AAuthor%3A%20Babak%20Akbari%20and%20Justin%20Frank%20and%20Melissa%20Greeff%0AAbstract%3A%20%20%20Tiny%20aerial%20robots%20show%20promise%20for%20applications%20like%20environmental%0Amonitoring%20and%20search-and-rescue%20but%20face%20challenges%20in%20control%20due%20to%20their%0Alimited%20computing%20power%20and%20complex%20dynamics.%20Model%20Predictive%20Control%20%28MPC%29%0Acan%20achieve%20agile%20trajectory%20tracking%20and%20handle%20constraints.%20Although%20current%0Alearning-based%20MPC%20methods%2C%20such%20as%20Gaussian%20Process%20%28GP%29%20MPC%2C%20improve%20control%0Aperformance%20by%20learning%20residual%20dynamics%2C%20they%20are%20computationally%20demanding%2C%0Alimiting%20their%20onboard%20application%20on%20tiny%20robots.%20This%20paper%20introduces%20Tiny%0ALearning-Based%20Model%20Predictive%20Control%20%28LB%20MPC%29%2C%20a%20novel%20framework%20for%0Aresource-constrained%20micro%20multirotor%20platforms.%20By%20exploiting%20multirotor%0Adynamics%27%20structure%20and%20developing%20an%20efficient%20solver%2C%20our%20approach%20enables%0Ahigh-rate%20control%20at%20100%20Hz%20on%20a%20Crazyflie%202.1%20with%20a%20Teensy%204.0%0Amicrocontroller.%20We%20demonstrate%20a%2023%25%20average%20improvement%20in%20tracking%0Aperformance%20over%20existing%20embedded%20MPC%20methods%2C%20achieving%20the%20first%20onboard%0Aimplementation%20of%20learning-based%20MPC%20on%20a%20tiny%20multirotor%20%2853%20g%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520Learning-Based%2520MPC%2520for%2520Multirotors%253A%2520Solver-Aware%2520Learning%2520for%250A%2520%2520Efficient%2520Embedded%2520Predictive%2520Control%26entry.906535625%3DBabak%2520Akbari%2520and%2520Justin%2520Frank%2520and%2520Melissa%2520Greeff%26entry.1292438233%3D%2520%2520Tiny%2520aerial%2520robots%2520show%2520promise%2520for%2520applications%2520like%2520environmental%250Amonitoring%2520and%2520search-and-rescue%2520but%2520face%2520challenges%2520in%2520control%2520due%2520to%2520their%250Alimited%2520computing%2520power%2520and%2520complex%2520dynamics.%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%250Acan%2520achieve%2520agile%2520trajectory%2520tracking%2520and%2520handle%2520constraints.%2520Although%2520current%250Alearning-based%2520MPC%2520methods%252C%2520such%2520as%2520Gaussian%2520Process%2520%2528GP%2529%2520MPC%252C%2520improve%2520control%250Aperformance%2520by%2520learning%2520residual%2520dynamics%252C%2520they%2520are%2520computationally%2520demanding%252C%250Alimiting%2520their%2520onboard%2520application%2520on%2520tiny%2520robots.%2520This%2520paper%2520introduces%2520Tiny%250ALearning-Based%2520Model%2520Predictive%2520Control%2520%2528LB%2520MPC%2529%252C%2520a%2520novel%2520framework%2520for%250Aresource-constrained%2520micro%2520multirotor%2520platforms.%2520By%2520exploiting%2520multirotor%250Adynamics%2527%2520structure%2520and%2520developing%2520an%2520efficient%2520solver%252C%2520our%2520approach%2520enables%250Ahigh-rate%2520control%2520at%2520100%2520Hz%2520on%2520a%2520Crazyflie%25202.1%2520with%2520a%2520Teensy%25204.0%250Amicrocontroller.%2520We%2520demonstrate%2520a%252023%2525%2520average%2520improvement%2520in%2520tracking%250Aperformance%2520over%2520existing%2520embedded%2520MPC%2520methods%252C%2520achieving%2520the%2520first%2520onboard%250Aimplementation%2520of%2520learning-based%2520MPC%2520on%2520a%2520tiny%2520multirotor%2520%252853%2520g%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20Learning-Based%20MPC%20for%20Multirotors%3A%20Solver-Aware%20Learning%20for%0A%20%20Efficient%20Embedded%20Predictive%20Control&entry.906535625=Babak%20Akbari%20and%20Justin%20Frank%20and%20Melissa%20Greeff&entry.1292438233=%20%20Tiny%20aerial%20robots%20show%20promise%20for%20applications%20like%20environmental%0Amonitoring%20and%20search-and-rescue%20but%20face%20challenges%20in%20control%20due%20to%20their%0Alimited%20computing%20power%20and%20complex%20dynamics.%20Model%20Predictive%20Control%20%28MPC%29%0Acan%20achieve%20agile%20trajectory%20tracking%20and%20handle%20constraints.%20Although%20current%0Alearning-based%20MPC%20methods%2C%20such%20as%20Gaussian%20Process%20%28GP%29%20MPC%2C%20improve%20control%0Aperformance%20by%20learning%20residual%20dynamics%2C%20they%20are%20computationally%20demanding%2C%0Alimiting%20their%20onboard%20application%20on%20tiny%20robots.%20This%20paper%20introduces%20Tiny%0ALearning-Based%20Model%20Predictive%20Control%20%28LB%20MPC%29%2C%20a%20novel%20framework%20for%0Aresource-constrained%20micro%20multirotor%20platforms.%20By%20exploiting%20multirotor%0Adynamics%27%20structure%20and%20developing%20an%20efficient%20solver%2C%20our%20approach%20enables%0Ahigh-rate%20control%20at%20100%20Hz%20on%20a%20Crazyflie%202.1%20with%20a%20Teensy%204.0%0Amicrocontroller.%20We%20demonstrate%20a%2023%25%20average%20improvement%20in%20tracking%0Aperformance%20over%20existing%20embedded%20MPC%20methods%2C%20achieving%20the%20first%20onboard%0Aimplementation%20of%20learning-based%20MPC%20on%20a%20tiny%20multirotor%20%2853%20g%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23634v2&entry.124074799=Read"},
{"title": "Scalable Training of Trustworthy and Energy-Efficient Predictive Graph\n  Foundation Models for Atomistic Materials Modeling: A Case Study with\n  HydraGNN", "author": "Massimiliano Lupo Pasini and Jong Youl Choi and Kshitij Mehta and Pei Zhang and David Rogers and Jonghyun Bae and Khaled Z. Ibrahim and Ashwin M. Aji and Karl W. Schulz and Jorda Polo and Prasanna Balaprakash", "abstract": "  We present our work on developing and training scalable, trustworthy, and\nenergy-efficient predictive graph foundation models (GFMs) using HydraGNN, a\nmulti-headed graph convolutional neural network architecture. HydraGNN expands\nthe boundaries of graph neural network (GNN) computations in both training\nscale and data diversity. It abstracts over message passing algorithms,\nallowing both reproduction of and comparison across algorithmic innovations\nthat define nearest-neighbor convolution in GNNs. This work discusses a series\nof optimizations that have allowed scaling up the GFMs training to tens of\nthousands of GPUs on datasets consisting of hundreds of millions of graphs. Our\nGFMs use multi-task learning (MTL) to simultaneously learn graph-level and\nnode-level properties of atomistic structures, such as energy and atomic\nforces. Using over 154 million atomistic structures for training, we illustrate\nthe performance of our approach along with the lessons learned on two\nstate-of-the-art United States Department of Energy (US-DOE) supercomputers,\nnamely the Perlmutter petascale system at the National Energy Research\nScientific Computing Center and the Frontier exascale system at Oak Ridge\nLeadership Computing Facility. The HydraGNN architecture enables the GFM to\nachieve near-linear strong scaling performance using more than 2,000 GPUs on\nPerlmutter and 16,000 GPUs on Frontier.\n", "link": "http://arxiv.org/abs/2406.12909v4", "date": "2024-11-01", "relevancy": 2.0086, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5216}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5104}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Training%20of%20Trustworthy%20and%20Energy-Efficient%20Predictive%20Graph%0A%20%20Foundation%20Models%20for%20Atomistic%20Materials%20Modeling%3A%20A%20Case%20Study%20with%0A%20%20HydraGNN&body=Title%3A%20Scalable%20Training%20of%20Trustworthy%20and%20Energy-Efficient%20Predictive%20Graph%0A%20%20Foundation%20Models%20for%20Atomistic%20Materials%20Modeling%3A%20A%20Case%20Study%20with%0A%20%20HydraGNN%0AAuthor%3A%20Massimiliano%20Lupo%20Pasini%20and%20Jong%20Youl%20Choi%20and%20Kshitij%20Mehta%20and%20Pei%20Zhang%20and%20David%20Rogers%20and%20Jonghyun%20Bae%20and%20Khaled%20Z.%20Ibrahim%20and%20Ashwin%20M.%20Aji%20and%20Karl%20W.%20Schulz%20and%20Jorda%20Polo%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20We%20present%20our%20work%20on%20developing%20and%20training%20scalable%2C%20trustworthy%2C%20and%0Aenergy-efficient%20predictive%20graph%20foundation%20models%20%28GFMs%29%20using%20HydraGNN%2C%20a%0Amulti-headed%20graph%20convolutional%20neural%20network%20architecture.%20HydraGNN%20expands%0Athe%20boundaries%20of%20graph%20neural%20network%20%28GNN%29%20computations%20in%20both%20training%0Ascale%20and%20data%20diversity.%20It%20abstracts%20over%20message%20passing%20algorithms%2C%0Aallowing%20both%20reproduction%20of%20and%20comparison%20across%20algorithmic%20innovations%0Athat%20define%20nearest-neighbor%20convolution%20in%20GNNs.%20This%20work%20discusses%20a%20series%0Aof%20optimizations%20that%20have%20allowed%20scaling%20up%20the%20GFMs%20training%20to%20tens%20of%0Athousands%20of%20GPUs%20on%20datasets%20consisting%20of%20hundreds%20of%20millions%20of%20graphs.%20Our%0AGFMs%20use%20multi-task%20learning%20%28MTL%29%20to%20simultaneously%20learn%20graph-level%20and%0Anode-level%20properties%20of%20atomistic%20structures%2C%20such%20as%20energy%20and%20atomic%0Aforces.%20Using%20over%20154%20million%20atomistic%20structures%20for%20training%2C%20we%20illustrate%0Athe%20performance%20of%20our%20approach%20along%20with%20the%20lessons%20learned%20on%20two%0Astate-of-the-art%20United%20States%20Department%20of%20Energy%20%28US-DOE%29%20supercomputers%2C%0Anamely%20the%20Perlmutter%20petascale%20system%20at%20the%20National%20Energy%20Research%0AScientific%20Computing%20Center%20and%20the%20Frontier%20exascale%20system%20at%20Oak%20Ridge%0ALeadership%20Computing%20Facility.%20The%20HydraGNN%20architecture%20enables%20the%20GFM%20to%0Aachieve%20near-linear%20strong%20scaling%20performance%20using%20more%20than%202%2C000%20GPUs%20on%0APerlmutter%20and%2016%2C000%20GPUs%20on%20Frontier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12909v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Training%2520of%2520Trustworthy%2520and%2520Energy-Efficient%2520Predictive%2520Graph%250A%2520%2520Foundation%2520Models%2520for%2520Atomistic%2520Materials%2520Modeling%253A%2520A%2520Case%2520Study%2520with%250A%2520%2520HydraGNN%26entry.906535625%3DMassimiliano%2520Lupo%2520Pasini%2520and%2520Jong%2520Youl%2520Choi%2520and%2520Kshitij%2520Mehta%2520and%2520Pei%2520Zhang%2520and%2520David%2520Rogers%2520and%2520Jonghyun%2520Bae%2520and%2520Khaled%2520Z.%2520Ibrahim%2520and%2520Ashwin%2520M.%2520Aji%2520and%2520Karl%2520W.%2520Schulz%2520and%2520Jorda%2520Polo%2520and%2520Prasanna%2520Balaprakash%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520work%2520on%2520developing%2520and%2520training%2520scalable%252C%2520trustworthy%252C%2520and%250Aenergy-efficient%2520predictive%2520graph%2520foundation%2520models%2520%2528GFMs%2529%2520using%2520HydraGNN%252C%2520a%250Amulti-headed%2520graph%2520convolutional%2520neural%2520network%2520architecture.%2520HydraGNN%2520expands%250Athe%2520boundaries%2520of%2520graph%2520neural%2520network%2520%2528GNN%2529%2520computations%2520in%2520both%2520training%250Ascale%2520and%2520data%2520diversity.%2520It%2520abstracts%2520over%2520message%2520passing%2520algorithms%252C%250Aallowing%2520both%2520reproduction%2520of%2520and%2520comparison%2520across%2520algorithmic%2520innovations%250Athat%2520define%2520nearest-neighbor%2520convolution%2520in%2520GNNs.%2520This%2520work%2520discusses%2520a%2520series%250Aof%2520optimizations%2520that%2520have%2520allowed%2520scaling%2520up%2520the%2520GFMs%2520training%2520to%2520tens%2520of%250Athousands%2520of%2520GPUs%2520on%2520datasets%2520consisting%2520of%2520hundreds%2520of%2520millions%2520of%2520graphs.%2520Our%250AGFMs%2520use%2520multi-task%2520learning%2520%2528MTL%2529%2520to%2520simultaneously%2520learn%2520graph-level%2520and%250Anode-level%2520properties%2520of%2520atomistic%2520structures%252C%2520such%2520as%2520energy%2520and%2520atomic%250Aforces.%2520Using%2520over%2520154%2520million%2520atomistic%2520structures%2520for%2520training%252C%2520we%2520illustrate%250Athe%2520performance%2520of%2520our%2520approach%2520along%2520with%2520the%2520lessons%2520learned%2520on%2520two%250Astate-of-the-art%2520United%2520States%2520Department%2520of%2520Energy%2520%2528US-DOE%2529%2520supercomputers%252C%250Anamely%2520the%2520Perlmutter%2520petascale%2520system%2520at%2520the%2520National%2520Energy%2520Research%250AScientific%2520Computing%2520Center%2520and%2520the%2520Frontier%2520exascale%2520system%2520at%2520Oak%2520Ridge%250ALeadership%2520Computing%2520Facility.%2520The%2520HydraGNN%2520architecture%2520enables%2520the%2520GFM%2520to%250Aachieve%2520near-linear%2520strong%2520scaling%2520performance%2520using%2520more%2520than%25202%252C000%2520GPUs%2520on%250APerlmutter%2520and%252016%252C000%2520GPUs%2520on%2520Frontier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12909v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Training%20of%20Trustworthy%20and%20Energy-Efficient%20Predictive%20Graph%0A%20%20Foundation%20Models%20for%20Atomistic%20Materials%20Modeling%3A%20A%20Case%20Study%20with%0A%20%20HydraGNN&entry.906535625=Massimiliano%20Lupo%20Pasini%20and%20Jong%20Youl%20Choi%20and%20Kshitij%20Mehta%20and%20Pei%20Zhang%20and%20David%20Rogers%20and%20Jonghyun%20Bae%20and%20Khaled%20Z.%20Ibrahim%20and%20Ashwin%20M.%20Aji%20and%20Karl%20W.%20Schulz%20and%20Jorda%20Polo%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20We%20present%20our%20work%20on%20developing%20and%20training%20scalable%2C%20trustworthy%2C%20and%0Aenergy-efficient%20predictive%20graph%20foundation%20models%20%28GFMs%29%20using%20HydraGNN%2C%20a%0Amulti-headed%20graph%20convolutional%20neural%20network%20architecture.%20HydraGNN%20expands%0Athe%20boundaries%20of%20graph%20neural%20network%20%28GNN%29%20computations%20in%20both%20training%0Ascale%20and%20data%20diversity.%20It%20abstracts%20over%20message%20passing%20algorithms%2C%0Aallowing%20both%20reproduction%20of%20and%20comparison%20across%20algorithmic%20innovations%0Athat%20define%20nearest-neighbor%20convolution%20in%20GNNs.%20This%20work%20discusses%20a%20series%0Aof%20optimizations%20that%20have%20allowed%20scaling%20up%20the%20GFMs%20training%20to%20tens%20of%0Athousands%20of%20GPUs%20on%20datasets%20consisting%20of%20hundreds%20of%20millions%20of%20graphs.%20Our%0AGFMs%20use%20multi-task%20learning%20%28MTL%29%20to%20simultaneously%20learn%20graph-level%20and%0Anode-level%20properties%20of%20atomistic%20structures%2C%20such%20as%20energy%20and%20atomic%0Aforces.%20Using%20over%20154%20million%20atomistic%20structures%20for%20training%2C%20we%20illustrate%0Athe%20performance%20of%20our%20approach%20along%20with%20the%20lessons%20learned%20on%20two%0Astate-of-the-art%20United%20States%20Department%20of%20Energy%20%28US-DOE%29%20supercomputers%2C%0Anamely%20the%20Perlmutter%20petascale%20system%20at%20the%20National%20Energy%20Research%0AScientific%20Computing%20Center%20and%20the%20Frontier%20exascale%20system%20at%20Oak%20Ridge%0ALeadership%20Computing%20Facility.%20The%20HydraGNN%20architecture%20enables%20the%20GFM%20to%0Aachieve%20near-linear%20strong%20scaling%20performance%20using%20more%20than%202%2C000%20GPUs%20on%0APerlmutter%20and%2016%2C000%20GPUs%20on%20Frontier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12909v4&entry.124074799=Read"},
{"title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs", "author": "Qizhang Li and Yiwen Guo and Wangmeng Zuo and Hao Chen", "abstract": "  Adversarial prompts generated using gradient-based methods exhibit\noutstanding performance in performing automatic jailbreak attacks against\nsafety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the\ninput gradient of LLMs struggles to precisely reflect the magnitude of loss\nchange that results from token replacements in the prompt, leading to limited\nattack success rates against safety-aligned LLMs, even in the white-box\nsetting. In this paper, we explore a new perspective on this problem,\nsuggesting that it can be alleviated by leveraging innovations inspired in\ntransfer-based attacks that were originally proposed for attacking black-box\nimage classification models. For the first time, we appropriate the ideologies\nof effective methods among these transfer-based attacks, i.e., Skip Gradient\nMethod and Intermediate Level Attack, into gradient-based adversarial prompt\ngeneration and achieve significant performance gains without introducing\nobvious computational cost. Meanwhile, by discussing mechanisms behind the\ngains, new insights are drawn, and proper combinations of these methods are\nalso developed. Our empirical results show that 87% of the query-specific\nadversarial suffixes generated by the developed combination can induce\nLlama-2-7B-Chat to produce the output that exactly matches the target string on\nAdvBench. This match rate is 33% higher than that of a very strong baseline\nknown as GCG, demonstrating advanced discrete optimization for adversarial\nprompt generation against LLMs. In addition, without introducing obvious cost,\nthe combination achieves >30% absolute increase in attack success rates\ncompared with GCG when generating both query-specific (38% -> 68%) and\nuniversal adversarial prompts (26.68% -> 60.32%) for attacking the\nLlama-2-7B-Chat model on AdvBench. Code at:\nhttps://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.\n", "link": "http://arxiv.org/abs/2405.20778v2", "date": "2024-11-01", "relevancy": 1.99, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5108}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4919}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Generation%20of%20Adversarial%20Examples%20Against%20Safety-aligned%20LLMs&body=Title%3A%20Improved%20Generation%20of%20Adversarial%20Examples%20Against%20Safety-aligned%20LLMs%0AAuthor%3A%20Qizhang%20Li%20and%20Yiwen%20Guo%20and%20Wangmeng%20Zuo%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Adversarial%20prompts%20generated%20using%20gradient-based%20methods%20exhibit%0Aoutstanding%20performance%20in%20performing%20automatic%20jailbreak%20attacks%20against%0Asafety-aligned%20LLMs.%20Nevertheless%2C%20due%20to%20the%20discrete%20nature%20of%20texts%2C%20the%0Ainput%20gradient%20of%20LLMs%20struggles%20to%20precisely%20reflect%20the%20magnitude%20of%20loss%0Achange%20that%20results%20from%20token%20replacements%20in%20the%20prompt%2C%20leading%20to%20limited%0Aattack%20success%20rates%20against%20safety-aligned%20LLMs%2C%20even%20in%20the%20white-box%0Asetting.%20In%20this%20paper%2C%20we%20explore%20a%20new%20perspective%20on%20this%20problem%2C%0Asuggesting%20that%20it%20can%20be%20alleviated%20by%20leveraging%20innovations%20inspired%20in%0Atransfer-based%20attacks%20that%20were%20originally%20proposed%20for%20attacking%20black-box%0Aimage%20classification%20models.%20For%20the%20first%20time%2C%20we%20appropriate%20the%20ideologies%0Aof%20effective%20methods%20among%20these%20transfer-based%20attacks%2C%20i.e.%2C%20Skip%20Gradient%0AMethod%20and%20Intermediate%20Level%20Attack%2C%20into%20gradient-based%20adversarial%20prompt%0Ageneration%20and%20achieve%20significant%20performance%20gains%20without%20introducing%0Aobvious%20computational%20cost.%20Meanwhile%2C%20by%20discussing%20mechanisms%20behind%20the%0Agains%2C%20new%20insights%20are%20drawn%2C%20and%20proper%20combinations%20of%20these%20methods%20are%0Aalso%20developed.%20Our%20empirical%20results%20show%20that%2087%25%20of%20the%20query-specific%0Aadversarial%20suffixes%20generated%20by%20the%20developed%20combination%20can%20induce%0ALlama-2-7B-Chat%20to%20produce%20the%20output%20that%20exactly%20matches%20the%20target%20string%20on%0AAdvBench.%20This%20match%20rate%20is%2033%25%20higher%20than%20that%20of%20a%20very%20strong%20baseline%0Aknown%20as%20GCG%2C%20demonstrating%20advanced%20discrete%20optimization%20for%20adversarial%0Aprompt%20generation%20against%20LLMs.%20In%20addition%2C%20without%20introducing%20obvious%20cost%2C%0Athe%20combination%20achieves%20%3E30%25%20absolute%20increase%20in%20attack%20success%20rates%0Acompared%20with%20GCG%20when%20generating%20both%20query-specific%20%2838%25%20-%3E%2068%25%29%20and%0Auniversal%20adversarial%20prompts%20%2826.68%25%20-%3E%2060.32%25%29%20for%20attacking%20the%0ALlama-2-7B-Chat%20model%20on%20AdvBench.%20Code%20at%3A%0Ahttps%3A//github.com/qizhangli/Gradient-based-Jailbreak-Attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Generation%2520of%2520Adversarial%2520Examples%2520Against%2520Safety-aligned%2520LLMs%26entry.906535625%3DQizhang%2520Li%2520and%2520Yiwen%2520Guo%2520and%2520Wangmeng%2520Zuo%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Adversarial%2520prompts%2520generated%2520using%2520gradient-based%2520methods%2520exhibit%250Aoutstanding%2520performance%2520in%2520performing%2520automatic%2520jailbreak%2520attacks%2520against%250Asafety-aligned%2520LLMs.%2520Nevertheless%252C%2520due%2520to%2520the%2520discrete%2520nature%2520of%2520texts%252C%2520the%250Ainput%2520gradient%2520of%2520LLMs%2520struggles%2520to%2520precisely%2520reflect%2520the%2520magnitude%2520of%2520loss%250Achange%2520that%2520results%2520from%2520token%2520replacements%2520in%2520the%2520prompt%252C%2520leading%2520to%2520limited%250Aattack%2520success%2520rates%2520against%2520safety-aligned%2520LLMs%252C%2520even%2520in%2520the%2520white-box%250Asetting.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520new%2520perspective%2520on%2520this%2520problem%252C%250Asuggesting%2520that%2520it%2520can%2520be%2520alleviated%2520by%2520leveraging%2520innovations%2520inspired%2520in%250Atransfer-based%2520attacks%2520that%2520were%2520originally%2520proposed%2520for%2520attacking%2520black-box%250Aimage%2520classification%2520models.%2520For%2520the%2520first%2520time%252C%2520we%2520appropriate%2520the%2520ideologies%250Aof%2520effective%2520methods%2520among%2520these%2520transfer-based%2520attacks%252C%2520i.e.%252C%2520Skip%2520Gradient%250AMethod%2520and%2520Intermediate%2520Level%2520Attack%252C%2520into%2520gradient-based%2520adversarial%2520prompt%250Ageneration%2520and%2520achieve%2520significant%2520performance%2520gains%2520without%2520introducing%250Aobvious%2520computational%2520cost.%2520Meanwhile%252C%2520by%2520discussing%2520mechanisms%2520behind%2520the%250Agains%252C%2520new%2520insights%2520are%2520drawn%252C%2520and%2520proper%2520combinations%2520of%2520these%2520methods%2520are%250Aalso%2520developed.%2520Our%2520empirical%2520results%2520show%2520that%252087%2525%2520of%2520the%2520query-specific%250Aadversarial%2520suffixes%2520generated%2520by%2520the%2520developed%2520combination%2520can%2520induce%250ALlama-2-7B-Chat%2520to%2520produce%2520the%2520output%2520that%2520exactly%2520matches%2520the%2520target%2520string%2520on%250AAdvBench.%2520This%2520match%2520rate%2520is%252033%2525%2520higher%2520than%2520that%2520of%2520a%2520very%2520strong%2520baseline%250Aknown%2520as%2520GCG%252C%2520demonstrating%2520advanced%2520discrete%2520optimization%2520for%2520adversarial%250Aprompt%2520generation%2520against%2520LLMs.%2520In%2520addition%252C%2520without%2520introducing%2520obvious%2520cost%252C%250Athe%2520combination%2520achieves%2520%253E30%2525%2520absolute%2520increase%2520in%2520attack%2520success%2520rates%250Acompared%2520with%2520GCG%2520when%2520generating%2520both%2520query-specific%2520%252838%2525%2520-%253E%252068%2525%2529%2520and%250Auniversal%2520adversarial%2520prompts%2520%252826.68%2525%2520-%253E%252060.32%2525%2529%2520for%2520attacking%2520the%250ALlama-2-7B-Chat%2520model%2520on%2520AdvBench.%2520Code%2520at%253A%250Ahttps%253A//github.com/qizhangli/Gradient-based-Jailbreak-Attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Generation%20of%20Adversarial%20Examples%20Against%20Safety-aligned%20LLMs&entry.906535625=Qizhang%20Li%20and%20Yiwen%20Guo%20and%20Wangmeng%20Zuo%20and%20Hao%20Chen&entry.1292438233=%20%20Adversarial%20prompts%20generated%20using%20gradient-based%20methods%20exhibit%0Aoutstanding%20performance%20in%20performing%20automatic%20jailbreak%20attacks%20against%0Asafety-aligned%20LLMs.%20Nevertheless%2C%20due%20to%20the%20discrete%20nature%20of%20texts%2C%20the%0Ainput%20gradient%20of%20LLMs%20struggles%20to%20precisely%20reflect%20the%20magnitude%20of%20loss%0Achange%20that%20results%20from%20token%20replacements%20in%20the%20prompt%2C%20leading%20to%20limited%0Aattack%20success%20rates%20against%20safety-aligned%20LLMs%2C%20even%20in%20the%20white-box%0Asetting.%20In%20this%20paper%2C%20we%20explore%20a%20new%20perspective%20on%20this%20problem%2C%0Asuggesting%20that%20it%20can%20be%20alleviated%20by%20leveraging%20innovations%20inspired%20in%0Atransfer-based%20attacks%20that%20were%20originally%20proposed%20for%20attacking%20black-box%0Aimage%20classification%20models.%20For%20the%20first%20time%2C%20we%20appropriate%20the%20ideologies%0Aof%20effective%20methods%20among%20these%20transfer-based%20attacks%2C%20i.e.%2C%20Skip%20Gradient%0AMethod%20and%20Intermediate%20Level%20Attack%2C%20into%20gradient-based%20adversarial%20prompt%0Ageneration%20and%20achieve%20significant%20performance%20gains%20without%20introducing%0Aobvious%20computational%20cost.%20Meanwhile%2C%20by%20discussing%20mechanisms%20behind%20the%0Agains%2C%20new%20insights%20are%20drawn%2C%20and%20proper%20combinations%20of%20these%20methods%20are%0Aalso%20developed.%20Our%20empirical%20results%20show%20that%2087%25%20of%20the%20query-specific%0Aadversarial%20suffixes%20generated%20by%20the%20developed%20combination%20can%20induce%0ALlama-2-7B-Chat%20to%20produce%20the%20output%20that%20exactly%20matches%20the%20target%20string%20on%0AAdvBench.%20This%20match%20rate%20is%2033%25%20higher%20than%20that%20of%20a%20very%20strong%20baseline%0Aknown%20as%20GCG%2C%20demonstrating%20advanced%20discrete%20optimization%20for%20adversarial%0Aprompt%20generation%20against%20LLMs.%20In%20addition%2C%20without%20introducing%20obvious%20cost%2C%0Athe%20combination%20achieves%20%3E30%25%20absolute%20increase%20in%20attack%20success%20rates%0Acompared%20with%20GCG%20when%20generating%20both%20query-specific%20%2838%25%20-%3E%2068%25%29%20and%0Auniversal%20adversarial%20prompts%20%2826.68%25%20-%3E%2060.32%25%29%20for%20attacking%20the%0ALlama-2-7B-Chat%20model%20on%20AdvBench.%20Code%20at%3A%0Ahttps%3A//github.com/qizhangli/Gradient-based-Jailbreak-Attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20778v2&entry.124074799=Read"},
{"title": "ConvBKI: Real-Time Probabilistic Semantic Mapping Network with\n  Quantifiable Uncertainty", "author": "Joey Wilson and Yuewei Fu and Joshua Friesen and Parker Ewen and Andrew Capodieci and Paramsothy Jayakumar and Kira Barton and Maani Ghaffari", "abstract": "  In this paper, we develop a modular neural network for real-time\n{\\color{black}(> 10 Hz)} semantic mapping in uncertain environments, which\nexplicitly updates per-voxel probabilistic distributions within a neural\nnetwork layer. Our approach combines the reliability of classical probabilistic\nalgorithms with the performance and efficiency of modern neural networks.\nAlthough robotic perception is often divided between modern differentiable\nmethods and classical explicit methods, a union of both is necessary for\nreal-time and trustworthy performance. We introduce a novel Convolutional\nBayesian Kernel Inference (ConvBKI) layer which incorporates semantic\nsegmentation predictions online into a 3D map through a depthwise convolution\nlayer by leveraging conjugate priors. We compare ConvBKI against\nstate-of-the-art deep learning approaches and probabilistic algorithms for\nmapping to evaluate reliability and performance. We also create a Robot\nOperating System (ROS) package of ConvBKI and test it on real-world\nperceptually challenging off-road driving data.\n", "link": "http://arxiv.org/abs/2310.16020v3", "date": "2024-11-01", "relevancy": 1.9733, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6976}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6788}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvBKI%3A%20Real-Time%20Probabilistic%20Semantic%20Mapping%20Network%20with%0A%20%20Quantifiable%20Uncertainty&body=Title%3A%20ConvBKI%3A%20Real-Time%20Probabilistic%20Semantic%20Mapping%20Network%20with%0A%20%20Quantifiable%20Uncertainty%0AAuthor%3A%20Joey%20Wilson%20and%20Yuewei%20Fu%20and%20Joshua%20Friesen%20and%20Parker%20Ewen%20and%20Andrew%20Capodieci%20and%20Paramsothy%20Jayakumar%20and%20Kira%20Barton%20and%20Maani%20Ghaffari%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20a%20modular%20neural%20network%20for%20real-time%0A%7B%5Ccolor%7Bblack%7D%28%3E%2010%20Hz%29%7D%20semantic%20mapping%20in%20uncertain%20environments%2C%20which%0Aexplicitly%20updates%20per-voxel%20probabilistic%20distributions%20within%20a%20neural%0Anetwork%20layer.%20Our%20approach%20combines%20the%20reliability%20of%20classical%20probabilistic%0Aalgorithms%20with%20the%20performance%20and%20efficiency%20of%20modern%20neural%20networks.%0AAlthough%20robotic%20perception%20is%20often%20divided%20between%20modern%20differentiable%0Amethods%20and%20classical%20explicit%20methods%2C%20a%20union%20of%20both%20is%20necessary%20for%0Areal-time%20and%20trustworthy%20performance.%20We%20introduce%20a%20novel%20Convolutional%0ABayesian%20Kernel%20Inference%20%28ConvBKI%29%20layer%20which%20incorporates%20semantic%0Asegmentation%20predictions%20online%20into%20a%203D%20map%20through%20a%20depthwise%20convolution%0Alayer%20by%20leveraging%20conjugate%20priors.%20We%20compare%20ConvBKI%20against%0Astate-of-the-art%20deep%20learning%20approaches%20and%20probabilistic%20algorithms%20for%0Amapping%20to%20evaluate%20reliability%20and%20performance.%20We%20also%20create%20a%20Robot%0AOperating%20System%20%28ROS%29%20package%20of%20ConvBKI%20and%20test%20it%20on%20real-world%0Aperceptually%20challenging%20off-road%20driving%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16020v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvBKI%253A%2520Real-Time%2520Probabilistic%2520Semantic%2520Mapping%2520Network%2520with%250A%2520%2520Quantifiable%2520Uncertainty%26entry.906535625%3DJoey%2520Wilson%2520and%2520Yuewei%2520Fu%2520and%2520Joshua%2520Friesen%2520and%2520Parker%2520Ewen%2520and%2520Andrew%2520Capodieci%2520and%2520Paramsothy%2520Jayakumar%2520and%2520Kira%2520Barton%2520and%2520Maani%2520Ghaffari%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520modular%2520neural%2520network%2520for%2520real-time%250A%257B%255Ccolor%257Bblack%257D%2528%253E%252010%2520Hz%2529%257D%2520semantic%2520mapping%2520in%2520uncertain%2520environments%252C%2520which%250Aexplicitly%2520updates%2520per-voxel%2520probabilistic%2520distributions%2520within%2520a%2520neural%250Anetwork%2520layer.%2520Our%2520approach%2520combines%2520the%2520reliability%2520of%2520classical%2520probabilistic%250Aalgorithms%2520with%2520the%2520performance%2520and%2520efficiency%2520of%2520modern%2520neural%2520networks.%250AAlthough%2520robotic%2520perception%2520is%2520often%2520divided%2520between%2520modern%2520differentiable%250Amethods%2520and%2520classical%2520explicit%2520methods%252C%2520a%2520union%2520of%2520both%2520is%2520necessary%2520for%250Areal-time%2520and%2520trustworthy%2520performance.%2520We%2520introduce%2520a%2520novel%2520Convolutional%250ABayesian%2520Kernel%2520Inference%2520%2528ConvBKI%2529%2520layer%2520which%2520incorporates%2520semantic%250Asegmentation%2520predictions%2520online%2520into%2520a%25203D%2520map%2520through%2520a%2520depthwise%2520convolution%250Alayer%2520by%2520leveraging%2520conjugate%2520priors.%2520We%2520compare%2520ConvBKI%2520against%250Astate-of-the-art%2520deep%2520learning%2520approaches%2520and%2520probabilistic%2520algorithms%2520for%250Amapping%2520to%2520evaluate%2520reliability%2520and%2520performance.%2520We%2520also%2520create%2520a%2520Robot%250AOperating%2520System%2520%2528ROS%2529%2520package%2520of%2520ConvBKI%2520and%2520test%2520it%2520on%2520real-world%250Aperceptually%2520challenging%2520off-road%2520driving%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16020v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvBKI%3A%20Real-Time%20Probabilistic%20Semantic%20Mapping%20Network%20with%0A%20%20Quantifiable%20Uncertainty&entry.906535625=Joey%20Wilson%20and%20Yuewei%20Fu%20and%20Joshua%20Friesen%20and%20Parker%20Ewen%20and%20Andrew%20Capodieci%20and%20Paramsothy%20Jayakumar%20and%20Kira%20Barton%20and%20Maani%20Ghaffari&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20a%20modular%20neural%20network%20for%20real-time%0A%7B%5Ccolor%7Bblack%7D%28%3E%2010%20Hz%29%7D%20semantic%20mapping%20in%20uncertain%20environments%2C%20which%0Aexplicitly%20updates%20per-voxel%20probabilistic%20distributions%20within%20a%20neural%0Anetwork%20layer.%20Our%20approach%20combines%20the%20reliability%20of%20classical%20probabilistic%0Aalgorithms%20with%20the%20performance%20and%20efficiency%20of%20modern%20neural%20networks.%0AAlthough%20robotic%20perception%20is%20often%20divided%20between%20modern%20differentiable%0Amethods%20and%20classical%20explicit%20methods%2C%20a%20union%20of%20both%20is%20necessary%20for%0Areal-time%20and%20trustworthy%20performance.%20We%20introduce%20a%20novel%20Convolutional%0ABayesian%20Kernel%20Inference%20%28ConvBKI%29%20layer%20which%20incorporates%20semantic%0Asegmentation%20predictions%20online%20into%20a%203D%20map%20through%20a%20depthwise%20convolution%0Alayer%20by%20leveraging%20conjugate%20priors.%20We%20compare%20ConvBKI%20against%0Astate-of-the-art%20deep%20learning%20approaches%20and%20probabilistic%20algorithms%20for%0Amapping%20to%20evaluate%20reliability%20and%20performance.%20We%20also%20create%20a%20Robot%0AOperating%20System%20%28ROS%29%20package%20of%20ConvBKI%20and%20test%20it%20on%20real-world%0Aperceptually%20challenging%20off-road%20driving%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16020v3&entry.124074799=Read"},
{"title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed\n  Tensor Adaptation", "author": "Zhuo Chen and Rumen Dangovski and Charlotte Loh and Owen Dugan and Di Luo and Marin Solja\u010di\u0107", "abstract": "  We propose Quantum-informed Tensor Adaptation (QuanTA), a novel,\neasy-to-implement, fine-tuning method with no inference overhead for\nlarge-scale pre-trained language models. By leveraging quantum-inspired methods\nderived from quantum circuit structures, QuanTA enables efficient high-rank\nfine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)--low-rank\napproximation may fail for complicated downstream tasks. Our approach is\ntheoretically supported by the universality theorem and the rank representation\ntheorem to achieve efficient high-rank adaptations. Experiments demonstrate\nthat QuanTA significantly enhances commonsense reasoning, arithmetic reasoning,\nand scalability compared to traditional methods. Furthermore, QuanTA shows\nsuperior performance with fewer trainable parameters compared to other\napproaches and can be designed to integrate with existing fine-tuning\nalgorithms for further improvement, providing a scalable and efficient solution\nfor fine-tuning large language models and advancing state-of-the-art in natural\nlanguage processing.\n", "link": "http://arxiv.org/abs/2406.00132v2", "date": "2024-11-01", "relevancy": 1.9654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4872}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuanTA%3A%20Efficient%20High-Rank%20Fine-Tuning%20of%20LLMs%20with%20Quantum-Informed%0A%20%20Tensor%20Adaptation&body=Title%3A%20QuanTA%3A%20Efficient%20High-Rank%20Fine-Tuning%20of%20LLMs%20with%20Quantum-Informed%0A%20%20Tensor%20Adaptation%0AAuthor%3A%20Zhuo%20Chen%20and%20Rumen%20Dangovski%20and%20Charlotte%20Loh%20and%20Owen%20Dugan%20and%20Di%20Luo%20and%20Marin%20Solja%C4%8Di%C4%87%0AAbstract%3A%20%20%20We%20propose%20Quantum-informed%20Tensor%20Adaptation%20%28QuanTA%29%2C%20a%20novel%2C%0Aeasy-to-implement%2C%20fine-tuning%20method%20with%20no%20inference%20overhead%20for%0Alarge-scale%20pre-trained%20language%20models.%20By%20leveraging%20quantum-inspired%20methods%0Aderived%20from%20quantum%20circuit%20structures%2C%20QuanTA%20enables%20efficient%20high-rank%0Afine-tuning%2C%20surpassing%20the%20limitations%20of%20Low-Rank%20Adaptation%20%28LoRA%29--low-rank%0Aapproximation%20may%20fail%20for%20complicated%20downstream%20tasks.%20Our%20approach%20is%0Atheoretically%20supported%20by%20the%20universality%20theorem%20and%20the%20rank%20representation%0Atheorem%20to%20achieve%20efficient%20high-rank%20adaptations.%20Experiments%20demonstrate%0Athat%20QuanTA%20significantly%20enhances%20commonsense%20reasoning%2C%20arithmetic%20reasoning%2C%0Aand%20scalability%20compared%20to%20traditional%20methods.%20Furthermore%2C%20QuanTA%20shows%0Asuperior%20performance%20with%20fewer%20trainable%20parameters%20compared%20to%20other%0Aapproaches%20and%20can%20be%20designed%20to%20integrate%20with%20existing%20fine-tuning%0Aalgorithms%20for%20further%20improvement%2C%20providing%20a%20scalable%20and%20efficient%20solution%0Afor%20fine-tuning%20large%20language%20models%20and%20advancing%20state-of-the-art%20in%20natural%0Alanguage%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuanTA%253A%2520Efficient%2520High-Rank%2520Fine-Tuning%2520of%2520LLMs%2520with%2520Quantum-Informed%250A%2520%2520Tensor%2520Adaptation%26entry.906535625%3DZhuo%2520Chen%2520and%2520Rumen%2520Dangovski%2520and%2520Charlotte%2520Loh%2520and%2520Owen%2520Dugan%2520and%2520Di%2520Luo%2520and%2520Marin%2520Solja%25C4%258Di%25C4%2587%26entry.1292438233%3D%2520%2520We%2520propose%2520Quantum-informed%2520Tensor%2520Adaptation%2520%2528QuanTA%2529%252C%2520a%2520novel%252C%250Aeasy-to-implement%252C%2520fine-tuning%2520method%2520with%2520no%2520inference%2520overhead%2520for%250Alarge-scale%2520pre-trained%2520language%2520models.%2520By%2520leveraging%2520quantum-inspired%2520methods%250Aderived%2520from%2520quantum%2520circuit%2520structures%252C%2520QuanTA%2520enables%2520efficient%2520high-rank%250Afine-tuning%252C%2520surpassing%2520the%2520limitations%2520of%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529--low-rank%250Aapproximation%2520may%2520fail%2520for%2520complicated%2520downstream%2520tasks.%2520Our%2520approach%2520is%250Atheoretically%2520supported%2520by%2520the%2520universality%2520theorem%2520and%2520the%2520rank%2520representation%250Atheorem%2520to%2520achieve%2520efficient%2520high-rank%2520adaptations.%2520Experiments%2520demonstrate%250Athat%2520QuanTA%2520significantly%2520enhances%2520commonsense%2520reasoning%252C%2520arithmetic%2520reasoning%252C%250Aand%2520scalability%2520compared%2520to%2520traditional%2520methods.%2520Furthermore%252C%2520QuanTA%2520shows%250Asuperior%2520performance%2520with%2520fewer%2520trainable%2520parameters%2520compared%2520to%2520other%250Aapproaches%2520and%2520can%2520be%2520designed%2520to%2520integrate%2520with%2520existing%2520fine-tuning%250Aalgorithms%2520for%2520further%2520improvement%252C%2520providing%2520a%2520scalable%2520and%2520efficient%2520solution%250Afor%2520fine-tuning%2520large%2520language%2520models%2520and%2520advancing%2520state-of-the-art%2520in%2520natural%250Alanguage%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuanTA%3A%20Efficient%20High-Rank%20Fine-Tuning%20of%20LLMs%20with%20Quantum-Informed%0A%20%20Tensor%20Adaptation&entry.906535625=Zhuo%20Chen%20and%20Rumen%20Dangovski%20and%20Charlotte%20Loh%20and%20Owen%20Dugan%20and%20Di%20Luo%20and%20Marin%20Solja%C4%8Di%C4%87&entry.1292438233=%20%20We%20propose%20Quantum-informed%20Tensor%20Adaptation%20%28QuanTA%29%2C%20a%20novel%2C%0Aeasy-to-implement%2C%20fine-tuning%20method%20with%20no%20inference%20overhead%20for%0Alarge-scale%20pre-trained%20language%20models.%20By%20leveraging%20quantum-inspired%20methods%0Aderived%20from%20quantum%20circuit%20structures%2C%20QuanTA%20enables%20efficient%20high-rank%0Afine-tuning%2C%20surpassing%20the%20limitations%20of%20Low-Rank%20Adaptation%20%28LoRA%29--low-rank%0Aapproximation%20may%20fail%20for%20complicated%20downstream%20tasks.%20Our%20approach%20is%0Atheoretically%20supported%20by%20the%20universality%20theorem%20and%20the%20rank%20representation%0Atheorem%20to%20achieve%20efficient%20high-rank%20adaptations.%20Experiments%20demonstrate%0Athat%20QuanTA%20significantly%20enhances%20commonsense%20reasoning%2C%20arithmetic%20reasoning%2C%0Aand%20scalability%20compared%20to%20traditional%20methods.%20Furthermore%2C%20QuanTA%20shows%0Asuperior%20performance%20with%20fewer%20trainable%20parameters%20compared%20to%20other%0Aapproaches%20and%20can%20be%20designed%20to%20integrate%20with%20existing%20fine-tuning%0Aalgorithms%20for%20further%20improvement%2C%20providing%20a%20scalable%20and%20efficient%20solution%0Afor%20fine-tuning%20large%20language%20models%20and%20advancing%20state-of-the-art%20in%20natural%0Alanguage%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00132v2&entry.124074799=Read"},
{"title": "Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal\n  satellite dataset for rapid flood mapping", "author": "Nikolaos Ioannis Bountos and Maria Sdraka and Angelos Zavras and Ilektra Karasante and Andreas Karavias and Themistocles Herekakis and Angeliki Thanasou and Dimitrios Michail and Ioannis Papoutsis", "abstract": "  Global floods, exacerbated by climate change, pose severe threats to human\nlife, infrastructure, and the environment. Recent catastrophic events in\nPakistan and New Zealand underscore the urgent need for precise flood mapping\nto guide restoration efforts, understand vulnerabilities, and prepare for\nfuture occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers\nday-and-night, all-weather imaging capabilities, its application in deep\nlearning for flood segmentation is limited by the lack of large annotated\ndatasets. To address this, we introduce Kuro Siwo, a manually annotated\nmulti-temporal dataset, spanning 43 flood events globally. Our dataset maps\nmore than 338 billion $m^2$ of land, with 33 billion designated as either\nflooded areas or permanent water bodies. Kuro Siwo includes a highly processed\nproduct optimized for flood mapping based on SAR Ground Range Detected, and a\nprimal SAR Single Look Complex product with minimal preprocessing, designed to\npromote research on the exploitation of both the phase and amplitude\ninformation and to offer maximum flexibility for downstream task preprocessing.\nTo leverage advances in large scale self-supervised pretraining methods for\nremote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR\nsamples. Finally, we provide an extensive benchmark, namely BlackBench,\noffering strong baselines for a diverse set of flood events from Europe,\nAmerica, Africa, Asia and Australia.\n", "link": "http://arxiv.org/abs/2311.12056v3", "date": "2024-11-01", "relevancy": 1.9522, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kuro%20Siwo%3A%2033%20billion%20%24m%5E2%24%20under%20the%20water.%20A%20global%20multi-temporal%0A%20%20satellite%20dataset%20for%20rapid%20flood%20mapping&body=Title%3A%20Kuro%20Siwo%3A%2033%20billion%20%24m%5E2%24%20under%20the%20water.%20A%20global%20multi-temporal%0A%20%20satellite%20dataset%20for%20rapid%20flood%20mapping%0AAuthor%3A%20Nikolaos%20Ioannis%20Bountos%20and%20Maria%20Sdraka%20and%20Angelos%20Zavras%20and%20Ilektra%20Karasante%20and%20Andreas%20Karavias%20and%20Themistocles%20Herekakis%20and%20Angeliki%20Thanasou%20and%20Dimitrios%20Michail%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20Global%20floods%2C%20exacerbated%20by%20climate%20change%2C%20pose%20severe%20threats%20to%20human%0Alife%2C%20infrastructure%2C%20and%20the%20environment.%20Recent%20catastrophic%20events%20in%0APakistan%20and%20New%20Zealand%20underscore%20the%20urgent%20need%20for%20precise%20flood%20mapping%0Ato%20guide%20restoration%20efforts%2C%20understand%20vulnerabilities%2C%20and%20prepare%20for%0Afuture%20occurrences.%20While%20Synthetic%20Aperture%20Radar%20%28SAR%29%20remote%20sensing%20offers%0Aday-and-night%2C%20all-weather%20imaging%20capabilities%2C%20its%20application%20in%20deep%0Alearning%20for%20flood%20segmentation%20is%20limited%20by%20the%20lack%20of%20large%20annotated%0Adatasets.%20To%20address%20this%2C%20we%20introduce%20Kuro%20Siwo%2C%20a%20manually%20annotated%0Amulti-temporal%20dataset%2C%20spanning%2043%20flood%20events%20globally.%20Our%20dataset%20maps%0Amore%20than%20338%20billion%20%24m%5E2%24%20of%20land%2C%20with%2033%20billion%20designated%20as%20either%0Aflooded%20areas%20or%20permanent%20water%20bodies.%20Kuro%20Siwo%20includes%20a%20highly%20processed%0Aproduct%20optimized%20for%20flood%20mapping%20based%20on%20SAR%20Ground%20Range%20Detected%2C%20and%20a%0Aprimal%20SAR%20Single%20Look%20Complex%20product%20with%20minimal%20preprocessing%2C%20designed%20to%0Apromote%20research%20on%20the%20exploitation%20of%20both%20the%20phase%20and%20amplitude%0Ainformation%20and%20to%20offer%20maximum%20flexibility%20for%20downstream%20task%20preprocessing.%0ATo%20leverage%20advances%20in%20large%20scale%20self-supervised%20pretraining%20methods%20for%0Aremote%20sensing%20data%2C%20we%20augment%20Kuro%20Siwo%20with%20a%20large%20unlabeled%20set%20of%20SAR%0Asamples.%20Finally%2C%20we%20provide%20an%20extensive%20benchmark%2C%20namely%20BlackBench%2C%0Aoffering%20strong%20baselines%20for%20a%20diverse%20set%20of%20flood%20events%20from%20Europe%2C%0AAmerica%2C%20Africa%2C%20Asia%20and%20Australia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12056v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKuro%2520Siwo%253A%252033%2520billion%2520%2524m%255E2%2524%2520under%2520the%2520water.%2520A%2520global%2520multi-temporal%250A%2520%2520satellite%2520dataset%2520for%2520rapid%2520flood%2520mapping%26entry.906535625%3DNikolaos%2520Ioannis%2520Bountos%2520and%2520Maria%2520Sdraka%2520and%2520Angelos%2520Zavras%2520and%2520Ilektra%2520Karasante%2520and%2520Andreas%2520Karavias%2520and%2520Themistocles%2520Herekakis%2520and%2520Angeliki%2520Thanasou%2520and%2520Dimitrios%2520Michail%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3D%2520%2520Global%2520floods%252C%2520exacerbated%2520by%2520climate%2520change%252C%2520pose%2520severe%2520threats%2520to%2520human%250Alife%252C%2520infrastructure%252C%2520and%2520the%2520environment.%2520Recent%2520catastrophic%2520events%2520in%250APakistan%2520and%2520New%2520Zealand%2520underscore%2520the%2520urgent%2520need%2520for%2520precise%2520flood%2520mapping%250Ato%2520guide%2520restoration%2520efforts%252C%2520understand%2520vulnerabilities%252C%2520and%2520prepare%2520for%250Afuture%2520occurrences.%2520While%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520remote%2520sensing%2520offers%250Aday-and-night%252C%2520all-weather%2520imaging%2520capabilities%252C%2520its%2520application%2520in%2520deep%250Alearning%2520for%2520flood%2520segmentation%2520is%2520limited%2520by%2520the%2520lack%2520of%2520large%2520annotated%250Adatasets.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Kuro%2520Siwo%252C%2520a%2520manually%2520annotated%250Amulti-temporal%2520dataset%252C%2520spanning%252043%2520flood%2520events%2520globally.%2520Our%2520dataset%2520maps%250Amore%2520than%2520338%2520billion%2520%2524m%255E2%2524%2520of%2520land%252C%2520with%252033%2520billion%2520designated%2520as%2520either%250Aflooded%2520areas%2520or%2520permanent%2520water%2520bodies.%2520Kuro%2520Siwo%2520includes%2520a%2520highly%2520processed%250Aproduct%2520optimized%2520for%2520flood%2520mapping%2520based%2520on%2520SAR%2520Ground%2520Range%2520Detected%252C%2520and%2520a%250Aprimal%2520SAR%2520Single%2520Look%2520Complex%2520product%2520with%2520minimal%2520preprocessing%252C%2520designed%2520to%250Apromote%2520research%2520on%2520the%2520exploitation%2520of%2520both%2520the%2520phase%2520and%2520amplitude%250Ainformation%2520and%2520to%2520offer%2520maximum%2520flexibility%2520for%2520downstream%2520task%2520preprocessing.%250ATo%2520leverage%2520advances%2520in%2520large%2520scale%2520self-supervised%2520pretraining%2520methods%2520for%250Aremote%2520sensing%2520data%252C%2520we%2520augment%2520Kuro%2520Siwo%2520with%2520a%2520large%2520unlabeled%2520set%2520of%2520SAR%250Asamples.%2520Finally%252C%2520we%2520provide%2520an%2520extensive%2520benchmark%252C%2520namely%2520BlackBench%252C%250Aoffering%2520strong%2520baselines%2520for%2520a%2520diverse%2520set%2520of%2520flood%2520events%2520from%2520Europe%252C%250AAmerica%252C%2520Africa%252C%2520Asia%2520and%2520Australia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12056v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kuro%20Siwo%3A%2033%20billion%20%24m%5E2%24%20under%20the%20water.%20A%20global%20multi-temporal%0A%20%20satellite%20dataset%20for%20rapid%20flood%20mapping&entry.906535625=Nikolaos%20Ioannis%20Bountos%20and%20Maria%20Sdraka%20and%20Angelos%20Zavras%20and%20Ilektra%20Karasante%20and%20Andreas%20Karavias%20and%20Themistocles%20Herekakis%20and%20Angeliki%20Thanasou%20and%20Dimitrios%20Michail%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20Global%20floods%2C%20exacerbated%20by%20climate%20change%2C%20pose%20severe%20threats%20to%20human%0Alife%2C%20infrastructure%2C%20and%20the%20environment.%20Recent%20catastrophic%20events%20in%0APakistan%20and%20New%20Zealand%20underscore%20the%20urgent%20need%20for%20precise%20flood%20mapping%0Ato%20guide%20restoration%20efforts%2C%20understand%20vulnerabilities%2C%20and%20prepare%20for%0Afuture%20occurrences.%20While%20Synthetic%20Aperture%20Radar%20%28SAR%29%20remote%20sensing%20offers%0Aday-and-night%2C%20all-weather%20imaging%20capabilities%2C%20its%20application%20in%20deep%0Alearning%20for%20flood%20segmentation%20is%20limited%20by%20the%20lack%20of%20large%20annotated%0Adatasets.%20To%20address%20this%2C%20we%20introduce%20Kuro%20Siwo%2C%20a%20manually%20annotated%0Amulti-temporal%20dataset%2C%20spanning%2043%20flood%20events%20globally.%20Our%20dataset%20maps%0Amore%20than%20338%20billion%20%24m%5E2%24%20of%20land%2C%20with%2033%20billion%20designated%20as%20either%0Aflooded%20areas%20or%20permanent%20water%20bodies.%20Kuro%20Siwo%20includes%20a%20highly%20processed%0Aproduct%20optimized%20for%20flood%20mapping%20based%20on%20SAR%20Ground%20Range%20Detected%2C%20and%20a%0Aprimal%20SAR%20Single%20Look%20Complex%20product%20with%20minimal%20preprocessing%2C%20designed%20to%0Apromote%20research%20on%20the%20exploitation%20of%20both%20the%20phase%20and%20amplitude%0Ainformation%20and%20to%20offer%20maximum%20flexibility%20for%20downstream%20task%20preprocessing.%0ATo%20leverage%20advances%20in%20large%20scale%20self-supervised%20pretraining%20methods%20for%0Aremote%20sensing%20data%2C%20we%20augment%20Kuro%20Siwo%20with%20a%20large%20unlabeled%20set%20of%20SAR%0Asamples.%20Finally%2C%20we%20provide%20an%20extensive%20benchmark%2C%20namely%20BlackBench%2C%0Aoffering%20strong%20baselines%20for%20a%20diverse%20set%20of%20flood%20events%20from%20Europe%2C%0AAmerica%2C%20Africa%2C%20Asia%20and%20Australia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12056v3&entry.124074799=Read"},
{"title": "Regression-aware Inference with LLMs", "author": "Michal Lukasik and Harikrishna Narasimhan and Aditya Krishna Menon and Felix Yu and Sanjiv Kumar", "abstract": "  Large language models (LLMs) have shown strong results on a range of\napplications, including regression and scoring tasks. Typically, one obtains\noutputs from an LLM via autoregressive sampling from the model's output\ndistribution. We show that this inference strategy can be sub-optimal for\ncommon regression and scoring evaluation metrics. As a remedy, we build on\nprior work on Minimum Bayes Risk decoding, and propose alternate inference\nstrategies that estimate the Bayes-optimal solution for regression and scoring\nmetrics in closed-form from sampled responses. We show that our proposal\nsignificantly improves over baselines across datasets and models.\n", "link": "http://arxiv.org/abs/2403.04182v3", "date": "2024-11-01", "relevancy": 1.9374, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5076}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4835}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regression-aware%20Inference%20with%20LLMs&body=Title%3A%20Regression-aware%20Inference%20with%20LLMs%0AAuthor%3A%20Michal%20Lukasik%20and%20Harikrishna%20Narasimhan%20and%20Aditya%20Krishna%20Menon%20and%20Felix%20Yu%20and%20Sanjiv%20Kumar%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20results%20on%20a%20range%20of%0Aapplications%2C%20including%20regression%20and%20scoring%20tasks.%20Typically%2C%20one%20obtains%0Aoutputs%20from%20an%20LLM%20via%20autoregressive%20sampling%20from%20the%20model%27s%20output%0Adistribution.%20We%20show%20that%20this%20inference%20strategy%20can%20be%20sub-optimal%20for%0Acommon%20regression%20and%20scoring%20evaluation%20metrics.%20As%20a%20remedy%2C%20we%20build%20on%0Aprior%20work%20on%20Minimum%20Bayes%20Risk%20decoding%2C%20and%20propose%20alternate%20inference%0Astrategies%20that%20estimate%20the%20Bayes-optimal%20solution%20for%20regression%20and%20scoring%0Ametrics%20in%20closed-form%20from%20sampled%20responses.%20We%20show%20that%20our%20proposal%0Asignificantly%20improves%20over%20baselines%20across%20datasets%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04182v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegression-aware%2520Inference%2520with%2520LLMs%26entry.906535625%3DMichal%2520Lukasik%2520and%2520Harikrishna%2520Narasimhan%2520and%2520Aditya%2520Krishna%2520Menon%2520and%2520Felix%2520Yu%2520and%2520Sanjiv%2520Kumar%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520results%2520on%2520a%2520range%2520of%250Aapplications%252C%2520including%2520regression%2520and%2520scoring%2520tasks.%2520Typically%252C%2520one%2520obtains%250Aoutputs%2520from%2520an%2520LLM%2520via%2520autoregressive%2520sampling%2520from%2520the%2520model%2527s%2520output%250Adistribution.%2520We%2520show%2520that%2520this%2520inference%2520strategy%2520can%2520be%2520sub-optimal%2520for%250Acommon%2520regression%2520and%2520scoring%2520evaluation%2520metrics.%2520As%2520a%2520remedy%252C%2520we%2520build%2520on%250Aprior%2520work%2520on%2520Minimum%2520Bayes%2520Risk%2520decoding%252C%2520and%2520propose%2520alternate%2520inference%250Astrategies%2520that%2520estimate%2520the%2520Bayes-optimal%2520solution%2520for%2520regression%2520and%2520scoring%250Ametrics%2520in%2520closed-form%2520from%2520sampled%2520responses.%2520We%2520show%2520that%2520our%2520proposal%250Asignificantly%2520improves%2520over%2520baselines%2520across%2520datasets%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04182v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regression-aware%20Inference%20with%20LLMs&entry.906535625=Michal%20Lukasik%20and%20Harikrishna%20Narasimhan%20and%20Aditya%20Krishna%20Menon%20and%20Felix%20Yu%20and%20Sanjiv%20Kumar&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20results%20on%20a%20range%20of%0Aapplications%2C%20including%20regression%20and%20scoring%20tasks.%20Typically%2C%20one%20obtains%0Aoutputs%20from%20an%20LLM%20via%20autoregressive%20sampling%20from%20the%20model%27s%20output%0Adistribution.%20We%20show%20that%20this%20inference%20strategy%20can%20be%20sub-optimal%20for%0Acommon%20regression%20and%20scoring%20evaluation%20metrics.%20As%20a%20remedy%2C%20we%20build%20on%0Aprior%20work%20on%20Minimum%20Bayes%20Risk%20decoding%2C%20and%20propose%20alternate%20inference%0Astrategies%20that%20estimate%20the%20Bayes-optimal%20solution%20for%20regression%20and%20scoring%0Ametrics%20in%20closed-form%20from%20sampled%20responses.%20We%20show%20that%20our%20proposal%0Asignificantly%20improves%20over%20baselines%20across%20datasets%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04182v3&entry.124074799=Read"},
{"title": "Leveraging Recurrent Neural Networks for Predicting Motor Movements from\n  Primate Motor Cortex Neural Recordings", "author": "Yuanxi Wang and Zuowen Wang and Shih-Chii Liu", "abstract": "  This paper presents an efficient deep learning solution for decoding motor\nmovements from neural recordings in non-human primates. An Autoencoder Gated\nRecurrent Unit (AEGRU) model was adopted as the model architecture for this\ntask. The autoencoder is only used during the training stage to achieve better\ngeneralization. Together with the preprocessing techniques, our model achieved\n0.71 $R^2$ score, surpassing the baseline models in Neurobench and is ranked\nfirst for $R^2$ in the IEEE BioCAS 2024 Grand Challenge on Neural Decoding.\nModel pruning is also applied leading to a reduction of 41.4% of the\nmultiply-accumulate (MAC) operations with little change in the $R^2$ score\ncompared to the unpruned model.\n", "link": "http://arxiv.org/abs/2410.22283v2", "date": "2024-11-01", "relevancy": 1.9224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.478}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Recurrent%20Neural%20Networks%20for%20Predicting%20Motor%20Movements%20from%0A%20%20Primate%20Motor%20Cortex%20Neural%20Recordings&body=Title%3A%20Leveraging%20Recurrent%20Neural%20Networks%20for%20Predicting%20Motor%20Movements%20from%0A%20%20Primate%20Motor%20Cortex%20Neural%20Recordings%0AAuthor%3A%20Yuanxi%20Wang%20and%20Zuowen%20Wang%20and%20Shih-Chii%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20efficient%20deep%20learning%20solution%20for%20decoding%20motor%0Amovements%20from%20neural%20recordings%20in%20non-human%20primates.%20An%20Autoencoder%20Gated%0ARecurrent%20Unit%20%28AEGRU%29%20model%20was%20adopted%20as%20the%20model%20architecture%20for%20this%0Atask.%20The%20autoencoder%20is%20only%20used%20during%20the%20training%20stage%20to%20achieve%20better%0Ageneralization.%20Together%20with%20the%20preprocessing%20techniques%2C%20our%20model%20achieved%0A0.71%20%24R%5E2%24%20score%2C%20surpassing%20the%20baseline%20models%20in%20Neurobench%20and%20is%20ranked%0Afirst%20for%20%24R%5E2%24%20in%20the%20IEEE%20BioCAS%202024%20Grand%20Challenge%20on%20Neural%20Decoding.%0AModel%20pruning%20is%20also%20applied%20leading%20to%20a%20reduction%20of%2041.4%25%20of%20the%0Amultiply-accumulate%20%28MAC%29%20operations%20with%20little%20change%20in%20the%20%24R%5E2%24%20score%0Acompared%20to%20the%20unpruned%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Recurrent%2520Neural%2520Networks%2520for%2520Predicting%2520Motor%2520Movements%2520from%250A%2520%2520Primate%2520Motor%2520Cortex%2520Neural%2520Recordings%26entry.906535625%3DYuanxi%2520Wang%2520and%2520Zuowen%2520Wang%2520and%2520Shih-Chii%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520efficient%2520deep%2520learning%2520solution%2520for%2520decoding%2520motor%250Amovements%2520from%2520neural%2520recordings%2520in%2520non-human%2520primates.%2520An%2520Autoencoder%2520Gated%250ARecurrent%2520Unit%2520%2528AEGRU%2529%2520model%2520was%2520adopted%2520as%2520the%2520model%2520architecture%2520for%2520this%250Atask.%2520The%2520autoencoder%2520is%2520only%2520used%2520during%2520the%2520training%2520stage%2520to%2520achieve%2520better%250Ageneralization.%2520Together%2520with%2520the%2520preprocessing%2520techniques%252C%2520our%2520model%2520achieved%250A0.71%2520%2524R%255E2%2524%2520score%252C%2520surpassing%2520the%2520baseline%2520models%2520in%2520Neurobench%2520and%2520is%2520ranked%250Afirst%2520for%2520%2524R%255E2%2524%2520in%2520the%2520IEEE%2520BioCAS%25202024%2520Grand%2520Challenge%2520on%2520Neural%2520Decoding.%250AModel%2520pruning%2520is%2520also%2520applied%2520leading%2520to%2520a%2520reduction%2520of%252041.4%2525%2520of%2520the%250Amultiply-accumulate%2520%2528MAC%2529%2520operations%2520with%2520little%2520change%2520in%2520the%2520%2524R%255E2%2524%2520score%250Acompared%2520to%2520the%2520unpruned%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Recurrent%20Neural%20Networks%20for%20Predicting%20Motor%20Movements%20from%0A%20%20Primate%20Motor%20Cortex%20Neural%20Recordings&entry.906535625=Yuanxi%20Wang%20and%20Zuowen%20Wang%20and%20Shih-Chii%20Liu&entry.1292438233=%20%20This%20paper%20presents%20an%20efficient%20deep%20learning%20solution%20for%20decoding%20motor%0Amovements%20from%20neural%20recordings%20in%20non-human%20primates.%20An%20Autoencoder%20Gated%0ARecurrent%20Unit%20%28AEGRU%29%20model%20was%20adopted%20as%20the%20model%20architecture%20for%20this%0Atask.%20The%20autoencoder%20is%20only%20used%20during%20the%20training%20stage%20to%20achieve%20better%0Ageneralization.%20Together%20with%20the%20preprocessing%20techniques%2C%20our%20model%20achieved%0A0.71%20%24R%5E2%24%20score%2C%20surpassing%20the%20baseline%20models%20in%20Neurobench%20and%20is%20ranked%0Afirst%20for%20%24R%5E2%24%20in%20the%20IEEE%20BioCAS%202024%20Grand%20Challenge%20on%20Neural%20Decoding.%0AModel%20pruning%20is%20also%20applied%20leading%20to%20a%20reduction%20of%2041.4%25%20of%20the%0Amultiply-accumulate%20%28MAC%29%20operations%20with%20little%20change%20in%20the%20%24R%5E2%24%20score%0Acompared%20to%20the%20unpruned%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22283v2&entry.124074799=Read"},
{"title": "TaskBench: Benchmarking Large Language Models for Task Automation", "author": "Yongliang Shen and Kaitao Song and Xu Tan and Wenqi Zhang and Kan Ren and Siyu Yuan and Weiming Lu and Dongsheng Li and Yueting Zhuang", "abstract": "  In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.\n", "link": "http://arxiv.org/abs/2311.18760v4", "date": "2024-11-01", "relevancy": 1.9204, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaskBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Task%20Automation&body=Title%3A%20TaskBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Task%20Automation%0AAuthor%3A%20Yongliang%20Shen%20and%20Kaitao%20Song%20and%20Xu%20Tan%20and%20Wenqi%20Zhang%20and%20Kan%20Ren%20and%20Siyu%20Yuan%20and%20Weiming%20Lu%20and%20Dongsheng%20Li%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20remarkable%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%0Asparked%20interest%20in%20task%20automation%2C%20which%20involves%20decomposing%20complex%20tasks%0Adescribed%20by%20user%20instructions%20into%20sub-tasks%20and%20invoking%20external%20tools%20to%0Aexecute%20them%2C%20playing%20a%20central%20role%20in%20autonomous%20agents.%20However%2C%20there%20is%20a%0Alack%20of%20systematic%20and%20standardized%20benchmarks%20to%20promote%20the%20development%20of%0ALLMs%20in%20task%20automation.%20To%20address%20this%2C%20we%20introduce%20TaskBench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20the%20capability%20of%20LLMs%20in%20task%20automation.%0ASpecifically%2C%20task%20automation%20can%20be%20divided%20into%20three%20critical%20stages%3A%20task%0Adecomposition%2C%20tool%20selection%2C%20and%20parameter%20prediction.%20To%20tackle%20the%0Acomplexities%20inherent%20in%20these%20stages%2C%20we%20introduce%20the%20concept%20of%20Tool%20Graph%0Ato%20represent%20decomposed%20tasks%20and%20adopt%20a%20back-instruct%20method%20to%20generate%0Ahigh-quality%20user%20instructions.%20We%20propose%20TaskEval%2C%20a%20multi-faceted%20evaluation%0Amethodology%20that%20assesses%20LLM%20performance%20across%20these%20three%20stages.%20Our%0Aapproach%20combines%20automated%20construction%20with%20rigorous%20human%20verification%2C%0Aensuring%20high%20consistency%20with%20human%20evaluation.%20Experimental%20results%0Ademonstrate%20that%20TaskBench%20effectively%20reflects%20the%20capabilities%20of%20various%0ALLMs%20in%20task%20automation.%20It%20provides%20insights%20into%20model%20performance%20across%0Adifferent%20task%20complexities%20and%20domains%2C%20pushing%20the%20boundaries%20of%20what%20current%0Amodels%20can%20achieve.%20TaskBench%20offers%20a%20scalable%2C%20adaptable%2C%20and%20reliable%0Abenchmark%20for%20advancing%20LLM-based%20autonomous%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18760v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaskBench%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520for%2520Task%2520Automation%26entry.906535625%3DYongliang%2520Shen%2520and%2520Kaitao%2520Song%2520and%2520Xu%2520Tan%2520and%2520Wenqi%2520Zhang%2520and%2520Kan%2520Ren%2520and%2520Siyu%2520Yuan%2520and%2520Weiming%2520Lu%2520and%2520Dongsheng%2520Li%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520remarkable%2520progress%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%250Asparked%2520interest%2520in%2520task%2520automation%252C%2520which%2520involves%2520decomposing%2520complex%2520tasks%250Adescribed%2520by%2520user%2520instructions%2520into%2520sub-tasks%2520and%2520invoking%2520external%2520tools%2520to%250Aexecute%2520them%252C%2520playing%2520a%2520central%2520role%2520in%2520autonomous%2520agents.%2520However%252C%2520there%2520is%2520a%250Alack%2520of%2520systematic%2520and%2520standardized%2520benchmarks%2520to%2520promote%2520the%2520development%2520of%250ALLMs%2520in%2520task%2520automation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520TaskBench%252C%2520a%250Acomprehensive%2520framework%2520to%2520evaluate%2520the%2520capability%2520of%2520LLMs%2520in%2520task%2520automation.%250ASpecifically%252C%2520task%2520automation%2520can%2520be%2520divided%2520into%2520three%2520critical%2520stages%253A%2520task%250Adecomposition%252C%2520tool%2520selection%252C%2520and%2520parameter%2520prediction.%2520To%2520tackle%2520the%250Acomplexities%2520inherent%2520in%2520these%2520stages%252C%2520we%2520introduce%2520the%2520concept%2520of%2520Tool%2520Graph%250Ato%2520represent%2520decomposed%2520tasks%2520and%2520adopt%2520a%2520back-instruct%2520method%2520to%2520generate%250Ahigh-quality%2520user%2520instructions.%2520We%2520propose%2520TaskEval%252C%2520a%2520multi-faceted%2520evaluation%250Amethodology%2520that%2520assesses%2520LLM%2520performance%2520across%2520these%2520three%2520stages.%2520Our%250Aapproach%2520combines%2520automated%2520construction%2520with%2520rigorous%2520human%2520verification%252C%250Aensuring%2520high%2520consistency%2520with%2520human%2520evaluation.%2520Experimental%2520results%250Ademonstrate%2520that%2520TaskBench%2520effectively%2520reflects%2520the%2520capabilities%2520of%2520various%250ALLMs%2520in%2520task%2520automation.%2520It%2520provides%2520insights%2520into%2520model%2520performance%2520across%250Adifferent%2520task%2520complexities%2520and%2520domains%252C%2520pushing%2520the%2520boundaries%2520of%2520what%2520current%250Amodels%2520can%2520achieve.%2520TaskBench%2520offers%2520a%2520scalable%252C%2520adaptable%252C%2520and%2520reliable%250Abenchmark%2520for%2520advancing%2520LLM-based%2520autonomous%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18760v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaskBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Task%20Automation&entry.906535625=Yongliang%20Shen%20and%20Kaitao%20Song%20and%20Xu%20Tan%20and%20Wenqi%20Zhang%20and%20Kan%20Ren%20and%20Siyu%20Yuan%20and%20Weiming%20Lu%20and%20Dongsheng%20Li%20and%20Yueting%20Zhuang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20remarkable%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%0Asparked%20interest%20in%20task%20automation%2C%20which%20involves%20decomposing%20complex%20tasks%0Adescribed%20by%20user%20instructions%20into%20sub-tasks%20and%20invoking%20external%20tools%20to%0Aexecute%20them%2C%20playing%20a%20central%20role%20in%20autonomous%20agents.%20However%2C%20there%20is%20a%0Alack%20of%20systematic%20and%20standardized%20benchmarks%20to%20promote%20the%20development%20of%0ALLMs%20in%20task%20automation.%20To%20address%20this%2C%20we%20introduce%20TaskBench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20the%20capability%20of%20LLMs%20in%20task%20automation.%0ASpecifically%2C%20task%20automation%20can%20be%20divided%20into%20three%20critical%20stages%3A%20task%0Adecomposition%2C%20tool%20selection%2C%20and%20parameter%20prediction.%20To%20tackle%20the%0Acomplexities%20inherent%20in%20these%20stages%2C%20we%20introduce%20the%20concept%20of%20Tool%20Graph%0Ato%20represent%20decomposed%20tasks%20and%20adopt%20a%20back-instruct%20method%20to%20generate%0Ahigh-quality%20user%20instructions.%20We%20propose%20TaskEval%2C%20a%20multi-faceted%20evaluation%0Amethodology%20that%20assesses%20LLM%20performance%20across%20these%20three%20stages.%20Our%0Aapproach%20combines%20automated%20construction%20with%20rigorous%20human%20verification%2C%0Aensuring%20high%20consistency%20with%20human%20evaluation.%20Experimental%20results%0Ademonstrate%20that%20TaskBench%20effectively%20reflects%20the%20capabilities%20of%20various%0ALLMs%20in%20task%20automation.%20It%20provides%20insights%20into%20model%20performance%20across%0Adifferent%20task%20complexities%20and%20domains%2C%20pushing%20the%20boundaries%20of%20what%20current%0Amodels%20can%20achieve.%20TaskBench%20offers%20a%20scalable%2C%20adaptable%2C%20and%20reliable%0Abenchmark%20for%20advancing%20LLM-based%20autonomous%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18760v4&entry.124074799=Read"},
{"title": "Accelerating Transfer Learning with Near-Data Computation on Cloud\n  Object Stores", "author": "Diana Petrescu and Arsany Guirguis and Do Le Quoc and Javier Picorel and Rachid Guerraoui and Florin Dinu", "abstract": "  Storage disaggregation underlies today's cloud and is naturally complemented\nby pushing down some computation to storage, thus mitigating the potential\nnetwork bottleneck between the storage and compute tiers. We show how ML\ntraining benefits from storage pushdowns by focusing on transfer learning (TL),\nthe widespread technique that democratizes ML by reusing existing knowledge on\nrelated tasks. We propose HAPI, a new TL processing system centered around two\ncomplementary techniques that address challenges introduced by disaggregation.\nFirst, applications must carefully balance execution across tiers for\nperformance. HAPI judiciously splits the TL computation during the feature\nextraction phase yielding pushdowns that not only improve network time but also\nimprove total TL training time by overlapping the execution of consecutive\ntraining iterations across tiers. Second, operators want resource efficiency\nfrom the storage-side computational resources. HAPI employs storage-side batch\nsize adaptation allowing increased storage-side pushdown concurrency without\naffecting training accuracy. HAPI yields up to 2.5x training speed-up while\nchoosing in 86.8% of cases the best performing split point or one that is at\nmost 5% off from the best.\n", "link": "http://arxiv.org/abs/2210.08650v3", "date": "2024-11-01", "relevancy": 1.9177, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4819}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Transfer%20Learning%20with%20Near-Data%20Computation%20on%20Cloud%0A%20%20Object%20Stores&body=Title%3A%20Accelerating%20Transfer%20Learning%20with%20Near-Data%20Computation%20on%20Cloud%0A%20%20Object%20Stores%0AAuthor%3A%20Diana%20Petrescu%20and%20Arsany%20Guirguis%20and%20Do%20Le%20Quoc%20and%20Javier%20Picorel%20and%20Rachid%20Guerraoui%20and%20Florin%20Dinu%0AAbstract%3A%20%20%20Storage%20disaggregation%20underlies%20today%27s%20cloud%20and%20is%20naturally%20complemented%0Aby%20pushing%20down%20some%20computation%20to%20storage%2C%20thus%20mitigating%20the%20potential%0Anetwork%20bottleneck%20between%20the%20storage%20and%20compute%20tiers.%20We%20show%20how%20ML%0Atraining%20benefits%20from%20storage%20pushdowns%20by%20focusing%20on%20transfer%20learning%20%28TL%29%2C%0Athe%20widespread%20technique%20that%20democratizes%20ML%20by%20reusing%20existing%20knowledge%20on%0Arelated%20tasks.%20We%20propose%20HAPI%2C%20a%20new%20TL%20processing%20system%20centered%20around%20two%0Acomplementary%20techniques%20that%20address%20challenges%20introduced%20by%20disaggregation.%0AFirst%2C%20applications%20must%20carefully%20balance%20execution%20across%20tiers%20for%0Aperformance.%20HAPI%20judiciously%20splits%20the%20TL%20computation%20during%20the%20feature%0Aextraction%20phase%20yielding%20pushdowns%20that%20not%20only%20improve%20network%20time%20but%20also%0Aimprove%20total%20TL%20training%20time%20by%20overlapping%20the%20execution%20of%20consecutive%0Atraining%20iterations%20across%20tiers.%20Second%2C%20operators%20want%20resource%20efficiency%0Afrom%20the%20storage-side%20computational%20resources.%20HAPI%20employs%20storage-side%20batch%0Asize%20adaptation%20allowing%20increased%20storage-side%20pushdown%20concurrency%20without%0Aaffecting%20training%20accuracy.%20HAPI%20yields%20up%20to%202.5x%20training%20speed-up%20while%0Achoosing%20in%2086.8%25%20of%20cases%20the%20best%20performing%20split%20point%20or%20one%20that%20is%20at%0Amost%205%25%20off%20from%20the%20best.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.08650v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Transfer%2520Learning%2520with%2520Near-Data%2520Computation%2520on%2520Cloud%250A%2520%2520Object%2520Stores%26entry.906535625%3DDiana%2520Petrescu%2520and%2520Arsany%2520Guirguis%2520and%2520Do%2520Le%2520Quoc%2520and%2520Javier%2520Picorel%2520and%2520Rachid%2520Guerraoui%2520and%2520Florin%2520Dinu%26entry.1292438233%3D%2520%2520Storage%2520disaggregation%2520underlies%2520today%2527s%2520cloud%2520and%2520is%2520naturally%2520complemented%250Aby%2520pushing%2520down%2520some%2520computation%2520to%2520storage%252C%2520thus%2520mitigating%2520the%2520potential%250Anetwork%2520bottleneck%2520between%2520the%2520storage%2520and%2520compute%2520tiers.%2520We%2520show%2520how%2520ML%250Atraining%2520benefits%2520from%2520storage%2520pushdowns%2520by%2520focusing%2520on%2520transfer%2520learning%2520%2528TL%2529%252C%250Athe%2520widespread%2520technique%2520that%2520democratizes%2520ML%2520by%2520reusing%2520existing%2520knowledge%2520on%250Arelated%2520tasks.%2520We%2520propose%2520HAPI%252C%2520a%2520new%2520TL%2520processing%2520system%2520centered%2520around%2520two%250Acomplementary%2520techniques%2520that%2520address%2520challenges%2520introduced%2520by%2520disaggregation.%250AFirst%252C%2520applications%2520must%2520carefully%2520balance%2520execution%2520across%2520tiers%2520for%250Aperformance.%2520HAPI%2520judiciously%2520splits%2520the%2520TL%2520computation%2520during%2520the%2520feature%250Aextraction%2520phase%2520yielding%2520pushdowns%2520that%2520not%2520only%2520improve%2520network%2520time%2520but%2520also%250Aimprove%2520total%2520TL%2520training%2520time%2520by%2520overlapping%2520the%2520execution%2520of%2520consecutive%250Atraining%2520iterations%2520across%2520tiers.%2520Second%252C%2520operators%2520want%2520resource%2520efficiency%250Afrom%2520the%2520storage-side%2520computational%2520resources.%2520HAPI%2520employs%2520storage-side%2520batch%250Asize%2520adaptation%2520allowing%2520increased%2520storage-side%2520pushdown%2520concurrency%2520without%250Aaffecting%2520training%2520accuracy.%2520HAPI%2520yields%2520up%2520to%25202.5x%2520training%2520speed-up%2520while%250Achoosing%2520in%252086.8%2525%2520of%2520cases%2520the%2520best%2520performing%2520split%2520point%2520or%2520one%2520that%2520is%2520at%250Amost%25205%2525%2520off%2520from%2520the%2520best.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.08650v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Transfer%20Learning%20with%20Near-Data%20Computation%20on%20Cloud%0A%20%20Object%20Stores&entry.906535625=Diana%20Petrescu%20and%20Arsany%20Guirguis%20and%20Do%20Le%20Quoc%20and%20Javier%20Picorel%20and%20Rachid%20Guerraoui%20and%20Florin%20Dinu&entry.1292438233=%20%20Storage%20disaggregation%20underlies%20today%27s%20cloud%20and%20is%20naturally%20complemented%0Aby%20pushing%20down%20some%20computation%20to%20storage%2C%20thus%20mitigating%20the%20potential%0Anetwork%20bottleneck%20between%20the%20storage%20and%20compute%20tiers.%20We%20show%20how%20ML%0Atraining%20benefits%20from%20storage%20pushdowns%20by%20focusing%20on%20transfer%20learning%20%28TL%29%2C%0Athe%20widespread%20technique%20that%20democratizes%20ML%20by%20reusing%20existing%20knowledge%20on%0Arelated%20tasks.%20We%20propose%20HAPI%2C%20a%20new%20TL%20processing%20system%20centered%20around%20two%0Acomplementary%20techniques%20that%20address%20challenges%20introduced%20by%20disaggregation.%0AFirst%2C%20applications%20must%20carefully%20balance%20execution%20across%20tiers%20for%0Aperformance.%20HAPI%20judiciously%20splits%20the%20TL%20computation%20during%20the%20feature%0Aextraction%20phase%20yielding%20pushdowns%20that%20not%20only%20improve%20network%20time%20but%20also%0Aimprove%20total%20TL%20training%20time%20by%20overlapping%20the%20execution%20of%20consecutive%0Atraining%20iterations%20across%20tiers.%20Second%2C%20operators%20want%20resource%20efficiency%0Afrom%20the%20storage-side%20computational%20resources.%20HAPI%20employs%20storage-side%20batch%0Asize%20adaptation%20allowing%20increased%20storage-side%20pushdown%20concurrency%20without%0Aaffecting%20training%20accuracy.%20HAPI%20yields%20up%20to%202.5x%20training%20speed-up%20while%0Achoosing%20in%2086.8%25%20of%20cases%20the%20best%20performing%20split%20point%20or%20one%20that%20is%20at%0Amost%205%25%20off%20from%20the%20best.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.08650v3&entry.124074799=Read"},
{"title": "Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and\n  non-occluded immature green fruits in complex orchard environment", "author": "Ranjan Sapkota and Manoj Karkee", "abstract": "  This study conducted a comprehensive performance evaluation on YOLO11 and\nYOLOv8, the latest in the \"You Only Look Once\" (YOLO) series, focusing on their\ninstance segmentation capabilities for immature green apples in orchard\nenvironments. YOLO11n-seg achieved the highest mask precision across all\ncategories with a notable score of 0.831, highlighting its effectiveness in\nfruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and\noccluded fruitlet segmentation with scores of 0.851 and 0.829, respectively.\nAdditionally, YOLO11x-seg led in mask recall for all categories, achieving a\nscore of 0.815, with YOLO11m-seg performing best for non-occluded immature\ngreen fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with\n0.800. In terms of mean average precision at a 50\\% intersection over union\n(mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores\nfor both box and mask segmentation, at 0.876 and 0.860 for the \"All\" class and\n0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg\nand YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at\n0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the\nadvancements in YOLO11, YOLOv8n surpassed its counterparts in image processing\nspeed, with an impressive inference speed of 3.3 milliseconds, compared to the\nfastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability\nfor real-time agricultural applications related to complex green fruit\nenvironments.\n", "link": "http://arxiv.org/abs/2410.19869v2", "date": "2024-11-01", "relevancy": 1.9051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20YOLO11%20and%20YOLOv8%20for%20instance%20segmentation%20of%20occluded%20and%0A%20%20non-occluded%20immature%20green%20fruits%20in%20complex%20orchard%20environment&body=Title%3A%20Comparing%20YOLO11%20and%20YOLOv8%20for%20instance%20segmentation%20of%20occluded%20and%0A%20%20non-occluded%20immature%20green%20fruits%20in%20complex%20orchard%20environment%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20study%20conducted%20a%20comprehensive%20performance%20evaluation%20on%20YOLO11%20and%0AYOLOv8%2C%20the%20latest%20in%20the%20%22You%20Only%20Look%20Once%22%20%28YOLO%29%20series%2C%20focusing%20on%20their%0Ainstance%20segmentation%20capabilities%20for%20immature%20green%20apples%20in%20orchard%0Aenvironments.%20YOLO11n-seg%20achieved%20the%20highest%20mask%20precision%20across%20all%0Acategories%20with%20a%20notable%20score%20of%200.831%2C%20highlighting%20its%20effectiveness%20in%0Afruit%20detection.%20YOLO11m-seg%20and%20YOLO11l-seg%20excelled%20in%20non-occluded%20and%0Aoccluded%20fruitlet%20segmentation%20with%20scores%20of%200.851%20and%200.829%2C%20respectively.%0AAdditionally%2C%20YOLO11x-seg%20led%20in%20mask%20recall%20for%20all%20categories%2C%20achieving%20a%0Ascore%20of%200.815%2C%20with%20YOLO11m-seg%20performing%20best%20for%20non-occluded%20immature%0Agreen%20fruitlets%20at%200.858%20and%20YOLOv8x-seg%20leading%20the%20occluded%20category%20with%0A0.800.%20In%20terms%20of%20mean%20average%20precision%20at%20a%2050%5C%25%20intersection%20over%20union%0A%28mAP%4050%29%2C%20YOLO11m-seg%20consistently%20outperformed%2C%20registering%20the%20highest%20scores%0Afor%20both%20box%20and%20mask%20segmentation%2C%20at%200.876%20and%200.860%20for%20the%20%22All%22%20class%20and%0A0.908%20and%200.909%20for%20non-occluded%20immature%20fruitlets%2C%20respectively.%20YOLO11l-seg%0Aand%20YOLOv8l-seg%20shared%20the%20top%20box%20mAP%4050%20for%20occluded%20immature%20fruitlets%20at%0A0.847%2C%20while%20YOLO11m-seg%20achieved%20the%20highest%20mask%20mAP%4050%20of%200.810.%20Despite%20the%0Aadvancements%20in%20YOLO11%2C%20YOLOv8n%20surpassed%20its%20counterparts%20in%20image%20processing%0Aspeed%2C%20with%20an%20impressive%20inference%20speed%20of%203.3%20milliseconds%2C%20compared%20to%20the%0Afastest%20YOLO11%20series%20model%20at%204.8%20milliseconds%2C%20underscoring%20its%20suitability%0Afor%20real-time%20agricultural%20applications%20related%20to%20complex%20green%20fruit%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520YOLO11%2520and%2520YOLOv8%2520for%2520instance%2520segmentation%2520of%2520occluded%2520and%250A%2520%2520non-occluded%2520immature%2520green%2520fruits%2520in%2520complex%2520orchard%2520environment%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520study%2520conducted%2520a%2520comprehensive%2520performance%2520evaluation%2520on%2520YOLO11%2520and%250AYOLOv8%252C%2520the%2520latest%2520in%2520the%2520%2522You%2520Only%2520Look%2520Once%2522%2520%2528YOLO%2529%2520series%252C%2520focusing%2520on%2520their%250Ainstance%2520segmentation%2520capabilities%2520for%2520immature%2520green%2520apples%2520in%2520orchard%250Aenvironments.%2520YOLO11n-seg%2520achieved%2520the%2520highest%2520mask%2520precision%2520across%2520all%250Acategories%2520with%2520a%2520notable%2520score%2520of%25200.831%252C%2520highlighting%2520its%2520effectiveness%2520in%250Afruit%2520detection.%2520YOLO11m-seg%2520and%2520YOLO11l-seg%2520excelled%2520in%2520non-occluded%2520and%250Aoccluded%2520fruitlet%2520segmentation%2520with%2520scores%2520of%25200.851%2520and%25200.829%252C%2520respectively.%250AAdditionally%252C%2520YOLO11x-seg%2520led%2520in%2520mask%2520recall%2520for%2520all%2520categories%252C%2520achieving%2520a%250Ascore%2520of%25200.815%252C%2520with%2520YOLO11m-seg%2520performing%2520best%2520for%2520non-occluded%2520immature%250Agreen%2520fruitlets%2520at%25200.858%2520and%2520YOLOv8x-seg%2520leading%2520the%2520occluded%2520category%2520with%250A0.800.%2520In%2520terms%2520of%2520mean%2520average%2520precision%2520at%2520a%252050%255C%2525%2520intersection%2520over%2520union%250A%2528mAP%254050%2529%252C%2520YOLO11m-seg%2520consistently%2520outperformed%252C%2520registering%2520the%2520highest%2520scores%250Afor%2520both%2520box%2520and%2520mask%2520segmentation%252C%2520at%25200.876%2520and%25200.860%2520for%2520the%2520%2522All%2522%2520class%2520and%250A0.908%2520and%25200.909%2520for%2520non-occluded%2520immature%2520fruitlets%252C%2520respectively.%2520YOLO11l-seg%250Aand%2520YOLOv8l-seg%2520shared%2520the%2520top%2520box%2520mAP%254050%2520for%2520occluded%2520immature%2520fruitlets%2520at%250A0.847%252C%2520while%2520YOLO11m-seg%2520achieved%2520the%2520highest%2520mask%2520mAP%254050%2520of%25200.810.%2520Despite%2520the%250Aadvancements%2520in%2520YOLO11%252C%2520YOLOv8n%2520surpassed%2520its%2520counterparts%2520in%2520image%2520processing%250Aspeed%252C%2520with%2520an%2520impressive%2520inference%2520speed%2520of%25203.3%2520milliseconds%252C%2520compared%2520to%2520the%250Afastest%2520YOLO11%2520series%2520model%2520at%25204.8%2520milliseconds%252C%2520underscoring%2520its%2520suitability%250Afor%2520real-time%2520agricultural%2520applications%2520related%2520to%2520complex%2520green%2520fruit%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20YOLO11%20and%20YOLOv8%20for%20instance%20segmentation%20of%20occluded%20and%0A%20%20non-occluded%20immature%20green%20fruits%20in%20complex%20orchard%20environment&entry.906535625=Ranjan%20Sapkota%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20study%20conducted%20a%20comprehensive%20performance%20evaluation%20on%20YOLO11%20and%0AYOLOv8%2C%20the%20latest%20in%20the%20%22You%20Only%20Look%20Once%22%20%28YOLO%29%20series%2C%20focusing%20on%20their%0Ainstance%20segmentation%20capabilities%20for%20immature%20green%20apples%20in%20orchard%0Aenvironments.%20YOLO11n-seg%20achieved%20the%20highest%20mask%20precision%20across%20all%0Acategories%20with%20a%20notable%20score%20of%200.831%2C%20highlighting%20its%20effectiveness%20in%0Afruit%20detection.%20YOLO11m-seg%20and%20YOLO11l-seg%20excelled%20in%20non-occluded%20and%0Aoccluded%20fruitlet%20segmentation%20with%20scores%20of%200.851%20and%200.829%2C%20respectively.%0AAdditionally%2C%20YOLO11x-seg%20led%20in%20mask%20recall%20for%20all%20categories%2C%20achieving%20a%0Ascore%20of%200.815%2C%20with%20YOLO11m-seg%20performing%20best%20for%20non-occluded%20immature%0Agreen%20fruitlets%20at%200.858%20and%20YOLOv8x-seg%20leading%20the%20occluded%20category%20with%0A0.800.%20In%20terms%20of%20mean%20average%20precision%20at%20a%2050%5C%25%20intersection%20over%20union%0A%28mAP%4050%29%2C%20YOLO11m-seg%20consistently%20outperformed%2C%20registering%20the%20highest%20scores%0Afor%20both%20box%20and%20mask%20segmentation%2C%20at%200.876%20and%200.860%20for%20the%20%22All%22%20class%20and%0A0.908%20and%200.909%20for%20non-occluded%20immature%20fruitlets%2C%20respectively.%20YOLO11l-seg%0Aand%20YOLOv8l-seg%20shared%20the%20top%20box%20mAP%4050%20for%20occluded%20immature%20fruitlets%20at%0A0.847%2C%20while%20YOLO11m-seg%20achieved%20the%20highest%20mask%20mAP%4050%20of%200.810.%20Despite%20the%0Aadvancements%20in%20YOLO11%2C%20YOLOv8n%20surpassed%20its%20counterparts%20in%20image%20processing%0Aspeed%2C%20with%20an%20impressive%20inference%20speed%20of%203.3%20milliseconds%2C%20compared%20to%20the%0Afastest%20YOLO11%20series%20model%20at%204.8%20milliseconds%2C%20underscoring%20its%20suitability%0Afor%20real-time%20agricultural%20applications%20related%20to%20complex%20green%20fruit%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19869v2&entry.124074799=Read"},
{"title": "Towards shutdownable agents via stochastic choice", "author": "Elliott Thornley and Alexander Roman and Christos Ziakas and Leyton Ho and Louis Thomson", "abstract": "  Some worry that advanced artificial agents may resist being shut down. The\nIncomplete Preferences Proposal (IPP) is an idea for ensuring that doesn't\nhappen. A key part of the IPP is using a novel 'Discounted REward for\nSame-Length Trajectories (DREST)' reward function to train agents to (1) pursue\ngoals effectively conditional on each trajectory-length (be 'USEFUL'), and (2)\nchoose stochastically between different trajectory-lengths (be 'NEUTRAL' about\ntrajectory-lengths). In this paper, we propose evaluation metrics for\nUSEFULNESS and NEUTRALITY. We use a DREST reward function to train simple\nagents to navigate gridworlds, and we find that these agents learn to be USEFUL\nand NEUTRAL. Our results thus suggest that DREST reward functions could also\ntrain advanced agents to be USEFUL and NEUTRAL, and thereby make these advanced\nagents useful and shutdownable.\n", "link": "http://arxiv.org/abs/2407.00805v2", "date": "2024-11-01", "relevancy": 1.8977, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5166}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20shutdownable%20agents%20via%20stochastic%20choice&body=Title%3A%20Towards%20shutdownable%20agents%20via%20stochastic%20choice%0AAuthor%3A%20Elliott%20Thornley%20and%20Alexander%20Roman%20and%20Christos%20Ziakas%20and%20Leyton%20Ho%20and%20Louis%20Thomson%0AAbstract%3A%20%20%20Some%20worry%20that%20advanced%20artificial%20agents%20may%20resist%20being%20shut%20down.%20The%0AIncomplete%20Preferences%20Proposal%20%28IPP%29%20is%20an%20idea%20for%20ensuring%20that%20doesn%27t%0Ahappen.%20A%20key%20part%20of%20the%20IPP%20is%20using%20a%20novel%20%27Discounted%20REward%20for%0ASame-Length%20Trajectories%20%28DREST%29%27%20reward%20function%20to%20train%20agents%20to%20%281%29%20pursue%0Agoals%20effectively%20conditional%20on%20each%20trajectory-length%20%28be%20%27USEFUL%27%29%2C%20and%20%282%29%0Achoose%20stochastically%20between%20different%20trajectory-lengths%20%28be%20%27NEUTRAL%27%20about%0Atrajectory-lengths%29.%20In%20this%20paper%2C%20we%20propose%20evaluation%20metrics%20for%0AUSEFULNESS%20and%20NEUTRALITY.%20We%20use%20a%20DREST%20reward%20function%20to%20train%20simple%0Aagents%20to%20navigate%20gridworlds%2C%20and%20we%20find%20that%20these%20agents%20learn%20to%20be%20USEFUL%0Aand%20NEUTRAL.%20Our%20results%20thus%20suggest%20that%20DREST%20reward%20functions%20could%20also%0Atrain%20advanced%20agents%20to%20be%20USEFUL%20and%20NEUTRAL%2C%20and%20thereby%20make%20these%20advanced%0Aagents%20useful%20and%20shutdownable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520shutdownable%2520agents%2520via%2520stochastic%2520choice%26entry.906535625%3DElliott%2520Thornley%2520and%2520Alexander%2520Roman%2520and%2520Christos%2520Ziakas%2520and%2520Leyton%2520Ho%2520and%2520Louis%2520Thomson%26entry.1292438233%3D%2520%2520Some%2520worry%2520that%2520advanced%2520artificial%2520agents%2520may%2520resist%2520being%2520shut%2520down.%2520The%250AIncomplete%2520Preferences%2520Proposal%2520%2528IPP%2529%2520is%2520an%2520idea%2520for%2520ensuring%2520that%2520doesn%2527t%250Ahappen.%2520A%2520key%2520part%2520of%2520the%2520IPP%2520is%2520using%2520a%2520novel%2520%2527Discounted%2520REward%2520for%250ASame-Length%2520Trajectories%2520%2528DREST%2529%2527%2520reward%2520function%2520to%2520train%2520agents%2520to%2520%25281%2529%2520pursue%250Agoals%2520effectively%2520conditional%2520on%2520each%2520trajectory-length%2520%2528be%2520%2527USEFUL%2527%2529%252C%2520and%2520%25282%2529%250Achoose%2520stochastically%2520between%2520different%2520trajectory-lengths%2520%2528be%2520%2527NEUTRAL%2527%2520about%250Atrajectory-lengths%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520evaluation%2520metrics%2520for%250AUSEFULNESS%2520and%2520NEUTRALITY.%2520We%2520use%2520a%2520DREST%2520reward%2520function%2520to%2520train%2520simple%250Aagents%2520to%2520navigate%2520gridworlds%252C%2520and%2520we%2520find%2520that%2520these%2520agents%2520learn%2520to%2520be%2520USEFUL%250Aand%2520NEUTRAL.%2520Our%2520results%2520thus%2520suggest%2520that%2520DREST%2520reward%2520functions%2520could%2520also%250Atrain%2520advanced%2520agents%2520to%2520be%2520USEFUL%2520and%2520NEUTRAL%252C%2520and%2520thereby%2520make%2520these%2520advanced%250Aagents%2520useful%2520and%2520shutdownable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20shutdownable%20agents%20via%20stochastic%20choice&entry.906535625=Elliott%20Thornley%20and%20Alexander%20Roman%20and%20Christos%20Ziakas%20and%20Leyton%20Ho%20and%20Louis%20Thomson&entry.1292438233=%20%20Some%20worry%20that%20advanced%20artificial%20agents%20may%20resist%20being%20shut%20down.%20The%0AIncomplete%20Preferences%20Proposal%20%28IPP%29%20is%20an%20idea%20for%20ensuring%20that%20doesn%27t%0Ahappen.%20A%20key%20part%20of%20the%20IPP%20is%20using%20a%20novel%20%27Discounted%20REward%20for%0ASame-Length%20Trajectories%20%28DREST%29%27%20reward%20function%20to%20train%20agents%20to%20%281%29%20pursue%0Agoals%20effectively%20conditional%20on%20each%20trajectory-length%20%28be%20%27USEFUL%27%29%2C%20and%20%282%29%0Achoose%20stochastically%20between%20different%20trajectory-lengths%20%28be%20%27NEUTRAL%27%20about%0Atrajectory-lengths%29.%20In%20this%20paper%2C%20we%20propose%20evaluation%20metrics%20for%0AUSEFULNESS%20and%20NEUTRALITY.%20We%20use%20a%20DREST%20reward%20function%20to%20train%20simple%0Aagents%20to%20navigate%20gridworlds%2C%20and%20we%20find%20that%20these%20agents%20learn%20to%20be%20USEFUL%0Aand%20NEUTRAL.%20Our%20results%20thus%20suggest%20that%20DREST%20reward%20functions%20could%20also%0Atrain%20advanced%20agents%20to%20be%20USEFUL%20and%20NEUTRAL%2C%20and%20thereby%20make%20these%20advanced%0Aagents%20useful%20and%20shutdownable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00805v2&entry.124074799=Read"},
{"title": "Analysis of Bootstrap and Subsampling in High-dimensional Regularized\n  Regression", "author": "Lucas Clart\u00e9 and Adrien Vandenbroucque and Guillaume Dalle and Bruno Loureiro and Florent Krzakala and Lenka Zdeborov\u00e1", "abstract": "  We investigate popular resampling methods for estimating the uncertainty of\nstatistical models, such as subsampling, bootstrap and the jackknife, and their\nperformance in high-dimensional supervised regression tasks. We provide a tight\nasymptotic description of the biases and variances estimated by these methods\nin the context of generalized linear models, such as ridge and logistic\nregression, taking the limit where the number of samples $n$ and dimension $d$\nof the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our\nfindings are three-fold: i) resampling methods are fraught with problems in\nhigh dimensions and exhibit the double-descent-like behavior typical of these\nsituations; ii) only when $\\alpha$ is large enough do they provide consistent\nand reliable error estimations (we give convergence rates); iii) in the\nover-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning\npractice, their predictions are not consistent, even with optimal\nregularization.\n", "link": "http://arxiv.org/abs/2402.13622v2", "date": "2024-11-01", "relevancy": 1.8875, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4822}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Bootstrap%20and%20Subsampling%20in%20High-dimensional%20Regularized%0A%20%20Regression&body=Title%3A%20Analysis%20of%20Bootstrap%20and%20Subsampling%20in%20High-dimensional%20Regularized%0A%20%20Regression%0AAuthor%3A%20Lucas%20Clart%C3%A9%20and%20Adrien%20Vandenbroucque%20and%20Guillaume%20Dalle%20and%20Bruno%20Loureiro%20and%20Florent%20Krzakala%20and%20Lenka%20Zdeborov%C3%A1%0AAbstract%3A%20%20%20We%20investigate%20popular%20resampling%20methods%20for%20estimating%20the%20uncertainty%20of%0Astatistical%20models%2C%20such%20as%20subsampling%2C%20bootstrap%20and%20the%20jackknife%2C%20and%20their%0Aperformance%20in%20high-dimensional%20supervised%20regression%20tasks.%20We%20provide%20a%20tight%0Aasymptotic%20description%20of%20the%20biases%20and%20variances%20estimated%20by%20these%20methods%0Ain%20the%20context%20of%20generalized%20linear%20models%2C%20such%20as%20ridge%20and%20logistic%0Aregression%2C%20taking%20the%20limit%20where%20the%20number%20of%20samples%20%24n%24%20and%20dimension%20%24d%24%0Aof%20the%20covariates%20grow%20at%20a%20comparable%20fixed%20rate%20%24%5Calpha%5C%21%3D%5C%21%20n/d%24.%20Our%0Afindings%20are%20three-fold%3A%20i%29%20resampling%20methods%20are%20fraught%20with%20problems%20in%0Ahigh%20dimensions%20and%20exhibit%20the%20double-descent-like%20behavior%20typical%20of%20these%0Asituations%3B%20ii%29%20only%20when%20%24%5Calpha%24%20is%20large%20enough%20do%20they%20provide%20consistent%0Aand%20reliable%20error%20estimations%20%28we%20give%20convergence%20rates%29%3B%20iii%29%20in%20the%0Aover-parametrized%20regime%20%24%5Calpha%5C%21%3C%5C%211%24%20relevant%20to%20modern%20machine%20learning%0Apractice%2C%20their%20predictions%20are%20not%20consistent%2C%20even%20with%20optimal%0Aregularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Bootstrap%2520and%2520Subsampling%2520in%2520High-dimensional%2520Regularized%250A%2520%2520Regression%26entry.906535625%3DLucas%2520Clart%25C3%25A9%2520and%2520Adrien%2520Vandenbroucque%2520and%2520Guillaume%2520Dalle%2520and%2520Bruno%2520Loureiro%2520and%2520Florent%2520Krzakala%2520and%2520Lenka%2520Zdeborov%25C3%25A1%26entry.1292438233%3D%2520%2520We%2520investigate%2520popular%2520resampling%2520methods%2520for%2520estimating%2520the%2520uncertainty%2520of%250Astatistical%2520models%252C%2520such%2520as%2520subsampling%252C%2520bootstrap%2520and%2520the%2520jackknife%252C%2520and%2520their%250Aperformance%2520in%2520high-dimensional%2520supervised%2520regression%2520tasks.%2520We%2520provide%2520a%2520tight%250Aasymptotic%2520description%2520of%2520the%2520biases%2520and%2520variances%2520estimated%2520by%2520these%2520methods%250Ain%2520the%2520context%2520of%2520generalized%2520linear%2520models%252C%2520such%2520as%2520ridge%2520and%2520logistic%250Aregression%252C%2520taking%2520the%2520limit%2520where%2520the%2520number%2520of%2520samples%2520%2524n%2524%2520and%2520dimension%2520%2524d%2524%250Aof%2520the%2520covariates%2520grow%2520at%2520a%2520comparable%2520fixed%2520rate%2520%2524%255Calpha%255C%2521%253D%255C%2521%2520n/d%2524.%2520Our%250Afindings%2520are%2520three-fold%253A%2520i%2529%2520resampling%2520methods%2520are%2520fraught%2520with%2520problems%2520in%250Ahigh%2520dimensions%2520and%2520exhibit%2520the%2520double-descent-like%2520behavior%2520typical%2520of%2520these%250Asituations%253B%2520ii%2529%2520only%2520when%2520%2524%255Calpha%2524%2520is%2520large%2520enough%2520do%2520they%2520provide%2520consistent%250Aand%2520reliable%2520error%2520estimations%2520%2528we%2520give%2520convergence%2520rates%2529%253B%2520iii%2529%2520in%2520the%250Aover-parametrized%2520regime%2520%2524%255Calpha%255C%2521%253C%255C%25211%2524%2520relevant%2520to%2520modern%2520machine%2520learning%250Apractice%252C%2520their%2520predictions%2520are%2520not%2520consistent%252C%2520even%2520with%2520optimal%250Aregularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Bootstrap%20and%20Subsampling%20in%20High-dimensional%20Regularized%0A%20%20Regression&entry.906535625=Lucas%20Clart%C3%A9%20and%20Adrien%20Vandenbroucque%20and%20Guillaume%20Dalle%20and%20Bruno%20Loureiro%20and%20Florent%20Krzakala%20and%20Lenka%20Zdeborov%C3%A1&entry.1292438233=%20%20We%20investigate%20popular%20resampling%20methods%20for%20estimating%20the%20uncertainty%20of%0Astatistical%20models%2C%20such%20as%20subsampling%2C%20bootstrap%20and%20the%20jackknife%2C%20and%20their%0Aperformance%20in%20high-dimensional%20supervised%20regression%20tasks.%20We%20provide%20a%20tight%0Aasymptotic%20description%20of%20the%20biases%20and%20variances%20estimated%20by%20these%20methods%0Ain%20the%20context%20of%20generalized%20linear%20models%2C%20such%20as%20ridge%20and%20logistic%0Aregression%2C%20taking%20the%20limit%20where%20the%20number%20of%20samples%20%24n%24%20and%20dimension%20%24d%24%0Aof%20the%20covariates%20grow%20at%20a%20comparable%20fixed%20rate%20%24%5Calpha%5C%21%3D%5C%21%20n/d%24.%20Our%0Afindings%20are%20three-fold%3A%20i%29%20resampling%20methods%20are%20fraught%20with%20problems%20in%0Ahigh%20dimensions%20and%20exhibit%20the%20double-descent-like%20behavior%20typical%20of%20these%0Asituations%3B%20ii%29%20only%20when%20%24%5Calpha%24%20is%20large%20enough%20do%20they%20provide%20consistent%0Aand%20reliable%20error%20estimations%20%28we%20give%20convergence%20rates%29%3B%20iii%29%20in%20the%0Aover-parametrized%20regime%20%24%5Calpha%5C%21%3C%5C%211%24%20relevant%20to%20modern%20machine%20learning%0Apractice%2C%20their%20predictions%20are%20not%20consistent%2C%20even%20with%20optimal%0Aregularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13622v2&entry.124074799=Read"},
{"title": "A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning", "author": "Arthur Juliani and Jordan T. Ash", "abstract": "  Continual learning with deep neural networks presents challenges distinct\nfrom both the fixed-dataset and convex continual learning regimes. One such\nchallenge is plasticity loss, wherein a neural network trained in an online\nfashion displays a degraded ability to fit new tasks. This problem has been\nextensively studied in both supervised learning and off-policy reinforcement\nlearning (RL), where a number of remedies have been proposed. Still, plasticity\nloss has received less attention in the on-policy deep RL setting. Here we\nperform an extensive set of experiments examining plasticity loss and a variety\nof mitigation methods in on-policy deep RL. We demonstrate that plasticity loss\nis pervasive under domain shift in this regime, and that a number of methods\ndeveloped to resolve it in other settings fail, sometimes even performing worse\nthan applying no intervention at all. In contrast, we find that a class of\n``regenerative'' methods are able to consistently mitigate plasticity loss in a\nvariety of contexts, including in gridworld tasks and more challenging\nenvironments like Montezuma's Revenge and ProcGen.\n", "link": "http://arxiv.org/abs/2405.19153v2", "date": "2024-11-01", "relevancy": 1.8521, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4546}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Plasticity%20Loss%20in%20On-Policy%20Deep%20Reinforcement%20Learning&body=Title%3A%20A%20Study%20of%20Plasticity%20Loss%20in%20On-Policy%20Deep%20Reinforcement%20Learning%0AAuthor%3A%20Arthur%20Juliani%20and%20Jordan%20T.%20Ash%0AAbstract%3A%20%20%20Continual%20learning%20with%20deep%20neural%20networks%20presents%20challenges%20distinct%0Afrom%20both%20the%20fixed-dataset%20and%20convex%20continual%20learning%20regimes.%20One%20such%0Achallenge%20is%20plasticity%20loss%2C%20wherein%20a%20neural%20network%20trained%20in%20an%20online%0Afashion%20displays%20a%20degraded%20ability%20to%20fit%20new%20tasks.%20This%20problem%20has%20been%0Aextensively%20studied%20in%20both%20supervised%20learning%20and%20off-policy%20reinforcement%0Alearning%20%28RL%29%2C%20where%20a%20number%20of%20remedies%20have%20been%20proposed.%20Still%2C%20plasticity%0Aloss%20has%20received%20less%20attention%20in%20the%20on-policy%20deep%20RL%20setting.%20Here%20we%0Aperform%20an%20extensive%20set%20of%20experiments%20examining%20plasticity%20loss%20and%20a%20variety%0Aof%20mitigation%20methods%20in%20on-policy%20deep%20RL.%20We%20demonstrate%20that%20plasticity%20loss%0Ais%20pervasive%20under%20domain%20shift%20in%20this%20regime%2C%20and%20that%20a%20number%20of%20methods%0Adeveloped%20to%20resolve%20it%20in%20other%20settings%20fail%2C%20sometimes%20even%20performing%20worse%0Athan%20applying%20no%20intervention%20at%20all.%20In%20contrast%2C%20we%20find%20that%20a%20class%20of%0A%60%60regenerative%27%27%20methods%20are%20able%20to%20consistently%20mitigate%20plasticity%20loss%20in%20a%0Avariety%20of%20contexts%2C%20including%20in%20gridworld%20tasks%20and%20more%20challenging%0Aenvironments%20like%20Montezuma%27s%20Revenge%20and%20ProcGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Plasticity%2520Loss%2520in%2520On-Policy%2520Deep%2520Reinforcement%2520Learning%26entry.906535625%3DArthur%2520Juliani%2520and%2520Jordan%2520T.%2520Ash%26entry.1292438233%3D%2520%2520Continual%2520learning%2520with%2520deep%2520neural%2520networks%2520presents%2520challenges%2520distinct%250Afrom%2520both%2520the%2520fixed-dataset%2520and%2520convex%2520continual%2520learning%2520regimes.%2520One%2520such%250Achallenge%2520is%2520plasticity%2520loss%252C%2520wherein%2520a%2520neural%2520network%2520trained%2520in%2520an%2520online%250Afashion%2520displays%2520a%2520degraded%2520ability%2520to%2520fit%2520new%2520tasks.%2520This%2520problem%2520has%2520been%250Aextensively%2520studied%2520in%2520both%2520supervised%2520learning%2520and%2520off-policy%2520reinforcement%250Alearning%2520%2528RL%2529%252C%2520where%2520a%2520number%2520of%2520remedies%2520have%2520been%2520proposed.%2520Still%252C%2520plasticity%250Aloss%2520has%2520received%2520less%2520attention%2520in%2520the%2520on-policy%2520deep%2520RL%2520setting.%2520Here%2520we%250Aperform%2520an%2520extensive%2520set%2520of%2520experiments%2520examining%2520plasticity%2520loss%2520and%2520a%2520variety%250Aof%2520mitigation%2520methods%2520in%2520on-policy%2520deep%2520RL.%2520We%2520demonstrate%2520that%2520plasticity%2520loss%250Ais%2520pervasive%2520under%2520domain%2520shift%2520in%2520this%2520regime%252C%2520and%2520that%2520a%2520number%2520of%2520methods%250Adeveloped%2520to%2520resolve%2520it%2520in%2520other%2520settings%2520fail%252C%2520sometimes%2520even%2520performing%2520worse%250Athan%2520applying%2520no%2520intervention%2520at%2520all.%2520In%2520contrast%252C%2520we%2520find%2520that%2520a%2520class%2520of%250A%2560%2560regenerative%2527%2527%2520methods%2520are%2520able%2520to%2520consistently%2520mitigate%2520plasticity%2520loss%2520in%2520a%250Avariety%2520of%2520contexts%252C%2520including%2520in%2520gridworld%2520tasks%2520and%2520more%2520challenging%250Aenvironments%2520like%2520Montezuma%2527s%2520Revenge%2520and%2520ProcGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Plasticity%20Loss%20in%20On-Policy%20Deep%20Reinforcement%20Learning&entry.906535625=Arthur%20Juliani%20and%20Jordan%20T.%20Ash&entry.1292438233=%20%20Continual%20learning%20with%20deep%20neural%20networks%20presents%20challenges%20distinct%0Afrom%20both%20the%20fixed-dataset%20and%20convex%20continual%20learning%20regimes.%20One%20such%0Achallenge%20is%20plasticity%20loss%2C%20wherein%20a%20neural%20network%20trained%20in%20an%20online%0Afashion%20displays%20a%20degraded%20ability%20to%20fit%20new%20tasks.%20This%20problem%20has%20been%0Aextensively%20studied%20in%20both%20supervised%20learning%20and%20off-policy%20reinforcement%0Alearning%20%28RL%29%2C%20where%20a%20number%20of%20remedies%20have%20been%20proposed.%20Still%2C%20plasticity%0Aloss%20has%20received%20less%20attention%20in%20the%20on-policy%20deep%20RL%20setting.%20Here%20we%0Aperform%20an%20extensive%20set%20of%20experiments%20examining%20plasticity%20loss%20and%20a%20variety%0Aof%20mitigation%20methods%20in%20on-policy%20deep%20RL.%20We%20demonstrate%20that%20plasticity%20loss%0Ais%20pervasive%20under%20domain%20shift%20in%20this%20regime%2C%20and%20that%20a%20number%20of%20methods%0Adeveloped%20to%20resolve%20it%20in%20other%20settings%20fail%2C%20sometimes%20even%20performing%20worse%0Athan%20applying%20no%20intervention%20at%20all.%20In%20contrast%2C%20we%20find%20that%20a%20class%20of%0A%60%60regenerative%27%27%20methods%20are%20able%20to%20consistently%20mitigate%20plasticity%20loss%20in%20a%0Avariety%20of%20contexts%2C%20including%20in%20gridworld%20tasks%20and%20more%20challenging%0Aenvironments%20like%20Montezuma%27s%20Revenge%20and%20ProcGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19153v2&entry.124074799=Read"},
{"title": "Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity", "author": "Jake Varley and Sumeet Singh and Deepali Jain and Krzysztof Choromanski and Andy Zeng and Somnath Basu Roy Chowdhury and Avinava Dubey and Vikas Sindhwani", "abstract": "  We present an embodied AI system which receives open-ended natural language\ninstructions from a human, and controls two arms to collaboratively accomplish\npotentially long-horizon tasks over a large workspace. Our system is modular:\nit deploys state of the art Large Language Models for task\nplanning,Vision-Language models for semantic perception, and Point Cloud\ntransformers for grasping. With semantic and physical safety in mind, these\nmodules are interfaced with a real-time trajectory optimizer and a compliant\ntracking controller to enable human-robot proximity. We demonstrate performance\nfor the following tasks: bi-arm sorting, bottle opening, and trash disposal\ntasks. These are done zero-shot where the models used have not been trained\nwith any real world data from this bi-arm robot, scenes or workspace. Composing\nboth learning- and non-learning-based components in a modular fashion with\ninterpretable inputs and outputs allows the user to easily debug points of\nfailures and fragilities. One may also in-place swap modules to improve the\nrobustness of the overall platform, for instance with imitation-learned\npolicies. Please see https://sites.google.com/corp/view/safe-robots .\n", "link": "http://arxiv.org/abs/2404.03570v3", "date": "2024-11-01", "relevancy": 1.7973, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6041}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity&body=Title%3A%20Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity%0AAuthor%3A%20Jake%20Varley%20and%20Sumeet%20Singh%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Andy%20Zeng%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Avinava%20Dubey%20and%20Vikas%20Sindhwani%0AAbstract%3A%20%20%20We%20present%20an%20embodied%20AI%20system%20which%20receives%20open-ended%20natural%20language%0Ainstructions%20from%20a%20human%2C%20and%20controls%20two%20arms%20to%20collaboratively%20accomplish%0Apotentially%20long-horizon%20tasks%20over%20a%20large%20workspace.%20Our%20system%20is%20modular%3A%0Ait%20deploys%20state%20of%20the%20art%20Large%20Language%20Models%20for%20task%0Aplanning%2CVision-Language%20models%20for%20semantic%20perception%2C%20and%20Point%20Cloud%0Atransformers%20for%20grasping.%20With%20semantic%20and%20physical%20safety%20in%20mind%2C%20these%0Amodules%20are%20interfaced%20with%20a%20real-time%20trajectory%20optimizer%20and%20a%20compliant%0Atracking%20controller%20to%20enable%20human-robot%20proximity.%20We%20demonstrate%20performance%0Afor%20the%20following%20tasks%3A%20bi-arm%20sorting%2C%20bottle%20opening%2C%20and%20trash%20disposal%0Atasks.%20These%20are%20done%20zero-shot%20where%20the%20models%20used%20have%20not%20been%20trained%0Awith%20any%20real%20world%20data%20from%20this%20bi-arm%20robot%2C%20scenes%20or%20workspace.%20Composing%0Aboth%20learning-%20and%20non-learning-based%20components%20in%20a%20modular%20fashion%20with%0Ainterpretable%20inputs%20and%20outputs%20allows%20the%20user%20to%20easily%20debug%20points%20of%0Afailures%20and%20fragilities.%20One%20may%20also%20in-place%20swap%20modules%20to%20improve%20the%0Arobustness%20of%20the%20overall%20platform%2C%20for%20instance%20with%20imitation-learned%0Apolicies.%20Please%20see%20https%3A//sites.google.com/corp/view/safe-robots%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03570v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520AI%2520with%2520Two%2520Arms%253A%2520Zero-shot%2520Learning%252C%2520Safety%2520and%2520Modularity%26entry.906535625%3DJake%2520Varley%2520and%2520Sumeet%2520Singh%2520and%2520Deepali%2520Jain%2520and%2520Krzysztof%2520Choromanski%2520and%2520Andy%2520Zeng%2520and%2520Somnath%2520Basu%2520Roy%2520Chowdhury%2520and%2520Avinava%2520Dubey%2520and%2520Vikas%2520Sindhwani%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520embodied%2520AI%2520system%2520which%2520receives%2520open-ended%2520natural%2520language%250Ainstructions%2520from%2520a%2520human%252C%2520and%2520controls%2520two%2520arms%2520to%2520collaboratively%2520accomplish%250Apotentially%2520long-horizon%2520tasks%2520over%2520a%2520large%2520workspace.%2520Our%2520system%2520is%2520modular%253A%250Ait%2520deploys%2520state%2520of%2520the%2520art%2520Large%2520Language%2520Models%2520for%2520task%250Aplanning%252CVision-Language%2520models%2520for%2520semantic%2520perception%252C%2520and%2520Point%2520Cloud%250Atransformers%2520for%2520grasping.%2520With%2520semantic%2520and%2520physical%2520safety%2520in%2520mind%252C%2520these%250Amodules%2520are%2520interfaced%2520with%2520a%2520real-time%2520trajectory%2520optimizer%2520and%2520a%2520compliant%250Atracking%2520controller%2520to%2520enable%2520human-robot%2520proximity.%2520We%2520demonstrate%2520performance%250Afor%2520the%2520following%2520tasks%253A%2520bi-arm%2520sorting%252C%2520bottle%2520opening%252C%2520and%2520trash%2520disposal%250Atasks.%2520These%2520are%2520done%2520zero-shot%2520where%2520the%2520models%2520used%2520have%2520not%2520been%2520trained%250Awith%2520any%2520real%2520world%2520data%2520from%2520this%2520bi-arm%2520robot%252C%2520scenes%2520or%2520workspace.%2520Composing%250Aboth%2520learning-%2520and%2520non-learning-based%2520components%2520in%2520a%2520modular%2520fashion%2520with%250Ainterpretable%2520inputs%2520and%2520outputs%2520allows%2520the%2520user%2520to%2520easily%2520debug%2520points%2520of%250Afailures%2520and%2520fragilities.%2520One%2520may%2520also%2520in-place%2520swap%2520modules%2520to%2520improve%2520the%250Arobustness%2520of%2520the%2520overall%2520platform%252C%2520for%2520instance%2520with%2520imitation-learned%250Apolicies.%2520Please%2520see%2520https%253A//sites.google.com/corp/view/safe-robots%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03570v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20AI%20with%20Two%20Arms%3A%20Zero-shot%20Learning%2C%20Safety%20and%20Modularity&entry.906535625=Jake%20Varley%20and%20Sumeet%20Singh%20and%20Deepali%20Jain%20and%20Krzysztof%20Choromanski%20and%20Andy%20Zeng%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Avinava%20Dubey%20and%20Vikas%20Sindhwani&entry.1292438233=%20%20We%20present%20an%20embodied%20AI%20system%20which%20receives%20open-ended%20natural%20language%0Ainstructions%20from%20a%20human%2C%20and%20controls%20two%20arms%20to%20collaboratively%20accomplish%0Apotentially%20long-horizon%20tasks%20over%20a%20large%20workspace.%20Our%20system%20is%20modular%3A%0Ait%20deploys%20state%20of%20the%20art%20Large%20Language%20Models%20for%20task%0Aplanning%2CVision-Language%20models%20for%20semantic%20perception%2C%20and%20Point%20Cloud%0Atransformers%20for%20grasping.%20With%20semantic%20and%20physical%20safety%20in%20mind%2C%20these%0Amodules%20are%20interfaced%20with%20a%20real-time%20trajectory%20optimizer%20and%20a%20compliant%0Atracking%20controller%20to%20enable%20human-robot%20proximity.%20We%20demonstrate%20performance%0Afor%20the%20following%20tasks%3A%20bi-arm%20sorting%2C%20bottle%20opening%2C%20and%20trash%20disposal%0Atasks.%20These%20are%20done%20zero-shot%20where%20the%20models%20used%20have%20not%20been%20trained%0Awith%20any%20real%20world%20data%20from%20this%20bi-arm%20robot%2C%20scenes%20or%20workspace.%20Composing%0Aboth%20learning-%20and%20non-learning-based%20components%20in%20a%20modular%20fashion%20with%0Ainterpretable%20inputs%20and%20outputs%20allows%20the%20user%20to%20easily%20debug%20points%20of%0Afailures%20and%20fragilities.%20One%20may%20also%20in-place%20swap%20modules%20to%20improve%20the%0Arobustness%20of%20the%20overall%20platform%2C%20for%20instance%20with%20imitation-learned%0Apolicies.%20Please%20see%20https%3A//sites.google.com/corp/view/safe-robots%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03570v3&entry.124074799=Read"},
{"title": "Intruding with Words: Towards Understanding Graph Injection Attacks at\n  the Text Level", "author": "Runlin Lei and Yuwei Hu and Yuchen Ren and Zhewei Wei", "abstract": "  Graph Neural Networks (GNNs) excel across various applications but remain\nvulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs),\nwhich inject malicious nodes into the original graph and pose realistic\nthreats. Text-attributed graphs (TAGs), where nodes are associated with textual\nfeatures, are crucial due to their prevalence in real-world applications and\nare commonly used to evaluate these vulnerabilities. However, existing research\nonly focuses on embedding-level GIAs, which inject node embeddings rather than\nactual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting\nthree novel attack designs that inject textual content into the graph. Through\ntheoretical and empirical analysis, we demonstrate that text interpretability,\na factor previously overlooked at the embedding level, plays a crucial role in\nattack strength. Among the designs we investigate, the Word-frequency-based\nText-level GIA (WTGIA) is particularly notable for its balance between\nperformance and interpretability. Despite the success of WTGIA, we discover\nthat defenders can easily enhance their defenses with customized text embedding\nmethods or large language model (LLM)--based predictors. These insights\nunderscore the necessity for further research into the potential and practical\nsignificance of text-level GIAs.\n", "link": "http://arxiv.org/abs/2405.16405v2", "date": "2024-11-01", "relevancy": 1.7803, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4528}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4482}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intruding%20with%20Words%3A%20Towards%20Understanding%20Graph%20Injection%20Attacks%20at%0A%20%20the%20Text%20Level&body=Title%3A%20Intruding%20with%20Words%3A%20Towards%20Understanding%20Graph%20Injection%20Attacks%20at%0A%20%20the%20Text%20Level%0AAuthor%3A%20Runlin%20Lei%20and%20Yuwei%20Hu%20and%20Yuchen%20Ren%20and%20Zhewei%20Wei%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20across%20various%20applications%20but%20remain%0Avulnerable%20to%20adversarial%20attacks%2C%20particularly%20Graph%20Injection%20Attacks%20%28GIAs%29%2C%0Awhich%20inject%20malicious%20nodes%20into%20the%20original%20graph%20and%20pose%20realistic%0Athreats.%20Text-attributed%20graphs%20%28TAGs%29%2C%20where%20nodes%20are%20associated%20with%20textual%0Afeatures%2C%20are%20crucial%20due%20to%20their%20prevalence%20in%20real-world%20applications%20and%0Aare%20commonly%20used%20to%20evaluate%20these%20vulnerabilities.%20However%2C%20existing%20research%0Aonly%20focuses%20on%20embedding-level%20GIAs%2C%20which%20inject%20node%20embeddings%20rather%20than%0Aactual%20textual%20content%2C%20limiting%20their%20applicability%20and%20simplifying%20detection.%0AIn%20this%20paper%2C%20we%20pioneer%20the%20exploration%20of%20GIAs%20at%20the%20text%20level%2C%20presenting%0Athree%20novel%20attack%20designs%20that%20inject%20textual%20content%20into%20the%20graph.%20Through%0Atheoretical%20and%20empirical%20analysis%2C%20we%20demonstrate%20that%20text%20interpretability%2C%0Aa%20factor%20previously%20overlooked%20at%20the%20embedding%20level%2C%20plays%20a%20crucial%20role%20in%0Aattack%20strength.%20Among%20the%20designs%20we%20investigate%2C%20the%20Word-frequency-based%0AText-level%20GIA%20%28WTGIA%29%20is%20particularly%20notable%20for%20its%20balance%20between%0Aperformance%20and%20interpretability.%20Despite%20the%20success%20of%20WTGIA%2C%20we%20discover%0Athat%20defenders%20can%20easily%20enhance%20their%20defenses%20with%20customized%20text%20embedding%0Amethods%20or%20large%20language%20model%20%28LLM%29--based%20predictors.%20These%20insights%0Aunderscore%20the%20necessity%20for%20further%20research%20into%20the%20potential%20and%20practical%0Asignificance%20of%20text-level%20GIAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntruding%2520with%2520Words%253A%2520Towards%2520Understanding%2520Graph%2520Injection%2520Attacks%2520at%250A%2520%2520the%2520Text%2520Level%26entry.906535625%3DRunlin%2520Lei%2520and%2520Yuwei%2520Hu%2520and%2520Yuchen%2520Ren%2520and%2520Zhewei%2520Wei%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520across%2520various%2520applications%2520but%2520remain%250Avulnerable%2520to%2520adversarial%2520attacks%252C%2520particularly%2520Graph%2520Injection%2520Attacks%2520%2528GIAs%2529%252C%250Awhich%2520inject%2520malicious%2520nodes%2520into%2520the%2520original%2520graph%2520and%2520pose%2520realistic%250Athreats.%2520Text-attributed%2520graphs%2520%2528TAGs%2529%252C%2520where%2520nodes%2520are%2520associated%2520with%2520textual%250Afeatures%252C%2520are%2520crucial%2520due%2520to%2520their%2520prevalence%2520in%2520real-world%2520applications%2520and%250Aare%2520commonly%2520used%2520to%2520evaluate%2520these%2520vulnerabilities.%2520However%252C%2520existing%2520research%250Aonly%2520focuses%2520on%2520embedding-level%2520GIAs%252C%2520which%2520inject%2520node%2520embeddings%2520rather%2520than%250Aactual%2520textual%2520content%252C%2520limiting%2520their%2520applicability%2520and%2520simplifying%2520detection.%250AIn%2520this%2520paper%252C%2520we%2520pioneer%2520the%2520exploration%2520of%2520GIAs%2520at%2520the%2520text%2520level%252C%2520presenting%250Athree%2520novel%2520attack%2520designs%2520that%2520inject%2520textual%2520content%2520into%2520the%2520graph.%2520Through%250Atheoretical%2520and%2520empirical%2520analysis%252C%2520we%2520demonstrate%2520that%2520text%2520interpretability%252C%250Aa%2520factor%2520previously%2520overlooked%2520at%2520the%2520embedding%2520level%252C%2520plays%2520a%2520crucial%2520role%2520in%250Aattack%2520strength.%2520Among%2520the%2520designs%2520we%2520investigate%252C%2520the%2520Word-frequency-based%250AText-level%2520GIA%2520%2528WTGIA%2529%2520is%2520particularly%2520notable%2520for%2520its%2520balance%2520between%250Aperformance%2520and%2520interpretability.%2520Despite%2520the%2520success%2520of%2520WTGIA%252C%2520we%2520discover%250Athat%2520defenders%2520can%2520easily%2520enhance%2520their%2520defenses%2520with%2520customized%2520text%2520embedding%250Amethods%2520or%2520large%2520language%2520model%2520%2528LLM%2529--based%2520predictors.%2520These%2520insights%250Aunderscore%2520the%2520necessity%2520for%2520further%2520research%2520into%2520the%2520potential%2520and%2520practical%250Asignificance%2520of%2520text-level%2520GIAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intruding%20with%20Words%3A%20Towards%20Understanding%20Graph%20Injection%20Attacks%20at%0A%20%20the%20Text%20Level&entry.906535625=Runlin%20Lei%20and%20Yuwei%20Hu%20and%20Yuchen%20Ren%20and%20Zhewei%20Wei&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20across%20various%20applications%20but%20remain%0Avulnerable%20to%20adversarial%20attacks%2C%20particularly%20Graph%20Injection%20Attacks%20%28GIAs%29%2C%0Awhich%20inject%20malicious%20nodes%20into%20the%20original%20graph%20and%20pose%20realistic%0Athreats.%20Text-attributed%20graphs%20%28TAGs%29%2C%20where%20nodes%20are%20associated%20with%20textual%0Afeatures%2C%20are%20crucial%20due%20to%20their%20prevalence%20in%20real-world%20applications%20and%0Aare%20commonly%20used%20to%20evaluate%20these%20vulnerabilities.%20However%2C%20existing%20research%0Aonly%20focuses%20on%20embedding-level%20GIAs%2C%20which%20inject%20node%20embeddings%20rather%20than%0Aactual%20textual%20content%2C%20limiting%20their%20applicability%20and%20simplifying%20detection.%0AIn%20this%20paper%2C%20we%20pioneer%20the%20exploration%20of%20GIAs%20at%20the%20text%20level%2C%20presenting%0Athree%20novel%20attack%20designs%20that%20inject%20textual%20content%20into%20the%20graph.%20Through%0Atheoretical%20and%20empirical%20analysis%2C%20we%20demonstrate%20that%20text%20interpretability%2C%0Aa%20factor%20previously%20overlooked%20at%20the%20embedding%20level%2C%20plays%20a%20crucial%20role%20in%0Aattack%20strength.%20Among%20the%20designs%20we%20investigate%2C%20the%20Word-frequency-based%0AText-level%20GIA%20%28WTGIA%29%20is%20particularly%20notable%20for%20its%20balance%20between%0Aperformance%20and%20interpretability.%20Despite%20the%20success%20of%20WTGIA%2C%20we%20discover%0Athat%20defenders%20can%20easily%20enhance%20their%20defenses%20with%20customized%20text%20embedding%0Amethods%20or%20large%20language%20model%20%28LLM%29--based%20predictors.%20These%20insights%0Aunderscore%20the%20necessity%20for%20further%20research%20into%20the%20potential%20and%20practical%0Asignificance%20of%20text-level%20GIAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16405v2&entry.124074799=Read"},
{"title": "Can Large Language Model Agents Simulate Human Trust Behavior?", "author": "Chengxing Xie and Canyu Chen and Feiran Jia and Ziyu Ye and Shiyang Lai and Kai Shu and Jindong Gu and Adel Bibi and Ziniu Hu and David Jurgens and James Evans and Philip Torr and Bernard Ghanem and Guohao Li", "abstract": "  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one critical and elemental\nbehavior in human interactions, trust, and investigate whether LLM agents can\nsimulate human trust behavior. We first find that LLM agents generally exhibit\ntrust behavior, referred to as agent trust, under the framework of Trust Games,\nwhich are widely recognized in behavioral economics. Then, we discover that\nGPT-4 agents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.\n", "link": "http://arxiv.org/abs/2402.04559v4", "date": "2024-11-01", "relevancy": 1.762, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4412}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4403}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Model%20Agents%20Simulate%20Human%20Trust%20Behavior%3F&body=Title%3A%20Can%20Large%20Language%20Model%20Agents%20Simulate%20Human%20Trust%20Behavior%3F%0AAuthor%3A%20Chengxing%20Xie%20and%20Canyu%20Chen%20and%20Feiran%20Jia%20and%20Ziyu%20Ye%20and%20Shiyang%20Lai%20and%20Kai%20Shu%20and%20Jindong%20Gu%20and%20Adel%20Bibi%20and%20Ziniu%20Hu%20and%20David%20Jurgens%20and%20James%20Evans%20and%20Philip%20Torr%20and%20Bernard%20Ghanem%20and%20Guohao%20Li%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20been%20increasingly%20adopted%20as%0Asimulation%20tools%20to%20model%20humans%20in%20social%20science%20and%20role-playing%0Aapplications.%20However%2C%20one%20fundamental%20question%20remains%3A%20can%20LLM%20agents%20really%0Asimulate%20human%20behavior%3F%20In%20this%20paper%2C%20we%20focus%20on%20one%20critical%20and%20elemental%0Abehavior%20in%20human%20interactions%2C%20trust%2C%20and%20investigate%20whether%20LLM%20agents%20can%0Asimulate%20human%20trust%20behavior.%20We%20first%20find%20that%20LLM%20agents%20generally%20exhibit%0Atrust%20behavior%2C%20referred%20to%20as%20agent%20trust%2C%20under%20the%20framework%20of%20Trust%20Games%2C%0Awhich%20are%20widely%20recognized%20in%20behavioral%20economics.%20Then%2C%20we%20discover%20that%0AGPT-4%20agents%20manifest%20high%20behavioral%20alignment%20with%20humans%20in%20terms%20of%20trust%0Abehavior%2C%20indicating%20the%20feasibility%20of%20simulating%20human%20trust%20behavior%20with%0ALLM%20agents.%20In%20addition%2C%20we%20probe%20the%20biases%20of%20agent%20trust%20and%20differences%20in%0Aagent%20trust%20towards%20other%20LLM%20agents%20and%20humans.%20We%20also%20explore%20the%20intrinsic%0Aproperties%20of%20agent%20trust%20under%20conditions%20including%20external%20manipulations%20and%0Aadvanced%20reasoning%20strategies.%20Our%20study%20provides%20new%20insights%20into%20the%0Abehaviors%20of%20LLM%20agents%20and%20the%20fundamental%20analogy%20between%20LLMs%20and%20humans%0Abeyond%20value%20alignment.%20We%20further%20illustrate%20broader%20implications%20of%20our%0Adiscoveries%20for%20applications%20where%20trust%20is%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04559v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Model%2520Agents%2520Simulate%2520Human%2520Trust%2520Behavior%253F%26entry.906535625%3DChengxing%2520Xie%2520and%2520Canyu%2520Chen%2520and%2520Feiran%2520Jia%2520and%2520Ziyu%2520Ye%2520and%2520Shiyang%2520Lai%2520and%2520Kai%2520Shu%2520and%2520Jindong%2520Gu%2520and%2520Adel%2520Bibi%2520and%2520Ziniu%2520Hu%2520and%2520David%2520Jurgens%2520and%2520James%2520Evans%2520and%2520Philip%2520Torr%2520and%2520Bernard%2520Ghanem%2520and%2520Guohao%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520been%2520increasingly%2520adopted%2520as%250Asimulation%2520tools%2520to%2520model%2520humans%2520in%2520social%2520science%2520and%2520role-playing%250Aapplications.%2520However%252C%2520one%2520fundamental%2520question%2520remains%253A%2520can%2520LLM%2520agents%2520really%250Asimulate%2520human%2520behavior%253F%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520one%2520critical%2520and%2520elemental%250Abehavior%2520in%2520human%2520interactions%252C%2520trust%252C%2520and%2520investigate%2520whether%2520LLM%2520agents%2520can%250Asimulate%2520human%2520trust%2520behavior.%2520We%2520first%2520find%2520that%2520LLM%2520agents%2520generally%2520exhibit%250Atrust%2520behavior%252C%2520referred%2520to%2520as%2520agent%2520trust%252C%2520under%2520the%2520framework%2520of%2520Trust%2520Games%252C%250Awhich%2520are%2520widely%2520recognized%2520in%2520behavioral%2520economics.%2520Then%252C%2520we%2520discover%2520that%250AGPT-4%2520agents%2520manifest%2520high%2520behavioral%2520alignment%2520with%2520humans%2520in%2520terms%2520of%2520trust%250Abehavior%252C%2520indicating%2520the%2520feasibility%2520of%2520simulating%2520human%2520trust%2520behavior%2520with%250ALLM%2520agents.%2520In%2520addition%252C%2520we%2520probe%2520the%2520biases%2520of%2520agent%2520trust%2520and%2520differences%2520in%250Aagent%2520trust%2520towards%2520other%2520LLM%2520agents%2520and%2520humans.%2520We%2520also%2520explore%2520the%2520intrinsic%250Aproperties%2520of%2520agent%2520trust%2520under%2520conditions%2520including%2520external%2520manipulations%2520and%250Aadvanced%2520reasoning%2520strategies.%2520Our%2520study%2520provides%2520new%2520insights%2520into%2520the%250Abehaviors%2520of%2520LLM%2520agents%2520and%2520the%2520fundamental%2520analogy%2520between%2520LLMs%2520and%2520humans%250Abeyond%2520value%2520alignment.%2520We%2520further%2520illustrate%2520broader%2520implications%2520of%2520our%250Adiscoveries%2520for%2520applications%2520where%2520trust%2520is%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04559v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Model%20Agents%20Simulate%20Human%20Trust%20Behavior%3F&entry.906535625=Chengxing%20Xie%20and%20Canyu%20Chen%20and%20Feiran%20Jia%20and%20Ziyu%20Ye%20and%20Shiyang%20Lai%20and%20Kai%20Shu%20and%20Jindong%20Gu%20and%20Adel%20Bibi%20and%20Ziniu%20Hu%20and%20David%20Jurgens%20and%20James%20Evans%20and%20Philip%20Torr%20and%20Bernard%20Ghanem%20and%20Guohao%20Li&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20been%20increasingly%20adopted%20as%0Asimulation%20tools%20to%20model%20humans%20in%20social%20science%20and%20role-playing%0Aapplications.%20However%2C%20one%20fundamental%20question%20remains%3A%20can%20LLM%20agents%20really%0Asimulate%20human%20behavior%3F%20In%20this%20paper%2C%20we%20focus%20on%20one%20critical%20and%20elemental%0Abehavior%20in%20human%20interactions%2C%20trust%2C%20and%20investigate%20whether%20LLM%20agents%20can%0Asimulate%20human%20trust%20behavior.%20We%20first%20find%20that%20LLM%20agents%20generally%20exhibit%0Atrust%20behavior%2C%20referred%20to%20as%20agent%20trust%2C%20under%20the%20framework%20of%20Trust%20Games%2C%0Awhich%20are%20widely%20recognized%20in%20behavioral%20economics.%20Then%2C%20we%20discover%20that%0AGPT-4%20agents%20manifest%20high%20behavioral%20alignment%20with%20humans%20in%20terms%20of%20trust%0Abehavior%2C%20indicating%20the%20feasibility%20of%20simulating%20human%20trust%20behavior%20with%0ALLM%20agents.%20In%20addition%2C%20we%20probe%20the%20biases%20of%20agent%20trust%20and%20differences%20in%0Aagent%20trust%20towards%20other%20LLM%20agents%20and%20humans.%20We%20also%20explore%20the%20intrinsic%0Aproperties%20of%20agent%20trust%20under%20conditions%20including%20external%20manipulations%20and%0Aadvanced%20reasoning%20strategies.%20Our%20study%20provides%20new%20insights%20into%20the%0Abehaviors%20of%20LLM%20agents%20and%20the%20fundamental%20analogy%20between%20LLMs%20and%20humans%0Abeyond%20value%20alignment.%20We%20further%20illustrate%20broader%20implications%20of%20our%0Adiscoveries%20for%20applications%20where%20trust%20is%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04559v4&entry.124074799=Read"},
{"title": "Neur2BiLO: Neural Bilevel Optimization", "author": "Justin Dumouchelle and Esther Julien and Jannis Kurtz and Elias B. Khalil", "abstract": "  Bilevel optimization deals with nested problems in which a leader takes the\nfirst decision to minimize their objective function while accounting for a\nfollower's best-response reaction. Constrained bilevel problems with integer\nvariables are particularly notorious for their hardness. While exact solvers\nhave been proposed for mixed-integer linear bilevel optimization, they tend to\nscale poorly with problem size and are hard to generalize to the non-linear\ncase. On the other hand, problem-specific algorithms (exact and heuristic) are\nlimited in scope. Under a data-driven setting in which similar instances of a\nbilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds\na neural network approximation of the leader's or follower's value function,\ntrained via supervised regression, into an easy-to-solve mixed-integer program.\nNeur2BiLO serves as a heuristic that produces high-quality solutions extremely\nfast for four applications with linear and non-linear objectives and pure and\nmixed-integer variables.\n", "link": "http://arxiv.org/abs/2402.02552v2", "date": "2024-11-01", "relevancy": 1.7541, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neur2BiLO%3A%20Neural%20Bilevel%20Optimization&body=Title%3A%20Neur2BiLO%3A%20Neural%20Bilevel%20Optimization%0AAuthor%3A%20Justin%20Dumouchelle%20and%20Esther%20Julien%20and%20Jannis%20Kurtz%20and%20Elias%20B.%20Khalil%0AAbstract%3A%20%20%20Bilevel%20optimization%20deals%20with%20nested%20problems%20in%20which%20a%20leader%20takes%20the%0Afirst%20decision%20to%20minimize%20their%20objective%20function%20while%20accounting%20for%20a%0Afollower%27s%20best-response%20reaction.%20Constrained%20bilevel%20problems%20with%20integer%0Avariables%20are%20particularly%20notorious%20for%20their%20hardness.%20While%20exact%20solvers%0Ahave%20been%20proposed%20for%20mixed-integer%20linear%20bilevel%20optimization%2C%20they%20tend%20to%0Ascale%20poorly%20with%20problem%20size%20and%20are%20hard%20to%20generalize%20to%20the%20non-linear%0Acase.%20On%20the%20other%20hand%2C%20problem-specific%20algorithms%20%28exact%20and%20heuristic%29%20are%0Alimited%20in%20scope.%20Under%20a%20data-driven%20setting%20in%20which%20similar%20instances%20of%20a%0Abilevel%20problem%20are%20solved%20routinely%2C%20our%20proposed%20framework%2C%20Neur2BiLO%2C%20embeds%0Aa%20neural%20network%20approximation%20of%20the%20leader%27s%20or%20follower%27s%20value%20function%2C%0Atrained%20via%20supervised%20regression%2C%20into%20an%20easy-to-solve%20mixed-integer%20program.%0ANeur2BiLO%20serves%20as%20a%20heuristic%20that%20produces%20high-quality%20solutions%20extremely%0Afast%20for%20four%20applications%20with%20linear%20and%20non-linear%20objectives%20and%20pure%20and%0Amixed-integer%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeur2BiLO%253A%2520Neural%2520Bilevel%2520Optimization%26entry.906535625%3DJustin%2520Dumouchelle%2520and%2520Esther%2520Julien%2520and%2520Jannis%2520Kurtz%2520and%2520Elias%2520B.%2520Khalil%26entry.1292438233%3D%2520%2520Bilevel%2520optimization%2520deals%2520with%2520nested%2520problems%2520in%2520which%2520a%2520leader%2520takes%2520the%250Afirst%2520decision%2520to%2520minimize%2520their%2520objective%2520function%2520while%2520accounting%2520for%2520a%250Afollower%2527s%2520best-response%2520reaction.%2520Constrained%2520bilevel%2520problems%2520with%2520integer%250Avariables%2520are%2520particularly%2520notorious%2520for%2520their%2520hardness.%2520While%2520exact%2520solvers%250Ahave%2520been%2520proposed%2520for%2520mixed-integer%2520linear%2520bilevel%2520optimization%252C%2520they%2520tend%2520to%250Ascale%2520poorly%2520with%2520problem%2520size%2520and%2520are%2520hard%2520to%2520generalize%2520to%2520the%2520non-linear%250Acase.%2520On%2520the%2520other%2520hand%252C%2520problem-specific%2520algorithms%2520%2528exact%2520and%2520heuristic%2529%2520are%250Alimited%2520in%2520scope.%2520Under%2520a%2520data-driven%2520setting%2520in%2520which%2520similar%2520instances%2520of%2520a%250Abilevel%2520problem%2520are%2520solved%2520routinely%252C%2520our%2520proposed%2520framework%252C%2520Neur2BiLO%252C%2520embeds%250Aa%2520neural%2520network%2520approximation%2520of%2520the%2520leader%2527s%2520or%2520follower%2527s%2520value%2520function%252C%250Atrained%2520via%2520supervised%2520regression%252C%2520into%2520an%2520easy-to-solve%2520mixed-integer%2520program.%250ANeur2BiLO%2520serves%2520as%2520a%2520heuristic%2520that%2520produces%2520high-quality%2520solutions%2520extremely%250Afast%2520for%2520four%2520applications%2520with%2520linear%2520and%2520non-linear%2520objectives%2520and%2520pure%2520and%250Amixed-integer%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neur2BiLO%3A%20Neural%20Bilevel%20Optimization&entry.906535625=Justin%20Dumouchelle%20and%20Esther%20Julien%20and%20Jannis%20Kurtz%20and%20Elias%20B.%20Khalil&entry.1292438233=%20%20Bilevel%20optimization%20deals%20with%20nested%20problems%20in%20which%20a%20leader%20takes%20the%0Afirst%20decision%20to%20minimize%20their%20objective%20function%20while%20accounting%20for%20a%0Afollower%27s%20best-response%20reaction.%20Constrained%20bilevel%20problems%20with%20integer%0Avariables%20are%20particularly%20notorious%20for%20their%20hardness.%20While%20exact%20solvers%0Ahave%20been%20proposed%20for%20mixed-integer%20linear%20bilevel%20optimization%2C%20they%20tend%20to%0Ascale%20poorly%20with%20problem%20size%20and%20are%20hard%20to%20generalize%20to%20the%20non-linear%0Acase.%20On%20the%20other%20hand%2C%20problem-specific%20algorithms%20%28exact%20and%20heuristic%29%20are%0Alimited%20in%20scope.%20Under%20a%20data-driven%20setting%20in%20which%20similar%20instances%20of%20a%0Abilevel%20problem%20are%20solved%20routinely%2C%20our%20proposed%20framework%2C%20Neur2BiLO%2C%20embeds%0Aa%20neural%20network%20approximation%20of%20the%20leader%27s%20or%20follower%27s%20value%20function%2C%0Atrained%20via%20supervised%20regression%2C%20into%20an%20easy-to-solve%20mixed-integer%20program.%0ANeur2BiLO%20serves%20as%20a%20heuristic%20that%20produces%20high-quality%20solutions%20extremely%0Afast%20for%20four%20applications%20with%20linear%20and%20non-linear%20objectives%20and%20pure%20and%0Amixed-integer%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02552v2&entry.124074799=Read"},
{"title": "Perceptive Pedipulation with Local Obstacle Avoidance", "author": "Jonas Stolle and Philip Arm and Mayank Mittal and Marco Hutter", "abstract": "  Pedipulation leverages the feet of legged robots for mobile manipulation,\neliminating the need for dedicated robotic arms. While previous works have\nshowcased blind and task-specific pedipulation skills, they fail to account for\nstatic and dynamic obstacles in the environment. To address this limitation, we\nintroduce a reinforcement learning-based approach to train a whole-body\nobstacle-aware policy that tracks foot position commands while simultaneously\navoiding obstacles. Despite training the policy in only five different static\nscenarios in simulation, we show that it generalizes to unknown environments\nwith different numbers and types of obstacles. We analyze the performance of\nour method through a set of simulation experiments and successfully deploy the\nlearned policy on the ANYmal quadruped, demonstrating its capability to follow\nfoot commands while navigating around static and dynamic obstacles.\n", "link": "http://arxiv.org/abs/2409.07195v2", "date": "2024-11-01", "relevancy": 1.746, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6224}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5751}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptive%20Pedipulation%20with%20Local%20Obstacle%20Avoidance&body=Title%3A%20Perceptive%20Pedipulation%20with%20Local%20Obstacle%20Avoidance%0AAuthor%3A%20Jonas%20Stolle%20and%20Philip%20Arm%20and%20Mayank%20Mittal%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Pedipulation%20leverages%20the%20feet%20of%20legged%20robots%20for%20mobile%20manipulation%2C%0Aeliminating%20the%20need%20for%20dedicated%20robotic%20arms.%20While%20previous%20works%20have%0Ashowcased%20blind%20and%20task-specific%20pedipulation%20skills%2C%20they%20fail%20to%20account%20for%0Astatic%20and%20dynamic%20obstacles%20in%20the%20environment.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20a%20reinforcement%20learning-based%20approach%20to%20train%20a%20whole-body%0Aobstacle-aware%20policy%20that%20tracks%20foot%20position%20commands%20while%20simultaneously%0Aavoiding%20obstacles.%20Despite%20training%20the%20policy%20in%20only%20five%20different%20static%0Ascenarios%20in%20simulation%2C%20we%20show%20that%20it%20generalizes%20to%20unknown%20environments%0Awith%20different%20numbers%20and%20types%20of%20obstacles.%20We%20analyze%20the%20performance%20of%0Aour%20method%20through%20a%20set%20of%20simulation%20experiments%20and%20successfully%20deploy%20the%0Alearned%20policy%20on%20the%20ANYmal%20quadruped%2C%20demonstrating%20its%20capability%20to%20follow%0Afoot%20commands%20while%20navigating%20around%20static%20and%20dynamic%20obstacles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptive%2520Pedipulation%2520with%2520Local%2520Obstacle%2520Avoidance%26entry.906535625%3DJonas%2520Stolle%2520and%2520Philip%2520Arm%2520and%2520Mayank%2520Mittal%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Pedipulation%2520leverages%2520the%2520feet%2520of%2520legged%2520robots%2520for%2520mobile%2520manipulation%252C%250Aeliminating%2520the%2520need%2520for%2520dedicated%2520robotic%2520arms.%2520While%2520previous%2520works%2520have%250Ashowcased%2520blind%2520and%2520task-specific%2520pedipulation%2520skills%252C%2520they%2520fail%2520to%2520account%2520for%250Astatic%2520and%2520dynamic%2520obstacles%2520in%2520the%2520environment.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520a%2520reinforcement%2520learning-based%2520approach%2520to%2520train%2520a%2520whole-body%250Aobstacle-aware%2520policy%2520that%2520tracks%2520foot%2520position%2520commands%2520while%2520simultaneously%250Aavoiding%2520obstacles.%2520Despite%2520training%2520the%2520policy%2520in%2520only%2520five%2520different%2520static%250Ascenarios%2520in%2520simulation%252C%2520we%2520show%2520that%2520it%2520generalizes%2520to%2520unknown%2520environments%250Awith%2520different%2520numbers%2520and%2520types%2520of%2520obstacles.%2520We%2520analyze%2520the%2520performance%2520of%250Aour%2520method%2520through%2520a%2520set%2520of%2520simulation%2520experiments%2520and%2520successfully%2520deploy%2520the%250Alearned%2520policy%2520on%2520the%2520ANYmal%2520quadruped%252C%2520demonstrating%2520its%2520capability%2520to%2520follow%250Afoot%2520commands%2520while%2520navigating%2520around%2520static%2520and%2520dynamic%2520obstacles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptive%20Pedipulation%20with%20Local%20Obstacle%20Avoidance&entry.906535625=Jonas%20Stolle%20and%20Philip%20Arm%20and%20Mayank%20Mittal%20and%20Marco%20Hutter&entry.1292438233=%20%20Pedipulation%20leverages%20the%20feet%20of%20legged%20robots%20for%20mobile%20manipulation%2C%0Aeliminating%20the%20need%20for%20dedicated%20robotic%20arms.%20While%20previous%20works%20have%0Ashowcased%20blind%20and%20task-specific%20pedipulation%20skills%2C%20they%20fail%20to%20account%20for%0Astatic%20and%20dynamic%20obstacles%20in%20the%20environment.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20a%20reinforcement%20learning-based%20approach%20to%20train%20a%20whole-body%0Aobstacle-aware%20policy%20that%20tracks%20foot%20position%20commands%20while%20simultaneously%0Aavoiding%20obstacles.%20Despite%20training%20the%20policy%20in%20only%20five%20different%20static%0Ascenarios%20in%20simulation%2C%20we%20show%20that%20it%20generalizes%20to%20unknown%20environments%0Awith%20different%20numbers%20and%20types%20of%20obstacles.%20We%20analyze%20the%20performance%20of%0Aour%20method%20through%20a%20set%20of%20simulation%20experiments%20and%20successfully%20deploy%20the%0Alearned%20policy%20on%20the%20ANYmal%20quadruped%2C%20demonstrating%20its%20capability%20to%20follow%0Afoot%20commands%20while%20navigating%20around%20static%20and%20dynamic%20obstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07195v2&entry.124074799=Read"},
{"title": "Highly Accurate Real-space Electron Densities with Neural Networks", "author": "Lixue Cheng and P. Bern\u00e1t Szab\u00f3 and Zeno Sch\u00e4tzle and Derk P. Kooi and Jonas K\u00f6hler and Klaas J. H. Giesbertz and Frank No\u00e9 and Jan Hermann and Paola Gori-Giorgi and Adam Foster", "abstract": "  Variational ab-initio methods in quantum chemistry stand out among other\nmethods in providing direct access to the wave function. This allows in\nprinciple straightforward extraction of any other observable of interest,\nbesides the energy, but in practice this extraction is often technically\ndifficult and computationally impractical. Here, we consider the electron\ndensity as a central observable in quantum chemistry and introduce a novel\nmethod to obtain accurate densities from real-space many-electron wave\nfunctions by representing the density with a neural network that captures known\nasymptotic properties and is trained from the wave function by score matching\nand noise-contrastive estimation. We use variational quantum Monte Carlo with\ndeep-learning ans\\\"atze (deep QMC) to obtain highly accurate wave functions\nfree of basis set errors, and from them, using our novel method,\ncorrespondingly accurate electron densities, which we demonstrate by\ncalculating dipole moments, nuclear forces, contact densities, and other\ndensity-based properties.\n", "link": "http://arxiv.org/abs/2409.01306v2", "date": "2024-11-01", "relevancy": 1.7173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4606}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4239}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20Accurate%20Real-space%20Electron%20Densities%20with%20Neural%20Networks&body=Title%3A%20Highly%20Accurate%20Real-space%20Electron%20Densities%20with%20Neural%20Networks%0AAuthor%3A%20Lixue%20Cheng%20and%20P.%20Bern%C3%A1t%20Szab%C3%B3%20and%20Zeno%20Sch%C3%A4tzle%20and%20Derk%20P.%20Kooi%20and%20Jonas%20K%C3%B6hler%20and%20Klaas%20J.%20H.%20Giesbertz%20and%20Frank%20No%C3%A9%20and%20Jan%20Hermann%20and%20Paola%20Gori-Giorgi%20and%20Adam%20Foster%0AAbstract%3A%20%20%20Variational%20ab-initio%20methods%20in%20quantum%20chemistry%20stand%20out%20among%20other%0Amethods%20in%20providing%20direct%20access%20to%20the%20wave%20function.%20This%20allows%20in%0Aprinciple%20straightforward%20extraction%20of%20any%20other%20observable%20of%20interest%2C%0Abesides%20the%20energy%2C%20but%20in%20practice%20this%20extraction%20is%20often%20technically%0Adifficult%20and%20computationally%20impractical.%20Here%2C%20we%20consider%20the%20electron%0Adensity%20as%20a%20central%20observable%20in%20quantum%20chemistry%20and%20introduce%20a%20novel%0Amethod%20to%20obtain%20accurate%20densities%20from%20real-space%20many-electron%20wave%0Afunctions%20by%20representing%20the%20density%20with%20a%20neural%20network%20that%20captures%20known%0Aasymptotic%20properties%20and%20is%20trained%20from%20the%20wave%20function%20by%20score%20matching%0Aand%20noise-contrastive%20estimation.%20We%20use%20variational%20quantum%20Monte%20Carlo%20with%0Adeep-learning%20ans%5C%22atze%20%28deep%20QMC%29%20to%20obtain%20highly%20accurate%20wave%20functions%0Afree%20of%20basis%20set%20errors%2C%20and%20from%20them%2C%20using%20our%20novel%20method%2C%0Acorrespondingly%20accurate%20electron%20densities%2C%20which%20we%20demonstrate%20by%0Acalculating%20dipole%20moments%2C%20nuclear%20forces%2C%20contact%20densities%2C%20and%20other%0Adensity-based%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520Accurate%2520Real-space%2520Electron%2520Densities%2520with%2520Neural%2520Networks%26entry.906535625%3DLixue%2520Cheng%2520and%2520P.%2520Bern%25C3%25A1t%2520Szab%25C3%25B3%2520and%2520Zeno%2520Sch%25C3%25A4tzle%2520and%2520Derk%2520P.%2520Kooi%2520and%2520Jonas%2520K%25C3%25B6hler%2520and%2520Klaas%2520J.%2520H.%2520Giesbertz%2520and%2520Frank%2520No%25C3%25A9%2520and%2520Jan%2520Hermann%2520and%2520Paola%2520Gori-Giorgi%2520and%2520Adam%2520Foster%26entry.1292438233%3D%2520%2520Variational%2520ab-initio%2520methods%2520in%2520quantum%2520chemistry%2520stand%2520out%2520among%2520other%250Amethods%2520in%2520providing%2520direct%2520access%2520to%2520the%2520wave%2520function.%2520This%2520allows%2520in%250Aprinciple%2520straightforward%2520extraction%2520of%2520any%2520other%2520observable%2520of%2520interest%252C%250Abesides%2520the%2520energy%252C%2520but%2520in%2520practice%2520this%2520extraction%2520is%2520often%2520technically%250Adifficult%2520and%2520computationally%2520impractical.%2520Here%252C%2520we%2520consider%2520the%2520electron%250Adensity%2520as%2520a%2520central%2520observable%2520in%2520quantum%2520chemistry%2520and%2520introduce%2520a%2520novel%250Amethod%2520to%2520obtain%2520accurate%2520densities%2520from%2520real-space%2520many-electron%2520wave%250Afunctions%2520by%2520representing%2520the%2520density%2520with%2520a%2520neural%2520network%2520that%2520captures%2520known%250Aasymptotic%2520properties%2520and%2520is%2520trained%2520from%2520the%2520wave%2520function%2520by%2520score%2520matching%250Aand%2520noise-contrastive%2520estimation.%2520We%2520use%2520variational%2520quantum%2520Monte%2520Carlo%2520with%250Adeep-learning%2520ans%255C%2522atze%2520%2528deep%2520QMC%2529%2520to%2520obtain%2520highly%2520accurate%2520wave%2520functions%250Afree%2520of%2520basis%2520set%2520errors%252C%2520and%2520from%2520them%252C%2520using%2520our%2520novel%2520method%252C%250Acorrespondingly%2520accurate%2520electron%2520densities%252C%2520which%2520we%2520demonstrate%2520by%250Acalculating%2520dipole%2520moments%252C%2520nuclear%2520forces%252C%2520contact%2520densities%252C%2520and%2520other%250Adensity-based%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20Accurate%20Real-space%20Electron%20Densities%20with%20Neural%20Networks&entry.906535625=Lixue%20Cheng%20and%20P.%20Bern%C3%A1t%20Szab%C3%B3%20and%20Zeno%20Sch%C3%A4tzle%20and%20Derk%20P.%20Kooi%20and%20Jonas%20K%C3%B6hler%20and%20Klaas%20J.%20H.%20Giesbertz%20and%20Frank%20No%C3%A9%20and%20Jan%20Hermann%20and%20Paola%20Gori-Giorgi%20and%20Adam%20Foster&entry.1292438233=%20%20Variational%20ab-initio%20methods%20in%20quantum%20chemistry%20stand%20out%20among%20other%0Amethods%20in%20providing%20direct%20access%20to%20the%20wave%20function.%20This%20allows%20in%0Aprinciple%20straightforward%20extraction%20of%20any%20other%20observable%20of%20interest%2C%0Abesides%20the%20energy%2C%20but%20in%20practice%20this%20extraction%20is%20often%20technically%0Adifficult%20and%20computationally%20impractical.%20Here%2C%20we%20consider%20the%20electron%0Adensity%20as%20a%20central%20observable%20in%20quantum%20chemistry%20and%20introduce%20a%20novel%0Amethod%20to%20obtain%20accurate%20densities%20from%20real-space%20many-electron%20wave%0Afunctions%20by%20representing%20the%20density%20with%20a%20neural%20network%20that%20captures%20known%0Aasymptotic%20properties%20and%20is%20trained%20from%20the%20wave%20function%20by%20score%20matching%0Aand%20noise-contrastive%20estimation.%20We%20use%20variational%20quantum%20Monte%20Carlo%20with%0Adeep-learning%20ans%5C%22atze%20%28deep%20QMC%29%20to%20obtain%20highly%20accurate%20wave%20functions%0Afree%20of%20basis%20set%20errors%2C%20and%20from%20them%2C%20using%20our%20novel%20method%2C%0Acorrespondingly%20accurate%20electron%20densities%2C%20which%20we%20demonstrate%20by%0Acalculating%20dipole%20moments%2C%20nuclear%20forces%2C%20contact%20densities%2C%20and%20other%0Adensity-based%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01306v2&entry.124074799=Read"},
{"title": "BehAVE: Behaviour Alignment of Video Game Encodings", "author": "Nemanja Ra\u0161ajski and Chintan Trivedi and Konstantinos Makantasis and Antonios Liapis and Georgios N. Yannakakis", "abstract": "  Domain randomisation enhances the transferability of vision models across\nvisually distinct domains with similar content. However, current methods\nheavily depend on intricate simulation engines, hampering feasibility and\nscalability. This paper introduces BehAVE, a video understanding framework that\nutilises existing commercial video games for domain randomisation without\naccessing their simulation engines. BehAVE taps into the visual diversity of\nvideo games for randomisation and uses textual descriptions of player actions\nto align videos with similar content. We evaluate BehAVE across 25 first-person\nshooter (FPS) games using various video and text foundation models,\ndemonstrating its robustness in domain randomisation. BehAVE effectively aligns\nplayer behavioural patterns and achieves zero-shot transfer to multiple unseen\nFPS games when trained on just one game. In a more challenging scenario, BehAVE\nenhances the zero-shot transferability of foundation models to unseen FPS\ngames, even when trained on a game of a different genre, with improvements of\nup to 22%. BehAVE is available online at https://github.com/nrasajski/BehAVE.\n", "link": "http://arxiv.org/abs/2402.01335v3", "date": "2024-11-01", "relevancy": 1.6791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5591}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BehAVE%3A%20Behaviour%20Alignment%20of%20Video%20Game%20Encodings&body=Title%3A%20BehAVE%3A%20Behaviour%20Alignment%20of%20Video%20Game%20Encodings%0AAuthor%3A%20Nemanja%20Ra%C5%A1ajski%20and%20Chintan%20Trivedi%20and%20Konstantinos%20Makantasis%20and%20Antonios%20Liapis%20and%20Georgios%20N.%20Yannakakis%0AAbstract%3A%20%20%20Domain%20randomisation%20enhances%20the%20transferability%20of%20vision%20models%20across%0Avisually%20distinct%20domains%20with%20similar%20content.%20However%2C%20current%20methods%0Aheavily%20depend%20on%20intricate%20simulation%20engines%2C%20hampering%20feasibility%20and%0Ascalability.%20This%20paper%20introduces%20BehAVE%2C%20a%20video%20understanding%20framework%20that%0Autilises%20existing%20commercial%20video%20games%20for%20domain%20randomisation%20without%0Aaccessing%20their%20simulation%20engines.%20BehAVE%20taps%20into%20the%20visual%20diversity%20of%0Avideo%20games%20for%20randomisation%20and%20uses%20textual%20descriptions%20of%20player%20actions%0Ato%20align%20videos%20with%20similar%20content.%20We%20evaluate%20BehAVE%20across%2025%20first-person%0Ashooter%20%28FPS%29%20games%20using%20various%20video%20and%20text%20foundation%20models%2C%0Ademonstrating%20its%20robustness%20in%20domain%20randomisation.%20BehAVE%20effectively%20aligns%0Aplayer%20behavioural%20patterns%20and%20achieves%20zero-shot%20transfer%20to%20multiple%20unseen%0AFPS%20games%20when%20trained%20on%20just%20one%20game.%20In%20a%20more%20challenging%20scenario%2C%20BehAVE%0Aenhances%20the%20zero-shot%20transferability%20of%20foundation%20models%20to%20unseen%20FPS%0Agames%2C%20even%20when%20trained%20on%20a%20game%20of%20a%20different%20genre%2C%20with%20improvements%20of%0Aup%20to%2022%25.%20BehAVE%20is%20available%20online%20at%20https%3A//github.com/nrasajski/BehAVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehAVE%253A%2520Behaviour%2520Alignment%2520of%2520Video%2520Game%2520Encodings%26entry.906535625%3DNemanja%2520Ra%25C5%25A1ajski%2520and%2520Chintan%2520Trivedi%2520and%2520Konstantinos%2520Makantasis%2520and%2520Antonios%2520Liapis%2520and%2520Georgios%2520N.%2520Yannakakis%26entry.1292438233%3D%2520%2520Domain%2520randomisation%2520enhances%2520the%2520transferability%2520of%2520vision%2520models%2520across%250Avisually%2520distinct%2520domains%2520with%2520similar%2520content.%2520However%252C%2520current%2520methods%250Aheavily%2520depend%2520on%2520intricate%2520simulation%2520engines%252C%2520hampering%2520feasibility%2520and%250Ascalability.%2520This%2520paper%2520introduces%2520BehAVE%252C%2520a%2520video%2520understanding%2520framework%2520that%250Autilises%2520existing%2520commercial%2520video%2520games%2520for%2520domain%2520randomisation%2520without%250Aaccessing%2520their%2520simulation%2520engines.%2520BehAVE%2520taps%2520into%2520the%2520visual%2520diversity%2520of%250Avideo%2520games%2520for%2520randomisation%2520and%2520uses%2520textual%2520descriptions%2520of%2520player%2520actions%250Ato%2520align%2520videos%2520with%2520similar%2520content.%2520We%2520evaluate%2520BehAVE%2520across%252025%2520first-person%250Ashooter%2520%2528FPS%2529%2520games%2520using%2520various%2520video%2520and%2520text%2520foundation%2520models%252C%250Ademonstrating%2520its%2520robustness%2520in%2520domain%2520randomisation.%2520BehAVE%2520effectively%2520aligns%250Aplayer%2520behavioural%2520patterns%2520and%2520achieves%2520zero-shot%2520transfer%2520to%2520multiple%2520unseen%250AFPS%2520games%2520when%2520trained%2520on%2520just%2520one%2520game.%2520In%2520a%2520more%2520challenging%2520scenario%252C%2520BehAVE%250Aenhances%2520the%2520zero-shot%2520transferability%2520of%2520foundation%2520models%2520to%2520unseen%2520FPS%250Agames%252C%2520even%2520when%2520trained%2520on%2520a%2520game%2520of%2520a%2520different%2520genre%252C%2520with%2520improvements%2520of%250Aup%2520to%252022%2525.%2520BehAVE%2520is%2520available%2520online%2520at%2520https%253A//github.com/nrasajski/BehAVE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BehAVE%3A%20Behaviour%20Alignment%20of%20Video%20Game%20Encodings&entry.906535625=Nemanja%20Ra%C5%A1ajski%20and%20Chintan%20Trivedi%20and%20Konstantinos%20Makantasis%20and%20Antonios%20Liapis%20and%20Georgios%20N.%20Yannakakis&entry.1292438233=%20%20Domain%20randomisation%20enhances%20the%20transferability%20of%20vision%20models%20across%0Avisually%20distinct%20domains%20with%20similar%20content.%20However%2C%20current%20methods%0Aheavily%20depend%20on%20intricate%20simulation%20engines%2C%20hampering%20feasibility%20and%0Ascalability.%20This%20paper%20introduces%20BehAVE%2C%20a%20video%20understanding%20framework%20that%0Autilises%20existing%20commercial%20video%20games%20for%20domain%20randomisation%20without%0Aaccessing%20their%20simulation%20engines.%20BehAVE%20taps%20into%20the%20visual%20diversity%20of%0Avideo%20games%20for%20randomisation%20and%20uses%20textual%20descriptions%20of%20player%20actions%0Ato%20align%20videos%20with%20similar%20content.%20We%20evaluate%20BehAVE%20across%2025%20first-person%0Ashooter%20%28FPS%29%20games%20using%20various%20video%20and%20text%20foundation%20models%2C%0Ademonstrating%20its%20robustness%20in%20domain%20randomisation.%20BehAVE%20effectively%20aligns%0Aplayer%20behavioural%20patterns%20and%20achieves%20zero-shot%20transfer%20to%20multiple%20unseen%0AFPS%20games%20when%20trained%20on%20just%20one%20game.%20In%20a%20more%20challenging%20scenario%2C%20BehAVE%0Aenhances%20the%20zero-shot%20transferability%20of%20foundation%20models%20to%20unseen%20FPS%0Agames%2C%20even%20when%20trained%20on%20a%20game%20of%20a%20different%20genre%2C%20with%20improvements%20of%0Aup%20to%2022%25.%20BehAVE%20is%20available%20online%20at%20https%3A//github.com/nrasajski/BehAVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01335v3&entry.124074799=Read"},
{"title": "Dimension-free deterministic equivalents for random feature regression", "author": "Leonardo Defilippis and Bruno Loureiro and Theodor Misiakiewicz", "abstract": "  In this work we investigate the generalization performance of random feature\nridge regression (RFRR). Our main contribution is a general deterministic\nequivalent for the test error of RFRR. Specifically, under a certain\nconcentration property, we show that the test error is well approximated by a\nclosed-form expression that only depends on the feature map eigenvalues.\nNotably, our approximation guarantee is non-asymptotic, multiplicative, and\nindependent of the feature map dimension -- allowing for infinite-dimensional\nfeatures. We expect this deterministic equivalent to hold broadly beyond our\ntheoretical analysis, and we empirically validate its predictions on various\nreal and synthetic datasets. As an application, we derive sharp excess error\nrates under standard power-law assumptions of the spectrum and target decay. In\nparticular, we provide a tight result for the smallest number of features\nachieving optimal minimax error rate.\n", "link": "http://arxiv.org/abs/2405.15699v2", "date": "2024-11-01", "relevancy": 1.6733, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4228}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.419}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension-free%20deterministic%20equivalents%20for%20random%20feature%20regression&body=Title%3A%20Dimension-free%20deterministic%20equivalents%20for%20random%20feature%20regression%0AAuthor%3A%20Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Theodor%20Misiakiewicz%0AAbstract%3A%20%20%20In%20this%20work%20we%20investigate%20the%20generalization%20performance%20of%20random%20feature%0Aridge%20regression%20%28RFRR%29.%20Our%20main%20contribution%20is%20a%20general%20deterministic%0Aequivalent%20for%20the%20test%20error%20of%20RFRR.%20Specifically%2C%20under%20a%20certain%0Aconcentration%20property%2C%20we%20show%20that%20the%20test%20error%20is%20well%20approximated%20by%20a%0Aclosed-form%20expression%20that%20only%20depends%20on%20the%20feature%20map%20eigenvalues.%0ANotably%2C%20our%20approximation%20guarantee%20is%20non-asymptotic%2C%20multiplicative%2C%20and%0Aindependent%20of%20the%20feature%20map%20dimension%20--%20allowing%20for%20infinite-dimensional%0Afeatures.%20We%20expect%20this%20deterministic%20equivalent%20to%20hold%20broadly%20beyond%20our%0Atheoretical%20analysis%2C%20and%20we%20empirically%20validate%20its%20predictions%20on%20various%0Areal%20and%20synthetic%20datasets.%20As%20an%20application%2C%20we%20derive%20sharp%20excess%20error%0Arates%20under%20standard%20power-law%20assumptions%20of%20the%20spectrum%20and%20target%20decay.%20In%0Aparticular%2C%20we%20provide%20a%20tight%20result%20for%20the%20smallest%20number%20of%20features%0Aachieving%20optimal%20minimax%20error%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension-free%2520deterministic%2520equivalents%2520for%2520random%2520feature%2520regression%26entry.906535625%3DLeonardo%2520Defilippis%2520and%2520Bruno%2520Loureiro%2520and%2520Theodor%2520Misiakiewicz%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520investigate%2520the%2520generalization%2520performance%2520of%2520random%2520feature%250Aridge%2520regression%2520%2528RFRR%2529.%2520Our%2520main%2520contribution%2520is%2520a%2520general%2520deterministic%250Aequivalent%2520for%2520the%2520test%2520error%2520of%2520RFRR.%2520Specifically%252C%2520under%2520a%2520certain%250Aconcentration%2520property%252C%2520we%2520show%2520that%2520the%2520test%2520error%2520is%2520well%2520approximated%2520by%2520a%250Aclosed-form%2520expression%2520that%2520only%2520depends%2520on%2520the%2520feature%2520map%2520eigenvalues.%250ANotably%252C%2520our%2520approximation%2520guarantee%2520is%2520non-asymptotic%252C%2520multiplicative%252C%2520and%250Aindependent%2520of%2520the%2520feature%2520map%2520dimension%2520--%2520allowing%2520for%2520infinite-dimensional%250Afeatures.%2520We%2520expect%2520this%2520deterministic%2520equivalent%2520to%2520hold%2520broadly%2520beyond%2520our%250Atheoretical%2520analysis%252C%2520and%2520we%2520empirically%2520validate%2520its%2520predictions%2520on%2520various%250Areal%2520and%2520synthetic%2520datasets.%2520As%2520an%2520application%252C%2520we%2520derive%2520sharp%2520excess%2520error%250Arates%2520under%2520standard%2520power-law%2520assumptions%2520of%2520the%2520spectrum%2520and%2520target%2520decay.%2520In%250Aparticular%252C%2520we%2520provide%2520a%2520tight%2520result%2520for%2520the%2520smallest%2520number%2520of%2520features%250Aachieving%2520optimal%2520minimax%2520error%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension-free%20deterministic%20equivalents%20for%20random%20feature%20regression&entry.906535625=Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Theodor%20Misiakiewicz&entry.1292438233=%20%20In%20this%20work%20we%20investigate%20the%20generalization%20performance%20of%20random%20feature%0Aridge%20regression%20%28RFRR%29.%20Our%20main%20contribution%20is%20a%20general%20deterministic%0Aequivalent%20for%20the%20test%20error%20of%20RFRR.%20Specifically%2C%20under%20a%20certain%0Aconcentration%20property%2C%20we%20show%20that%20the%20test%20error%20is%20well%20approximated%20by%20a%0Aclosed-form%20expression%20that%20only%20depends%20on%20the%20feature%20map%20eigenvalues.%0ANotably%2C%20our%20approximation%20guarantee%20is%20non-asymptotic%2C%20multiplicative%2C%20and%0Aindependent%20of%20the%20feature%20map%20dimension%20--%20allowing%20for%20infinite-dimensional%0Afeatures.%20We%20expect%20this%20deterministic%20equivalent%20to%20hold%20broadly%20beyond%20our%0Atheoretical%20analysis%2C%20and%20we%20empirically%20validate%20its%20predictions%20on%20various%0Areal%20and%20synthetic%20datasets.%20As%20an%20application%2C%20we%20derive%20sharp%20excess%20error%0Arates%20under%20standard%20power-law%20assumptions%20of%20the%20spectrum%20and%20target%20decay.%20In%0Aparticular%2C%20we%20provide%20a%20tight%20result%20for%20the%20smallest%20number%20of%20features%0Aachieving%20optimal%20minimax%20error%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15699v2&entry.124074799=Read"},
{"title": "MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language", "author": "Yoel Shoshan and Moshiko Raboh and Michal Ozery-Flato and Vadim Ratner and Alex Golts and Jeffrey K. Weber and Ella Barkan and Simona Rabinovici-Cohen and Sagi Polaczek and Ido Amos and Ben Shapira and Liam Hazan and Matan Ninio and Sivan Ravid and Michael M. Danziger and Joseph A. Morrone and Parthasarathy Suryanarayanan and Michal Rosen-Zvi and Efrat Hexter", "abstract": "  Drug discovery typically consists of multiple steps, including identifying a\ntarget protein key to a disease's etiology, validating that interacting with\nthis target could prevent symptoms or cure the disease, discovering a small\nmolecule or biologic therapeutic to interact with it, and optimizing the\ncandidate molecule through a complex landscape of required properties. Drug\ndiscovery related tasks often involve prediction and generation while\nconsidering multiple entities that potentially interact, which poses a\nchallenge for typical AI models. For this purpose we present MAMMAL - Molecular\nAligned Multi-Modal Architecture and Language - a method that we applied to\ncreate a versatile multi-task multi-align foundation model that learns from\nlarge-scale biological datasets (2 billion samples) across diverse modalities,\nincluding proteins, small molecules, and genes. We introduce a prompt syntax\nthat supports a wide range of classification, regression, and generation tasks.\nIt allows combining different modalities and entity types as inputs and/or\noutputs. Our model handles combinations of tokens and scalars and enables the\ngeneration of small molecules and proteins, property prediction, and\ntranscriptomic lab test predictions. We evaluated the model on 11 diverse\ndownstream tasks spanning different steps within a typical drug discovery\npipeline, where it reaches new SOTA in 9 tasks and is comparable to SOTA in 2\ntasks. This performance is achieved while using a unified architecture serving\nall tasks, in contrast to the original SOTA performance achieved using tailored\narchitectures.\n  The model code and pretrained weights are publicly available at\nhttps://github.com/BiomedSciAI/biomed-multi-alignment and\nhttps://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.\n", "link": "http://arxiv.org/abs/2410.22367v2", "date": "2024-11-01", "relevancy": 1.671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5425}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAMMAL%20--%20Molecular%20Aligned%20Multi-Modal%20Architecture%20and%20Language&body=Title%3A%20MAMMAL%20--%20Molecular%20Aligned%20Multi-Modal%20Architecture%20and%20Language%0AAuthor%3A%20Yoel%20Shoshan%20and%20Moshiko%20Raboh%20and%20Michal%20Ozery-Flato%20and%20Vadim%20Ratner%20and%20Alex%20Golts%20and%20Jeffrey%20K.%20Weber%20and%20Ella%20Barkan%20and%20Simona%20Rabinovici-Cohen%20and%20Sagi%20Polaczek%20and%20Ido%20Amos%20and%20Ben%20Shapira%20and%20Liam%20Hazan%20and%20Matan%20Ninio%20and%20Sivan%20Ravid%20and%20Michael%20M.%20Danziger%20and%20Joseph%20A.%20Morrone%20and%20Parthasarathy%20Suryanarayanan%20and%20Michal%20Rosen-Zvi%20and%20Efrat%20Hexter%0AAbstract%3A%20%20%20Drug%20discovery%20typically%20consists%20of%20multiple%20steps%2C%20including%20identifying%20a%0Atarget%20protein%20key%20to%20a%20disease%27s%20etiology%2C%20validating%20that%20interacting%20with%0Athis%20target%20could%20prevent%20symptoms%20or%20cure%20the%20disease%2C%20discovering%20a%20small%0Amolecule%20or%20biologic%20therapeutic%20to%20interact%20with%20it%2C%20and%20optimizing%20the%0Acandidate%20molecule%20through%20a%20complex%20landscape%20of%20required%20properties.%20Drug%0Adiscovery%20related%20tasks%20often%20involve%20prediction%20and%20generation%20while%0Aconsidering%20multiple%20entities%20that%20potentially%20interact%2C%20which%20poses%20a%0Achallenge%20for%20typical%20AI%20models.%20For%20this%20purpose%20we%20present%20MAMMAL%20-%20Molecular%0AAligned%20Multi-Modal%20Architecture%20and%20Language%20-%20a%20method%20that%20we%20applied%20to%0Acreate%20a%20versatile%20multi-task%20multi-align%20foundation%20model%20that%20learns%20from%0Alarge-scale%20biological%20datasets%20%282%20billion%20samples%29%20across%20diverse%20modalities%2C%0Aincluding%20proteins%2C%20small%20molecules%2C%20and%20genes.%20We%20introduce%20a%20prompt%20syntax%0Athat%20supports%20a%20wide%20range%20of%20classification%2C%20regression%2C%20and%20generation%20tasks.%0AIt%20allows%20combining%20different%20modalities%20and%20entity%20types%20as%20inputs%20and/or%0Aoutputs.%20Our%20model%20handles%20combinations%20of%20tokens%20and%20scalars%20and%20enables%20the%0Ageneration%20of%20small%20molecules%20and%20proteins%2C%20property%20prediction%2C%20and%0Atranscriptomic%20lab%20test%20predictions.%20We%20evaluated%20the%20model%20on%2011%20diverse%0Adownstream%20tasks%20spanning%20different%20steps%20within%20a%20typical%20drug%20discovery%0Apipeline%2C%20where%20it%20reaches%20new%20SOTA%20in%209%20tasks%20and%20is%20comparable%20to%20SOTA%20in%202%0Atasks.%20This%20performance%20is%20achieved%20while%20using%20a%20unified%20architecture%20serving%0Aall%20tasks%2C%20in%20contrast%20to%20the%20original%20SOTA%20performance%20achieved%20using%20tailored%0Aarchitectures.%0A%20%20The%20model%20code%20and%20pretrained%20weights%20are%20publicly%20available%20at%0Ahttps%3A//github.com/BiomedSciAI/biomed-multi-alignment%20and%0Ahttps%3A//huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAMMAL%2520--%2520Molecular%2520Aligned%2520Multi-Modal%2520Architecture%2520and%2520Language%26entry.906535625%3DYoel%2520Shoshan%2520and%2520Moshiko%2520Raboh%2520and%2520Michal%2520Ozery-Flato%2520and%2520Vadim%2520Ratner%2520and%2520Alex%2520Golts%2520and%2520Jeffrey%2520K.%2520Weber%2520and%2520Ella%2520Barkan%2520and%2520Simona%2520Rabinovici-Cohen%2520and%2520Sagi%2520Polaczek%2520and%2520Ido%2520Amos%2520and%2520Ben%2520Shapira%2520and%2520Liam%2520Hazan%2520and%2520Matan%2520Ninio%2520and%2520Sivan%2520Ravid%2520and%2520Michael%2520M.%2520Danziger%2520and%2520Joseph%2520A.%2520Morrone%2520and%2520Parthasarathy%2520Suryanarayanan%2520and%2520Michal%2520Rosen-Zvi%2520and%2520Efrat%2520Hexter%26entry.1292438233%3D%2520%2520Drug%2520discovery%2520typically%2520consists%2520of%2520multiple%2520steps%252C%2520including%2520identifying%2520a%250Atarget%2520protein%2520key%2520to%2520a%2520disease%2527s%2520etiology%252C%2520validating%2520that%2520interacting%2520with%250Athis%2520target%2520could%2520prevent%2520symptoms%2520or%2520cure%2520the%2520disease%252C%2520discovering%2520a%2520small%250Amolecule%2520or%2520biologic%2520therapeutic%2520to%2520interact%2520with%2520it%252C%2520and%2520optimizing%2520the%250Acandidate%2520molecule%2520through%2520a%2520complex%2520landscape%2520of%2520required%2520properties.%2520Drug%250Adiscovery%2520related%2520tasks%2520often%2520involve%2520prediction%2520and%2520generation%2520while%250Aconsidering%2520multiple%2520entities%2520that%2520potentially%2520interact%252C%2520which%2520poses%2520a%250Achallenge%2520for%2520typical%2520AI%2520models.%2520For%2520this%2520purpose%2520we%2520present%2520MAMMAL%2520-%2520Molecular%250AAligned%2520Multi-Modal%2520Architecture%2520and%2520Language%2520-%2520a%2520method%2520that%2520we%2520applied%2520to%250Acreate%2520a%2520versatile%2520multi-task%2520multi-align%2520foundation%2520model%2520that%2520learns%2520from%250Alarge-scale%2520biological%2520datasets%2520%25282%2520billion%2520samples%2529%2520across%2520diverse%2520modalities%252C%250Aincluding%2520proteins%252C%2520small%2520molecules%252C%2520and%2520genes.%2520We%2520introduce%2520a%2520prompt%2520syntax%250Athat%2520supports%2520a%2520wide%2520range%2520of%2520classification%252C%2520regression%252C%2520and%2520generation%2520tasks.%250AIt%2520allows%2520combining%2520different%2520modalities%2520and%2520entity%2520types%2520as%2520inputs%2520and/or%250Aoutputs.%2520Our%2520model%2520handles%2520combinations%2520of%2520tokens%2520and%2520scalars%2520and%2520enables%2520the%250Ageneration%2520of%2520small%2520molecules%2520and%2520proteins%252C%2520property%2520prediction%252C%2520and%250Atranscriptomic%2520lab%2520test%2520predictions.%2520We%2520evaluated%2520the%2520model%2520on%252011%2520diverse%250Adownstream%2520tasks%2520spanning%2520different%2520steps%2520within%2520a%2520typical%2520drug%2520discovery%250Apipeline%252C%2520where%2520it%2520reaches%2520new%2520SOTA%2520in%25209%2520tasks%2520and%2520is%2520comparable%2520to%2520SOTA%2520in%25202%250Atasks.%2520This%2520performance%2520is%2520achieved%2520while%2520using%2520a%2520unified%2520architecture%2520serving%250Aall%2520tasks%252C%2520in%2520contrast%2520to%2520the%2520original%2520SOTA%2520performance%2520achieved%2520using%2520tailored%250Aarchitectures.%250A%2520%2520The%2520model%2520code%2520and%2520pretrained%2520weights%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/BiomedSciAI/biomed-multi-alignment%2520and%250Ahttps%253A//huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAMMAL%20--%20Molecular%20Aligned%20Multi-Modal%20Architecture%20and%20Language&entry.906535625=Yoel%20Shoshan%20and%20Moshiko%20Raboh%20and%20Michal%20Ozery-Flato%20and%20Vadim%20Ratner%20and%20Alex%20Golts%20and%20Jeffrey%20K.%20Weber%20and%20Ella%20Barkan%20and%20Simona%20Rabinovici-Cohen%20and%20Sagi%20Polaczek%20and%20Ido%20Amos%20and%20Ben%20Shapira%20and%20Liam%20Hazan%20and%20Matan%20Ninio%20and%20Sivan%20Ravid%20and%20Michael%20M.%20Danziger%20and%20Joseph%20A.%20Morrone%20and%20Parthasarathy%20Suryanarayanan%20and%20Michal%20Rosen-Zvi%20and%20Efrat%20Hexter&entry.1292438233=%20%20Drug%20discovery%20typically%20consists%20of%20multiple%20steps%2C%20including%20identifying%20a%0Atarget%20protein%20key%20to%20a%20disease%27s%20etiology%2C%20validating%20that%20interacting%20with%0Athis%20target%20could%20prevent%20symptoms%20or%20cure%20the%20disease%2C%20discovering%20a%20small%0Amolecule%20or%20biologic%20therapeutic%20to%20interact%20with%20it%2C%20and%20optimizing%20the%0Acandidate%20molecule%20through%20a%20complex%20landscape%20of%20required%20properties.%20Drug%0Adiscovery%20related%20tasks%20often%20involve%20prediction%20and%20generation%20while%0Aconsidering%20multiple%20entities%20that%20potentially%20interact%2C%20which%20poses%20a%0Achallenge%20for%20typical%20AI%20models.%20For%20this%20purpose%20we%20present%20MAMMAL%20-%20Molecular%0AAligned%20Multi-Modal%20Architecture%20and%20Language%20-%20a%20method%20that%20we%20applied%20to%0Acreate%20a%20versatile%20multi-task%20multi-align%20foundation%20model%20that%20learns%20from%0Alarge-scale%20biological%20datasets%20%282%20billion%20samples%29%20across%20diverse%20modalities%2C%0Aincluding%20proteins%2C%20small%20molecules%2C%20and%20genes.%20We%20introduce%20a%20prompt%20syntax%0Athat%20supports%20a%20wide%20range%20of%20classification%2C%20regression%2C%20and%20generation%20tasks.%0AIt%20allows%20combining%20different%20modalities%20and%20entity%20types%20as%20inputs%20and/or%0Aoutputs.%20Our%20model%20handles%20combinations%20of%20tokens%20and%20scalars%20and%20enables%20the%0Ageneration%20of%20small%20molecules%20and%20proteins%2C%20property%20prediction%2C%20and%0Atranscriptomic%20lab%20test%20predictions.%20We%20evaluated%20the%20model%20on%2011%20diverse%0Adownstream%20tasks%20spanning%20different%20steps%20within%20a%20typical%20drug%20discovery%0Apipeline%2C%20where%20it%20reaches%20new%20SOTA%20in%209%20tasks%20and%20is%20comparable%20to%20SOTA%20in%202%0Atasks.%20This%20performance%20is%20achieved%20while%20using%20a%20unified%20architecture%20serving%0Aall%20tasks%2C%20in%20contrast%20to%20the%20original%20SOTA%20performance%20achieved%20using%20tailored%0Aarchitectures.%0A%20%20The%20model%20code%20and%20pretrained%20weights%20are%20publicly%20available%20at%0Ahttps%3A//github.com/BiomedSciAI/biomed-multi-alignment%20and%0Ahttps%3A//huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22367v2&entry.124074799=Read"},
{"title": "Unity by Diversity: Improved Representation Learning in Multimodal VAEs", "author": "Thomas M. Sutter and Yang Meng and Andrea Agostini and Daphn\u00e9 Chopard and Norbert Fortin and Julia E. Vogt and Bahbak Shahbaba and Stephan Mandt", "abstract": "  Variational Autoencoders for multimodal data hold promise for many tasks in\ndata analysis, such as representation learning, conditional generation, and\nimputation. Current architectures either share the encoder output, decoder\ninput, or both across modalities to learn a shared representation. Such\narchitectures impose hard constraints on the model. In this work, we show that\na better latent representation can be obtained by replacing these hard\nconstraints with a soft constraint. We propose a new mixture-of-experts prior,\nsoftly guiding each modality's latent representation towards a shared aggregate\nposterior. This approach results in a superior latent representation and allows\neach encoding to preserve information better from its uncompressed original\nfeatures. In extensive experiments on multiple benchmark datasets and two\nchallenging real-world datasets, we show improved learned latent\nrepresentations and imputation of missing data modalities compared to existing\nmethods.\n", "link": "http://arxiv.org/abs/2403.05300v4", "date": "2024-11-01", "relevancy": 1.6604, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unity%20by%20Diversity%3A%20Improved%20Representation%20Learning%20in%20Multimodal%20VAEs&body=Title%3A%20Unity%20by%20Diversity%3A%20Improved%20Representation%20Learning%20in%20Multimodal%20VAEs%0AAuthor%3A%20Thomas%20M.%20Sutter%20and%20Yang%20Meng%20and%20Andrea%20Agostini%20and%20Daphn%C3%A9%20Chopard%20and%20Norbert%20Fortin%20and%20Julia%20E.%20Vogt%20and%20Bahbak%20Shahbaba%20and%20Stephan%20Mandt%0AAbstract%3A%20%20%20Variational%20Autoencoders%20for%20multimodal%20data%20hold%20promise%20for%20many%20tasks%20in%0Adata%20analysis%2C%20such%20as%20representation%20learning%2C%20conditional%20generation%2C%20and%0Aimputation.%20Current%20architectures%20either%20share%20the%20encoder%20output%2C%20decoder%0Ainput%2C%20or%20both%20across%20modalities%20to%20learn%20a%20shared%20representation.%20Such%0Aarchitectures%20impose%20hard%20constraints%20on%20the%20model.%20In%20this%20work%2C%20we%20show%20that%0Aa%20better%20latent%20representation%20can%20be%20obtained%20by%20replacing%20these%20hard%0Aconstraints%20with%20a%20soft%20constraint.%20We%20propose%20a%20new%20mixture-of-experts%20prior%2C%0Asoftly%20guiding%20each%20modality%27s%20latent%20representation%20towards%20a%20shared%20aggregate%0Aposterior.%20This%20approach%20results%20in%20a%20superior%20latent%20representation%20and%20allows%0Aeach%20encoding%20to%20preserve%20information%20better%20from%20its%20uncompressed%20original%0Afeatures.%20In%20extensive%20experiments%20on%20multiple%20benchmark%20datasets%20and%20two%0Achallenging%20real-world%20datasets%2C%20we%20show%20improved%20learned%20latent%0Arepresentations%20and%20imputation%20of%20missing%20data%20modalities%20compared%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05300v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnity%2520by%2520Diversity%253A%2520Improved%2520Representation%2520Learning%2520in%2520Multimodal%2520VAEs%26entry.906535625%3DThomas%2520M.%2520Sutter%2520and%2520Yang%2520Meng%2520and%2520Andrea%2520Agostini%2520and%2520Daphn%25C3%25A9%2520Chopard%2520and%2520Norbert%2520Fortin%2520and%2520Julia%2520E.%2520Vogt%2520and%2520Bahbak%2520Shahbaba%2520and%2520Stephan%2520Mandt%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520for%2520multimodal%2520data%2520hold%2520promise%2520for%2520many%2520tasks%2520in%250Adata%2520analysis%252C%2520such%2520as%2520representation%2520learning%252C%2520conditional%2520generation%252C%2520and%250Aimputation.%2520Current%2520architectures%2520either%2520share%2520the%2520encoder%2520output%252C%2520decoder%250Ainput%252C%2520or%2520both%2520across%2520modalities%2520to%2520learn%2520a%2520shared%2520representation.%2520Such%250Aarchitectures%2520impose%2520hard%2520constraints%2520on%2520the%2520model.%2520In%2520this%2520work%252C%2520we%2520show%2520that%250Aa%2520better%2520latent%2520representation%2520can%2520be%2520obtained%2520by%2520replacing%2520these%2520hard%250Aconstraints%2520with%2520a%2520soft%2520constraint.%2520We%2520propose%2520a%2520new%2520mixture-of-experts%2520prior%252C%250Asoftly%2520guiding%2520each%2520modality%2527s%2520latent%2520representation%2520towards%2520a%2520shared%2520aggregate%250Aposterior.%2520This%2520approach%2520results%2520in%2520a%2520superior%2520latent%2520representation%2520and%2520allows%250Aeach%2520encoding%2520to%2520preserve%2520information%2520better%2520from%2520its%2520uncompressed%2520original%250Afeatures.%2520In%2520extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%2520and%2520two%250Achallenging%2520real-world%2520datasets%252C%2520we%2520show%2520improved%2520learned%2520latent%250Arepresentations%2520and%2520imputation%2520of%2520missing%2520data%2520modalities%2520compared%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05300v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unity%20by%20Diversity%3A%20Improved%20Representation%20Learning%20in%20Multimodal%20VAEs&entry.906535625=Thomas%20M.%20Sutter%20and%20Yang%20Meng%20and%20Andrea%20Agostini%20and%20Daphn%C3%A9%20Chopard%20and%20Norbert%20Fortin%20and%20Julia%20E.%20Vogt%20and%20Bahbak%20Shahbaba%20and%20Stephan%20Mandt&entry.1292438233=%20%20Variational%20Autoencoders%20for%20multimodal%20data%20hold%20promise%20for%20many%20tasks%20in%0Adata%20analysis%2C%20such%20as%20representation%20learning%2C%20conditional%20generation%2C%20and%0Aimputation.%20Current%20architectures%20either%20share%20the%20encoder%20output%2C%20decoder%0Ainput%2C%20or%20both%20across%20modalities%20to%20learn%20a%20shared%20representation.%20Such%0Aarchitectures%20impose%20hard%20constraints%20on%20the%20model.%20In%20this%20work%2C%20we%20show%20that%0Aa%20better%20latent%20representation%20can%20be%20obtained%20by%20replacing%20these%20hard%0Aconstraints%20with%20a%20soft%20constraint.%20We%20propose%20a%20new%20mixture-of-experts%20prior%2C%0Asoftly%20guiding%20each%20modality%27s%20latent%20representation%20towards%20a%20shared%20aggregate%0Aposterior.%20This%20approach%20results%20in%20a%20superior%20latent%20representation%20and%20allows%0Aeach%20encoding%20to%20preserve%20information%20better%20from%20its%20uncompressed%20original%0Afeatures.%20In%20extensive%20experiments%20on%20multiple%20benchmark%20datasets%20and%20two%0Achallenging%20real-world%20datasets%2C%20we%20show%20improved%20learned%20latent%0Arepresentations%20and%20imputation%20of%20missing%20data%20modalities%20compared%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05300v4&entry.124074799=Read"},
{"title": "Semantic Density: Uncertainty Quantification for Large Language Models\n  through Confidence Measurement in Semantic Space", "author": "Xin Qiu and Risto Miikkulainen", "abstract": "  With the widespread application of Large Language Models (LLMs) to various\ndomains, concerns regarding the trustworthiness of LLMs in safety-critical\nscenarios have been raised, due to their unpredictable tendency to hallucinate\nand generate misinformation. Existing LLMs do not have an inherent\nfunctionality to provide the users with an uncertainty/confidence metric for\neach response it generates, making it difficult to evaluate trustworthiness.\nAlthough several studies aim to develop uncertainty quantification methods for\nLLMs, they have fundamental limitations, such as being restricted to\nclassification tasks, requiring additional training and data, considering only\nlexical instead of semantic information, and being prompt-wise but not\nresponse-wise. A new framework is proposed in this paper to address these\nissues. Semantic density extracts uncertainty/confidence information for each\nresponse from a probability distribution perspective in semantic space. It has\nno restriction on task types and is \"off-the-shelf\" for new models and tasks.\nExperiments on seven state-of-the-art LLMs, including the latest Llama 3 and\nMixtral-8x22B models, on four free-form question-answering benchmarks\ndemonstrate the superior performance and robustness of semantic density\ncompared to prior approaches.\n", "link": "http://arxiv.org/abs/2405.13845v3", "date": "2024-11-01", "relevancy": 1.6563, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5864}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Density%3A%20Uncertainty%20Quantification%20for%20Large%20Language%20Models%0A%20%20through%20Confidence%20Measurement%20in%20Semantic%20Space&body=Title%3A%20Semantic%20Density%3A%20Uncertainty%20Quantification%20for%20Large%20Language%20Models%0A%20%20through%20Confidence%20Measurement%20in%20Semantic%20Space%0AAuthor%3A%20Xin%20Qiu%20and%20Risto%20Miikkulainen%0AAbstract%3A%20%20%20With%20the%20widespread%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20various%0Adomains%2C%20concerns%20regarding%20the%20trustworthiness%20of%20LLMs%20in%20safety-critical%0Ascenarios%20have%20been%20raised%2C%20due%20to%20their%20unpredictable%20tendency%20to%20hallucinate%0Aand%20generate%20misinformation.%20Existing%20LLMs%20do%20not%20have%20an%20inherent%0Afunctionality%20to%20provide%20the%20users%20with%20an%20uncertainty/confidence%20metric%20for%0Aeach%20response%20it%20generates%2C%20making%20it%20difficult%20to%20evaluate%20trustworthiness.%0AAlthough%20several%20studies%20aim%20to%20develop%20uncertainty%20quantification%20methods%20for%0ALLMs%2C%20they%20have%20fundamental%20limitations%2C%20such%20as%20being%20restricted%20to%0Aclassification%20tasks%2C%20requiring%20additional%20training%20and%20data%2C%20considering%20only%0Alexical%20instead%20of%20semantic%20information%2C%20and%20being%20prompt-wise%20but%20not%0Aresponse-wise.%20A%20new%20framework%20is%20proposed%20in%20this%20paper%20to%20address%20these%0Aissues.%20Semantic%20density%20extracts%20uncertainty/confidence%20information%20for%20each%0Aresponse%20from%20a%20probability%20distribution%20perspective%20in%20semantic%20space.%20It%20has%0Ano%20restriction%20on%20task%20types%20and%20is%20%22off-the-shelf%22%20for%20new%20models%20and%20tasks.%0AExperiments%20on%20seven%20state-of-the-art%20LLMs%2C%20including%20the%20latest%20Llama%203%20and%0AMixtral-8x22B%20models%2C%20on%20four%20free-form%20question-answering%20benchmarks%0Ademonstrate%20the%20superior%20performance%20and%20robustness%20of%20semantic%20density%0Acompared%20to%20prior%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13845v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Density%253A%2520Uncertainty%2520Quantification%2520for%2520Large%2520Language%2520Models%250A%2520%2520through%2520Confidence%2520Measurement%2520in%2520Semantic%2520Space%26entry.906535625%3DXin%2520Qiu%2520and%2520Risto%2520Miikkulainen%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520application%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520various%250Adomains%252C%2520concerns%2520regarding%2520the%2520trustworthiness%2520of%2520LLMs%2520in%2520safety-critical%250Ascenarios%2520have%2520been%2520raised%252C%2520due%2520to%2520their%2520unpredictable%2520tendency%2520to%2520hallucinate%250Aand%2520generate%2520misinformation.%2520Existing%2520LLMs%2520do%2520not%2520have%2520an%2520inherent%250Afunctionality%2520to%2520provide%2520the%2520users%2520with%2520an%2520uncertainty/confidence%2520metric%2520for%250Aeach%2520response%2520it%2520generates%252C%2520making%2520it%2520difficult%2520to%2520evaluate%2520trustworthiness.%250AAlthough%2520several%2520studies%2520aim%2520to%2520develop%2520uncertainty%2520quantification%2520methods%2520for%250ALLMs%252C%2520they%2520have%2520fundamental%2520limitations%252C%2520such%2520as%2520being%2520restricted%2520to%250Aclassification%2520tasks%252C%2520requiring%2520additional%2520training%2520and%2520data%252C%2520considering%2520only%250Alexical%2520instead%2520of%2520semantic%2520information%252C%2520and%2520being%2520prompt-wise%2520but%2520not%250Aresponse-wise.%2520A%2520new%2520framework%2520is%2520proposed%2520in%2520this%2520paper%2520to%2520address%2520these%250Aissues.%2520Semantic%2520density%2520extracts%2520uncertainty/confidence%2520information%2520for%2520each%250Aresponse%2520from%2520a%2520probability%2520distribution%2520perspective%2520in%2520semantic%2520space.%2520It%2520has%250Ano%2520restriction%2520on%2520task%2520types%2520and%2520is%2520%2522off-the-shelf%2522%2520for%2520new%2520models%2520and%2520tasks.%250AExperiments%2520on%2520seven%2520state-of-the-art%2520LLMs%252C%2520including%2520the%2520latest%2520Llama%25203%2520and%250AMixtral-8x22B%2520models%252C%2520on%2520four%2520free-form%2520question-answering%2520benchmarks%250Ademonstrate%2520the%2520superior%2520performance%2520and%2520robustness%2520of%2520semantic%2520density%250Acompared%2520to%2520prior%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13845v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Density%3A%20Uncertainty%20Quantification%20for%20Large%20Language%20Models%0A%20%20through%20Confidence%20Measurement%20in%20Semantic%20Space&entry.906535625=Xin%20Qiu%20and%20Risto%20Miikkulainen&entry.1292438233=%20%20With%20the%20widespread%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20various%0Adomains%2C%20concerns%20regarding%20the%20trustworthiness%20of%20LLMs%20in%20safety-critical%0Ascenarios%20have%20been%20raised%2C%20due%20to%20their%20unpredictable%20tendency%20to%20hallucinate%0Aand%20generate%20misinformation.%20Existing%20LLMs%20do%20not%20have%20an%20inherent%0Afunctionality%20to%20provide%20the%20users%20with%20an%20uncertainty/confidence%20metric%20for%0Aeach%20response%20it%20generates%2C%20making%20it%20difficult%20to%20evaluate%20trustworthiness.%0AAlthough%20several%20studies%20aim%20to%20develop%20uncertainty%20quantification%20methods%20for%0ALLMs%2C%20they%20have%20fundamental%20limitations%2C%20such%20as%20being%20restricted%20to%0Aclassification%20tasks%2C%20requiring%20additional%20training%20and%20data%2C%20considering%20only%0Alexical%20instead%20of%20semantic%20information%2C%20and%20being%20prompt-wise%20but%20not%0Aresponse-wise.%20A%20new%20framework%20is%20proposed%20in%20this%20paper%20to%20address%20these%0Aissues.%20Semantic%20density%20extracts%20uncertainty/confidence%20information%20for%20each%0Aresponse%20from%20a%20probability%20distribution%20perspective%20in%20semantic%20space.%20It%20has%0Ano%20restriction%20on%20task%20types%20and%20is%20%22off-the-shelf%22%20for%20new%20models%20and%20tasks.%0AExperiments%20on%20seven%20state-of-the-art%20LLMs%2C%20including%20the%20latest%20Llama%203%20and%0AMixtral-8x22B%20models%2C%20on%20four%20free-form%20question-answering%20benchmarks%0Ademonstrate%20the%20superior%20performance%20and%20robustness%20of%20semantic%20density%0Acompared%20to%20prior%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13845v3&entry.124074799=Read"},
{"title": "Aligning Motion-Blurred Images Using Contrastive Learning on\n  Overcomplete Pixels", "author": "Leonid Pogorelyuk and Stefan T. Radev", "abstract": "  We propose a new contrastive objective for learning overcomplete pixel-level\nfeatures that are invariant to motion blur. Other invariances (e.g., pose,\nillumination, or weather) can be learned by applying the corresponding\ntransformations on unlabeled images during self-supervised training. We\nshowcase that a simple U-Net trained with our objective can produce local\nfeatures useful for aligning the frames of an unseen video captured with a\nmoving camera under realistic and challenging conditions. Using a carefully\ndesigned toy example, we also show that the overcomplete pixels can encode the\nidentity of objects in an image and the pixel coordinates relative to these\nobjects.\n", "link": "http://arxiv.org/abs/2410.07410v2", "date": "2024-11-01", "relevancy": 1.6557, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5632}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5423}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Motion-Blurred%20Images%20Using%20Contrastive%20Learning%20on%0A%20%20Overcomplete%20Pixels&body=Title%3A%20Aligning%20Motion-Blurred%20Images%20Using%20Contrastive%20Learning%20on%0A%20%20Overcomplete%20Pixels%0AAuthor%3A%20Leonid%20Pogorelyuk%20and%20Stefan%20T.%20Radev%0AAbstract%3A%20%20%20We%20propose%20a%20new%20contrastive%20objective%20for%20learning%20overcomplete%20pixel-level%0Afeatures%20that%20are%20invariant%20to%20motion%20blur.%20Other%20invariances%20%28e.g.%2C%20pose%2C%0Aillumination%2C%20or%20weather%29%20can%20be%20learned%20by%20applying%20the%20corresponding%0Atransformations%20on%20unlabeled%20images%20during%20self-supervised%20training.%20We%0Ashowcase%20that%20a%20simple%20U-Net%20trained%20with%20our%20objective%20can%20produce%20local%0Afeatures%20useful%20for%20aligning%20the%20frames%20of%20an%20unseen%20video%20captured%20with%20a%0Amoving%20camera%20under%20realistic%20and%20challenging%20conditions.%20Using%20a%20carefully%0Adesigned%20toy%20example%2C%20we%20also%20show%20that%20the%20overcomplete%20pixels%20can%20encode%20the%0Aidentity%20of%20objects%20in%20an%20image%20and%20the%20pixel%20coordinates%20relative%20to%20these%0Aobjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Motion-Blurred%2520Images%2520Using%2520Contrastive%2520Learning%2520on%250A%2520%2520Overcomplete%2520Pixels%26entry.906535625%3DLeonid%2520Pogorelyuk%2520and%2520Stefan%2520T.%2520Radev%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520contrastive%2520objective%2520for%2520learning%2520overcomplete%2520pixel-level%250Afeatures%2520that%2520are%2520invariant%2520to%2520motion%2520blur.%2520Other%2520invariances%2520%2528e.g.%252C%2520pose%252C%250Aillumination%252C%2520or%2520weather%2529%2520can%2520be%2520learned%2520by%2520applying%2520the%2520corresponding%250Atransformations%2520on%2520unlabeled%2520images%2520during%2520self-supervised%2520training.%2520We%250Ashowcase%2520that%2520a%2520simple%2520U-Net%2520trained%2520with%2520our%2520objective%2520can%2520produce%2520local%250Afeatures%2520useful%2520for%2520aligning%2520the%2520frames%2520of%2520an%2520unseen%2520video%2520captured%2520with%2520a%250Amoving%2520camera%2520under%2520realistic%2520and%2520challenging%2520conditions.%2520Using%2520a%2520carefully%250Adesigned%2520toy%2520example%252C%2520we%2520also%2520show%2520that%2520the%2520overcomplete%2520pixels%2520can%2520encode%2520the%250Aidentity%2520of%2520objects%2520in%2520an%2520image%2520and%2520the%2520pixel%2520coordinates%2520relative%2520to%2520these%250Aobjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Motion-Blurred%20Images%20Using%20Contrastive%20Learning%20on%0A%20%20Overcomplete%20Pixels&entry.906535625=Leonid%20Pogorelyuk%20and%20Stefan%20T.%20Radev&entry.1292438233=%20%20We%20propose%20a%20new%20contrastive%20objective%20for%20learning%20overcomplete%20pixel-level%0Afeatures%20that%20are%20invariant%20to%20motion%20blur.%20Other%20invariances%20%28e.g.%2C%20pose%2C%0Aillumination%2C%20or%20weather%29%20can%20be%20learned%20by%20applying%20the%20corresponding%0Atransformations%20on%20unlabeled%20images%20during%20self-supervised%20training.%20We%0Ashowcase%20that%20a%20simple%20U-Net%20trained%20with%20our%20objective%20can%20produce%20local%0Afeatures%20useful%20for%20aligning%20the%20frames%20of%20an%20unseen%20video%20captured%20with%20a%0Amoving%20camera%20under%20realistic%20and%20challenging%20conditions.%20Using%20a%20carefully%0Adesigned%20toy%20example%2C%20we%20also%20show%20that%20the%20overcomplete%20pixels%20can%20encode%20the%0Aidentity%20of%20objects%20in%20an%20image%20and%20the%20pixel%20coordinates%20relative%20to%20these%0Aobjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07410v2&entry.124074799=Read"},
{"title": "Nystr\u00f6m Kernel Stein Discrepancy", "author": "Florian Kalinke and Zoltan Szabo and Bharath K. Sriperumbudur", "abstract": "  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith the flexibility of kernel techniques, gained considerable attention.\nThrough the Stein operator, KSD allows the construction of powerful\ngoodness-of-fit tests where it is sufficient to know the target distribution up\nto a multiplicative constant. However, the typical U- and V-statistic-based KSD\nestimators suffer from a quadratic runtime complexity, which hinders their\napplication in large-scale settings. In this work, we propose a Nystr\\\"om-based\nKSD acceleration -- with runtime $\\mathcal O\\left(mn+m^3\\right)$ for $n$\nsamples and $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency with\na classical sub-Gaussian assumption, and demonstrate its applicability for\ngoodness-of-fit testing on a suite of benchmarks.\n", "link": "http://arxiv.org/abs/2406.08401v3", "date": "2024-11-01", "relevancy": 1.6252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4293}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4094}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nystr%C3%B6m%20Kernel%20Stein%20Discrepancy&body=Title%3A%20Nystr%C3%B6m%20Kernel%20Stein%20Discrepancy%0AAuthor%3A%20Florian%20Kalinke%20and%20Zoltan%20Szabo%20and%20Bharath%20K.%20Sriperumbudur%0AAbstract%3A%20%20%20Kernel%20methods%20underpin%20many%20of%20the%20most%20successful%20approaches%20in%20data%0Ascience%20and%20statistics%2C%20and%20they%20allow%20representing%20probability%20measures%20as%0Aelements%20of%20a%20reproducing%20kernel%20Hilbert%20space%20without%20loss%20of%20information.%0ARecently%2C%20the%20kernel%20Stein%20discrepancy%20%28KSD%29%2C%20which%20combines%20Stein%27s%20method%0Awith%20the%20flexibility%20of%20kernel%20techniques%2C%20gained%20considerable%20attention.%0AThrough%20the%20Stein%20operator%2C%20KSD%20allows%20the%20construction%20of%20powerful%0Agoodness-of-fit%20tests%20where%20it%20is%20sufficient%20to%20know%20the%20target%20distribution%20up%0Ato%20a%20multiplicative%20constant.%20However%2C%20the%20typical%20U-%20and%20V-statistic-based%20KSD%0Aestimators%20suffer%20from%20a%20quadratic%20runtime%20complexity%2C%20which%20hinders%20their%0Aapplication%20in%20large-scale%20settings.%20In%20this%20work%2C%20we%20propose%20a%20Nystr%5C%22om-based%0AKSD%20acceleration%20--%20with%20runtime%20%24%5Cmathcal%20O%5Cleft%28mn%2Bm%5E3%5Cright%29%24%20for%20%24n%24%0Asamples%20and%20%24m%5Cll%20n%24%20Nystr%5C%22om%20points%20--%20%2C%20show%20its%20%24%5Csqrt%7Bn%7D%24-consistency%20with%0Aa%20classical%20sub-Gaussian%20assumption%2C%20and%20demonstrate%20its%20applicability%20for%0Agoodness-of-fit%20testing%20on%20a%20suite%20of%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNystr%25C3%25B6m%2520Kernel%2520Stein%2520Discrepancy%26entry.906535625%3DFlorian%2520Kalinke%2520and%2520Zoltan%2520Szabo%2520and%2520Bharath%2520K.%2520Sriperumbudur%26entry.1292438233%3D%2520%2520Kernel%2520methods%2520underpin%2520many%2520of%2520the%2520most%2520successful%2520approaches%2520in%2520data%250Ascience%2520and%2520statistics%252C%2520and%2520they%2520allow%2520representing%2520probability%2520measures%2520as%250Aelements%2520of%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%2520without%2520loss%2520of%2520information.%250ARecently%252C%2520the%2520kernel%2520Stein%2520discrepancy%2520%2528KSD%2529%252C%2520which%2520combines%2520Stein%2527s%2520method%250Awith%2520the%2520flexibility%2520of%2520kernel%2520techniques%252C%2520gained%2520considerable%2520attention.%250AThrough%2520the%2520Stein%2520operator%252C%2520KSD%2520allows%2520the%2520construction%2520of%2520powerful%250Agoodness-of-fit%2520tests%2520where%2520it%2520is%2520sufficient%2520to%2520know%2520the%2520target%2520distribution%2520up%250Ato%2520a%2520multiplicative%2520constant.%2520However%252C%2520the%2520typical%2520U-%2520and%2520V-statistic-based%2520KSD%250Aestimators%2520suffer%2520from%2520a%2520quadratic%2520runtime%2520complexity%252C%2520which%2520hinders%2520their%250Aapplication%2520in%2520large-scale%2520settings.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Nystr%255C%2522om-based%250AKSD%2520acceleration%2520--%2520with%2520runtime%2520%2524%255Cmathcal%2520O%255Cleft%2528mn%252Bm%255E3%255Cright%2529%2524%2520for%2520%2524n%2524%250Asamples%2520and%2520%2524m%255Cll%2520n%2524%2520Nystr%255C%2522om%2520points%2520--%2520%252C%2520show%2520its%2520%2524%255Csqrt%257Bn%257D%2524-consistency%2520with%250Aa%2520classical%2520sub-Gaussian%2520assumption%252C%2520and%2520demonstrate%2520its%2520applicability%2520for%250Agoodness-of-fit%2520testing%2520on%2520a%2520suite%2520of%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nystr%C3%B6m%20Kernel%20Stein%20Discrepancy&entry.906535625=Florian%20Kalinke%20and%20Zoltan%20Szabo%20and%20Bharath%20K.%20Sriperumbudur&entry.1292438233=%20%20Kernel%20methods%20underpin%20many%20of%20the%20most%20successful%20approaches%20in%20data%0Ascience%20and%20statistics%2C%20and%20they%20allow%20representing%20probability%20measures%20as%0Aelements%20of%20a%20reproducing%20kernel%20Hilbert%20space%20without%20loss%20of%20information.%0ARecently%2C%20the%20kernel%20Stein%20discrepancy%20%28KSD%29%2C%20which%20combines%20Stein%27s%20method%0Awith%20the%20flexibility%20of%20kernel%20techniques%2C%20gained%20considerable%20attention.%0AThrough%20the%20Stein%20operator%2C%20KSD%20allows%20the%20construction%20of%20powerful%0Agoodness-of-fit%20tests%20where%20it%20is%20sufficient%20to%20know%20the%20target%20distribution%20up%0Ato%20a%20multiplicative%20constant.%20However%2C%20the%20typical%20U-%20and%20V-statistic-based%20KSD%0Aestimators%20suffer%20from%20a%20quadratic%20runtime%20complexity%2C%20which%20hinders%20their%0Aapplication%20in%20large-scale%20settings.%20In%20this%20work%2C%20we%20propose%20a%20Nystr%5C%22om-based%0AKSD%20acceleration%20--%20with%20runtime%20%24%5Cmathcal%20O%5Cleft%28mn%2Bm%5E3%5Cright%29%24%20for%20%24n%24%0Asamples%20and%20%24m%5Cll%20n%24%20Nystr%5C%22om%20points%20--%20%2C%20show%20its%20%24%5Csqrt%7Bn%7D%24-consistency%20with%0Aa%20classical%20sub-Gaussian%20assumption%2C%20and%20demonstrate%20its%20applicability%20for%0Agoodness-of-fit%20testing%20on%20a%20suite%20of%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08401v3&entry.124074799=Read"},
{"title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "author": "Sirui Hong and Mingchen Zhuge and Jiaqi Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J\u00fcrgen Schmidhuber", "abstract": "  Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT\n", "link": "http://arxiv.org/abs/2308.00352v7", "date": "2024-11-01", "relevancy": 1.5842, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5429}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaGPT%3A%20Meta%20Programming%20for%20A%20Multi-Agent%20Collaborative%20Framework&body=Title%3A%20MetaGPT%3A%20Meta%20Programming%20for%20A%20Multi-Agent%20Collaborative%20Framework%0AAuthor%3A%20Sirui%20Hong%20and%20Mingchen%20Zhuge%20and%20Jiaqi%20Chen%20and%20Xiawu%20Zheng%20and%20Yuheng%20Cheng%20and%20Ceyao%20Zhang%20and%20Jinlin%20Wang%20and%20Zili%20Wang%20and%20Steven%20Ka%20Shing%20Yau%20and%20Zijuan%20Lin%20and%20Liyang%20Zhou%20and%20Chenyu%20Ran%20and%20Lingfeng%20Xiao%20and%20Chenglin%20Wu%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Remarkable%20progress%20has%20been%20made%20on%20automated%20problem%20solving%20through%0Asocieties%20of%20agents%20based%20on%20large%20language%20models%20%28LLMs%29.%20Existing%20LLM-based%0Amulti-agent%20systems%20can%20already%20solve%20simple%20dialogue%20tasks.%20Solutions%20to%20more%0Acomplex%20tasks%2C%20however%2C%20are%20complicated%20through%20logic%20inconsistencies%20due%20to%0Acascading%20hallucinations%20caused%20by%20naively%20chaining%20LLMs.%20Here%20we%20introduce%0AMetaGPT%2C%20an%20innovative%20meta-programming%20framework%20incorporating%20efficient%20human%0Aworkflows%20into%20LLM-based%20multi-agent%20collaborations.%20MetaGPT%20encodes%0AStandardized%20Operating%20Procedures%20%28SOPs%29%20into%20prompt%20sequences%20for%20more%0Astreamlined%20workflows%2C%20thus%20allowing%20agents%20with%20human-like%20domain%20expertise%20to%0Averify%20intermediate%20results%20and%20reduce%20errors.%20MetaGPT%20utilizes%20an%20assembly%0Aline%20paradigm%20to%20assign%20diverse%20roles%20to%20various%20agents%2C%20efficiently%20breaking%0Adown%20complex%20tasks%20into%20subtasks%20involving%20many%20agents%20working%20together.%20On%0Acollaborative%20software%20engineering%20benchmarks%2C%20MetaGPT%20generates%20more%20coherent%0Asolutions%20than%20previous%20chat-based%20multi-agent%20systems.%20Our%20project%20can%20be%0Afound%20at%20https%3A//github.com/geekan/MetaGPT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00352v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaGPT%253A%2520Meta%2520Programming%2520for%2520A%2520Multi-Agent%2520Collaborative%2520Framework%26entry.906535625%3DSirui%2520Hong%2520and%2520Mingchen%2520Zhuge%2520and%2520Jiaqi%2520Chen%2520and%2520Xiawu%2520Zheng%2520and%2520Yuheng%2520Cheng%2520and%2520Ceyao%2520Zhang%2520and%2520Jinlin%2520Wang%2520and%2520Zili%2520Wang%2520and%2520Steven%2520Ka%2520Shing%2520Yau%2520and%2520Zijuan%2520Lin%2520and%2520Liyang%2520Zhou%2520and%2520Chenyu%2520Ran%2520and%2520Lingfeng%2520Xiao%2520and%2520Chenglin%2520Wu%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Remarkable%2520progress%2520has%2520been%2520made%2520on%2520automated%2520problem%2520solving%2520through%250Asocieties%2520of%2520agents%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Existing%2520LLM-based%250Amulti-agent%2520systems%2520can%2520already%2520solve%2520simple%2520dialogue%2520tasks.%2520Solutions%2520to%2520more%250Acomplex%2520tasks%252C%2520however%252C%2520are%2520complicated%2520through%2520logic%2520inconsistencies%2520due%2520to%250Acascading%2520hallucinations%2520caused%2520by%2520naively%2520chaining%2520LLMs.%2520Here%2520we%2520introduce%250AMetaGPT%252C%2520an%2520innovative%2520meta-programming%2520framework%2520incorporating%2520efficient%2520human%250Aworkflows%2520into%2520LLM-based%2520multi-agent%2520collaborations.%2520MetaGPT%2520encodes%250AStandardized%2520Operating%2520Procedures%2520%2528SOPs%2529%2520into%2520prompt%2520sequences%2520for%2520more%250Astreamlined%2520workflows%252C%2520thus%2520allowing%2520agents%2520with%2520human-like%2520domain%2520expertise%2520to%250Averify%2520intermediate%2520results%2520and%2520reduce%2520errors.%2520MetaGPT%2520utilizes%2520an%2520assembly%250Aline%2520paradigm%2520to%2520assign%2520diverse%2520roles%2520to%2520various%2520agents%252C%2520efficiently%2520breaking%250Adown%2520complex%2520tasks%2520into%2520subtasks%2520involving%2520many%2520agents%2520working%2520together.%2520On%250Acollaborative%2520software%2520engineering%2520benchmarks%252C%2520MetaGPT%2520generates%2520more%2520coherent%250Asolutions%2520than%2520previous%2520chat-based%2520multi-agent%2520systems.%2520Our%2520project%2520can%2520be%250Afound%2520at%2520https%253A//github.com/geekan/MetaGPT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.00352v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaGPT%3A%20Meta%20Programming%20for%20A%20Multi-Agent%20Collaborative%20Framework&entry.906535625=Sirui%20Hong%20and%20Mingchen%20Zhuge%20and%20Jiaqi%20Chen%20and%20Xiawu%20Zheng%20and%20Yuheng%20Cheng%20and%20Ceyao%20Zhang%20and%20Jinlin%20Wang%20and%20Zili%20Wang%20and%20Steven%20Ka%20Shing%20Yau%20and%20Zijuan%20Lin%20and%20Liyang%20Zhou%20and%20Chenyu%20Ran%20and%20Lingfeng%20Xiao%20and%20Chenglin%20Wu%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Remarkable%20progress%20has%20been%20made%20on%20automated%20problem%20solving%20through%0Asocieties%20of%20agents%20based%20on%20large%20language%20models%20%28LLMs%29.%20Existing%20LLM-based%0Amulti-agent%20systems%20can%20already%20solve%20simple%20dialogue%20tasks.%20Solutions%20to%20more%0Acomplex%20tasks%2C%20however%2C%20are%20complicated%20through%20logic%20inconsistencies%20due%20to%0Acascading%20hallucinations%20caused%20by%20naively%20chaining%20LLMs.%20Here%20we%20introduce%0AMetaGPT%2C%20an%20innovative%20meta-programming%20framework%20incorporating%20efficient%20human%0Aworkflows%20into%20LLM-based%20multi-agent%20collaborations.%20MetaGPT%20encodes%0AStandardized%20Operating%20Procedures%20%28SOPs%29%20into%20prompt%20sequences%20for%20more%0Astreamlined%20workflows%2C%20thus%20allowing%20agents%20with%20human-like%20domain%20expertise%20to%0Averify%20intermediate%20results%20and%20reduce%20errors.%20MetaGPT%20utilizes%20an%20assembly%0Aline%20paradigm%20to%20assign%20diverse%20roles%20to%20various%20agents%2C%20efficiently%20breaking%0Adown%20complex%20tasks%20into%20subtasks%20involving%20many%20agents%20working%20together.%20On%0Acollaborative%20software%20engineering%20benchmarks%2C%20MetaGPT%20generates%20more%20coherent%0Asolutions%20than%20previous%20chat-based%20multi-agent%20systems.%20Our%20project%20can%20be%0Afound%20at%20https%3A//github.com/geekan/MetaGPT%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00352v7&entry.124074799=Read"},
{"title": "Multimodal Fusion on Low-quality Data: A Comprehensive Survey", "author": "Qingyang Zhang and Yake Wei and Zongbo Han and Huazhu Fu and Xi Peng and Cheng Deng and Qinghua Hu and Cai Xu and Jie Wen and Di Hu and Changqing Zhang", "abstract": "  Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.\n", "link": "http://arxiv.org/abs/2404.18947v3", "date": "2024-11-01", "relevancy": 1.5658, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Fusion%20on%20Low-quality%20Data%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Multimodal%20Fusion%20on%20Low-quality%20Data%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Qingyang%20Zhang%20and%20Yake%20Wei%20and%20Zongbo%20Han%20and%20Huazhu%20Fu%20and%20Xi%20Peng%20and%20Cheng%20Deng%20and%20Qinghua%20Hu%20and%20Cai%20Xu%20and%20Jie%20Wen%20and%20Di%20Hu%20and%20Changqing%20Zhang%0AAbstract%3A%20%20%20Multimodal%20fusion%20focuses%20on%20integrating%20information%20from%20multiple%20modalities%0Awith%20the%20goal%20of%20more%20accurate%20prediction%2C%20which%20has%20achieved%20remarkable%0Aprogress%20in%20a%20wide%20range%20of%20scenarios%2C%20including%20autonomous%20driving%20and%20medical%0Adiagnosis.%20However%2C%20the%20reliability%20of%20multimodal%20fusion%20remains%20largely%0Aunexplored%20especially%20under%20low-quality%20data%20settings.%20This%20paper%20surveys%20the%0Acommon%20challenges%20and%20recent%20advances%20of%20multimodal%20fusion%20in%20the%20wild%20and%0Apresents%20them%20in%20a%20comprehensive%20taxonomy.%20From%20a%20data-centric%20view%2C%20we%0Aidentify%20four%20main%20challenges%20that%20are%20faced%20by%20multimodal%20fusion%20on%0Alow-quality%20data%2C%20namely%20%281%29%20noisy%20multimodal%20data%20that%20are%20contaminated%20with%0Aheterogeneous%20noises%2C%20%282%29%20incomplete%20multimodal%20data%20that%20some%20modalities%20are%0Amissing%2C%20%283%29%20imbalanced%20multimodal%20data%20that%20the%20qualities%20or%20properties%20of%0Adifferent%20modalities%20are%20significantly%20different%20and%20%284%29%20quality-varying%0Amultimodal%20data%20that%20the%20quality%20of%20each%20modality%20dynamically%20changes%20with%0Arespect%20to%20different%20samples.%20This%20new%20taxonomy%20will%20enable%20researchers%20to%0Aunderstand%20the%20state%20of%20the%20field%20and%20identify%20several%20potential%20directions.%20We%0Aalso%20provide%20discussion%20for%20the%20open%20problems%20in%20this%20field%20together%20with%0Ainteresting%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18947v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Fusion%2520on%2520Low-quality%2520Data%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DQingyang%2520Zhang%2520and%2520Yake%2520Wei%2520and%2520Zongbo%2520Han%2520and%2520Huazhu%2520Fu%2520and%2520Xi%2520Peng%2520and%2520Cheng%2520Deng%2520and%2520Qinghua%2520Hu%2520and%2520Cai%2520Xu%2520and%2520Jie%2520Wen%2520and%2520Di%2520Hu%2520and%2520Changqing%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520fusion%2520focuses%2520on%2520integrating%2520information%2520from%2520multiple%2520modalities%250Awith%2520the%2520goal%2520of%2520more%2520accurate%2520prediction%252C%2520which%2520has%2520achieved%2520remarkable%250Aprogress%2520in%2520a%2520wide%2520range%2520of%2520scenarios%252C%2520including%2520autonomous%2520driving%2520and%2520medical%250Adiagnosis.%2520However%252C%2520the%2520reliability%2520of%2520multimodal%2520fusion%2520remains%2520largely%250Aunexplored%2520especially%2520under%2520low-quality%2520data%2520settings.%2520This%2520paper%2520surveys%2520the%250Acommon%2520challenges%2520and%2520recent%2520advances%2520of%2520multimodal%2520fusion%2520in%2520the%2520wild%2520and%250Apresents%2520them%2520in%2520a%2520comprehensive%2520taxonomy.%2520From%2520a%2520data-centric%2520view%252C%2520we%250Aidentify%2520four%2520main%2520challenges%2520that%2520are%2520faced%2520by%2520multimodal%2520fusion%2520on%250Alow-quality%2520data%252C%2520namely%2520%25281%2529%2520noisy%2520multimodal%2520data%2520that%2520are%2520contaminated%2520with%250Aheterogeneous%2520noises%252C%2520%25282%2529%2520incomplete%2520multimodal%2520data%2520that%2520some%2520modalities%2520are%250Amissing%252C%2520%25283%2529%2520imbalanced%2520multimodal%2520data%2520that%2520the%2520qualities%2520or%2520properties%2520of%250Adifferent%2520modalities%2520are%2520significantly%2520different%2520and%2520%25284%2529%2520quality-varying%250Amultimodal%2520data%2520that%2520the%2520quality%2520of%2520each%2520modality%2520dynamically%2520changes%2520with%250Arespect%2520to%2520different%2520samples.%2520This%2520new%2520taxonomy%2520will%2520enable%2520researchers%2520to%250Aunderstand%2520the%2520state%2520of%2520the%2520field%2520and%2520identify%2520several%2520potential%2520directions.%2520We%250Aalso%2520provide%2520discussion%2520for%2520the%2520open%2520problems%2520in%2520this%2520field%2520together%2520with%250Ainteresting%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18947v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Fusion%20on%20Low-quality%20Data%3A%20A%20Comprehensive%20Survey&entry.906535625=Qingyang%20Zhang%20and%20Yake%20Wei%20and%20Zongbo%20Han%20and%20Huazhu%20Fu%20and%20Xi%20Peng%20and%20Cheng%20Deng%20and%20Qinghua%20Hu%20and%20Cai%20Xu%20and%20Jie%20Wen%20and%20Di%20Hu%20and%20Changqing%20Zhang&entry.1292438233=%20%20Multimodal%20fusion%20focuses%20on%20integrating%20information%20from%20multiple%20modalities%0Awith%20the%20goal%20of%20more%20accurate%20prediction%2C%20which%20has%20achieved%20remarkable%0Aprogress%20in%20a%20wide%20range%20of%20scenarios%2C%20including%20autonomous%20driving%20and%20medical%0Adiagnosis.%20However%2C%20the%20reliability%20of%20multimodal%20fusion%20remains%20largely%0Aunexplored%20especially%20under%20low-quality%20data%20settings.%20This%20paper%20surveys%20the%0Acommon%20challenges%20and%20recent%20advances%20of%20multimodal%20fusion%20in%20the%20wild%20and%0Apresents%20them%20in%20a%20comprehensive%20taxonomy.%20From%20a%20data-centric%20view%2C%20we%0Aidentify%20four%20main%20challenges%20that%20are%20faced%20by%20multimodal%20fusion%20on%0Alow-quality%20data%2C%20namely%20%281%29%20noisy%20multimodal%20data%20that%20are%20contaminated%20with%0Aheterogeneous%20noises%2C%20%282%29%20incomplete%20multimodal%20data%20that%20some%20modalities%20are%0Amissing%2C%20%283%29%20imbalanced%20multimodal%20data%20that%20the%20qualities%20or%20properties%20of%0Adifferent%20modalities%20are%20significantly%20different%20and%20%284%29%20quality-varying%0Amultimodal%20data%20that%20the%20quality%20of%20each%20modality%20dynamically%20changes%20with%0Arespect%20to%20different%20samples.%20This%20new%20taxonomy%20will%20enable%20researchers%20to%0Aunderstand%20the%20state%20of%20the%20field%20and%20identify%20several%20potential%20directions.%20We%0Aalso%20provide%20discussion%20for%20the%20open%20problems%20in%20this%20field%20together%20with%0Ainteresting%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18947v3&entry.124074799=Read"},
{"title": "Diffusion Spectral Representation for Reinforcement Learning", "author": "Dmitry Shribak and Chen-Xiao Gao and Yitong Li and Chenjun Xiao and Bo Dai", "abstract": "  Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings.\n", "link": "http://arxiv.org/abs/2406.16121v2", "date": "2024-11-01", "relevancy": 1.5592, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5653}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Spectral%20Representation%20for%20Reinforcement%20Learning&body=Title%3A%20Diffusion%20Spectral%20Representation%20for%20Reinforcement%20Learning%0AAuthor%3A%20Dmitry%20Shribak%20and%20Chen-Xiao%20Gao%20and%20Yitong%20Li%20and%20Chenjun%20Xiao%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Diffusion-based%20models%20have%20achieved%20notable%20empirical%20successes%20in%0Areinforcement%20learning%20%28RL%29%20due%20to%20their%20expressiveness%20in%20modeling%20complex%0Adistributions.%20Despite%20existing%20methods%20being%20promising%2C%20the%20key%20challenge%20of%0Aextending%20existing%20methods%20for%20broader%20real-world%20applications%20lies%20in%20the%0Acomputational%20cost%20at%20inference%20time%2C%20i.e.%2C%20sampling%20from%20a%20diffusion%20model%20is%0Aconsiderably%20slow%20as%20it%20often%20requires%20tens%20to%20hundreds%20of%20iterations%20to%0Agenerate%20even%20one%20sample.%20To%20circumvent%20this%20issue%2C%20we%20propose%20to%20leverage%20the%0Aflexibility%20of%20diffusion%20models%20for%20RL%20from%20a%20representation%20learning%0Aperspective.%20In%20particular%2C%20by%20exploiting%20the%20connection%20between%20diffusion%0Amodels%20and%20energy-based%20models%2C%20we%20develop%20Diffusion%20Spectral%20Representation%0A%28Diff-SR%29%2C%20a%20coherent%20algorithm%20framework%20that%20enables%20extracting%20sufficient%0Arepresentations%20for%20value%20functions%20in%20Markov%20decision%20processes%20%28MDP%29%20and%0Apartially%20observable%20Markov%20decision%20processes%20%28POMDP%29.%20We%20further%20demonstrate%0Ahow%20Diff-SR%20facilitates%20efficient%20policy%20optimization%20and%20practical%20algorithms%0Awhile%20explicitly%20bypassing%20the%20difficulty%20and%20inference%20cost%20of%20sampling%20from%0Athe%20diffusion%20model.%20Finally%2C%20we%20provide%20comprehensive%20empirical%20studies%20to%0Averify%20the%20benefits%20of%20Diff-SR%20in%20delivering%20robust%20and%20advantageous%0Aperformance%20across%20various%20benchmarks%20with%20both%20fully%20and%20partially%20observable%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16121v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Spectral%2520Representation%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DDmitry%2520Shribak%2520and%2520Chen-Xiao%2520Gao%2520and%2520Yitong%2520Li%2520and%2520Chenjun%2520Xiao%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Diffusion-based%2520models%2520have%2520achieved%2520notable%2520empirical%2520successes%2520in%250Areinforcement%2520learning%2520%2528RL%2529%2520due%2520to%2520their%2520expressiveness%2520in%2520modeling%2520complex%250Adistributions.%2520Despite%2520existing%2520methods%2520being%2520promising%252C%2520the%2520key%2520challenge%2520of%250Aextending%2520existing%2520methods%2520for%2520broader%2520real-world%2520applications%2520lies%2520in%2520the%250Acomputational%2520cost%2520at%2520inference%2520time%252C%2520i.e.%252C%2520sampling%2520from%2520a%2520diffusion%2520model%2520is%250Aconsiderably%2520slow%2520as%2520it%2520often%2520requires%2520tens%2520to%2520hundreds%2520of%2520iterations%2520to%250Agenerate%2520even%2520one%2520sample.%2520To%2520circumvent%2520this%2520issue%252C%2520we%2520propose%2520to%2520leverage%2520the%250Aflexibility%2520of%2520diffusion%2520models%2520for%2520RL%2520from%2520a%2520representation%2520learning%250Aperspective.%2520In%2520particular%252C%2520by%2520exploiting%2520the%2520connection%2520between%2520diffusion%250Amodels%2520and%2520energy-based%2520models%252C%2520we%2520develop%2520Diffusion%2520Spectral%2520Representation%250A%2528Diff-SR%2529%252C%2520a%2520coherent%2520algorithm%2520framework%2520that%2520enables%2520extracting%2520sufficient%250Arepresentations%2520for%2520value%2520functions%2520in%2520Markov%2520decision%2520processes%2520%2528MDP%2529%2520and%250Apartially%2520observable%2520Markov%2520decision%2520processes%2520%2528POMDP%2529.%2520We%2520further%2520demonstrate%250Ahow%2520Diff-SR%2520facilitates%2520efficient%2520policy%2520optimization%2520and%2520practical%2520algorithms%250Awhile%2520explicitly%2520bypassing%2520the%2520difficulty%2520and%2520inference%2520cost%2520of%2520sampling%2520from%250Athe%2520diffusion%2520model.%2520Finally%252C%2520we%2520provide%2520comprehensive%2520empirical%2520studies%2520to%250Averify%2520the%2520benefits%2520of%2520Diff-SR%2520in%2520delivering%2520robust%2520and%2520advantageous%250Aperformance%2520across%2520various%2520benchmarks%2520with%2520both%2520fully%2520and%2520partially%2520observable%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16121v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Spectral%20Representation%20for%20Reinforcement%20Learning&entry.906535625=Dmitry%20Shribak%20and%20Chen-Xiao%20Gao%20and%20Yitong%20Li%20and%20Chenjun%20Xiao%20and%20Bo%20Dai&entry.1292438233=%20%20Diffusion-based%20models%20have%20achieved%20notable%20empirical%20successes%20in%0Areinforcement%20learning%20%28RL%29%20due%20to%20their%20expressiveness%20in%20modeling%20complex%0Adistributions.%20Despite%20existing%20methods%20being%20promising%2C%20the%20key%20challenge%20of%0Aextending%20existing%20methods%20for%20broader%20real-world%20applications%20lies%20in%20the%0Acomputational%20cost%20at%20inference%20time%2C%20i.e.%2C%20sampling%20from%20a%20diffusion%20model%20is%0Aconsiderably%20slow%20as%20it%20often%20requires%20tens%20to%20hundreds%20of%20iterations%20to%0Agenerate%20even%20one%20sample.%20To%20circumvent%20this%20issue%2C%20we%20propose%20to%20leverage%20the%0Aflexibility%20of%20diffusion%20models%20for%20RL%20from%20a%20representation%20learning%0Aperspective.%20In%20particular%2C%20by%20exploiting%20the%20connection%20between%20diffusion%0Amodels%20and%20energy-based%20models%2C%20we%20develop%20Diffusion%20Spectral%20Representation%0A%28Diff-SR%29%2C%20a%20coherent%20algorithm%20framework%20that%20enables%20extracting%20sufficient%0Arepresentations%20for%20value%20functions%20in%20Markov%20decision%20processes%20%28MDP%29%20and%0Apartially%20observable%20Markov%20decision%20processes%20%28POMDP%29.%20We%20further%20demonstrate%0Ahow%20Diff-SR%20facilitates%20efficient%20policy%20optimization%20and%20practical%20algorithms%0Awhile%20explicitly%20bypassing%20the%20difficulty%20and%20inference%20cost%20of%20sampling%20from%0Athe%20diffusion%20model.%20Finally%2C%20we%20provide%20comprehensive%20empirical%20studies%20to%0Averify%20the%20benefits%20of%20Diff-SR%20in%20delivering%20robust%20and%20advantageous%0Aperformance%20across%20various%20benchmarks%20with%20both%20fully%20and%20partially%20observable%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16121v2&entry.124074799=Read"},
{"title": "Theoretical Foundations of Deep Selective State-Space Models", "author": "Nicola Muca Cirone and Antonio Orvieto and Benjamin Walker and Cristopher Salvi and Terry Lyons", "abstract": "  Structured state-space models (SSMs) such as S4, stemming from the seminal\nwork of Gu et al., are gaining popularity as effective approaches for modeling\nsequential data. Deep SSMs demonstrate outstanding performance across a diverse\nset of domains, at a reduced training and inference cost compared to\nattention-based transformers. Recent developments show that if the linear\nrecurrence powering SSMs allows for multiplicative interactions between inputs\nand hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture\ncan surpass in both in accuracy and efficiency attention-powered foundation\nmodels trained on text, at scales of billion parameters. In this paper, we give\ntheoretical grounding to this recent finding using tools from Rough Path\nTheory: we show that when random linear recurrences are equipped with simple\ninput-controlled transitions (selectivity mechanism), then the hidden state is\nprovably a low-dimensional projection of a powerful mathematical object called\nthe signature of the input -- capturing non-linear interactions between tokens\nat distinct timescales. Our theory not only motivates the success of modern\nselective state-space models such as Mamba but also provides a solid framework\nto understand the expressive power of future SSM variants.\n", "link": "http://arxiv.org/abs/2402.19047v3", "date": "2024-11-01", "relevancy": 1.4988, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5208}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5068}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Foundations%20of%20Deep%20Selective%20State-Space%20Models&body=Title%3A%20Theoretical%20Foundations%20of%20Deep%20Selective%20State-Space%20Models%0AAuthor%3A%20Nicola%20Muca%20Cirone%20and%20Antonio%20Orvieto%20and%20Benjamin%20Walker%20and%20Cristopher%20Salvi%20and%20Terry%20Lyons%0AAbstract%3A%20%20%20Structured%20state-space%20models%20%28SSMs%29%20such%20as%20S4%2C%20stemming%20from%20the%20seminal%0Awork%20of%20Gu%20et%20al.%2C%20are%20gaining%20popularity%20as%20effective%20approaches%20for%20modeling%0Asequential%20data.%20Deep%20SSMs%20demonstrate%20outstanding%20performance%20across%20a%20diverse%0Aset%20of%20domains%2C%20at%20a%20reduced%20training%20and%20inference%20cost%20compared%20to%0Aattention-based%20transformers.%20Recent%20developments%20show%20that%20if%20the%20linear%0Arecurrence%20powering%20SSMs%20allows%20for%20multiplicative%20interactions%20between%20inputs%0Aand%20hidden%20states%20%28e.g.%20GateLoop%2C%20Mamba%2C%20GLA%29%2C%20then%20the%20resulting%20architecture%0Acan%20surpass%20in%20both%20in%20accuracy%20and%20efficiency%20attention-powered%20foundation%0Amodels%20trained%20on%20text%2C%20at%20scales%20of%20billion%20parameters.%20In%20this%20paper%2C%20we%20give%0Atheoretical%20grounding%20to%20this%20recent%20finding%20using%20tools%20from%20Rough%20Path%0ATheory%3A%20we%20show%20that%20when%20random%20linear%20recurrences%20are%20equipped%20with%20simple%0Ainput-controlled%20transitions%20%28selectivity%20mechanism%29%2C%20then%20the%20hidden%20state%20is%0Aprovably%20a%20low-dimensional%20projection%20of%20a%20powerful%20mathematical%20object%20called%0Athe%20signature%20of%20the%20input%20--%20capturing%20non-linear%20interactions%20between%20tokens%0Aat%20distinct%20timescales.%20Our%20theory%20not%20only%20motivates%20the%20success%20of%20modern%0Aselective%20state-space%20models%20such%20as%20Mamba%20but%20also%20provides%20a%20solid%20framework%0Ato%20understand%20the%20expressive%20power%20of%20future%20SSM%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19047v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Foundations%2520of%2520Deep%2520Selective%2520State-Space%2520Models%26entry.906535625%3DNicola%2520Muca%2520Cirone%2520and%2520Antonio%2520Orvieto%2520and%2520Benjamin%2520Walker%2520and%2520Cristopher%2520Salvi%2520and%2520Terry%2520Lyons%26entry.1292438233%3D%2520%2520Structured%2520state-space%2520models%2520%2528SSMs%2529%2520such%2520as%2520S4%252C%2520stemming%2520from%2520the%2520seminal%250Awork%2520of%2520Gu%2520et%2520al.%252C%2520are%2520gaining%2520popularity%2520as%2520effective%2520approaches%2520for%2520modeling%250Asequential%2520data.%2520Deep%2520SSMs%2520demonstrate%2520outstanding%2520performance%2520across%2520a%2520diverse%250Aset%2520of%2520domains%252C%2520at%2520a%2520reduced%2520training%2520and%2520inference%2520cost%2520compared%2520to%250Aattention-based%2520transformers.%2520Recent%2520developments%2520show%2520that%2520if%2520the%2520linear%250Arecurrence%2520powering%2520SSMs%2520allows%2520for%2520multiplicative%2520interactions%2520between%2520inputs%250Aand%2520hidden%2520states%2520%2528e.g.%2520GateLoop%252C%2520Mamba%252C%2520GLA%2529%252C%2520then%2520the%2520resulting%2520architecture%250Acan%2520surpass%2520in%2520both%2520in%2520accuracy%2520and%2520efficiency%2520attention-powered%2520foundation%250Amodels%2520trained%2520on%2520text%252C%2520at%2520scales%2520of%2520billion%2520parameters.%2520In%2520this%2520paper%252C%2520we%2520give%250Atheoretical%2520grounding%2520to%2520this%2520recent%2520finding%2520using%2520tools%2520from%2520Rough%2520Path%250ATheory%253A%2520we%2520show%2520that%2520when%2520random%2520linear%2520recurrences%2520are%2520equipped%2520with%2520simple%250Ainput-controlled%2520transitions%2520%2528selectivity%2520mechanism%2529%252C%2520then%2520the%2520hidden%2520state%2520is%250Aprovably%2520a%2520low-dimensional%2520projection%2520of%2520a%2520powerful%2520mathematical%2520object%2520called%250Athe%2520signature%2520of%2520the%2520input%2520--%2520capturing%2520non-linear%2520interactions%2520between%2520tokens%250Aat%2520distinct%2520timescales.%2520Our%2520theory%2520not%2520only%2520motivates%2520the%2520success%2520of%2520modern%250Aselective%2520state-space%2520models%2520such%2520as%2520Mamba%2520but%2520also%2520provides%2520a%2520solid%2520framework%250Ato%2520understand%2520the%2520expressive%2520power%2520of%2520future%2520SSM%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19047v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Foundations%20of%20Deep%20Selective%20State-Space%20Models&entry.906535625=Nicola%20Muca%20Cirone%20and%20Antonio%20Orvieto%20and%20Benjamin%20Walker%20and%20Cristopher%20Salvi%20and%20Terry%20Lyons&entry.1292438233=%20%20Structured%20state-space%20models%20%28SSMs%29%20such%20as%20S4%2C%20stemming%20from%20the%20seminal%0Awork%20of%20Gu%20et%20al.%2C%20are%20gaining%20popularity%20as%20effective%20approaches%20for%20modeling%0Asequential%20data.%20Deep%20SSMs%20demonstrate%20outstanding%20performance%20across%20a%20diverse%0Aset%20of%20domains%2C%20at%20a%20reduced%20training%20and%20inference%20cost%20compared%20to%0Aattention-based%20transformers.%20Recent%20developments%20show%20that%20if%20the%20linear%0Arecurrence%20powering%20SSMs%20allows%20for%20multiplicative%20interactions%20between%20inputs%0Aand%20hidden%20states%20%28e.g.%20GateLoop%2C%20Mamba%2C%20GLA%29%2C%20then%20the%20resulting%20architecture%0Acan%20surpass%20in%20both%20in%20accuracy%20and%20efficiency%20attention-powered%20foundation%0Amodels%20trained%20on%20text%2C%20at%20scales%20of%20billion%20parameters.%20In%20this%20paper%2C%20we%20give%0Atheoretical%20grounding%20to%20this%20recent%20finding%20using%20tools%20from%20Rough%20Path%0ATheory%3A%20we%20show%20that%20when%20random%20linear%20recurrences%20are%20equipped%20with%20simple%0Ainput-controlled%20transitions%20%28selectivity%20mechanism%29%2C%20then%20the%20hidden%20state%20is%0Aprovably%20a%20low-dimensional%20projection%20of%20a%20powerful%20mathematical%20object%20called%0Athe%20signature%20of%20the%20input%20--%20capturing%20non-linear%20interactions%20between%20tokens%0Aat%20distinct%20timescales.%20Our%20theory%20not%20only%20motivates%20the%20success%20of%20modern%0Aselective%20state-space%20models%20such%20as%20Mamba%20but%20also%20provides%20a%20solid%20framework%0Ato%20understand%20the%20expressive%20power%20of%20future%20SSM%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19047v3&entry.124074799=Read"},
{"title": "Flexible Fairness-Aware Learning via Inverse Conditional Permutation", "author": "Yuheng Lai and Leying Guan", "abstract": "  Equalized odds, as a popular notion of algorithmic fairness, aims to ensure\nthat sensitive variables, such as race and gender, do not unfairly influence\nthe algorithm's prediction when conditioning on the true outcome. Despite rapid\nadvancements, current research primarily focuses on equalized odds violations\ncaused by a single sensitive attribute, leaving the challenge of simultaneously\naccounting for multiple attributes largely unaddressed. We bridge this gap by\nintroducing an in-processing fairness-aware learning approach, FairICP, which\nintegrates adversarial learning with a novel inverse conditional permutation\nscheme. FairICP offers a theoretically justified, flexible, and efficient\nscheme to promote equalized odds under fairness conditions described by complex\nand multidimensional sensitive attributes. The efficacy and adaptability of our\nmethod are demonstrated through both simulation studies and empirical analyses\nof real-world datasets.\n", "link": "http://arxiv.org/abs/2404.05678v3", "date": "2024-11-01", "relevancy": 1.4934, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4808}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20Fairness-Aware%20Learning%20via%20Inverse%20Conditional%20Permutation&body=Title%3A%20Flexible%20Fairness-Aware%20Learning%20via%20Inverse%20Conditional%20Permutation%0AAuthor%3A%20Yuheng%20Lai%20and%20Leying%20Guan%0AAbstract%3A%20%20%20Equalized%20odds%2C%20as%20a%20popular%20notion%20of%20algorithmic%20fairness%2C%20aims%20to%20ensure%0Athat%20sensitive%20variables%2C%20such%20as%20race%20and%20gender%2C%20do%20not%20unfairly%20influence%0Athe%20algorithm%27s%20prediction%20when%20conditioning%20on%20the%20true%20outcome.%20Despite%20rapid%0Aadvancements%2C%20current%20research%20primarily%20focuses%20on%20equalized%20odds%20violations%0Acaused%20by%20a%20single%20sensitive%20attribute%2C%20leaving%20the%20challenge%20of%20simultaneously%0Aaccounting%20for%20multiple%20attributes%20largely%20unaddressed.%20We%20bridge%20this%20gap%20by%0Aintroducing%20an%20in-processing%20fairness-aware%20learning%20approach%2C%20FairICP%2C%20which%0Aintegrates%20adversarial%20learning%20with%20a%20novel%20inverse%20conditional%20permutation%0Ascheme.%20FairICP%20offers%20a%20theoretically%20justified%2C%20flexible%2C%20and%20efficient%0Ascheme%20to%20promote%20equalized%20odds%20under%20fairness%20conditions%20described%20by%20complex%0Aand%20multidimensional%20sensitive%20attributes.%20The%20efficacy%20and%20adaptability%20of%20our%0Amethod%20are%20demonstrated%20through%20both%20simulation%20studies%20and%20empirical%20analyses%0Aof%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05678v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520Fairness-Aware%2520Learning%2520via%2520Inverse%2520Conditional%2520Permutation%26entry.906535625%3DYuheng%2520Lai%2520and%2520Leying%2520Guan%26entry.1292438233%3D%2520%2520Equalized%2520odds%252C%2520as%2520a%2520popular%2520notion%2520of%2520algorithmic%2520fairness%252C%2520aims%2520to%2520ensure%250Athat%2520sensitive%2520variables%252C%2520such%2520as%2520race%2520and%2520gender%252C%2520do%2520not%2520unfairly%2520influence%250Athe%2520algorithm%2527s%2520prediction%2520when%2520conditioning%2520on%2520the%2520true%2520outcome.%2520Despite%2520rapid%250Aadvancements%252C%2520current%2520research%2520primarily%2520focuses%2520on%2520equalized%2520odds%2520violations%250Acaused%2520by%2520a%2520single%2520sensitive%2520attribute%252C%2520leaving%2520the%2520challenge%2520of%2520simultaneously%250Aaccounting%2520for%2520multiple%2520attributes%2520largely%2520unaddressed.%2520We%2520bridge%2520this%2520gap%2520by%250Aintroducing%2520an%2520in-processing%2520fairness-aware%2520learning%2520approach%252C%2520FairICP%252C%2520which%250Aintegrates%2520adversarial%2520learning%2520with%2520a%2520novel%2520inverse%2520conditional%2520permutation%250Ascheme.%2520FairICP%2520offers%2520a%2520theoretically%2520justified%252C%2520flexible%252C%2520and%2520efficient%250Ascheme%2520to%2520promote%2520equalized%2520odds%2520under%2520fairness%2520conditions%2520described%2520by%2520complex%250Aand%2520multidimensional%2520sensitive%2520attributes.%2520The%2520efficacy%2520and%2520adaptability%2520of%2520our%250Amethod%2520are%2520demonstrated%2520through%2520both%2520simulation%2520studies%2520and%2520empirical%2520analyses%250Aof%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05678v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20Fairness-Aware%20Learning%20via%20Inverse%20Conditional%20Permutation&entry.906535625=Yuheng%20Lai%20and%20Leying%20Guan&entry.1292438233=%20%20Equalized%20odds%2C%20as%20a%20popular%20notion%20of%20algorithmic%20fairness%2C%20aims%20to%20ensure%0Athat%20sensitive%20variables%2C%20such%20as%20race%20and%20gender%2C%20do%20not%20unfairly%20influence%0Athe%20algorithm%27s%20prediction%20when%20conditioning%20on%20the%20true%20outcome.%20Despite%20rapid%0Aadvancements%2C%20current%20research%20primarily%20focuses%20on%20equalized%20odds%20violations%0Acaused%20by%20a%20single%20sensitive%20attribute%2C%20leaving%20the%20challenge%20of%20simultaneously%0Aaccounting%20for%20multiple%20attributes%20largely%20unaddressed.%20We%20bridge%20this%20gap%20by%0Aintroducing%20an%20in-processing%20fairness-aware%20learning%20approach%2C%20FairICP%2C%20which%0Aintegrates%20adversarial%20learning%20with%20a%20novel%20inverse%20conditional%20permutation%0Ascheme.%20FairICP%20offers%20a%20theoretically%20justified%2C%20flexible%2C%20and%20efficient%0Ascheme%20to%20promote%20equalized%20odds%20under%20fairness%20conditions%20described%20by%20complex%0Aand%20multidimensional%20sensitive%20attributes.%20The%20efficacy%20and%20adaptability%20of%20our%0Amethod%20are%20demonstrated%20through%20both%20simulation%20studies%20and%20empirical%20analyses%0Aof%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05678v3&entry.124074799=Read"},
{"title": "Tree Ensembles for Contextual Bandits", "author": "Hannes Nilsson and Rikard Johansson and Niklas \u00c5kerblom and Morteza Haghir Chehreghani", "abstract": "  We propose a new framework for contextual multi-armed bandits based on tree\nensembles. Our framework adapts two widely used bandit methods, Upper\nConfidence Bound and Thompson Sampling, for both standard and combinatorial\nsettings. As part of this framework, we propose a novel method of estimating\nthe uncertainty in tree ensemble predictions. We further demonstrate the\neffectiveness of our framework via several experimental studies, employing\nXGBoost and random forests, two popular tree ensemble methods. Compared to\nstate-of-the-art methods based on decision trees and neural networks, our\nmethods exhibit superior performance in terms of both regret minimization and\ncomputational runtime, when applied to benchmark datasets and the real-world\napplication of navigation over road networks.\n", "link": "http://arxiv.org/abs/2402.06963v3", "date": "2024-11-01", "relevancy": 1.4668, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4868}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20Ensembles%20for%20Contextual%20Bandits&body=Title%3A%20Tree%20Ensembles%20for%20Contextual%20Bandits%0AAuthor%3A%20Hannes%20Nilsson%20and%20Rikard%20Johansson%20and%20Niklas%20%C3%85kerblom%20and%20Morteza%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20We%20propose%20a%20new%20framework%20for%20contextual%20multi-armed%20bandits%20based%20on%20tree%0Aensembles.%20Our%20framework%20adapts%20two%20widely%20used%20bandit%20methods%2C%20Upper%0AConfidence%20Bound%20and%20Thompson%20Sampling%2C%20for%20both%20standard%20and%20combinatorial%0Asettings.%20As%20part%20of%20this%20framework%2C%20we%20propose%20a%20novel%20method%20of%20estimating%0Athe%20uncertainty%20in%20tree%20ensemble%20predictions.%20We%20further%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20via%20several%20experimental%20studies%2C%20employing%0AXGBoost%20and%20random%20forests%2C%20two%20popular%20tree%20ensemble%20methods.%20Compared%20to%0Astate-of-the-art%20methods%20based%20on%20decision%20trees%20and%20neural%20networks%2C%20our%0Amethods%20exhibit%20superior%20performance%20in%20terms%20of%20both%20regret%20minimization%20and%0Acomputational%20runtime%2C%20when%20applied%20to%20benchmark%20datasets%20and%20the%20real-world%0Aapplication%20of%20navigation%20over%20road%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06963v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520Ensembles%2520for%2520Contextual%2520Bandits%26entry.906535625%3DHannes%2520Nilsson%2520and%2520Rikard%2520Johansson%2520and%2520Niklas%2520%25C3%2585kerblom%2520and%2520Morteza%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520framework%2520for%2520contextual%2520multi-armed%2520bandits%2520based%2520on%2520tree%250Aensembles.%2520Our%2520framework%2520adapts%2520two%2520widely%2520used%2520bandit%2520methods%252C%2520Upper%250AConfidence%2520Bound%2520and%2520Thompson%2520Sampling%252C%2520for%2520both%2520standard%2520and%2520combinatorial%250Asettings.%2520As%2520part%2520of%2520this%2520framework%252C%2520we%2520propose%2520a%2520novel%2520method%2520of%2520estimating%250Athe%2520uncertainty%2520in%2520tree%2520ensemble%2520predictions.%2520We%2520further%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520framework%2520via%2520several%2520experimental%2520studies%252C%2520employing%250AXGBoost%2520and%2520random%2520forests%252C%2520two%2520popular%2520tree%2520ensemble%2520methods.%2520Compared%2520to%250Astate-of-the-art%2520methods%2520based%2520on%2520decision%2520trees%2520and%2520neural%2520networks%252C%2520our%250Amethods%2520exhibit%2520superior%2520performance%2520in%2520terms%2520of%2520both%2520regret%2520minimization%2520and%250Acomputational%2520runtime%252C%2520when%2520applied%2520to%2520benchmark%2520datasets%2520and%2520the%2520real-world%250Aapplication%2520of%2520navigation%2520over%2520road%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06963v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20Ensembles%20for%20Contextual%20Bandits&entry.906535625=Hannes%20Nilsson%20and%20Rikard%20Johansson%20and%20Niklas%20%C3%85kerblom%20and%20Morteza%20Haghir%20Chehreghani&entry.1292438233=%20%20We%20propose%20a%20new%20framework%20for%20contextual%20multi-armed%20bandits%20based%20on%20tree%0Aensembles.%20Our%20framework%20adapts%20two%20widely%20used%20bandit%20methods%2C%20Upper%0AConfidence%20Bound%20and%20Thompson%20Sampling%2C%20for%20both%20standard%20and%20combinatorial%0Asettings.%20As%20part%20of%20this%20framework%2C%20we%20propose%20a%20novel%20method%20of%20estimating%0Athe%20uncertainty%20in%20tree%20ensemble%20predictions.%20We%20further%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20via%20several%20experimental%20studies%2C%20employing%0AXGBoost%20and%20random%20forests%2C%20two%20popular%20tree%20ensemble%20methods.%20Compared%20to%0Astate-of-the-art%20methods%20based%20on%20decision%20trees%20and%20neural%20networks%2C%20our%0Amethods%20exhibit%20superior%20performance%20in%20terms%20of%20both%20regret%20minimization%20and%0Acomputational%20runtime%2C%20when%20applied%20to%20benchmark%20datasets%20and%20the%20real-world%0Aapplication%20of%20navigation%20over%20road%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06963v3&entry.124074799=Read"},
{"title": "StatWhy: Formal Verification Tool for Statistical Hypothesis Testing\n  Programs", "author": "Yusuke Kawamoto and Kentaro Kobayashi and Kohei Suenaga", "abstract": "  Statistical methods have been widely misused and misinterpreted in various\nscientific fields, raising significant concerns about the integrity of\nscientific research. To mitigate this problem, we propose a new method for\nformally specifying and automatically verifying the correctness of statistical\nprograms. In this method, programmers are required to annotate the source code\nof the statistical programs with the requirements for these methods. Through\nthis annotation, they are reminded to check the requirements for statistical\nmethods, including those that cannot be formally verified, such as the\ndistribution of the unknown true population. Our software tool StatWhy\nautomatically checks whether programmers have properly specified the\nrequirements for the statistical methods, thereby identifying any missing\nrequirements that need to be addressed. This tool is implemented using the Why3\nplatform to verify the correctness of OCaml programs that conduct statistical\nhypothesis testing. We demonstrate how StatWhy can be used to avoid common\nerrors in various popular statistical hypothesis testing programs.\n", "link": "http://arxiv.org/abs/2405.17492v2", "date": "2024-11-01", "relevancy": 1.4587, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4048}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3587}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StatWhy%3A%20Formal%20Verification%20Tool%20for%20Statistical%20Hypothesis%20Testing%0A%20%20Programs&body=Title%3A%20StatWhy%3A%20Formal%20Verification%20Tool%20for%20Statistical%20Hypothesis%20Testing%0A%20%20Programs%0AAuthor%3A%20Yusuke%20Kawamoto%20and%20Kentaro%20Kobayashi%20and%20Kohei%20Suenaga%0AAbstract%3A%20%20%20Statistical%20methods%20have%20been%20widely%20misused%20and%20misinterpreted%20in%20various%0Ascientific%20fields%2C%20raising%20significant%20concerns%20about%20the%20integrity%20of%0Ascientific%20research.%20To%20mitigate%20this%20problem%2C%20we%20propose%20a%20new%20method%20for%0Aformally%20specifying%20and%20automatically%20verifying%20the%20correctness%20of%20statistical%0Aprograms.%20In%20this%20method%2C%20programmers%20are%20required%20to%20annotate%20the%20source%20code%0Aof%20the%20statistical%20programs%20with%20the%20requirements%20for%20these%20methods.%20Through%0Athis%20annotation%2C%20they%20are%20reminded%20to%20check%20the%20requirements%20for%20statistical%0Amethods%2C%20including%20those%20that%20cannot%20be%20formally%20verified%2C%20such%20as%20the%0Adistribution%20of%20the%20unknown%20true%20population.%20Our%20software%20tool%20StatWhy%0Aautomatically%20checks%20whether%20programmers%20have%20properly%20specified%20the%0Arequirements%20for%20the%20statistical%20methods%2C%20thereby%20identifying%20any%20missing%0Arequirements%20that%20need%20to%20be%20addressed.%20This%20tool%20is%20implemented%20using%20the%20Why3%0Aplatform%20to%20verify%20the%20correctness%20of%20OCaml%20programs%20that%20conduct%20statistical%0Ahypothesis%20testing.%20We%20demonstrate%20how%20StatWhy%20can%20be%20used%20to%20avoid%20common%0Aerrors%20in%20various%20popular%20statistical%20hypothesis%20testing%20programs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatWhy%253A%2520Formal%2520Verification%2520Tool%2520for%2520Statistical%2520Hypothesis%2520Testing%250A%2520%2520Programs%26entry.906535625%3DYusuke%2520Kawamoto%2520and%2520Kentaro%2520Kobayashi%2520and%2520Kohei%2520Suenaga%26entry.1292438233%3D%2520%2520Statistical%2520methods%2520have%2520been%2520widely%2520misused%2520and%2520misinterpreted%2520in%2520various%250Ascientific%2520fields%252C%2520raising%2520significant%2520concerns%2520about%2520the%2520integrity%2520of%250Ascientific%2520research.%2520To%2520mitigate%2520this%2520problem%252C%2520we%2520propose%2520a%2520new%2520method%2520for%250Aformally%2520specifying%2520and%2520automatically%2520verifying%2520the%2520correctness%2520of%2520statistical%250Aprograms.%2520In%2520this%2520method%252C%2520programmers%2520are%2520required%2520to%2520annotate%2520the%2520source%2520code%250Aof%2520the%2520statistical%2520programs%2520with%2520the%2520requirements%2520for%2520these%2520methods.%2520Through%250Athis%2520annotation%252C%2520they%2520are%2520reminded%2520to%2520check%2520the%2520requirements%2520for%2520statistical%250Amethods%252C%2520including%2520those%2520that%2520cannot%2520be%2520formally%2520verified%252C%2520such%2520as%2520the%250Adistribution%2520of%2520the%2520unknown%2520true%2520population.%2520Our%2520software%2520tool%2520StatWhy%250Aautomatically%2520checks%2520whether%2520programmers%2520have%2520properly%2520specified%2520the%250Arequirements%2520for%2520the%2520statistical%2520methods%252C%2520thereby%2520identifying%2520any%2520missing%250Arequirements%2520that%2520need%2520to%2520be%2520addressed.%2520This%2520tool%2520is%2520implemented%2520using%2520the%2520Why3%250Aplatform%2520to%2520verify%2520the%2520correctness%2520of%2520OCaml%2520programs%2520that%2520conduct%2520statistical%250Ahypothesis%2520testing.%2520We%2520demonstrate%2520how%2520StatWhy%2520can%2520be%2520used%2520to%2520avoid%2520common%250Aerrors%2520in%2520various%2520popular%2520statistical%2520hypothesis%2520testing%2520programs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StatWhy%3A%20Formal%20Verification%20Tool%20for%20Statistical%20Hypothesis%20Testing%0A%20%20Programs&entry.906535625=Yusuke%20Kawamoto%20and%20Kentaro%20Kobayashi%20and%20Kohei%20Suenaga&entry.1292438233=%20%20Statistical%20methods%20have%20been%20widely%20misused%20and%20misinterpreted%20in%20various%0Ascientific%20fields%2C%20raising%20significant%20concerns%20about%20the%20integrity%20of%0Ascientific%20research.%20To%20mitigate%20this%20problem%2C%20we%20propose%20a%20new%20method%20for%0Aformally%20specifying%20and%20automatically%20verifying%20the%20correctness%20of%20statistical%0Aprograms.%20In%20this%20method%2C%20programmers%20are%20required%20to%20annotate%20the%20source%20code%0Aof%20the%20statistical%20programs%20with%20the%20requirements%20for%20these%20methods.%20Through%0Athis%20annotation%2C%20they%20are%20reminded%20to%20check%20the%20requirements%20for%20statistical%0Amethods%2C%20including%20those%20that%20cannot%20be%20formally%20verified%2C%20such%20as%20the%0Adistribution%20of%20the%20unknown%20true%20population.%20Our%20software%20tool%20StatWhy%0Aautomatically%20checks%20whether%20programmers%20have%20properly%20specified%20the%0Arequirements%20for%20the%20statistical%20methods%2C%20thereby%20identifying%20any%20missing%0Arequirements%20that%20need%20to%20be%20addressed.%20This%20tool%20is%20implemented%20using%20the%20Why3%0Aplatform%20to%20verify%20the%20correctness%20of%20OCaml%20programs%20that%20conduct%20statistical%0Ahypothesis%20testing.%20We%20demonstrate%20how%20StatWhy%20can%20be%20used%20to%20avoid%20common%0Aerrors%20in%20various%20popular%20statistical%20hypothesis%20testing%20programs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17492v2&entry.124074799=Read"},
{"title": "Human-in-the-Loop Causal Discovery under Latent Confounding using\n  Ancestral GFlowNets", "author": "Tiago da Silva and Eliezer Silva and Ant\u00f3nio G\u00f3is and Dominik Heider and Samuel Kaski and Diego Mesquita and Ad\u00e8le Ribeiro", "abstract": "  Structure learning is the crux of causal inference. Notably, causal discovery\n(CD) algorithms are brittle when data is scarce, possibly inferring imprecise\ncausal relations that contradict expert knowledge -- especially when\nconsidering latent confounders. To aggravate the issue, most CD methods do not\nprovide uncertainty estimates, making it hard for users to interpret results\nand improve the inference process. Surprisingly, while CD is a human-centered\naffair, no works have focused on building methods that both 1) output\nuncertainty estimates that can be verified by experts and 2) interact with\nthose experts to iteratively refine CD. To solve these issues, we start by\nproposing to sample (causal) ancestral graphs proportionally to a belief\ndistribution based on a score function, such as the Bayesian information\ncriterion (BIC), using generative flow networks. Then, we leverage the\ndiversity in candidate graphs and introduce an optimal experimental design to\niteratively probe the expert about the relations among variables, effectively\nreducing the uncertainty of our belief over ancestral graphs. Finally, we\nupdate our samples to incorporate human feedback via importance sampling.\nImportantly, our method does not require causal sufficiency (i.e., unobserved\nconfounders may exist). Experiments with synthetic observational data show that\nour method can accurately sample from distributions over ancestral graphs and\nthat we can greatly improve inference quality with human aid.\n", "link": "http://arxiv.org/abs/2309.12032v2", "date": "2024-11-01", "relevancy": 1.4167, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4822}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4797}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-in-the-Loop%20Causal%20Discovery%20under%20Latent%20Confounding%20using%0A%20%20Ancestral%20GFlowNets&body=Title%3A%20Human-in-the-Loop%20Causal%20Discovery%20under%20Latent%20Confounding%20using%0A%20%20Ancestral%20GFlowNets%0AAuthor%3A%20Tiago%20da%20Silva%20and%20Eliezer%20Silva%20and%20Ant%C3%B3nio%20G%C3%B3is%20and%20Dominik%20Heider%20and%20Samuel%20Kaski%20and%20Diego%20Mesquita%20and%20Ad%C3%A8le%20Ribeiro%0AAbstract%3A%20%20%20Structure%20learning%20is%20the%20crux%20of%20causal%20inference.%20Notably%2C%20causal%20discovery%0A%28CD%29%20algorithms%20are%20brittle%20when%20data%20is%20scarce%2C%20possibly%20inferring%20imprecise%0Acausal%20relations%20that%20contradict%20expert%20knowledge%20--%20especially%20when%0Aconsidering%20latent%20confounders.%20To%20aggravate%20the%20issue%2C%20most%20CD%20methods%20do%20not%0Aprovide%20uncertainty%20estimates%2C%20making%20it%20hard%20for%20users%20to%20interpret%20results%0Aand%20improve%20the%20inference%20process.%20Surprisingly%2C%20while%20CD%20is%20a%20human-centered%0Aaffair%2C%20no%20works%20have%20focused%20on%20building%20methods%20that%20both%201%29%20output%0Auncertainty%20estimates%20that%20can%20be%20verified%20by%20experts%20and%202%29%20interact%20with%0Athose%20experts%20to%20iteratively%20refine%20CD.%20To%20solve%20these%20issues%2C%20we%20start%20by%0Aproposing%20to%20sample%20%28causal%29%20ancestral%20graphs%20proportionally%20to%20a%20belief%0Adistribution%20based%20on%20a%20score%20function%2C%20such%20as%20the%20Bayesian%20information%0Acriterion%20%28BIC%29%2C%20using%20generative%20flow%20networks.%20Then%2C%20we%20leverage%20the%0Adiversity%20in%20candidate%20graphs%20and%20introduce%20an%20optimal%20experimental%20design%20to%0Aiteratively%20probe%20the%20expert%20about%20the%20relations%20among%20variables%2C%20effectively%0Areducing%20the%20uncertainty%20of%20our%20belief%20over%20ancestral%20graphs.%20Finally%2C%20we%0Aupdate%20our%20samples%20to%20incorporate%20human%20feedback%20via%20importance%20sampling.%0AImportantly%2C%20our%20method%20does%20not%20require%20causal%20sufficiency%20%28i.e.%2C%20unobserved%0Aconfounders%20may%20exist%29.%20Experiments%20with%20synthetic%20observational%20data%20show%20that%0Aour%20method%20can%20accurately%20sample%20from%20distributions%20over%20ancestral%20graphs%20and%0Athat%20we%20can%20greatly%20improve%20inference%20quality%20with%20human%20aid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-in-the-Loop%2520Causal%2520Discovery%2520under%2520Latent%2520Confounding%2520using%250A%2520%2520Ancestral%2520GFlowNets%26entry.906535625%3DTiago%2520da%2520Silva%2520and%2520Eliezer%2520Silva%2520and%2520Ant%25C3%25B3nio%2520G%25C3%25B3is%2520and%2520Dominik%2520Heider%2520and%2520Samuel%2520Kaski%2520and%2520Diego%2520Mesquita%2520and%2520Ad%25C3%25A8le%2520Ribeiro%26entry.1292438233%3D%2520%2520Structure%2520learning%2520is%2520the%2520crux%2520of%2520causal%2520inference.%2520Notably%252C%2520causal%2520discovery%250A%2528CD%2529%2520algorithms%2520are%2520brittle%2520when%2520data%2520is%2520scarce%252C%2520possibly%2520inferring%2520imprecise%250Acausal%2520relations%2520that%2520contradict%2520expert%2520knowledge%2520--%2520especially%2520when%250Aconsidering%2520latent%2520confounders.%2520To%2520aggravate%2520the%2520issue%252C%2520most%2520CD%2520methods%2520do%2520not%250Aprovide%2520uncertainty%2520estimates%252C%2520making%2520it%2520hard%2520for%2520users%2520to%2520interpret%2520results%250Aand%2520improve%2520the%2520inference%2520process.%2520Surprisingly%252C%2520while%2520CD%2520is%2520a%2520human-centered%250Aaffair%252C%2520no%2520works%2520have%2520focused%2520on%2520building%2520methods%2520that%2520both%25201%2529%2520output%250Auncertainty%2520estimates%2520that%2520can%2520be%2520verified%2520by%2520experts%2520and%25202%2529%2520interact%2520with%250Athose%2520experts%2520to%2520iteratively%2520refine%2520CD.%2520To%2520solve%2520these%2520issues%252C%2520we%2520start%2520by%250Aproposing%2520to%2520sample%2520%2528causal%2529%2520ancestral%2520graphs%2520proportionally%2520to%2520a%2520belief%250Adistribution%2520based%2520on%2520a%2520score%2520function%252C%2520such%2520as%2520the%2520Bayesian%2520information%250Acriterion%2520%2528BIC%2529%252C%2520using%2520generative%2520flow%2520networks.%2520Then%252C%2520we%2520leverage%2520the%250Adiversity%2520in%2520candidate%2520graphs%2520and%2520introduce%2520an%2520optimal%2520experimental%2520design%2520to%250Aiteratively%2520probe%2520the%2520expert%2520about%2520the%2520relations%2520among%2520variables%252C%2520effectively%250Areducing%2520the%2520uncertainty%2520of%2520our%2520belief%2520over%2520ancestral%2520graphs.%2520Finally%252C%2520we%250Aupdate%2520our%2520samples%2520to%2520incorporate%2520human%2520feedback%2520via%2520importance%2520sampling.%250AImportantly%252C%2520our%2520method%2520does%2520not%2520require%2520causal%2520sufficiency%2520%2528i.e.%252C%2520unobserved%250Aconfounders%2520may%2520exist%2529.%2520Experiments%2520with%2520synthetic%2520observational%2520data%2520show%2520that%250Aour%2520method%2520can%2520accurately%2520sample%2520from%2520distributions%2520over%2520ancestral%2520graphs%2520and%250Athat%2520we%2520can%2520greatly%2520improve%2520inference%2520quality%2520with%2520human%2520aid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-in-the-Loop%20Causal%20Discovery%20under%20Latent%20Confounding%20using%0A%20%20Ancestral%20GFlowNets&entry.906535625=Tiago%20da%20Silva%20and%20Eliezer%20Silva%20and%20Ant%C3%B3nio%20G%C3%B3is%20and%20Dominik%20Heider%20and%20Samuel%20Kaski%20and%20Diego%20Mesquita%20and%20Ad%C3%A8le%20Ribeiro&entry.1292438233=%20%20Structure%20learning%20is%20the%20crux%20of%20causal%20inference.%20Notably%2C%20causal%20discovery%0A%28CD%29%20algorithms%20are%20brittle%20when%20data%20is%20scarce%2C%20possibly%20inferring%20imprecise%0Acausal%20relations%20that%20contradict%20expert%20knowledge%20--%20especially%20when%0Aconsidering%20latent%20confounders.%20To%20aggravate%20the%20issue%2C%20most%20CD%20methods%20do%20not%0Aprovide%20uncertainty%20estimates%2C%20making%20it%20hard%20for%20users%20to%20interpret%20results%0Aand%20improve%20the%20inference%20process.%20Surprisingly%2C%20while%20CD%20is%20a%20human-centered%0Aaffair%2C%20no%20works%20have%20focused%20on%20building%20methods%20that%20both%201%29%20output%0Auncertainty%20estimates%20that%20can%20be%20verified%20by%20experts%20and%202%29%20interact%20with%0Athose%20experts%20to%20iteratively%20refine%20CD.%20To%20solve%20these%20issues%2C%20we%20start%20by%0Aproposing%20to%20sample%20%28causal%29%20ancestral%20graphs%20proportionally%20to%20a%20belief%0Adistribution%20based%20on%20a%20score%20function%2C%20such%20as%20the%20Bayesian%20information%0Acriterion%20%28BIC%29%2C%20using%20generative%20flow%20networks.%20Then%2C%20we%20leverage%20the%0Adiversity%20in%20candidate%20graphs%20and%20introduce%20an%20optimal%20experimental%20design%20to%0Aiteratively%20probe%20the%20expert%20about%20the%20relations%20among%20variables%2C%20effectively%0Areducing%20the%20uncertainty%20of%20our%20belief%20over%20ancestral%20graphs.%20Finally%2C%20we%0Aupdate%20our%20samples%20to%20incorporate%20human%20feedback%20via%20importance%20sampling.%0AImportantly%2C%20our%20method%20does%20not%20require%20causal%20sufficiency%20%28i.e.%2C%20unobserved%0Aconfounders%20may%20exist%29.%20Experiments%20with%20synthetic%20observational%20data%20show%20that%0Aour%20method%20can%20accurately%20sample%20from%20distributions%20over%20ancestral%20graphs%20and%0Athat%20we%20can%20greatly%20improve%20inference%20quality%20with%20human%20aid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12032v2&entry.124074799=Read"},
{"title": "Adversarially Robust Decision Transformer", "author": "Xiaohang Tang and Afonso Marques and Parameswaran Kamalaruban and Ilija Bogunovic", "abstract": "  Decision Transformer (DT), as one of the representative Reinforcement\nLearning via Supervised Learning (RvS) methods, has achieved strong performance\nin offline learning tasks by leveraging the powerful Transformer architecture\nfor sequential decision-making. However, in adversarial environments, these\nmethods can be non-robust, since the return is dependent on the strategies of\nboth the decision-maker and adversary. Training a probabilistic model\nconditioned on observed return to predict action can fail to generalize, as the\ntrajectories that achieve a return in the dataset might have done so due to a\nsuboptimal behavior adversary. To address this, we propose a worst-case-aware\nRvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which\nlearns and conditions the policy on in-sample minimax returns-to-go. ARDT\naligns the target return with the worst-case return learned through minimax\nexpectile regression, thereby enhancing robustness against powerful test-time\nadversaries. In experiments conducted on sequential games with full data\ncoverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution\nwith the largest adversarial robustness. In large-scale sequential games and\ncontinuous adversarial RL environments with partial data coverage, ARDT\ndemonstrates significantly superior robustness to powerful test-time\nadversaries and attains higher worst-case returns compared to contemporary DT\nmethods.\n", "link": "http://arxiv.org/abs/2407.18414v2", "date": "2024-11-01", "relevancy": 1.3597, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4778}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.47}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarially%20Robust%20Decision%20Transformer&body=Title%3A%20Adversarially%20Robust%20Decision%20Transformer%0AAuthor%3A%20Xiaohang%20Tang%20and%20Afonso%20Marques%20and%20Parameswaran%20Kamalaruban%20and%20Ilija%20Bogunovic%0AAbstract%3A%20%20%20Decision%20Transformer%20%28DT%29%2C%20as%20one%20of%20the%20representative%20Reinforcement%0ALearning%20via%20Supervised%20Learning%20%28RvS%29%20methods%2C%20has%20achieved%20strong%20performance%0Ain%20offline%20learning%20tasks%20by%20leveraging%20the%20powerful%20Transformer%20architecture%0Afor%20sequential%20decision-making.%20However%2C%20in%20adversarial%20environments%2C%20these%0Amethods%20can%20be%20non-robust%2C%20since%20the%20return%20is%20dependent%20on%20the%20strategies%20of%0Aboth%20the%20decision-maker%20and%20adversary.%20Training%20a%20probabilistic%20model%0Aconditioned%20on%20observed%20return%20to%20predict%20action%20can%20fail%20to%20generalize%2C%20as%20the%0Atrajectories%20that%20achieve%20a%20return%20in%20the%20dataset%20might%20have%20done%20so%20due%20to%20a%0Asuboptimal%20behavior%20adversary.%20To%20address%20this%2C%20we%20propose%20a%20worst-case-aware%0ARvS%20algorithm%2C%20the%20Adversarially%20Robust%20Decision%20Transformer%20%28ARDT%29%2C%20which%0Alearns%20and%20conditions%20the%20policy%20on%20in-sample%20minimax%20returns-to-go.%20ARDT%0Aaligns%20the%20target%20return%20with%20the%20worst-case%20return%20learned%20through%20minimax%0Aexpectile%20regression%2C%20thereby%20enhancing%20robustness%20against%20powerful%20test-time%0Aadversaries.%20In%20experiments%20conducted%20on%20sequential%20games%20with%20full%20data%0Acoverage%2C%20ARDT%20can%20generate%20a%20maximin%20%28Nash%20Equilibrium%29%20strategy%2C%20the%20solution%0Awith%20the%20largest%20adversarial%20robustness.%20In%20large-scale%20sequential%20games%20and%0Acontinuous%20adversarial%20RL%20environments%20with%20partial%20data%20coverage%2C%20ARDT%0Ademonstrates%20significantly%20superior%20robustness%20to%20powerful%20test-time%0Aadversaries%20and%20attains%20higher%20worst-case%20returns%20compared%20to%20contemporary%20DT%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarially%2520Robust%2520Decision%2520Transformer%26entry.906535625%3DXiaohang%2520Tang%2520and%2520Afonso%2520Marques%2520and%2520Parameswaran%2520Kamalaruban%2520and%2520Ilija%2520Bogunovic%26entry.1292438233%3D%2520%2520Decision%2520Transformer%2520%2528DT%2529%252C%2520as%2520one%2520of%2520the%2520representative%2520Reinforcement%250ALearning%2520via%2520Supervised%2520Learning%2520%2528RvS%2529%2520methods%252C%2520has%2520achieved%2520strong%2520performance%250Ain%2520offline%2520learning%2520tasks%2520by%2520leveraging%2520the%2520powerful%2520Transformer%2520architecture%250Afor%2520sequential%2520decision-making.%2520However%252C%2520in%2520adversarial%2520environments%252C%2520these%250Amethods%2520can%2520be%2520non-robust%252C%2520since%2520the%2520return%2520is%2520dependent%2520on%2520the%2520strategies%2520of%250Aboth%2520the%2520decision-maker%2520and%2520adversary.%2520Training%2520a%2520probabilistic%2520model%250Aconditioned%2520on%2520observed%2520return%2520to%2520predict%2520action%2520can%2520fail%2520to%2520generalize%252C%2520as%2520the%250Atrajectories%2520that%2520achieve%2520a%2520return%2520in%2520the%2520dataset%2520might%2520have%2520done%2520so%2520due%2520to%2520a%250Asuboptimal%2520behavior%2520adversary.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520worst-case-aware%250ARvS%2520algorithm%252C%2520the%2520Adversarially%2520Robust%2520Decision%2520Transformer%2520%2528ARDT%2529%252C%2520which%250Alearns%2520and%2520conditions%2520the%2520policy%2520on%2520in-sample%2520minimax%2520returns-to-go.%2520ARDT%250Aaligns%2520the%2520target%2520return%2520with%2520the%2520worst-case%2520return%2520learned%2520through%2520minimax%250Aexpectile%2520regression%252C%2520thereby%2520enhancing%2520robustness%2520against%2520powerful%2520test-time%250Aadversaries.%2520In%2520experiments%2520conducted%2520on%2520sequential%2520games%2520with%2520full%2520data%250Acoverage%252C%2520ARDT%2520can%2520generate%2520a%2520maximin%2520%2528Nash%2520Equilibrium%2529%2520strategy%252C%2520the%2520solution%250Awith%2520the%2520largest%2520adversarial%2520robustness.%2520In%2520large-scale%2520sequential%2520games%2520and%250Acontinuous%2520adversarial%2520RL%2520environments%2520with%2520partial%2520data%2520coverage%252C%2520ARDT%250Ademonstrates%2520significantly%2520superior%2520robustness%2520to%2520powerful%2520test-time%250Aadversaries%2520and%2520attains%2520higher%2520worst-case%2520returns%2520compared%2520to%2520contemporary%2520DT%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarially%20Robust%20Decision%20Transformer&entry.906535625=Xiaohang%20Tang%20and%20Afonso%20Marques%20and%20Parameswaran%20Kamalaruban%20and%20Ilija%20Bogunovic&entry.1292438233=%20%20Decision%20Transformer%20%28DT%29%2C%20as%20one%20of%20the%20representative%20Reinforcement%0ALearning%20via%20Supervised%20Learning%20%28RvS%29%20methods%2C%20has%20achieved%20strong%20performance%0Ain%20offline%20learning%20tasks%20by%20leveraging%20the%20powerful%20Transformer%20architecture%0Afor%20sequential%20decision-making.%20However%2C%20in%20adversarial%20environments%2C%20these%0Amethods%20can%20be%20non-robust%2C%20since%20the%20return%20is%20dependent%20on%20the%20strategies%20of%0Aboth%20the%20decision-maker%20and%20adversary.%20Training%20a%20probabilistic%20model%0Aconditioned%20on%20observed%20return%20to%20predict%20action%20can%20fail%20to%20generalize%2C%20as%20the%0Atrajectories%20that%20achieve%20a%20return%20in%20the%20dataset%20might%20have%20done%20so%20due%20to%20a%0Asuboptimal%20behavior%20adversary.%20To%20address%20this%2C%20we%20propose%20a%20worst-case-aware%0ARvS%20algorithm%2C%20the%20Adversarially%20Robust%20Decision%20Transformer%20%28ARDT%29%2C%20which%0Alearns%20and%20conditions%20the%20policy%20on%20in-sample%20minimax%20returns-to-go.%20ARDT%0Aaligns%20the%20target%20return%20with%20the%20worst-case%20return%20learned%20through%20minimax%0Aexpectile%20regression%2C%20thereby%20enhancing%20robustness%20against%20powerful%20test-time%0Aadversaries.%20In%20experiments%20conducted%20on%20sequential%20games%20with%20full%20data%0Acoverage%2C%20ARDT%20can%20generate%20a%20maximin%20%28Nash%20Equilibrium%29%20strategy%2C%20the%20solution%0Awith%20the%20largest%20adversarial%20robustness.%20In%20large-scale%20sequential%20games%20and%0Acontinuous%20adversarial%20RL%20environments%20with%20partial%20data%20coverage%2C%20ARDT%0Ademonstrates%20significantly%20superior%20robustness%20to%20powerful%20test-time%0Aadversaries%20and%20attains%20higher%20worst-case%20returns%20compared%20to%20contemporary%20DT%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18414v2&entry.124074799=Read"},
{"title": "On the Limitations of Fractal Dimension as a Measure of Generalization", "author": "Charlie B. Tan and In\u00e9s Garc\u00eda-Redondo and Qiquan Wang and Michael M. Bronstein and Anthea Monod", "abstract": "  Bounding and predicting the generalization gap of overparameterized neural\nnetworks remains a central open problem in theoretical machine learning. There\nis a recent and growing body of literature that proposes the framework of\nfractals to model optimization trajectories of neural networks, motivating\ngeneralization bounds and measures based on the fractal dimension of the\ntrajectory. Notably, the persistent homology dimension has been proposed to\ncorrelate with the generalization gap. This paper performs an empirical\nevaluation of these persistent homology-based generalization measures, with an\nin-depth statistical analysis. Our study reveals confounding effects in the\nobserved correlation between generalization and topological measures due to the\nvariation of hyperparameters. We also observe that fractal dimension fails to\npredict generalization of models trained from poor initializations. We lastly\nreveal the intriguing manifestation of model-wise double descent in these\ntopological generalization measures. Our work forms a basis for a deeper\ninvestigation of the causal relationships between fractal geometry, topological\ndata analysis, and neural network optimization.\n", "link": "http://arxiv.org/abs/2406.02234v2", "date": "2024-11-01", "relevancy": 1.3075, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Limitations%20of%20Fractal%20Dimension%20as%20a%20Measure%20of%20Generalization&body=Title%3A%20On%20the%20Limitations%20of%20Fractal%20Dimension%20as%20a%20Measure%20of%20Generalization%0AAuthor%3A%20Charlie%20B.%20Tan%20and%20In%C3%A9s%20Garc%C3%ADa-Redondo%20and%20Qiquan%20Wang%20and%20Michael%20M.%20Bronstein%20and%20Anthea%20Monod%0AAbstract%3A%20%20%20Bounding%20and%20predicting%20the%20generalization%20gap%20of%20overparameterized%20neural%0Anetworks%20remains%20a%20central%20open%20problem%20in%20theoretical%20machine%20learning.%20There%0Ais%20a%20recent%20and%20growing%20body%20of%20literature%20that%20proposes%20the%20framework%20of%0Afractals%20to%20model%20optimization%20trajectories%20of%20neural%20networks%2C%20motivating%0Ageneralization%20bounds%20and%20measures%20based%20on%20the%20fractal%20dimension%20of%20the%0Atrajectory.%20Notably%2C%20the%20persistent%20homology%20dimension%20has%20been%20proposed%20to%0Acorrelate%20with%20the%20generalization%20gap.%20This%20paper%20performs%20an%20empirical%0Aevaluation%20of%20these%20persistent%20homology-based%20generalization%20measures%2C%20with%20an%0Ain-depth%20statistical%20analysis.%20Our%20study%20reveals%20confounding%20effects%20in%20the%0Aobserved%20correlation%20between%20generalization%20and%20topological%20measures%20due%20to%20the%0Avariation%20of%20hyperparameters.%20We%20also%20observe%20that%20fractal%20dimension%20fails%20to%0Apredict%20generalization%20of%20models%20trained%20from%20poor%20initializations.%20We%20lastly%0Areveal%20the%20intriguing%20manifestation%20of%20model-wise%20double%20descent%20in%20these%0Atopological%20generalization%20measures.%20Our%20work%20forms%20a%20basis%20for%20a%20deeper%0Ainvestigation%20of%20the%20causal%20relationships%20between%20fractal%20geometry%2C%20topological%0Adata%20analysis%2C%20and%20neural%20network%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Limitations%2520of%2520Fractal%2520Dimension%2520as%2520a%2520Measure%2520of%2520Generalization%26entry.906535625%3DCharlie%2520B.%2520Tan%2520and%2520In%25C3%25A9s%2520Garc%25C3%25ADa-Redondo%2520and%2520Qiquan%2520Wang%2520and%2520Michael%2520M.%2520Bronstein%2520and%2520Anthea%2520Monod%26entry.1292438233%3D%2520%2520Bounding%2520and%2520predicting%2520the%2520generalization%2520gap%2520of%2520overparameterized%2520neural%250Anetworks%2520remains%2520a%2520central%2520open%2520problem%2520in%2520theoretical%2520machine%2520learning.%2520There%250Ais%2520a%2520recent%2520and%2520growing%2520body%2520of%2520literature%2520that%2520proposes%2520the%2520framework%2520of%250Afractals%2520to%2520model%2520optimization%2520trajectories%2520of%2520neural%2520networks%252C%2520motivating%250Ageneralization%2520bounds%2520and%2520measures%2520based%2520on%2520the%2520fractal%2520dimension%2520of%2520the%250Atrajectory.%2520Notably%252C%2520the%2520persistent%2520homology%2520dimension%2520has%2520been%2520proposed%2520to%250Acorrelate%2520with%2520the%2520generalization%2520gap.%2520This%2520paper%2520performs%2520an%2520empirical%250Aevaluation%2520of%2520these%2520persistent%2520homology-based%2520generalization%2520measures%252C%2520with%2520an%250Ain-depth%2520statistical%2520analysis.%2520Our%2520study%2520reveals%2520confounding%2520effects%2520in%2520the%250Aobserved%2520correlation%2520between%2520generalization%2520and%2520topological%2520measures%2520due%2520to%2520the%250Avariation%2520of%2520hyperparameters.%2520We%2520also%2520observe%2520that%2520fractal%2520dimension%2520fails%2520to%250Apredict%2520generalization%2520of%2520models%2520trained%2520from%2520poor%2520initializations.%2520We%2520lastly%250Areveal%2520the%2520intriguing%2520manifestation%2520of%2520model-wise%2520double%2520descent%2520in%2520these%250Atopological%2520generalization%2520measures.%2520Our%2520work%2520forms%2520a%2520basis%2520for%2520a%2520deeper%250Ainvestigation%2520of%2520the%2520causal%2520relationships%2520between%2520fractal%2520geometry%252C%2520topological%250Adata%2520analysis%252C%2520and%2520neural%2520network%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Limitations%20of%20Fractal%20Dimension%20as%20a%20Measure%20of%20Generalization&entry.906535625=Charlie%20B.%20Tan%20and%20In%C3%A9s%20Garc%C3%ADa-Redondo%20and%20Qiquan%20Wang%20and%20Michael%20M.%20Bronstein%20and%20Anthea%20Monod&entry.1292438233=%20%20Bounding%20and%20predicting%20the%20generalization%20gap%20of%20overparameterized%20neural%0Anetworks%20remains%20a%20central%20open%20problem%20in%20theoretical%20machine%20learning.%20There%0Ais%20a%20recent%20and%20growing%20body%20of%20literature%20that%20proposes%20the%20framework%20of%0Afractals%20to%20model%20optimization%20trajectories%20of%20neural%20networks%2C%20motivating%0Ageneralization%20bounds%20and%20measures%20based%20on%20the%20fractal%20dimension%20of%20the%0Atrajectory.%20Notably%2C%20the%20persistent%20homology%20dimension%20has%20been%20proposed%20to%0Acorrelate%20with%20the%20generalization%20gap.%20This%20paper%20performs%20an%20empirical%0Aevaluation%20of%20these%20persistent%20homology-based%20generalization%20measures%2C%20with%20an%0Ain-depth%20statistical%20analysis.%20Our%20study%20reveals%20confounding%20effects%20in%20the%0Aobserved%20correlation%20between%20generalization%20and%20topological%20measures%20due%20to%20the%0Avariation%20of%20hyperparameters.%20We%20also%20observe%20that%20fractal%20dimension%20fails%20to%0Apredict%20generalization%20of%20models%20trained%20from%20poor%20initializations.%20We%20lastly%0Areveal%20the%20intriguing%20manifestation%20of%20model-wise%20double%20descent%20in%20these%0Atopological%20generalization%20measures.%20Our%20work%20forms%20a%20basis%20for%20a%20deeper%0Ainvestigation%20of%20the%20causal%20relationships%20between%20fractal%20geometry%2C%20topological%0Adata%20analysis%2C%20and%20neural%20network%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02234v2&entry.124074799=Read"},
{"title": "Simplifying Latent Dynamics with Softly State-Invariant World Models", "author": "Tankred Saanum and Peter Dayan and Eric Schulz", "abstract": "  To solve control problems via model-based reasoning or planning, an agent\nneeds to know how its actions affect the state of the world. The actions an\nagent has at its disposal often change the state of the environment in\nsystematic ways. However, existing techniques for world modelling do not\nguarantee that the effect of actions are represented in such systematic ways.\nWe introduce the Parsimonious Latent Space Model (PLSM), a world model that\nregularizes the latent dynamics to make the effect of the agent's actions more\npredictable. Our approach minimizes the mutual information between latent\nstates and the change that an action produces in the agent's latent state, in\nturn minimizing the dependence the state has on the dynamics. This makes the\nworld model softly state-invariant. We combine PLSM with different model\nclasses used for i) future latent state prediction, ii) planning, and iii)\nmodel-free reinforcement learning. We find that our regularization improves\naccuracy, generalization, and performance in downstream tasks, highlighting the\nimportance of systematic treatment of actions in world models.\n", "link": "http://arxiv.org/abs/2401.17835v2", "date": "2024-11-01", "relevancy": 1.0463, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.553}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5166}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20Latent%20Dynamics%20with%20Softly%20State-Invariant%20World%20Models&body=Title%3A%20Simplifying%20Latent%20Dynamics%20with%20Softly%20State-Invariant%20World%20Models%0AAuthor%3A%20Tankred%20Saanum%20and%20Peter%20Dayan%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20To%20solve%20control%20problems%20via%20model-based%20reasoning%20or%20planning%2C%20an%20agent%0Aneeds%20to%20know%20how%20its%20actions%20affect%20the%20state%20of%20the%20world.%20The%20actions%20an%0Aagent%20has%20at%20its%20disposal%20often%20change%20the%20state%20of%20the%20environment%20in%0Asystematic%20ways.%20However%2C%20existing%20techniques%20for%20world%20modelling%20do%20not%0Aguarantee%20that%20the%20effect%20of%20actions%20are%20represented%20in%20such%20systematic%20ways.%0AWe%20introduce%20the%20Parsimonious%20Latent%20Space%20Model%20%28PLSM%29%2C%20a%20world%20model%20that%0Aregularizes%20the%20latent%20dynamics%20to%20make%20the%20effect%20of%20the%20agent%27s%20actions%20more%0Apredictable.%20Our%20approach%20minimizes%20the%20mutual%20information%20between%20latent%0Astates%20and%20the%20change%20that%20an%20action%20produces%20in%20the%20agent%27s%20latent%20state%2C%20in%0Aturn%20minimizing%20the%20dependence%20the%20state%20has%20on%20the%20dynamics.%20This%20makes%20the%0Aworld%20model%20softly%20state-invariant.%20We%20combine%20PLSM%20with%20different%20model%0Aclasses%20used%20for%20i%29%20future%20latent%20state%20prediction%2C%20ii%29%20planning%2C%20and%20iii%29%0Amodel-free%20reinforcement%20learning.%20We%20find%20that%20our%20regularization%20improves%0Aaccuracy%2C%20generalization%2C%20and%20performance%20in%20downstream%20tasks%2C%20highlighting%20the%0Aimportance%20of%20systematic%20treatment%20of%20actions%20in%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520Latent%2520Dynamics%2520with%2520Softly%2520State-Invariant%2520World%2520Models%26entry.906535625%3DTankred%2520Saanum%2520and%2520Peter%2520Dayan%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520To%2520solve%2520control%2520problems%2520via%2520model-based%2520reasoning%2520or%2520planning%252C%2520an%2520agent%250Aneeds%2520to%2520know%2520how%2520its%2520actions%2520affect%2520the%2520state%2520of%2520the%2520world.%2520The%2520actions%2520an%250Aagent%2520has%2520at%2520its%2520disposal%2520often%2520change%2520the%2520state%2520of%2520the%2520environment%2520in%250Asystematic%2520ways.%2520However%252C%2520existing%2520techniques%2520for%2520world%2520modelling%2520do%2520not%250Aguarantee%2520that%2520the%2520effect%2520of%2520actions%2520are%2520represented%2520in%2520such%2520systematic%2520ways.%250AWe%2520introduce%2520the%2520Parsimonious%2520Latent%2520Space%2520Model%2520%2528PLSM%2529%252C%2520a%2520world%2520model%2520that%250Aregularizes%2520the%2520latent%2520dynamics%2520to%2520make%2520the%2520effect%2520of%2520the%2520agent%2527s%2520actions%2520more%250Apredictable.%2520Our%2520approach%2520minimizes%2520the%2520mutual%2520information%2520between%2520latent%250Astates%2520and%2520the%2520change%2520that%2520an%2520action%2520produces%2520in%2520the%2520agent%2527s%2520latent%2520state%252C%2520in%250Aturn%2520minimizing%2520the%2520dependence%2520the%2520state%2520has%2520on%2520the%2520dynamics.%2520This%2520makes%2520the%250Aworld%2520model%2520softly%2520state-invariant.%2520We%2520combine%2520PLSM%2520with%2520different%2520model%250Aclasses%2520used%2520for%2520i%2529%2520future%2520latent%2520state%2520prediction%252C%2520ii%2529%2520planning%252C%2520and%2520iii%2529%250Amodel-free%2520reinforcement%2520learning.%2520We%2520find%2520that%2520our%2520regularization%2520improves%250Aaccuracy%252C%2520generalization%252C%2520and%2520performance%2520in%2520downstream%2520tasks%252C%2520highlighting%2520the%250Aimportance%2520of%2520systematic%2520treatment%2520of%2520actions%2520in%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20Latent%20Dynamics%20with%20Softly%20State-Invariant%20World%20Models&entry.906535625=Tankred%20Saanum%20and%20Peter%20Dayan%20and%20Eric%20Schulz&entry.1292438233=%20%20To%20solve%20control%20problems%20via%20model-based%20reasoning%20or%20planning%2C%20an%20agent%0Aneeds%20to%20know%20how%20its%20actions%20affect%20the%20state%20of%20the%20world.%20The%20actions%20an%0Aagent%20has%20at%20its%20disposal%20often%20change%20the%20state%20of%20the%20environment%20in%0Asystematic%20ways.%20However%2C%20existing%20techniques%20for%20world%20modelling%20do%20not%0Aguarantee%20that%20the%20effect%20of%20actions%20are%20represented%20in%20such%20systematic%20ways.%0AWe%20introduce%20the%20Parsimonious%20Latent%20Space%20Model%20%28PLSM%29%2C%20a%20world%20model%20that%0Aregularizes%20the%20latent%20dynamics%20to%20make%20the%20effect%20of%20the%20agent%27s%20actions%20more%0Apredictable.%20Our%20approach%20minimizes%20the%20mutual%20information%20between%20latent%0Astates%20and%20the%20change%20that%20an%20action%20produces%20in%20the%20agent%27s%20latent%20state%2C%20in%0Aturn%20minimizing%20the%20dependence%20the%20state%20has%20on%20the%20dynamics.%20This%20makes%20the%0Aworld%20model%20softly%20state-invariant.%20We%20combine%20PLSM%20with%20different%20model%0Aclasses%20used%20for%20i%29%20future%20latent%20state%20prediction%2C%20ii%29%20planning%2C%20and%20iii%29%0Amodel-free%20reinforcement%20learning.%20We%20find%20that%20our%20regularization%20improves%0Aaccuracy%2C%20generalization%2C%20and%20performance%20in%20downstream%20tasks%2C%20highlighting%20the%0Aimportance%20of%20systematic%20treatment%20of%20actions%20in%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17835v2&entry.124074799=Read"},
{"title": "Reinforced In-Context Black-Box Optimization", "author": "Lei Song and Chenxiao Gao and Ke Xue and Chenyang Wu and Dong Li and Jianye Hao and Zongzhang Zhang and Chao Qian", "abstract": "  Black-Box Optimization (BBO) has found successful applications in many fields\nof science and engineering. Recently, there has been a growing interest in\nmeta-learning particular components of BBO algorithms to speed up optimization\nand get rid of tedious hand-crafted heuristics. As an extension, learning the\nentire algorithm from data requires the least labor from experts and can\nprovide the most flexibility. In this paper, we propose RIBBO, a method to\nreinforce-learn a BBO algorithm from offline data in an end-to-end fashion.\nRIBBO employs expressive sequence models to learn the optimization histories\nproduced by multiple behavior algorithms and tasks, leveraging the in-context\nlearning ability of large models to extract task information and make decisions\naccordingly. Central to our method is to augment the optimization histories\nwith \\textit{regret-to-go} tokens, which are designed to represent the\nperformance of an algorithm based on cumulative regret over the future part of\nthe histories. The integration of regret-to-go tokens enables RIBBO to\nautomatically generate sequences of query points that satisfy the user-desired\nregret, which is verified by its universally good empirical performance on\ndiverse problems, including BBO benchmark functions, hyper-parameter\noptimization and robot control problems.\n", "link": "http://arxiv.org/abs/2402.17423v3", "date": "2024-11-01", "relevancy": 0.9761, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4824}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20In-Context%20Black-Box%20Optimization&body=Title%3A%20Reinforced%20In-Context%20Black-Box%20Optimization%0AAuthor%3A%20Lei%20Song%20and%20Chenxiao%20Gao%20and%20Ke%20Xue%20and%20Chenyang%20Wu%20and%20Dong%20Li%20and%20Jianye%20Hao%20and%20Zongzhang%20Zhang%20and%20Chao%20Qian%0AAbstract%3A%20%20%20Black-Box%20Optimization%20%28BBO%29%20has%20found%20successful%20applications%20in%20many%20fields%0Aof%20science%20and%20engineering.%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%0Ameta-learning%20particular%20components%20of%20BBO%20algorithms%20to%20speed%20up%20optimization%0Aand%20get%20rid%20of%20tedious%20hand-crafted%20heuristics.%20As%20an%20extension%2C%20learning%20the%0Aentire%20algorithm%20from%20data%20requires%20the%20least%20labor%20from%20experts%20and%20can%0Aprovide%20the%20most%20flexibility.%20In%20this%20paper%2C%20we%20propose%20RIBBO%2C%20a%20method%20to%0Areinforce-learn%20a%20BBO%20algorithm%20from%20offline%20data%20in%20an%20end-to-end%20fashion.%0ARIBBO%20employs%20expressive%20sequence%20models%20to%20learn%20the%20optimization%20histories%0Aproduced%20by%20multiple%20behavior%20algorithms%20and%20tasks%2C%20leveraging%20the%20in-context%0Alearning%20ability%20of%20large%20models%20to%20extract%20task%20information%20and%20make%20decisions%0Aaccordingly.%20Central%20to%20our%20method%20is%20to%20augment%20the%20optimization%20histories%0Awith%20%5Ctextit%7Bregret-to-go%7D%20tokens%2C%20which%20are%20designed%20to%20represent%20the%0Aperformance%20of%20an%20algorithm%20based%20on%20cumulative%20regret%20over%20the%20future%20part%20of%0Athe%20histories.%20The%20integration%20of%20regret-to-go%20tokens%20enables%20RIBBO%20to%0Aautomatically%20generate%20sequences%20of%20query%20points%20that%20satisfy%20the%20user-desired%0Aregret%2C%20which%20is%20verified%20by%20its%20universally%20good%20empirical%20performance%20on%0Adiverse%20problems%2C%20including%20BBO%20benchmark%20functions%2C%20hyper-parameter%0Aoptimization%20and%20robot%20control%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17423v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520In-Context%2520Black-Box%2520Optimization%26entry.906535625%3DLei%2520Song%2520and%2520Chenxiao%2520Gao%2520and%2520Ke%2520Xue%2520and%2520Chenyang%2520Wu%2520and%2520Dong%2520Li%2520and%2520Jianye%2520Hao%2520and%2520Zongzhang%2520Zhang%2520and%2520Chao%2520Qian%26entry.1292438233%3D%2520%2520Black-Box%2520Optimization%2520%2528BBO%2529%2520has%2520found%2520successful%2520applications%2520in%2520many%2520fields%250Aof%2520science%2520and%2520engineering.%2520Recently%252C%2520there%2520has%2520been%2520a%2520growing%2520interest%2520in%250Ameta-learning%2520particular%2520components%2520of%2520BBO%2520algorithms%2520to%2520speed%2520up%2520optimization%250Aand%2520get%2520rid%2520of%2520tedious%2520hand-crafted%2520heuristics.%2520As%2520an%2520extension%252C%2520learning%2520the%250Aentire%2520algorithm%2520from%2520data%2520requires%2520the%2520least%2520labor%2520from%2520experts%2520and%2520can%250Aprovide%2520the%2520most%2520flexibility.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RIBBO%252C%2520a%2520method%2520to%250Areinforce-learn%2520a%2520BBO%2520algorithm%2520from%2520offline%2520data%2520in%2520an%2520end-to-end%2520fashion.%250ARIBBO%2520employs%2520expressive%2520sequence%2520models%2520to%2520learn%2520the%2520optimization%2520histories%250Aproduced%2520by%2520multiple%2520behavior%2520algorithms%2520and%2520tasks%252C%2520leveraging%2520the%2520in-context%250Alearning%2520ability%2520of%2520large%2520models%2520to%2520extract%2520task%2520information%2520and%2520make%2520decisions%250Aaccordingly.%2520Central%2520to%2520our%2520method%2520is%2520to%2520augment%2520the%2520optimization%2520histories%250Awith%2520%255Ctextit%257Bregret-to-go%257D%2520tokens%252C%2520which%2520are%2520designed%2520to%2520represent%2520the%250Aperformance%2520of%2520an%2520algorithm%2520based%2520on%2520cumulative%2520regret%2520over%2520the%2520future%2520part%2520of%250Athe%2520histories.%2520The%2520integration%2520of%2520regret-to-go%2520tokens%2520enables%2520RIBBO%2520to%250Aautomatically%2520generate%2520sequences%2520of%2520query%2520points%2520that%2520satisfy%2520the%2520user-desired%250Aregret%252C%2520which%2520is%2520verified%2520by%2520its%2520universally%2520good%2520empirical%2520performance%2520on%250Adiverse%2520problems%252C%2520including%2520BBO%2520benchmark%2520functions%252C%2520hyper-parameter%250Aoptimization%2520and%2520robot%2520control%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17423v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20In-Context%20Black-Box%20Optimization&entry.906535625=Lei%20Song%20and%20Chenxiao%20Gao%20and%20Ke%20Xue%20and%20Chenyang%20Wu%20and%20Dong%20Li%20and%20Jianye%20Hao%20and%20Zongzhang%20Zhang%20and%20Chao%20Qian&entry.1292438233=%20%20Black-Box%20Optimization%20%28BBO%29%20has%20found%20successful%20applications%20in%20many%20fields%0Aof%20science%20and%20engineering.%20Recently%2C%20there%20has%20been%20a%20growing%20interest%20in%0Ameta-learning%20particular%20components%20of%20BBO%20algorithms%20to%20speed%20up%20optimization%0Aand%20get%20rid%20of%20tedious%20hand-crafted%20heuristics.%20As%20an%20extension%2C%20learning%20the%0Aentire%20algorithm%20from%20data%20requires%20the%20least%20labor%20from%20experts%20and%20can%0Aprovide%20the%20most%20flexibility.%20In%20this%20paper%2C%20we%20propose%20RIBBO%2C%20a%20method%20to%0Areinforce-learn%20a%20BBO%20algorithm%20from%20offline%20data%20in%20an%20end-to-end%20fashion.%0ARIBBO%20employs%20expressive%20sequence%20models%20to%20learn%20the%20optimization%20histories%0Aproduced%20by%20multiple%20behavior%20algorithms%20and%20tasks%2C%20leveraging%20the%20in-context%0Alearning%20ability%20of%20large%20models%20to%20extract%20task%20information%20and%20make%20decisions%0Aaccordingly.%20Central%20to%20our%20method%20is%20to%20augment%20the%20optimization%20histories%0Awith%20%5Ctextit%7Bregret-to-go%7D%20tokens%2C%20which%20are%20designed%20to%20represent%20the%0Aperformance%20of%20an%20algorithm%20based%20on%20cumulative%20regret%20over%20the%20future%20part%20of%0Athe%20histories.%20The%20integration%20of%20regret-to-go%20tokens%20enables%20RIBBO%20to%0Aautomatically%20generate%20sequences%20of%20query%20points%20that%20satisfy%20the%20user-desired%0Aregret%2C%20which%20is%20verified%20by%20its%20universally%20good%20empirical%20performance%20on%0Adiverse%20problems%2C%20including%20BBO%20benchmark%20functions%2C%20hyper-parameter%0Aoptimization%20and%20robot%20control%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17423v3&entry.124074799=Read"},
{"title": "Designing User-Centric Behavioral Interventions to Prevent Dysglycemia\n  with Novel Counterfactual Explanations", "author": "Asiful Arefeen and Hassan Ghasemzadeh", "abstract": "  Monitoring unexpected health events and taking actionable measures to avert\nthem beforehand is central to maintaining health and preventing disease.\nTherefore, a tool capable of predicting adverse health events and offering\nusers actionable feedback about how to make changes in their diet, exercise,\nand medication to prevent abnormal health events could have significant\nsocietal impacts. Counterfactual explanations can provide insights into why a\nmodel made a particular prediction by generating hypothetical instances that\nare similar to the original input but lead to a different prediction outcome.\nTherefore, counterfactuals can be viewed as a means to design AI-driven health\ninterventions to not only predict but also prevent adverse health outcomes such\nas blood glucose spikes, diabetes, and heart disease. In this paper, we design\n\\textit{\\textbf{ExAct}}, a novel model-agnostic framework for generating\ncounterfactual explanations for chronic disease prevention and management.\nLeveraging insights from adversarial learning, ExAct characterizes the decision\nboundary for high-dimensional data and performs a grid search to generate\nactionable interventions. ExAct is unique in integrating prior knowledge about\nuser preferences of feasible explanations into the process of counterfactual\ngeneration. ExAct is evaluated extensively using four real-world datasets and\nexternal simulators. With $82.8\\%$ average validity in the simulation-aided\nvalidation, ExAct surpasses the state-of-the-art techniques for generating\ncounterfactual explanations by at least $10\\%$. Besides, counterfactuals from\nExAct exhibit at least $6.6\\%$ improved proximity compared to previous\nresearch.\n", "link": "http://arxiv.org/abs/2310.01684v2", "date": "2024-11-01", "relevancy": 0.9256, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4632}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4628}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20User-Centric%20Behavioral%20Interventions%20to%20Prevent%20Dysglycemia%0A%20%20with%20Novel%20Counterfactual%20Explanations&body=Title%3A%20Designing%20User-Centric%20Behavioral%20Interventions%20to%20Prevent%20Dysglycemia%0A%20%20with%20Novel%20Counterfactual%20Explanations%0AAuthor%3A%20Asiful%20Arefeen%20and%20Hassan%20Ghasemzadeh%0AAbstract%3A%20%20%20Monitoring%20unexpected%20health%20events%20and%20taking%20actionable%20measures%20to%20avert%0Athem%20beforehand%20is%20central%20to%20maintaining%20health%20and%20preventing%20disease.%0ATherefore%2C%20a%20tool%20capable%20of%20predicting%20adverse%20health%20events%20and%20offering%0Ausers%20actionable%20feedback%20about%20how%20to%20make%20changes%20in%20their%20diet%2C%20exercise%2C%0Aand%20medication%20to%20prevent%20abnormal%20health%20events%20could%20have%20significant%0Asocietal%20impacts.%20Counterfactual%20explanations%20can%20provide%20insights%20into%20why%20a%0Amodel%20made%20a%20particular%20prediction%20by%20generating%20hypothetical%20instances%20that%0Aare%20similar%20to%20the%20original%20input%20but%20lead%20to%20a%20different%20prediction%20outcome.%0ATherefore%2C%20counterfactuals%20can%20be%20viewed%20as%20a%20means%20to%20design%20AI-driven%20health%0Ainterventions%20to%20not%20only%20predict%20but%20also%20prevent%20adverse%20health%20outcomes%20such%0Aas%20blood%20glucose%20spikes%2C%20diabetes%2C%20and%20heart%20disease.%20In%20this%20paper%2C%20we%20design%0A%5Ctextit%7B%5Ctextbf%7BExAct%7D%7D%2C%20a%20novel%20model-agnostic%20framework%20for%20generating%0Acounterfactual%20explanations%20for%20chronic%20disease%20prevention%20and%20management.%0ALeveraging%20insights%20from%20adversarial%20learning%2C%20ExAct%20characterizes%20the%20decision%0Aboundary%20for%20high-dimensional%20data%20and%20performs%20a%20grid%20search%20to%20generate%0Aactionable%20interventions.%20ExAct%20is%20unique%20in%20integrating%20prior%20knowledge%20about%0Auser%20preferences%20of%20feasible%20explanations%20into%20the%20process%20of%20counterfactual%0Ageneration.%20ExAct%20is%20evaluated%20extensively%20using%20four%20real-world%20datasets%20and%0Aexternal%20simulators.%20With%20%2482.8%5C%25%24%20average%20validity%20in%20the%20simulation-aided%0Avalidation%2C%20ExAct%20surpasses%20the%20state-of-the-art%20techniques%20for%20generating%0Acounterfactual%20explanations%20by%20at%20least%20%2410%5C%25%24.%20Besides%2C%20counterfactuals%20from%0AExAct%20exhibit%20at%20least%20%246.6%5C%25%24%20improved%20proximity%20compared%20to%20previous%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520User-Centric%2520Behavioral%2520Interventions%2520to%2520Prevent%2520Dysglycemia%250A%2520%2520with%2520Novel%2520Counterfactual%2520Explanations%26entry.906535625%3DAsiful%2520Arefeen%2520and%2520Hassan%2520Ghasemzadeh%26entry.1292438233%3D%2520%2520Monitoring%2520unexpected%2520health%2520events%2520and%2520taking%2520actionable%2520measures%2520to%2520avert%250Athem%2520beforehand%2520is%2520central%2520to%2520maintaining%2520health%2520and%2520preventing%2520disease.%250ATherefore%252C%2520a%2520tool%2520capable%2520of%2520predicting%2520adverse%2520health%2520events%2520and%2520offering%250Ausers%2520actionable%2520feedback%2520about%2520how%2520to%2520make%2520changes%2520in%2520their%2520diet%252C%2520exercise%252C%250Aand%2520medication%2520to%2520prevent%2520abnormal%2520health%2520events%2520could%2520have%2520significant%250Asocietal%2520impacts.%2520Counterfactual%2520explanations%2520can%2520provide%2520insights%2520into%2520why%2520a%250Amodel%2520made%2520a%2520particular%2520prediction%2520by%2520generating%2520hypothetical%2520instances%2520that%250Aare%2520similar%2520to%2520the%2520original%2520input%2520but%2520lead%2520to%2520a%2520different%2520prediction%2520outcome.%250ATherefore%252C%2520counterfactuals%2520can%2520be%2520viewed%2520as%2520a%2520means%2520to%2520design%2520AI-driven%2520health%250Ainterventions%2520to%2520not%2520only%2520predict%2520but%2520also%2520prevent%2520adverse%2520health%2520outcomes%2520such%250Aas%2520blood%2520glucose%2520spikes%252C%2520diabetes%252C%2520and%2520heart%2520disease.%2520In%2520this%2520paper%252C%2520we%2520design%250A%255Ctextit%257B%255Ctextbf%257BExAct%257D%257D%252C%2520a%2520novel%2520model-agnostic%2520framework%2520for%2520generating%250Acounterfactual%2520explanations%2520for%2520chronic%2520disease%2520prevention%2520and%2520management.%250ALeveraging%2520insights%2520from%2520adversarial%2520learning%252C%2520ExAct%2520characterizes%2520the%2520decision%250Aboundary%2520for%2520high-dimensional%2520data%2520and%2520performs%2520a%2520grid%2520search%2520to%2520generate%250Aactionable%2520interventions.%2520ExAct%2520is%2520unique%2520in%2520integrating%2520prior%2520knowledge%2520about%250Auser%2520preferences%2520of%2520feasible%2520explanations%2520into%2520the%2520process%2520of%2520counterfactual%250Ageneration.%2520ExAct%2520is%2520evaluated%2520extensively%2520using%2520four%2520real-world%2520datasets%2520and%250Aexternal%2520simulators.%2520With%2520%252482.8%255C%2525%2524%2520average%2520validity%2520in%2520the%2520simulation-aided%250Avalidation%252C%2520ExAct%2520surpasses%2520the%2520state-of-the-art%2520techniques%2520for%2520generating%250Acounterfactual%2520explanations%2520by%2520at%2520least%2520%252410%255C%2525%2524.%2520Besides%252C%2520counterfactuals%2520from%250AExAct%2520exhibit%2520at%2520least%2520%25246.6%255C%2525%2524%2520improved%2520proximity%2520compared%2520to%2520previous%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20User-Centric%20Behavioral%20Interventions%20to%20Prevent%20Dysglycemia%0A%20%20with%20Novel%20Counterfactual%20Explanations&entry.906535625=Asiful%20Arefeen%20and%20Hassan%20Ghasemzadeh&entry.1292438233=%20%20Monitoring%20unexpected%20health%20events%20and%20taking%20actionable%20measures%20to%20avert%0Athem%20beforehand%20is%20central%20to%20maintaining%20health%20and%20preventing%20disease.%0ATherefore%2C%20a%20tool%20capable%20of%20predicting%20adverse%20health%20events%20and%20offering%0Ausers%20actionable%20feedback%20about%20how%20to%20make%20changes%20in%20their%20diet%2C%20exercise%2C%0Aand%20medication%20to%20prevent%20abnormal%20health%20events%20could%20have%20significant%0Asocietal%20impacts.%20Counterfactual%20explanations%20can%20provide%20insights%20into%20why%20a%0Amodel%20made%20a%20particular%20prediction%20by%20generating%20hypothetical%20instances%20that%0Aare%20similar%20to%20the%20original%20input%20but%20lead%20to%20a%20different%20prediction%20outcome.%0ATherefore%2C%20counterfactuals%20can%20be%20viewed%20as%20a%20means%20to%20design%20AI-driven%20health%0Ainterventions%20to%20not%20only%20predict%20but%20also%20prevent%20adverse%20health%20outcomes%20such%0Aas%20blood%20glucose%20spikes%2C%20diabetes%2C%20and%20heart%20disease.%20In%20this%20paper%2C%20we%20design%0A%5Ctextit%7B%5Ctextbf%7BExAct%7D%7D%2C%20a%20novel%20model-agnostic%20framework%20for%20generating%0Acounterfactual%20explanations%20for%20chronic%20disease%20prevention%20and%20management.%0ALeveraging%20insights%20from%20adversarial%20learning%2C%20ExAct%20characterizes%20the%20decision%0Aboundary%20for%20high-dimensional%20data%20and%20performs%20a%20grid%20search%20to%20generate%0Aactionable%20interventions.%20ExAct%20is%20unique%20in%20integrating%20prior%20knowledge%20about%0Auser%20preferences%20of%20feasible%20explanations%20into%20the%20process%20of%20counterfactual%0Ageneration.%20ExAct%20is%20evaluated%20extensively%20using%20four%20real-world%20datasets%20and%0Aexternal%20simulators.%20With%20%2482.8%5C%25%24%20average%20validity%20in%20the%20simulation-aided%0Avalidation%2C%20ExAct%20surpasses%20the%20state-of-the-art%20techniques%20for%20generating%0Acounterfactual%20explanations%20by%20at%20least%20%2410%5C%25%24.%20Besides%2C%20counterfactuals%20from%0AExAct%20exhibit%20at%20least%20%246.6%5C%25%24%20improved%20proximity%20compared%20to%20previous%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01684v2&entry.124074799=Read"},
{"title": "Double-Ended Synthesis Planning with Goal-Constrained Bidirectional\n  Search", "author": "Kevin Yu and Jihye Roh and Ziang Li and Wenhao Gao and Runzhong Wang and Connor W. Coley", "abstract": "  Computer-aided synthesis planning (CASP) algorithms have demonstrated\nexpert-level abilities in planning retrosynthetic routes to molecules of low to\nmoderate complexity. However, current search methods assume the sufficiency of\nreaching arbitrary building blocks, failing to address the common real-world\nconstraint where using specific molecules is desired. To this end, we present a\nformulation of synthesis planning with starting material constraints. Under\nthis formulation, we propose Double-Ended Synthesis Planning (DESP), a novel\nCASP algorithm under a bidirectional graph search scheme that interleaves\nexpansions from the target and from the goal starting materials to ensure\nconstraint satisfiability. The search algorithm is guided by a goal-conditioned\ncost network learned offline from a partially observed hypergraph of valid\nchemical reactions. We demonstrate the utility of DESP in improving solve rates\nand reducing the number of search expansions by biasing synthesis planning\ntowards expert goals on multiple new benchmarks. DESP can make use of existing\none-step retrosynthesis models, and we anticipate its performance to scale as\nthese one-step model capabilities improve.\n", "link": "http://arxiv.org/abs/2407.06334v2", "date": "2024-11-01", "relevancy": 0.8889, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4885}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Double-Ended%20Synthesis%20Planning%20with%20Goal-Constrained%20Bidirectional%0A%20%20Search&body=Title%3A%20Double-Ended%20Synthesis%20Planning%20with%20Goal-Constrained%20Bidirectional%0A%20%20Search%0AAuthor%3A%20Kevin%20Yu%20and%20Jihye%20Roh%20and%20Ziang%20Li%20and%20Wenhao%20Gao%20and%20Runzhong%20Wang%20and%20Connor%20W.%20Coley%0AAbstract%3A%20%20%20Computer-aided%20synthesis%20planning%20%28CASP%29%20algorithms%20have%20demonstrated%0Aexpert-level%20abilities%20in%20planning%20retrosynthetic%20routes%20to%20molecules%20of%20low%20to%0Amoderate%20complexity.%20However%2C%20current%20search%20methods%20assume%20the%20sufficiency%20of%0Areaching%20arbitrary%20building%20blocks%2C%20failing%20to%20address%20the%20common%20real-world%0Aconstraint%20where%20using%20specific%20molecules%20is%20desired.%20To%20this%20end%2C%20we%20present%20a%0Aformulation%20of%20synthesis%20planning%20with%20starting%20material%20constraints.%20Under%0Athis%20formulation%2C%20we%20propose%20Double-Ended%20Synthesis%20Planning%20%28DESP%29%2C%20a%20novel%0ACASP%20algorithm%20under%20a%20bidirectional%20graph%20search%20scheme%20that%20interleaves%0Aexpansions%20from%20the%20target%20and%20from%20the%20goal%20starting%20materials%20to%20ensure%0Aconstraint%20satisfiability.%20The%20search%20algorithm%20is%20guided%20by%20a%20goal-conditioned%0Acost%20network%20learned%20offline%20from%20a%20partially%20observed%20hypergraph%20of%20valid%0Achemical%20reactions.%20We%20demonstrate%20the%20utility%20of%20DESP%20in%20improving%20solve%20rates%0Aand%20reducing%20the%20number%20of%20search%20expansions%20by%20biasing%20synthesis%20planning%0Atowards%20expert%20goals%20on%20multiple%20new%20benchmarks.%20DESP%20can%20make%20use%20of%20existing%0Aone-step%20retrosynthesis%20models%2C%20and%20we%20anticipate%20its%20performance%20to%20scale%20as%0Athese%20one-step%20model%20capabilities%20improve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDouble-Ended%2520Synthesis%2520Planning%2520with%2520Goal-Constrained%2520Bidirectional%250A%2520%2520Search%26entry.906535625%3DKevin%2520Yu%2520and%2520Jihye%2520Roh%2520and%2520Ziang%2520Li%2520and%2520Wenhao%2520Gao%2520and%2520Runzhong%2520Wang%2520and%2520Connor%2520W.%2520Coley%26entry.1292438233%3D%2520%2520Computer-aided%2520synthesis%2520planning%2520%2528CASP%2529%2520algorithms%2520have%2520demonstrated%250Aexpert-level%2520abilities%2520in%2520planning%2520retrosynthetic%2520routes%2520to%2520molecules%2520of%2520low%2520to%250Amoderate%2520complexity.%2520However%252C%2520current%2520search%2520methods%2520assume%2520the%2520sufficiency%2520of%250Areaching%2520arbitrary%2520building%2520blocks%252C%2520failing%2520to%2520address%2520the%2520common%2520real-world%250Aconstraint%2520where%2520using%2520specific%2520molecules%2520is%2520desired.%2520To%2520this%2520end%252C%2520we%2520present%2520a%250Aformulation%2520of%2520synthesis%2520planning%2520with%2520starting%2520material%2520constraints.%2520Under%250Athis%2520formulation%252C%2520we%2520propose%2520Double-Ended%2520Synthesis%2520Planning%2520%2528DESP%2529%252C%2520a%2520novel%250ACASP%2520algorithm%2520under%2520a%2520bidirectional%2520graph%2520search%2520scheme%2520that%2520interleaves%250Aexpansions%2520from%2520the%2520target%2520and%2520from%2520the%2520goal%2520starting%2520materials%2520to%2520ensure%250Aconstraint%2520satisfiability.%2520The%2520search%2520algorithm%2520is%2520guided%2520by%2520a%2520goal-conditioned%250Acost%2520network%2520learned%2520offline%2520from%2520a%2520partially%2520observed%2520hypergraph%2520of%2520valid%250Achemical%2520reactions.%2520We%2520demonstrate%2520the%2520utility%2520of%2520DESP%2520in%2520improving%2520solve%2520rates%250Aand%2520reducing%2520the%2520number%2520of%2520search%2520expansions%2520by%2520biasing%2520synthesis%2520planning%250Atowards%2520expert%2520goals%2520on%2520multiple%2520new%2520benchmarks.%2520DESP%2520can%2520make%2520use%2520of%2520existing%250Aone-step%2520retrosynthesis%2520models%252C%2520and%2520we%2520anticipate%2520its%2520performance%2520to%2520scale%2520as%250Athese%2520one-step%2520model%2520capabilities%2520improve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Double-Ended%20Synthesis%20Planning%20with%20Goal-Constrained%20Bidirectional%0A%20%20Search&entry.906535625=Kevin%20Yu%20and%20Jihye%20Roh%20and%20Ziang%20Li%20and%20Wenhao%20Gao%20and%20Runzhong%20Wang%20and%20Connor%20W.%20Coley&entry.1292438233=%20%20Computer-aided%20synthesis%20planning%20%28CASP%29%20algorithms%20have%20demonstrated%0Aexpert-level%20abilities%20in%20planning%20retrosynthetic%20routes%20to%20molecules%20of%20low%20to%0Amoderate%20complexity.%20However%2C%20current%20search%20methods%20assume%20the%20sufficiency%20of%0Areaching%20arbitrary%20building%20blocks%2C%20failing%20to%20address%20the%20common%20real-world%0Aconstraint%20where%20using%20specific%20molecules%20is%20desired.%20To%20this%20end%2C%20we%20present%20a%0Aformulation%20of%20synthesis%20planning%20with%20starting%20material%20constraints.%20Under%0Athis%20formulation%2C%20we%20propose%20Double-Ended%20Synthesis%20Planning%20%28DESP%29%2C%20a%20novel%0ACASP%20algorithm%20under%20a%20bidirectional%20graph%20search%20scheme%20that%20interleaves%0Aexpansions%20from%20the%20target%20and%20from%20the%20goal%20starting%20materials%20to%20ensure%0Aconstraint%20satisfiability.%20The%20search%20algorithm%20is%20guided%20by%20a%20goal-conditioned%0Acost%20network%20learned%20offline%20from%20a%20partially%20observed%20hypergraph%20of%20valid%0Achemical%20reactions.%20We%20demonstrate%20the%20utility%20of%20DESP%20in%20improving%20solve%20rates%0Aand%20reducing%20the%20number%20of%20search%20expansions%20by%20biasing%20synthesis%20planning%0Atowards%20expert%20goals%20on%20multiple%20new%20benchmarks.%20DESP%20can%20make%20use%20of%20existing%0Aone-step%20retrosynthesis%20models%2C%20and%20we%20anticipate%20its%20performance%20to%20scale%20as%0Athese%20one-step%20model%20capabilities%20improve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06334v2&entry.124074799=Read"},
{"title": "Bounds and Sensitivity Analysis of the Causal Effect Under\n  Outcome-Independent MNAR Confounding", "author": "Jose M. Pe\u00f1a", "abstract": "  We report assumption-free bounds for any contrast between the probabilities\nof the potential outcome under exposure and non-exposure when the confounders\nare missing not at random. We assume that the missingness mechanism is\noutcome-independent. We also report a sensitivity analysis method to complement\nour bounds.\n", "link": "http://arxiv.org/abs/2410.06726v2", "date": "2024-11-01", "relevancy": 0.7583, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3925}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3811}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bounds%20and%20Sensitivity%20Analysis%20of%20the%20Causal%20Effect%20Under%0A%20%20Outcome-Independent%20MNAR%20Confounding&body=Title%3A%20Bounds%20and%20Sensitivity%20Analysis%20of%20the%20Causal%20Effect%20Under%0A%20%20Outcome-Independent%20MNAR%20Confounding%0AAuthor%3A%20Jose%20M.%20Pe%C3%B1a%0AAbstract%3A%20%20%20We%20report%20assumption-free%20bounds%20for%20any%20contrast%20between%20the%20probabilities%0Aof%20the%20potential%20outcome%20under%20exposure%20and%20non-exposure%20when%20the%20confounders%0Aare%20missing%20not%20at%20random.%20We%20assume%20that%20the%20missingness%20mechanism%20is%0Aoutcome-independent.%20We%20also%20report%20a%20sensitivity%20analysis%20method%20to%20complement%0Aour%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBounds%2520and%2520Sensitivity%2520Analysis%2520of%2520the%2520Causal%2520Effect%2520Under%250A%2520%2520Outcome-Independent%2520MNAR%2520Confounding%26entry.906535625%3DJose%2520M.%2520Pe%25C3%25B1a%26entry.1292438233%3D%2520%2520We%2520report%2520assumption-free%2520bounds%2520for%2520any%2520contrast%2520between%2520the%2520probabilities%250Aof%2520the%2520potential%2520outcome%2520under%2520exposure%2520and%2520non-exposure%2520when%2520the%2520confounders%250Aare%2520missing%2520not%2520at%2520random.%2520We%2520assume%2520that%2520the%2520missingness%2520mechanism%2520is%250Aoutcome-independent.%2520We%2520also%2520report%2520a%2520sensitivity%2520analysis%2520method%2520to%2520complement%250Aour%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bounds%20and%20Sensitivity%20Analysis%20of%20the%20Causal%20Effect%20Under%0A%20%20Outcome-Independent%20MNAR%20Confounding&entry.906535625=Jose%20M.%20Pe%C3%B1a&entry.1292438233=%20%20We%20report%20assumption-free%20bounds%20for%20any%20contrast%20between%20the%20probabilities%0Aof%20the%20potential%20outcome%20under%20exposure%20and%20non-exposure%20when%20the%20confounders%0Aare%20missing%20not%20at%20random.%20We%20assume%20that%20the%20missingness%20mechanism%20is%0Aoutcome-independent.%20We%20also%20report%20a%20sensitivity%20analysis%20method%20to%20complement%0Aour%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06726v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


