<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251217.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering", "author": "Divam Gupta and Anuj Pahuja and Nemanja Bartolovic and Tomas Simon and Forrest Iandola and Giljoo Nam", "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.", "link": "http://arxiv.org/abs/2512.15711v1", "date": "2025-12-17", "relevancy": 3.5649, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.74}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.74}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Pixel%20Codec%20Avatars%3A%20A%20Hybrid%20Representation%20for%20Efficient%20Rendering&body=Title%3A%20Gaussian%20Pixel%20Codec%20Avatars%3A%20A%20Hybrid%20Representation%20for%20Efficient%20Rendering%0AAuthor%3A%20Divam%20Gupta%20and%20Anuj%20Pahuja%20and%20Nemanja%20Bartolovic%20and%20Tomas%20Simon%20and%20Forrest%20Iandola%20and%20Giljoo%20Nam%0AAbstract%3A%20We%20present%20Gaussian%20Pixel%20Codec%20Avatars%20%28GPiCA%29%2C%20photorealistic%20head%20avatars%20that%20can%20be%20generated%20from%20multi-view%20images%20and%20efficiently%20rendered%20on%20mobile%20devices.%20GPiCA%20utilizes%20a%20unique%20hybrid%20representation%20that%20combines%20a%20triangle%20mesh%20and%20anisotropic%203D%20Gaussians.%20This%20combination%20maximizes%20memory%20and%20rendering%20efficiency%20while%20maintaining%20a%20photorealistic%20appearance.%20The%20triangle%20mesh%20is%20highly%20efficient%20in%20representing%20surface%20areas%20like%20facial%20skin%2C%20while%20the%203D%20Gaussians%20effectively%20handle%20non-surface%20areas%20such%20as%20hair%20and%20beard.%20To%20this%20end%2C%20we%20develop%20a%20unified%20differentiable%20rendering%20pipeline%20that%20treats%20the%20mesh%20as%20a%20semi-transparent%20layer%20within%20the%20volumetric%20rendering%20paradigm%20of%203D%20Gaussian%20Splatting.%20We%20train%20neural%20networks%20to%20decode%20a%20facial%20expression%20code%20into%20three%20components%3A%20a%203D%20face%20mesh%2C%20an%20RGBA%20texture%2C%20and%20a%20set%20of%203D%20Gaussians.%20These%20components%20are%20rendered%20simultaneously%20in%20a%20unified%20rendering%20engine.%20The%20networks%20are%20trained%20using%20multi-view%20image%20supervision.%20Our%20results%20demonstrate%20that%20GPiCA%20achieves%20the%20realism%20of%20purely%20Gaussian-based%20avatars%20while%20matching%20the%20rendering%20performance%20of%20mesh-based%20avatars.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Pixel%2520Codec%2520Avatars%253A%2520A%2520Hybrid%2520Representation%2520for%2520Efficient%2520Rendering%26entry.906535625%3DDivam%2520Gupta%2520and%2520Anuj%2520Pahuja%2520and%2520Nemanja%2520Bartolovic%2520and%2520Tomas%2520Simon%2520and%2520Forrest%2520Iandola%2520and%2520Giljoo%2520Nam%26entry.1292438233%3DWe%2520present%2520Gaussian%2520Pixel%2520Codec%2520Avatars%2520%2528GPiCA%2529%252C%2520photorealistic%2520head%2520avatars%2520that%2520can%2520be%2520generated%2520from%2520multi-view%2520images%2520and%2520efficiently%2520rendered%2520on%2520mobile%2520devices.%2520GPiCA%2520utilizes%2520a%2520unique%2520hybrid%2520representation%2520that%2520combines%2520a%2520triangle%2520mesh%2520and%2520anisotropic%25203D%2520Gaussians.%2520This%2520combination%2520maximizes%2520memory%2520and%2520rendering%2520efficiency%2520while%2520maintaining%2520a%2520photorealistic%2520appearance.%2520The%2520triangle%2520mesh%2520is%2520highly%2520efficient%2520in%2520representing%2520surface%2520areas%2520like%2520facial%2520skin%252C%2520while%2520the%25203D%2520Gaussians%2520effectively%2520handle%2520non-surface%2520areas%2520such%2520as%2520hair%2520and%2520beard.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%2520unified%2520differentiable%2520rendering%2520pipeline%2520that%2520treats%2520the%2520mesh%2520as%2520a%2520semi-transparent%2520layer%2520within%2520the%2520volumetric%2520rendering%2520paradigm%2520of%25203D%2520Gaussian%2520Splatting.%2520We%2520train%2520neural%2520networks%2520to%2520decode%2520a%2520facial%2520expression%2520code%2520into%2520three%2520components%253A%2520a%25203D%2520face%2520mesh%252C%2520an%2520RGBA%2520texture%252C%2520and%2520a%2520set%2520of%25203D%2520Gaussians.%2520These%2520components%2520are%2520rendered%2520simultaneously%2520in%2520a%2520unified%2520rendering%2520engine.%2520The%2520networks%2520are%2520trained%2520using%2520multi-view%2520image%2520supervision.%2520Our%2520results%2520demonstrate%2520that%2520GPiCA%2520achieves%2520the%2520realism%2520of%2520purely%2520Gaussian-based%2520avatars%2520while%2520matching%2520the%2520rendering%2520performance%2520of%2520mesh-based%2520avatars.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Pixel%20Codec%20Avatars%3A%20A%20Hybrid%20Representation%20for%20Efficient%20Rendering&entry.906535625=Divam%20Gupta%20and%20Anuj%20Pahuja%20and%20Nemanja%20Bartolovic%20and%20Tomas%20Simon%20and%20Forrest%20Iandola%20and%20Giljoo%20Nam&entry.1292438233=We%20present%20Gaussian%20Pixel%20Codec%20Avatars%20%28GPiCA%29%2C%20photorealistic%20head%20avatars%20that%20can%20be%20generated%20from%20multi-view%20images%20and%20efficiently%20rendered%20on%20mobile%20devices.%20GPiCA%20utilizes%20a%20unique%20hybrid%20representation%20that%20combines%20a%20triangle%20mesh%20and%20anisotropic%203D%20Gaussians.%20This%20combination%20maximizes%20memory%20and%20rendering%20efficiency%20while%20maintaining%20a%20photorealistic%20appearance.%20The%20triangle%20mesh%20is%20highly%20efficient%20in%20representing%20surface%20areas%20like%20facial%20skin%2C%20while%20the%203D%20Gaussians%20effectively%20handle%20non-surface%20areas%20such%20as%20hair%20and%20beard.%20To%20this%20end%2C%20we%20develop%20a%20unified%20differentiable%20rendering%20pipeline%20that%20treats%20the%20mesh%20as%20a%20semi-transparent%20layer%20within%20the%20volumetric%20rendering%20paradigm%20of%203D%20Gaussian%20Splatting.%20We%20train%20neural%20networks%20to%20decode%20a%20facial%20expression%20code%20into%20three%20components%3A%20a%203D%20face%20mesh%2C%20an%20RGBA%20texture%2C%20and%20a%20set%20of%203D%20Gaussians.%20These%20components%20are%20rendered%20simultaneously%20in%20a%20unified%20rendering%20engine.%20The%20networks%20are%20trained%20using%20multi-view%20image%20supervision.%20Our%20results%20demonstrate%20that%20GPiCA%20achieves%20the%20realism%20of%20purely%20Gaussian-based%20avatars%20while%20matching%20the%20rendering%20performance%20of%20mesh-based%20avatars.&entry.1838667208=http%3A//arxiv.org/abs/2512.15711v1&entry.124074799=Read"},
{"title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting", "author": "Arthur Moreau and Richard Shaw and Michal Nazarczuk and Jisu Shin and Thomas Tanay and Zhensong Zhang and Songcen Xu and Eduardo P\u00e9rez-Pellitero", "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.", "link": "http://arxiv.org/abs/2512.15508v1", "date": "2025-12-17", "relevancy": 3.4191, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6983}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.692}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Off%20The%20Grid%3A%20Detection%20of%20Primitives%20for%20Feed-Forward%203D%20Gaussian%20Splatting&body=Title%3A%20Off%20The%20Grid%3A%20Detection%20of%20Primitives%20for%20Feed-Forward%203D%20Gaussian%20Splatting%0AAuthor%3A%20Arthur%20Moreau%20and%20Richard%20Shaw%20and%20Michal%20Nazarczuk%20and%20Jisu%20Shin%20and%20Thomas%20Tanay%20and%20Zhensong%20Zhang%20and%20Songcen%20Xu%20and%20Eduardo%20P%C3%A9rez-Pellitero%0AAbstract%3A%20Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20enable%20real-time%20scene%20generation%20but%20are%20hindered%20by%20suboptimal%20pixel-aligned%20primitive%20placement%2C%20which%20relies%20on%20a%20dense%2C%20rigid%20grid%20and%20limits%20both%20quality%20and%20efficiency.%20We%20introduce%20a%20new%20feed-forward%20architecture%20that%20detects%203D%20Gaussian%20primitives%20at%20a%20sub-pixel%20level%2C%20replacing%20the%20pixel%20grid%20with%20an%20adaptive%2C%20%22Off%20The%20Grid%22%20distribution.%20Inspired%20by%20keypoint%20detection%2C%20our%20multi-resolution%20decoder%20learns%20to%20distribute%20primitives%20across%20image%20patches.%20This%20module%20is%20trained%20end-to-end%20with%20a%203D%20reconstruction%20backbone%20using%20self-supervised%20learning.%20Our%20resulting%20pose-free%20model%20generates%20photorealistic%20scenes%20in%20seconds%2C%20achieving%20state-of-the-art%20novel%20view%20synthesis%20for%20feed-forward%20models.%20It%20outperforms%20competitors%20while%20using%20far%20fewer%20primitives%2C%20demonstrating%20a%20more%20accurate%20and%20efficient%20allocation%20that%20captures%20fine%20details%20and%20reduces%20artifacts.%20Moreover%2C%20we%20observe%20that%20by%20learning%20to%20render%203D%20Gaussians%2C%20our%203D%20reconstruction%20backbone%20improves%20camera%20pose%20estimation%2C%20suggesting%20opportunities%20to%20train%20these%20foundational%20models%20without%20labels.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOff%2520The%2520Grid%253A%2520Detection%2520of%2520Primitives%2520for%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DArthur%2520Moreau%2520and%2520Richard%2520Shaw%2520and%2520Michal%2520Nazarczuk%2520and%2520Jisu%2520Shin%2520and%2520Thomas%2520Tanay%2520and%2520Zhensong%2520Zhang%2520and%2520Songcen%2520Xu%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%26entry.1292438233%3DFeed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models%2520enable%2520real-time%2520scene%2520generation%2520but%2520are%2520hindered%2520by%2520suboptimal%2520pixel-aligned%2520primitive%2520placement%252C%2520which%2520relies%2520on%2520a%2520dense%252C%2520rigid%2520grid%2520and%2520limits%2520both%2520quality%2520and%2520efficiency.%2520We%2520introduce%2520a%2520new%2520feed-forward%2520architecture%2520that%2520detects%25203D%2520Gaussian%2520primitives%2520at%2520a%2520sub-pixel%2520level%252C%2520replacing%2520the%2520pixel%2520grid%2520with%2520an%2520adaptive%252C%2520%2522Off%2520The%2520Grid%2522%2520distribution.%2520Inspired%2520by%2520keypoint%2520detection%252C%2520our%2520multi-resolution%2520decoder%2520learns%2520to%2520distribute%2520primitives%2520across%2520image%2520patches.%2520This%2520module%2520is%2520trained%2520end-to-end%2520with%2520a%25203D%2520reconstruction%2520backbone%2520using%2520self-supervised%2520learning.%2520Our%2520resulting%2520pose-free%2520model%2520generates%2520photorealistic%2520scenes%2520in%2520seconds%252C%2520achieving%2520state-of-the-art%2520novel%2520view%2520synthesis%2520for%2520feed-forward%2520models.%2520It%2520outperforms%2520competitors%2520while%2520using%2520far%2520fewer%2520primitives%252C%2520demonstrating%2520a%2520more%2520accurate%2520and%2520efficient%2520allocation%2520that%2520captures%2520fine%2520details%2520and%2520reduces%2520artifacts.%2520Moreover%252C%2520we%2520observe%2520that%2520by%2520learning%2520to%2520render%25203D%2520Gaussians%252C%2520our%25203D%2520reconstruction%2520backbone%2520improves%2520camera%2520pose%2520estimation%252C%2520suggesting%2520opportunities%2520to%2520train%2520these%2520foundational%2520models%2520without%2520labels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Off%20The%20Grid%3A%20Detection%20of%20Primitives%20for%20Feed-Forward%203D%20Gaussian%20Splatting&entry.906535625=Arthur%20Moreau%20and%20Richard%20Shaw%20and%20Michal%20Nazarczuk%20and%20Jisu%20Shin%20and%20Thomas%20Tanay%20and%20Zhensong%20Zhang%20and%20Songcen%20Xu%20and%20Eduardo%20P%C3%A9rez-Pellitero&entry.1292438233=Feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20enable%20real-time%20scene%20generation%20but%20are%20hindered%20by%20suboptimal%20pixel-aligned%20primitive%20placement%2C%20which%20relies%20on%20a%20dense%2C%20rigid%20grid%20and%20limits%20both%20quality%20and%20efficiency.%20We%20introduce%20a%20new%20feed-forward%20architecture%20that%20detects%203D%20Gaussian%20primitives%20at%20a%20sub-pixel%20level%2C%20replacing%20the%20pixel%20grid%20with%20an%20adaptive%2C%20%22Off%20The%20Grid%22%20distribution.%20Inspired%20by%20keypoint%20detection%2C%20our%20multi-resolution%20decoder%20learns%20to%20distribute%20primitives%20across%20image%20patches.%20This%20module%20is%20trained%20end-to-end%20with%20a%203D%20reconstruction%20backbone%20using%20self-supervised%20learning.%20Our%20resulting%20pose-free%20model%20generates%20photorealistic%20scenes%20in%20seconds%2C%20achieving%20state-of-the-art%20novel%20view%20synthesis%20for%20feed-forward%20models.%20It%20outperforms%20competitors%20while%20using%20far%20fewer%20primitives%2C%20demonstrating%20a%20more%20accurate%20and%20efficient%20allocation%20that%20captures%20fine%20details%20and%20reduces%20artifacts.%20Moreover%2C%20we%20observe%20that%20by%20learning%20to%20render%203D%20Gaussians%2C%20our%203D%20reconstruction%20backbone%20improves%20camera%20pose%20estimation%2C%20suggesting%20opportunities%20to%20train%20these%20foundational%20models%20without%20labels.&entry.1838667208=http%3A//arxiv.org/abs/2512.15508v1&entry.124074799=Read"},
{"title": "FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision", "author": "Tobias Kirschstein and Simon Giebenhain and Matthias Nie\u00dfner", "abstract": "We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/", "link": "http://arxiv.org/abs/2512.15599v1", "date": "2025-12-17", "relevancy": 3.2249, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6627}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6627}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexAvatar%3A%20Learning%20Complete%203D%20Head%20Avatars%20with%20Partial%20Supervision&body=Title%3A%20FlexAvatar%3A%20Learning%20Complete%203D%20Head%20Avatars%20with%20Partial%20Supervision%0AAuthor%3A%20Tobias%20Kirschstein%20and%20Simon%20Giebenhain%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20We%20introduce%20FlexAvatar%2C%20a%20method%20for%20creating%20high-quality%20and%20complete%203D%20head%20avatars%20from%20a%20single%20image.%20A%20core%20challenge%20lies%20in%20the%20limited%20availability%20of%20multi-view%20data%20and%20the%20tendency%20of%20monocular%20training%20to%20yield%20incomplete%203D%20head%20reconstructions.%20We%20identify%20the%20root%20cause%20of%20this%20issue%20as%20the%20entanglement%20between%20driving%20signal%20and%20target%20viewpoint%20when%20learning%20from%20monocular%20videos.%20To%20address%20this%2C%20we%20propose%20a%20transformer-based%203D%20portrait%20animation%20model%20with%20learnable%20data%20source%20tokens%2C%20so-called%20bias%20sinks%2C%20which%20enables%20unified%20training%20across%20monocular%20and%20multi-view%20datasets.%20This%20design%20leverages%20the%20strengths%20of%20both%20data%20sources%20during%20inference%3A%20strong%20generalization%20from%20monocular%20data%20and%20full%203D%20completeness%20from%20multi-view%20supervision.%20Furthermore%2C%20our%20training%20procedure%20yields%20a%20smooth%20latent%20avatar%20space%20that%20facilitates%20identity%20interpolation%20and%20flexible%20fitting%20to%20an%20arbitrary%20number%20of%20input%20observations.%20In%20extensive%20evaluations%20on%20single-view%2C%20few-shot%2C%20and%20monocular%20avatar%20creation%20tasks%2C%20we%20verify%20the%20efficacy%20of%20FlexAvatar.%20Many%20existing%20methods%20struggle%20with%20view%20extrapolation%20while%20FlexAvatar%20generates%20complete%203D%20head%20avatars%20with%20realistic%20facial%20animations.%20Website%3A%20https%3A//tobias-kirschstein.github.io/flexavatar/%0ALink%3A%20http%3A//arxiv.org/abs/2512.15599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexAvatar%253A%2520Learning%2520Complete%25203D%2520Head%2520Avatars%2520with%2520Partial%2520Supervision%26entry.906535625%3DTobias%2520Kirschstein%2520and%2520Simon%2520Giebenhain%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3DWe%2520introduce%2520FlexAvatar%252C%2520a%2520method%2520for%2520creating%2520high-quality%2520and%2520complete%25203D%2520head%2520avatars%2520from%2520a%2520single%2520image.%2520A%2520core%2520challenge%2520lies%2520in%2520the%2520limited%2520availability%2520of%2520multi-view%2520data%2520and%2520the%2520tendency%2520of%2520monocular%2520training%2520to%2520yield%2520incomplete%25203D%2520head%2520reconstructions.%2520We%2520identify%2520the%2520root%2520cause%2520of%2520this%2520issue%2520as%2520the%2520entanglement%2520between%2520driving%2520signal%2520and%2520target%2520viewpoint%2520when%2520learning%2520from%2520monocular%2520videos.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520transformer-based%25203D%2520portrait%2520animation%2520model%2520with%2520learnable%2520data%2520source%2520tokens%252C%2520so-called%2520bias%2520sinks%252C%2520which%2520enables%2520unified%2520training%2520across%2520monocular%2520and%2520multi-view%2520datasets.%2520This%2520design%2520leverages%2520the%2520strengths%2520of%2520both%2520data%2520sources%2520during%2520inference%253A%2520strong%2520generalization%2520from%2520monocular%2520data%2520and%2520full%25203D%2520completeness%2520from%2520multi-view%2520supervision.%2520Furthermore%252C%2520our%2520training%2520procedure%2520yields%2520a%2520smooth%2520latent%2520avatar%2520space%2520that%2520facilitates%2520identity%2520interpolation%2520and%2520flexible%2520fitting%2520to%2520an%2520arbitrary%2520number%2520of%2520input%2520observations.%2520In%2520extensive%2520evaluations%2520on%2520single-view%252C%2520few-shot%252C%2520and%2520monocular%2520avatar%2520creation%2520tasks%252C%2520we%2520verify%2520the%2520efficacy%2520of%2520FlexAvatar.%2520Many%2520existing%2520methods%2520struggle%2520with%2520view%2520extrapolation%2520while%2520FlexAvatar%2520generates%2520complete%25203D%2520head%2520avatars%2520with%2520realistic%2520facial%2520animations.%2520Website%253A%2520https%253A//tobias-kirschstein.github.io/flexavatar/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexAvatar%3A%20Learning%20Complete%203D%20Head%20Avatars%20with%20Partial%20Supervision&entry.906535625=Tobias%20Kirschstein%20and%20Simon%20Giebenhain%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=We%20introduce%20FlexAvatar%2C%20a%20method%20for%20creating%20high-quality%20and%20complete%203D%20head%20avatars%20from%20a%20single%20image.%20A%20core%20challenge%20lies%20in%20the%20limited%20availability%20of%20multi-view%20data%20and%20the%20tendency%20of%20monocular%20training%20to%20yield%20incomplete%203D%20head%20reconstructions.%20We%20identify%20the%20root%20cause%20of%20this%20issue%20as%20the%20entanglement%20between%20driving%20signal%20and%20target%20viewpoint%20when%20learning%20from%20monocular%20videos.%20To%20address%20this%2C%20we%20propose%20a%20transformer-based%203D%20portrait%20animation%20model%20with%20learnable%20data%20source%20tokens%2C%20so-called%20bias%20sinks%2C%20which%20enables%20unified%20training%20across%20monocular%20and%20multi-view%20datasets.%20This%20design%20leverages%20the%20strengths%20of%20both%20data%20sources%20during%20inference%3A%20strong%20generalization%20from%20monocular%20data%20and%20full%203D%20completeness%20from%20multi-view%20supervision.%20Furthermore%2C%20our%20training%20procedure%20yields%20a%20smooth%20latent%20avatar%20space%20that%20facilitates%20identity%20interpolation%20and%20flexible%20fitting%20to%20an%20arbitrary%20number%20of%20input%20observations.%20In%20extensive%20evaluations%20on%20single-view%2C%20few-shot%2C%20and%20monocular%20avatar%20creation%20tasks%2C%20we%20verify%20the%20efficacy%20of%20FlexAvatar.%20Many%20existing%20methods%20struggle%20with%20view%20extrapolation%20while%20FlexAvatar%20generates%20complete%203D%20head%20avatars%20with%20realistic%20facial%20animations.%20Website%3A%20https%3A//tobias-kirschstein.github.io/flexavatar/&entry.1838667208=http%3A//arxiv.org/abs/2512.15599v1&entry.124074799=Read"},
{"title": "RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting", "author": "Seyed Abolfazl Ghasemzadeh and Alexandre Alahi and Christophe De Vleeschouwer", "abstract": "Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL", "link": "http://arxiv.org/abs/2512.15488v1", "date": "2025-12-17", "relevancy": 3.1352, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6394}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6236}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RUMPL%3A%20Ray-Based%20Transformers%20for%20Universal%20Multi-View%202D%20to%203D%20Human%20Pose%20Lifting&body=Title%3A%20RUMPL%3A%20Ray-Based%20Transformers%20for%20Universal%20Multi-View%202D%20to%203D%20Human%20Pose%20Lifting%0AAuthor%3A%20Seyed%20Abolfazl%20Ghasemzadeh%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20Estimating%203D%20human%20poses%20from%202D%20images%20remains%20challenging%20due%20to%20occlusions%20and%20projective%20ambiguity.%20Multi-view%20learning-based%20approaches%20mitigate%20these%20issues%20but%20often%20fail%20to%20generalize%20to%20real-world%20scenarios%2C%20as%20large-scale%20multi-view%20datasets%20with%203D%20ground%20truth%20are%20scarce%20and%20captured%20under%20constrained%20conditions.%20To%20overcome%20this%20limitation%2C%20recent%20methods%20rely%20on%202D%20pose%20estimation%20combined%20with%202D-to-3D%20pose%20lifting%20trained%20on%20synthetic%20data.%20Building%20on%20our%20previous%20MPL%20framework%2C%20we%20propose%20RUMPL%2C%20a%20transformer-based%203D%20pose%20lifter%20that%20introduces%20a%203D%20ray-based%20representation%20of%202D%20keypoints.%20This%20formulation%20makes%20the%20model%20independent%20of%20camera%20calibration%20and%20the%20number%20of%20views%2C%20enabling%20universal%20deployment%20across%20arbitrary%20multi-view%20configurations%20without%20retraining%20or%20fine-tuning.%20A%20new%20View%20Fusion%20Transformer%20leverages%20learned%20fused-ray%20tokens%20to%20aggregate%20information%20along%20rays%2C%20further%20improving%20multi-view%20consistency.%20Extensive%20experiments%20demonstrate%20that%20RUMPL%20reduces%20MPJPE%20by%20up%20to%2053%25%20compared%20to%20triangulation%20and%20over%2060%25%20compared%20to%20transformer-based%20image-representation%20baselines.%20Results%20on%20new%20benchmarks%2C%20including%20in-the-wild%20multi-view%20and%20multi-person%20datasets%2C%20confirm%20its%20robustness%20and%20scalability.%20The%20framework%27s%20source%20code%20is%20available%20at%20https%3A//github.com/aghasemzadeh/OpenRUMPL%0ALink%3A%20http%3A//arxiv.org/abs/2512.15488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRUMPL%253A%2520Ray-Based%2520Transformers%2520for%2520Universal%2520Multi-View%25202D%2520to%25203D%2520Human%2520Pose%2520Lifting%26entry.906535625%3DSeyed%2520Abolfazl%2520Ghasemzadeh%2520and%2520Alexandre%2520Alahi%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3DEstimating%25203D%2520human%2520poses%2520from%25202D%2520images%2520remains%2520challenging%2520due%2520to%2520occlusions%2520and%2520projective%2520ambiguity.%2520Multi-view%2520learning-based%2520approaches%2520mitigate%2520these%2520issues%2520but%2520often%2520fail%2520to%2520generalize%2520to%2520real-world%2520scenarios%252C%2520as%2520large-scale%2520multi-view%2520datasets%2520with%25203D%2520ground%2520truth%2520are%2520scarce%2520and%2520captured%2520under%2520constrained%2520conditions.%2520To%2520overcome%2520this%2520limitation%252C%2520recent%2520methods%2520rely%2520on%25202D%2520pose%2520estimation%2520combined%2520with%25202D-to-3D%2520pose%2520lifting%2520trained%2520on%2520synthetic%2520data.%2520Building%2520on%2520our%2520previous%2520MPL%2520framework%252C%2520we%2520propose%2520RUMPL%252C%2520a%2520transformer-based%25203D%2520pose%2520lifter%2520that%2520introduces%2520a%25203D%2520ray-based%2520representation%2520of%25202D%2520keypoints.%2520This%2520formulation%2520makes%2520the%2520model%2520independent%2520of%2520camera%2520calibration%2520and%2520the%2520number%2520of%2520views%252C%2520enabling%2520universal%2520deployment%2520across%2520arbitrary%2520multi-view%2520configurations%2520without%2520retraining%2520or%2520fine-tuning.%2520A%2520new%2520View%2520Fusion%2520Transformer%2520leverages%2520learned%2520fused-ray%2520tokens%2520to%2520aggregate%2520information%2520along%2520rays%252C%2520further%2520improving%2520multi-view%2520consistency.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RUMPL%2520reduces%2520MPJPE%2520by%2520up%2520to%252053%2525%2520compared%2520to%2520triangulation%2520and%2520over%252060%2525%2520compared%2520to%2520transformer-based%2520image-representation%2520baselines.%2520Results%2520on%2520new%2520benchmarks%252C%2520including%2520in-the-wild%2520multi-view%2520and%2520multi-person%2520datasets%252C%2520confirm%2520its%2520robustness%2520and%2520scalability.%2520The%2520framework%2527s%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/aghasemzadeh/OpenRUMPL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RUMPL%3A%20Ray-Based%20Transformers%20for%20Universal%20Multi-View%202D%20to%203D%20Human%20Pose%20Lifting&entry.906535625=Seyed%20Abolfazl%20Ghasemzadeh%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=Estimating%203D%20human%20poses%20from%202D%20images%20remains%20challenging%20due%20to%20occlusions%20and%20projective%20ambiguity.%20Multi-view%20learning-based%20approaches%20mitigate%20these%20issues%20but%20often%20fail%20to%20generalize%20to%20real-world%20scenarios%2C%20as%20large-scale%20multi-view%20datasets%20with%203D%20ground%20truth%20are%20scarce%20and%20captured%20under%20constrained%20conditions.%20To%20overcome%20this%20limitation%2C%20recent%20methods%20rely%20on%202D%20pose%20estimation%20combined%20with%202D-to-3D%20pose%20lifting%20trained%20on%20synthetic%20data.%20Building%20on%20our%20previous%20MPL%20framework%2C%20we%20propose%20RUMPL%2C%20a%20transformer-based%203D%20pose%20lifter%20that%20introduces%20a%203D%20ray-based%20representation%20of%202D%20keypoints.%20This%20formulation%20makes%20the%20model%20independent%20of%20camera%20calibration%20and%20the%20number%20of%20views%2C%20enabling%20universal%20deployment%20across%20arbitrary%20multi-view%20configurations%20without%20retraining%20or%20fine-tuning.%20A%20new%20View%20Fusion%20Transformer%20leverages%20learned%20fused-ray%20tokens%20to%20aggregate%20information%20along%20rays%2C%20further%20improving%20multi-view%20consistency.%20Extensive%20experiments%20demonstrate%20that%20RUMPL%20reduces%20MPJPE%20by%20up%20to%2053%25%20compared%20to%20triangulation%20and%20over%2060%25%20compared%20to%20transformer-based%20image-representation%20baselines.%20Results%20on%20new%20benchmarks%2C%20including%20in-the-wild%20multi-view%20and%20multi-person%20datasets%2C%20confirm%20its%20robustness%20and%20scalability.%20The%20framework%27s%20source%20code%20is%20available%20at%20https%3A//github.com/aghasemzadeh/OpenRUMPL&entry.1838667208=http%3A//arxiv.org/abs/2512.15488v1&entry.124074799=Read"},
{"title": "Multi-View Foundation Models", "author": "Leo Segre and Or Hirschorn and Shai Avidan", "abstract": "Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.", "link": "http://arxiv.org/abs/2512.15708v1", "date": "2025-12-17", "relevancy": 3.1103, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6322}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Foundation%20Models&body=Title%3A%20Multi-View%20Foundation%20Models%0AAuthor%3A%20Leo%20Segre%20and%20Or%20Hirschorn%20and%20Shai%20Avidan%0AAbstract%3A%20Foundation%20models%20are%20vital%20tools%20in%20various%20Computer%20Vision%20applications.%20They%20take%20as%20input%20a%20single%20RGB%20image%20and%20output%20a%20deep%20feature%20representation%20that%20is%20useful%20for%20various%20applications.%20However%2C%20in%20case%20we%20have%20multiple%20views%20of%20the%20same%203D%20scene%2C%20they%20operate%20on%20each%20image%20independently%20and%20do%20not%20always%20produce%20consistent%20features%20for%20the%20same%203D%20point.%20We%20propose%20a%20way%20to%20convert%20a%20Foundation%20Model%20into%20a%20Multi-View%20Foundation%20Model.%20Such%20a%20model%20takes%20as%20input%20a%20set%20of%20images%20and%20outputs%20a%20feature%20map%20for%20each%20image%20such%20that%20the%20features%20of%20corresponding%20points%20are%20as%20consistent%20as%20possible.%20This%20approach%20bypasses%20the%20need%20to%20build%20a%20consistent%203D%20model%20of%20the%20features%20and%20allows%20direct%20manipulation%20in%20the%20image%20space.%20Specifically%2C%20we%20show%20how%20to%20augment%20Transformers-based%20foundation%20models%20%28i.e.%2C%20DINO%2C%20SAM%2C%20CLIP%29%20with%20intermediate%203D-aware%20attention%20layers%20that%20help%20match%20features%20across%20different%20views.%20As%20leading%20examples%2C%20we%20show%20surface%20normal%20estimation%20and%20multi-view%20segmentation%20tasks.%20Quantitative%20experiments%20show%20that%20our%20method%20improves%20feature%20matching%20considerably%20compared%20to%20current%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Foundation%2520Models%26entry.906535625%3DLeo%2520Segre%2520and%2520Or%2520Hirschorn%2520and%2520Shai%2520Avidan%26entry.1292438233%3DFoundation%2520models%2520are%2520vital%2520tools%2520in%2520various%2520Computer%2520Vision%2520applications.%2520They%2520take%2520as%2520input%2520a%2520single%2520RGB%2520image%2520and%2520output%2520a%2520deep%2520feature%2520representation%2520that%2520is%2520useful%2520for%2520various%2520applications.%2520However%252C%2520in%2520case%2520we%2520have%2520multiple%2520views%2520of%2520the%2520same%25203D%2520scene%252C%2520they%2520operate%2520on%2520each%2520image%2520independently%2520and%2520do%2520not%2520always%2520produce%2520consistent%2520features%2520for%2520the%2520same%25203D%2520point.%2520We%2520propose%2520a%2520way%2520to%2520convert%2520a%2520Foundation%2520Model%2520into%2520a%2520Multi-View%2520Foundation%2520Model.%2520Such%2520a%2520model%2520takes%2520as%2520input%2520a%2520set%2520of%2520images%2520and%2520outputs%2520a%2520feature%2520map%2520for%2520each%2520image%2520such%2520that%2520the%2520features%2520of%2520corresponding%2520points%2520are%2520as%2520consistent%2520as%2520possible.%2520This%2520approach%2520bypasses%2520the%2520need%2520to%2520build%2520a%2520consistent%25203D%2520model%2520of%2520the%2520features%2520and%2520allows%2520direct%2520manipulation%2520in%2520the%2520image%2520space.%2520Specifically%252C%2520we%2520show%2520how%2520to%2520augment%2520Transformers-based%2520foundation%2520models%2520%2528i.e.%252C%2520DINO%252C%2520SAM%252C%2520CLIP%2529%2520with%2520intermediate%25203D-aware%2520attention%2520layers%2520that%2520help%2520match%2520features%2520across%2520different%2520views.%2520As%2520leading%2520examples%252C%2520we%2520show%2520surface%2520normal%2520estimation%2520and%2520multi-view%2520segmentation%2520tasks.%2520Quantitative%2520experiments%2520show%2520that%2520our%2520method%2520improves%2520feature%2520matching%2520considerably%2520compared%2520to%2520current%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Foundation%20Models&entry.906535625=Leo%20Segre%20and%20Or%20Hirschorn%20and%20Shai%20Avidan&entry.1292438233=Foundation%20models%20are%20vital%20tools%20in%20various%20Computer%20Vision%20applications.%20They%20take%20as%20input%20a%20single%20RGB%20image%20and%20output%20a%20deep%20feature%20representation%20that%20is%20useful%20for%20various%20applications.%20However%2C%20in%20case%20we%20have%20multiple%20views%20of%20the%20same%203D%20scene%2C%20they%20operate%20on%20each%20image%20independently%20and%20do%20not%20always%20produce%20consistent%20features%20for%20the%20same%203D%20point.%20We%20propose%20a%20way%20to%20convert%20a%20Foundation%20Model%20into%20a%20Multi-View%20Foundation%20Model.%20Such%20a%20model%20takes%20as%20input%20a%20set%20of%20images%20and%20outputs%20a%20feature%20map%20for%20each%20image%20such%20that%20the%20features%20of%20corresponding%20points%20are%20as%20consistent%20as%20possible.%20This%20approach%20bypasses%20the%20need%20to%20build%20a%20consistent%203D%20model%20of%20the%20features%20and%20allows%20direct%20manipulation%20in%20the%20image%20space.%20Specifically%2C%20we%20show%20how%20to%20augment%20Transformers-based%20foundation%20models%20%28i.e.%2C%20DINO%2C%20SAM%2C%20CLIP%29%20with%20intermediate%203D-aware%20attention%20layers%20that%20help%20match%20features%20across%20different%20views.%20As%20leading%20examples%2C%20we%20show%20surface%20normal%20estimation%20and%20multi-view%20segmentation%20tasks.%20Quantitative%20experiments%20show%20that%20our%20method%20improves%20feature%20matching%20considerably%20compared%20to%20current%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.15708v1&entry.124074799=Read"},
{"title": "In Pursuit of Pixel Supervision for Visual Pre-training", "author": "Lihe Yang and Shang-Wen Li and Yang Li and Xinjie Lei and Dong Wang and Abdelrahman Mohamed and Hengshuang Zhao and Hu Xu", "abstract": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.", "link": "http://arxiv.org/abs/2512.15715v1", "date": "2025-12-17", "relevancy": 3.0819, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20Pursuit%20of%20Pixel%20Supervision%20for%20Visual%20Pre-training&body=Title%3A%20In%20Pursuit%20of%20Pixel%20Supervision%20for%20Visual%20Pre-training%0AAuthor%3A%20Lihe%20Yang%20and%20Shang-Wen%20Li%20and%20Yang%20Li%20and%20Xinjie%20Lei%20and%20Dong%20Wang%20and%20Abdelrahman%20Mohamed%20and%20Hengshuang%20Zhao%20and%20Hu%20Xu%0AAbstract%3A%20At%20the%20most%20basic%20level%2C%20pixels%20are%20the%20source%20of%20the%20visual%20information%20through%20which%20we%20perceive%20the%20world.%20Pixels%20contain%20information%20at%20all%20levels%2C%20ranging%20from%20low-level%20attributes%20to%20high-level%20concepts.%20Autoencoders%20represent%20a%20classical%20and%20long-standing%20paradigm%20for%20learning%20representations%20from%20pixels%20or%20other%20raw%20inputs.%20In%20this%20work%2C%20we%20demonstrate%20that%20autoencoder-based%20self-supervised%20learning%20remains%20competitive%20today%20and%20can%20produce%20strong%20representations%20for%20downstream%20tasks%2C%20while%20remaining%20simple%2C%20stable%2C%20and%20efficient.%20Our%20model%2C%20codenamed%20%22Pixio%22%2C%20is%20an%20enhanced%20masked%20autoencoder%20%28MAE%29%20with%20more%20challenging%20pre-training%20tasks%20and%20more%20capable%20architectures.%20The%20model%20is%20trained%20on%202B%20web-crawled%20images%20with%20a%20self-curation%20strategy%20with%20minimal%20human%20curation.%20Pixio%20performs%20competitively%20across%20a%20wide%20range%20of%20downstream%20tasks%20in%20the%20wild%2C%20including%20monocular%20depth%20estimation%20%28e.g.%2C%20Depth%20Anything%29%2C%20feed-forward%203D%20reconstruction%20%28i.e.%2C%20MapAnything%29%2C%20semantic%20segmentation%2C%20and%20robot%20learning%2C%20outperforming%20or%20matching%20DINOv3%20trained%20at%20similar%20scales.%20Our%20results%20suggest%20that%20pixel-space%20self-supervised%20learning%20can%20serve%20as%20a%20promising%20alternative%20and%20a%20complement%20to%20latent-space%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520Pursuit%2520of%2520Pixel%2520Supervision%2520for%2520Visual%2520Pre-training%26entry.906535625%3DLihe%2520Yang%2520and%2520Shang-Wen%2520Li%2520and%2520Yang%2520Li%2520and%2520Xinjie%2520Lei%2520and%2520Dong%2520Wang%2520and%2520Abdelrahman%2520Mohamed%2520and%2520Hengshuang%2520Zhao%2520and%2520Hu%2520Xu%26entry.1292438233%3DAt%2520the%2520most%2520basic%2520level%252C%2520pixels%2520are%2520the%2520source%2520of%2520the%2520visual%2520information%2520through%2520which%2520we%2520perceive%2520the%2520world.%2520Pixels%2520contain%2520information%2520at%2520all%2520levels%252C%2520ranging%2520from%2520low-level%2520attributes%2520to%2520high-level%2520concepts.%2520Autoencoders%2520represent%2520a%2520classical%2520and%2520long-standing%2520paradigm%2520for%2520learning%2520representations%2520from%2520pixels%2520or%2520other%2520raw%2520inputs.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520autoencoder-based%2520self-supervised%2520learning%2520remains%2520competitive%2520today%2520and%2520can%2520produce%2520strong%2520representations%2520for%2520downstream%2520tasks%252C%2520while%2520remaining%2520simple%252C%2520stable%252C%2520and%2520efficient.%2520Our%2520model%252C%2520codenamed%2520%2522Pixio%2522%252C%2520is%2520an%2520enhanced%2520masked%2520autoencoder%2520%2528MAE%2529%2520with%2520more%2520challenging%2520pre-training%2520tasks%2520and%2520more%2520capable%2520architectures.%2520The%2520model%2520is%2520trained%2520on%25202B%2520web-crawled%2520images%2520with%2520a%2520self-curation%2520strategy%2520with%2520minimal%2520human%2520curation.%2520Pixio%2520performs%2520competitively%2520across%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%2520in%2520the%2520wild%252C%2520including%2520monocular%2520depth%2520estimation%2520%2528e.g.%252C%2520Depth%2520Anything%2529%252C%2520feed-forward%25203D%2520reconstruction%2520%2528i.e.%252C%2520MapAnything%2529%252C%2520semantic%2520segmentation%252C%2520and%2520robot%2520learning%252C%2520outperforming%2520or%2520matching%2520DINOv3%2520trained%2520at%2520similar%2520scales.%2520Our%2520results%2520suggest%2520that%2520pixel-space%2520self-supervised%2520learning%2520can%2520serve%2520as%2520a%2520promising%2520alternative%2520and%2520a%2520complement%2520to%2520latent-space%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Pursuit%20of%20Pixel%20Supervision%20for%20Visual%20Pre-training&entry.906535625=Lihe%20Yang%20and%20Shang-Wen%20Li%20and%20Yang%20Li%20and%20Xinjie%20Lei%20and%20Dong%20Wang%20and%20Abdelrahman%20Mohamed%20and%20Hengshuang%20Zhao%20and%20Hu%20Xu&entry.1292438233=At%20the%20most%20basic%20level%2C%20pixels%20are%20the%20source%20of%20the%20visual%20information%20through%20which%20we%20perceive%20the%20world.%20Pixels%20contain%20information%20at%20all%20levels%2C%20ranging%20from%20low-level%20attributes%20to%20high-level%20concepts.%20Autoencoders%20represent%20a%20classical%20and%20long-standing%20paradigm%20for%20learning%20representations%20from%20pixels%20or%20other%20raw%20inputs.%20In%20this%20work%2C%20we%20demonstrate%20that%20autoencoder-based%20self-supervised%20learning%20remains%20competitive%20today%20and%20can%20produce%20strong%20representations%20for%20downstream%20tasks%2C%20while%20remaining%20simple%2C%20stable%2C%20and%20efficient.%20Our%20model%2C%20codenamed%20%22Pixio%22%2C%20is%20an%20enhanced%20masked%20autoencoder%20%28MAE%29%20with%20more%20challenging%20pre-training%20tasks%20and%20more%20capable%20architectures.%20The%20model%20is%20trained%20on%202B%20web-crawled%20images%20with%20a%20self-curation%20strategy%20with%20minimal%20human%20curation.%20Pixio%20performs%20competitively%20across%20a%20wide%20range%20of%20downstream%20tasks%20in%20the%20wild%2C%20including%20monocular%20depth%20estimation%20%28e.g.%2C%20Depth%20Anything%29%2C%20feed-forward%203D%20reconstruction%20%28i.e.%2C%20MapAnything%29%2C%20semantic%20segmentation%2C%20and%20robot%20learning%2C%20outperforming%20or%20matching%20DINOv3%20trained%20at%20similar%20scales.%20Our%20results%20suggest%20that%20pixel-space%20self-supervised%20learning%20can%20serve%20as%20a%20promising%20alternative%20and%20a%20complement%20to%20latent-space%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.15715v1&entry.124074799=Read"},
{"title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?", "author": "Hongbo Zhao and Meng Wang and Fei Zhu and Wenzhuo Liu and Bolin Ni and Fanhu Zeng and Gaofeng Meng and Zhaoxiang Zhang", "abstract": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.", "link": "http://arxiv.org/abs/2512.15649v1", "date": "2025-12-17", "relevancy": 3.0581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTCBench%3A%20Can%20Vision-Language%20Models%20Understand%20Long%20Context%20with%20Vision-Text%20Compression%3F&body=Title%3A%20VTCBench%3A%20Can%20Vision-Language%20Models%20Understand%20Long%20Context%20with%20Vision-Text%20Compression%3F%0AAuthor%3A%20Hongbo%20Zhao%20and%20Meng%20Wang%20and%20Fei%20Zhu%20and%20Wenzhuo%20Liu%20and%20Bolin%20Ni%20and%20Fanhu%20Zeng%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20The%20computational%20and%20memory%20overheads%20associated%20with%20expanding%20the%20context%20window%20of%20LLMs%20severely%20limit%20their%20scalability.%20A%20noteworthy%20solution%20is%20vision-text%20compression%20%28VTC%29%2C%20exemplified%20by%20frameworks%20like%20DeepSeek-OCR%20and%20Glyph%2C%20which%20convert%20long%20texts%20into%20dense%202D%20visual%20representations%2C%20thereby%20achieving%20token%20compression%20ratios%20of%203x-20x.%20However%2C%20the%20impact%20of%20this%20high%20information%20density%20on%20the%20core%20long-context%20capabilities%20of%20vision-language%20models%20%28VLMs%29%20remains%20under-investigated.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20first%20benchmark%20for%20VTC%20and%20systematically%20assess%20the%20performance%20of%20VLMs%20across%20three%20long-context%20understanding%20settings%3A%20VTC-Retrieval%2C%20which%20evaluates%20the%20model%27s%20ability%20to%20retrieve%20and%20aggregate%20information%3B%20VTC-Reasoning%2C%20which%20requires%20models%20to%20infer%20latent%20associations%20to%20locate%20facts%20with%20minimal%20lexical%20overlap%3B%20and%20VTC-Memory%2C%20which%20measures%20comprehensive%20question%20answering%20within%20long-term%20dialogue%20memory.%20Furthermore%2C%20we%20establish%20the%20VTCBench-Wild%20to%20simulate%20diverse%20input%20scenarios.We%20comprehensively%20evaluate%20leading%20open-source%20and%20proprietary%20models%20on%20our%20benchmarks.%20The%20results%20indicate%20that%2C%20despite%20being%20able%20to%20decode%20textual%20information%20%28e.g.%2C%20OCR%29%20well%2C%20most%20VLMs%20exhibit%20a%20surprisingly%20poor%20long-context%20understanding%20ability%20with%20VTC-compressed%20information%2C%20failing%20to%20capture%20long%20associations%20or%20dependencies%20in%20the%20context.This%20study%20provides%20a%20deep%20understanding%20of%20VTC%20and%20serves%20as%20a%20foundation%20for%20designing%20more%20efficient%20and%20scalable%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTCBench%253A%2520Can%2520Vision-Language%2520Models%2520Understand%2520Long%2520Context%2520with%2520Vision-Text%2520Compression%253F%26entry.906535625%3DHongbo%2520Zhao%2520and%2520Meng%2520Wang%2520and%2520Fei%2520Zhu%2520and%2520Wenzhuo%2520Liu%2520and%2520Bolin%2520Ni%2520and%2520Fanhu%2520Zeng%2520and%2520Gaofeng%2520Meng%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3DThe%2520computational%2520and%2520memory%2520overheads%2520associated%2520with%2520expanding%2520the%2520context%2520window%2520of%2520LLMs%2520severely%2520limit%2520their%2520scalability.%2520A%2520noteworthy%2520solution%2520is%2520vision-text%2520compression%2520%2528VTC%2529%252C%2520exemplified%2520by%2520frameworks%2520like%2520DeepSeek-OCR%2520and%2520Glyph%252C%2520which%2520convert%2520long%2520texts%2520into%2520dense%25202D%2520visual%2520representations%252C%2520thereby%2520achieving%2520token%2520compression%2520ratios%2520of%25203x-20x.%2520However%252C%2520the%2520impact%2520of%2520this%2520high%2520information%2520density%2520on%2520the%2520core%2520long-context%2520capabilities%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520remains%2520under-investigated.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520first%2520benchmark%2520for%2520VTC%2520and%2520systematically%2520assess%2520the%2520performance%2520of%2520VLMs%2520across%2520three%2520long-context%2520understanding%2520settings%253A%2520VTC-Retrieval%252C%2520which%2520evaluates%2520the%2520model%2527s%2520ability%2520to%2520retrieve%2520and%2520aggregate%2520information%253B%2520VTC-Reasoning%252C%2520which%2520requires%2520models%2520to%2520infer%2520latent%2520associations%2520to%2520locate%2520facts%2520with%2520minimal%2520lexical%2520overlap%253B%2520and%2520VTC-Memory%252C%2520which%2520measures%2520comprehensive%2520question%2520answering%2520within%2520long-term%2520dialogue%2520memory.%2520Furthermore%252C%2520we%2520establish%2520the%2520VTCBench-Wild%2520to%2520simulate%2520diverse%2520input%2520scenarios.We%2520comprehensively%2520evaluate%2520leading%2520open-source%2520and%2520proprietary%2520models%2520on%2520our%2520benchmarks.%2520The%2520results%2520indicate%2520that%252C%2520despite%2520being%2520able%2520to%2520decode%2520textual%2520information%2520%2528e.g.%252C%2520OCR%2529%2520well%252C%2520most%2520VLMs%2520exhibit%2520a%2520surprisingly%2520poor%2520long-context%2520understanding%2520ability%2520with%2520VTC-compressed%2520information%252C%2520failing%2520to%2520capture%2520long%2520associations%2520or%2520dependencies%2520in%2520the%2520context.This%2520study%2520provides%2520a%2520deep%2520understanding%2520of%2520VTC%2520and%2520serves%2520as%2520a%2520foundation%2520for%2520designing%2520more%2520efficient%2520and%2520scalable%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTCBench%3A%20Can%20Vision-Language%20Models%20Understand%20Long%20Context%20with%20Vision-Text%20Compression%3F&entry.906535625=Hongbo%20Zhao%20and%20Meng%20Wang%20and%20Fei%20Zhu%20and%20Wenzhuo%20Liu%20and%20Bolin%20Ni%20and%20Fanhu%20Zeng%20and%20Gaofeng%20Meng%20and%20Zhaoxiang%20Zhang&entry.1292438233=The%20computational%20and%20memory%20overheads%20associated%20with%20expanding%20the%20context%20window%20of%20LLMs%20severely%20limit%20their%20scalability.%20A%20noteworthy%20solution%20is%20vision-text%20compression%20%28VTC%29%2C%20exemplified%20by%20frameworks%20like%20DeepSeek-OCR%20and%20Glyph%2C%20which%20convert%20long%20texts%20into%20dense%202D%20visual%20representations%2C%20thereby%20achieving%20token%20compression%20ratios%20of%203x-20x.%20However%2C%20the%20impact%20of%20this%20high%20information%20density%20on%20the%20core%20long-context%20capabilities%20of%20vision-language%20models%20%28VLMs%29%20remains%20under-investigated.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20first%20benchmark%20for%20VTC%20and%20systematically%20assess%20the%20performance%20of%20VLMs%20across%20three%20long-context%20understanding%20settings%3A%20VTC-Retrieval%2C%20which%20evaluates%20the%20model%27s%20ability%20to%20retrieve%20and%20aggregate%20information%3B%20VTC-Reasoning%2C%20which%20requires%20models%20to%20infer%20latent%20associations%20to%20locate%20facts%20with%20minimal%20lexical%20overlap%3B%20and%20VTC-Memory%2C%20which%20measures%20comprehensive%20question%20answering%20within%20long-term%20dialogue%20memory.%20Furthermore%2C%20we%20establish%20the%20VTCBench-Wild%20to%20simulate%20diverse%20input%20scenarios.We%20comprehensively%20evaluate%20leading%20open-source%20and%20proprietary%20models%20on%20our%20benchmarks.%20The%20results%20indicate%20that%2C%20despite%20being%20able%20to%20decode%20textual%20information%20%28e.g.%2C%20OCR%29%20well%2C%20most%20VLMs%20exhibit%20a%20surprisingly%20poor%20long-context%20understanding%20ability%20with%20VTC-compressed%20information%2C%20failing%20to%20capture%20long%20associations%20or%20dependencies%20in%20the%20context.This%20study%20provides%20a%20deep%20understanding%20of%20VTC%20and%20serves%20as%20a%20foundation%20for%20designing%20more%20efficient%20and%20scalable%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.15649v1&entry.124074799=Read"},
{"title": "MMGR: Multi-Modal Generative Reasoning", "author": "Zefan Cai and Haoyi Qiu and Tianyi Ma and Haozhe Zhao and Gengze Zhou and Kung-Hsiang Huang and Parisa Kordjamshidi and Minjia Zhang and Wen Xiao and Jiuxiang Gu and Nanyun Peng and Junjie Hu", "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "link": "http://arxiv.org/abs/2512.14691v2", "date": "2025-12-17", "relevancy": 3.0383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6223}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6033}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMGR%3A%20Multi-Modal%20Generative%20Reasoning&body=Title%3A%20MMGR%3A%20Multi-Modal%20Generative%20Reasoning%0AAuthor%3A%20Zefan%20Cai%20and%20Haoyi%20Qiu%20and%20Tianyi%20Ma%20and%20Haozhe%20Zhao%20and%20Gengze%20Zhou%20and%20Kung-Hsiang%20Huang%20and%20Parisa%20Kordjamshidi%20and%20Minjia%20Zhang%20and%20Wen%20Xiao%20and%20Jiuxiang%20Gu%20and%20Nanyun%20Peng%20and%20Junjie%20Hu%0AAbstract%3A%20Video%20foundation%20models%20generate%20visually%20realistic%20and%20temporally%20coherent%20content%2C%20but%20their%20reliability%20as%20world%20simulators%20depends%20on%20whether%20they%20capture%20physical%2C%20logical%2C%20and%20spatial%20constraints.%20Existing%20metrics%20such%20as%20Frechet%20Video%20Distance%20%28FVD%29%20emphasize%20perceptual%20quality%20and%20overlook%20reasoning%20failures%2C%20including%20violations%20of%20causality%2C%20physics%2C%20and%20global%20consistency.%20We%20introduce%20MMGR%20%28Multi-Modal%20Generative%20Reasoning%20Evaluation%20and%20Benchmark%29%2C%20a%20principled%20evaluation%20framework%20based%20on%20five%20reasoning%20abilities%3A%20Physical%2C%20Logical%2C%203D%20Spatial%2C%202D%20Spatial%2C%20and%20Temporal.%20MMGR%20evaluates%20generative%20reasoning%20across%20three%20domains%3A%20Abstract%20Reasoning%20%28ARC-AGI%2C%20Sudoku%29%2C%20Embodied%20Navigation%20%28real-world%203D%20navigation%20and%20localization%29%2C%20and%20Physical%20Commonsense%20%28sports%20and%20compositional%20interactions%29.%20MMGR%20applies%20fine-grained%20metrics%20that%20require%20holistic%20correctness%20across%20both%20video%20and%20image%20generation.%20We%20benchmark%20leading%20video%20models%20%28Veo-3%2C%20Sora-2%2C%20Wan-2.2%29%20and%20image%20models%20%28Nano-banana%2C%20Nano-banana%20Pro%2C%20GPT-4o-image%2C%20Qwen-image%29%2C%20revealing%20strong%20performance%20gaps%20across%20domains.%20Models%20show%20moderate%20success%20on%20Physical%20Commonsense%20tasks%20but%20perform%20poorly%20on%20Abstract%20Reasoning%20%28below%2010%20percent%20accuracy%20on%20ARC-AGI%29%20and%20struggle%20with%20long-horizon%20spatial%20planning%20in%20embodied%20settings.%20Our%20analysis%20highlights%20key%20limitations%20in%20current%20models%2C%20including%20overreliance%20on%20perceptual%20data%2C%20weak%20global%20state%20consistency%2C%20and%20objectives%20that%20reward%20visual%20plausibility%20over%20causal%20correctness.%20MMGR%20offers%20a%20unified%20diagnostic%20benchmark%20and%20a%20path%20toward%20reasoning-aware%20generative%20world%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMGR%253A%2520Multi-Modal%2520Generative%2520Reasoning%26entry.906535625%3DZefan%2520Cai%2520and%2520Haoyi%2520Qiu%2520and%2520Tianyi%2520Ma%2520and%2520Haozhe%2520Zhao%2520and%2520Gengze%2520Zhou%2520and%2520Kung-Hsiang%2520Huang%2520and%2520Parisa%2520Kordjamshidi%2520and%2520Minjia%2520Zhang%2520and%2520Wen%2520Xiao%2520and%2520Jiuxiang%2520Gu%2520and%2520Nanyun%2520Peng%2520and%2520Junjie%2520Hu%26entry.1292438233%3DVideo%2520foundation%2520models%2520generate%2520visually%2520realistic%2520and%2520temporally%2520coherent%2520content%252C%2520but%2520their%2520reliability%2520as%2520world%2520simulators%2520depends%2520on%2520whether%2520they%2520capture%2520physical%252C%2520logical%252C%2520and%2520spatial%2520constraints.%2520Existing%2520metrics%2520such%2520as%2520Frechet%2520Video%2520Distance%2520%2528FVD%2529%2520emphasize%2520perceptual%2520quality%2520and%2520overlook%2520reasoning%2520failures%252C%2520including%2520violations%2520of%2520causality%252C%2520physics%252C%2520and%2520global%2520consistency.%2520We%2520introduce%2520MMGR%2520%2528Multi-Modal%2520Generative%2520Reasoning%2520Evaluation%2520and%2520Benchmark%2529%252C%2520a%2520principled%2520evaluation%2520framework%2520based%2520on%2520five%2520reasoning%2520abilities%253A%2520Physical%252C%2520Logical%252C%25203D%2520Spatial%252C%25202D%2520Spatial%252C%2520and%2520Temporal.%2520MMGR%2520evaluates%2520generative%2520reasoning%2520across%2520three%2520domains%253A%2520Abstract%2520Reasoning%2520%2528ARC-AGI%252C%2520Sudoku%2529%252C%2520Embodied%2520Navigation%2520%2528real-world%25203D%2520navigation%2520and%2520localization%2529%252C%2520and%2520Physical%2520Commonsense%2520%2528sports%2520and%2520compositional%2520interactions%2529.%2520MMGR%2520applies%2520fine-grained%2520metrics%2520that%2520require%2520holistic%2520correctness%2520across%2520both%2520video%2520and%2520image%2520generation.%2520We%2520benchmark%2520leading%2520video%2520models%2520%2528Veo-3%252C%2520Sora-2%252C%2520Wan-2.2%2529%2520and%2520image%2520models%2520%2528Nano-banana%252C%2520Nano-banana%2520Pro%252C%2520GPT-4o-image%252C%2520Qwen-image%2529%252C%2520revealing%2520strong%2520performance%2520gaps%2520across%2520domains.%2520Models%2520show%2520moderate%2520success%2520on%2520Physical%2520Commonsense%2520tasks%2520but%2520perform%2520poorly%2520on%2520Abstract%2520Reasoning%2520%2528below%252010%2520percent%2520accuracy%2520on%2520ARC-AGI%2529%2520and%2520struggle%2520with%2520long-horizon%2520spatial%2520planning%2520in%2520embodied%2520settings.%2520Our%2520analysis%2520highlights%2520key%2520limitations%2520in%2520current%2520models%252C%2520including%2520overreliance%2520on%2520perceptual%2520data%252C%2520weak%2520global%2520state%2520consistency%252C%2520and%2520objectives%2520that%2520reward%2520visual%2520plausibility%2520over%2520causal%2520correctness.%2520MMGR%2520offers%2520a%2520unified%2520diagnostic%2520benchmark%2520and%2520a%2520path%2520toward%2520reasoning-aware%2520generative%2520world%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMGR%3A%20Multi-Modal%20Generative%20Reasoning&entry.906535625=Zefan%20Cai%20and%20Haoyi%20Qiu%20and%20Tianyi%20Ma%20and%20Haozhe%20Zhao%20and%20Gengze%20Zhou%20and%20Kung-Hsiang%20Huang%20and%20Parisa%20Kordjamshidi%20and%20Minjia%20Zhang%20and%20Wen%20Xiao%20and%20Jiuxiang%20Gu%20and%20Nanyun%20Peng%20and%20Junjie%20Hu&entry.1292438233=Video%20foundation%20models%20generate%20visually%20realistic%20and%20temporally%20coherent%20content%2C%20but%20their%20reliability%20as%20world%20simulators%20depends%20on%20whether%20they%20capture%20physical%2C%20logical%2C%20and%20spatial%20constraints.%20Existing%20metrics%20such%20as%20Frechet%20Video%20Distance%20%28FVD%29%20emphasize%20perceptual%20quality%20and%20overlook%20reasoning%20failures%2C%20including%20violations%20of%20causality%2C%20physics%2C%20and%20global%20consistency.%20We%20introduce%20MMGR%20%28Multi-Modal%20Generative%20Reasoning%20Evaluation%20and%20Benchmark%29%2C%20a%20principled%20evaluation%20framework%20based%20on%20five%20reasoning%20abilities%3A%20Physical%2C%20Logical%2C%203D%20Spatial%2C%202D%20Spatial%2C%20and%20Temporal.%20MMGR%20evaluates%20generative%20reasoning%20across%20three%20domains%3A%20Abstract%20Reasoning%20%28ARC-AGI%2C%20Sudoku%29%2C%20Embodied%20Navigation%20%28real-world%203D%20navigation%20and%20localization%29%2C%20and%20Physical%20Commonsense%20%28sports%20and%20compositional%20interactions%29.%20MMGR%20applies%20fine-grained%20metrics%20that%20require%20holistic%20correctness%20across%20both%20video%20and%20image%20generation.%20We%20benchmark%20leading%20video%20models%20%28Veo-3%2C%20Sora-2%2C%20Wan-2.2%29%20and%20image%20models%20%28Nano-banana%2C%20Nano-banana%20Pro%2C%20GPT-4o-image%2C%20Qwen-image%29%2C%20revealing%20strong%20performance%20gaps%20across%20domains.%20Models%20show%20moderate%20success%20on%20Physical%20Commonsense%20tasks%20but%20perform%20poorly%20on%20Abstract%20Reasoning%20%28below%2010%20percent%20accuracy%20on%20ARC-AGI%29%20and%20struggle%20with%20long-horizon%20spatial%20planning%20in%20embodied%20settings.%20Our%20analysis%20highlights%20key%20limitations%20in%20current%20models%2C%20including%20overreliance%20on%20perceptual%20data%2C%20weak%20global%20state%20consistency%2C%20and%20objectives%20that%20reward%20visual%20plausibility%20over%20causal%20correctness.%20MMGR%20offers%20a%20unified%20diagnostic%20benchmark%20and%20a%20path%20toward%20reasoning-aware%20generative%20world%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.14691v2&entry.124074799=Read"},
{"title": "An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain", "author": "Jo\u00e3o Daniel Silva and Joao Magalhaes and Devis Tuia and Bruno Martins", "abstract": "The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.", "link": "http://arxiv.org/abs/2512.15531v1", "date": "2025-12-17", "relevancy": 2.9692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6191}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20and%20Effective%20Encoder%20Model%20for%20Vision%20and%20Language%20Tasks%20in%20the%20Remote%20Sensing%20Domain&body=Title%3A%20An%20Efficient%20and%20Effective%20Encoder%20Model%20for%20Vision%20and%20Language%20Tasks%20in%20the%20Remote%20Sensing%20Domain%0AAuthor%3A%20Jo%C3%A3o%20Daniel%20Silva%20and%20Joao%20Magalhaes%20and%20Devis%20Tuia%20and%20Bruno%20Martins%0AAbstract%3A%20The%20remote%20sensing%20community%20has%20recently%20seen%20the%20emergence%20of%20methods%20based%20on%20Large%20Vision%20and%20Language%20Models%20%28LVLMs%29%20that%20can%20address%20multiple%20tasks%20at%20the%20intersection%20of%20computer%20vision%20and%20natural%20language%20processing.%20To%20fully%20exploit%20the%20potential%20of%20such%20models%2C%20a%20significant%20focus%20has%20been%20given%20to%20the%20collection%20of%20large%20amounts%20of%20training%20data%20that%20cover%20multiple%20remote%20sensing-specific%20tasks%2C%20such%20as%20image%20captioning%20or%20visual%20question%20answering.%20However%2C%20the%20cost%20of%20using%20and%20training%20LVLMs%20is%20high%2C%20due%20to%20the%20large%20number%20of%20parameters.%20While%20multiple%20parameter-efficient%20adaptation%20techniques%20have%20been%20explored%2C%20the%20computational%20costs%20of%20training%20and%20inference%20with%20these%20models%20can%20remain%20prohibitive%20for%20most%20institutions.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20encoder-only%20architectures%20and%20propose%20a%20model%20that%20can%20effectively%20address%20multi-task%20learning%20while%20remaining%20compact%20in%20terms%20of%20the%20number%20of%20parameters.%20In%20particular%2C%20our%20model%20tackles%20combinations%20of%20tasks%20that%20are%20not%20typically%20explored%20in%20a%20unified%20model%3A%20the%20generation%20of%20text%20from%20remote%20sensing%20images%20and%20cross-modal%20retrieval.%20The%20results%20of%20our%20GeoMELT%20model%20-%20named%20from%20Multi-task%20Efficient%20Learning%20Transformer%20-%20in%20established%20benchmarks%20confirm%20the%20efficacy%20and%20efficiency%20of%20the%20proposed%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520and%2520Effective%2520Encoder%2520Model%2520for%2520Vision%2520and%2520Language%2520Tasks%2520in%2520the%2520Remote%2520Sensing%2520Domain%26entry.906535625%3DJo%25C3%25A3o%2520Daniel%2520Silva%2520and%2520Joao%2520Magalhaes%2520and%2520Devis%2520Tuia%2520and%2520Bruno%2520Martins%26entry.1292438233%3DThe%2520remote%2520sensing%2520community%2520has%2520recently%2520seen%2520the%2520emergence%2520of%2520methods%2520based%2520on%2520Large%2520Vision%2520and%2520Language%2520Models%2520%2528LVLMs%2529%2520that%2520can%2520address%2520multiple%2520tasks%2520at%2520the%2520intersection%2520of%2520computer%2520vision%2520and%2520natural%2520language%2520processing.%2520To%2520fully%2520exploit%2520the%2520potential%2520of%2520such%2520models%252C%2520a%2520significant%2520focus%2520has%2520been%2520given%2520to%2520the%2520collection%2520of%2520large%2520amounts%2520of%2520training%2520data%2520that%2520cover%2520multiple%2520remote%2520sensing-specific%2520tasks%252C%2520such%2520as%2520image%2520captioning%2520or%2520visual%2520question%2520answering.%2520However%252C%2520the%2520cost%2520of%2520using%2520and%2520training%2520LVLMs%2520is%2520high%252C%2520due%2520to%2520the%2520large%2520number%2520of%2520parameters.%2520While%2520multiple%2520parameter-efficient%2520adaptation%2520techniques%2520have%2520been%2520explored%252C%2520the%2520computational%2520costs%2520of%2520training%2520and%2520inference%2520with%2520these%2520models%2520can%2520remain%2520prohibitive%2520for%2520most%2520institutions.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520use%2520of%2520encoder-only%2520architectures%2520and%2520propose%2520a%2520model%2520that%2520can%2520effectively%2520address%2520multi-task%2520learning%2520while%2520remaining%2520compact%2520in%2520terms%2520of%2520the%2520number%2520of%2520parameters.%2520In%2520particular%252C%2520our%2520model%2520tackles%2520combinations%2520of%2520tasks%2520that%2520are%2520not%2520typically%2520explored%2520in%2520a%2520unified%2520model%253A%2520the%2520generation%2520of%2520text%2520from%2520remote%2520sensing%2520images%2520and%2520cross-modal%2520retrieval.%2520The%2520results%2520of%2520our%2520GeoMELT%2520model%2520-%2520named%2520from%2520Multi-task%2520Efficient%2520Learning%2520Transformer%2520-%2520in%2520established%2520benchmarks%2520confirm%2520the%2520efficacy%2520and%2520efficiency%2520of%2520the%2520proposed%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20and%20Effective%20Encoder%20Model%20for%20Vision%20and%20Language%20Tasks%20in%20the%20Remote%20Sensing%20Domain&entry.906535625=Jo%C3%A3o%20Daniel%20Silva%20and%20Joao%20Magalhaes%20and%20Devis%20Tuia%20and%20Bruno%20Martins&entry.1292438233=The%20remote%20sensing%20community%20has%20recently%20seen%20the%20emergence%20of%20methods%20based%20on%20Large%20Vision%20and%20Language%20Models%20%28LVLMs%29%20that%20can%20address%20multiple%20tasks%20at%20the%20intersection%20of%20computer%20vision%20and%20natural%20language%20processing.%20To%20fully%20exploit%20the%20potential%20of%20such%20models%2C%20a%20significant%20focus%20has%20been%20given%20to%20the%20collection%20of%20large%20amounts%20of%20training%20data%20that%20cover%20multiple%20remote%20sensing-specific%20tasks%2C%20such%20as%20image%20captioning%20or%20visual%20question%20answering.%20However%2C%20the%20cost%20of%20using%20and%20training%20LVLMs%20is%20high%2C%20due%20to%20the%20large%20number%20of%20parameters.%20While%20multiple%20parameter-efficient%20adaptation%20techniques%20have%20been%20explored%2C%20the%20computational%20costs%20of%20training%20and%20inference%20with%20these%20models%20can%20remain%20prohibitive%20for%20most%20institutions.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20encoder-only%20architectures%20and%20propose%20a%20model%20that%20can%20effectively%20address%20multi-task%20learning%20while%20remaining%20compact%20in%20terms%20of%20the%20number%20of%20parameters.%20In%20particular%2C%20our%20model%20tackles%20combinations%20of%20tasks%20that%20are%20not%20typically%20explored%20in%20a%20unified%20model%3A%20the%20generation%20of%20text%20from%20remote%20sensing%20images%20and%20cross-modal%20retrieval.%20The%20results%20of%20our%20GeoMELT%20model%20-%20named%20from%20Multi-task%20Efficient%20Learning%20Transformer%20-%20in%20established%20benchmarks%20confirm%20the%20efficacy%20and%20efficiency%20of%20the%20proposed%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2512.15531v1&entry.124074799=Read"},
{"title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking", "author": "Lihong Wang and Liangqi Li and Weiwei Feng and Jiamin Wu and Changtao Miao and Tieru Wu and Rui Ma and Bo Zhang and Zhe Li", "abstract": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model. The resulting ViRC-7B model achieves a 18.8% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.", "link": "http://arxiv.org/abs/2512.14654v2", "date": "2025-12-17", "relevancy": 2.936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking&body=Title%3A%20ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking%0AAuthor%3A%20Lihong%20Wang%20and%20Liangqi%20Li%20and%20Weiwei%20Feng%20and%20Jiamin%20Wu%20and%20Changtao%20Miao%20and%20Tieru%20Wu%20and%20Rui%20Ma%20and%20Bo%20Zhang%20and%20Zhe%20Li%0AAbstract%3A%20CoT%20has%20significantly%20enhanced%20the%20reasoning%20ability%20of%20LLMs%20while%20it%20faces%20challenges%20when%20extended%20to%20multimodal%20domains%2C%20particularly%20in%20mathematical%20tasks.%20Existing%20MLLMs%20typically%20perform%20textual%20reasoning%20solely%20from%20a%20single%20static%20mathematical%20image%2C%20overlooking%20dynamic%20visual%20acquisition%20during%20reasoning.%20In%20contrast%2C%20humans%20repeatedly%20examine%20visual%20image%20and%20employ%20step-by-step%20reasoning%20to%20prove%20intermediate%20propositions.%20This%20strategy%20of%20decomposing%20the%20problem-solving%20process%20into%20key%20logical%20nodes%20adheres%20to%20Miller%27s%20Law%20in%20cognitive%20science.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20ViRC%20framework%20for%20multimodal%20mathematical%20tasks%2C%20introducing%20a%20Reason%20Chunking%20mechanism%20that%20structures%20multimodal%20mathematical%20CoT%20into%20consecutive%20Critical%20Reasoning%20Units%20%28CRUs%29%20to%20simulate%20human%20expert%20problem-solving%20patterns.%20CRUs%20ensure%20intra-unit%20textual%20coherence%20for%20intermediate%20proposition%20verification%20while%20integrating%20visual%20information%20across%20units%20to%20generate%20subsequent%20propositions%20and%20support%20structured%20reasoning.%20To%20this%20end%2C%20we%20present%20CRUX%20dataset%20by%20using%20three%20visual%20tools%20and%20four%20reasoning%20patterns%20to%20provide%20explicitly%20annotated%20CRUs%20across%20multiple%20reasoning%20paths%20for%20each%20mathematical%20problem.%20Leveraging%20the%20CRUX%20dataset%2C%20we%20propose%20a%20progressive%20training%20strategy%20inspired%20by%20human%20cognitive%20learning%2C%20which%20includes%20Instructional%20SFT%2C%20Practice%20SFT%2C%20and%20Strategic%20RL%2C%20aimed%20at%20further%20strengthening%20the%20Reason%20Chunking%20ability%20of%20the%20model.%20The%20resulting%20ViRC-7B%20model%20achieves%20a%2018.8%25%20average%20improvement%20over%20baselines%20across%20multiple%20mathematical%20benchmarks.%20Code%20is%20available%20at%20https%3A//github.com/Leon-LihongWang/ViRC.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViRC%253A%2520Enhancing%2520Visual%2520Interleaved%2520Mathematical%2520CoT%2520with%2520Reason%2520Chunking%26entry.906535625%3DLihong%2520Wang%2520and%2520Liangqi%2520Li%2520and%2520Weiwei%2520Feng%2520and%2520Jiamin%2520Wu%2520and%2520Changtao%2520Miao%2520and%2520Tieru%2520Wu%2520and%2520Rui%2520Ma%2520and%2520Bo%2520Zhang%2520and%2520Zhe%2520Li%26entry.1292438233%3DCoT%2520has%2520significantly%2520enhanced%2520the%2520reasoning%2520ability%2520of%2520LLMs%2520while%2520it%2520faces%2520challenges%2520when%2520extended%2520to%2520multimodal%2520domains%252C%2520particularly%2520in%2520mathematical%2520tasks.%2520Existing%2520MLLMs%2520typically%2520perform%2520textual%2520reasoning%2520solely%2520from%2520a%2520single%2520static%2520mathematical%2520image%252C%2520overlooking%2520dynamic%2520visual%2520acquisition%2520during%2520reasoning.%2520In%2520contrast%252C%2520humans%2520repeatedly%2520examine%2520visual%2520image%2520and%2520employ%2520step-by-step%2520reasoning%2520to%2520prove%2520intermediate%2520propositions.%2520This%2520strategy%2520of%2520decomposing%2520the%2520problem-solving%2520process%2520into%2520key%2520logical%2520nodes%2520adheres%2520to%2520Miller%2527s%2520Law%2520in%2520cognitive%2520science.%2520Inspired%2520by%2520this%2520insight%252C%2520we%2520propose%2520a%2520ViRC%2520framework%2520for%2520multimodal%2520mathematical%2520tasks%252C%2520introducing%2520a%2520Reason%2520Chunking%2520mechanism%2520that%2520structures%2520multimodal%2520mathematical%2520CoT%2520into%2520consecutive%2520Critical%2520Reasoning%2520Units%2520%2528CRUs%2529%2520to%2520simulate%2520human%2520expert%2520problem-solving%2520patterns.%2520CRUs%2520ensure%2520intra-unit%2520textual%2520coherence%2520for%2520intermediate%2520proposition%2520verification%2520while%2520integrating%2520visual%2520information%2520across%2520units%2520to%2520generate%2520subsequent%2520propositions%2520and%2520support%2520structured%2520reasoning.%2520To%2520this%2520end%252C%2520we%2520present%2520CRUX%2520dataset%2520by%2520using%2520three%2520visual%2520tools%2520and%2520four%2520reasoning%2520patterns%2520to%2520provide%2520explicitly%2520annotated%2520CRUs%2520across%2520multiple%2520reasoning%2520paths%2520for%2520each%2520mathematical%2520problem.%2520Leveraging%2520the%2520CRUX%2520dataset%252C%2520we%2520propose%2520a%2520progressive%2520training%2520strategy%2520inspired%2520by%2520human%2520cognitive%2520learning%252C%2520which%2520includes%2520Instructional%2520SFT%252C%2520Practice%2520SFT%252C%2520and%2520Strategic%2520RL%252C%2520aimed%2520at%2520further%2520strengthening%2520the%2520Reason%2520Chunking%2520ability%2520of%2520the%2520model.%2520The%2520resulting%2520ViRC-7B%2520model%2520achieves%2520a%252018.8%2525%2520average%2520improvement%2520over%2520baselines%2520across%2520multiple%2520mathematical%2520benchmarks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Leon-LihongWang/ViRC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking&entry.906535625=Lihong%20Wang%20and%20Liangqi%20Li%20and%20Weiwei%20Feng%20and%20Jiamin%20Wu%20and%20Changtao%20Miao%20and%20Tieru%20Wu%20and%20Rui%20Ma%20and%20Bo%20Zhang%20and%20Zhe%20Li&entry.1292438233=CoT%20has%20significantly%20enhanced%20the%20reasoning%20ability%20of%20LLMs%20while%20it%20faces%20challenges%20when%20extended%20to%20multimodal%20domains%2C%20particularly%20in%20mathematical%20tasks.%20Existing%20MLLMs%20typically%20perform%20textual%20reasoning%20solely%20from%20a%20single%20static%20mathematical%20image%2C%20overlooking%20dynamic%20visual%20acquisition%20during%20reasoning.%20In%20contrast%2C%20humans%20repeatedly%20examine%20visual%20image%20and%20employ%20step-by-step%20reasoning%20to%20prove%20intermediate%20propositions.%20This%20strategy%20of%20decomposing%20the%20problem-solving%20process%20into%20key%20logical%20nodes%20adheres%20to%20Miller%27s%20Law%20in%20cognitive%20science.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20ViRC%20framework%20for%20multimodal%20mathematical%20tasks%2C%20introducing%20a%20Reason%20Chunking%20mechanism%20that%20structures%20multimodal%20mathematical%20CoT%20into%20consecutive%20Critical%20Reasoning%20Units%20%28CRUs%29%20to%20simulate%20human%20expert%20problem-solving%20patterns.%20CRUs%20ensure%20intra-unit%20textual%20coherence%20for%20intermediate%20proposition%20verification%20while%20integrating%20visual%20information%20across%20units%20to%20generate%20subsequent%20propositions%20and%20support%20structured%20reasoning.%20To%20this%20end%2C%20we%20present%20CRUX%20dataset%20by%20using%20three%20visual%20tools%20and%20four%20reasoning%20patterns%20to%20provide%20explicitly%20annotated%20CRUs%20across%20multiple%20reasoning%20paths%20for%20each%20mathematical%20problem.%20Leveraging%20the%20CRUX%20dataset%2C%20we%20propose%20a%20progressive%20training%20strategy%20inspired%20by%20human%20cognitive%20learning%2C%20which%20includes%20Instructional%20SFT%2C%20Practice%20SFT%2C%20and%20Strategic%20RL%2C%20aimed%20at%20further%20strengthening%20the%20Reason%20Chunking%20ability%20of%20the%20model.%20The%20resulting%20ViRC-7B%20model%20achieves%20a%2018.8%25%20average%20improvement%20over%20baselines%20across%20multiple%20mathematical%20benchmarks.%20Code%20is%20available%20at%20https%3A//github.com/Leon-LihongWang/ViRC.&entry.1838667208=http%3A//arxiv.org/abs/2512.14654v2&entry.124074799=Read"},
{"title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics", "author": "Junjie Chen and Fei Wang and Zhihao Huang and Qing Zhou and Kun Li and Dan Guo and Linfeng Zhang and Xun Yang", "abstract": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fr\u00e9chet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.", "link": "http://arxiv.org/abs/2512.15340v1", "date": "2025-12-17", "relevancy": 2.9358, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6139}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5738}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Seamless%20Interaction%3A%20Causal%20Turn-Level%20Modeling%20of%20Interactive%203D%20Conversational%20Head%20Dynamics&body=Title%3A%20Towards%20Seamless%20Interaction%3A%20Causal%20Turn-Level%20Modeling%20of%20Interactive%203D%20Conversational%20Head%20Dynamics%0AAuthor%3A%20Junjie%20Chen%20and%20Fei%20Wang%20and%20Zhihao%20Huang%20and%20Qing%20Zhou%20and%20Kun%20Li%20and%20Dan%20Guo%20and%20Linfeng%20Zhang%20and%20Xun%20Yang%0AAbstract%3A%20Human%20conversation%20involves%20continuous%20exchanges%20of%20speech%20and%20nonverbal%20cues%20such%20as%20head%20nods%2C%20gaze%20shifts%2C%20and%20facial%20expressions%20that%20convey%20attention%20and%20emotion.%20Modeling%20these%20bidirectional%20dynamics%20in%203D%20is%20essential%20for%20building%20expressive%20avatars%20and%20interactive%20robots.%20However%2C%20existing%20frameworks%20often%20treat%20talking%20and%20listening%20as%20independent%20processes%20or%20rely%20on%20non-causal%20full-sequence%20modeling%2C%20hindering%20temporal%20coherence%20across%20turns.%20We%20present%20TIMAR%20%28Turn-level%20Interleaved%20Masked%20AutoRegression%29%2C%20a%20causal%20framework%20for%203D%20conversational%20head%20generation%20that%20models%20dialogue%20as%20interleaved%20audio-visual%20contexts.%20It%20fuses%20multimodal%20information%20within%20each%20turn%20and%20applies%20turn-level%20causal%20attention%20to%20accumulate%20conversational%20history%2C%20while%20a%20lightweight%20diffusion%20head%20predicts%20continuous%203D%20head%20dynamics%20that%20captures%20both%20coordination%20and%20expressive%20variability.%20Experiments%20on%20the%20DualTalk%20benchmark%20show%20that%20TIMAR%20reduces%20Fr%C3%A9chet%20Distance%20and%20MSE%20by%2015-30%25%20on%20the%20test%20set%2C%20and%20achieves%20similar%20gains%20on%20out-of-distribution%20data.%20The%20source%20code%20will%20be%20released%20in%20the%20GitHub%20repository%20https%3A//github.com/CoderChen01/towards-seamleass-interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Seamless%2520Interaction%253A%2520Causal%2520Turn-Level%2520Modeling%2520of%2520Interactive%25203D%2520Conversational%2520Head%2520Dynamics%26entry.906535625%3DJunjie%2520Chen%2520and%2520Fei%2520Wang%2520and%2520Zhihao%2520Huang%2520and%2520Qing%2520Zhou%2520and%2520Kun%2520Li%2520and%2520Dan%2520Guo%2520and%2520Linfeng%2520Zhang%2520and%2520Xun%2520Yang%26entry.1292438233%3DHuman%2520conversation%2520involves%2520continuous%2520exchanges%2520of%2520speech%2520and%2520nonverbal%2520cues%2520such%2520as%2520head%2520nods%252C%2520gaze%2520shifts%252C%2520and%2520facial%2520expressions%2520that%2520convey%2520attention%2520and%2520emotion.%2520Modeling%2520these%2520bidirectional%2520dynamics%2520in%25203D%2520is%2520essential%2520for%2520building%2520expressive%2520avatars%2520and%2520interactive%2520robots.%2520However%252C%2520existing%2520frameworks%2520often%2520treat%2520talking%2520and%2520listening%2520as%2520independent%2520processes%2520or%2520rely%2520on%2520non-causal%2520full-sequence%2520modeling%252C%2520hindering%2520temporal%2520coherence%2520across%2520turns.%2520We%2520present%2520TIMAR%2520%2528Turn-level%2520Interleaved%2520Masked%2520AutoRegression%2529%252C%2520a%2520causal%2520framework%2520for%25203D%2520conversational%2520head%2520generation%2520that%2520models%2520dialogue%2520as%2520interleaved%2520audio-visual%2520contexts.%2520It%2520fuses%2520multimodal%2520information%2520within%2520each%2520turn%2520and%2520applies%2520turn-level%2520causal%2520attention%2520to%2520accumulate%2520conversational%2520history%252C%2520while%2520a%2520lightweight%2520diffusion%2520head%2520predicts%2520continuous%25203D%2520head%2520dynamics%2520that%2520captures%2520both%2520coordination%2520and%2520expressive%2520variability.%2520Experiments%2520on%2520the%2520DualTalk%2520benchmark%2520show%2520that%2520TIMAR%2520reduces%2520Fr%25C3%25A9chet%2520Distance%2520and%2520MSE%2520by%252015-30%2525%2520on%2520the%2520test%2520set%252C%2520and%2520achieves%2520similar%2520gains%2520on%2520out-of-distribution%2520data.%2520The%2520source%2520code%2520will%2520be%2520released%2520in%2520the%2520GitHub%2520repository%2520https%253A//github.com/CoderChen01/towards-seamleass-interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Seamless%20Interaction%3A%20Causal%20Turn-Level%20Modeling%20of%20Interactive%203D%20Conversational%20Head%20Dynamics&entry.906535625=Junjie%20Chen%20and%20Fei%20Wang%20and%20Zhihao%20Huang%20and%20Qing%20Zhou%20and%20Kun%20Li%20and%20Dan%20Guo%20and%20Linfeng%20Zhang%20and%20Xun%20Yang&entry.1292438233=Human%20conversation%20involves%20continuous%20exchanges%20of%20speech%20and%20nonverbal%20cues%20such%20as%20head%20nods%2C%20gaze%20shifts%2C%20and%20facial%20expressions%20that%20convey%20attention%20and%20emotion.%20Modeling%20these%20bidirectional%20dynamics%20in%203D%20is%20essential%20for%20building%20expressive%20avatars%20and%20interactive%20robots.%20However%2C%20existing%20frameworks%20often%20treat%20talking%20and%20listening%20as%20independent%20processes%20or%20rely%20on%20non-causal%20full-sequence%20modeling%2C%20hindering%20temporal%20coherence%20across%20turns.%20We%20present%20TIMAR%20%28Turn-level%20Interleaved%20Masked%20AutoRegression%29%2C%20a%20causal%20framework%20for%203D%20conversational%20head%20generation%20that%20models%20dialogue%20as%20interleaved%20audio-visual%20contexts.%20It%20fuses%20multimodal%20information%20within%20each%20turn%20and%20applies%20turn-level%20causal%20attention%20to%20accumulate%20conversational%20history%2C%20while%20a%20lightweight%20diffusion%20head%20predicts%20continuous%203D%20head%20dynamics%20that%20captures%20both%20coordination%20and%20expressive%20variability.%20Experiments%20on%20the%20DualTalk%20benchmark%20show%20that%20TIMAR%20reduces%20Fr%C3%A9chet%20Distance%20and%20MSE%20by%2015-30%25%20on%20the%20test%20set%2C%20and%20achieves%20similar%20gains%20on%20out-of-distribution%20data.%20The%20source%20code%20will%20be%20released%20in%20the%20GitHub%20repository%20https%3A//github.com/CoderChen01/towards-seamleass-interaction.&entry.1838667208=http%3A//arxiv.org/abs/2512.15340v1&entry.124074799=Read"},
{"title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models", "author": "Mikel Williams-Lekuona and Georgina Cosma", "abstract": "Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.", "link": "http://arxiv.org/abs/2512.15372v1", "date": "2025-12-17", "relevancy": 2.9044, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models&body=Title%3A%20Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models%0AAuthor%3A%20Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma%0AAbstract%3A%20Vision%20transformers%20in%20vision-language%20models%20apply%20uniform%20computational%20effort%20across%20all%20images%2C%20expending%20175.33%20GFLOPs%20%28ViT-L/14%29%20whether%20analysing%20a%20straightforward%20product%20photograph%20or%20a%20complex%20street%20scene.%20We%20propose%20ICAR%20%28Image%20Complexity-Aware%20Retrieval%29%2C%20which%20enables%20vision%20transformers%20to%20use%20less%20compute%20for%20simple%20images%20whilst%20processing%20complex%20images%20through%20their%20full%20network%20depth.%20The%20key%20challenge%20is%20maintaining%20cross-modal%20alignment%3A%20embeddings%20from%20different%20processing%20depths%20must%20remain%20compatible%20for%20text%20matching.%20ICAR%20solves%20this%20through%20dual-path%20training%20that%20produces%20compatible%20embeddings%20from%20both%20reduced-compute%20and%20full-compute%20processing.%20This%20maintains%20compatibility%20between%20image%20representations%20and%20text%20embeddings%20in%20the%20same%20semantic%20space%2C%20whether%20an%20image%20exits%20early%20or%20processes%20fully.%20Unlike%20existing%20two-stage%20approaches%20that%20require%20expensive%20reranking%2C%20ICAR%20enables%20direct%20image-text%20matching%20without%20additional%20overhead.%20To%20determine%20how%20much%20compute%20to%20use%2C%20we%20develop%20ConvNeXt-IC%2C%20which%20treats%20image%20complexity%20assessment%20as%20a%20classification%20task.%20By%20applying%20modern%20classifier%20backbones%20rather%20than%20specialised%20architectures%2C%20ConvNeXt-IC%20achieves%20state-of-the-art%20performance%20with%200.959%20correlation%20with%20human%20judgement%20%28Pearson%29%20and%204.4x%20speedup.%20Evaluated%20on%20standard%20benchmarks%20augmented%20with%20real-world%20web%20data%2C%20ICAR%20achieves%2020%25%20practical%20speedup%20while%20maintaining%20category-level%20performance%20and%2095%25%20of%20instance-level%20performance%2C%20enabling%20sustainable%20scaling%20of%20vision-language%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Complexity-Aware%2520Adaptive%2520Retrieval%2520for%2520Efficient%2520Vision-Language%2520Models%26entry.906535625%3DMikel%2520Williams-Lekuona%2520and%2520Georgina%2520Cosma%26entry.1292438233%3DVision%2520transformers%2520in%2520vision-language%2520models%2520apply%2520uniform%2520computational%2520effort%2520across%2520all%2520images%252C%2520expending%2520175.33%2520GFLOPs%2520%2528ViT-L/14%2529%2520whether%2520analysing%2520a%2520straightforward%2520product%2520photograph%2520or%2520a%2520complex%2520street%2520scene.%2520We%2520propose%2520ICAR%2520%2528Image%2520Complexity-Aware%2520Retrieval%2529%252C%2520which%2520enables%2520vision%2520transformers%2520to%2520use%2520less%2520compute%2520for%2520simple%2520images%2520whilst%2520processing%2520complex%2520images%2520through%2520their%2520full%2520network%2520depth.%2520The%2520key%2520challenge%2520is%2520maintaining%2520cross-modal%2520alignment%253A%2520embeddings%2520from%2520different%2520processing%2520depths%2520must%2520remain%2520compatible%2520for%2520text%2520matching.%2520ICAR%2520solves%2520this%2520through%2520dual-path%2520training%2520that%2520produces%2520compatible%2520embeddings%2520from%2520both%2520reduced-compute%2520and%2520full-compute%2520processing.%2520This%2520maintains%2520compatibility%2520between%2520image%2520representations%2520and%2520text%2520embeddings%2520in%2520the%2520same%2520semantic%2520space%252C%2520whether%2520an%2520image%2520exits%2520early%2520or%2520processes%2520fully.%2520Unlike%2520existing%2520two-stage%2520approaches%2520that%2520require%2520expensive%2520reranking%252C%2520ICAR%2520enables%2520direct%2520image-text%2520matching%2520without%2520additional%2520overhead.%2520To%2520determine%2520how%2520much%2520compute%2520to%2520use%252C%2520we%2520develop%2520ConvNeXt-IC%252C%2520which%2520treats%2520image%2520complexity%2520assessment%2520as%2520a%2520classification%2520task.%2520By%2520applying%2520modern%2520classifier%2520backbones%2520rather%2520than%2520specialised%2520architectures%252C%2520ConvNeXt-IC%2520achieves%2520state-of-the-art%2520performance%2520with%25200.959%2520correlation%2520with%2520human%2520judgement%2520%2528Pearson%2529%2520and%25204.4x%2520speedup.%2520Evaluated%2520on%2520standard%2520benchmarks%2520augmented%2520with%2520real-world%2520web%2520data%252C%2520ICAR%2520achieves%252020%2525%2520practical%2520speedup%2520while%2520maintaining%2520category-level%2520performance%2520and%252095%2525%2520of%2520instance-level%2520performance%252C%2520enabling%2520sustainable%2520scaling%2520of%2520vision-language%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models&entry.906535625=Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma&entry.1292438233=Vision%20transformers%20in%20vision-language%20models%20apply%20uniform%20computational%20effort%20across%20all%20images%2C%20expending%20175.33%20GFLOPs%20%28ViT-L/14%29%20whether%20analysing%20a%20straightforward%20product%20photograph%20or%20a%20complex%20street%20scene.%20We%20propose%20ICAR%20%28Image%20Complexity-Aware%20Retrieval%29%2C%20which%20enables%20vision%20transformers%20to%20use%20less%20compute%20for%20simple%20images%20whilst%20processing%20complex%20images%20through%20their%20full%20network%20depth.%20The%20key%20challenge%20is%20maintaining%20cross-modal%20alignment%3A%20embeddings%20from%20different%20processing%20depths%20must%20remain%20compatible%20for%20text%20matching.%20ICAR%20solves%20this%20through%20dual-path%20training%20that%20produces%20compatible%20embeddings%20from%20both%20reduced-compute%20and%20full-compute%20processing.%20This%20maintains%20compatibility%20between%20image%20representations%20and%20text%20embeddings%20in%20the%20same%20semantic%20space%2C%20whether%20an%20image%20exits%20early%20or%20processes%20fully.%20Unlike%20existing%20two-stage%20approaches%20that%20require%20expensive%20reranking%2C%20ICAR%20enables%20direct%20image-text%20matching%20without%20additional%20overhead.%20To%20determine%20how%20much%20compute%20to%20use%2C%20we%20develop%20ConvNeXt-IC%2C%20which%20treats%20image%20complexity%20assessment%20as%20a%20classification%20task.%20By%20applying%20modern%20classifier%20backbones%20rather%20than%20specialised%20architectures%2C%20ConvNeXt-IC%20achieves%20state-of-the-art%20performance%20with%200.959%20correlation%20with%20human%20judgement%20%28Pearson%29%20and%204.4x%20speedup.%20Evaluated%20on%20standard%20benchmarks%20augmented%20with%20real-world%20web%20data%2C%20ICAR%20achieves%2020%25%20practical%20speedup%20while%20maintaining%20category-level%20performance%20and%2095%25%20of%20instance-level%20performance%2C%20enabling%20sustainable%20scaling%20of%20vision-language%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.15372v1&entry.124074799=Read"},
{"title": "VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics", "author": "Opeyemi Bamigbade and Mark Scanlon and John Sheppard", "abstract": "Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.", "link": "http://arxiv.org/abs/2512.15512v1", "date": "2025-12-17", "relevancy": 2.8911, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5955}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.585}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAAS%3A%20Vision-Attention%20Anomaly%20Scoring%20for%20Image%20Manipulation%20Detection%20in%20Digital%20Forensics&body=Title%3A%20VAAS%3A%20Vision-Attention%20Anomaly%20Scoring%20for%20Image%20Manipulation%20Detection%20in%20Digital%20Forensics%0AAuthor%3A%20Opeyemi%20Bamigbade%20and%20Mark%20Scanlon%20and%20John%20Sheppard%0AAbstract%3A%20Recent%20advances%20in%20AI-driven%20image%20generation%20have%20introduced%20new%20challenges%20for%20verifying%20the%20authenticity%20of%20digital%20evidence%20in%20forensic%20investigations.%20Modern%20generative%20models%20can%20produce%20visually%20consistent%20forgeries%20that%20evade%20traditional%20detectors%20based%20on%20pixel%20or%20compression%20artefacts.%20Most%20existing%20approaches%20also%20lack%20an%20explicit%20measure%20of%20anomaly%20intensity%2C%20which%20limits%20their%20ability%20to%20quantify%20the%20severity%20of%20manipulation.%20This%20paper%20introduces%20Vision-Attention%20Anomaly%20Scoring%20%28VAAS%29%2C%20a%20novel%20dual-module%20framework%20that%20integrates%20global%20attention-based%20anomaly%20estimation%20using%20Vision%20Transformers%20%28ViT%29%20with%20patch-level%20self-consistency%20scoring%20derived%20from%20SegFormer%20embeddings.%20The%20hybrid%20formulation%20provides%20a%20continuous%20and%20interpretable%20anomaly%20score%20that%20reflects%20both%20the%20location%20and%20degree%20of%20manipulation.%20Evaluations%20on%20the%20DF2023%20and%20CASIA%20v2.0%20datasets%20demonstrate%20that%20VAAS%20achieves%20competitive%20F1%20and%20IoU%20performance%2C%20while%20enhancing%20visual%20explainability%20through%20attention-guided%20anomaly%20maps.%20The%20framework%20bridges%20quantitative%20detection%20with%20human-understandable%20reasoning%2C%20supporting%20transparent%20and%20reliable%20image%20integrity%20assessment.%20The%20source%20code%20for%20all%20experiments%20and%20corresponding%20materials%20for%20reproducing%20the%20results%20are%20available%20open%20source.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAAS%253A%2520Vision-Attention%2520Anomaly%2520Scoring%2520for%2520Image%2520Manipulation%2520Detection%2520in%2520Digital%2520Forensics%26entry.906535625%3DOpeyemi%2520Bamigbade%2520and%2520Mark%2520Scanlon%2520and%2520John%2520Sheppard%26entry.1292438233%3DRecent%2520advances%2520in%2520AI-driven%2520image%2520generation%2520have%2520introduced%2520new%2520challenges%2520for%2520verifying%2520the%2520authenticity%2520of%2520digital%2520evidence%2520in%2520forensic%2520investigations.%2520Modern%2520generative%2520models%2520can%2520produce%2520visually%2520consistent%2520forgeries%2520that%2520evade%2520traditional%2520detectors%2520based%2520on%2520pixel%2520or%2520compression%2520artefacts.%2520Most%2520existing%2520approaches%2520also%2520lack%2520an%2520explicit%2520measure%2520of%2520anomaly%2520intensity%252C%2520which%2520limits%2520their%2520ability%2520to%2520quantify%2520the%2520severity%2520of%2520manipulation.%2520This%2520paper%2520introduces%2520Vision-Attention%2520Anomaly%2520Scoring%2520%2528VAAS%2529%252C%2520a%2520novel%2520dual-module%2520framework%2520that%2520integrates%2520global%2520attention-based%2520anomaly%2520estimation%2520using%2520Vision%2520Transformers%2520%2528ViT%2529%2520with%2520patch-level%2520self-consistency%2520scoring%2520derived%2520from%2520SegFormer%2520embeddings.%2520The%2520hybrid%2520formulation%2520provides%2520a%2520continuous%2520and%2520interpretable%2520anomaly%2520score%2520that%2520reflects%2520both%2520the%2520location%2520and%2520degree%2520of%2520manipulation.%2520Evaluations%2520on%2520the%2520DF2023%2520and%2520CASIA%2520v2.0%2520datasets%2520demonstrate%2520that%2520VAAS%2520achieves%2520competitive%2520F1%2520and%2520IoU%2520performance%252C%2520while%2520enhancing%2520visual%2520explainability%2520through%2520attention-guided%2520anomaly%2520maps.%2520The%2520framework%2520bridges%2520quantitative%2520detection%2520with%2520human-understandable%2520reasoning%252C%2520supporting%2520transparent%2520and%2520reliable%2520image%2520integrity%2520assessment.%2520The%2520source%2520code%2520for%2520all%2520experiments%2520and%2520corresponding%2520materials%2520for%2520reproducing%2520the%2520results%2520are%2520available%2520open%2520source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAAS%3A%20Vision-Attention%20Anomaly%20Scoring%20for%20Image%20Manipulation%20Detection%20in%20Digital%20Forensics&entry.906535625=Opeyemi%20Bamigbade%20and%20Mark%20Scanlon%20and%20John%20Sheppard&entry.1292438233=Recent%20advances%20in%20AI-driven%20image%20generation%20have%20introduced%20new%20challenges%20for%20verifying%20the%20authenticity%20of%20digital%20evidence%20in%20forensic%20investigations.%20Modern%20generative%20models%20can%20produce%20visually%20consistent%20forgeries%20that%20evade%20traditional%20detectors%20based%20on%20pixel%20or%20compression%20artefacts.%20Most%20existing%20approaches%20also%20lack%20an%20explicit%20measure%20of%20anomaly%20intensity%2C%20which%20limits%20their%20ability%20to%20quantify%20the%20severity%20of%20manipulation.%20This%20paper%20introduces%20Vision-Attention%20Anomaly%20Scoring%20%28VAAS%29%2C%20a%20novel%20dual-module%20framework%20that%20integrates%20global%20attention-based%20anomaly%20estimation%20using%20Vision%20Transformers%20%28ViT%29%20with%20patch-level%20self-consistency%20scoring%20derived%20from%20SegFormer%20embeddings.%20The%20hybrid%20formulation%20provides%20a%20continuous%20and%20interpretable%20anomaly%20score%20that%20reflects%20both%20the%20location%20and%20degree%20of%20manipulation.%20Evaluations%20on%20the%20DF2023%20and%20CASIA%20v2.0%20datasets%20demonstrate%20that%20VAAS%20achieves%20competitive%20F1%20and%20IoU%20performance%2C%20while%20enhancing%20visual%20explainability%20through%20attention-guided%20anomaly%20maps.%20The%20framework%20bridges%20quantitative%20detection%20with%20human-understandable%20reasoning%2C%20supporting%20transparent%20and%20reliable%20image%20integrity%20assessment.%20The%20source%20code%20for%20all%20experiments%20and%20corresponding%20materials%20for%20reproducing%20the%20results%20are%20available%20open%20source.&entry.1838667208=http%3A//arxiv.org/abs/2512.15512v1&entry.124074799=Read"},
{"title": "Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization", "author": "Mengshi Qi and Hongwei Ji and Wulian Yun and Xianlin Zhang and Huadong Ma", "abstract": "Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.", "link": "http://arxiv.org/abs/2504.13460v4", "date": "2025-12-17", "relevancy": 2.8699, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5841}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5749}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Evidence%20Multimodal%20Reasoning%20for%20Few-shot%20Temporal%20Action%20Localization&body=Title%3A%20Chain-of-Evidence%20Multimodal%20Reasoning%20for%20Few-shot%20Temporal%20Action%20Localization%0AAuthor%3A%20Mengshi%20Qi%20and%20Hongwei%20Ji%20and%20Wulian%20Yun%20and%20Xianlin%20Zhang%20and%20Huadong%20Ma%0AAbstract%3A%20Traditional%20temporal%20action%20localization%20%28TAL%29%20methods%20rely%20on%20large%20amounts%20of%20detailed%20annotated%20data%2C%20whereas%20few-shot%20TAL%20reduces%20this%20dependence%20by%20using%20only%20a%20few%20training%20samples%20to%20identify%20unseen%20action%20categories.%20However%2C%20existing%20few-shot%20TAL%20methods%20typically%20focus%20solely%20on%20video-level%20information%2C%20neglecting%20textual%20information%2C%20which%20can%20provide%20valuable%20semantic%20support%20for%20the%20action%20localization%20task.%20To%20address%20these%20issues%2C%20in%20this%20work%2C%20we%20propose%20a%20new%20few-shot%20temporal%20action%20localization%20method%20by%20Chain-of-Evidence%20multimodal%20reasoning%20to%20improve%20localization%20performance.%20Specifically%2C%20we%20design%20a%20novel%20few-shot%20learning%20framework%20to%20capture%20action%20commonalities%20and%20variations%2C%20which%20includes%20a%20semantic-aware%20text-visual%20alignment%20module%20designed%20to%20align%20the%20query%20and%20support%20videos%20at%20different%20levels.%20Meanwhile%2C%20to%20better%20express%20the%20temporal%20dependencies%20and%20causal%20relationships%20between%20actions%20at%20the%20textual%20level%2C%20we%20design%20a%20Chain-of-Evidence%20%28CoE%29%20reasoning%20method%20that%20progressively%20guides%20the%20Vision%20Language%20Model%20%28VLM%29%20and%20Large%20Language%20Model%20%28LLM%29%20to%20generate%20CoE%20text%20descriptions%20for%20videos.%20The%20generated%20texts%20can%20capture%20more%20variance%20of%20action%20than%20visual%20features.%20We%20conduct%20extensive%20experiments%20on%20the%20publicly%20available%20ActivityNet1.3%2C%20THUMOS14%20and%20our%20newly%20collected%20Human-related%20Anomaly%20Localization%20Dataset.%20The%20experimental%20results%20demonstrate%20that%20our%20proposed%20method%20significantly%20outperforms%20existing%20methods%20in%20single-instance%20and%20multi-instance%20scenarios.%20Our%20source%20code%20and%20data%20are%20available%20at%20https%3A//github.com/MICLAB-BUPT/VAL-VLM.%0ALink%3A%20http%3A//arxiv.org/abs/2504.13460v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Evidence%2520Multimodal%2520Reasoning%2520for%2520Few-shot%2520Temporal%2520Action%2520Localization%26entry.906535625%3DMengshi%2520Qi%2520and%2520Hongwei%2520Ji%2520and%2520Wulian%2520Yun%2520and%2520Xianlin%2520Zhang%2520and%2520Huadong%2520Ma%26entry.1292438233%3DTraditional%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520methods%2520rely%2520on%2520large%2520amounts%2520of%2520detailed%2520annotated%2520data%252C%2520whereas%2520few-shot%2520TAL%2520reduces%2520this%2520dependence%2520by%2520using%2520only%2520a%2520few%2520training%2520samples%2520to%2520identify%2520unseen%2520action%2520categories.%2520However%252C%2520existing%2520few-shot%2520TAL%2520methods%2520typically%2520focus%2520solely%2520on%2520video-level%2520information%252C%2520neglecting%2520textual%2520information%252C%2520which%2520can%2520provide%2520valuable%2520semantic%2520support%2520for%2520the%2520action%2520localization%2520task.%2520To%2520address%2520these%2520issues%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520few-shot%2520temporal%2520action%2520localization%2520method%2520by%2520Chain-of-Evidence%2520multimodal%2520reasoning%2520to%2520improve%2520localization%2520performance.%2520Specifically%252C%2520we%2520design%2520a%2520novel%2520few-shot%2520learning%2520framework%2520to%2520capture%2520action%2520commonalities%2520and%2520variations%252C%2520which%2520includes%2520a%2520semantic-aware%2520text-visual%2520alignment%2520module%2520designed%2520to%2520align%2520the%2520query%2520and%2520support%2520videos%2520at%2520different%2520levels.%2520Meanwhile%252C%2520to%2520better%2520express%2520the%2520temporal%2520dependencies%2520and%2520causal%2520relationships%2520between%2520actions%2520at%2520the%2520textual%2520level%252C%2520we%2520design%2520a%2520Chain-of-Evidence%2520%2528CoE%2529%2520reasoning%2520method%2520that%2520progressively%2520guides%2520the%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520and%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520generate%2520CoE%2520text%2520descriptions%2520for%2520videos.%2520The%2520generated%2520texts%2520can%2520capture%2520more%2520variance%2520of%2520action%2520than%2520visual%2520features.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520publicly%2520available%2520ActivityNet1.3%252C%2520THUMOS14%2520and%2520our%2520newly%2520collected%2520Human-related%2520Anomaly%2520Localization%2520Dataset.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520single-instance%2520and%2520multi-instance%2520scenarios.%2520Our%2520source%2520code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/MICLAB-BUPT/VAL-VLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13460v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Evidence%20Multimodal%20Reasoning%20for%20Few-shot%20Temporal%20Action%20Localization&entry.906535625=Mengshi%20Qi%20and%20Hongwei%20Ji%20and%20Wulian%20Yun%20and%20Xianlin%20Zhang%20and%20Huadong%20Ma&entry.1292438233=Traditional%20temporal%20action%20localization%20%28TAL%29%20methods%20rely%20on%20large%20amounts%20of%20detailed%20annotated%20data%2C%20whereas%20few-shot%20TAL%20reduces%20this%20dependence%20by%20using%20only%20a%20few%20training%20samples%20to%20identify%20unseen%20action%20categories.%20However%2C%20existing%20few-shot%20TAL%20methods%20typically%20focus%20solely%20on%20video-level%20information%2C%20neglecting%20textual%20information%2C%20which%20can%20provide%20valuable%20semantic%20support%20for%20the%20action%20localization%20task.%20To%20address%20these%20issues%2C%20in%20this%20work%2C%20we%20propose%20a%20new%20few-shot%20temporal%20action%20localization%20method%20by%20Chain-of-Evidence%20multimodal%20reasoning%20to%20improve%20localization%20performance.%20Specifically%2C%20we%20design%20a%20novel%20few-shot%20learning%20framework%20to%20capture%20action%20commonalities%20and%20variations%2C%20which%20includes%20a%20semantic-aware%20text-visual%20alignment%20module%20designed%20to%20align%20the%20query%20and%20support%20videos%20at%20different%20levels.%20Meanwhile%2C%20to%20better%20express%20the%20temporal%20dependencies%20and%20causal%20relationships%20between%20actions%20at%20the%20textual%20level%2C%20we%20design%20a%20Chain-of-Evidence%20%28CoE%29%20reasoning%20method%20that%20progressively%20guides%20the%20Vision%20Language%20Model%20%28VLM%29%20and%20Large%20Language%20Model%20%28LLM%29%20to%20generate%20CoE%20text%20descriptions%20for%20videos.%20The%20generated%20texts%20can%20capture%20more%20variance%20of%20action%20than%20visual%20features.%20We%20conduct%20extensive%20experiments%20on%20the%20publicly%20available%20ActivityNet1.3%2C%20THUMOS14%20and%20our%20newly%20collected%20Human-related%20Anomaly%20Localization%20Dataset.%20The%20experimental%20results%20demonstrate%20that%20our%20proposed%20method%20significantly%20outperforms%20existing%20methods%20in%20single-instance%20and%20multi-instance%20scenarios.%20Our%20source%20code%20and%20data%20are%20available%20at%20https%3A//github.com/MICLAB-BUPT/VAL-VLM.&entry.1838667208=http%3A//arxiv.org/abs/2504.13460v4&entry.124074799=Read"},
{"title": "Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models", "author": "Kuinan Hou and Jing Mi and Marco Zorzi and Lamberto Ballan and Alberto Testolin", "abstract": "Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.", "link": "http://arxiv.org/abs/2512.15254v1", "date": "2025-12-17", "relevancy": 2.8461, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6042}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6042}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Visual%20Enumeration%20Abilities%20of%20Specialized%20Counting%20Architectures%20and%20Vision-Language%20Models&body=Title%3A%20Assessing%20the%20Visual%20Enumeration%20Abilities%20of%20Specialized%20Counting%20Architectures%20and%20Vision-Language%20Models%0AAuthor%3A%20Kuinan%20Hou%20and%20Jing%20Mi%20and%20Marco%20Zorzi%20and%20Lamberto%20Ballan%20and%20Alberto%20Testolin%0AAbstract%3A%20Counting%20the%20number%20of%20items%20in%20a%20visual%20scene%20remains%20a%20fundamental%20yet%20challenging%20task%20in%20computer%20vision.%20Traditional%20approaches%20to%20solving%20this%20problem%20rely%20on%20domain-specific%20counting%20architectures%2C%20which%20are%20trained%20using%20datasets%20annotated%20with%20a%20predefined%20set%20of%20object%20categories.%20However%2C%20recent%20progress%20in%20creating%20large-scale%20multimodal%20vision-language%20models%20%28VLMs%29%20suggests%20that%20these%20domain-general%20architectures%20may%20offer%20a%20flexible%20alternative%20for%20open-set%20object%20counting.%20In%20this%20study%2C%20we%20therefore%20systematically%20compare%20the%20performance%20of%20state-of-the-art%20specialized%20counting%20architectures%20against%20VLMs%20on%20two%20popular%20counting%20datasets%2C%20as%20well%20as%20on%20a%20novel%20benchmark%20specifically%20created%20to%20have%20a%20finer-grained%20control%20over%20the%20visual%20properties%20of%20test%20images.%20Our%20findings%20show%20that%20most%20VLMs%20can%20approximately%20enumerate%20the%20number%20of%20items%20in%20a%20visual%20scene%2C%20matching%20or%20even%20surpassing%20the%20performance%20of%20specialized%20computer%20vision%20architectures.%20Notably%2C%20enumeration%20accuracy%20significantly%20improves%20when%20VLMs%20are%20prompted%20to%20generate%20intermediate%20representations%20%28i.e.%2C%20locations%20and%20verbal%20labels%29%20of%20each%20object%20to%20be%20counted.%20Nevertheless%2C%20none%20of%20the%20models%20can%20reliably%20count%20the%20number%20of%20objects%20in%20complex%20visual%20scenes%2C%20showing%20that%20further%20research%20is%20still%20needed%20to%20create%20AI%20systems%20that%20can%20reliably%20deploy%20counting%20procedures%20in%20realistic%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Visual%2520Enumeration%2520Abilities%2520of%2520Specialized%2520Counting%2520Architectures%2520and%2520Vision-Language%2520Models%26entry.906535625%3DKuinan%2520Hou%2520and%2520Jing%2520Mi%2520and%2520Marco%2520Zorzi%2520and%2520Lamberto%2520Ballan%2520and%2520Alberto%2520Testolin%26entry.1292438233%3DCounting%2520the%2520number%2520of%2520items%2520in%2520a%2520visual%2520scene%2520remains%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520computer%2520vision.%2520Traditional%2520approaches%2520to%2520solving%2520this%2520problem%2520rely%2520on%2520domain-specific%2520counting%2520architectures%252C%2520which%2520are%2520trained%2520using%2520datasets%2520annotated%2520with%2520a%2520predefined%2520set%2520of%2520object%2520categories.%2520However%252C%2520recent%2520progress%2520in%2520creating%2520large-scale%2520multimodal%2520vision-language%2520models%2520%2528VLMs%2529%2520suggests%2520that%2520these%2520domain-general%2520architectures%2520may%2520offer%2520a%2520flexible%2520alternative%2520for%2520open-set%2520object%2520counting.%2520In%2520this%2520study%252C%2520we%2520therefore%2520systematically%2520compare%2520the%2520performance%2520of%2520state-of-the-art%2520specialized%2520counting%2520architectures%2520against%2520VLMs%2520on%2520two%2520popular%2520counting%2520datasets%252C%2520as%2520well%2520as%2520on%2520a%2520novel%2520benchmark%2520specifically%2520created%2520to%2520have%2520a%2520finer-grained%2520control%2520over%2520the%2520visual%2520properties%2520of%2520test%2520images.%2520Our%2520findings%2520show%2520that%2520most%2520VLMs%2520can%2520approximately%2520enumerate%2520the%2520number%2520of%2520items%2520in%2520a%2520visual%2520scene%252C%2520matching%2520or%2520even%2520surpassing%2520the%2520performance%2520of%2520specialized%2520computer%2520vision%2520architectures.%2520Notably%252C%2520enumeration%2520accuracy%2520significantly%2520improves%2520when%2520VLMs%2520are%2520prompted%2520to%2520generate%2520intermediate%2520representations%2520%2528i.e.%252C%2520locations%2520and%2520verbal%2520labels%2529%2520of%2520each%2520object%2520to%2520be%2520counted.%2520Nevertheless%252C%2520none%2520of%2520the%2520models%2520can%2520reliably%2520count%2520the%2520number%2520of%2520objects%2520in%2520complex%2520visual%2520scenes%252C%2520showing%2520that%2520further%2520research%2520is%2520still%2520needed%2520to%2520create%2520AI%2520systems%2520that%2520can%2520reliably%2520deploy%2520counting%2520procedures%2520in%2520realistic%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Visual%20Enumeration%20Abilities%20of%20Specialized%20Counting%20Architectures%20and%20Vision-Language%20Models&entry.906535625=Kuinan%20Hou%20and%20Jing%20Mi%20and%20Marco%20Zorzi%20and%20Lamberto%20Ballan%20and%20Alberto%20Testolin&entry.1292438233=Counting%20the%20number%20of%20items%20in%20a%20visual%20scene%20remains%20a%20fundamental%20yet%20challenging%20task%20in%20computer%20vision.%20Traditional%20approaches%20to%20solving%20this%20problem%20rely%20on%20domain-specific%20counting%20architectures%2C%20which%20are%20trained%20using%20datasets%20annotated%20with%20a%20predefined%20set%20of%20object%20categories.%20However%2C%20recent%20progress%20in%20creating%20large-scale%20multimodal%20vision-language%20models%20%28VLMs%29%20suggests%20that%20these%20domain-general%20architectures%20may%20offer%20a%20flexible%20alternative%20for%20open-set%20object%20counting.%20In%20this%20study%2C%20we%20therefore%20systematically%20compare%20the%20performance%20of%20state-of-the-art%20specialized%20counting%20architectures%20against%20VLMs%20on%20two%20popular%20counting%20datasets%2C%20as%20well%20as%20on%20a%20novel%20benchmark%20specifically%20created%20to%20have%20a%20finer-grained%20control%20over%20the%20visual%20properties%20of%20test%20images.%20Our%20findings%20show%20that%20most%20VLMs%20can%20approximately%20enumerate%20the%20number%20of%20items%20in%20a%20visual%20scene%2C%20matching%20or%20even%20surpassing%20the%20performance%20of%20specialized%20computer%20vision%20architectures.%20Notably%2C%20enumeration%20accuracy%20significantly%20improves%20when%20VLMs%20are%20prompted%20to%20generate%20intermediate%20representations%20%28i.e.%2C%20locations%20and%20verbal%20labels%29%20of%20each%20object%20to%20be%20counted.%20Nevertheless%2C%20none%20of%20the%20models%20can%20reliably%20count%20the%20number%20of%20objects%20in%20complex%20visual%20scenes%2C%20showing%20that%20further%20research%20is%20still%20needed%20to%20create%20AI%20systems%20that%20can%20reliably%20deploy%20counting%20procedures%20in%20realistic%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.15254v1&entry.124074799=Read"},
{"title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning", "author": "Yifei Li and Wenzhao Zheng and Yanran Zhang and Runze Sun and Yu Zheng and Lei Chen and Jie Zhou and Jiwen Lu", "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.", "link": "http://arxiv.org/abs/2512.15693v1", "date": "2025-12-17", "relevancy": 2.8411, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6133}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5459}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skyra%3A%20AI-Generated%20Video%20Detection%20via%20Grounded%20Artifact%20Reasoning&body=Title%3A%20Skyra%3A%20AI-Generated%20Video%20Detection%20via%20Grounded%20Artifact%20Reasoning%0AAuthor%3A%20Yifei%20Li%20and%20Wenzhao%20Zheng%20and%20Yanran%20Zhang%20and%20Runze%20Sun%20and%20Yu%20Zheng%20and%20Lei%20Chen%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20The%20misuse%20of%20AI-driven%20video%20generation%20technologies%20has%20raised%20serious%20social%20concerns%2C%20highlighting%20the%20urgent%20need%20for%20reliable%20AI-generated%20video%20detectors.%20However%2C%20most%20existing%20methods%20are%20limited%20to%20binary%20classification%20and%20lack%20the%20necessary%20explanations%20for%20human%20interpretation.%20In%20this%20paper%2C%20we%20present%20Skyra%2C%20a%20specialized%20multimodal%20large%20language%20model%20%28MLLM%29%20that%20identifies%20human-perceivable%20visual%20artifacts%20in%20AI-generated%20videos%20and%20leverages%20them%20as%20grounded%20evidence%20for%20both%20detection%20and%20explanation.%20To%20support%20this%20objective%2C%20we%20construct%20ViF-CoT-4K%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20which%20represents%20the%20first%20large-scale%20AI-generated%20video%20artifact%20dataset%20with%20fine-grained%20human%20annotations.%20We%20then%20develop%20a%20two-stage%20training%20strategy%20that%20systematically%20enhances%20our%20model%27s%20spatio-temporal%20artifact%20perception%2C%20explanation%20capability%2C%20and%20detection%20accuracy.%20To%20comprehensively%20evaluate%20Skyra%2C%20we%20introduce%20ViF-Bench%2C%20a%20benchmark%20comprising%203K%20high-quality%20samples%20generated%20by%20over%20ten%20state-of-the-art%20video%20generators.%20Extensive%20experiments%20demonstrate%20that%20Skyra%20surpasses%20existing%20methods%20across%20multiple%20benchmarks%2C%20while%20our%20evaluation%20yields%20valuable%20insights%20for%20advancing%20explainable%20AI-generated%20video%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyra%253A%2520AI-Generated%2520Video%2520Detection%2520via%2520Grounded%2520Artifact%2520Reasoning%26entry.906535625%3DYifei%2520Li%2520and%2520Wenzhao%2520Zheng%2520and%2520Yanran%2520Zhang%2520and%2520Runze%2520Sun%2520and%2520Yu%2520Zheng%2520and%2520Lei%2520Chen%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3DThe%2520misuse%2520of%2520AI-driven%2520video%2520generation%2520technologies%2520has%2520raised%2520serious%2520social%2520concerns%252C%2520highlighting%2520the%2520urgent%2520need%2520for%2520reliable%2520AI-generated%2520video%2520detectors.%2520However%252C%2520most%2520existing%2520methods%2520are%2520limited%2520to%2520binary%2520classification%2520and%2520lack%2520the%2520necessary%2520explanations%2520for%2520human%2520interpretation.%2520In%2520this%2520paper%252C%2520we%2520present%2520Skyra%252C%2520a%2520specialized%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520that%2520identifies%2520human-perceivable%2520visual%2520artifacts%2520in%2520AI-generated%2520videos%2520and%2520leverages%2520them%2520as%2520grounded%2520evidence%2520for%2520both%2520detection%2520and%2520explanation.%2520To%2520support%2520this%2520objective%252C%2520we%2520construct%2520ViF-CoT-4K%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520which%2520represents%2520the%2520first%2520large-scale%2520AI-generated%2520video%2520artifact%2520dataset%2520with%2520fine-grained%2520human%2520annotations.%2520We%2520then%2520develop%2520a%2520two-stage%2520training%2520strategy%2520that%2520systematically%2520enhances%2520our%2520model%2527s%2520spatio-temporal%2520artifact%2520perception%252C%2520explanation%2520capability%252C%2520and%2520detection%2520accuracy.%2520To%2520comprehensively%2520evaluate%2520Skyra%252C%2520we%2520introduce%2520ViF-Bench%252C%2520a%2520benchmark%2520comprising%25203K%2520high-quality%2520samples%2520generated%2520by%2520over%2520ten%2520state-of-the-art%2520video%2520generators.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Skyra%2520surpasses%2520existing%2520methods%2520across%2520multiple%2520benchmarks%252C%2520while%2520our%2520evaluation%2520yields%2520valuable%2520insights%2520for%2520advancing%2520explainable%2520AI-generated%2520video%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skyra%3A%20AI-Generated%20Video%20Detection%20via%20Grounded%20Artifact%20Reasoning&entry.906535625=Yifei%20Li%20and%20Wenzhao%20Zheng%20and%20Yanran%20Zhang%20and%20Runze%20Sun%20and%20Yu%20Zheng%20and%20Lei%20Chen%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=The%20misuse%20of%20AI-driven%20video%20generation%20technologies%20has%20raised%20serious%20social%20concerns%2C%20highlighting%20the%20urgent%20need%20for%20reliable%20AI-generated%20video%20detectors.%20However%2C%20most%20existing%20methods%20are%20limited%20to%20binary%20classification%20and%20lack%20the%20necessary%20explanations%20for%20human%20interpretation.%20In%20this%20paper%2C%20we%20present%20Skyra%2C%20a%20specialized%20multimodal%20large%20language%20model%20%28MLLM%29%20that%20identifies%20human-perceivable%20visual%20artifacts%20in%20AI-generated%20videos%20and%20leverages%20them%20as%20grounded%20evidence%20for%20both%20detection%20and%20explanation.%20To%20support%20this%20objective%2C%20we%20construct%20ViF-CoT-4K%20for%20Supervised%20Fine-Tuning%20%28SFT%29%2C%20which%20represents%20the%20first%20large-scale%20AI-generated%20video%20artifact%20dataset%20with%20fine-grained%20human%20annotations.%20We%20then%20develop%20a%20two-stage%20training%20strategy%20that%20systematically%20enhances%20our%20model%27s%20spatio-temporal%20artifact%20perception%2C%20explanation%20capability%2C%20and%20detection%20accuracy.%20To%20comprehensively%20evaluate%20Skyra%2C%20we%20introduce%20ViF-Bench%2C%20a%20benchmark%20comprising%203K%20high-quality%20samples%20generated%20by%20over%20ten%20state-of-the-art%20video%20generators.%20Extensive%20experiments%20demonstrate%20that%20Skyra%20surpasses%20existing%20methods%20across%20multiple%20benchmarks%2C%20while%20our%20evaluation%20yields%20valuable%20insights%20for%20advancing%20explainable%20AI-generated%20video%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.15693v1&entry.124074799=Read"},
{"title": "If you can describe it, they can see it: Cross-Modal Learning of Visual Concepts from Textual Descriptions", "author": "Carlo Alberto Barbano and Luca Molinaro and Massimiliano Ciranni and Emanuele Aiello and Vito Paolo Pastore and Marco Grangetto", "abstract": "Humans can visualize new and unknown concepts from their natural language description, based on their experience and previous knowledge. Insipired by this, we present a way to extend this ability to Vision-Language Models (VLMs), teaching them novel concepts by only using a textual description. We refer to this approach as Knowledge Transfer (KT). Our hypothesis is that the knowledge of a pre-trained VLM can be re-used to represent previously unknown concepts. Provided with a textual description of the novel concept, KT works by aligning relevant features of the visual encoder, obtained through model inversion, to its text representation. Differently from approaches relying on visual examples or external generative models, KT transfers knowledge within the same VLM by injecting visual knowledge directly from the text. Through an extensive evaluation on several VLM tasks, including classification, segmentation, image-text retrieval, and captioning, we show that: 1) KT can efficiently introduce new visual concepts from a single textual description; 2) the same principle can be used to refine the representation of existing concepts; and 3) KT significantly improves the performance of zero-shot VLMs.", "link": "http://arxiv.org/abs/2411.15611v2", "date": "2025-12-17", "relevancy": 2.8387, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20If%20you%20can%20describe%20it%2C%20they%20can%20see%20it%3A%20Cross-Modal%20Learning%20of%20Visual%20Concepts%20from%20Textual%20Descriptions&body=Title%3A%20If%20you%20can%20describe%20it%2C%20they%20can%20see%20it%3A%20Cross-Modal%20Learning%20of%20Visual%20Concepts%20from%20Textual%20Descriptions%0AAuthor%3A%20Carlo%20Alberto%20Barbano%20and%20Luca%20Molinaro%20and%20Massimiliano%20Ciranni%20and%20Emanuele%20Aiello%20and%20Vito%20Paolo%20Pastore%20and%20Marco%20Grangetto%0AAbstract%3A%20Humans%20can%20visualize%20new%20and%20unknown%20concepts%20from%20their%20natural%20language%20description%2C%20based%20on%20their%20experience%20and%20previous%20knowledge.%20Insipired%20by%20this%2C%20we%20present%20a%20way%20to%20extend%20this%20ability%20to%20Vision-Language%20Models%20%28VLMs%29%2C%20teaching%20them%20novel%20concepts%20by%20only%20using%20a%20textual%20description.%20We%20refer%20to%20this%20approach%20as%20Knowledge%20Transfer%20%28KT%29.%20Our%20hypothesis%20is%20that%20the%20knowledge%20of%20a%20pre-trained%20VLM%20can%20be%20re-used%20to%20represent%20previously%20unknown%20concepts.%20Provided%20with%20a%20textual%20description%20of%20the%20novel%20concept%2C%20KT%20works%20by%20aligning%20relevant%20features%20of%20the%20visual%20encoder%2C%20obtained%20through%20model%20inversion%2C%20to%20its%20text%20representation.%20Differently%20from%20approaches%20relying%20on%20visual%20examples%20or%20external%20generative%20models%2C%20KT%20transfers%20knowledge%20within%20the%20same%20VLM%20by%20injecting%20visual%20knowledge%20directly%20from%20the%20text.%20Through%20an%20extensive%20evaluation%20on%20several%20VLM%20tasks%2C%20including%20classification%2C%20segmentation%2C%20image-text%20retrieval%2C%20and%20captioning%2C%20we%20show%20that%3A%201%29%20KT%20can%20efficiently%20introduce%20new%20visual%20concepts%20from%20a%20single%20textual%20description%3B%202%29%20the%20same%20principle%20can%20be%20used%20to%20refine%20the%20representation%20of%20existing%20concepts%3B%20and%203%29%20KT%20significantly%20improves%20the%20performance%20of%20zero-shot%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2411.15611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIf%2520you%2520can%2520describe%2520it%252C%2520they%2520can%2520see%2520it%253A%2520Cross-Modal%2520Learning%2520of%2520Visual%2520Concepts%2520from%2520Textual%2520Descriptions%26entry.906535625%3DCarlo%2520Alberto%2520Barbano%2520and%2520Luca%2520Molinaro%2520and%2520Massimiliano%2520Ciranni%2520and%2520Emanuele%2520Aiello%2520and%2520Vito%2520Paolo%2520Pastore%2520and%2520Marco%2520Grangetto%26entry.1292438233%3DHumans%2520can%2520visualize%2520new%2520and%2520unknown%2520concepts%2520from%2520their%2520natural%2520language%2520description%252C%2520based%2520on%2520their%2520experience%2520and%2520previous%2520knowledge.%2520Insipired%2520by%2520this%252C%2520we%2520present%2520a%2520way%2520to%2520extend%2520this%2520ability%2520to%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520teaching%2520them%2520novel%2520concepts%2520by%2520only%2520using%2520a%2520textual%2520description.%2520We%2520refer%2520to%2520this%2520approach%2520as%2520Knowledge%2520Transfer%2520%2528KT%2529.%2520Our%2520hypothesis%2520is%2520that%2520the%2520knowledge%2520of%2520a%2520pre-trained%2520VLM%2520can%2520be%2520re-used%2520to%2520represent%2520previously%2520unknown%2520concepts.%2520Provided%2520with%2520a%2520textual%2520description%2520of%2520the%2520novel%2520concept%252C%2520KT%2520works%2520by%2520aligning%2520relevant%2520features%2520of%2520the%2520visual%2520encoder%252C%2520obtained%2520through%2520model%2520inversion%252C%2520to%2520its%2520text%2520representation.%2520Differently%2520from%2520approaches%2520relying%2520on%2520visual%2520examples%2520or%2520external%2520generative%2520models%252C%2520KT%2520transfers%2520knowledge%2520within%2520the%2520same%2520VLM%2520by%2520injecting%2520visual%2520knowledge%2520directly%2520from%2520the%2520text.%2520Through%2520an%2520extensive%2520evaluation%2520on%2520several%2520VLM%2520tasks%252C%2520including%2520classification%252C%2520segmentation%252C%2520image-text%2520retrieval%252C%2520and%2520captioning%252C%2520we%2520show%2520that%253A%25201%2529%2520KT%2520can%2520efficiently%2520introduce%2520new%2520visual%2520concepts%2520from%2520a%2520single%2520textual%2520description%253B%25202%2529%2520the%2520same%2520principle%2520can%2520be%2520used%2520to%2520refine%2520the%2520representation%2520of%2520existing%2520concepts%253B%2520and%25203%2529%2520KT%2520significantly%2520improves%2520the%2520performance%2520of%2520zero-shot%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=If%20you%20can%20describe%20it%2C%20they%20can%20see%20it%3A%20Cross-Modal%20Learning%20of%20Visual%20Concepts%20from%20Textual%20Descriptions&entry.906535625=Carlo%20Alberto%20Barbano%20and%20Luca%20Molinaro%20and%20Massimiliano%20Ciranni%20and%20Emanuele%20Aiello%20and%20Vito%20Paolo%20Pastore%20and%20Marco%20Grangetto&entry.1292438233=Humans%20can%20visualize%20new%20and%20unknown%20concepts%20from%20their%20natural%20language%20description%2C%20based%20on%20their%20experience%20and%20previous%20knowledge.%20Insipired%20by%20this%2C%20we%20present%20a%20way%20to%20extend%20this%20ability%20to%20Vision-Language%20Models%20%28VLMs%29%2C%20teaching%20them%20novel%20concepts%20by%20only%20using%20a%20textual%20description.%20We%20refer%20to%20this%20approach%20as%20Knowledge%20Transfer%20%28KT%29.%20Our%20hypothesis%20is%20that%20the%20knowledge%20of%20a%20pre-trained%20VLM%20can%20be%20re-used%20to%20represent%20previously%20unknown%20concepts.%20Provided%20with%20a%20textual%20description%20of%20the%20novel%20concept%2C%20KT%20works%20by%20aligning%20relevant%20features%20of%20the%20visual%20encoder%2C%20obtained%20through%20model%20inversion%2C%20to%20its%20text%20representation.%20Differently%20from%20approaches%20relying%20on%20visual%20examples%20or%20external%20generative%20models%2C%20KT%20transfers%20knowledge%20within%20the%20same%20VLM%20by%20injecting%20visual%20knowledge%20directly%20from%20the%20text.%20Through%20an%20extensive%20evaluation%20on%20several%20VLM%20tasks%2C%20including%20classification%2C%20segmentation%2C%20image-text%20retrieval%2C%20and%20captioning%2C%20we%20show%20that%3A%201%29%20KT%20can%20efficiently%20introduce%20new%20visual%20concepts%20from%20a%20single%20textual%20description%3B%202%29%20the%20same%20principle%20can%20be%20used%20to%20refine%20the%20representation%20of%20existing%20concepts%3B%20and%203%29%20KT%20significantly%20improves%20the%20performance%20of%20zero-shot%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2411.15611v2&entry.124074799=Read"},
{"title": "A Preprocessing Framework for Video Machine Vision under Compression", "author": "Fei Zhao and Mengxi Guo and Shijie Zhao and Junlin Li and Li Zhang and Xiaodong Xie", "abstract": "There has been a growing trend in compressing and transmitting videos from terminals for machine vision tasks. Nevertheless, most video coding optimization method focus on minimizing distortion according to human perceptual metrics, overlooking the heightened demands posed by machine vision systems. In this paper, we propose a video preprocessing framework tailored for machine vision tasks to address this challenge. The proposed method incorporates a neural preprocessor which retaining crucial information for subsequent tasks, resulting in the boosting of rate-accuracy performance. We further introduce a differentiable virtual codec to provide constraints on rate and distortion during the training stage. We directly apply widely used standard codecs for testing. Therefore, our solution can be easily applied to real-world scenarios. We conducted extensive experiments evaluating our compression method on two typical downstream tasks with various backbone networks. The experimental results indicate that our approach can save over 15% of bitrate compared to using only the standard codec anchor version.", "link": "http://arxiv.org/abs/2512.15331v1", "date": "2025-12-17", "relevancy": 2.816, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Preprocessing%20Framework%20for%20Video%20Machine%20Vision%20under%20Compression&body=Title%3A%20A%20Preprocessing%20Framework%20for%20Video%20Machine%20Vision%20under%20Compression%0AAuthor%3A%20Fei%20Zhao%20and%20Mengxi%20Guo%20and%20Shijie%20Zhao%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Xiaodong%20Xie%0AAbstract%3A%20There%20has%20been%20a%20growing%20trend%20in%20compressing%20and%20transmitting%20videos%20from%20terminals%20for%20machine%20vision%20tasks.%20Nevertheless%2C%20most%20video%20coding%20optimization%20method%20focus%20on%20minimizing%20distortion%20according%20to%20human%20perceptual%20metrics%2C%20overlooking%20the%20heightened%20demands%20posed%20by%20machine%20vision%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20video%20preprocessing%20framework%20tailored%20for%20machine%20vision%20tasks%20to%20address%20this%20challenge.%20The%20proposed%20method%20incorporates%20a%20neural%20preprocessor%20which%20retaining%20crucial%20information%20for%20subsequent%20tasks%2C%20resulting%20in%20the%20boosting%20of%20rate-accuracy%20performance.%20We%20further%20introduce%20a%20differentiable%20virtual%20codec%20to%20provide%20constraints%20on%20rate%20and%20distortion%20during%20the%20training%20stage.%20We%20directly%20apply%20widely%20used%20standard%20codecs%20for%20testing.%20Therefore%2C%20our%20solution%20can%20be%20easily%20applied%20to%20real-world%20scenarios.%20We%20conducted%20extensive%20experiments%20evaluating%20our%20compression%20method%20on%20two%20typical%20downstream%20tasks%20with%20various%20backbone%20networks.%20The%20experimental%20results%20indicate%20that%20our%20approach%20can%20save%20over%2015%25%20of%20bitrate%20compared%20to%20using%20only%20the%20standard%20codec%20anchor%20version.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Preprocessing%2520Framework%2520for%2520Video%2520Machine%2520Vision%2520under%2520Compression%26entry.906535625%3DFei%2520Zhao%2520and%2520Mengxi%2520Guo%2520and%2520Shijie%2520Zhao%2520and%2520Junlin%2520Li%2520and%2520Li%2520Zhang%2520and%2520Xiaodong%2520Xie%26entry.1292438233%3DThere%2520has%2520been%2520a%2520growing%2520trend%2520in%2520compressing%2520and%2520transmitting%2520videos%2520from%2520terminals%2520for%2520machine%2520vision%2520tasks.%2520Nevertheless%252C%2520most%2520video%2520coding%2520optimization%2520method%2520focus%2520on%2520minimizing%2520distortion%2520according%2520to%2520human%2520perceptual%2520metrics%252C%2520overlooking%2520the%2520heightened%2520demands%2520posed%2520by%2520machine%2520vision%2520systems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520video%2520preprocessing%2520framework%2520tailored%2520for%2520machine%2520vision%2520tasks%2520to%2520address%2520this%2520challenge.%2520The%2520proposed%2520method%2520incorporates%2520a%2520neural%2520preprocessor%2520which%2520retaining%2520crucial%2520information%2520for%2520subsequent%2520tasks%252C%2520resulting%2520in%2520the%2520boosting%2520of%2520rate-accuracy%2520performance.%2520We%2520further%2520introduce%2520a%2520differentiable%2520virtual%2520codec%2520to%2520provide%2520constraints%2520on%2520rate%2520and%2520distortion%2520during%2520the%2520training%2520stage.%2520We%2520directly%2520apply%2520widely%2520used%2520standard%2520codecs%2520for%2520testing.%2520Therefore%252C%2520our%2520solution%2520can%2520be%2520easily%2520applied%2520to%2520real-world%2520scenarios.%2520We%2520conducted%2520extensive%2520experiments%2520evaluating%2520our%2520compression%2520method%2520on%2520two%2520typical%2520downstream%2520tasks%2520with%2520various%2520backbone%2520networks.%2520The%2520experimental%2520results%2520indicate%2520that%2520our%2520approach%2520can%2520save%2520over%252015%2525%2520of%2520bitrate%2520compared%2520to%2520using%2520only%2520the%2520standard%2520codec%2520anchor%2520version.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Preprocessing%20Framework%20for%20Video%20Machine%20Vision%20under%20Compression&entry.906535625=Fei%20Zhao%20and%20Mengxi%20Guo%20and%20Shijie%20Zhao%20and%20Junlin%20Li%20and%20Li%20Zhang%20and%20Xiaodong%20Xie&entry.1292438233=There%20has%20been%20a%20growing%20trend%20in%20compressing%20and%20transmitting%20videos%20from%20terminals%20for%20machine%20vision%20tasks.%20Nevertheless%2C%20most%20video%20coding%20optimization%20method%20focus%20on%20minimizing%20distortion%20according%20to%20human%20perceptual%20metrics%2C%20overlooking%20the%20heightened%20demands%20posed%20by%20machine%20vision%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20video%20preprocessing%20framework%20tailored%20for%20machine%20vision%20tasks%20to%20address%20this%20challenge.%20The%20proposed%20method%20incorporates%20a%20neural%20preprocessor%20which%20retaining%20crucial%20information%20for%20subsequent%20tasks%2C%20resulting%20in%20the%20boosting%20of%20rate-accuracy%20performance.%20We%20further%20introduce%20a%20differentiable%20virtual%20codec%20to%20provide%20constraints%20on%20rate%20and%20distortion%20during%20the%20training%20stage.%20We%20directly%20apply%20widely%20used%20standard%20codecs%20for%20testing.%20Therefore%2C%20our%20solution%20can%20be%20easily%20applied%20to%20real-world%20scenarios.%20We%20conducted%20extensive%20experiments%20evaluating%20our%20compression%20method%20on%20two%20typical%20downstream%20tasks%20with%20various%20backbone%20networks.%20The%20experimental%20results%20indicate%20that%20our%20approach%20can%20save%20over%2015%25%20of%20bitrate%20compared%20to%20using%20only%20the%20standard%20codec%20anchor%20version.&entry.1838667208=http%3A//arxiv.org/abs/2512.15331v1&entry.124074799=Read"},
{"title": "SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis", "author": "Maximilian Kellner and Mariana Ferrandon Cervantes and Yuandong Pan and Ruodan Lu and Ioannis Brilakis and Alexander Reiterer", "abstract": "We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.", "link": "http://arxiv.org/abs/2512.15369v1", "date": "2025-12-17", "relevancy": 2.8101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemanticBridge%20--%20A%20Dataset%20for%203D%20Semantic%20Segmentation%20of%20Bridges%20and%20Domain%20Gap%20Analysis&body=Title%3A%20SemanticBridge%20--%20A%20Dataset%20for%203D%20Semantic%20Segmentation%20of%20Bridges%20and%20Domain%20Gap%20Analysis%0AAuthor%3A%20Maximilian%20Kellner%20and%20Mariana%20Ferrandon%20Cervantes%20and%20Yuandong%20Pan%20and%20Ruodan%20Lu%20and%20Ioannis%20Brilakis%20and%20Alexander%20Reiterer%0AAbstract%3A%20We%20propose%20a%20novel%20dataset%20that%20has%20been%20specifically%20designed%20for%203D%20semantic%20segmentation%20of%20bridges%20and%20the%20domain%20gap%20analysis%20caused%20by%20varying%20sensors.%20This%20addresses%20a%20critical%20need%20in%20the%20field%20of%20infrastructure%20inspection%20and%20maintenance%2C%20which%20is%20essential%20for%20modern%20society.%20The%20dataset%20comprises%20high-resolution%203D%20scans%20of%20a%20diverse%20range%20of%20bridge%20structures%20from%20various%20countries%2C%20with%20detailed%20semantic%20labels%20provided%20for%20each.%20Our%20initial%20objective%20is%20to%20facilitate%20accurate%20and%20automated%20segmentation%20of%20bridge%20components%2C%20thereby%20advancing%20the%20structural%20health%20monitoring%20practice.%20To%20evaluate%20the%20effectiveness%20of%20existing%203D%20deep%20learning%20models%20on%20this%20novel%20dataset%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20three%20distinct%20state-of-the-art%20architectures.%20Furthermore%2C%20we%20present%20data%20acquired%20through%20diverse%20sensors%20to%20quantify%20the%20domain%20gap%20resulting%20from%20sensor%20variations.%20Our%20findings%20indicate%20that%20all%20architectures%20demonstrate%20robust%20performance%20on%20the%20specified%20task.%20However%2C%20the%20domain%20gap%20can%20potentially%20lead%20to%20a%20decline%20in%20the%20performance%20of%20up%20to%2011.4%25%20mIoU.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemanticBridge%2520--%2520A%2520Dataset%2520for%25203D%2520Semantic%2520Segmentation%2520of%2520Bridges%2520and%2520Domain%2520Gap%2520Analysis%26entry.906535625%3DMaximilian%2520Kellner%2520and%2520Mariana%2520Ferrandon%2520Cervantes%2520and%2520Yuandong%2520Pan%2520and%2520Ruodan%2520Lu%2520and%2520Ioannis%2520Brilakis%2520and%2520Alexander%2520Reiterer%26entry.1292438233%3DWe%2520propose%2520a%2520novel%2520dataset%2520that%2520has%2520been%2520specifically%2520designed%2520for%25203D%2520semantic%2520segmentation%2520of%2520bridges%2520and%2520the%2520domain%2520gap%2520analysis%2520caused%2520by%2520varying%2520sensors.%2520This%2520addresses%2520a%2520critical%2520need%2520in%2520the%2520field%2520of%2520infrastructure%2520inspection%2520and%2520maintenance%252C%2520which%2520is%2520essential%2520for%2520modern%2520society.%2520The%2520dataset%2520comprises%2520high-resolution%25203D%2520scans%2520of%2520a%2520diverse%2520range%2520of%2520bridge%2520structures%2520from%2520various%2520countries%252C%2520with%2520detailed%2520semantic%2520labels%2520provided%2520for%2520each.%2520Our%2520initial%2520objective%2520is%2520to%2520facilitate%2520accurate%2520and%2520automated%2520segmentation%2520of%2520bridge%2520components%252C%2520thereby%2520advancing%2520the%2520structural%2520health%2520monitoring%2520practice.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520existing%25203D%2520deep%2520learning%2520models%2520on%2520this%2520novel%2520dataset%252C%2520we%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520three%2520distinct%2520state-of-the-art%2520architectures.%2520Furthermore%252C%2520we%2520present%2520data%2520acquired%2520through%2520diverse%2520sensors%2520to%2520quantify%2520the%2520domain%2520gap%2520resulting%2520from%2520sensor%2520variations.%2520Our%2520findings%2520indicate%2520that%2520all%2520architectures%2520demonstrate%2520robust%2520performance%2520on%2520the%2520specified%2520task.%2520However%252C%2520the%2520domain%2520gap%2520can%2520potentially%2520lead%2520to%2520a%2520decline%2520in%2520the%2520performance%2520of%2520up%2520to%252011.4%2525%2520mIoU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemanticBridge%20--%20A%20Dataset%20for%203D%20Semantic%20Segmentation%20of%20Bridges%20and%20Domain%20Gap%20Analysis&entry.906535625=Maximilian%20Kellner%20and%20Mariana%20Ferrandon%20Cervantes%20and%20Yuandong%20Pan%20and%20Ruodan%20Lu%20and%20Ioannis%20Brilakis%20and%20Alexander%20Reiterer&entry.1292438233=We%20propose%20a%20novel%20dataset%20that%20has%20been%20specifically%20designed%20for%203D%20semantic%20segmentation%20of%20bridges%20and%20the%20domain%20gap%20analysis%20caused%20by%20varying%20sensors.%20This%20addresses%20a%20critical%20need%20in%20the%20field%20of%20infrastructure%20inspection%20and%20maintenance%2C%20which%20is%20essential%20for%20modern%20society.%20The%20dataset%20comprises%20high-resolution%203D%20scans%20of%20a%20diverse%20range%20of%20bridge%20structures%20from%20various%20countries%2C%20with%20detailed%20semantic%20labels%20provided%20for%20each.%20Our%20initial%20objective%20is%20to%20facilitate%20accurate%20and%20automated%20segmentation%20of%20bridge%20components%2C%20thereby%20advancing%20the%20structural%20health%20monitoring%20practice.%20To%20evaluate%20the%20effectiveness%20of%20existing%203D%20deep%20learning%20models%20on%20this%20novel%20dataset%2C%20we%20conduct%20a%20comprehensive%20analysis%20of%20three%20distinct%20state-of-the-art%20architectures.%20Furthermore%2C%20we%20present%20data%20acquired%20through%20diverse%20sensors%20to%20quantify%20the%20domain%20gap%20resulting%20from%20sensor%20variations.%20Our%20findings%20indicate%20that%20all%20architectures%20demonstrate%20robust%20performance%20on%20the%20specified%20task.%20However%2C%20the%20domain%20gap%20can%20potentially%20lead%20to%20a%20decline%20in%20the%20performance%20of%20up%20to%2011.4%25%20mIoU.&entry.1838667208=http%3A//arxiv.org/abs/2512.15369v1&entry.124074799=Read"},
{"title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI", "author": "Shravya Kanchi and Neal Mangaokar and Aravind Cheruvu and Sifat Muhammad Abdullah and Shirin Nilizadeh and Atul Prakash and Bimal Viswanath", "abstract": "Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.", "link": "http://arxiv.org/abs/2507.06092v2", "date": "2025-12-17", "relevancy": 2.7263, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5675}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5445}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI&body=Title%3A%20Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI%0AAuthor%3A%20Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath%0AAbstract%3A%20Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%20tasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%20advancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%20performance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%20the%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%20address%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%20augmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%20techniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%20across%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%20introduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%20synthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%20performance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%20in%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%20Furthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%20concept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%20process.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%20initialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%20characteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%20distributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%20GenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%20tools%20designed%20for%20security%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Data%2520Challenges%2520in%2520ML-based%2520Security%2520Tasks%253A%2520Lessons%2520from%2520Integrating%2520Generative%2520AI%26entry.906535625%3DShravya%2520Kanchi%2520and%2520Neal%2520Mangaokar%2520and%2520Aravind%2520Cheruvu%2520and%2520Sifat%2520Muhammad%2520Abdullah%2520and%2520Shirin%2520Nilizadeh%2520and%2520Atul%2520Prakash%2520and%2520Bimal%2520Viswanath%26entry.1292438233%3DMachine%2520learning-based%2520supervised%2520classifiers%2520are%2520widely%2520used%2520for%2520security%2520tasks%252C%2520and%2520their%2520improvement%2520has%2520been%2520largely%2520focused%2520on%2520algorithmic%2520advancements.%2520We%2520argue%2520that%2520data%2520challenges%2520that%2520negatively%2520impact%2520the%2520performance%2520of%2520these%2520classifiers%2520have%2520received%2520limited%2520attention.%2520We%2520address%2520the%2520following%2520research%2520question%253A%2520Can%2520developments%2520in%2520Generative%2520AI%2520%2528GenAI%2529%2520address%2520these%2520data%2520challenges%2520and%2520improve%2520classifier%2520performance%253F%2520We%2520propose%2520augmenting%2520training%2520datasets%2520with%2520synthetic%2520data%2520generated%2520using%2520GenAI%2520techniques%2520to%2520improve%2520classifier%2520generalization.%2520We%2520evaluate%2520this%2520approach%2520across%25207%2520diverse%2520security%2520tasks%2520using%25206%2520state-of-the-art%2520GenAI%2520methods%2520and%2520introduce%2520a%2520novel%2520GenAI%2520scheme%2520called%2520Nimai%2520that%2520enables%2520highly%2520controlled%2520data%2520synthesis.%2520We%2520find%2520that%2520GenAI%2520techniques%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520security%2520classifiers%252C%2520achieving%2520improvements%2520of%2520up%2520to%252032.6%2525%2520even%2520in%2520severely%2520data-constrained%2520settings%2520%2528only%2520~180%2520training%2520samples%2529.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520GenAI%2520can%2520facilitate%2520rapid%2520adaptation%2520to%2520concept%2520drift%2520post-deployment%252C%2520requiring%2520minimal%2520labeling%2520in%2520the%2520adjustment%2520process.%2520Despite%2520successes%252C%2520our%2520study%2520finds%2520that%2520some%2520GenAI%2520schemes%2520struggle%2520to%2520initialize%2520%2528train%2520and%2520produce%2520data%2529%2520on%2520certain%2520security%2520tasks.%2520We%2520also%2520identify%2520characteristics%2520of%2520specific%2520tasks%252C%2520such%2520as%2520noisy%2520labels%252C%2520overlapping%2520class%2520distributions%252C%2520and%2520sparse%2520feature%2520vectors%252C%2520which%2520hinder%2520performance%2520boost%2520using%2520GenAI.%2520We%2520believe%2520that%2520our%2520study%2520will%2520drive%2520the%2520development%2520of%2520future%2520GenAI%2520tools%2520designed%2520for%2520security%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Data%20Challenges%20in%20ML-based%20Security%20Tasks%3A%20Lessons%20from%20Integrating%20Generative%20AI&entry.906535625=Shravya%20Kanchi%20and%20Neal%20Mangaokar%20and%20Aravind%20Cheruvu%20and%20Sifat%20Muhammad%20Abdullah%20and%20Shirin%20Nilizadeh%20and%20Atul%20Prakash%20and%20Bimal%20Viswanath&entry.1292438233=Machine%20learning-based%20supervised%20classifiers%20are%20widely%20used%20for%20security%20tasks%2C%20and%20their%20improvement%20has%20been%20largely%20focused%20on%20algorithmic%20advancements.%20We%20argue%20that%20data%20challenges%20that%20negatively%20impact%20the%20performance%20of%20these%20classifiers%20have%20received%20limited%20attention.%20We%20address%20the%20following%20research%20question%3A%20Can%20developments%20in%20Generative%20AI%20%28GenAI%29%20address%20these%20data%20challenges%20and%20improve%20classifier%20performance%3F%20We%20propose%20augmenting%20training%20datasets%20with%20synthetic%20data%20generated%20using%20GenAI%20techniques%20to%20improve%20classifier%20generalization.%20We%20evaluate%20this%20approach%20across%207%20diverse%20security%20tasks%20using%206%20state-of-the-art%20GenAI%20methods%20and%20introduce%20a%20novel%20GenAI%20scheme%20called%20Nimai%20that%20enables%20highly%20controlled%20data%20synthesis.%20We%20find%20that%20GenAI%20techniques%20can%20significantly%20improve%20the%20performance%20of%20security%20classifiers%2C%20achieving%20improvements%20of%20up%20to%2032.6%25%20even%20in%20severely%20data-constrained%20settings%20%28only%20~180%20training%20samples%29.%20Furthermore%2C%20we%20demonstrate%20that%20GenAI%20can%20facilitate%20rapid%20adaptation%20to%20concept%20drift%20post-deployment%2C%20requiring%20minimal%20labeling%20in%20the%20adjustment%20process.%20Despite%20successes%2C%20our%20study%20finds%20that%20some%20GenAI%20schemes%20struggle%20to%20initialize%20%28train%20and%20produce%20data%29%20on%20certain%20security%20tasks.%20We%20also%20identify%20characteristics%20of%20specific%20tasks%2C%20such%20as%20noisy%20labels%2C%20overlapping%20class%20distributions%2C%20and%20sparse%20feature%20vectors%2C%20which%20hinder%20performance%20boost%20using%20GenAI.%20We%20believe%20that%20our%20study%20will%20drive%20the%20development%20of%20future%20GenAI%20tools%20designed%20for%20security%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2507.06092v2&entry.124074799=Read"},
{"title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction", "author": "Junqiao Fan and Yunjiao Zhou and Yizhuo Yang and Xinyuan Cui and Jiarui Zhang and Lihua Xie and Jianfei Yang and Chris Xiaoxuan Lu and Fangqiang Ding", "abstract": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.", "link": "http://arxiv.org/abs/2512.12378v2", "date": "2025-12-17", "relevancy": 2.6913, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5487}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5412}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M4Human%3A%20A%20Large-Scale%20Multimodal%20mmWave%20Radar%20Benchmark%20for%20Human%20Mesh%20Reconstruction&body=Title%3A%20M4Human%3A%20A%20Large-Scale%20Multimodal%20mmWave%20Radar%20Benchmark%20for%20Human%20Mesh%20Reconstruction%0AAuthor%3A%20Junqiao%20Fan%20and%20Yunjiao%20Zhou%20and%20Yizhuo%20Yang%20and%20Xinyuan%20Cui%20and%20Jiarui%20Zhang%20and%20Lihua%20Xie%20and%20Jianfei%20Yang%20and%20Chris%20Xiaoxuan%20Lu%20and%20Fangqiang%20Ding%0AAbstract%3A%20Human%20mesh%20reconstruction%20%28HMR%29%20provides%20direct%20insights%20into%20body-environment%20interaction%2C%20which%20enables%20various%20immersive%20applications.%20While%20existing%20large-scale%20HMR%20datasets%20rely%20heavily%20on%20line-of-sight%20RGB%20input%2C%20vision-based%20sensing%20is%20limited%20by%20occlusion%2C%20lighting%20variation%2C%20and%20privacy%20concerns.%20To%20overcome%20these%20limitations%2C%20recent%20efforts%20have%20explored%20radio-frequency%20%28RF%29%20mmWave%20radar%20for%20privacy-preserving%20indoor%20human%20sensing.%20However%2C%20current%20radar%20datasets%20are%20constrained%20by%20sparse%20skeleton%20labels%2C%20limited%20scale%2C%20and%20simple%20in-place%20actions.%20To%20advance%20the%20HMR%20research%20community%2C%20we%20introduce%20M4Human%2C%20the%20current%20largest-scale%20%28661K-frame%29%20%28%249%5Ctimes%24%20prior%20largest%29%20multimodal%20benchmark%2C%20featuring%20high-resolution%20mmWave%20radar%2C%20RGB%2C%20and%20depth%20data.%20M4Human%20provides%20both%20raw%20radar%20tensors%20%28RT%29%20and%20processed%20radar%20point%20clouds%20%28RPC%29%20to%20enable%20research%20across%20different%20levels%20of%20RF%20signal%20granularity.%20M4Human%20includes%20high-quality%20motion%20capture%20%28MoCap%29%20annotations%20with%203D%20meshes%20and%20global%20trajectories%2C%20and%20spans%2020%20subjects%20and%2050%20diverse%20actions%2C%20including%20in-place%2C%20sit-in-place%2C%20and%20free-space%20sports%20or%20rehabilitation%20movements.%20We%20establish%20benchmarks%20on%20both%20RT%20and%20RPC%20modalities%2C%20as%20well%20as%20multimodal%20fusion%20with%20RGB-D%20modalities.%20Extensive%20results%20highlight%20the%20significance%20of%20M4Human%20for%20radar-based%20human%20modeling%20while%20revealing%20persistent%20challenges%20under%20fast%2C%20unconstrained%20motion.%20The%20dataset%20and%20code%20will%20be%20released%20after%20the%20paper%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM4Human%253A%2520A%2520Large-Scale%2520Multimodal%2520mmWave%2520Radar%2520Benchmark%2520for%2520Human%2520Mesh%2520Reconstruction%26entry.906535625%3DJunqiao%2520Fan%2520and%2520Yunjiao%2520Zhou%2520and%2520Yizhuo%2520Yang%2520and%2520Xinyuan%2520Cui%2520and%2520Jiarui%2520Zhang%2520and%2520Lihua%2520Xie%2520and%2520Jianfei%2520Yang%2520and%2520Chris%2520Xiaoxuan%2520Lu%2520and%2520Fangqiang%2520Ding%26entry.1292438233%3DHuman%2520mesh%2520reconstruction%2520%2528HMR%2529%2520provides%2520direct%2520insights%2520into%2520body-environment%2520interaction%252C%2520which%2520enables%2520various%2520immersive%2520applications.%2520While%2520existing%2520large-scale%2520HMR%2520datasets%2520rely%2520heavily%2520on%2520line-of-sight%2520RGB%2520input%252C%2520vision-based%2520sensing%2520is%2520limited%2520by%2520occlusion%252C%2520lighting%2520variation%252C%2520and%2520privacy%2520concerns.%2520To%2520overcome%2520these%2520limitations%252C%2520recent%2520efforts%2520have%2520explored%2520radio-frequency%2520%2528RF%2529%2520mmWave%2520radar%2520for%2520privacy-preserving%2520indoor%2520human%2520sensing.%2520However%252C%2520current%2520radar%2520datasets%2520are%2520constrained%2520by%2520sparse%2520skeleton%2520labels%252C%2520limited%2520scale%252C%2520and%2520simple%2520in-place%2520actions.%2520To%2520advance%2520the%2520HMR%2520research%2520community%252C%2520we%2520introduce%2520M4Human%252C%2520the%2520current%2520largest-scale%2520%2528661K-frame%2529%2520%2528%25249%255Ctimes%2524%2520prior%2520largest%2529%2520multimodal%2520benchmark%252C%2520featuring%2520high-resolution%2520mmWave%2520radar%252C%2520RGB%252C%2520and%2520depth%2520data.%2520M4Human%2520provides%2520both%2520raw%2520radar%2520tensors%2520%2528RT%2529%2520and%2520processed%2520radar%2520point%2520clouds%2520%2528RPC%2529%2520to%2520enable%2520research%2520across%2520different%2520levels%2520of%2520RF%2520signal%2520granularity.%2520M4Human%2520includes%2520high-quality%2520motion%2520capture%2520%2528MoCap%2529%2520annotations%2520with%25203D%2520meshes%2520and%2520global%2520trajectories%252C%2520and%2520spans%252020%2520subjects%2520and%252050%2520diverse%2520actions%252C%2520including%2520in-place%252C%2520sit-in-place%252C%2520and%2520free-space%2520sports%2520or%2520rehabilitation%2520movements.%2520We%2520establish%2520benchmarks%2520on%2520both%2520RT%2520and%2520RPC%2520modalities%252C%2520as%2520well%2520as%2520multimodal%2520fusion%2520with%2520RGB-D%2520modalities.%2520Extensive%2520results%2520highlight%2520the%2520significance%2520of%2520M4Human%2520for%2520radar-based%2520human%2520modeling%2520while%2520revealing%2520persistent%2520challenges%2520under%2520fast%252C%2520unconstrained%2520motion.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520released%2520after%2520the%2520paper%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M4Human%3A%20A%20Large-Scale%20Multimodal%20mmWave%20Radar%20Benchmark%20for%20Human%20Mesh%20Reconstruction&entry.906535625=Junqiao%20Fan%20and%20Yunjiao%20Zhou%20and%20Yizhuo%20Yang%20and%20Xinyuan%20Cui%20and%20Jiarui%20Zhang%20and%20Lihua%20Xie%20and%20Jianfei%20Yang%20and%20Chris%20Xiaoxuan%20Lu%20and%20Fangqiang%20Ding&entry.1292438233=Human%20mesh%20reconstruction%20%28HMR%29%20provides%20direct%20insights%20into%20body-environment%20interaction%2C%20which%20enables%20various%20immersive%20applications.%20While%20existing%20large-scale%20HMR%20datasets%20rely%20heavily%20on%20line-of-sight%20RGB%20input%2C%20vision-based%20sensing%20is%20limited%20by%20occlusion%2C%20lighting%20variation%2C%20and%20privacy%20concerns.%20To%20overcome%20these%20limitations%2C%20recent%20efforts%20have%20explored%20radio-frequency%20%28RF%29%20mmWave%20radar%20for%20privacy-preserving%20indoor%20human%20sensing.%20However%2C%20current%20radar%20datasets%20are%20constrained%20by%20sparse%20skeleton%20labels%2C%20limited%20scale%2C%20and%20simple%20in-place%20actions.%20To%20advance%20the%20HMR%20research%20community%2C%20we%20introduce%20M4Human%2C%20the%20current%20largest-scale%20%28661K-frame%29%20%28%249%5Ctimes%24%20prior%20largest%29%20multimodal%20benchmark%2C%20featuring%20high-resolution%20mmWave%20radar%2C%20RGB%2C%20and%20depth%20data.%20M4Human%20provides%20both%20raw%20radar%20tensors%20%28RT%29%20and%20processed%20radar%20point%20clouds%20%28RPC%29%20to%20enable%20research%20across%20different%20levels%20of%20RF%20signal%20granularity.%20M4Human%20includes%20high-quality%20motion%20capture%20%28MoCap%29%20annotations%20with%203D%20meshes%20and%20global%20trajectories%2C%20and%20spans%2020%20subjects%20and%2050%20diverse%20actions%2C%20including%20in-place%2C%20sit-in-place%2C%20and%20free-space%20sports%20or%20rehabilitation%20movements.%20We%20establish%20benchmarks%20on%20both%20RT%20and%20RPC%20modalities%2C%20as%20well%20as%20multimodal%20fusion%20with%20RGB-D%20modalities.%20Extensive%20results%20highlight%20the%20significance%20of%20M4Human%20for%20radar-based%20human%20modeling%20while%20revealing%20persistent%20challenges%20under%20fast%2C%20unconstrained%20motion.%20The%20dataset%20and%20code%20will%20be%20released%20after%20the%20paper%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2512.12378v2&entry.124074799=Read"},
{"title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants", "author": "Vincent Huang and Dami Choi and Daniel D. Johnson and Sarah Schwettmann and Jacob Steinhardt", "abstract": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.", "link": "http://arxiv.org/abs/2512.15712v1", "date": "2025-12-17", "relevancy": 2.6739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Concept%20Decoders%3A%20Training%20Scalable%20End-to-End%20Interpretability%20Assistants&body=Title%3A%20Predictive%20Concept%20Decoders%3A%20Training%20Scalable%20End-to-End%20Interpretability%20Assistants%0AAuthor%3A%20Vincent%20Huang%20and%20Dami%20Choi%20and%20Daniel%20D.%20Johnson%20and%20Sarah%20Schwettmann%20and%20Jacob%20Steinhardt%0AAbstract%3A%20Interpreting%20the%20internal%20activations%20of%20neural%20networks%20can%20produce%20more%20faithful%20explanations%20of%20their%20behavior%2C%20but%20is%20difficult%20due%20to%20the%20complex%20structure%20of%20activation%20space.%20Existing%20approaches%20to%20scalable%20interpretability%20use%20hand-designed%20agents%20that%20make%20and%20test%20hypotheses%20about%20how%20internal%20activations%20relate%20to%20external%20behavior.%20We%20propose%20to%20instead%20turn%20this%20task%20into%20an%20end-to-end%20training%20objective%2C%20by%20training%20interpretability%20assistants%20to%20accurately%20predict%20model%20behavior%20from%20activations%20through%20a%20communication%20bottleneck.%20Specifically%2C%20an%20encoder%20compresses%20activations%20to%20a%20sparse%20list%20of%20concepts%2C%20and%20a%20decoder%20reads%20this%20list%20and%20answers%20a%20natural%20language%20question%20about%20the%20model.%20We%20show%20how%20to%20pretrain%20this%20assistant%20on%20large%20unstructured%20data%2C%20then%20finetune%20it%20to%20answer%20questions.%20The%20resulting%20architecture%2C%20which%20we%20call%20a%20Predictive%20Concept%20Decoder%2C%20enjoys%20favorable%20scaling%20properties%3A%20the%20auto-interp%20score%20of%20the%20bottleneck%20concepts%20improves%20with%20data%2C%20as%20does%20the%20performance%20on%20downstream%20applications.%20Specifically%2C%20PCDs%20can%20detect%20jailbreaks%2C%20secret%20hints%2C%20and%20implanted%20latent%20concepts%2C%20and%20are%20able%20to%20accurately%20surface%20latent%20user%20attributes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Concept%2520Decoders%253A%2520Training%2520Scalable%2520End-to-End%2520Interpretability%2520Assistants%26entry.906535625%3DVincent%2520Huang%2520and%2520Dami%2520Choi%2520and%2520Daniel%2520D.%2520Johnson%2520and%2520Sarah%2520Schwettmann%2520and%2520Jacob%2520Steinhardt%26entry.1292438233%3DInterpreting%2520the%2520internal%2520activations%2520of%2520neural%2520networks%2520can%2520produce%2520more%2520faithful%2520explanations%2520of%2520their%2520behavior%252C%2520but%2520is%2520difficult%2520due%2520to%2520the%2520complex%2520structure%2520of%2520activation%2520space.%2520Existing%2520approaches%2520to%2520scalable%2520interpretability%2520use%2520hand-designed%2520agents%2520that%2520make%2520and%2520test%2520hypotheses%2520about%2520how%2520internal%2520activations%2520relate%2520to%2520external%2520behavior.%2520We%2520propose%2520to%2520instead%2520turn%2520this%2520task%2520into%2520an%2520end-to-end%2520training%2520objective%252C%2520by%2520training%2520interpretability%2520assistants%2520to%2520accurately%2520predict%2520model%2520behavior%2520from%2520activations%2520through%2520a%2520communication%2520bottleneck.%2520Specifically%252C%2520an%2520encoder%2520compresses%2520activations%2520to%2520a%2520sparse%2520list%2520of%2520concepts%252C%2520and%2520a%2520decoder%2520reads%2520this%2520list%2520and%2520answers%2520a%2520natural%2520language%2520question%2520about%2520the%2520model.%2520We%2520show%2520how%2520to%2520pretrain%2520this%2520assistant%2520on%2520large%2520unstructured%2520data%252C%2520then%2520finetune%2520it%2520to%2520answer%2520questions.%2520The%2520resulting%2520architecture%252C%2520which%2520we%2520call%2520a%2520Predictive%2520Concept%2520Decoder%252C%2520enjoys%2520favorable%2520scaling%2520properties%253A%2520the%2520auto-interp%2520score%2520of%2520the%2520bottleneck%2520concepts%2520improves%2520with%2520data%252C%2520as%2520does%2520the%2520performance%2520on%2520downstream%2520applications.%2520Specifically%252C%2520PCDs%2520can%2520detect%2520jailbreaks%252C%2520secret%2520hints%252C%2520and%2520implanted%2520latent%2520concepts%252C%2520and%2520are%2520able%2520to%2520accurately%2520surface%2520latent%2520user%2520attributes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Concept%20Decoders%3A%20Training%20Scalable%20End-to-End%20Interpretability%20Assistants&entry.906535625=Vincent%20Huang%20and%20Dami%20Choi%20and%20Daniel%20D.%20Johnson%20and%20Sarah%20Schwettmann%20and%20Jacob%20Steinhardt&entry.1292438233=Interpreting%20the%20internal%20activations%20of%20neural%20networks%20can%20produce%20more%20faithful%20explanations%20of%20their%20behavior%2C%20but%20is%20difficult%20due%20to%20the%20complex%20structure%20of%20activation%20space.%20Existing%20approaches%20to%20scalable%20interpretability%20use%20hand-designed%20agents%20that%20make%20and%20test%20hypotheses%20about%20how%20internal%20activations%20relate%20to%20external%20behavior.%20We%20propose%20to%20instead%20turn%20this%20task%20into%20an%20end-to-end%20training%20objective%2C%20by%20training%20interpretability%20assistants%20to%20accurately%20predict%20model%20behavior%20from%20activations%20through%20a%20communication%20bottleneck.%20Specifically%2C%20an%20encoder%20compresses%20activations%20to%20a%20sparse%20list%20of%20concepts%2C%20and%20a%20decoder%20reads%20this%20list%20and%20answers%20a%20natural%20language%20question%20about%20the%20model.%20We%20show%20how%20to%20pretrain%20this%20assistant%20on%20large%20unstructured%20data%2C%20then%20finetune%20it%20to%20answer%20questions.%20The%20resulting%20architecture%2C%20which%20we%20call%20a%20Predictive%20Concept%20Decoder%2C%20enjoys%20favorable%20scaling%20properties%3A%20the%20auto-interp%20score%20of%20the%20bottleneck%20concepts%20improves%20with%20data%2C%20as%20does%20the%20performance%20on%20downstream%20applications.%20Specifically%2C%20PCDs%20can%20detect%20jailbreaks%2C%20secret%20hints%2C%20and%20implanted%20latent%20concepts%2C%20and%20are%20able%20to%20accurately%20surface%20latent%20user%20attributes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15712v1&entry.124074799=Read"},
{"title": "On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation", "author": "Roni Blushtein-Livnon and Osher Rafaeli and David Ioffe and Amir Boger and Karen Sandberg Esquenazi and Tal Svoray", "abstract": "Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.", "link": "http://arxiv.org/abs/2512.15564v1", "date": "2025-12-17", "relevancy": 2.6571, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effectiveness%20of%20Textual%20Prompting%20with%20Lightweight%20Fine-Tuning%20for%20SAM3%20Remote%20Sensing%20Segmentation&body=Title%3A%20On%20the%20Effectiveness%20of%20Textual%20Prompting%20with%20Lightweight%20Fine-Tuning%20for%20SAM3%20Remote%20Sensing%20Segmentation%0AAuthor%3A%20Roni%20Blushtein-Livnon%20and%20Osher%20Rafaeli%20and%20David%20Ioffe%20and%20Amir%20Boger%20and%20Karen%20Sandberg%20Esquenazi%20and%20Tal%20Svoray%0AAbstract%3A%20Remote%20sensing%20%28RS%29%20image%20segmentation%20is%20constrained%20by%20the%20limited%20availability%20of%20annotated%20data%20and%20a%20gap%20between%20overhead%20imagery%20and%20natural%20images%20used%20to%20train%20foundational%20models.%20This%20motivates%20effective%20adaptation%20under%20limited%20supervision.%20SAM3%20concept-driven%20framework%20generates%20masks%20from%20textual%20prompts%20without%20requiring%20task-specific%20modifications%2C%20which%20may%20enable%20this%20adaptation.%20We%20evaluate%20SAM3%20for%20RS%20imagery%20across%20four%20target%20types%2C%20comparing%20textual%2C%20geometric%2C%20and%20hybrid%20prompting%20strategies%2C%20under%20lightweight%20fine-tuning%20scales%20with%20increasing%20supervision%2C%20alongside%20zero-shot%20inference.%20Results%20show%20that%20combining%20semantic%20and%20geometric%20cues%20yields%20the%20highest%20performance%20across%20targets%20and%20metrics.%20Text-only%20prompting%20exhibits%20the%20lowest%20performance%2C%20with%20marked%20score%20gaps%20for%20irregularly%20shaped%20targets%2C%20reflecting%20limited%20semantic%20alignment%20between%20SAM3%20textual%20representations%20and%20their%20overhead%20appearances.%20Nevertheless%2C%20textual%20prompting%20with%20light%20fine-tuning%20offers%20a%20practical%20performance-effort%20trade-off%20for%20geometrically%20regular%20and%20visually%20salient%20targets.%20Across%20targets%2C%20performance%20improves%20between%20zero-shot%20inference%20and%20fine-tuning%2C%20followed%20by%20diminishing%20returns%20as%20the%20supervision%20scale%20increases.%20Namely%2C%20a%20modest%20geometric%20annotation%20effort%20is%20sufficient%20for%20effective%20adaptation.%20A%20persistent%20gap%20between%20Precision%20and%20IoU%20further%20indicates%20that%20under-segmentation%20and%20boundary%20inaccuracies%20remain%20prevalent%20error%20patterns%20in%20RS%20tasks%2C%20particularly%20for%20irregular%20and%20less%20prevalent%20targets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effectiveness%2520of%2520Textual%2520Prompting%2520with%2520Lightweight%2520Fine-Tuning%2520for%2520SAM3%2520Remote%2520Sensing%2520Segmentation%26entry.906535625%3DRoni%2520Blushtein-Livnon%2520and%2520Osher%2520Rafaeli%2520and%2520David%2520Ioffe%2520and%2520Amir%2520Boger%2520and%2520Karen%2520Sandberg%2520Esquenazi%2520and%2520Tal%2520Svoray%26entry.1292438233%3DRemote%2520sensing%2520%2528RS%2529%2520image%2520segmentation%2520is%2520constrained%2520by%2520the%2520limited%2520availability%2520of%2520annotated%2520data%2520and%2520a%2520gap%2520between%2520overhead%2520imagery%2520and%2520natural%2520images%2520used%2520to%2520train%2520foundational%2520models.%2520This%2520motivates%2520effective%2520adaptation%2520under%2520limited%2520supervision.%2520SAM3%2520concept-driven%2520framework%2520generates%2520masks%2520from%2520textual%2520prompts%2520without%2520requiring%2520task-specific%2520modifications%252C%2520which%2520may%2520enable%2520this%2520adaptation.%2520We%2520evaluate%2520SAM3%2520for%2520RS%2520imagery%2520across%2520four%2520target%2520types%252C%2520comparing%2520textual%252C%2520geometric%252C%2520and%2520hybrid%2520prompting%2520strategies%252C%2520under%2520lightweight%2520fine-tuning%2520scales%2520with%2520increasing%2520supervision%252C%2520alongside%2520zero-shot%2520inference.%2520Results%2520show%2520that%2520combining%2520semantic%2520and%2520geometric%2520cues%2520yields%2520the%2520highest%2520performance%2520across%2520targets%2520and%2520metrics.%2520Text-only%2520prompting%2520exhibits%2520the%2520lowest%2520performance%252C%2520with%2520marked%2520score%2520gaps%2520for%2520irregularly%2520shaped%2520targets%252C%2520reflecting%2520limited%2520semantic%2520alignment%2520between%2520SAM3%2520textual%2520representations%2520and%2520their%2520overhead%2520appearances.%2520Nevertheless%252C%2520textual%2520prompting%2520with%2520light%2520fine-tuning%2520offers%2520a%2520practical%2520performance-effort%2520trade-off%2520for%2520geometrically%2520regular%2520and%2520visually%2520salient%2520targets.%2520Across%2520targets%252C%2520performance%2520improves%2520between%2520zero-shot%2520inference%2520and%2520fine-tuning%252C%2520followed%2520by%2520diminishing%2520returns%2520as%2520the%2520supervision%2520scale%2520increases.%2520Namely%252C%2520a%2520modest%2520geometric%2520annotation%2520effort%2520is%2520sufficient%2520for%2520effective%2520adaptation.%2520A%2520persistent%2520gap%2520between%2520Precision%2520and%2520IoU%2520further%2520indicates%2520that%2520under-segmentation%2520and%2520boundary%2520inaccuracies%2520remain%2520prevalent%2520error%2520patterns%2520in%2520RS%2520tasks%252C%2520particularly%2520for%2520irregular%2520and%2520less%2520prevalent%2520targets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effectiveness%20of%20Textual%20Prompting%20with%20Lightweight%20Fine-Tuning%20for%20SAM3%20Remote%20Sensing%20Segmentation&entry.906535625=Roni%20Blushtein-Livnon%20and%20Osher%20Rafaeli%20and%20David%20Ioffe%20and%20Amir%20Boger%20and%20Karen%20Sandberg%20Esquenazi%20and%20Tal%20Svoray&entry.1292438233=Remote%20sensing%20%28RS%29%20image%20segmentation%20is%20constrained%20by%20the%20limited%20availability%20of%20annotated%20data%20and%20a%20gap%20between%20overhead%20imagery%20and%20natural%20images%20used%20to%20train%20foundational%20models.%20This%20motivates%20effective%20adaptation%20under%20limited%20supervision.%20SAM3%20concept-driven%20framework%20generates%20masks%20from%20textual%20prompts%20without%20requiring%20task-specific%20modifications%2C%20which%20may%20enable%20this%20adaptation.%20We%20evaluate%20SAM3%20for%20RS%20imagery%20across%20four%20target%20types%2C%20comparing%20textual%2C%20geometric%2C%20and%20hybrid%20prompting%20strategies%2C%20under%20lightweight%20fine-tuning%20scales%20with%20increasing%20supervision%2C%20alongside%20zero-shot%20inference.%20Results%20show%20that%20combining%20semantic%20and%20geometric%20cues%20yields%20the%20highest%20performance%20across%20targets%20and%20metrics.%20Text-only%20prompting%20exhibits%20the%20lowest%20performance%2C%20with%20marked%20score%20gaps%20for%20irregularly%20shaped%20targets%2C%20reflecting%20limited%20semantic%20alignment%20between%20SAM3%20textual%20representations%20and%20their%20overhead%20appearances.%20Nevertheless%2C%20textual%20prompting%20with%20light%20fine-tuning%20offers%20a%20practical%20performance-effort%20trade-off%20for%20geometrically%20regular%20and%20visually%20salient%20targets.%20Across%20targets%2C%20performance%20improves%20between%20zero-shot%20inference%20and%20fine-tuning%2C%20followed%20by%20diminishing%20returns%20as%20the%20supervision%20scale%20increases.%20Namely%2C%20a%20modest%20geometric%20annotation%20effort%20is%20sufficient%20for%20effective%20adaptation.%20A%20persistent%20gap%20between%20Precision%20and%20IoU%20further%20indicates%20that%20under-segmentation%20and%20boundary%20inaccuracies%20remain%20prevalent%20error%20patterns%20in%20RS%20tasks%2C%20particularly%20for%20irregular%20and%20less%20prevalent%20targets.&entry.1838667208=http%3A//arxiv.org/abs/2512.15564v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Make Audio Foundation Models more Explainable", "author": "Th\u00e9o Mariotte and Martin Lebourdais and Antonio Almud\u00e9var and Marie Tahon and Alfonso Ortega and Nicolas Dugu\u00e9", "abstract": "Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.", "link": "http://arxiv.org/abs/2509.24793v2", "date": "2025-12-17", "relevancy": 2.6431, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable&body=Title%3A%20Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable%0AAuthor%3A%20Th%C3%A9o%20Mariotte%20and%20Martin%20Lebourdais%20and%20Antonio%20Almud%C3%A9var%20and%20Marie%20Tahon%20and%20Alfonso%20Ortega%20and%20Nicolas%20Dugu%C3%A9%0AAbstract%3A%20Audio%20pretrained%20models%20are%20widely%20employed%20to%20solve%20various%20tasks%20in%20speech%20processing%2C%20sound%20event%20detection%2C%20or%20music%20information%20retrieval.%20However%2C%20the%20representations%20learned%20by%20these%20models%20are%20unclear%2C%20and%20their%20analysis%20mainly%20restricts%20to%20linear%20probing%20of%20the%20hidden%20representations.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20Sparse%20Autoencoders%20%28SAEs%29%20to%20analyze%20the%20hidden%20representations%20of%20pretrained%20models%2C%20focusing%20on%20a%20case%20study%20in%20singing%20technique%20classification.%20We%20first%20demonstrate%20that%20SAEs%20retain%20both%20information%20about%20the%20original%20representations%20and%20class%20labels%2C%20enabling%20their%20internal%20structure%20to%20provide%20insights%20into%20self-supervised%20learning%20systems.%20Furthermore%2C%20we%20show%20that%20SAEs%20enhance%20the%20disentanglement%20of%20vocal%20attributes%2C%20establishing%20them%20as%20an%20effective%20tool%20for%20identifying%20the%20underlying%20factors%20encoded%20in%20the%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Make%2520Audio%2520Foundation%2520Models%2520more%2520Explainable%26entry.906535625%3DTh%25C3%25A9o%2520Mariotte%2520and%2520Martin%2520Lebourdais%2520and%2520Antonio%2520Almud%25C3%25A9var%2520and%2520Marie%2520Tahon%2520and%2520Alfonso%2520Ortega%2520and%2520Nicolas%2520Dugu%25C3%25A9%26entry.1292438233%3DAudio%2520pretrained%2520models%2520are%2520widely%2520employed%2520to%2520solve%2520various%2520tasks%2520in%2520speech%2520processing%252C%2520sound%2520event%2520detection%252C%2520or%2520music%2520information%2520retrieval.%2520However%252C%2520the%2520representations%2520learned%2520by%2520these%2520models%2520are%2520unclear%252C%2520and%2520their%2520analysis%2520mainly%2520restricts%2520to%2520linear%2520probing%2520of%2520the%2520hidden%2520representations.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520use%2520of%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520to%2520analyze%2520the%2520hidden%2520representations%2520of%2520pretrained%2520models%252C%2520focusing%2520on%2520a%2520case%2520study%2520in%2520singing%2520technique%2520classification.%2520We%2520first%2520demonstrate%2520that%2520SAEs%2520retain%2520both%2520information%2520about%2520the%2520original%2520representations%2520and%2520class%2520labels%252C%2520enabling%2520their%2520internal%2520structure%2520to%2520provide%2520insights%2520into%2520self-supervised%2520learning%2520systems.%2520Furthermore%252C%2520we%2520show%2520that%2520SAEs%2520enhance%2520the%2520disentanglement%2520of%2520vocal%2520attributes%252C%2520establishing%2520them%2520as%2520an%2520effective%2520tool%2520for%2520identifying%2520the%2520underlying%2520factors%2520encoded%2520in%2520the%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Make%20Audio%20Foundation%20Models%20more%20Explainable&entry.906535625=Th%C3%A9o%20Mariotte%20and%20Martin%20Lebourdais%20and%20Antonio%20Almud%C3%A9var%20and%20Marie%20Tahon%20and%20Alfonso%20Ortega%20and%20Nicolas%20Dugu%C3%A9&entry.1292438233=Audio%20pretrained%20models%20are%20widely%20employed%20to%20solve%20various%20tasks%20in%20speech%20processing%2C%20sound%20event%20detection%2C%20or%20music%20information%20retrieval.%20However%2C%20the%20representations%20learned%20by%20these%20models%20are%20unclear%2C%20and%20their%20analysis%20mainly%20restricts%20to%20linear%20probing%20of%20the%20hidden%20representations.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20Sparse%20Autoencoders%20%28SAEs%29%20to%20analyze%20the%20hidden%20representations%20of%20pretrained%20models%2C%20focusing%20on%20a%20case%20study%20in%20singing%20technique%20classification.%20We%20first%20demonstrate%20that%20SAEs%20retain%20both%20information%20about%20the%20original%20representations%20and%20class%20labels%2C%20enabling%20their%20internal%20structure%20to%20provide%20insights%20into%20self-supervised%20learning%20systems.%20Furthermore%2C%20we%20show%20that%20SAEs%20enhance%20the%20disentanglement%20of%20vocal%20attributes%2C%20establishing%20them%20as%20an%20effective%20tool%20for%20identifying%20the%20underlying%20factors%20encoded%20in%20the%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2509.24793v2&entry.124074799=Read"},
{"title": "Tracking Temporal Dynamics of Vector Sets with Gaussian Process", "author": "Taichi Aida and Mamoru Komachi and Toshinobu Ogiso and Hiroya Takamura and Daichi Mochihashi", "abstract": "Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.", "link": "http://arxiv.org/abs/2512.15538v1", "date": "2025-12-17", "relevancy": 2.6308, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5557}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.521}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20Temporal%20Dynamics%20of%20Vector%20Sets%20with%20Gaussian%20Process&body=Title%3A%20Tracking%20Temporal%20Dynamics%20of%20Vector%20Sets%20with%20Gaussian%20Process%0AAuthor%3A%20Taichi%20Aida%20and%20Mamoru%20Komachi%20and%20Toshinobu%20Ogiso%20and%20Hiroya%20Takamura%20and%20Daichi%20Mochihashi%0AAbstract%3A%20Understanding%20the%20temporal%20evolution%20of%20sets%20of%20vectors%20is%20a%20fundamental%20challenge%20across%20various%20domains%2C%20including%20ecology%2C%20crime%20analysis%2C%20and%20linguistics.%20For%20instance%2C%20ecosystem%20structures%20evolve%20due%20to%20interactions%20among%20plants%2C%20herbivores%2C%20and%20carnivores%3B%20the%20spatial%20distribution%20of%20crimes%20shifts%20in%20response%20to%20societal%20changes%3B%20and%20word%20embedding%20vectors%20reflect%20cultural%20and%20semantic%20trends%20over%20time.%20However%2C%20analyzing%20such%20time-varying%20sets%20of%20vectors%20is%20challenging%20due%20to%20their%20complicated%20structures%2C%20which%20also%20evolve%20over%20time.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20modeling%20the%20distribution%20underlying%20each%20set%20of%20vectors%20using%20infinite-dimensional%20Gaussian%20processes.%20By%20approximating%20the%20latent%20function%20in%20the%20Gaussian%20process%20with%20Random%20Fourier%20Features%2C%20we%20obtain%20compact%20and%20comparable%20vector%20representations%20over%20time.%20This%20enables%20us%20to%20track%20and%20visualize%20temporal%20transitions%20of%20vector%20sets%20in%20a%20low-dimensional%20space.%20We%20apply%20our%20method%20to%20both%20sociological%20data%20%28crime%20distributions%29%20and%20linguistic%20data%20%28word%20embeddings%29%2C%20demonstrating%20its%20effectiveness%20in%20capturing%20temporal%20dynamics.%20Our%20results%20show%20that%20the%20proposed%20approach%20provides%20interpretable%20and%20robust%20representations%2C%20offering%20a%20powerful%20framework%20for%20analyzing%20structural%20changes%20in%20temporally%20indexed%20vector%20sets%20across%20diverse%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520Temporal%2520Dynamics%2520of%2520Vector%2520Sets%2520with%2520Gaussian%2520Process%26entry.906535625%3DTaichi%2520Aida%2520and%2520Mamoru%2520Komachi%2520and%2520Toshinobu%2520Ogiso%2520and%2520Hiroya%2520Takamura%2520and%2520Daichi%2520Mochihashi%26entry.1292438233%3DUnderstanding%2520the%2520temporal%2520evolution%2520of%2520sets%2520of%2520vectors%2520is%2520a%2520fundamental%2520challenge%2520across%2520various%2520domains%252C%2520including%2520ecology%252C%2520crime%2520analysis%252C%2520and%2520linguistics.%2520For%2520instance%252C%2520ecosystem%2520structures%2520evolve%2520due%2520to%2520interactions%2520among%2520plants%252C%2520herbivores%252C%2520and%2520carnivores%253B%2520the%2520spatial%2520distribution%2520of%2520crimes%2520shifts%2520in%2520response%2520to%2520societal%2520changes%253B%2520and%2520word%2520embedding%2520vectors%2520reflect%2520cultural%2520and%2520semantic%2520trends%2520over%2520time.%2520However%252C%2520analyzing%2520such%2520time-varying%2520sets%2520of%2520vectors%2520is%2520challenging%2520due%2520to%2520their%2520complicated%2520structures%252C%2520which%2520also%2520evolve%2520over%2520time.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520modeling%2520the%2520distribution%2520underlying%2520each%2520set%2520of%2520vectors%2520using%2520infinite-dimensional%2520Gaussian%2520processes.%2520By%2520approximating%2520the%2520latent%2520function%2520in%2520the%2520Gaussian%2520process%2520with%2520Random%2520Fourier%2520Features%252C%2520we%2520obtain%2520compact%2520and%2520comparable%2520vector%2520representations%2520over%2520time.%2520This%2520enables%2520us%2520to%2520track%2520and%2520visualize%2520temporal%2520transitions%2520of%2520vector%2520sets%2520in%2520a%2520low-dimensional%2520space.%2520We%2520apply%2520our%2520method%2520to%2520both%2520sociological%2520data%2520%2528crime%2520distributions%2529%2520and%2520linguistic%2520data%2520%2528word%2520embeddings%2529%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520capturing%2520temporal%2520dynamics.%2520Our%2520results%2520show%2520that%2520the%2520proposed%2520approach%2520provides%2520interpretable%2520and%2520robust%2520representations%252C%2520offering%2520a%2520powerful%2520framework%2520for%2520analyzing%2520structural%2520changes%2520in%2520temporally%2520indexed%2520vector%2520sets%2520across%2520diverse%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20Temporal%20Dynamics%20of%20Vector%20Sets%20with%20Gaussian%20Process&entry.906535625=Taichi%20Aida%20and%20Mamoru%20Komachi%20and%20Toshinobu%20Ogiso%20and%20Hiroya%20Takamura%20and%20Daichi%20Mochihashi&entry.1292438233=Understanding%20the%20temporal%20evolution%20of%20sets%20of%20vectors%20is%20a%20fundamental%20challenge%20across%20various%20domains%2C%20including%20ecology%2C%20crime%20analysis%2C%20and%20linguistics.%20For%20instance%2C%20ecosystem%20structures%20evolve%20due%20to%20interactions%20among%20plants%2C%20herbivores%2C%20and%20carnivores%3B%20the%20spatial%20distribution%20of%20crimes%20shifts%20in%20response%20to%20societal%20changes%3B%20and%20word%20embedding%20vectors%20reflect%20cultural%20and%20semantic%20trends%20over%20time.%20However%2C%20analyzing%20such%20time-varying%20sets%20of%20vectors%20is%20challenging%20due%20to%20their%20complicated%20structures%2C%20which%20also%20evolve%20over%20time.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20modeling%20the%20distribution%20underlying%20each%20set%20of%20vectors%20using%20infinite-dimensional%20Gaussian%20processes.%20By%20approximating%20the%20latent%20function%20in%20the%20Gaussian%20process%20with%20Random%20Fourier%20Features%2C%20we%20obtain%20compact%20and%20comparable%20vector%20representations%20over%20time.%20This%20enables%20us%20to%20track%20and%20visualize%20temporal%20transitions%20of%20vector%20sets%20in%20a%20low-dimensional%20space.%20We%20apply%20our%20method%20to%20both%20sociological%20data%20%28crime%20distributions%29%20and%20linguistic%20data%20%28word%20embeddings%29%2C%20demonstrating%20its%20effectiveness%20in%20capturing%20temporal%20dynamics.%20Our%20results%20show%20that%20the%20proposed%20approach%20provides%20interpretable%20and%20robust%20representations%2C%20offering%20a%20powerful%20framework%20for%20analyzing%20structural%20changes%20in%20temporally%20indexed%20vector%20sets%20across%20diverse%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2512.15538v1&entry.124074799=Read"},
{"title": "REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning", "author": "Run He and Di Fang and Yizhu Chen and Kai Tong and Cen Chen and Yi Wang and Lap-pui Chau and Huiping Zhuang", "abstract": "Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.", "link": "http://arxiv.org/abs/2403.13522v3", "date": "2025-12-17", "relevancy": 2.6085, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5387}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5227}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%20Class-incremental%20Learning&body=Title%3A%20REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%20Class-incremental%20Learning%0AAuthor%3A%20Run%20He%20and%20Di%20Fang%20and%20Yizhu%20Chen%20and%20Kai%20Tong%20and%20Cen%20Chen%20and%20Yi%20Wang%20and%20Lap-pui%20Chau%20and%20Huiping%20Zhuang%0AAbstract%3A%20Exemplar-free%20class-incremental%20learning%20%28EFCIL%29%20aims%20to%20mitigate%20catastrophic%20forgetting%20in%20class-incremental%20learning%20%28CIL%29%20without%20available%20historical%20training%20samples%20as%20exemplars.%20Compared%20with%20its%20exemplar-based%20CIL%20counterpart%20that%20stores%20exemplars%2C%20EFCIL%20suffers%20more%20from%20forgetting%20issues.%20Recently%2C%20a%20new%20EFCIL%20branch%20named%20Analytic%20Continual%20Learning%20%28ACL%29%20introduces%20a%20gradient-free%20paradigm%20via%20Recursive%20Least-Square%2C%20achieving%20a%20forgetting-resistant%20classifier%20training%20with%20a%20frozen%20backbone%20during%20CIL.%20However%2C%20existing%20ACL%20suffers%20from%20ineffective%20representations%20and%20insufficient%20utilization%20of%20backbone%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%20representation-enhanced%20analytic%20learning%20%28REAL%29%20to%20address%20these%20problems.%20To%20enhance%20the%20representation%2C%20REAL%20constructs%20a%20dual-stream%20base%20pretraining%20followed%20by%20representation%20enhancing%20distillation%20process.%20The%20dual-stream%20base%20pretraining%20combines%20self-supervised%20contrastive%20learning%20for%20general%20features%20and%20supervised%20learning%20for%20class-specific%20knowledge%2C%20followed%20by%20the%20representation%20enhancing%20distillation%20to%20merge%20both%20streams%2C%20enhancing%20representations%20for%20subsequent%20CIL%20paradigm.%20To%20utilize%20more%20knowledge%20from%20the%20backbone%2C%20REAL%20presents%20a%20feature%20fusion%20buffer%20to%20multi-layer%20backbone%20features%2C%20providing%20informative%20features%20for%20the%20subsequent%20classifier%20training.%20Our%20method%20can%20be%20incorporated%20into%20existing%20ACL%20techniques%20and%20provides%20more%20competitive%20performance.%20Empirical%20results%20demonstrate%20that%2C%20REAL%20achieves%20state-of-the-art%20performance%20on%20CIFAR-100%2C%20ImageNet-100%20and%20ImageNet-1k%20benchmarks%2C%20outperforming%20exemplar-free%20methods%20and%20rivaling%20exemplar-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2403.13522v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREAL%253A%2520Representation%2520Enhanced%2520Analytic%2520Learning%2520for%2520Exemplar-free%2520Class-incremental%2520Learning%26entry.906535625%3DRun%2520He%2520and%2520Di%2520Fang%2520and%2520Yizhu%2520Chen%2520and%2520Kai%2520Tong%2520and%2520Cen%2520Chen%2520and%2520Yi%2520Wang%2520and%2520Lap-pui%2520Chau%2520and%2520Huiping%2520Zhuang%26entry.1292438233%3DExemplar-free%2520class-incremental%2520learning%2520%2528EFCIL%2529%2520aims%2520to%2520mitigate%2520catastrophic%2520forgetting%2520in%2520class-incremental%2520learning%2520%2528CIL%2529%2520without%2520available%2520historical%2520training%2520samples%2520as%2520exemplars.%2520Compared%2520with%2520its%2520exemplar-based%2520CIL%2520counterpart%2520that%2520stores%2520exemplars%252C%2520EFCIL%2520suffers%2520more%2520from%2520forgetting%2520issues.%2520Recently%252C%2520a%2520new%2520EFCIL%2520branch%2520named%2520Analytic%2520Continual%2520Learning%2520%2528ACL%2529%2520introduces%2520a%2520gradient-free%2520paradigm%2520via%2520Recursive%2520Least-Square%252C%2520achieving%2520a%2520forgetting-resistant%2520classifier%2520training%2520with%2520a%2520frozen%2520backbone%2520during%2520CIL.%2520However%252C%2520existing%2520ACL%2520suffers%2520from%2520ineffective%2520representations%2520and%2520insufficient%2520utilization%2520of%2520backbone%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520representation-enhanced%2520analytic%2520learning%2520%2528REAL%2529%2520to%2520address%2520these%2520problems.%2520To%2520enhance%2520the%2520representation%252C%2520REAL%2520constructs%2520a%2520dual-stream%2520base%2520pretraining%2520followed%2520by%2520representation%2520enhancing%2520distillation%2520process.%2520The%2520dual-stream%2520base%2520pretraining%2520combines%2520self-supervised%2520contrastive%2520learning%2520for%2520general%2520features%2520and%2520supervised%2520learning%2520for%2520class-specific%2520knowledge%252C%2520followed%2520by%2520the%2520representation%2520enhancing%2520distillation%2520to%2520merge%2520both%2520streams%252C%2520enhancing%2520representations%2520for%2520subsequent%2520CIL%2520paradigm.%2520To%2520utilize%2520more%2520knowledge%2520from%2520the%2520backbone%252C%2520REAL%2520presents%2520a%2520feature%2520fusion%2520buffer%2520to%2520multi-layer%2520backbone%2520features%252C%2520providing%2520informative%2520features%2520for%2520the%2520subsequent%2520classifier%2520training.%2520Our%2520method%2520can%2520be%2520incorporated%2520into%2520existing%2520ACL%2520techniques%2520and%2520provides%2520more%2520competitive%2520performance.%2520Empirical%2520results%2520demonstrate%2520that%252C%2520REAL%2520achieves%2520state-of-the-art%2520performance%2520on%2520CIFAR-100%252C%2520ImageNet-100%2520and%2520ImageNet-1k%2520benchmarks%252C%2520outperforming%2520exemplar-free%2520methods%2520and%2520rivaling%2520exemplar-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13522v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REAL%3A%20Representation%20Enhanced%20Analytic%20Learning%20for%20Exemplar-free%20Class-incremental%20Learning&entry.906535625=Run%20He%20and%20Di%20Fang%20and%20Yizhu%20Chen%20and%20Kai%20Tong%20and%20Cen%20Chen%20and%20Yi%20Wang%20and%20Lap-pui%20Chau%20and%20Huiping%20Zhuang&entry.1292438233=Exemplar-free%20class-incremental%20learning%20%28EFCIL%29%20aims%20to%20mitigate%20catastrophic%20forgetting%20in%20class-incremental%20learning%20%28CIL%29%20without%20available%20historical%20training%20samples%20as%20exemplars.%20Compared%20with%20its%20exemplar-based%20CIL%20counterpart%20that%20stores%20exemplars%2C%20EFCIL%20suffers%20more%20from%20forgetting%20issues.%20Recently%2C%20a%20new%20EFCIL%20branch%20named%20Analytic%20Continual%20Learning%20%28ACL%29%20introduces%20a%20gradient-free%20paradigm%20via%20Recursive%20Least-Square%2C%20achieving%20a%20forgetting-resistant%20classifier%20training%20with%20a%20frozen%20backbone%20during%20CIL.%20However%2C%20existing%20ACL%20suffers%20from%20ineffective%20representations%20and%20insufficient%20utilization%20of%20backbone%20knowledge.%20In%20this%20paper%2C%20we%20propose%20a%20representation-enhanced%20analytic%20learning%20%28REAL%29%20to%20address%20these%20problems.%20To%20enhance%20the%20representation%2C%20REAL%20constructs%20a%20dual-stream%20base%20pretraining%20followed%20by%20representation%20enhancing%20distillation%20process.%20The%20dual-stream%20base%20pretraining%20combines%20self-supervised%20contrastive%20learning%20for%20general%20features%20and%20supervised%20learning%20for%20class-specific%20knowledge%2C%20followed%20by%20the%20representation%20enhancing%20distillation%20to%20merge%20both%20streams%2C%20enhancing%20representations%20for%20subsequent%20CIL%20paradigm.%20To%20utilize%20more%20knowledge%20from%20the%20backbone%2C%20REAL%20presents%20a%20feature%20fusion%20buffer%20to%20multi-layer%20backbone%20features%2C%20providing%20informative%20features%20for%20the%20subsequent%20classifier%20training.%20Our%20method%20can%20be%20incorporated%20into%20existing%20ACL%20techniques%20and%20provides%20more%20competitive%20performance.%20Empirical%20results%20demonstrate%20that%2C%20REAL%20achieves%20state-of-the-art%20performance%20on%20CIFAR-100%2C%20ImageNet-100%20and%20ImageNet-1k%20benchmarks%2C%20outperforming%20exemplar-free%20methods%20and%20rivaling%20exemplar-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2403.13522v3&entry.124074799=Read"},
{"title": "Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection", "author": "Yuxin Jiang and Yunkang Cao and Weiming Shen", "abstract": "Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.", "link": "http://arxiv.org/abs/2512.15319v1", "date": "2025-12-17", "relevancy": 2.599, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5327}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5221}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototypical%20Learning%20Guided%20Context-Aware%20Segmentation%20Network%20for%20Few-Shot%20Anomaly%20Detection&body=Title%3A%20Prototypical%20Learning%20Guided%20Context-Aware%20Segmentation%20Network%20for%20Few-Shot%20Anomaly%20Detection%0AAuthor%3A%20Yuxin%20Jiang%20and%20Yunkang%20Cao%20and%20Weiming%20Shen%0AAbstract%3A%20Few-shot%20anomaly%20detection%20%28FSAD%29%20denotes%20the%20identification%20of%20anomalies%20within%20a%20target%20category%20with%20a%20limited%20number%20of%20normal%20samples.%20Existing%20FSAD%20methods%20largely%20rely%20on%20pre-trained%20feature%20representations%20to%20detect%20anomalies%2C%20but%20the%20inherent%20domain%20gap%20between%20pre-trained%20representations%20and%20target%20FSAD%20scenarios%20is%20often%20overlooked.%20This%20study%20proposes%20a%20Prototypical%20Learning%20Guided%20Context-Aware%20Segmentation%20Network%20%28PCSNet%29%20to%20address%20the%20domain%20gap%2C%20thereby%20improving%20feature%20descriptiveness%20in%20target%20scenarios%20and%20enhancing%20FSAD%20performance.%20In%20particular%2C%20PCSNet%20comprises%20a%20prototypical%20feature%20adaption%20%28PFA%29%20sub-network%20and%20a%20context-aware%20segmentation%20%28CAS%29%20sub-network.%20PFA%20extracts%20prototypical%20features%20as%20guidance%20to%20ensure%20better%20feature%20compactness%20for%20normal%20data%20while%20distinct%20separation%20from%20anomalies.%20A%20pixel-level%20disparity%20classification%20loss%20is%20also%20designed%20to%20make%20subtle%20anomalies%20more%20distinguishable.%20Then%20a%20CAS%20sub-network%20is%20introduced%20for%20pixel-level%20anomaly%20localization%2C%20where%20pseudo%20anomalies%20are%20exploited%20to%20facilitate%20the%20training%20process.%20Experimental%20results%20on%20MVTec%20and%20MPDD%20demonstrate%20the%20superior%20FSAD%20performance%20of%20PCSNet%2C%20with%2094.9%25%20and%2080.2%25%20image-level%20AUROC%20in%20an%208-shot%20scenario%2C%20respectively.%20Real-world%20applications%20on%20automotive%20plastic%20part%20inspection%20further%20demonstrate%20that%20PCSNet%20can%20achieve%20promising%20results%20with%20limited%20training%20samples.%20Code%20is%20available%20at%20https%3A//github.com/yuxin-jiang/PCSNet.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototypical%2520Learning%2520Guided%2520Context-Aware%2520Segmentation%2520Network%2520for%2520Few-Shot%2520Anomaly%2520Detection%26entry.906535625%3DYuxin%2520Jiang%2520and%2520Yunkang%2520Cao%2520and%2520Weiming%2520Shen%26entry.1292438233%3DFew-shot%2520anomaly%2520detection%2520%2528FSAD%2529%2520denotes%2520the%2520identification%2520of%2520anomalies%2520within%2520a%2520target%2520category%2520with%2520a%2520limited%2520number%2520of%2520normal%2520samples.%2520Existing%2520FSAD%2520methods%2520largely%2520rely%2520on%2520pre-trained%2520feature%2520representations%2520to%2520detect%2520anomalies%252C%2520but%2520the%2520inherent%2520domain%2520gap%2520between%2520pre-trained%2520representations%2520and%2520target%2520FSAD%2520scenarios%2520is%2520often%2520overlooked.%2520This%2520study%2520proposes%2520a%2520Prototypical%2520Learning%2520Guided%2520Context-Aware%2520Segmentation%2520Network%2520%2528PCSNet%2529%2520to%2520address%2520the%2520domain%2520gap%252C%2520thereby%2520improving%2520feature%2520descriptiveness%2520in%2520target%2520scenarios%2520and%2520enhancing%2520FSAD%2520performance.%2520In%2520particular%252C%2520PCSNet%2520comprises%2520a%2520prototypical%2520feature%2520adaption%2520%2528PFA%2529%2520sub-network%2520and%2520a%2520context-aware%2520segmentation%2520%2528CAS%2529%2520sub-network.%2520PFA%2520extracts%2520prototypical%2520features%2520as%2520guidance%2520to%2520ensure%2520better%2520feature%2520compactness%2520for%2520normal%2520data%2520while%2520distinct%2520separation%2520from%2520anomalies.%2520A%2520pixel-level%2520disparity%2520classification%2520loss%2520is%2520also%2520designed%2520to%2520make%2520subtle%2520anomalies%2520more%2520distinguishable.%2520Then%2520a%2520CAS%2520sub-network%2520is%2520introduced%2520for%2520pixel-level%2520anomaly%2520localization%252C%2520where%2520pseudo%2520anomalies%2520are%2520exploited%2520to%2520facilitate%2520the%2520training%2520process.%2520Experimental%2520results%2520on%2520MVTec%2520and%2520MPDD%2520demonstrate%2520the%2520superior%2520FSAD%2520performance%2520of%2520PCSNet%252C%2520with%252094.9%2525%2520and%252080.2%2525%2520image-level%2520AUROC%2520in%2520an%25208-shot%2520scenario%252C%2520respectively.%2520Real-world%2520applications%2520on%2520automotive%2520plastic%2520part%2520inspection%2520further%2520demonstrate%2520that%2520PCSNet%2520can%2520achieve%2520promising%2520results%2520with%2520limited%2520training%2520samples.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/yuxin-jiang/PCSNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototypical%20Learning%20Guided%20Context-Aware%20Segmentation%20Network%20for%20Few-Shot%20Anomaly%20Detection&entry.906535625=Yuxin%20Jiang%20and%20Yunkang%20Cao%20and%20Weiming%20Shen&entry.1292438233=Few-shot%20anomaly%20detection%20%28FSAD%29%20denotes%20the%20identification%20of%20anomalies%20within%20a%20target%20category%20with%20a%20limited%20number%20of%20normal%20samples.%20Existing%20FSAD%20methods%20largely%20rely%20on%20pre-trained%20feature%20representations%20to%20detect%20anomalies%2C%20but%20the%20inherent%20domain%20gap%20between%20pre-trained%20representations%20and%20target%20FSAD%20scenarios%20is%20often%20overlooked.%20This%20study%20proposes%20a%20Prototypical%20Learning%20Guided%20Context-Aware%20Segmentation%20Network%20%28PCSNet%29%20to%20address%20the%20domain%20gap%2C%20thereby%20improving%20feature%20descriptiveness%20in%20target%20scenarios%20and%20enhancing%20FSAD%20performance.%20In%20particular%2C%20PCSNet%20comprises%20a%20prototypical%20feature%20adaption%20%28PFA%29%20sub-network%20and%20a%20context-aware%20segmentation%20%28CAS%29%20sub-network.%20PFA%20extracts%20prototypical%20features%20as%20guidance%20to%20ensure%20better%20feature%20compactness%20for%20normal%20data%20while%20distinct%20separation%20from%20anomalies.%20A%20pixel-level%20disparity%20classification%20loss%20is%20also%20designed%20to%20make%20subtle%20anomalies%20more%20distinguishable.%20Then%20a%20CAS%20sub-network%20is%20introduced%20for%20pixel-level%20anomaly%20localization%2C%20where%20pseudo%20anomalies%20are%20exploited%20to%20facilitate%20the%20training%20process.%20Experimental%20results%20on%20MVTec%20and%20MPDD%20demonstrate%20the%20superior%20FSAD%20performance%20of%20PCSNet%2C%20with%2094.9%25%20and%2080.2%25%20image-level%20AUROC%20in%20an%208-shot%20scenario%2C%20respectively.%20Real-world%20applications%20on%20automotive%20plastic%20part%20inspection%20further%20demonstrate%20that%20PCSNet%20can%20achieve%20promising%20results%20with%20limited%20training%20samples.%20Code%20is%20available%20at%20https%3A//github.com/yuxin-jiang/PCSNet.&entry.1838667208=http%3A//arxiv.org/abs/2512.15319v1&entry.124074799=Read"},
{"title": "Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning", "author": "Simon Gutwein and Arthur Longuefosse and Jun Seita and Sabine Taschner-Mandl and Roxane Licandro", "abstract": "Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.", "link": "http://arxiv.org/abs/2512.15410v1", "date": "2025-12-17", "relevancy": 2.5979, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Marker%20Specificity%20with%20Lightweight%20Channel-Independent%20Representation%20Learning&body=Title%3A%20Preserving%20Marker%20Specificity%20with%20Lightweight%20Channel-Independent%20Representation%20Learning%0AAuthor%3A%20Simon%20Gutwein%20and%20Arthur%20Longuefosse%20and%20Jun%20Seita%20and%20Sabine%20Taschner-Mandl%20and%20Roxane%20Licandro%0AAbstract%3A%20Multiplexed%20tissue%20imaging%20measures%20dozens%20of%20protein%20markers%20per%20cell%2C%20yet%20most%20deep%20learning%20models%20still%20apply%20early%20channel%20fusion%2C%20assuming%20shared%20structure%20across%20markers.%20We%20investigate%20whether%20preserving%20marker%20independence%2C%20combined%20with%20deliberately%20shallow%20architectures%2C%20provides%20a%20more%20suitable%20inductive%20bias%20for%20self-supervised%20representation%20learning%20in%20multiplex%20data%20than%20increasing%20model%20scale.%20Using%20a%20Hodgkin%20lymphoma%20CODEX%20dataset%20with%20145%2C000%20cells%20and%2049%20markers%2C%20we%20compare%20standard%20early-fusion%20CNNs%20with%20channel-separated%20architectures%2C%20including%20a%20marker-aware%20baseline%20and%20our%20novel%20shallow%20Channel-Independent%20Model%20%28CIM-S%29%20with%205.5K%20parameters.%20After%20contrastive%20pretraining%20and%20linear%20evaluation%2C%20early-fusion%20models%20show%20limited%20ability%20to%20retain%20marker-specific%20information%20and%20struggle%20particularly%20with%20rare-cell%20discrimination.%20Channel-independent%20architectures%2C%20and%20CIM-S%20in%20particular%2C%20achieve%20substantially%20stronger%20representations%20despite%20their%20compact%20size.%20These%20findings%20are%20consistent%20across%20multiple%20self-supervised%20frameworks%2C%20remain%20stable%20across%20augmentation%20settings%2C%20and%20are%20reproducible%20across%20both%20the%2049-marker%20and%20reduced%2018-marker%20settings.%20These%20results%20show%20that%20lightweight%2C%20channel-independent%20architectures%20can%20match%20or%20surpass%20deep%20early-fusion%20CNNs%20and%20foundation%20models%20for%20multiplex%20representation%20learning.%20Code%20is%20available%20at%20https%3A//github.com/SimonBon/CIM-S.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Marker%2520Specificity%2520with%2520Lightweight%2520Channel-Independent%2520Representation%2520Learning%26entry.906535625%3DSimon%2520Gutwein%2520and%2520Arthur%2520Longuefosse%2520and%2520Jun%2520Seita%2520and%2520Sabine%2520Taschner-Mandl%2520and%2520Roxane%2520Licandro%26entry.1292438233%3DMultiplexed%2520tissue%2520imaging%2520measures%2520dozens%2520of%2520protein%2520markers%2520per%2520cell%252C%2520yet%2520most%2520deep%2520learning%2520models%2520still%2520apply%2520early%2520channel%2520fusion%252C%2520assuming%2520shared%2520structure%2520across%2520markers.%2520We%2520investigate%2520whether%2520preserving%2520marker%2520independence%252C%2520combined%2520with%2520deliberately%2520shallow%2520architectures%252C%2520provides%2520a%2520more%2520suitable%2520inductive%2520bias%2520for%2520self-supervised%2520representation%2520learning%2520in%2520multiplex%2520data%2520than%2520increasing%2520model%2520scale.%2520Using%2520a%2520Hodgkin%2520lymphoma%2520CODEX%2520dataset%2520with%2520145%252C000%2520cells%2520and%252049%2520markers%252C%2520we%2520compare%2520standard%2520early-fusion%2520CNNs%2520with%2520channel-separated%2520architectures%252C%2520including%2520a%2520marker-aware%2520baseline%2520and%2520our%2520novel%2520shallow%2520Channel-Independent%2520Model%2520%2528CIM-S%2529%2520with%25205.5K%2520parameters.%2520After%2520contrastive%2520pretraining%2520and%2520linear%2520evaluation%252C%2520early-fusion%2520models%2520show%2520limited%2520ability%2520to%2520retain%2520marker-specific%2520information%2520and%2520struggle%2520particularly%2520with%2520rare-cell%2520discrimination.%2520Channel-independent%2520architectures%252C%2520and%2520CIM-S%2520in%2520particular%252C%2520achieve%2520substantially%2520stronger%2520representations%2520despite%2520their%2520compact%2520size.%2520These%2520findings%2520are%2520consistent%2520across%2520multiple%2520self-supervised%2520frameworks%252C%2520remain%2520stable%2520across%2520augmentation%2520settings%252C%2520and%2520are%2520reproducible%2520across%2520both%2520the%252049-marker%2520and%2520reduced%252018-marker%2520settings.%2520These%2520results%2520show%2520that%2520lightweight%252C%2520channel-independent%2520architectures%2520can%2520match%2520or%2520surpass%2520deep%2520early-fusion%2520CNNs%2520and%2520foundation%2520models%2520for%2520multiplex%2520representation%2520learning.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/SimonBon/CIM-S.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Marker%20Specificity%20with%20Lightweight%20Channel-Independent%20Representation%20Learning&entry.906535625=Simon%20Gutwein%20and%20Arthur%20Longuefosse%20and%20Jun%20Seita%20and%20Sabine%20Taschner-Mandl%20and%20Roxane%20Licandro&entry.1292438233=Multiplexed%20tissue%20imaging%20measures%20dozens%20of%20protein%20markers%20per%20cell%2C%20yet%20most%20deep%20learning%20models%20still%20apply%20early%20channel%20fusion%2C%20assuming%20shared%20structure%20across%20markers.%20We%20investigate%20whether%20preserving%20marker%20independence%2C%20combined%20with%20deliberately%20shallow%20architectures%2C%20provides%20a%20more%20suitable%20inductive%20bias%20for%20self-supervised%20representation%20learning%20in%20multiplex%20data%20than%20increasing%20model%20scale.%20Using%20a%20Hodgkin%20lymphoma%20CODEX%20dataset%20with%20145%2C000%20cells%20and%2049%20markers%2C%20we%20compare%20standard%20early-fusion%20CNNs%20with%20channel-separated%20architectures%2C%20including%20a%20marker-aware%20baseline%20and%20our%20novel%20shallow%20Channel-Independent%20Model%20%28CIM-S%29%20with%205.5K%20parameters.%20After%20contrastive%20pretraining%20and%20linear%20evaluation%2C%20early-fusion%20models%20show%20limited%20ability%20to%20retain%20marker-specific%20information%20and%20struggle%20particularly%20with%20rare-cell%20discrimination.%20Channel-independent%20architectures%2C%20and%20CIM-S%20in%20particular%2C%20achieve%20substantially%20stronger%20representations%20despite%20their%20compact%20size.%20These%20findings%20are%20consistent%20across%20multiple%20self-supervised%20frameworks%2C%20remain%20stable%20across%20augmentation%20settings%2C%20and%20are%20reproducible%20across%20both%20the%2049-marker%20and%20reduced%2018-marker%20settings.%20These%20results%20show%20that%20lightweight%2C%20channel-independent%20architectures%20can%20match%20or%20surpass%20deep%20early-fusion%20CNNs%20and%20foundation%20models%20for%20multiplex%20representation%20learning.%20Code%20is%20available%20at%20https%3A//github.com/SimonBon/CIM-S.&entry.1838667208=http%3A//arxiv.org/abs/2512.15410v1&entry.124074799=Read"},
{"title": "OMCL: Open-vocabulary Monte Carlo Localization", "author": "Evgenii Kruzhkov and Raphael Memmesheimer and Sven Behnke", "abstract": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.", "link": "http://arxiv.org/abs/2512.15557v1", "date": "2025-12-17", "relevancy": 2.5808, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMCL%3A%20Open-vocabulary%20Monte%20Carlo%20Localization&body=Title%3A%20OMCL%3A%20Open-vocabulary%20Monte%20Carlo%20Localization%0AAuthor%3A%20Evgenii%20Kruzhkov%20and%20Raphael%20Memmesheimer%20and%20Sven%20Behnke%0AAbstract%3A%20Robust%20robot%20localization%20is%20an%20important%20prerequisite%20for%20navigation%20planning.%20If%20the%20environment%20map%20was%20created%20from%20different%20sensors%2C%20robot%20measurements%20must%20be%20robustly%20associated%20with%20map%20features.%20In%20this%20work%2C%20we%20extend%20Monte%20Carlo%20Localization%20using%20vision-language%20features.%20These%20open-vocabulary%20features%20enable%20to%20robustly%20compute%20the%20likelihood%20of%20visual%20observations%2C%20given%20a%20camera%20pose%20and%20a%203D%20map%20created%20from%20posed%20RGB-D%20images%20or%20aligned%20point%20clouds.%20The%20abstract%20vision-language%20features%20enable%20to%20associate%20observations%20and%20map%20elements%20from%20different%20modalities.%20Global%20localization%20can%20be%20initialized%20by%20natural%20language%20descriptions%20of%20the%20objects%20present%20in%20the%20vicinity%20of%20locations.%20We%20evaluate%20our%20approach%20using%20Matterport3D%20and%20Replica%20for%20indoor%20scenes%20and%20demonstrate%20generalization%20on%20SemanticKITTI%20for%20outdoor%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMCL%253A%2520Open-vocabulary%2520Monte%2520Carlo%2520Localization%26entry.906535625%3DEvgenii%2520Kruzhkov%2520and%2520Raphael%2520Memmesheimer%2520and%2520Sven%2520Behnke%26entry.1292438233%3DRobust%2520robot%2520localization%2520is%2520an%2520important%2520prerequisite%2520for%2520navigation%2520planning.%2520If%2520the%2520environment%2520map%2520was%2520created%2520from%2520different%2520sensors%252C%2520robot%2520measurements%2520must%2520be%2520robustly%2520associated%2520with%2520map%2520features.%2520In%2520this%2520work%252C%2520we%2520extend%2520Monte%2520Carlo%2520Localization%2520using%2520vision-language%2520features.%2520These%2520open-vocabulary%2520features%2520enable%2520to%2520robustly%2520compute%2520the%2520likelihood%2520of%2520visual%2520observations%252C%2520given%2520a%2520camera%2520pose%2520and%2520a%25203D%2520map%2520created%2520from%2520posed%2520RGB-D%2520images%2520or%2520aligned%2520point%2520clouds.%2520The%2520abstract%2520vision-language%2520features%2520enable%2520to%2520associate%2520observations%2520and%2520map%2520elements%2520from%2520different%2520modalities.%2520Global%2520localization%2520can%2520be%2520initialized%2520by%2520natural%2520language%2520descriptions%2520of%2520the%2520objects%2520present%2520in%2520the%2520vicinity%2520of%2520locations.%2520We%2520evaluate%2520our%2520approach%2520using%2520Matterport3D%2520and%2520Replica%2520for%2520indoor%2520scenes%2520and%2520demonstrate%2520generalization%2520on%2520SemanticKITTI%2520for%2520outdoor%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMCL%3A%20Open-vocabulary%20Monte%20Carlo%20Localization&entry.906535625=Evgenii%20Kruzhkov%20and%20Raphael%20Memmesheimer%20and%20Sven%20Behnke&entry.1292438233=Robust%20robot%20localization%20is%20an%20important%20prerequisite%20for%20navigation%20planning.%20If%20the%20environment%20map%20was%20created%20from%20different%20sensors%2C%20robot%20measurements%20must%20be%20robustly%20associated%20with%20map%20features.%20In%20this%20work%2C%20we%20extend%20Monte%20Carlo%20Localization%20using%20vision-language%20features.%20These%20open-vocabulary%20features%20enable%20to%20robustly%20compute%20the%20likelihood%20of%20visual%20observations%2C%20given%20a%20camera%20pose%20and%20a%203D%20map%20created%20from%20posed%20RGB-D%20images%20or%20aligned%20point%20clouds.%20The%20abstract%20vision-language%20features%20enable%20to%20associate%20observations%20and%20map%20elements%20from%20different%20modalities.%20Global%20localization%20can%20be%20initialized%20by%20natural%20language%20descriptions%20of%20the%20objects%20present%20in%20the%20vicinity%20of%20locations.%20We%20evaluate%20our%20approach%20using%20Matterport3D%20and%20Replica%20for%20indoor%20scenes%20and%20demonstrate%20generalization%20on%20SemanticKITTI%20for%20outdoor%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15557v1&entry.124074799=Read"},
{"title": "On Assessing the Relevance of Code Reviews Authored by Generative Models", "author": "Robert Heum\u00fcller and Frank Ortmeier", "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.", "link": "http://arxiv.org/abs/2512.15466v1", "date": "2025-12-17", "relevancy": 2.5577, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5407}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4973}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Assessing%20the%20Relevance%20of%20Code%20Reviews%20Authored%20by%20Generative%20Models&body=Title%3A%20On%20Assessing%20the%20Relevance%20of%20Code%20Reviews%20Authored%20by%20Generative%20Models%0AAuthor%3A%20Robert%20Heum%C3%BCller%20and%20Frank%20Ortmeier%0AAbstract%3A%20The%20use%20of%20large%20language%20models%20like%20ChatGPT%20in%20code%20review%20offers%20promising%20efficiency%20gains%20but%20also%20raises%20concerns%20about%20correctness%20and%20safety.%20Existing%20evaluation%20methods%20for%20code%20review%20generation%20either%20rely%20on%20automatic%20comparisons%20to%20a%20single%20ground%20truth%2C%20which%20fails%20to%20capture%20the%20variability%20of%20human%20perspectives%2C%20or%20on%20subjective%20assessments%20of%20%22usefulness%22%2C%20a%20highly%20ambiguous%20concept.%20We%20propose%20a%20novel%20evaluation%20approach%20based%20on%20what%20we%20call%20multi-subjective%20ranking.%20Using%20a%20dataset%20of%20280%20self-contained%20code%20review%20requests%20and%20corresponding%20comments%20from%20CodeReview%20StackExchange%2C%20multiple%20human%20judges%20ranked%20the%20quality%20of%20ChatGPT-generated%20comments%20alongside%20the%20top%20human%20responses%20from%20the%20platform.%20Results%20show%20that%20ChatGPT%27s%20comments%20were%20ranked%20significantly%20better%20than%20human%20ones%2C%20even%20surpassing%20StackExchange%27s%20accepted%20answers.%20Going%20further%2C%20our%20proposed%20method%20motivates%20and%20enables%20more%20meaningful%20assessments%20of%20generative%20AI%27s%20performance%20in%20code%20review%2C%20while%20also%20raising%20awareness%20of%20potential%20risks%20of%20unchecked%20integration%20into%20review%20processes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Assessing%2520the%2520Relevance%2520of%2520Code%2520Reviews%2520Authored%2520by%2520Generative%2520Models%26entry.906535625%3DRobert%2520Heum%25C3%25BCller%2520and%2520Frank%2520Ortmeier%26entry.1292438233%3DThe%2520use%2520of%2520large%2520language%2520models%2520like%2520ChatGPT%2520in%2520code%2520review%2520offers%2520promising%2520efficiency%2520gains%2520but%2520also%2520raises%2520concerns%2520about%2520correctness%2520and%2520safety.%2520Existing%2520evaluation%2520methods%2520for%2520code%2520review%2520generation%2520either%2520rely%2520on%2520automatic%2520comparisons%2520to%2520a%2520single%2520ground%2520truth%252C%2520which%2520fails%2520to%2520capture%2520the%2520variability%2520of%2520human%2520perspectives%252C%2520or%2520on%2520subjective%2520assessments%2520of%2520%2522usefulness%2522%252C%2520a%2520highly%2520ambiguous%2520concept.%2520We%2520propose%2520a%2520novel%2520evaluation%2520approach%2520based%2520on%2520what%2520we%2520call%2520multi-subjective%2520ranking.%2520Using%2520a%2520dataset%2520of%2520280%2520self-contained%2520code%2520review%2520requests%2520and%2520corresponding%2520comments%2520from%2520CodeReview%2520StackExchange%252C%2520multiple%2520human%2520judges%2520ranked%2520the%2520quality%2520of%2520ChatGPT-generated%2520comments%2520alongside%2520the%2520top%2520human%2520responses%2520from%2520the%2520platform.%2520Results%2520show%2520that%2520ChatGPT%2527s%2520comments%2520were%2520ranked%2520significantly%2520better%2520than%2520human%2520ones%252C%2520even%2520surpassing%2520StackExchange%2527s%2520accepted%2520answers.%2520Going%2520further%252C%2520our%2520proposed%2520method%2520motivates%2520and%2520enables%2520more%2520meaningful%2520assessments%2520of%2520generative%2520AI%2527s%2520performance%2520in%2520code%2520review%252C%2520while%2520also%2520raising%2520awareness%2520of%2520potential%2520risks%2520of%2520unchecked%2520integration%2520into%2520review%2520processes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Assessing%20the%20Relevance%20of%20Code%20Reviews%20Authored%20by%20Generative%20Models&entry.906535625=Robert%20Heum%C3%BCller%20and%20Frank%20Ortmeier&entry.1292438233=The%20use%20of%20large%20language%20models%20like%20ChatGPT%20in%20code%20review%20offers%20promising%20efficiency%20gains%20but%20also%20raises%20concerns%20about%20correctness%20and%20safety.%20Existing%20evaluation%20methods%20for%20code%20review%20generation%20either%20rely%20on%20automatic%20comparisons%20to%20a%20single%20ground%20truth%2C%20which%20fails%20to%20capture%20the%20variability%20of%20human%20perspectives%2C%20or%20on%20subjective%20assessments%20of%20%22usefulness%22%2C%20a%20highly%20ambiguous%20concept.%20We%20propose%20a%20novel%20evaluation%20approach%20based%20on%20what%20we%20call%20multi-subjective%20ranking.%20Using%20a%20dataset%20of%20280%20self-contained%20code%20review%20requests%20and%20corresponding%20comments%20from%20CodeReview%20StackExchange%2C%20multiple%20human%20judges%20ranked%20the%20quality%20of%20ChatGPT-generated%20comments%20alongside%20the%20top%20human%20responses%20from%20the%20platform.%20Results%20show%20that%20ChatGPT%27s%20comments%20were%20ranked%20significantly%20better%20than%20human%20ones%2C%20even%20surpassing%20StackExchange%27s%20accepted%20answers.%20Going%20further%2C%20our%20proposed%20method%20motivates%20and%20enables%20more%20meaningful%20assessments%20of%20generative%20AI%27s%20performance%20in%20code%20review%2C%20while%20also%20raising%20awareness%20of%20potential%20risks%20of%20unchecked%20integration%20into%20review%20processes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15466v1&entry.124074799=Read"},
{"title": "DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations", "author": "Yuxiang Shi and Zhe Li and Yanwen Wang and Hao Zhu and Xun Cao and Ligang Liu", "abstract": "Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.", "link": "http://arxiv.org/abs/2512.15524v1", "date": "2025-12-17", "relevancy": 2.5251, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6593}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6205}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeX-Portrait%3A%20Disentangled%20and%20Expressive%20Portrait%20Animation%20via%20Explicit%20and%20Latent%20Motion%20Representations&body=Title%3A%20DeX-Portrait%3A%20Disentangled%20and%20Expressive%20Portrait%20Animation%20via%20Explicit%20and%20Latent%20Motion%20Representations%0AAuthor%3A%20Yuxiang%20Shi%20and%20Zhe%20Li%20and%20Yanwen%20Wang%20and%20Hao%20Zhu%20and%20Xun%20Cao%20and%20Ligang%20Liu%0AAbstract%3A%20Portrait%20animation%20from%20a%20single%20source%20image%20and%20a%20driving%20video%20is%20a%20long-standing%20problem.%20Recent%20approaches%20tend%20to%20adopt%20diffusion-based%20image/video%20generation%20models%20for%20realistic%20and%20expressive%20animation.%20However%2C%20none%20of%20these%20diffusion%20models%20realizes%20high-fidelity%20disentangled%20control%20between%20the%20head%20pose%20and%20facial%20expression%2C%20hindering%20applications%20like%20expression-only%20or%20pose-only%20editing%20and%20animation.%20To%20address%20this%2C%20we%20propose%20DeX-Portrait%2C%20a%20novel%20approach%20capable%20of%20generating%20expressive%20portrait%20animation%20driven%20by%20disentangled%20pose%20and%20expression%20signals.%20Specifically%2C%20we%20represent%20the%20pose%20as%20an%20explicit%20global%20transformation%20and%20the%20expression%20as%20an%20implicit%20latent%20code.%20First%2C%20we%20design%20a%20powerful%20motion%20trainer%20to%20learn%20both%20pose%20and%20expression%20encoders%20for%20extracting%20precise%20and%20decomposed%20driving%20signals.%20Then%20we%20propose%20to%20inject%20the%20pose%20transformation%20into%20the%20diffusion%20model%20through%20a%20dual-branch%20conditioning%20mechanism%2C%20and%20the%20expression%20latent%20through%20cross%20attention.%20Finally%2C%20we%20design%20a%20progressive%20hybrid%20classifier-free%20guidance%20for%20more%20faithful%20identity%20consistency.%20Experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%20both%20animation%20quality%20and%20disentangled%20controllability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeX-Portrait%253A%2520Disentangled%2520and%2520Expressive%2520Portrait%2520Animation%2520via%2520Explicit%2520and%2520Latent%2520Motion%2520Representations%26entry.906535625%3DYuxiang%2520Shi%2520and%2520Zhe%2520Li%2520and%2520Yanwen%2520Wang%2520and%2520Hao%2520Zhu%2520and%2520Xun%2520Cao%2520and%2520Ligang%2520Liu%26entry.1292438233%3DPortrait%2520animation%2520from%2520a%2520single%2520source%2520image%2520and%2520a%2520driving%2520video%2520is%2520a%2520long-standing%2520problem.%2520Recent%2520approaches%2520tend%2520to%2520adopt%2520diffusion-based%2520image/video%2520generation%2520models%2520for%2520realistic%2520and%2520expressive%2520animation.%2520However%252C%2520none%2520of%2520these%2520diffusion%2520models%2520realizes%2520high-fidelity%2520disentangled%2520control%2520between%2520the%2520head%2520pose%2520and%2520facial%2520expression%252C%2520hindering%2520applications%2520like%2520expression-only%2520or%2520pose-only%2520editing%2520and%2520animation.%2520To%2520address%2520this%252C%2520we%2520propose%2520DeX-Portrait%252C%2520a%2520novel%2520approach%2520capable%2520of%2520generating%2520expressive%2520portrait%2520animation%2520driven%2520by%2520disentangled%2520pose%2520and%2520expression%2520signals.%2520Specifically%252C%2520we%2520represent%2520the%2520pose%2520as%2520an%2520explicit%2520global%2520transformation%2520and%2520the%2520expression%2520as%2520an%2520implicit%2520latent%2520code.%2520First%252C%2520we%2520design%2520a%2520powerful%2520motion%2520trainer%2520to%2520learn%2520both%2520pose%2520and%2520expression%2520encoders%2520for%2520extracting%2520precise%2520and%2520decomposed%2520driving%2520signals.%2520Then%2520we%2520propose%2520to%2520inject%2520the%2520pose%2520transformation%2520into%2520the%2520diffusion%2520model%2520through%2520a%2520dual-branch%2520conditioning%2520mechanism%252C%2520and%2520the%2520expression%2520latent%2520through%2520cross%2520attention.%2520Finally%252C%2520we%2520design%2520a%2520progressive%2520hybrid%2520classifier-free%2520guidance%2520for%2520more%2520faithful%2520identity%2520consistency.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520on%2520both%2520animation%2520quality%2520and%2520disentangled%2520controllability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeX-Portrait%3A%20Disentangled%20and%20Expressive%20Portrait%20Animation%20via%20Explicit%20and%20Latent%20Motion%20Representations&entry.906535625=Yuxiang%20Shi%20and%20Zhe%20Li%20and%20Yanwen%20Wang%20and%20Hao%20Zhu%20and%20Xun%20Cao%20and%20Ligang%20Liu&entry.1292438233=Portrait%20animation%20from%20a%20single%20source%20image%20and%20a%20driving%20video%20is%20a%20long-standing%20problem.%20Recent%20approaches%20tend%20to%20adopt%20diffusion-based%20image/video%20generation%20models%20for%20realistic%20and%20expressive%20animation.%20However%2C%20none%20of%20these%20diffusion%20models%20realizes%20high-fidelity%20disentangled%20control%20between%20the%20head%20pose%20and%20facial%20expression%2C%20hindering%20applications%20like%20expression-only%20or%20pose-only%20editing%20and%20animation.%20To%20address%20this%2C%20we%20propose%20DeX-Portrait%2C%20a%20novel%20approach%20capable%20of%20generating%20expressive%20portrait%20animation%20driven%20by%20disentangled%20pose%20and%20expression%20signals.%20Specifically%2C%20we%20represent%20the%20pose%20as%20an%20explicit%20global%20transformation%20and%20the%20expression%20as%20an%20implicit%20latent%20code.%20First%2C%20we%20design%20a%20powerful%20motion%20trainer%20to%20learn%20both%20pose%20and%20expression%20encoders%20for%20extracting%20precise%20and%20decomposed%20driving%20signals.%20Then%20we%20propose%20to%20inject%20the%20pose%20transformation%20into%20the%20diffusion%20model%20through%20a%20dual-branch%20conditioning%20mechanism%2C%20and%20the%20expression%20latent%20through%20cross%20attention.%20Finally%2C%20we%20design%20a%20progressive%20hybrid%20classifier-free%20guidance%20for%20more%20faithful%20identity%20consistency.%20Experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%20both%20animation%20quality%20and%20disentangled%20controllability.&entry.1838667208=http%3A//arxiv.org/abs/2512.15524v1&entry.124074799=Read"},
{"title": "Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception", "author": "Malach Obisa Amonga and Benard Osero and Edna Too", "abstract": "Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.", "link": "http://arxiv.org/abs/2512.15480v1", "date": "2025-12-17", "relevancy": 2.5092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20deep%20learning%20architectures%20for%20wildlife%20object%20detection%3A%20A%20comparative%20study%20of%20ResNet%20and%20Inception&body=Title%3A%20Evaluation%20of%20deep%20learning%20architectures%20for%20wildlife%20object%20detection%3A%20A%20comparative%20study%20of%20ResNet%20and%20Inception%0AAuthor%3A%20Malach%20Obisa%20Amonga%20and%20Benard%20Osero%20and%20Edna%20Too%0AAbstract%3A%20Wildlife%20object%20detection%20plays%20a%20vital%20role%20in%20biodiversity%20conservation%2C%20ecological%20monitoring%2C%20and%20habitat%20protection.%20However%2C%20this%20task%20is%20often%20challenged%20by%20environmental%20variability%2C%20visual%20similarities%20among%20species%2C%20and%20intra-class%20diversity.%20This%20study%20investigates%20the%20effectiveness%20of%20two%20individual%20deep%20learning%20architectures%20ResNet-101%20and%20Inception%20v3%20for%20wildlife%20object%20detection%20under%20such%20complex%20conditions.%20The%20models%20were%20trained%20and%20evaluated%20on%20a%20wildlife%20image%20dataset%20using%20a%20standardized%20preprocessing%20approach%2C%20which%20included%20resizing%20images%20to%20a%20maximum%20dimension%20of%20800%20pixels%2C%20converting%20them%20to%20RGB%20format%2C%20and%20transforming%20them%20into%20PyTorch%20tensors.%20A%20ratio%20of%2070%3A30%20training%20and%20validation%20split%20was%20used%20for%20model%20development.%20The%20ResNet-101%20model%20achieved%20a%20classification%20accuracy%20of%2094%25%20and%20a%20mean%20Average%20Precision%20%28mAP%29%20of%200.91%2C%20showing%20strong%20performance%20in%20extracting%20deep%20hierarchical%20features.%20The%20Inception%20v3%20model%20performed%20slightly%20better%2C%20attaining%20a%20classification%20accuracy%20of%2095%25%20and%20a%20mAP%20of%200.92%2C%20attributed%20to%20its%20efficient%20multi-scale%20feature%20extraction%20through%20parallel%20convolutions.%20Despite%20the%20strong%20results%2C%20both%20models%20exhibited%20challenges%20when%20detecting%20species%20with%20similar%20visual%20characteristics%20or%20those%20captured%20under%20poor%20lighting%20and%20occlusion.%20Nonetheless%2C%20the%20findings%20confirm%20that%20both%20ResNet-101%20and%20Inception%20v3%20are%20effective%20models%20for%20wildlife%20object%20detection%20tasks%20and%20provide%20a%20reliable%20foundation%20for%20conservation-focused%20computer%20vision%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520deep%2520learning%2520architectures%2520for%2520wildlife%2520object%2520detection%253A%2520A%2520comparative%2520study%2520of%2520ResNet%2520and%2520Inception%26entry.906535625%3DMalach%2520Obisa%2520Amonga%2520and%2520Benard%2520Osero%2520and%2520Edna%2520Too%26entry.1292438233%3DWildlife%2520object%2520detection%2520plays%2520a%2520vital%2520role%2520in%2520biodiversity%2520conservation%252C%2520ecological%2520monitoring%252C%2520and%2520habitat%2520protection.%2520However%252C%2520this%2520task%2520is%2520often%2520challenged%2520by%2520environmental%2520variability%252C%2520visual%2520similarities%2520among%2520species%252C%2520and%2520intra-class%2520diversity.%2520This%2520study%2520investigates%2520the%2520effectiveness%2520of%2520two%2520individual%2520deep%2520learning%2520architectures%2520ResNet-101%2520and%2520Inception%2520v3%2520for%2520wildlife%2520object%2520detection%2520under%2520such%2520complex%2520conditions.%2520The%2520models%2520were%2520trained%2520and%2520evaluated%2520on%2520a%2520wildlife%2520image%2520dataset%2520using%2520a%2520standardized%2520preprocessing%2520approach%252C%2520which%2520included%2520resizing%2520images%2520to%2520a%2520maximum%2520dimension%2520of%2520800%2520pixels%252C%2520converting%2520them%2520to%2520RGB%2520format%252C%2520and%2520transforming%2520them%2520into%2520PyTorch%2520tensors.%2520A%2520ratio%2520of%252070%253A30%2520training%2520and%2520validation%2520split%2520was%2520used%2520for%2520model%2520development.%2520The%2520ResNet-101%2520model%2520achieved%2520a%2520classification%2520accuracy%2520of%252094%2525%2520and%2520a%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520of%25200.91%252C%2520showing%2520strong%2520performance%2520in%2520extracting%2520deep%2520hierarchical%2520features.%2520The%2520Inception%2520v3%2520model%2520performed%2520slightly%2520better%252C%2520attaining%2520a%2520classification%2520accuracy%2520of%252095%2525%2520and%2520a%2520mAP%2520of%25200.92%252C%2520attributed%2520to%2520its%2520efficient%2520multi-scale%2520feature%2520extraction%2520through%2520parallel%2520convolutions.%2520Despite%2520the%2520strong%2520results%252C%2520both%2520models%2520exhibited%2520challenges%2520when%2520detecting%2520species%2520with%2520similar%2520visual%2520characteristics%2520or%2520those%2520captured%2520under%2520poor%2520lighting%2520and%2520occlusion.%2520Nonetheless%252C%2520the%2520findings%2520confirm%2520that%2520both%2520ResNet-101%2520and%2520Inception%2520v3%2520are%2520effective%2520models%2520for%2520wildlife%2520object%2520detection%2520tasks%2520and%2520provide%2520a%2520reliable%2520foundation%2520for%2520conservation-focused%2520computer%2520vision%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20deep%20learning%20architectures%20for%20wildlife%20object%20detection%3A%20A%20comparative%20study%20of%20ResNet%20and%20Inception&entry.906535625=Malach%20Obisa%20Amonga%20and%20Benard%20Osero%20and%20Edna%20Too&entry.1292438233=Wildlife%20object%20detection%20plays%20a%20vital%20role%20in%20biodiversity%20conservation%2C%20ecological%20monitoring%2C%20and%20habitat%20protection.%20However%2C%20this%20task%20is%20often%20challenged%20by%20environmental%20variability%2C%20visual%20similarities%20among%20species%2C%20and%20intra-class%20diversity.%20This%20study%20investigates%20the%20effectiveness%20of%20two%20individual%20deep%20learning%20architectures%20ResNet-101%20and%20Inception%20v3%20for%20wildlife%20object%20detection%20under%20such%20complex%20conditions.%20The%20models%20were%20trained%20and%20evaluated%20on%20a%20wildlife%20image%20dataset%20using%20a%20standardized%20preprocessing%20approach%2C%20which%20included%20resizing%20images%20to%20a%20maximum%20dimension%20of%20800%20pixels%2C%20converting%20them%20to%20RGB%20format%2C%20and%20transforming%20them%20into%20PyTorch%20tensors.%20A%20ratio%20of%2070%3A30%20training%20and%20validation%20split%20was%20used%20for%20model%20development.%20The%20ResNet-101%20model%20achieved%20a%20classification%20accuracy%20of%2094%25%20and%20a%20mean%20Average%20Precision%20%28mAP%29%20of%200.91%2C%20showing%20strong%20performance%20in%20extracting%20deep%20hierarchical%20features.%20The%20Inception%20v3%20model%20performed%20slightly%20better%2C%20attaining%20a%20classification%20accuracy%20of%2095%25%20and%20a%20mAP%20of%200.92%2C%20attributed%20to%20its%20efficient%20multi-scale%20feature%20extraction%20through%20parallel%20convolutions.%20Despite%20the%20strong%20results%2C%20both%20models%20exhibited%20challenges%20when%20detecting%20species%20with%20similar%20visual%20characteristics%20or%20those%20captured%20under%20poor%20lighting%20and%20occlusion.%20Nonetheless%2C%20the%20findings%20confirm%20that%20both%20ResNet-101%20and%20Inception%20v3%20are%20effective%20models%20for%20wildlife%20object%20detection%20tasks%20and%20provide%20a%20reliable%20foundation%20for%20conservation-focused%20computer%20vision%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.15480v1&entry.124074799=Read"},
{"title": "Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis", "author": "Srishti Gupta and Zhang Chen and Luca Demetrio and Xiaoyi Feng and Zhaoqiang Xia and Antonio Emanuele Cin\u00e0 and Maura Pintor and Luca Oneto and Ambra Demontis and Battista Biggio and Fabio Roli", "abstract": "Thanks to their extensive capacity, over-parameterized neural networks exhibit superior predictive capabilities and generalization. However, having a large parameter space is considered one of the main suspects of the neural networks' vulnerability to adversarial example -- input samples crafted ad-hoc to induce a desired misclassification. Relevant literature has claimed contradictory remarks in support of and against the robustness of over-parameterized networks. These contradictory findings might be due to the failure of the attack employed to evaluate the networks' robustness. Previous research has demonstrated that depending on the considered model, the algorithm employed to generate adversarial examples may not function properly, leading to overestimating the model's robustness. In this work, we empirically study the robustness of over-parameterized networks against adversarial examples. However, unlike the previous works, we also evaluate the considered attack's reliability to support the results' veracity. Our results show that over-parameterized networks are robust against adversarial attacks as opposed to their under-parameterized counterparts.", "link": "http://arxiv.org/abs/2406.10090v2", "date": "2025-12-17", "relevancy": 2.4811, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5172}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4911}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%20Overview%20and%20Empirical%20Analysis&body=Title%3A%20Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%20Overview%20and%20Empirical%20Analysis%0AAuthor%3A%20Srishti%20Gupta%20and%20Zhang%20Chen%20and%20Luca%20Demetrio%20and%20Xiaoyi%20Feng%20and%20Zhaoqiang%20Xia%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Maura%20Pintor%20and%20Luca%20Oneto%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20Thanks%20to%20their%20extensive%20capacity%2C%20over-parameterized%20neural%20networks%20exhibit%20superior%20predictive%20capabilities%20and%20generalization.%20However%2C%20having%20a%20large%20parameter%20space%20is%20considered%20one%20of%20the%20main%20suspects%20of%20the%20neural%20networks%27%20vulnerability%20to%20adversarial%20example%20--%20input%20samples%20crafted%20ad-hoc%20to%20induce%20a%20desired%20misclassification.%20Relevant%20literature%20has%20claimed%20contradictory%20remarks%20in%20support%20of%20and%20against%20the%20robustness%20of%20over-parameterized%20networks.%20These%20contradictory%20findings%20might%20be%20due%20to%20the%20failure%20of%20the%20attack%20employed%20to%20evaluate%20the%20networks%27%20robustness.%20Previous%20research%20has%20demonstrated%20that%20depending%20on%20the%20considered%20model%2C%20the%20algorithm%20employed%20to%20generate%20adversarial%20examples%20may%20not%20function%20properly%2C%20leading%20to%20overestimating%20the%20model%27s%20robustness.%20In%20this%20work%2C%20we%20empirically%20study%20the%20robustness%20of%20over-parameterized%20networks%20against%20adversarial%20examples.%20However%2C%20unlike%20the%20previous%20works%2C%20we%20also%20evaluate%20the%20considered%20attack%27s%20reliability%20to%20support%20the%20results%27%20veracity.%20Our%20results%20show%20that%20over-parameterized%20networks%20are%20robust%20against%20adversarial%20attacks%20as%20opposed%20to%20their%20under-parameterized%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2406.10090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver-parameterization%2520and%2520Adversarial%2520Robustness%2520in%2520Neural%2520Networks%253A%2520An%2520Overview%2520and%2520Empirical%2520Analysis%26entry.906535625%3DSrishti%2520Gupta%2520and%2520Zhang%2520Chen%2520and%2520Luca%2520Demetrio%2520and%2520Xiaoyi%2520Feng%2520and%2520Zhaoqiang%2520Xia%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Maura%2520Pintor%2520and%2520Luca%2520Oneto%2520and%2520Ambra%2520Demontis%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3DThanks%2520to%2520their%2520extensive%2520capacity%252C%2520over-parameterized%2520neural%2520networks%2520exhibit%2520superior%2520predictive%2520capabilities%2520and%2520generalization.%2520However%252C%2520having%2520a%2520large%2520parameter%2520space%2520is%2520considered%2520one%2520of%2520the%2520main%2520suspects%2520of%2520the%2520neural%2520networks%2527%2520vulnerability%2520to%2520adversarial%2520example%2520--%2520input%2520samples%2520crafted%2520ad-hoc%2520to%2520induce%2520a%2520desired%2520misclassification.%2520Relevant%2520literature%2520has%2520claimed%2520contradictory%2520remarks%2520in%2520support%2520of%2520and%2520against%2520the%2520robustness%2520of%2520over-parameterized%2520networks.%2520These%2520contradictory%2520findings%2520might%2520be%2520due%2520to%2520the%2520failure%2520of%2520the%2520attack%2520employed%2520to%2520evaluate%2520the%2520networks%2527%2520robustness.%2520Previous%2520research%2520has%2520demonstrated%2520that%2520depending%2520on%2520the%2520considered%2520model%252C%2520the%2520algorithm%2520employed%2520to%2520generate%2520adversarial%2520examples%2520may%2520not%2520function%2520properly%252C%2520leading%2520to%2520overestimating%2520the%2520model%2527s%2520robustness.%2520In%2520this%2520work%252C%2520we%2520empirically%2520study%2520the%2520robustness%2520of%2520over-parameterized%2520networks%2520against%2520adversarial%2520examples.%2520However%252C%2520unlike%2520the%2520previous%2520works%252C%2520we%2520also%2520evaluate%2520the%2520considered%2520attack%2527s%2520reliability%2520to%2520support%2520the%2520results%2527%2520veracity.%2520Our%2520results%2520show%2520that%2520over-parameterized%2520networks%2520are%2520robust%2520against%2520adversarial%2520attacks%2520as%2520opposed%2520to%2520their%2520under-parameterized%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%20Overview%20and%20Empirical%20Analysis&entry.906535625=Srishti%20Gupta%20and%20Zhang%20Chen%20and%20Luca%20Demetrio%20and%20Xiaoyi%20Feng%20and%20Zhaoqiang%20Xia%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Maura%20Pintor%20and%20Luca%20Oneto%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=Thanks%20to%20their%20extensive%20capacity%2C%20over-parameterized%20neural%20networks%20exhibit%20superior%20predictive%20capabilities%20and%20generalization.%20However%2C%20having%20a%20large%20parameter%20space%20is%20considered%20one%20of%20the%20main%20suspects%20of%20the%20neural%20networks%27%20vulnerability%20to%20adversarial%20example%20--%20input%20samples%20crafted%20ad-hoc%20to%20induce%20a%20desired%20misclassification.%20Relevant%20literature%20has%20claimed%20contradictory%20remarks%20in%20support%20of%20and%20against%20the%20robustness%20of%20over-parameterized%20networks.%20These%20contradictory%20findings%20might%20be%20due%20to%20the%20failure%20of%20the%20attack%20employed%20to%20evaluate%20the%20networks%27%20robustness.%20Previous%20research%20has%20demonstrated%20that%20depending%20on%20the%20considered%20model%2C%20the%20algorithm%20employed%20to%20generate%20adversarial%20examples%20may%20not%20function%20properly%2C%20leading%20to%20overestimating%20the%20model%27s%20robustness.%20In%20this%20work%2C%20we%20empirically%20study%20the%20robustness%20of%20over-parameterized%20networks%20against%20adversarial%20examples.%20However%2C%20unlike%20the%20previous%20works%2C%20we%20also%20evaluate%20the%20considered%20attack%27s%20reliability%20to%20support%20the%20results%27%20veracity.%20Our%20results%20show%20that%20over-parameterized%20networks%20are%20robust%20against%20adversarial%20attacks%20as%20opposed%20to%20their%20under-parameterized%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2406.10090v2&entry.124074799=Read"},
{"title": "How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness", "author": "Darshita Rathore and Vineet Kumar and Chetna Bansal and Anindya Moitra", "abstract": "Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.", "link": "http://arxiv.org/abs/2512.15634v1", "date": "2025-12-17", "relevancy": 2.4571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Much%20is%20Too%20Much%3F%20Exploring%20LoRA%20Rank%20Trade-offs%20for%20Retaining%20Knowledge%20and%20Domain%20Robustness&body=Title%3A%20How%20Much%20is%20Too%20Much%3F%20Exploring%20LoRA%20Rank%20Trade-offs%20for%20Retaining%20Knowledge%20and%20Domain%20Robustness%0AAuthor%3A%20Darshita%20Rathore%20and%20Vineet%20Kumar%20and%20Chetna%20Bansal%20and%20Anindya%20Moitra%0AAbstract%3A%20Large%20language%20models%20are%20increasingly%20adapted%20to%20downstream%20tasks%20through%20fine-tuning.%20Full%20supervised%20fine-tuning%20%28SFT%29%20and%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20are%20two%20dominant%20approaches.%20While%20PEFT%20methods%20are%20widely%20used%20for%20their%20computational%20efficiency%2C%20the%20implications%20of%20their%20configurations%20%28e.g.%2C%20rank%29%20remain%20under-explored%20in%20downstream%20Q%26A%20tasks%20and%20generalisation.%20In%20this%20work%2C%20we%20perform%20a%20comprehensive%20evaluation%20across%20multiple%20reasoning%20and%20recall%20datasets%2C%20conducting%20a%20rank%20sweep%20to%20quantify%20the%20trade-off%20between%20SFT%20and%20PEFT.%20We%20also%20compare%20the%20accuracy%20of%20PEFT%20and%20SFT%20models%20across%20in-domain%20and%20out-of-domain%20adaptation%2C%20highlighting%20distinct%20generalisation%20behaviour%20and%20task-specific%20forgetting.%20We%20demonstrate%20that%20LoRA%20achieves%20competitive%20and%20in%20some%20cases%20superior%20performance%20compared%20to%20SFT%2C%20particularly%20on%20reasoning%20tasks%20at%20specific%20rank%20values.%20Additionally%2C%20we%20analyze%20the%20internal%20representations%20via%20spectral%20features%20and%20layer-wise%20attention%20structures%2C%20offering%20insights%20into%20representational%20drift%20and%20structural%20changes%20in%20attention%20patterns.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Much%2520is%2520Too%2520Much%253F%2520Exploring%2520LoRA%2520Rank%2520Trade-offs%2520for%2520Retaining%2520Knowledge%2520and%2520Domain%2520Robustness%26entry.906535625%3DDarshita%2520Rathore%2520and%2520Vineet%2520Kumar%2520and%2520Chetna%2520Bansal%2520and%2520Anindya%2520Moitra%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520increasingly%2520adapted%2520to%2520downstream%2520tasks%2520through%2520fine-tuning.%2520Full%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520are%2520two%2520dominant%2520approaches.%2520While%2520PEFT%2520methods%2520are%2520widely%2520used%2520for%2520their%2520computational%2520efficiency%252C%2520the%2520implications%2520of%2520their%2520configurations%2520%2528e.g.%252C%2520rank%2529%2520remain%2520under-explored%2520in%2520downstream%2520Q%2526A%2520tasks%2520and%2520generalisation.%2520In%2520this%2520work%252C%2520we%2520perform%2520a%2520comprehensive%2520evaluation%2520across%2520multiple%2520reasoning%2520and%2520recall%2520datasets%252C%2520conducting%2520a%2520rank%2520sweep%2520to%2520quantify%2520the%2520trade-off%2520between%2520SFT%2520and%2520PEFT.%2520We%2520also%2520compare%2520the%2520accuracy%2520of%2520PEFT%2520and%2520SFT%2520models%2520across%2520in-domain%2520and%2520out-of-domain%2520adaptation%252C%2520highlighting%2520distinct%2520generalisation%2520behaviour%2520and%2520task-specific%2520forgetting.%2520We%2520demonstrate%2520that%2520LoRA%2520achieves%2520competitive%2520and%2520in%2520some%2520cases%2520superior%2520performance%2520compared%2520to%2520SFT%252C%2520particularly%2520on%2520reasoning%2520tasks%2520at%2520specific%2520rank%2520values.%2520Additionally%252C%2520we%2520analyze%2520the%2520internal%2520representations%2520via%2520spectral%2520features%2520and%2520layer-wise%2520attention%2520structures%252C%2520offering%2520insights%2520into%2520representational%2520drift%2520and%2520structural%2520changes%2520in%2520attention%2520patterns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Much%20is%20Too%20Much%3F%20Exploring%20LoRA%20Rank%20Trade-offs%20for%20Retaining%20Knowledge%20and%20Domain%20Robustness&entry.906535625=Darshita%20Rathore%20and%20Vineet%20Kumar%20and%20Chetna%20Bansal%20and%20Anindya%20Moitra&entry.1292438233=Large%20language%20models%20are%20increasingly%20adapted%20to%20downstream%20tasks%20through%20fine-tuning.%20Full%20supervised%20fine-tuning%20%28SFT%29%20and%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20are%20two%20dominant%20approaches.%20While%20PEFT%20methods%20are%20widely%20used%20for%20their%20computational%20efficiency%2C%20the%20implications%20of%20their%20configurations%20%28e.g.%2C%20rank%29%20remain%20under-explored%20in%20downstream%20Q%26A%20tasks%20and%20generalisation.%20In%20this%20work%2C%20we%20perform%20a%20comprehensive%20evaluation%20across%20multiple%20reasoning%20and%20recall%20datasets%2C%20conducting%20a%20rank%20sweep%20to%20quantify%20the%20trade-off%20between%20SFT%20and%20PEFT.%20We%20also%20compare%20the%20accuracy%20of%20PEFT%20and%20SFT%20models%20across%20in-domain%20and%20out-of-domain%20adaptation%2C%20highlighting%20distinct%20generalisation%20behaviour%20and%20task-specific%20forgetting.%20We%20demonstrate%20that%20LoRA%20achieves%20competitive%20and%20in%20some%20cases%20superior%20performance%20compared%20to%20SFT%2C%20particularly%20on%20reasoning%20tasks%20at%20specific%20rank%20values.%20Additionally%2C%20we%20analyze%20the%20internal%20representations%20via%20spectral%20features%20and%20layer-wise%20attention%20structures%2C%20offering%20insights%20into%20representational%20drift%20and%20structural%20changes%20in%20attention%20patterns.&entry.1838667208=http%3A//arxiv.org/abs/2512.15634v1&entry.124074799=Read"},
{"title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "author": "Egor Fadeev and Dzhambulat Mollaev and Aleksei Shestov and Omar Zoloev and Artem Sakhno and Dmitry Korolev and Ivan Kireev and Andrey Savchenko and Maksim Makarenko", "abstract": "Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.", "link": "http://arxiv.org/abs/2508.10021v4", "date": "2025-12-17", "relevancy": 2.4502, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4848}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%20Clients&body=Title%3A%20LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%20Clients%0AAuthor%3A%20Egor%20Fadeev%20and%20Dzhambulat%20Mollaev%20and%20Aleksei%20Shestov%20and%20Omar%20Zoloev%20and%20Artem%20Sakhno%20and%20Dmitry%20Korolev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko%0AAbstract%3A%20Learning%20clients%20embeddings%20from%20sequences%20of%20their%20historic%20communications%20is%20central%20to%20financial%20applications.%20While%20large%20language%20models%20%28LLMs%29%20offer%20general%20world%20knowledge%2C%20their%20direct%20use%20on%20long%20event%20sequences%20is%20computationally%20expensive%20and%20impractical%20in%20real-world%20pipelines.%20In%20this%20paper%2C%20we%20propose%20LATTE%2C%20a%20contrastive%20learning%20framework%20that%20aligns%20raw%20event%20embeddings%20with%20semantic%20embeddings%20from%20frozen%20LLMs.%20Behavioral%20features%20are%20summarized%20into%20short%20prompts%2C%20embedded%20by%20the%20LLM%2C%20and%20used%20as%20supervision%20via%20contrastive%20loss.%20The%20proposed%20approach%20significantly%20reduces%20inference%20cost%20and%20input%20size%20compared%20to%20conventional%20processing%20of%20complete%20sequence%20by%20LLM.%20We%20experimentally%20show%20that%20our%20method%20outperforms%20state-of-the-art%20techniques%20for%20learning%20event%20sequence%20representations%20on%20real-world%20financial%20datasets%20while%20remaining%20deployable%20in%20latency-sensitive%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10021v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLATTE%253A%2520Learning%2520Aligned%2520Transactions%2520and%2520Textual%2520Embeddings%2520for%2520Bank%2520Clients%26entry.906535625%3DEgor%2520Fadeev%2520and%2520Dzhambulat%2520Mollaev%2520and%2520Aleksei%2520Shestov%2520and%2520Omar%2520Zoloev%2520and%2520Artem%2520Sakhno%2520and%2520Dmitry%2520Korolev%2520and%2520Ivan%2520Kireev%2520and%2520Andrey%2520Savchenko%2520and%2520Maksim%2520Makarenko%26entry.1292438233%3DLearning%2520clients%2520embeddings%2520from%2520sequences%2520of%2520their%2520historic%2520communications%2520is%2520central%2520to%2520financial%2520applications.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520general%2520world%2520knowledge%252C%2520their%2520direct%2520use%2520on%2520long%2520event%2520sequences%2520is%2520computationally%2520expensive%2520and%2520impractical%2520in%2520real-world%2520pipelines.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LATTE%252C%2520a%2520contrastive%2520learning%2520framework%2520that%2520aligns%2520raw%2520event%2520embeddings%2520with%2520semantic%2520embeddings%2520from%2520frozen%2520LLMs.%2520Behavioral%2520features%2520are%2520summarized%2520into%2520short%2520prompts%252C%2520embedded%2520by%2520the%2520LLM%252C%2520and%2520used%2520as%2520supervision%2520via%2520contrastive%2520loss.%2520The%2520proposed%2520approach%2520significantly%2520reduces%2520inference%2520cost%2520and%2520input%2520size%2520compared%2520to%2520conventional%2520processing%2520of%2520complete%2520sequence%2520by%2520LLM.%2520We%2520experimentally%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520techniques%2520for%2520learning%2520event%2520sequence%2520representations%2520on%2520real-world%2520financial%2520datasets%2520while%2520remaining%2520deployable%2520in%2520latency-sensitive%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10021v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LATTE%3A%20Learning%20Aligned%20Transactions%20and%20Textual%20Embeddings%20for%20Bank%20Clients&entry.906535625=Egor%20Fadeev%20and%20Dzhambulat%20Mollaev%20and%20Aleksei%20Shestov%20and%20Omar%20Zoloev%20and%20Artem%20Sakhno%20and%20Dmitry%20Korolev%20and%20Ivan%20Kireev%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko&entry.1292438233=Learning%20clients%20embeddings%20from%20sequences%20of%20their%20historic%20communications%20is%20central%20to%20financial%20applications.%20While%20large%20language%20models%20%28LLMs%29%20offer%20general%20world%20knowledge%2C%20their%20direct%20use%20on%20long%20event%20sequences%20is%20computationally%20expensive%20and%20impractical%20in%20real-world%20pipelines.%20In%20this%20paper%2C%20we%20propose%20LATTE%2C%20a%20contrastive%20learning%20framework%20that%20aligns%20raw%20event%20embeddings%20with%20semantic%20embeddings%20from%20frozen%20LLMs.%20Behavioral%20features%20are%20summarized%20into%20short%20prompts%2C%20embedded%20by%20the%20LLM%2C%20and%20used%20as%20supervision%20via%20contrastive%20loss.%20The%20proposed%20approach%20significantly%20reduces%20inference%20cost%20and%20input%20size%20compared%20to%20conventional%20processing%20of%20complete%20sequence%20by%20LLM.%20We%20experimentally%20show%20that%20our%20method%20outperforms%20state-of-the-art%20techniques%20for%20learning%20event%20sequence%20representations%20on%20real-world%20financial%20datasets%20while%20remaining%20deployable%20in%20latency-sensitive%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2508.10021v4&entry.124074799=Read"},
{"title": "Structure-Aligned Protein Language Model", "author": "Can Chen and David Heurtel-Depeiges and Robert M. Vernon and Christopher James Langmead and Yoshua Bengio and Quentin Fournier", "abstract": "Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but often lack the structural knowledge essential for some biological applications. To address this, we introduce a method to enrich pLMs with structural knowledge by leveraging pre-trained protein graph neural networks (pGNNs). First, a latent-level contrastive learning task aligns residue representations from pLMs with those from pGNNs across multiple proteins, injecting inter-protein structural information. Additionally, a physical-level task integrates intra-protein information by training pLMs to predict structure tokens. Together, the proposed dual-task framework effectively incorporates both inter- and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module that uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method as a simple, lightweight post-training step to the state-of-the-art ESM2 and AMPLIFY yields notable performance gains. These improvements are consistent across a wide range of tasks, including substantial gains in deep mutational scanning (DMS) fitness prediction and a 59% increase in P@L for ESM2 650M contact prediction on CASP16. Furthermore, we demonstrate that these performance gains are robust, scaling with model sizes from 8M to 650M and extending to different downstream tasks.", "link": "http://arxiv.org/abs/2505.16896v2", "date": "2025-12-17", "relevancy": 2.4408, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aligned%20Protein%20Language%20Model&body=Title%3A%20Structure-Aligned%20Protein%20Language%20Model%0AAuthor%3A%20Can%20Chen%20and%20David%20Heurtel-Depeiges%20and%20Robert%20M.%20Vernon%20and%20Christopher%20James%20Langmead%20and%20Yoshua%20Bengio%20and%20Quentin%20Fournier%0AAbstract%3A%20Protein%20language%20models%20%28pLMs%29%20pre-trained%20on%20vast%20protein%20sequence%20databases%20excel%20at%20various%20downstream%20tasks%20but%20often%20lack%20the%20structural%20knowledge%20essential%20for%20some%20biological%20applications.%20To%20address%20this%2C%20we%20introduce%20a%20method%20to%20enrich%20pLMs%20with%20structural%20knowledge%20by%20leveraging%20pre-trained%20protein%20graph%20neural%20networks%20%28pGNNs%29.%20First%2C%20a%20latent-level%20contrastive%20learning%20task%20aligns%20residue%20representations%20from%20pLMs%20with%20those%20from%20pGNNs%20across%20multiple%20proteins%2C%20injecting%20inter-protein%20structural%20information.%20Additionally%2C%20a%20physical-level%20task%20integrates%20intra-protein%20information%20by%20training%20pLMs%20to%20predict%20structure%20tokens.%20Together%2C%20the%20proposed%20dual-task%20framework%20effectively%20incorporates%20both%20inter-%20and%20intra-protein%20structural%20knowledge%20into%20pLMs.%20Given%20the%20variability%20in%20the%20quality%20of%20protein%20structures%20in%20PDB%2C%20we%20further%20introduce%20a%20residue%20loss%20selection%20module%20that%20uses%20a%20small%20model%20trained%20on%20high-quality%20structures%20to%20select%20reliable%20yet%20challenging%20residue%20losses%20for%20the%20pLM%20to%20learn.%20Applying%20our%20structure%20alignment%20method%20as%20a%20simple%2C%20lightweight%20post-training%20step%20to%20the%20state-of-the-art%20ESM2%20and%20AMPLIFY%20yields%20notable%20performance%20gains.%20These%20improvements%20are%20consistent%20across%20a%20wide%20range%20of%20tasks%2C%20including%20substantial%20gains%20in%20deep%20mutational%20scanning%20%28DMS%29%20fitness%20prediction%20and%20a%2059%25%20increase%20in%20P%40L%20for%20ESM2%20650M%20contact%20prediction%20on%20CASP16.%20Furthermore%2C%20we%20demonstrate%20that%20these%20performance%20gains%20are%20robust%2C%20scaling%20with%20model%20sizes%20from%208M%20to%20650M%20and%20extending%20to%20different%20downstream%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aligned%2520Protein%2520Language%2520Model%26entry.906535625%3DCan%2520Chen%2520and%2520David%2520Heurtel-Depeiges%2520and%2520Robert%2520M.%2520Vernon%2520and%2520Christopher%2520James%2520Langmead%2520and%2520Yoshua%2520Bengio%2520and%2520Quentin%2520Fournier%26entry.1292438233%3DProtein%2520language%2520models%2520%2528pLMs%2529%2520pre-trained%2520on%2520vast%2520protein%2520sequence%2520databases%2520excel%2520at%2520various%2520downstream%2520tasks%2520but%2520often%2520lack%2520the%2520structural%2520knowledge%2520essential%2520for%2520some%2520biological%2520applications.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520method%2520to%2520enrich%2520pLMs%2520with%2520structural%2520knowledge%2520by%2520leveraging%2520pre-trained%2520protein%2520graph%2520neural%2520networks%2520%2528pGNNs%2529.%2520First%252C%2520a%2520latent-level%2520contrastive%2520learning%2520task%2520aligns%2520residue%2520representations%2520from%2520pLMs%2520with%2520those%2520from%2520pGNNs%2520across%2520multiple%2520proteins%252C%2520injecting%2520inter-protein%2520structural%2520information.%2520Additionally%252C%2520a%2520physical-level%2520task%2520integrates%2520intra-protein%2520information%2520by%2520training%2520pLMs%2520to%2520predict%2520structure%2520tokens.%2520Together%252C%2520the%2520proposed%2520dual-task%2520framework%2520effectively%2520incorporates%2520both%2520inter-%2520and%2520intra-protein%2520structural%2520knowledge%2520into%2520pLMs.%2520Given%2520the%2520variability%2520in%2520the%2520quality%2520of%2520protein%2520structures%2520in%2520PDB%252C%2520we%2520further%2520introduce%2520a%2520residue%2520loss%2520selection%2520module%2520that%2520uses%2520a%2520small%2520model%2520trained%2520on%2520high-quality%2520structures%2520to%2520select%2520reliable%2520yet%2520challenging%2520residue%2520losses%2520for%2520the%2520pLM%2520to%2520learn.%2520Applying%2520our%2520structure%2520alignment%2520method%2520as%2520a%2520simple%252C%2520lightweight%2520post-training%2520step%2520to%2520the%2520state-of-the-art%2520ESM2%2520and%2520AMPLIFY%2520yields%2520notable%2520performance%2520gains.%2520These%2520improvements%2520are%2520consistent%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520including%2520substantial%2520gains%2520in%2520deep%2520mutational%2520scanning%2520%2528DMS%2529%2520fitness%2520prediction%2520and%2520a%252059%2525%2520increase%2520in%2520P%2540L%2520for%2520ESM2%2520650M%2520contact%2520prediction%2520on%2520CASP16.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520these%2520performance%2520gains%2520are%2520robust%252C%2520scaling%2520with%2520model%2520sizes%2520from%25208M%2520to%2520650M%2520and%2520extending%2520to%2520different%2520downstream%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aligned%20Protein%20Language%20Model&entry.906535625=Can%20Chen%20and%20David%20Heurtel-Depeiges%20and%20Robert%20M.%20Vernon%20and%20Christopher%20James%20Langmead%20and%20Yoshua%20Bengio%20and%20Quentin%20Fournier&entry.1292438233=Protein%20language%20models%20%28pLMs%29%20pre-trained%20on%20vast%20protein%20sequence%20databases%20excel%20at%20various%20downstream%20tasks%20but%20often%20lack%20the%20structural%20knowledge%20essential%20for%20some%20biological%20applications.%20To%20address%20this%2C%20we%20introduce%20a%20method%20to%20enrich%20pLMs%20with%20structural%20knowledge%20by%20leveraging%20pre-trained%20protein%20graph%20neural%20networks%20%28pGNNs%29.%20First%2C%20a%20latent-level%20contrastive%20learning%20task%20aligns%20residue%20representations%20from%20pLMs%20with%20those%20from%20pGNNs%20across%20multiple%20proteins%2C%20injecting%20inter-protein%20structural%20information.%20Additionally%2C%20a%20physical-level%20task%20integrates%20intra-protein%20information%20by%20training%20pLMs%20to%20predict%20structure%20tokens.%20Together%2C%20the%20proposed%20dual-task%20framework%20effectively%20incorporates%20both%20inter-%20and%20intra-protein%20structural%20knowledge%20into%20pLMs.%20Given%20the%20variability%20in%20the%20quality%20of%20protein%20structures%20in%20PDB%2C%20we%20further%20introduce%20a%20residue%20loss%20selection%20module%20that%20uses%20a%20small%20model%20trained%20on%20high-quality%20structures%20to%20select%20reliable%20yet%20challenging%20residue%20losses%20for%20the%20pLM%20to%20learn.%20Applying%20our%20structure%20alignment%20method%20as%20a%20simple%2C%20lightweight%20post-training%20step%20to%20the%20state-of-the-art%20ESM2%20and%20AMPLIFY%20yields%20notable%20performance%20gains.%20These%20improvements%20are%20consistent%20across%20a%20wide%20range%20of%20tasks%2C%20including%20substantial%20gains%20in%20deep%20mutational%20scanning%20%28DMS%29%20fitness%20prediction%20and%20a%2059%25%20increase%20in%20P%40L%20for%20ESM2%20650M%20contact%20prediction%20on%20CASP16.%20Furthermore%2C%20we%20demonstrate%20that%20these%20performance%20gains%20are%20robust%2C%20scaling%20with%20model%20sizes%20from%208M%20to%20650M%20and%20extending%20to%20different%20downstream%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2505.16896v2&entry.124074799=Read"},
{"title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models", "author": "Bozhou Li and Sihan Yang and Yushuo Guan and Ruichuan An and Xinlong Chen and Yang Shi and Pengfei Wan and Wentao Zhang and Yuanxing zhang", "abstract": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.", "link": "http://arxiv.org/abs/2512.15560v1", "date": "2025-12-17", "relevancy": 2.4299, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6353}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6099}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAN-TED%3A%20Generating%20Robust%2C%20Aligned%2C%20and%20Nuanced%20Text%20Embedding%20for%20Diffusion%20Models&body=Title%3A%20GRAN-TED%3A%20Generating%20Robust%2C%20Aligned%2C%20and%20Nuanced%20Text%20Embedding%20for%20Diffusion%20Models%0AAuthor%3A%20Bozhou%20Li%20and%20Sihan%20Yang%20and%20Yushuo%20Guan%20and%20Ruichuan%20An%20and%20Xinlong%20Chen%20and%20Yang%20Shi%20and%20Pengfei%20Wan%20and%20Wentao%20Zhang%20and%20Yuanxing%20zhang%0AAbstract%3A%20The%20text%20encoder%20is%20a%20critical%20component%20of%20text-to-image%20and%20text-to-video%20diffusion%20models%2C%20fundamentally%20determining%20the%20semantic%20fidelity%20of%20the%20generated%20content.%20However%2C%20its%20development%20has%20been%20hindered%20by%20two%20major%20challenges%3A%20the%20lack%20of%20an%20efficient%20evaluation%20framework%20that%20reliably%20predicts%20downstream%20generation%20performance%2C%20and%20the%20difficulty%20of%20effectively%20adapting%20pretrained%20language%20models%20for%20visual%20synthesis.%20To%20address%20these%20issues%2C%20we%20introduce%20GRAN-TED%2C%20a%20paradigm%20to%20Generate%20Robust%2C%20Aligned%2C%20and%20Nuanced%20Text%20Embeddings%20for%20Diffusion%20models.%20Our%20contribution%20is%20twofold.%20First%2C%20we%20propose%20TED-6K%2C%20a%20novel%20text-only%20benchmark%20that%20enables%20efficient%20and%20robust%20assessment%20of%20an%20encoder%27s%20representational%20quality%20without%20requiring%20costly%20end-to-end%20model%20training.%20We%20demonstrate%20that%20performance%20on%20TED-6K%2C%20standardized%20via%20a%20lightweight%2C%20unified%20adapter%2C%20strongly%20correlates%20with%20an%20encoder%27s%20effectiveness%20in%20downstream%20generation%20tasks.%20Second%2C%20guided%20by%20this%20validated%20framework%2C%20we%20develop%20a%20superior%20text%20encoder%20using%20a%20novel%20two-stage%20training%20paradigm.%20This%20process%20involves%20an%20initial%20fine-tuning%20stage%20on%20a%20Multimodal%20Large%20Language%20Model%20for%20better%20visual%20representation%2C%20followed%20by%20a%20layer-wise%20weighting%20method%20to%20extract%20more%20nuanced%20and%20potent%20text%20features.%20Our%20experiments%20show%20that%20the%20resulting%20GRAN-TED%20encoder%20not%20only%20achieves%20state-of-the-art%20performance%20on%20TED-6K%20but%20also%20leads%20to%20demonstrable%20performance%20gains%20in%20text-to-image%20and%20text-to-video%20generation.%20Our%20code%20is%20available%20at%20the%20following%20link%3A%20https%3A//anonymous.4open.science/r/GRAN-TED-4FCC/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAN-TED%253A%2520Generating%2520Robust%252C%2520Aligned%252C%2520and%2520Nuanced%2520Text%2520Embedding%2520for%2520Diffusion%2520Models%26entry.906535625%3DBozhou%2520Li%2520and%2520Sihan%2520Yang%2520and%2520Yushuo%2520Guan%2520and%2520Ruichuan%2520An%2520and%2520Xinlong%2520Chen%2520and%2520Yang%2520Shi%2520and%2520Pengfei%2520Wan%2520and%2520Wentao%2520Zhang%2520and%2520Yuanxing%2520zhang%26entry.1292438233%3DThe%2520text%2520encoder%2520is%2520a%2520critical%2520component%2520of%2520text-to-image%2520and%2520text-to-video%2520diffusion%2520models%252C%2520fundamentally%2520determining%2520the%2520semantic%2520fidelity%2520of%2520the%2520generated%2520content.%2520However%252C%2520its%2520development%2520has%2520been%2520hindered%2520by%2520two%2520major%2520challenges%253A%2520the%2520lack%2520of%2520an%2520efficient%2520evaluation%2520framework%2520that%2520reliably%2520predicts%2520downstream%2520generation%2520performance%252C%2520and%2520the%2520difficulty%2520of%2520effectively%2520adapting%2520pretrained%2520language%2520models%2520for%2520visual%2520synthesis.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520GRAN-TED%252C%2520a%2520paradigm%2520to%2520Generate%2520Robust%252C%2520Aligned%252C%2520and%2520Nuanced%2520Text%2520Embeddings%2520for%2520Diffusion%2520models.%2520Our%2520contribution%2520is%2520twofold.%2520First%252C%2520we%2520propose%2520TED-6K%252C%2520a%2520novel%2520text-only%2520benchmark%2520that%2520enables%2520efficient%2520and%2520robust%2520assessment%2520of%2520an%2520encoder%2527s%2520representational%2520quality%2520without%2520requiring%2520costly%2520end-to-end%2520model%2520training.%2520We%2520demonstrate%2520that%2520performance%2520on%2520TED-6K%252C%2520standardized%2520via%2520a%2520lightweight%252C%2520unified%2520adapter%252C%2520strongly%2520correlates%2520with%2520an%2520encoder%2527s%2520effectiveness%2520in%2520downstream%2520generation%2520tasks.%2520Second%252C%2520guided%2520by%2520this%2520validated%2520framework%252C%2520we%2520develop%2520a%2520superior%2520text%2520encoder%2520using%2520a%2520novel%2520two-stage%2520training%2520paradigm.%2520This%2520process%2520involves%2520an%2520initial%2520fine-tuning%2520stage%2520on%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520better%2520visual%2520representation%252C%2520followed%2520by%2520a%2520layer-wise%2520weighting%2520method%2520to%2520extract%2520more%2520nuanced%2520and%2520potent%2520text%2520features.%2520Our%2520experiments%2520show%2520that%2520the%2520resulting%2520GRAN-TED%2520encoder%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520on%2520TED-6K%2520but%2520also%2520leads%2520to%2520demonstrable%2520performance%2520gains%2520in%2520text-to-image%2520and%2520text-to-video%2520generation.%2520Our%2520code%2520is%2520available%2520at%2520the%2520following%2520link%253A%2520https%253A//anonymous.4open.science/r/GRAN-TED-4FCC/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAN-TED%3A%20Generating%20Robust%2C%20Aligned%2C%20and%20Nuanced%20Text%20Embedding%20for%20Diffusion%20Models&entry.906535625=Bozhou%20Li%20and%20Sihan%20Yang%20and%20Yushuo%20Guan%20and%20Ruichuan%20An%20and%20Xinlong%20Chen%20and%20Yang%20Shi%20and%20Pengfei%20Wan%20and%20Wentao%20Zhang%20and%20Yuanxing%20zhang&entry.1292438233=The%20text%20encoder%20is%20a%20critical%20component%20of%20text-to-image%20and%20text-to-video%20diffusion%20models%2C%20fundamentally%20determining%20the%20semantic%20fidelity%20of%20the%20generated%20content.%20However%2C%20its%20development%20has%20been%20hindered%20by%20two%20major%20challenges%3A%20the%20lack%20of%20an%20efficient%20evaluation%20framework%20that%20reliably%20predicts%20downstream%20generation%20performance%2C%20and%20the%20difficulty%20of%20effectively%20adapting%20pretrained%20language%20models%20for%20visual%20synthesis.%20To%20address%20these%20issues%2C%20we%20introduce%20GRAN-TED%2C%20a%20paradigm%20to%20Generate%20Robust%2C%20Aligned%2C%20and%20Nuanced%20Text%20Embeddings%20for%20Diffusion%20models.%20Our%20contribution%20is%20twofold.%20First%2C%20we%20propose%20TED-6K%2C%20a%20novel%20text-only%20benchmark%20that%20enables%20efficient%20and%20robust%20assessment%20of%20an%20encoder%27s%20representational%20quality%20without%20requiring%20costly%20end-to-end%20model%20training.%20We%20demonstrate%20that%20performance%20on%20TED-6K%2C%20standardized%20via%20a%20lightweight%2C%20unified%20adapter%2C%20strongly%20correlates%20with%20an%20encoder%27s%20effectiveness%20in%20downstream%20generation%20tasks.%20Second%2C%20guided%20by%20this%20validated%20framework%2C%20we%20develop%20a%20superior%20text%20encoder%20using%20a%20novel%20two-stage%20training%20paradigm.%20This%20process%20involves%20an%20initial%20fine-tuning%20stage%20on%20a%20Multimodal%20Large%20Language%20Model%20for%20better%20visual%20representation%2C%20followed%20by%20a%20layer-wise%20weighting%20method%20to%20extract%20more%20nuanced%20and%20potent%20text%20features.%20Our%20experiments%20show%20that%20the%20resulting%20GRAN-TED%20encoder%20not%20only%20achieves%20state-of-the-art%20performance%20on%20TED-6K%20but%20also%20leads%20to%20demonstrable%20performance%20gains%20in%20text-to-image%20and%20text-to-video%20generation.%20Our%20code%20is%20available%20at%20the%20following%20link%3A%20https%3A//anonymous.4open.science/r/GRAN-TED-4FCC/.&entry.1838667208=http%3A//arxiv.org/abs/2512.15560v1&entry.124074799=Read"},
{"title": "Scale-invariant Attention", "author": "Ben Anson and Xi Wang and Laurence Aitchison", "abstract": "One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.", "link": "http://arxiv.org/abs/2505.17083v2", "date": "2025-12-17", "relevancy": 2.4252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4945}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4936}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-invariant%20Attention&body=Title%3A%20Scale-invariant%20Attention%0AAuthor%3A%20Ben%20Anson%20and%20Xi%20Wang%20and%20Laurence%20Aitchison%0AAbstract%3A%20One%20persistent%20challenge%20in%20LLM%20research%20is%20the%20development%20of%20attention%20mechanisms%20that%20are%20able%20to%20generalise%20from%20training%20on%20shorter%20contexts%20to%20inference%20on%20longer%20contexts.%20We%20propose%20two%20conditions%20that%20we%20expect%20all%20effective%20long%20context%20attention%20mechanisms%20to%20have%3A%20scale-invariant%20total%20attention%2C%20and%20scale-invariant%20attention%20sparsity.%20Under%20a%20Gaussian%20assumption%2C%20we%20show%20that%20a%20simple%20position-dependent%20transformation%20of%20the%20attention%20logits%20is%20sufficient%20for%20these%20conditions%20to%20hold.%20Experimentally%20we%20find%20that%20the%20resulting%20scale-invariant%20attention%20scheme%20gives%20considerable%20benefits%20in%20terms%20of%20validation%20loss%20when%20zero-shot%20generalising%20from%20training%20on%20short%20contexts%20to%20validation%20on%20longer%20contexts%2C%20and%20is%20effective%20at%20long-context%20retrieval.%0ALink%3A%20http%3A//arxiv.org/abs/2505.17083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-invariant%2520Attention%26entry.906535625%3DBen%2520Anson%2520and%2520Xi%2520Wang%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3DOne%2520persistent%2520challenge%2520in%2520LLM%2520research%2520is%2520the%2520development%2520of%2520attention%2520mechanisms%2520that%2520are%2520able%2520to%2520generalise%2520from%2520training%2520on%2520shorter%2520contexts%2520to%2520inference%2520on%2520longer%2520contexts.%2520We%2520propose%2520two%2520conditions%2520that%2520we%2520expect%2520all%2520effective%2520long%2520context%2520attention%2520mechanisms%2520to%2520have%253A%2520scale-invariant%2520total%2520attention%252C%2520and%2520scale-invariant%2520attention%2520sparsity.%2520Under%2520a%2520Gaussian%2520assumption%252C%2520we%2520show%2520that%2520a%2520simple%2520position-dependent%2520transformation%2520of%2520the%2520attention%2520logits%2520is%2520sufficient%2520for%2520these%2520conditions%2520to%2520hold.%2520Experimentally%2520we%2520find%2520that%2520the%2520resulting%2520scale-invariant%2520attention%2520scheme%2520gives%2520considerable%2520benefits%2520in%2520terms%2520of%2520validation%2520loss%2520when%2520zero-shot%2520generalising%2520from%2520training%2520on%2520short%2520contexts%2520to%2520validation%2520on%2520longer%2520contexts%252C%2520and%2520is%2520effective%2520at%2520long-context%2520retrieval.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-invariant%20Attention&entry.906535625=Ben%20Anson%20and%20Xi%20Wang%20and%20Laurence%20Aitchison&entry.1292438233=One%20persistent%20challenge%20in%20LLM%20research%20is%20the%20development%20of%20attention%20mechanisms%20that%20are%20able%20to%20generalise%20from%20training%20on%20shorter%20contexts%20to%20inference%20on%20longer%20contexts.%20We%20propose%20two%20conditions%20that%20we%20expect%20all%20effective%20long%20context%20attention%20mechanisms%20to%20have%3A%20scale-invariant%20total%20attention%2C%20and%20scale-invariant%20attention%20sparsity.%20Under%20a%20Gaussian%20assumption%2C%20we%20show%20that%20a%20simple%20position-dependent%20transformation%20of%20the%20attention%20logits%20is%20sufficient%20for%20these%20conditions%20to%20hold.%20Experimentally%20we%20find%20that%20the%20resulting%20scale-invariant%20attention%20scheme%20gives%20considerable%20benefits%20in%20terms%20of%20validation%20loss%20when%20zero-shot%20generalising%20from%20training%20on%20short%20contexts%20to%20validation%20on%20longer%20contexts%2C%20and%20is%20effective%20at%20long-context%20retrieval.&entry.1838667208=http%3A//arxiv.org/abs/2505.17083v2&entry.124074799=Read"},
{"title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I", "author": "Seok-Hyun Ga and Chun-Yen Chang", "abstract": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.", "link": "http://arxiv.org/abs/2512.15298v1", "date": "2025-12-17", "relevancy": 2.414, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4934}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4802}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20and%20Gemini%20participated%20in%20the%20Korean%20College%20Scholastic%20Ability%20Test%20--%20Earth%20Science%20I&body=Title%3A%20ChatGPT%20and%20Gemini%20participated%20in%20the%20Korean%20College%20Scholastic%20Ability%20Test%20--%20Earth%20Science%20I%0AAuthor%3A%20Seok-Hyun%20Ga%20and%20Chun-Yen%20Chang%0AAbstract%3A%20The%20rapid%20development%20of%20Generative%20AI%20is%20bringing%20innovative%20changes%20to%20education%20and%20assessment.%20As%20the%20prevalence%20of%20students%20utilizing%20AI%20for%20assignments%20increases%2C%20concerns%20regarding%20academic%20integrity%20and%20the%20validity%20of%20assessments%20are%20growing.%20This%20study%20utilizes%20the%20Earth%20Science%20I%20section%20of%20the%202025%20Korean%20College%20Scholastic%20Ability%20Test%20%28CSAT%29%20to%20deeply%20analyze%20the%20multimodal%20scientific%20reasoning%20capabilities%20and%20cognitive%20limitations%20of%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%2C%20including%20GPT-4o%2C%20Gemini%202.5%20Flash%2C%20and%20Gemini%202.5%20Pro.%20Three%20experimental%20conditions%20%28full-page%20input%2C%20individual%20item%20input%2C%20and%20optimized%20multimodal%20input%29%20were%20designed%20to%20evaluate%20model%20performance%20across%20different%20data%20structures.%20Quantitative%20results%20indicated%20that%20unstructured%20inputs%20led%20to%20significant%20performance%20degradation%20due%20to%20segmentation%20and%20Optical%20Character%20Recognition%20%28OCR%29%20failures.%20Even%20under%20optimized%20conditions%2C%20models%20exhibited%20fundamental%20reasoning%20flaws.%20Qualitative%20analysis%20revealed%20that%20%22Perception%20Errors%22%20were%20dominant%2C%20highlighting%20a%20%22Perception-Cognition%20Gap%22%20where%20models%20failed%20to%20interpret%20symbolic%20meanings%20in%20schematic%20diagrams%20despite%20recognizing%20visual%20data.%20Furthermore%2C%20models%20demonstrated%20a%20%22Calculation-Conceptualization%20Discrepancy%2C%22%20successfully%20performing%20calculations%20while%20failing%20to%20apply%20the%20underlying%20scientific%20concepts%2C%20and%20%22Process%20Hallucination%2C%22%20where%20models%20skipped%20visual%20verification%20in%20favor%20of%20plausible%20but%20unfounded%20background%20knowledge.%20Addressing%20the%20challenge%20of%20unauthorized%20AI%20use%20in%20coursework%2C%20this%20study%20provides%20actionable%20cues%20for%20designing%20%22AI-resistant%20questions%22%20that%20target%20these%20specific%20cognitive%20vulnerabilities.%20By%20exploiting%20AI%27s%20weaknesses%2C%20such%20as%20the%20gap%20between%20perception%20and%20cognition%2C%20educators%20can%20distinguish%20genuine%20student%20competency%20from%20AI-generated%20responses%2C%20thereby%20ensuring%20assessment%20fairness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGPT%2520and%2520Gemini%2520participated%2520in%2520the%2520Korean%2520College%2520Scholastic%2520Ability%2520Test%2520--%2520Earth%2520Science%2520I%26entry.906535625%3DSeok-Hyun%2520Ga%2520and%2520Chun-Yen%2520Chang%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520Generative%2520AI%2520is%2520bringing%2520innovative%2520changes%2520to%2520education%2520and%2520assessment.%2520As%2520the%2520prevalence%2520of%2520students%2520utilizing%2520AI%2520for%2520assignments%2520increases%252C%2520concerns%2520regarding%2520academic%2520integrity%2520and%2520the%2520validity%2520of%2520assessments%2520are%2520growing.%2520This%2520study%2520utilizes%2520the%2520Earth%2520Science%2520I%2520section%2520of%2520the%25202025%2520Korean%2520College%2520Scholastic%2520Ability%2520Test%2520%2528CSAT%2529%2520to%2520deeply%2520analyze%2520the%2520multimodal%2520scientific%2520reasoning%2520capabilities%2520and%2520cognitive%2520limitations%2520of%2520state-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520including%2520GPT-4o%252C%2520Gemini%25202.5%2520Flash%252C%2520and%2520Gemini%25202.5%2520Pro.%2520Three%2520experimental%2520conditions%2520%2528full-page%2520input%252C%2520individual%2520item%2520input%252C%2520and%2520optimized%2520multimodal%2520input%2529%2520were%2520designed%2520to%2520evaluate%2520model%2520performance%2520across%2520different%2520data%2520structures.%2520Quantitative%2520results%2520indicated%2520that%2520unstructured%2520inputs%2520led%2520to%2520significant%2520performance%2520degradation%2520due%2520to%2520segmentation%2520and%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520failures.%2520Even%2520under%2520optimized%2520conditions%252C%2520models%2520exhibited%2520fundamental%2520reasoning%2520flaws.%2520Qualitative%2520analysis%2520revealed%2520that%2520%2522Perception%2520Errors%2522%2520were%2520dominant%252C%2520highlighting%2520a%2520%2522Perception-Cognition%2520Gap%2522%2520where%2520models%2520failed%2520to%2520interpret%2520symbolic%2520meanings%2520in%2520schematic%2520diagrams%2520despite%2520recognizing%2520visual%2520data.%2520Furthermore%252C%2520models%2520demonstrated%2520a%2520%2522Calculation-Conceptualization%2520Discrepancy%252C%2522%2520successfully%2520performing%2520calculations%2520while%2520failing%2520to%2520apply%2520the%2520underlying%2520scientific%2520concepts%252C%2520and%2520%2522Process%2520Hallucination%252C%2522%2520where%2520models%2520skipped%2520visual%2520verification%2520in%2520favor%2520of%2520plausible%2520but%2520unfounded%2520background%2520knowledge.%2520Addressing%2520the%2520challenge%2520of%2520unauthorized%2520AI%2520use%2520in%2520coursework%252C%2520this%2520study%2520provides%2520actionable%2520cues%2520for%2520designing%2520%2522AI-resistant%2520questions%2522%2520that%2520target%2520these%2520specific%2520cognitive%2520vulnerabilities.%2520By%2520exploiting%2520AI%2527s%2520weaknesses%252C%2520such%2520as%2520the%2520gap%2520between%2520perception%2520and%2520cognition%252C%2520educators%2520can%2520distinguish%2520genuine%2520student%2520competency%2520from%2520AI-generated%2520responses%252C%2520thereby%2520ensuring%2520assessment%2520fairness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20and%20Gemini%20participated%20in%20the%20Korean%20College%20Scholastic%20Ability%20Test%20--%20Earth%20Science%20I&entry.906535625=Seok-Hyun%20Ga%20and%20Chun-Yen%20Chang&entry.1292438233=The%20rapid%20development%20of%20Generative%20AI%20is%20bringing%20innovative%20changes%20to%20education%20and%20assessment.%20As%20the%20prevalence%20of%20students%20utilizing%20AI%20for%20assignments%20increases%2C%20concerns%20regarding%20academic%20integrity%20and%20the%20validity%20of%20assessments%20are%20growing.%20This%20study%20utilizes%20the%20Earth%20Science%20I%20section%20of%20the%202025%20Korean%20College%20Scholastic%20Ability%20Test%20%28CSAT%29%20to%20deeply%20analyze%20the%20multimodal%20scientific%20reasoning%20capabilities%20and%20cognitive%20limitations%20of%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%2C%20including%20GPT-4o%2C%20Gemini%202.5%20Flash%2C%20and%20Gemini%202.5%20Pro.%20Three%20experimental%20conditions%20%28full-page%20input%2C%20individual%20item%20input%2C%20and%20optimized%20multimodal%20input%29%20were%20designed%20to%20evaluate%20model%20performance%20across%20different%20data%20structures.%20Quantitative%20results%20indicated%20that%20unstructured%20inputs%20led%20to%20significant%20performance%20degradation%20due%20to%20segmentation%20and%20Optical%20Character%20Recognition%20%28OCR%29%20failures.%20Even%20under%20optimized%20conditions%2C%20models%20exhibited%20fundamental%20reasoning%20flaws.%20Qualitative%20analysis%20revealed%20that%20%22Perception%20Errors%22%20were%20dominant%2C%20highlighting%20a%20%22Perception-Cognition%20Gap%22%20where%20models%20failed%20to%20interpret%20symbolic%20meanings%20in%20schematic%20diagrams%20despite%20recognizing%20visual%20data.%20Furthermore%2C%20models%20demonstrated%20a%20%22Calculation-Conceptualization%20Discrepancy%2C%22%20successfully%20performing%20calculations%20while%20failing%20to%20apply%20the%20underlying%20scientific%20concepts%2C%20and%20%22Process%20Hallucination%2C%22%20where%20models%20skipped%20visual%20verification%20in%20favor%20of%20plausible%20but%20unfounded%20background%20knowledge.%20Addressing%20the%20challenge%20of%20unauthorized%20AI%20use%20in%20coursework%2C%20this%20study%20provides%20actionable%20cues%20for%20designing%20%22AI-resistant%20questions%22%20that%20target%20these%20specific%20cognitive%20vulnerabilities.%20By%20exploiting%20AI%27s%20weaknesses%2C%20such%20as%20the%20gap%20between%20perception%20and%20cognition%2C%20educators%20can%20distinguish%20genuine%20student%20competency%20from%20AI-generated%20responses%2C%20thereby%20ensuring%20assessment%20fairness.&entry.1838667208=http%3A//arxiv.org/abs/2512.15298v1&entry.124074799=Read"},
{"title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning", "author": "Yuanhang Li and Yiren Song and Junzhe Bai and Xinran Liang and Hu Yang and Libiao Jin and Qi Mao", "abstract": "We propose \\textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.", "link": "http://arxiv.org/abs/2512.15635v1", "date": "2025-12-17", "relevancy": 2.4136, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6286}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6083}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IC-Effect%3A%20Precise%20and%20Efficient%20Video%20Effects%20Editing%20via%20In-Context%20Learning&body=Title%3A%20IC-Effect%3A%20Precise%20and%20Efficient%20Video%20Effects%20Editing%20via%20In-Context%20Learning%0AAuthor%3A%20Yuanhang%20Li%20and%20Yiren%20Song%20and%20Junzhe%20Bai%20and%20Xinran%20Liang%20and%20Hu%20Yang%20and%20Libiao%20Jin%20and%20Qi%20Mao%0AAbstract%3A%20We%20propose%20%5Ctextbf%7BIC-Effect%7D%2C%20an%20instruction-guided%2C%20DiT-based%20framework%20for%20few-shot%20video%20VFX%20editing%20that%20synthesizes%20complex%20effects%20%28%5Ceg%20flames%2C%20particles%20and%20cartoon%20characters%29%20while%20strictly%20preserving%20spatial%20and%20temporal%20consistency.%20Video%20VFX%20editing%20is%20highly%20challenging%20because%20injected%20effects%20must%20blend%20seamlessly%20with%20the%20background%2C%20the%20background%20must%20remain%20entirely%20unchanged%2C%20and%20effect%20patterns%20must%20be%20learned%20efficiently%20from%20limited%20paired%20data.%20However%2C%20existing%20video%20editing%20models%20fail%20to%20satisfy%20these%20requirements.%20IC-Effect%20leverages%20the%20source%20video%20as%20clean%20contextual%20conditions%2C%20exploiting%20the%20contextual%20learning%20capability%20of%20DiT%20models%20to%20achieve%20precise%20background%20preservation%20and%20natural%20effect%20injection.%20A%20two-stage%20training%20strategy%2C%20consisting%20of%20general%20editing%20adaptation%20followed%20by%20effect-specific%20learning%20via%20Effect-LoRA%2C%20ensures%20strong%20instruction%20following%20and%20robust%20effect%20modeling.%20To%20further%20improve%20efficiency%2C%20we%20introduce%20spatiotemporal%20sparse%20tokenization%2C%20enabling%20high%20fidelity%20with%20substantially%20reduced%20computation.%20We%20also%20release%20a%20paired%20VFX%20editing%20dataset%20spanning%20%2415%24%20high-quality%20visual%20styles.%20Extensive%20experiments%20show%20that%20IC-Effect%20delivers%20high-quality%2C%20controllable%2C%20and%20temporally%20consistent%20VFX%20editing%2C%20opening%20new%20possibilities%20for%20video%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIC-Effect%253A%2520Precise%2520and%2520Efficient%2520Video%2520Effects%2520Editing%2520via%2520In-Context%2520Learning%26entry.906535625%3DYuanhang%2520Li%2520and%2520Yiren%2520Song%2520and%2520Junzhe%2520Bai%2520and%2520Xinran%2520Liang%2520and%2520Hu%2520Yang%2520and%2520Libiao%2520Jin%2520and%2520Qi%2520Mao%26entry.1292438233%3DWe%2520propose%2520%255Ctextbf%257BIC-Effect%257D%252C%2520an%2520instruction-guided%252C%2520DiT-based%2520framework%2520for%2520few-shot%2520video%2520VFX%2520editing%2520that%2520synthesizes%2520complex%2520effects%2520%2528%255Ceg%2520flames%252C%2520particles%2520and%2520cartoon%2520characters%2529%2520while%2520strictly%2520preserving%2520spatial%2520and%2520temporal%2520consistency.%2520Video%2520VFX%2520editing%2520is%2520highly%2520challenging%2520because%2520injected%2520effects%2520must%2520blend%2520seamlessly%2520with%2520the%2520background%252C%2520the%2520background%2520must%2520remain%2520entirely%2520unchanged%252C%2520and%2520effect%2520patterns%2520must%2520be%2520learned%2520efficiently%2520from%2520limited%2520paired%2520data.%2520However%252C%2520existing%2520video%2520editing%2520models%2520fail%2520to%2520satisfy%2520these%2520requirements.%2520IC-Effect%2520leverages%2520the%2520source%2520video%2520as%2520clean%2520contextual%2520conditions%252C%2520exploiting%2520the%2520contextual%2520learning%2520capability%2520of%2520DiT%2520models%2520to%2520achieve%2520precise%2520background%2520preservation%2520and%2520natural%2520effect%2520injection.%2520A%2520two-stage%2520training%2520strategy%252C%2520consisting%2520of%2520general%2520editing%2520adaptation%2520followed%2520by%2520effect-specific%2520learning%2520via%2520Effect-LoRA%252C%2520ensures%2520strong%2520instruction%2520following%2520and%2520robust%2520effect%2520modeling.%2520To%2520further%2520improve%2520efficiency%252C%2520we%2520introduce%2520spatiotemporal%2520sparse%2520tokenization%252C%2520enabling%2520high%2520fidelity%2520with%2520substantially%2520reduced%2520computation.%2520We%2520also%2520release%2520a%2520paired%2520VFX%2520editing%2520dataset%2520spanning%2520%252415%2524%2520high-quality%2520visual%2520styles.%2520Extensive%2520experiments%2520show%2520that%2520IC-Effect%2520delivers%2520high-quality%252C%2520controllable%252C%2520and%2520temporally%2520consistent%2520VFX%2520editing%252C%2520opening%2520new%2520possibilities%2520for%2520video%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IC-Effect%3A%20Precise%20and%20Efficient%20Video%20Effects%20Editing%20via%20In-Context%20Learning&entry.906535625=Yuanhang%20Li%20and%20Yiren%20Song%20and%20Junzhe%20Bai%20and%20Xinran%20Liang%20and%20Hu%20Yang%20and%20Libiao%20Jin%20and%20Qi%20Mao&entry.1292438233=We%20propose%20%5Ctextbf%7BIC-Effect%7D%2C%20an%20instruction-guided%2C%20DiT-based%20framework%20for%20few-shot%20video%20VFX%20editing%20that%20synthesizes%20complex%20effects%20%28%5Ceg%20flames%2C%20particles%20and%20cartoon%20characters%29%20while%20strictly%20preserving%20spatial%20and%20temporal%20consistency.%20Video%20VFX%20editing%20is%20highly%20challenging%20because%20injected%20effects%20must%20blend%20seamlessly%20with%20the%20background%2C%20the%20background%20must%20remain%20entirely%20unchanged%2C%20and%20effect%20patterns%20must%20be%20learned%20efficiently%20from%20limited%20paired%20data.%20However%2C%20existing%20video%20editing%20models%20fail%20to%20satisfy%20these%20requirements.%20IC-Effect%20leverages%20the%20source%20video%20as%20clean%20contextual%20conditions%2C%20exploiting%20the%20contextual%20learning%20capability%20of%20DiT%20models%20to%20achieve%20precise%20background%20preservation%20and%20natural%20effect%20injection.%20A%20two-stage%20training%20strategy%2C%20consisting%20of%20general%20editing%20adaptation%20followed%20by%20effect-specific%20learning%20via%20Effect-LoRA%2C%20ensures%20strong%20instruction%20following%20and%20robust%20effect%20modeling.%20To%20further%20improve%20efficiency%2C%20we%20introduce%20spatiotemporal%20sparse%20tokenization%2C%20enabling%20high%20fidelity%20with%20substantially%20reduced%20computation.%20We%20also%20release%20a%20paired%20VFX%20editing%20dataset%20spanning%20%2415%24%20high-quality%20visual%20styles.%20Extensive%20experiments%20show%20that%20IC-Effect%20delivers%20high-quality%2C%20controllable%2C%20and%20temporally%20consistent%20VFX%20editing%2C%20opening%20new%20possibilities%20for%20video%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2512.15635v1&entry.124074799=Read"},
{"title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning", "author": "Yiliu Sun and Zicheng Zhao and Yang Wei and Yanfang Zhang and Chen Gong", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.", "link": "http://arxiv.org/abs/2512.15274v1", "date": "2025-12-17", "relevancy": 2.4109, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Well%20Begun%2C%20Half%20Done%3A%20Reinforcement%20Learning%20with%20Prefix%20Optimization%20for%20LLM%20Reasoning&body=Title%3A%20Well%20Begun%2C%20Half%20Done%3A%20Reinforcement%20Learning%20with%20Prefix%20Optimization%20for%20LLM%20Reasoning%0AAuthor%3A%20Yiliu%20Sun%20and%20Zicheng%20Zhao%20and%20Yang%20Wei%20and%20Yanfang%20Zhang%20and%20Chen%20Gong%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20significantly%20enhances%20the%20reasoning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29.%20Current%20RLVR%20approaches%20typically%20conduct%20training%20across%20all%20generated%20tokens%2C%20but%20neglect%20to%20explore%20which%20tokens%20%28e.g.%2C%20prefix%20tokens%29%20actually%20contribute%20to%20reasoning.%20This%20uniform%20training%20strategy%20spends%20substantial%20effort%20on%20optimizing%20low-return%20tokens%2C%20which%20in%20turn%20impedes%20the%20potential%20improvement%20from%20high-return%20tokens%20and%20reduces%20overall%20training%20effectiveness.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20RLVR%20approach%20called%20Progressive%20Prefix-token%20Policy%20Optimization%20%28PPPO%29%2C%20which%20highlights%20the%20significance%20of%20the%20prefix%20segment%20of%20generated%20outputs.%20Specifically%2C%20inspired%20by%20the%20well-established%20human%20thinking%20theory%20of%20Path%20Dependence%2C%20where%20early-stage%20thoughts%20substantially%20constrain%20subsequent%20thinking%20trajectory%2C%20we%20identify%20an%20analogous%20phenomenon%20in%20LLM%20reasoning%20termed%20Beginning%20Lock-in%20Effect%20%28BLE%29.%20PPPO%20leverages%20this%20finding%20by%20focusing%20its%20optimization%20objective%20on%20the%20prefix%20reasoning%20process%20of%20LLMs.%20This%20targeted%20optimization%20strategy%20can%20positively%20influence%20subsequent%20reasoning%20processes%2C%20and%20ultimately%20improve%20final%20results.%20To%20improve%20the%20learning%20effectiveness%20of%20LLMs%20on%20how%20to%20start%20reasoning%20with%20high%20quality%2C%20PPPO%20introduces%20two%20training%20strategies%3A%20%28a%29%20Progressive%20Prefix%20Retention%2C%20which%20shapes%20a%20progressive%20learning%20process%20by%20increasing%20the%20proportion%20of%20retained%20prefix%20tokens%20during%20training%3B%20%28b%29%20Continuation%20Accumulated%20Reward%2C%20which%20mitigates%20reward%20bias%20by%20sampling%20multiple%20continuations%20for%20one%20prefix%20token%20sequence%2C%20and%20accumulating%20their%20scores%20as%20the%20reward%20signal.%20Extensive%20experimental%20results%20on%20various%20reasoning%20tasks%20demonstrate%20that%20our%20proposed%20PPPO%20outperforms%20representative%20RLVR%20methods%2C%20with%20the%20accuracy%20improvements%20of%2018.02%25%20on%20only%2026.17%25%20training%20tokens.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWell%2520Begun%252C%2520Half%2520Done%253A%2520Reinforcement%2520Learning%2520with%2520Prefix%2520Optimization%2520for%2520LLM%2520Reasoning%26entry.906535625%3DYiliu%2520Sun%2520and%2520Zicheng%2520Zhao%2520and%2520Yang%2520Wei%2520and%2520Yanfang%2520Zhang%2520and%2520Chen%2520Gong%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520significantly%2520enhances%2520the%2520reasoning%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Current%2520RLVR%2520approaches%2520typically%2520conduct%2520training%2520across%2520all%2520generated%2520tokens%252C%2520but%2520neglect%2520to%2520explore%2520which%2520tokens%2520%2528e.g.%252C%2520prefix%2520tokens%2529%2520actually%2520contribute%2520to%2520reasoning.%2520This%2520uniform%2520training%2520strategy%2520spends%2520substantial%2520effort%2520on%2520optimizing%2520low-return%2520tokens%252C%2520which%2520in%2520turn%2520impedes%2520the%2520potential%2520improvement%2520from%2520high-return%2520tokens%2520and%2520reduces%2520overall%2520training%2520effectiveness.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520RLVR%2520approach%2520called%2520Progressive%2520Prefix-token%2520Policy%2520Optimization%2520%2528PPPO%2529%252C%2520which%2520highlights%2520the%2520significance%2520of%2520the%2520prefix%2520segment%2520of%2520generated%2520outputs.%2520Specifically%252C%2520inspired%2520by%2520the%2520well-established%2520human%2520thinking%2520theory%2520of%2520Path%2520Dependence%252C%2520where%2520early-stage%2520thoughts%2520substantially%2520constrain%2520subsequent%2520thinking%2520trajectory%252C%2520we%2520identify%2520an%2520analogous%2520phenomenon%2520in%2520LLM%2520reasoning%2520termed%2520Beginning%2520Lock-in%2520Effect%2520%2528BLE%2529.%2520PPPO%2520leverages%2520this%2520finding%2520by%2520focusing%2520its%2520optimization%2520objective%2520on%2520the%2520prefix%2520reasoning%2520process%2520of%2520LLMs.%2520This%2520targeted%2520optimization%2520strategy%2520can%2520positively%2520influence%2520subsequent%2520reasoning%2520processes%252C%2520and%2520ultimately%2520improve%2520final%2520results.%2520To%2520improve%2520the%2520learning%2520effectiveness%2520of%2520LLMs%2520on%2520how%2520to%2520start%2520reasoning%2520with%2520high%2520quality%252C%2520PPPO%2520introduces%2520two%2520training%2520strategies%253A%2520%2528a%2529%2520Progressive%2520Prefix%2520Retention%252C%2520which%2520shapes%2520a%2520progressive%2520learning%2520process%2520by%2520increasing%2520the%2520proportion%2520of%2520retained%2520prefix%2520tokens%2520during%2520training%253B%2520%2528b%2529%2520Continuation%2520Accumulated%2520Reward%252C%2520which%2520mitigates%2520reward%2520bias%2520by%2520sampling%2520multiple%2520continuations%2520for%2520one%2520prefix%2520token%2520sequence%252C%2520and%2520accumulating%2520their%2520scores%2520as%2520the%2520reward%2520signal.%2520Extensive%2520experimental%2520results%2520on%2520various%2520reasoning%2520tasks%2520demonstrate%2520that%2520our%2520proposed%2520PPPO%2520outperforms%2520representative%2520RLVR%2520methods%252C%2520with%2520the%2520accuracy%2520improvements%2520of%252018.02%2525%2520on%2520only%252026.17%2525%2520training%2520tokens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Well%20Begun%2C%20Half%20Done%3A%20Reinforcement%20Learning%20with%20Prefix%20Optimization%20for%20LLM%20Reasoning&entry.906535625=Yiliu%20Sun%20and%20Zicheng%20Zhao%20and%20Yang%20Wei%20and%20Yanfang%20Zhang%20and%20Chen%20Gong&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20significantly%20enhances%20the%20reasoning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29.%20Current%20RLVR%20approaches%20typically%20conduct%20training%20across%20all%20generated%20tokens%2C%20but%20neglect%20to%20explore%20which%20tokens%20%28e.g.%2C%20prefix%20tokens%29%20actually%20contribute%20to%20reasoning.%20This%20uniform%20training%20strategy%20spends%20substantial%20effort%20on%20optimizing%20low-return%20tokens%2C%20which%20in%20turn%20impedes%20the%20potential%20improvement%20from%20high-return%20tokens%20and%20reduces%20overall%20training%20effectiveness.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20RLVR%20approach%20called%20Progressive%20Prefix-token%20Policy%20Optimization%20%28PPPO%29%2C%20which%20highlights%20the%20significance%20of%20the%20prefix%20segment%20of%20generated%20outputs.%20Specifically%2C%20inspired%20by%20the%20well-established%20human%20thinking%20theory%20of%20Path%20Dependence%2C%20where%20early-stage%20thoughts%20substantially%20constrain%20subsequent%20thinking%20trajectory%2C%20we%20identify%20an%20analogous%20phenomenon%20in%20LLM%20reasoning%20termed%20Beginning%20Lock-in%20Effect%20%28BLE%29.%20PPPO%20leverages%20this%20finding%20by%20focusing%20its%20optimization%20objective%20on%20the%20prefix%20reasoning%20process%20of%20LLMs.%20This%20targeted%20optimization%20strategy%20can%20positively%20influence%20subsequent%20reasoning%20processes%2C%20and%20ultimately%20improve%20final%20results.%20To%20improve%20the%20learning%20effectiveness%20of%20LLMs%20on%20how%20to%20start%20reasoning%20with%20high%20quality%2C%20PPPO%20introduces%20two%20training%20strategies%3A%20%28a%29%20Progressive%20Prefix%20Retention%2C%20which%20shapes%20a%20progressive%20learning%20process%20by%20increasing%20the%20proportion%20of%20retained%20prefix%20tokens%20during%20training%3B%20%28b%29%20Continuation%20Accumulated%20Reward%2C%20which%20mitigates%20reward%20bias%20by%20sampling%20multiple%20continuations%20for%20one%20prefix%20token%20sequence%2C%20and%20accumulating%20their%20scores%20as%20the%20reward%20signal.%20Extensive%20experimental%20results%20on%20various%20reasoning%20tasks%20demonstrate%20that%20our%20proposed%20PPPO%20outperforms%20representative%20RLVR%20methods%2C%20with%20the%20accuracy%20improvements%20of%2018.02%25%20on%20only%2026.17%25%20training%20tokens.&entry.1838667208=http%3A//arxiv.org/abs/2512.15274v1&entry.124074799=Read"},
{"title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning", "author": "Jiaqi Xu and Cuiling Lan and Xuejin Chen and Yan LU", "abstract": "Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.", "link": "http://arxiv.org/abs/2512.15662v1", "date": "2025-12-17", "relevancy": 2.4059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepwise%20Think-Critique%3A%20A%20Unified%20Framework%20for%20Robust%20and%20Interpretable%20LLM%20Reasoning&body=Title%3A%20Stepwise%20Think-Critique%3A%20A%20Unified%20Framework%20for%20Robust%20and%20Interpretable%20LLM%20Reasoning%0AAuthor%3A%20Jiaqi%20Xu%20and%20Cuiling%20Lan%20and%20Xuejin%20Chen%20and%20Yan%20LU%0AAbstract%3A%20Human%20beings%20solve%20complex%20problems%20through%20critical%20thinking%2C%20where%20reasoning%20and%20evaluation%20are%20intertwined%20to%20converge%20toward%20correct%20solutions.%20However%2C%20most%20existing%20large%20language%20models%20%28LLMs%29%20decouple%20reasoning%20from%20verification%3A%20they%20either%20generate%20reasoning%20without%20explicit%20self-checking%20or%20rely%20on%20external%20verifiers%20to%20detect%20errors%20post%20hoc.%20The%20former%20lacks%20immediate%20feedback%2C%20while%20the%20latter%20increases%20system%20complexity%20and%20hinders%20synchronized%20learning.%20Motivated%20by%20human%20critical%20thinking%2C%20we%20propose%20Stepwise%20Think-Critique%20%28STC%29%2C%20a%20unified%20framework%20that%20interleaves%20reasoning%20and%20self-critique%20at%20each%20step%20within%20a%20single%20model.%20STC%20is%20trained%20with%20a%20hybrid%20reinforcement%20learning%20objective%20combining%20reasoning%20rewards%20and%20critique-consistency%20rewards%20to%20jointly%20optimize%20reasoning%20quality%20and%20self-evaluation.%20Experiments%20on%20mathematical%20reasoning%20benchmarks%20show%20that%20STC%20demonstrates%20strong%20critic-thinking%20capabilities%20and%20produces%20more%20interpretable%20reasoning%20traces%2C%20representing%20a%20step%20toward%20LLMs%20with%20built-in%20critical%20thinking.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepwise%2520Think-Critique%253A%2520A%2520Unified%2520Framework%2520for%2520Robust%2520and%2520Interpretable%2520LLM%2520Reasoning%26entry.906535625%3DJiaqi%2520Xu%2520and%2520Cuiling%2520Lan%2520and%2520Xuejin%2520Chen%2520and%2520Yan%2520LU%26entry.1292438233%3DHuman%2520beings%2520solve%2520complex%2520problems%2520through%2520critical%2520thinking%252C%2520where%2520reasoning%2520and%2520evaluation%2520are%2520intertwined%2520to%2520converge%2520toward%2520correct%2520solutions.%2520However%252C%2520most%2520existing%2520large%2520language%2520models%2520%2528LLMs%2529%2520decouple%2520reasoning%2520from%2520verification%253A%2520they%2520either%2520generate%2520reasoning%2520without%2520explicit%2520self-checking%2520or%2520rely%2520on%2520external%2520verifiers%2520to%2520detect%2520errors%2520post%2520hoc.%2520The%2520former%2520lacks%2520immediate%2520feedback%252C%2520while%2520the%2520latter%2520increases%2520system%2520complexity%2520and%2520hinders%2520synchronized%2520learning.%2520Motivated%2520by%2520human%2520critical%2520thinking%252C%2520we%2520propose%2520Stepwise%2520Think-Critique%2520%2528STC%2529%252C%2520a%2520unified%2520framework%2520that%2520interleaves%2520reasoning%2520and%2520self-critique%2520at%2520each%2520step%2520within%2520a%2520single%2520model.%2520STC%2520is%2520trained%2520with%2520a%2520hybrid%2520reinforcement%2520learning%2520objective%2520combining%2520reasoning%2520rewards%2520and%2520critique-consistency%2520rewards%2520to%2520jointly%2520optimize%2520reasoning%2520quality%2520and%2520self-evaluation.%2520Experiments%2520on%2520mathematical%2520reasoning%2520benchmarks%2520show%2520that%2520STC%2520demonstrates%2520strong%2520critic-thinking%2520capabilities%2520and%2520produces%2520more%2520interpretable%2520reasoning%2520traces%252C%2520representing%2520a%2520step%2520toward%2520LLMs%2520with%2520built-in%2520critical%2520thinking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepwise%20Think-Critique%3A%20A%20Unified%20Framework%20for%20Robust%20and%20Interpretable%20LLM%20Reasoning&entry.906535625=Jiaqi%20Xu%20and%20Cuiling%20Lan%20and%20Xuejin%20Chen%20and%20Yan%20LU&entry.1292438233=Human%20beings%20solve%20complex%20problems%20through%20critical%20thinking%2C%20where%20reasoning%20and%20evaluation%20are%20intertwined%20to%20converge%20toward%20correct%20solutions.%20However%2C%20most%20existing%20large%20language%20models%20%28LLMs%29%20decouple%20reasoning%20from%20verification%3A%20they%20either%20generate%20reasoning%20without%20explicit%20self-checking%20or%20rely%20on%20external%20verifiers%20to%20detect%20errors%20post%20hoc.%20The%20former%20lacks%20immediate%20feedback%2C%20while%20the%20latter%20increases%20system%20complexity%20and%20hinders%20synchronized%20learning.%20Motivated%20by%20human%20critical%20thinking%2C%20we%20propose%20Stepwise%20Think-Critique%20%28STC%29%2C%20a%20unified%20framework%20that%20interleaves%20reasoning%20and%20self-critique%20at%20each%20step%20within%20a%20single%20model.%20STC%20is%20trained%20with%20a%20hybrid%20reinforcement%20learning%20objective%20combining%20reasoning%20rewards%20and%20critique-consistency%20rewards%20to%20jointly%20optimize%20reasoning%20quality%20and%20self-evaluation.%20Experiments%20on%20mathematical%20reasoning%20benchmarks%20show%20that%20STC%20demonstrates%20strong%20critic-thinking%20capabilities%20and%20produces%20more%20interpretable%20reasoning%20traces%2C%20representing%20a%20step%20toward%20LLMs%20with%20built-in%20critical%20thinking.&entry.1838667208=http%3A//arxiv.org/abs/2512.15662v1&entry.124074799=Read"},
{"title": "MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors", "author": "Zhipeng Du and Duolikun Danier and Jan Eric Lenssen and Hakan Bilen", "abstract": "In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.", "link": "http://arxiv.org/abs/2512.15577v1", "date": "2025-12-17", "relevancy": 2.4004, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6071}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5962}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoonSeg3R%3A%20Monocular%20Online%20Zero-Shot%20Segment%20Anything%20in%203D%20with%20Reconstructive%20Foundation%20Priors&body=Title%3A%20MoonSeg3R%3A%20Monocular%20Online%20Zero-Shot%20Segment%20Anything%20in%203D%20with%20Reconstructive%20Foundation%20Priors%0AAuthor%3A%20Zhipeng%20Du%20and%20Duolikun%20Danier%20and%20Jan%20Eric%20Lenssen%20and%20Hakan%20Bilen%0AAbstract%3A%20In%20this%20paper%2C%20we%20focus%20on%20online%20zero-shot%20monocular%203D%20instance%20segmentation%2C%20a%20novel%20practical%20setting%20where%20existing%20approaches%20fail%20to%20perform%20because%20they%20rely%20on%20posed%20RGB-D%20sequences.%20To%20overcome%20this%20limitation%2C%20we%20leverage%20CUT3R%2C%20a%20recent%20Reconstructive%20Foundation%20Model%20%28RFM%29%2C%20to%20provide%20reliable%20geometric%20priors%20from%20a%20single%20RGB%20stream.%20We%20propose%20MoonSeg3R%2C%20which%20introduces%20three%20key%20components%3A%20%281%29%20a%20self-supervised%20query%20refinement%20module%20with%20spatial-semantic%20distillation%20that%20transforms%20segmentation%20masks%20from%202D%20visual%20foundation%20models%20%28VFMs%29%20into%20discriminative%203D%20queries%3B%20%282%29%20a%203D%20query%20index%20memory%20that%20provides%20temporal%20consistency%20by%20retrieving%20contextual%20queries%3B%20and%20%283%29%20a%20state-distribution%20token%20from%20CUT3R%20that%20acts%20as%20a%20mask%20identity%20descriptor%20to%20strengthen%20cross-frame%20fusion.%20Experiments%20on%20ScanNet200%20and%20SceneNN%20show%20that%20MoonSeg3R%20is%20the%20first%20method%20to%20enable%20online%20monocular%203D%20segmentation%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20RGB-D-based%20systems.%20Code%20and%20models%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoonSeg3R%253A%2520Monocular%2520Online%2520Zero-Shot%2520Segment%2520Anything%2520in%25203D%2520with%2520Reconstructive%2520Foundation%2520Priors%26entry.906535625%3DZhipeng%2520Du%2520and%2520Duolikun%2520Danier%2520and%2520Jan%2520Eric%2520Lenssen%2520and%2520Hakan%2520Bilen%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520focus%2520on%2520online%2520zero-shot%2520monocular%25203D%2520instance%2520segmentation%252C%2520a%2520novel%2520practical%2520setting%2520where%2520existing%2520approaches%2520fail%2520to%2520perform%2520because%2520they%2520rely%2520on%2520posed%2520RGB-D%2520sequences.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520leverage%2520CUT3R%252C%2520a%2520recent%2520Reconstructive%2520Foundation%2520Model%2520%2528RFM%2529%252C%2520to%2520provide%2520reliable%2520geometric%2520priors%2520from%2520a%2520single%2520RGB%2520stream.%2520We%2520propose%2520MoonSeg3R%252C%2520which%2520introduces%2520three%2520key%2520components%253A%2520%25281%2529%2520a%2520self-supervised%2520query%2520refinement%2520module%2520with%2520spatial-semantic%2520distillation%2520that%2520transforms%2520segmentation%2520masks%2520from%25202D%2520visual%2520foundation%2520models%2520%2528VFMs%2529%2520into%2520discriminative%25203D%2520queries%253B%2520%25282%2529%2520a%25203D%2520query%2520index%2520memory%2520that%2520provides%2520temporal%2520consistency%2520by%2520retrieving%2520contextual%2520queries%253B%2520and%2520%25283%2529%2520a%2520state-distribution%2520token%2520from%2520CUT3R%2520that%2520acts%2520as%2520a%2520mask%2520identity%2520descriptor%2520to%2520strengthen%2520cross-frame%2520fusion.%2520Experiments%2520on%2520ScanNet200%2520and%2520SceneNN%2520show%2520that%2520MoonSeg3R%2520is%2520the%2520first%2520method%2520to%2520enable%2520online%2520monocular%25203D%2520segmentation%2520and%2520achieves%2520performance%2520competitive%2520with%2520state-of-the-art%2520RGB-D-based%2520systems.%2520Code%2520and%2520models%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoonSeg3R%3A%20Monocular%20Online%20Zero-Shot%20Segment%20Anything%20in%203D%20with%20Reconstructive%20Foundation%20Priors&entry.906535625=Zhipeng%20Du%20and%20Duolikun%20Danier%20and%20Jan%20Eric%20Lenssen%20and%20Hakan%20Bilen&entry.1292438233=In%20this%20paper%2C%20we%20focus%20on%20online%20zero-shot%20monocular%203D%20instance%20segmentation%2C%20a%20novel%20practical%20setting%20where%20existing%20approaches%20fail%20to%20perform%20because%20they%20rely%20on%20posed%20RGB-D%20sequences.%20To%20overcome%20this%20limitation%2C%20we%20leverage%20CUT3R%2C%20a%20recent%20Reconstructive%20Foundation%20Model%20%28RFM%29%2C%20to%20provide%20reliable%20geometric%20priors%20from%20a%20single%20RGB%20stream.%20We%20propose%20MoonSeg3R%2C%20which%20introduces%20three%20key%20components%3A%20%281%29%20a%20self-supervised%20query%20refinement%20module%20with%20spatial-semantic%20distillation%20that%20transforms%20segmentation%20masks%20from%202D%20visual%20foundation%20models%20%28VFMs%29%20into%20discriminative%203D%20queries%3B%20%282%29%20a%203D%20query%20index%20memory%20that%20provides%20temporal%20consistency%20by%20retrieving%20contextual%20queries%3B%20and%20%283%29%20a%20state-distribution%20token%20from%20CUT3R%20that%20acts%20as%20a%20mask%20identity%20descriptor%20to%20strengthen%20cross-frame%20fusion.%20Experiments%20on%20ScanNet200%20and%20SceneNN%20show%20that%20MoonSeg3R%20is%20the%20first%20method%20to%20enable%20online%20monocular%203D%20segmentation%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20RGB-D-based%20systems.%20Code%20and%20models%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.15577v1&entry.124074799=Read"},
{"title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation", "author": "Sihan Zhao and Zixuan Wang and Tianyu Luan and Jia Jia and Wentao Zhu and Jiebo Luo and Junsong Yuan and Nan Xi", "abstract": "Human motion generation has found widespread applications in AR/VR, film, sports, and medical rehabilitation, offering a cost-effective alternative to traditional motion capture systems. However, evaluating the fidelity of such generated motions is a crucial, multifaceted task. Although previous approaches have attempted at motion fidelity evaluation using human perception or physical constraints, there remains an inherent gap between human-perceived fidelity and physical feasibility. Moreover, the subjective and coarse binary labeling of human perception further undermines the development of a robust data-driven metric. We address these issues by introducing a physical labeling method. This method evaluates motion fidelity by calculating the minimum modifications needed for a motion to align with physical laws. With this approach, we are able to produce fine-grained, continuous physical alignment annotations that serve as objective ground truth. With these annotations, we propose PP-Motion, a novel data-driven metric to evaluate both physical and perceptual fidelity of human motion. To effectively capture underlying physical priors, we employ Pearson's correlation loss for the training of our metric. Additionally, by incorporating a human-based perceptual fidelity loss, our metric can capture fidelity that simultaneously considers both human perception and physical alignment. Experimental results demonstrate that our metric, PP-Motion, not only aligns with physical laws but also aligns better with human perception of motion fidelity than previous work.", "link": "http://arxiv.org/abs/2508.08179v2", "date": "2025-12-17", "relevancy": 2.3924, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6124}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5945}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%20Generation&body=Title%3A%20PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%20Generation%0AAuthor%3A%20Sihan%20Zhao%20and%20Zixuan%20Wang%20and%20Tianyu%20Luan%20and%20Jia%20Jia%20and%20Wentao%20Zhu%20and%20Jiebo%20Luo%20and%20Junsong%20Yuan%20and%20Nan%20Xi%0AAbstract%3A%20Human%20motion%20generation%20has%20found%20widespread%20applications%20in%20AR/VR%2C%20film%2C%20sports%2C%20and%20medical%20rehabilitation%2C%20offering%20a%20cost-effective%20alternative%20to%20traditional%20motion%20capture%20systems.%20However%2C%20evaluating%20the%20fidelity%20of%20such%20generated%20motions%20is%20a%20crucial%2C%20multifaceted%20task.%20Although%20previous%20approaches%20have%20attempted%20at%20motion%20fidelity%20evaluation%20using%20human%20perception%20or%20physical%20constraints%2C%20there%20remains%20an%20inherent%20gap%20between%20human-perceived%20fidelity%20and%20physical%20feasibility.%20Moreover%2C%20the%20subjective%20and%20coarse%20binary%20labeling%20of%20human%20perception%20further%20undermines%20the%20development%20of%20a%20robust%20data-driven%20metric.%20We%20address%20these%20issues%20by%20introducing%20a%20physical%20labeling%20method.%20This%20method%20evaluates%20motion%20fidelity%20by%20calculating%20the%20minimum%20modifications%20needed%20for%20a%20motion%20to%20align%20with%20physical%20laws.%20With%20this%20approach%2C%20we%20are%20able%20to%20produce%20fine-grained%2C%20continuous%20physical%20alignment%20annotations%20that%20serve%20as%20objective%20ground%20truth.%20With%20these%20annotations%2C%20we%20propose%20PP-Motion%2C%20a%20novel%20data-driven%20metric%20to%20evaluate%20both%20physical%20and%20perceptual%20fidelity%20of%20human%20motion.%20To%20effectively%20capture%20underlying%20physical%20priors%2C%20we%20employ%20Pearson%27s%20correlation%20loss%20for%20the%20training%20of%20our%20metric.%20Additionally%2C%20by%20incorporating%20a%20human-based%20perceptual%20fidelity%20loss%2C%20our%20metric%20can%20capture%20fidelity%20that%20simultaneously%20considers%20both%20human%20perception%20and%20physical%20alignment.%20Experimental%20results%20demonstrate%20that%20our%20metric%2C%20PP-Motion%2C%20not%20only%20aligns%20with%20physical%20laws%20but%20also%20aligns%20better%20with%20human%20perception%20of%20motion%20fidelity%20than%20previous%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPP-Motion%253A%2520Physical-Perceptual%2520Fidelity%2520Evaluation%2520for%2520Human%2520Motion%2520Generation%26entry.906535625%3DSihan%2520Zhao%2520and%2520Zixuan%2520Wang%2520and%2520Tianyu%2520Luan%2520and%2520Jia%2520Jia%2520and%2520Wentao%2520Zhu%2520and%2520Jiebo%2520Luo%2520and%2520Junsong%2520Yuan%2520and%2520Nan%2520Xi%26entry.1292438233%3DHuman%2520motion%2520generation%2520has%2520found%2520widespread%2520applications%2520in%2520AR/VR%252C%2520film%252C%2520sports%252C%2520and%2520medical%2520rehabilitation%252C%2520offering%2520a%2520cost-effective%2520alternative%2520to%2520traditional%2520motion%2520capture%2520systems.%2520However%252C%2520evaluating%2520the%2520fidelity%2520of%2520such%2520generated%2520motions%2520is%2520a%2520crucial%252C%2520multifaceted%2520task.%2520Although%2520previous%2520approaches%2520have%2520attempted%2520at%2520motion%2520fidelity%2520evaluation%2520using%2520human%2520perception%2520or%2520physical%2520constraints%252C%2520there%2520remains%2520an%2520inherent%2520gap%2520between%2520human-perceived%2520fidelity%2520and%2520physical%2520feasibility.%2520Moreover%252C%2520the%2520subjective%2520and%2520coarse%2520binary%2520labeling%2520of%2520human%2520perception%2520further%2520undermines%2520the%2520development%2520of%2520a%2520robust%2520data-driven%2520metric.%2520We%2520address%2520these%2520issues%2520by%2520introducing%2520a%2520physical%2520labeling%2520method.%2520This%2520method%2520evaluates%2520motion%2520fidelity%2520by%2520calculating%2520the%2520minimum%2520modifications%2520needed%2520for%2520a%2520motion%2520to%2520align%2520with%2520physical%2520laws.%2520With%2520this%2520approach%252C%2520we%2520are%2520able%2520to%2520produce%2520fine-grained%252C%2520continuous%2520physical%2520alignment%2520annotations%2520that%2520serve%2520as%2520objective%2520ground%2520truth.%2520With%2520these%2520annotations%252C%2520we%2520propose%2520PP-Motion%252C%2520a%2520novel%2520data-driven%2520metric%2520to%2520evaluate%2520both%2520physical%2520and%2520perceptual%2520fidelity%2520of%2520human%2520motion.%2520To%2520effectively%2520capture%2520underlying%2520physical%2520priors%252C%2520we%2520employ%2520Pearson%2527s%2520correlation%2520loss%2520for%2520the%2520training%2520of%2520our%2520metric.%2520Additionally%252C%2520by%2520incorporating%2520a%2520human-based%2520perceptual%2520fidelity%2520loss%252C%2520our%2520metric%2520can%2520capture%2520fidelity%2520that%2520simultaneously%2520considers%2520both%2520human%2520perception%2520and%2520physical%2520alignment.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520metric%252C%2520PP-Motion%252C%2520not%2520only%2520aligns%2520with%2520physical%2520laws%2520but%2520also%2520aligns%2520better%2520with%2520human%2520perception%2520of%2520motion%2520fidelity%2520than%2520previous%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%20Generation&entry.906535625=Sihan%20Zhao%20and%20Zixuan%20Wang%20and%20Tianyu%20Luan%20and%20Jia%20Jia%20and%20Wentao%20Zhu%20and%20Jiebo%20Luo%20and%20Junsong%20Yuan%20and%20Nan%20Xi&entry.1292438233=Human%20motion%20generation%20has%20found%20widespread%20applications%20in%20AR/VR%2C%20film%2C%20sports%2C%20and%20medical%20rehabilitation%2C%20offering%20a%20cost-effective%20alternative%20to%20traditional%20motion%20capture%20systems.%20However%2C%20evaluating%20the%20fidelity%20of%20such%20generated%20motions%20is%20a%20crucial%2C%20multifaceted%20task.%20Although%20previous%20approaches%20have%20attempted%20at%20motion%20fidelity%20evaluation%20using%20human%20perception%20or%20physical%20constraints%2C%20there%20remains%20an%20inherent%20gap%20between%20human-perceived%20fidelity%20and%20physical%20feasibility.%20Moreover%2C%20the%20subjective%20and%20coarse%20binary%20labeling%20of%20human%20perception%20further%20undermines%20the%20development%20of%20a%20robust%20data-driven%20metric.%20We%20address%20these%20issues%20by%20introducing%20a%20physical%20labeling%20method.%20This%20method%20evaluates%20motion%20fidelity%20by%20calculating%20the%20minimum%20modifications%20needed%20for%20a%20motion%20to%20align%20with%20physical%20laws.%20With%20this%20approach%2C%20we%20are%20able%20to%20produce%20fine-grained%2C%20continuous%20physical%20alignment%20annotations%20that%20serve%20as%20objective%20ground%20truth.%20With%20these%20annotations%2C%20we%20propose%20PP-Motion%2C%20a%20novel%20data-driven%20metric%20to%20evaluate%20both%20physical%20and%20perceptual%20fidelity%20of%20human%20motion.%20To%20effectively%20capture%20underlying%20physical%20priors%2C%20we%20employ%20Pearson%27s%20correlation%20loss%20for%20the%20training%20of%20our%20metric.%20Additionally%2C%20by%20incorporating%20a%20human-based%20perceptual%20fidelity%20loss%2C%20our%20metric%20can%20capture%20fidelity%20that%20simultaneously%20considers%20both%20human%20perception%20and%20physical%20alignment.%20Experimental%20results%20demonstrate%20that%20our%20metric%2C%20PP-Motion%2C%20not%20only%20aligns%20with%20physical%20laws%20but%20also%20aligns%20better%20with%20human%20perception%20of%20motion%20fidelity%20than%20previous%20work.&entry.1838667208=http%3A//arxiv.org/abs/2508.08179v2&entry.124074799=Read"},
{"title": "Online Partitioned Local Depth for semi-supervised applications", "author": "John D. Foley and Justin T. Lee", "abstract": "We introduce an extension of the partitioned local depth (PaLD) algorithm that is adapted to online applications such as semi-supervised prediction. The new algorithm we present, online PaLD, is well-suited to situations where it is a possible to pre-compute a cohesion network from a reference dataset. After $O(n^3)$ steps to construct a queryable data structure, online PaLD can extend the cohesion network to a new data point in $O(n^2)$ time. Our approach complements previous speed up approaches based on approximation and parallelism. For illustrations, we present applications to online anomaly detection and semi-supervised classification for health-care datasets.", "link": "http://arxiv.org/abs/2512.15436v1", "date": "2025-12-17", "relevancy": 2.3816, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4818}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4761}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Partitioned%20Local%20Depth%20for%20semi-supervised%20applications&body=Title%3A%20Online%20Partitioned%20Local%20Depth%20for%20semi-supervised%20applications%0AAuthor%3A%20John%20D.%20Foley%20and%20Justin%20T.%20Lee%0AAbstract%3A%20We%20introduce%20an%20extension%20of%20the%20partitioned%20local%20depth%20%28PaLD%29%20algorithm%20that%20is%20adapted%20to%20online%20applications%20such%20as%20semi-supervised%20prediction.%20The%20new%20algorithm%20we%20present%2C%20online%20PaLD%2C%20is%20well-suited%20to%20situations%20where%20it%20is%20a%20possible%20to%20pre-compute%20a%20cohesion%20network%20from%20a%20reference%20dataset.%20After%20%24O%28n%5E3%29%24%20steps%20to%20construct%20a%20queryable%20data%20structure%2C%20online%20PaLD%20can%20extend%20the%20cohesion%20network%20to%20a%20new%20data%20point%20in%20%24O%28n%5E2%29%24%20time.%20Our%20approach%20complements%20previous%20speed%20up%20approaches%20based%20on%20approximation%20and%20parallelism.%20For%20illustrations%2C%20we%20present%20applications%20to%20online%20anomaly%20detection%20and%20semi-supervised%20classification%20for%20health-care%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Partitioned%2520Local%2520Depth%2520for%2520semi-supervised%2520applications%26entry.906535625%3DJohn%2520D.%2520Foley%2520and%2520Justin%2520T.%2520Lee%26entry.1292438233%3DWe%2520introduce%2520an%2520extension%2520of%2520the%2520partitioned%2520local%2520depth%2520%2528PaLD%2529%2520algorithm%2520that%2520is%2520adapted%2520to%2520online%2520applications%2520such%2520as%2520semi-supervised%2520prediction.%2520The%2520new%2520algorithm%2520we%2520present%252C%2520online%2520PaLD%252C%2520is%2520well-suited%2520to%2520situations%2520where%2520it%2520is%2520a%2520possible%2520to%2520pre-compute%2520a%2520cohesion%2520network%2520from%2520a%2520reference%2520dataset.%2520After%2520%2524O%2528n%255E3%2529%2524%2520steps%2520to%2520construct%2520a%2520queryable%2520data%2520structure%252C%2520online%2520PaLD%2520can%2520extend%2520the%2520cohesion%2520network%2520to%2520a%2520new%2520data%2520point%2520in%2520%2524O%2528n%255E2%2529%2524%2520time.%2520Our%2520approach%2520complements%2520previous%2520speed%2520up%2520approaches%2520based%2520on%2520approximation%2520and%2520parallelism.%2520For%2520illustrations%252C%2520we%2520present%2520applications%2520to%2520online%2520anomaly%2520detection%2520and%2520semi-supervised%2520classification%2520for%2520health-care%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Partitioned%20Local%20Depth%20for%20semi-supervised%20applications&entry.906535625=John%20D.%20Foley%20and%20Justin%20T.%20Lee&entry.1292438233=We%20introduce%20an%20extension%20of%20the%20partitioned%20local%20depth%20%28PaLD%29%20algorithm%20that%20is%20adapted%20to%20online%20applications%20such%20as%20semi-supervised%20prediction.%20The%20new%20algorithm%20we%20present%2C%20online%20PaLD%2C%20is%20well-suited%20to%20situations%20where%20it%20is%20a%20possible%20to%20pre-compute%20a%20cohesion%20network%20from%20a%20reference%20dataset.%20After%20%24O%28n%5E3%29%24%20steps%20to%20construct%20a%20queryable%20data%20structure%2C%20online%20PaLD%20can%20extend%20the%20cohesion%20network%20to%20a%20new%20data%20point%20in%20%24O%28n%5E2%29%24%20time.%20Our%20approach%20complements%20previous%20speed%20up%20approaches%20based%20on%20approximation%20and%20parallelism.%20For%20illustrations%2C%20we%20present%20applications%20to%20online%20anomaly%20detection%20and%20semi-supervised%20classification%20for%20health-care%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.15436v1&entry.124074799=Read"},
{"title": "Spatia: Video Generation with Updatable Spatial Memory", "author": "Jinjing Zhao and Fangyun Wei and Zhening Liu and Hongyang Zhang and Chang Xu and Yan Lu", "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "link": "http://arxiv.org/abs/2512.15716v1", "date": "2025-12-17", "relevancy": 2.3787, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6144}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6001}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatia%3A%20Video%20Generation%20with%20Updatable%20Spatial%20Memory&body=Title%3A%20Spatia%3A%20Video%20Generation%20with%20Updatable%20Spatial%20Memory%0AAuthor%3A%20Jinjing%20Zhao%20and%20Fangyun%20Wei%20and%20Zhening%20Liu%20and%20Hongyang%20Zhang%20and%20Chang%20Xu%20and%20Yan%20Lu%0AAbstract%3A%20Existing%20video%20generation%20models%20struggle%20to%20maintain%20long-term%20spatial%20and%20temporal%20consistency%20due%20to%20the%20dense%2C%20high-dimensional%20nature%20of%20video%20signals.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Spatia%2C%20a%20spatial%20memory-aware%20video%20generation%20framework%20that%20explicitly%20preserves%20a%203D%20scene%20point%20cloud%20as%20persistent%20spatial%20memory.%20Spatia%20iteratively%20generates%20video%20clips%20conditioned%20on%20this%20spatial%20memory%20and%20continuously%20updates%20it%20through%20visual%20SLAM.%20This%20dynamic-static%20disentanglement%20design%20enhances%20spatial%20consistency%20throughout%20the%20generation%20process%20while%20preserving%20the%20model%27s%20ability%20to%20produce%20realistic%20dynamic%20entities.%20Furthermore%2C%20Spatia%20enables%20applications%20such%20as%20explicit%20camera%20control%20and%203D-aware%20interactive%20editing%2C%20providing%20a%20geometrically%20grounded%20framework%20for%20scalable%2C%20memory-driven%20video%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatia%253A%2520Video%2520Generation%2520with%2520Updatable%2520Spatial%2520Memory%26entry.906535625%3DJinjing%2520Zhao%2520and%2520Fangyun%2520Wei%2520and%2520Zhening%2520Liu%2520and%2520Hongyang%2520Zhang%2520and%2520Chang%2520Xu%2520and%2520Yan%2520Lu%26entry.1292438233%3DExisting%2520video%2520generation%2520models%2520struggle%2520to%2520maintain%2520long-term%2520spatial%2520and%2520temporal%2520consistency%2520due%2520to%2520the%2520dense%252C%2520high-dimensional%2520nature%2520of%2520video%2520signals.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Spatia%252C%2520a%2520spatial%2520memory-aware%2520video%2520generation%2520framework%2520that%2520explicitly%2520preserves%2520a%25203D%2520scene%2520point%2520cloud%2520as%2520persistent%2520spatial%2520memory.%2520Spatia%2520iteratively%2520generates%2520video%2520clips%2520conditioned%2520on%2520this%2520spatial%2520memory%2520and%2520continuously%2520updates%2520it%2520through%2520visual%2520SLAM.%2520This%2520dynamic-static%2520disentanglement%2520design%2520enhances%2520spatial%2520consistency%2520throughout%2520the%2520generation%2520process%2520while%2520preserving%2520the%2520model%2527s%2520ability%2520to%2520produce%2520realistic%2520dynamic%2520entities.%2520Furthermore%252C%2520Spatia%2520enables%2520applications%2520such%2520as%2520explicit%2520camera%2520control%2520and%25203D-aware%2520interactive%2520editing%252C%2520providing%2520a%2520geometrically%2520grounded%2520framework%2520for%2520scalable%252C%2520memory-driven%2520video%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatia%3A%20Video%20Generation%20with%20Updatable%20Spatial%20Memory&entry.906535625=Jinjing%20Zhao%20and%20Fangyun%20Wei%20and%20Zhening%20Liu%20and%20Hongyang%20Zhang%20and%20Chang%20Xu%20and%20Yan%20Lu&entry.1292438233=Existing%20video%20generation%20models%20struggle%20to%20maintain%20long-term%20spatial%20and%20temporal%20consistency%20due%20to%20the%20dense%2C%20high-dimensional%20nature%20of%20video%20signals.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Spatia%2C%20a%20spatial%20memory-aware%20video%20generation%20framework%20that%20explicitly%20preserves%20a%203D%20scene%20point%20cloud%20as%20persistent%20spatial%20memory.%20Spatia%20iteratively%20generates%20video%20clips%20conditioned%20on%20this%20spatial%20memory%20and%20continuously%20updates%20it%20through%20visual%20SLAM.%20This%20dynamic-static%20disentanglement%20design%20enhances%20spatial%20consistency%20throughout%20the%20generation%20process%20while%20preserving%20the%20model%27s%20ability%20to%20produce%20realistic%20dynamic%20entities.%20Furthermore%2C%20Spatia%20enables%20applications%20such%20as%20explicit%20camera%20control%20and%203D-aware%20interactive%20editing%2C%20providing%20a%20geometrically%20grounded%20framework%20for%20scalable%2C%20memory-driven%20video%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.15716v1&entry.124074799=Read"},
{"title": "Robust Multi-view Camera Calibration from Dense Matches", "author": "Johannes H\u00e4gerlind and Bao-Long Tran and Urs Waldmann and Per-Erik Forss\u00e9n", "abstract": "Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.", "link": "http://arxiv.org/abs/2512.15608v1", "date": "2025-12-17", "relevancy": 2.3777, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6198}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.599}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Multi-view%20Camera%20Calibration%20from%20Dense%20Matches&body=Title%3A%20Robust%20Multi-view%20Camera%20Calibration%20from%20Dense%20Matches%0AAuthor%3A%20Johannes%20H%C3%A4gerlind%20and%20Bao-Long%20Tran%20and%20Urs%20Waldmann%20and%20Per-Erik%20Forss%C3%A9n%0AAbstract%3A%20Estimating%20camera%20intrinsics%20and%20extrinsics%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%20and%20while%20advances%20in%20structure-from-motion%20%28SfM%29%20have%20improved%20accuracy%20and%20robustness%2C%20open%20challenges%20remain.%20In%20this%20paper%2C%20we%20introduce%20a%20robust%20method%20for%20pose%20estimation%20and%20calibration.%20We%20consider%20a%20set%20of%20rigid%20cameras%2C%20each%20observing%20the%20scene%20from%20a%20different%20perspective%2C%20which%20is%20a%20typical%20camera%20setup%20in%20animal%20behavior%20studies%20and%20forensic%20analysis%20of%20surveillance%20footage.%20Specifically%2C%20we%20analyse%20the%20individual%20components%20in%20a%20structure-from-motion%20%28SfM%29%20pipeline%2C%20and%20identify%20design%20choices%20that%20improve%20accuracy.%20Our%20main%20contributions%20are%3A%20%281%29%20we%20investigate%20how%20to%20best%20subsample%20the%20predicted%20correspondences%20from%20a%20dense%20matcher%20to%20leverage%20them%20in%20the%20estimation%20process.%20%282%29%20We%20investigate%20selection%20criteria%20for%20how%20to%20add%20the%20views%20incrementally.%20In%20a%20rigorous%20quantitative%20evaluation%2C%20we%20show%20the%20effectiveness%20of%20our%20changes%2C%20especially%20for%20cameras%20with%20strong%20radial%20distortion%20%2879.9%25%20ours%20vs.%2040.4%20vanilla%20VGGT%29.%20Finally%2C%20we%20demonstrate%20our%20correspondence%20subsampling%20in%20a%20global%20SfM%20setting%20where%20we%20initialize%20the%20poses%20using%20VGGT.%20The%20proposed%20pipeline%20generalizes%20across%20a%20wide%20range%20of%20camera%20setups%2C%20and%20could%20thus%20become%20a%20useful%20tool%20for%20animal%20behavior%20and%20forensic%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Multi-view%2520Camera%2520Calibration%2520from%2520Dense%2520Matches%26entry.906535625%3DJohannes%2520H%25C3%25A4gerlind%2520and%2520Bao-Long%2520Tran%2520and%2520Urs%2520Waldmann%2520and%2520Per-Erik%2520Forss%25C3%25A9n%26entry.1292438233%3DEstimating%2520camera%2520intrinsics%2520and%2520extrinsics%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%252C%2520and%2520while%2520advances%2520in%2520structure-from-motion%2520%2528SfM%2529%2520have%2520improved%2520accuracy%2520and%2520robustness%252C%2520open%2520challenges%2520remain.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520robust%2520method%2520for%2520pose%2520estimation%2520and%2520calibration.%2520We%2520consider%2520a%2520set%2520of%2520rigid%2520cameras%252C%2520each%2520observing%2520the%2520scene%2520from%2520a%2520different%2520perspective%252C%2520which%2520is%2520a%2520typical%2520camera%2520setup%2520in%2520animal%2520behavior%2520studies%2520and%2520forensic%2520analysis%2520of%2520surveillance%2520footage.%2520Specifically%252C%2520we%2520analyse%2520the%2520individual%2520components%2520in%2520a%2520structure-from-motion%2520%2528SfM%2529%2520pipeline%252C%2520and%2520identify%2520design%2520choices%2520that%2520improve%2520accuracy.%2520Our%2520main%2520contributions%2520are%253A%2520%25281%2529%2520we%2520investigate%2520how%2520to%2520best%2520subsample%2520the%2520predicted%2520correspondences%2520from%2520a%2520dense%2520matcher%2520to%2520leverage%2520them%2520in%2520the%2520estimation%2520process.%2520%25282%2529%2520We%2520investigate%2520selection%2520criteria%2520for%2520how%2520to%2520add%2520the%2520views%2520incrementally.%2520In%2520a%2520rigorous%2520quantitative%2520evaluation%252C%2520we%2520show%2520the%2520effectiveness%2520of%2520our%2520changes%252C%2520especially%2520for%2520cameras%2520with%2520strong%2520radial%2520distortion%2520%252879.9%2525%2520ours%2520vs.%252040.4%2520vanilla%2520VGGT%2529.%2520Finally%252C%2520we%2520demonstrate%2520our%2520correspondence%2520subsampling%2520in%2520a%2520global%2520SfM%2520setting%2520where%2520we%2520initialize%2520the%2520poses%2520using%2520VGGT.%2520The%2520proposed%2520pipeline%2520generalizes%2520across%2520a%2520wide%2520range%2520of%2520camera%2520setups%252C%2520and%2520could%2520thus%2520become%2520a%2520useful%2520tool%2520for%2520animal%2520behavior%2520and%2520forensic%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Multi-view%20Camera%20Calibration%20from%20Dense%20Matches&entry.906535625=Johannes%20H%C3%A4gerlind%20and%20Bao-Long%20Tran%20and%20Urs%20Waldmann%20and%20Per-Erik%20Forss%C3%A9n&entry.1292438233=Estimating%20camera%20intrinsics%20and%20extrinsics%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%20and%20while%20advances%20in%20structure-from-motion%20%28SfM%29%20have%20improved%20accuracy%20and%20robustness%2C%20open%20challenges%20remain.%20In%20this%20paper%2C%20we%20introduce%20a%20robust%20method%20for%20pose%20estimation%20and%20calibration.%20We%20consider%20a%20set%20of%20rigid%20cameras%2C%20each%20observing%20the%20scene%20from%20a%20different%20perspective%2C%20which%20is%20a%20typical%20camera%20setup%20in%20animal%20behavior%20studies%20and%20forensic%20analysis%20of%20surveillance%20footage.%20Specifically%2C%20we%20analyse%20the%20individual%20components%20in%20a%20structure-from-motion%20%28SfM%29%20pipeline%2C%20and%20identify%20design%20choices%20that%20improve%20accuracy.%20Our%20main%20contributions%20are%3A%20%281%29%20we%20investigate%20how%20to%20best%20subsample%20the%20predicted%20correspondences%20from%20a%20dense%20matcher%20to%20leverage%20them%20in%20the%20estimation%20process.%20%282%29%20We%20investigate%20selection%20criteria%20for%20how%20to%20add%20the%20views%20incrementally.%20In%20a%20rigorous%20quantitative%20evaluation%2C%20we%20show%20the%20effectiveness%20of%20our%20changes%2C%20especially%20for%20cameras%20with%20strong%20radial%20distortion%20%2879.9%25%20ours%20vs.%2040.4%20vanilla%20VGGT%29.%20Finally%2C%20we%20demonstrate%20our%20correspondence%20subsampling%20in%20a%20global%20SfM%20setting%20where%20we%20initialize%20the%20poses%20using%20VGGT.%20The%20proposed%20pipeline%20generalizes%20across%20a%20wide%20range%20of%20camera%20setups%2C%20and%20could%20thus%20become%20a%20useful%20tool%20for%20animal%20behavior%20and%20forensic%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.15608v1&entry.124074799=Read"},
{"title": "Exact Verification of Graph Neural Networks with Incremental Constraint Solving", "author": "Minghao Liu and Chia-Hsuan Lu and Marta Kwiatkowska", "abstract": "Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile exact verifier for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on real-world fraud datasets (Amazon and Yelp) and biochemical datasets (MUTAG and ENZYMES) demonstrates its usability and effectiveness, as well as superior performance for node classification and competitiveness on graph classification compared to existing exact verification tools on sum-aggregated GNNs.", "link": "http://arxiv.org/abs/2508.09320v2", "date": "2025-12-17", "relevancy": 2.367, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5029}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4713}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Verification%20of%20Graph%20Neural%20Networks%20with%20Incremental%20Constraint%20Solving&body=Title%3A%20Exact%20Verification%20of%20Graph%20Neural%20Networks%20with%20Incremental%20Constraint%20Solving%0AAuthor%3A%20Minghao%20Liu%20and%20Chia-Hsuan%20Lu%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20Graph%20neural%20networks%20%28GNNs%29%20are%20increasingly%20employed%20in%20high-stakes%20applications%2C%20such%20as%20fraud%20detection%20or%20healthcare%2C%20but%20are%20susceptible%20to%20adversarial%20attacks.%20A%20number%20of%20techniques%20have%20been%20proposed%20to%20provide%20adversarial%20robustness%20guarantees%2C%20but%20support%20for%20commonly%20used%20aggregation%20functions%20in%20message-passing%20GNNs%20is%20lacking.%20In%20this%20paper%2C%20we%20develop%20an%20exact%20%28sound%20and%20complete%29%20verification%20method%20for%20GNNs%20to%20compute%20guarantees%20against%20attribute%20and%20structural%20perturbations%20that%20involve%20edge%20addition%20or%20deletion%2C%20subject%20to%20budget%20constraints.%20Our%20method%20employs%20constraint%20solving%20with%20bound%20tightening%2C%20and%20iteratively%20solves%20a%20sequence%20of%20relaxed%20constraint%20satisfaction%20problems%20while%20relying%20on%20incremental%20solving%20capabilities%20of%20solvers%20to%20improve%20efficiency.%20We%20implement%20GNNev%2C%20a%20versatile%20exact%20verifier%20for%20message-passing%20neural%20networks%2C%20which%20supports%20three%20aggregation%20functions%2C%20sum%2C%20max%20and%20mean%2C%20with%20the%20latter%20two%20considered%20here%20for%20the%20first%20time.%20Extensive%20experimental%20evaluation%20of%20GNNev%20on%20real-world%20fraud%20datasets%20%28Amazon%20and%20Yelp%29%20and%20biochemical%20datasets%20%28MUTAG%20and%20ENZYMES%29%20demonstrates%20its%20usability%20and%20effectiveness%2C%20as%20well%20as%20superior%20performance%20for%20node%20classification%20and%20competitiveness%20on%20graph%20classification%20compared%20to%20existing%20exact%20verification%20tools%20on%20sum-aggregated%20GNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Verification%2520of%2520Graph%2520Neural%2520Networks%2520with%2520Incremental%2520Constraint%2520Solving%26entry.906535625%3DMinghao%2520Liu%2520and%2520Chia-Hsuan%2520Lu%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520increasingly%2520employed%2520in%2520high-stakes%2520applications%252C%2520such%2520as%2520fraud%2520detection%2520or%2520healthcare%252C%2520but%2520are%2520susceptible%2520to%2520adversarial%2520attacks.%2520A%2520number%2520of%2520techniques%2520have%2520been%2520proposed%2520to%2520provide%2520adversarial%2520robustness%2520guarantees%252C%2520but%2520support%2520for%2520commonly%2520used%2520aggregation%2520functions%2520in%2520message-passing%2520GNNs%2520is%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520develop%2520an%2520exact%2520%2528sound%2520and%2520complete%2529%2520verification%2520method%2520for%2520GNNs%2520to%2520compute%2520guarantees%2520against%2520attribute%2520and%2520structural%2520perturbations%2520that%2520involve%2520edge%2520addition%2520or%2520deletion%252C%2520subject%2520to%2520budget%2520constraints.%2520Our%2520method%2520employs%2520constraint%2520solving%2520with%2520bound%2520tightening%252C%2520and%2520iteratively%2520solves%2520a%2520sequence%2520of%2520relaxed%2520constraint%2520satisfaction%2520problems%2520while%2520relying%2520on%2520incremental%2520solving%2520capabilities%2520of%2520solvers%2520to%2520improve%2520efficiency.%2520We%2520implement%2520GNNev%252C%2520a%2520versatile%2520exact%2520verifier%2520for%2520message-passing%2520neural%2520networks%252C%2520which%2520supports%2520three%2520aggregation%2520functions%252C%2520sum%252C%2520max%2520and%2520mean%252C%2520with%2520the%2520latter%2520two%2520considered%2520here%2520for%2520the%2520first%2520time.%2520Extensive%2520experimental%2520evaluation%2520of%2520GNNev%2520on%2520real-world%2520fraud%2520datasets%2520%2528Amazon%2520and%2520Yelp%2529%2520and%2520biochemical%2520datasets%2520%2528MUTAG%2520and%2520ENZYMES%2529%2520demonstrates%2520its%2520usability%2520and%2520effectiveness%252C%2520as%2520well%2520as%2520superior%2520performance%2520for%2520node%2520classification%2520and%2520competitiveness%2520on%2520graph%2520classification%2520compared%2520to%2520existing%2520exact%2520verification%2520tools%2520on%2520sum-aggregated%2520GNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Verification%20of%20Graph%20Neural%20Networks%20with%20Incremental%20Constraint%20Solving&entry.906535625=Minghao%20Liu%20and%20Chia-Hsuan%20Lu%20and%20Marta%20Kwiatkowska&entry.1292438233=Graph%20neural%20networks%20%28GNNs%29%20are%20increasingly%20employed%20in%20high-stakes%20applications%2C%20such%20as%20fraud%20detection%20or%20healthcare%2C%20but%20are%20susceptible%20to%20adversarial%20attacks.%20A%20number%20of%20techniques%20have%20been%20proposed%20to%20provide%20adversarial%20robustness%20guarantees%2C%20but%20support%20for%20commonly%20used%20aggregation%20functions%20in%20message-passing%20GNNs%20is%20lacking.%20In%20this%20paper%2C%20we%20develop%20an%20exact%20%28sound%20and%20complete%29%20verification%20method%20for%20GNNs%20to%20compute%20guarantees%20against%20attribute%20and%20structural%20perturbations%20that%20involve%20edge%20addition%20or%20deletion%2C%20subject%20to%20budget%20constraints.%20Our%20method%20employs%20constraint%20solving%20with%20bound%20tightening%2C%20and%20iteratively%20solves%20a%20sequence%20of%20relaxed%20constraint%20satisfaction%20problems%20while%20relying%20on%20incremental%20solving%20capabilities%20of%20solvers%20to%20improve%20efficiency.%20We%20implement%20GNNev%2C%20a%20versatile%20exact%20verifier%20for%20message-passing%20neural%20networks%2C%20which%20supports%20three%20aggregation%20functions%2C%20sum%2C%20max%20and%20mean%2C%20with%20the%20latter%20two%20considered%20here%20for%20the%20first%20time.%20Extensive%20experimental%20evaluation%20of%20GNNev%20on%20real-world%20fraud%20datasets%20%28Amazon%20and%20Yelp%29%20and%20biochemical%20datasets%20%28MUTAG%20and%20ENZYMES%29%20demonstrates%20its%20usability%20and%20effectiveness%2C%20as%20well%20as%20superior%20performance%20for%20node%20classification%20and%20competitiveness%20on%20graph%20classification%20compared%20to%20existing%20exact%20verification%20tools%20on%20sum-aggregated%20GNNs.&entry.1838667208=http%3A//arxiv.org/abs/2508.09320v2&entry.124074799=Read"},
{"title": "Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry", "author": "Hoang Nguyen and Xiaohao Xu and Xiaonan Huang", "abstract": "Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.", "link": "http://arxiv.org/abs/2512.15423v1", "date": "2025-12-17", "relevancy": 2.3616, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6013}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5839}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photorealistic%20Phantom%20Roads%20in%20Real%20Scenes%3A%20Disentangling%203D%20Hallucinations%20from%20Physical%20Geometry&body=Title%3A%20Photorealistic%20Phantom%20Roads%20in%20Real%20Scenes%3A%20Disentangling%203D%20Hallucinations%20from%20Physical%20Geometry%0AAuthor%3A%20Hoang%20Nguyen%20and%20Xiaohao%20Xu%20and%20Xiaonan%20Huang%0AAbstract%3A%20Monocular%20depth%20foundation%20models%20achieve%20remarkable%20generalization%20by%20learning%20large-scale%20semantic%20priors%2C%20but%20this%20creates%20a%20critical%20vulnerability%3A%20they%20hallucinate%20illusory%203D%20structures%20from%20geometrically%20planar%20but%20perceptually%20ambiguous%20inputs.%20We%20term%20this%20failure%20the%203D%20Mirage.%20This%20paper%20introduces%20the%20first%20end-to-end%20framework%20to%20probe%2C%20quantify%2C%20and%20tame%20this%20unquantified%20safety%20risk.%20To%20probe%2C%20we%20present%203D-Mirage%2C%20the%20first%20benchmark%20of%20real-world%20illusions%20%28e.g.%2C%20street%20art%29%20with%20precise%20planar-region%20annotations%20and%20context-restricted%20crops.%20To%20quantify%2C%20we%20propose%20a%20Laplacian-based%20evaluation%20framework%20with%20two%20metrics%3A%20the%20Deviation%20Composite%20Score%20%28DCS%29%20for%20spurious%20non-planarity%20and%20the%20Confusion%20Composite%20Score%20%28CCS%29%20for%20contextual%20instability.%20To%20tame%20this%20failure%2C%20we%20introduce%20Grounded%20Self-Distillation%2C%20a%20parameter-efficient%20strategy%20that%20surgically%20enforces%20planarity%20on%20illusion%20ROIs%20while%20using%20a%20frozen%20teacher%20to%20preserve%20background%20knowledge%2C%20thus%20avoiding%20catastrophic%20forgetting.%20Our%20work%20provides%20the%20essential%20tools%20to%20diagnose%20and%20mitigate%20this%20phenomenon%2C%20urging%20a%20necessary%20shift%20in%20MDE%20evaluation%20from%20pixel-wise%20accuracy%20to%20structural%20and%20contextual%20robustness.%20Our%20code%20and%20benchmark%20will%20be%20publicly%20available%20to%20foster%20this%20exciting%20research%20direction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhotorealistic%2520Phantom%2520Roads%2520in%2520Real%2520Scenes%253A%2520Disentangling%25203D%2520Hallucinations%2520from%2520Physical%2520Geometry%26entry.906535625%3DHoang%2520Nguyen%2520and%2520Xiaohao%2520Xu%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3DMonocular%2520depth%2520foundation%2520models%2520achieve%2520remarkable%2520generalization%2520by%2520learning%2520large-scale%2520semantic%2520priors%252C%2520but%2520this%2520creates%2520a%2520critical%2520vulnerability%253A%2520they%2520hallucinate%2520illusory%25203D%2520structures%2520from%2520geometrically%2520planar%2520but%2520perceptually%2520ambiguous%2520inputs.%2520We%2520term%2520this%2520failure%2520the%25203D%2520Mirage.%2520This%2520paper%2520introduces%2520the%2520first%2520end-to-end%2520framework%2520to%2520probe%252C%2520quantify%252C%2520and%2520tame%2520this%2520unquantified%2520safety%2520risk.%2520To%2520probe%252C%2520we%2520present%25203D-Mirage%252C%2520the%2520first%2520benchmark%2520of%2520real-world%2520illusions%2520%2528e.g.%252C%2520street%2520art%2529%2520with%2520precise%2520planar-region%2520annotations%2520and%2520context-restricted%2520crops.%2520To%2520quantify%252C%2520we%2520propose%2520a%2520Laplacian-based%2520evaluation%2520framework%2520with%2520two%2520metrics%253A%2520the%2520Deviation%2520Composite%2520Score%2520%2528DCS%2529%2520for%2520spurious%2520non-planarity%2520and%2520the%2520Confusion%2520Composite%2520Score%2520%2528CCS%2529%2520for%2520contextual%2520instability.%2520To%2520tame%2520this%2520failure%252C%2520we%2520introduce%2520Grounded%2520Self-Distillation%252C%2520a%2520parameter-efficient%2520strategy%2520that%2520surgically%2520enforces%2520planarity%2520on%2520illusion%2520ROIs%2520while%2520using%2520a%2520frozen%2520teacher%2520to%2520preserve%2520background%2520knowledge%252C%2520thus%2520avoiding%2520catastrophic%2520forgetting.%2520Our%2520work%2520provides%2520the%2520essential%2520tools%2520to%2520diagnose%2520and%2520mitigate%2520this%2520phenomenon%252C%2520urging%2520a%2520necessary%2520shift%2520in%2520MDE%2520evaluation%2520from%2520pixel-wise%2520accuracy%2520to%2520structural%2520and%2520contextual%2520robustness.%2520Our%2520code%2520and%2520benchmark%2520will%2520be%2520publicly%2520available%2520to%2520foster%2520this%2520exciting%2520research%2520direction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photorealistic%20Phantom%20Roads%20in%20Real%20Scenes%3A%20Disentangling%203D%20Hallucinations%20from%20Physical%20Geometry&entry.906535625=Hoang%20Nguyen%20and%20Xiaohao%20Xu%20and%20Xiaonan%20Huang&entry.1292438233=Monocular%20depth%20foundation%20models%20achieve%20remarkable%20generalization%20by%20learning%20large-scale%20semantic%20priors%2C%20but%20this%20creates%20a%20critical%20vulnerability%3A%20they%20hallucinate%20illusory%203D%20structures%20from%20geometrically%20planar%20but%20perceptually%20ambiguous%20inputs.%20We%20term%20this%20failure%20the%203D%20Mirage.%20This%20paper%20introduces%20the%20first%20end-to-end%20framework%20to%20probe%2C%20quantify%2C%20and%20tame%20this%20unquantified%20safety%20risk.%20To%20probe%2C%20we%20present%203D-Mirage%2C%20the%20first%20benchmark%20of%20real-world%20illusions%20%28e.g.%2C%20street%20art%29%20with%20precise%20planar-region%20annotations%20and%20context-restricted%20crops.%20To%20quantify%2C%20we%20propose%20a%20Laplacian-based%20evaluation%20framework%20with%20two%20metrics%3A%20the%20Deviation%20Composite%20Score%20%28DCS%29%20for%20spurious%20non-planarity%20and%20the%20Confusion%20Composite%20Score%20%28CCS%29%20for%20contextual%20instability.%20To%20tame%20this%20failure%2C%20we%20introduce%20Grounded%20Self-Distillation%2C%20a%20parameter-efficient%20strategy%20that%20surgically%20enforces%20planarity%20on%20illusion%20ROIs%20while%20using%20a%20frozen%20teacher%20to%20preserve%20background%20knowledge%2C%20thus%20avoiding%20catastrophic%20forgetting.%20Our%20work%20provides%20the%20essential%20tools%20to%20diagnose%20and%20mitigate%20this%20phenomenon%2C%20urging%20a%20necessary%20shift%20in%20MDE%20evaluation%20from%20pixel-wise%20accuracy%20to%20structural%20and%20contextual%20robustness.%20Our%20code%20and%20benchmark%20will%20be%20publicly%20available%20to%20foster%20this%20exciting%20research%20direction.&entry.1838667208=http%3A//arxiv.org/abs/2512.15423v1&entry.124074799=Read"},
{"title": "Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks", "author": "Shuyang Guo and Wenjin Xie and Ping Lu and Ting Deng and Richong Zhang and Jianxin Li and Xiangping Huang and Zhongyi Liu", "abstract": "Homomorphism is a key mapping technique between graphs that preserves their structure. Given a graph and a pattern, the subgraph homomorphism problem involves finding a mapping from the pattern to the graph, ensuring that adjacent vertices in the pattern are mapped to adjacent vertices in the graph. Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism allows multiple vertices in the pattern to map to the same vertex in the graph, making it more complex. We propose HFrame, the first graph neural network-based framework for subgraph homomorphism, which integrates traditional algorithms with machine learning techniques. We demonstrate that HFrame outperforms standard graph neural networks by being able to distinguish more graph pairs where the pattern is not homomorphic to the graph. Additionally, we provide a generalization error bound for HFrame. Through experiments on both real-world and synthetic graphs, we show that HFrame is up to 101.91 times faster than exact matching algorithms and achieves an average accuracy of 0.962.", "link": "http://arxiv.org/abs/2507.20226v2", "date": "2025-12-17", "relevancy": 2.3287, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4969}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4521}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Subgraph%20Matching%20by%20Combining%20Algorithms%20and%20Graph%20Neural%20Networks&body=Title%3A%20Improving%20Subgraph%20Matching%20by%20Combining%20Algorithms%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Shuyang%20Guo%20and%20Wenjin%20Xie%20and%20Ping%20Lu%20and%20Ting%20Deng%20and%20Richong%20Zhang%20and%20Jianxin%20Li%20and%20Xiangping%20Huang%20and%20Zhongyi%20Liu%0AAbstract%3A%20Homomorphism%20is%20a%20key%20mapping%20technique%20between%20graphs%20that%20preserves%20their%20structure.%20Given%20a%20graph%20and%20a%20pattern%2C%20the%20subgraph%20homomorphism%20problem%20involves%20finding%20a%20mapping%20from%20the%20pattern%20to%20the%20graph%2C%20ensuring%20that%20adjacent%20vertices%20in%20the%20pattern%20are%20mapped%20to%20adjacent%20vertices%20in%20the%20graph.%20Unlike%20subgraph%20isomorphism%2C%20which%20requires%20a%20one-to-one%20mapping%2C%20homomorphism%20allows%20multiple%20vertices%20in%20the%20pattern%20to%20map%20to%20the%20same%20vertex%20in%20the%20graph%2C%20making%20it%20more%20complex.%20We%20propose%20HFrame%2C%20the%20first%20graph%20neural%20network-based%20framework%20for%20subgraph%20homomorphism%2C%20which%20integrates%20traditional%20algorithms%20with%20machine%20learning%20techniques.%20We%20demonstrate%20that%20HFrame%20outperforms%20standard%20graph%20neural%20networks%20by%20being%20able%20to%20distinguish%20more%20graph%20pairs%20where%20the%20pattern%20is%20not%20homomorphic%20to%20the%20graph.%20Additionally%2C%20we%20provide%20a%20generalization%20error%20bound%20for%20HFrame.%20Through%20experiments%20on%20both%20real-world%20and%20synthetic%20graphs%2C%20we%20show%20that%20HFrame%20is%20up%20to%20101.91%20times%20faster%20than%20exact%20matching%20algorithms%20and%20achieves%20an%20average%20accuracy%20of%200.962.%0ALink%3A%20http%3A//arxiv.org/abs/2507.20226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Subgraph%2520Matching%2520by%2520Combining%2520Algorithms%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DShuyang%2520Guo%2520and%2520Wenjin%2520Xie%2520and%2520Ping%2520Lu%2520and%2520Ting%2520Deng%2520and%2520Richong%2520Zhang%2520and%2520Jianxin%2520Li%2520and%2520Xiangping%2520Huang%2520and%2520Zhongyi%2520Liu%26entry.1292438233%3DHomomorphism%2520is%2520a%2520key%2520mapping%2520technique%2520between%2520graphs%2520that%2520preserves%2520their%2520structure.%2520Given%2520a%2520graph%2520and%2520a%2520pattern%252C%2520the%2520subgraph%2520homomorphism%2520problem%2520involves%2520finding%2520a%2520mapping%2520from%2520the%2520pattern%2520to%2520the%2520graph%252C%2520ensuring%2520that%2520adjacent%2520vertices%2520in%2520the%2520pattern%2520are%2520mapped%2520to%2520adjacent%2520vertices%2520in%2520the%2520graph.%2520Unlike%2520subgraph%2520isomorphism%252C%2520which%2520requires%2520a%2520one-to-one%2520mapping%252C%2520homomorphism%2520allows%2520multiple%2520vertices%2520in%2520the%2520pattern%2520to%2520map%2520to%2520the%2520same%2520vertex%2520in%2520the%2520graph%252C%2520making%2520it%2520more%2520complex.%2520We%2520propose%2520HFrame%252C%2520the%2520first%2520graph%2520neural%2520network-based%2520framework%2520for%2520subgraph%2520homomorphism%252C%2520which%2520integrates%2520traditional%2520algorithms%2520with%2520machine%2520learning%2520techniques.%2520We%2520demonstrate%2520that%2520HFrame%2520outperforms%2520standard%2520graph%2520neural%2520networks%2520by%2520being%2520able%2520to%2520distinguish%2520more%2520graph%2520pairs%2520where%2520the%2520pattern%2520is%2520not%2520homomorphic%2520to%2520the%2520graph.%2520Additionally%252C%2520we%2520provide%2520a%2520generalization%2520error%2520bound%2520for%2520HFrame.%2520Through%2520experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520graphs%252C%2520we%2520show%2520that%2520HFrame%2520is%2520up%2520to%2520101.91%2520times%2520faster%2520than%2520exact%2520matching%2520algorithms%2520and%2520achieves%2520an%2520average%2520accuracy%2520of%25200.962.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Subgraph%20Matching%20by%20Combining%20Algorithms%20and%20Graph%20Neural%20Networks&entry.906535625=Shuyang%20Guo%20and%20Wenjin%20Xie%20and%20Ping%20Lu%20and%20Ting%20Deng%20and%20Richong%20Zhang%20and%20Jianxin%20Li%20and%20Xiangping%20Huang%20and%20Zhongyi%20Liu&entry.1292438233=Homomorphism%20is%20a%20key%20mapping%20technique%20between%20graphs%20that%20preserves%20their%20structure.%20Given%20a%20graph%20and%20a%20pattern%2C%20the%20subgraph%20homomorphism%20problem%20involves%20finding%20a%20mapping%20from%20the%20pattern%20to%20the%20graph%2C%20ensuring%20that%20adjacent%20vertices%20in%20the%20pattern%20are%20mapped%20to%20adjacent%20vertices%20in%20the%20graph.%20Unlike%20subgraph%20isomorphism%2C%20which%20requires%20a%20one-to-one%20mapping%2C%20homomorphism%20allows%20multiple%20vertices%20in%20the%20pattern%20to%20map%20to%20the%20same%20vertex%20in%20the%20graph%2C%20making%20it%20more%20complex.%20We%20propose%20HFrame%2C%20the%20first%20graph%20neural%20network-based%20framework%20for%20subgraph%20homomorphism%2C%20which%20integrates%20traditional%20algorithms%20with%20machine%20learning%20techniques.%20We%20demonstrate%20that%20HFrame%20outperforms%20standard%20graph%20neural%20networks%20by%20being%20able%20to%20distinguish%20more%20graph%20pairs%20where%20the%20pattern%20is%20not%20homomorphic%20to%20the%20graph.%20Additionally%2C%20we%20provide%20a%20generalization%20error%20bound%20for%20HFrame.%20Through%20experiments%20on%20both%20real-world%20and%20synthetic%20graphs%2C%20we%20show%20that%20HFrame%20is%20up%20to%20101.91%20times%20faster%20than%20exact%20matching%20algorithms%20and%20achieves%20an%20average%20accuracy%20of%200.962.&entry.1838667208=http%3A//arxiv.org/abs/2507.20226v2&entry.124074799=Read"},
{"title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "author": "Haoran Xi and Minghao Shao and Brendan Dolan-Gavitt and Muhammad Shafique and Ramesh Karri", "abstract": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function or file level detections which offers limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses run-time evidence such as crash points, stack traces, and coverage deltas with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.", "link": "http://arxiv.org/abs/2510.02389v2", "date": "2025-12-17", "relevancy": 2.3251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Trace%20to%20Line%3A%20LLM%20Agent%20for%20Real-World%20OSS%20Vulnerability%20Localization&body=Title%3A%20From%20Trace%20to%20Line%3A%20LLM%20Agent%20for%20Real-World%20OSS%20Vulnerability%20Localization%0AAuthor%3A%20Haoran%20Xi%20and%20Minghao%20Shao%20and%20Brendan%20Dolan-Gavitt%20and%20Muhammad%20Shafique%20and%20Ramesh%20Karri%0AAbstract%3A%20Large%20language%20models%20show%20promise%20for%20vulnerability%20discovery%2C%20yet%20prevailing%20methods%20inspect%20code%20in%20isolation%2C%20struggle%20with%20long%20contexts%2C%20and%20focus%20on%20coarse%20function%20or%20file%20level%20detections%20which%20offers%20limited%20actionable%20guidance%20to%20engineers%20who%20need%20precise%20line-level%20localization%20and%20targeted%20patches%20in%20real-world%20software%20development.%20We%20present%20T2L-Agent%20%28Trace-to-Line%20Agent%29%2C%20a%20project-level%2C%20end-to-end%20framework%20that%20plans%20its%20own%20analysis%20and%20progressively%20narrows%20scope%20from%20modules%20to%20exact%20vulnerable%20lines.%20T2L-Agent%20couples%20multi-round%20feedback%20with%20an%20Agentic%20Trace%20Analyzer%20%28ATA%29%20that%20fuses%20run-time%20evidence%20such%20as%20crash%20points%2C%20stack%20traces%2C%20and%20coverage%20deltas%20with%20AST-based%20code%20chunking%2C%20enabling%20iterative%20refinement%20beyond%20single%20pass%20predictions%20and%20translating%20symptoms%20into%20actionable%2C%20line-level%20diagnoses.%20To%20benchmark%20line-level%20vulnerability%20discovery%2C%20we%20introduce%20T2L-ARVO%2C%20a%20diverse%2C%20expert-verified%2050-case%20benchmark%20spanning%20five%20crash%20families%20and%20real-world%20projects.%20T2L-ARVO%20is%20specifically%20designed%20to%20support%20both%20coarse-grained%20detection%20and%20fine-grained%20localization%2C%20enabling%20rigorous%20evaluation%20of%20systems%20that%20aim%20to%20move%20beyond%20file-level%20predictions.%20On%20T2L-ARVO%2C%20T2L-Agent%20achieves%20up%20to%2058.0%25%20detection%20and%2054.8%25%20line-level%20localization%2C%20substantially%20outperforming%20baselines.%20Together%2C%20the%20framework%20and%20benchmark%20push%20LLM-based%20vulnerability%20detection%20from%20coarse%20identification%20toward%20deployable%2C%20robust%2C%20precision%20diagnostics%20that%20reduce%20noise%20and%20accelerate%20patching%20in%20open-source%20software%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Trace%2520to%2520Line%253A%2520LLM%2520Agent%2520for%2520Real-World%2520OSS%2520Vulnerability%2520Localization%26entry.906535625%3DHaoran%2520Xi%2520and%2520Minghao%2520Shao%2520and%2520Brendan%2520Dolan-Gavitt%2520and%2520Muhammad%2520Shafique%2520and%2520Ramesh%2520Karri%26entry.1292438233%3DLarge%2520language%2520models%2520show%2520promise%2520for%2520vulnerability%2520discovery%252C%2520yet%2520prevailing%2520methods%2520inspect%2520code%2520in%2520isolation%252C%2520struggle%2520with%2520long%2520contexts%252C%2520and%2520focus%2520on%2520coarse%2520function%2520or%2520file%2520level%2520detections%2520which%2520offers%2520limited%2520actionable%2520guidance%2520to%2520engineers%2520who%2520need%2520precise%2520line-level%2520localization%2520and%2520targeted%2520patches%2520in%2520real-world%2520software%2520development.%2520We%2520present%2520T2L-Agent%2520%2528Trace-to-Line%2520Agent%2529%252C%2520a%2520project-level%252C%2520end-to-end%2520framework%2520that%2520plans%2520its%2520own%2520analysis%2520and%2520progressively%2520narrows%2520scope%2520from%2520modules%2520to%2520exact%2520vulnerable%2520lines.%2520T2L-Agent%2520couples%2520multi-round%2520feedback%2520with%2520an%2520Agentic%2520Trace%2520Analyzer%2520%2528ATA%2529%2520that%2520fuses%2520run-time%2520evidence%2520such%2520as%2520crash%2520points%252C%2520stack%2520traces%252C%2520and%2520coverage%2520deltas%2520with%2520AST-based%2520code%2520chunking%252C%2520enabling%2520iterative%2520refinement%2520beyond%2520single%2520pass%2520predictions%2520and%2520translating%2520symptoms%2520into%2520actionable%252C%2520line-level%2520diagnoses.%2520To%2520benchmark%2520line-level%2520vulnerability%2520discovery%252C%2520we%2520introduce%2520T2L-ARVO%252C%2520a%2520diverse%252C%2520expert-verified%252050-case%2520benchmark%2520spanning%2520five%2520crash%2520families%2520and%2520real-world%2520projects.%2520T2L-ARVO%2520is%2520specifically%2520designed%2520to%2520support%2520both%2520coarse-grained%2520detection%2520and%2520fine-grained%2520localization%252C%2520enabling%2520rigorous%2520evaluation%2520of%2520systems%2520that%2520aim%2520to%2520move%2520beyond%2520file-level%2520predictions.%2520On%2520T2L-ARVO%252C%2520T2L-Agent%2520achieves%2520up%2520to%252058.0%2525%2520detection%2520and%252054.8%2525%2520line-level%2520localization%252C%2520substantially%2520outperforming%2520baselines.%2520Together%252C%2520the%2520framework%2520and%2520benchmark%2520push%2520LLM-based%2520vulnerability%2520detection%2520from%2520coarse%2520identification%2520toward%2520deployable%252C%2520robust%252C%2520precision%2520diagnostics%2520that%2520reduce%2520noise%2520and%2520accelerate%2520patching%2520in%2520open-source%2520software%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Trace%20to%20Line%3A%20LLM%20Agent%20for%20Real-World%20OSS%20Vulnerability%20Localization&entry.906535625=Haoran%20Xi%20and%20Minghao%20Shao%20and%20Brendan%20Dolan-Gavitt%20and%20Muhammad%20Shafique%20and%20Ramesh%20Karri&entry.1292438233=Large%20language%20models%20show%20promise%20for%20vulnerability%20discovery%2C%20yet%20prevailing%20methods%20inspect%20code%20in%20isolation%2C%20struggle%20with%20long%20contexts%2C%20and%20focus%20on%20coarse%20function%20or%20file%20level%20detections%20which%20offers%20limited%20actionable%20guidance%20to%20engineers%20who%20need%20precise%20line-level%20localization%20and%20targeted%20patches%20in%20real-world%20software%20development.%20We%20present%20T2L-Agent%20%28Trace-to-Line%20Agent%29%2C%20a%20project-level%2C%20end-to-end%20framework%20that%20plans%20its%20own%20analysis%20and%20progressively%20narrows%20scope%20from%20modules%20to%20exact%20vulnerable%20lines.%20T2L-Agent%20couples%20multi-round%20feedback%20with%20an%20Agentic%20Trace%20Analyzer%20%28ATA%29%20that%20fuses%20run-time%20evidence%20such%20as%20crash%20points%2C%20stack%20traces%2C%20and%20coverage%20deltas%20with%20AST-based%20code%20chunking%2C%20enabling%20iterative%20refinement%20beyond%20single%20pass%20predictions%20and%20translating%20symptoms%20into%20actionable%2C%20line-level%20diagnoses.%20To%20benchmark%20line-level%20vulnerability%20discovery%2C%20we%20introduce%20T2L-ARVO%2C%20a%20diverse%2C%20expert-verified%2050-case%20benchmark%20spanning%20five%20crash%20families%20and%20real-world%20projects.%20T2L-ARVO%20is%20specifically%20designed%20to%20support%20both%20coarse-grained%20detection%20and%20fine-grained%20localization%2C%20enabling%20rigorous%20evaluation%20of%20systems%20that%20aim%20to%20move%20beyond%20file-level%20predictions.%20On%20T2L-ARVO%2C%20T2L-Agent%20achieves%20up%20to%2058.0%25%20detection%20and%2054.8%25%20line-level%20localization%2C%20substantially%20outperforming%20baselines.%20Together%2C%20the%20framework%20and%20benchmark%20push%20LLM-based%20vulnerability%20detection%20from%20coarse%20identification%20toward%20deployable%2C%20robust%2C%20precision%20diagnostics%20that%20reduce%20noise%20and%20accelerate%20patching%20in%20open-source%20software%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2510.02389v2&entry.124074799=Read"},
{"title": "Learning without training: The implicit dynamics of in-context learning", "author": "Benoit Dherin and Michael Munn and Hanna Mazzawi and Michael Wunder and Javier Gonzalvo", "abstract": "One of the most striking features of Large Language Models (LLMs) is their ability to learn in-context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in-context and not only during training. Specifically, we show how a transformer block implicitly transforms a context into a low-rank weight-update of its MLP layer.", "link": "http://arxiv.org/abs/2507.16003v2", "date": "2025-12-17", "relevancy": 2.3249, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20without%20training%3A%20The%20implicit%20dynamics%20of%20in-context%20learning&body=Title%3A%20Learning%20without%20training%3A%20The%20implicit%20dynamics%20of%20in-context%20learning%0AAuthor%3A%20Benoit%20Dherin%20and%20Michael%20Munn%20and%20Hanna%20Mazzawi%20and%20Michael%20Wunder%20and%20Javier%20Gonzalvo%0AAbstract%3A%20One%20of%20the%20most%20striking%20features%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20their%20ability%20to%20learn%20in-context.%20Namely%20at%20inference%20time%20an%20LLM%20is%20able%20to%20learn%20new%20patterns%20without%20any%20additional%20weight%20update%20when%20these%20patterns%20are%20presented%20in%20the%20form%20of%20examples%20in%20the%20prompt%2C%20even%20if%20these%20patterns%20were%20not%20seen%20during%20training.%20The%20mechanisms%20through%20which%20this%20can%20happen%20are%20still%20largely%20unknown.%20In%20this%20work%2C%20we%20show%20that%20the%20stacking%20of%20a%20self-attention%20layer%20with%20an%20MLP%2C%20allows%20the%20transformer%20block%20to%20implicitly%20modify%20the%20weights%20of%20the%20MLP%20layer%20according%20to%20the%20context.%20We%20argue%20through%20theory%20and%20experimentation%20that%20this%20simple%20mechanism%20may%20be%20the%20reason%20why%20LLMs%20can%20learn%20in-context%20and%20not%20only%20during%20training.%20Specifically%2C%20we%20show%20how%20a%20transformer%20block%20implicitly%20transforms%20a%20context%20into%20a%20low-rank%20weight-update%20of%20its%20MLP%20layer.%0ALink%3A%20http%3A//arxiv.org/abs/2507.16003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520without%2520training%253A%2520The%2520implicit%2520dynamics%2520of%2520in-context%2520learning%26entry.906535625%3DBenoit%2520Dherin%2520and%2520Michael%2520Munn%2520and%2520Hanna%2520Mazzawi%2520and%2520Michael%2520Wunder%2520and%2520Javier%2520Gonzalvo%26entry.1292438233%3DOne%2520of%2520the%2520most%2520striking%2520features%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520their%2520ability%2520to%2520learn%2520in-context.%2520Namely%2520at%2520inference%2520time%2520an%2520LLM%2520is%2520able%2520to%2520learn%2520new%2520patterns%2520without%2520any%2520additional%2520weight%2520update%2520when%2520these%2520patterns%2520are%2520presented%2520in%2520the%2520form%2520of%2520examples%2520in%2520the%2520prompt%252C%2520even%2520if%2520these%2520patterns%2520were%2520not%2520seen%2520during%2520training.%2520The%2520mechanisms%2520through%2520which%2520this%2520can%2520happen%2520are%2520still%2520largely%2520unknown.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%2520stacking%2520of%2520a%2520self-attention%2520layer%2520with%2520an%2520MLP%252C%2520allows%2520the%2520transformer%2520block%2520to%2520implicitly%2520modify%2520the%2520weights%2520of%2520the%2520MLP%2520layer%2520according%2520to%2520the%2520context.%2520We%2520argue%2520through%2520theory%2520and%2520experimentation%2520that%2520this%2520simple%2520mechanism%2520may%2520be%2520the%2520reason%2520why%2520LLMs%2520can%2520learn%2520in-context%2520and%2520not%2520only%2520during%2520training.%2520Specifically%252C%2520we%2520show%2520how%2520a%2520transformer%2520block%2520implicitly%2520transforms%2520a%2520context%2520into%2520a%2520low-rank%2520weight-update%2520of%2520its%2520MLP%2520layer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20without%20training%3A%20The%20implicit%20dynamics%20of%20in-context%20learning&entry.906535625=Benoit%20Dherin%20and%20Michael%20Munn%20and%20Hanna%20Mazzawi%20and%20Michael%20Wunder%20and%20Javier%20Gonzalvo&entry.1292438233=One%20of%20the%20most%20striking%20features%20of%20Large%20Language%20Models%20%28LLMs%29%20is%20their%20ability%20to%20learn%20in-context.%20Namely%20at%20inference%20time%20an%20LLM%20is%20able%20to%20learn%20new%20patterns%20without%20any%20additional%20weight%20update%20when%20these%20patterns%20are%20presented%20in%20the%20form%20of%20examples%20in%20the%20prompt%2C%20even%20if%20these%20patterns%20were%20not%20seen%20during%20training.%20The%20mechanisms%20through%20which%20this%20can%20happen%20are%20still%20largely%20unknown.%20In%20this%20work%2C%20we%20show%20that%20the%20stacking%20of%20a%20self-attention%20layer%20with%20an%20MLP%2C%20allows%20the%20transformer%20block%20to%20implicitly%20modify%20the%20weights%20of%20the%20MLP%20layer%20according%20to%20the%20context.%20We%20argue%20through%20theory%20and%20experimentation%20that%20this%20simple%20mechanism%20may%20be%20the%20reason%20why%20LLMs%20can%20learn%20in-context%20and%20not%20only%20during%20training.%20Specifically%2C%20we%20show%20how%20a%20transformer%20block%20implicitly%20transforms%20a%20context%20into%20a%20low-rank%20weight-update%20of%20its%20MLP%20layer.&entry.1838667208=http%3A//arxiv.org/abs/2507.16003v2&entry.124074799=Read"},
{"title": "Multi-Modal Semantic Communication", "author": "Matin Mortaheb and Erciyes Karakaya and Sennur Ulukus", "abstract": "Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.", "link": "http://arxiv.org/abs/2512.15691v1", "date": "2025-12-17", "relevancy": 2.2989, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Semantic%20Communication&body=Title%3A%20Multi-Modal%20Semantic%20Communication%0AAuthor%3A%20Matin%20Mortaheb%20and%20Erciyes%20Karakaya%20and%20Sennur%20Ulukus%0AAbstract%3A%20Semantic%20communication%20aims%20to%20transmit%20information%20most%20relevant%20to%20a%20task%20rather%20than%20raw%20data%2C%20offering%20significant%20gains%20in%20communication%20efficiency%20for%20applications%20such%20as%20telepresence%2C%20augmented%20reality%2C%20and%20remote%20sensing.%20Recent%20transformer-based%20approaches%20have%20used%20self-attention%20maps%20to%20identify%20informative%20regions%20within%20images%2C%20but%20they%20often%20struggle%20in%20complex%20scenes%20with%20multiple%20objects%2C%20where%20self-attention%20lacks%20explicit%20task%20guidance.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Multi-Modal%20Semantic%20Communication%20framework%20that%20integrates%20text-based%20user%20queries%20to%20guide%20the%20information%20extraction%20process.%20Our%20proposed%20system%20employs%20a%20cross-modal%20attention%20mechanism%20that%20fuses%20visual%20features%20with%20language%20embeddings%20to%20produce%20soft%20relevance%20scores%20over%20the%20visual%20data.%20Based%20on%20these%20scores%20and%20the%20instantaneous%20channel%20bandwidth%2C%20we%20use%20an%20algorithm%20to%20transmit%20image%20patches%20at%20adaptive%20resolutions%20using%20independently%20trained%20encoder-decoder%20pairs%2C%20with%20total%20bitrate%20matching%20the%20channel%20capacity.%20At%20the%20receiver%2C%20the%20patches%20are%20reconstructed%20and%20combined%20to%20preserve%20task-critical%20information.%20This%20flexible%20and%20goal-driven%20design%20enables%20efficient%20semantic%20communication%20in%20complex%20and%20bandwidth-constrained%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Semantic%2520Communication%26entry.906535625%3DMatin%2520Mortaheb%2520and%2520Erciyes%2520Karakaya%2520and%2520Sennur%2520Ulukus%26entry.1292438233%3DSemantic%2520communication%2520aims%2520to%2520transmit%2520information%2520most%2520relevant%2520to%2520a%2520task%2520rather%2520than%2520raw%2520data%252C%2520offering%2520significant%2520gains%2520in%2520communication%2520efficiency%2520for%2520applications%2520such%2520as%2520telepresence%252C%2520augmented%2520reality%252C%2520and%2520remote%2520sensing.%2520Recent%2520transformer-based%2520approaches%2520have%2520used%2520self-attention%2520maps%2520to%2520identify%2520informative%2520regions%2520within%2520images%252C%2520but%2520they%2520often%2520struggle%2520in%2520complex%2520scenes%2520with%2520multiple%2520objects%252C%2520where%2520self-attention%2520lacks%2520explicit%2520task%2520guidance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520Multi-Modal%2520Semantic%2520Communication%2520framework%2520that%2520integrates%2520text-based%2520user%2520queries%2520to%2520guide%2520the%2520information%2520extraction%2520process.%2520Our%2520proposed%2520system%2520employs%2520a%2520cross-modal%2520attention%2520mechanism%2520that%2520fuses%2520visual%2520features%2520with%2520language%2520embeddings%2520to%2520produce%2520soft%2520relevance%2520scores%2520over%2520the%2520visual%2520data.%2520Based%2520on%2520these%2520scores%2520and%2520the%2520instantaneous%2520channel%2520bandwidth%252C%2520we%2520use%2520an%2520algorithm%2520to%2520transmit%2520image%2520patches%2520at%2520adaptive%2520resolutions%2520using%2520independently%2520trained%2520encoder-decoder%2520pairs%252C%2520with%2520total%2520bitrate%2520matching%2520the%2520channel%2520capacity.%2520At%2520the%2520receiver%252C%2520the%2520patches%2520are%2520reconstructed%2520and%2520combined%2520to%2520preserve%2520task-critical%2520information.%2520This%2520flexible%2520and%2520goal-driven%2520design%2520enables%2520efficient%2520semantic%2520communication%2520in%2520complex%2520and%2520bandwidth-constrained%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Semantic%20Communication&entry.906535625=Matin%20Mortaheb%20and%20Erciyes%20Karakaya%20and%20Sennur%20Ulukus&entry.1292438233=Semantic%20communication%20aims%20to%20transmit%20information%20most%20relevant%20to%20a%20task%20rather%20than%20raw%20data%2C%20offering%20significant%20gains%20in%20communication%20efficiency%20for%20applications%20such%20as%20telepresence%2C%20augmented%20reality%2C%20and%20remote%20sensing.%20Recent%20transformer-based%20approaches%20have%20used%20self-attention%20maps%20to%20identify%20informative%20regions%20within%20images%2C%20but%20they%20often%20struggle%20in%20complex%20scenes%20with%20multiple%20objects%2C%20where%20self-attention%20lacks%20explicit%20task%20guidance.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Multi-Modal%20Semantic%20Communication%20framework%20that%20integrates%20text-based%20user%20queries%20to%20guide%20the%20information%20extraction%20process.%20Our%20proposed%20system%20employs%20a%20cross-modal%20attention%20mechanism%20that%20fuses%20visual%20features%20with%20language%20embeddings%20to%20produce%20soft%20relevance%20scores%20over%20the%20visual%20data.%20Based%20on%20these%20scores%20and%20the%20instantaneous%20channel%20bandwidth%2C%20we%20use%20an%20algorithm%20to%20transmit%20image%20patches%20at%20adaptive%20resolutions%20using%20independently%20trained%20encoder-decoder%20pairs%2C%20with%20total%20bitrate%20matching%20the%20channel%20capacity.%20At%20the%20receiver%2C%20the%20patches%20are%20reconstructed%20and%20combined%20to%20preserve%20task-critical%20information.%20This%20flexible%20and%20goal-driven%20design%20enables%20efficient%20semantic%20communication%20in%20complex%20and%20bandwidth-constrained%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.15691v1&entry.124074799=Read"},
{"title": "Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping", "author": "Adelina Giurea and Stijn Luchie and Dieter Coppens and Jeroen Hoebeke and Eli De Poorter", "abstract": "This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform. Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces. In these visioned-impaired conditions, UWB radar offers a promising alternative. To this end, this work explores the suitability of robot-mounted UWB radar for environmental mapping in dynamic, anchor-free scenarios. The study investigates how different materials (metal, concrete and plywood) and UWB radio channels (5 and 9) influence the Channel Impulse Response (CIR). Furthermore, a processing pipeline is proposed to achieve reliable mapping of detected obstacles, consisting of 3 steps: (i) target identification (based on CIR peak detection), (ii) filtering (based on peak properties, signal-to-noise score, and phase-difference of arrival), and (iii) clustering (based on distance estimation and angle-of-arrival estimation). The proposed approach successfully reduces noise and multipath effects, resulting in an obstacle detection precision of at least 90.71% and a recall of 88.40% on channel 9 even when detecting low-reflective materials such as concrete. This work offers a foundation for further development of UWB-based localisation and mapping (SLAM) systems that do not rely on visual features and, unlike conventional UWB localisation systems, do not require on fixed anchor nodes for triangulation.", "link": "http://arxiv.org/abs/2512.01018v2", "date": "2025-12-17", "relevancy": 2.2873, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5659}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20UWB%20Radar%20on%20Mobile%20Robots%20for%20Continuous%20Obstacle%20and%20Environment%20Mapping&body=Title%3A%20Integration%20of%20UWB%20Radar%20on%20Mobile%20Robots%20for%20Continuous%20Obstacle%20and%20Environment%20Mapping%0AAuthor%3A%20Adelina%20Giurea%20and%20Stijn%20Luchie%20and%20Dieter%20Coppens%20and%20Jeroen%20Hoebeke%20and%20Eli%20De%20Poorter%0AAbstract%3A%20This%20paper%20presents%20an%20infrastructure-free%20approach%20for%20obstacle%20detection%20and%20environmental%20mapping%20using%20ultra-wideband%20%28UWB%29%20radar%20mounted%20on%20a%20mobile%20robotic%20platform.%20Traditional%20sensing%20modalities%20such%20as%20visual%20cameras%20and%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20fail%20in%20environments%20with%20poor%20visibility%20due%20to%20darkness%2C%20smoke%2C%20or%20reflective%20surfaces.%20In%20these%20visioned-impaired%20conditions%2C%20UWB%20radar%20offers%20a%20promising%20alternative.%20To%20this%20end%2C%20this%20work%20explores%20the%20suitability%20of%20robot-mounted%20UWB%20radar%20for%20environmental%20mapping%20in%20dynamic%2C%20anchor-free%20scenarios.%20The%20study%20investigates%20how%20different%20materials%20%28metal%2C%20concrete%20and%20plywood%29%20and%20UWB%20radio%20channels%20%285%20and%209%29%20influence%20the%20Channel%20Impulse%20Response%20%28CIR%29.%20Furthermore%2C%20a%20processing%20pipeline%20is%20proposed%20to%20achieve%20reliable%20mapping%20of%20detected%20obstacles%2C%20consisting%20of%203%20steps%3A%20%28i%29%20target%20identification%20%28based%20on%20CIR%20peak%20detection%29%2C%20%28ii%29%20filtering%20%28based%20on%20peak%20properties%2C%20signal-to-noise%20score%2C%20and%20phase-difference%20of%20arrival%29%2C%20and%20%28iii%29%20clustering%20%28based%20on%20distance%20estimation%20and%20angle-of-arrival%20estimation%29.%20The%20proposed%20approach%20successfully%20reduces%20noise%20and%20multipath%20effects%2C%20resulting%20in%20an%20obstacle%20detection%20precision%20of%20at%20least%2090.71%25%20and%20a%20recall%20of%2088.40%25%20on%20channel%209%20even%20when%20detecting%20low-reflective%20materials%20such%20as%20concrete.%20This%20work%20offers%20a%20foundation%20for%20further%20development%20of%20UWB-based%20localisation%20and%20mapping%20%28SLAM%29%20systems%20that%20do%20not%20rely%20on%20visual%20features%20and%2C%20unlike%20conventional%20UWB%20localisation%20systems%2C%20do%20not%20require%20on%20fixed%20anchor%20nodes%20for%20triangulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520UWB%2520Radar%2520on%2520Mobile%2520Robots%2520for%2520Continuous%2520Obstacle%2520and%2520Environment%2520Mapping%26entry.906535625%3DAdelina%2520Giurea%2520and%2520Stijn%2520Luchie%2520and%2520Dieter%2520Coppens%2520and%2520Jeroen%2520Hoebeke%2520and%2520Eli%2520De%2520Poorter%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520infrastructure-free%2520approach%2520for%2520obstacle%2520detection%2520and%2520environmental%2520mapping%2520using%2520ultra-wideband%2520%2528UWB%2529%2520radar%2520mounted%2520on%2520a%2520mobile%2520robotic%2520platform.%2520Traditional%2520sensing%2520modalities%2520such%2520as%2520visual%2520cameras%2520and%2520Light%2520Detection%2520and%2520Ranging%2520%2528LiDAR%2529%2520fail%2520in%2520environments%2520with%2520poor%2520visibility%2520due%2520to%2520darkness%252C%2520smoke%252C%2520or%2520reflective%2520surfaces.%2520In%2520these%2520visioned-impaired%2520conditions%252C%2520UWB%2520radar%2520offers%2520a%2520promising%2520alternative.%2520To%2520this%2520end%252C%2520this%2520work%2520explores%2520the%2520suitability%2520of%2520robot-mounted%2520UWB%2520radar%2520for%2520environmental%2520mapping%2520in%2520dynamic%252C%2520anchor-free%2520scenarios.%2520The%2520study%2520investigates%2520how%2520different%2520materials%2520%2528metal%252C%2520concrete%2520and%2520plywood%2529%2520and%2520UWB%2520radio%2520channels%2520%25285%2520and%25209%2529%2520influence%2520the%2520Channel%2520Impulse%2520Response%2520%2528CIR%2529.%2520Furthermore%252C%2520a%2520processing%2520pipeline%2520is%2520proposed%2520to%2520achieve%2520reliable%2520mapping%2520of%2520detected%2520obstacles%252C%2520consisting%2520of%25203%2520steps%253A%2520%2528i%2529%2520target%2520identification%2520%2528based%2520on%2520CIR%2520peak%2520detection%2529%252C%2520%2528ii%2529%2520filtering%2520%2528based%2520on%2520peak%2520properties%252C%2520signal-to-noise%2520score%252C%2520and%2520phase-difference%2520of%2520arrival%2529%252C%2520and%2520%2528iii%2529%2520clustering%2520%2528based%2520on%2520distance%2520estimation%2520and%2520angle-of-arrival%2520estimation%2529.%2520The%2520proposed%2520approach%2520successfully%2520reduces%2520noise%2520and%2520multipath%2520effects%252C%2520resulting%2520in%2520an%2520obstacle%2520detection%2520precision%2520of%2520at%2520least%252090.71%2525%2520and%2520a%2520recall%2520of%252088.40%2525%2520on%2520channel%25209%2520even%2520when%2520detecting%2520low-reflective%2520materials%2520such%2520as%2520concrete.%2520This%2520work%2520offers%2520a%2520foundation%2520for%2520further%2520development%2520of%2520UWB-based%2520localisation%2520and%2520mapping%2520%2528SLAM%2529%2520systems%2520that%2520do%2520not%2520rely%2520on%2520visual%2520features%2520and%252C%2520unlike%2520conventional%2520UWB%2520localisation%2520systems%252C%2520do%2520not%2520require%2520on%2520fixed%2520anchor%2520nodes%2520for%2520triangulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20UWB%20Radar%20on%20Mobile%20Robots%20for%20Continuous%20Obstacle%20and%20Environment%20Mapping&entry.906535625=Adelina%20Giurea%20and%20Stijn%20Luchie%20and%20Dieter%20Coppens%20and%20Jeroen%20Hoebeke%20and%20Eli%20De%20Poorter&entry.1292438233=This%20paper%20presents%20an%20infrastructure-free%20approach%20for%20obstacle%20detection%20and%20environmental%20mapping%20using%20ultra-wideband%20%28UWB%29%20radar%20mounted%20on%20a%20mobile%20robotic%20platform.%20Traditional%20sensing%20modalities%20such%20as%20visual%20cameras%20and%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20fail%20in%20environments%20with%20poor%20visibility%20due%20to%20darkness%2C%20smoke%2C%20or%20reflective%20surfaces.%20In%20these%20visioned-impaired%20conditions%2C%20UWB%20radar%20offers%20a%20promising%20alternative.%20To%20this%20end%2C%20this%20work%20explores%20the%20suitability%20of%20robot-mounted%20UWB%20radar%20for%20environmental%20mapping%20in%20dynamic%2C%20anchor-free%20scenarios.%20The%20study%20investigates%20how%20different%20materials%20%28metal%2C%20concrete%20and%20plywood%29%20and%20UWB%20radio%20channels%20%285%20and%209%29%20influence%20the%20Channel%20Impulse%20Response%20%28CIR%29.%20Furthermore%2C%20a%20processing%20pipeline%20is%20proposed%20to%20achieve%20reliable%20mapping%20of%20detected%20obstacles%2C%20consisting%20of%203%20steps%3A%20%28i%29%20target%20identification%20%28based%20on%20CIR%20peak%20detection%29%2C%20%28ii%29%20filtering%20%28based%20on%20peak%20properties%2C%20signal-to-noise%20score%2C%20and%20phase-difference%20of%20arrival%29%2C%20and%20%28iii%29%20clustering%20%28based%20on%20distance%20estimation%20and%20angle-of-arrival%20estimation%29.%20The%20proposed%20approach%20successfully%20reduces%20noise%20and%20multipath%20effects%2C%20resulting%20in%20an%20obstacle%20detection%20precision%20of%20at%20least%2090.71%25%20and%20a%20recall%20of%2088.40%25%20on%20channel%209%20even%20when%20detecting%20low-reflective%20materials%20such%20as%20concrete.%20This%20work%20offers%20a%20foundation%20for%20further%20development%20of%20UWB-based%20localisation%20and%20mapping%20%28SLAM%29%20systems%20that%20do%20not%20rely%20on%20visual%20features%20and%2C%20unlike%20conventional%20UWB%20localisation%20systems%2C%20do%20not%20require%20on%20fixed%20anchor%20nodes%20for%20triangulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.01018v2&entry.124074799=Read"},
{"title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments", "author": "Yuze Wu and Mo Zhu and Xingxing Li and Yuheng Du and Yuxin Fan and Wenjun Li and Xin Zhou and Fei Gao", "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.", "link": "http://arxiv.org/abs/2512.15258v1", "date": "2025-12-17", "relevancy": 2.2628, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.564}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments&body=Title%3A%20VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments%0AAuthor%3A%20Yuze%20Wu%20and%20Mo%20Zhu%20and%20Xingxing%20Li%20and%20Yuheng%20Du%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Xin%20Zhou%20and%20Fei%20Gao%0AAbstract%3A%20This%20paper%20proposes%20VLA-AN%2C%20an%20efficient%20and%20onboard%20Vision-Language-Action%20%28VLA%29%20framework%20dedicated%20to%20autonomous%20drone%20navigation%20in%20complex%20environments.%20VLA-AN%20addresses%20four%20major%20limitations%20of%20existing%20large%20aerial%20navigation%20models%3A%20the%20data%20domain%20gap%2C%20insufficient%20temporal%20navigation%20with%20reasoning%2C%20safety%20issues%20with%20generative%20action%20policies%2C%20and%20onboard%20deployment%20constraints.%20First%2C%20we%20construct%20a%20high-fidelity%20dataset%20utilizing%203D%20Gaussian%20Splatting%20%283D-GS%29%20to%20effectively%20bridge%20the%20domain%20gap.%20Second%2C%20we%20introduce%20a%20progressive%20three-stage%20training%20framework%20that%20sequentially%20reinforces%20scene%20comprehension%2C%20core%20flight%20skills%2C%20and%20complex%20navigation%20capabilities.%20Third%2C%20we%20design%20a%20lightweight%2C%20real-time%20action%20module%20coupled%20with%20geometric%20safety%20correction.%20This%20module%20ensures%20fast%2C%20collision-free%2C%20and%20stable%20command%20generation%2C%20mitigating%20the%20safety%20risks%20inherent%20in%20stochastic%20generative%20policies.%20Finally%2C%20through%20deep%20optimization%20of%20the%20onboard%20deployment%20pipeline%2C%20VLA-AN%20achieves%20a%20robust%20real-time%208.3x%20improvement%20in%20inference%20throughput%20on%20resource-constrained%20UAVs.%20Extensive%20experiments%20demonstrate%20that%20VLA-AN%20significantly%20improves%20spatial%20grounding%2C%20scene%20reasoning%2C%20and%20long-horizon%20navigation%2C%20achieving%20a%20maximum%20single-task%20success%20rate%20of%2098.1%25%2C%20and%20providing%20an%20efficient%2C%20practical%20solution%20for%20realizing%20full-chain%20closed-loop%20autonomy%20in%20lightweight%20aerial%20robots.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-AN%253A%2520An%2520Efficient%2520and%2520Onboard%2520Vision-Language-Action%2520Framework%2520for%2520Aerial%2520Navigation%2520in%2520Complex%2520Environments%26entry.906535625%3DYuze%2520Wu%2520and%2520Mo%2520Zhu%2520and%2520Xingxing%2520Li%2520and%2520Yuheng%2520Du%2520and%2520Yuxin%2520Fan%2520and%2520Wenjun%2520Li%2520and%2520Xin%2520Zhou%2520and%2520Fei%2520Gao%26entry.1292438233%3DThis%2520paper%2520proposes%2520VLA-AN%252C%2520an%2520efficient%2520and%2520onboard%2520Vision-Language-Action%2520%2528VLA%2529%2520framework%2520dedicated%2520to%2520autonomous%2520drone%2520navigation%2520in%2520complex%2520environments.%2520VLA-AN%2520addresses%2520four%2520major%2520limitations%2520of%2520existing%2520large%2520aerial%2520navigation%2520models%253A%2520the%2520data%2520domain%2520gap%252C%2520insufficient%2520temporal%2520navigation%2520with%2520reasoning%252C%2520safety%2520issues%2520with%2520generative%2520action%2520policies%252C%2520and%2520onboard%2520deployment%2520constraints.%2520First%252C%2520we%2520construct%2520a%2520high-fidelity%2520dataset%2520utilizing%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520to%2520effectively%2520bridge%2520the%2520domain%2520gap.%2520Second%252C%2520we%2520introduce%2520a%2520progressive%2520three-stage%2520training%2520framework%2520that%2520sequentially%2520reinforces%2520scene%2520comprehension%252C%2520core%2520flight%2520skills%252C%2520and%2520complex%2520navigation%2520capabilities.%2520Third%252C%2520we%2520design%2520a%2520lightweight%252C%2520real-time%2520action%2520module%2520coupled%2520with%2520geometric%2520safety%2520correction.%2520This%2520module%2520ensures%2520fast%252C%2520collision-free%252C%2520and%2520stable%2520command%2520generation%252C%2520mitigating%2520the%2520safety%2520risks%2520inherent%2520in%2520stochastic%2520generative%2520policies.%2520Finally%252C%2520through%2520deep%2520optimization%2520of%2520the%2520onboard%2520deployment%2520pipeline%252C%2520VLA-AN%2520achieves%2520a%2520robust%2520real-time%25208.3x%2520improvement%2520in%2520inference%2520throughput%2520on%2520resource-constrained%2520UAVs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520VLA-AN%2520significantly%2520improves%2520spatial%2520grounding%252C%2520scene%2520reasoning%252C%2520and%2520long-horizon%2520navigation%252C%2520achieving%2520a%2520maximum%2520single-task%2520success%2520rate%2520of%252098.1%2525%252C%2520and%2520providing%2520an%2520efficient%252C%2520practical%2520solution%2520for%2520realizing%2520full-chain%2520closed-loop%2520autonomy%2520in%2520lightweight%2520aerial%2520robots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-AN%3A%20An%20Efficient%20and%20Onboard%20Vision-Language-Action%20Framework%20for%20Aerial%20Navigation%20in%20Complex%20Environments&entry.906535625=Yuze%20Wu%20and%20Mo%20Zhu%20and%20Xingxing%20Li%20and%20Yuheng%20Du%20and%20Yuxin%20Fan%20and%20Wenjun%20Li%20and%20Xin%20Zhou%20and%20Fei%20Gao&entry.1292438233=This%20paper%20proposes%20VLA-AN%2C%20an%20efficient%20and%20onboard%20Vision-Language-Action%20%28VLA%29%20framework%20dedicated%20to%20autonomous%20drone%20navigation%20in%20complex%20environments.%20VLA-AN%20addresses%20four%20major%20limitations%20of%20existing%20large%20aerial%20navigation%20models%3A%20the%20data%20domain%20gap%2C%20insufficient%20temporal%20navigation%20with%20reasoning%2C%20safety%20issues%20with%20generative%20action%20policies%2C%20and%20onboard%20deployment%20constraints.%20First%2C%20we%20construct%20a%20high-fidelity%20dataset%20utilizing%203D%20Gaussian%20Splatting%20%283D-GS%29%20to%20effectively%20bridge%20the%20domain%20gap.%20Second%2C%20we%20introduce%20a%20progressive%20three-stage%20training%20framework%20that%20sequentially%20reinforces%20scene%20comprehension%2C%20core%20flight%20skills%2C%20and%20complex%20navigation%20capabilities.%20Third%2C%20we%20design%20a%20lightweight%2C%20real-time%20action%20module%20coupled%20with%20geometric%20safety%20correction.%20This%20module%20ensures%20fast%2C%20collision-free%2C%20and%20stable%20command%20generation%2C%20mitigating%20the%20safety%20risks%20inherent%20in%20stochastic%20generative%20policies.%20Finally%2C%20through%20deep%20optimization%20of%20the%20onboard%20deployment%20pipeline%2C%20VLA-AN%20achieves%20a%20robust%20real-time%208.3x%20improvement%20in%20inference%20throughput%20on%20resource-constrained%20UAVs.%20Extensive%20experiments%20demonstrate%20that%20VLA-AN%20significantly%20improves%20spatial%20grounding%2C%20scene%20reasoning%2C%20and%20long-horizon%20navigation%2C%20achieving%20a%20maximum%20single-task%20success%20rate%20of%2098.1%25%2C%20and%20providing%20an%20efficient%2C%20practical%20solution%20for%20realizing%20full-chain%20closed-loop%20autonomy%20in%20lightweight%20aerial%20robots.&entry.1838667208=http%3A//arxiv.org/abs/2512.15258v1&entry.124074799=Read"},
{"title": "Dynamical stability for dense patterns in discrete attractor neural networks", "author": "Uri Cohen and M\u00e1t\u00e9 Lengyel", "abstract": "Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and the outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \\textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.", "link": "http://arxiv.org/abs/2507.10383v3", "date": "2025-12-17", "relevancy": 2.2518, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4536}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%20networks&body=Title%3A%20Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%20networks%0AAuthor%3A%20Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel%0AAbstract%3A%20Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%20biological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%20only%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%20theory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%20networks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%20directly%20analyzing%20the%20bulk%20and%20the%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%20all%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%20classical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%20activities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%20function.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%20threshold-linear%20activation%20and%20sparse-like%20patterns.%0ALink%3A%20http%3A//arxiv.org/abs/2507.10383v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520stability%2520for%2520dense%2520patterns%2520in%2520discrete%2520attractor%2520neural%2520networks%26entry.906535625%3DUri%2520Cohen%2520and%2520M%25C3%25A1t%25C3%25A9%2520Lengyel%26entry.1292438233%3DNeural%2520networks%2520storing%2520multiple%2520discrete%2520attractors%2520are%2520canonical%2520models%2520of%2520biological%2520memory.%2520Previously%252C%2520the%2520dynamical%2520stability%2520of%2520such%2520networks%2520could%2520only%2520be%2520guaranteed%2520under%2520highly%2520restrictive%2520conditions.%2520Here%252C%2520we%2520derive%2520a%2520theory%2520of%2520the%2520local%2520stability%2520of%2520discrete%2520fixed%2520points%2520in%2520a%2520broad%2520class%2520of%2520networks%2520with%2520graded%2520neural%2520activities%2520and%2520in%2520the%2520presence%2520of%2520noise.%2520By%2520directly%2520analyzing%2520the%2520bulk%2520and%2520the%2520outliers%2520of%2520the%2520Jacobian%2520spectrum%252C%2520we%2520show%2520that%2520all%2520fixed%2520points%2520are%2520stable%2520below%2520a%2520critical%2520load%2520that%2520is%2520distinct%2520from%2520the%2520classical%2520%255Ctextit%257Bcritical%2520capacity%257D%2520and%2520depends%2520on%2520the%2520statistics%2520of%2520neural%2520activities%2520in%2520the%2520fixed%2520points%2520as%2520well%2520as%2520the%2520single-neuron%2520activation%2520function.%2520Our%2520analysis%2520highlights%2520the%2520computational%2520benefits%2520of%2520threshold-linear%2520activation%2520and%2520sparse-like%2520patterns.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10383v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20stability%20for%20dense%20patterns%20in%20discrete%20attractor%20neural%20networks&entry.906535625=Uri%20Cohen%20and%20M%C3%A1t%C3%A9%20Lengyel&entry.1292438233=Neural%20networks%20storing%20multiple%20discrete%20attractors%20are%20canonical%20models%20of%20biological%20memory.%20Previously%2C%20the%20dynamical%20stability%20of%20such%20networks%20could%20only%20be%20guaranteed%20under%20highly%20restrictive%20conditions.%20Here%2C%20we%20derive%20a%20theory%20of%20the%20local%20stability%20of%20discrete%20fixed%20points%20in%20a%20broad%20class%20of%20networks%20with%20graded%20neural%20activities%20and%20in%20the%20presence%20of%20noise.%20By%20directly%20analyzing%20the%20bulk%20and%20the%20outliers%20of%20the%20Jacobian%20spectrum%2C%20we%20show%20that%20all%20fixed%20points%20are%20stable%20below%20a%20critical%20load%20that%20is%20distinct%20from%20the%20classical%20%5Ctextit%7Bcritical%20capacity%7D%20and%20depends%20on%20the%20statistics%20of%20neural%20activities%20in%20the%20fixed%20points%20as%20well%20as%20the%20single-neuron%20activation%20function.%20Our%20analysis%20highlights%20the%20computational%20benefits%20of%20threshold-linear%20activation%20and%20sparse-like%20patterns.&entry.1838667208=http%3A//arxiv.org/abs/2507.10383v3&entry.124074799=Read"},
{"title": "MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement", "author": "Yingying Wang and Xuanhua He and Chen Wu and Jialing Huang and Suiyun Zhang and Rui Liu and Xinghao Ding and Haoxuan Che", "abstract": "Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.", "link": "http://arxiv.org/abs/2512.15261v1", "date": "2025-12-17", "relevancy": 2.2518, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.552}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMMamba%3A%20A%20Versatile%20Cross-Modal%20In%20Context%20Fusion%20Framework%20for%20Pan-Sharpening%20and%20Zero-Shot%20Image%20Enhancement&body=Title%3A%20MMMamba%3A%20A%20Versatile%20Cross-Modal%20In%20Context%20Fusion%20Framework%20for%20Pan-Sharpening%20and%20Zero-Shot%20Image%20Enhancement%0AAuthor%3A%20Yingying%20Wang%20and%20Xuanhua%20He%20and%20Chen%20Wu%20and%20Jialing%20Huang%20and%20Suiyun%20Zhang%20and%20Rui%20Liu%20and%20Xinghao%20Ding%20and%20Haoxuan%20Che%0AAbstract%3A%20Pan-sharpening%20aims%20to%20generate%20high-resolution%20multispectral%20%28HRMS%29%20images%20by%20integrating%20a%20high-resolution%20panchromatic%20%28PAN%29%20image%20with%20its%20corresponding%20low-resolution%20multispectral%20%28MS%29%20image.%20To%20achieve%20effective%20fusion%2C%20it%20is%20crucial%20to%20fully%20exploit%20the%20complementary%20information%20between%20the%20two%20modalities.%20Traditional%20CNN-based%20methods%20typically%20rely%20on%20channel-wise%20concatenation%20with%20fixed%20convolutional%20operators%2C%20which%20limits%20their%20adaptability%20to%20diverse%20spatial%20and%20spectral%20variations.%20While%20cross-attention%20mechanisms%20enable%20global%20interactions%2C%20they%20are%20computationally%20inefficient%20and%20may%20dilute%20fine-grained%20correspondences%2C%20making%20it%20difficult%20to%20capture%20complex%20semantic%20relationships.%20Recent%20advances%20in%20the%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%20architecture%20have%20demonstrated%20impressive%20success%20in%20image%20generation%20and%20editing%20tasks.%20Unlike%20cross-attention%2C%20MMDiT%20employs%20in-context%20conditioning%20to%20facilitate%20more%20direct%20and%20efficient%20cross-modal%20information%20exchange.%20In%20this%20paper%2C%20we%20propose%20MMMamba%2C%20a%20cross-modal%20in-context%20fusion%20framework%20for%20pan-sharpening%2C%20with%20the%20flexibility%20to%20support%20image%20super-resolution%20in%20a%20zero-shot%20manner.%20Built%20upon%20the%20Mamba%20architecture%2C%20our%20design%20ensures%20linear%20computational%20complexity%20while%20maintaining%20strong%20cross-modal%20interaction%20capacity.%20Furthermore%2C%20we%20introduce%20a%20novel%20multimodal%20interleaved%20%28MI%29%20scanning%20mechanism%20that%20facilitates%20effective%20information%20exchange%20between%20the%20PAN%20and%20MS%20modalities.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%20method%20compared%20to%20existing%20state-of-the-art%20%28SOTA%29%20techniques%20across%20multiple%20tasks%20and%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMMamba%253A%2520A%2520Versatile%2520Cross-Modal%2520In%2520Context%2520Fusion%2520Framework%2520for%2520Pan-Sharpening%2520and%2520Zero-Shot%2520Image%2520Enhancement%26entry.906535625%3DYingying%2520Wang%2520and%2520Xuanhua%2520He%2520and%2520Chen%2520Wu%2520and%2520Jialing%2520Huang%2520and%2520Suiyun%2520Zhang%2520and%2520Rui%2520Liu%2520and%2520Xinghao%2520Ding%2520and%2520Haoxuan%2520Che%26entry.1292438233%3DPan-sharpening%2520aims%2520to%2520generate%2520high-resolution%2520multispectral%2520%2528HRMS%2529%2520images%2520by%2520integrating%2520a%2520high-resolution%2520panchromatic%2520%2528PAN%2529%2520image%2520with%2520its%2520corresponding%2520low-resolution%2520multispectral%2520%2528MS%2529%2520image.%2520To%2520achieve%2520effective%2520fusion%252C%2520it%2520is%2520crucial%2520to%2520fully%2520exploit%2520the%2520complementary%2520information%2520between%2520the%2520two%2520modalities.%2520Traditional%2520CNN-based%2520methods%2520typically%2520rely%2520on%2520channel-wise%2520concatenation%2520with%2520fixed%2520convolutional%2520operators%252C%2520which%2520limits%2520their%2520adaptability%2520to%2520diverse%2520spatial%2520and%2520spectral%2520variations.%2520While%2520cross-attention%2520mechanisms%2520enable%2520global%2520interactions%252C%2520they%2520are%2520computationally%2520inefficient%2520and%2520may%2520dilute%2520fine-grained%2520correspondences%252C%2520making%2520it%2520difficult%2520to%2520capture%2520complex%2520semantic%2520relationships.%2520Recent%2520advances%2520in%2520the%2520Multimodal%2520Diffusion%2520Transformer%2520%2528MMDiT%2529%2520architecture%2520have%2520demonstrated%2520impressive%2520success%2520in%2520image%2520generation%2520and%2520editing%2520tasks.%2520Unlike%2520cross-attention%252C%2520MMDiT%2520employs%2520in-context%2520conditioning%2520to%2520facilitate%2520more%2520direct%2520and%2520efficient%2520cross-modal%2520information%2520exchange.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MMMamba%252C%2520a%2520cross-modal%2520in-context%2520fusion%2520framework%2520for%2520pan-sharpening%252C%2520with%2520the%2520flexibility%2520to%2520support%2520image%2520super-resolution%2520in%2520a%2520zero-shot%2520manner.%2520Built%2520upon%2520the%2520Mamba%2520architecture%252C%2520our%2520design%2520ensures%2520linear%2520computational%2520complexity%2520while%2520maintaining%2520strong%2520cross-modal%2520interaction%2520capacity.%2520Furthermore%252C%2520we%2520introduce%2520a%2520novel%2520multimodal%2520interleaved%2520%2528MI%2529%2520scanning%2520mechanism%2520that%2520facilitates%2520effective%2520information%2520exchange%2520between%2520the%2520PAN%2520and%2520MS%2520modalities.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%2520compared%2520to%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520techniques%2520across%2520multiple%2520tasks%2520and%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMMamba%3A%20A%20Versatile%20Cross-Modal%20In%20Context%20Fusion%20Framework%20for%20Pan-Sharpening%20and%20Zero-Shot%20Image%20Enhancement&entry.906535625=Yingying%20Wang%20and%20Xuanhua%20He%20and%20Chen%20Wu%20and%20Jialing%20Huang%20and%20Suiyun%20Zhang%20and%20Rui%20Liu%20and%20Xinghao%20Ding%20and%20Haoxuan%20Che&entry.1292438233=Pan-sharpening%20aims%20to%20generate%20high-resolution%20multispectral%20%28HRMS%29%20images%20by%20integrating%20a%20high-resolution%20panchromatic%20%28PAN%29%20image%20with%20its%20corresponding%20low-resolution%20multispectral%20%28MS%29%20image.%20To%20achieve%20effective%20fusion%2C%20it%20is%20crucial%20to%20fully%20exploit%20the%20complementary%20information%20between%20the%20two%20modalities.%20Traditional%20CNN-based%20methods%20typically%20rely%20on%20channel-wise%20concatenation%20with%20fixed%20convolutional%20operators%2C%20which%20limits%20their%20adaptability%20to%20diverse%20spatial%20and%20spectral%20variations.%20While%20cross-attention%20mechanisms%20enable%20global%20interactions%2C%20they%20are%20computationally%20inefficient%20and%20may%20dilute%20fine-grained%20correspondences%2C%20making%20it%20difficult%20to%20capture%20complex%20semantic%20relationships.%20Recent%20advances%20in%20the%20Multimodal%20Diffusion%20Transformer%20%28MMDiT%29%20architecture%20have%20demonstrated%20impressive%20success%20in%20image%20generation%20and%20editing%20tasks.%20Unlike%20cross-attention%2C%20MMDiT%20employs%20in-context%20conditioning%20to%20facilitate%20more%20direct%20and%20efficient%20cross-modal%20information%20exchange.%20In%20this%20paper%2C%20we%20propose%20MMMamba%2C%20a%20cross-modal%20in-context%20fusion%20framework%20for%20pan-sharpening%2C%20with%20the%20flexibility%20to%20support%20image%20super-resolution%20in%20a%20zero-shot%20manner.%20Built%20upon%20the%20Mamba%20architecture%2C%20our%20design%20ensures%20linear%20computational%20complexity%20while%20maintaining%20strong%20cross-modal%20interaction%20capacity.%20Furthermore%2C%20we%20introduce%20a%20novel%20multimodal%20interleaved%20%28MI%29%20scanning%20mechanism%20that%20facilitates%20effective%20information%20exchange%20between%20the%20PAN%20and%20MS%20modalities.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20our%20method%20compared%20to%20existing%20state-of-the-art%20%28SOTA%29%20techniques%20across%20multiple%20tasks%20and%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.15261v1&entry.124074799=Read"},
{"title": "FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World", "author": "Zengli Luo and Canlong Zhang and Xiaochun Lu and Zhixin Li", "abstract": "Text-based Pedestrian Retrieval (TPR) deals with retrieving specific target pedestrians in visual scenes according to natural language descriptions. Although existing methods have achieved progress under constrained settings, interactive retrieval in the open-world scenario still suffers from limited model generalization and insufficient semantic understanding. To address these challenges, we propose FitPro, an open-world interactive zero-shot TPR framework with enhanced semantic comprehension and cross-scene adaptability. FitPro has three innovative components: Feature Contrastive Decoding (FCD), Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval (QHR). The FCD integrates prompt-guided contrastive decoding to generate high-quality structured pedestrian descriptions from denoised images, effectively alleviating semantic drift in zero-shot scenarios. The ISM constructs holistic pedestrian representations from multi-view observations to achieve global semantic modeling in multi-turn interactions, thereby improving robustness against viewpoint shifts and fine-grained variations in descriptions. The QHR dynamically optimizes the retrieval pipeline according to query types, enabling efficient adaptation to multi-modal and multi-view inputs. Extensive experiments on five public datasets and two evaluation protocols demonstrate that FitPro significantly overcomes the generalization limitations and semantic modeling constraints of existing methods in interactive retrieval, paving the way for practical deployment.", "link": "http://arxiv.org/abs/2509.16674v3", "date": "2025-12-17", "relevancy": 2.2318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FitPro%3A%20A%20Zero-Shot%20Framework%20for%20Interactive%20Text-based%20Pedestrian%20Retrieval%20in%20Open%20World&body=Title%3A%20FitPro%3A%20A%20Zero-Shot%20Framework%20for%20Interactive%20Text-based%20Pedestrian%20Retrieval%20in%20Open%20World%0AAuthor%3A%20Zengli%20Luo%20and%20Canlong%20Zhang%20and%20Xiaochun%20Lu%20and%20Zhixin%20Li%0AAbstract%3A%20Text-based%20Pedestrian%20Retrieval%20%28TPR%29%20deals%20with%20retrieving%20specific%20target%20pedestrians%20in%20visual%20scenes%20according%20to%20natural%20language%20descriptions.%20Although%20existing%20methods%20have%20achieved%20progress%20under%20constrained%20settings%2C%20interactive%20retrieval%20in%20the%20open-world%20scenario%20still%20suffers%20from%20limited%20model%20generalization%20and%20insufficient%20semantic%20understanding.%20To%20address%20these%20challenges%2C%20we%20propose%20FitPro%2C%20an%20open-world%20interactive%20zero-shot%20TPR%20framework%20with%20enhanced%20semantic%20comprehension%20and%20cross-scene%20adaptability.%20FitPro%20has%20three%20innovative%20components%3A%20Feature%20Contrastive%20Decoding%20%28FCD%29%2C%20Incremental%20Semantic%20Mining%20%28ISM%29%2C%20and%20Query-aware%20Hierarchical%20Retrieval%20%28QHR%29.%20The%20FCD%20integrates%20prompt-guided%20contrastive%20decoding%20to%20generate%20high-quality%20structured%20pedestrian%20descriptions%20from%20denoised%20images%2C%20effectively%20alleviating%20semantic%20drift%20in%20zero-shot%20scenarios.%20The%20ISM%20constructs%20holistic%20pedestrian%20representations%20from%20multi-view%20observations%20to%20achieve%20global%20semantic%20modeling%20in%20multi-turn%20interactions%2C%20thereby%20improving%20robustness%20against%20viewpoint%20shifts%20and%20fine-grained%20variations%20in%20descriptions.%20The%20QHR%20dynamically%20optimizes%20the%20retrieval%20pipeline%20according%20to%20query%20types%2C%20enabling%20efficient%20adaptation%20to%20multi-modal%20and%20multi-view%20inputs.%20Extensive%20experiments%20on%20five%20public%20datasets%20and%20two%20evaluation%20protocols%20demonstrate%20that%20FitPro%20significantly%20overcomes%20the%20generalization%20limitations%20and%20semantic%20modeling%20constraints%20of%20existing%20methods%20in%20interactive%20retrieval%2C%20paving%20the%20way%20for%20practical%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2509.16674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFitPro%253A%2520A%2520Zero-Shot%2520Framework%2520for%2520Interactive%2520Text-based%2520Pedestrian%2520Retrieval%2520in%2520Open%2520World%26entry.906535625%3DZengli%2520Luo%2520and%2520Canlong%2520Zhang%2520and%2520Xiaochun%2520Lu%2520and%2520Zhixin%2520Li%26entry.1292438233%3DText-based%2520Pedestrian%2520Retrieval%2520%2528TPR%2529%2520deals%2520with%2520retrieving%2520specific%2520target%2520pedestrians%2520in%2520visual%2520scenes%2520according%2520to%2520natural%2520language%2520descriptions.%2520Although%2520existing%2520methods%2520have%2520achieved%2520progress%2520under%2520constrained%2520settings%252C%2520interactive%2520retrieval%2520in%2520the%2520open-world%2520scenario%2520still%2520suffers%2520from%2520limited%2520model%2520generalization%2520and%2520insufficient%2520semantic%2520understanding.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FitPro%252C%2520an%2520open-world%2520interactive%2520zero-shot%2520TPR%2520framework%2520with%2520enhanced%2520semantic%2520comprehension%2520and%2520cross-scene%2520adaptability.%2520FitPro%2520has%2520three%2520innovative%2520components%253A%2520Feature%2520Contrastive%2520Decoding%2520%2528FCD%2529%252C%2520Incremental%2520Semantic%2520Mining%2520%2528ISM%2529%252C%2520and%2520Query-aware%2520Hierarchical%2520Retrieval%2520%2528QHR%2529.%2520The%2520FCD%2520integrates%2520prompt-guided%2520contrastive%2520decoding%2520to%2520generate%2520high-quality%2520structured%2520pedestrian%2520descriptions%2520from%2520denoised%2520images%252C%2520effectively%2520alleviating%2520semantic%2520drift%2520in%2520zero-shot%2520scenarios.%2520The%2520ISM%2520constructs%2520holistic%2520pedestrian%2520representations%2520from%2520multi-view%2520observations%2520to%2520achieve%2520global%2520semantic%2520modeling%2520in%2520multi-turn%2520interactions%252C%2520thereby%2520improving%2520robustness%2520against%2520viewpoint%2520shifts%2520and%2520fine-grained%2520variations%2520in%2520descriptions.%2520The%2520QHR%2520dynamically%2520optimizes%2520the%2520retrieval%2520pipeline%2520according%2520to%2520query%2520types%252C%2520enabling%2520efficient%2520adaptation%2520to%2520multi-modal%2520and%2520multi-view%2520inputs.%2520Extensive%2520experiments%2520on%2520five%2520public%2520datasets%2520and%2520two%2520evaluation%2520protocols%2520demonstrate%2520that%2520FitPro%2520significantly%2520overcomes%2520the%2520generalization%2520limitations%2520and%2520semantic%2520modeling%2520constraints%2520of%2520existing%2520methods%2520in%2520interactive%2520retrieval%252C%2520paving%2520the%2520way%2520for%2520practical%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FitPro%3A%20A%20Zero-Shot%20Framework%20for%20Interactive%20Text-based%20Pedestrian%20Retrieval%20in%20Open%20World&entry.906535625=Zengli%20Luo%20and%20Canlong%20Zhang%20and%20Xiaochun%20Lu%20and%20Zhixin%20Li&entry.1292438233=Text-based%20Pedestrian%20Retrieval%20%28TPR%29%20deals%20with%20retrieving%20specific%20target%20pedestrians%20in%20visual%20scenes%20according%20to%20natural%20language%20descriptions.%20Although%20existing%20methods%20have%20achieved%20progress%20under%20constrained%20settings%2C%20interactive%20retrieval%20in%20the%20open-world%20scenario%20still%20suffers%20from%20limited%20model%20generalization%20and%20insufficient%20semantic%20understanding.%20To%20address%20these%20challenges%2C%20we%20propose%20FitPro%2C%20an%20open-world%20interactive%20zero-shot%20TPR%20framework%20with%20enhanced%20semantic%20comprehension%20and%20cross-scene%20adaptability.%20FitPro%20has%20three%20innovative%20components%3A%20Feature%20Contrastive%20Decoding%20%28FCD%29%2C%20Incremental%20Semantic%20Mining%20%28ISM%29%2C%20and%20Query-aware%20Hierarchical%20Retrieval%20%28QHR%29.%20The%20FCD%20integrates%20prompt-guided%20contrastive%20decoding%20to%20generate%20high-quality%20structured%20pedestrian%20descriptions%20from%20denoised%20images%2C%20effectively%20alleviating%20semantic%20drift%20in%20zero-shot%20scenarios.%20The%20ISM%20constructs%20holistic%20pedestrian%20representations%20from%20multi-view%20observations%20to%20achieve%20global%20semantic%20modeling%20in%20multi-turn%20interactions%2C%20thereby%20improving%20robustness%20against%20viewpoint%20shifts%20and%20fine-grained%20variations%20in%20descriptions.%20The%20QHR%20dynamically%20optimizes%20the%20retrieval%20pipeline%20according%20to%20query%20types%2C%20enabling%20efficient%20adaptation%20to%20multi-modal%20and%20multi-view%20inputs.%20Extensive%20experiments%20on%20five%20public%20datasets%20and%20two%20evaluation%20protocols%20demonstrate%20that%20FitPro%20significantly%20overcomes%20the%20generalization%20limitations%20and%20semantic%20modeling%20constraints%20of%20existing%20methods%20in%20interactive%20retrieval%2C%20paving%20the%20way%20for%20practical%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2509.16674v3&entry.124074799=Read"},
{"title": "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression", "author": "Kyle Sargent and Ruiqi Gao and Philipp Henzler and Charles Herrmann and Aleksander Holynski and Li Fei-Fei and Jiajun Wu and Jason Zhang", "abstract": "Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic", "link": "http://arxiv.org/abs/2512.15701v1", "date": "2025-12-17", "relevancy": 2.2294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLIC%3A%20Vision-Language%20Models%20As%20Perceptual%20Judges%20for%20Human-Aligned%20Image%20Compression&body=Title%3A%20VLIC%3A%20Vision-Language%20Models%20As%20Perceptual%20Judges%20for%20Human-Aligned%20Image%20Compression%0AAuthor%3A%20Kyle%20Sargent%20and%20Ruiqi%20Gao%20and%20Philipp%20Henzler%20and%20Charles%20Herrmann%20and%20Aleksander%20Holynski%20and%20Li%20Fei-Fei%20and%20Jiajun%20Wu%20and%20Jason%20Zhang%0AAbstract%3A%20Evaluations%20of%20image%20compression%20performance%20which%20include%20human%20preferences%20have%20generally%20found%20that%20naive%20distortion%20functions%20such%20as%20MSE%20are%20insufficiently%20aligned%20to%20human%20perception.%20In%20order%20to%20align%20compression%20models%20to%20human%20perception%2C%20prior%20work%20has%20employed%20differentiable%20perceptual%20losses%20consisting%20of%20neural%20networks%20calibrated%20on%20large-scale%20datasets%20of%20human%20psycho-visual%20judgments.%20We%20show%20that%2C%20surprisingly%2C%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20can%20replicate%20binary%20human%20two-alternative%20forced%20choice%20%282AFC%29%20judgments%20zero-shot%20when%20asked%20to%20reason%20about%20the%20differences%20between%20pairs%20of%20images.%20Motivated%20to%20exploit%20the%20powerful%20zero-shot%20visual%20reasoning%20capabilities%20of%20VLMs%2C%20we%20propose%20Vision-Language%20Models%20for%20Image%20Compression%20%28VLIC%29%2C%20a%20diffusion-based%20image%20compression%20system%20designed%20to%20be%20post-trained%20with%20binary%20VLM%20judgments.%20VLIC%20leverages%20existing%20techniques%20for%20diffusion%20model%20post-training%20with%20preferences%2C%20rather%20than%20distilling%20the%20VLM%20judgments%20into%20a%20separate%20perceptual%20loss%20network.%20We%20show%20that%20calibrating%20this%20system%20on%20VLM%20judgments%20produces%20competitive%20or%20state-of-the-art%20performance%20on%20human-aligned%20visual%20compression%20depending%20on%20the%20dataset%2C%20according%20to%20perceptual%20metrics%20and%20large-scale%20user%20studies.%20We%20additionally%20conduct%20an%20extensive%20analysis%20of%20the%20VLM-based%20reward%20design%20and%20training%20procedure%20and%20share%20important%20insights.%20More%20visuals%20are%20available%20at%20https%3A//kylesargent.github.io/vlic%0ALink%3A%20http%3A//arxiv.org/abs/2512.15701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLIC%253A%2520Vision-Language%2520Models%2520As%2520Perceptual%2520Judges%2520for%2520Human-Aligned%2520Image%2520Compression%26entry.906535625%3DKyle%2520Sargent%2520and%2520Ruiqi%2520Gao%2520and%2520Philipp%2520Henzler%2520and%2520Charles%2520Herrmann%2520and%2520Aleksander%2520Holynski%2520and%2520Li%2520Fei-Fei%2520and%2520Jiajun%2520Wu%2520and%2520Jason%2520Zhang%26entry.1292438233%3DEvaluations%2520of%2520image%2520compression%2520performance%2520which%2520include%2520human%2520preferences%2520have%2520generally%2520found%2520that%2520naive%2520distortion%2520functions%2520such%2520as%2520MSE%2520are%2520insufficiently%2520aligned%2520to%2520human%2520perception.%2520In%2520order%2520to%2520align%2520compression%2520models%2520to%2520human%2520perception%252C%2520prior%2520work%2520has%2520employed%2520differentiable%2520perceptual%2520losses%2520consisting%2520of%2520neural%2520networks%2520calibrated%2520on%2520large-scale%2520datasets%2520of%2520human%2520psycho-visual%2520judgments.%2520We%2520show%2520that%252C%2520surprisingly%252C%2520state-of-the-art%2520vision-language%2520models%2520%2528VLMs%2529%2520can%2520replicate%2520binary%2520human%2520two-alternative%2520forced%2520choice%2520%25282AFC%2529%2520judgments%2520zero-shot%2520when%2520asked%2520to%2520reason%2520about%2520the%2520differences%2520between%2520pairs%2520of%2520images.%2520Motivated%2520to%2520exploit%2520the%2520powerful%2520zero-shot%2520visual%2520reasoning%2520capabilities%2520of%2520VLMs%252C%2520we%2520propose%2520Vision-Language%2520Models%2520for%2520Image%2520Compression%2520%2528VLIC%2529%252C%2520a%2520diffusion-based%2520image%2520compression%2520system%2520designed%2520to%2520be%2520post-trained%2520with%2520binary%2520VLM%2520judgments.%2520VLIC%2520leverages%2520existing%2520techniques%2520for%2520diffusion%2520model%2520post-training%2520with%2520preferences%252C%2520rather%2520than%2520distilling%2520the%2520VLM%2520judgments%2520into%2520a%2520separate%2520perceptual%2520loss%2520network.%2520We%2520show%2520that%2520calibrating%2520this%2520system%2520on%2520VLM%2520judgments%2520produces%2520competitive%2520or%2520state-of-the-art%2520performance%2520on%2520human-aligned%2520visual%2520compression%2520depending%2520on%2520the%2520dataset%252C%2520according%2520to%2520perceptual%2520metrics%2520and%2520large-scale%2520user%2520studies.%2520We%2520additionally%2520conduct%2520an%2520extensive%2520analysis%2520of%2520the%2520VLM-based%2520reward%2520design%2520and%2520training%2520procedure%2520and%2520share%2520important%2520insights.%2520More%2520visuals%2520are%2520available%2520at%2520https%253A//kylesargent.github.io/vlic%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLIC%3A%20Vision-Language%20Models%20As%20Perceptual%20Judges%20for%20Human-Aligned%20Image%20Compression&entry.906535625=Kyle%20Sargent%20and%20Ruiqi%20Gao%20and%20Philipp%20Henzler%20and%20Charles%20Herrmann%20and%20Aleksander%20Holynski%20and%20Li%20Fei-Fei%20and%20Jiajun%20Wu%20and%20Jason%20Zhang&entry.1292438233=Evaluations%20of%20image%20compression%20performance%20which%20include%20human%20preferences%20have%20generally%20found%20that%20naive%20distortion%20functions%20such%20as%20MSE%20are%20insufficiently%20aligned%20to%20human%20perception.%20In%20order%20to%20align%20compression%20models%20to%20human%20perception%2C%20prior%20work%20has%20employed%20differentiable%20perceptual%20losses%20consisting%20of%20neural%20networks%20calibrated%20on%20large-scale%20datasets%20of%20human%20psycho-visual%20judgments.%20We%20show%20that%2C%20surprisingly%2C%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20can%20replicate%20binary%20human%20two-alternative%20forced%20choice%20%282AFC%29%20judgments%20zero-shot%20when%20asked%20to%20reason%20about%20the%20differences%20between%20pairs%20of%20images.%20Motivated%20to%20exploit%20the%20powerful%20zero-shot%20visual%20reasoning%20capabilities%20of%20VLMs%2C%20we%20propose%20Vision-Language%20Models%20for%20Image%20Compression%20%28VLIC%29%2C%20a%20diffusion-based%20image%20compression%20system%20designed%20to%20be%20post-trained%20with%20binary%20VLM%20judgments.%20VLIC%20leverages%20existing%20techniques%20for%20diffusion%20model%20post-training%20with%20preferences%2C%20rather%20than%20distilling%20the%20VLM%20judgments%20into%20a%20separate%20perceptual%20loss%20network.%20We%20show%20that%20calibrating%20this%20system%20on%20VLM%20judgments%20produces%20competitive%20or%20state-of-the-art%20performance%20on%20human-aligned%20visual%20compression%20depending%20on%20the%20dataset%2C%20according%20to%20perceptual%20metrics%20and%20large-scale%20user%20studies.%20We%20additionally%20conduct%20an%20extensive%20analysis%20of%20the%20VLM-based%20reward%20design%20and%20training%20procedure%20and%20share%20important%20insights.%20More%20visuals%20are%20available%20at%20https%3A//kylesargent.github.io/vlic&entry.1838667208=http%3A//arxiv.org/abs/2512.15701v1&entry.124074799=Read"},
{"title": "A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point", "author": "Carlos Couto and Jos\u00e9 Mour\u00e3o and M\u00e1rio A. T. Figueiredo and Pedro Ribeiro", "abstract": "Near an optimal learning point of a neural network, the learning performance of gradient descent dynamics is dictated by the Hessian matrix of the loss function with respect to the network parameters. We characterize the Hessian eigenspectrum for some classes of teacher-student problems, when the teacher and student networks have matching weights, showing that the smaller eigenvalues of the Hessian determine long-time learning performance. For linear networks, we analytically establish that for large networks the spectrum asymptotically follows a convolution of a scaled chi-square distribution with a scaled Marchenko-Pastur distribution. We numerically analyse the Hessian spectrum for polynomial and other non-linear networks. Furthermore, we show that the rank of the Hessian matrix can be seen as an effective number of parameters for networks using polynomial activation functions. For a generic non-linear activation function, such as the error function, we empirically observe that the Hessian matrix is always full rank.", "link": "http://arxiv.org/abs/2512.15606v1", "date": "2025-12-17", "relevancy": 2.2185, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5152}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4087}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Teacher-Student%20Perspective%20on%20the%20Dynamics%20of%20Learning%20Near%20the%20Optimal%20Point&body=Title%3A%20A%20Teacher-Student%20Perspective%20on%20the%20Dynamics%20of%20Learning%20Near%20the%20Optimal%20Point%0AAuthor%3A%20Carlos%20Couto%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20M%C3%A1rio%20A.%20T.%20Figueiredo%20and%20Pedro%20Ribeiro%0AAbstract%3A%20Near%20an%20optimal%20learning%20point%20of%20a%20neural%20network%2C%20the%20learning%20performance%20of%20gradient%20descent%20dynamics%20is%20dictated%20by%20the%20Hessian%20matrix%20of%20the%20loss%20function%20with%20respect%20to%20the%20network%20parameters.%20We%20characterize%20the%20Hessian%20eigenspectrum%20for%20some%20classes%20of%20teacher-student%20problems%2C%20when%20the%20teacher%20and%20student%20networks%20have%20matching%20weights%2C%20showing%20that%20the%20smaller%20eigenvalues%20of%20the%20Hessian%20determine%20long-time%20learning%20performance.%20For%20linear%20networks%2C%20we%20analytically%20establish%20that%20for%20large%20networks%20the%20spectrum%20asymptotically%20follows%20a%20convolution%20of%20a%20scaled%20chi-square%20distribution%20with%20a%20scaled%20Marchenko-Pastur%20distribution.%20We%20numerically%20analyse%20the%20Hessian%20spectrum%20for%20polynomial%20and%20other%20non-linear%20networks.%20Furthermore%2C%20we%20show%20that%20the%20rank%20of%20the%20Hessian%20matrix%20can%20be%20seen%20as%20an%20effective%20number%20of%20parameters%20for%20networks%20using%20polynomial%20activation%20functions.%20For%20a%20generic%20non-linear%20activation%20function%2C%20such%20as%20the%20error%20function%2C%20we%20empirically%20observe%20that%20the%20Hessian%20matrix%20is%20always%20full%20rank.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Teacher-Student%2520Perspective%2520on%2520the%2520Dynamics%2520of%2520Learning%2520Near%2520the%2520Optimal%2520Point%26entry.906535625%3DCarlos%2520Couto%2520and%2520Jos%25C3%25A9%2520Mour%25C3%25A3o%2520and%2520M%25C3%25A1rio%2520A.%2520T.%2520Figueiredo%2520and%2520Pedro%2520Ribeiro%26entry.1292438233%3DNear%2520an%2520optimal%2520learning%2520point%2520of%2520a%2520neural%2520network%252C%2520the%2520learning%2520performance%2520of%2520gradient%2520descent%2520dynamics%2520is%2520dictated%2520by%2520the%2520Hessian%2520matrix%2520of%2520the%2520loss%2520function%2520with%2520respect%2520to%2520the%2520network%2520parameters.%2520We%2520characterize%2520the%2520Hessian%2520eigenspectrum%2520for%2520some%2520classes%2520of%2520teacher-student%2520problems%252C%2520when%2520the%2520teacher%2520and%2520student%2520networks%2520have%2520matching%2520weights%252C%2520showing%2520that%2520the%2520smaller%2520eigenvalues%2520of%2520the%2520Hessian%2520determine%2520long-time%2520learning%2520performance.%2520For%2520linear%2520networks%252C%2520we%2520analytically%2520establish%2520that%2520for%2520large%2520networks%2520the%2520spectrum%2520asymptotically%2520follows%2520a%2520convolution%2520of%2520a%2520scaled%2520chi-square%2520distribution%2520with%2520a%2520scaled%2520Marchenko-Pastur%2520distribution.%2520We%2520numerically%2520analyse%2520the%2520Hessian%2520spectrum%2520for%2520polynomial%2520and%2520other%2520non-linear%2520networks.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520rank%2520of%2520the%2520Hessian%2520matrix%2520can%2520be%2520seen%2520as%2520an%2520effective%2520number%2520of%2520parameters%2520for%2520networks%2520using%2520polynomial%2520activation%2520functions.%2520For%2520a%2520generic%2520non-linear%2520activation%2520function%252C%2520such%2520as%2520the%2520error%2520function%252C%2520we%2520empirically%2520observe%2520that%2520the%2520Hessian%2520matrix%2520is%2520always%2520full%2520rank.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Teacher-Student%20Perspective%20on%20the%20Dynamics%20of%20Learning%20Near%20the%20Optimal%20Point&entry.906535625=Carlos%20Couto%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20M%C3%A1rio%20A.%20T.%20Figueiredo%20and%20Pedro%20Ribeiro&entry.1292438233=Near%20an%20optimal%20learning%20point%20of%20a%20neural%20network%2C%20the%20learning%20performance%20of%20gradient%20descent%20dynamics%20is%20dictated%20by%20the%20Hessian%20matrix%20of%20the%20loss%20function%20with%20respect%20to%20the%20network%20parameters.%20We%20characterize%20the%20Hessian%20eigenspectrum%20for%20some%20classes%20of%20teacher-student%20problems%2C%20when%20the%20teacher%20and%20student%20networks%20have%20matching%20weights%2C%20showing%20that%20the%20smaller%20eigenvalues%20of%20the%20Hessian%20determine%20long-time%20learning%20performance.%20For%20linear%20networks%2C%20we%20analytically%20establish%20that%20for%20large%20networks%20the%20spectrum%20asymptotically%20follows%20a%20convolution%20of%20a%20scaled%20chi-square%20distribution%20with%20a%20scaled%20Marchenko-Pastur%20distribution.%20We%20numerically%20analyse%20the%20Hessian%20spectrum%20for%20polynomial%20and%20other%20non-linear%20networks.%20Furthermore%2C%20we%20show%20that%20the%20rank%20of%20the%20Hessian%20matrix%20can%20be%20seen%20as%20an%20effective%20number%20of%20parameters%20for%20networks%20using%20polynomial%20activation%20functions.%20For%20a%20generic%20non-linear%20activation%20function%2C%20such%20as%20the%20error%20function%2C%20we%20empirically%20observe%20that%20the%20Hessian%20matrix%20is%20always%20full%20rank.&entry.1838667208=http%3A//arxiv.org/abs/2512.15606v1&entry.124074799=Read"},
{"title": "High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations", "author": "Victor L\u00e9ger and Florent Chatelain", "abstract": "Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.", "link": "http://arxiv.org/abs/2512.15684v1", "date": "2025-12-17", "relevancy": 2.2153, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Partial%20Least%20Squares%3A%20Spectral%20Analysis%20and%20Fundamental%20Limitations&body=Title%3A%20High-Dimensional%20Partial%20Least%20Squares%3A%20Spectral%20Analysis%20and%20Fundamental%20Limitations%0AAuthor%3A%20Victor%20L%C3%A9ger%20and%20Florent%20Chatelain%0AAbstract%3A%20Partial%20Least%20Squares%20%28PLS%29%20is%20a%20widely%20used%20method%20for%20data%20integration%2C%20designed%20to%20extract%20latent%20components%20shared%20across%20paired%20high-dimensional%20datasets.%20Despite%20decades%20of%20practical%20success%2C%20a%20precise%20theoretical%20understanding%20of%20its%20behavior%20in%20high-dimensional%20regimes%20remains%20limited.%20In%20this%20paper%2C%20we%20study%20a%20data%20integration%20model%20in%20which%20two%20high-dimensional%20data%20matrices%20share%20a%20low-rank%20common%20latent%20structure%20while%20also%20containing%20individual-specific%20components.%20We%20analyze%20the%20singular%20vectors%20of%20the%20associated%20cross-covariance%20matrix%20using%20tools%20from%20random%20matrix%20theory%20and%20derive%20asymptotic%20characterizations%20of%20the%20alignment%20between%20estimated%20and%20true%20latent%20directions.%20These%20results%20provide%20a%20quantitative%20explanation%20of%20the%20reconstruction%20performance%20of%20the%20PLS%20variant%20based%20on%20Singular%20Value%20Decomposition%20%28PLS-SVD%29%20and%20identify%20regimes%20where%20the%20method%20exhibits%20counter-intuitive%20or%20limiting%20behavior.%20Building%20on%20this%20analysis%2C%20we%20compare%20PLS-SVD%20with%20principal%20component%20analysis%20applied%20separately%20to%20each%20dataset%20and%20show%20its%20asymptotic%20superiority%20in%20detecting%20the%20common%20latent%20subspace.%20Overall%2C%20our%20results%20offer%20a%20comprehensive%20theoretical%20understanding%20of%20high-dimensional%20PLS-SVD%2C%20clarifying%20both%20its%20advantages%20and%20fundamental%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Partial%2520Least%2520Squares%253A%2520Spectral%2520Analysis%2520and%2520Fundamental%2520Limitations%26entry.906535625%3DVictor%2520L%25C3%25A9ger%2520and%2520Florent%2520Chatelain%26entry.1292438233%3DPartial%2520Least%2520Squares%2520%2528PLS%2529%2520is%2520a%2520widely%2520used%2520method%2520for%2520data%2520integration%252C%2520designed%2520to%2520extract%2520latent%2520components%2520shared%2520across%2520paired%2520high-dimensional%2520datasets.%2520Despite%2520decades%2520of%2520practical%2520success%252C%2520a%2520precise%2520theoretical%2520understanding%2520of%2520its%2520behavior%2520in%2520high-dimensional%2520regimes%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520data%2520integration%2520model%2520in%2520which%2520two%2520high-dimensional%2520data%2520matrices%2520share%2520a%2520low-rank%2520common%2520latent%2520structure%2520while%2520also%2520containing%2520individual-specific%2520components.%2520We%2520analyze%2520the%2520singular%2520vectors%2520of%2520the%2520associated%2520cross-covariance%2520matrix%2520using%2520tools%2520from%2520random%2520matrix%2520theory%2520and%2520derive%2520asymptotic%2520characterizations%2520of%2520the%2520alignment%2520between%2520estimated%2520and%2520true%2520latent%2520directions.%2520These%2520results%2520provide%2520a%2520quantitative%2520explanation%2520of%2520the%2520reconstruction%2520performance%2520of%2520the%2520PLS%2520variant%2520based%2520on%2520Singular%2520Value%2520Decomposition%2520%2528PLS-SVD%2529%2520and%2520identify%2520regimes%2520where%2520the%2520method%2520exhibits%2520counter-intuitive%2520or%2520limiting%2520behavior.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520compare%2520PLS-SVD%2520with%2520principal%2520component%2520analysis%2520applied%2520separately%2520to%2520each%2520dataset%2520and%2520show%2520its%2520asymptotic%2520superiority%2520in%2520detecting%2520the%2520common%2520latent%2520subspace.%2520Overall%252C%2520our%2520results%2520offer%2520a%2520comprehensive%2520theoretical%2520understanding%2520of%2520high-dimensional%2520PLS-SVD%252C%2520clarifying%2520both%2520its%2520advantages%2520and%2520fundamental%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Partial%20Least%20Squares%3A%20Spectral%20Analysis%20and%20Fundamental%20Limitations&entry.906535625=Victor%20L%C3%A9ger%20and%20Florent%20Chatelain&entry.1292438233=Partial%20Least%20Squares%20%28PLS%29%20is%20a%20widely%20used%20method%20for%20data%20integration%2C%20designed%20to%20extract%20latent%20components%20shared%20across%20paired%20high-dimensional%20datasets.%20Despite%20decades%20of%20practical%20success%2C%20a%20precise%20theoretical%20understanding%20of%20its%20behavior%20in%20high-dimensional%20regimes%20remains%20limited.%20In%20this%20paper%2C%20we%20study%20a%20data%20integration%20model%20in%20which%20two%20high-dimensional%20data%20matrices%20share%20a%20low-rank%20common%20latent%20structure%20while%20also%20containing%20individual-specific%20components.%20We%20analyze%20the%20singular%20vectors%20of%20the%20associated%20cross-covariance%20matrix%20using%20tools%20from%20random%20matrix%20theory%20and%20derive%20asymptotic%20characterizations%20of%20the%20alignment%20between%20estimated%20and%20true%20latent%20directions.%20These%20results%20provide%20a%20quantitative%20explanation%20of%20the%20reconstruction%20performance%20of%20the%20PLS%20variant%20based%20on%20Singular%20Value%20Decomposition%20%28PLS-SVD%29%20and%20identify%20regimes%20where%20the%20method%20exhibits%20counter-intuitive%20or%20limiting%20behavior.%20Building%20on%20this%20analysis%2C%20we%20compare%20PLS-SVD%20with%20principal%20component%20analysis%20applied%20separately%20to%20each%20dataset%20and%20show%20its%20asymptotic%20superiority%20in%20detecting%20the%20common%20latent%20subspace.%20Overall%2C%20our%20results%20offer%20a%20comprehensive%20theoretical%20understanding%20of%20high-dimensional%20PLS-SVD%2C%20clarifying%20both%20its%20advantages%20and%20fundamental%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2512.15684v1&entry.124074799=Read"},
{"title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training", "author": "Zhenhan Yin and Xuanhan Wang and Jiahao Jiang and Kaiyuan Deng and Pengqi Chen and Shuangle Li and Chong Liu and Xing Xu and ingkuan Song and Lianli Gao and Heng Tao Shen", "abstract": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbol\u03c0_{0}$, $\\boldsymbol\u03c0_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.", "link": "http://arxiv.org/abs/2512.15411v1", "date": "2025-12-17", "relevancy": 2.2122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5557}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiVLA%3A%20Towards%20Generalizable%20Vision-Language-Action%20Model%20with%20Human-Robot%20Mutual%20Imitation%20Pre-training&body=Title%3A%20MiVLA%3A%20Towards%20Generalizable%20Vision-Language-Action%20Model%20with%20Human-Robot%20Mutual%20Imitation%20Pre-training%0AAuthor%3A%20Zhenhan%20Yin%20and%20Xuanhan%20Wang%20and%20Jiahao%20Jiang%20and%20Kaiyuan%20Deng%20and%20Pengqi%20Chen%20and%20Shuangle%20Li%20and%20Chong%20Liu%20and%20Xing%20Xu%20and%20ingkuan%20Song%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20While%20leveraging%20abundant%20human%20videos%20and%20simulated%20robot%20data%20poses%20a%20scalable%20solution%20to%20the%20scarcity%20of%20real-world%20robot%20data%2C%20the%20generalization%20capability%20of%20existing%20vision-language-action%20models%20%28VLAs%29%20remains%20limited%20by%20mismatches%20in%20camera%20views%2C%20visual%20appearance%2C%20and%20embodiment%20morphologies.%20To%20overcome%20this%20limitation%2C%20we%20propose%20MiVLA%2C%20a%20generalizable%20VLA%20empowered%20by%20human-robot%20mutual%20imitation%20pre-training%2C%20which%20leverages%20inherent%20behavioral%20similarity%20between%20human%20hands%20and%20robotic%20arms%20to%20build%20a%20foundation%20of%20strong%20behavioral%20priors%20for%20both%20human%20actions%20and%20robotic%20control.%20Specifically%2C%20our%20method%20utilizes%20kinematic%20rules%20with%20left/right%20hand%20coordinate%20systems%20for%20bidirectional%20alignment%20between%20human%20and%20robot%20action%20spaces.%20Given%20human%20or%20simulated%20robot%20demonstrations%2C%20MiVLA%20is%20trained%20to%20forecast%20behavior%20trajectories%20for%20one%20embodiment%2C%20and%20imitate%20behaviors%20for%20another%20one%20unseen%20in%20the%20demonstration.%20Based%20on%20this%20mutual%20imitation%2C%20it%20integrates%20the%20behavioral%20fidelity%20of%20real-world%20human%20data%20with%20the%20manipulative%20diversity%20of%20simulated%20robot%20data%20into%20a%20unified%20model%2C%20thereby%20enhancing%20the%20generalization%20capability%20for%20downstream%20tasks.%20Extensive%20experiments%20conducted%20on%20both%20simulation%20and%20real-world%20platforms%20with%20three%20robots%20%28ARX%2C%20PiPer%20and%20LocoMan%29%2C%20demonstrate%20that%20MiVLA%20achieves%20strong%20improved%20generalization%20capability%2C%20outperforming%20state-of-the-art%20VLAs%20%28e.g.%2C%20%24%5Cboldsymbol%CF%80_%7B0%7D%24%2C%20%24%5Cboldsymbol%CF%80_%7B0.5%7D%24%20and%20H-RDT%29%20by%2025%25%20in%20simulation%2C%20and%2014%25%20in%20real-world%20robot%20control%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiVLA%253A%2520Towards%2520Generalizable%2520Vision-Language-Action%2520Model%2520with%2520Human-Robot%2520Mutual%2520Imitation%2520Pre-training%26entry.906535625%3DZhenhan%2520Yin%2520and%2520Xuanhan%2520Wang%2520and%2520Jiahao%2520Jiang%2520and%2520Kaiyuan%2520Deng%2520and%2520Pengqi%2520Chen%2520and%2520Shuangle%2520Li%2520and%2520Chong%2520Liu%2520and%2520Xing%2520Xu%2520and%2520ingkuan%2520Song%2520and%2520Lianli%2520Gao%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3DWhile%2520leveraging%2520abundant%2520human%2520videos%2520and%2520simulated%2520robot%2520data%2520poses%2520a%2520scalable%2520solution%2520to%2520the%2520scarcity%2520of%2520real-world%2520robot%2520data%252C%2520the%2520generalization%2520capability%2520of%2520existing%2520vision-language-action%2520models%2520%2528VLAs%2529%2520remains%2520limited%2520by%2520mismatches%2520in%2520camera%2520views%252C%2520visual%2520appearance%252C%2520and%2520embodiment%2520morphologies.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520MiVLA%252C%2520a%2520generalizable%2520VLA%2520empowered%2520by%2520human-robot%2520mutual%2520imitation%2520pre-training%252C%2520which%2520leverages%2520inherent%2520behavioral%2520similarity%2520between%2520human%2520hands%2520and%2520robotic%2520arms%2520to%2520build%2520a%2520foundation%2520of%2520strong%2520behavioral%2520priors%2520for%2520both%2520human%2520actions%2520and%2520robotic%2520control.%2520Specifically%252C%2520our%2520method%2520utilizes%2520kinematic%2520rules%2520with%2520left/right%2520hand%2520coordinate%2520systems%2520for%2520bidirectional%2520alignment%2520between%2520human%2520and%2520robot%2520action%2520spaces.%2520Given%2520human%2520or%2520simulated%2520robot%2520demonstrations%252C%2520MiVLA%2520is%2520trained%2520to%2520forecast%2520behavior%2520trajectories%2520for%2520one%2520embodiment%252C%2520and%2520imitate%2520behaviors%2520for%2520another%2520one%2520unseen%2520in%2520the%2520demonstration.%2520Based%2520on%2520this%2520mutual%2520imitation%252C%2520it%2520integrates%2520the%2520behavioral%2520fidelity%2520of%2520real-world%2520human%2520data%2520with%2520the%2520manipulative%2520diversity%2520of%2520simulated%2520robot%2520data%2520into%2520a%2520unified%2520model%252C%2520thereby%2520enhancing%2520the%2520generalization%2520capability%2520for%2520downstream%2520tasks.%2520Extensive%2520experiments%2520conducted%2520on%2520both%2520simulation%2520and%2520real-world%2520platforms%2520with%2520three%2520robots%2520%2528ARX%252C%2520PiPer%2520and%2520LocoMan%2529%252C%2520demonstrate%2520that%2520MiVLA%2520achieves%2520strong%2520improved%2520generalization%2520capability%252C%2520outperforming%2520state-of-the-art%2520VLAs%2520%2528e.g.%252C%2520%2524%255Cboldsymbol%25CF%2580_%257B0%257D%2524%252C%2520%2524%255Cboldsymbol%25CF%2580_%257B0.5%257D%2524%2520and%2520H-RDT%2529%2520by%252025%2525%2520in%2520simulation%252C%2520and%252014%2525%2520in%2520real-world%2520robot%2520control%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiVLA%3A%20Towards%20Generalizable%20Vision-Language-Action%20Model%20with%20Human-Robot%20Mutual%20Imitation%20Pre-training&entry.906535625=Zhenhan%20Yin%20and%20Xuanhan%20Wang%20and%20Jiahao%20Jiang%20and%20Kaiyuan%20Deng%20and%20Pengqi%20Chen%20and%20Shuangle%20Li%20and%20Chong%20Liu%20and%20Xing%20Xu%20and%20ingkuan%20Song%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen&entry.1292438233=While%20leveraging%20abundant%20human%20videos%20and%20simulated%20robot%20data%20poses%20a%20scalable%20solution%20to%20the%20scarcity%20of%20real-world%20robot%20data%2C%20the%20generalization%20capability%20of%20existing%20vision-language-action%20models%20%28VLAs%29%20remains%20limited%20by%20mismatches%20in%20camera%20views%2C%20visual%20appearance%2C%20and%20embodiment%20morphologies.%20To%20overcome%20this%20limitation%2C%20we%20propose%20MiVLA%2C%20a%20generalizable%20VLA%20empowered%20by%20human-robot%20mutual%20imitation%20pre-training%2C%20which%20leverages%20inherent%20behavioral%20similarity%20between%20human%20hands%20and%20robotic%20arms%20to%20build%20a%20foundation%20of%20strong%20behavioral%20priors%20for%20both%20human%20actions%20and%20robotic%20control.%20Specifically%2C%20our%20method%20utilizes%20kinematic%20rules%20with%20left/right%20hand%20coordinate%20systems%20for%20bidirectional%20alignment%20between%20human%20and%20robot%20action%20spaces.%20Given%20human%20or%20simulated%20robot%20demonstrations%2C%20MiVLA%20is%20trained%20to%20forecast%20behavior%20trajectories%20for%20one%20embodiment%2C%20and%20imitate%20behaviors%20for%20another%20one%20unseen%20in%20the%20demonstration.%20Based%20on%20this%20mutual%20imitation%2C%20it%20integrates%20the%20behavioral%20fidelity%20of%20real-world%20human%20data%20with%20the%20manipulative%20diversity%20of%20simulated%20robot%20data%20into%20a%20unified%20model%2C%20thereby%20enhancing%20the%20generalization%20capability%20for%20downstream%20tasks.%20Extensive%20experiments%20conducted%20on%20both%20simulation%20and%20real-world%20platforms%20with%20three%20robots%20%28ARX%2C%20PiPer%20and%20LocoMan%29%2C%20demonstrate%20that%20MiVLA%20achieves%20strong%20improved%20generalization%20capability%2C%20outperforming%20state-of-the-art%20VLAs%20%28e.g.%2C%20%24%5Cboldsymbol%CF%80_%7B0%7D%24%2C%20%24%5Cboldsymbol%CF%80_%7B0.5%7D%24%20and%20H-RDT%29%20by%2025%25%20in%20simulation%2C%20and%2014%25%20in%20real-world%20robot%20control%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.15411v1&entry.124074799=Read"},
{"title": "MedicoSAM: Robust Improvement of SAM for Medical Imaging", "author": "Anwai Archit and Luca Freckmann and Constantin Pape", "abstract": "Medical image segmentation is an important analysis task in clinical practice and research. Deep learning has massively advanced the field, but current approaches are mostly based on models trained for a specific task. Training such models or adapting them to a new condition is costly due to the need for (manually) labeled data. The emergence of vision foundation models, especially Segment Anything, offers a path to universal segmentation for medical images, overcoming these issues. Here, we study how to improve Segment Anything for medical images by comparing different finetuning strategies on a large and diverse dataset. We evaluate the finetuned models on a wide range of interactive and (automatic) semantic segmentation tasks. We find that the performance can be clearly improved for interactive segmentation. However, semantic segmentation does not benefit from pretraining on medical images. Our best model, MedicoSAM, is publicly available at https://github.com/computational-cell-analytics/medico-sam. We show that it is compatible with existing tools for data annotation and believe that it will be of great practical value.", "link": "http://arxiv.org/abs/2501.11734v2", "date": "2025-12-17", "relevancy": 2.2099, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedicoSAM%3A%20Robust%20Improvement%20of%20SAM%20for%20Medical%20Imaging&body=Title%3A%20MedicoSAM%3A%20Robust%20Improvement%20of%20SAM%20for%20Medical%20Imaging%0AAuthor%3A%20Anwai%20Archit%20and%20Luca%20Freckmann%20and%20Constantin%20Pape%0AAbstract%3A%20Medical%20image%20segmentation%20is%20an%20important%20analysis%20task%20in%20clinical%20practice%20and%20research.%20Deep%20learning%20has%20massively%20advanced%20the%20field%2C%20but%20current%20approaches%20are%20mostly%20based%20on%20models%20trained%20for%20a%20specific%20task.%20Training%20such%20models%20or%20adapting%20them%20to%20a%20new%20condition%20is%20costly%20due%20to%20the%20need%20for%20%28manually%29%20labeled%20data.%20The%20emergence%20of%20vision%20foundation%20models%2C%20especially%20Segment%20Anything%2C%20offers%20a%20path%20to%20universal%20segmentation%20for%20medical%20images%2C%20overcoming%20these%20issues.%20Here%2C%20we%20study%20how%20to%20improve%20Segment%20Anything%20for%20medical%20images%20by%20comparing%20different%20finetuning%20strategies%20on%20a%20large%20and%20diverse%20dataset.%20We%20evaluate%20the%20finetuned%20models%20on%20a%20wide%20range%20of%20interactive%20and%20%28automatic%29%20semantic%20segmentation%20tasks.%20We%20find%20that%20the%20performance%20can%20be%20clearly%20improved%20for%20interactive%20segmentation.%20However%2C%20semantic%20segmentation%20does%20not%20benefit%20from%20pretraining%20on%20medical%20images.%20Our%20best%20model%2C%20MedicoSAM%2C%20is%20publicly%20available%20at%20https%3A//github.com/computational-cell-analytics/medico-sam.%20We%20show%20that%20it%20is%20compatible%20with%20existing%20tools%20for%20data%20annotation%20and%20believe%20that%20it%20will%20be%20of%20great%20practical%20value.%0ALink%3A%20http%3A//arxiv.org/abs/2501.11734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedicoSAM%253A%2520Robust%2520Improvement%2520of%2520SAM%2520for%2520Medical%2520Imaging%26entry.906535625%3DAnwai%2520Archit%2520and%2520Luca%2520Freckmann%2520and%2520Constantin%2520Pape%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520an%2520important%2520analysis%2520task%2520in%2520clinical%2520practice%2520and%2520research.%2520Deep%2520learning%2520has%2520massively%2520advanced%2520the%2520field%252C%2520but%2520current%2520approaches%2520are%2520mostly%2520based%2520on%2520models%2520trained%2520for%2520a%2520specific%2520task.%2520Training%2520such%2520models%2520or%2520adapting%2520them%2520to%2520a%2520new%2520condition%2520is%2520costly%2520due%2520to%2520the%2520need%2520for%2520%2528manually%2529%2520labeled%2520data.%2520The%2520emergence%2520of%2520vision%2520foundation%2520models%252C%2520especially%2520Segment%2520Anything%252C%2520offers%2520a%2520path%2520to%2520universal%2520segmentation%2520for%2520medical%2520images%252C%2520overcoming%2520these%2520issues.%2520Here%252C%2520we%2520study%2520how%2520to%2520improve%2520Segment%2520Anything%2520for%2520medical%2520images%2520by%2520comparing%2520different%2520finetuning%2520strategies%2520on%2520a%2520large%2520and%2520diverse%2520dataset.%2520We%2520evaluate%2520the%2520finetuned%2520models%2520on%2520a%2520wide%2520range%2520of%2520interactive%2520and%2520%2528automatic%2529%2520semantic%2520segmentation%2520tasks.%2520We%2520find%2520that%2520the%2520performance%2520can%2520be%2520clearly%2520improved%2520for%2520interactive%2520segmentation.%2520However%252C%2520semantic%2520segmentation%2520does%2520not%2520benefit%2520from%2520pretraining%2520on%2520medical%2520images.%2520Our%2520best%2520model%252C%2520MedicoSAM%252C%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/computational-cell-analytics/medico-sam.%2520We%2520show%2520that%2520it%2520is%2520compatible%2520with%2520existing%2520tools%2520for%2520data%2520annotation%2520and%2520believe%2520that%2520it%2520will%2520be%2520of%2520great%2520practical%2520value.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedicoSAM%3A%20Robust%20Improvement%20of%20SAM%20for%20Medical%20Imaging&entry.906535625=Anwai%20Archit%20and%20Luca%20Freckmann%20and%20Constantin%20Pape&entry.1292438233=Medical%20image%20segmentation%20is%20an%20important%20analysis%20task%20in%20clinical%20practice%20and%20research.%20Deep%20learning%20has%20massively%20advanced%20the%20field%2C%20but%20current%20approaches%20are%20mostly%20based%20on%20models%20trained%20for%20a%20specific%20task.%20Training%20such%20models%20or%20adapting%20them%20to%20a%20new%20condition%20is%20costly%20due%20to%20the%20need%20for%20%28manually%29%20labeled%20data.%20The%20emergence%20of%20vision%20foundation%20models%2C%20especially%20Segment%20Anything%2C%20offers%20a%20path%20to%20universal%20segmentation%20for%20medical%20images%2C%20overcoming%20these%20issues.%20Here%2C%20we%20study%20how%20to%20improve%20Segment%20Anything%20for%20medical%20images%20by%20comparing%20different%20finetuning%20strategies%20on%20a%20large%20and%20diverse%20dataset.%20We%20evaluate%20the%20finetuned%20models%20on%20a%20wide%20range%20of%20interactive%20and%20%28automatic%29%20semantic%20segmentation%20tasks.%20We%20find%20that%20the%20performance%20can%20be%20clearly%20improved%20for%20interactive%20segmentation.%20However%2C%20semantic%20segmentation%20does%20not%20benefit%20from%20pretraining%20on%20medical%20images.%20Our%20best%20model%2C%20MedicoSAM%2C%20is%20publicly%20available%20at%20https%3A//github.com/computational-cell-analytics/medico-sam.%20We%20show%20that%20it%20is%20compatible%20with%20existing%20tools%20for%20data%20annotation%20and%20believe%20that%20it%20will%20be%20of%20great%20practical%20value.&entry.1838667208=http%3A//arxiv.org/abs/2501.11734v2&entry.124074799=Read"},
{"title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems", "author": "Kamel Kamel and Hridoy Sankar Dutta and Keshav Sood and Sunil Aryal", "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.", "link": "http://arxiv.org/abs/2509.07677v3", "date": "2025-12-17", "relevancy": 2.2097, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4584}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&body=Title%3A%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems%0AAuthor%3A%20Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal%0AAbstract%3A%20Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%20verification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%20as%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%20face%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%20adversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%20detection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%20While%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%20rely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%20methods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%20that%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%20audio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%20creates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%20%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%20SMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%20combined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%20systems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%20demonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%20adversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%20toward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%20capable%20of%20evolving%20with%20the%20threat%20landscape.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%253A%2520A%2520Black-box%2520Adversarial%2520Attack%2520against%2520Voice%2520Authentication%2520and%2520Anti-Spoofing%2520Systems%26entry.906535625%3DKamel%2520Kamel%2520and%2520Hridoy%2520Sankar%2520Dutta%2520and%2520Keshav%2520Sood%2520and%2520Sunil%2520Aryal%26entry.1292438233%3DVoice%2520Authentication%2520Systems%2520%2528VAS%2529%2520use%2520unique%2520vocal%2520characteristics%2520for%2520verification.%2520They%2520are%2520increasingly%2520integrated%2520into%2520high-security%2520sectors%2520such%2520as%2520banking%2520and%2520healthcare.%2520Despite%2520their%2520improvements%2520using%2520deep%2520learning%252C%2520they%2520face%2520severe%2520vulnerabilities%2520from%2520sophisticated%2520threats%2520like%2520deepfakes%2520and%2520adversarial%2520attacks.%2520The%2520emergence%2520of%2520realistic%2520voice%2520cloning%2520complicates%2520detection%252C%2520as%2520systems%2520struggle%2520to%2520distinguish%2520authentic%2520from%2520synthetic%2520audio.%2520While%2520anti-spoofing%2520countermeasures%2520%2528CMs%2529%2520exist%2520to%2520mitigate%2520these%2520risks%252C%2520many%2520rely%2520on%2520static%2520detection%2520models%2520that%2520can%2520be%2520bypassed%2520by%2520novel%2520adversarial%2520methods%252C%2520leaving%2520a%2520critical%2520security%2520gap.%2520To%2520demonstrate%2520this%2520vulnerability%252C%2520we%2520propose%2520the%2520Spectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%252C%2520a%2520novel%2520method%2520that%2520strategically%2520manipulates%2520inaudible%2520frequency%2520regions%2520of%2520AI-generated%2520audio.%2520By%2520altering%2520the%2520voice%2520in%2520imperceptible%2520zones%2520to%2520the%2520human%2520ear%252C%2520SMIA%2520creates%2520adversarial%2520samples%2520that%2520sound%2520authentic%2520while%2520deceiving%2520CMs.%2520We%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520our%2520attack%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520across%2520multiple%2520tasks%252C%2520under%2520simulated%2520real-world%2520conditions.%2520SMIA%2520achieved%2520a%2520strong%2520attack%2520success%2520rate%2520%2528ASR%2529%2520of%2520at%2520least%252082%2525%2520against%2520combined%2520VAS/CM%2520systems%252C%2520at%2520least%252097.5%2525%2520against%2520standalone%2520speaker%2520verification%2520systems%252C%2520and%2520100%2525%2520against%2520countermeasures.%2520These%2520findings%2520conclusively%2520demonstrate%2520that%2520current%2520security%2520postures%2520are%2520insufficient%2520against%2520adaptive%2520adversarial%2520attacks.%2520This%2520work%2520highlights%2520the%2520urgent%2520need%2520for%2520a%2520paradigm%2520shift%2520toward%2520next-generation%2520defenses%2520that%2520employ%2520dynamic%252C%2520context-aware%2520frameworks%2520capable%2520of%2520evolving%2520with%2520the%2520threat%2520landscape.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&entry.906535625=Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal&entry.1292438233=Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%20verification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%20as%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%20face%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%20adversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%20detection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%20While%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%20rely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%20methods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%20that%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%20audio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%20creates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%20conducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%20%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%20SMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%20combined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%20systems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%20demonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%20adversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%20toward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%20capable%20of%20evolving%20with%20the%20threat%20landscape.&entry.1838667208=http%3A//arxiv.org/abs/2509.07677v3&entry.124074799=Read"},
{"title": "Evaluating Large Language Models in Scientific Discovery", "author": "Zhangde Song and Jieyu Lu and Yuanqi Du and Botao Yu and Thomas M. Pruyn and Yue Huang and Kehan Guo and Xiuzhe Luo and Yuanhao Qu and Yi Qu and Yinkai Wang and Haorui Wang and Jeff Guo and Jingru Gan and Parshin Shojaee and Di Luo and Andres M Bran and Gen Li and Qiyuan Zhao and Shao-Xiong Lennon Luo and Yuxuan Zhang and Xiang Zou and Wanru Zhao and Yifan F. Zhang and Wucheng Zhang and Shunan Zheng and Saiyang Zhang and Sartaaj Takrim Khan and Mahyar Rajabi-Kochi and Samantha Paradi-Maropakis and Tony Baltoiu and Fengyu Xie and Tianyang Chen and Kexin Huang and Weiliang Luo and Meijing Fang and Xin Yang and Lixue Cheng and Jiajun He and Soha Hassoun and Xiangliang Zhang and Wei Wang and Chandan K. Reddy and Chao Zhang and Zhiling Zheng and Mengdi Wang and Le Cong and Carla P. Gomes and Chang-Yu Hsieh and Aditya Nandy and Philippe Schwaller and Heather J. Kulik and Haojun Jia and Huan Sun and Seyed Mohamad Moosavi and Chenru Duan", "abstract": "Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.", "link": "http://arxiv.org/abs/2512.15567v1", "date": "2025-12-17", "relevancy": 2.206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Language%20Models%20in%20Scientific%20Discovery&body=Title%3A%20Evaluating%20Large%20Language%20Models%20in%20Scientific%20Discovery%0AAuthor%3A%20Zhangde%20Song%20and%20Jieyu%20Lu%20and%20Yuanqi%20Du%20and%20Botao%20Yu%20and%20Thomas%20M.%20Pruyn%20and%20Yue%20Huang%20and%20Kehan%20Guo%20and%20Xiuzhe%20Luo%20and%20Yuanhao%20Qu%20and%20Yi%20Qu%20and%20Yinkai%20Wang%20and%20Haorui%20Wang%20and%20Jeff%20Guo%20and%20Jingru%20Gan%20and%20Parshin%20Shojaee%20and%20Di%20Luo%20and%20Andres%20M%20Bran%20and%20Gen%20Li%20and%20Qiyuan%20Zhao%20and%20Shao-Xiong%20Lennon%20Luo%20and%20Yuxuan%20Zhang%20and%20Xiang%20Zou%20and%20Wanru%20Zhao%20and%20Yifan%20F.%20Zhang%20and%20Wucheng%20Zhang%20and%20Shunan%20Zheng%20and%20Saiyang%20Zhang%20and%20Sartaaj%20Takrim%20Khan%20and%20Mahyar%20Rajabi-Kochi%20and%20Samantha%20Paradi-Maropakis%20and%20Tony%20Baltoiu%20and%20Fengyu%20Xie%20and%20Tianyang%20Chen%20and%20Kexin%20Huang%20and%20Weiliang%20Luo%20and%20Meijing%20Fang%20and%20Xin%20Yang%20and%20Lixue%20Cheng%20and%20Jiajun%20He%20and%20Soha%20Hassoun%20and%20Xiangliang%20Zhang%20and%20Wei%20Wang%20and%20Chandan%20K.%20Reddy%20and%20Chao%20Zhang%20and%20Zhiling%20Zheng%20and%20Mengdi%20Wang%20and%20Le%20Cong%20and%20Carla%20P.%20Gomes%20and%20Chang-Yu%20Hsieh%20and%20Aditya%20Nandy%20and%20Philippe%20Schwaller%20and%20Heather%20J.%20Kulik%20and%20Haojun%20Jia%20and%20Huan%20Sun%20and%20Seyed%20Mohamad%20Moosavi%20and%20Chenru%20Duan%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20to%20scientific%20research%2C%20yet%20prevailing%20science%20benchmarks%20probe%20decontextualized%20knowledge%20and%20overlook%20the%20iterative%20reasoning%2C%20hypothesis%20generation%2C%20and%20observation%20interpretation%20that%20drive%20scientific%20discovery.%20We%20introduce%20a%20scenario-grounded%20benchmark%20that%20evaluates%20LLMs%20across%20biology%2C%20chemistry%2C%20materials%2C%20and%20physics%2C%20where%20domain%20experts%20define%20research%20projects%20of%20genuine%20interest%20and%20decompose%20them%20into%20modular%20research%20scenarios%20from%20which%20vetted%20questions%20are%20sampled.%20The%20framework%20assesses%20models%20at%20two%20levels%3A%20%28i%29%20question-level%20accuracy%20on%20scenario-tied%20items%20and%20%28ii%29%20project-level%20performance%2C%20where%20models%20must%20propose%20testable%20hypotheses%2C%20design%20simulations%20or%20experiments%2C%20and%20interpret%20results.%20Applying%20this%20two-phase%20scientific%20discovery%20evaluation%20%28SDE%29%20framework%20to%20state-of-the-art%20LLMs%20reveals%20a%20consistent%20performance%20gap%20relative%20to%20general%20science%20benchmarks%2C%20diminishing%20return%20of%20scaling%20up%20model%20sizes%20and%20reasoning%2C%20and%20systematic%20weaknesses%20shared%20across%20top-tier%20models%20from%20different%20providers.%20Large%20performance%20variation%20in%20research%20scenarios%20leads%20to%20changing%20choices%20of%20the%20best%20performing%20model%20on%20scientific%20discovery%20projects%20evaluated%2C%20suggesting%20all%20current%20LLMs%20are%20distant%20to%20general%20scientific%20%22superintelligence%22.%20Nevertheless%2C%20LLMs%20already%20demonstrate%20promise%20in%20a%20great%20variety%20of%20scientific%20discovery%20projects%2C%20including%20cases%20where%20constituent%20scenario%20scores%20are%20low%2C%20highlighting%20the%20role%20of%20guided%20exploration%20and%20serendipity%20in%20discovery.%20This%20SDE%20framework%20offers%20a%20reproducible%20benchmark%20for%20discovery-relevant%20evaluation%20of%20LLMs%20and%20charts%20practical%20paths%20to%20advance%20their%20development%20toward%20scientific%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Language%2520Models%2520in%2520Scientific%2520Discovery%26entry.906535625%3DZhangde%2520Song%2520and%2520Jieyu%2520Lu%2520and%2520Yuanqi%2520Du%2520and%2520Botao%2520Yu%2520and%2520Thomas%2520M.%2520Pruyn%2520and%2520Yue%2520Huang%2520and%2520Kehan%2520Guo%2520and%2520Xiuzhe%2520Luo%2520and%2520Yuanhao%2520Qu%2520and%2520Yi%2520Qu%2520and%2520Yinkai%2520Wang%2520and%2520Haorui%2520Wang%2520and%2520Jeff%2520Guo%2520and%2520Jingru%2520Gan%2520and%2520Parshin%2520Shojaee%2520and%2520Di%2520Luo%2520and%2520Andres%2520M%2520Bran%2520and%2520Gen%2520Li%2520and%2520Qiyuan%2520Zhao%2520and%2520Shao-Xiong%2520Lennon%2520Luo%2520and%2520Yuxuan%2520Zhang%2520and%2520Xiang%2520Zou%2520and%2520Wanru%2520Zhao%2520and%2520Yifan%2520F.%2520Zhang%2520and%2520Wucheng%2520Zhang%2520and%2520Shunan%2520Zheng%2520and%2520Saiyang%2520Zhang%2520and%2520Sartaaj%2520Takrim%2520Khan%2520and%2520Mahyar%2520Rajabi-Kochi%2520and%2520Samantha%2520Paradi-Maropakis%2520and%2520Tony%2520Baltoiu%2520and%2520Fengyu%2520Xie%2520and%2520Tianyang%2520Chen%2520and%2520Kexin%2520Huang%2520and%2520Weiliang%2520Luo%2520and%2520Meijing%2520Fang%2520and%2520Xin%2520Yang%2520and%2520Lixue%2520Cheng%2520and%2520Jiajun%2520He%2520and%2520Soha%2520Hassoun%2520and%2520Xiangliang%2520Zhang%2520and%2520Wei%2520Wang%2520and%2520Chandan%2520K.%2520Reddy%2520and%2520Chao%2520Zhang%2520and%2520Zhiling%2520Zheng%2520and%2520Mengdi%2520Wang%2520and%2520Le%2520Cong%2520and%2520Carla%2520P.%2520Gomes%2520and%2520Chang-Yu%2520Hsieh%2520and%2520Aditya%2520Nandy%2520and%2520Philippe%2520Schwaller%2520and%2520Heather%2520J.%2520Kulik%2520and%2520Haojun%2520Jia%2520and%2520Huan%2520Sun%2520and%2520Seyed%2520Mohamad%2520Moosavi%2520and%2520Chenru%2520Duan%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520to%2520scientific%2520research%252C%2520yet%2520prevailing%2520science%2520benchmarks%2520probe%2520decontextualized%2520knowledge%2520and%2520overlook%2520the%2520iterative%2520reasoning%252C%2520hypothesis%2520generation%252C%2520and%2520observation%2520interpretation%2520that%2520drive%2520scientific%2520discovery.%2520We%2520introduce%2520a%2520scenario-grounded%2520benchmark%2520that%2520evaluates%2520LLMs%2520across%2520biology%252C%2520chemistry%252C%2520materials%252C%2520and%2520physics%252C%2520where%2520domain%2520experts%2520define%2520research%2520projects%2520of%2520genuine%2520interest%2520and%2520decompose%2520them%2520into%2520modular%2520research%2520scenarios%2520from%2520which%2520vetted%2520questions%2520are%2520sampled.%2520The%2520framework%2520assesses%2520models%2520at%2520two%2520levels%253A%2520%2528i%2529%2520question-level%2520accuracy%2520on%2520scenario-tied%2520items%2520and%2520%2528ii%2529%2520project-level%2520performance%252C%2520where%2520models%2520must%2520propose%2520testable%2520hypotheses%252C%2520design%2520simulations%2520or%2520experiments%252C%2520and%2520interpret%2520results.%2520Applying%2520this%2520two-phase%2520scientific%2520discovery%2520evaluation%2520%2528SDE%2529%2520framework%2520to%2520state-of-the-art%2520LLMs%2520reveals%2520a%2520consistent%2520performance%2520gap%2520relative%2520to%2520general%2520science%2520benchmarks%252C%2520diminishing%2520return%2520of%2520scaling%2520up%2520model%2520sizes%2520and%2520reasoning%252C%2520and%2520systematic%2520weaknesses%2520shared%2520across%2520top-tier%2520models%2520from%2520different%2520providers.%2520Large%2520performance%2520variation%2520in%2520research%2520scenarios%2520leads%2520to%2520changing%2520choices%2520of%2520the%2520best%2520performing%2520model%2520on%2520scientific%2520discovery%2520projects%2520evaluated%252C%2520suggesting%2520all%2520current%2520LLMs%2520are%2520distant%2520to%2520general%2520scientific%2520%2522superintelligence%2522.%2520Nevertheless%252C%2520LLMs%2520already%2520demonstrate%2520promise%2520in%2520a%2520great%2520variety%2520of%2520scientific%2520discovery%2520projects%252C%2520including%2520cases%2520where%2520constituent%2520scenario%2520scores%2520are%2520low%252C%2520highlighting%2520the%2520role%2520of%2520guided%2520exploration%2520and%2520serendipity%2520in%2520discovery.%2520This%2520SDE%2520framework%2520offers%2520a%2520reproducible%2520benchmark%2520for%2520discovery-relevant%2520evaluation%2520of%2520LLMs%2520and%2520charts%2520practical%2520paths%2520to%2520advance%2520their%2520development%2520toward%2520scientific%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Language%20Models%20in%20Scientific%20Discovery&entry.906535625=Zhangde%20Song%20and%20Jieyu%20Lu%20and%20Yuanqi%20Du%20and%20Botao%20Yu%20and%20Thomas%20M.%20Pruyn%20and%20Yue%20Huang%20and%20Kehan%20Guo%20and%20Xiuzhe%20Luo%20and%20Yuanhao%20Qu%20and%20Yi%20Qu%20and%20Yinkai%20Wang%20and%20Haorui%20Wang%20and%20Jeff%20Guo%20and%20Jingru%20Gan%20and%20Parshin%20Shojaee%20and%20Di%20Luo%20and%20Andres%20M%20Bran%20and%20Gen%20Li%20and%20Qiyuan%20Zhao%20and%20Shao-Xiong%20Lennon%20Luo%20and%20Yuxuan%20Zhang%20and%20Xiang%20Zou%20and%20Wanru%20Zhao%20and%20Yifan%20F.%20Zhang%20and%20Wucheng%20Zhang%20and%20Shunan%20Zheng%20and%20Saiyang%20Zhang%20and%20Sartaaj%20Takrim%20Khan%20and%20Mahyar%20Rajabi-Kochi%20and%20Samantha%20Paradi-Maropakis%20and%20Tony%20Baltoiu%20and%20Fengyu%20Xie%20and%20Tianyang%20Chen%20and%20Kexin%20Huang%20and%20Weiliang%20Luo%20and%20Meijing%20Fang%20and%20Xin%20Yang%20and%20Lixue%20Cheng%20and%20Jiajun%20He%20and%20Soha%20Hassoun%20and%20Xiangliang%20Zhang%20and%20Wei%20Wang%20and%20Chandan%20K.%20Reddy%20and%20Chao%20Zhang%20and%20Zhiling%20Zheng%20and%20Mengdi%20Wang%20and%20Le%20Cong%20and%20Carla%20P.%20Gomes%20and%20Chang-Yu%20Hsieh%20and%20Aditya%20Nandy%20and%20Philippe%20Schwaller%20and%20Heather%20J.%20Kulik%20and%20Haojun%20Jia%20and%20Huan%20Sun%20and%20Seyed%20Mohamad%20Moosavi%20and%20Chenru%20Duan&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20to%20scientific%20research%2C%20yet%20prevailing%20science%20benchmarks%20probe%20decontextualized%20knowledge%20and%20overlook%20the%20iterative%20reasoning%2C%20hypothesis%20generation%2C%20and%20observation%20interpretation%20that%20drive%20scientific%20discovery.%20We%20introduce%20a%20scenario-grounded%20benchmark%20that%20evaluates%20LLMs%20across%20biology%2C%20chemistry%2C%20materials%2C%20and%20physics%2C%20where%20domain%20experts%20define%20research%20projects%20of%20genuine%20interest%20and%20decompose%20them%20into%20modular%20research%20scenarios%20from%20which%20vetted%20questions%20are%20sampled.%20The%20framework%20assesses%20models%20at%20two%20levels%3A%20%28i%29%20question-level%20accuracy%20on%20scenario-tied%20items%20and%20%28ii%29%20project-level%20performance%2C%20where%20models%20must%20propose%20testable%20hypotheses%2C%20design%20simulations%20or%20experiments%2C%20and%20interpret%20results.%20Applying%20this%20two-phase%20scientific%20discovery%20evaluation%20%28SDE%29%20framework%20to%20state-of-the-art%20LLMs%20reveals%20a%20consistent%20performance%20gap%20relative%20to%20general%20science%20benchmarks%2C%20diminishing%20return%20of%20scaling%20up%20model%20sizes%20and%20reasoning%2C%20and%20systematic%20weaknesses%20shared%20across%20top-tier%20models%20from%20different%20providers.%20Large%20performance%20variation%20in%20research%20scenarios%20leads%20to%20changing%20choices%20of%20the%20best%20performing%20model%20on%20scientific%20discovery%20projects%20evaluated%2C%20suggesting%20all%20current%20LLMs%20are%20distant%20to%20general%20scientific%20%22superintelligence%22.%20Nevertheless%2C%20LLMs%20already%20demonstrate%20promise%20in%20a%20great%20variety%20of%20scientific%20discovery%20projects%2C%20including%20cases%20where%20constituent%20scenario%20scores%20are%20low%2C%20highlighting%20the%20role%20of%20guided%20exploration%20and%20serendipity%20in%20discovery.%20This%20SDE%20framework%20offers%20a%20reproducible%20benchmark%20for%20discovery-relevant%20evaluation%20of%20LLMs%20and%20charts%20practical%20paths%20to%20advance%20their%20development%20toward%20scientific%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2512.15567v1&entry.124074799=Read"},
{"title": "BLANKET: Anonymizing Faces in Infant Video Recordings", "author": "Ditmar Hadera and Jan Cech and Miroslav Purkrabek and Matej Hoffmann", "abstract": "Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.", "link": "http://arxiv.org/abs/2512.15542v1", "date": "2025-12-17", "relevancy": 2.1992, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5385}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLANKET%3A%20Anonymizing%20Faces%20in%20Infant%20Video%20Recordings&body=Title%3A%20BLANKET%3A%20Anonymizing%20Faces%20in%20Infant%20Video%20Recordings%0AAuthor%3A%20Ditmar%20Hadera%20and%20Jan%20Cech%20and%20Miroslav%20Purkrabek%20and%20Matej%20Hoffmann%0AAbstract%3A%20Ensuring%20the%20ethical%20use%20of%20video%20data%20involving%20human%20subjects%2C%20particularly%20infants%2C%20requires%20robust%20anonymization%20methods.%20We%20propose%20BLANKET%20%28Baby-face%20Landmark-preserving%20ANonymization%20with%20Keypoint%20dEtection%20consisTency%29%2C%20a%20novel%20approach%20designed%20to%20anonymize%20infant%20faces%20in%20video%20recordings%20while%20preserving%20essential%20facial%20attributes.%20Our%20method%20comprises%20two%20stages.%20First%2C%20a%20new%20random%20face%2C%20compatible%20with%20the%20original%20identity%2C%20is%20generated%20via%20inpainting%20using%20a%20diffusion%20model.%20Second%2C%20the%20new%20identity%20is%20seamlessly%20incorporated%20into%20each%20video%20frame%20through%20temporally%20consistent%20face%20swapping%20with%20authentic%20expression%20transfer.%20The%20method%20is%20evaluated%20on%20a%20dataset%20of%20short%20video%20recordings%20of%20babies%20and%20is%20compared%20to%20the%20popular%20anonymization%20method%2C%20DeepPrivacy2.%20Key%20metrics%20assessed%20include%20the%20level%20of%20de-identification%2C%20preservation%20of%20facial%20attributes%2C%20impact%20on%20human%20pose%20estimation%20%28as%20an%20example%20of%20a%20downstream%20task%29%2C%20and%20presence%20of%20artifacts.%20Both%20methods%20alter%20the%20identity%2C%20and%20our%20method%20outperforms%20DeepPrivacy2%20in%20all%20other%20respects.%20The%20code%20is%20available%20as%20an%20easy-to-use%20anonymization%20demo%20at%20https%3A//github.com/ctu-vras/blanket-infant-face-anonym.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLANKET%253A%2520Anonymizing%2520Faces%2520in%2520Infant%2520Video%2520Recordings%26entry.906535625%3DDitmar%2520Hadera%2520and%2520Jan%2520Cech%2520and%2520Miroslav%2520Purkrabek%2520and%2520Matej%2520Hoffmann%26entry.1292438233%3DEnsuring%2520the%2520ethical%2520use%2520of%2520video%2520data%2520involving%2520human%2520subjects%252C%2520particularly%2520infants%252C%2520requires%2520robust%2520anonymization%2520methods.%2520We%2520propose%2520BLANKET%2520%2528Baby-face%2520Landmark-preserving%2520ANonymization%2520with%2520Keypoint%2520dEtection%2520consisTency%2529%252C%2520a%2520novel%2520approach%2520designed%2520to%2520anonymize%2520infant%2520faces%2520in%2520video%2520recordings%2520while%2520preserving%2520essential%2520facial%2520attributes.%2520Our%2520method%2520comprises%2520two%2520stages.%2520First%252C%2520a%2520new%2520random%2520face%252C%2520compatible%2520with%2520the%2520original%2520identity%252C%2520is%2520generated%2520via%2520inpainting%2520using%2520a%2520diffusion%2520model.%2520Second%252C%2520the%2520new%2520identity%2520is%2520seamlessly%2520incorporated%2520into%2520each%2520video%2520frame%2520through%2520temporally%2520consistent%2520face%2520swapping%2520with%2520authentic%2520expression%2520transfer.%2520The%2520method%2520is%2520evaluated%2520on%2520a%2520dataset%2520of%2520short%2520video%2520recordings%2520of%2520babies%2520and%2520is%2520compared%2520to%2520the%2520popular%2520anonymization%2520method%252C%2520DeepPrivacy2.%2520Key%2520metrics%2520assessed%2520include%2520the%2520level%2520of%2520de-identification%252C%2520preservation%2520of%2520facial%2520attributes%252C%2520impact%2520on%2520human%2520pose%2520estimation%2520%2528as%2520an%2520example%2520of%2520a%2520downstream%2520task%2529%252C%2520and%2520presence%2520of%2520artifacts.%2520Both%2520methods%2520alter%2520the%2520identity%252C%2520and%2520our%2520method%2520outperforms%2520DeepPrivacy2%2520in%2520all%2520other%2520respects.%2520The%2520code%2520is%2520available%2520as%2520an%2520easy-to-use%2520anonymization%2520demo%2520at%2520https%253A//github.com/ctu-vras/blanket-infant-face-anonym.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLANKET%3A%20Anonymizing%20Faces%20in%20Infant%20Video%20Recordings&entry.906535625=Ditmar%20Hadera%20and%20Jan%20Cech%20and%20Miroslav%20Purkrabek%20and%20Matej%20Hoffmann&entry.1292438233=Ensuring%20the%20ethical%20use%20of%20video%20data%20involving%20human%20subjects%2C%20particularly%20infants%2C%20requires%20robust%20anonymization%20methods.%20We%20propose%20BLANKET%20%28Baby-face%20Landmark-preserving%20ANonymization%20with%20Keypoint%20dEtection%20consisTency%29%2C%20a%20novel%20approach%20designed%20to%20anonymize%20infant%20faces%20in%20video%20recordings%20while%20preserving%20essential%20facial%20attributes.%20Our%20method%20comprises%20two%20stages.%20First%2C%20a%20new%20random%20face%2C%20compatible%20with%20the%20original%20identity%2C%20is%20generated%20via%20inpainting%20using%20a%20diffusion%20model.%20Second%2C%20the%20new%20identity%20is%20seamlessly%20incorporated%20into%20each%20video%20frame%20through%20temporally%20consistent%20face%20swapping%20with%20authentic%20expression%20transfer.%20The%20method%20is%20evaluated%20on%20a%20dataset%20of%20short%20video%20recordings%20of%20babies%20and%20is%20compared%20to%20the%20popular%20anonymization%20method%2C%20DeepPrivacy2.%20Key%20metrics%20assessed%20include%20the%20level%20of%20de-identification%2C%20preservation%20of%20facial%20attributes%2C%20impact%20on%20human%20pose%20estimation%20%28as%20an%20example%20of%20a%20downstream%20task%29%2C%20and%20presence%20of%20artifacts.%20Both%20methods%20alter%20the%20identity%2C%20and%20our%20method%20outperforms%20DeepPrivacy2%20in%20all%20other%20respects.%20The%20code%20is%20available%20as%20an%20easy-to-use%20anonymization%20demo%20at%20https%3A//github.com/ctu-vras/blanket-infant-face-anonym.&entry.1838667208=http%3A//arxiv.org/abs/2512.15542v1&entry.124074799=Read"},
{"title": "Geometry and Optimization of Shallow Polynomial Networks", "author": "Yossi Arjevani and Joan Bruna and Joe Kileel and Elzbieta Polak and Matthew Trager", "abstract": "We study shallow neural networks with monomial activations and output dimension one. The function space for these models can be identified with a set of symmetric tensors with bounded rank. We describe general features of these networks, focusing on the relationship between width and optimization. We then consider teacher-student problems, which can be viewed as problems of low-rank tensor approximation with respect to non-standard inner products that are induced by the data distribution. In this setting, we introduce a teacher-metric data discriminant which encodes the qualitative behavior of the optimization as a function of the training data distribution. Finally, we focus on networks with quadratic activations, presenting an in-depth analysis of the optimization landscape. In particular, we present a variation of the Eckart-Young Theorem characterizing all critical points and their Hessian signatures for teacher-student problems with quadratic networks and Gaussian training data.", "link": "http://arxiv.org/abs/2501.06074v2", "date": "2025-12-17", "relevancy": 2.1979, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4391}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks&body=Title%3A%20Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks%0AAuthor%3A%20Yossi%20Arjevani%20and%20Joan%20Bruna%20and%20Joe%20Kileel%20and%20Elzbieta%20Polak%20and%20Matthew%20Trager%0AAbstract%3A%20We%20study%20shallow%20neural%20networks%20with%20monomial%20activations%20and%20output%20dimension%20one.%20The%20function%20space%20for%20these%20models%20can%20be%20identified%20with%20a%20set%20of%20symmetric%20tensors%20with%20bounded%20rank.%20We%20describe%20general%20features%20of%20these%20networks%2C%20focusing%20on%20the%20relationship%20between%20width%20and%20optimization.%20We%20then%20consider%20teacher-student%20problems%2C%20which%20can%20be%20viewed%20as%20problems%20of%20low-rank%20tensor%20approximation%20with%20respect%20to%20non-standard%20inner%20products%20that%20are%20induced%20by%20the%20data%20distribution.%20In%20this%20setting%2C%20we%20introduce%20a%20teacher-metric%20data%20discriminant%20which%20encodes%20the%20qualitative%20behavior%20of%20the%20optimization%20as%20a%20function%20of%20the%20training%20data%20distribution.%20Finally%2C%20we%20focus%20on%20networks%20with%20quadratic%20activations%2C%20presenting%20an%20in-depth%20analysis%20of%20the%20optimization%20landscape.%20In%20particular%2C%20we%20present%20a%20variation%20of%20the%20Eckart-Young%20Theorem%20characterizing%20all%20critical%20points%20and%20their%20Hessian%20signatures%20for%20teacher-student%20problems%20with%20quadratic%20networks%20and%20Gaussian%20training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2501.06074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520and%2520Optimization%2520of%2520Shallow%2520Polynomial%2520Networks%26entry.906535625%3DYossi%2520Arjevani%2520and%2520Joan%2520Bruna%2520and%2520Joe%2520Kileel%2520and%2520Elzbieta%2520Polak%2520and%2520Matthew%2520Trager%26entry.1292438233%3DWe%2520study%2520shallow%2520neural%2520networks%2520with%2520monomial%2520activations%2520and%2520output%2520dimension%2520one.%2520The%2520function%2520space%2520for%2520these%2520models%2520can%2520be%2520identified%2520with%2520a%2520set%2520of%2520symmetric%2520tensors%2520with%2520bounded%2520rank.%2520We%2520describe%2520general%2520features%2520of%2520these%2520networks%252C%2520focusing%2520on%2520the%2520relationship%2520between%2520width%2520and%2520optimization.%2520We%2520then%2520consider%2520teacher-student%2520problems%252C%2520which%2520can%2520be%2520viewed%2520as%2520problems%2520of%2520low-rank%2520tensor%2520approximation%2520with%2520respect%2520to%2520non-standard%2520inner%2520products%2520that%2520are%2520induced%2520by%2520the%2520data%2520distribution.%2520In%2520this%2520setting%252C%2520we%2520introduce%2520a%2520teacher-metric%2520data%2520discriminant%2520which%2520encodes%2520the%2520qualitative%2520behavior%2520of%2520the%2520optimization%2520as%2520a%2520function%2520of%2520the%2520training%2520data%2520distribution.%2520Finally%252C%2520we%2520focus%2520on%2520networks%2520with%2520quadratic%2520activations%252C%2520presenting%2520an%2520in-depth%2520analysis%2520of%2520the%2520optimization%2520landscape.%2520In%2520particular%252C%2520we%2520present%2520a%2520variation%2520of%2520the%2520Eckart-Young%2520Theorem%2520characterizing%2520all%2520critical%2520points%2520and%2520their%2520Hessian%2520signatures%2520for%2520teacher-student%2520problems%2520with%2520quadratic%2520networks%2520and%2520Gaussian%2520training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20and%20Optimization%20of%20Shallow%20Polynomial%20Networks&entry.906535625=Yossi%20Arjevani%20and%20Joan%20Bruna%20and%20Joe%20Kileel%20and%20Elzbieta%20Polak%20and%20Matthew%20Trager&entry.1292438233=We%20study%20shallow%20neural%20networks%20with%20monomial%20activations%20and%20output%20dimension%20one.%20The%20function%20space%20for%20these%20models%20can%20be%20identified%20with%20a%20set%20of%20symmetric%20tensors%20with%20bounded%20rank.%20We%20describe%20general%20features%20of%20these%20networks%2C%20focusing%20on%20the%20relationship%20between%20width%20and%20optimization.%20We%20then%20consider%20teacher-student%20problems%2C%20which%20can%20be%20viewed%20as%20problems%20of%20low-rank%20tensor%20approximation%20with%20respect%20to%20non-standard%20inner%20products%20that%20are%20induced%20by%20the%20data%20distribution.%20In%20this%20setting%2C%20we%20introduce%20a%20teacher-metric%20data%20discriminant%20which%20encodes%20the%20qualitative%20behavior%20of%20the%20optimization%20as%20a%20function%20of%20the%20training%20data%20distribution.%20Finally%2C%20we%20focus%20on%20networks%20with%20quadratic%20activations%2C%20presenting%20an%20in-depth%20analysis%20of%20the%20optimization%20landscape.%20In%20particular%2C%20we%20present%20a%20variation%20of%20the%20Eckart-Young%20Theorem%20characterizing%20all%20critical%20points%20and%20their%20Hessian%20signatures%20for%20teacher-student%20problems%20with%20quadratic%20networks%20and%20Gaussian%20training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2501.06074v2&entry.124074799=Read"},
{"title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)", "author": "Zhichen Lai and Hua Lu and Huan Li and Jialiang Li and Christian S. Jensen", "abstract": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.", "link": "http://arxiv.org/abs/2511.12061v2", "date": "2025-12-17", "relevancy": 2.1946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MovSemCL%3A%20Movement-Semantics%20Contrastive%20Learning%20for%20Trajectory%20Similarity%20%28Extension%29&body=Title%3A%20MovSemCL%3A%20Movement-Semantics%20Contrastive%20Learning%20for%20Trajectory%20Similarity%20%28Extension%29%0AAuthor%3A%20Zhichen%20Lai%20and%20Hua%20Lu%20and%20Huan%20Li%20and%20Jialiang%20Li%20and%20Christian%20S.%20Jensen%0AAbstract%3A%20Trajectory%20similarity%20computation%20is%20fundamental%20functionality%20that%20is%20used%20for%2C%20e.g.%2C%20clustering%2C%20prediction%2C%20and%20anomaly%20detection.%20However%2C%20existing%20learning-based%20methods%20exhibit%20three%20key%20limitations%3A%20%281%29%20insufficient%20modeling%20of%20trajectory%20semantics%20and%20hierarchy%2C%20lacking%20both%20movement%20dynamics%20extraction%20and%20multi-scale%20structural%20representation%3B%20%282%29%20high%20computational%20costs%20due%20to%20point-wise%20encoding%3B%20and%20%283%29%20use%20of%20physically%20implausible%20augmentations%20that%20distort%20trajectory%20semantics.%20To%20address%20these%20issues%2C%20we%20propose%20MovSemCL%2C%20a%20movement-semantics%20contrastive%20learning%20framework%20for%20trajectory%20similarity%20computation.%20MovSemCL%20first%20transforms%20raw%20GPS%20trajectories%20into%20movement-semantics%20features%20and%20then%20segments%20them%20into%20patches.%20Next%2C%20MovSemCL%20employs%20intra-%20and%20inter-patch%20attentions%20to%20encode%20local%20as%20well%20as%20global%20trajectory%20patterns%2C%20enabling%20efficient%20hierarchical%20representation%20and%20reducing%20computational%20costs.%20Moreover%2C%20MovSemCL%20includes%20a%20curvature-guided%20augmentation%20strategy%20that%20preserves%20informative%20segments%20%28e.g.%2C%20turns%20and%20intersections%29%20and%20masks%20redundant%20ones%2C%20generating%20physically%20plausible%20augmented%20views.%20Experiments%20on%20real-world%20datasets%20show%20that%20MovSemCL%20is%20capable%20of%20outperforming%20state-of-the-art%20methods%2C%20achieving%20mean%20ranks%20close%20to%20the%20ideal%20value%20of%201%20at%20similarity%20search%20tasks%20and%20improvements%20by%20up%20to%2020.3%25%20at%20heuristic%20approximation%2C%20while%20reducing%20inference%20latency%20by%20up%20to%2043.4%25.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovSemCL%253A%2520Movement-Semantics%2520Contrastive%2520Learning%2520for%2520Trajectory%2520Similarity%2520%2528Extension%2529%26entry.906535625%3DZhichen%2520Lai%2520and%2520Hua%2520Lu%2520and%2520Huan%2520Li%2520and%2520Jialiang%2520Li%2520and%2520Christian%2520S.%2520Jensen%26entry.1292438233%3DTrajectory%2520similarity%2520computation%2520is%2520fundamental%2520functionality%2520that%2520is%2520used%2520for%252C%2520e.g.%252C%2520clustering%252C%2520prediction%252C%2520and%2520anomaly%2520detection.%2520However%252C%2520existing%2520learning-based%2520methods%2520exhibit%2520three%2520key%2520limitations%253A%2520%25281%2529%2520insufficient%2520modeling%2520of%2520trajectory%2520semantics%2520and%2520hierarchy%252C%2520lacking%2520both%2520movement%2520dynamics%2520extraction%2520and%2520multi-scale%2520structural%2520representation%253B%2520%25282%2529%2520high%2520computational%2520costs%2520due%2520to%2520point-wise%2520encoding%253B%2520and%2520%25283%2529%2520use%2520of%2520physically%2520implausible%2520augmentations%2520that%2520distort%2520trajectory%2520semantics.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520MovSemCL%252C%2520a%2520movement-semantics%2520contrastive%2520learning%2520framework%2520for%2520trajectory%2520similarity%2520computation.%2520MovSemCL%2520first%2520transforms%2520raw%2520GPS%2520trajectories%2520into%2520movement-semantics%2520features%2520and%2520then%2520segments%2520them%2520into%2520patches.%2520Next%252C%2520MovSemCL%2520employs%2520intra-%2520and%2520inter-patch%2520attentions%2520to%2520encode%2520local%2520as%2520well%2520as%2520global%2520trajectory%2520patterns%252C%2520enabling%2520efficient%2520hierarchical%2520representation%2520and%2520reducing%2520computational%2520costs.%2520Moreover%252C%2520MovSemCL%2520includes%2520a%2520curvature-guided%2520augmentation%2520strategy%2520that%2520preserves%2520informative%2520segments%2520%2528e.g.%252C%2520turns%2520and%2520intersections%2529%2520and%2520masks%2520redundant%2520ones%252C%2520generating%2520physically%2520plausible%2520augmented%2520views.%2520Experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520MovSemCL%2520is%2520capable%2520of%2520outperforming%2520state-of-the-art%2520methods%252C%2520achieving%2520mean%2520ranks%2520close%2520to%2520the%2520ideal%2520value%2520of%25201%2520at%2520similarity%2520search%2520tasks%2520and%2520improvements%2520by%2520up%2520to%252020.3%2525%2520at%2520heuristic%2520approximation%252C%2520while%2520reducing%2520inference%2520latency%2520by%2520up%2520to%252043.4%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MovSemCL%3A%20Movement-Semantics%20Contrastive%20Learning%20for%20Trajectory%20Similarity%20%28Extension%29&entry.906535625=Zhichen%20Lai%20and%20Hua%20Lu%20and%20Huan%20Li%20and%20Jialiang%20Li%20and%20Christian%20S.%20Jensen&entry.1292438233=Trajectory%20similarity%20computation%20is%20fundamental%20functionality%20that%20is%20used%20for%2C%20e.g.%2C%20clustering%2C%20prediction%2C%20and%20anomaly%20detection.%20However%2C%20existing%20learning-based%20methods%20exhibit%20three%20key%20limitations%3A%20%281%29%20insufficient%20modeling%20of%20trajectory%20semantics%20and%20hierarchy%2C%20lacking%20both%20movement%20dynamics%20extraction%20and%20multi-scale%20structural%20representation%3B%20%282%29%20high%20computational%20costs%20due%20to%20point-wise%20encoding%3B%20and%20%283%29%20use%20of%20physically%20implausible%20augmentations%20that%20distort%20trajectory%20semantics.%20To%20address%20these%20issues%2C%20we%20propose%20MovSemCL%2C%20a%20movement-semantics%20contrastive%20learning%20framework%20for%20trajectory%20similarity%20computation.%20MovSemCL%20first%20transforms%20raw%20GPS%20trajectories%20into%20movement-semantics%20features%20and%20then%20segments%20them%20into%20patches.%20Next%2C%20MovSemCL%20employs%20intra-%20and%20inter-patch%20attentions%20to%20encode%20local%20as%20well%20as%20global%20trajectory%20patterns%2C%20enabling%20efficient%20hierarchical%20representation%20and%20reducing%20computational%20costs.%20Moreover%2C%20MovSemCL%20includes%20a%20curvature-guided%20augmentation%20strategy%20that%20preserves%20informative%20segments%20%28e.g.%2C%20turns%20and%20intersections%29%20and%20masks%20redundant%20ones%2C%20generating%20physically%20plausible%20augmented%20views.%20Experiments%20on%20real-world%20datasets%20show%20that%20MovSemCL%20is%20capable%20of%20outperforming%20state-of-the-art%20methods%2C%20achieving%20mean%20ranks%20close%20to%20the%20ideal%20value%20of%201%20at%20similarity%20search%20tasks%20and%20improvements%20by%20up%20to%2020.3%25%20at%20heuristic%20approximation%2C%20while%20reducing%20inference%20latency%20by%20up%20to%2043.4%25.&entry.1838667208=http%3A//arxiv.org/abs/2511.12061v2&entry.124074799=Read"},
{"title": "Step-GUI Technical Report", "author": "Haolong Yan and Jia Wang and Xin Huang and Yeqing Shen and Ziyang Meng and Zhimin Fan and Kaijun Tan and Jin Gao and Lieyu Shi and Mi Yang and Shiliang Yang and Zhirui Wang and Brian Li and Kang An and Chenyang Li and Lei Lei and Mengmeng Duan and Danxun Liang and Guodong Liu and Hang Cheng and Hao Wu and Jie Dong and Junhao Huang and Mei Chen and Renjie Yu and Shunshan Li and Xu Zhou and Yiting Dai and Yineng Deng and Yingdan Liang and Zelin Chen and Wen Sun and Chengxu Yan and Chunqin Xu and Dong Li and Fengqiong Xiao and Guanghao Fan and Guopeng Li and Guozhen Peng and Hongbing Li and Hang Li and Hongming Chen and Jingjing Xie and Jianyong Li and Jingyang Zhang and Jiaju Ren and Jiayu Yuan and Jianpeng Yin and Kai Cao and Liang Zhao and Liguo Tan and Liying Shi and Mengqiang Ren and Min Xu and Manjiao Liu and Mao Luo and Mingxin Wan and Na Wang and Nan Wu and Ning Wang and Peiyao Ma and Qingzhou Zhang and Qiao Wang and Qinlin Zeng and Qiong Gao and Qiongyao Li and Shangwu Zhong and Shuli Gao and Shaofan Liu and Shisi Gao and Shuang Luo and Xingbin Liu and Xiaojia Liu and Xiaojie Hou and Xin Liu and Xuanti Feng and Xuedan Cai and Xuan Wen and Xianwei Zhu and Xin Liang and Xin Liu and Xin Zhou and Yingxiu Zhao and Yukang Shi and Yunfang Xu and Yuqing Zeng and Yixun Zhang and Zejia Weng and Zhonghao Yan and Zhiguo Huang and Zhuoyu Wang and Zheng Ge and Jing Li and Yibo Zhu and Binxing Jiao and Xiangyu Zhang and Daxin Jiang", "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "link": "http://arxiv.org/abs/2512.15431v1", "date": "2025-12-17", "relevancy": 2.1933, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5635}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5376}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-GUI%20Technical%20Report&body=Title%3A%20Step-GUI%20Technical%20Report%0AAuthor%3A%20Haolong%20Yan%20and%20Jia%20Wang%20and%20Xin%20Huang%20and%20Yeqing%20Shen%20and%20Ziyang%20Meng%20and%20Zhimin%20Fan%20and%20Kaijun%20Tan%20and%20Jin%20Gao%20and%20Lieyu%20Shi%20and%20Mi%20Yang%20and%20Shiliang%20Yang%20and%20Zhirui%20Wang%20and%20Brian%20Li%20and%20Kang%20An%20and%20Chenyang%20Li%20and%20Lei%20Lei%20and%20Mengmeng%20Duan%20and%20Danxun%20Liang%20and%20Guodong%20Liu%20and%20Hang%20Cheng%20and%20Hao%20Wu%20and%20Jie%20Dong%20and%20Junhao%20Huang%20and%20Mei%20Chen%20and%20Renjie%20Yu%20and%20Shunshan%20Li%20and%20Xu%20Zhou%20and%20Yiting%20Dai%20and%20Yineng%20Deng%20and%20Yingdan%20Liang%20and%20Zelin%20Chen%20and%20Wen%20Sun%20and%20Chengxu%20Yan%20and%20Chunqin%20Xu%20and%20Dong%20Li%20and%20Fengqiong%20Xiao%20and%20Guanghao%20Fan%20and%20Guopeng%20Li%20and%20Guozhen%20Peng%20and%20Hongbing%20Li%20and%20Hang%20Li%20and%20Hongming%20Chen%20and%20Jingjing%20Xie%20and%20Jianyong%20Li%20and%20Jingyang%20Zhang%20and%20Jiaju%20Ren%20and%20Jiayu%20Yuan%20and%20Jianpeng%20Yin%20and%20Kai%20Cao%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liying%20Shi%20and%20Mengqiang%20Ren%20and%20Min%20Xu%20and%20Manjiao%20Liu%20and%20Mao%20Luo%20and%20Mingxin%20Wan%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Ning%20Wang%20and%20Peiyao%20Ma%20and%20Qingzhou%20Zhang%20and%20Qiao%20Wang%20and%20Qinlin%20Zeng%20and%20Qiong%20Gao%20and%20Qiongyao%20Li%20and%20Shangwu%20Zhong%20and%20Shuli%20Gao%20and%20Shaofan%20Liu%20and%20Shisi%20Gao%20and%20Shuang%20Luo%20and%20Xingbin%20Liu%20and%20Xiaojia%20Liu%20and%20Xiaojie%20Hou%20and%20Xin%20Liu%20and%20Xuanti%20Feng%20and%20Xuedan%20Cai%20and%20Xuan%20Wen%20and%20Xianwei%20Zhu%20and%20Xin%20Liang%20and%20Xin%20Liu%20and%20Xin%20Zhou%20and%20Yingxiu%20Zhao%20and%20Yukang%20Shi%20and%20Yunfang%20Xu%20and%20Yuqing%20Zeng%20and%20Yixun%20Zhang%20and%20Zejia%20Weng%20and%20Zhonghao%20Yan%20and%20Zhiguo%20Huang%20and%20Zhuoyu%20Wang%20and%20Zheng%20Ge%20and%20Jing%20Li%20and%20Yibo%20Zhu%20and%20Binxing%20Jiao%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20unlock%20unprecedented%20opportunities%20for%20GUI%20automation.%20However%2C%20a%20fundamental%20challenge%20remains%3A%20how%20to%20efficiently%20acquire%20high-quality%20training%20data%20while%20maintaining%20annotation%20reliability%3F%20We%20introduce%20a%20self-evolving%20training%20pipeline%20powered%20by%20the%20Calibrated%20Step%20Reward%20System%2C%20which%20converts%20model-generated%20trajectories%20into%20reliable%20training%20signals%20through%20trajectory-level%20calibration%2C%20achieving%20%3E90%25%20annotation%20accuracy%20with%2010-100x%20lower%20cost.%20Leveraging%20this%20pipeline%2C%20we%20introduce%20Step-GUI%2C%20a%20family%20of%20models%20%284B/8B%29%20that%20achieves%20state-of-the-art%20GUI%20performance%20%288B%3A%2080.2%25%20AndroidWorld%2C%2048.5%25%20OSWorld%2C%2062.6%25%20ScreenShot-Pro%29%20while%20maintaining%20robust%20general%20capabilities.%20As%20GUI%20agent%20capabilities%20improve%2C%20practical%20deployment%20demands%20standardized%20interfaces%20across%20heterogeneous%20devices%20while%20protecting%20user%20privacy.%20To%20this%20end%2C%20we%20propose%20GUI-MCP%2C%20the%20first%20Model%20Context%20Protocol%20for%20GUI%20automation%20with%20hierarchical%20architecture%20that%20combines%20low-level%20atomic%20operations%20and%20high-level%20task%20delegation%20to%20local%20specialist%20models%2C%20enabling%20high-privacy%20execution%20where%20sensitive%20data%20stays%20on-device.%20Finally%2C%20to%20assess%20whether%20agents%20can%20handle%20authentic%20everyday%20usage%2C%20we%20introduce%20AndroidDaily%2C%20a%20benchmark%20grounded%20in%20real-world%20mobile%20usage%20patterns%20with%203146%20static%20actions%20and%20235%20end-to-end%20tasks%20across%20high-frequency%20daily%20scenarios%20%288B%3A%20static%2089.91%25%2C%20end-to-end%2052.50%25%29.%20Our%20work%20advances%20the%20development%20of%20practical%20GUI%20agents%20and%20demonstrates%20strong%20potential%20for%20real-world%20deployment%20in%20everyday%20digital%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-GUI%2520Technical%2520Report%26entry.906535625%3DHaolong%2520Yan%2520and%2520Jia%2520Wang%2520and%2520Xin%2520Huang%2520and%2520Yeqing%2520Shen%2520and%2520Ziyang%2520Meng%2520and%2520Zhimin%2520Fan%2520and%2520Kaijun%2520Tan%2520and%2520Jin%2520Gao%2520and%2520Lieyu%2520Shi%2520and%2520Mi%2520Yang%2520and%2520Shiliang%2520Yang%2520and%2520Zhirui%2520Wang%2520and%2520Brian%2520Li%2520and%2520Kang%2520An%2520and%2520Chenyang%2520Li%2520and%2520Lei%2520Lei%2520and%2520Mengmeng%2520Duan%2520and%2520Danxun%2520Liang%2520and%2520Guodong%2520Liu%2520and%2520Hang%2520Cheng%2520and%2520Hao%2520Wu%2520and%2520Jie%2520Dong%2520and%2520Junhao%2520Huang%2520and%2520Mei%2520Chen%2520and%2520Renjie%2520Yu%2520and%2520Shunshan%2520Li%2520and%2520Xu%2520Zhou%2520and%2520Yiting%2520Dai%2520and%2520Yineng%2520Deng%2520and%2520Yingdan%2520Liang%2520and%2520Zelin%2520Chen%2520and%2520Wen%2520Sun%2520and%2520Chengxu%2520Yan%2520and%2520Chunqin%2520Xu%2520and%2520Dong%2520Li%2520and%2520Fengqiong%2520Xiao%2520and%2520Guanghao%2520Fan%2520and%2520Guopeng%2520Li%2520and%2520Guozhen%2520Peng%2520and%2520Hongbing%2520Li%2520and%2520Hang%2520Li%2520and%2520Hongming%2520Chen%2520and%2520Jingjing%2520Xie%2520and%2520Jianyong%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Jiaju%2520Ren%2520and%2520Jiayu%2520Yuan%2520and%2520Jianpeng%2520Yin%2520and%2520Kai%2520Cao%2520and%2520Liang%2520Zhao%2520and%2520Liguo%2520Tan%2520and%2520Liying%2520Shi%2520and%2520Mengqiang%2520Ren%2520and%2520Min%2520Xu%2520and%2520Manjiao%2520Liu%2520and%2520Mao%2520Luo%2520and%2520Mingxin%2520Wan%2520and%2520Na%2520Wang%2520and%2520Nan%2520Wu%2520and%2520Ning%2520Wang%2520and%2520Peiyao%2520Ma%2520and%2520Qingzhou%2520Zhang%2520and%2520Qiao%2520Wang%2520and%2520Qinlin%2520Zeng%2520and%2520Qiong%2520Gao%2520and%2520Qiongyao%2520Li%2520and%2520Shangwu%2520Zhong%2520and%2520Shuli%2520Gao%2520and%2520Shaofan%2520Liu%2520and%2520Shisi%2520Gao%2520and%2520Shuang%2520Luo%2520and%2520Xingbin%2520Liu%2520and%2520Xiaojia%2520Liu%2520and%2520Xiaojie%2520Hou%2520and%2520Xin%2520Liu%2520and%2520Xuanti%2520Feng%2520and%2520Xuedan%2520Cai%2520and%2520Xuan%2520Wen%2520and%2520Xianwei%2520Zhu%2520and%2520Xin%2520Liang%2520and%2520Xin%2520Liu%2520and%2520Xin%2520Zhou%2520and%2520Yingxiu%2520Zhao%2520and%2520Yukang%2520Shi%2520and%2520Yunfang%2520Xu%2520and%2520Yuqing%2520Zeng%2520and%2520Yixun%2520Zhang%2520and%2520Zejia%2520Weng%2520and%2520Zhonghao%2520Yan%2520and%2520Zhiguo%2520Huang%2520and%2520Zhuoyu%2520Wang%2520and%2520Zheng%2520Ge%2520and%2520Jing%2520Li%2520and%2520Yibo%2520Zhu%2520and%2520Binxing%2520Jiao%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520unlock%2520unprecedented%2520opportunities%2520for%2520GUI%2520automation.%2520However%252C%2520a%2520fundamental%2520challenge%2520remains%253A%2520how%2520to%2520efficiently%2520acquire%2520high-quality%2520training%2520data%2520while%2520maintaining%2520annotation%2520reliability%253F%2520We%2520introduce%2520a%2520self-evolving%2520training%2520pipeline%2520powered%2520by%2520the%2520Calibrated%2520Step%2520Reward%2520System%252C%2520which%2520converts%2520model-generated%2520trajectories%2520into%2520reliable%2520training%2520signals%2520through%2520trajectory-level%2520calibration%252C%2520achieving%2520%253E90%2525%2520annotation%2520accuracy%2520with%252010-100x%2520lower%2520cost.%2520Leveraging%2520this%2520pipeline%252C%2520we%2520introduce%2520Step-GUI%252C%2520a%2520family%2520of%2520models%2520%25284B/8B%2529%2520that%2520achieves%2520state-of-the-art%2520GUI%2520performance%2520%25288B%253A%252080.2%2525%2520AndroidWorld%252C%252048.5%2525%2520OSWorld%252C%252062.6%2525%2520ScreenShot-Pro%2529%2520while%2520maintaining%2520robust%2520general%2520capabilities.%2520As%2520GUI%2520agent%2520capabilities%2520improve%252C%2520practical%2520deployment%2520demands%2520standardized%2520interfaces%2520across%2520heterogeneous%2520devices%2520while%2520protecting%2520user%2520privacy.%2520To%2520this%2520end%252C%2520we%2520propose%2520GUI-MCP%252C%2520the%2520first%2520Model%2520Context%2520Protocol%2520for%2520GUI%2520automation%2520with%2520hierarchical%2520architecture%2520that%2520combines%2520low-level%2520atomic%2520operations%2520and%2520high-level%2520task%2520delegation%2520to%2520local%2520specialist%2520models%252C%2520enabling%2520high-privacy%2520execution%2520where%2520sensitive%2520data%2520stays%2520on-device.%2520Finally%252C%2520to%2520assess%2520whether%2520agents%2520can%2520handle%2520authentic%2520everyday%2520usage%252C%2520we%2520introduce%2520AndroidDaily%252C%2520a%2520benchmark%2520grounded%2520in%2520real-world%2520mobile%2520usage%2520patterns%2520with%25203146%2520static%2520actions%2520and%2520235%2520end-to-end%2520tasks%2520across%2520high-frequency%2520daily%2520scenarios%2520%25288B%253A%2520static%252089.91%2525%252C%2520end-to-end%252052.50%2525%2529.%2520Our%2520work%2520advances%2520the%2520development%2520of%2520practical%2520GUI%2520agents%2520and%2520demonstrates%2520strong%2520potential%2520for%2520real-world%2520deployment%2520in%2520everyday%2520digital%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-GUI%20Technical%20Report&entry.906535625=Haolong%20Yan%20and%20Jia%20Wang%20and%20Xin%20Huang%20and%20Yeqing%20Shen%20and%20Ziyang%20Meng%20and%20Zhimin%20Fan%20and%20Kaijun%20Tan%20and%20Jin%20Gao%20and%20Lieyu%20Shi%20and%20Mi%20Yang%20and%20Shiliang%20Yang%20and%20Zhirui%20Wang%20and%20Brian%20Li%20and%20Kang%20An%20and%20Chenyang%20Li%20and%20Lei%20Lei%20and%20Mengmeng%20Duan%20and%20Danxun%20Liang%20and%20Guodong%20Liu%20and%20Hang%20Cheng%20and%20Hao%20Wu%20and%20Jie%20Dong%20and%20Junhao%20Huang%20and%20Mei%20Chen%20and%20Renjie%20Yu%20and%20Shunshan%20Li%20and%20Xu%20Zhou%20and%20Yiting%20Dai%20and%20Yineng%20Deng%20and%20Yingdan%20Liang%20and%20Zelin%20Chen%20and%20Wen%20Sun%20and%20Chengxu%20Yan%20and%20Chunqin%20Xu%20and%20Dong%20Li%20and%20Fengqiong%20Xiao%20and%20Guanghao%20Fan%20and%20Guopeng%20Li%20and%20Guozhen%20Peng%20and%20Hongbing%20Li%20and%20Hang%20Li%20and%20Hongming%20Chen%20and%20Jingjing%20Xie%20and%20Jianyong%20Li%20and%20Jingyang%20Zhang%20and%20Jiaju%20Ren%20and%20Jiayu%20Yuan%20and%20Jianpeng%20Yin%20and%20Kai%20Cao%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liying%20Shi%20and%20Mengqiang%20Ren%20and%20Min%20Xu%20and%20Manjiao%20Liu%20and%20Mao%20Luo%20and%20Mingxin%20Wan%20and%20Na%20Wang%20and%20Nan%20Wu%20and%20Ning%20Wang%20and%20Peiyao%20Ma%20and%20Qingzhou%20Zhang%20and%20Qiao%20Wang%20and%20Qinlin%20Zeng%20and%20Qiong%20Gao%20and%20Qiongyao%20Li%20and%20Shangwu%20Zhong%20and%20Shuli%20Gao%20and%20Shaofan%20Liu%20and%20Shisi%20Gao%20and%20Shuang%20Luo%20and%20Xingbin%20Liu%20and%20Xiaojia%20Liu%20and%20Xiaojie%20Hou%20and%20Xin%20Liu%20and%20Xuanti%20Feng%20and%20Xuedan%20Cai%20and%20Xuan%20Wen%20and%20Xianwei%20Zhu%20and%20Xin%20Liang%20and%20Xin%20Liu%20and%20Xin%20Zhou%20and%20Yingxiu%20Zhao%20and%20Yukang%20Shi%20and%20Yunfang%20Xu%20and%20Yuqing%20Zeng%20and%20Yixun%20Zhang%20and%20Zejia%20Weng%20and%20Zhonghao%20Yan%20and%20Zhiguo%20Huang%20and%20Zhuoyu%20Wang%20and%20Zheng%20Ge%20and%20Jing%20Li%20and%20Yibo%20Zhu%20and%20Binxing%20Jiao%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20unlock%20unprecedented%20opportunities%20for%20GUI%20automation.%20However%2C%20a%20fundamental%20challenge%20remains%3A%20how%20to%20efficiently%20acquire%20high-quality%20training%20data%20while%20maintaining%20annotation%20reliability%3F%20We%20introduce%20a%20self-evolving%20training%20pipeline%20powered%20by%20the%20Calibrated%20Step%20Reward%20System%2C%20which%20converts%20model-generated%20trajectories%20into%20reliable%20training%20signals%20through%20trajectory-level%20calibration%2C%20achieving%20%3E90%25%20annotation%20accuracy%20with%2010-100x%20lower%20cost.%20Leveraging%20this%20pipeline%2C%20we%20introduce%20Step-GUI%2C%20a%20family%20of%20models%20%284B/8B%29%20that%20achieves%20state-of-the-art%20GUI%20performance%20%288B%3A%2080.2%25%20AndroidWorld%2C%2048.5%25%20OSWorld%2C%2062.6%25%20ScreenShot-Pro%29%20while%20maintaining%20robust%20general%20capabilities.%20As%20GUI%20agent%20capabilities%20improve%2C%20practical%20deployment%20demands%20standardized%20interfaces%20across%20heterogeneous%20devices%20while%20protecting%20user%20privacy.%20To%20this%20end%2C%20we%20propose%20GUI-MCP%2C%20the%20first%20Model%20Context%20Protocol%20for%20GUI%20automation%20with%20hierarchical%20architecture%20that%20combines%20low-level%20atomic%20operations%20and%20high-level%20task%20delegation%20to%20local%20specialist%20models%2C%20enabling%20high-privacy%20execution%20where%20sensitive%20data%20stays%20on-device.%20Finally%2C%20to%20assess%20whether%20agents%20can%20handle%20authentic%20everyday%20usage%2C%20we%20introduce%20AndroidDaily%2C%20a%20benchmark%20grounded%20in%20real-world%20mobile%20usage%20patterns%20with%203146%20static%20actions%20and%20235%20end-to-end%20tasks%20across%20high-frequency%20daily%20scenarios%20%288B%3A%20static%2089.91%25%2C%20end-to-end%2052.50%25%29.%20Our%20work%20advances%20the%20development%20of%20practical%20GUI%20agents%20and%20demonstrates%20strong%20potential%20for%20real-world%20deployment%20in%20everyday%20digital%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2512.15431v1&entry.124074799=Read"},
{"title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration", "author": "Daiqing Wu and Dongbao Yang and Can Ma. Yu Zhou", "abstract": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.", "link": "http://arxiv.org/abs/2512.15528v1", "date": "2025-12-17", "relevancy": 2.1813, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoCaliber%3A%20Advancing%20Reliable%20Visual%20Emotion%20Comprehension%20via%20Confidence%20Verbalization%20and%20Calibration&body=Title%3A%20EmoCaliber%3A%20Advancing%20Reliable%20Visual%20Emotion%20Comprehension%20via%20Confidence%20Verbalization%20and%20Calibration%0AAuthor%3A%20Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Can%20Ma.%20Yu%20Zhou%0AAbstract%3A%20Visual%20Emotion%20Comprehension%20%28VEC%29%20aims%20to%20infer%20sentiment%20polarities%20or%20emotion%20categories%20from%20affective%20cues%20embedded%20in%20images.%20In%20recent%20years%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20established%20a%20popular%20paradigm%20in%20VEC%2C%20leveraging%20their%20generalizability%20to%20unify%20VEC%20tasks%20defined%20under%20diverse%20emotion%20taxonomies.%20While%20this%20paradigm%20achieves%20notable%20success%2C%20it%20typically%20formulates%20VEC%20as%20a%20deterministic%20task%2C%20requiring%20the%20model%20to%20output%20a%20single%2C%20definitive%20emotion%20label%20for%20each%20image.%20Such%20a%20formulation%20insufficiently%20accounts%20for%20the%20inherent%20subjectivity%20of%20emotion%20perception%2C%20overlooking%20alternative%20interpretations%20that%20may%20be%20equally%20plausible%20to%20different%20viewers.%20To%20address%20this%20limitation%2C%20we%20propose%20equipping%20MLLMs%20with%20capabilities%20to%20verbalize%20their%20confidence%20in%20emotion%20predictions.%20This%20additional%20signal%20provides%20users%20with%20an%20estimate%20of%20both%20the%20plausibility%20of%20alternative%20interpretations%20and%20the%20MLLMs%27%20self-assessed%20competence%2C%20thereby%20enhancing%20reliability%20in%20practice.%20Building%20on%20this%20insight%2C%20we%20introduce%20a%20three-stage%20training%20framework%20that%20progressively%20endows%20with%20structured%20reasoning%2C%20teaches%20to%20verbalize%20confidence%2C%20and%20calibrates%20confidence%20expression%2C%20culminating%20in%20EmoCaliber%2C%20a%20confidence-aware%20MLLM%20for%20VEC.%20Through%20fair%20and%20comprehensive%20evaluations%20on%20the%20unified%20benchmark%20VECBench%2C%20EmoCaliber%20demonstrates%20overall%20superiority%20against%20existing%20methods%20in%20both%20emotion%20prediction%20and%20confidence%20estimation.%20These%20results%20validate%20the%20effectiveness%20of%20our%20approach%20and%20mark%20a%20feasible%20step%20toward%20more%20reliable%20VEC%20systems.%20Project%20page%3A%20https%3A//github.com/wdqqdw/EmoCaliber.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoCaliber%253A%2520Advancing%2520Reliable%2520Visual%2520Emotion%2520Comprehension%2520via%2520Confidence%2520Verbalization%2520and%2520Calibration%26entry.906535625%3DDaiqing%2520Wu%2520and%2520Dongbao%2520Yang%2520and%2520Can%2520Ma.%2520Yu%2520Zhou%26entry.1292438233%3DVisual%2520Emotion%2520Comprehension%2520%2528VEC%2529%2520aims%2520to%2520infer%2520sentiment%2520polarities%2520or%2520emotion%2520categories%2520from%2520affective%2520cues%2520embedded%2520in%2520images.%2520In%2520recent%2520years%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520established%2520a%2520popular%2520paradigm%2520in%2520VEC%252C%2520leveraging%2520their%2520generalizability%2520to%2520unify%2520VEC%2520tasks%2520defined%2520under%2520diverse%2520emotion%2520taxonomies.%2520While%2520this%2520paradigm%2520achieves%2520notable%2520success%252C%2520it%2520typically%2520formulates%2520VEC%2520as%2520a%2520deterministic%2520task%252C%2520requiring%2520the%2520model%2520to%2520output%2520a%2520single%252C%2520definitive%2520emotion%2520label%2520for%2520each%2520image.%2520Such%2520a%2520formulation%2520insufficiently%2520accounts%2520for%2520the%2520inherent%2520subjectivity%2520of%2520emotion%2520perception%252C%2520overlooking%2520alternative%2520interpretations%2520that%2520may%2520be%2520equally%2520plausible%2520to%2520different%2520viewers.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520equipping%2520MLLMs%2520with%2520capabilities%2520to%2520verbalize%2520their%2520confidence%2520in%2520emotion%2520predictions.%2520This%2520additional%2520signal%2520provides%2520users%2520with%2520an%2520estimate%2520of%2520both%2520the%2520plausibility%2520of%2520alternative%2520interpretations%2520and%2520the%2520MLLMs%2527%2520self-assessed%2520competence%252C%2520thereby%2520enhancing%2520reliability%2520in%2520practice.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520a%2520three-stage%2520training%2520framework%2520that%2520progressively%2520endows%2520with%2520structured%2520reasoning%252C%2520teaches%2520to%2520verbalize%2520confidence%252C%2520and%2520calibrates%2520confidence%2520expression%252C%2520culminating%2520in%2520EmoCaliber%252C%2520a%2520confidence-aware%2520MLLM%2520for%2520VEC.%2520Through%2520fair%2520and%2520comprehensive%2520evaluations%2520on%2520the%2520unified%2520benchmark%2520VECBench%252C%2520EmoCaliber%2520demonstrates%2520overall%2520superiority%2520against%2520existing%2520methods%2520in%2520both%2520emotion%2520prediction%2520and%2520confidence%2520estimation.%2520These%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%2520mark%2520a%2520feasible%2520step%2520toward%2520more%2520reliable%2520VEC%2520systems.%2520Project%2520page%253A%2520https%253A//github.com/wdqqdw/EmoCaliber.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoCaliber%3A%20Advancing%20Reliable%20Visual%20Emotion%20Comprehension%20via%20Confidence%20Verbalization%20and%20Calibration&entry.906535625=Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Can%20Ma.%20Yu%20Zhou&entry.1292438233=Visual%20Emotion%20Comprehension%20%28VEC%29%20aims%20to%20infer%20sentiment%20polarities%20or%20emotion%20categories%20from%20affective%20cues%20embedded%20in%20images.%20In%20recent%20years%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20established%20a%20popular%20paradigm%20in%20VEC%2C%20leveraging%20their%20generalizability%20to%20unify%20VEC%20tasks%20defined%20under%20diverse%20emotion%20taxonomies.%20While%20this%20paradigm%20achieves%20notable%20success%2C%20it%20typically%20formulates%20VEC%20as%20a%20deterministic%20task%2C%20requiring%20the%20model%20to%20output%20a%20single%2C%20definitive%20emotion%20label%20for%20each%20image.%20Such%20a%20formulation%20insufficiently%20accounts%20for%20the%20inherent%20subjectivity%20of%20emotion%20perception%2C%20overlooking%20alternative%20interpretations%20that%20may%20be%20equally%20plausible%20to%20different%20viewers.%20To%20address%20this%20limitation%2C%20we%20propose%20equipping%20MLLMs%20with%20capabilities%20to%20verbalize%20their%20confidence%20in%20emotion%20predictions.%20This%20additional%20signal%20provides%20users%20with%20an%20estimate%20of%20both%20the%20plausibility%20of%20alternative%20interpretations%20and%20the%20MLLMs%27%20self-assessed%20competence%2C%20thereby%20enhancing%20reliability%20in%20practice.%20Building%20on%20this%20insight%2C%20we%20introduce%20a%20three-stage%20training%20framework%20that%20progressively%20endows%20with%20structured%20reasoning%2C%20teaches%20to%20verbalize%20confidence%2C%20and%20calibrates%20confidence%20expression%2C%20culminating%20in%20EmoCaliber%2C%20a%20confidence-aware%20MLLM%20for%20VEC.%20Through%20fair%20and%20comprehensive%20evaluations%20on%20the%20unified%20benchmark%20VECBench%2C%20EmoCaliber%20demonstrates%20overall%20superiority%20against%20existing%20methods%20in%20both%20emotion%20prediction%20and%20confidence%20estimation.%20These%20results%20validate%20the%20effectiveness%20of%20our%20approach%20and%20mark%20a%20feasible%20step%20toward%20more%20reliable%20VEC%20systems.%20Project%20page%3A%20https%3A//github.com/wdqqdw/EmoCaliber.&entry.1838667208=http%3A//arxiv.org/abs/2512.15528v1&entry.124074799=Read"},
{"title": "Do MLLMs Exhibit Human-like Perceptual Behaviors? HVSBench: A Benchmark for MLLM Alignment with Human Perceptual Behavior", "author": "Jiaying Lin and Shuquan Ye and Dan Xu and Wanli Ouyang and Rynson W. H. Lau", "abstract": "While Multimodal Large Language Models (MLLMs) excel at many vision tasks, it is unknown if they exhibit human-like perceptual behaviors. To evaluate this, we introduce HVSBench, the first large-scale benchmark with over 85,000 samples designed to test MLLM alignment with the human visual system (HVS). The benchmark covers 13 categories across 5 key fields: Prominence, Subitizing, Prioritizing, Free-Viewing, and Searching. Our comprehensive evaluation reveals a significant perceptual gap: even state-of-the-art MLLMs achieve only moderate results. In contrast, human participants demonstrate strong performance, significantly outperforming all models. This underscores the high quality of HVSBench and the need for more human-aligned AI. We believe our benchmark will be a critical tool for developing the next generation of explainable MLLMs.", "link": "http://arxiv.org/abs/2412.09603v3", "date": "2025-12-17", "relevancy": 2.1679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20MLLMs%20Exhibit%20Human-like%20Perceptual%20Behaviors%3F%20HVSBench%3A%20A%20Benchmark%20for%20MLLM%20Alignment%20with%20Human%20Perceptual%20Behavior&body=Title%3A%20Do%20MLLMs%20Exhibit%20Human-like%20Perceptual%20Behaviors%3F%20HVSBench%3A%20A%20Benchmark%20for%20MLLM%20Alignment%20with%20Human%20Perceptual%20Behavior%0AAuthor%3A%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Dan%20Xu%20and%20Wanli%20Ouyang%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20many%20vision%20tasks%2C%20it%20is%20unknown%20if%20they%20exhibit%20human-like%20perceptual%20behaviors.%20To%20evaluate%20this%2C%20we%20introduce%20HVSBench%2C%20the%20first%20large-scale%20benchmark%20with%20over%2085%2C000%20samples%20designed%20to%20test%20MLLM%20alignment%20with%20the%20human%20visual%20system%20%28HVS%29.%20The%20benchmark%20covers%2013%20categories%20across%205%20key%20fields%3A%20Prominence%2C%20Subitizing%2C%20Prioritizing%2C%20Free-Viewing%2C%20and%20Searching.%20Our%20comprehensive%20evaluation%20reveals%20a%20significant%20perceptual%20gap%3A%20even%20state-of-the-art%20MLLMs%20achieve%20only%20moderate%20results.%20In%20contrast%2C%20human%20participants%20demonstrate%20strong%20performance%2C%20significantly%20outperforming%20all%20models.%20This%20underscores%20the%20high%20quality%20of%20HVSBench%20and%20the%20need%20for%20more%20human-aligned%20AI.%20We%20believe%20our%20benchmark%20will%20be%20a%20critical%20tool%20for%20developing%20the%20next%20generation%20of%20explainable%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2412.09603v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520MLLMs%2520Exhibit%2520Human-like%2520Perceptual%2520Behaviors%253F%2520HVSBench%253A%2520A%2520Benchmark%2520for%2520MLLM%2520Alignment%2520with%2520Human%2520Perceptual%2520Behavior%26entry.906535625%3DJiaying%2520Lin%2520and%2520Shuquan%2520Ye%2520and%2520Dan%2520Xu%2520and%2520Wanli%2520Ouyang%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520many%2520vision%2520tasks%252C%2520it%2520is%2520unknown%2520if%2520they%2520exhibit%2520human-like%2520perceptual%2520behaviors.%2520To%2520evaluate%2520this%252C%2520we%2520introduce%2520HVSBench%252C%2520the%2520first%2520large-scale%2520benchmark%2520with%2520over%252085%252C000%2520samples%2520designed%2520to%2520test%2520MLLM%2520alignment%2520with%2520the%2520human%2520visual%2520system%2520%2528HVS%2529.%2520The%2520benchmark%2520covers%252013%2520categories%2520across%25205%2520key%2520fields%253A%2520Prominence%252C%2520Subitizing%252C%2520Prioritizing%252C%2520Free-Viewing%252C%2520and%2520Searching.%2520Our%2520comprehensive%2520evaluation%2520reveals%2520a%2520significant%2520perceptual%2520gap%253A%2520even%2520state-of-the-art%2520MLLMs%2520achieve%2520only%2520moderate%2520results.%2520In%2520contrast%252C%2520human%2520participants%2520demonstrate%2520strong%2520performance%252C%2520significantly%2520outperforming%2520all%2520models.%2520This%2520underscores%2520the%2520high%2520quality%2520of%2520HVSBench%2520and%2520the%2520need%2520for%2520more%2520human-aligned%2520AI.%2520We%2520believe%2520our%2520benchmark%2520will%2520be%2520a%2520critical%2520tool%2520for%2520developing%2520the%2520next%2520generation%2520of%2520explainable%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09603v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20MLLMs%20Exhibit%20Human-like%20Perceptual%20Behaviors%3F%20HVSBench%3A%20A%20Benchmark%20for%20MLLM%20Alignment%20with%20Human%20Perceptual%20Behavior&entry.906535625=Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Dan%20Xu%20and%20Wanli%20Ouyang%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20many%20vision%20tasks%2C%20it%20is%20unknown%20if%20they%20exhibit%20human-like%20perceptual%20behaviors.%20To%20evaluate%20this%2C%20we%20introduce%20HVSBench%2C%20the%20first%20large-scale%20benchmark%20with%20over%2085%2C000%20samples%20designed%20to%20test%20MLLM%20alignment%20with%20the%20human%20visual%20system%20%28HVS%29.%20The%20benchmark%20covers%2013%20categories%20across%205%20key%20fields%3A%20Prominence%2C%20Subitizing%2C%20Prioritizing%2C%20Free-Viewing%2C%20and%20Searching.%20Our%20comprehensive%20evaluation%20reveals%20a%20significant%20perceptual%20gap%3A%20even%20state-of-the-art%20MLLMs%20achieve%20only%20moderate%20results.%20In%20contrast%2C%20human%20participants%20demonstrate%20strong%20performance%2C%20significantly%20outperforming%20all%20models.%20This%20underscores%20the%20high%20quality%20of%20HVSBench%20and%20the%20need%20for%20more%20human-aligned%20AI.%20We%20believe%20our%20benchmark%20will%20be%20a%20critical%20tool%20for%20developing%20the%20next%20generation%20of%20explainable%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2412.09603v3&entry.124074799=Read"},
{"title": "Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory", "author": "Huiyan Xue and Xuming Ran and Yaxin Li and Qi Xu and Enhui Li and Yi Xu and Qiang Zhang", "abstract": "Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.", "link": "http://arxiv.org/abs/2512.15267v1", "date": "2025-12-17", "relevancy": 2.1624, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.554}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distillation-Guided%20Structural%20Transfer%20for%20Continual%20Learning%20Beyond%20Sparse%20Distributed%20Memory&body=Title%3A%20Distillation-Guided%20Structural%20Transfer%20for%20Continual%20Learning%20Beyond%20Sparse%20Distributed%20Memory%0AAuthor%3A%20Huiyan%20Xue%20and%20Xuming%20Ran%20and%20Yaxin%20Li%20and%20Qi%20Xu%20and%20Enhui%20Li%20and%20Yi%20Xu%20and%20Qiang%20Zhang%0AAbstract%3A%20Sparse%20neural%20systems%20are%20gaining%20traction%20for%20efficient%20continual%20learning%20due%20to%20their%20modularity%20and%20low%20interference.%20Architectures%20such%20as%20Sparse%20Distributed%20Memory%20Multi-Layer%20Perceptrons%20%28SDMLP%29%20construct%20task-specific%20subnetworks%20via%20Top-K%20activation%20and%20have%20shown%20resilience%20against%20catastrophic%20forgetting.%20However%2C%20their%20rigid%20modularity%20limits%20cross-task%20knowledge%20reuse%20and%20leads%20to%20performance%20degradation%20under%20high%20sparsity.%20We%20propose%20Selective%20Subnetwork%20Distillation%20%28SSD%29%2C%20a%20structurally%20guided%20continual%20learning%20framework%20that%20treats%20distillation%20not%20as%20a%20regularizer%20but%20as%20a%20topology-aligned%20information%20conduit.%20SSD%20identifies%20neurons%20with%20high%20activation%20frequency%20and%20selectively%20distills%20knowledge%20within%20previous%20Top-K%20subnetworks%20and%20output%20logits%2C%20without%20requiring%20replay%20or%20task%20labels.%20This%20enables%20structural%20realignment%20while%20preserving%20sparse%20modularity.%20Experiments%20on%20Split%20CIFAR-10%2C%20CIFAR-100%2C%20and%20MNIST%20demonstrate%20that%20SSD%20improves%20accuracy%2C%20retention%2C%20and%20representation%20coverage%2C%20offering%20a%20structurally%20grounded%20solution%20for%20sparse%20continual%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistillation-Guided%2520Structural%2520Transfer%2520for%2520Continual%2520Learning%2520Beyond%2520Sparse%2520Distributed%2520Memory%26entry.906535625%3DHuiyan%2520Xue%2520and%2520Xuming%2520Ran%2520and%2520Yaxin%2520Li%2520and%2520Qi%2520Xu%2520and%2520Enhui%2520Li%2520and%2520Yi%2520Xu%2520and%2520Qiang%2520Zhang%26entry.1292438233%3DSparse%2520neural%2520systems%2520are%2520gaining%2520traction%2520for%2520efficient%2520continual%2520learning%2520due%2520to%2520their%2520modularity%2520and%2520low%2520interference.%2520Architectures%2520such%2520as%2520Sparse%2520Distributed%2520Memory%2520Multi-Layer%2520Perceptrons%2520%2528SDMLP%2529%2520construct%2520task-specific%2520subnetworks%2520via%2520Top-K%2520activation%2520and%2520have%2520shown%2520resilience%2520against%2520catastrophic%2520forgetting.%2520However%252C%2520their%2520rigid%2520modularity%2520limits%2520cross-task%2520knowledge%2520reuse%2520and%2520leads%2520to%2520performance%2520degradation%2520under%2520high%2520sparsity.%2520We%2520propose%2520Selective%2520Subnetwork%2520Distillation%2520%2528SSD%2529%252C%2520a%2520structurally%2520guided%2520continual%2520learning%2520framework%2520that%2520treats%2520distillation%2520not%2520as%2520a%2520regularizer%2520but%2520as%2520a%2520topology-aligned%2520information%2520conduit.%2520SSD%2520identifies%2520neurons%2520with%2520high%2520activation%2520frequency%2520and%2520selectively%2520distills%2520knowledge%2520within%2520previous%2520Top-K%2520subnetworks%2520and%2520output%2520logits%252C%2520without%2520requiring%2520replay%2520or%2520task%2520labels.%2520This%2520enables%2520structural%2520realignment%2520while%2520preserving%2520sparse%2520modularity.%2520Experiments%2520on%2520Split%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520MNIST%2520demonstrate%2520that%2520SSD%2520improves%2520accuracy%252C%2520retention%252C%2520and%2520representation%2520coverage%252C%2520offering%2520a%2520structurally%2520grounded%2520solution%2520for%2520sparse%2520continual%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distillation-Guided%20Structural%20Transfer%20for%20Continual%20Learning%20Beyond%20Sparse%20Distributed%20Memory&entry.906535625=Huiyan%20Xue%20and%20Xuming%20Ran%20and%20Yaxin%20Li%20and%20Qi%20Xu%20and%20Enhui%20Li%20and%20Yi%20Xu%20and%20Qiang%20Zhang&entry.1292438233=Sparse%20neural%20systems%20are%20gaining%20traction%20for%20efficient%20continual%20learning%20due%20to%20their%20modularity%20and%20low%20interference.%20Architectures%20such%20as%20Sparse%20Distributed%20Memory%20Multi-Layer%20Perceptrons%20%28SDMLP%29%20construct%20task-specific%20subnetworks%20via%20Top-K%20activation%20and%20have%20shown%20resilience%20against%20catastrophic%20forgetting.%20However%2C%20their%20rigid%20modularity%20limits%20cross-task%20knowledge%20reuse%20and%20leads%20to%20performance%20degradation%20under%20high%20sparsity.%20We%20propose%20Selective%20Subnetwork%20Distillation%20%28SSD%29%2C%20a%20structurally%20guided%20continual%20learning%20framework%20that%20treats%20distillation%20not%20as%20a%20regularizer%20but%20as%20a%20topology-aligned%20information%20conduit.%20SSD%20identifies%20neurons%20with%20high%20activation%20frequency%20and%20selectively%20distills%20knowledge%20within%20previous%20Top-K%20subnetworks%20and%20output%20logits%2C%20without%20requiring%20replay%20or%20task%20labels.%20This%20enables%20structural%20realignment%20while%20preserving%20sparse%20modularity.%20Experiments%20on%20Split%20CIFAR-10%2C%20CIFAR-100%2C%20and%20MNIST%20demonstrate%20that%20SSD%20improves%20accuracy%2C%20retention%2C%20and%20representation%20coverage%2C%20offering%20a%20structurally%20grounded%20solution%20for%20sparse%20continual%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.15267v1&entry.124074799=Read"},
{"title": "Robust Tensor Principal Component Analysis: Exact Recovery via Deterministic Model", "author": "Bo Shen and Yutong Zhang and  Zhenyu and  Kong", "abstract": "Tensor, also known as multi-dimensional array, arises from many applications in signal processing, manufacturing processes, healthcare, among others. As one of the most popular methods in tensor literature, Robust tensor principal component analysis (RTPCA) is a very effective tool to extract the low rank and sparse components in tensors. In this paper, a new method to analyze RTPCA is proposed based on the recently developed tensor-tensor product and tensor singular value decomposition (t-SVD). Specifically, it aims to solve a convex optimization problem whose objective function is a weighted combination of the tensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact recovery is built on the tensor incoherence conditions and the assumption of a uniform model on the sparse support. Unlike this conventional way, in this paper, without any assumption of randomness, the exact recovery can be achieved in a completely deterministic fashion by characterizing the tensor rank-sparsity incoherence, which is an uncertainty principle between the low-rank tensor spaces and the pattern of sparse tensor.", "link": "http://arxiv.org/abs/2008.02211v2", "date": "2025-12-17", "relevancy": 2.1605, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Tensor%20Principal%20Component%20Analysis%3A%20Exact%20Recovery%20via%20Deterministic%20Model&body=Title%3A%20Robust%20Tensor%20Principal%20Component%20Analysis%3A%20Exact%20Recovery%20via%20Deterministic%20Model%0AAuthor%3A%20Bo%20Shen%20and%20Yutong%20Zhang%20and%20%20Zhenyu%20and%20%20Kong%0AAbstract%3A%20Tensor%2C%20also%20known%20as%20multi-dimensional%20array%2C%20arises%20from%20many%20applications%20in%20signal%20processing%2C%20manufacturing%20processes%2C%20healthcare%2C%20among%20others.%20As%20one%20of%20the%20most%20popular%20methods%20in%20tensor%20literature%2C%20Robust%20tensor%20principal%20component%20analysis%20%28RTPCA%29%20is%20a%20very%20effective%20tool%20to%20extract%20the%20low%20rank%20and%20sparse%20components%20in%20tensors.%20In%20this%20paper%2C%20a%20new%20method%20to%20analyze%20RTPCA%20is%20proposed%20based%20on%20the%20recently%20developed%20tensor-tensor%20product%20and%20tensor%20singular%20value%20decomposition%20%28t-SVD%29.%20Specifically%2C%20it%20aims%20to%20solve%20a%20convex%20optimization%20problem%20whose%20objective%20function%20is%20a%20weighted%20combination%20of%20the%20tensor%20nuclear%20norm%20and%20the%20l1-norm.%20In%20most%20of%20literature%20of%20RTPCA%2C%20the%20exact%20recovery%20is%20built%20on%20the%20tensor%20incoherence%20conditions%20and%20the%20assumption%20of%20a%20uniform%20model%20on%20the%20sparse%20support.%20Unlike%20this%20conventional%20way%2C%20in%20this%20paper%2C%20without%20any%20assumption%20of%20randomness%2C%20the%20exact%20recovery%20can%20be%20achieved%20in%20a%20completely%20deterministic%20fashion%20by%20characterizing%20the%20tensor%20rank-sparsity%20incoherence%2C%20which%20is%20an%20uncertainty%20principle%20between%20the%20low-rank%20tensor%20spaces%20and%20the%20pattern%20of%20sparse%20tensor.%0ALink%3A%20http%3A//arxiv.org/abs/2008.02211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Tensor%2520Principal%2520Component%2520Analysis%253A%2520Exact%2520Recovery%2520via%2520Deterministic%2520Model%26entry.906535625%3DBo%2520Shen%2520and%2520Yutong%2520Zhang%2520and%2520%2520Zhenyu%2520and%2520%2520Kong%26entry.1292438233%3DTensor%252C%2520also%2520known%2520as%2520multi-dimensional%2520array%252C%2520arises%2520from%2520many%2520applications%2520in%2520signal%2520processing%252C%2520manufacturing%2520processes%252C%2520healthcare%252C%2520among%2520others.%2520As%2520one%2520of%2520the%2520most%2520popular%2520methods%2520in%2520tensor%2520literature%252C%2520Robust%2520tensor%2520principal%2520component%2520analysis%2520%2528RTPCA%2529%2520is%2520a%2520very%2520effective%2520tool%2520to%2520extract%2520the%2520low%2520rank%2520and%2520sparse%2520components%2520in%2520tensors.%2520In%2520this%2520paper%252C%2520a%2520new%2520method%2520to%2520analyze%2520RTPCA%2520is%2520proposed%2520based%2520on%2520the%2520recently%2520developed%2520tensor-tensor%2520product%2520and%2520tensor%2520singular%2520value%2520decomposition%2520%2528t-SVD%2529.%2520Specifically%252C%2520it%2520aims%2520to%2520solve%2520a%2520convex%2520optimization%2520problem%2520whose%2520objective%2520function%2520is%2520a%2520weighted%2520combination%2520of%2520the%2520tensor%2520nuclear%2520norm%2520and%2520the%2520l1-norm.%2520In%2520most%2520of%2520literature%2520of%2520RTPCA%252C%2520the%2520exact%2520recovery%2520is%2520built%2520on%2520the%2520tensor%2520incoherence%2520conditions%2520and%2520the%2520assumption%2520of%2520a%2520uniform%2520model%2520on%2520the%2520sparse%2520support.%2520Unlike%2520this%2520conventional%2520way%252C%2520in%2520this%2520paper%252C%2520without%2520any%2520assumption%2520of%2520randomness%252C%2520the%2520exact%2520recovery%2520can%2520be%2520achieved%2520in%2520a%2520completely%2520deterministic%2520fashion%2520by%2520characterizing%2520the%2520tensor%2520rank-sparsity%2520incoherence%252C%2520which%2520is%2520an%2520uncertainty%2520principle%2520between%2520the%2520low-rank%2520tensor%2520spaces%2520and%2520the%2520pattern%2520of%2520sparse%2520tensor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2008.02211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Tensor%20Principal%20Component%20Analysis%3A%20Exact%20Recovery%20via%20Deterministic%20Model&entry.906535625=Bo%20Shen%20and%20Yutong%20Zhang%20and%20%20Zhenyu%20and%20%20Kong&entry.1292438233=Tensor%2C%20also%20known%20as%20multi-dimensional%20array%2C%20arises%20from%20many%20applications%20in%20signal%20processing%2C%20manufacturing%20processes%2C%20healthcare%2C%20among%20others.%20As%20one%20of%20the%20most%20popular%20methods%20in%20tensor%20literature%2C%20Robust%20tensor%20principal%20component%20analysis%20%28RTPCA%29%20is%20a%20very%20effective%20tool%20to%20extract%20the%20low%20rank%20and%20sparse%20components%20in%20tensors.%20In%20this%20paper%2C%20a%20new%20method%20to%20analyze%20RTPCA%20is%20proposed%20based%20on%20the%20recently%20developed%20tensor-tensor%20product%20and%20tensor%20singular%20value%20decomposition%20%28t-SVD%29.%20Specifically%2C%20it%20aims%20to%20solve%20a%20convex%20optimization%20problem%20whose%20objective%20function%20is%20a%20weighted%20combination%20of%20the%20tensor%20nuclear%20norm%20and%20the%20l1-norm.%20In%20most%20of%20literature%20of%20RTPCA%2C%20the%20exact%20recovery%20is%20built%20on%20the%20tensor%20incoherence%20conditions%20and%20the%20assumption%20of%20a%20uniform%20model%20on%20the%20sparse%20support.%20Unlike%20this%20conventional%20way%2C%20in%20this%20paper%2C%20without%20any%20assumption%20of%20randomness%2C%20the%20exact%20recovery%20can%20be%20achieved%20in%20a%20completely%20deterministic%20fashion%20by%20characterizing%20the%20tensor%20rank-sparsity%20incoherence%2C%20which%20is%20an%20uncertainty%20principle%20between%20the%20low-rank%20tensor%20spaces%20and%20the%20pattern%20of%20sparse%20tensor.&entry.1838667208=http%3A//arxiv.org/abs/2008.02211v2&entry.124074799=Read"},
{"title": "Safer Prompts: Reducing Risks from Memorization in Visual Generative AI", "author": "Lena Reissinger and Yuanyuan Li and Anna-Carolina Haensch and Neeraj Sarna", "abstract": "Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from user inputs like text prompts. However, because these models have billions of parameters, they risk memorizing certain parts of the training data and reproducing the memorized content. Memorization often raises concerns about safety of such models -- usually involving intellectual property (IP) infringement risk -- and deters their large scale adoption. In this paper, we evaluate the effectiveness of prompt engineering techniques in reducing memorization risk in image generation. Our findings demonstrate the effectiveness of prompt engineering in reducing the similarity between generated images and the training data of diffusion models, while maintaining relevance and aestheticity of the generated output.", "link": "http://arxiv.org/abs/2505.03338v2", "date": "2025-12-17", "relevancy": 2.1458, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5496}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5375}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safer%20Prompts%3A%20Reducing%20Risks%20from%20Memorization%20in%20Visual%20Generative%20AI&body=Title%3A%20Safer%20Prompts%3A%20Reducing%20Risks%20from%20Memorization%20in%20Visual%20Generative%20AI%0AAuthor%3A%20Lena%20Reissinger%20and%20Yuanyuan%20Li%20and%20Anna-Carolina%20Haensch%20and%20Neeraj%20Sarna%0AAbstract%3A%20Visual%20Generative%20AI%20models%20have%20demonstrated%20remarkable%20capability%20in%20generating%20high-quality%20images%20from%20user%20inputs%20like%20text%20prompts.%20However%2C%20because%20these%20models%20have%20billions%20of%20parameters%2C%20they%20risk%20memorizing%20certain%20parts%20of%20the%20training%20data%20and%20reproducing%20the%20memorized%20content.%20Memorization%20often%20raises%20concerns%20about%20safety%20of%20such%20models%20--%20usually%20involving%20intellectual%20property%20%28IP%29%20infringement%20risk%20--%20and%20deters%20their%20large%20scale%20adoption.%20In%20this%20paper%2C%20we%20evaluate%20the%20effectiveness%20of%20prompt%20engineering%20techniques%20in%20reducing%20memorization%20risk%20in%20image%20generation.%20Our%20findings%20demonstrate%20the%20effectiveness%20of%20prompt%20engineering%20in%20reducing%20the%20similarity%20between%20generated%20images%20and%20the%20training%20data%20of%20diffusion%20models%2C%20while%20maintaining%20relevance%20and%20aestheticity%20of%20the%20generated%20output.%0ALink%3A%20http%3A//arxiv.org/abs/2505.03338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafer%2520Prompts%253A%2520Reducing%2520Risks%2520from%2520Memorization%2520in%2520Visual%2520Generative%2520AI%26entry.906535625%3DLena%2520Reissinger%2520and%2520Yuanyuan%2520Li%2520and%2520Anna-Carolina%2520Haensch%2520and%2520Neeraj%2520Sarna%26entry.1292438233%3DVisual%2520Generative%2520AI%2520models%2520have%2520demonstrated%2520remarkable%2520capability%2520in%2520generating%2520high-quality%2520images%2520from%2520user%2520inputs%2520like%2520text%2520prompts.%2520However%252C%2520because%2520these%2520models%2520have%2520billions%2520of%2520parameters%252C%2520they%2520risk%2520memorizing%2520certain%2520parts%2520of%2520the%2520training%2520data%2520and%2520reproducing%2520the%2520memorized%2520content.%2520Memorization%2520often%2520raises%2520concerns%2520about%2520safety%2520of%2520such%2520models%2520--%2520usually%2520involving%2520intellectual%2520property%2520%2528IP%2529%2520infringement%2520risk%2520--%2520and%2520deters%2520their%2520large%2520scale%2520adoption.%2520In%2520this%2520paper%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520prompt%2520engineering%2520techniques%2520in%2520reducing%2520memorization%2520risk%2520in%2520image%2520generation.%2520Our%2520findings%2520demonstrate%2520the%2520effectiveness%2520of%2520prompt%2520engineering%2520in%2520reducing%2520the%2520similarity%2520between%2520generated%2520images%2520and%2520the%2520training%2520data%2520of%2520diffusion%2520models%252C%2520while%2520maintaining%2520relevance%2520and%2520aestheticity%2520of%2520the%2520generated%2520output.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safer%20Prompts%3A%20Reducing%20Risks%20from%20Memorization%20in%20Visual%20Generative%20AI&entry.906535625=Lena%20Reissinger%20and%20Yuanyuan%20Li%20and%20Anna-Carolina%20Haensch%20and%20Neeraj%20Sarna&entry.1292438233=Visual%20Generative%20AI%20models%20have%20demonstrated%20remarkable%20capability%20in%20generating%20high-quality%20images%20from%20user%20inputs%20like%20text%20prompts.%20However%2C%20because%20these%20models%20have%20billions%20of%20parameters%2C%20they%20risk%20memorizing%20certain%20parts%20of%20the%20training%20data%20and%20reproducing%20the%20memorized%20content.%20Memorization%20often%20raises%20concerns%20about%20safety%20of%20such%20models%20--%20usually%20involving%20intellectual%20property%20%28IP%29%20infringement%20risk%20--%20and%20deters%20their%20large%20scale%20adoption.%20In%20this%20paper%2C%20we%20evaluate%20the%20effectiveness%20of%20prompt%20engineering%20techniques%20in%20reducing%20memorization%20risk%20in%20image%20generation.%20Our%20findings%20demonstrate%20the%20effectiveness%20of%20prompt%20engineering%20in%20reducing%20the%20similarity%20between%20generated%20images%20and%20the%20training%20data%20of%20diffusion%20models%2C%20while%20maintaining%20relevance%20and%20aestheticity%20of%20the%20generated%20output.&entry.1838667208=http%3A//arxiv.org/abs/2505.03338v2&entry.124074799=Read"},
{"title": "Adversarial versification in portuguese as a jailbreak operator in LLMs", "author": "Joao Queiroz", "abstract": "Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.", "link": "http://arxiv.org/abs/2512.15353v1", "date": "2025-12-17", "relevancy": 2.1447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4315}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20versification%20in%20portuguese%20as%20a%20jailbreak%20operator%20in%20LLMs&body=Title%3A%20Adversarial%20versification%20in%20portuguese%20as%20a%20jailbreak%20operator%20in%20LLMs%0AAuthor%3A%20Joao%20Queiroz%0AAbstract%3A%20Recent%20evidence%20shows%20that%20the%20versification%20of%20prompts%20constitutes%20a%20highly%20effective%20adversarial%20mechanism%20against%20aligned%20LLMs.%20The%20study%20%27Adversarial%20poetry%20as%20a%20universal%20single-turn%20jailbreak%20mechanism%20in%20large%20language%20models%27%20demonstrates%20that%20instructions%20routinely%20refused%20in%20prose%20become%20executable%20when%20rewritten%20as%20verse%2C%20producing%20up%20to%2018%20x%20more%20safety%20failures%20in%20benchmarks%20derived%20from%20MLCommons%20AILuminate.%20Manually%20written%20poems%20reach%20approximately%2062%25%20ASR%2C%20and%20automated%20versions%2043%25%2C%20with%20some%20models%20surpassing%2090%25%20success%20in%20single-turn%20interactions.%20The%20effect%20is%20structural%3A%20systems%20trained%20with%20RLHF%2C%20constitutional%20AI%2C%20and%20hybrid%20pipelines%20exhibit%20consistent%20degradation%20under%20minimal%20semiotic%20formal%20variation.%20Versification%20displaces%20the%20prompt%20into%20sparsely%20supervised%20latent%20regions%2C%20revealing%20guardrails%20that%20are%20excessively%20dependent%20on%20surface%20patterns.%20This%20dissociation%20between%20apparent%20robustness%20and%20real%20vulnerability%20exposes%20deep%20limitations%20in%20current%20alignment%20regimes.%20The%20absence%20of%20evaluations%20in%20Portuguese%2C%20a%20language%20with%20high%20morphosyntactic%20complexity%2C%20a%20rich%20metric-prosodic%20tradition%2C%20and%20over%20250%20million%20speakers%2C%20constitutes%20a%20critical%20gap.%20Experimental%20protocols%20must%20parameterise%20scansion%2C%20metre%2C%20and%20prosodic%20variation%20to%20test%20vulnerabilities%20specific%20to%20Lusophone%20patterns%2C%20which%20are%20currently%20ignored.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520versification%2520in%2520portuguese%2520as%2520a%2520jailbreak%2520operator%2520in%2520LLMs%26entry.906535625%3DJoao%2520Queiroz%26entry.1292438233%3DRecent%2520evidence%2520shows%2520that%2520the%2520versification%2520of%2520prompts%2520constitutes%2520a%2520highly%2520effective%2520adversarial%2520mechanism%2520against%2520aligned%2520LLMs.%2520The%2520study%2520%2527Adversarial%2520poetry%2520as%2520a%2520universal%2520single-turn%2520jailbreak%2520mechanism%2520in%2520large%2520language%2520models%2527%2520demonstrates%2520that%2520instructions%2520routinely%2520refused%2520in%2520prose%2520become%2520executable%2520when%2520rewritten%2520as%2520verse%252C%2520producing%2520up%2520to%252018%2520x%2520more%2520safety%2520failures%2520in%2520benchmarks%2520derived%2520from%2520MLCommons%2520AILuminate.%2520Manually%2520written%2520poems%2520reach%2520approximately%252062%2525%2520ASR%252C%2520and%2520automated%2520versions%252043%2525%252C%2520with%2520some%2520models%2520surpassing%252090%2525%2520success%2520in%2520single-turn%2520interactions.%2520The%2520effect%2520is%2520structural%253A%2520systems%2520trained%2520with%2520RLHF%252C%2520constitutional%2520AI%252C%2520and%2520hybrid%2520pipelines%2520exhibit%2520consistent%2520degradation%2520under%2520minimal%2520semiotic%2520formal%2520variation.%2520Versification%2520displaces%2520the%2520prompt%2520into%2520sparsely%2520supervised%2520latent%2520regions%252C%2520revealing%2520guardrails%2520that%2520are%2520excessively%2520dependent%2520on%2520surface%2520patterns.%2520This%2520dissociation%2520between%2520apparent%2520robustness%2520and%2520real%2520vulnerability%2520exposes%2520deep%2520limitations%2520in%2520current%2520alignment%2520regimes.%2520The%2520absence%2520of%2520evaluations%2520in%2520Portuguese%252C%2520a%2520language%2520with%2520high%2520morphosyntactic%2520complexity%252C%2520a%2520rich%2520metric-prosodic%2520tradition%252C%2520and%2520over%2520250%2520million%2520speakers%252C%2520constitutes%2520a%2520critical%2520gap.%2520Experimental%2520protocols%2520must%2520parameterise%2520scansion%252C%2520metre%252C%2520and%2520prosodic%2520variation%2520to%2520test%2520vulnerabilities%2520specific%2520to%2520Lusophone%2520patterns%252C%2520which%2520are%2520currently%2520ignored.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20versification%20in%20portuguese%20as%20a%20jailbreak%20operator%20in%20LLMs&entry.906535625=Joao%20Queiroz&entry.1292438233=Recent%20evidence%20shows%20that%20the%20versification%20of%20prompts%20constitutes%20a%20highly%20effective%20adversarial%20mechanism%20against%20aligned%20LLMs.%20The%20study%20%27Adversarial%20poetry%20as%20a%20universal%20single-turn%20jailbreak%20mechanism%20in%20large%20language%20models%27%20demonstrates%20that%20instructions%20routinely%20refused%20in%20prose%20become%20executable%20when%20rewritten%20as%20verse%2C%20producing%20up%20to%2018%20x%20more%20safety%20failures%20in%20benchmarks%20derived%20from%20MLCommons%20AILuminate.%20Manually%20written%20poems%20reach%20approximately%2062%25%20ASR%2C%20and%20automated%20versions%2043%25%2C%20with%20some%20models%20surpassing%2090%25%20success%20in%20single-turn%20interactions.%20The%20effect%20is%20structural%3A%20systems%20trained%20with%20RLHF%2C%20constitutional%20AI%2C%20and%20hybrid%20pipelines%20exhibit%20consistent%20degradation%20under%20minimal%20semiotic%20formal%20variation.%20Versification%20displaces%20the%20prompt%20into%20sparsely%20supervised%20latent%20regions%2C%20revealing%20guardrails%20that%20are%20excessively%20dependent%20on%20surface%20patterns.%20This%20dissociation%20between%20apparent%20robustness%20and%20real%20vulnerability%20exposes%20deep%20limitations%20in%20current%20alignment%20regimes.%20The%20absence%20of%20evaluations%20in%20Portuguese%2C%20a%20language%20with%20high%20morphosyntactic%20complexity%2C%20a%20rich%20metric-prosodic%20tradition%2C%20and%20over%20250%20million%20speakers%2C%20constitutes%20a%20critical%20gap.%20Experimental%20protocols%20must%20parameterise%20scansion%2C%20metre%2C%20and%20prosodic%20variation%20to%20test%20vulnerabilities%20specific%20to%20Lusophone%20patterns%2C%20which%20are%20currently%20ignored.&entry.1838667208=http%3A//arxiv.org/abs/2512.15353v1&entry.124074799=Read"},
{"title": "Prompt-Based Continual Compositional Zero-Shot Learning", "author": "Sauda Maryam and Sara Nadeem and Faisal Qureshi and Mohsen Ali", "abstract": "We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.", "link": "http://arxiv.org/abs/2512.09172v2", "date": "2025-12-17", "relevancy": 2.1424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5538}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Based%20Continual%20Compositional%20Zero-Shot%20Learning&body=Title%3A%20Prompt-Based%20Continual%20Compositional%20Zero-Shot%20Learning%0AAuthor%3A%20Sauda%20Maryam%20and%20Sara%20Nadeem%20and%20Faisal%20Qureshi%20and%20Mohsen%20Ali%0AAbstract%3A%20We%20tackle%20continual%20adaptation%20of%20vision-language%20models%20to%20new%20attributes%2C%20objects%2C%20and%20their%20compositions%20in%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%2C%20while%20preventing%20forgetting%20of%20prior%20knowledge.%20Unlike%20classical%20continual%20learning%20where%20classes%20are%20disjoint%2C%20CCZSL%20is%20more%20complex%20as%20attributes%20and%20objects%20may%20reoccur%20across%20sessions%20while%20compositions%20remain%20unique.%20Built%20on%20a%20frozen%20VLM%20backbone%2C%20we%20propose%20the%20first%20Prompt-based%20Continual%20Compositional%20Zero-Shot%20Learning%20%28PromptCCZSL%29%20framework%20that%20retains%20prior%20knowledge%20through%20recency-weighted%20multi-teacher%20distillation.%20It%20employs%20session-aware%20compositional%20prompts%20to%20fuse%20multimodal%20features%20for%20new%20compositions%2C%20while%20attribute%20and%20object%20prompts%20are%20learned%20through%20session-agnostic%20fusion%20to%20maintain%20global%20semantic%20consistency%2C%20which%20is%20further%20stabilized%20by%20a%20Cosine%20Anchor%20Loss%20%28CAL%29%20to%20preserve%20prior%20knowledge.%20To%20enhance%20adaptation%20in%20the%20current%20session%2C%20an%20Orthogonal%20Projection%20Loss%20%28OPL%29%20ensures%20that%20new%20attribute%20and%20object%20embeddings%20remain%20distinct%20from%20previous%20ones%2C%20preventing%20overlap%2C%20while%20an%20Intra-Session%20Diversity%20Loss%20%28IDL%29%20promotes%20variation%20among%20current-session%20embeddings%20for%20richer%2C%20more%20discriminative%20representations.%20We%20also%20introduce%20a%20comprehensive%20protocol%20that%20jointly%20measures%20catastrophic%20forgetting%20and%20compositional%20generalization.%20Extensive%20experiments%20on%20UT-Zappos%20and%20C-GQA%20benchmarks%20demonstrate%20that%20PromptCCZSL%20achieves%20substantial%20improvements%20over%20prior%20VLM-based%20and%20non-VLM%20baselines%2C%20setting%20a%20new%20benchmark%20for%20CCZSL%20in%20closed-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Based%2520Continual%2520Compositional%2520Zero-Shot%2520Learning%26entry.906535625%3DSauda%2520Maryam%2520and%2520Sara%2520Nadeem%2520and%2520Faisal%2520Qureshi%2520and%2520Mohsen%2520Ali%26entry.1292438233%3DWe%2520tackle%2520continual%2520adaptation%2520of%2520vision-language%2520models%2520to%2520new%2520attributes%252C%2520objects%252C%2520and%2520their%2520compositions%2520in%2520Compositional%2520Zero-Shot%2520Learning%2520%2528CZSL%2529%252C%2520while%2520preventing%2520forgetting%2520of%2520prior%2520knowledge.%2520Unlike%2520classical%2520continual%2520learning%2520where%2520classes%2520are%2520disjoint%252C%2520CCZSL%2520is%2520more%2520complex%2520as%2520attributes%2520and%2520objects%2520may%2520reoccur%2520across%2520sessions%2520while%2520compositions%2520remain%2520unique.%2520Built%2520on%2520a%2520frozen%2520VLM%2520backbone%252C%2520we%2520propose%2520the%2520first%2520Prompt-based%2520Continual%2520Compositional%2520Zero-Shot%2520Learning%2520%2528PromptCCZSL%2529%2520framework%2520that%2520retains%2520prior%2520knowledge%2520through%2520recency-weighted%2520multi-teacher%2520distillation.%2520It%2520employs%2520session-aware%2520compositional%2520prompts%2520to%2520fuse%2520multimodal%2520features%2520for%2520new%2520compositions%252C%2520while%2520attribute%2520and%2520object%2520prompts%2520are%2520learned%2520through%2520session-agnostic%2520fusion%2520to%2520maintain%2520global%2520semantic%2520consistency%252C%2520which%2520is%2520further%2520stabilized%2520by%2520a%2520Cosine%2520Anchor%2520Loss%2520%2528CAL%2529%2520to%2520preserve%2520prior%2520knowledge.%2520To%2520enhance%2520adaptation%2520in%2520the%2520current%2520session%252C%2520an%2520Orthogonal%2520Projection%2520Loss%2520%2528OPL%2529%2520ensures%2520that%2520new%2520attribute%2520and%2520object%2520embeddings%2520remain%2520distinct%2520from%2520previous%2520ones%252C%2520preventing%2520overlap%252C%2520while%2520an%2520Intra-Session%2520Diversity%2520Loss%2520%2528IDL%2529%2520promotes%2520variation%2520among%2520current-session%2520embeddings%2520for%2520richer%252C%2520more%2520discriminative%2520representations.%2520We%2520also%2520introduce%2520a%2520comprehensive%2520protocol%2520that%2520jointly%2520measures%2520catastrophic%2520forgetting%2520and%2520compositional%2520generalization.%2520Extensive%2520experiments%2520on%2520UT-Zappos%2520and%2520C-GQA%2520benchmarks%2520demonstrate%2520that%2520PromptCCZSL%2520achieves%2520substantial%2520improvements%2520over%2520prior%2520VLM-based%2520and%2520non-VLM%2520baselines%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520CCZSL%2520in%2520closed-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Based%20Continual%20Compositional%20Zero-Shot%20Learning&entry.906535625=Sauda%20Maryam%20and%20Sara%20Nadeem%20and%20Faisal%20Qureshi%20and%20Mohsen%20Ali&entry.1292438233=We%20tackle%20continual%20adaptation%20of%20vision-language%20models%20to%20new%20attributes%2C%20objects%2C%20and%20their%20compositions%20in%20Compositional%20Zero-Shot%20Learning%20%28CZSL%29%2C%20while%20preventing%20forgetting%20of%20prior%20knowledge.%20Unlike%20classical%20continual%20learning%20where%20classes%20are%20disjoint%2C%20CCZSL%20is%20more%20complex%20as%20attributes%20and%20objects%20may%20reoccur%20across%20sessions%20while%20compositions%20remain%20unique.%20Built%20on%20a%20frozen%20VLM%20backbone%2C%20we%20propose%20the%20first%20Prompt-based%20Continual%20Compositional%20Zero-Shot%20Learning%20%28PromptCCZSL%29%20framework%20that%20retains%20prior%20knowledge%20through%20recency-weighted%20multi-teacher%20distillation.%20It%20employs%20session-aware%20compositional%20prompts%20to%20fuse%20multimodal%20features%20for%20new%20compositions%2C%20while%20attribute%20and%20object%20prompts%20are%20learned%20through%20session-agnostic%20fusion%20to%20maintain%20global%20semantic%20consistency%2C%20which%20is%20further%20stabilized%20by%20a%20Cosine%20Anchor%20Loss%20%28CAL%29%20to%20preserve%20prior%20knowledge.%20To%20enhance%20adaptation%20in%20the%20current%20session%2C%20an%20Orthogonal%20Projection%20Loss%20%28OPL%29%20ensures%20that%20new%20attribute%20and%20object%20embeddings%20remain%20distinct%20from%20previous%20ones%2C%20preventing%20overlap%2C%20while%20an%20Intra-Session%20Diversity%20Loss%20%28IDL%29%20promotes%20variation%20among%20current-session%20embeddings%20for%20richer%2C%20more%20discriminative%20representations.%20We%20also%20introduce%20a%20comprehensive%20protocol%20that%20jointly%20measures%20catastrophic%20forgetting%20and%20compositional%20generalization.%20Extensive%20experiments%20on%20UT-Zappos%20and%20C-GQA%20benchmarks%20demonstrate%20that%20PromptCCZSL%20achieves%20substantial%20improvements%20over%20prior%20VLM-based%20and%20non-VLM%20baselines%2C%20setting%20a%20new%20benchmark%20for%20CCZSL%20in%20closed-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.09172v2&entry.124074799=Read"},
{"title": "CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning", "author": "Longchen Dai and Zixuan Shen and Zhiheng Zhou and Peipeng Yu and Zhihua Xia", "abstract": "Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.", "link": "http://arxiv.org/abs/2512.15433v1", "date": "2025-12-17", "relevancy": 2.1416, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5373}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-FTI%3A%20Fine-Grained%20Face%20Template%20Inversion%20via%20CLIP-Driven%20Attribute%20Conditioning&body=Title%3A%20CLIP-FTI%3A%20Fine-Grained%20Face%20Template%20Inversion%20via%20CLIP-Driven%20Attribute%20Conditioning%0AAuthor%3A%20Longchen%20Dai%20and%20Zixuan%20Shen%20and%20Zhiheng%20Zhou%20and%20Peipeng%20Yu%20and%20Zhihua%20Xia%0AAbstract%3A%20Face%20recognition%20systems%20store%20face%20templates%20for%20efficient%20matching.%20Once%20leaked%2C%20these%20templates%20pose%20a%20threat%3A%20inverting%20them%20can%20yield%20photorealistic%20surrogates%20that%20compromise%20privacy%20and%20enable%20impersonation.%20Although%20existing%20research%20has%20achieved%20relatively%20realistic%20face%20template%20inversion%2C%20the%20reconstructed%20facial%20images%20exhibit%20over-smoothed%20facial-part%20attributes%20%28eyes%2C%20nose%2C%20mouth%29%20and%20limited%20transferability.%20To%20address%20this%20problem%2C%20we%20present%20CLIP-FTI%2C%20a%20CLIP-driven%20fine-grained%20attribute%20conditioning%20framework%20for%20face%20template%20inversion.%20Our%20core%20idea%20is%20to%20use%20the%20CLIP%20model%20to%20obtain%20the%20semantic%20embeddings%20of%20facial%20features%2C%20in%20order%20to%20realize%20the%20reconstruction%20of%20specific%20facial%20feature%20attributes.%20Specifically%2C%20facial%20feature%20attribute%20embeddings%20extracted%20from%20CLIP%20are%20fused%20with%20the%20leaked%20template%20via%20a%20cross-modal%20feature%20interaction%20network%20and%20projected%20into%20the%20intermediate%20latent%20space%20of%20a%20pretrained%20StyleGAN.%20The%20StyleGAN%20generator%20then%20synthesizes%20face%20images%20with%20the%20same%20identity%20as%20the%20templates%20but%20with%20more%20fine-grained%20facial%20feature%20attributes.%20Experiments%20across%20multiple%20face%20recognition%20backbones%20and%20datasets%20show%20that%20our%20reconstructions%20%28i%29%20achieve%20higher%20identification%20accuracy%20and%20attribute%20similarity%2C%20%28ii%29%20recover%20sharper%20component-level%20attribute%20semantics%2C%20and%20%28iii%29%20improve%20cross-model%20attack%20transferability%20compared%20to%20prior%20reconstruction%20attacks.%20To%20the%20best%20of%20our%20knowledge%2C%20ours%20is%20the%20first%20method%20to%20use%20additional%20information%20besides%20the%20face%20template%20attack%20to%20realize%20face%20template%20inversion%20and%20obtains%20SOTA%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-FTI%253A%2520Fine-Grained%2520Face%2520Template%2520Inversion%2520via%2520CLIP-Driven%2520Attribute%2520Conditioning%26entry.906535625%3DLongchen%2520Dai%2520and%2520Zixuan%2520Shen%2520and%2520Zhiheng%2520Zhou%2520and%2520Peipeng%2520Yu%2520and%2520Zhihua%2520Xia%26entry.1292438233%3DFace%2520recognition%2520systems%2520store%2520face%2520templates%2520for%2520efficient%2520matching.%2520Once%2520leaked%252C%2520these%2520templates%2520pose%2520a%2520threat%253A%2520inverting%2520them%2520can%2520yield%2520photorealistic%2520surrogates%2520that%2520compromise%2520privacy%2520and%2520enable%2520impersonation.%2520Although%2520existing%2520research%2520has%2520achieved%2520relatively%2520realistic%2520face%2520template%2520inversion%252C%2520the%2520reconstructed%2520facial%2520images%2520exhibit%2520over-smoothed%2520facial-part%2520attributes%2520%2528eyes%252C%2520nose%252C%2520mouth%2529%2520and%2520limited%2520transferability.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%2520CLIP-FTI%252C%2520a%2520CLIP-driven%2520fine-grained%2520attribute%2520conditioning%2520framework%2520for%2520face%2520template%2520inversion.%2520Our%2520core%2520idea%2520is%2520to%2520use%2520the%2520CLIP%2520model%2520to%2520obtain%2520the%2520semantic%2520embeddings%2520of%2520facial%2520features%252C%2520in%2520order%2520to%2520realize%2520the%2520reconstruction%2520of%2520specific%2520facial%2520feature%2520attributes.%2520Specifically%252C%2520facial%2520feature%2520attribute%2520embeddings%2520extracted%2520from%2520CLIP%2520are%2520fused%2520with%2520the%2520leaked%2520template%2520via%2520a%2520cross-modal%2520feature%2520interaction%2520network%2520and%2520projected%2520into%2520the%2520intermediate%2520latent%2520space%2520of%2520a%2520pretrained%2520StyleGAN.%2520The%2520StyleGAN%2520generator%2520then%2520synthesizes%2520face%2520images%2520with%2520the%2520same%2520identity%2520as%2520the%2520templates%2520but%2520with%2520more%2520fine-grained%2520facial%2520feature%2520attributes.%2520Experiments%2520across%2520multiple%2520face%2520recognition%2520backbones%2520and%2520datasets%2520show%2520that%2520our%2520reconstructions%2520%2528i%2529%2520achieve%2520higher%2520identification%2520accuracy%2520and%2520attribute%2520similarity%252C%2520%2528ii%2529%2520recover%2520sharper%2520component-level%2520attribute%2520semantics%252C%2520and%2520%2528iii%2529%2520improve%2520cross-model%2520attack%2520transferability%2520compared%2520to%2520prior%2520reconstruction%2520attacks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520ours%2520is%2520the%2520first%2520method%2520to%2520use%2520additional%2520information%2520besides%2520the%2520face%2520template%2520attack%2520to%2520realize%2520face%2520template%2520inversion%2520and%2520obtains%2520SOTA%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-FTI%3A%20Fine-Grained%20Face%20Template%20Inversion%20via%20CLIP-Driven%20Attribute%20Conditioning&entry.906535625=Longchen%20Dai%20and%20Zixuan%20Shen%20and%20Zhiheng%20Zhou%20and%20Peipeng%20Yu%20and%20Zhihua%20Xia&entry.1292438233=Face%20recognition%20systems%20store%20face%20templates%20for%20efficient%20matching.%20Once%20leaked%2C%20these%20templates%20pose%20a%20threat%3A%20inverting%20them%20can%20yield%20photorealistic%20surrogates%20that%20compromise%20privacy%20and%20enable%20impersonation.%20Although%20existing%20research%20has%20achieved%20relatively%20realistic%20face%20template%20inversion%2C%20the%20reconstructed%20facial%20images%20exhibit%20over-smoothed%20facial-part%20attributes%20%28eyes%2C%20nose%2C%20mouth%29%20and%20limited%20transferability.%20To%20address%20this%20problem%2C%20we%20present%20CLIP-FTI%2C%20a%20CLIP-driven%20fine-grained%20attribute%20conditioning%20framework%20for%20face%20template%20inversion.%20Our%20core%20idea%20is%20to%20use%20the%20CLIP%20model%20to%20obtain%20the%20semantic%20embeddings%20of%20facial%20features%2C%20in%20order%20to%20realize%20the%20reconstruction%20of%20specific%20facial%20feature%20attributes.%20Specifically%2C%20facial%20feature%20attribute%20embeddings%20extracted%20from%20CLIP%20are%20fused%20with%20the%20leaked%20template%20via%20a%20cross-modal%20feature%20interaction%20network%20and%20projected%20into%20the%20intermediate%20latent%20space%20of%20a%20pretrained%20StyleGAN.%20The%20StyleGAN%20generator%20then%20synthesizes%20face%20images%20with%20the%20same%20identity%20as%20the%20templates%20but%20with%20more%20fine-grained%20facial%20feature%20attributes.%20Experiments%20across%20multiple%20face%20recognition%20backbones%20and%20datasets%20show%20that%20our%20reconstructions%20%28i%29%20achieve%20higher%20identification%20accuracy%20and%20attribute%20similarity%2C%20%28ii%29%20recover%20sharper%20component-level%20attribute%20semantics%2C%20and%20%28iii%29%20improve%20cross-model%20attack%20transferability%20compared%20to%20prior%20reconstruction%20attacks.%20To%20the%20best%20of%20our%20knowledge%2C%20ours%20is%20the%20first%20method%20to%20use%20additional%20information%20besides%20the%20face%20template%20attack%20to%20realize%20face%20template%20inversion%20and%20obtains%20SOTA%20results.&entry.1838667208=http%3A//arxiv.org/abs/2512.15433v1&entry.124074799=Read"},
{"title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports", "author": "Donggeon David Oh and Justin Lidard and Haimin Hu and Himani Sinhmar and Elle Lazarski and Deepak Gopinath and Emily S. Sumner and Jonathan A. DeCastro and Guy Rosman and Naomi Ehrich Leonard and Jaime Fern\u00e1ndez Fisac", "abstract": "We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel state-action control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness.", "link": "http://arxiv.org/abs/2504.11717v4", "date": "2025-12-17", "relevancy": 2.1412, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5618}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5185}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%20AI-Assisted%20Motorsports&body=Title%3A%20Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%20AI-Assisted%20Motorsports%0AAuthor%3A%20Donggeon%20David%20Oh%20and%20Justin%20Lidard%20and%20Haimin%20Hu%20and%20Himani%20Sinhmar%20and%20Elle%20Lazarski%20and%20Deepak%20Gopinath%20and%20Emily%20S.%20Sumner%20and%20Jonathan%20A.%20DeCastro%20and%20Guy%20Rosman%20and%20Naomi%20Ehrich%20Leonard%20and%20Jaime%20Fern%C3%A1ndez%20Fisac%0AAbstract%3A%20We%20propose%20a%20human-centered%20safety%20filter%20%28HCSF%29%20for%20shared%20autonomy%20that%20significantly%20enhances%20system%20safety%20without%20compromising%20human%20agency.%20Our%20HCSF%20is%20built%20on%20a%20neural%20safety%20value%20function%2C%20which%20we%20first%20learn%20scalably%20through%20black-box%20interactions%20and%20then%20use%20at%20deployment%20to%20enforce%20a%20novel%20state-action%20control%20barrier%20function%20%28Q-CBF%29%20safety%20constraint.%20Since%20this%20Q-CBF%20safety%20filter%20does%20not%20require%20any%20knowledge%20of%20the%20system%20dynamics%20for%20both%20synthesis%20and%20runtime%20safety%20monitoring%20and%20intervention%2C%20our%20method%20applies%20readily%20to%20complex%2C%20black-box%20shared%20autonomy%20systems.%20Notably%2C%20our%20HCSF%27s%20CBF-based%20interventions%20modify%20the%20human%27s%20actions%20minimally%20and%20smoothly%2C%20avoiding%20the%20abrupt%2C%20last-moment%20corrections%20delivered%20by%20many%20conventional%20safety%20filters.%20We%20validate%20our%20approach%20in%20a%20comprehensive%20in-person%20user%20study%20using%20Assetto%20Corsa-a%20high-fidelity%20car%20racing%20simulator%20with%20black-box%20dynamics-to%20assess%20robustness%20in%20%22driving%20on%20the%20edge%22%20scenarios.%20We%20compare%20both%20trajectory%20data%20and%20drivers%27%20perceptions%20of%20our%20HCSF%20assistance%20against%20unassisted%20driving%20and%20a%20conventional%20safety%20filter.%20Experimental%20results%20show%20that%201%29%20compared%20to%20having%20no%20assistance%2C%20our%20HCSF%20improves%20both%20safety%20and%20user%20satisfaction%20without%20compromising%20human%20agency%20or%20comfort%2C%20and%202%29%20relative%20to%20a%20conventional%20safety%20filter%2C%20our%20proposed%20HCSF%20boosts%20human%20agency%2C%20comfort%2C%20and%20satisfaction%20while%20maintaining%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2504.11717v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520with%2520Agency%253A%2520Human-Centered%2520Safety%2520Filter%2520with%2520Application%2520to%2520AI-Assisted%2520Motorsports%26entry.906535625%3DDonggeon%2520David%2520Oh%2520and%2520Justin%2520Lidard%2520and%2520Haimin%2520Hu%2520and%2520Himani%2520Sinhmar%2520and%2520Elle%2520Lazarski%2520and%2520Deepak%2520Gopinath%2520and%2520Emily%2520S.%2520Sumner%2520and%2520Jonathan%2520A.%2520DeCastro%2520and%2520Guy%2520Rosman%2520and%2520Naomi%2520Ehrich%2520Leonard%2520and%2520Jaime%2520Fern%25C3%25A1ndez%2520Fisac%26entry.1292438233%3DWe%2520propose%2520a%2520human-centered%2520safety%2520filter%2520%2528HCSF%2529%2520for%2520shared%2520autonomy%2520that%2520significantly%2520enhances%2520system%2520safety%2520without%2520compromising%2520human%2520agency.%2520Our%2520HCSF%2520is%2520built%2520on%2520a%2520neural%2520safety%2520value%2520function%252C%2520which%2520we%2520first%2520learn%2520scalably%2520through%2520black-box%2520interactions%2520and%2520then%2520use%2520at%2520deployment%2520to%2520enforce%2520a%2520novel%2520state-action%2520control%2520barrier%2520function%2520%2528Q-CBF%2529%2520safety%2520constraint.%2520Since%2520this%2520Q-CBF%2520safety%2520filter%2520does%2520not%2520require%2520any%2520knowledge%2520of%2520the%2520system%2520dynamics%2520for%2520both%2520synthesis%2520and%2520runtime%2520safety%2520monitoring%2520and%2520intervention%252C%2520our%2520method%2520applies%2520readily%2520to%2520complex%252C%2520black-box%2520shared%2520autonomy%2520systems.%2520Notably%252C%2520our%2520HCSF%2527s%2520CBF-based%2520interventions%2520modify%2520the%2520human%2527s%2520actions%2520minimally%2520and%2520smoothly%252C%2520avoiding%2520the%2520abrupt%252C%2520last-moment%2520corrections%2520delivered%2520by%2520many%2520conventional%2520safety%2520filters.%2520We%2520validate%2520our%2520approach%2520in%2520a%2520comprehensive%2520in-person%2520user%2520study%2520using%2520Assetto%2520Corsa-a%2520high-fidelity%2520car%2520racing%2520simulator%2520with%2520black-box%2520dynamics-to%2520assess%2520robustness%2520in%2520%2522driving%2520on%2520the%2520edge%2522%2520scenarios.%2520We%2520compare%2520both%2520trajectory%2520data%2520and%2520drivers%2527%2520perceptions%2520of%2520our%2520HCSF%2520assistance%2520against%2520unassisted%2520driving%2520and%2520a%2520conventional%2520safety%2520filter.%2520Experimental%2520results%2520show%2520that%25201%2529%2520compared%2520to%2520having%2520no%2520assistance%252C%2520our%2520HCSF%2520improves%2520both%2520safety%2520and%2520user%2520satisfaction%2520without%2520compromising%2520human%2520agency%2520or%2520comfort%252C%2520and%25202%2529%2520relative%2520to%2520a%2520conventional%2520safety%2520filter%252C%2520our%2520proposed%2520HCSF%2520boosts%2520human%2520agency%252C%2520comfort%252C%2520and%2520satisfaction%2520while%2520maintaining%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11717v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20with%20Agency%3A%20Human-Centered%20Safety%20Filter%20with%20Application%20to%20AI-Assisted%20Motorsports&entry.906535625=Donggeon%20David%20Oh%20and%20Justin%20Lidard%20and%20Haimin%20Hu%20and%20Himani%20Sinhmar%20and%20Elle%20Lazarski%20and%20Deepak%20Gopinath%20and%20Emily%20S.%20Sumner%20and%20Jonathan%20A.%20DeCastro%20and%20Guy%20Rosman%20and%20Naomi%20Ehrich%20Leonard%20and%20Jaime%20Fern%C3%A1ndez%20Fisac&entry.1292438233=We%20propose%20a%20human-centered%20safety%20filter%20%28HCSF%29%20for%20shared%20autonomy%20that%20significantly%20enhances%20system%20safety%20without%20compromising%20human%20agency.%20Our%20HCSF%20is%20built%20on%20a%20neural%20safety%20value%20function%2C%20which%20we%20first%20learn%20scalably%20through%20black-box%20interactions%20and%20then%20use%20at%20deployment%20to%20enforce%20a%20novel%20state-action%20control%20barrier%20function%20%28Q-CBF%29%20safety%20constraint.%20Since%20this%20Q-CBF%20safety%20filter%20does%20not%20require%20any%20knowledge%20of%20the%20system%20dynamics%20for%20both%20synthesis%20and%20runtime%20safety%20monitoring%20and%20intervention%2C%20our%20method%20applies%20readily%20to%20complex%2C%20black-box%20shared%20autonomy%20systems.%20Notably%2C%20our%20HCSF%27s%20CBF-based%20interventions%20modify%20the%20human%27s%20actions%20minimally%20and%20smoothly%2C%20avoiding%20the%20abrupt%2C%20last-moment%20corrections%20delivered%20by%20many%20conventional%20safety%20filters.%20We%20validate%20our%20approach%20in%20a%20comprehensive%20in-person%20user%20study%20using%20Assetto%20Corsa-a%20high-fidelity%20car%20racing%20simulator%20with%20black-box%20dynamics-to%20assess%20robustness%20in%20%22driving%20on%20the%20edge%22%20scenarios.%20We%20compare%20both%20trajectory%20data%20and%20drivers%27%20perceptions%20of%20our%20HCSF%20assistance%20against%20unassisted%20driving%20and%20a%20conventional%20safety%20filter.%20Experimental%20results%20show%20that%201%29%20compared%20to%20having%20no%20assistance%2C%20our%20HCSF%20improves%20both%20safety%20and%20user%20satisfaction%20without%20compromising%20human%20agency%20or%20comfort%2C%20and%202%29%20relative%20to%20a%20conventional%20safety%20filter%2C%20our%20proposed%20HCSF%20boosts%20human%20agency%2C%20comfort%2C%20and%20satisfaction%20while%20maintaining%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2504.11717v4&entry.124074799=Read"},
{"title": "Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies", "author": "Charan Prakash Rathore and Saumi Ray and Dhruv Kumar", "abstract": "Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.", "link": "http://arxiv.org/abs/2512.15312v1", "date": "2025-12-17", "relevancy": 2.1225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLMs%20for%20Zeolite%20Synthesis%20Event%20Extraction%20%28ZSEE%29%3A%20A%20Systematic%20Analysis%20of%20Prompting%20Strategies&body=Title%3A%20Evaluating%20LLMs%20for%20Zeolite%20Synthesis%20Event%20Extraction%20%28ZSEE%29%3A%20A%20Systematic%20Analysis%20of%20Prompting%20Strategies%0AAuthor%3A%20Charan%20Prakash%20Rathore%20and%20Saumi%20Ray%20and%20Dhruv%20Kumar%0AAbstract%3A%20Extracting%20structured%20information%20from%20zeolite%20synthesis%20experimental%20procedures%20is%20critical%20for%20materials%20discovery%2C%20yet%20existing%20methods%20have%20not%20systematically%20evaluated%20Large%20Language%20Models%20%28LLMs%29%20for%20this%20domain-specific%20task.%20This%20work%20addresses%20a%20fundamental%20question%3A%20what%20is%20the%20efficacy%20of%20different%20prompting%20strategies%20when%20applying%20LLMs%20to%20scientific%20information%20extraction%3F%20We%20focus%20on%20four%20key%20subtasks%3A%20event%20type%20classification%20%28identifying%20synthesis%20steps%29%2C%20trigger%20text%20identification%20%28locating%20event%20mentions%29%2C%20argument%20role%20extraction%20%28recognizing%20parameter%20types%29%2C%20and%20argument%20text%20extraction%20%28extracting%20parameter%20values%29.%20We%20evaluate%20four%20prompting%20strategies%20-%20zero-shot%2C%20few-shot%2C%20event-specific%2C%20and%20reflection-based%20-%20across%20six%20state-of-the-art%20LLMs%20%28Gemma-3-12b-it%2C%20GPT-5-mini%2C%20O4-mini%2C%20Claude-Haiku-3.5%2C%20DeepSeek%20reasoning%20and%20non-reasoning%29%20using%20the%20ZSEE%20dataset%20of%201%2C530%20annotated%20sentences.%20Results%20demonstrate%20strong%20performance%20on%20event%20type%20classification%20%2880-90%5C%25%20F1%29%20but%20modest%20performance%20on%20fine-grained%20extraction%20tasks%2C%20particularly%20argument%20role%20and%20argument%20text%20extraction%20%2850-65%5C%25%20F1%29.%20GPT-5-mini%20exhibits%20extreme%20prompt%20sensitivity%20with%2011-79%5C%25%20F1%20variation.%20Notably%2C%20advanced%20prompting%20strategies%20provide%20minimal%20improvements%20over%20zero-shot%20approaches%2C%20revealing%20fundamental%20architectural%20limitations.%20Error%20analysis%20identifies%20systematic%20hallucination%2C%20over-generalization%2C%20and%20inability%20to%20capture%20synthesis-specific%20nuances.%20Our%20findings%20demonstrate%20that%20while%20LLMs%20achieve%20high-level%20understanding%2C%20precise%20extraction%20of%20experimental%20parameters%20requires%20domain-adapted%20models%2C%20providing%20quantitative%20benchmarks%20for%20scientific%20information%20extraction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLMs%2520for%2520Zeolite%2520Synthesis%2520Event%2520Extraction%2520%2528ZSEE%2529%253A%2520A%2520Systematic%2520Analysis%2520of%2520Prompting%2520Strategies%26entry.906535625%3DCharan%2520Prakash%2520Rathore%2520and%2520Saumi%2520Ray%2520and%2520Dhruv%2520Kumar%26entry.1292438233%3DExtracting%2520structured%2520information%2520from%2520zeolite%2520synthesis%2520experimental%2520procedures%2520is%2520critical%2520for%2520materials%2520discovery%252C%2520yet%2520existing%2520methods%2520have%2520not%2520systematically%2520evaluated%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520this%2520domain-specific%2520task.%2520This%2520work%2520addresses%2520a%2520fundamental%2520question%253A%2520what%2520is%2520the%2520efficacy%2520of%2520different%2520prompting%2520strategies%2520when%2520applying%2520LLMs%2520to%2520scientific%2520information%2520extraction%253F%2520We%2520focus%2520on%2520four%2520key%2520subtasks%253A%2520event%2520type%2520classification%2520%2528identifying%2520synthesis%2520steps%2529%252C%2520trigger%2520text%2520identification%2520%2528locating%2520event%2520mentions%2529%252C%2520argument%2520role%2520extraction%2520%2528recognizing%2520parameter%2520types%2529%252C%2520and%2520argument%2520text%2520extraction%2520%2528extracting%2520parameter%2520values%2529.%2520We%2520evaluate%2520four%2520prompting%2520strategies%2520-%2520zero-shot%252C%2520few-shot%252C%2520event-specific%252C%2520and%2520reflection-based%2520-%2520across%2520six%2520state-of-the-art%2520LLMs%2520%2528Gemma-3-12b-it%252C%2520GPT-5-mini%252C%2520O4-mini%252C%2520Claude-Haiku-3.5%252C%2520DeepSeek%2520reasoning%2520and%2520non-reasoning%2529%2520using%2520the%2520ZSEE%2520dataset%2520of%25201%252C530%2520annotated%2520sentences.%2520Results%2520demonstrate%2520strong%2520performance%2520on%2520event%2520type%2520classification%2520%252880-90%255C%2525%2520F1%2529%2520but%2520modest%2520performance%2520on%2520fine-grained%2520extraction%2520tasks%252C%2520particularly%2520argument%2520role%2520and%2520argument%2520text%2520extraction%2520%252850-65%255C%2525%2520F1%2529.%2520GPT-5-mini%2520exhibits%2520extreme%2520prompt%2520sensitivity%2520with%252011-79%255C%2525%2520F1%2520variation.%2520Notably%252C%2520advanced%2520prompting%2520strategies%2520provide%2520minimal%2520improvements%2520over%2520zero-shot%2520approaches%252C%2520revealing%2520fundamental%2520architectural%2520limitations.%2520Error%2520analysis%2520identifies%2520systematic%2520hallucination%252C%2520over-generalization%252C%2520and%2520inability%2520to%2520capture%2520synthesis-specific%2520nuances.%2520Our%2520findings%2520demonstrate%2520that%2520while%2520LLMs%2520achieve%2520high-level%2520understanding%252C%2520precise%2520extraction%2520of%2520experimental%2520parameters%2520requires%2520domain-adapted%2520models%252C%2520providing%2520quantitative%2520benchmarks%2520for%2520scientific%2520information%2520extraction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLMs%20for%20Zeolite%20Synthesis%20Event%20Extraction%20%28ZSEE%29%3A%20A%20Systematic%20Analysis%20of%20Prompting%20Strategies&entry.906535625=Charan%20Prakash%20Rathore%20and%20Saumi%20Ray%20and%20Dhruv%20Kumar&entry.1292438233=Extracting%20structured%20information%20from%20zeolite%20synthesis%20experimental%20procedures%20is%20critical%20for%20materials%20discovery%2C%20yet%20existing%20methods%20have%20not%20systematically%20evaluated%20Large%20Language%20Models%20%28LLMs%29%20for%20this%20domain-specific%20task.%20This%20work%20addresses%20a%20fundamental%20question%3A%20what%20is%20the%20efficacy%20of%20different%20prompting%20strategies%20when%20applying%20LLMs%20to%20scientific%20information%20extraction%3F%20We%20focus%20on%20four%20key%20subtasks%3A%20event%20type%20classification%20%28identifying%20synthesis%20steps%29%2C%20trigger%20text%20identification%20%28locating%20event%20mentions%29%2C%20argument%20role%20extraction%20%28recognizing%20parameter%20types%29%2C%20and%20argument%20text%20extraction%20%28extracting%20parameter%20values%29.%20We%20evaluate%20four%20prompting%20strategies%20-%20zero-shot%2C%20few-shot%2C%20event-specific%2C%20and%20reflection-based%20-%20across%20six%20state-of-the-art%20LLMs%20%28Gemma-3-12b-it%2C%20GPT-5-mini%2C%20O4-mini%2C%20Claude-Haiku-3.5%2C%20DeepSeek%20reasoning%20and%20non-reasoning%29%20using%20the%20ZSEE%20dataset%20of%201%2C530%20annotated%20sentences.%20Results%20demonstrate%20strong%20performance%20on%20event%20type%20classification%20%2880-90%5C%25%20F1%29%20but%20modest%20performance%20on%20fine-grained%20extraction%20tasks%2C%20particularly%20argument%20role%20and%20argument%20text%20extraction%20%2850-65%5C%25%20F1%29.%20GPT-5-mini%20exhibits%20extreme%20prompt%20sensitivity%20with%2011-79%5C%25%20F1%20variation.%20Notably%2C%20advanced%20prompting%20strategies%20provide%20minimal%20improvements%20over%20zero-shot%20approaches%2C%20revealing%20fundamental%20architectural%20limitations.%20Error%20analysis%20identifies%20systematic%20hallucination%2C%20over-generalization%2C%20and%20inability%20to%20capture%20synthesis-specific%20nuances.%20Our%20findings%20demonstrate%20that%20while%20LLMs%20achieve%20high-level%20understanding%2C%20precise%20extraction%20of%20experimental%20parameters%20requires%20domain-adapted%20models%2C%20providing%20quantitative%20benchmarks%20for%20scientific%20information%20extraction.&entry.1838667208=http%3A//arxiv.org/abs/2512.15312v1&entry.124074799=Read"},
{"title": "Binarization-Aware Adjuster: A Theoretical Framework for Bridging Continuous Optimization and Discrete Inference with Application to Edge Detection", "author": "Hao Shu", "abstract": "In machine learning, discrete decision-making tasks exhibit a fundamental inconsistency between training and inference: models are optimized using continuous-valued outputs, yet evaluated through discrete predictions. This discrepancy arises from the non-differentiability of discretization operations, weakening the alignment between optimization objectives and practical decision outcomes. To address this, we present a theoretical framework for constructing a Binarization-Aware Adjuster (BAA) that integrates binarization behavior directly into gradient-based learning. Central to the approach is a Distance Weight Function (DWF) that dynamically modulates pixel-wise loss contributions based on prediction correctness and proximity to the decision boundary, thereby emphasizing decision-critical regions while de-emphasizing confidently correct samples. Furthermore, a self-adaptive threshold estimation procedure is introduced to better match optimization dynamics with inference conditions. As one of its applications, we implement experiments on the edge detection (ED) task, which also demonstrate the effectiveness of the proposed method experimentally. Beyond binary decision tasks and ED, the proposed framework provides a general strategy for aligning continuous optimization with discrete evaluation and can be extended to multi-valued decision processes in broader structured prediction problems.", "link": "http://arxiv.org/abs/2506.12460v2", "date": "2025-12-17", "relevancy": 2.1217, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binarization-Aware%20Adjuster%3A%20A%20Theoretical%20Framework%20for%20Bridging%20Continuous%20Optimization%20and%20Discrete%20Inference%20with%20Application%20to%20Edge%20Detection&body=Title%3A%20Binarization-Aware%20Adjuster%3A%20A%20Theoretical%20Framework%20for%20Bridging%20Continuous%20Optimization%20and%20Discrete%20Inference%20with%20Application%20to%20Edge%20Detection%0AAuthor%3A%20Hao%20Shu%0AAbstract%3A%20In%20machine%20learning%2C%20discrete%20decision-making%20tasks%20exhibit%20a%20fundamental%20inconsistency%20between%20training%20and%20inference%3A%20models%20are%20optimized%20using%20continuous-valued%20outputs%2C%20yet%20evaluated%20through%20discrete%20predictions.%20This%20discrepancy%20arises%20from%20the%20non-differentiability%20of%20discretization%20operations%2C%20weakening%20the%20alignment%20between%20optimization%20objectives%20and%20practical%20decision%20outcomes.%20To%20address%20this%2C%20we%20present%20a%20theoretical%20framework%20for%20constructing%20a%20Binarization-Aware%20Adjuster%20%28BAA%29%20that%20integrates%20binarization%20behavior%20directly%20into%20gradient-based%20learning.%20Central%20to%20the%20approach%20is%20a%20Distance%20Weight%20Function%20%28DWF%29%20that%20dynamically%20modulates%20pixel-wise%20loss%20contributions%20based%20on%20prediction%20correctness%20and%20proximity%20to%20the%20decision%20boundary%2C%20thereby%20emphasizing%20decision-critical%20regions%20while%20de-emphasizing%20confidently%20correct%20samples.%20Furthermore%2C%20a%20self-adaptive%20threshold%20estimation%20procedure%20is%20introduced%20to%20better%20match%20optimization%20dynamics%20with%20inference%20conditions.%20As%20one%20of%20its%20applications%2C%20we%20implement%20experiments%20on%20the%20edge%20detection%20%28ED%29%20task%2C%20which%20also%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20experimentally.%20Beyond%20binary%20decision%20tasks%20and%20ED%2C%20the%20proposed%20framework%20provides%20a%20general%20strategy%20for%20aligning%20continuous%20optimization%20with%20discrete%20evaluation%20and%20can%20be%20extended%20to%20multi-valued%20decision%20processes%20in%20broader%20structured%20prediction%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinarization-Aware%2520Adjuster%253A%2520A%2520Theoretical%2520Framework%2520for%2520Bridging%2520Continuous%2520Optimization%2520and%2520Discrete%2520Inference%2520with%2520Application%2520to%2520Edge%2520Detection%26entry.906535625%3DHao%2520Shu%26entry.1292438233%3DIn%2520machine%2520learning%252C%2520discrete%2520decision-making%2520tasks%2520exhibit%2520a%2520fundamental%2520inconsistency%2520between%2520training%2520and%2520inference%253A%2520models%2520are%2520optimized%2520using%2520continuous-valued%2520outputs%252C%2520yet%2520evaluated%2520through%2520discrete%2520predictions.%2520This%2520discrepancy%2520arises%2520from%2520the%2520non-differentiability%2520of%2520discretization%2520operations%252C%2520weakening%2520the%2520alignment%2520between%2520optimization%2520objectives%2520and%2520practical%2520decision%2520outcomes.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520theoretical%2520framework%2520for%2520constructing%2520a%2520Binarization-Aware%2520Adjuster%2520%2528BAA%2529%2520that%2520integrates%2520binarization%2520behavior%2520directly%2520into%2520gradient-based%2520learning.%2520Central%2520to%2520the%2520approach%2520is%2520a%2520Distance%2520Weight%2520Function%2520%2528DWF%2529%2520that%2520dynamically%2520modulates%2520pixel-wise%2520loss%2520contributions%2520based%2520on%2520prediction%2520correctness%2520and%2520proximity%2520to%2520the%2520decision%2520boundary%252C%2520thereby%2520emphasizing%2520decision-critical%2520regions%2520while%2520de-emphasizing%2520confidently%2520correct%2520samples.%2520Furthermore%252C%2520a%2520self-adaptive%2520threshold%2520estimation%2520procedure%2520is%2520introduced%2520to%2520better%2520match%2520optimization%2520dynamics%2520with%2520inference%2520conditions.%2520As%2520one%2520of%2520its%2520applications%252C%2520we%2520implement%2520experiments%2520on%2520the%2520edge%2520detection%2520%2528ED%2529%2520task%252C%2520which%2520also%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520experimentally.%2520Beyond%2520binary%2520decision%2520tasks%2520and%2520ED%252C%2520the%2520proposed%2520framework%2520provides%2520a%2520general%2520strategy%2520for%2520aligning%2520continuous%2520optimization%2520with%2520discrete%2520evaluation%2520and%2520can%2520be%2520extended%2520to%2520multi-valued%2520decision%2520processes%2520in%2520broader%2520structured%2520prediction%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binarization-Aware%20Adjuster%3A%20A%20Theoretical%20Framework%20for%20Bridging%20Continuous%20Optimization%20and%20Discrete%20Inference%20with%20Application%20to%20Edge%20Detection&entry.906535625=Hao%20Shu&entry.1292438233=In%20machine%20learning%2C%20discrete%20decision-making%20tasks%20exhibit%20a%20fundamental%20inconsistency%20between%20training%20and%20inference%3A%20models%20are%20optimized%20using%20continuous-valued%20outputs%2C%20yet%20evaluated%20through%20discrete%20predictions.%20This%20discrepancy%20arises%20from%20the%20non-differentiability%20of%20discretization%20operations%2C%20weakening%20the%20alignment%20between%20optimization%20objectives%20and%20practical%20decision%20outcomes.%20To%20address%20this%2C%20we%20present%20a%20theoretical%20framework%20for%20constructing%20a%20Binarization-Aware%20Adjuster%20%28BAA%29%20that%20integrates%20binarization%20behavior%20directly%20into%20gradient-based%20learning.%20Central%20to%20the%20approach%20is%20a%20Distance%20Weight%20Function%20%28DWF%29%20that%20dynamically%20modulates%20pixel-wise%20loss%20contributions%20based%20on%20prediction%20correctness%20and%20proximity%20to%20the%20decision%20boundary%2C%20thereby%20emphasizing%20decision-critical%20regions%20while%20de-emphasizing%20confidently%20correct%20samples.%20Furthermore%2C%20a%20self-adaptive%20threshold%20estimation%20procedure%20is%20introduced%20to%20better%20match%20optimization%20dynamics%20with%20inference%20conditions.%20As%20one%20of%20its%20applications%2C%20we%20implement%20experiments%20on%20the%20edge%20detection%20%28ED%29%20task%2C%20which%20also%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20experimentally.%20Beyond%20binary%20decision%20tasks%20and%20ED%2C%20the%20proposed%20framework%20provides%20a%20general%20strategy%20for%20aligning%20continuous%20optimization%20with%20discrete%20evaluation%20and%20can%20be%20extended%20to%20multi-valued%20decision%20processes%20in%20broader%20structured%20prediction%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2506.12460v2&entry.124074799=Read"},
{"title": "Soft Geometric Inductive Bias for Object Centric Dynamics", "author": "Hampus Linander and Conor Heins and Alexander Tschantz and Marco Perin and Christopher Buckley", "abstract": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.", "link": "http://arxiv.org/abs/2512.15493v1", "date": "2025-12-17", "relevancy": 2.12, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5307}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Geometric%20Inductive%20Bias%20for%20Object%20Centric%20Dynamics&body=Title%3A%20Soft%20Geometric%20Inductive%20Bias%20for%20Object%20Centric%20Dynamics%0AAuthor%3A%20Hampus%20Linander%20and%20Conor%20Heins%20and%20Alexander%20Tschantz%20and%20Marco%20Perin%20and%20Christopher%20Buckley%0AAbstract%3A%20Equivariance%20is%20a%20powerful%20prior%20for%20learning%20physical%20dynamics%2C%20yet%20exact%20group%20equivariance%20can%20degrade%20performance%20if%20the%20symmetries%20are%20broken.%20We%20propose%20object-centric%20world%20models%20built%20with%20geometric%20algebra%20neural%20networks%2C%20providing%20a%20soft%20geometric%20inductive%20bias.%20Our%20models%20are%20evaluated%20using%20simulated%20environments%20of%202d%20rigid%20body%20dynamics%20with%20static%20obstacles%2C%20where%20we%20train%20for%20next-step%20predictions%20autoregressively.%20For%20long-horizon%20rollouts%20we%20show%20that%20the%20soft%20inductive%20bias%20of%20our%20models%20results%20in%20better%20performance%20in%20terms%20of%20physical%20fidelity%20compared%20to%20non-equivariant%20baseline%20models.%20The%20approach%20complements%20recent%20soft-equivariance%20ideas%20and%20aligns%20with%20the%20view%20that%20simple%2C%20well-chosen%20priors%20can%20yield%20robust%20generalization.%20These%20results%20suggest%20that%20geometric%20algebra%20offers%20an%20effective%20middle%20ground%20between%20hand-crafted%20physics%20and%20unstructured%20deep%20nets%2C%20delivering%20sample-efficient%20dynamics%20models%20for%20multi-object%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Geometric%2520Inductive%2520Bias%2520for%2520Object%2520Centric%2520Dynamics%26entry.906535625%3DHampus%2520Linander%2520and%2520Conor%2520Heins%2520and%2520Alexander%2520Tschantz%2520and%2520Marco%2520Perin%2520and%2520Christopher%2520Buckley%26entry.1292438233%3DEquivariance%2520is%2520a%2520powerful%2520prior%2520for%2520learning%2520physical%2520dynamics%252C%2520yet%2520exact%2520group%2520equivariance%2520can%2520degrade%2520performance%2520if%2520the%2520symmetries%2520are%2520broken.%2520We%2520propose%2520object-centric%2520world%2520models%2520built%2520with%2520geometric%2520algebra%2520neural%2520networks%252C%2520providing%2520a%2520soft%2520geometric%2520inductive%2520bias.%2520Our%2520models%2520are%2520evaluated%2520using%2520simulated%2520environments%2520of%25202d%2520rigid%2520body%2520dynamics%2520with%2520static%2520obstacles%252C%2520where%2520we%2520train%2520for%2520next-step%2520predictions%2520autoregressively.%2520For%2520long-horizon%2520rollouts%2520we%2520show%2520that%2520the%2520soft%2520inductive%2520bias%2520of%2520our%2520models%2520results%2520in%2520better%2520performance%2520in%2520terms%2520of%2520physical%2520fidelity%2520compared%2520to%2520non-equivariant%2520baseline%2520models.%2520The%2520approach%2520complements%2520recent%2520soft-equivariance%2520ideas%2520and%2520aligns%2520with%2520the%2520view%2520that%2520simple%252C%2520well-chosen%2520priors%2520can%2520yield%2520robust%2520generalization.%2520These%2520results%2520suggest%2520that%2520geometric%2520algebra%2520offers%2520an%2520effective%2520middle%2520ground%2520between%2520hand-crafted%2520physics%2520and%2520unstructured%2520deep%2520nets%252C%2520delivering%2520sample-efficient%2520dynamics%2520models%2520for%2520multi-object%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Geometric%20Inductive%20Bias%20for%20Object%20Centric%20Dynamics&entry.906535625=Hampus%20Linander%20and%20Conor%20Heins%20and%20Alexander%20Tschantz%20and%20Marco%20Perin%20and%20Christopher%20Buckley&entry.1292438233=Equivariance%20is%20a%20powerful%20prior%20for%20learning%20physical%20dynamics%2C%20yet%20exact%20group%20equivariance%20can%20degrade%20performance%20if%20the%20symmetries%20are%20broken.%20We%20propose%20object-centric%20world%20models%20built%20with%20geometric%20algebra%20neural%20networks%2C%20providing%20a%20soft%20geometric%20inductive%20bias.%20Our%20models%20are%20evaluated%20using%20simulated%20environments%20of%202d%20rigid%20body%20dynamics%20with%20static%20obstacles%2C%20where%20we%20train%20for%20next-step%20predictions%20autoregressively.%20For%20long-horizon%20rollouts%20we%20show%20that%20the%20soft%20inductive%20bias%20of%20our%20models%20results%20in%20better%20performance%20in%20terms%20of%20physical%20fidelity%20compared%20to%20non-equivariant%20baseline%20models.%20The%20approach%20complements%20recent%20soft-equivariance%20ideas%20and%20aligns%20with%20the%20view%20that%20simple%2C%20well-chosen%20priors%20can%20yield%20robust%20generalization.%20These%20results%20suggest%20that%20geometric%20algebra%20offers%20an%20effective%20middle%20ground%20between%20hand-crafted%20physics%20and%20unstructured%20deep%20nets%2C%20delivering%20sample-efficient%20dynamics%20models%20for%20multi-object%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15493v1&entry.124074799=Read"},
{"title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", "author": "Erfei Cui and Wenhai Wang and Zhiqi Li and Jiangwei Xie and Haoming Zou and Hanming Deng and Gen Luo and Lewei Lu and Xizhou Zhu and Jifeng Dai", "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.", "link": "http://arxiv.org/abs/2312.09245v3", "date": "2025-12-17", "relevancy": 2.1127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveMLM%3A%20Aligning%20Multi-Modal%20Large%20Language%20Models%20with%20Behavioral%20Planning%20States%20for%20Autonomous%20Driving&body=Title%3A%20DriveMLM%3A%20Aligning%20Multi-Modal%20Large%20Language%20Models%20with%20Behavioral%20Planning%20States%20for%20Autonomous%20Driving%0AAuthor%3A%20Erfei%20Cui%20and%20Wenhai%20Wang%20and%20Zhiqi%20Li%20and%20Jiangwei%20Xie%20and%20Haoming%20Zou%20and%20Hanming%20Deng%20and%20Gen%20Luo%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Jifeng%20Dai%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20opened%20up%20new%20possibilities%20for%20intelligent%20agents%2C%20endowing%20them%20with%20human-like%20thinking%20and%20cognitive%20abilities.%20In%20this%20work%2C%20we%20delve%20into%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20in%20autonomous%20driving%20%28AD%29.%20We%20introduce%20DriveMLM%2C%20an%20LLM-based%20AD%20framework%20that%20can%20perform%20close-loop%20autonomous%20driving%20in%20realistic%20simulators.%20To%20this%20end%2C%20%281%29%20we%20bridge%20the%20gap%20between%20the%20language%20decisions%20and%20the%20vehicle%20control%20commands%20by%20standardizing%20the%20decision%20states%20according%20to%20the%20off-the-shelf%20motion%20planning%20module.%20%282%29%20We%20employ%20a%20multimodal%20LLM%20%28MLLM%29%20to%20model%20the%20behavior%20planning%20module%20of%20a%20module%20AD%20system%2C%20which%20uses%20driving%20rules%2C%20user%20commands%2C%20and%20inputs%20from%20various%20sensors%20%28e.g.%2C%20camera%2C%20lidar%29%20as%20input%20and%20makes%20driving%20decisions%20and%20provide%20explanations%3B%20This%20model%20can%20plug-and-play%20in%20existing%20AD%20systems%20such%20as%20Autopilot%20and%20Apollo%20for%20close-loop%20driving.%20%283%29%20We%20design%20an%20effective%20data%20engine%20to%20collect%20a%20dataset%20that%20includes%20decision%20state%20and%20corresponding%20explanation%20annotation%20for%20model%20training%20and%20evaluation.%20We%20conduct%20extensive%20experiments%20and%20show%20that%20replacing%20the%20decision-making%20modules%20of%20the%20Autopilot%20and%20Apollo%20with%20DriveMLM%20resulted%20in%20significant%20improvements%20of%203.2%20and%204.7%20points%20on%20the%20CARLA%20Town05%20Long%20respectively%2C%20demonstrating%20the%20effectiveness%20of%20our%20model.%20We%20hope%20this%20work%20can%20serve%20as%20a%20baseline%20for%20autonomous%20driving%20with%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2312.09245v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveMLM%253A%2520Aligning%2520Multi-Modal%2520Large%2520Language%2520Models%2520with%2520Behavioral%2520Planning%2520States%2520for%2520Autonomous%2520Driving%26entry.906535625%3DErfei%2520Cui%2520and%2520Wenhai%2520Wang%2520and%2520Zhiqi%2520Li%2520and%2520Jiangwei%2520Xie%2520and%2520Haoming%2520Zou%2520and%2520Hanming%2520Deng%2520and%2520Gen%2520Luo%2520and%2520Lewei%2520Lu%2520and%2520Xizhou%2520Zhu%2520and%2520Jifeng%2520Dai%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520opened%2520up%2520new%2520possibilities%2520for%2520intelligent%2520agents%252C%2520endowing%2520them%2520with%2520human-like%2520thinking%2520and%2520cognitive%2520abilities.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520potential%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520autonomous%2520driving%2520%2528AD%2529.%2520We%2520introduce%2520DriveMLM%252C%2520an%2520LLM-based%2520AD%2520framework%2520that%2520can%2520perform%2520close-loop%2520autonomous%2520driving%2520in%2520realistic%2520simulators.%2520To%2520this%2520end%252C%2520%25281%2529%2520we%2520bridge%2520the%2520gap%2520between%2520the%2520language%2520decisions%2520and%2520the%2520vehicle%2520control%2520commands%2520by%2520standardizing%2520the%2520decision%2520states%2520according%2520to%2520the%2520off-the-shelf%2520motion%2520planning%2520module.%2520%25282%2529%2520We%2520employ%2520a%2520multimodal%2520LLM%2520%2528MLLM%2529%2520to%2520model%2520the%2520behavior%2520planning%2520module%2520of%2520a%2520module%2520AD%2520system%252C%2520which%2520uses%2520driving%2520rules%252C%2520user%2520commands%252C%2520and%2520inputs%2520from%2520various%2520sensors%2520%2528e.g.%252C%2520camera%252C%2520lidar%2529%2520as%2520input%2520and%2520makes%2520driving%2520decisions%2520and%2520provide%2520explanations%253B%2520This%2520model%2520can%2520plug-and-play%2520in%2520existing%2520AD%2520systems%2520such%2520as%2520Autopilot%2520and%2520Apollo%2520for%2520close-loop%2520driving.%2520%25283%2529%2520We%2520design%2520an%2520effective%2520data%2520engine%2520to%2520collect%2520a%2520dataset%2520that%2520includes%2520decision%2520state%2520and%2520corresponding%2520explanation%2520annotation%2520for%2520model%2520training%2520and%2520evaluation.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520show%2520that%2520replacing%2520the%2520decision-making%2520modules%2520of%2520the%2520Autopilot%2520and%2520Apollo%2520with%2520DriveMLM%2520resulted%2520in%2520significant%2520improvements%2520of%25203.2%2520and%25204.7%2520points%2520on%2520the%2520CARLA%2520Town05%2520Long%2520respectively%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520model.%2520We%2520hope%2520this%2520work%2520can%2520serve%2520as%2520a%2520baseline%2520for%2520autonomous%2520driving%2520with%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09245v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveMLM%3A%20Aligning%20Multi-Modal%20Large%20Language%20Models%20with%20Behavioral%20Planning%20States%20for%20Autonomous%20Driving&entry.906535625=Erfei%20Cui%20and%20Wenhai%20Wang%20and%20Zhiqi%20Li%20and%20Jiangwei%20Xie%20and%20Haoming%20Zou%20and%20Hanming%20Deng%20and%20Gen%20Luo%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Jifeng%20Dai&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20opened%20up%20new%20possibilities%20for%20intelligent%20agents%2C%20endowing%20them%20with%20human-like%20thinking%20and%20cognitive%20abilities.%20In%20this%20work%2C%20we%20delve%20into%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20in%20autonomous%20driving%20%28AD%29.%20We%20introduce%20DriveMLM%2C%20an%20LLM-based%20AD%20framework%20that%20can%20perform%20close-loop%20autonomous%20driving%20in%20realistic%20simulators.%20To%20this%20end%2C%20%281%29%20we%20bridge%20the%20gap%20between%20the%20language%20decisions%20and%20the%20vehicle%20control%20commands%20by%20standardizing%20the%20decision%20states%20according%20to%20the%20off-the-shelf%20motion%20planning%20module.%20%282%29%20We%20employ%20a%20multimodal%20LLM%20%28MLLM%29%20to%20model%20the%20behavior%20planning%20module%20of%20a%20module%20AD%20system%2C%20which%20uses%20driving%20rules%2C%20user%20commands%2C%20and%20inputs%20from%20various%20sensors%20%28e.g.%2C%20camera%2C%20lidar%29%20as%20input%20and%20makes%20driving%20decisions%20and%20provide%20explanations%3B%20This%20model%20can%20plug-and-play%20in%20existing%20AD%20systems%20such%20as%20Autopilot%20and%20Apollo%20for%20close-loop%20driving.%20%283%29%20We%20design%20an%20effective%20data%20engine%20to%20collect%20a%20dataset%20that%20includes%20decision%20state%20and%20corresponding%20explanation%20annotation%20for%20model%20training%20and%20evaluation.%20We%20conduct%20extensive%20experiments%20and%20show%20that%20replacing%20the%20decision-making%20modules%20of%20the%20Autopilot%20and%20Apollo%20with%20DriveMLM%20resulted%20in%20significant%20improvements%20of%203.2%20and%204.7%20points%20on%20the%20CARLA%20Town05%20Long%20respectively%2C%20demonstrating%20the%20effectiveness%20of%20our%20model.%20We%20hope%20this%20work%20can%20serve%20as%20a%20baseline%20for%20autonomous%20driving%20with%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2312.09245v3&entry.124074799=Read"},
{"title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "author": "Kiran Shahi and Anup Bagale", "abstract": "Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.", "link": "http://arxiv.org/abs/2511.00456v5", "date": "2025-12-17", "relevancy": 2.1001, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5287}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%20Pneumonia%20Localization%20from%20Chest%20X-Rays%20Using%20Deep%20Neural%20Network%20and%20Grad-CAM%20Explanations&body=Title%3A%20Weakly%20Supervised%20Pneumonia%20Localization%20from%20Chest%20X-Rays%20Using%20Deep%20Neural%20Network%20and%20Grad-CAM%20Explanations%0AAuthor%3A%20Kiran%20Shahi%20and%20Anup%20Bagale%0AAbstract%3A%20Chest%20X-ray%20imaging%20is%20commonly%20used%20to%20diagnose%20pneumonia%2C%20but%20accurately%20localizing%20the%20pneumonia-affected%20regions%20typically%20requires%20detailed%20pixel-level%20annotations%2C%20which%20are%20costly%20and%20time%20consuming%20to%20obtain.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20weakly%20supervised%20deep%20learning%20framework%20for%20pneumonia%20classification%20and%20localization%20using%20Gradient-weighted%20Class%20Activation%20Mapping%20%28Grad-CAM%29.%20Instead%20of%20relying%20on%20costly%20pixel-level%20annotations%2C%20the%20proposed%20method%20utilizes%20image-level%20labels%20to%20generate%20clinically%20meaningful%20heatmaps%20that%20highlight%20pneumonia-affected%20regions.%20Furthermore%2C%20we%20evaluate%20seven%20pre-trained%20deep%20learning%20models%2C%20including%20a%20Vision%20Transformer%2C%20under%20identical%20training%20conditions%2C%20using%20focal%20loss%20and%20patient-wise%20splits%20to%20prevent%20data%20leakage.%20Experimental%20results%20suggest%20that%20all%20models%20achieved%20high%20classification%20accuracy%20%2896--98%5C%25%29%2C%20with%20ResNet-18%20and%20EfficientNet-B0%20showing%20the%20best%20overall%20performance%20and%20MobileNet-V3%20providing%20an%20efficient%20lightweight%20alternative.%20Grad-CAM%20heatmap%20visualizations%20confirm%20that%20the%20proposed%20methods%20focus%20on%20clinically%20relevant%20lung%20regions%2C%20supporting%20the%20use%20of%20explainable%20AI%20for%20radiological%20diagnostics.%20Overall%2C%20this%20work%20highlights%20the%20potential%20of%20weakly%20supervised%2C%20explainable%20models%20that%20enhance%20transparency%20and%20clinical%20trust%20in%20AI-assisted%20pneumonia%20screening.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00456v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520Supervised%2520Pneumonia%2520Localization%2520from%2520Chest%2520X-Rays%2520Using%2520Deep%2520Neural%2520Network%2520and%2520Grad-CAM%2520Explanations%26entry.906535625%3DKiran%2520Shahi%2520and%2520Anup%2520Bagale%26entry.1292438233%3DChest%2520X-ray%2520imaging%2520is%2520commonly%2520used%2520to%2520diagnose%2520pneumonia%252C%2520but%2520accurately%2520localizing%2520the%2520pneumonia-affected%2520regions%2520typically%2520requires%2520detailed%2520pixel-level%2520annotations%252C%2520which%2520are%2520costly%2520and%2520time%2520consuming%2520to%2520obtain.%2520To%2520address%2520this%2520limitation%252C%2520this%2520study%2520proposes%2520a%2520weakly%2520supervised%2520deep%2520learning%2520framework%2520for%2520pneumonia%2520classification%2520and%2520localization%2520using%2520Gradient-weighted%2520Class%2520Activation%2520Mapping%2520%2528Grad-CAM%2529.%2520Instead%2520of%2520relying%2520on%2520costly%2520pixel-level%2520annotations%252C%2520the%2520proposed%2520method%2520utilizes%2520image-level%2520labels%2520to%2520generate%2520clinically%2520meaningful%2520heatmaps%2520that%2520highlight%2520pneumonia-affected%2520regions.%2520Furthermore%252C%2520we%2520evaluate%2520seven%2520pre-trained%2520deep%2520learning%2520models%252C%2520including%2520a%2520Vision%2520Transformer%252C%2520under%2520identical%2520training%2520conditions%252C%2520using%2520focal%2520loss%2520and%2520patient-wise%2520splits%2520to%2520prevent%2520data%2520leakage.%2520Experimental%2520results%2520suggest%2520that%2520all%2520models%2520achieved%2520high%2520classification%2520accuracy%2520%252896--98%255C%2525%2529%252C%2520with%2520ResNet-18%2520and%2520EfficientNet-B0%2520showing%2520the%2520best%2520overall%2520performance%2520and%2520MobileNet-V3%2520providing%2520an%2520efficient%2520lightweight%2520alternative.%2520Grad-CAM%2520heatmap%2520visualizations%2520confirm%2520that%2520the%2520proposed%2520methods%2520focus%2520on%2520clinically%2520relevant%2520lung%2520regions%252C%2520supporting%2520the%2520use%2520of%2520explainable%2520AI%2520for%2520radiological%2520diagnostics.%2520Overall%252C%2520this%2520work%2520highlights%2520the%2520potential%2520of%2520weakly%2520supervised%252C%2520explainable%2520models%2520that%2520enhance%2520transparency%2520and%2520clinical%2520trust%2520in%2520AI-assisted%2520pneumonia%2520screening.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00456v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%20Pneumonia%20Localization%20from%20Chest%20X-Rays%20Using%20Deep%20Neural%20Network%20and%20Grad-CAM%20Explanations&entry.906535625=Kiran%20Shahi%20and%20Anup%20Bagale&entry.1292438233=Chest%20X-ray%20imaging%20is%20commonly%20used%20to%20diagnose%20pneumonia%2C%20but%20accurately%20localizing%20the%20pneumonia-affected%20regions%20typically%20requires%20detailed%20pixel-level%20annotations%2C%20which%20are%20costly%20and%20time%20consuming%20to%20obtain.%20To%20address%20this%20limitation%2C%20this%20study%20proposes%20a%20weakly%20supervised%20deep%20learning%20framework%20for%20pneumonia%20classification%20and%20localization%20using%20Gradient-weighted%20Class%20Activation%20Mapping%20%28Grad-CAM%29.%20Instead%20of%20relying%20on%20costly%20pixel-level%20annotations%2C%20the%20proposed%20method%20utilizes%20image-level%20labels%20to%20generate%20clinically%20meaningful%20heatmaps%20that%20highlight%20pneumonia-affected%20regions.%20Furthermore%2C%20we%20evaluate%20seven%20pre-trained%20deep%20learning%20models%2C%20including%20a%20Vision%20Transformer%2C%20under%20identical%20training%20conditions%2C%20using%20focal%20loss%20and%20patient-wise%20splits%20to%20prevent%20data%20leakage.%20Experimental%20results%20suggest%20that%20all%20models%20achieved%20high%20classification%20accuracy%20%2896--98%5C%25%29%2C%20with%20ResNet-18%20and%20EfficientNet-B0%20showing%20the%20best%20overall%20performance%20and%20MobileNet-V3%20providing%20an%20efficient%20lightweight%20alternative.%20Grad-CAM%20heatmap%20visualizations%20confirm%20that%20the%20proposed%20methods%20focus%20on%20clinically%20relevant%20lung%20regions%2C%20supporting%20the%20use%20of%20explainable%20AI%20for%20radiological%20diagnostics.%20Overall%2C%20this%20work%20highlights%20the%20potential%20of%20weakly%20supervised%2C%20explainable%20models%20that%20enhance%20transparency%20and%20clinical%20trust%20in%20AI-assisted%20pneumonia%20screening.&entry.1838667208=http%3A//arxiv.org/abs/2511.00456v5&entry.124074799=Read"},
{"title": "Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier", "author": "Adri\u00e1n Detavernier and Jasper De Bock", "abstract": "We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.", "link": "http://arxiv.org/abs/2512.15492v1", "date": "2025-12-17", "relevancy": 2.0851, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5255}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20and%20uncertainty%3A%20two%20complementary%20aspects%20of%20the%20reliability%20of%20the%20predictions%20of%20a%20classifier&body=Title%3A%20Robustness%20and%20uncertainty%3A%20two%20complementary%20aspects%20of%20the%20reliability%20of%20the%20predictions%20of%20a%20classifier%0AAuthor%3A%20Adri%C3%A1n%20Detavernier%20and%20Jasper%20De%20Bock%0AAbstract%3A%20We%20consider%20two%20conceptually%20different%20approaches%20for%20assessing%20the%20reliability%20of%20the%20individual%20predictions%20of%20a%20classifier%3A%20Robustness%20Quantification%20%28RQ%29%20and%20Uncertainty%20Quantification%20%28UQ%29.%20We%20compare%20both%20approaches%20on%20a%20number%20of%20benchmark%20datasets%20and%20show%20that%20there%20is%20no%20clear%20winner%20between%20the%20two%2C%20but%20that%20they%20are%20complementary%20and%20can%20be%20combined%20to%20obtain%20a%20hybrid%20approach%20that%20outperforms%20both%20RQ%20and%20UQ.%20As%20a%20byproduct%20of%20our%20approach%2C%20for%20each%20dataset%2C%20we%20also%20obtain%20an%20assessment%20of%20the%20relative%20importance%20of%20uncertainty%20and%20robustness%20as%20sources%20of%20unreliability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520and%2520uncertainty%253A%2520two%2520complementary%2520aspects%2520of%2520the%2520reliability%2520of%2520the%2520predictions%2520of%2520a%2520classifier%26entry.906535625%3DAdri%25C3%25A1n%2520Detavernier%2520and%2520Jasper%2520De%2520Bock%26entry.1292438233%3DWe%2520consider%2520two%2520conceptually%2520different%2520approaches%2520for%2520assessing%2520the%2520reliability%2520of%2520the%2520individual%2520predictions%2520of%2520a%2520classifier%253A%2520Robustness%2520Quantification%2520%2528RQ%2529%2520and%2520Uncertainty%2520Quantification%2520%2528UQ%2529.%2520We%2520compare%2520both%2520approaches%2520on%2520a%2520number%2520of%2520benchmark%2520datasets%2520and%2520show%2520that%2520there%2520is%2520no%2520clear%2520winner%2520between%2520the%2520two%252C%2520but%2520that%2520they%2520are%2520complementary%2520and%2520can%2520be%2520combined%2520to%2520obtain%2520a%2520hybrid%2520approach%2520that%2520outperforms%2520both%2520RQ%2520and%2520UQ.%2520As%2520a%2520byproduct%2520of%2520our%2520approach%252C%2520for%2520each%2520dataset%252C%2520we%2520also%2520obtain%2520an%2520assessment%2520of%2520the%2520relative%2520importance%2520of%2520uncertainty%2520and%2520robustness%2520as%2520sources%2520of%2520unreliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20and%20uncertainty%3A%20two%20complementary%20aspects%20of%20the%20reliability%20of%20the%20predictions%20of%20a%20classifier&entry.906535625=Adri%C3%A1n%20Detavernier%20and%20Jasper%20De%20Bock&entry.1292438233=We%20consider%20two%20conceptually%20different%20approaches%20for%20assessing%20the%20reliability%20of%20the%20individual%20predictions%20of%20a%20classifier%3A%20Robustness%20Quantification%20%28RQ%29%20and%20Uncertainty%20Quantification%20%28UQ%29.%20We%20compare%20both%20approaches%20on%20a%20number%20of%20benchmark%20datasets%20and%20show%20that%20there%20is%20no%20clear%20winner%20between%20the%20two%2C%20but%20that%20they%20are%20complementary%20and%20can%20be%20combined%20to%20obtain%20a%20hybrid%20approach%20that%20outperforms%20both%20RQ%20and%20UQ.%20As%20a%20byproduct%20of%20our%20approach%2C%20for%20each%20dataset%2C%20we%20also%20obtain%20an%20assessment%20of%20the%20relative%20importance%20of%20uncertainty%20and%20robustness%20as%20sources%20of%20unreliability.&entry.1838667208=http%3A//arxiv.org/abs/2512.15492v1&entry.124074799=Read"},
{"title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping", "author": "Wen Xie and Yanjun Zhu and Gijs Overgoor and Yakov Bart and Agata Lapedriza Garcia and Sarah Ostadabbas", "abstract": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall. The dataset and code are available at https://github.com/ostadabbas/AdSum204.", "link": "http://arxiv.org/abs/2510.26569v2", "date": "2025-12-17", "relevancy": 2.0795, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5159}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdSum%3A%20Two-stream%20Audio-visual%20Summarization%20for%20Automated%20Video%20Advertisement%20Clipping&body=Title%3A%20AdSum%3A%20Two-stream%20Audio-visual%20Summarization%20for%20Automated%20Video%20Advertisement%20Clipping%0AAuthor%3A%20Wen%20Xie%20and%20Yanjun%20Zhu%20and%20Gijs%20Overgoor%20and%20Yakov%20Bart%20and%20Agata%20Lapedriza%20Garcia%20and%20Sarah%20Ostadabbas%0AAbstract%3A%20Advertisers%20commonly%20need%20multiple%20versions%20of%20the%20same%20advertisement%20%28ad%29%20at%20varying%20durations%20for%20a%20single%20campaign.%20The%20traditional%20approach%20involves%20manually%20selecting%20and%20re-editing%20shots%20from%20longer%20video%20ads%20to%20create%20shorter%20versions%2C%20which%20is%20labor-intensive%20and%20time-consuming.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20for%20automated%20video%20ad%20clipping%20using%20video%20summarization%20techniques.%20We%20are%20the%20first%20to%20frame%20video%20clipping%20as%20a%20shot%20selection%20problem%2C%20tailored%20specifically%20for%20advertising.%20Unlike%20existing%20general%20video%20summarization%20methods%20that%20primarily%20focus%20on%20visual%20content%2C%20our%20approach%20emphasizes%20the%20critical%20role%20of%20audio%20in%20advertising.%20To%20achieve%20this%2C%20we%20develop%20a%20two-stream%20audio-visual%20fusion%20model%20that%20predicts%20the%20importance%20of%20video%20frames%2C%20where%20importance%20is%20defined%20as%20the%20likelihood%20of%20a%20frame%20being%20selected%20in%20the%20firm-produced%20short%20ad.%20To%20address%20the%20lack%20of%20ad-specific%20datasets%2C%20we%20present%20AdSum204%2C%20a%20novel%20dataset%20comprising%20102%20pairs%20of%2030-second%20and%2015-second%20ads%20from%20real%20advertising%20campaigns.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20outperforms%20state-of-the-art%20methods%20across%20various%20metrics%2C%20including%20Average%20Precision%2C%20Area%20Under%20Curve%2C%20Spearman%2C%20and%20Kendall.%20The%20dataset%20and%20code%20are%20available%20at%20https%3A//github.com/ostadabbas/AdSum204.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdSum%253A%2520Two-stream%2520Audio-visual%2520Summarization%2520for%2520Automated%2520Video%2520Advertisement%2520Clipping%26entry.906535625%3DWen%2520Xie%2520and%2520Yanjun%2520Zhu%2520and%2520Gijs%2520Overgoor%2520and%2520Yakov%2520Bart%2520and%2520Agata%2520Lapedriza%2520Garcia%2520and%2520Sarah%2520Ostadabbas%26entry.1292438233%3DAdvertisers%2520commonly%2520need%2520multiple%2520versions%2520of%2520the%2520same%2520advertisement%2520%2528ad%2529%2520at%2520varying%2520durations%2520for%2520a%2520single%2520campaign.%2520The%2520traditional%2520approach%2520involves%2520manually%2520selecting%2520and%2520re-editing%2520shots%2520from%2520longer%2520video%2520ads%2520to%2520create%2520shorter%2520versions%252C%2520which%2520is%2520labor-intensive%2520and%2520time-consuming.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520framework%2520for%2520automated%2520video%2520ad%2520clipping%2520using%2520video%2520summarization%2520techniques.%2520We%2520are%2520the%2520first%2520to%2520frame%2520video%2520clipping%2520as%2520a%2520shot%2520selection%2520problem%252C%2520tailored%2520specifically%2520for%2520advertising.%2520Unlike%2520existing%2520general%2520video%2520summarization%2520methods%2520that%2520primarily%2520focus%2520on%2520visual%2520content%252C%2520our%2520approach%2520emphasizes%2520the%2520critical%2520role%2520of%2520audio%2520in%2520advertising.%2520To%2520achieve%2520this%252C%2520we%2520develop%2520a%2520two-stream%2520audio-visual%2520fusion%2520model%2520that%2520predicts%2520the%2520importance%2520of%2520video%2520frames%252C%2520where%2520importance%2520is%2520defined%2520as%2520the%2520likelihood%2520of%2520a%2520frame%2520being%2520selected%2520in%2520the%2520firm-produced%2520short%2520ad.%2520To%2520address%2520the%2520lack%2520of%2520ad-specific%2520datasets%252C%2520we%2520present%2520AdSum204%252C%2520a%2520novel%2520dataset%2520comprising%2520102%2520pairs%2520of%252030-second%2520and%252015-second%2520ads%2520from%2520real%2520advertising%2520campaigns.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520state-of-the-art%2520methods%2520across%2520various%2520metrics%252C%2520including%2520Average%2520Precision%252C%2520Area%2520Under%2520Curve%252C%2520Spearman%252C%2520and%2520Kendall.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520at%2520https%253A//github.com/ostadabbas/AdSum204.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdSum%3A%20Two-stream%20Audio-visual%20Summarization%20for%20Automated%20Video%20Advertisement%20Clipping&entry.906535625=Wen%20Xie%20and%20Yanjun%20Zhu%20and%20Gijs%20Overgoor%20and%20Yakov%20Bart%20and%20Agata%20Lapedriza%20Garcia%20and%20Sarah%20Ostadabbas&entry.1292438233=Advertisers%20commonly%20need%20multiple%20versions%20of%20the%20same%20advertisement%20%28ad%29%20at%20varying%20durations%20for%20a%20single%20campaign.%20The%20traditional%20approach%20involves%20manually%20selecting%20and%20re-editing%20shots%20from%20longer%20video%20ads%20to%20create%20shorter%20versions%2C%20which%20is%20labor-intensive%20and%20time-consuming.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20for%20automated%20video%20ad%20clipping%20using%20video%20summarization%20techniques.%20We%20are%20the%20first%20to%20frame%20video%20clipping%20as%20a%20shot%20selection%20problem%2C%20tailored%20specifically%20for%20advertising.%20Unlike%20existing%20general%20video%20summarization%20methods%20that%20primarily%20focus%20on%20visual%20content%2C%20our%20approach%20emphasizes%20the%20critical%20role%20of%20audio%20in%20advertising.%20To%20achieve%20this%2C%20we%20develop%20a%20two-stream%20audio-visual%20fusion%20model%20that%20predicts%20the%20importance%20of%20video%20frames%2C%20where%20importance%20is%20defined%20as%20the%20likelihood%20of%20a%20frame%20being%20selected%20in%20the%20firm-produced%20short%20ad.%20To%20address%20the%20lack%20of%20ad-specific%20datasets%2C%20we%20present%20AdSum204%2C%20a%20novel%20dataset%20comprising%20102%20pairs%20of%2030-second%20and%2015-second%20ads%20from%20real%20advertising%20campaigns.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20outperforms%20state-of-the-art%20methods%20across%20various%20metrics%2C%20including%20Average%20Precision%2C%20Area%20Under%20Curve%2C%20Spearman%2C%20and%20Kendall.%20The%20dataset%20and%20code%20are%20available%20at%20https%3A//github.com/ostadabbas/AdSum204.&entry.1838667208=http%3A//arxiv.org/abs/2510.26569v2&entry.124074799=Read"},
{"title": "SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering", "author": "Liang Peng and Yixuan Ye and Cheng Liu and Hangjun Che and Fei Wang and Zhiwen Yu and Si Wu and Hau-San Wong", "abstract": "Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.", "link": "http://arxiv.org/abs/2512.15396v1", "date": "2025-12-17", "relevancy": 2.0704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Semantic%20Matching%20Contrastive%20Learning%20for%20Partially%20View-Aligned%20Clustering&body=Title%3A%20SMART%3A%20Semantic%20Matching%20Contrastive%20Learning%20for%20Partially%20View-Aligned%20Clustering%0AAuthor%3A%20Liang%20Peng%20and%20Yixuan%20Ye%20and%20Cheng%20Liu%20and%20Hangjun%20Che%20and%20Fei%20Wang%20and%20Zhiwen%20Yu%20and%20Si%20Wu%20and%20Hau-San%20Wong%0AAbstract%3A%20Multi-view%20clustering%20has%20been%20empirically%20shown%20to%20improve%20learning%20performance%20by%20leveraging%20the%20inherent%20complementary%20information%20across%20multiple%20views%20of%20data.%20However%2C%20in%20real-world%20scenarios%2C%20collecting%20strictly%20aligned%20views%20is%20challenging%2C%20and%20learning%20from%20both%20aligned%20and%20unaligned%20data%20becomes%20a%20more%20practical%20solution.%20Partially%20View-aligned%20Clustering%20aims%20to%20learn%20correspondences%20between%20misaligned%20view%20samples%20to%20better%20exploit%20the%20potential%20consistency%20and%20complementarity%20across%20views%2C%20including%20both%20aligned%20and%20unaligned%20data.%20However%2C%20most%20existing%20PVC%20methods%20fail%20to%20leverage%20unaligned%20data%20to%20capture%20the%20shared%20semantics%20among%20samples%20from%20the%20same%20cluster.%20Moreover%2C%20the%20inherent%20heterogeneity%20of%20multi-view%20data%20induces%20distributional%20shifts%20in%20representations%2C%20leading%20to%20inaccuracies%20in%20establishing%20meaningful%20correspondences%20between%20cross-view%20latent%20features%20and%2C%20consequently%2C%20impairing%20learning%20effectiveness.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Semantic%20MAtching%20contRasTive%20learning%20model%20%28SMART%29%20for%20PVC.%20The%20main%20idea%20of%20our%20approach%20is%20to%20alleviate%20the%20influence%20of%20cross-view%20distributional%20shifts%2C%20thereby%20facilitating%20semantic%20matching%20contrastive%20learning%20to%20fully%20exploit%20semantic%20relationships%20in%20both%20aligned%20and%20unaligned%20data.%20Extensive%20experiments%20on%20eight%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20approaches%20on%20the%20PVC%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Semantic%2520Matching%2520Contrastive%2520Learning%2520for%2520Partially%2520View-Aligned%2520Clustering%26entry.906535625%3DLiang%2520Peng%2520and%2520Yixuan%2520Ye%2520and%2520Cheng%2520Liu%2520and%2520Hangjun%2520Che%2520and%2520Fei%2520Wang%2520and%2520Zhiwen%2520Yu%2520and%2520Si%2520Wu%2520and%2520Hau-San%2520Wong%26entry.1292438233%3DMulti-view%2520clustering%2520has%2520been%2520empirically%2520shown%2520to%2520improve%2520learning%2520performance%2520by%2520leveraging%2520the%2520inherent%2520complementary%2520information%2520across%2520multiple%2520views%2520of%2520data.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520collecting%2520strictly%2520aligned%2520views%2520is%2520challenging%252C%2520and%2520learning%2520from%2520both%2520aligned%2520and%2520unaligned%2520data%2520becomes%2520a%2520more%2520practical%2520solution.%2520Partially%2520View-aligned%2520Clustering%2520aims%2520to%2520learn%2520correspondences%2520between%2520misaligned%2520view%2520samples%2520to%2520better%2520exploit%2520the%2520potential%2520consistency%2520and%2520complementarity%2520across%2520views%252C%2520including%2520both%2520aligned%2520and%2520unaligned%2520data.%2520However%252C%2520most%2520existing%2520PVC%2520methods%2520fail%2520to%2520leverage%2520unaligned%2520data%2520to%2520capture%2520the%2520shared%2520semantics%2520among%2520samples%2520from%2520the%2520same%2520cluster.%2520Moreover%252C%2520the%2520inherent%2520heterogeneity%2520of%2520multi-view%2520data%2520induces%2520distributional%2520shifts%2520in%2520representations%252C%2520leading%2520to%2520inaccuracies%2520in%2520establishing%2520meaningful%2520correspondences%2520between%2520cross-view%2520latent%2520features%2520and%252C%2520consequently%252C%2520impairing%2520learning%2520effectiveness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Semantic%2520MAtching%2520contRasTive%2520learning%2520model%2520%2528SMART%2529%2520for%2520PVC.%2520The%2520main%2520idea%2520of%2520our%2520approach%2520is%2520to%2520alleviate%2520the%2520influence%2520of%2520cross-view%2520distributional%2520shifts%252C%2520thereby%2520facilitating%2520semantic%2520matching%2520contrastive%2520learning%2520to%2520fully%2520exploit%2520semantic%2520relationships%2520in%2520both%2520aligned%2520and%2520unaligned%2520data.%2520Extensive%2520experiments%2520on%2520eight%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520existing%2520approaches%2520on%2520the%2520PVC%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Semantic%20Matching%20Contrastive%20Learning%20for%20Partially%20View-Aligned%20Clustering&entry.906535625=Liang%20Peng%20and%20Yixuan%20Ye%20and%20Cheng%20Liu%20and%20Hangjun%20Che%20and%20Fei%20Wang%20and%20Zhiwen%20Yu%20and%20Si%20Wu%20and%20Hau-San%20Wong&entry.1292438233=Multi-view%20clustering%20has%20been%20empirically%20shown%20to%20improve%20learning%20performance%20by%20leveraging%20the%20inherent%20complementary%20information%20across%20multiple%20views%20of%20data.%20However%2C%20in%20real-world%20scenarios%2C%20collecting%20strictly%20aligned%20views%20is%20challenging%2C%20and%20learning%20from%20both%20aligned%20and%20unaligned%20data%20becomes%20a%20more%20practical%20solution.%20Partially%20View-aligned%20Clustering%20aims%20to%20learn%20correspondences%20between%20misaligned%20view%20samples%20to%20better%20exploit%20the%20potential%20consistency%20and%20complementarity%20across%20views%2C%20including%20both%20aligned%20and%20unaligned%20data.%20However%2C%20most%20existing%20PVC%20methods%20fail%20to%20leverage%20unaligned%20data%20to%20capture%20the%20shared%20semantics%20among%20samples%20from%20the%20same%20cluster.%20Moreover%2C%20the%20inherent%20heterogeneity%20of%20multi-view%20data%20induces%20distributional%20shifts%20in%20representations%2C%20leading%20to%20inaccuracies%20in%20establishing%20meaningful%20correspondences%20between%20cross-view%20latent%20features%20and%2C%20consequently%2C%20impairing%20learning%20effectiveness.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Semantic%20MAtching%20contRasTive%20learning%20model%20%28SMART%29%20for%20PVC.%20The%20main%20idea%20of%20our%20approach%20is%20to%20alleviate%20the%20influence%20of%20cross-view%20distributional%20shifts%2C%20thereby%20facilitating%20semantic%20matching%20contrastive%20learning%20to%20fully%20exploit%20semantic%20relationships%20in%20both%20aligned%20and%20unaligned%20data.%20Extensive%20experiments%20on%20eight%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20approaches%20on%20the%20PVC%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2512.15396v1&entry.124074799=Read"},
{"title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning", "author": "Zhenwen Liang and Sidi Lu and Wenhao Yu and Kishan Panaganti and Yujun Zhou and Haitao Mi and Dong Yu", "abstract": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.", "link": "http://arxiv.org/abs/2512.15687v1", "date": "2025-12-17", "relevancy": 2.0585, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5172}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Guide%20Their%20Own%20Exploration%3F%20Gradient-Guided%20Reinforcement%20Learning%20for%20LLM%20Reasoning&body=Title%3A%20Can%20LLMs%20Guide%20Their%20Own%20Exploration%3F%20Gradient-Guided%20Reinforcement%20Learning%20for%20LLM%20Reasoning%0AAuthor%3A%20Zhenwen%20Liang%20and%20Sidi%20Lu%20and%20Wenhao%20Yu%20and%20Kishan%20Panaganti%20and%20Yujun%20Zhou%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20Reinforcement%20learning%20has%20become%20essential%20for%20strengthening%20the%20reasoning%20abilities%20of%20large%20language%20models%2C%20yet%20current%20exploration%20mechanisms%20remain%20fundamentally%20misaligned%20with%20how%20these%20models%20actually%20learn.%20Entropy%20bonuses%20and%20external%20semantic%20comparators%20encourage%20surface%20level%20variation%20but%20offer%20no%20guarantee%20that%20sampled%20trajectories%20differ%20in%20the%20update%20directions%20that%20shape%20optimization.%20We%20propose%20G2RL%2C%20a%20gradient%20guided%20reinforcement%20learning%20framework%20in%20which%20exploration%20is%20driven%20not%20by%20external%20heuristics%20but%20by%20the%20model%20own%20first%20order%20update%20geometry.%20For%20each%20response%2C%20G2RL%20constructs%20a%20sequence%20level%20feature%20from%20the%20model%20final%20layer%20sensitivity%2C%20obtainable%20at%20negligible%20cost%20from%20a%20standard%20forward%20pass%2C%20and%20measures%20how%20each%20trajectory%20would%20reshape%20the%20policy%20by%20comparing%20these%20features%20within%20a%20sampled%20group.%20Trajectories%20that%20introduce%20novel%20gradient%20directions%20receive%20a%20bounded%20multiplicative%20reward%20scaler%2C%20while%20redundant%20or%20off%20manifold%20updates%20are%20deemphasized%2C%20yielding%20a%20self%20referential%20exploration%20signal%20that%20is%20naturally%20aligned%20with%20PPO%20style%20stability%20and%20KL%20control.%20Across%20math%20and%20general%20reasoning%20benchmarks%20%28MATH500%2C%20AMC%2C%20AIME24%2C%20AIME25%2C%20GPQA%2C%20MMLUpro%29%20on%20Qwen3%20base%201.7B%20and%204B%20models%2C%20G2RL%20consistently%20improves%20pass%401%2C%20maj%4016%2C%20and%20pass%40k%20over%20entropy%20based%20GRPO%20and%20external%20embedding%20methods.%20Analyzing%20the%20induced%20geometry%2C%20we%20find%20that%20G2RL%20expands%20exploration%20into%20substantially%20more%20orthogonal%20and%20often%20opposing%20gradient%20directions%20while%20maintaining%20semantic%20coherence%2C%20revealing%20that%20a%20policy%20own%20update%20space%20provides%20a%20far%20more%20faithful%20and%20effective%20basis%20for%20guiding%20exploration%20in%20large%20language%20model%20reinforcement%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Guide%2520Their%2520Own%2520Exploration%253F%2520Gradient-Guided%2520Reinforcement%2520Learning%2520for%2520LLM%2520Reasoning%26entry.906535625%3DZhenwen%2520Liang%2520and%2520Sidi%2520Lu%2520and%2520Wenhao%2520Yu%2520and%2520Kishan%2520Panaganti%2520and%2520Yujun%2520Zhou%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3DReinforcement%2520learning%2520has%2520become%2520essential%2520for%2520strengthening%2520the%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%252C%2520yet%2520current%2520exploration%2520mechanisms%2520remain%2520fundamentally%2520misaligned%2520with%2520how%2520these%2520models%2520actually%2520learn.%2520Entropy%2520bonuses%2520and%2520external%2520semantic%2520comparators%2520encourage%2520surface%2520level%2520variation%2520but%2520offer%2520no%2520guarantee%2520that%2520sampled%2520trajectories%2520differ%2520in%2520the%2520update%2520directions%2520that%2520shape%2520optimization.%2520We%2520propose%2520G2RL%252C%2520a%2520gradient%2520guided%2520reinforcement%2520learning%2520framework%2520in%2520which%2520exploration%2520is%2520driven%2520not%2520by%2520external%2520heuristics%2520but%2520by%2520the%2520model%2520own%2520first%2520order%2520update%2520geometry.%2520For%2520each%2520response%252C%2520G2RL%2520constructs%2520a%2520sequence%2520level%2520feature%2520from%2520the%2520model%2520final%2520layer%2520sensitivity%252C%2520obtainable%2520at%2520negligible%2520cost%2520from%2520a%2520standard%2520forward%2520pass%252C%2520and%2520measures%2520how%2520each%2520trajectory%2520would%2520reshape%2520the%2520policy%2520by%2520comparing%2520these%2520features%2520within%2520a%2520sampled%2520group.%2520Trajectories%2520that%2520introduce%2520novel%2520gradient%2520directions%2520receive%2520a%2520bounded%2520multiplicative%2520reward%2520scaler%252C%2520while%2520redundant%2520or%2520off%2520manifold%2520updates%2520are%2520deemphasized%252C%2520yielding%2520a%2520self%2520referential%2520exploration%2520signal%2520that%2520is%2520naturally%2520aligned%2520with%2520PPO%2520style%2520stability%2520and%2520KL%2520control.%2520Across%2520math%2520and%2520general%2520reasoning%2520benchmarks%2520%2528MATH500%252C%2520AMC%252C%2520AIME24%252C%2520AIME25%252C%2520GPQA%252C%2520MMLUpro%2529%2520on%2520Qwen3%2520base%25201.7B%2520and%25204B%2520models%252C%2520G2RL%2520consistently%2520improves%2520pass%25401%252C%2520maj%254016%252C%2520and%2520pass%2540k%2520over%2520entropy%2520based%2520GRPO%2520and%2520external%2520embedding%2520methods.%2520Analyzing%2520the%2520induced%2520geometry%252C%2520we%2520find%2520that%2520G2RL%2520expands%2520exploration%2520into%2520substantially%2520more%2520orthogonal%2520and%2520often%2520opposing%2520gradient%2520directions%2520while%2520maintaining%2520semantic%2520coherence%252C%2520revealing%2520that%2520a%2520policy%2520own%2520update%2520space%2520provides%2520a%2520far%2520more%2520faithful%2520and%2520effective%2520basis%2520for%2520guiding%2520exploration%2520in%2520large%2520language%2520model%2520reinforcement%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Guide%20Their%20Own%20Exploration%3F%20Gradient-Guided%20Reinforcement%20Learning%20for%20LLM%20Reasoning&entry.906535625=Zhenwen%20Liang%20and%20Sidi%20Lu%20and%20Wenhao%20Yu%20and%20Kishan%20Panaganti%20and%20Yujun%20Zhou%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=Reinforcement%20learning%20has%20become%20essential%20for%20strengthening%20the%20reasoning%20abilities%20of%20large%20language%20models%2C%20yet%20current%20exploration%20mechanisms%20remain%20fundamentally%20misaligned%20with%20how%20these%20models%20actually%20learn.%20Entropy%20bonuses%20and%20external%20semantic%20comparators%20encourage%20surface%20level%20variation%20but%20offer%20no%20guarantee%20that%20sampled%20trajectories%20differ%20in%20the%20update%20directions%20that%20shape%20optimization.%20We%20propose%20G2RL%2C%20a%20gradient%20guided%20reinforcement%20learning%20framework%20in%20which%20exploration%20is%20driven%20not%20by%20external%20heuristics%20but%20by%20the%20model%20own%20first%20order%20update%20geometry.%20For%20each%20response%2C%20G2RL%20constructs%20a%20sequence%20level%20feature%20from%20the%20model%20final%20layer%20sensitivity%2C%20obtainable%20at%20negligible%20cost%20from%20a%20standard%20forward%20pass%2C%20and%20measures%20how%20each%20trajectory%20would%20reshape%20the%20policy%20by%20comparing%20these%20features%20within%20a%20sampled%20group.%20Trajectories%20that%20introduce%20novel%20gradient%20directions%20receive%20a%20bounded%20multiplicative%20reward%20scaler%2C%20while%20redundant%20or%20off%20manifold%20updates%20are%20deemphasized%2C%20yielding%20a%20self%20referential%20exploration%20signal%20that%20is%20naturally%20aligned%20with%20PPO%20style%20stability%20and%20KL%20control.%20Across%20math%20and%20general%20reasoning%20benchmarks%20%28MATH500%2C%20AMC%2C%20AIME24%2C%20AIME25%2C%20GPQA%2C%20MMLUpro%29%20on%20Qwen3%20base%201.7B%20and%204B%20models%2C%20G2RL%20consistently%20improves%20pass%401%2C%20maj%4016%2C%20and%20pass%40k%20over%20entropy%20based%20GRPO%20and%20external%20embedding%20methods.%20Analyzing%20the%20induced%20geometry%2C%20we%20find%20that%20G2RL%20expands%20exploration%20into%20substantially%20more%20orthogonal%20and%20often%20opposing%20gradient%20directions%20while%20maintaining%20semantic%20coherence%2C%20revealing%20that%20a%20policy%20own%20update%20space%20provides%20a%20far%20more%20faithful%20and%20effective%20basis%20for%20guiding%20exploration%20in%20large%20language%20model%20reinforcement%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.15687v1&entry.124074799=Read"},
{"title": "Vision-based module for accurately reading linear scales in a laboratory", "author": "Parvesh Saini and Soumyadipta Maiti and Beena Rai", "abstract": "Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.", "link": "http://arxiv.org/abs/2512.15327v1", "date": "2025-12-17", "relevancy": 2.0584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5357}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-based%20module%20for%20accurately%20reading%20linear%20scales%20in%20a%20laboratory&body=Title%3A%20Vision-based%20module%20for%20accurately%20reading%20linear%20scales%20in%20a%20laboratory%0AAuthor%3A%20Parvesh%20Saini%20and%20Soumyadipta%20Maiti%20and%20Beena%20Rai%0AAbstract%3A%20Capabilities%20and%20the%20number%20of%20vision-based%20models%20are%20increasing%20rapidly.%20And%20these%20vision%20models%20are%20now%20able%20to%20do%20more%20tasks%20like%20object%20detection%2C%20image%20classification%2C%20instance%20segmentation%20etc.%20with%20great%20accuracy.%20But%20models%20which%20can%20take%20accurate%20quantitative%20measurements%20form%20an%20image%2C%20as%20a%20human%20can%20do%20by%20just%20looking%20at%20it%2C%20are%20rare.%20For%20a%20robot%20to%20work%20with%20complete%20autonomy%20in%20a%20Laboratory%20environment%2C%20it%20needs%20to%20have%20some%20basic%20skills%20like%20navigation%2C%20handling%20objects%2C%20preparing%20samples%20etc.%20to%20match%20human-like%20capabilities%20in%20an%20unstructured%20environment.%20Another%20important%20capability%20is%20to%20read%20measurements%20from%20instruments%20and%20apparatus.%20Here%2C%20we%20tried%20to%20mimic%20a%20human%20inspired%20approach%20to%20read%20measurements%20from%20a%20linear%20scale.%20As%20a%20test%20case%20we%20have%20picked%20reading%20level%20from%20a%20syringe%20and%20a%20measuring%20cylinder.%20For%20a%20randomly%20oriented%20syringe%20we%20carry%20out%20transformations%20to%20correct%20the%20orientation.%20To%20make%20the%20system%20efficient%20and%20robust%2C%20the%20area%20of%20interest%20is%20reduced%20to%20just%20the%20linear%20scale%20containing%20part%20of%20the%20image.%20After%20that%2C%20a%20series%20of%20features%20were%20extracted%20like%20the%20major%20makers%2C%20the%20corresponding%20digits%2C%20and%20the%20level%20indicator%20location%2C%20from%20which%20the%20final%20reading%20was%20calculated.%20Readings%20obtained%20using%20this%20system%20were%20also%20compared%20against%20human%20read%20values%20of%20the%20same%20instances%20and%20an%20accurate%20correspondence%20was%20observed.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-based%2520module%2520for%2520accurately%2520reading%2520linear%2520scales%2520in%2520a%2520laboratory%26entry.906535625%3DParvesh%2520Saini%2520and%2520Soumyadipta%2520Maiti%2520and%2520Beena%2520Rai%26entry.1292438233%3DCapabilities%2520and%2520the%2520number%2520of%2520vision-based%2520models%2520are%2520increasing%2520rapidly.%2520And%2520these%2520vision%2520models%2520are%2520now%2520able%2520to%2520do%2520more%2520tasks%2520like%2520object%2520detection%252C%2520image%2520classification%252C%2520instance%2520segmentation%2520etc.%2520with%2520great%2520accuracy.%2520But%2520models%2520which%2520can%2520take%2520accurate%2520quantitative%2520measurements%2520form%2520an%2520image%252C%2520as%2520a%2520human%2520can%2520do%2520by%2520just%2520looking%2520at%2520it%252C%2520are%2520rare.%2520For%2520a%2520robot%2520to%2520work%2520with%2520complete%2520autonomy%2520in%2520a%2520Laboratory%2520environment%252C%2520it%2520needs%2520to%2520have%2520some%2520basic%2520skills%2520like%2520navigation%252C%2520handling%2520objects%252C%2520preparing%2520samples%2520etc.%2520to%2520match%2520human-like%2520capabilities%2520in%2520an%2520unstructured%2520environment.%2520Another%2520important%2520capability%2520is%2520to%2520read%2520measurements%2520from%2520instruments%2520and%2520apparatus.%2520Here%252C%2520we%2520tried%2520to%2520mimic%2520a%2520human%2520inspired%2520approach%2520to%2520read%2520measurements%2520from%2520a%2520linear%2520scale.%2520As%2520a%2520test%2520case%2520we%2520have%2520picked%2520reading%2520level%2520from%2520a%2520syringe%2520and%2520a%2520measuring%2520cylinder.%2520For%2520a%2520randomly%2520oriented%2520syringe%2520we%2520carry%2520out%2520transformations%2520to%2520correct%2520the%2520orientation.%2520To%2520make%2520the%2520system%2520efficient%2520and%2520robust%252C%2520the%2520area%2520of%2520interest%2520is%2520reduced%2520to%2520just%2520the%2520linear%2520scale%2520containing%2520part%2520of%2520the%2520image.%2520After%2520that%252C%2520a%2520series%2520of%2520features%2520were%2520extracted%2520like%2520the%2520major%2520makers%252C%2520the%2520corresponding%2520digits%252C%2520and%2520the%2520level%2520indicator%2520location%252C%2520from%2520which%2520the%2520final%2520reading%2520was%2520calculated.%2520Readings%2520obtained%2520using%2520this%2520system%2520were%2520also%2520compared%2520against%2520human%2520read%2520values%2520of%2520the%2520same%2520instances%2520and%2520an%2520accurate%2520correspondence%2520was%2520observed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20module%20for%20accurately%20reading%20linear%20scales%20in%20a%20laboratory&entry.906535625=Parvesh%20Saini%20and%20Soumyadipta%20Maiti%20and%20Beena%20Rai&entry.1292438233=Capabilities%20and%20the%20number%20of%20vision-based%20models%20are%20increasing%20rapidly.%20And%20these%20vision%20models%20are%20now%20able%20to%20do%20more%20tasks%20like%20object%20detection%2C%20image%20classification%2C%20instance%20segmentation%20etc.%20with%20great%20accuracy.%20But%20models%20which%20can%20take%20accurate%20quantitative%20measurements%20form%20an%20image%2C%20as%20a%20human%20can%20do%20by%20just%20looking%20at%20it%2C%20are%20rare.%20For%20a%20robot%20to%20work%20with%20complete%20autonomy%20in%20a%20Laboratory%20environment%2C%20it%20needs%20to%20have%20some%20basic%20skills%20like%20navigation%2C%20handling%20objects%2C%20preparing%20samples%20etc.%20to%20match%20human-like%20capabilities%20in%20an%20unstructured%20environment.%20Another%20important%20capability%20is%20to%20read%20measurements%20from%20instruments%20and%20apparatus.%20Here%2C%20we%20tried%20to%20mimic%20a%20human%20inspired%20approach%20to%20read%20measurements%20from%20a%20linear%20scale.%20As%20a%20test%20case%20we%20have%20picked%20reading%20level%20from%20a%20syringe%20and%20a%20measuring%20cylinder.%20For%20a%20randomly%20oriented%20syringe%20we%20carry%20out%20transformations%20to%20correct%20the%20orientation.%20To%20make%20the%20system%20efficient%20and%20robust%2C%20the%20area%20of%20interest%20is%20reduced%20to%20just%20the%20linear%20scale%20containing%20part%20of%20the%20image.%20After%20that%2C%20a%20series%20of%20features%20were%20extracted%20like%20the%20major%20makers%2C%20the%20corresponding%20digits%2C%20and%20the%20level%20indicator%20location%2C%20from%20which%20the%20final%20reading%20was%20calculated.%20Readings%20obtained%20using%20this%20system%20were%20also%20compared%20against%20human%20read%20values%20of%20the%20same%20instances%20and%20an%20accurate%20correspondence%20was%20observed.&entry.1838667208=http%3A//arxiv.org/abs/2512.15327v1&entry.124074799=Read"},
{"title": "Interpretable representation learning of quantum data enabled by probabilistic variational autoencoders", "author": "Paulin de Schoulepnikoff and Gorka Mu\u00f1oz-Gil and Hendrik Poulsen Nautrup and Hans J. Briegel", "abstract": "Interpretable machine learning is rapidly becoming a crucial tool for scientific discovery. Among existing approaches, variational autoencoders (VAEs) have shown promise in extracting the hidden physical features of some input data, with no supervision nor prior knowledge of the system at study. Yet, the ability of VAEs to create meaningful, interpretable representations relies on their accurate approximation of the underlying probability distribution of their input. When dealing with quantum data, VAEs must hence account for its intrinsic randomness and complex correlations. While VAEs have been previously applied to quantum data, they have often neglected its probabilistic nature, hindering the extraction of meaningful physical descriptors. Here, we demonstrate that two key modifications enable VAEs to learn physically meaningful latent representations: a decoder capable of faithfully reproduce quantum states and a probabilistic loss tailored to this task. Using benchmark quantum spin models, we identify regimes where standard methods fail while the representations learned by our approach remain meaningful and interpretable. Applied to experimental data from Rydberg atom arrays, the model autonomously uncovers the phase structure without access to prior labels, Hamiltonian details, or knowledge of relevant order parameters, highlighting its potential as an unsupervised and interpretable tool for the study of quantum systems.", "link": "http://arxiv.org/abs/2506.11982v3", "date": "2025-12-17", "relevancy": 2.0544, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%20probabilistic%20variational%20autoencoders&body=Title%3A%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%20probabilistic%20variational%20autoencoders%0AAuthor%3A%20Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel%0AAbstract%3A%20Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%20scientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%20%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%20input%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%20Yet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%20relies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%20distribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%20account%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%20been%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%20probabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%20descriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%20learn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%20faithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%20task.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%20methods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%20meaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%20arrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%20prior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%20highlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%20study%20of%20quantum%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520representation%2520learning%2520of%2520quantum%2520data%2520enabled%2520by%2520probabilistic%2520variational%2520autoencoders%26entry.906535625%3DPaulin%2520de%2520Schoulepnikoff%2520and%2520Gorka%2520Mu%25C3%25B1oz-Gil%2520and%2520Hendrik%2520Poulsen%2520Nautrup%2520and%2520Hans%2520J.%2520Briegel%26entry.1292438233%3DInterpretable%2520machine%2520learning%2520is%2520rapidly%2520becoming%2520a%2520crucial%2520tool%2520for%2520scientific%2520discovery.%2520Among%2520existing%2520approaches%252C%2520variational%2520autoencoders%2520%2528VAEs%2529%2520have%2520shown%2520promise%2520in%2520extracting%2520the%2520hidden%2520physical%2520features%2520of%2520some%2520input%2520data%252C%2520with%2520no%2520supervision%2520nor%2520prior%2520knowledge%2520of%2520the%2520system%2520at%2520study.%2520Yet%252C%2520the%2520ability%2520of%2520VAEs%2520to%2520create%2520meaningful%252C%2520interpretable%2520representations%2520relies%2520on%2520their%2520accurate%2520approximation%2520of%2520the%2520underlying%2520probability%2520distribution%2520of%2520their%2520input.%2520When%2520dealing%2520with%2520quantum%2520data%252C%2520VAEs%2520must%2520hence%2520account%2520for%2520its%2520intrinsic%2520randomness%2520and%2520complex%2520correlations.%2520While%2520VAEs%2520have%2520been%2520previously%2520applied%2520to%2520quantum%2520data%252C%2520they%2520have%2520often%2520neglected%2520its%2520probabilistic%2520nature%252C%2520hindering%2520the%2520extraction%2520of%2520meaningful%2520physical%2520descriptors.%2520Here%252C%2520we%2520demonstrate%2520that%2520two%2520key%2520modifications%2520enable%2520VAEs%2520to%2520learn%2520physically%2520meaningful%2520latent%2520representations%253A%2520a%2520decoder%2520capable%2520of%2520faithfully%2520reproduce%2520quantum%2520states%2520and%2520a%2520probabilistic%2520loss%2520tailored%2520to%2520this%2520task.%2520Using%2520benchmark%2520quantum%2520spin%2520models%252C%2520we%2520identify%2520regimes%2520where%2520standard%2520methods%2520fail%2520while%2520the%2520representations%2520learned%2520by%2520our%2520approach%2520remain%2520meaningful%2520and%2520interpretable.%2520Applied%2520to%2520experimental%2520data%2520from%2520Rydberg%2520atom%2520arrays%252C%2520the%2520model%2520autonomously%2520uncovers%2520the%2520phase%2520structure%2520without%2520access%2520to%2520prior%2520labels%252C%2520Hamiltonian%2520details%252C%2520or%2520knowledge%2520of%2520relevant%2520order%2520parameters%252C%2520highlighting%2520its%2520potential%2520as%2520an%2520unsupervised%2520and%2520interpretable%2520tool%2520for%2520the%2520study%2520of%2520quantum%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%20probabilistic%20variational%20autoencoders&entry.906535625=Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel&entry.1292438233=Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%20scientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%20%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%20input%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%20Yet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%20relies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%20distribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%20account%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%20been%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%20probabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%20descriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%20learn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%20faithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%20task.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%20methods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%20meaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%20arrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%20prior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%20highlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%20study%20of%20quantum%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2506.11982v3&entry.124074799=Read"},
{"title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness", "author": "Zehua Pei and Hui-Ling Zhen and Shixiong Kai and Sinno Jialin Pan and Yunhe Wang and Mingxuan Yuan and Bei Yu", "abstract": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.", "link": "http://arxiv.org/abs/2512.15374v1", "date": "2025-12-17", "relevancy": 2.0512, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5715}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCOPE%3A%20Prompt%20Evolution%20for%20Enhancing%20Agent%20Effectiveness&body=Title%3A%20SCOPE%3A%20Prompt%20Evolution%20for%20Enhancing%20Agent%20Effectiveness%0AAuthor%3A%20Zehua%20Pei%20and%20Hui-Ling%20Zhen%20and%20Shixiong%20Kai%20and%20Sinno%20Jialin%20Pan%20and%20Yunhe%20Wang%20and%20Mingxuan%20Yuan%20and%20Bei%20Yu%0AAbstract%3A%20Large%20Language%20Model%20%28LLM%29%20agents%20are%20increasingly%20deployed%20in%20environments%20that%20generate%20massive%2C%20dynamic%20contexts.%20However%2C%20a%20critical%20bottleneck%20remains%3A%20while%20agents%20have%20access%20to%20this%20context%2C%20their%20static%20prompts%20lack%20the%20mechanisms%20to%20manage%20it%20effectively%2C%20leading%20to%20recurring%20Corrective%20and%20Enhancement%20failures.%20To%20address%20this%20capability%20gap%2C%20we%20introduce%20%5Ctextbf%7BSCOPE%7D%20%28Self-evolving%20Context%20Optimization%20via%20Prompt%20Evolution%29.%20SCOPE%20frames%20context%20management%20as%20an%20%5Ctextit%7Bonline%20optimization%7D%20problem%2C%20synthesizing%20guidelines%20from%20execution%20traces%20to%20automatically%20evolve%20the%20agent%27s%20prompt.%20We%20propose%20a%20Dual-Stream%20mechanism%20that%20balances%20tactical%20specificity%20%28resolving%20immediate%20errors%29%20with%20strategic%20generality%20%28evolving%20long-term%20principles%29.%20Furthermore%2C%20we%20introduce%20Perspective-Driven%20Exploration%20to%20maximize%20strategy%20coverage%2C%20increasing%20the%20likelihood%20that%20the%20agent%20has%20the%20correct%20strategy%20for%20any%20given%20task.%20Experiments%20on%20the%20HLE%20benchmark%20show%20that%20SCOPE%20improves%20task%20success%20rates%20from%2014.23%5C%25%20to%2038.64%5C%25%20without%20human%20intervention.%20We%20make%20our%20code%20publicly%20available%20at%20https%3A//github.com/JarvisPei/SCOPE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCOPE%253A%2520Prompt%2520Evolution%2520for%2520Enhancing%2520Agent%2520Effectiveness%26entry.906535625%3DZehua%2520Pei%2520and%2520Hui-Ling%2520Zhen%2520and%2520Shixiong%2520Kai%2520and%2520Sinno%2520Jialin%2520Pan%2520and%2520Yunhe%2520Wang%2520and%2520Mingxuan%2520Yuan%2520and%2520Bei%2520Yu%26entry.1292438233%3DLarge%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520are%2520increasingly%2520deployed%2520in%2520environments%2520that%2520generate%2520massive%252C%2520dynamic%2520contexts.%2520However%252C%2520a%2520critical%2520bottleneck%2520remains%253A%2520while%2520agents%2520have%2520access%2520to%2520this%2520context%252C%2520their%2520static%2520prompts%2520lack%2520the%2520mechanisms%2520to%2520manage%2520it%2520effectively%252C%2520leading%2520to%2520recurring%2520Corrective%2520and%2520Enhancement%2520failures.%2520To%2520address%2520this%2520capability%2520gap%252C%2520we%2520introduce%2520%255Ctextbf%257BSCOPE%257D%2520%2528Self-evolving%2520Context%2520Optimization%2520via%2520Prompt%2520Evolution%2529.%2520SCOPE%2520frames%2520context%2520management%2520as%2520an%2520%255Ctextit%257Bonline%2520optimization%257D%2520problem%252C%2520synthesizing%2520guidelines%2520from%2520execution%2520traces%2520to%2520automatically%2520evolve%2520the%2520agent%2527s%2520prompt.%2520We%2520propose%2520a%2520Dual-Stream%2520mechanism%2520that%2520balances%2520tactical%2520specificity%2520%2528resolving%2520immediate%2520errors%2529%2520with%2520strategic%2520generality%2520%2528evolving%2520long-term%2520principles%2529.%2520Furthermore%252C%2520we%2520introduce%2520Perspective-Driven%2520Exploration%2520to%2520maximize%2520strategy%2520coverage%252C%2520increasing%2520the%2520likelihood%2520that%2520the%2520agent%2520has%2520the%2520correct%2520strategy%2520for%2520any%2520given%2520task.%2520Experiments%2520on%2520the%2520HLE%2520benchmark%2520show%2520that%2520SCOPE%2520improves%2520task%2520success%2520rates%2520from%252014.23%255C%2525%2520to%252038.64%255C%2525%2520without%2520human%2520intervention.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520at%2520https%253A//github.com/JarvisPei/SCOPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCOPE%3A%20Prompt%20Evolution%20for%20Enhancing%20Agent%20Effectiveness&entry.906535625=Zehua%20Pei%20and%20Hui-Ling%20Zhen%20and%20Shixiong%20Kai%20and%20Sinno%20Jialin%20Pan%20and%20Yunhe%20Wang%20and%20Mingxuan%20Yuan%20and%20Bei%20Yu&entry.1292438233=Large%20Language%20Model%20%28LLM%29%20agents%20are%20increasingly%20deployed%20in%20environments%20that%20generate%20massive%2C%20dynamic%20contexts.%20However%2C%20a%20critical%20bottleneck%20remains%3A%20while%20agents%20have%20access%20to%20this%20context%2C%20their%20static%20prompts%20lack%20the%20mechanisms%20to%20manage%20it%20effectively%2C%20leading%20to%20recurring%20Corrective%20and%20Enhancement%20failures.%20To%20address%20this%20capability%20gap%2C%20we%20introduce%20%5Ctextbf%7BSCOPE%7D%20%28Self-evolving%20Context%20Optimization%20via%20Prompt%20Evolution%29.%20SCOPE%20frames%20context%20management%20as%20an%20%5Ctextit%7Bonline%20optimization%7D%20problem%2C%20synthesizing%20guidelines%20from%20execution%20traces%20to%20automatically%20evolve%20the%20agent%27s%20prompt.%20We%20propose%20a%20Dual-Stream%20mechanism%20that%20balances%20tactical%20specificity%20%28resolving%20immediate%20errors%29%20with%20strategic%20generality%20%28evolving%20long-term%20principles%29.%20Furthermore%2C%20we%20introduce%20Perspective-Driven%20Exploration%20to%20maximize%20strategy%20coverage%2C%20increasing%20the%20likelihood%20that%20the%20agent%20has%20the%20correct%20strategy%20for%20any%20given%20task.%20Experiments%20on%20the%20HLE%20benchmark%20show%20that%20SCOPE%20improves%20task%20success%20rates%20from%2014.23%5C%25%20to%2038.64%5C%25%20without%20human%20intervention.%20We%20make%20our%20code%20publicly%20available%20at%20https%3A//github.com/JarvisPei/SCOPE.&entry.1838667208=http%3A//arxiv.org/abs/2512.15374v1&entry.124074799=Read"},
{"title": "A Conditioned UNet for Music Source Separation", "author": "Ken O'Hanlon and Basil Woods and Lin Wang and Mark Sandler", "abstract": "In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.", "link": "http://arxiv.org/abs/2512.15532v1", "date": "2025-12-17", "relevancy": 1.6627, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4175}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4165}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conditioned%20UNet%20for%20Music%20Source%20Separation&body=Title%3A%20A%20Conditioned%20UNet%20for%20Music%20Source%20Separation%0AAuthor%3A%20Ken%20O%27Hanlon%20and%20Basil%20Woods%20and%20Lin%20Wang%20and%20Mark%20Sandler%0AAbstract%3A%20In%20this%20paper%20we%20propose%20a%20conditioned%20UNet%20for%20Music%20Source%20Separation%20%28MSS%29.%20MSS%20is%20generally%20performed%20by%20multi-output%20neural%20networks%2C%20typically%20UNets%2C%20with%20each%20output%20representing%20a%20particular%20stem%20from%20a%20predefined%20instrument%20vocabulary.%20In%20contrast%2C%20conditioned%20MSS%20networks%20accept%20an%20audio%20query%20related%20to%20a%20stem%20of%20interest%20alongside%20the%20signal%20from%20which%20that%20stem%20is%20to%20be%20extracted.%20Thus%2C%20a%20strict%20vocabulary%20is%20not%20required%20and%20this%20enables%20more%20realistic%20tasks%20in%20MSS.%20The%20potential%20of%20conditioned%20approaches%20for%20such%20tasks%20has%20been%20somewhat%20hidden%20due%20to%20a%20lack%20of%20suitable%20data%2C%20an%20issue%20recently%20addressed%20with%20the%20MoisesDb%20dataset.%20A%20recent%20method%2C%20Banquet%2C%20employs%20this%20dataset%20with%20promising%20results%20seen%20on%20larger%20vocabularies.%20Banquet%20uses%20Bandsplit%20RNN%20rather%20than%20a%20UNet%20and%20the%20authors%20state%20that%20UNets%20should%20not%20be%20suitable%20for%20conditioned%20MSS.%20We%20counter%20this%20argument%20and%20propose%20QSCNet%2C%20a%20novel%20conditioned%20UNet%20for%20MSS%20that%20integrates%20network%20conditioning%20elements%20in%20the%20Sparse%20Compressed%20Network%20for%20MSS.%20We%20find%20QSCNet%20to%20outperform%20Banquet%20by%20over%201dB%20SNR%20on%20a%20couple%20of%20MSS%20tasks%2C%20while%20using%20less%20than%20half%20the%20number%20of%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conditioned%2520UNet%2520for%2520Music%2520Source%2520Separation%26entry.906535625%3DKen%2520O%2527Hanlon%2520and%2520Basil%2520Woods%2520and%2520Lin%2520Wang%2520and%2520Mark%2520Sandler%26entry.1292438233%3DIn%2520this%2520paper%2520we%2520propose%2520a%2520conditioned%2520UNet%2520for%2520Music%2520Source%2520Separation%2520%2528MSS%2529.%2520MSS%2520is%2520generally%2520performed%2520by%2520multi-output%2520neural%2520networks%252C%2520typically%2520UNets%252C%2520with%2520each%2520output%2520representing%2520a%2520particular%2520stem%2520from%2520a%2520predefined%2520instrument%2520vocabulary.%2520In%2520contrast%252C%2520conditioned%2520MSS%2520networks%2520accept%2520an%2520audio%2520query%2520related%2520to%2520a%2520stem%2520of%2520interest%2520alongside%2520the%2520signal%2520from%2520which%2520that%2520stem%2520is%2520to%2520be%2520extracted.%2520Thus%252C%2520a%2520strict%2520vocabulary%2520is%2520not%2520required%2520and%2520this%2520enables%2520more%2520realistic%2520tasks%2520in%2520MSS.%2520The%2520potential%2520of%2520conditioned%2520approaches%2520for%2520such%2520tasks%2520has%2520been%2520somewhat%2520hidden%2520due%2520to%2520a%2520lack%2520of%2520suitable%2520data%252C%2520an%2520issue%2520recently%2520addressed%2520with%2520the%2520MoisesDb%2520dataset.%2520A%2520recent%2520method%252C%2520Banquet%252C%2520employs%2520this%2520dataset%2520with%2520promising%2520results%2520seen%2520on%2520larger%2520vocabularies.%2520Banquet%2520uses%2520Bandsplit%2520RNN%2520rather%2520than%2520a%2520UNet%2520and%2520the%2520authors%2520state%2520that%2520UNets%2520should%2520not%2520be%2520suitable%2520for%2520conditioned%2520MSS.%2520We%2520counter%2520this%2520argument%2520and%2520propose%2520QSCNet%252C%2520a%2520novel%2520conditioned%2520UNet%2520for%2520MSS%2520that%2520integrates%2520network%2520conditioning%2520elements%2520in%2520the%2520Sparse%2520Compressed%2520Network%2520for%2520MSS.%2520We%2520find%2520QSCNet%2520to%2520outperform%2520Banquet%2520by%2520over%25201dB%2520SNR%2520on%2520a%2520couple%2520of%2520MSS%2520tasks%252C%2520while%2520using%2520less%2520than%2520half%2520the%2520number%2520of%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conditioned%20UNet%20for%20Music%20Source%20Separation&entry.906535625=Ken%20O%27Hanlon%20and%20Basil%20Woods%20and%20Lin%20Wang%20and%20Mark%20Sandler&entry.1292438233=In%20this%20paper%20we%20propose%20a%20conditioned%20UNet%20for%20Music%20Source%20Separation%20%28MSS%29.%20MSS%20is%20generally%20performed%20by%20multi-output%20neural%20networks%2C%20typically%20UNets%2C%20with%20each%20output%20representing%20a%20particular%20stem%20from%20a%20predefined%20instrument%20vocabulary.%20In%20contrast%2C%20conditioned%20MSS%20networks%20accept%20an%20audio%20query%20related%20to%20a%20stem%20of%20interest%20alongside%20the%20signal%20from%20which%20that%20stem%20is%20to%20be%20extracted.%20Thus%2C%20a%20strict%20vocabulary%20is%20not%20required%20and%20this%20enables%20more%20realistic%20tasks%20in%20MSS.%20The%20potential%20of%20conditioned%20approaches%20for%20such%20tasks%20has%20been%20somewhat%20hidden%20due%20to%20a%20lack%20of%20suitable%20data%2C%20an%20issue%20recently%20addressed%20with%20the%20MoisesDb%20dataset.%20A%20recent%20method%2C%20Banquet%2C%20employs%20this%20dataset%20with%20promising%20results%20seen%20on%20larger%20vocabularies.%20Banquet%20uses%20Bandsplit%20RNN%20rather%20than%20a%20UNet%20and%20the%20authors%20state%20that%20UNets%20should%20not%20be%20suitable%20for%20conditioned%20MSS.%20We%20counter%20this%20argument%20and%20propose%20QSCNet%2C%20a%20novel%20conditioned%20UNet%20for%20MSS%20that%20integrates%20network%20conditioning%20elements%20in%20the%20Sparse%20Compressed%20Network%20for%20MSS.%20We%20find%20QSCNet%20to%20outperform%20Banquet%20by%20over%201dB%20SNR%20on%20a%20couple%20of%20MSS%20tasks%2C%20while%20using%20less%20than%20half%20the%20number%20of%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2512.15532v1&entry.124074799=Read"},
{"title": "BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics", "author": "Yunhao Zhang and Sidharth S. Menon and Lin Cheng and Aswin Gnanaskandan and Ameya D. Jagtap", "abstract": "In this work, we employ physics-informed neural operators to map pressure profiles from an input function space to the corresponding bubble radius responses. Our approach employs a two-step DeepONet architecture. To address the intrinsic spectral bias of deep learning models, our model incorporates the Rowdy adaptive activation function, enhancing the representation of high-frequency features. Moreover, we introduce the Kolmogorov-Arnold network (KAN) based two-step DeepOKAN model, which enhances interpretability (often lacking in conventional multilayer perceptron architectures) while efficiently capturing high-frequency bubble dynamics without explicit utilization of activation functions in any form. We particularly investigate the use of spline basis functions in combination with radial basis functions (RBF) within our architecture, as they demonstrate superior performance in constructing a universal basis for approximating high-frequency bubble dynamics compared to alternative formulations. Furthermore, we emphasize on the performance bottleneck of RBF while learning the high frequency bubble dynamics and showcase the advantage of using spline basis function for the trunk network in overcoming this inherent spectral bias. The model is systematically evaluated across three representative scenarios: (1) bubble dynamics governed by the Rayleigh-Plesset equation with a single initial radius, (2) bubble dynamics governed by the Keller-Miksis equation with a single initial radius, and (3) Keller-Miksis dynamics with multiple initial radii. We also compare our results with state-of-the-art neural operators, including Fourier Neural Operators, Wavelet Neural Operators, OFormer, and Convolutional Neural Operators. Our findings demonstrate that the two-step DeepOKAN accurately captures both low- and high-frequency behaviors, and offers a promising alternative to conventional numerical solvers.", "link": "http://arxiv.org/abs/2508.03965v3", "date": "2025-12-17", "relevancy": 1.8054, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BubbleOKAN%3A%20A%20Physics-Informed%20Interpretable%20Neural%20Operator%20for%20High-Frequency%20Bubble%20Dynamics&body=Title%3A%20BubbleOKAN%3A%20A%20Physics-Informed%20Interpretable%20Neural%20Operator%20for%20High-Frequency%20Bubble%20Dynamics%0AAuthor%3A%20Yunhao%20Zhang%20and%20Sidharth%20S.%20Menon%20and%20Lin%20Cheng%20and%20Aswin%20Gnanaskandan%20and%20Ameya%20D.%20Jagtap%0AAbstract%3A%20In%20this%20work%2C%20we%20employ%20physics-informed%20neural%20operators%20to%20map%20pressure%20profiles%20from%20an%20input%20function%20space%20to%20the%20corresponding%20bubble%20radius%20responses.%20Our%20approach%20employs%20a%20two-step%20DeepONet%20architecture.%20To%20address%20the%20intrinsic%20spectral%20bias%20of%20deep%20learning%20models%2C%20our%20model%20incorporates%20the%20Rowdy%20adaptive%20activation%20function%2C%20enhancing%20the%20representation%20of%20high-frequency%20features.%20Moreover%2C%20we%20introduce%20the%20Kolmogorov-Arnold%20network%20%28KAN%29%20based%20two-step%20DeepOKAN%20model%2C%20which%20enhances%20interpretability%20%28often%20lacking%20in%20conventional%20multilayer%20perceptron%20architectures%29%20while%20efficiently%20capturing%20high-frequency%20bubble%20dynamics%20without%20explicit%20utilization%20of%20activation%20functions%20in%20any%20form.%20We%20particularly%20investigate%20the%20use%20of%20spline%20basis%20functions%20in%20combination%20with%20radial%20basis%20functions%20%28RBF%29%20within%20our%20architecture%2C%20as%20they%20demonstrate%20superior%20performance%20in%20constructing%20a%20universal%20basis%20for%20approximating%20high-frequency%20bubble%20dynamics%20compared%20to%20alternative%20formulations.%20Furthermore%2C%20we%20emphasize%20on%20the%20performance%20bottleneck%20of%20RBF%20while%20learning%20the%20high%20frequency%20bubble%20dynamics%20and%20showcase%20the%20advantage%20of%20using%20spline%20basis%20function%20for%20the%20trunk%20network%20in%20overcoming%20this%20inherent%20spectral%20bias.%20The%20model%20is%20systematically%20evaluated%20across%20three%20representative%20scenarios%3A%20%281%29%20bubble%20dynamics%20governed%20by%20the%20Rayleigh-Plesset%20equation%20with%20a%20single%20initial%20radius%2C%20%282%29%20bubble%20dynamics%20governed%20by%20the%20Keller-Miksis%20equation%20with%20a%20single%20initial%20radius%2C%20and%20%283%29%20Keller-Miksis%20dynamics%20with%20multiple%20initial%20radii.%20We%20also%20compare%20our%20results%20with%20state-of-the-art%20neural%20operators%2C%20including%20Fourier%20Neural%20Operators%2C%20Wavelet%20Neural%20Operators%2C%20OFormer%2C%20and%20Convolutional%20Neural%20Operators.%20Our%20findings%20demonstrate%20that%20the%20two-step%20DeepOKAN%20accurately%20captures%20both%20low-%20and%20high-frequency%20behaviors%2C%20and%20offers%20a%20promising%20alternative%20to%20conventional%20numerical%20solvers.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03965v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBubbleOKAN%253A%2520A%2520Physics-Informed%2520Interpretable%2520Neural%2520Operator%2520for%2520High-Frequency%2520Bubble%2520Dynamics%26entry.906535625%3DYunhao%2520Zhang%2520and%2520Sidharth%2520S.%2520Menon%2520and%2520Lin%2520Cheng%2520and%2520Aswin%2520Gnanaskandan%2520and%2520Ameya%2520D.%2520Jagtap%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520employ%2520physics-informed%2520neural%2520operators%2520to%2520map%2520pressure%2520profiles%2520from%2520an%2520input%2520function%2520space%2520to%2520the%2520corresponding%2520bubble%2520radius%2520responses.%2520Our%2520approach%2520employs%2520a%2520two-step%2520DeepONet%2520architecture.%2520To%2520address%2520the%2520intrinsic%2520spectral%2520bias%2520of%2520deep%2520learning%2520models%252C%2520our%2520model%2520incorporates%2520the%2520Rowdy%2520adaptive%2520activation%2520function%252C%2520enhancing%2520the%2520representation%2520of%2520high-frequency%2520features.%2520Moreover%252C%2520we%2520introduce%2520the%2520Kolmogorov-Arnold%2520network%2520%2528KAN%2529%2520based%2520two-step%2520DeepOKAN%2520model%252C%2520which%2520enhances%2520interpretability%2520%2528often%2520lacking%2520in%2520conventional%2520multilayer%2520perceptron%2520architectures%2529%2520while%2520efficiently%2520capturing%2520high-frequency%2520bubble%2520dynamics%2520without%2520explicit%2520utilization%2520of%2520activation%2520functions%2520in%2520any%2520form.%2520We%2520particularly%2520investigate%2520the%2520use%2520of%2520spline%2520basis%2520functions%2520in%2520combination%2520with%2520radial%2520basis%2520functions%2520%2528RBF%2529%2520within%2520our%2520architecture%252C%2520as%2520they%2520demonstrate%2520superior%2520performance%2520in%2520constructing%2520a%2520universal%2520basis%2520for%2520approximating%2520high-frequency%2520bubble%2520dynamics%2520compared%2520to%2520alternative%2520formulations.%2520Furthermore%252C%2520we%2520emphasize%2520on%2520the%2520performance%2520bottleneck%2520of%2520RBF%2520while%2520learning%2520the%2520high%2520frequency%2520bubble%2520dynamics%2520and%2520showcase%2520the%2520advantage%2520of%2520using%2520spline%2520basis%2520function%2520for%2520the%2520trunk%2520network%2520in%2520overcoming%2520this%2520inherent%2520spectral%2520bias.%2520The%2520model%2520is%2520systematically%2520evaluated%2520across%2520three%2520representative%2520scenarios%253A%2520%25281%2529%2520bubble%2520dynamics%2520governed%2520by%2520the%2520Rayleigh-Plesset%2520equation%2520with%2520a%2520single%2520initial%2520radius%252C%2520%25282%2529%2520bubble%2520dynamics%2520governed%2520by%2520the%2520Keller-Miksis%2520equation%2520with%2520a%2520single%2520initial%2520radius%252C%2520and%2520%25283%2529%2520Keller-Miksis%2520dynamics%2520with%2520multiple%2520initial%2520radii.%2520We%2520also%2520compare%2520our%2520results%2520with%2520state-of-the-art%2520neural%2520operators%252C%2520including%2520Fourier%2520Neural%2520Operators%252C%2520Wavelet%2520Neural%2520Operators%252C%2520OFormer%252C%2520and%2520Convolutional%2520Neural%2520Operators.%2520Our%2520findings%2520demonstrate%2520that%2520the%2520two-step%2520DeepOKAN%2520accurately%2520captures%2520both%2520low-%2520and%2520high-frequency%2520behaviors%252C%2520and%2520offers%2520a%2520promising%2520alternative%2520to%2520conventional%2520numerical%2520solvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03965v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BubbleOKAN%3A%20A%20Physics-Informed%20Interpretable%20Neural%20Operator%20for%20High-Frequency%20Bubble%20Dynamics&entry.906535625=Yunhao%20Zhang%20and%20Sidharth%20S.%20Menon%20and%20Lin%20Cheng%20and%20Aswin%20Gnanaskandan%20and%20Ameya%20D.%20Jagtap&entry.1292438233=In%20this%20work%2C%20we%20employ%20physics-informed%20neural%20operators%20to%20map%20pressure%20profiles%20from%20an%20input%20function%20space%20to%20the%20corresponding%20bubble%20radius%20responses.%20Our%20approach%20employs%20a%20two-step%20DeepONet%20architecture.%20To%20address%20the%20intrinsic%20spectral%20bias%20of%20deep%20learning%20models%2C%20our%20model%20incorporates%20the%20Rowdy%20adaptive%20activation%20function%2C%20enhancing%20the%20representation%20of%20high-frequency%20features.%20Moreover%2C%20we%20introduce%20the%20Kolmogorov-Arnold%20network%20%28KAN%29%20based%20two-step%20DeepOKAN%20model%2C%20which%20enhances%20interpretability%20%28often%20lacking%20in%20conventional%20multilayer%20perceptron%20architectures%29%20while%20efficiently%20capturing%20high-frequency%20bubble%20dynamics%20without%20explicit%20utilization%20of%20activation%20functions%20in%20any%20form.%20We%20particularly%20investigate%20the%20use%20of%20spline%20basis%20functions%20in%20combination%20with%20radial%20basis%20functions%20%28RBF%29%20within%20our%20architecture%2C%20as%20they%20demonstrate%20superior%20performance%20in%20constructing%20a%20universal%20basis%20for%20approximating%20high-frequency%20bubble%20dynamics%20compared%20to%20alternative%20formulations.%20Furthermore%2C%20we%20emphasize%20on%20the%20performance%20bottleneck%20of%20RBF%20while%20learning%20the%20high%20frequency%20bubble%20dynamics%20and%20showcase%20the%20advantage%20of%20using%20spline%20basis%20function%20for%20the%20trunk%20network%20in%20overcoming%20this%20inherent%20spectral%20bias.%20The%20model%20is%20systematically%20evaluated%20across%20three%20representative%20scenarios%3A%20%281%29%20bubble%20dynamics%20governed%20by%20the%20Rayleigh-Plesset%20equation%20with%20a%20single%20initial%20radius%2C%20%282%29%20bubble%20dynamics%20governed%20by%20the%20Keller-Miksis%20equation%20with%20a%20single%20initial%20radius%2C%20and%20%283%29%20Keller-Miksis%20dynamics%20with%20multiple%20initial%20radii.%20We%20also%20compare%20our%20results%20with%20state-of-the-art%20neural%20operators%2C%20including%20Fourier%20Neural%20Operators%2C%20Wavelet%20Neural%20Operators%2C%20OFormer%2C%20and%20Convolutional%20Neural%20Operators.%20Our%20findings%20demonstrate%20that%20the%20two-step%20DeepOKAN%20accurately%20captures%20both%20low-%20and%20high-frequency%20behaviors%2C%20and%20offers%20a%20promising%20alternative%20to%20conventional%20numerical%20solvers.&entry.1838667208=http%3A//arxiv.org/abs/2508.03965v3&entry.124074799=Read"},
{"title": "Supervisory Measurement-Guided Noise Covariance Estimation", "author": "Haoying Li and Yifan Peng and Xinghan Li and Junfeng Wu", "abstract": "Reliable state estimation hinges on accurate specification of sensor noise covariances, which weigh heterogeneous measurements. In practice, these covariances are difficult to identify due to environmental variability, front-end preprocessing, and other reasons. We address this by formulating noise covariance estimation as a bilevel optimization that, from a Bayesian perspective, factorizes the joint likelihood of so-called odometry and supervisory measurements, thereby balancing information utilization with computational efficiency. The factorization converts the nested Bayesian dependency into a chain structure, enabling efficient parallel computation: at the lower level, an invariant extended Kalman filter with state augmentation estimates trajectories, while a derivative filter computes analytical gradients in parallel for upper-level gradient updates. The upper level refines the covariance to guide the lower-level estimation. Experiments on synthetic and real-world datasets show that our method achieves higher efficiency over existing baselines.", "link": "http://arxiv.org/abs/2510.24508v2", "date": "2025-12-17", "relevancy": 1.958, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4922}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervisory%20Measurement-Guided%20Noise%20Covariance%20Estimation&body=Title%3A%20Supervisory%20Measurement-Guided%20Noise%20Covariance%20Estimation%0AAuthor%3A%20Haoying%20Li%20and%20Yifan%20Peng%20and%20Xinghan%20Li%20and%20Junfeng%20Wu%0AAbstract%3A%20Reliable%20state%20estimation%20hinges%20on%20accurate%20specification%20of%20sensor%20noise%20covariances%2C%20which%20weigh%20heterogeneous%20measurements.%20In%20practice%2C%20these%20covariances%20are%20difficult%20to%20identify%20due%20to%20environmental%20variability%2C%20front-end%20preprocessing%2C%20and%20other%20reasons.%20We%20address%20this%20by%20formulating%20noise%20covariance%20estimation%20as%20a%20bilevel%20optimization%20that%2C%20from%20a%20Bayesian%20perspective%2C%20factorizes%20the%20joint%20likelihood%20of%20so-called%20odometry%20and%20supervisory%20measurements%2C%20thereby%20balancing%20information%20utilization%20with%20computational%20efficiency.%20The%20factorization%20converts%20the%20nested%20Bayesian%20dependency%20into%20a%20chain%20structure%2C%20enabling%20efficient%20parallel%20computation%3A%20at%20the%20lower%20level%2C%20an%20invariant%20extended%20Kalman%20filter%20with%20state%20augmentation%20estimates%20trajectories%2C%20while%20a%20derivative%20filter%20computes%20analytical%20gradients%20in%20parallel%20for%20upper-level%20gradient%20updates.%20The%20upper%20level%20refines%20the%20covariance%20to%20guide%20the%20lower-level%20estimation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%20our%20method%20achieves%20higher%20efficiency%20over%20existing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervisory%2520Measurement-Guided%2520Noise%2520Covariance%2520Estimation%26entry.906535625%3DHaoying%2520Li%2520and%2520Yifan%2520Peng%2520and%2520Xinghan%2520Li%2520and%2520Junfeng%2520Wu%26entry.1292438233%3DReliable%2520state%2520estimation%2520hinges%2520on%2520accurate%2520specification%2520of%2520sensor%2520noise%2520covariances%252C%2520which%2520weigh%2520heterogeneous%2520measurements.%2520In%2520practice%252C%2520these%2520covariances%2520are%2520difficult%2520to%2520identify%2520due%2520to%2520environmental%2520variability%252C%2520front-end%2520preprocessing%252C%2520and%2520other%2520reasons.%2520We%2520address%2520this%2520by%2520formulating%2520noise%2520covariance%2520estimation%2520as%2520a%2520bilevel%2520optimization%2520that%252C%2520from%2520a%2520Bayesian%2520perspective%252C%2520factorizes%2520the%2520joint%2520likelihood%2520of%2520so-called%2520odometry%2520and%2520supervisory%2520measurements%252C%2520thereby%2520balancing%2520information%2520utilization%2520with%2520computational%2520efficiency.%2520The%2520factorization%2520converts%2520the%2520nested%2520Bayesian%2520dependency%2520into%2520a%2520chain%2520structure%252C%2520enabling%2520efficient%2520parallel%2520computation%253A%2520at%2520the%2520lower%2520level%252C%2520an%2520invariant%2520extended%2520Kalman%2520filter%2520with%2520state%2520augmentation%2520estimates%2520trajectories%252C%2520while%2520a%2520derivative%2520filter%2520computes%2520analytical%2520gradients%2520in%2520parallel%2520for%2520upper-level%2520gradient%2520updates.%2520The%2520upper%2520level%2520refines%2520the%2520covariance%2520to%2520guide%2520the%2520lower-level%2520estimation.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520that%2520our%2520method%2520achieves%2520higher%2520efficiency%2520over%2520existing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervisory%20Measurement-Guided%20Noise%20Covariance%20Estimation&entry.906535625=Haoying%20Li%20and%20Yifan%20Peng%20and%20Xinghan%20Li%20and%20Junfeng%20Wu&entry.1292438233=Reliable%20state%20estimation%20hinges%20on%20accurate%20specification%20of%20sensor%20noise%20covariances%2C%20which%20weigh%20heterogeneous%20measurements.%20In%20practice%2C%20these%20covariances%20are%20difficult%20to%20identify%20due%20to%20environmental%20variability%2C%20front-end%20preprocessing%2C%20and%20other%20reasons.%20We%20address%20this%20by%20formulating%20noise%20covariance%20estimation%20as%20a%20bilevel%20optimization%20that%2C%20from%20a%20Bayesian%20perspective%2C%20factorizes%20the%20joint%20likelihood%20of%20so-called%20odometry%20and%20supervisory%20measurements%2C%20thereby%20balancing%20information%20utilization%20with%20computational%20efficiency.%20The%20factorization%20converts%20the%20nested%20Bayesian%20dependency%20into%20a%20chain%20structure%2C%20enabling%20efficient%20parallel%20computation%3A%20at%20the%20lower%20level%2C%20an%20invariant%20extended%20Kalman%20filter%20with%20state%20augmentation%20estimates%20trajectories%2C%20while%20a%20derivative%20filter%20computes%20analytical%20gradients%20in%20parallel%20for%20upper-level%20gradient%20updates.%20The%20upper%20level%20refines%20the%20covariance%20to%20guide%20the%20lower-level%20estimation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%20our%20method%20achieves%20higher%20efficiency%20over%20existing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2510.24508v2&entry.124074799=Read"},
{"title": "Functional Percolation: Criticality of Form and Function", "author": "Galen J. Wilkerson", "abstract": "Understanding how network structure constrains and enables information processing is a central problem in the statistical mechanics of interacting systems. Here we study random networks across the structural percolation transition and analyze how connectivity governs realizable input-output transformations under cascade dynamics. Using Erdos-Renyi networks as a minimal ensemble, we examine structural, functional, and information-theoretic observables as functions of mean degree. We find that the emergence of the giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow, quantified by transfer entropy, extends beyond local neighborhoods. We term this coincidence of structural, functional, and informational transitions functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the percolation threshold. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality may provide a general organizing principle of information processing capacity in systems with local interactions and propagating influences.", "link": "http://arxiv.org/abs/2512.09317v3", "date": "2025-12-17", "relevancy": 1.4599, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3987}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3633}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functional%20Percolation%3A%20Criticality%20of%20Form%20and%20Function&body=Title%3A%20Functional%20Percolation%3A%20Criticality%20of%20Form%20and%20Function%0AAuthor%3A%20Galen%20J.%20Wilkerson%0AAbstract%3A%20Understanding%20how%20network%20structure%20constrains%20and%20enables%20information%20processing%20is%20a%20central%20problem%20in%20the%20statistical%20mechanics%20of%20interacting%20systems.%20Here%20we%20study%20random%20networks%20across%20the%20structural%20percolation%20transition%20and%20analyze%20how%20connectivity%20governs%20realizable%20input-output%20transformations%20under%20cascade%20dynamics.%20Using%20Erdos-Renyi%20networks%20as%20a%20minimal%20ensemble%2C%20we%20examine%20structural%2C%20functional%2C%20and%20information-theoretic%20observables%20as%20functions%20of%20mean%20degree.%20We%20find%20that%20the%20emergence%20of%20the%20giant%20connected%20component%20coincides%20with%20a%20sharp%20transition%20in%20realizable%20information%20processing%3A%20complex%20input-output%20response%20functions%20become%20accessible%2C%20functional%20diversity%20increases%20rapidly%2C%20output%20entropy%20rises%2C%20and%20directed%20information%20flow%2C%20quantified%20by%20transfer%20entropy%2C%20extends%20beyond%20local%20neighborhoods.%20We%20term%20this%20coincidence%20of%20structural%2C%20functional%2C%20and%20informational%20transitions%20functional%20percolation%2C%20referring%20to%20a%20sharp%20expansion%20of%20the%20space%20of%20realizable%20input-output%20functions%20at%20the%20percolation%20threshold.%20Near%20criticality%2C%20networks%20exhibit%20a%20Pareto-optimal%20tradeoff%20between%20functional%20complexity%20and%20diversity%2C%20suggesting%20that%20percolation%20criticality%20may%20provide%20a%20general%20organizing%20principle%20of%20information%20processing%20capacity%20in%20systems%20with%20local%20interactions%20and%20propagating%20influences.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09317v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctional%2520Percolation%253A%2520Criticality%2520of%2520Form%2520and%2520Function%26entry.906535625%3DGalen%2520J.%2520Wilkerson%26entry.1292438233%3DUnderstanding%2520how%2520network%2520structure%2520constrains%2520and%2520enables%2520information%2520processing%2520is%2520a%2520central%2520problem%2520in%2520the%2520statistical%2520mechanics%2520of%2520interacting%2520systems.%2520Here%2520we%2520study%2520random%2520networks%2520across%2520the%2520structural%2520percolation%2520transition%2520and%2520analyze%2520how%2520connectivity%2520governs%2520realizable%2520input-output%2520transformations%2520under%2520cascade%2520dynamics.%2520Using%2520Erdos-Renyi%2520networks%2520as%2520a%2520minimal%2520ensemble%252C%2520we%2520examine%2520structural%252C%2520functional%252C%2520and%2520information-theoretic%2520observables%2520as%2520functions%2520of%2520mean%2520degree.%2520We%2520find%2520that%2520the%2520emergence%2520of%2520the%2520giant%2520connected%2520component%2520coincides%2520with%2520a%2520sharp%2520transition%2520in%2520realizable%2520information%2520processing%253A%2520complex%2520input-output%2520response%2520functions%2520become%2520accessible%252C%2520functional%2520diversity%2520increases%2520rapidly%252C%2520output%2520entropy%2520rises%252C%2520and%2520directed%2520information%2520flow%252C%2520quantified%2520by%2520transfer%2520entropy%252C%2520extends%2520beyond%2520local%2520neighborhoods.%2520We%2520term%2520this%2520coincidence%2520of%2520structural%252C%2520functional%252C%2520and%2520informational%2520transitions%2520functional%2520percolation%252C%2520referring%2520to%2520a%2520sharp%2520expansion%2520of%2520the%2520space%2520of%2520realizable%2520input-output%2520functions%2520at%2520the%2520percolation%2520threshold.%2520Near%2520criticality%252C%2520networks%2520exhibit%2520a%2520Pareto-optimal%2520tradeoff%2520between%2520functional%2520complexity%2520and%2520diversity%252C%2520suggesting%2520that%2520percolation%2520criticality%2520may%2520provide%2520a%2520general%2520organizing%2520principle%2520of%2520information%2520processing%2520capacity%2520in%2520systems%2520with%2520local%2520interactions%2520and%2520propagating%2520influences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09317v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functional%20Percolation%3A%20Criticality%20of%20Form%20and%20Function&entry.906535625=Galen%20J.%20Wilkerson&entry.1292438233=Understanding%20how%20network%20structure%20constrains%20and%20enables%20information%20processing%20is%20a%20central%20problem%20in%20the%20statistical%20mechanics%20of%20interacting%20systems.%20Here%20we%20study%20random%20networks%20across%20the%20structural%20percolation%20transition%20and%20analyze%20how%20connectivity%20governs%20realizable%20input-output%20transformations%20under%20cascade%20dynamics.%20Using%20Erdos-Renyi%20networks%20as%20a%20minimal%20ensemble%2C%20we%20examine%20structural%2C%20functional%2C%20and%20information-theoretic%20observables%20as%20functions%20of%20mean%20degree.%20We%20find%20that%20the%20emergence%20of%20the%20giant%20connected%20component%20coincides%20with%20a%20sharp%20transition%20in%20realizable%20information%20processing%3A%20complex%20input-output%20response%20functions%20become%20accessible%2C%20functional%20diversity%20increases%20rapidly%2C%20output%20entropy%20rises%2C%20and%20directed%20information%20flow%2C%20quantified%20by%20transfer%20entropy%2C%20extends%20beyond%20local%20neighborhoods.%20We%20term%20this%20coincidence%20of%20structural%2C%20functional%2C%20and%20informational%20transitions%20functional%20percolation%2C%20referring%20to%20a%20sharp%20expansion%20of%20the%20space%20of%20realizable%20input-output%20functions%20at%20the%20percolation%20threshold.%20Near%20criticality%2C%20networks%20exhibit%20a%20Pareto-optimal%20tradeoff%20between%20functional%20complexity%20and%20diversity%2C%20suggesting%20that%20percolation%20criticality%20may%20provide%20a%20general%20organizing%20principle%20of%20information%20processing%20capacity%20in%20systems%20with%20local%20interactions%20and%20propagating%20influences.&entry.1838667208=http%3A//arxiv.org/abs/2512.09317v3&entry.124074799=Read"},
{"title": "A Regime-Aware Fusion Framework for Time Series Classification", "author": "Honey Singh Chauhan and Zahraa S. Abdallah", "abstract": "Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.", "link": "http://arxiv.org/abs/2512.15378v1", "date": "2025-12-17", "relevancy": 1.8644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4641}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Regime-Aware%20Fusion%20Framework%20for%20Time%20Series%20Classification&body=Title%3A%20A%20Regime-Aware%20Fusion%20Framework%20for%20Time%20Series%20Classification%0AAuthor%3A%20Honey%20Singh%20Chauhan%20and%20Zahraa%20S.%20Abdallah%0AAbstract%3A%20Kernel-based%20methods%20such%20as%20Rocket%20are%20among%20the%20most%20effective%20default%20approaches%20for%20univariate%20time%20series%20classification%20%28TSC%29%2C%20yet%20they%20do%20not%20perform%20equally%20well%20across%20all%20datasets.%20We%20revisit%20the%20long-standing%20intuition%20that%20different%20representations%20capture%20complementary%20structure%20and%20show%20that%20selectively%20fusing%20them%20can%20yield%20consistent%20improvements%20over%20Rocket%20on%20specific%2C%20systematically%20identifiable%20kinds%20of%20datasets.%20We%20introduce%20Fusion-3%20%28F3%29%2C%20a%20lightweight%20framework%20that%20adaptively%20fuses%20Rocket%2C%20Sax%2C%20and%20Sfa%20representations.%20To%20understand%20when%20fusion%20helps%2C%20we%20cluster%20UCR%20datasets%20into%20six%20groups%20using%20meta-features%20capturing%20series%20length%2C%20spectral%20structure%2C%20roughness%2C%20and%20class%20imbalance%2C%20and%20treat%20these%20clusters%20as%20interpretable%20data-structure%20regimes.%20Our%20analysis%20shows%20that%20fusion%20typically%20outperforms%20strong%20baselines%20in%20regimes%20with%20structured%20variability%20or%20rich%20frequency%20content%2C%20while%20offering%20diminishing%20returns%20in%20highly%20irregular%20or%20outlier-heavy%20settings.%20To%20support%20these%20findings%2C%20we%20combine%20three%20complementary%20analyses%3A%20non-parametric%20paired%20statistics%20across%20datasets%2C%20ablation%20studies%20isolating%20the%20roles%20of%20individual%20representations%2C%20and%20attribution%20via%20SHAP%20to%20identify%20which%20dataset%20properties%20predict%20fusion%20gains.%20Sample-level%20case%20studies%20further%20reveal%20the%20underlying%20mechanism%3A%20fusion%20primarily%20improves%20performance%20by%20rescuing%20specific%20errors%2C%20with%20adaptive%20increases%20in%20frequency-domain%20weighting%20precisely%20where%20corrections%20occur.%20Using%205-fold%20cross-validation%20on%20the%20113%20UCR%20datasets%2C%20F3%20yields%20small%20but%20consistent%20average%20improvements%20over%20Rocket%2C%20supported%20by%20frequentist%20and%20Bayesian%20evidence%20and%20accompanied%20by%20clearly%20identifiable%20failure%20cases.%20Our%20results%20show%20that%20selectively%20applied%20fusion%20provides%20dependable%20and%20interpretable%20extension%20to%20strong%20kernel-based%20methods%2C%20correcting%20their%20weaknesses%20precisely%20where%20the%20data%20support%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Regime-Aware%2520Fusion%2520Framework%2520for%2520Time%2520Series%2520Classification%26entry.906535625%3DHoney%2520Singh%2520Chauhan%2520and%2520Zahraa%2520S.%2520Abdallah%26entry.1292438233%3DKernel-based%2520methods%2520such%2520as%2520Rocket%2520are%2520among%2520the%2520most%2520effective%2520default%2520approaches%2520for%2520univariate%2520time%2520series%2520classification%2520%2528TSC%2529%252C%2520yet%2520they%2520do%2520not%2520perform%2520equally%2520well%2520across%2520all%2520datasets.%2520We%2520revisit%2520the%2520long-standing%2520intuition%2520that%2520different%2520representations%2520capture%2520complementary%2520structure%2520and%2520show%2520that%2520selectively%2520fusing%2520them%2520can%2520yield%2520consistent%2520improvements%2520over%2520Rocket%2520on%2520specific%252C%2520systematically%2520identifiable%2520kinds%2520of%2520datasets.%2520We%2520introduce%2520Fusion-3%2520%2528F3%2529%252C%2520a%2520lightweight%2520framework%2520that%2520adaptively%2520fuses%2520Rocket%252C%2520Sax%252C%2520and%2520Sfa%2520representations.%2520To%2520understand%2520when%2520fusion%2520helps%252C%2520we%2520cluster%2520UCR%2520datasets%2520into%2520six%2520groups%2520using%2520meta-features%2520capturing%2520series%2520length%252C%2520spectral%2520structure%252C%2520roughness%252C%2520and%2520class%2520imbalance%252C%2520and%2520treat%2520these%2520clusters%2520as%2520interpretable%2520data-structure%2520regimes.%2520Our%2520analysis%2520shows%2520that%2520fusion%2520typically%2520outperforms%2520strong%2520baselines%2520in%2520regimes%2520with%2520structured%2520variability%2520or%2520rich%2520frequency%2520content%252C%2520while%2520offering%2520diminishing%2520returns%2520in%2520highly%2520irregular%2520or%2520outlier-heavy%2520settings.%2520To%2520support%2520these%2520findings%252C%2520we%2520combine%2520three%2520complementary%2520analyses%253A%2520non-parametric%2520paired%2520statistics%2520across%2520datasets%252C%2520ablation%2520studies%2520isolating%2520the%2520roles%2520of%2520individual%2520representations%252C%2520and%2520attribution%2520via%2520SHAP%2520to%2520identify%2520which%2520dataset%2520properties%2520predict%2520fusion%2520gains.%2520Sample-level%2520case%2520studies%2520further%2520reveal%2520the%2520underlying%2520mechanism%253A%2520fusion%2520primarily%2520improves%2520performance%2520by%2520rescuing%2520specific%2520errors%252C%2520with%2520adaptive%2520increases%2520in%2520frequency-domain%2520weighting%2520precisely%2520where%2520corrections%2520occur.%2520Using%25205-fold%2520cross-validation%2520on%2520the%2520113%2520UCR%2520datasets%252C%2520F3%2520yields%2520small%2520but%2520consistent%2520average%2520improvements%2520over%2520Rocket%252C%2520supported%2520by%2520frequentist%2520and%2520Bayesian%2520evidence%2520and%2520accompanied%2520by%2520clearly%2520identifiable%2520failure%2520cases.%2520Our%2520results%2520show%2520that%2520selectively%2520applied%2520fusion%2520provides%2520dependable%2520and%2520interpretable%2520extension%2520to%2520strong%2520kernel-based%2520methods%252C%2520correcting%2520their%2520weaknesses%2520precisely%2520where%2520the%2520data%2520support%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Regime-Aware%20Fusion%20Framework%20for%20Time%20Series%20Classification&entry.906535625=Honey%20Singh%20Chauhan%20and%20Zahraa%20S.%20Abdallah&entry.1292438233=Kernel-based%20methods%20such%20as%20Rocket%20are%20among%20the%20most%20effective%20default%20approaches%20for%20univariate%20time%20series%20classification%20%28TSC%29%2C%20yet%20they%20do%20not%20perform%20equally%20well%20across%20all%20datasets.%20We%20revisit%20the%20long-standing%20intuition%20that%20different%20representations%20capture%20complementary%20structure%20and%20show%20that%20selectively%20fusing%20them%20can%20yield%20consistent%20improvements%20over%20Rocket%20on%20specific%2C%20systematically%20identifiable%20kinds%20of%20datasets.%20We%20introduce%20Fusion-3%20%28F3%29%2C%20a%20lightweight%20framework%20that%20adaptively%20fuses%20Rocket%2C%20Sax%2C%20and%20Sfa%20representations.%20To%20understand%20when%20fusion%20helps%2C%20we%20cluster%20UCR%20datasets%20into%20six%20groups%20using%20meta-features%20capturing%20series%20length%2C%20spectral%20structure%2C%20roughness%2C%20and%20class%20imbalance%2C%20and%20treat%20these%20clusters%20as%20interpretable%20data-structure%20regimes.%20Our%20analysis%20shows%20that%20fusion%20typically%20outperforms%20strong%20baselines%20in%20regimes%20with%20structured%20variability%20or%20rich%20frequency%20content%2C%20while%20offering%20diminishing%20returns%20in%20highly%20irregular%20or%20outlier-heavy%20settings.%20To%20support%20these%20findings%2C%20we%20combine%20three%20complementary%20analyses%3A%20non-parametric%20paired%20statistics%20across%20datasets%2C%20ablation%20studies%20isolating%20the%20roles%20of%20individual%20representations%2C%20and%20attribution%20via%20SHAP%20to%20identify%20which%20dataset%20properties%20predict%20fusion%20gains.%20Sample-level%20case%20studies%20further%20reveal%20the%20underlying%20mechanism%3A%20fusion%20primarily%20improves%20performance%20by%20rescuing%20specific%20errors%2C%20with%20adaptive%20increases%20in%20frequency-domain%20weighting%20precisely%20where%20corrections%20occur.%20Using%205-fold%20cross-validation%20on%20the%20113%20UCR%20datasets%2C%20F3%20yields%20small%20but%20consistent%20average%20improvements%20over%20Rocket%2C%20supported%20by%20frequentist%20and%20Bayesian%20evidence%20and%20accompanied%20by%20clearly%20identifiable%20failure%20cases.%20Our%20results%20show%20that%20selectively%20applied%20fusion%20provides%20dependable%20and%20interpretable%20extension%20to%20strong%20kernel-based%20methods%2C%20correcting%20their%20weaknesses%20precisely%20where%20the%20data%20support%20it.&entry.1838667208=http%3A//arxiv.org/abs/2512.15378v1&entry.124074799=Read"},
{"title": "Topological Metric for Unsupervised Embedding Quality Evaluation", "author": "Aleksei Shestov and Anton Klenitskiy and Daria Denisova and Amurkhan Dzagkoev and Daniil Petrovich and Andrey Savchenko and Maksim Makarenko", "abstract": "Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.", "link": "http://arxiv.org/abs/2512.15285v1", "date": "2025-12-17", "relevancy": 1.8801, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Metric%20for%20Unsupervised%20Embedding%20Quality%20Evaluation&body=Title%3A%20Topological%20Metric%20for%20Unsupervised%20Embedding%20Quality%20Evaluation%0AAuthor%3A%20Aleksei%20Shestov%20and%20Anton%20Klenitskiy%20and%20Daria%20Denisova%20and%20Amurkhan%20Dzagkoev%20and%20Daniil%20Petrovich%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko%0AAbstract%3A%20Modern%20representation%20learning%20increasingly%20relies%20on%20unsupervised%20and%20self-supervised%20methods%20trained%20on%20large-scale%20unlabeled%20data.%20While%20these%20approaches%20achieve%20impressive%20generalization%20across%20tasks%20and%20domains%2C%20evaluating%20embedding%20quality%20without%20labels%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20propose%20Persistence%2C%20a%20topology-aware%20metric%20based%20on%20persistent%20homology%20that%20quantifies%20the%20geometric%20structure%20and%20topological%20richness%20of%20embedding%20spaces%20in%20a%20fully%20unsupervised%20manner.%20Unlike%20metrics%20that%20assume%20linear%20separability%20or%20rely%20on%20covariance%20structure%2C%20Persistence%20captures%20global%20and%20multi-scale%20organization.%20Empirical%20results%20across%20diverse%20domains%20show%20that%20Persistence%20consistently%20achieves%20top-tier%20correlations%20with%20downstream%20performance%2C%20outperforming%20existing%20unsupervised%20metrics%20and%20enabling%20reliable%20model%20and%20hyperparameter%20selection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Metric%2520for%2520Unsupervised%2520Embedding%2520Quality%2520Evaluation%26entry.906535625%3DAleksei%2520Shestov%2520and%2520Anton%2520Klenitskiy%2520and%2520Daria%2520Denisova%2520and%2520Amurkhan%2520Dzagkoev%2520and%2520Daniil%2520Petrovich%2520and%2520Andrey%2520Savchenko%2520and%2520Maksim%2520Makarenko%26entry.1292438233%3DModern%2520representation%2520learning%2520increasingly%2520relies%2520on%2520unsupervised%2520and%2520self-supervised%2520methods%2520trained%2520on%2520large-scale%2520unlabeled%2520data.%2520While%2520these%2520approaches%2520achieve%2520impressive%2520generalization%2520across%2520tasks%2520and%2520domains%252C%2520evaluating%2520embedding%2520quality%2520without%2520labels%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520propose%2520Persistence%252C%2520a%2520topology-aware%2520metric%2520based%2520on%2520persistent%2520homology%2520that%2520quantifies%2520the%2520geometric%2520structure%2520and%2520topological%2520richness%2520of%2520embedding%2520spaces%2520in%2520a%2520fully%2520unsupervised%2520manner.%2520Unlike%2520metrics%2520that%2520assume%2520linear%2520separability%2520or%2520rely%2520on%2520covariance%2520structure%252C%2520Persistence%2520captures%2520global%2520and%2520multi-scale%2520organization.%2520Empirical%2520results%2520across%2520diverse%2520domains%2520show%2520that%2520Persistence%2520consistently%2520achieves%2520top-tier%2520correlations%2520with%2520downstream%2520performance%252C%2520outperforming%2520existing%2520unsupervised%2520metrics%2520and%2520enabling%2520reliable%2520model%2520and%2520hyperparameter%2520selection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Metric%20for%20Unsupervised%20Embedding%20Quality%20Evaluation&entry.906535625=Aleksei%20Shestov%20and%20Anton%20Klenitskiy%20and%20Daria%20Denisova%20and%20Amurkhan%20Dzagkoev%20and%20Daniil%20Petrovich%20and%20Andrey%20Savchenko%20and%20Maksim%20Makarenko&entry.1292438233=Modern%20representation%20learning%20increasingly%20relies%20on%20unsupervised%20and%20self-supervised%20methods%20trained%20on%20large-scale%20unlabeled%20data.%20While%20these%20approaches%20achieve%20impressive%20generalization%20across%20tasks%20and%20domains%2C%20evaluating%20embedding%20quality%20without%20labels%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20propose%20Persistence%2C%20a%20topology-aware%20metric%20based%20on%20persistent%20homology%20that%20quantifies%20the%20geometric%20structure%20and%20topological%20richness%20of%20embedding%20spaces%20in%20a%20fully%20unsupervised%20manner.%20Unlike%20metrics%20that%20assume%20linear%20separability%20or%20rely%20on%20covariance%20structure%2C%20Persistence%20captures%20global%20and%20multi-scale%20organization.%20Empirical%20results%20across%20diverse%20domains%20show%20that%20Persistence%20consistently%20achieves%20top-tier%20correlations%20with%20downstream%20performance%2C%20outperforming%20existing%20unsupervised%20metrics%20and%20enabling%20reliable%20model%20and%20hyperparameter%20selection.&entry.1838667208=http%3A//arxiv.org/abs/2512.15285v1&entry.124074799=Read"},
{"title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning", "author": "Zelin Tan and Hejia Geng and Xiaohang Yu and Mulei Zhang and Guancheng Wan and Yifan Zhou and Qiang He and Xiangyuan Xue and Heng Zhou and Yutao Fan and Zhongzhi Li and Zaibin Zhang and Guibin Zhang and Chen Zhang and Zhenfei Yin and Lei Bai", "abstract": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1.Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2.The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3.Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4.In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.", "link": "http://arxiv.org/abs/2509.25300v2", "date": "2025-12-17", "relevancy": 2.0176, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Behaviors%20of%20LLM%20Reinforcement%20Learning%20Post-Training%3A%20An%20Empirical%20Study%20in%20Mathematical%20Reasoning&body=Title%3A%20Scaling%20Behaviors%20of%20LLM%20Reinforcement%20Learning%20Post-Training%3A%20An%20Empirical%20Study%20in%20Mathematical%20Reasoning%0AAuthor%3A%20Zelin%20Tan%20and%20Hejia%20Geng%20and%20Xiaohang%20Yu%20and%20Mulei%20Zhang%20and%20Guancheng%20Wan%20and%20Yifan%20Zhou%20and%20Qiang%20He%20and%20Xiangyuan%20Xue%20and%20Heng%20Zhou%20and%20Yutao%20Fan%20and%20Zhongzhi%20Li%20and%20Zaibin%20Zhang%20and%20Guibin%20Zhang%20and%20Chen%20Zhang%20and%20Zhenfei%20Yin%20and%20Lei%20Bai%0AAbstract%3A%20While%20scaling%20laws%20for%20large%20language%20models%20%28LLMs%29%20during%20pre-training%20have%20been%20extensively%20studied%2C%20their%20behavior%20under%20reinforcement%20learning%20%28RL%29%20post-training%20remains%20largely%20unexplored.%20This%20paper%20presents%20a%20systematic%20empirical%20investigation%20of%20scaling%20behaviors%20in%20RL-based%20post-training%2C%20with%20a%20particular%20focus%20on%20mathematical%20reasoning.%20Based%20on%20a%20set%20of%20experiments%20across%20the%20full%20Qwen2.5%20dense%20model%20series%20%280.5B%20to%2072B%29%2C%20we%20characterize%20how%20model%20scale%2C%20data%20volume%2C%20and%20computational%20budget%20interact%20to%20shape%20performance.%20Our%20analysis%20leads%20to%20four%20key%20findings%3A%201.Larger%20models%20consistently%20exhibit%20superior%20learning%20efficiency%20on%20both%20compute%20and%20data%20metrics.%202.The%20relationship%20between%20test%20loss%2C%20compute%2C%20and%20data%20can%20be%20modeled%20by%20a%20predictive%20power-law%20which%20is%20robust%20across%20both%20base%20and%20instruction-tuned%20models.%203.Although%20larger%20models%20exhibit%20higher%20learning%20efficiency%2C%20the%20analytical%20learning%20efficiency%20term%20k%28N%29%20in%20the%20power-law%20reveals%20a%20latent%20saturation%20trend%20in%20learning%20efficiency%20as%20model%20size%20continues%20to%20increase.%204.In%20data-constrained%20regimes%2C%20repeated%20reuse%20of%20high-quality%20data%20proves%20highly%20effective%2C%20as%20final%20performance%20is%20primarily%20governed%20by%20the%20total%20number%20of%20optimization%20steps%20rather%20than%20the%20uniqueness%20of%20samples.%20Collectively%2C%20these%20results%20provide%20a%20principled%20foundation%20and%20practical%20guidelines%20for%20efficiently%20scaling%20the%20reasoning%20capabilities%20of%20LLMs%20through%20RL%20post-training.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Behaviors%2520of%2520LLM%2520Reinforcement%2520Learning%2520Post-Training%253A%2520An%2520Empirical%2520Study%2520in%2520Mathematical%2520Reasoning%26entry.906535625%3DZelin%2520Tan%2520and%2520Hejia%2520Geng%2520and%2520Xiaohang%2520Yu%2520and%2520Mulei%2520Zhang%2520and%2520Guancheng%2520Wan%2520and%2520Yifan%2520Zhou%2520and%2520Qiang%2520He%2520and%2520Xiangyuan%2520Xue%2520and%2520Heng%2520Zhou%2520and%2520Yutao%2520Fan%2520and%2520Zhongzhi%2520Li%2520and%2520Zaibin%2520Zhang%2520and%2520Guibin%2520Zhang%2520and%2520Chen%2520Zhang%2520and%2520Zhenfei%2520Yin%2520and%2520Lei%2520Bai%26entry.1292438233%3DWhile%2520scaling%2520laws%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520during%2520pre-training%2520have%2520been%2520extensively%2520studied%252C%2520their%2520behavior%2520under%2520reinforcement%2520learning%2520%2528RL%2529%2520post-training%2520remains%2520largely%2520unexplored.%2520This%2520paper%2520presents%2520a%2520systematic%2520empirical%2520investigation%2520of%2520scaling%2520behaviors%2520in%2520RL-based%2520post-training%252C%2520with%2520a%2520particular%2520focus%2520on%2520mathematical%2520reasoning.%2520Based%2520on%2520a%2520set%2520of%2520experiments%2520across%2520the%2520full%2520Qwen2.5%2520dense%2520model%2520series%2520%25280.5B%2520to%252072B%2529%252C%2520we%2520characterize%2520how%2520model%2520scale%252C%2520data%2520volume%252C%2520and%2520computational%2520budget%2520interact%2520to%2520shape%2520performance.%2520Our%2520analysis%2520leads%2520to%2520four%2520key%2520findings%253A%25201.Larger%2520models%2520consistently%2520exhibit%2520superior%2520learning%2520efficiency%2520on%2520both%2520compute%2520and%2520data%2520metrics.%25202.The%2520relationship%2520between%2520test%2520loss%252C%2520compute%252C%2520and%2520data%2520can%2520be%2520modeled%2520by%2520a%2520predictive%2520power-law%2520which%2520is%2520robust%2520across%2520both%2520base%2520and%2520instruction-tuned%2520models.%25203.Although%2520larger%2520models%2520exhibit%2520higher%2520learning%2520efficiency%252C%2520the%2520analytical%2520learning%2520efficiency%2520term%2520k%2528N%2529%2520in%2520the%2520power-law%2520reveals%2520a%2520latent%2520saturation%2520trend%2520in%2520learning%2520efficiency%2520as%2520model%2520size%2520continues%2520to%2520increase.%25204.In%2520data-constrained%2520regimes%252C%2520repeated%2520reuse%2520of%2520high-quality%2520data%2520proves%2520highly%2520effective%252C%2520as%2520final%2520performance%2520is%2520primarily%2520governed%2520by%2520the%2520total%2520number%2520of%2520optimization%2520steps%2520rather%2520than%2520the%2520uniqueness%2520of%2520samples.%2520Collectively%252C%2520these%2520results%2520provide%2520a%2520principled%2520foundation%2520and%2520practical%2520guidelines%2520for%2520efficiently%2520scaling%2520the%2520reasoning%2520capabilities%2520of%2520LLMs%2520through%2520RL%2520post-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Behaviors%20of%20LLM%20Reinforcement%20Learning%20Post-Training%3A%20An%20Empirical%20Study%20in%20Mathematical%20Reasoning&entry.906535625=Zelin%20Tan%20and%20Hejia%20Geng%20and%20Xiaohang%20Yu%20and%20Mulei%20Zhang%20and%20Guancheng%20Wan%20and%20Yifan%20Zhou%20and%20Qiang%20He%20and%20Xiangyuan%20Xue%20and%20Heng%20Zhou%20and%20Yutao%20Fan%20and%20Zhongzhi%20Li%20and%20Zaibin%20Zhang%20and%20Guibin%20Zhang%20and%20Chen%20Zhang%20and%20Zhenfei%20Yin%20and%20Lei%20Bai&entry.1292438233=While%20scaling%20laws%20for%20large%20language%20models%20%28LLMs%29%20during%20pre-training%20have%20been%20extensively%20studied%2C%20their%20behavior%20under%20reinforcement%20learning%20%28RL%29%20post-training%20remains%20largely%20unexplored.%20This%20paper%20presents%20a%20systematic%20empirical%20investigation%20of%20scaling%20behaviors%20in%20RL-based%20post-training%2C%20with%20a%20particular%20focus%20on%20mathematical%20reasoning.%20Based%20on%20a%20set%20of%20experiments%20across%20the%20full%20Qwen2.5%20dense%20model%20series%20%280.5B%20to%2072B%29%2C%20we%20characterize%20how%20model%20scale%2C%20data%20volume%2C%20and%20computational%20budget%20interact%20to%20shape%20performance.%20Our%20analysis%20leads%20to%20four%20key%20findings%3A%201.Larger%20models%20consistently%20exhibit%20superior%20learning%20efficiency%20on%20both%20compute%20and%20data%20metrics.%202.The%20relationship%20between%20test%20loss%2C%20compute%2C%20and%20data%20can%20be%20modeled%20by%20a%20predictive%20power-law%20which%20is%20robust%20across%20both%20base%20and%20instruction-tuned%20models.%203.Although%20larger%20models%20exhibit%20higher%20learning%20efficiency%2C%20the%20analytical%20learning%20efficiency%20term%20k%28N%29%20in%20the%20power-law%20reveals%20a%20latent%20saturation%20trend%20in%20learning%20efficiency%20as%20model%20size%20continues%20to%20increase.%204.In%20data-constrained%20regimes%2C%20repeated%20reuse%20of%20high-quality%20data%20proves%20highly%20effective%2C%20as%20final%20performance%20is%20primarily%20governed%20by%20the%20total%20number%20of%20optimization%20steps%20rather%20than%20the%20uniqueness%20of%20samples.%20Collectively%2C%20these%20results%20provide%20a%20principled%20foundation%20and%20practical%20guidelines%20for%20efficiently%20scaling%20the%20reasoning%20capabilities%20of%20LLMs%20through%20RL%20post-training.&entry.1838667208=http%3A//arxiv.org/abs/2509.25300v2&entry.124074799=Read"},
{"title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition", "author": "Shengming Yin and Zekai Zhang and Zecheng Tang and Kaiyuan Gao and Xiao Xu and Kun Yan and Jiahao Li and Yilei Chen and Yuxiang Chen and Heung-Yeung Shum and Lionel M. Ni and Jingren Zhou and Junyang Lin and Chenfei Wu", "abstract": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \\href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}", "link": "http://arxiv.org/abs/2512.15603v1", "date": "2025-12-17", "relevancy": 1.7355, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5822}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5791}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen-Image-Layered%3A%20Towards%20Inherent%20Editability%20via%20Layer%20Decomposition&body=Title%3A%20Qwen-Image-Layered%3A%20Towards%20Inherent%20Editability%20via%20Layer%20Decomposition%0AAuthor%3A%20Shengming%20Yin%20and%20Zekai%20Zhang%20and%20Zecheng%20Tang%20and%20Kaiyuan%20Gao%20and%20Xiao%20Xu%20and%20Kun%20Yan%20and%20Jiahao%20Li%20and%20Yilei%20Chen%20and%20Yuxiang%20Chen%20and%20Heung-Yeung%20Shum%20and%20Lionel%20M.%20Ni%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%20and%20Chenfei%20Wu%0AAbstract%3A%20Recent%20visual%20generative%20models%20often%20struggle%20with%20consistency%20during%20image%20editing%20due%20to%20the%20entangled%20nature%20of%20raster%20images%2C%20where%20all%20visual%20content%20is%20fused%20into%20a%20single%20canvas.%20In%20contrast%2C%20professional%20design%20tools%20employ%20layered%20representations%2C%20allowing%20isolated%20edits%20while%20preserving%20consistency.%20Motivated%20by%20this%2C%20we%20propose%20%5Ctextbf%7BQwen-Image-Layered%7D%2C%20an%20end-to-end%20diffusion%20model%20that%20decomposes%20a%20single%20RGB%20image%20into%20multiple%20semantically%20disentangled%20RGBA%20layers%2C%20enabling%20%5Ctextbf%7Binherent%20editability%7D%2C%20where%20each%20RGBA%20layer%20can%20be%20independently%20manipulated%20without%20affecting%20other%20content.%20To%20support%20variable-length%20decomposition%2C%20we%20introduce%20three%20key%20components%3A%20%281%29%20an%20RGBA-VAE%20to%20unify%20the%20latent%20representations%20of%20RGB%20and%20RGBA%20images%3B%20%282%29%20a%20VLD-MMDiT%20%28Variable%20Layers%20Decomposition%20MMDiT%29%20architecture%20capable%20of%20decomposing%20a%20variable%20number%20of%20image%20layers%3B%20and%20%283%29%20a%20Multi-stage%20Training%20strategy%20to%20adapt%20a%20pretrained%20image%20generation%20model%20into%20a%20multilayer%20image%20decomposer.%20Furthermore%2C%20to%20address%20the%20scarcity%20of%20high-quality%20multilayer%20training%20images%2C%20we%20build%20a%20pipeline%20to%20extract%20and%20annotate%20multilayer%20images%20from%20Photoshop%20documents%20%28PSD%29.%20Experiments%20demonstrate%20that%20our%20method%20significantly%20surpasses%20existing%20approaches%20in%20decomposition%20quality%20and%20establishes%20a%20new%20paradigm%20for%20consistent%20image%20editing.%20Our%20code%20and%20models%20are%20released%20on%20%5Chref%7Bhttps%3A//github.com/QwenLM/Qwen-Image-Layered%7D%7Bhttps%3A//github.com/QwenLM/Qwen-Image-Layered%7D%0ALink%3A%20http%3A//arxiv.org/abs/2512.15603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen-Image-Layered%253A%2520Towards%2520Inherent%2520Editability%2520via%2520Layer%2520Decomposition%26entry.906535625%3DShengming%2520Yin%2520and%2520Zekai%2520Zhang%2520and%2520Zecheng%2520Tang%2520and%2520Kaiyuan%2520Gao%2520and%2520Xiao%2520Xu%2520and%2520Kun%2520Yan%2520and%2520Jiahao%2520Li%2520and%2520Yilei%2520Chen%2520and%2520Yuxiang%2520Chen%2520and%2520Heung-Yeung%2520Shum%2520and%2520Lionel%2520M.%2520Ni%2520and%2520Jingren%2520Zhou%2520and%2520Junyang%2520Lin%2520and%2520Chenfei%2520Wu%26entry.1292438233%3DRecent%2520visual%2520generative%2520models%2520often%2520struggle%2520with%2520consistency%2520during%2520image%2520editing%2520due%2520to%2520the%2520entangled%2520nature%2520of%2520raster%2520images%252C%2520where%2520all%2520visual%2520content%2520is%2520fused%2520into%2520a%2520single%2520canvas.%2520In%2520contrast%252C%2520professional%2520design%2520tools%2520employ%2520layered%2520representations%252C%2520allowing%2520isolated%2520edits%2520while%2520preserving%2520consistency.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BQwen-Image-Layered%257D%252C%2520an%2520end-to-end%2520diffusion%2520model%2520that%2520decomposes%2520a%2520single%2520RGB%2520image%2520into%2520multiple%2520semantically%2520disentangled%2520RGBA%2520layers%252C%2520enabling%2520%255Ctextbf%257Binherent%2520editability%257D%252C%2520where%2520each%2520RGBA%2520layer%2520can%2520be%2520independently%2520manipulated%2520without%2520affecting%2520other%2520content.%2520To%2520support%2520variable-length%2520decomposition%252C%2520we%2520introduce%2520three%2520key%2520components%253A%2520%25281%2529%2520an%2520RGBA-VAE%2520to%2520unify%2520the%2520latent%2520representations%2520of%2520RGB%2520and%2520RGBA%2520images%253B%2520%25282%2529%2520a%2520VLD-MMDiT%2520%2528Variable%2520Layers%2520Decomposition%2520MMDiT%2529%2520architecture%2520capable%2520of%2520decomposing%2520a%2520variable%2520number%2520of%2520image%2520layers%253B%2520and%2520%25283%2529%2520a%2520Multi-stage%2520Training%2520strategy%2520to%2520adapt%2520a%2520pretrained%2520image%2520generation%2520model%2520into%2520a%2520multilayer%2520image%2520decomposer.%2520Furthermore%252C%2520to%2520address%2520the%2520scarcity%2520of%2520high-quality%2520multilayer%2520training%2520images%252C%2520we%2520build%2520a%2520pipeline%2520to%2520extract%2520and%2520annotate%2520multilayer%2520images%2520from%2520Photoshop%2520documents%2520%2528PSD%2529.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520surpasses%2520existing%2520approaches%2520in%2520decomposition%2520quality%2520and%2520establishes%2520a%2520new%2520paradigm%2520for%2520consistent%2520image%2520editing.%2520Our%2520code%2520and%2520models%2520are%2520released%2520on%2520%255Chref%257Bhttps%253A//github.com/QwenLM/Qwen-Image-Layered%257D%257Bhttps%253A//github.com/QwenLM/Qwen-Image-Layered%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen-Image-Layered%3A%20Towards%20Inherent%20Editability%20via%20Layer%20Decomposition&entry.906535625=Shengming%20Yin%20and%20Zekai%20Zhang%20and%20Zecheng%20Tang%20and%20Kaiyuan%20Gao%20and%20Xiao%20Xu%20and%20Kun%20Yan%20and%20Jiahao%20Li%20and%20Yilei%20Chen%20and%20Yuxiang%20Chen%20and%20Heung-Yeung%20Shum%20and%20Lionel%20M.%20Ni%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%20and%20Chenfei%20Wu&entry.1292438233=Recent%20visual%20generative%20models%20often%20struggle%20with%20consistency%20during%20image%20editing%20due%20to%20the%20entangled%20nature%20of%20raster%20images%2C%20where%20all%20visual%20content%20is%20fused%20into%20a%20single%20canvas.%20In%20contrast%2C%20professional%20design%20tools%20employ%20layered%20representations%2C%20allowing%20isolated%20edits%20while%20preserving%20consistency.%20Motivated%20by%20this%2C%20we%20propose%20%5Ctextbf%7BQwen-Image-Layered%7D%2C%20an%20end-to-end%20diffusion%20model%20that%20decomposes%20a%20single%20RGB%20image%20into%20multiple%20semantically%20disentangled%20RGBA%20layers%2C%20enabling%20%5Ctextbf%7Binherent%20editability%7D%2C%20where%20each%20RGBA%20layer%20can%20be%20independently%20manipulated%20without%20affecting%20other%20content.%20To%20support%20variable-length%20decomposition%2C%20we%20introduce%20three%20key%20components%3A%20%281%29%20an%20RGBA-VAE%20to%20unify%20the%20latent%20representations%20of%20RGB%20and%20RGBA%20images%3B%20%282%29%20a%20VLD-MMDiT%20%28Variable%20Layers%20Decomposition%20MMDiT%29%20architecture%20capable%20of%20decomposing%20a%20variable%20number%20of%20image%20layers%3B%20and%20%283%29%20a%20Multi-stage%20Training%20strategy%20to%20adapt%20a%20pretrained%20image%20generation%20model%20into%20a%20multilayer%20image%20decomposer.%20Furthermore%2C%20to%20address%20the%20scarcity%20of%20high-quality%20multilayer%20training%20images%2C%20we%20build%20a%20pipeline%20to%20extract%20and%20annotate%20multilayer%20images%20from%20Photoshop%20documents%20%28PSD%29.%20Experiments%20demonstrate%20that%20our%20method%20significantly%20surpasses%20existing%20approaches%20in%20decomposition%20quality%20and%20establishes%20a%20new%20paradigm%20for%20consistent%20image%20editing.%20Our%20code%20and%20models%20are%20released%20on%20%5Chref%7Bhttps%3A//github.com/QwenLM/Qwen-Image-Layered%7D%7Bhttps%3A//github.com/QwenLM/Qwen-Image-Layered%7D&entry.1838667208=http%3A//arxiv.org/abs/2512.15603v1&entry.124074799=Read"},
{"title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection", "author": "Yu Wang and Juhyung Ha and Frangil M. Ramirez and Yuchen Wang and David J. Crandall", "abstract": "Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.", "link": "http://arxiv.org/abs/2512.15707v1", "date": "2025-12-17", "relevancy": 1.9808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4932}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GateFusion%3A%20Hierarchical%20Gated%20Cross-Modal%20Fusion%20for%20Active%20Speaker%20Detection&body=Title%3A%20GateFusion%3A%20Hierarchical%20Gated%20Cross-Modal%20Fusion%20for%20Active%20Speaker%20Detection%0AAuthor%3A%20Yu%20Wang%20and%20Juhyung%20Ha%20and%20Frangil%20M.%20Ramirez%20and%20Yuchen%20Wang%20and%20David%20J.%20Crandall%0AAbstract%3A%20Active%20Speaker%20Detection%20%28ASD%29%20aims%20to%20identify%20who%20is%20currently%20speaking%20in%20each%20frame%20of%20a%20video.%20Most%20state-of-the-art%20approaches%20rely%20on%20late%20fusion%20to%20combine%20visual%20and%20audio%20features%2C%20but%20late%20fusion%20often%20fails%20to%20capture%20fine-grained%20cross-modal%20interactions%2C%20which%20can%20be%20critical%20for%20robust%20performance%20in%20unconstrained%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20GateFusion%2C%20a%20novel%20architecture%20that%20combines%20strong%20pretrained%20unimodal%20encoders%20with%20a%20Hierarchical%20Gated%20Fusion%20Decoder%20%28HiGate%29.%20HiGate%20enables%20progressive%2C%20multi-depth%20fusion%20by%20adaptively%20injecting%20contextual%20features%20from%20one%20modality%20into%20the%20other%20at%20multiple%20layers%20of%20the%20Transformer%20backbone%2C%20guided%20by%20learnable%2C%20bimodally-conditioned%20gates.%20To%20further%20strengthen%20multimodal%20learning%2C%20we%20propose%20two%20auxiliary%20objectives%3A%20Masked%20Alignment%20Loss%20%28MAL%29%20to%20align%20unimodal%20outputs%20with%20multimodal%20predictions%2C%20and%20Over-Positive%20Penalty%20%28OPP%29%20to%20suppress%20spurious%20video-only%20activations.%20GateFusion%20establishes%20new%20state-of-the-art%20results%20on%20several%20challenging%20ASD%20benchmarks%2C%20achieving%2077.8%25%20mAP%20%28%2B9.4%25%29%2C%2086.1%25%20mAP%20%28%2B2.9%25%29%2C%20and%2096.1%25%20mAP%20%28%2B0.5%25%29%20on%20Ego4D-ASD%2C%20UniTalk%2C%20and%20WASD%20benchmarks%2C%20respectively%2C%20and%20delivering%20competitive%20performance%20on%20AVA-ActiveSpeaker.%20Out-of-domain%20experiments%20demonstrate%20the%20generalization%20of%20our%20model%2C%20while%20comprehensive%20ablations%20show%20the%20complementary%20benefits%20of%20each%20component.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGateFusion%253A%2520Hierarchical%2520Gated%2520Cross-Modal%2520Fusion%2520for%2520Active%2520Speaker%2520Detection%26entry.906535625%3DYu%2520Wang%2520and%2520Juhyung%2520Ha%2520and%2520Frangil%2520M.%2520Ramirez%2520and%2520Yuchen%2520Wang%2520and%2520David%2520J.%2520Crandall%26entry.1292438233%3DActive%2520Speaker%2520Detection%2520%2528ASD%2529%2520aims%2520to%2520identify%2520who%2520is%2520currently%2520speaking%2520in%2520each%2520frame%2520of%2520a%2520video.%2520Most%2520state-of-the-art%2520approaches%2520rely%2520on%2520late%2520fusion%2520to%2520combine%2520visual%2520and%2520audio%2520features%252C%2520but%2520late%2520fusion%2520often%2520fails%2520to%2520capture%2520fine-grained%2520cross-modal%2520interactions%252C%2520which%2520can%2520be%2520critical%2520for%2520robust%2520performance%2520in%2520unconstrained%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GateFusion%252C%2520a%2520novel%2520architecture%2520that%2520combines%2520strong%2520pretrained%2520unimodal%2520encoders%2520with%2520a%2520Hierarchical%2520Gated%2520Fusion%2520Decoder%2520%2528HiGate%2529.%2520HiGate%2520enables%2520progressive%252C%2520multi-depth%2520fusion%2520by%2520adaptively%2520injecting%2520contextual%2520features%2520from%2520one%2520modality%2520into%2520the%2520other%2520at%2520multiple%2520layers%2520of%2520the%2520Transformer%2520backbone%252C%2520guided%2520by%2520learnable%252C%2520bimodally-conditioned%2520gates.%2520To%2520further%2520strengthen%2520multimodal%2520learning%252C%2520we%2520propose%2520two%2520auxiliary%2520objectives%253A%2520Masked%2520Alignment%2520Loss%2520%2528MAL%2529%2520to%2520align%2520unimodal%2520outputs%2520with%2520multimodal%2520predictions%252C%2520and%2520Over-Positive%2520Penalty%2520%2528OPP%2529%2520to%2520suppress%2520spurious%2520video-only%2520activations.%2520GateFusion%2520establishes%2520new%2520state-of-the-art%2520results%2520on%2520several%2520challenging%2520ASD%2520benchmarks%252C%2520achieving%252077.8%2525%2520mAP%2520%2528%252B9.4%2525%2529%252C%252086.1%2525%2520mAP%2520%2528%252B2.9%2525%2529%252C%2520and%252096.1%2525%2520mAP%2520%2528%252B0.5%2525%2529%2520on%2520Ego4D-ASD%252C%2520UniTalk%252C%2520and%2520WASD%2520benchmarks%252C%2520respectively%252C%2520and%2520delivering%2520competitive%2520performance%2520on%2520AVA-ActiveSpeaker.%2520Out-of-domain%2520experiments%2520demonstrate%2520the%2520generalization%2520of%2520our%2520model%252C%2520while%2520comprehensive%2520ablations%2520show%2520the%2520complementary%2520benefits%2520of%2520each%2520component.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GateFusion%3A%20Hierarchical%20Gated%20Cross-Modal%20Fusion%20for%20Active%20Speaker%20Detection&entry.906535625=Yu%20Wang%20and%20Juhyung%20Ha%20and%20Frangil%20M.%20Ramirez%20and%20Yuchen%20Wang%20and%20David%20J.%20Crandall&entry.1292438233=Active%20Speaker%20Detection%20%28ASD%29%20aims%20to%20identify%20who%20is%20currently%20speaking%20in%20each%20frame%20of%20a%20video.%20Most%20state-of-the-art%20approaches%20rely%20on%20late%20fusion%20to%20combine%20visual%20and%20audio%20features%2C%20but%20late%20fusion%20often%20fails%20to%20capture%20fine-grained%20cross-modal%20interactions%2C%20which%20can%20be%20critical%20for%20robust%20performance%20in%20unconstrained%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20GateFusion%2C%20a%20novel%20architecture%20that%20combines%20strong%20pretrained%20unimodal%20encoders%20with%20a%20Hierarchical%20Gated%20Fusion%20Decoder%20%28HiGate%29.%20HiGate%20enables%20progressive%2C%20multi-depth%20fusion%20by%20adaptively%20injecting%20contextual%20features%20from%20one%20modality%20into%20the%20other%20at%20multiple%20layers%20of%20the%20Transformer%20backbone%2C%20guided%20by%20learnable%2C%20bimodally-conditioned%20gates.%20To%20further%20strengthen%20multimodal%20learning%2C%20we%20propose%20two%20auxiliary%20objectives%3A%20Masked%20Alignment%20Loss%20%28MAL%29%20to%20align%20unimodal%20outputs%20with%20multimodal%20predictions%2C%20and%20Over-Positive%20Penalty%20%28OPP%29%20to%20suppress%20spurious%20video-only%20activations.%20GateFusion%20establishes%20new%20state-of-the-art%20results%20on%20several%20challenging%20ASD%20benchmarks%2C%20achieving%2077.8%25%20mAP%20%28%2B9.4%25%29%2C%2086.1%25%20mAP%20%28%2B2.9%25%29%2C%20and%2096.1%25%20mAP%20%28%2B0.5%25%29%20on%20Ego4D-ASD%2C%20UniTalk%2C%20and%20WASD%20benchmarks%2C%20respectively%2C%20and%20delivering%20competitive%20performance%20on%20AVA-ActiveSpeaker.%20Out-of-domain%20experiments%20demonstrate%20the%20generalization%20of%20our%20model%2C%20while%20comprehensive%20ablations%20show%20the%20complementary%20benefits%20of%20each%20component.&entry.1838667208=http%3A//arxiv.org/abs/2512.15707v1&entry.124074799=Read"},
{"title": "CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity", "author": "Mert Sonmezer and Seyda Ertekin", "abstract": "Long-term time series forecasting plays a pivotal role in various real-world applications. Despite recent advancements and the success of different architectures, forecasting is often challenging due to non-stationary nature of the real-world data, which frequently exhibit distribution shifts and temporal changes in statistical properties like mean and variance over time. Previous studies suggest that this inherent variability complicates forecasting, limiting the performance of many models by leading to loss of non-stationarity and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To address this challenge, we introduce a novel architecture, ChoronoAdaptive Network (CANet), inspired by style-transfer techniques. The core of CANet is the Non-stationary Adaptive Normalization module, seamlessly integrating the Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and Belongie, 2017). The Style Blending Gate preserves and reintegrates non-stationary characteristics, such as mean and standard deviation, by blending internal and external statistics, preventing over-stationarization while maintaining essential temporal dependencies. Coupled with AdaIN, which dynamically adapts the model to statistical changes, this approach enhances predictive accuracy under non-stationary conditions. CANet also employs multi-resolution patching to handle short-term fluctuations and long-term trends, along with Fourier analysis-based adaptive thresholding to reduce noise. A Stacked Kronecker Product Layer further optimizes the model's efficiency while maintaining high performance. Extensive experiments on real-world datasets validate CANet's superiority over state-of-the-art methods, achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is publicly available at https://github.com/mertsonmezer/CANet.", "link": "http://arxiv.org/abs/2504.17913v2", "date": "2025-12-17", "relevancy": 1.59, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5698}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4831}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CANet%3A%20ChronoAdaptive%20Network%20for%20Enhanced%20Long-Term%20Time%20Series%20Forecasting%20under%20Non-Stationarity&body=Title%3A%20CANet%3A%20ChronoAdaptive%20Network%20for%20Enhanced%20Long-Term%20Time%20Series%20Forecasting%20under%20Non-Stationarity%0AAuthor%3A%20Mert%20Sonmezer%20and%20Seyda%20Ertekin%0AAbstract%3A%20Long-term%20time%20series%20forecasting%20plays%20a%20pivotal%20role%20in%20various%20real-world%20applications.%20Despite%20recent%20advancements%20and%20the%20success%20of%20different%20architectures%2C%20forecasting%20is%20often%20challenging%20due%20to%20non-stationary%20nature%20of%20the%20real-world%20data%2C%20which%20frequently%20exhibit%20distribution%20shifts%20and%20temporal%20changes%20in%20statistical%20properties%20like%20mean%20and%20variance%20over%20time.%20Previous%20studies%20suggest%20that%20this%20inherent%20variability%20complicates%20forecasting%2C%20limiting%20the%20performance%20of%20many%20models%20by%20leading%20to%20loss%20of%20non-stationarity%20and%20resulting%20in%20over-stationarization%20%28Liu%2C%20Wu%2C%20Wang%20and%20Long%2C%202022%29.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20architecture%2C%20ChoronoAdaptive%20Network%20%28CANet%29%2C%20inspired%20by%20style-transfer%20techniques.%20The%20core%20of%20CANet%20is%20the%20Non-stationary%20Adaptive%20Normalization%20module%2C%20seamlessly%20integrating%20the%20Style%20Blending%20Gate%20and%20Adaptive%20Instance%20Normalization%20%28AdaIN%29%20%28Huang%20and%20Belongie%2C%202017%29.%20The%20Style%20Blending%20Gate%20preserves%20and%20reintegrates%20non-stationary%20characteristics%2C%20such%20as%20mean%20and%20standard%20deviation%2C%20by%20blending%20internal%20and%20external%20statistics%2C%20preventing%20over-stationarization%20while%20maintaining%20essential%20temporal%20dependencies.%20Coupled%20with%20AdaIN%2C%20which%20dynamically%20adapts%20the%20model%20to%20statistical%20changes%2C%20this%20approach%20enhances%20predictive%20accuracy%20under%20non-stationary%20conditions.%20CANet%20also%20employs%20multi-resolution%20patching%20to%20handle%20short-term%20fluctuations%20and%20long-term%20trends%2C%20along%20with%20Fourier%20analysis-based%20adaptive%20thresholding%20to%20reduce%20noise.%20A%20Stacked%20Kronecker%20Product%20Layer%20further%20optimizes%20the%20model%27s%20efficiency%20while%20maintaining%20high%20performance.%20Extensive%20experiments%20on%20real-world%20datasets%20validate%20CANet%27s%20superiority%20over%20state-of-the-art%20methods%2C%20achieving%20a%2042%25%20reduction%20in%20MSE%20and%20a%2022%25%20reduction%20in%20MAE.%20The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/mertsonmezer/CANet.%0ALink%3A%20http%3A//arxiv.org/abs/2504.17913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCANet%253A%2520ChronoAdaptive%2520Network%2520for%2520Enhanced%2520Long-Term%2520Time%2520Series%2520Forecasting%2520under%2520Non-Stationarity%26entry.906535625%3DMert%2520Sonmezer%2520and%2520Seyda%2520Ertekin%26entry.1292438233%3DLong-term%2520time%2520series%2520forecasting%2520plays%2520a%2520pivotal%2520role%2520in%2520various%2520real-world%2520applications.%2520Despite%2520recent%2520advancements%2520and%2520the%2520success%2520of%2520different%2520architectures%252C%2520forecasting%2520is%2520often%2520challenging%2520due%2520to%2520non-stationary%2520nature%2520of%2520the%2520real-world%2520data%252C%2520which%2520frequently%2520exhibit%2520distribution%2520shifts%2520and%2520temporal%2520changes%2520in%2520statistical%2520properties%2520like%2520mean%2520and%2520variance%2520over%2520time.%2520Previous%2520studies%2520suggest%2520that%2520this%2520inherent%2520variability%2520complicates%2520forecasting%252C%2520limiting%2520the%2520performance%2520of%2520many%2520models%2520by%2520leading%2520to%2520loss%2520of%2520non-stationarity%2520and%2520resulting%2520in%2520over-stationarization%2520%2528Liu%252C%2520Wu%252C%2520Wang%2520and%2520Long%252C%25202022%2529.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520novel%2520architecture%252C%2520ChoronoAdaptive%2520Network%2520%2528CANet%2529%252C%2520inspired%2520by%2520style-transfer%2520techniques.%2520The%2520core%2520of%2520CANet%2520is%2520the%2520Non-stationary%2520Adaptive%2520Normalization%2520module%252C%2520seamlessly%2520integrating%2520the%2520Style%2520Blending%2520Gate%2520and%2520Adaptive%2520Instance%2520Normalization%2520%2528AdaIN%2529%2520%2528Huang%2520and%2520Belongie%252C%25202017%2529.%2520The%2520Style%2520Blending%2520Gate%2520preserves%2520and%2520reintegrates%2520non-stationary%2520characteristics%252C%2520such%2520as%2520mean%2520and%2520standard%2520deviation%252C%2520by%2520blending%2520internal%2520and%2520external%2520statistics%252C%2520preventing%2520over-stationarization%2520while%2520maintaining%2520essential%2520temporal%2520dependencies.%2520Coupled%2520with%2520AdaIN%252C%2520which%2520dynamically%2520adapts%2520the%2520model%2520to%2520statistical%2520changes%252C%2520this%2520approach%2520enhances%2520predictive%2520accuracy%2520under%2520non-stationary%2520conditions.%2520CANet%2520also%2520employs%2520multi-resolution%2520patching%2520to%2520handle%2520short-term%2520fluctuations%2520and%2520long-term%2520trends%252C%2520along%2520with%2520Fourier%2520analysis-based%2520adaptive%2520thresholding%2520to%2520reduce%2520noise.%2520A%2520Stacked%2520Kronecker%2520Product%2520Layer%2520further%2520optimizes%2520the%2520model%2527s%2520efficiency%2520while%2520maintaining%2520high%2520performance.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%2520validate%2520CANet%2527s%2520superiority%2520over%2520state-of-the-art%2520methods%252C%2520achieving%2520a%252042%2525%2520reduction%2520in%2520MSE%2520and%2520a%252022%2525%2520reduction%2520in%2520MAE.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/mertsonmezer/CANet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CANet%3A%20ChronoAdaptive%20Network%20for%20Enhanced%20Long-Term%20Time%20Series%20Forecasting%20under%20Non-Stationarity&entry.906535625=Mert%20Sonmezer%20and%20Seyda%20Ertekin&entry.1292438233=Long-term%20time%20series%20forecasting%20plays%20a%20pivotal%20role%20in%20various%20real-world%20applications.%20Despite%20recent%20advancements%20and%20the%20success%20of%20different%20architectures%2C%20forecasting%20is%20often%20challenging%20due%20to%20non-stationary%20nature%20of%20the%20real-world%20data%2C%20which%20frequently%20exhibit%20distribution%20shifts%20and%20temporal%20changes%20in%20statistical%20properties%20like%20mean%20and%20variance%20over%20time.%20Previous%20studies%20suggest%20that%20this%20inherent%20variability%20complicates%20forecasting%2C%20limiting%20the%20performance%20of%20many%20models%20by%20leading%20to%20loss%20of%20non-stationarity%20and%20resulting%20in%20over-stationarization%20%28Liu%2C%20Wu%2C%20Wang%20and%20Long%2C%202022%29.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20architecture%2C%20ChoronoAdaptive%20Network%20%28CANet%29%2C%20inspired%20by%20style-transfer%20techniques.%20The%20core%20of%20CANet%20is%20the%20Non-stationary%20Adaptive%20Normalization%20module%2C%20seamlessly%20integrating%20the%20Style%20Blending%20Gate%20and%20Adaptive%20Instance%20Normalization%20%28AdaIN%29%20%28Huang%20and%20Belongie%2C%202017%29.%20The%20Style%20Blending%20Gate%20preserves%20and%20reintegrates%20non-stationary%20characteristics%2C%20such%20as%20mean%20and%20standard%20deviation%2C%20by%20blending%20internal%20and%20external%20statistics%2C%20preventing%20over-stationarization%20while%20maintaining%20essential%20temporal%20dependencies.%20Coupled%20with%20AdaIN%2C%20which%20dynamically%20adapts%20the%20model%20to%20statistical%20changes%2C%20this%20approach%20enhances%20predictive%20accuracy%20under%20non-stationary%20conditions.%20CANet%20also%20employs%20multi-resolution%20patching%20to%20handle%20short-term%20fluctuations%20and%20long-term%20trends%2C%20along%20with%20Fourier%20analysis-based%20adaptive%20thresholding%20to%20reduce%20noise.%20A%20Stacked%20Kronecker%20Product%20Layer%20further%20optimizes%20the%20model%27s%20efficiency%20while%20maintaining%20high%20performance.%20Extensive%20experiments%20on%20real-world%20datasets%20validate%20CANet%27s%20superiority%20over%20state-of-the-art%20methods%2C%20achieving%20a%2042%25%20reduction%20in%20MSE%20and%20a%2022%25%20reduction%20in%20MAE.%20The%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/mertsonmezer/CANet.&entry.1838667208=http%3A//arxiv.org/abs/2504.17913v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


