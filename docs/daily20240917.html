<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240916.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering", "author": "Euntae Choi and Sungjoo Yoo", "abstract": "  We propose two novel ideas (adoption of deferred rendering and mesh-based\nrepresentation) to improve the quality of 3D Gaussian splatting (3DGS) based\ninverse rendering. We first report a problem incurred by hidden Gaussians,\nwhere Gaussians beneath the surface adversely affect the pixel color in the\nvolume rendering adopted by the existing methods. In order to resolve the\nproblem, we propose applying deferred rendering and report new problems\nincurred in a naive application of deferred rendering to the existing\n3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based\ninverse rendering under deferred rendering, we propose a novel two-step\ntraining approach which (1) exploits mesh extraction and utilizes a hybrid\nmesh-3DGS representation and (2) applies novel regularization methods to better\nexploit the mesh. Our experiments show that, under relighting, the proposed\nmethod offers significantly better rendering quality than the existing\n3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based\ninverse rendering method, it gives better rendering quality while offering\nreal-time rendering.\n", "link": "http://arxiv.org/abs/2409.10335v1", "date": "2024-09-16", "relevancy": 3.3475, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6837}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6797}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phys3DGS%3A%20Physically-based%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering&body=Title%3A%20Phys3DGS%3A%20Physically-based%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%0AAuthor%3A%20Euntae%20Choi%20and%20Sungjoo%20Yoo%0AAbstract%3A%20%20%20We%20propose%20two%20novel%20ideas%20%28adoption%20of%20deferred%20rendering%20and%20mesh-based%0Arepresentation%29%20to%20improve%20the%20quality%20of%203D%20Gaussian%20splatting%20%283DGS%29%20based%0Ainverse%20rendering.%20We%20first%20report%20a%20problem%20incurred%20by%20hidden%20Gaussians%2C%0Awhere%20Gaussians%20beneath%20the%20surface%20adversely%20affect%20the%20pixel%20color%20in%20the%0Avolume%20rendering%20adopted%20by%20the%20existing%20methods.%20In%20order%20to%20resolve%20the%0Aproblem%2C%20we%20propose%20applying%20deferred%20rendering%20and%20report%20new%20problems%0Aincurred%20in%20a%20naive%20application%20of%20deferred%20rendering%20to%20the%20existing%0A3DGS-based%20inverse%20rendering.%20In%20an%20effort%20to%20improve%20the%20quality%20of%203DGS-based%0Ainverse%20rendering%20under%20deferred%20rendering%2C%20we%20propose%20a%20novel%20two-step%0Atraining%20approach%20which%20%281%29%20exploits%20mesh%20extraction%20and%20utilizes%20a%20hybrid%0Amesh-3DGS%20representation%20and%20%282%29%20applies%20novel%20regularization%20methods%20to%20better%0Aexploit%20the%20mesh.%20Our%20experiments%20show%20that%2C%20under%20relighting%2C%20the%20proposed%0Amethod%20offers%20significantly%20better%20rendering%20quality%20than%20the%20existing%0A3DGS-based%20inverse%20rendering%20methods.%20Compared%20with%20the%20SOTA%20voxel%20grid-based%0Ainverse%20rendering%20method%2C%20it%20gives%20better%20rendering%20quality%20while%20offering%0Areal-time%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhys3DGS%253A%2520Physically-based%25203D%2520Gaussian%2520Splatting%2520for%2520Inverse%2520Rendering%26entry.906535625%3DEuntae%2520Choi%2520and%2520Sungjoo%2520Yoo%26entry.1292438233%3D%2520%2520We%2520propose%2520two%2520novel%2520ideas%2520%2528adoption%2520of%2520deferred%2520rendering%2520and%2520mesh-based%250Arepresentation%2529%2520to%2520improve%2520the%2520quality%2520of%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520based%250Ainverse%2520rendering.%2520We%2520first%2520report%2520a%2520problem%2520incurred%2520by%2520hidden%2520Gaussians%252C%250Awhere%2520Gaussians%2520beneath%2520the%2520surface%2520adversely%2520affect%2520the%2520pixel%2520color%2520in%2520the%250Avolume%2520rendering%2520adopted%2520by%2520the%2520existing%2520methods.%2520In%2520order%2520to%2520resolve%2520the%250Aproblem%252C%2520we%2520propose%2520applying%2520deferred%2520rendering%2520and%2520report%2520new%2520problems%250Aincurred%2520in%2520a%2520naive%2520application%2520of%2520deferred%2520rendering%2520to%2520the%2520existing%250A3DGS-based%2520inverse%2520rendering.%2520In%2520an%2520effort%2520to%2520improve%2520the%2520quality%2520of%25203DGS-based%250Ainverse%2520rendering%2520under%2520deferred%2520rendering%252C%2520we%2520propose%2520a%2520novel%2520two-step%250Atraining%2520approach%2520which%2520%25281%2529%2520exploits%2520mesh%2520extraction%2520and%2520utilizes%2520a%2520hybrid%250Amesh-3DGS%2520representation%2520and%2520%25282%2529%2520applies%2520novel%2520regularization%2520methods%2520to%2520better%250Aexploit%2520the%2520mesh.%2520Our%2520experiments%2520show%2520that%252C%2520under%2520relighting%252C%2520the%2520proposed%250Amethod%2520offers%2520significantly%2520better%2520rendering%2520quality%2520than%2520the%2520existing%250A3DGS-based%2520inverse%2520rendering%2520methods.%2520Compared%2520with%2520the%2520SOTA%2520voxel%2520grid-based%250Ainverse%2520rendering%2520method%252C%2520it%2520gives%2520better%2520rendering%2520quality%2520while%2520offering%250Areal-time%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phys3DGS%3A%20Physically-based%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering&entry.906535625=Euntae%20Choi%20and%20Sungjoo%20Yoo&entry.1292438233=%20%20We%20propose%20two%20novel%20ideas%20%28adoption%20of%20deferred%20rendering%20and%20mesh-based%0Arepresentation%29%20to%20improve%20the%20quality%20of%203D%20Gaussian%20splatting%20%283DGS%29%20based%0Ainverse%20rendering.%20We%20first%20report%20a%20problem%20incurred%20by%20hidden%20Gaussians%2C%0Awhere%20Gaussians%20beneath%20the%20surface%20adversely%20affect%20the%20pixel%20color%20in%20the%0Avolume%20rendering%20adopted%20by%20the%20existing%20methods.%20In%20order%20to%20resolve%20the%0Aproblem%2C%20we%20propose%20applying%20deferred%20rendering%20and%20report%20new%20problems%0Aincurred%20in%20a%20naive%20application%20of%20deferred%20rendering%20to%20the%20existing%0A3DGS-based%20inverse%20rendering.%20In%20an%20effort%20to%20improve%20the%20quality%20of%203DGS-based%0Ainverse%20rendering%20under%20deferred%20rendering%2C%20we%20propose%20a%20novel%20two-step%0Atraining%20approach%20which%20%281%29%20exploits%20mesh%20extraction%20and%20utilizes%20a%20hybrid%0Amesh-3DGS%20representation%20and%20%282%29%20applies%20novel%20regularization%20methods%20to%20better%0Aexploit%20the%20mesh.%20Our%20experiments%20show%20that%2C%20under%20relighting%2C%20the%20proposed%0Amethod%20offers%20significantly%20better%20rendering%20quality%20than%20the%20existing%0A3DGS-based%20inverse%20rendering%20methods.%20Compared%20with%20the%20SOTA%20voxel%20grid-based%0Ainverse%20rendering%20method%2C%20it%20gives%20better%20rendering%20quality%20while%20offering%0Areal-time%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10335v1&entry.124074799=Read"},
{"title": "PSHuman: Photorealistic Single-view Human Reconstruction using\n  Cross-Scale Diffusion", "author": "Peng Li and Wangguandong Zheng and Yuan Liu and Tao Yu and Yangguang Li and Xingqun Qi and Mengfei Li and Xiaowei Chi and Siyu Xia and Wei Xue and Wenhan Luo and Qifeng Liu and Yike Guo", "abstract": "  Detailed and photorealistic 3D human modeling is essential for various\napplications and has seen tremendous progress. However, full-body\nreconstruction from a monocular RGB image remains challenging due to the\nill-posed nature of the problem and sophisticated clothing topology with\nself-occlusions. In this paper, we propose PSHuman, a novel framework that\nexplicitly reconstructs human meshes utilizing priors from the multiview\ndiffusion model. It is found that directly applying multiview diffusion on\nsingle-view human images leads to severe geometric distortions, especially on\ngenerated faces. To address it, we propose a cross-scale diffusion that models\nthe joint probability distribution of global full-body shape and local facial\ncharacteristics, enabling detailed and identity-preserved novel-view generation\nwithout any geometric distortion. Moreover, to enhance cross-view body shape\nconsistency of varied human poses, we condition the generative model on\nparametric models like SMPL-X, which provide body priors and prevent unnatural\nviews inconsistent with human anatomy. Leveraging the generated multi-view\nnormal and color images, we present SMPLX-initialized explicit human carving to\nrecover realistic textured human meshes efficiently. Extensive experimental\nresults and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate\nPSHumans superiority in geometry details, texture fidelity, and generalization\ncapability.\n", "link": "http://arxiv.org/abs/2409.10141v1", "date": "2024-09-16", "relevancy": 3.2742, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6801}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSHuman%3A%20Photorealistic%20Single-view%20Human%20Reconstruction%20using%0A%20%20Cross-Scale%20Diffusion&body=Title%3A%20PSHuman%3A%20Photorealistic%20Single-view%20Human%20Reconstruction%20using%0A%20%20Cross-Scale%20Diffusion%0AAuthor%3A%20Peng%20Li%20and%20Wangguandong%20Zheng%20and%20Yuan%20Liu%20and%20Tao%20Yu%20and%20Yangguang%20Li%20and%20Xingqun%20Qi%20and%20Mengfei%20Li%20and%20Xiaowei%20Chi%20and%20Siyu%20Xia%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qifeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Detailed%20and%20photorealistic%203D%20human%20modeling%20is%20essential%20for%20various%0Aapplications%20and%20has%20seen%20tremendous%20progress.%20However%2C%20full-body%0Areconstruction%20from%20a%20monocular%20RGB%20image%20remains%20challenging%20due%20to%20the%0Aill-posed%20nature%20of%20the%20problem%20and%20sophisticated%20clothing%20topology%20with%0Aself-occlusions.%20In%20this%20paper%2C%20we%20propose%20PSHuman%2C%20a%20novel%20framework%20that%0Aexplicitly%20reconstructs%20human%20meshes%20utilizing%20priors%20from%20the%20multiview%0Adiffusion%20model.%20It%20is%20found%20that%20directly%20applying%20multiview%20diffusion%20on%0Asingle-view%20human%20images%20leads%20to%20severe%20geometric%20distortions%2C%20especially%20on%0Agenerated%20faces.%20To%20address%20it%2C%20we%20propose%20a%20cross-scale%20diffusion%20that%20models%0Athe%20joint%20probability%20distribution%20of%20global%20full-body%20shape%20and%20local%20facial%0Acharacteristics%2C%20enabling%20detailed%20and%20identity-preserved%20novel-view%20generation%0Awithout%20any%20geometric%20distortion.%20Moreover%2C%20to%20enhance%20cross-view%20body%20shape%0Aconsistency%20of%20varied%20human%20poses%2C%20we%20condition%20the%20generative%20model%20on%0Aparametric%20models%20like%20SMPL-X%2C%20which%20provide%20body%20priors%20and%20prevent%20unnatural%0Aviews%20inconsistent%20with%20human%20anatomy.%20Leveraging%20the%20generated%20multi-view%0Anormal%20and%20color%20images%2C%20we%20present%20SMPLX-initialized%20explicit%20human%20carving%20to%0Arecover%20realistic%20textured%20human%20meshes%20efficiently.%20Extensive%20experimental%0Aresults%20and%20quantitative%20evaluations%20on%20CAPE%20and%20THuman2.1%20datasets%20demonstrate%0APSHumans%20superiority%20in%20geometry%20details%2C%20texture%20fidelity%2C%20and%20generalization%0Acapability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSHuman%253A%2520Photorealistic%2520Single-view%2520Human%2520Reconstruction%2520using%250A%2520%2520Cross-Scale%2520Diffusion%26entry.906535625%3DPeng%2520Li%2520and%2520Wangguandong%2520Zheng%2520and%2520Yuan%2520Liu%2520and%2520Tao%2520Yu%2520and%2520Yangguang%2520Li%2520and%2520Xingqun%2520Qi%2520and%2520Mengfei%2520Li%2520and%2520Xiaowei%2520Chi%2520and%2520Siyu%2520Xia%2520and%2520Wei%2520Xue%2520and%2520Wenhan%2520Luo%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Detailed%2520and%2520photorealistic%25203D%2520human%2520modeling%2520is%2520essential%2520for%2520various%250Aapplications%2520and%2520has%2520seen%2520tremendous%2520progress.%2520However%252C%2520full-body%250Areconstruction%2520from%2520a%2520monocular%2520RGB%2520image%2520remains%2520challenging%2520due%2520to%2520the%250Aill-posed%2520nature%2520of%2520the%2520problem%2520and%2520sophisticated%2520clothing%2520topology%2520with%250Aself-occlusions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PSHuman%252C%2520a%2520novel%2520framework%2520that%250Aexplicitly%2520reconstructs%2520human%2520meshes%2520utilizing%2520priors%2520from%2520the%2520multiview%250Adiffusion%2520model.%2520It%2520is%2520found%2520that%2520directly%2520applying%2520multiview%2520diffusion%2520on%250Asingle-view%2520human%2520images%2520leads%2520to%2520severe%2520geometric%2520distortions%252C%2520especially%2520on%250Agenerated%2520faces.%2520To%2520address%2520it%252C%2520we%2520propose%2520a%2520cross-scale%2520diffusion%2520that%2520models%250Athe%2520joint%2520probability%2520distribution%2520of%2520global%2520full-body%2520shape%2520and%2520local%2520facial%250Acharacteristics%252C%2520enabling%2520detailed%2520and%2520identity-preserved%2520novel-view%2520generation%250Awithout%2520any%2520geometric%2520distortion.%2520Moreover%252C%2520to%2520enhance%2520cross-view%2520body%2520shape%250Aconsistency%2520of%2520varied%2520human%2520poses%252C%2520we%2520condition%2520the%2520generative%2520model%2520on%250Aparametric%2520models%2520like%2520SMPL-X%252C%2520which%2520provide%2520body%2520priors%2520and%2520prevent%2520unnatural%250Aviews%2520inconsistent%2520with%2520human%2520anatomy.%2520Leveraging%2520the%2520generated%2520multi-view%250Anormal%2520and%2520color%2520images%252C%2520we%2520present%2520SMPLX-initialized%2520explicit%2520human%2520carving%2520to%250Arecover%2520realistic%2520textured%2520human%2520meshes%2520efficiently.%2520Extensive%2520experimental%250Aresults%2520and%2520quantitative%2520evaluations%2520on%2520CAPE%2520and%2520THuman2.1%2520datasets%2520demonstrate%250APSHumans%2520superiority%2520in%2520geometry%2520details%252C%2520texture%2520fidelity%252C%2520and%2520generalization%250Acapability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSHuman%3A%20Photorealistic%20Single-view%20Human%20Reconstruction%20using%0A%20%20Cross-Scale%20Diffusion&entry.906535625=Peng%20Li%20and%20Wangguandong%20Zheng%20and%20Yuan%20Liu%20and%20Tao%20Yu%20and%20Yangguang%20Li%20and%20Xingqun%20Qi%20and%20Mengfei%20Li%20and%20Xiaowei%20Chi%20and%20Siyu%20Xia%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Qifeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20Detailed%20and%20photorealistic%203D%20human%20modeling%20is%20essential%20for%20various%0Aapplications%20and%20has%20seen%20tremendous%20progress.%20However%2C%20full-body%0Areconstruction%20from%20a%20monocular%20RGB%20image%20remains%20challenging%20due%20to%20the%0Aill-posed%20nature%20of%20the%20problem%20and%20sophisticated%20clothing%20topology%20with%0Aself-occlusions.%20In%20this%20paper%2C%20we%20propose%20PSHuman%2C%20a%20novel%20framework%20that%0Aexplicitly%20reconstructs%20human%20meshes%20utilizing%20priors%20from%20the%20multiview%0Adiffusion%20model.%20It%20is%20found%20that%20directly%20applying%20multiview%20diffusion%20on%0Asingle-view%20human%20images%20leads%20to%20severe%20geometric%20distortions%2C%20especially%20on%0Agenerated%20faces.%20To%20address%20it%2C%20we%20propose%20a%20cross-scale%20diffusion%20that%20models%0Athe%20joint%20probability%20distribution%20of%20global%20full-body%20shape%20and%20local%20facial%0Acharacteristics%2C%20enabling%20detailed%20and%20identity-preserved%20novel-view%20generation%0Awithout%20any%20geometric%20distortion.%20Moreover%2C%20to%20enhance%20cross-view%20body%20shape%0Aconsistency%20of%20varied%20human%20poses%2C%20we%20condition%20the%20generative%20model%20on%0Aparametric%20models%20like%20SMPL-X%2C%20which%20provide%20body%20priors%20and%20prevent%20unnatural%0Aviews%20inconsistent%20with%20human%20anatomy.%20Leveraging%20the%20generated%20multi-view%0Anormal%20and%20color%20images%2C%20we%20present%20SMPLX-initialized%20explicit%20human%20carving%20to%0Arecover%20realistic%20textured%20human%20meshes%20efficiently.%20Extensive%20experimental%0Aresults%20and%20quantitative%20evaluations%20on%20CAPE%20and%20THuman2.1%20datasets%20demonstrate%0APSHumans%20superiority%20in%20geometry%20details%2C%20texture%20fidelity%2C%20and%20generalization%0Acapability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10141v1&entry.124074799=Read"},
{"title": "Do Pre-trained Vision-Language Models Encode Object States?", "author": "Kaleb Newman and Shijie Wang and Yuan Zang and David Heffren and Chen Sun", "abstract": "  For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.\n", "link": "http://arxiv.org/abs/2409.10488v1", "date": "2024-09-16", "relevancy": 2.9919, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Pre-trained%20Vision-Language%20Models%20Encode%20Object%20States%3F&body=Title%3A%20Do%20Pre-trained%20Vision-Language%20Models%20Encode%20Object%20States%3F%0AAuthor%3A%20Kaleb%20Newman%20and%20Shijie%20Wang%20and%20Yuan%20Zang%20and%20David%20Heffren%20and%20Chen%20Sun%0AAbstract%3A%20%20%20For%20a%20vision-language%20model%20%28VLM%29%20to%20understand%20the%20physical%20world%2C%20such%20as%0Acause%20and%20effect%2C%20a%20first%20step%20is%20to%20capture%20the%20temporal%20dynamics%20of%20the%0Avisual%20world%2C%20for%20example%20how%20the%20physical%20states%20of%20objects%20evolve%20over%20time%0A%28e.g.%20a%20whole%20apple%20into%20a%20sliced%20apple%29.%20Our%20paper%20aims%20to%20investigate%20if%20VLMs%0Apre-trained%20on%20web-scale%20data%20learn%20to%20encode%20object%20states%2C%20which%20can%20be%0Aextracted%20with%20zero-shot%20text%20prompts.%20We%20curate%20an%20object%20state%20recognition%0Adataset%20ChangeIt-Frames%2C%20and%20evaluate%20nine%20open-source%20VLMs%2C%20including%20models%0Atrained%20with%20contrastive%20and%20generative%20objectives.%20We%20observe%20that%20while%20these%0Astate-of-the-art%20vision-language%20models%20can%20reliably%20perform%20object%0Arecognition%2C%20they%20consistently%20fail%20to%20accurately%20distinguish%20the%20objects%27%0Aphysical%20states.%20Through%20extensive%20experiments%2C%20we%20identify%20three%20areas%20for%0Aimprovements%20for%20VLMs%20to%20better%20encode%20object%20states%2C%20namely%20the%20quality%20of%0Aobject%20localization%2C%20the%20architecture%20to%20bind%20concepts%20to%20objects%2C%20and%20the%0Aobjective%20to%20learn%20discriminative%20visual%20and%20language%20encoders%20on%20object%0Astates.%20Data%20and%20code%20are%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Pre-trained%2520Vision-Language%2520Models%2520Encode%2520Object%2520States%253F%26entry.906535625%3DKaleb%2520Newman%2520and%2520Shijie%2520Wang%2520and%2520Yuan%2520Zang%2520and%2520David%2520Heffren%2520and%2520Chen%2520Sun%26entry.1292438233%3D%2520%2520For%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%2520understand%2520the%2520physical%2520world%252C%2520such%2520as%250Acause%2520and%2520effect%252C%2520a%2520first%2520step%2520is%2520to%2520capture%2520the%2520temporal%2520dynamics%2520of%2520the%250Avisual%2520world%252C%2520for%2520example%2520how%2520the%2520physical%2520states%2520of%2520objects%2520evolve%2520over%2520time%250A%2528e.g.%2520a%2520whole%2520apple%2520into%2520a%2520sliced%2520apple%2529.%2520Our%2520paper%2520aims%2520to%2520investigate%2520if%2520VLMs%250Apre-trained%2520on%2520web-scale%2520data%2520learn%2520to%2520encode%2520object%2520states%252C%2520which%2520can%2520be%250Aextracted%2520with%2520zero-shot%2520text%2520prompts.%2520We%2520curate%2520an%2520object%2520state%2520recognition%250Adataset%2520ChangeIt-Frames%252C%2520and%2520evaluate%2520nine%2520open-source%2520VLMs%252C%2520including%2520models%250Atrained%2520with%2520contrastive%2520and%2520generative%2520objectives.%2520We%2520observe%2520that%2520while%2520these%250Astate-of-the-art%2520vision-language%2520models%2520can%2520reliably%2520perform%2520object%250Arecognition%252C%2520they%2520consistently%2520fail%2520to%2520accurately%2520distinguish%2520the%2520objects%2527%250Aphysical%2520states.%2520Through%2520extensive%2520experiments%252C%2520we%2520identify%2520three%2520areas%2520for%250Aimprovements%2520for%2520VLMs%2520to%2520better%2520encode%2520object%2520states%252C%2520namely%2520the%2520quality%2520of%250Aobject%2520localization%252C%2520the%2520architecture%2520to%2520bind%2520concepts%2520to%2520objects%252C%2520and%2520the%250Aobjective%2520to%2520learn%2520discriminative%2520visual%2520and%2520language%2520encoders%2520on%2520object%250Astates.%2520Data%2520and%2520code%2520are%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Pre-trained%20Vision-Language%20Models%20Encode%20Object%20States%3F&entry.906535625=Kaleb%20Newman%20and%20Shijie%20Wang%20and%20Yuan%20Zang%20and%20David%20Heffren%20and%20Chen%20Sun&entry.1292438233=%20%20For%20a%20vision-language%20model%20%28VLM%29%20to%20understand%20the%20physical%20world%2C%20such%20as%0Acause%20and%20effect%2C%20a%20first%20step%20is%20to%20capture%20the%20temporal%20dynamics%20of%20the%0Avisual%20world%2C%20for%20example%20how%20the%20physical%20states%20of%20objects%20evolve%20over%20time%0A%28e.g.%20a%20whole%20apple%20into%20a%20sliced%20apple%29.%20Our%20paper%20aims%20to%20investigate%20if%20VLMs%0Apre-trained%20on%20web-scale%20data%20learn%20to%20encode%20object%20states%2C%20which%20can%20be%0Aextracted%20with%20zero-shot%20text%20prompts.%20We%20curate%20an%20object%20state%20recognition%0Adataset%20ChangeIt-Frames%2C%20and%20evaluate%20nine%20open-source%20VLMs%2C%20including%20models%0Atrained%20with%20contrastive%20and%20generative%20objectives.%20We%20observe%20that%20while%20these%0Astate-of-the-art%20vision-language%20models%20can%20reliably%20perform%20object%0Arecognition%2C%20they%20consistently%20fail%20to%20accurately%20distinguish%20the%20objects%27%0Aphysical%20states.%20Through%20extensive%20experiments%2C%20we%20identify%20three%20areas%20for%0Aimprovements%20for%20VLMs%20to%20better%20encode%20object%20states%2C%20namely%20the%20quality%20of%0Aobject%20localization%2C%20the%20architecture%20to%20bind%20concepts%20to%20objects%2C%20and%20the%0Aobjective%20to%20learn%20discriminative%20visual%20and%20language%20encoders%20on%20object%0Astates.%20Data%20and%20code%20are%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10488v1&entry.124074799=Read"},
{"title": "SOLVR: Submap Oriented LiDAR-Visual Re-Localisation", "author": "Joshua Knights and Sebasti\u00e1n Barbas Laina and Peyman Moghadam and Stefan Leutenegger", "abstract": "  This paper proposes SOLVR, a unified pipeline for learning based LiDAR-Visual\nre-localisation which performs place recognition and 6-DoF registration across\nsensor modalities. We propose a strategy to align the input sensor modalities\nby leveraging stereo image streams to produce metric depth predictions with\npose information, followed by fusing multiple scene views from a local window\nusing a probabilistic occupancy framework to expand the limited field-of-view\nof the camera. Additionally, SOLVR adopts a flexible definition of what\nconstitutes positive examples for different training losses, allowing us to\nsimultaneously optimise place recognition and registration performance.\nFurthermore, we replace RANSAC with a registration function that weights a\nsimple least-squares fitting with the estimated inlier likelihood of sparse\nkeypoint correspondences, improving performance in scenarios with a low inlier\nratio between the query and retrieved place. Our experiments on the KITTI and\nKITTI360 datasets show that SOLVR achieves state-of-the-art performance for\nLiDAR-Visual place recognition and registration, particularly improving\nregistration accuracy over larger distances between the query and retrieved\nplace.\n", "link": "http://arxiv.org/abs/2409.10247v1", "date": "2024-09-16", "relevancy": 2.9914, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6125}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6124}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOLVR%3A%20Submap%20Oriented%20LiDAR-Visual%20Re-Localisation&body=Title%3A%20SOLVR%3A%20Submap%20Oriented%20LiDAR-Visual%20Re-Localisation%0AAuthor%3A%20Joshua%20Knights%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Peyman%20Moghadam%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20This%20paper%20proposes%20SOLVR%2C%20a%20unified%20pipeline%20for%20learning%20based%20LiDAR-Visual%0Are-localisation%20which%20performs%20place%20recognition%20and%206-DoF%20registration%20across%0Asensor%20modalities.%20We%20propose%20a%20strategy%20to%20align%20the%20input%20sensor%20modalities%0Aby%20leveraging%20stereo%20image%20streams%20to%20produce%20metric%20depth%20predictions%20with%0Apose%20information%2C%20followed%20by%20fusing%20multiple%20scene%20views%20from%20a%20local%20window%0Ausing%20a%20probabilistic%20occupancy%20framework%20to%20expand%20the%20limited%20field-of-view%0Aof%20the%20camera.%20Additionally%2C%20SOLVR%20adopts%20a%20flexible%20definition%20of%20what%0Aconstitutes%20positive%20examples%20for%20different%20training%20losses%2C%20allowing%20us%20to%0Asimultaneously%20optimise%20place%20recognition%20and%20registration%20performance.%0AFurthermore%2C%20we%20replace%20RANSAC%20with%20a%20registration%20function%20that%20weights%20a%0Asimple%20least-squares%20fitting%20with%20the%20estimated%20inlier%20likelihood%20of%20sparse%0Akeypoint%20correspondences%2C%20improving%20performance%20in%20scenarios%20with%20a%20low%20inlier%0Aratio%20between%20the%20query%20and%20retrieved%20place.%20Our%20experiments%20on%20the%20KITTI%20and%0AKITTI360%20datasets%20show%20that%20SOLVR%20achieves%20state-of-the-art%20performance%20for%0ALiDAR-Visual%20place%20recognition%20and%20registration%2C%20particularly%20improving%0Aregistration%20accuracy%20over%20larger%20distances%20between%20the%20query%20and%20retrieved%0Aplace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOLVR%253A%2520Submap%2520Oriented%2520LiDAR-Visual%2520Re-Localisation%26entry.906535625%3DJoshua%2520Knights%2520and%2520Sebasti%25C3%25A1n%2520Barbas%2520Laina%2520and%2520Peyman%2520Moghadam%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520SOLVR%252C%2520a%2520unified%2520pipeline%2520for%2520learning%2520based%2520LiDAR-Visual%250Are-localisation%2520which%2520performs%2520place%2520recognition%2520and%25206-DoF%2520registration%2520across%250Asensor%2520modalities.%2520We%2520propose%2520a%2520strategy%2520to%2520align%2520the%2520input%2520sensor%2520modalities%250Aby%2520leveraging%2520stereo%2520image%2520streams%2520to%2520produce%2520metric%2520depth%2520predictions%2520with%250Apose%2520information%252C%2520followed%2520by%2520fusing%2520multiple%2520scene%2520views%2520from%2520a%2520local%2520window%250Ausing%2520a%2520probabilistic%2520occupancy%2520framework%2520to%2520expand%2520the%2520limited%2520field-of-view%250Aof%2520the%2520camera.%2520Additionally%252C%2520SOLVR%2520adopts%2520a%2520flexible%2520definition%2520of%2520what%250Aconstitutes%2520positive%2520examples%2520for%2520different%2520training%2520losses%252C%2520allowing%2520us%2520to%250Asimultaneously%2520optimise%2520place%2520recognition%2520and%2520registration%2520performance.%250AFurthermore%252C%2520we%2520replace%2520RANSAC%2520with%2520a%2520registration%2520function%2520that%2520weights%2520a%250Asimple%2520least-squares%2520fitting%2520with%2520the%2520estimated%2520inlier%2520likelihood%2520of%2520sparse%250Akeypoint%2520correspondences%252C%2520improving%2520performance%2520in%2520scenarios%2520with%2520a%2520low%2520inlier%250Aratio%2520between%2520the%2520query%2520and%2520retrieved%2520place.%2520Our%2520experiments%2520on%2520the%2520KITTI%2520and%250AKITTI360%2520datasets%2520show%2520that%2520SOLVR%2520achieves%2520state-of-the-art%2520performance%2520for%250ALiDAR-Visual%2520place%2520recognition%2520and%2520registration%252C%2520particularly%2520improving%250Aregistration%2520accuracy%2520over%2520larger%2520distances%2520between%2520the%2520query%2520and%2520retrieved%250Aplace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOLVR%3A%20Submap%20Oriented%20LiDAR-Visual%20Re-Localisation&entry.906535625=Joshua%20Knights%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Peyman%20Moghadam%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20This%20paper%20proposes%20SOLVR%2C%20a%20unified%20pipeline%20for%20learning%20based%20LiDAR-Visual%0Are-localisation%20which%20performs%20place%20recognition%20and%206-DoF%20registration%20across%0Asensor%20modalities.%20We%20propose%20a%20strategy%20to%20align%20the%20input%20sensor%20modalities%0Aby%20leveraging%20stereo%20image%20streams%20to%20produce%20metric%20depth%20predictions%20with%0Apose%20information%2C%20followed%20by%20fusing%20multiple%20scene%20views%20from%20a%20local%20window%0Ausing%20a%20probabilistic%20occupancy%20framework%20to%20expand%20the%20limited%20field-of-view%0Aof%20the%20camera.%20Additionally%2C%20SOLVR%20adopts%20a%20flexible%20definition%20of%20what%0Aconstitutes%20positive%20examples%20for%20different%20training%20losses%2C%20allowing%20us%20to%0Asimultaneously%20optimise%20place%20recognition%20and%20registration%20performance.%0AFurthermore%2C%20we%20replace%20RANSAC%20with%20a%20registration%20function%20that%20weights%20a%0Asimple%20least-squares%20fitting%20with%20the%20estimated%20inlier%20likelihood%20of%20sparse%0Akeypoint%20correspondences%2C%20improving%20performance%20in%20scenarios%20with%20a%20low%20inlier%0Aratio%20between%20the%20query%20and%20retrieved%20place.%20Our%20experiments%20on%20the%20KITTI%20and%0AKITTI360%20datasets%20show%20that%20SOLVR%20achieves%20state-of-the-art%20performance%20for%0ALiDAR-Visual%20place%20recognition%20and%20registration%2C%20particularly%20improving%0Aregistration%20accuracy%20over%20larger%20distances%20between%20the%20query%20and%20retrieved%0Aplace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10247v1&entry.124074799=Read"},
{"title": "Robust Bird's Eye View Segmentation by Adapting DINOv2", "author": "Merve Rabia Bar\u0131n and G\u00f6rkay Aydemir and Fatma G\u00fcney", "abstract": "  Extracting a Bird's Eye View (BEV) representation from multiple camera images\noffers a cost-effective, scalable alternative to LIDAR-based solutions in\nautonomous driving. However, the performance of the existing BEV methods drops\nsignificantly under various corruptions such as brightness and weather changes\nor camera failures. To improve the robustness of BEV perception, we propose to\nadapt a large vision foundational model, DINOv2, to BEV estimation using Low\nRank Adaptation (LoRA). Our approach builds on the strong representation space\nof DINOv2 by adapting it to the BEV task in a state-of-the-art framework,\nSimpleBEV. Our experiments show increased robustness of BEV perception under\nvarious corruptions, with increasing gains from scaling up the model and the\ninput resolution. We also showcase the effectiveness of the adapted\nrepresentations in terms of fewer learnable parameters and faster convergence\nduring training.\n", "link": "http://arxiv.org/abs/2409.10228v1", "date": "2024-09-16", "relevancy": 2.9471, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Bird%27s%20Eye%20View%20Segmentation%20by%20Adapting%20DINOv2&body=Title%3A%20Robust%20Bird%27s%20Eye%20View%20Segmentation%20by%20Adapting%20DINOv2%0AAuthor%3A%20Merve%20Rabia%20Bar%C4%B1n%20and%20G%C3%B6rkay%20Aydemir%20and%20Fatma%20G%C3%BCney%0AAbstract%3A%20%20%20Extracting%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20from%20multiple%20camera%20images%0Aoffers%20a%20cost-effective%2C%20scalable%20alternative%20to%20LIDAR-based%20solutions%20in%0Aautonomous%20driving.%20However%2C%20the%20performance%20of%20the%20existing%20BEV%20methods%20drops%0Asignificantly%20under%20various%20corruptions%20such%20as%20brightness%20and%20weather%20changes%0Aor%20camera%20failures.%20To%20improve%20the%20robustness%20of%20BEV%20perception%2C%20we%20propose%20to%0Aadapt%20a%20large%20vision%20foundational%20model%2C%20DINOv2%2C%20to%20BEV%20estimation%20using%20Low%0ARank%20Adaptation%20%28LoRA%29.%20Our%20approach%20builds%20on%20the%20strong%20representation%20space%0Aof%20DINOv2%20by%20adapting%20it%20to%20the%20BEV%20task%20in%20a%20state-of-the-art%20framework%2C%0ASimpleBEV.%20Our%20experiments%20show%20increased%20robustness%20of%20BEV%20perception%20under%0Avarious%20corruptions%2C%20with%20increasing%20gains%20from%20scaling%20up%20the%20model%20and%20the%0Ainput%20resolution.%20We%20also%20showcase%20the%20effectiveness%20of%20the%20adapted%0Arepresentations%20in%20terms%20of%20fewer%20learnable%20parameters%20and%20faster%20convergence%0Aduring%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Bird%2527s%2520Eye%2520View%2520Segmentation%2520by%2520Adapting%2520DINOv2%26entry.906535625%3DMerve%2520Rabia%2520Bar%25C4%25B1n%2520and%2520G%25C3%25B6rkay%2520Aydemir%2520and%2520Fatma%2520G%25C3%25BCney%26entry.1292438233%3D%2520%2520Extracting%2520a%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520representation%2520from%2520multiple%2520camera%2520images%250Aoffers%2520a%2520cost-effective%252C%2520scalable%2520alternative%2520to%2520LIDAR-based%2520solutions%2520in%250Aautonomous%2520driving.%2520However%252C%2520the%2520performance%2520of%2520the%2520existing%2520BEV%2520methods%2520drops%250Asignificantly%2520under%2520various%2520corruptions%2520such%2520as%2520brightness%2520and%2520weather%2520changes%250Aor%2520camera%2520failures.%2520To%2520improve%2520the%2520robustness%2520of%2520BEV%2520perception%252C%2520we%2520propose%2520to%250Aadapt%2520a%2520large%2520vision%2520foundational%2520model%252C%2520DINOv2%252C%2520to%2520BEV%2520estimation%2520using%2520Low%250ARank%2520Adaptation%2520%2528LoRA%2529.%2520Our%2520approach%2520builds%2520on%2520the%2520strong%2520representation%2520space%250Aof%2520DINOv2%2520by%2520adapting%2520it%2520to%2520the%2520BEV%2520task%2520in%2520a%2520state-of-the-art%2520framework%252C%250ASimpleBEV.%2520Our%2520experiments%2520show%2520increased%2520robustness%2520of%2520BEV%2520perception%2520under%250Avarious%2520corruptions%252C%2520with%2520increasing%2520gains%2520from%2520scaling%2520up%2520the%2520model%2520and%2520the%250Ainput%2520resolution.%2520We%2520also%2520showcase%2520the%2520effectiveness%2520of%2520the%2520adapted%250Arepresentations%2520in%2520terms%2520of%2520fewer%2520learnable%2520parameters%2520and%2520faster%2520convergence%250Aduring%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Bird%27s%20Eye%20View%20Segmentation%20by%20Adapting%20DINOv2&entry.906535625=Merve%20Rabia%20Bar%C4%B1n%20and%20G%C3%B6rkay%20Aydemir%20and%20Fatma%20G%C3%BCney&entry.1292438233=%20%20Extracting%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20from%20multiple%20camera%20images%0Aoffers%20a%20cost-effective%2C%20scalable%20alternative%20to%20LIDAR-based%20solutions%20in%0Aautonomous%20driving.%20However%2C%20the%20performance%20of%20the%20existing%20BEV%20methods%20drops%0Asignificantly%20under%20various%20corruptions%20such%20as%20brightness%20and%20weather%20changes%0Aor%20camera%20failures.%20To%20improve%20the%20robustness%20of%20BEV%20perception%2C%20we%20propose%20to%0Aadapt%20a%20large%20vision%20foundational%20model%2C%20DINOv2%2C%20to%20BEV%20estimation%20using%20Low%0ARank%20Adaptation%20%28LoRA%29.%20Our%20approach%20builds%20on%20the%20strong%20representation%20space%0Aof%20DINOv2%20by%20adapting%20it%20to%20the%20BEV%20task%20in%20a%20state-of-the-art%20framework%2C%0ASimpleBEV.%20Our%20experiments%20show%20increased%20robustness%20of%20BEV%20perception%20under%0Avarious%20corruptions%2C%20with%20increasing%20gains%20from%20scaling%20up%20the%20model%20and%20the%0Ainput%20resolution.%20We%20also%20showcase%20the%20effectiveness%20of%20the%20adapted%0Arepresentations%20in%20terms%20of%20fewer%20learnable%20parameters%20and%20faster%20convergence%0Aduring%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10228v1&entry.124074799=Read"},
{"title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP", "author": "Sedigheh Eslami and Gerard de Melo", "abstract": "  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.\n", "link": "http://arxiv.org/abs/2406.17639v3", "date": "2024-09-16", "relevancy": 2.9412, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&body=Title%3A%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP%0AAuthor%3A%20Sedigheh%20Eslami%20and%20Gerard%20de%20Melo%0AAbstract%3A%20%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20three%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%203.%20How%20do%20these%20gap%0Areduction%20approaches%20affect%20the%20downstream%20performance%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20through%20extensive%20experiments%2C%20we%20show%20that%0AAlignCLIP%20achieves%20noticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%0Aembeddings%2C%20and%20thereby%2C%20reduces%20the%20modality%20gap%2C%20while%20improving%20the%0Aperformance%20across%20several%20zero-shot%20and%20fine-tuning%20downstream%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17639v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigate%2520the%2520Gap%253A%2520Investigating%2520Approaches%2520for%2520Improving%2520Cross-Modal%250A%2520%2520Alignment%2520in%2520CLIP%26entry.906535625%3DSedigheh%2520Eslami%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3D%2520%2520Contrastive%2520Language--Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520manifested%2520remarkable%250Aimprovements%2520in%2520zero-shot%2520classification%2520and%2520cross-modal%2520vision-language%2520tasks.%250AYet%252C%2520from%2520a%2520geometrical%2520point%2520of%2520view%252C%2520the%2520CLIP%2520embedding%2520space%2520has%2520been%2520found%250Ato%2520have%2520a%2520pronounced%2520modality%2520gap.%2520This%2520gap%2520renders%2520the%2520embedding%2520space%2520overly%250Asparse%2520and%2520disconnected%252C%2520with%2520different%2520modalities%2520being%2520densely%2520distributed%2520in%250Adistinct%2520subregions%2520of%2520the%2520hypersphere.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%2520answering%2520three%250Amain%2520questions%253A%25201.%2520Does%2520sharing%2520the%2520parameter%2520space%2520between%2520the%2520multi-modal%250Aencoders%2520reduce%2520the%2520modality%2520gap%253F%25202.%2520Can%2520the%2520gap%2520be%2520mitigated%2520by%2520pushing%2520apart%250Athe%2520uni-modal%2520embeddings%2520via%2520intra-modality%2520separation%253F%25203.%2520How%2520do%2520these%2520gap%250Areduction%2520approaches%2520affect%2520the%2520downstream%2520performance%253F%2520We%2520design%2520AlignCLIP%252C%2520in%250Aorder%2520to%2520answer%2520these%2520questions%2520and%2520through%2520extensive%2520experiments%252C%2520we%2520show%2520that%250AAlignCLIP%2520achieves%2520noticeable%2520enhancements%2520in%2520the%2520cross-modal%2520alignment%2520of%2520the%250Aembeddings%252C%2520and%2520thereby%252C%2520reduces%2520the%2520modality%2520gap%252C%2520while%2520improving%2520the%250Aperformance%2520across%2520several%2520zero-shot%2520and%2520fine-tuning%2520downstream%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17639v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&entry.906535625=Sedigheh%20Eslami%20and%20Gerard%20de%20Melo&entry.1292438233=%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20three%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%203.%20How%20do%20these%20gap%0Areduction%20approaches%20affect%20the%20downstream%20performance%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20through%20extensive%20experiments%2C%20we%20show%20that%0AAlignCLIP%20achieves%20noticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%0Aembeddings%2C%20and%20thereby%2C%20reduces%20the%20modality%20gap%2C%20while%20improving%20the%0Aperformance%20across%20several%20zero-shot%20and%20fine-tuning%20downstream%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17639v3&entry.124074799=Read"},
{"title": "Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval", "author": "Yiyang Jiang and Wengyu Zhang and Xulu Zhang and Xiaoyong Wei and Chang Wen Chen and Qing Li", "abstract": "  In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET.\n", "link": "http://arxiv.org/abs/2407.15051v3", "date": "2024-09-16", "relevancy": 2.9348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior%20Knowledge%20Integration%20via%20LLM%20Encoding%20and%20Pseudo%20Event%20Regulation%0A%20%20for%20Video%20Moment%20Retrieval&body=Title%3A%20Prior%20Knowledge%20Integration%20via%20LLM%20Encoding%20and%20Pseudo%20Event%20Regulation%0A%20%20for%20Video%20Moment%20Retrieval%0AAuthor%3A%20Yiyang%20Jiang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Xiaoyong%20Wei%20and%20Chang%20Wen%20Chen%20and%20Qing%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20feasibility%20of%20leveraging%20large%20language%0Amodels%20%28LLMs%29%20for%20integrating%20general%20knowledge%20and%20incorporating%20pseudo-events%0Aas%20priors%20for%20temporal%20content%20distribution%20in%20video%20moment%20retrieval%20%28VMR%29%0Amodels.%20The%20motivation%20behind%20this%20study%20arises%20from%20the%20limitations%20of%20using%0ALLMs%20as%20decoders%20for%20generating%20discrete%20textual%20descriptions%2C%20which%20hinders%0Atheir%20direct%20application%20to%20continuous%20outputs%20like%20salience%20scores%20and%0Ainter-frame%20embeddings%20that%20capture%20inter-frame%20relations.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20utilizing%20LLM%20encoders%20instead%20of%20decoders.%20Through%20a%0Afeasibility%20study%2C%20we%20demonstrate%20that%20LLM%20encoders%20effectively%20refine%0Ainter-concept%20relations%20in%20multimodal%20embeddings%2C%20even%20without%20being%20trained%20on%0Atextual%20embeddings.%20We%20also%20show%20that%20the%20refinement%20capability%20of%20LLM%20encoders%0Acan%20be%20transferred%20to%20other%20embeddings%2C%20such%20as%20BLIP%20and%20T5%2C%20as%20long%20as%20these%0Aembeddings%20exhibit%20similar%20inter-concept%20similarity%20patterns%20to%20CLIP%0Aembeddings.%20We%20present%20a%20general%20framework%20for%20integrating%20LLM%20encoders%20into%0Aexisting%20VMR%20architectures%2C%20specifically%20within%20the%20fusion%20module.%20Through%0Aexperimental%20validation%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethods%20by%20achieving%20state-of-the-art%20performance%20in%20VMR.%20The%20source%20code%20can%0Abe%20accessed%20at%20https%3A//github.com/fletcherjiang/LLMEPET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15051v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior%2520Knowledge%2520Integration%2520via%2520LLM%2520Encoding%2520and%2520Pseudo%2520Event%2520Regulation%250A%2520%2520for%2520Video%2520Moment%2520Retrieval%26entry.906535625%3DYiyang%2520Jiang%2520and%2520Wengyu%2520Zhang%2520and%2520Xulu%2520Zhang%2520and%2520Xiaoyong%2520Wei%2520and%2520Chang%2520Wen%2520Chen%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520feasibility%2520of%2520leveraging%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520for%2520integrating%2520general%2520knowledge%2520and%2520incorporating%2520pseudo-events%250Aas%2520priors%2520for%2520temporal%2520content%2520distribution%2520in%2520video%2520moment%2520retrieval%2520%2528VMR%2529%250Amodels.%2520The%2520motivation%2520behind%2520this%2520study%2520arises%2520from%2520the%2520limitations%2520of%2520using%250ALLMs%2520as%2520decoders%2520for%2520generating%2520discrete%2520textual%2520descriptions%252C%2520which%2520hinders%250Atheir%2520direct%2520application%2520to%2520continuous%2520outputs%2520like%2520salience%2520scores%2520and%250Ainter-frame%2520embeddings%2520that%2520capture%2520inter-frame%2520relations.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520utilizing%2520LLM%2520encoders%2520instead%2520of%2520decoders.%2520Through%2520a%250Afeasibility%2520study%252C%2520we%2520demonstrate%2520that%2520LLM%2520encoders%2520effectively%2520refine%250Ainter-concept%2520relations%2520in%2520multimodal%2520embeddings%252C%2520even%2520without%2520being%2520trained%2520on%250Atextual%2520embeddings.%2520We%2520also%2520show%2520that%2520the%2520refinement%2520capability%2520of%2520LLM%2520encoders%250Acan%2520be%2520transferred%2520to%2520other%2520embeddings%252C%2520such%2520as%2520BLIP%2520and%2520T5%252C%2520as%2520long%2520as%2520these%250Aembeddings%2520exhibit%2520similar%2520inter-concept%2520similarity%2520patterns%2520to%2520CLIP%250Aembeddings.%2520We%2520present%2520a%2520general%2520framework%2520for%2520integrating%2520LLM%2520encoders%2520into%250Aexisting%2520VMR%2520architectures%252C%2520specifically%2520within%2520the%2520fusion%2520module.%2520Through%250Aexperimental%2520validation%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethods%2520by%2520achieving%2520state-of-the-art%2520performance%2520in%2520VMR.%2520The%2520source%2520code%2520can%250Abe%2520accessed%2520at%2520https%253A//github.com/fletcherjiang/LLMEPET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15051v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior%20Knowledge%20Integration%20via%20LLM%20Encoding%20and%20Pseudo%20Event%20Regulation%0A%20%20for%20Video%20Moment%20Retrieval&entry.906535625=Yiyang%20Jiang%20and%20Wengyu%20Zhang%20and%20Xulu%20Zhang%20and%20Xiaoyong%20Wei%20and%20Chang%20Wen%20Chen%20and%20Qing%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20feasibility%20of%20leveraging%20large%20language%0Amodels%20%28LLMs%29%20for%20integrating%20general%20knowledge%20and%20incorporating%20pseudo-events%0Aas%20priors%20for%20temporal%20content%20distribution%20in%20video%20moment%20retrieval%20%28VMR%29%0Amodels.%20The%20motivation%20behind%20this%20study%20arises%20from%20the%20limitations%20of%20using%0ALLMs%20as%20decoders%20for%20generating%20discrete%20textual%20descriptions%2C%20which%20hinders%0Atheir%20direct%20application%20to%20continuous%20outputs%20like%20salience%20scores%20and%0Ainter-frame%20embeddings%20that%20capture%20inter-frame%20relations.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20utilizing%20LLM%20encoders%20instead%20of%20decoders.%20Through%20a%0Afeasibility%20study%2C%20we%20demonstrate%20that%20LLM%20encoders%20effectively%20refine%0Ainter-concept%20relations%20in%20multimodal%20embeddings%2C%20even%20without%20being%20trained%20on%0Atextual%20embeddings.%20We%20also%20show%20that%20the%20refinement%20capability%20of%20LLM%20encoders%0Acan%20be%20transferred%20to%20other%20embeddings%2C%20such%20as%20BLIP%20and%20T5%2C%20as%20long%20as%20these%0Aembeddings%20exhibit%20similar%20inter-concept%20similarity%20patterns%20to%20CLIP%0Aembeddings.%20We%20present%20a%20general%20framework%20for%20integrating%20LLM%20encoders%20into%0Aexisting%20VMR%20architectures%2C%20specifically%20within%20the%20fusion%20module.%20Through%0Aexperimental%20validation%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethods%20by%20achieving%20state-of-the-art%20performance%20in%20VMR.%20The%20source%20code%20can%0Abe%20accessed%20at%20https%3A//github.com/fletcherjiang/LLMEPET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15051v3&entry.124074799=Read"},
{"title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction", "author": "Yuchong Zhang and Yong Ma and Danica Kragic", "abstract": "  The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.\n", "link": "http://arxiv.org/abs/2404.14965v5", "date": "2024-09-16", "relevancy": 2.9181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5918}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&body=Title%3A%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction%0AAuthor%3A%20Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14965v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Beyond%2520Boundaries%253A%2520An%2520Initial%2520Design%2520Space%2520of%2520Domain-specific%250A%2520%2520Large%2520Vision%2520Models%2520in%2520Human-robot%2520Interaction%26entry.906535625%3DYuchong%2520Zhang%2520and%2520Yong%2520Ma%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520vision%2520models%2520%2528LVMs%2529%2520is%2520following%2520in%2520the%2520footsteps%2520of%250Athe%2520recent%2520prosperity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520following%2520years.%250AHowever%252C%2520there%2527s%2520a%2520noticeable%2520gap%2520in%2520structured%2520research%2520applying%2520LVMs%2520to%250Ahuman-robot%2520interaction%2520%2528HRI%2529%252C%2520despite%2520extensive%2520evidence%2520supporting%2520the%250Aefficacy%2520of%2520vision%2520models%2520in%2520enhancing%2520interactions%2520between%2520humans%2520and%2520robots.%250ARecognizing%2520the%2520vast%2520and%2520anticipated%2520potential%252C%2520we%2520introduce%2520an%2520initial%2520design%250Aspace%2520that%2520incorporates%2520domain-specific%2520LVMs%252C%2520chosen%2520for%2520their%2520superior%250Aperformance%2520over%2520normal%2520models.%2520We%2520delve%2520into%2520three%2520primary%2520dimensions%253A%2520HRI%250Acontexts%252C%2520vision-based%2520tasks%252C%2520and%2520specific%2520domains.%2520The%2520empirical%2520evaluation%250Awas%2520implemented%2520among%252015%2520experts%2520across%2520six%2520evaluated%2520metrics%252C%2520showcasing%2520the%250Aprimary%2520efficacy%2520in%2520relevant%2520decision-making%2520scenarios.%2520We%2520explore%2520the%2520process%250Aof%2520ideation%2520and%2520potential%2520application%2520scenarios%252C%2520envisioning%2520this%2520design%2520space%250Aas%2520a%2520foundational%2520guideline%2520for%2520future%2520HRI%2520system%2520design%252C%2520emphasizing%2520accurate%250Adomain%2520alignment%2520and%2520model%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14965v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&entry.906535625=Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic&entry.1292438233=%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14965v5&entry.124074799=Read"},
{"title": "Exploring 3D Face Reconstruction and Fusion Methods for Face\n  Verification: A Case-Study in Video Surveillance", "author": "Simone Maurizio La Cava and Sara Concas and Ruben Tolosana and Roberto Casula and Giulia Orr\u00f9 and Martin Drahansky and Julian Fierrez and Gian Luca Marcialis", "abstract": "  3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.\n", "link": "http://arxiv.org/abs/2409.10481v1", "date": "2024-09-16", "relevancy": 2.8712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%203D%20Face%20Reconstruction%20and%20Fusion%20Methods%20for%20Face%0A%20%20Verification%3A%20A%20Case-Study%20in%20Video%20Surveillance&body=Title%3A%20Exploring%203D%20Face%20Reconstruction%20and%20Fusion%20Methods%20for%20Face%0A%20%20Verification%3A%20A%20Case-Study%20in%20Video%20Surveillance%0AAuthor%3A%20Simone%20Maurizio%20La%20Cava%20and%20Sara%20Concas%20and%20Ruben%20Tolosana%20and%20Roberto%20Casula%20and%20Giulia%20Orr%C3%B9%20and%20Martin%20Drahansky%20and%20Julian%20Fierrez%20and%20Gian%20Luca%20Marcialis%0AAbstract%3A%20%20%203D%20face%20reconstruction%20%283DFR%29%20algorithms%20are%20based%20on%20specific%20assumptions%0Atailored%20to%20distinct%20application%20scenarios.%20These%20assumptions%20limit%20their%20use%0Awhen%20acquisition%20conditions%2C%20such%20as%20the%20subject%27s%20distance%20from%20the%20camera%20or%0Athe%20camera%27s%20characteristics%2C%20are%20different%20than%20expected%2C%20as%20typically%20happens%0Ain%20video%20surveillance.%20Additionally%2C%203DFR%20algorithms%20follow%20various%20strategies%0Ato%20address%20the%20reconstruction%20of%20a%203D%20shape%20from%202D%20data%2C%20such%20as%20statistical%0Amodel%20fitting%2C%20photometric%20stereo%2C%20or%20deep%20learning.%20In%20the%20present%20study%2C%20we%0Aexplore%20the%20application%20of%20three%203DFR%20algorithms%20representative%20of%20the%20SOTA%2C%0Aemploying%20each%20one%20as%20the%20template%20set%20generator%20for%20a%20face%20verification%0Asystem.%20The%20scores%20provided%20by%20each%20system%20are%20combined%20by%20score-level%20fusion.%0AWe%20show%20that%20the%20complementarity%20induced%20by%20different%203DFR%20algorithms%20improves%0Aperformance%20when%20tests%20are%20conducted%20at%20never-seen-before%20distances%20from%20the%0Acamera%20and%20camera%20characteristics%20%28cross-distance%20and%20cross-camera%20settings%29%2C%0Athus%20encouraging%20further%20investigations%20on%20multiple%203DFR-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%25203D%2520Face%2520Reconstruction%2520and%2520Fusion%2520Methods%2520for%2520Face%250A%2520%2520Verification%253A%2520A%2520Case-Study%2520in%2520Video%2520Surveillance%26entry.906535625%3DSimone%2520Maurizio%2520La%2520Cava%2520and%2520Sara%2520Concas%2520and%2520Ruben%2520Tolosana%2520and%2520Roberto%2520Casula%2520and%2520Giulia%2520Orr%25C3%25B9%2520and%2520Martin%2520Drahansky%2520and%2520Julian%2520Fierrez%2520and%2520Gian%2520Luca%2520Marcialis%26entry.1292438233%3D%2520%25203D%2520face%2520reconstruction%2520%25283DFR%2529%2520algorithms%2520are%2520based%2520on%2520specific%2520assumptions%250Atailored%2520to%2520distinct%2520application%2520scenarios.%2520These%2520assumptions%2520limit%2520their%2520use%250Awhen%2520acquisition%2520conditions%252C%2520such%2520as%2520the%2520subject%2527s%2520distance%2520from%2520the%2520camera%2520or%250Athe%2520camera%2527s%2520characteristics%252C%2520are%2520different%2520than%2520expected%252C%2520as%2520typically%2520happens%250Ain%2520video%2520surveillance.%2520Additionally%252C%25203DFR%2520algorithms%2520follow%2520various%2520strategies%250Ato%2520address%2520the%2520reconstruction%2520of%2520a%25203D%2520shape%2520from%25202D%2520data%252C%2520such%2520as%2520statistical%250Amodel%2520fitting%252C%2520photometric%2520stereo%252C%2520or%2520deep%2520learning.%2520In%2520the%2520present%2520study%252C%2520we%250Aexplore%2520the%2520application%2520of%2520three%25203DFR%2520algorithms%2520representative%2520of%2520the%2520SOTA%252C%250Aemploying%2520each%2520one%2520as%2520the%2520template%2520set%2520generator%2520for%2520a%2520face%2520verification%250Asystem.%2520The%2520scores%2520provided%2520by%2520each%2520system%2520are%2520combined%2520by%2520score-level%2520fusion.%250AWe%2520show%2520that%2520the%2520complementarity%2520induced%2520by%2520different%25203DFR%2520algorithms%2520improves%250Aperformance%2520when%2520tests%2520are%2520conducted%2520at%2520never-seen-before%2520distances%2520from%2520the%250Acamera%2520and%2520camera%2520characteristics%2520%2528cross-distance%2520and%2520cross-camera%2520settings%2529%252C%250Athus%2520encouraging%2520further%2520investigations%2520on%2520multiple%25203DFR-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%203D%20Face%20Reconstruction%20and%20Fusion%20Methods%20for%20Face%0A%20%20Verification%3A%20A%20Case-Study%20in%20Video%20Surveillance&entry.906535625=Simone%20Maurizio%20La%20Cava%20and%20Sara%20Concas%20and%20Ruben%20Tolosana%20and%20Roberto%20Casula%20and%20Giulia%20Orr%C3%B9%20and%20Martin%20Drahansky%20and%20Julian%20Fierrez%20and%20Gian%20Luca%20Marcialis&entry.1292438233=%20%203D%20face%20reconstruction%20%283DFR%29%20algorithms%20are%20based%20on%20specific%20assumptions%0Atailored%20to%20distinct%20application%20scenarios.%20These%20assumptions%20limit%20their%20use%0Awhen%20acquisition%20conditions%2C%20such%20as%20the%20subject%27s%20distance%20from%20the%20camera%20or%0Athe%20camera%27s%20characteristics%2C%20are%20different%20than%20expected%2C%20as%20typically%20happens%0Ain%20video%20surveillance.%20Additionally%2C%203DFR%20algorithms%20follow%20various%20strategies%0Ato%20address%20the%20reconstruction%20of%20a%203D%20shape%20from%202D%20data%2C%20such%20as%20statistical%0Amodel%20fitting%2C%20photometric%20stereo%2C%20or%20deep%20learning.%20In%20the%20present%20study%2C%20we%0Aexplore%20the%20application%20of%20three%203DFR%20algorithms%20representative%20of%20the%20SOTA%2C%0Aemploying%20each%20one%20as%20the%20template%20set%20generator%20for%20a%20face%20verification%0Asystem.%20The%20scores%20provided%20by%20each%20system%20are%20combined%20by%20score-level%20fusion.%0AWe%20show%20that%20the%20complementarity%20induced%20by%20different%203DFR%20algorithms%20improves%0Aperformance%20when%20tests%20are%20conducted%20at%20never-seen-before%20distances%20from%20the%0Acamera%20and%20camera%20characteristics%20%28cross-distance%20and%20cross-camera%20settings%29%2C%0Athus%20encouraging%20further%20investigations%20on%20multiple%203DFR-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10481v1&entry.124074799=Read"},
{"title": "PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for\n  Efficient Point Cloud Classification", "author": "Qiang Zheng and Chao Zhang and Jian Sun", "abstract": "  Advances in self-supervised learning are essential for enhancing feature\nextraction and understanding in point cloud processing. This paper introduces\nPMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervised\nlearning framework for point cloud classification. PMT-MAE features a\ndual-branch architecture that integrates Transformer and MLP components to\ncapture rich features. The Transformer branch leverages global self-attention\nfor intricate feature interactions, while the parallel MLP branch processes\ntokens through shared fully connected layers, offering a complementary feature\ntransformation pathway. A fusion mechanism then combines these features,\nenhancing the model's capacity to learn comprehensive 3D representations.\nGuided by the sophisticated teacher model Point-M2AE, PMT-MAE employs a\ndistillation strategy that includes feature distillation during pre-training\nand logit distillation during fine-tuning, ensuring effective knowledge\ntransfer. On the ModelNet40 classification task, achieving an accuracy of\n93.6\\% without employing voting strategy, PMT-MAE surpasses the baseline\nPoint-MAE (93.2\\%) and the teacher Point-M2AE (93.4\\%), underscoring its\nability to learn discriminative 3D point cloud representations. Additionally,\nthis framework demonstrates high efficiency, requiring only 40 epochs for both\npre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render it\nwell-suited for scenarios with limited computational resources, positioning it\nas a promising solution for practical point cloud analysis.\n", "link": "http://arxiv.org/abs/2409.02007v2", "date": "2024-09-16", "relevancy": 2.8546, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMT-MAE%3A%20Dual-Branch%20Self-Supervised%20Learning%20with%20Distillation%20for%0A%20%20Efficient%20Point%20Cloud%20Classification&body=Title%3A%20PMT-MAE%3A%20Dual-Branch%20Self-Supervised%20Learning%20with%20Distillation%20for%0A%20%20Efficient%20Point%20Cloud%20Classification%0AAuthor%3A%20Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20Advances%20in%20self-supervised%20learning%20are%20essential%20for%20enhancing%20feature%0Aextraction%20and%20understanding%20in%20point%20cloud%20processing.%20This%20paper%20introduces%0APMT-MAE%20%28Point%20MLP-Transformer%20Masked%20Autoencoder%29%2C%20a%20novel%20self-supervised%0Alearning%20framework%20for%20point%20cloud%20classification.%20PMT-MAE%20features%20a%0Adual-branch%20architecture%20that%20integrates%20Transformer%20and%20MLP%20components%20to%0Acapture%20rich%20features.%20The%20Transformer%20branch%20leverages%20global%20self-attention%0Afor%20intricate%20feature%20interactions%2C%20while%20the%20parallel%20MLP%20branch%20processes%0Atokens%20through%20shared%20fully%20connected%20layers%2C%20offering%20a%20complementary%20feature%0Atransformation%20pathway.%20A%20fusion%20mechanism%20then%20combines%20these%20features%2C%0Aenhancing%20the%20model%27s%20capacity%20to%20learn%20comprehensive%203D%20representations.%0AGuided%20by%20the%20sophisticated%20teacher%20model%20Point-M2AE%2C%20PMT-MAE%20employs%20a%0Adistillation%20strategy%20that%20includes%20feature%20distillation%20during%20pre-training%0Aand%20logit%20distillation%20during%20fine-tuning%2C%20ensuring%20effective%20knowledge%0Atransfer.%20On%20the%20ModelNet40%20classification%20task%2C%20achieving%20an%20accuracy%20of%0A93.6%5C%25%20without%20employing%20voting%20strategy%2C%20PMT-MAE%20surpasses%20the%20baseline%0APoint-MAE%20%2893.2%5C%25%29%20and%20the%20teacher%20Point-M2AE%20%2893.4%5C%25%29%2C%20underscoring%20its%0Aability%20to%20learn%20discriminative%203D%20point%20cloud%20representations.%20Additionally%2C%0Athis%20framework%20demonstrates%20high%20efficiency%2C%20requiring%20only%2040%20epochs%20for%20both%0Apre-training%20and%20fine-tuning.%20PMT-MAE%27s%20effectiveness%20and%20efficiency%20render%20it%0Awell-suited%20for%20scenarios%20with%20limited%20computational%20resources%2C%20positioning%20it%0Aas%20a%20promising%20solution%20for%20practical%20point%20cloud%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMT-MAE%253A%2520Dual-Branch%2520Self-Supervised%2520Learning%2520with%2520Distillation%2520for%250A%2520%2520Efficient%2520Point%2520Cloud%2520Classification%26entry.906535625%3DQiang%2520Zheng%2520and%2520Chao%2520Zhang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520Advances%2520in%2520self-supervised%2520learning%2520are%2520essential%2520for%2520enhancing%2520feature%250Aextraction%2520and%2520understanding%2520in%2520point%2520cloud%2520processing.%2520This%2520paper%2520introduces%250APMT-MAE%2520%2528Point%2520MLP-Transformer%2520Masked%2520Autoencoder%2529%252C%2520a%2520novel%2520self-supervised%250Alearning%2520framework%2520for%2520point%2520cloud%2520classification.%2520PMT-MAE%2520features%2520a%250Adual-branch%2520architecture%2520that%2520integrates%2520Transformer%2520and%2520MLP%2520components%2520to%250Acapture%2520rich%2520features.%2520The%2520Transformer%2520branch%2520leverages%2520global%2520self-attention%250Afor%2520intricate%2520feature%2520interactions%252C%2520while%2520the%2520parallel%2520MLP%2520branch%2520processes%250Atokens%2520through%2520shared%2520fully%2520connected%2520layers%252C%2520offering%2520a%2520complementary%2520feature%250Atransformation%2520pathway.%2520A%2520fusion%2520mechanism%2520then%2520combines%2520these%2520features%252C%250Aenhancing%2520the%2520model%2527s%2520capacity%2520to%2520learn%2520comprehensive%25203D%2520representations.%250AGuided%2520by%2520the%2520sophisticated%2520teacher%2520model%2520Point-M2AE%252C%2520PMT-MAE%2520employs%2520a%250Adistillation%2520strategy%2520that%2520includes%2520feature%2520distillation%2520during%2520pre-training%250Aand%2520logit%2520distillation%2520during%2520fine-tuning%252C%2520ensuring%2520effective%2520knowledge%250Atransfer.%2520On%2520the%2520ModelNet40%2520classification%2520task%252C%2520achieving%2520an%2520accuracy%2520of%250A93.6%255C%2525%2520without%2520employing%2520voting%2520strategy%252C%2520PMT-MAE%2520surpasses%2520the%2520baseline%250APoint-MAE%2520%252893.2%255C%2525%2529%2520and%2520the%2520teacher%2520Point-M2AE%2520%252893.4%255C%2525%2529%252C%2520underscoring%2520its%250Aability%2520to%2520learn%2520discriminative%25203D%2520point%2520cloud%2520representations.%2520Additionally%252C%250Athis%2520framework%2520demonstrates%2520high%2520efficiency%252C%2520requiring%2520only%252040%2520epochs%2520for%2520both%250Apre-training%2520and%2520fine-tuning.%2520PMT-MAE%2527s%2520effectiveness%2520and%2520efficiency%2520render%2520it%250Awell-suited%2520for%2520scenarios%2520with%2520limited%2520computational%2520resources%252C%2520positioning%2520it%250Aas%2520a%2520promising%2520solution%2520for%2520practical%2520point%2520cloud%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMT-MAE%3A%20Dual-Branch%20Self-Supervised%20Learning%20with%20Distillation%20for%0A%20%20Efficient%20Point%20Cloud%20Classification&entry.906535625=Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun&entry.1292438233=%20%20Advances%20in%20self-supervised%20learning%20are%20essential%20for%20enhancing%20feature%0Aextraction%20and%20understanding%20in%20point%20cloud%20processing.%20This%20paper%20introduces%0APMT-MAE%20%28Point%20MLP-Transformer%20Masked%20Autoencoder%29%2C%20a%20novel%20self-supervised%0Alearning%20framework%20for%20point%20cloud%20classification.%20PMT-MAE%20features%20a%0Adual-branch%20architecture%20that%20integrates%20Transformer%20and%20MLP%20components%20to%0Acapture%20rich%20features.%20The%20Transformer%20branch%20leverages%20global%20self-attention%0Afor%20intricate%20feature%20interactions%2C%20while%20the%20parallel%20MLP%20branch%20processes%0Atokens%20through%20shared%20fully%20connected%20layers%2C%20offering%20a%20complementary%20feature%0Atransformation%20pathway.%20A%20fusion%20mechanism%20then%20combines%20these%20features%2C%0Aenhancing%20the%20model%27s%20capacity%20to%20learn%20comprehensive%203D%20representations.%0AGuided%20by%20the%20sophisticated%20teacher%20model%20Point-M2AE%2C%20PMT-MAE%20employs%20a%0Adistillation%20strategy%20that%20includes%20feature%20distillation%20during%20pre-training%0Aand%20logit%20distillation%20during%20fine-tuning%2C%20ensuring%20effective%20knowledge%0Atransfer.%20On%20the%20ModelNet40%20classification%20task%2C%20achieving%20an%20accuracy%20of%0A93.6%5C%25%20without%20employing%20voting%20strategy%2C%20PMT-MAE%20surpasses%20the%20baseline%0APoint-MAE%20%2893.2%5C%25%29%20and%20the%20teacher%20Point-M2AE%20%2893.4%5C%25%29%2C%20underscoring%20its%0Aability%20to%20learn%20discriminative%203D%20point%20cloud%20representations.%20Additionally%2C%0Athis%20framework%20demonstrates%20high%20efficiency%2C%20requiring%20only%2040%20epochs%20for%20both%0Apre-training%20and%20fine-tuning.%20PMT-MAE%27s%20effectiveness%20and%20efficiency%20render%20it%0Awell-suited%20for%20scenarios%20with%20limited%20computational%20resources%2C%20positioning%20it%0Aas%20a%20promising%20solution%20for%20practical%20point%20cloud%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02007v2&entry.124074799=Read"},
{"title": "Aligning Machine and Human Visual Representations across Abstraction\n  Levels", "author": "Lukas Muttenthaler and Klaus Greff and Frieda Born and Bernhard Spitzer and Simon Kornblith and Michael C. Mozer and Klaus-Robert M\u00fcller and Thomas Unterthiner and Andrew K. Lampinen", "abstract": "  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n", "link": "http://arxiv.org/abs/2409.06509v2", "date": "2024-09-16", "relevancy": 2.7848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&body=Title%3A%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels%0AAuthor%3A%20Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20in%20vision%20tasks.%20However%2C%0Aneural%20network%20training%20and%20human%20learning%20differ%20in%20fundamental%20ways%2C%20and%0Aneural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%20humans%20do%2C%20raising%0Aquestions%20regarding%20the%20similarity%20of%20their%20underlying%20representations.%20What%20is%0Amissing%20for%20modern%20learning%20systems%20to%20exhibit%20more%20human-like%20behavior%3F%20We%0Ahighlight%20a%20key%20misalignment%20between%20vision%20models%20and%20humans%3A%20whereas%20human%0Aconceptual%20knowledge%20is%20hierarchically%20organized%20from%20fine-%20to%20coarse-scale%0Adistinctions%2C%20model%20representations%20do%20not%20accurately%20capture%20all%20these%20levels%0Aof%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%20train%20a%20teacher%20model%20to%0Aimitate%20human%20judgments%2C%20then%20transfer%20human-like%20structure%20from%20its%0Arepresentations%20into%20pretrained%20state-of-the-art%20vision%20foundation%20models.%0AThese%20human-aligned%20models%20more%20accurately%20approximate%20human%20behavior%20and%0Auncertainty%20across%20a%20wide%20range%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%0Ahuman%20judgments%20spanning%20multiple%20levels%20of%20semantic%20abstractions.%20They%20also%0Aperform%20better%20on%20a%20diverse%20set%20of%20machine%20learning%20tasks%2C%20increasing%0Ageneralization%20and%20out-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%0Anetworks%20with%20additional%20human%20knowledge%20yields%20a%20best-of-both-worlds%0Arepresentation%20that%20is%20both%20more%20consistent%20with%20human%20cognition%20and%20more%0Apractically%20useful%2C%20thus%20paving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%0Ahuman-like%20artificial%20intelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Machine%2520and%2520Human%2520Visual%2520Representations%2520across%2520Abstraction%250A%2520%2520Levels%26entry.906535625%3DLukas%2520Muttenthaler%2520and%2520Klaus%2520Greff%2520and%2520Frieda%2520Born%2520and%2520Bernhard%2520Spitzer%2520and%2520Simon%2520Kornblith%2520and%2520Michael%2520C.%2520Mozer%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Thomas%2520Unterthiner%2520and%2520Andrew%2520K.%2520Lampinen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520success%2520across%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520including%2520as%2520models%2520of%2520human%2520behavior%2520in%2520vision%2520tasks.%2520However%252C%250Aneural%2520network%2520training%2520and%2520human%2520learning%2520differ%2520in%2520fundamental%2520ways%252C%2520and%250Aneural%2520networks%2520often%2520fail%2520to%2520generalize%2520as%2520robustly%2520as%2520humans%2520do%252C%2520raising%250Aquestions%2520regarding%2520the%2520similarity%2520of%2520their%2520underlying%2520representations.%2520What%2520is%250Amissing%2520for%2520modern%2520learning%2520systems%2520to%2520exhibit%2520more%2520human-like%2520behavior%253F%2520We%250Ahighlight%2520a%2520key%2520misalignment%2520between%2520vision%2520models%2520and%2520humans%253A%2520whereas%2520human%250Aconceptual%2520knowledge%2520is%2520hierarchically%2520organized%2520from%2520fine-%2520to%2520coarse-scale%250Adistinctions%252C%2520model%2520representations%2520do%2520not%2520accurately%2520capture%2520all%2520these%2520levels%250Aof%2520abstraction.%2520To%2520address%2520this%2520misalignment%252C%2520we%2520first%2520train%2520a%2520teacher%2520model%2520to%250Aimitate%2520human%2520judgments%252C%2520then%2520transfer%2520human-like%2520structure%2520from%2520its%250Arepresentations%2520into%2520pretrained%2520state-of-the-art%2520vision%2520foundation%2520models.%250AThese%2520human-aligned%2520models%2520more%2520accurately%2520approximate%2520human%2520behavior%2520and%250Auncertainty%2520across%2520a%2520wide%2520range%2520of%2520similarity%2520tasks%252C%2520including%2520a%2520new%2520dataset%2520of%250Ahuman%2520judgments%2520spanning%2520multiple%2520levels%2520of%2520semantic%2520abstractions.%2520They%2520also%250Aperform%2520better%2520on%2520a%2520diverse%2520set%2520of%2520machine%2520learning%2520tasks%252C%2520increasing%250Ageneralization%2520and%2520out-of-distribution%2520robustness.%2520Thus%252C%2520infusing%2520neural%250Anetworks%2520with%2520additional%2520human%2520knowledge%2520yields%2520a%2520best-of-both-worlds%250Arepresentation%2520that%2520is%2520both%2520more%2520consistent%2520with%2520human%2520cognition%2520and%2520more%250Apractically%2520useful%252C%2520thus%2520paving%2520the%2520way%2520toward%2520more%2520robust%252C%2520interpretable%252C%2520and%250Ahuman-like%2520artificial%2520intelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&entry.906535625=Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20in%20vision%20tasks.%20However%2C%0Aneural%20network%20training%20and%20human%20learning%20differ%20in%20fundamental%20ways%2C%20and%0Aneural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%20humans%20do%2C%20raising%0Aquestions%20regarding%20the%20similarity%20of%20their%20underlying%20representations.%20What%20is%0Amissing%20for%20modern%20learning%20systems%20to%20exhibit%20more%20human-like%20behavior%3F%20We%0Ahighlight%20a%20key%20misalignment%20between%20vision%20models%20and%20humans%3A%20whereas%20human%0Aconceptual%20knowledge%20is%20hierarchically%20organized%20from%20fine-%20to%20coarse-scale%0Adistinctions%2C%20model%20representations%20do%20not%20accurately%20capture%20all%20these%20levels%0Aof%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%20train%20a%20teacher%20model%20to%0Aimitate%20human%20judgments%2C%20then%20transfer%20human-like%20structure%20from%20its%0Arepresentations%20into%20pretrained%20state-of-the-art%20vision%20foundation%20models.%0AThese%20human-aligned%20models%20more%20accurately%20approximate%20human%20behavior%20and%0Auncertainty%20across%20a%20wide%20range%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%0Ahuman%20judgments%20spanning%20multiple%20levels%20of%20semantic%20abstractions.%20They%20also%0Aperform%20better%20on%20a%20diverse%20set%20of%20machine%20learning%20tasks%2C%20increasing%0Ageneralization%20and%20out-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%0Anetworks%20with%20additional%20human%20knowledge%20yields%20a%20best-of-both-worlds%0Arepresentation%20that%20is%20both%20more%20consistent%20with%20human%20cognition%20and%20more%0Apractically%20useful%2C%20thus%20paving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%0Ahuman-like%20artificial%20intelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06509v2&entry.124074799=Read"},
{"title": "Boundary Attention: Learning curves, corners, junctions and grouping", "author": "Mia Gaia Polansky and Charles Herrmann and Junhwa Hur and Deqing Sun and Dor Verbin and Todd Zickler", "abstract": "  We present a lightweight network that infers grouping and boundaries,\nincluding curves, corners and junctions. It operates in a bottom-up fashion,\nanalogous to classical methods for sub-pixel edge localization and\nedge-linking, but with a higher-dimensional representation of local boundary\nstructure, and notions of local scale and spatial consistency that are learned\ninstead of designed. Our network uses a mechanism that we call boundary\nattention: a geometry-aware local attention operation that, when applied\ndensely and repeatedly, progressively refines a pixel-resolution field of\nvariables that specify the boundary structure in every overlapping patch within\nan image. Unlike many edge detectors that produce rasterized binary edge maps,\nour model provides a rich, unrasterized representation of the geometric\nstructure in every local region. We find that its intentional geometric bias\nallows it to be trained on simple synthetic shapes and then generalize to\nextracting boundaries from noisy low-light photographs.\n", "link": "http://arxiv.org/abs/2401.00935v3", "date": "2024-09-16", "relevancy": 2.7662, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5685}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5475}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary%20Attention%3A%20Learning%20curves%2C%20corners%2C%20junctions%20and%20grouping&body=Title%3A%20Boundary%20Attention%3A%20Learning%20curves%2C%20corners%2C%20junctions%20and%20grouping%0AAuthor%3A%20Mia%20Gaia%20Polansky%20and%20Charles%20Herrmann%20and%20Junhwa%20Hur%20and%20Deqing%20Sun%20and%20Dor%20Verbin%20and%20Todd%20Zickler%0AAbstract%3A%20%20%20We%20present%20a%20lightweight%20network%20that%20infers%20grouping%20and%20boundaries%2C%0Aincluding%20curves%2C%20corners%20and%20junctions.%20It%20operates%20in%20a%20bottom-up%20fashion%2C%0Aanalogous%20to%20classical%20methods%20for%20sub-pixel%20edge%20localization%20and%0Aedge-linking%2C%20but%20with%20a%20higher-dimensional%20representation%20of%20local%20boundary%0Astructure%2C%20and%20notions%20of%20local%20scale%20and%20spatial%20consistency%20that%20are%20learned%0Ainstead%20of%20designed.%20Our%20network%20uses%20a%20mechanism%20that%20we%20call%20boundary%0Aattention%3A%20a%20geometry-aware%20local%20attention%20operation%20that%2C%20when%20applied%0Adensely%20and%20repeatedly%2C%20progressively%20refines%20a%20pixel-resolution%20field%20of%0Avariables%20that%20specify%20the%20boundary%20structure%20in%20every%20overlapping%20patch%20within%0Aan%20image.%20Unlike%20many%20edge%20detectors%20that%20produce%20rasterized%20binary%20edge%20maps%2C%0Aour%20model%20provides%20a%20rich%2C%20unrasterized%20representation%20of%20the%20geometric%0Astructure%20in%20every%20local%20region.%20We%20find%20that%20its%20intentional%20geometric%20bias%0Aallows%20it%20to%20be%20trained%20on%20simple%20synthetic%20shapes%20and%20then%20generalize%20to%0Aextracting%20boundaries%20from%20noisy%20low-light%20photographs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00935v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary%2520Attention%253A%2520Learning%2520curves%252C%2520corners%252C%2520junctions%2520and%2520grouping%26entry.906535625%3DMia%2520Gaia%2520Polansky%2520and%2520Charles%2520Herrmann%2520and%2520Junhwa%2520Hur%2520and%2520Deqing%2520Sun%2520and%2520Dor%2520Verbin%2520and%2520Todd%2520Zickler%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520lightweight%2520network%2520that%2520infers%2520grouping%2520and%2520boundaries%252C%250Aincluding%2520curves%252C%2520corners%2520and%2520junctions.%2520It%2520operates%2520in%2520a%2520bottom-up%2520fashion%252C%250Aanalogous%2520to%2520classical%2520methods%2520for%2520sub-pixel%2520edge%2520localization%2520and%250Aedge-linking%252C%2520but%2520with%2520a%2520higher-dimensional%2520representation%2520of%2520local%2520boundary%250Astructure%252C%2520and%2520notions%2520of%2520local%2520scale%2520and%2520spatial%2520consistency%2520that%2520are%2520learned%250Ainstead%2520of%2520designed.%2520Our%2520network%2520uses%2520a%2520mechanism%2520that%2520we%2520call%2520boundary%250Aattention%253A%2520a%2520geometry-aware%2520local%2520attention%2520operation%2520that%252C%2520when%2520applied%250Adensely%2520and%2520repeatedly%252C%2520progressively%2520refines%2520a%2520pixel-resolution%2520field%2520of%250Avariables%2520that%2520specify%2520the%2520boundary%2520structure%2520in%2520every%2520overlapping%2520patch%2520within%250Aan%2520image.%2520Unlike%2520many%2520edge%2520detectors%2520that%2520produce%2520rasterized%2520binary%2520edge%2520maps%252C%250Aour%2520model%2520provides%2520a%2520rich%252C%2520unrasterized%2520representation%2520of%2520the%2520geometric%250Astructure%2520in%2520every%2520local%2520region.%2520We%2520find%2520that%2520its%2520intentional%2520geometric%2520bias%250Aallows%2520it%2520to%2520be%2520trained%2520on%2520simple%2520synthetic%2520shapes%2520and%2520then%2520generalize%2520to%250Aextracting%2520boundaries%2520from%2520noisy%2520low-light%2520photographs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00935v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary%20Attention%3A%20Learning%20curves%2C%20corners%2C%20junctions%20and%20grouping&entry.906535625=Mia%20Gaia%20Polansky%20and%20Charles%20Herrmann%20and%20Junhwa%20Hur%20and%20Deqing%20Sun%20and%20Dor%20Verbin%20and%20Todd%20Zickler&entry.1292438233=%20%20We%20present%20a%20lightweight%20network%20that%20infers%20grouping%20and%20boundaries%2C%0Aincluding%20curves%2C%20corners%20and%20junctions.%20It%20operates%20in%20a%20bottom-up%20fashion%2C%0Aanalogous%20to%20classical%20methods%20for%20sub-pixel%20edge%20localization%20and%0Aedge-linking%2C%20but%20with%20a%20higher-dimensional%20representation%20of%20local%20boundary%0Astructure%2C%20and%20notions%20of%20local%20scale%20and%20spatial%20consistency%20that%20are%20learned%0Ainstead%20of%20designed.%20Our%20network%20uses%20a%20mechanism%20that%20we%20call%20boundary%0Aattention%3A%20a%20geometry-aware%20local%20attention%20operation%20that%2C%20when%20applied%0Adensely%20and%20repeatedly%2C%20progressively%20refines%20a%20pixel-resolution%20field%20of%0Avariables%20that%20specify%20the%20boundary%20structure%20in%20every%20overlapping%20patch%20within%0Aan%20image.%20Unlike%20many%20edge%20detectors%20that%20produce%20rasterized%20binary%20edge%20maps%2C%0Aour%20model%20provides%20a%20rich%2C%20unrasterized%20representation%20of%20the%20geometric%0Astructure%20in%20every%20local%20region.%20We%20find%20that%20its%20intentional%20geometric%20bias%0Aallows%20it%20to%20be%20trained%20on%20simple%20synthetic%20shapes%20and%20then%20generalize%20to%0Aextracting%20boundaries%20from%20noisy%20low-light%20photographs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00935v3&entry.124074799=Read"},
{"title": "AD-CLIP: Adapting Domains in Prompt Space Using CLIP", "author": "Mainak Singha and Harsh Pal and Ankit Jha and Biplab Banerjee", "abstract": "  Although deep learning models have shown impressive performance on supervised\nlearning tasks, they often struggle to generalize well when the training\n(source) and test (target) domains differ. Unsupervised domain adaptation (DA)\nhas emerged as a popular solution to this problem. However, current DA\ntechniques rely on visual backbones, which may lack semantic richness. Despite\nthe potential of large-scale vision-language foundation models like CLIP, their\neffectiveness for DA has yet to be fully explored. To address this gap, we\nintroduce \\textsc{AD-CLIP}, a domain-agnostic prompt learning strategy for CLIP\nthat aims to solve the DA problem in the prompt space. We leverage the frozen\nvision backbone of CLIP to extract both image style (domain) and content\ninformation, which we apply to learn prompt tokens. Our prompts are designed to\nbe domain-invariant and class-generalizable, by conditioning prompt learning on\nimage style and content features simultaneously. We use standard supervised\ncontrastive learning in the source domain, while proposing an entropy\nminimization strategy to align domains in the embedding space given the target\ndomain data. We also consider a scenario where only target domain samples are\navailable during testing, without any source domain data, and propose a\ncross-domain style mapping network to hallucinate domain-agnostic tokens. Our\nextensive experiments on three benchmark DA datasets demonstrate the\neffectiveness of \\textsc{AD-CLIP} compared to existing literature. Code is\navailable at \\url{https://github.com/mainaksingha01/AD-CLIP}\n", "link": "http://arxiv.org/abs/2308.05659v2", "date": "2024-09-16", "relevancy": 2.7457, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6108}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD-CLIP%3A%20Adapting%20Domains%20in%20Prompt%20Space%20Using%20CLIP&body=Title%3A%20AD-CLIP%3A%20Adapting%20Domains%20in%20Prompt%20Space%20Using%20CLIP%0AAuthor%3A%20Mainak%20Singha%20and%20Harsh%20Pal%20and%20Ankit%20Jha%20and%20Biplab%20Banerjee%0AAbstract%3A%20%20%20Although%20deep%20learning%20models%20have%20shown%20impressive%20performance%20on%20supervised%0Alearning%20tasks%2C%20they%20often%20struggle%20to%20generalize%20well%20when%20the%20training%0A%28source%29%20and%20test%20%28target%29%20domains%20differ.%20Unsupervised%20domain%20adaptation%20%28DA%29%0Ahas%20emerged%20as%20a%20popular%20solution%20to%20this%20problem.%20However%2C%20current%20DA%0Atechniques%20rely%20on%20visual%20backbones%2C%20which%20may%20lack%20semantic%20richness.%20Despite%0Athe%20potential%20of%20large-scale%20vision-language%20foundation%20models%20like%20CLIP%2C%20their%0Aeffectiveness%20for%20DA%20has%20yet%20to%20be%20fully%20explored.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20%5Ctextsc%7BAD-CLIP%7D%2C%20a%20domain-agnostic%20prompt%20learning%20strategy%20for%20CLIP%0Athat%20aims%20to%20solve%20the%20DA%20problem%20in%20the%20prompt%20space.%20We%20leverage%20the%20frozen%0Avision%20backbone%20of%20CLIP%20to%20extract%20both%20image%20style%20%28domain%29%20and%20content%0Ainformation%2C%20which%20we%20apply%20to%20learn%20prompt%20tokens.%20Our%20prompts%20are%20designed%20to%0Abe%20domain-invariant%20and%20class-generalizable%2C%20by%20conditioning%20prompt%20learning%20on%0Aimage%20style%20and%20content%20features%20simultaneously.%20We%20use%20standard%20supervised%0Acontrastive%20learning%20in%20the%20source%20domain%2C%20while%20proposing%20an%20entropy%0Aminimization%20strategy%20to%20align%20domains%20in%20the%20embedding%20space%20given%20the%20target%0Adomain%20data.%20We%20also%20consider%20a%20scenario%20where%20only%20target%20domain%20samples%20are%0Aavailable%20during%20testing%2C%20without%20any%20source%20domain%20data%2C%20and%20propose%20a%0Across-domain%20style%20mapping%20network%20to%20hallucinate%20domain-agnostic%20tokens.%20Our%0Aextensive%20experiments%20on%20three%20benchmark%20DA%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20%5Ctextsc%7BAD-CLIP%7D%20compared%20to%20existing%20literature.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/mainaksingha01/AD-CLIP%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD-CLIP%253A%2520Adapting%2520Domains%2520in%2520Prompt%2520Space%2520Using%2520CLIP%26entry.906535625%3DMainak%2520Singha%2520and%2520Harsh%2520Pal%2520and%2520Ankit%2520Jha%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3D%2520%2520Although%2520deep%2520learning%2520models%2520have%2520shown%2520impressive%2520performance%2520on%2520supervised%250Alearning%2520tasks%252C%2520they%2520often%2520struggle%2520to%2520generalize%2520well%2520when%2520the%2520training%250A%2528source%2529%2520and%2520test%2520%2528target%2529%2520domains%2520differ.%2520Unsupervised%2520domain%2520adaptation%2520%2528DA%2529%250Ahas%2520emerged%2520as%2520a%2520popular%2520solution%2520to%2520this%2520problem.%2520However%252C%2520current%2520DA%250Atechniques%2520rely%2520on%2520visual%2520backbones%252C%2520which%2520may%2520lack%2520semantic%2520richness.%2520Despite%250Athe%2520potential%2520of%2520large-scale%2520vision-language%2520foundation%2520models%2520like%2520CLIP%252C%2520their%250Aeffectiveness%2520for%2520DA%2520has%2520yet%2520to%2520be%2520fully%2520explored.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520%255Ctextsc%257BAD-CLIP%257D%252C%2520a%2520domain-agnostic%2520prompt%2520learning%2520strategy%2520for%2520CLIP%250Athat%2520aims%2520to%2520solve%2520the%2520DA%2520problem%2520in%2520the%2520prompt%2520space.%2520We%2520leverage%2520the%2520frozen%250Avision%2520backbone%2520of%2520CLIP%2520to%2520extract%2520both%2520image%2520style%2520%2528domain%2529%2520and%2520content%250Ainformation%252C%2520which%2520we%2520apply%2520to%2520learn%2520prompt%2520tokens.%2520Our%2520prompts%2520are%2520designed%2520to%250Abe%2520domain-invariant%2520and%2520class-generalizable%252C%2520by%2520conditioning%2520prompt%2520learning%2520on%250Aimage%2520style%2520and%2520content%2520features%2520simultaneously.%2520We%2520use%2520standard%2520supervised%250Acontrastive%2520learning%2520in%2520the%2520source%2520domain%252C%2520while%2520proposing%2520an%2520entropy%250Aminimization%2520strategy%2520to%2520align%2520domains%2520in%2520the%2520embedding%2520space%2520given%2520the%2520target%250Adomain%2520data.%2520We%2520also%2520consider%2520a%2520scenario%2520where%2520only%2520target%2520domain%2520samples%2520are%250Aavailable%2520during%2520testing%252C%2520without%2520any%2520source%2520domain%2520data%252C%2520and%2520propose%2520a%250Across-domain%2520style%2520mapping%2520network%2520to%2520hallucinate%2520domain-agnostic%2520tokens.%2520Our%250Aextensive%2520experiments%2520on%2520three%2520benchmark%2520DA%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520%255Ctextsc%257BAD-CLIP%257D%2520compared%2520to%2520existing%2520literature.%2520Code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/mainaksingha01/AD-CLIP%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD-CLIP%3A%20Adapting%20Domains%20in%20Prompt%20Space%20Using%20CLIP&entry.906535625=Mainak%20Singha%20and%20Harsh%20Pal%20and%20Ankit%20Jha%20and%20Biplab%20Banerjee&entry.1292438233=%20%20Although%20deep%20learning%20models%20have%20shown%20impressive%20performance%20on%20supervised%0Alearning%20tasks%2C%20they%20often%20struggle%20to%20generalize%20well%20when%20the%20training%0A%28source%29%20and%20test%20%28target%29%20domains%20differ.%20Unsupervised%20domain%20adaptation%20%28DA%29%0Ahas%20emerged%20as%20a%20popular%20solution%20to%20this%20problem.%20However%2C%20current%20DA%0Atechniques%20rely%20on%20visual%20backbones%2C%20which%20may%20lack%20semantic%20richness.%20Despite%0Athe%20potential%20of%20large-scale%20vision-language%20foundation%20models%20like%20CLIP%2C%20their%0Aeffectiveness%20for%20DA%20has%20yet%20to%20be%20fully%20explored.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20%5Ctextsc%7BAD-CLIP%7D%2C%20a%20domain-agnostic%20prompt%20learning%20strategy%20for%20CLIP%0Athat%20aims%20to%20solve%20the%20DA%20problem%20in%20the%20prompt%20space.%20We%20leverage%20the%20frozen%0Avision%20backbone%20of%20CLIP%20to%20extract%20both%20image%20style%20%28domain%29%20and%20content%0Ainformation%2C%20which%20we%20apply%20to%20learn%20prompt%20tokens.%20Our%20prompts%20are%20designed%20to%0Abe%20domain-invariant%20and%20class-generalizable%2C%20by%20conditioning%20prompt%20learning%20on%0Aimage%20style%20and%20content%20features%20simultaneously.%20We%20use%20standard%20supervised%0Acontrastive%20learning%20in%20the%20source%20domain%2C%20while%20proposing%20an%20entropy%0Aminimization%20strategy%20to%20align%20domains%20in%20the%20embedding%20space%20given%20the%20target%0Adomain%20data.%20We%20also%20consider%20a%20scenario%20where%20only%20target%20domain%20samples%20are%0Aavailable%20during%20testing%2C%20without%20any%20source%20domain%20data%2C%20and%20propose%20a%0Across-domain%20style%20mapping%20network%20to%20hallucinate%20domain-agnostic%20tokens.%20Our%0Aextensive%20experiments%20on%20three%20benchmark%20DA%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20%5Ctextsc%7BAD-CLIP%7D%20compared%20to%20existing%20literature.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/mainaksingha01/AD-CLIP%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05659v2&entry.124074799=Read"},
{"title": "Neuromorphic Facial Analysis with Cross-Modal Supervision", "author": "Federico Becattini and Luca Cultrera and Lorenzo Berlincioni and Claudio Ferrari and Andrea Leonardo and Alberto Del Bimbo", "abstract": "  Traditional approaches for analyzing RGB frames are capable of providing a\nfine-grained understanding of a face from different angles by inferring\nemotions, poses, shapes, landmarks. However, when it comes to subtle movements\nstandard RGB cameras might fall behind due to their latency, making it hard to\ndetect micro-movements that carry highly informative cues to infer the true\nemotions of a subject. To address this issue, the usage of event cameras to\nanalyze faces is gaining increasing interest. Nonetheless, all the expertise\nmatured for RGB processing is not directly transferrable to neuromorphic data\ndue to a strong domain shift and intrinsic differences in how data is\nrepresented. The lack of labeled data can be considered one of the main causes\nof this gap, yet gathering data is harder in the event domain since it cannot\nbe crawled from the web and labeling frames should take into account event\naggregation rates and the fact that static parts might not be visible in\ncertain frames. In this paper, we first present FACEMORPHIC, a multimodal\ntemporally synchronized face dataset comprising both RGB videos and event\nstreams. The data is labeled at a video level with facial Action Units and also\ncontains streams collected with a variety of applications in mind, ranging from\n3D shape estimation to lip-reading. We then show how temporal synchronization\ncan allow effective neuromorphic face analysis without the need to manually\nannotate videos: we instead leverage cross-modal supervision bridging the\ndomain gap by representing face shapes in a 3D space.\n", "link": "http://arxiv.org/abs/2409.10213v1", "date": "2024-09-16", "relevancy": 2.7292, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20Facial%20Analysis%20with%20Cross-Modal%20Supervision&body=Title%3A%20Neuromorphic%20Facial%20Analysis%20with%20Cross-Modal%20Supervision%0AAuthor%3A%20Federico%20Becattini%20and%20Luca%20Cultrera%20and%20Lorenzo%20Berlincioni%20and%20Claudio%20Ferrari%20and%20Andrea%20Leonardo%20and%20Alberto%20Del%20Bimbo%0AAbstract%3A%20%20%20Traditional%20approaches%20for%20analyzing%20RGB%20frames%20are%20capable%20of%20providing%20a%0Afine-grained%20understanding%20of%20a%20face%20from%20different%20angles%20by%20inferring%0Aemotions%2C%20poses%2C%20shapes%2C%20landmarks.%20However%2C%20when%20it%20comes%20to%20subtle%20movements%0Astandard%20RGB%20cameras%20might%20fall%20behind%20due%20to%20their%20latency%2C%20making%20it%20hard%20to%0Adetect%20micro-movements%20that%20carry%20highly%20informative%20cues%20to%20infer%20the%20true%0Aemotions%20of%20a%20subject.%20To%20address%20this%20issue%2C%20the%20usage%20of%20event%20cameras%20to%0Aanalyze%20faces%20is%20gaining%20increasing%20interest.%20Nonetheless%2C%20all%20the%20expertise%0Amatured%20for%20RGB%20processing%20is%20not%20directly%20transferrable%20to%20neuromorphic%20data%0Adue%20to%20a%20strong%20domain%20shift%20and%20intrinsic%20differences%20in%20how%20data%20is%0Arepresented.%20The%20lack%20of%20labeled%20data%20can%20be%20considered%20one%20of%20the%20main%20causes%0Aof%20this%20gap%2C%20yet%20gathering%20data%20is%20harder%20in%20the%20event%20domain%20since%20it%20cannot%0Abe%20crawled%20from%20the%20web%20and%20labeling%20frames%20should%20take%20into%20account%20event%0Aaggregation%20rates%20and%20the%20fact%20that%20static%20parts%20might%20not%20be%20visible%20in%0Acertain%20frames.%20In%20this%20paper%2C%20we%20first%20present%20FACEMORPHIC%2C%20a%20multimodal%0Atemporally%20synchronized%20face%20dataset%20comprising%20both%20RGB%20videos%20and%20event%0Astreams.%20The%20data%20is%20labeled%20at%20a%20video%20level%20with%20facial%20Action%20Units%20and%20also%0Acontains%20streams%20collected%20with%20a%20variety%20of%20applications%20in%20mind%2C%20ranging%20from%0A3D%20shape%20estimation%20to%20lip-reading.%20We%20then%20show%20how%20temporal%20synchronization%0Acan%20allow%20effective%20neuromorphic%20face%20analysis%20without%20the%20need%20to%20manually%0Aannotate%20videos%3A%20we%20instead%20leverage%20cross-modal%20supervision%20bridging%20the%0Adomain%20gap%20by%20representing%20face%20shapes%20in%20a%203D%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520Facial%2520Analysis%2520with%2520Cross-Modal%2520Supervision%26entry.906535625%3DFederico%2520Becattini%2520and%2520Luca%2520Cultrera%2520and%2520Lorenzo%2520Berlincioni%2520and%2520Claudio%2520Ferrari%2520and%2520Andrea%2520Leonardo%2520and%2520Alberto%2520Del%2520Bimbo%26entry.1292438233%3D%2520%2520Traditional%2520approaches%2520for%2520analyzing%2520RGB%2520frames%2520are%2520capable%2520of%2520providing%2520a%250Afine-grained%2520understanding%2520of%2520a%2520face%2520from%2520different%2520angles%2520by%2520inferring%250Aemotions%252C%2520poses%252C%2520shapes%252C%2520landmarks.%2520However%252C%2520when%2520it%2520comes%2520to%2520subtle%2520movements%250Astandard%2520RGB%2520cameras%2520might%2520fall%2520behind%2520due%2520to%2520their%2520latency%252C%2520making%2520it%2520hard%2520to%250Adetect%2520micro-movements%2520that%2520carry%2520highly%2520informative%2520cues%2520to%2520infer%2520the%2520true%250Aemotions%2520of%2520a%2520subject.%2520To%2520address%2520this%2520issue%252C%2520the%2520usage%2520of%2520event%2520cameras%2520to%250Aanalyze%2520faces%2520is%2520gaining%2520increasing%2520interest.%2520Nonetheless%252C%2520all%2520the%2520expertise%250Amatured%2520for%2520RGB%2520processing%2520is%2520not%2520directly%2520transferrable%2520to%2520neuromorphic%2520data%250Adue%2520to%2520a%2520strong%2520domain%2520shift%2520and%2520intrinsic%2520differences%2520in%2520how%2520data%2520is%250Arepresented.%2520The%2520lack%2520of%2520labeled%2520data%2520can%2520be%2520considered%2520one%2520of%2520the%2520main%2520causes%250Aof%2520this%2520gap%252C%2520yet%2520gathering%2520data%2520is%2520harder%2520in%2520the%2520event%2520domain%2520since%2520it%2520cannot%250Abe%2520crawled%2520from%2520the%2520web%2520and%2520labeling%2520frames%2520should%2520take%2520into%2520account%2520event%250Aaggregation%2520rates%2520and%2520the%2520fact%2520that%2520static%2520parts%2520might%2520not%2520be%2520visible%2520in%250Acertain%2520frames.%2520In%2520this%2520paper%252C%2520we%2520first%2520present%2520FACEMORPHIC%252C%2520a%2520multimodal%250Atemporally%2520synchronized%2520face%2520dataset%2520comprising%2520both%2520RGB%2520videos%2520and%2520event%250Astreams.%2520The%2520data%2520is%2520labeled%2520at%2520a%2520video%2520level%2520with%2520facial%2520Action%2520Units%2520and%2520also%250Acontains%2520streams%2520collected%2520with%2520a%2520variety%2520of%2520applications%2520in%2520mind%252C%2520ranging%2520from%250A3D%2520shape%2520estimation%2520to%2520lip-reading.%2520We%2520then%2520show%2520how%2520temporal%2520synchronization%250Acan%2520allow%2520effective%2520neuromorphic%2520face%2520analysis%2520without%2520the%2520need%2520to%2520manually%250Aannotate%2520videos%253A%2520we%2520instead%2520leverage%2520cross-modal%2520supervision%2520bridging%2520the%250Adomain%2520gap%2520by%2520representing%2520face%2520shapes%2520in%2520a%25203D%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20Facial%20Analysis%20with%20Cross-Modal%20Supervision&entry.906535625=Federico%20Becattini%20and%20Luca%20Cultrera%20and%20Lorenzo%20Berlincioni%20and%20Claudio%20Ferrari%20and%20Andrea%20Leonardo%20and%20Alberto%20Del%20Bimbo&entry.1292438233=%20%20Traditional%20approaches%20for%20analyzing%20RGB%20frames%20are%20capable%20of%20providing%20a%0Afine-grained%20understanding%20of%20a%20face%20from%20different%20angles%20by%20inferring%0Aemotions%2C%20poses%2C%20shapes%2C%20landmarks.%20However%2C%20when%20it%20comes%20to%20subtle%20movements%0Astandard%20RGB%20cameras%20might%20fall%20behind%20due%20to%20their%20latency%2C%20making%20it%20hard%20to%0Adetect%20micro-movements%20that%20carry%20highly%20informative%20cues%20to%20infer%20the%20true%0Aemotions%20of%20a%20subject.%20To%20address%20this%20issue%2C%20the%20usage%20of%20event%20cameras%20to%0Aanalyze%20faces%20is%20gaining%20increasing%20interest.%20Nonetheless%2C%20all%20the%20expertise%0Amatured%20for%20RGB%20processing%20is%20not%20directly%20transferrable%20to%20neuromorphic%20data%0Adue%20to%20a%20strong%20domain%20shift%20and%20intrinsic%20differences%20in%20how%20data%20is%0Arepresented.%20The%20lack%20of%20labeled%20data%20can%20be%20considered%20one%20of%20the%20main%20causes%0Aof%20this%20gap%2C%20yet%20gathering%20data%20is%20harder%20in%20the%20event%20domain%20since%20it%20cannot%0Abe%20crawled%20from%20the%20web%20and%20labeling%20frames%20should%20take%20into%20account%20event%0Aaggregation%20rates%20and%20the%20fact%20that%20static%20parts%20might%20not%20be%20visible%20in%0Acertain%20frames.%20In%20this%20paper%2C%20we%20first%20present%20FACEMORPHIC%2C%20a%20multimodal%0Atemporally%20synchronized%20face%20dataset%20comprising%20both%20RGB%20videos%20and%20event%0Astreams.%20The%20data%20is%20labeled%20at%20a%20video%20level%20with%20facial%20Action%20Units%20and%20also%0Acontains%20streams%20collected%20with%20a%20variety%20of%20applications%20in%20mind%2C%20ranging%20from%0A3D%20shape%20estimation%20to%20lip-reading.%20We%20then%20show%20how%20temporal%20synchronization%0Acan%20allow%20effective%20neuromorphic%20face%20analysis%20without%20the%20need%20to%20manually%0Aannotate%20videos%3A%20we%20instead%20leverage%20cross-modal%20supervision%20bridging%20the%0Adomain%20gap%20by%20representing%20face%20shapes%20in%20a%203D%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10213v1&entry.124074799=Read"},
{"title": "DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in\n  Autonomous Driving", "author": "Songning Lai and Tianlang Xue and Hongru Xiao and Lijie Hu and Jiemin Wu and Ninghui Feng and Runwei Guan and Haicheng Liao and Zhenning Li and Yutao Yue", "abstract": "  Recent advancements in autonomous driving have seen a paradigm shift towards\nend-to-end learning paradigms, which map sensory inputs directly to driving\nactions, thereby enhancing the robustness and adaptability of autonomous\nvehicles. However, these models often sacrifice interpretability, posing\nsignificant challenges to trust, safety, and regulatory compliance. To address\nthese issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary\nEnsemble Framework in Autonomous Driving, a comprehensive framework designed to\nimprove the dependability and stability of explanations in end-to-end\nunsupervised autonomous driving models. Our work specifically targets the\ninherent instability problems observed in the Driving through the Concept\nGridlock (DCG) model, which undermine the trustworthiness of its explanations\nand decision-making processes. We define four key attributes of DRIVE:\nconsistent interpretability, stable interpretability, consistent output, and\nstable output. These attributes collectively ensure that explanations remain\nreliable and robust across different scenarios and perturbations. Through\nextensive empirical evaluations, we demonstrate the effectiveness of our\nframework in enhancing the stability and dependability of explanations, thereby\naddressing the limitations of current models. Our contributions include an\nin-depth analysis of the dependability issues within the DCG model, a rigorous\ndefinition of DRIVE with its fundamental properties, a framework to implement\nDRIVE, and novel metrics for evaluating the dependability of concept-based\nexplainable autonomous driving models. These advancements lay the groundwork\nfor the development of more reliable and trusted autonomous driving systems,\npaving the way for their broader acceptance and deployment in real-world\napplications.\n", "link": "http://arxiv.org/abs/2409.10330v1", "date": "2024-09-16", "relevancy": 2.7246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRIVE%3A%20Dependable%20Robust%20Interpretable%20Visionary%20Ensemble%20Framework%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20DRIVE%3A%20Dependable%20Robust%20Interpretable%20Visionary%20Ensemble%20Framework%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Songning%20Lai%20and%20Tianlang%20Xue%20and%20Hongru%20Xiao%20and%20Lijie%20Hu%20and%20Jiemin%20Wu%20and%20Ninghui%20Feng%20and%20Runwei%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20Recent%20advancements%20in%20autonomous%20driving%20have%20seen%20a%20paradigm%20shift%20towards%0Aend-to-end%20learning%20paradigms%2C%20which%20map%20sensory%20inputs%20directly%20to%20driving%0Aactions%2C%20thereby%20enhancing%20the%20robustness%20and%20adaptability%20of%20autonomous%0Avehicles.%20However%2C%20these%20models%20often%20sacrifice%20interpretability%2C%20posing%0Asignificant%20challenges%20to%20trust%2C%20safety%2C%20and%20regulatory%20compliance.%20To%20address%0Athese%20issues%2C%20we%20introduce%20DRIVE%20--%20Dependable%20Robust%20Interpretable%20Visionary%0AEnsemble%20Framework%20in%20Autonomous%20Driving%2C%20a%20comprehensive%20framework%20designed%20to%0Aimprove%20the%20dependability%20and%20stability%20of%20explanations%20in%20end-to-end%0Aunsupervised%20autonomous%20driving%20models.%20Our%20work%20specifically%20targets%20the%0Ainherent%20instability%20problems%20observed%20in%20the%20Driving%20through%20the%20Concept%0AGridlock%20%28DCG%29%20model%2C%20which%20undermine%20the%20trustworthiness%20of%20its%20explanations%0Aand%20decision-making%20processes.%20We%20define%20four%20key%20attributes%20of%20DRIVE%3A%0Aconsistent%20interpretability%2C%20stable%20interpretability%2C%20consistent%20output%2C%20and%0Astable%20output.%20These%20attributes%20collectively%20ensure%20that%20explanations%20remain%0Areliable%20and%20robust%20across%20different%20scenarios%20and%20perturbations.%20Through%0Aextensive%20empirical%20evaluations%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%0Aframework%20in%20enhancing%20the%20stability%20and%20dependability%20of%20explanations%2C%20thereby%0Aaddressing%20the%20limitations%20of%20current%20models.%20Our%20contributions%20include%20an%0Ain-depth%20analysis%20of%20the%20dependability%20issues%20within%20the%20DCG%20model%2C%20a%20rigorous%0Adefinition%20of%20DRIVE%20with%20its%20fundamental%20properties%2C%20a%20framework%20to%20implement%0ADRIVE%2C%20and%20novel%20metrics%20for%20evaluating%20the%20dependability%20of%20concept-based%0Aexplainable%20autonomous%20driving%20models.%20These%20advancements%20lay%20the%20groundwork%0Afor%20the%20development%20of%20more%20reliable%20and%20trusted%20autonomous%20driving%20systems%2C%0Apaving%20the%20way%20for%20their%20broader%20acceptance%20and%20deployment%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRIVE%253A%2520Dependable%2520Robust%2520Interpretable%2520Visionary%2520Ensemble%2520Framework%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DSongning%2520Lai%2520and%2520Tianlang%2520Xue%2520and%2520Hongru%2520Xiao%2520and%2520Lijie%2520Hu%2520and%2520Jiemin%2520Wu%2520and%2520Ninghui%2520Feng%2520and%2520Runwei%2520Guan%2520and%2520Haicheng%2520Liao%2520and%2520Zhenning%2520Li%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520autonomous%2520driving%2520have%2520seen%2520a%2520paradigm%2520shift%2520towards%250Aend-to-end%2520learning%2520paradigms%252C%2520which%2520map%2520sensory%2520inputs%2520directly%2520to%2520driving%250Aactions%252C%2520thereby%2520enhancing%2520the%2520robustness%2520and%2520adaptability%2520of%2520autonomous%250Avehicles.%2520However%252C%2520these%2520models%2520often%2520sacrifice%2520interpretability%252C%2520posing%250Asignificant%2520challenges%2520to%2520trust%252C%2520safety%252C%2520and%2520regulatory%2520compliance.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520DRIVE%2520--%2520Dependable%2520Robust%2520Interpretable%2520Visionary%250AEnsemble%2520Framework%2520in%2520Autonomous%2520Driving%252C%2520a%2520comprehensive%2520framework%2520designed%2520to%250Aimprove%2520the%2520dependability%2520and%2520stability%2520of%2520explanations%2520in%2520end-to-end%250Aunsupervised%2520autonomous%2520driving%2520models.%2520Our%2520work%2520specifically%2520targets%2520the%250Ainherent%2520instability%2520problems%2520observed%2520in%2520the%2520Driving%2520through%2520the%2520Concept%250AGridlock%2520%2528DCG%2529%2520model%252C%2520which%2520undermine%2520the%2520trustworthiness%2520of%2520its%2520explanations%250Aand%2520decision-making%2520processes.%2520We%2520define%2520four%2520key%2520attributes%2520of%2520DRIVE%253A%250Aconsistent%2520interpretability%252C%2520stable%2520interpretability%252C%2520consistent%2520output%252C%2520and%250Astable%2520output.%2520These%2520attributes%2520collectively%2520ensure%2520that%2520explanations%2520remain%250Areliable%2520and%2520robust%2520across%2520different%2520scenarios%2520and%2520perturbations.%2520Through%250Aextensive%2520empirical%2520evaluations%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aframework%2520in%2520enhancing%2520the%2520stability%2520and%2520dependability%2520of%2520explanations%252C%2520thereby%250Aaddressing%2520the%2520limitations%2520of%2520current%2520models.%2520Our%2520contributions%2520include%2520an%250Ain-depth%2520analysis%2520of%2520the%2520dependability%2520issues%2520within%2520the%2520DCG%2520model%252C%2520a%2520rigorous%250Adefinition%2520of%2520DRIVE%2520with%2520its%2520fundamental%2520properties%252C%2520a%2520framework%2520to%2520implement%250ADRIVE%252C%2520and%2520novel%2520metrics%2520for%2520evaluating%2520the%2520dependability%2520of%2520concept-based%250Aexplainable%2520autonomous%2520driving%2520models.%2520These%2520advancements%2520lay%2520the%2520groundwork%250Afor%2520the%2520development%2520of%2520more%2520reliable%2520and%2520trusted%2520autonomous%2520driving%2520systems%252C%250Apaving%2520the%2520way%2520for%2520their%2520broader%2520acceptance%2520and%2520deployment%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRIVE%3A%20Dependable%20Robust%20Interpretable%20Visionary%20Ensemble%20Framework%20in%0A%20%20Autonomous%20Driving&entry.906535625=Songning%20Lai%20and%20Tianlang%20Xue%20and%20Hongru%20Xiao%20and%20Lijie%20Hu%20and%20Jiemin%20Wu%20and%20Ninghui%20Feng%20and%20Runwei%20Guan%20and%20Haicheng%20Liao%20and%20Zhenning%20Li%20and%20Yutao%20Yue&entry.1292438233=%20%20Recent%20advancements%20in%20autonomous%20driving%20have%20seen%20a%20paradigm%20shift%20towards%0Aend-to-end%20learning%20paradigms%2C%20which%20map%20sensory%20inputs%20directly%20to%20driving%0Aactions%2C%20thereby%20enhancing%20the%20robustness%20and%20adaptability%20of%20autonomous%0Avehicles.%20However%2C%20these%20models%20often%20sacrifice%20interpretability%2C%20posing%0Asignificant%20challenges%20to%20trust%2C%20safety%2C%20and%20regulatory%20compliance.%20To%20address%0Athese%20issues%2C%20we%20introduce%20DRIVE%20--%20Dependable%20Robust%20Interpretable%20Visionary%0AEnsemble%20Framework%20in%20Autonomous%20Driving%2C%20a%20comprehensive%20framework%20designed%20to%0Aimprove%20the%20dependability%20and%20stability%20of%20explanations%20in%20end-to-end%0Aunsupervised%20autonomous%20driving%20models.%20Our%20work%20specifically%20targets%20the%0Ainherent%20instability%20problems%20observed%20in%20the%20Driving%20through%20the%20Concept%0AGridlock%20%28DCG%29%20model%2C%20which%20undermine%20the%20trustworthiness%20of%20its%20explanations%0Aand%20decision-making%20processes.%20We%20define%20four%20key%20attributes%20of%20DRIVE%3A%0Aconsistent%20interpretability%2C%20stable%20interpretability%2C%20consistent%20output%2C%20and%0Astable%20output.%20These%20attributes%20collectively%20ensure%20that%20explanations%20remain%0Areliable%20and%20robust%20across%20different%20scenarios%20and%20perturbations.%20Through%0Aextensive%20empirical%20evaluations%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%0Aframework%20in%20enhancing%20the%20stability%20and%20dependability%20of%20explanations%2C%20thereby%0Aaddressing%20the%20limitations%20of%20current%20models.%20Our%20contributions%20include%20an%0Ain-depth%20analysis%20of%20the%20dependability%20issues%20within%20the%20DCG%20model%2C%20a%20rigorous%0Adefinition%20of%20DRIVE%20with%20its%20fundamental%20properties%2C%20a%20framework%20to%20implement%0ADRIVE%2C%20and%20novel%20metrics%20for%20evaluating%20the%20dependability%20of%20concept-based%0Aexplainable%20autonomous%20driving%20models.%20These%20advancements%20lay%20the%20groundwork%0Afor%20the%20development%20of%20more%20reliable%20and%20trusted%20autonomous%20driving%20systems%2C%0Apaving%20the%20way%20for%20their%20broader%20acceptance%20and%20deployment%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10330v1&entry.124074799=Read"},
{"title": "DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention\n  and Text Guidance", "author": "Cong Wang and Jiaxi Gu and Panwen Hu and Songcen Xu and Hang Xu and Xiaodan Liang", "abstract": "  Image-to-video generation, which aims to generate a video starting from a\ngiven reference image, has drawn great attention. Existing methods try to\nextend pre-trained text-guided image diffusion models to image-guided video\ngeneration models. Nevertheless, these methods often result in either low\nfidelity or flickering over time due to their limitation to shallow image\nguidance and poor temporal consistency. To tackle these problems, we propose a\nhigh-fidelity image-to-video generation method by devising a frame retention\nbranch based on a pre-trained video diffusion model, named DreamVideo. Instead\nof integrating the reference image into the diffusion process at a semantic\nlevel, our DreamVideo perceives the reference image via convolution layers and\nconcatenates the features with the noisy latents as model input. By this means,\nthe details of the reference image can be preserved to the greatest extent. In\naddition, by incorporating double-condition classifier-free guidance, a single\nimage can be directed to videos of different actions by providing varying\nprompt texts. This has significant implications for controllable video\ngeneration and holds broad application prospects. We conduct comprehensive\nexperiments on the public dataset, and both quantitative and qualitative\nresults indicate that our method outperforms the state-of-the-art method.\nEspecially for fidelity, our model has a powerful image retention ability and\ndelivers the best results in UCF101 compared to other image-to-video models to\nour best knowledge. Also, precise control can be achieved by giving different\ntext prompts. Further details and comprehensive results of our model will be\npresented in https://anonymous0769.github.io/DreamVideo/.\n", "link": "http://arxiv.org/abs/2312.03018v4", "date": "2024-09-16", "relevancy": 2.7189, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6984}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6775}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamVideo%3A%20High-Fidelity%20Image-to-Video%20Generation%20with%20Image%20Retention%0A%20%20and%20Text%20Guidance&body=Title%3A%20DreamVideo%3A%20High-Fidelity%20Image-to-Video%20Generation%20with%20Image%20Retention%0A%20%20and%20Text%20Guidance%0AAuthor%3A%20Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Songcen%20Xu%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Image-to-video%20generation%2C%20which%20aims%20to%20generate%20a%20video%20starting%20from%20a%0Agiven%20reference%20image%2C%20has%20drawn%20great%20attention.%20Existing%20methods%20try%20to%0Aextend%20pre-trained%20text-guided%20image%20diffusion%20models%20to%20image-guided%20video%0Ageneration%20models.%20Nevertheless%2C%20these%20methods%20often%20result%20in%20either%20low%0Afidelity%20or%20flickering%20over%20time%20due%20to%20their%20limitation%20to%20shallow%20image%0Aguidance%20and%20poor%20temporal%20consistency.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%0Ahigh-fidelity%20image-to-video%20generation%20method%20by%20devising%20a%20frame%20retention%0Abranch%20based%20on%20a%20pre-trained%20video%20diffusion%20model%2C%20named%20DreamVideo.%20Instead%0Aof%20integrating%20the%20reference%20image%20into%20the%20diffusion%20process%20at%20a%20semantic%0Alevel%2C%20our%20DreamVideo%20perceives%20the%20reference%20image%20via%20convolution%20layers%20and%0Aconcatenates%20the%20features%20with%20the%20noisy%20latents%20as%20model%20input.%20By%20this%20means%2C%0Athe%20details%20of%20the%20reference%20image%20can%20be%20preserved%20to%20the%20greatest%20extent.%20In%0Aaddition%2C%20by%20incorporating%20double-condition%20classifier-free%20guidance%2C%20a%20single%0Aimage%20can%20be%20directed%20to%20videos%20of%20different%20actions%20by%20providing%20varying%0Aprompt%20texts.%20This%20has%20significant%20implications%20for%20controllable%20video%0Ageneration%20and%20holds%20broad%20application%20prospects.%20We%20conduct%20comprehensive%0Aexperiments%20on%20the%20public%20dataset%2C%20and%20both%20quantitative%20and%20qualitative%0Aresults%20indicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20method.%0AEspecially%20for%20fidelity%2C%20our%20model%20has%20a%20powerful%20image%20retention%20ability%20and%0Adelivers%20the%20best%20results%20in%20UCF101%20compared%20to%20other%20image-to-video%20models%20to%0Aour%20best%20knowledge.%20Also%2C%20precise%20control%20can%20be%20achieved%20by%20giving%20different%0Atext%20prompts.%20Further%20details%20and%20comprehensive%20results%20of%20our%20model%20will%20be%0Apresented%20in%20https%3A//anonymous0769.github.io/DreamVideo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03018v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamVideo%253A%2520High-Fidelity%2520Image-to-Video%2520Generation%2520with%2520Image%2520Retention%250A%2520%2520and%2520Text%2520Guidance%26entry.906535625%3DCong%2520Wang%2520and%2520Jiaxi%2520Gu%2520and%2520Panwen%2520Hu%2520and%2520Songcen%2520Xu%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Image-to-video%2520generation%252C%2520which%2520aims%2520to%2520generate%2520a%2520video%2520starting%2520from%2520a%250Agiven%2520reference%2520image%252C%2520has%2520drawn%2520great%2520attention.%2520Existing%2520methods%2520try%2520to%250Aextend%2520pre-trained%2520text-guided%2520image%2520diffusion%2520models%2520to%2520image-guided%2520video%250Ageneration%2520models.%2520Nevertheless%252C%2520these%2520methods%2520often%2520result%2520in%2520either%2520low%250Afidelity%2520or%2520flickering%2520over%2520time%2520due%2520to%2520their%2520limitation%2520to%2520shallow%2520image%250Aguidance%2520and%2520poor%2520temporal%2520consistency.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%250Ahigh-fidelity%2520image-to-video%2520generation%2520method%2520by%2520devising%2520a%2520frame%2520retention%250Abranch%2520based%2520on%2520a%2520pre-trained%2520video%2520diffusion%2520model%252C%2520named%2520DreamVideo.%2520Instead%250Aof%2520integrating%2520the%2520reference%2520image%2520into%2520the%2520diffusion%2520process%2520at%2520a%2520semantic%250Alevel%252C%2520our%2520DreamVideo%2520perceives%2520the%2520reference%2520image%2520via%2520convolution%2520layers%2520and%250Aconcatenates%2520the%2520features%2520with%2520the%2520noisy%2520latents%2520as%2520model%2520input.%2520By%2520this%2520means%252C%250Athe%2520details%2520of%2520the%2520reference%2520image%2520can%2520be%2520preserved%2520to%2520the%2520greatest%2520extent.%2520In%250Aaddition%252C%2520by%2520incorporating%2520double-condition%2520classifier-free%2520guidance%252C%2520a%2520single%250Aimage%2520can%2520be%2520directed%2520to%2520videos%2520of%2520different%2520actions%2520by%2520providing%2520varying%250Aprompt%2520texts.%2520This%2520has%2520significant%2520implications%2520for%2520controllable%2520video%250Ageneration%2520and%2520holds%2520broad%2520application%2520prospects.%2520We%2520conduct%2520comprehensive%250Aexperiments%2520on%2520the%2520public%2520dataset%252C%2520and%2520both%2520quantitative%2520and%2520qualitative%250Aresults%2520indicate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520method.%250AEspecially%2520for%2520fidelity%252C%2520our%2520model%2520has%2520a%2520powerful%2520image%2520retention%2520ability%2520and%250Adelivers%2520the%2520best%2520results%2520in%2520UCF101%2520compared%2520to%2520other%2520image-to-video%2520models%2520to%250Aour%2520best%2520knowledge.%2520Also%252C%2520precise%2520control%2520can%2520be%2520achieved%2520by%2520giving%2520different%250Atext%2520prompts.%2520Further%2520details%2520and%2520comprehensive%2520results%2520of%2520our%2520model%2520will%2520be%250Apresented%2520in%2520https%253A//anonymous0769.github.io/DreamVideo/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03018v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamVideo%3A%20High-Fidelity%20Image-to-Video%20Generation%20with%20Image%20Retention%0A%20%20and%20Text%20Guidance&entry.906535625=Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Songcen%20Xu%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Image-to-video%20generation%2C%20which%20aims%20to%20generate%20a%20video%20starting%20from%20a%0Agiven%20reference%20image%2C%20has%20drawn%20great%20attention.%20Existing%20methods%20try%20to%0Aextend%20pre-trained%20text-guided%20image%20diffusion%20models%20to%20image-guided%20video%0Ageneration%20models.%20Nevertheless%2C%20these%20methods%20often%20result%20in%20either%20low%0Afidelity%20or%20flickering%20over%20time%20due%20to%20their%20limitation%20to%20shallow%20image%0Aguidance%20and%20poor%20temporal%20consistency.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%0Ahigh-fidelity%20image-to-video%20generation%20method%20by%20devising%20a%20frame%20retention%0Abranch%20based%20on%20a%20pre-trained%20video%20diffusion%20model%2C%20named%20DreamVideo.%20Instead%0Aof%20integrating%20the%20reference%20image%20into%20the%20diffusion%20process%20at%20a%20semantic%0Alevel%2C%20our%20DreamVideo%20perceives%20the%20reference%20image%20via%20convolution%20layers%20and%0Aconcatenates%20the%20features%20with%20the%20noisy%20latents%20as%20model%20input.%20By%20this%20means%2C%0Athe%20details%20of%20the%20reference%20image%20can%20be%20preserved%20to%20the%20greatest%20extent.%20In%0Aaddition%2C%20by%20incorporating%20double-condition%20classifier-free%20guidance%2C%20a%20single%0Aimage%20can%20be%20directed%20to%20videos%20of%20different%20actions%20by%20providing%20varying%0Aprompt%20texts.%20This%20has%20significant%20implications%20for%20controllable%20video%0Ageneration%20and%20holds%20broad%20application%20prospects.%20We%20conduct%20comprehensive%0Aexperiments%20on%20the%20public%20dataset%2C%20and%20both%20quantitative%20and%20qualitative%0Aresults%20indicate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20method.%0AEspecially%20for%20fidelity%2C%20our%20model%20has%20a%20powerful%20image%20retention%20ability%20and%0Adelivers%20the%20best%20results%20in%20UCF101%20compared%20to%20other%20image-to-video%20models%20to%0Aour%20best%20knowledge.%20Also%2C%20precise%20control%20can%20be%20achieved%20by%20giving%20different%0Atext%20prompts.%20Further%20details%20and%20comprehensive%20results%20of%20our%20model%20will%20be%0Apresented%20in%20https%3A//anonymous0769.github.io/DreamVideo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03018v4&entry.124074799=Read"},
{"title": "Learning Semi-Supervised Medical Image Segmentation from Spatial\n  Registration", "author": "Qianying Liu and Paul Henderson and Xiao Gu and Hang Dai and Fani Deligianni", "abstract": "  Semi-supervised medical image segmentation has shown promise in training\nmodels with limited labeled data and abundant unlabeled data. However,\nstate-of-the-art methods ignore a potentially valuable source of unsupervised\nsemantic information -- spatial registration transforms between image volumes.\nTo address this, we propose CCT-R, a contrastive cross-teaching framework\nincorporating registration information. To leverage the semantic information\navailable in registrations between volume pairs, CCT-R incorporates two\nproposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced\nPositive Sampling (REPS). The RSL leverages segmentation knowledge derived from\ntransforms between labeled and unlabeled volume pairs, providing an additional\nsource of pseudo-labels. REPS enhances contrastive learning by identifying\nanatomically-corresponding positives across volumes using registration\ntransforms. Experimental results on two challenging medical segmentation\nbenchmarks demonstrate the effectiveness and superiority of CCT-R across\nvarious semi-supervised settings, with as few as one labeled case. Our code is\navailable at\nhttps://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.\n", "link": "http://arxiv.org/abs/2409.10422v1", "date": "2024-09-16", "relevancy": 2.6987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5327}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Semi-Supervised%20Medical%20Image%20Segmentation%20from%20Spatial%0A%20%20Registration&body=Title%3A%20Learning%20Semi-Supervised%20Medical%20Image%20Segmentation%20from%20Spatial%0A%20%20Registration%0AAuthor%3A%20Qianying%20Liu%20and%20Paul%20Henderson%20and%20Xiao%20Gu%20and%20Hang%20Dai%20and%20Fani%20Deligianni%0AAbstract%3A%20%20%20Semi-supervised%20medical%20image%20segmentation%20has%20shown%20promise%20in%20training%0Amodels%20with%20limited%20labeled%20data%20and%20abundant%20unlabeled%20data.%20However%2C%0Astate-of-the-art%20methods%20ignore%20a%20potentially%20valuable%20source%20of%20unsupervised%0Asemantic%20information%20--%20spatial%20registration%20transforms%20between%20image%20volumes.%0ATo%20address%20this%2C%20we%20propose%20CCT-R%2C%20a%20contrastive%20cross-teaching%20framework%0Aincorporating%20registration%20information.%20To%20leverage%20the%20semantic%20information%0Aavailable%20in%20registrations%20between%20volume%20pairs%2C%20CCT-R%20incorporates%20two%0Aproposed%20modules%3A%20Registration%20Supervision%20Loss%20%28RSL%29%20and%20Registration-Enhanced%0APositive%20Sampling%20%28REPS%29.%20The%20RSL%20leverages%20segmentation%20knowledge%20derived%20from%0Atransforms%20between%20labeled%20and%20unlabeled%20volume%20pairs%2C%20providing%20an%20additional%0Asource%20of%20pseudo-labels.%20REPS%20enhances%20contrastive%20learning%20by%20identifying%0Aanatomically-corresponding%20positives%20across%20volumes%20using%20registration%0Atransforms.%20Experimental%20results%20on%20two%20challenging%20medical%20segmentation%0Abenchmarks%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20CCT-R%20across%0Avarious%20semi-supervised%20settings%2C%20with%20as%20few%20as%20one%20labeled%20case.%20Our%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Semi-Supervised%2520Medical%2520Image%2520Segmentation%2520from%2520Spatial%250A%2520%2520Registration%26entry.906535625%3DQianying%2520Liu%2520and%2520Paul%2520Henderson%2520and%2520Xiao%2520Gu%2520and%2520Hang%2520Dai%2520and%2520Fani%2520Deligianni%26entry.1292438233%3D%2520%2520Semi-supervised%2520medical%2520image%2520segmentation%2520has%2520shown%2520promise%2520in%2520training%250Amodels%2520with%2520limited%2520labeled%2520data%2520and%2520abundant%2520unlabeled%2520data.%2520However%252C%250Astate-of-the-art%2520methods%2520ignore%2520a%2520potentially%2520valuable%2520source%2520of%2520unsupervised%250Asemantic%2520information%2520--%2520spatial%2520registration%2520transforms%2520between%2520image%2520volumes.%250ATo%2520address%2520this%252C%2520we%2520propose%2520CCT-R%252C%2520a%2520contrastive%2520cross-teaching%2520framework%250Aincorporating%2520registration%2520information.%2520To%2520leverage%2520the%2520semantic%2520information%250Aavailable%2520in%2520registrations%2520between%2520volume%2520pairs%252C%2520CCT-R%2520incorporates%2520two%250Aproposed%2520modules%253A%2520Registration%2520Supervision%2520Loss%2520%2528RSL%2529%2520and%2520Registration-Enhanced%250APositive%2520Sampling%2520%2528REPS%2529.%2520The%2520RSL%2520leverages%2520segmentation%2520knowledge%2520derived%2520from%250Atransforms%2520between%2520labeled%2520and%2520unlabeled%2520volume%2520pairs%252C%2520providing%2520an%2520additional%250Asource%2520of%2520pseudo-labels.%2520REPS%2520enhances%2520contrastive%2520learning%2520by%2520identifying%250Aanatomically-corresponding%2520positives%2520across%2520volumes%2520using%2520registration%250Atransforms.%2520Experimental%2520results%2520on%2520two%2520challenging%2520medical%2520segmentation%250Abenchmarks%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520CCT-R%2520across%250Avarious%2520semi-supervised%2520settings%252C%2520with%2520as%2520few%2520as%2520one%2520labeled%2520case.%2520Our%2520code%2520is%250Aavailable%2520at%250Ahttps%253A//github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Semi-Supervised%20Medical%20Image%20Segmentation%20from%20Spatial%0A%20%20Registration&entry.906535625=Qianying%20Liu%20and%20Paul%20Henderson%20and%20Xiao%20Gu%20and%20Hang%20Dai%20and%20Fani%20Deligianni&entry.1292438233=%20%20Semi-supervised%20medical%20image%20segmentation%20has%20shown%20promise%20in%20training%0Amodels%20with%20limited%20labeled%20data%20and%20abundant%20unlabeled%20data.%20However%2C%0Astate-of-the-art%20methods%20ignore%20a%20potentially%20valuable%20source%20of%20unsupervised%0Asemantic%20information%20--%20spatial%20registration%20transforms%20between%20image%20volumes.%0ATo%20address%20this%2C%20we%20propose%20CCT-R%2C%20a%20contrastive%20cross-teaching%20framework%0Aincorporating%20registration%20information.%20To%20leverage%20the%20semantic%20information%0Aavailable%20in%20registrations%20between%20volume%20pairs%2C%20CCT-R%20incorporates%20two%0Aproposed%20modules%3A%20Registration%20Supervision%20Loss%20%28RSL%29%20and%20Registration-Enhanced%0APositive%20Sampling%20%28REPS%29.%20The%20RSL%20leverages%20segmentation%20knowledge%20derived%20from%0Atransforms%20between%20labeled%20and%20unlabeled%20volume%20pairs%2C%20providing%20an%20additional%0Asource%20of%20pseudo-labels.%20REPS%20enhances%20contrastive%20learning%20by%20identifying%0Aanatomically-corresponding%20positives%20across%20volumes%20using%20registration%0Atransforms.%20Experimental%20results%20on%20two%20challenging%20medical%20segmentation%0Abenchmarks%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20CCT-R%20across%0Avarious%20semi-supervised%20settings%2C%20with%20as%20few%20as%20one%20labeled%20case.%20Our%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10422v1&entry.124074799=Read"},
{"title": "P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty\n  and Pose Uncertainty", "author": "Yufan Zhang and Kailun Yang and Ze Wang and Kaiwei Wang", "abstract": "  This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping\n(SLAM) system with a wide Field of View (FoV) camera, which utilizes pose\nuncertainty and point uncertainty. While the wide FoV enables considerable\nrepetitive observations of historical map points for matching cross-view\nfeatures, the data properties of the historical map points and the poses of\nhistorical keyframes have changed during the optimization process. The neglect\nof data property changes triggers the absence of a partial information matrix\nin optimization and leads to the risk of long-term positioning performance\ndegradation. The purpose of our research is to reduce the risk of the wide\nfield of view visual input to the SLAM system. Based on the conditional\nprobability model, this work reveals the definite impact of the above data\nproperties changes on the optimization process, concretizes it as point\nuncertainty and pose uncertainty, and gives a specific mathematical form.\nP2U-SLAM respectively embeds point uncertainty and pose uncertainty into the\ntracking module and local mapping, and updates these uncertainties after each\noptimization operation including local mapping, map merging, and loop closing.\nWe present an exhaustive evaluation in 27 sequences from two popular public\ndatasets with wide-FoV visual input. P2U-SLAM shows excellent performance\ncompared with other state-of-the-art methods. The source code will be made\npublicly available at https://github.com/BambValley/P2U-SLAM.\n", "link": "http://arxiv.org/abs/2409.10143v1", "date": "2024-09-16", "relevancy": 2.6899, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7688}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6053}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P2U-SLAM%3A%20A%20Monocular%20Wide-FoV%20SLAM%20System%20Based%20on%20Point%20Uncertainty%0A%20%20and%20Pose%20Uncertainty&body=Title%3A%20P2U-SLAM%3A%20A%20Monocular%20Wide-FoV%20SLAM%20System%20Based%20on%20Point%20Uncertainty%0A%20%20and%20Pose%20Uncertainty%0AAuthor%3A%20Yufan%20Zhang%20and%20Kailun%20Yang%20and%20Ze%20Wang%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20This%20paper%20presents%20P2U-SLAM%2C%20a%20visual%20Simultaneous%20Localization%20And%20Mapping%0A%28SLAM%29%20system%20with%20a%20wide%20Field%20of%20View%20%28FoV%29%20camera%2C%20which%20utilizes%20pose%0Auncertainty%20and%20point%20uncertainty.%20While%20the%20wide%20FoV%20enables%20considerable%0Arepetitive%20observations%20of%20historical%20map%20points%20for%20matching%20cross-view%0Afeatures%2C%20the%20data%20properties%20of%20the%20historical%20map%20points%20and%20the%20poses%20of%0Ahistorical%20keyframes%20have%20changed%20during%20the%20optimization%20process.%20The%20neglect%0Aof%20data%20property%20changes%20triggers%20the%20absence%20of%20a%20partial%20information%20matrix%0Ain%20optimization%20and%20leads%20to%20the%20risk%20of%20long-term%20positioning%20performance%0Adegradation.%20The%20purpose%20of%20our%20research%20is%20to%20reduce%20the%20risk%20of%20the%20wide%0Afield%20of%20view%20visual%20input%20to%20the%20SLAM%20system.%20Based%20on%20the%20conditional%0Aprobability%20model%2C%20this%20work%20reveals%20the%20definite%20impact%20of%20the%20above%20data%0Aproperties%20changes%20on%20the%20optimization%20process%2C%20concretizes%20it%20as%20point%0Auncertainty%20and%20pose%20uncertainty%2C%20and%20gives%20a%20specific%20mathematical%20form.%0AP2U-SLAM%20respectively%20embeds%20point%20uncertainty%20and%20pose%20uncertainty%20into%20the%0Atracking%20module%20and%20local%20mapping%2C%20and%20updates%20these%20uncertainties%20after%20each%0Aoptimization%20operation%20including%20local%20mapping%2C%20map%20merging%2C%20and%20loop%20closing.%0AWe%20present%20an%20exhaustive%20evaluation%20in%2027%20sequences%20from%20two%20popular%20public%0Adatasets%20with%20wide-FoV%20visual%20input.%20P2U-SLAM%20shows%20excellent%20performance%0Acompared%20with%20other%20state-of-the-art%20methods.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/BambValley/P2U-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP2U-SLAM%253A%2520A%2520Monocular%2520Wide-FoV%2520SLAM%2520System%2520Based%2520on%2520Point%2520Uncertainty%250A%2520%2520and%2520Pose%2520Uncertainty%26entry.906535625%3DYufan%2520Zhang%2520and%2520Kailun%2520Yang%2520and%2520Ze%2520Wang%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520P2U-SLAM%252C%2520a%2520visual%2520Simultaneous%2520Localization%2520And%2520Mapping%250A%2528SLAM%2529%2520system%2520with%2520a%2520wide%2520Field%2520of%2520View%2520%2528FoV%2529%2520camera%252C%2520which%2520utilizes%2520pose%250Auncertainty%2520and%2520point%2520uncertainty.%2520While%2520the%2520wide%2520FoV%2520enables%2520considerable%250Arepetitive%2520observations%2520of%2520historical%2520map%2520points%2520for%2520matching%2520cross-view%250Afeatures%252C%2520the%2520data%2520properties%2520of%2520the%2520historical%2520map%2520points%2520and%2520the%2520poses%2520of%250Ahistorical%2520keyframes%2520have%2520changed%2520during%2520the%2520optimization%2520process.%2520The%2520neglect%250Aof%2520data%2520property%2520changes%2520triggers%2520the%2520absence%2520of%2520a%2520partial%2520information%2520matrix%250Ain%2520optimization%2520and%2520leads%2520to%2520the%2520risk%2520of%2520long-term%2520positioning%2520performance%250Adegradation.%2520The%2520purpose%2520of%2520our%2520research%2520is%2520to%2520reduce%2520the%2520risk%2520of%2520the%2520wide%250Afield%2520of%2520view%2520visual%2520input%2520to%2520the%2520SLAM%2520system.%2520Based%2520on%2520the%2520conditional%250Aprobability%2520model%252C%2520this%2520work%2520reveals%2520the%2520definite%2520impact%2520of%2520the%2520above%2520data%250Aproperties%2520changes%2520on%2520the%2520optimization%2520process%252C%2520concretizes%2520it%2520as%2520point%250Auncertainty%2520and%2520pose%2520uncertainty%252C%2520and%2520gives%2520a%2520specific%2520mathematical%2520form.%250AP2U-SLAM%2520respectively%2520embeds%2520point%2520uncertainty%2520and%2520pose%2520uncertainty%2520into%2520the%250Atracking%2520module%2520and%2520local%2520mapping%252C%2520and%2520updates%2520these%2520uncertainties%2520after%2520each%250Aoptimization%2520operation%2520including%2520local%2520mapping%252C%2520map%2520merging%252C%2520and%2520loop%2520closing.%250AWe%2520present%2520an%2520exhaustive%2520evaluation%2520in%252027%2520sequences%2520from%2520two%2520popular%2520public%250Adatasets%2520with%2520wide-FoV%2520visual%2520input.%2520P2U-SLAM%2520shows%2520excellent%2520performance%250Acompared%2520with%2520other%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520will%2520be%2520made%250Apublicly%2520available%2520at%2520https%253A//github.com/BambValley/P2U-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P2U-SLAM%3A%20A%20Monocular%20Wide-FoV%20SLAM%20System%20Based%20on%20Point%20Uncertainty%0A%20%20and%20Pose%20Uncertainty&entry.906535625=Yufan%20Zhang%20and%20Kailun%20Yang%20and%20Ze%20Wang%20and%20Kaiwei%20Wang&entry.1292438233=%20%20This%20paper%20presents%20P2U-SLAM%2C%20a%20visual%20Simultaneous%20Localization%20And%20Mapping%0A%28SLAM%29%20system%20with%20a%20wide%20Field%20of%20View%20%28FoV%29%20camera%2C%20which%20utilizes%20pose%0Auncertainty%20and%20point%20uncertainty.%20While%20the%20wide%20FoV%20enables%20considerable%0Arepetitive%20observations%20of%20historical%20map%20points%20for%20matching%20cross-view%0Afeatures%2C%20the%20data%20properties%20of%20the%20historical%20map%20points%20and%20the%20poses%20of%0Ahistorical%20keyframes%20have%20changed%20during%20the%20optimization%20process.%20The%20neglect%0Aof%20data%20property%20changes%20triggers%20the%20absence%20of%20a%20partial%20information%20matrix%0Ain%20optimization%20and%20leads%20to%20the%20risk%20of%20long-term%20positioning%20performance%0Adegradation.%20The%20purpose%20of%20our%20research%20is%20to%20reduce%20the%20risk%20of%20the%20wide%0Afield%20of%20view%20visual%20input%20to%20the%20SLAM%20system.%20Based%20on%20the%20conditional%0Aprobability%20model%2C%20this%20work%20reveals%20the%20definite%20impact%20of%20the%20above%20data%0Aproperties%20changes%20on%20the%20optimization%20process%2C%20concretizes%20it%20as%20point%0Auncertainty%20and%20pose%20uncertainty%2C%20and%20gives%20a%20specific%20mathematical%20form.%0AP2U-SLAM%20respectively%20embeds%20point%20uncertainty%20and%20pose%20uncertainty%20into%20the%0Atracking%20module%20and%20local%20mapping%2C%20and%20updates%20these%20uncertainties%20after%20each%0Aoptimization%20operation%20including%20local%20mapping%2C%20map%20merging%2C%20and%20loop%20closing.%0AWe%20present%20an%20exhaustive%20evaluation%20in%2027%20sequences%20from%20two%20popular%20public%0Adatasets%20with%20wide-FoV%20visual%20input.%20P2U-SLAM%20shows%20excellent%20performance%0Acompared%20with%20other%20state-of-the-art%20methods.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/BambValley/P2U-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10143v1&entry.124074799=Read"},
{"title": "Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph\n  Generation", "author": "Minghan Chen and Guikun Chen and Wenguan Wang and Yi Yang", "abstract": "  DETR introduces a simplified one-stage framework for scene graph generation\n(SGG). However, DETR-based SGG models face two challenges: i) Sparse\nsupervision, as each image typically contains fewer than 10 relation\nannotations, while the models employ over 100 relation queries. This sparsity\narises because each ground truth relation is assigned to only one single query\nduring training. ii) False negative samples, since one ground truth relation\nmay have multiple queries with similar matching scores. These suboptimally\nmatched queries are simply treated as negative samples, causing the loss of\nvaluable supervisory signals. As a response, we devise Hydra-SGG, a one-stage\nSGG method that adopts a new Hybrid Relation Assignment. This assignment\ncombines a One-to-One Relation Assignment with a newly introduced IoU-based\nOne-to-Many Relation Assignment. Specifically, each ground truth is assigned to\nmultiple relation queries with high IoU subject-object boxes. This Hybrid\nRelation Assignment increases the number of positive training samples,\nalleviating sparse supervision. Moreover, we, for the first time, empirically\nshow that self-attention over relation queries helps reduce duplicated relation\npredictions. We, therefore, propose Hydra Branch, a parameter-sharing auxiliary\ndecoder without a self-attention layer. This design promotes One-to-Many\nRelation Assignment by enabling different queries to predict the same relation.\nHydra-SGG achieves state-of-the-art performance with 10.6 mR@20 and 16.0 mR@50\non VG150, while only requiring 12 training epochs. It also sets a new\nstate-of-the-art on Open Images V6 and and GQA.\n", "link": "http://arxiv.org/abs/2409.10262v1", "date": "2024-09-16", "relevancy": 2.687, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5649}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5242}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hydra-SGG%3A%20Hybrid%20Relation%20Assignment%20for%20One-stage%20Scene%20Graph%0A%20%20Generation&body=Title%3A%20Hydra-SGG%3A%20Hybrid%20Relation%20Assignment%20for%20One-stage%20Scene%20Graph%0A%20%20Generation%0AAuthor%3A%20Minghan%20Chen%20and%20Guikun%20Chen%20and%20Wenguan%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20DETR%20introduces%20a%20simplified%20one-stage%20framework%20for%20scene%20graph%20generation%0A%28SGG%29.%20However%2C%20DETR-based%20SGG%20models%20face%20two%20challenges%3A%20i%29%20Sparse%0Asupervision%2C%20as%20each%20image%20typically%20contains%20fewer%20than%2010%20relation%0Aannotations%2C%20while%20the%20models%20employ%20over%20100%20relation%20queries.%20This%20sparsity%0Aarises%20because%20each%20ground%20truth%20relation%20is%20assigned%20to%20only%20one%20single%20query%0Aduring%20training.%20ii%29%20False%20negative%20samples%2C%20since%20one%20ground%20truth%20relation%0Amay%20have%20multiple%20queries%20with%20similar%20matching%20scores.%20These%20suboptimally%0Amatched%20queries%20are%20simply%20treated%20as%20negative%20samples%2C%20causing%20the%20loss%20of%0Avaluable%20supervisory%20signals.%20As%20a%20response%2C%20we%20devise%20Hydra-SGG%2C%20a%20one-stage%0ASGG%20method%20that%20adopts%20a%20new%20Hybrid%20Relation%20Assignment.%20This%20assignment%0Acombines%20a%20One-to-One%20Relation%20Assignment%20with%20a%20newly%20introduced%20IoU-based%0AOne-to-Many%20Relation%20Assignment.%20Specifically%2C%20each%20ground%20truth%20is%20assigned%20to%0Amultiple%20relation%20queries%20with%20high%20IoU%20subject-object%20boxes.%20This%20Hybrid%0ARelation%20Assignment%20increases%20the%20number%20of%20positive%20training%20samples%2C%0Aalleviating%20sparse%20supervision.%20Moreover%2C%20we%2C%20for%20the%20first%20time%2C%20empirically%0Ashow%20that%20self-attention%20over%20relation%20queries%20helps%20reduce%20duplicated%20relation%0Apredictions.%20We%2C%20therefore%2C%20propose%20Hydra%20Branch%2C%20a%20parameter-sharing%20auxiliary%0Adecoder%20without%20a%20self-attention%20layer.%20This%20design%20promotes%20One-to-Many%0ARelation%20Assignment%20by%20enabling%20different%20queries%20to%20predict%20the%20same%20relation.%0AHydra-SGG%20achieves%20state-of-the-art%20performance%20with%2010.6%20mR%4020%20and%2016.0%20mR%4050%0Aon%20VG150%2C%20while%20only%20requiring%2012%20training%20epochs.%20It%20also%20sets%20a%20new%0Astate-of-the-art%20on%20Open%20Images%20V6%20and%20and%20GQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHydra-SGG%253A%2520Hybrid%2520Relation%2520Assignment%2520for%2520One-stage%2520Scene%2520Graph%250A%2520%2520Generation%26entry.906535625%3DMinghan%2520Chen%2520and%2520Guikun%2520Chen%2520and%2520Wenguan%2520Wang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520DETR%2520introduces%2520a%2520simplified%2520one-stage%2520framework%2520for%2520scene%2520graph%2520generation%250A%2528SGG%2529.%2520However%252C%2520DETR-based%2520SGG%2520models%2520face%2520two%2520challenges%253A%2520i%2529%2520Sparse%250Asupervision%252C%2520as%2520each%2520image%2520typically%2520contains%2520fewer%2520than%252010%2520relation%250Aannotations%252C%2520while%2520the%2520models%2520employ%2520over%2520100%2520relation%2520queries.%2520This%2520sparsity%250Aarises%2520because%2520each%2520ground%2520truth%2520relation%2520is%2520assigned%2520to%2520only%2520one%2520single%2520query%250Aduring%2520training.%2520ii%2529%2520False%2520negative%2520samples%252C%2520since%2520one%2520ground%2520truth%2520relation%250Amay%2520have%2520multiple%2520queries%2520with%2520similar%2520matching%2520scores.%2520These%2520suboptimally%250Amatched%2520queries%2520are%2520simply%2520treated%2520as%2520negative%2520samples%252C%2520causing%2520the%2520loss%2520of%250Avaluable%2520supervisory%2520signals.%2520As%2520a%2520response%252C%2520we%2520devise%2520Hydra-SGG%252C%2520a%2520one-stage%250ASGG%2520method%2520that%2520adopts%2520a%2520new%2520Hybrid%2520Relation%2520Assignment.%2520This%2520assignment%250Acombines%2520a%2520One-to-One%2520Relation%2520Assignment%2520with%2520a%2520newly%2520introduced%2520IoU-based%250AOne-to-Many%2520Relation%2520Assignment.%2520Specifically%252C%2520each%2520ground%2520truth%2520is%2520assigned%2520to%250Amultiple%2520relation%2520queries%2520with%2520high%2520IoU%2520subject-object%2520boxes.%2520This%2520Hybrid%250ARelation%2520Assignment%2520increases%2520the%2520number%2520of%2520positive%2520training%2520samples%252C%250Aalleviating%2520sparse%2520supervision.%2520Moreover%252C%2520we%252C%2520for%2520the%2520first%2520time%252C%2520empirically%250Ashow%2520that%2520self-attention%2520over%2520relation%2520queries%2520helps%2520reduce%2520duplicated%2520relation%250Apredictions.%2520We%252C%2520therefore%252C%2520propose%2520Hydra%2520Branch%252C%2520a%2520parameter-sharing%2520auxiliary%250Adecoder%2520without%2520a%2520self-attention%2520layer.%2520This%2520design%2520promotes%2520One-to-Many%250ARelation%2520Assignment%2520by%2520enabling%2520different%2520queries%2520to%2520predict%2520the%2520same%2520relation.%250AHydra-SGG%2520achieves%2520state-of-the-art%2520performance%2520with%252010.6%2520mR%254020%2520and%252016.0%2520mR%254050%250Aon%2520VG150%252C%2520while%2520only%2520requiring%252012%2520training%2520epochs.%2520It%2520also%2520sets%2520a%2520new%250Astate-of-the-art%2520on%2520Open%2520Images%2520V6%2520and%2520and%2520GQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hydra-SGG%3A%20Hybrid%20Relation%20Assignment%20for%20One-stage%20Scene%20Graph%0A%20%20Generation&entry.906535625=Minghan%20Chen%20and%20Guikun%20Chen%20and%20Wenguan%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20DETR%20introduces%20a%20simplified%20one-stage%20framework%20for%20scene%20graph%20generation%0A%28SGG%29.%20However%2C%20DETR-based%20SGG%20models%20face%20two%20challenges%3A%20i%29%20Sparse%0Asupervision%2C%20as%20each%20image%20typically%20contains%20fewer%20than%2010%20relation%0Aannotations%2C%20while%20the%20models%20employ%20over%20100%20relation%20queries.%20This%20sparsity%0Aarises%20because%20each%20ground%20truth%20relation%20is%20assigned%20to%20only%20one%20single%20query%0Aduring%20training.%20ii%29%20False%20negative%20samples%2C%20since%20one%20ground%20truth%20relation%0Amay%20have%20multiple%20queries%20with%20similar%20matching%20scores.%20These%20suboptimally%0Amatched%20queries%20are%20simply%20treated%20as%20negative%20samples%2C%20causing%20the%20loss%20of%0Avaluable%20supervisory%20signals.%20As%20a%20response%2C%20we%20devise%20Hydra-SGG%2C%20a%20one-stage%0ASGG%20method%20that%20adopts%20a%20new%20Hybrid%20Relation%20Assignment.%20This%20assignment%0Acombines%20a%20One-to-One%20Relation%20Assignment%20with%20a%20newly%20introduced%20IoU-based%0AOne-to-Many%20Relation%20Assignment.%20Specifically%2C%20each%20ground%20truth%20is%20assigned%20to%0Amultiple%20relation%20queries%20with%20high%20IoU%20subject-object%20boxes.%20This%20Hybrid%0ARelation%20Assignment%20increases%20the%20number%20of%20positive%20training%20samples%2C%0Aalleviating%20sparse%20supervision.%20Moreover%2C%20we%2C%20for%20the%20first%20time%2C%20empirically%0Ashow%20that%20self-attention%20over%20relation%20queries%20helps%20reduce%20duplicated%20relation%0Apredictions.%20We%2C%20therefore%2C%20propose%20Hydra%20Branch%2C%20a%20parameter-sharing%20auxiliary%0Adecoder%20without%20a%20self-attention%20layer.%20This%20design%20promotes%20One-to-Many%0ARelation%20Assignment%20by%20enabling%20different%20queries%20to%20predict%20the%20same%20relation.%0AHydra-SGG%20achieves%20state-of-the-art%20performance%20with%2010.6%20mR%4020%20and%2016.0%20mR%4050%0Aon%20VG150%2C%20while%20only%20requiring%2012%20training%20epochs.%20It%20also%20sets%20a%20new%0Astate-of-the-art%20on%20Open%20Images%20V6%20and%20and%20GQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10262v1&entry.124074799=Read"},
{"title": "Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene\n  Graph", "author": "Sergey Linok and Tatiana Zemskova and Svetlana Ladanova and Roman Titkov and Dmitry Yudin and Maxim Monastyrny and Aleksei Valenkov", "abstract": "  Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.\n", "link": "http://arxiv.org/abs/2406.07113v3", "date": "2024-09-16", "relevancy": 2.685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6733}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph&body=Title%3A%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph%0AAuthor%3A%20Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%20and%20Maxim%20Monastyrny%20and%20Aleksei%20Valenkov%0AAbstract%3A%20%20%20Locating%20objects%20described%20in%20natural%20language%20presents%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20grounding%20with%20simple%20%28bare%29%20queries%2C%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20graph%20representation%20with%0Ametric%20and%20semantic%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%0Ahuman-to-agent%20interface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%0Aemploys%20robust%20DINO-powered%20associations%20to%20construct%203D%20object-centric%20map%20and%0Aan%20advanced%20raycasting%20algorithm%20with%20a%202D%20vision-language%20model%20to%20describe%0Athem%20as%20graph%20nodes.%20On%20the%20Replica%20and%20ScanNet%20datasets%2C%20we%20have%20demonstrated%0Athat%20BBQ%20takes%20a%20leading%20place%20in%20open-vocabulary%203D%20semantic%20segmentation%0Acompared%20to%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20challenging%20Sr3D%2B%2C%20Nr3D%20and%20ScanRefer%20benchmarks%2C%0Aour%20deductive%20approach%20demonstrates%20a%20significant%20improvement%2C%20enabling%20objects%0Agrounding%20by%20complex%20queries%20compared%20to%20other%20state-of-the-art%20methods.%20The%0Acombination%20of%20our%20design%20choices%20and%20software%20implementation%20has%20resulted%20in%0Asignificant%20data%20processing%20speed%20in%20experiments%20on%20the%20robot%20on-board%0Acomputer.%20This%20promising%20performance%20enables%20the%20application%20of%20our%20approach%20in%0Aintelligent%20robotics%20projects.%20We%20made%20the%20code%20publicly%20available%20at%0Ahttps%3A//linukc.github.io/BeyondBareQueries/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07113v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Bare%2520Queries%253A%2520Open-Vocabulary%2520Object%2520Grounding%2520with%25203D%2520Scene%250A%2520%2520Graph%26entry.906535625%3DSergey%2520Linok%2520and%2520Tatiana%2520Zemskova%2520and%2520Svetlana%2520Ladanova%2520and%2520Roman%2520Titkov%2520and%2520Dmitry%2520Yudin%2520and%2520Maxim%2520Monastyrny%2520and%2520Aleksei%2520Valenkov%26entry.1292438233%3D%2520%2520Locating%2520objects%2520described%2520in%2520natural%2520language%2520presents%2520a%2520significant%250Achallenge%2520for%2520autonomous%2520agents.%2520Existing%2520CLIP-based%2520open-vocabulary%2520methods%250Asuccessfully%2520perform%25203D%2520object%2520grounding%2520with%2520simple%2520%2528bare%2529%2520queries%252C%2520but%2520cannot%250Acope%2520with%2520ambiguous%2520descriptions%2520that%2520demand%2520an%2520understanding%2520of%2520object%250Arelations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520modular%2520approach%2520called%2520BBQ%250A%2528Beyond%2520Bare%2520Queries%2529%252C%2520which%2520constructs%25203D%2520scene%2520graph%2520representation%2520with%250Ametric%2520and%2520semantic%2520edges%2520and%2520utilizes%2520a%2520large%2520language%2520model%2520as%2520a%250Ahuman-to-agent%2520interface%2520through%2520our%2520deductive%2520scene%2520reasoning%2520algorithm.%2520BBQ%250Aemploys%2520robust%2520DINO-powered%2520associations%2520to%2520construct%25203D%2520object-centric%2520map%2520and%250Aan%2520advanced%2520raycasting%2520algorithm%2520with%2520a%25202D%2520vision-language%2520model%2520to%2520describe%250Athem%2520as%2520graph%2520nodes.%2520On%2520the%2520Replica%2520and%2520ScanNet%2520datasets%252C%2520we%2520have%2520demonstrated%250Athat%2520BBQ%2520takes%2520a%2520leading%2520place%2520in%2520open-vocabulary%25203D%2520semantic%2520segmentation%250Acompared%2520to%2520other%2520zero-shot%2520methods.%2520Also%252C%2520we%2520show%2520that%2520leveraging%2520spatial%250Arelations%2520is%2520especially%2520effective%2520for%2520scenes%2520containing%2520multiple%2520entities%2520of%250Athe%2520same%2520semantic%2520class.%2520On%2520challenging%2520Sr3D%252B%252C%2520Nr3D%2520and%2520ScanRefer%2520benchmarks%252C%250Aour%2520deductive%2520approach%2520demonstrates%2520a%2520significant%2520improvement%252C%2520enabling%2520objects%250Agrounding%2520by%2520complex%2520queries%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%2520The%250Acombination%2520of%2520our%2520design%2520choices%2520and%2520software%2520implementation%2520has%2520resulted%2520in%250Asignificant%2520data%2520processing%2520speed%2520in%2520experiments%2520on%2520the%2520robot%2520on-board%250Acomputer.%2520This%2520promising%2520performance%2520enables%2520the%2520application%2520of%2520our%2520approach%2520in%250Aintelligent%2520robotics%2520projects.%2520We%2520made%2520the%2520code%2520publicly%2520available%2520at%250Ahttps%253A//linukc.github.io/BeyondBareQueries/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07113v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Grounding%20with%203D%20Scene%0A%20%20Graph&entry.906535625=Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%20and%20Maxim%20Monastyrny%20and%20Aleksei%20Valenkov&entry.1292438233=%20%20Locating%20objects%20described%20in%20natural%20language%20presents%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20grounding%20with%20simple%20%28bare%29%20queries%2C%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20graph%20representation%20with%0Ametric%20and%20semantic%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%0Ahuman-to-agent%20interface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%0Aemploys%20robust%20DINO-powered%20associations%20to%20construct%203D%20object-centric%20map%20and%0Aan%20advanced%20raycasting%20algorithm%20with%20a%202D%20vision-language%20model%20to%20describe%0Athem%20as%20graph%20nodes.%20On%20the%20Replica%20and%20ScanNet%20datasets%2C%20we%20have%20demonstrated%0Athat%20BBQ%20takes%20a%20leading%20place%20in%20open-vocabulary%203D%20semantic%20segmentation%0Acompared%20to%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20challenging%20Sr3D%2B%2C%20Nr3D%20and%20ScanRefer%20benchmarks%2C%0Aour%20deductive%20approach%20demonstrates%20a%20significant%20improvement%2C%20enabling%20objects%0Agrounding%20by%20complex%20queries%20compared%20to%20other%20state-of-the-art%20methods.%20The%0Acombination%20of%20our%20design%20choices%20and%20software%20implementation%20has%20resulted%20in%0Asignificant%20data%20processing%20speed%20in%20experiments%20on%20the%20robot%20on-board%0Acomputer.%20This%20promising%20performance%20enables%20the%20application%20of%20our%20approach%20in%0Aintelligent%20robotics%20projects.%20We%20made%20the%20code%20publicly%20available%20at%0Ahttps%3A//linukc.github.io/BeyondBareQueries/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07113v3&entry.124074799=Read"},
{"title": "Anatomical Positional Embeddings", "author": "Mikhail Goncharov and Valentin Samokhin and Eugenia Soboleva and Roman Sokolov and Boris Shirokikh and Mikhail Belyaev and Anvar Kurmukov and Ivan Oseledets", "abstract": "  We propose a self-supervised model producing 3D anatomical positional\nembeddings (APE) of individual medical image voxels. APE encodes voxels'\nanatomical closeness, i.e., voxels of the same organ or nearby organs always\nhave closer positional embeddings than the voxels of more distant body parts.\nIn contrast to the existing models of anatomical positional embeddings, our\nmethod is able to efficiently produce a map of voxel-wise embeddings for a\nwhole volumetric input image, which makes it an optimal choice for different\ndownstream applications. We train our APE model on 8400 publicly available CT\nimages of abdomen and chest regions. We demonstrate its superior performance\ncompared with the existing models on anatomical landmark retrieval and\nweakly-supervised few-shot localization of 13 abdominal organs. As a practical\napplication, we show how to cheaply train APE to crop raw CT images to\ndifferent anatomical regions of interest with 0.99 recall, while reducing the\nimage volume by 10-100 times. The code and the pre-trained APE model are\navailable at https://github.com/mishgon/ape .\n", "link": "http://arxiv.org/abs/2409.10291v1", "date": "2024-09-16", "relevancy": 2.6802, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5397}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Positional%20Embeddings&body=Title%3A%20Anatomical%20Positional%20Embeddings%0AAuthor%3A%20Mikhail%20Goncharov%20and%20Valentin%20Samokhin%20and%20Eugenia%20Soboleva%20and%20Roman%20Sokolov%20and%20Boris%20Shirokikh%20and%20Mikhail%20Belyaev%20and%20Anvar%20Kurmukov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20We%20propose%20a%20self-supervised%20model%20producing%203D%20anatomical%20positional%0Aembeddings%20%28APE%29%20of%20individual%20medical%20image%20voxels.%20APE%20encodes%20voxels%27%0Aanatomical%20closeness%2C%20i.e.%2C%20voxels%20of%20the%20same%20organ%20or%20nearby%20organs%20always%0Ahave%20closer%20positional%20embeddings%20than%20the%20voxels%20of%20more%20distant%20body%20parts.%0AIn%20contrast%20to%20the%20existing%20models%20of%20anatomical%20positional%20embeddings%2C%20our%0Amethod%20is%20able%20to%20efficiently%20produce%20a%20map%20of%20voxel-wise%20embeddings%20for%20a%0Awhole%20volumetric%20input%20image%2C%20which%20makes%20it%20an%20optimal%20choice%20for%20different%0Adownstream%20applications.%20We%20train%20our%20APE%20model%20on%208400%20publicly%20available%20CT%0Aimages%20of%20abdomen%20and%20chest%20regions.%20We%20demonstrate%20its%20superior%20performance%0Acompared%20with%20the%20existing%20models%20on%20anatomical%20landmark%20retrieval%20and%0Aweakly-supervised%20few-shot%20localization%20of%2013%20abdominal%20organs.%20As%20a%20practical%0Aapplication%2C%20we%20show%20how%20to%20cheaply%20train%20APE%20to%20crop%20raw%20CT%20images%20to%0Adifferent%20anatomical%20regions%20of%20interest%20with%200.99%20recall%2C%20while%20reducing%20the%0Aimage%20volume%20by%2010-100%20times.%20The%20code%20and%20the%20pre-trained%20APE%20model%20are%0Aavailable%20at%20https%3A//github.com/mishgon/ape%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomical%2520Positional%2520Embeddings%26entry.906535625%3DMikhail%2520Goncharov%2520and%2520Valentin%2520Samokhin%2520and%2520Eugenia%2520Soboleva%2520and%2520Roman%2520Sokolov%2520and%2520Boris%2520Shirokikh%2520and%2520Mikhail%2520Belyaev%2520and%2520Anvar%2520Kurmukov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520self-supervised%2520model%2520producing%25203D%2520anatomical%2520positional%250Aembeddings%2520%2528APE%2529%2520of%2520individual%2520medical%2520image%2520voxels.%2520APE%2520encodes%2520voxels%2527%250Aanatomical%2520closeness%252C%2520i.e.%252C%2520voxels%2520of%2520the%2520same%2520organ%2520or%2520nearby%2520organs%2520always%250Ahave%2520closer%2520positional%2520embeddings%2520than%2520the%2520voxels%2520of%2520more%2520distant%2520body%2520parts.%250AIn%2520contrast%2520to%2520the%2520existing%2520models%2520of%2520anatomical%2520positional%2520embeddings%252C%2520our%250Amethod%2520is%2520able%2520to%2520efficiently%2520produce%2520a%2520map%2520of%2520voxel-wise%2520embeddings%2520for%2520a%250Awhole%2520volumetric%2520input%2520image%252C%2520which%2520makes%2520it%2520an%2520optimal%2520choice%2520for%2520different%250Adownstream%2520applications.%2520We%2520train%2520our%2520APE%2520model%2520on%25208400%2520publicly%2520available%2520CT%250Aimages%2520of%2520abdomen%2520and%2520chest%2520regions.%2520We%2520demonstrate%2520its%2520superior%2520performance%250Acompared%2520with%2520the%2520existing%2520models%2520on%2520anatomical%2520landmark%2520retrieval%2520and%250Aweakly-supervised%2520few-shot%2520localization%2520of%252013%2520abdominal%2520organs.%2520As%2520a%2520practical%250Aapplication%252C%2520we%2520show%2520how%2520to%2520cheaply%2520train%2520APE%2520to%2520crop%2520raw%2520CT%2520images%2520to%250Adifferent%2520anatomical%2520regions%2520of%2520interest%2520with%25200.99%2520recall%252C%2520while%2520reducing%2520the%250Aimage%2520volume%2520by%252010-100%2520times.%2520The%2520code%2520and%2520the%2520pre-trained%2520APE%2520model%2520are%250Aavailable%2520at%2520https%253A//github.com/mishgon/ape%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Positional%20Embeddings&entry.906535625=Mikhail%20Goncharov%20and%20Valentin%20Samokhin%20and%20Eugenia%20Soboleva%20and%20Roman%20Sokolov%20and%20Boris%20Shirokikh%20and%20Mikhail%20Belyaev%20and%20Anvar%20Kurmukov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20We%20propose%20a%20self-supervised%20model%20producing%203D%20anatomical%20positional%0Aembeddings%20%28APE%29%20of%20individual%20medical%20image%20voxels.%20APE%20encodes%20voxels%27%0Aanatomical%20closeness%2C%20i.e.%2C%20voxels%20of%20the%20same%20organ%20or%20nearby%20organs%20always%0Ahave%20closer%20positional%20embeddings%20than%20the%20voxels%20of%20more%20distant%20body%20parts.%0AIn%20contrast%20to%20the%20existing%20models%20of%20anatomical%20positional%20embeddings%2C%20our%0Amethod%20is%20able%20to%20efficiently%20produce%20a%20map%20of%20voxel-wise%20embeddings%20for%20a%0Awhole%20volumetric%20input%20image%2C%20which%20makes%20it%20an%20optimal%20choice%20for%20different%0Adownstream%20applications.%20We%20train%20our%20APE%20model%20on%208400%20publicly%20available%20CT%0Aimages%20of%20abdomen%20and%20chest%20regions.%20We%20demonstrate%20its%20superior%20performance%0Acompared%20with%20the%20existing%20models%20on%20anatomical%20landmark%20retrieval%20and%0Aweakly-supervised%20few-shot%20localization%20of%2013%20abdominal%20organs.%20As%20a%20practical%0Aapplication%2C%20we%20show%20how%20to%20cheaply%20train%20APE%20to%20crop%20raw%20CT%20images%20to%0Adifferent%20anatomical%20regions%20of%20interest%20with%200.99%20recall%2C%20while%20reducing%20the%0Aimage%20volume%20by%2010-100%20times.%20The%20code%20and%20the%20pre-trained%20APE%20model%20are%0Aavailable%20at%20https%3A//github.com/mishgon/ape%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10291v1&entry.124074799=Read"},
{"title": "PointViG: A Lightweight GNN-based Model for Efficient Point Cloud\n  Analysis", "author": "Qiang Zheng and Yafei Qi and Chen Wang and Chao Zhang and Jian Sun", "abstract": "  In the domain of point cloud analysis, despite the significant capabilities\nof Graph Neural Networks (GNNs) in managing complex 3D datasets, existing\napproaches encounter challenges like high computational costs and scalability\nissues with extensive scenarios. These limitations restrict the practical\ndeployment of GNNs, notably in resource-constrained environments. To address\nthese issues, this study introduce <b>Point<\\b> <b>Vi<\\b>sion <b>G<\\b>NN\n(PointViG), an efficient framework for point cloud analysis. PointViG\nincorporates a lightweight graph convolutional module to efficiently aggregate\nlocal features and mitigate over-smoothing. For large-scale point cloud scenes,\nwe propose an adaptive dilated graph convolution technique that searches for\nsparse neighboring nodes within a dilated neighborhood based on semantic\ncorrelation, thereby expanding the receptive field and ensuring computational\nefficiency. Experiments demonstrate that PointViG achieves performance\ncomparable to state-of-the-art models while balancing performance and\ncomplexity. On the ModelNet40 classification task, PointViG achieved 94.3%\naccuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an\nmIoU of 71.7% with 5.3M parameters. These results underscore the potential and\nefficiency of PointViG in point cloud analysis.\n", "link": "http://arxiv.org/abs/2407.00921v2", "date": "2024-09-16", "relevancy": 2.663, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5487}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5335}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointViG%3A%20A%20Lightweight%20GNN-based%20Model%20for%20Efficient%20Point%20Cloud%0A%20%20Analysis&body=Title%3A%20PointViG%3A%20A%20Lightweight%20GNN-based%20Model%20for%20Efficient%20Point%20Cloud%0A%20%20Analysis%0AAuthor%3A%20Qiang%20Zheng%20and%20Yafei%20Qi%20and%20Chen%20Wang%20and%20Chao%20Zhang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20In%20the%20domain%20of%20point%20cloud%20analysis%2C%20despite%20the%20significant%20capabilities%0Aof%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20managing%20complex%203D%20datasets%2C%20existing%0Aapproaches%20encounter%20challenges%20like%20high%20computational%20costs%20and%20scalability%0Aissues%20with%20extensive%20scenarios.%20These%20limitations%20restrict%20the%20practical%0Adeployment%20of%20GNNs%2C%20notably%20in%20resource-constrained%20environments.%20To%20address%0Athese%20issues%2C%20this%20study%20introduce%20%3Cb%3EPoint%3C%5Cb%3E%20%3Cb%3EVi%3C%5Cb%3Esion%20%3Cb%3EG%3C%5Cb%3ENN%0A%28PointViG%29%2C%20an%20efficient%20framework%20for%20point%20cloud%20analysis.%20PointViG%0Aincorporates%20a%20lightweight%20graph%20convolutional%20module%20to%20efficiently%20aggregate%0Alocal%20features%20and%20mitigate%20over-smoothing.%20For%20large-scale%20point%20cloud%20scenes%2C%0Awe%20propose%20an%20adaptive%20dilated%20graph%20convolution%20technique%20that%20searches%20for%0Asparse%20neighboring%20nodes%20within%20a%20dilated%20neighborhood%20based%20on%20semantic%0Acorrelation%2C%20thereby%20expanding%20the%20receptive%20field%20and%20ensuring%20computational%0Aefficiency.%20Experiments%20demonstrate%20that%20PointViG%20achieves%20performance%0Acomparable%20to%20state-of-the-art%20models%20while%20balancing%20performance%20and%0Acomplexity.%20On%20the%20ModelNet40%20classification%20task%2C%20PointViG%20achieved%2094.3%25%0Aaccuracy%20with%201.5M%20parameters.%20For%20the%20S3DIS%20segmentation%20task%2C%20it%20achieved%20an%0AmIoU%20of%2071.7%25%20with%205.3M%20parameters.%20These%20results%20underscore%20the%20potential%20and%0Aefficiency%20of%20PointViG%20in%20point%20cloud%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointViG%253A%2520A%2520Lightweight%2520GNN-based%2520Model%2520for%2520Efficient%2520Point%2520Cloud%250A%2520%2520Analysis%26entry.906535625%3DQiang%2520Zheng%2520and%2520Yafei%2520Qi%2520and%2520Chen%2520Wang%2520and%2520Chao%2520Zhang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520point%2520cloud%2520analysis%252C%2520despite%2520the%2520significant%2520capabilities%250Aof%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520in%2520managing%2520complex%25203D%2520datasets%252C%2520existing%250Aapproaches%2520encounter%2520challenges%2520like%2520high%2520computational%2520costs%2520and%2520scalability%250Aissues%2520with%2520extensive%2520scenarios.%2520These%2520limitations%2520restrict%2520the%2520practical%250Adeployment%2520of%2520GNNs%252C%2520notably%2520in%2520resource-constrained%2520environments.%2520To%2520address%250Athese%2520issues%252C%2520this%2520study%2520introduce%2520%253Cb%253EPoint%253C%255Cb%253E%2520%253Cb%253EVi%253C%255Cb%253Esion%2520%253Cb%253EG%253C%255Cb%253ENN%250A%2528PointViG%2529%252C%2520an%2520efficient%2520framework%2520for%2520point%2520cloud%2520analysis.%2520PointViG%250Aincorporates%2520a%2520lightweight%2520graph%2520convolutional%2520module%2520to%2520efficiently%2520aggregate%250Alocal%2520features%2520and%2520mitigate%2520over-smoothing.%2520For%2520large-scale%2520point%2520cloud%2520scenes%252C%250Awe%2520propose%2520an%2520adaptive%2520dilated%2520graph%2520convolution%2520technique%2520that%2520searches%2520for%250Asparse%2520neighboring%2520nodes%2520within%2520a%2520dilated%2520neighborhood%2520based%2520on%2520semantic%250Acorrelation%252C%2520thereby%2520expanding%2520the%2520receptive%2520field%2520and%2520ensuring%2520computational%250Aefficiency.%2520Experiments%2520demonstrate%2520that%2520PointViG%2520achieves%2520performance%250Acomparable%2520to%2520state-of-the-art%2520models%2520while%2520balancing%2520performance%2520and%250Acomplexity.%2520On%2520the%2520ModelNet40%2520classification%2520task%252C%2520PointViG%2520achieved%252094.3%2525%250Aaccuracy%2520with%25201.5M%2520parameters.%2520For%2520the%2520S3DIS%2520segmentation%2520task%252C%2520it%2520achieved%2520an%250AmIoU%2520of%252071.7%2525%2520with%25205.3M%2520parameters.%2520These%2520results%2520underscore%2520the%2520potential%2520and%250Aefficiency%2520of%2520PointViG%2520in%2520point%2520cloud%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointViG%3A%20A%20Lightweight%20GNN-based%20Model%20for%20Efficient%20Point%20Cloud%0A%20%20Analysis&entry.906535625=Qiang%20Zheng%20and%20Yafei%20Qi%20and%20Chen%20Wang%20and%20Chao%20Zhang%20and%20Jian%20Sun&entry.1292438233=%20%20In%20the%20domain%20of%20point%20cloud%20analysis%2C%20despite%20the%20significant%20capabilities%0Aof%20Graph%20Neural%20Networks%20%28GNNs%29%20in%20managing%20complex%203D%20datasets%2C%20existing%0Aapproaches%20encounter%20challenges%20like%20high%20computational%20costs%20and%20scalability%0Aissues%20with%20extensive%20scenarios.%20These%20limitations%20restrict%20the%20practical%0Adeployment%20of%20GNNs%2C%20notably%20in%20resource-constrained%20environments.%20To%20address%0Athese%20issues%2C%20this%20study%20introduce%20%3Cb%3EPoint%3C%5Cb%3E%20%3Cb%3EVi%3C%5Cb%3Esion%20%3Cb%3EG%3C%5Cb%3ENN%0A%28PointViG%29%2C%20an%20efficient%20framework%20for%20point%20cloud%20analysis.%20PointViG%0Aincorporates%20a%20lightweight%20graph%20convolutional%20module%20to%20efficiently%20aggregate%0Alocal%20features%20and%20mitigate%20over-smoothing.%20For%20large-scale%20point%20cloud%20scenes%2C%0Awe%20propose%20an%20adaptive%20dilated%20graph%20convolution%20technique%20that%20searches%20for%0Asparse%20neighboring%20nodes%20within%20a%20dilated%20neighborhood%20based%20on%20semantic%0Acorrelation%2C%20thereby%20expanding%20the%20receptive%20field%20and%20ensuring%20computational%0Aefficiency.%20Experiments%20demonstrate%20that%20PointViG%20achieves%20performance%0Acomparable%20to%20state-of-the-art%20models%20while%20balancing%20performance%20and%0Acomplexity.%20On%20the%20ModelNet40%20classification%20task%2C%20PointViG%20achieved%2094.3%25%0Aaccuracy%20with%201.5M%20parameters.%20For%20the%20S3DIS%20segmentation%20task%2C%20it%20achieved%20an%0AmIoU%20of%2071.7%25%20with%205.3M%20parameters.%20These%20results%20underscore%20the%20potential%20and%0Aefficiency%20of%20PointViG%20in%20point%20cloud%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00921v2&entry.124074799=Read"},
{"title": "SPAC: Sampling-based Progressive Attribute Compression for Dense Point\n  Clouds", "author": "Xiaolong Mao and Hui Yuan and Tian Guo and Shiqi Jiang and Raouf Hamzaoui and Sam Kwong", "abstract": "  We propose an end-to-end attribute compression method for dense point clouds.\nThe proposed method combines a frequency sampling module, an adaptive scale\nfeature extraction module with geometry assistance, and a global hyperprior\nentropy model. The frequency sampling module uses a Hamming window and the Fast\nFourier Transform to extract high-frequency components of the point cloud. The\ndifference between the original point cloud and the sampled point cloud is\ndivided into multiple sub-point clouds. These sub-point clouds are then\npartitioned using an octree, providing a structured input for feature\nextraction. The feature extraction module integrates adaptive convolutional\nlayers and uses offset-attention to capture both local and global features.\nThen, a geometry-assisted attribute feature refinement module is used to refine\nthe extracted attribute features. Finally, a global hyperprior model is\nintroduced for entropy encoding. This model propagates hyperprior parameters\nfrom the deepest (base) layer to the other layers, further enhancing the\nencoding efficiency. At the decoder, a mirrored network is used to\nprogressively restore features and reconstruct the color attribute through\ntransposed convolutional layers. The proposed method encodes base layer\ninformation at a low bitrate and progressively adds enhancement layer\ninformation to improve reconstruction accuracy. Compared to the latest G-PCC\ntest model (TMC13v23) under the MPEG common test conditions (CTCs), the\nproposed method achieved an average Bjontegaard delta bitrate reduction of\n24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Solid\ndataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEG\nCategory Dense dataset. This is the first instance of a learning-based codec\noutperforming the G-PCC standard on these datasets under the MPEG CTCs.\n", "link": "http://arxiv.org/abs/2409.10293v1", "date": "2024-09-16", "relevancy": 2.6599, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5376}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5331}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAC%3A%20Sampling-based%20Progressive%20Attribute%20Compression%20for%20Dense%20Point%0A%20%20Clouds&body=Title%3A%20SPAC%3A%20Sampling-based%20Progressive%20Attribute%20Compression%20for%20Dense%20Point%0A%20%20Clouds%0AAuthor%3A%20Xiaolong%20Mao%20and%20Hui%20Yuan%20and%20Tian%20Guo%20and%20Shiqi%20Jiang%20and%20Raouf%20Hamzaoui%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20We%20propose%20an%20end-to-end%20attribute%20compression%20method%20for%20dense%20point%20clouds.%0AThe%20proposed%20method%20combines%20a%20frequency%20sampling%20module%2C%20an%20adaptive%20scale%0Afeature%20extraction%20module%20with%20geometry%20assistance%2C%20and%20a%20global%20hyperprior%0Aentropy%20model.%20The%20frequency%20sampling%20module%20uses%20a%20Hamming%20window%20and%20the%20Fast%0AFourier%20Transform%20to%20extract%20high-frequency%20components%20of%20the%20point%20cloud.%20The%0Adifference%20between%20the%20original%20point%20cloud%20and%20the%20sampled%20point%20cloud%20is%0Adivided%20into%20multiple%20sub-point%20clouds.%20These%20sub-point%20clouds%20are%20then%0Apartitioned%20using%20an%20octree%2C%20providing%20a%20structured%20input%20for%20feature%0Aextraction.%20The%20feature%20extraction%20module%20integrates%20adaptive%20convolutional%0Alayers%20and%20uses%20offset-attention%20to%20capture%20both%20local%20and%20global%20features.%0AThen%2C%20a%20geometry-assisted%20attribute%20feature%20refinement%20module%20is%20used%20to%20refine%0Athe%20extracted%20attribute%20features.%20Finally%2C%20a%20global%20hyperprior%20model%20is%0Aintroduced%20for%20entropy%20encoding.%20This%20model%20propagates%20hyperprior%20parameters%0Afrom%20the%20deepest%20%28base%29%20layer%20to%20the%20other%20layers%2C%20further%20enhancing%20the%0Aencoding%20efficiency.%20At%20the%20decoder%2C%20a%20mirrored%20network%20is%20used%20to%0Aprogressively%20restore%20features%20and%20reconstruct%20the%20color%20attribute%20through%0Atransposed%20convolutional%20layers.%20The%20proposed%20method%20encodes%20base%20layer%0Ainformation%20at%20a%20low%20bitrate%20and%20progressively%20adds%20enhancement%20layer%0Ainformation%20to%20improve%20reconstruction%20accuracy.%20Compared%20to%20the%20latest%20G-PCC%0Atest%20model%20%28TMC13v23%29%20under%20the%20MPEG%20common%20test%20conditions%20%28CTCs%29%2C%20the%0Aproposed%20method%20achieved%20an%20average%20Bjontegaard%20delta%20bitrate%20reduction%20of%0A24.58%25%20for%20the%20Y%20component%20%2821.23%25%20for%20YUV%20combined%29%20on%20the%20MPEG%20Category%20Solid%0Adataset%20and%2022.48%25%20for%20the%20Y%20component%20%2817.19%25%20for%20YUV%20combined%29%20on%20the%20MPEG%0ACategory%20Dense%20dataset.%20This%20is%20the%20first%20instance%20of%20a%20learning-based%20codec%0Aoutperforming%20the%20G-PCC%20standard%20on%20these%20datasets%20under%20the%20MPEG%20CTCs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAC%253A%2520Sampling-based%2520Progressive%2520Attribute%2520Compression%2520for%2520Dense%2520Point%250A%2520%2520Clouds%26entry.906535625%3DXiaolong%2520Mao%2520and%2520Hui%2520Yuan%2520and%2520Tian%2520Guo%2520and%2520Shiqi%2520Jiang%2520and%2520Raouf%2520Hamzaoui%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520end-to-end%2520attribute%2520compression%2520method%2520for%2520dense%2520point%2520clouds.%250AThe%2520proposed%2520method%2520combines%2520a%2520frequency%2520sampling%2520module%252C%2520an%2520adaptive%2520scale%250Afeature%2520extraction%2520module%2520with%2520geometry%2520assistance%252C%2520and%2520a%2520global%2520hyperprior%250Aentropy%2520model.%2520The%2520frequency%2520sampling%2520module%2520uses%2520a%2520Hamming%2520window%2520and%2520the%2520Fast%250AFourier%2520Transform%2520to%2520extract%2520high-frequency%2520components%2520of%2520the%2520point%2520cloud.%2520The%250Adifference%2520between%2520the%2520original%2520point%2520cloud%2520and%2520the%2520sampled%2520point%2520cloud%2520is%250Adivided%2520into%2520multiple%2520sub-point%2520clouds.%2520These%2520sub-point%2520clouds%2520are%2520then%250Apartitioned%2520using%2520an%2520octree%252C%2520providing%2520a%2520structured%2520input%2520for%2520feature%250Aextraction.%2520The%2520feature%2520extraction%2520module%2520integrates%2520adaptive%2520convolutional%250Alayers%2520and%2520uses%2520offset-attention%2520to%2520capture%2520both%2520local%2520and%2520global%2520features.%250AThen%252C%2520a%2520geometry-assisted%2520attribute%2520feature%2520refinement%2520module%2520is%2520used%2520to%2520refine%250Athe%2520extracted%2520attribute%2520features.%2520Finally%252C%2520a%2520global%2520hyperprior%2520model%2520is%250Aintroduced%2520for%2520entropy%2520encoding.%2520This%2520model%2520propagates%2520hyperprior%2520parameters%250Afrom%2520the%2520deepest%2520%2528base%2529%2520layer%2520to%2520the%2520other%2520layers%252C%2520further%2520enhancing%2520the%250Aencoding%2520efficiency.%2520At%2520the%2520decoder%252C%2520a%2520mirrored%2520network%2520is%2520used%2520to%250Aprogressively%2520restore%2520features%2520and%2520reconstruct%2520the%2520color%2520attribute%2520through%250Atransposed%2520convolutional%2520layers.%2520The%2520proposed%2520method%2520encodes%2520base%2520layer%250Ainformation%2520at%2520a%2520low%2520bitrate%2520and%2520progressively%2520adds%2520enhancement%2520layer%250Ainformation%2520to%2520improve%2520reconstruction%2520accuracy.%2520Compared%2520to%2520the%2520latest%2520G-PCC%250Atest%2520model%2520%2528TMC13v23%2529%2520under%2520the%2520MPEG%2520common%2520test%2520conditions%2520%2528CTCs%2529%252C%2520the%250Aproposed%2520method%2520achieved%2520an%2520average%2520Bjontegaard%2520delta%2520bitrate%2520reduction%2520of%250A24.58%2525%2520for%2520the%2520Y%2520component%2520%252821.23%2525%2520for%2520YUV%2520combined%2529%2520on%2520the%2520MPEG%2520Category%2520Solid%250Adataset%2520and%252022.48%2525%2520for%2520the%2520Y%2520component%2520%252817.19%2525%2520for%2520YUV%2520combined%2529%2520on%2520the%2520MPEG%250ACategory%2520Dense%2520dataset.%2520This%2520is%2520the%2520first%2520instance%2520of%2520a%2520learning-based%2520codec%250Aoutperforming%2520the%2520G-PCC%2520standard%2520on%2520these%2520datasets%2520under%2520the%2520MPEG%2520CTCs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAC%3A%20Sampling-based%20Progressive%20Attribute%20Compression%20for%20Dense%20Point%0A%20%20Clouds&entry.906535625=Xiaolong%20Mao%20and%20Hui%20Yuan%20and%20Tian%20Guo%20and%20Shiqi%20Jiang%20and%20Raouf%20Hamzaoui%20and%20Sam%20Kwong&entry.1292438233=%20%20We%20propose%20an%20end-to-end%20attribute%20compression%20method%20for%20dense%20point%20clouds.%0AThe%20proposed%20method%20combines%20a%20frequency%20sampling%20module%2C%20an%20adaptive%20scale%0Afeature%20extraction%20module%20with%20geometry%20assistance%2C%20and%20a%20global%20hyperprior%0Aentropy%20model.%20The%20frequency%20sampling%20module%20uses%20a%20Hamming%20window%20and%20the%20Fast%0AFourier%20Transform%20to%20extract%20high-frequency%20components%20of%20the%20point%20cloud.%20The%0Adifference%20between%20the%20original%20point%20cloud%20and%20the%20sampled%20point%20cloud%20is%0Adivided%20into%20multiple%20sub-point%20clouds.%20These%20sub-point%20clouds%20are%20then%0Apartitioned%20using%20an%20octree%2C%20providing%20a%20structured%20input%20for%20feature%0Aextraction.%20The%20feature%20extraction%20module%20integrates%20adaptive%20convolutional%0Alayers%20and%20uses%20offset-attention%20to%20capture%20both%20local%20and%20global%20features.%0AThen%2C%20a%20geometry-assisted%20attribute%20feature%20refinement%20module%20is%20used%20to%20refine%0Athe%20extracted%20attribute%20features.%20Finally%2C%20a%20global%20hyperprior%20model%20is%0Aintroduced%20for%20entropy%20encoding.%20This%20model%20propagates%20hyperprior%20parameters%0Afrom%20the%20deepest%20%28base%29%20layer%20to%20the%20other%20layers%2C%20further%20enhancing%20the%0Aencoding%20efficiency.%20At%20the%20decoder%2C%20a%20mirrored%20network%20is%20used%20to%0Aprogressively%20restore%20features%20and%20reconstruct%20the%20color%20attribute%20through%0Atransposed%20convolutional%20layers.%20The%20proposed%20method%20encodes%20base%20layer%0Ainformation%20at%20a%20low%20bitrate%20and%20progressively%20adds%20enhancement%20layer%0Ainformation%20to%20improve%20reconstruction%20accuracy.%20Compared%20to%20the%20latest%20G-PCC%0Atest%20model%20%28TMC13v23%29%20under%20the%20MPEG%20common%20test%20conditions%20%28CTCs%29%2C%20the%0Aproposed%20method%20achieved%20an%20average%20Bjontegaard%20delta%20bitrate%20reduction%20of%0A24.58%25%20for%20the%20Y%20component%20%2821.23%25%20for%20YUV%20combined%29%20on%20the%20MPEG%20Category%20Solid%0Adataset%20and%2022.48%25%20for%20the%20Y%20component%20%2817.19%25%20for%20YUV%20combined%29%20on%20the%20MPEG%0ACategory%20Dense%20dataset.%20This%20is%20the%20first%20instance%20of%20a%20learning-based%20codec%0Aoutperforming%20the%20G-PCC%20standard%20on%20these%20datasets%20under%20the%20MPEG%20CTCs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10293v1&entry.124074799=Read"},
{"title": "Partial Distribution Matching via Partial Wasserstein Adversarial\n  Networks", "author": "Zi-Ming Wang and Nan Xue and Ling Lei and Rebecka J\u00f6rnsten and Gui-Song Xia", "abstract": "  This paper studies the problem of distribution matching (DM), which is a\nfundamental machine learning problem seeking to robustly align two probability\ndistributions. Our approach is established on a relaxed formulation, called\npartial distribution matching (PDM), which seeks to match a fraction of the\ndistributions instead of matching them completely. We theoretically derive the\nKantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy,\nand develop a partial Wasserstein adversarial network (PWAN) that efficiently\napproximates the PW discrepancy based on this dual form. Partial matching can\nthen be achieved by optimizing the network using gradient descent. Two\npractical tasks, point set registration and partial domain adaptation are\ninvestigated, where the goals are to partially match distributions in 3D space\nand high-dimensional feature space respectively. The experiment results confirm\nthat the proposed PWAN effectively produces highly robust matching results,\nperforming better or on par with the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2409.10499v1", "date": "2024-09-16", "relevancy": 2.593, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5581}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Distribution%20Matching%20via%20Partial%20Wasserstein%20Adversarial%0A%20%20Networks&body=Title%3A%20Partial%20Distribution%20Matching%20via%20Partial%20Wasserstein%20Adversarial%0A%20%20Networks%0AAuthor%3A%20Zi-Ming%20Wang%20and%20Nan%20Xue%20and%20Ling%20Lei%20and%20Rebecka%20J%C3%B6rnsten%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20distribution%20matching%20%28DM%29%2C%20which%20is%20a%0Afundamental%20machine%20learning%20problem%20seeking%20to%20robustly%20align%20two%20probability%0Adistributions.%20Our%20approach%20is%20established%20on%20a%20relaxed%20formulation%2C%20called%0Apartial%20distribution%20matching%20%28PDM%29%2C%20which%20seeks%20to%20match%20a%20fraction%20of%20the%0Adistributions%20instead%20of%20matching%20them%20completely.%20We%20theoretically%20derive%20the%0AKantorovich-Rubinstein%20duality%20for%20the%20partial%20Wasserstain-1%20%28PW%29%20discrepancy%2C%0Aand%20develop%20a%20partial%20Wasserstein%20adversarial%20network%20%28PWAN%29%20that%20efficiently%0Aapproximates%20the%20PW%20discrepancy%20based%20on%20this%20dual%20form.%20Partial%20matching%20can%0Athen%20be%20achieved%20by%20optimizing%20the%20network%20using%20gradient%20descent.%20Two%0Apractical%20tasks%2C%20point%20set%20registration%20and%20partial%20domain%20adaptation%20are%0Ainvestigated%2C%20where%20the%20goals%20are%20to%20partially%20match%20distributions%20in%203D%20space%0Aand%20high-dimensional%20feature%20space%20respectively.%20The%20experiment%20results%20confirm%0Athat%20the%20proposed%20PWAN%20effectively%20produces%20highly%20robust%20matching%20results%2C%0Aperforming%20better%20or%20on%20par%20with%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Distribution%2520Matching%2520via%2520Partial%2520Wasserstein%2520Adversarial%250A%2520%2520Networks%26entry.906535625%3DZi-Ming%2520Wang%2520and%2520Nan%2520Xue%2520and%2520Ling%2520Lei%2520and%2520Rebecka%2520J%25C3%25B6rnsten%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520problem%2520of%2520distribution%2520matching%2520%2528DM%2529%252C%2520which%2520is%2520a%250Afundamental%2520machine%2520learning%2520problem%2520seeking%2520to%2520robustly%2520align%2520two%2520probability%250Adistributions.%2520Our%2520approach%2520is%2520established%2520on%2520a%2520relaxed%2520formulation%252C%2520called%250Apartial%2520distribution%2520matching%2520%2528PDM%2529%252C%2520which%2520seeks%2520to%2520match%2520a%2520fraction%2520of%2520the%250Adistributions%2520instead%2520of%2520matching%2520them%2520completely.%2520We%2520theoretically%2520derive%2520the%250AKantorovich-Rubinstein%2520duality%2520for%2520the%2520partial%2520Wasserstain-1%2520%2528PW%2529%2520discrepancy%252C%250Aand%2520develop%2520a%2520partial%2520Wasserstein%2520adversarial%2520network%2520%2528PWAN%2529%2520that%2520efficiently%250Aapproximates%2520the%2520PW%2520discrepancy%2520based%2520on%2520this%2520dual%2520form.%2520Partial%2520matching%2520can%250Athen%2520be%2520achieved%2520by%2520optimizing%2520the%2520network%2520using%2520gradient%2520descent.%2520Two%250Apractical%2520tasks%252C%2520point%2520set%2520registration%2520and%2520partial%2520domain%2520adaptation%2520are%250Ainvestigated%252C%2520where%2520the%2520goals%2520are%2520to%2520partially%2520match%2520distributions%2520in%25203D%2520space%250Aand%2520high-dimensional%2520feature%2520space%2520respectively.%2520The%2520experiment%2520results%2520confirm%250Athat%2520the%2520proposed%2520PWAN%2520effectively%2520produces%2520highly%2520robust%2520matching%2520results%252C%250Aperforming%2520better%2520or%2520on%2520par%2520with%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Distribution%20Matching%20via%20Partial%20Wasserstein%20Adversarial%0A%20%20Networks&entry.906535625=Zi-Ming%20Wang%20and%20Nan%20Xue%20and%20Ling%20Lei%20and%20Rebecka%20J%C3%B6rnsten%20and%20Gui-Song%20Xia&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20distribution%20matching%20%28DM%29%2C%20which%20is%20a%0Afundamental%20machine%20learning%20problem%20seeking%20to%20robustly%20align%20two%20probability%0Adistributions.%20Our%20approach%20is%20established%20on%20a%20relaxed%20formulation%2C%20called%0Apartial%20distribution%20matching%20%28PDM%29%2C%20which%20seeks%20to%20match%20a%20fraction%20of%20the%0Adistributions%20instead%20of%20matching%20them%20completely.%20We%20theoretically%20derive%20the%0AKantorovich-Rubinstein%20duality%20for%20the%20partial%20Wasserstain-1%20%28PW%29%20discrepancy%2C%0Aand%20develop%20a%20partial%20Wasserstein%20adversarial%20network%20%28PWAN%29%20that%20efficiently%0Aapproximates%20the%20PW%20discrepancy%20based%20on%20this%20dual%20form.%20Partial%20matching%20can%0Athen%20be%20achieved%20by%20optimizing%20the%20network%20using%20gradient%20descent.%20Two%0Apractical%20tasks%2C%20point%20set%20registration%20and%20partial%20domain%20adaptation%20are%0Ainvestigated%2C%20where%20the%20goals%20are%20to%20partially%20match%20distributions%20in%203D%20space%0Aand%20high-dimensional%20feature%20space%20respectively.%20The%20experiment%20results%20confirm%0Athat%20the%20proposed%20PWAN%20effectively%20produces%20highly%20robust%20matching%20results%2C%0Aperforming%20better%20or%20on%20par%20with%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10499v1&entry.124074799=Read"},
{"title": "VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation", "author": "Aaron Mark Thomas and Sharu Theresa Jose", "abstract": "  This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,\nwhich combines the strengths of a classical Variational AutoEncoder (VAE) with\na hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The\nVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum\nmodel with shared parameters, utilizing the VAE's encoder for latent vector\nsampling during training. To generate new data from the trained model at\ninference, input latent vectors are sampled from a Gaussian Mixture Model\n(GMM), learnt on the training latent vectors. This, in turn, enhances the\ndiversity and quality of generated images. We evaluate the model's performance\non MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity\nof generated images compared to existing approaches.\n", "link": "http://arxiv.org/abs/2409.10339v1", "date": "2024-09-16", "relevancy": 2.5834, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5158}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAE-QWGAN%3A%20Improving%20Quantum%20GANs%20for%20High%20Resolution%20Image%20Generation&body=Title%3A%20VAE-QWGAN%3A%20Improving%20Quantum%20GANs%20for%20High%20Resolution%20Image%20Generation%0AAuthor%3A%20Aaron%20Mark%20Thomas%20and%20Sharu%20Theresa%20Jose%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20hybrid%20quantum%20generative%20model%2C%20the%20VAE-QWGAN%2C%0Awhich%20combines%20the%20strengths%20of%20a%20classical%20Variational%20AutoEncoder%20%28VAE%29%20with%0Aa%20hybrid%20Quantum%20Wasserstein%20Generative%20Adversarial%20Network%20%28QWGAN%29.%20The%0AVAE-QWGAN%20integrates%20the%20VAE%20decoder%20and%20QGAN%20generator%20into%20a%20single%20quantum%0Amodel%20with%20shared%20parameters%2C%20utilizing%20the%20VAE%27s%20encoder%20for%20latent%20vector%0Asampling%20during%20training.%20To%20generate%20new%20data%20from%20the%20trained%20model%20at%0Ainference%2C%20input%20latent%20vectors%20are%20sampled%20from%20a%20Gaussian%20Mixture%20Model%0A%28GMM%29%2C%20learnt%20on%20the%20training%20latent%20vectors.%20This%2C%20in%20turn%2C%20enhances%20the%0Adiversity%20and%20quality%20of%20generated%20images.%20We%20evaluate%20the%20model%27s%20performance%0Aon%20MNIST/Fashion-MNIST%20datasets%2C%20and%20demonstrate%20improved%20quality%20and%20diversity%0Aof%20generated%20images%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAE-QWGAN%253A%2520Improving%2520Quantum%2520GANs%2520for%2520High%2520Resolution%2520Image%2520Generation%26entry.906535625%3DAaron%2520Mark%2520Thomas%2520and%2520Sharu%2520Theresa%2520Jose%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520hybrid%2520quantum%2520generative%2520model%252C%2520the%2520VAE-QWGAN%252C%250Awhich%2520combines%2520the%2520strengths%2520of%2520a%2520classical%2520Variational%2520AutoEncoder%2520%2528VAE%2529%2520with%250Aa%2520hybrid%2520Quantum%2520Wasserstein%2520Generative%2520Adversarial%2520Network%2520%2528QWGAN%2529.%2520The%250AVAE-QWGAN%2520integrates%2520the%2520VAE%2520decoder%2520and%2520QGAN%2520generator%2520into%2520a%2520single%2520quantum%250Amodel%2520with%2520shared%2520parameters%252C%2520utilizing%2520the%2520VAE%2527s%2520encoder%2520for%2520latent%2520vector%250Asampling%2520during%2520training.%2520To%2520generate%2520new%2520data%2520from%2520the%2520trained%2520model%2520at%250Ainference%252C%2520input%2520latent%2520vectors%2520are%2520sampled%2520from%2520a%2520Gaussian%2520Mixture%2520Model%250A%2528GMM%2529%252C%2520learnt%2520on%2520the%2520training%2520latent%2520vectors.%2520This%252C%2520in%2520turn%252C%2520enhances%2520the%250Adiversity%2520and%2520quality%2520of%2520generated%2520images.%2520We%2520evaluate%2520the%2520model%2527s%2520performance%250Aon%2520MNIST/Fashion-MNIST%2520datasets%252C%2520and%2520demonstrate%2520improved%2520quality%2520and%2520diversity%250Aof%2520generated%2520images%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAE-QWGAN%3A%20Improving%20Quantum%20GANs%20for%20High%20Resolution%20Image%20Generation&entry.906535625=Aaron%20Mark%20Thomas%20and%20Sharu%20Theresa%20Jose&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20hybrid%20quantum%20generative%20model%2C%20the%20VAE-QWGAN%2C%0Awhich%20combines%20the%20strengths%20of%20a%20classical%20Variational%20AutoEncoder%20%28VAE%29%20with%0Aa%20hybrid%20Quantum%20Wasserstein%20Generative%20Adversarial%20Network%20%28QWGAN%29.%20The%0AVAE-QWGAN%20integrates%20the%20VAE%20decoder%20and%20QGAN%20generator%20into%20a%20single%20quantum%0Amodel%20with%20shared%20parameters%2C%20utilizing%20the%20VAE%27s%20encoder%20for%20latent%20vector%0Asampling%20during%20training.%20To%20generate%20new%20data%20from%20the%20trained%20model%20at%0Ainference%2C%20input%20latent%20vectors%20are%20sampled%20from%20a%20Gaussian%20Mixture%20Model%0A%28GMM%29%2C%20learnt%20on%20the%20training%20latent%20vectors.%20This%2C%20in%20turn%2C%20enhances%20the%0Adiversity%20and%20quality%20of%20generated%20images.%20We%20evaluate%20the%20model%27s%20performance%0Aon%20MNIST/Fashion-MNIST%20datasets%2C%20and%20demonstrate%20improved%20quality%20and%20diversity%0Aof%20generated%20images%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10339v1&entry.124074799=Read"},
{"title": "MOST: MR reconstruction Optimization for multiple downStream Tasks via\n  continual learning", "author": "Hwihun Jeong and Se Young Chun and Jongho Lee", "abstract": "  Deep learning-based Magnetic Resonance (MR) reconstruction methods have\nfocused on generating high-quality images but they often overlook the impact on\ndownstream tasks (e.g., segmentation) that utilize the reconstructed images.\nCascading separately trained reconstruction network and downstream task network\nhas been shown to introduce performance degradation due to error propagation\nand domain gaps between training datasets. To mitigate this issue, downstream\ntask-oriented reconstruction optimization has been proposed for a single\ndownstream task. Expanding this optimization to multi-task scenarios is not\nstraightforward. In this work, we extended this optimization to sequentially\nintroduced multiple downstream tasks and demonstrated that a single MR\nreconstruction network can be optimized for multiple downstream tasks by\ndeploying continual learning (MOST). MOST integrated techniques from\nreplay-based continual learning and image-guided loss to overcome catastrophic\nforgetting. Comparative experiments demonstrated that MOST outperformed a\nreconstruction network without finetuning, a reconstruction network with\nna\\\"ive finetuning, and conventional continual learning methods. This\nadvancement empowers the application of a single MR reconstruction network for\nmultiple downstream tasks. The source code is available at:\nhttps://github.com/SNU-LIST/MOST\n", "link": "http://arxiv.org/abs/2409.10394v1", "date": "2024-09-16", "relevancy": 2.5746, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5305}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOST%3A%20MR%20reconstruction%20Optimization%20for%20multiple%20downStream%20Tasks%20via%0A%20%20continual%20learning&body=Title%3A%20MOST%3A%20MR%20reconstruction%20Optimization%20for%20multiple%20downStream%20Tasks%20via%0A%20%20continual%20learning%0AAuthor%3A%20Hwihun%20Jeong%20and%20Se%20Young%20Chun%20and%20Jongho%20Lee%0AAbstract%3A%20%20%20Deep%20learning-based%20Magnetic%20Resonance%20%28MR%29%20reconstruction%20methods%20have%0Afocused%20on%20generating%20high-quality%20images%20but%20they%20often%20overlook%20the%20impact%20on%0Adownstream%20tasks%20%28e.g.%2C%20segmentation%29%20that%20utilize%20the%20reconstructed%20images.%0ACascading%20separately%20trained%20reconstruction%20network%20and%20downstream%20task%20network%0Ahas%20been%20shown%20to%20introduce%20performance%20degradation%20due%20to%20error%20propagation%0Aand%20domain%20gaps%20between%20training%20datasets.%20To%20mitigate%20this%20issue%2C%20downstream%0Atask-oriented%20reconstruction%20optimization%20has%20been%20proposed%20for%20a%20single%0Adownstream%20task.%20Expanding%20this%20optimization%20to%20multi-task%20scenarios%20is%20not%0Astraightforward.%20In%20this%20work%2C%20we%20extended%20this%20optimization%20to%20sequentially%0Aintroduced%20multiple%20downstream%20tasks%20and%20demonstrated%20that%20a%20single%20MR%0Areconstruction%20network%20can%20be%20optimized%20for%20multiple%20downstream%20tasks%20by%0Adeploying%20continual%20learning%20%28MOST%29.%20MOST%20integrated%20techniques%20from%0Areplay-based%20continual%20learning%20and%20image-guided%20loss%20to%20overcome%20catastrophic%0Aforgetting.%20Comparative%20experiments%20demonstrated%20that%20MOST%20outperformed%20a%0Areconstruction%20network%20without%20finetuning%2C%20a%20reconstruction%20network%20with%0Ana%5C%22ive%20finetuning%2C%20and%20conventional%20continual%20learning%20methods.%20This%0Aadvancement%20empowers%20the%20application%20of%20a%20single%20MR%20reconstruction%20network%20for%0Amultiple%20downstream%20tasks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SNU-LIST/MOST%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOST%253A%2520MR%2520reconstruction%2520Optimization%2520for%2520multiple%2520downStream%2520Tasks%2520via%250A%2520%2520continual%2520learning%26entry.906535625%3DHwihun%2520Jeong%2520and%2520Se%2520Young%2520Chun%2520and%2520Jongho%2520Lee%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520Magnetic%2520Resonance%2520%2528MR%2529%2520reconstruction%2520methods%2520have%250Afocused%2520on%2520generating%2520high-quality%2520images%2520but%2520they%2520often%2520overlook%2520the%2520impact%2520on%250Adownstream%2520tasks%2520%2528e.g.%252C%2520segmentation%2529%2520that%2520utilize%2520the%2520reconstructed%2520images.%250ACascading%2520separately%2520trained%2520reconstruction%2520network%2520and%2520downstream%2520task%2520network%250Ahas%2520been%2520shown%2520to%2520introduce%2520performance%2520degradation%2520due%2520to%2520error%2520propagation%250Aand%2520domain%2520gaps%2520between%2520training%2520datasets.%2520To%2520mitigate%2520this%2520issue%252C%2520downstream%250Atask-oriented%2520reconstruction%2520optimization%2520has%2520been%2520proposed%2520for%2520a%2520single%250Adownstream%2520task.%2520Expanding%2520this%2520optimization%2520to%2520multi-task%2520scenarios%2520is%2520not%250Astraightforward.%2520In%2520this%2520work%252C%2520we%2520extended%2520this%2520optimization%2520to%2520sequentially%250Aintroduced%2520multiple%2520downstream%2520tasks%2520and%2520demonstrated%2520that%2520a%2520single%2520MR%250Areconstruction%2520network%2520can%2520be%2520optimized%2520for%2520multiple%2520downstream%2520tasks%2520by%250Adeploying%2520continual%2520learning%2520%2528MOST%2529.%2520MOST%2520integrated%2520techniques%2520from%250Areplay-based%2520continual%2520learning%2520and%2520image-guided%2520loss%2520to%2520overcome%2520catastrophic%250Aforgetting.%2520Comparative%2520experiments%2520demonstrated%2520that%2520MOST%2520outperformed%2520a%250Areconstruction%2520network%2520without%2520finetuning%252C%2520a%2520reconstruction%2520network%2520with%250Ana%255C%2522ive%2520finetuning%252C%2520and%2520conventional%2520continual%2520learning%2520methods.%2520This%250Aadvancement%2520empowers%2520the%2520application%2520of%2520a%2520single%2520MR%2520reconstruction%2520network%2520for%250Amultiple%2520downstream%2520tasks.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/SNU-LIST/MOST%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOST%3A%20MR%20reconstruction%20Optimization%20for%20multiple%20downStream%20Tasks%20via%0A%20%20continual%20learning&entry.906535625=Hwihun%20Jeong%20and%20Se%20Young%20Chun%20and%20Jongho%20Lee&entry.1292438233=%20%20Deep%20learning-based%20Magnetic%20Resonance%20%28MR%29%20reconstruction%20methods%20have%0Afocused%20on%20generating%20high-quality%20images%20but%20they%20often%20overlook%20the%20impact%20on%0Adownstream%20tasks%20%28e.g.%2C%20segmentation%29%20that%20utilize%20the%20reconstructed%20images.%0ACascading%20separately%20trained%20reconstruction%20network%20and%20downstream%20task%20network%0Ahas%20been%20shown%20to%20introduce%20performance%20degradation%20due%20to%20error%20propagation%0Aand%20domain%20gaps%20between%20training%20datasets.%20To%20mitigate%20this%20issue%2C%20downstream%0Atask-oriented%20reconstruction%20optimization%20has%20been%20proposed%20for%20a%20single%0Adownstream%20task.%20Expanding%20this%20optimization%20to%20multi-task%20scenarios%20is%20not%0Astraightforward.%20In%20this%20work%2C%20we%20extended%20this%20optimization%20to%20sequentially%0Aintroduced%20multiple%20downstream%20tasks%20and%20demonstrated%20that%20a%20single%20MR%0Areconstruction%20network%20can%20be%20optimized%20for%20multiple%20downstream%20tasks%20by%0Adeploying%20continual%20learning%20%28MOST%29.%20MOST%20integrated%20techniques%20from%0Areplay-based%20continual%20learning%20and%20image-guided%20loss%20to%20overcome%20catastrophic%0Aforgetting.%20Comparative%20experiments%20demonstrated%20that%20MOST%20outperformed%20a%0Areconstruction%20network%20without%20finetuning%2C%20a%20reconstruction%20network%20with%0Ana%5C%22ive%20finetuning%2C%20and%20conventional%20continual%20learning%20methods.%20This%0Aadvancement%20empowers%20the%20application%20of%20a%20single%20MR%20reconstruction%20network%20for%0Amultiple%20downstream%20tasks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SNU-LIST/MOST%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10394v1&entry.124074799=Read"},
{"title": "Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning", "author": "Amin Karimi Monsefi and Mengxi Zhou and Nastaran Karimi Monsefi and Ser-Nam Lim and Wei-Lun Chao and Rajiv Ramnath", "abstract": "  We present a novel frequency-based Self-Supervised Learning (SSL) approach\nthat significantly enhances its efficacy for pre-training. Prior work in this\ndirection masks out pre-defined frequencies in the input image and employs a\nreconstruction loss to pre-train the model. While achieving promising results,\nsuch an implementation has two fundamental limitations as identified in our\npaper. First, using pre-defined frequencies overlooks the variability of image\nfrequency responses. Second, pre-trained with frequency-filtered images, the\nresulting model needs relatively more data to adapt to naturally looking images\nduring fine-tuning. To address these drawbacks, we propose FOurier transform\ncompression with seLf-Knowledge distillation (FOLK), integrating two dedicated\nideas. First, inspired by image compression, we adaptively select the\nmasked-out frequencies based on image frequency responses, creating more\nsuitable SSL tasks for pre-training. Second, we employ a two-branch framework\nempowered by knowledge distillation, enabling the model to take both the\nfiltered and original images as input, largely reducing the burden of\ndownstream tasks. Our experimental results demonstrate the effectiveness of\nFOLK in achieving competitive performance to many state-of-the-art SSL methods\nacross various downstream tasks, including image classification, few-shot\nlearning, and semantic segmentation.\n", "link": "http://arxiv.org/abs/2409.10362v1", "date": "2024-09-16", "relevancy": 2.5619, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5279}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Guided%20Masking%20for%20Enhanced%20Vision%20Self-Supervised%20Learning&body=Title%3A%20Frequency-Guided%20Masking%20for%20Enhanced%20Vision%20Self-Supervised%20Learning%0AAuthor%3A%20Amin%20Karimi%20Monsefi%20and%20Mengxi%20Zhou%20and%20Nastaran%20Karimi%20Monsefi%20and%20Ser-Nam%20Lim%20and%20Wei-Lun%20Chao%20and%20Rajiv%20Ramnath%0AAbstract%3A%20%20%20We%20present%20a%20novel%20frequency-based%20Self-Supervised%20Learning%20%28SSL%29%20approach%0Athat%20significantly%20enhances%20its%20efficacy%20for%20pre-training.%20Prior%20work%20in%20this%0Adirection%20masks%20out%20pre-defined%20frequencies%20in%20the%20input%20image%20and%20employs%20a%0Areconstruction%20loss%20to%20pre-train%20the%20model.%20While%20achieving%20promising%20results%2C%0Asuch%20an%20implementation%20has%20two%20fundamental%20limitations%20as%20identified%20in%20our%0Apaper.%20First%2C%20using%20pre-defined%20frequencies%20overlooks%20the%20variability%20of%20image%0Afrequency%20responses.%20Second%2C%20pre-trained%20with%20frequency-filtered%20images%2C%20the%0Aresulting%20model%20needs%20relatively%20more%20data%20to%20adapt%20to%20naturally%20looking%20images%0Aduring%20fine-tuning.%20To%20address%20these%20drawbacks%2C%20we%20propose%20FOurier%20transform%0Acompression%20with%20seLf-Knowledge%20distillation%20%28FOLK%29%2C%20integrating%20two%20dedicated%0Aideas.%20First%2C%20inspired%20by%20image%20compression%2C%20we%20adaptively%20select%20the%0Amasked-out%20frequencies%20based%20on%20image%20frequency%20responses%2C%20creating%20more%0Asuitable%20SSL%20tasks%20for%20pre-training.%20Second%2C%20we%20employ%20a%20two-branch%20framework%0Aempowered%20by%20knowledge%20distillation%2C%20enabling%20the%20model%20to%20take%20both%20the%0Afiltered%20and%20original%20images%20as%20input%2C%20largely%20reducing%20the%20burden%20of%0Adownstream%20tasks.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%0AFOLK%20in%20achieving%20competitive%20performance%20to%20many%20state-of-the-art%20SSL%20methods%0Aacross%20various%20downstream%20tasks%2C%20including%20image%20classification%2C%20few-shot%0Alearning%2C%20and%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Guided%2520Masking%2520for%2520Enhanced%2520Vision%2520Self-Supervised%2520Learning%26entry.906535625%3DAmin%2520Karimi%2520Monsefi%2520and%2520Mengxi%2520Zhou%2520and%2520Nastaran%2520Karimi%2520Monsefi%2520and%2520Ser-Nam%2520Lim%2520and%2520Wei-Lun%2520Chao%2520and%2520Rajiv%2520Ramnath%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520frequency-based%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520approach%250Athat%2520significantly%2520enhances%2520its%2520efficacy%2520for%2520pre-training.%2520Prior%2520work%2520in%2520this%250Adirection%2520masks%2520out%2520pre-defined%2520frequencies%2520in%2520the%2520input%2520image%2520and%2520employs%2520a%250Areconstruction%2520loss%2520to%2520pre-train%2520the%2520model.%2520While%2520achieving%2520promising%2520results%252C%250Asuch%2520an%2520implementation%2520has%2520two%2520fundamental%2520limitations%2520as%2520identified%2520in%2520our%250Apaper.%2520First%252C%2520using%2520pre-defined%2520frequencies%2520overlooks%2520the%2520variability%2520of%2520image%250Afrequency%2520responses.%2520Second%252C%2520pre-trained%2520with%2520frequency-filtered%2520images%252C%2520the%250Aresulting%2520model%2520needs%2520relatively%2520more%2520data%2520to%2520adapt%2520to%2520naturally%2520looking%2520images%250Aduring%2520fine-tuning.%2520To%2520address%2520these%2520drawbacks%252C%2520we%2520propose%2520FOurier%2520transform%250Acompression%2520with%2520seLf-Knowledge%2520distillation%2520%2528FOLK%2529%252C%2520integrating%2520two%2520dedicated%250Aideas.%2520First%252C%2520inspired%2520by%2520image%2520compression%252C%2520we%2520adaptively%2520select%2520the%250Amasked-out%2520frequencies%2520based%2520on%2520image%2520frequency%2520responses%252C%2520creating%2520more%250Asuitable%2520SSL%2520tasks%2520for%2520pre-training.%2520Second%252C%2520we%2520employ%2520a%2520two-branch%2520framework%250Aempowered%2520by%2520knowledge%2520distillation%252C%2520enabling%2520the%2520model%2520to%2520take%2520both%2520the%250Afiltered%2520and%2520original%2520images%2520as%2520input%252C%2520largely%2520reducing%2520the%2520burden%2520of%250Adownstream%2520tasks.%2520Our%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250AFOLK%2520in%2520achieving%2520competitive%2520performance%2520to%2520many%2520state-of-the-art%2520SSL%2520methods%250Aacross%2520various%2520downstream%2520tasks%252C%2520including%2520image%2520classification%252C%2520few-shot%250Alearning%252C%2520and%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Guided%20Masking%20for%20Enhanced%20Vision%20Self-Supervised%20Learning&entry.906535625=Amin%20Karimi%20Monsefi%20and%20Mengxi%20Zhou%20and%20Nastaran%20Karimi%20Monsefi%20and%20Ser-Nam%20Lim%20and%20Wei-Lun%20Chao%20and%20Rajiv%20Ramnath&entry.1292438233=%20%20We%20present%20a%20novel%20frequency-based%20Self-Supervised%20Learning%20%28SSL%29%20approach%0Athat%20significantly%20enhances%20its%20efficacy%20for%20pre-training.%20Prior%20work%20in%20this%0Adirection%20masks%20out%20pre-defined%20frequencies%20in%20the%20input%20image%20and%20employs%20a%0Areconstruction%20loss%20to%20pre-train%20the%20model.%20While%20achieving%20promising%20results%2C%0Asuch%20an%20implementation%20has%20two%20fundamental%20limitations%20as%20identified%20in%20our%0Apaper.%20First%2C%20using%20pre-defined%20frequencies%20overlooks%20the%20variability%20of%20image%0Afrequency%20responses.%20Second%2C%20pre-trained%20with%20frequency-filtered%20images%2C%20the%0Aresulting%20model%20needs%20relatively%20more%20data%20to%20adapt%20to%20naturally%20looking%20images%0Aduring%20fine-tuning.%20To%20address%20these%20drawbacks%2C%20we%20propose%20FOurier%20transform%0Acompression%20with%20seLf-Knowledge%20distillation%20%28FOLK%29%2C%20integrating%20two%20dedicated%0Aideas.%20First%2C%20inspired%20by%20image%20compression%2C%20we%20adaptively%20select%20the%0Amasked-out%20frequencies%20based%20on%20image%20frequency%20responses%2C%20creating%20more%0Asuitable%20SSL%20tasks%20for%20pre-training.%20Second%2C%20we%20employ%20a%20two-branch%20framework%0Aempowered%20by%20knowledge%20distillation%2C%20enabling%20the%20model%20to%20take%20both%20the%0Afiltered%20and%20original%20images%20as%20input%2C%20largely%20reducing%20the%20burden%20of%0Adownstream%20tasks.%20Our%20experimental%20results%20demonstrate%20the%20effectiveness%20of%0AFOLK%20in%20achieving%20competitive%20performance%20to%20many%20state-of-the-art%20SSL%20methods%0Aacross%20various%20downstream%20tasks%2C%20including%20image%20classification%2C%20few-shot%0Alearning%2C%20and%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10362v1&entry.124074799=Read"},
{"title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using\n  Gaussian Splatting", "author": "Mohammad Nomaan Qureshi and Sparsh Garg and Francisco Yandun and David Held and George Kantor and Abhishesh Silwal", "abstract": "  Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim}and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data.\n", "link": "http://arxiv.org/abs/2409.10161v1", "date": "2024-09-16", "relevancy": 2.5599, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6975}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6256}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatSim%3A%20Zero-Shot%20Sim2Real%20Transfer%20of%20RGB%20Manipulation%20Policies%20Using%0A%20%20Gaussian%20Splatting&body=Title%3A%20SplatSim%3A%20Zero-Shot%20Sim2Real%20Transfer%20of%20RGB%20Manipulation%20Policies%20Using%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Mohammad%20Nomaan%20Qureshi%20and%20Sparsh%20Garg%20and%20Francisco%20Yandun%20and%20David%20Held%20and%20George%20Kantor%20and%20Abhishesh%20Silwal%0AAbstract%3A%20%20%20Sim2Real%20transfer%2C%20particularly%20for%20manipulation%20policies%20relying%20on%20RGB%0Aimages%2C%20remains%20a%20critical%20challenge%20in%20robotics%20due%20to%20the%20significant%20domain%0Ashift%20between%20synthetic%20and%20real-world%20visual%20data.%20In%20this%20paper%2C%20we%20propose%0ASplatSim%2C%20a%20novel%20framework%20that%20leverages%20Gaussian%20Splatting%20as%20the%20primary%0Arendering%20primitive%20to%20reduce%20the%20Sim2Real%20gap%20for%20RGB-based%20manipulation%0Apolicies.%20By%20replacing%20traditional%20mesh%20representations%20with%20Gaussian%20Splats%20in%0Asimulators%2C%20SplatSim%20produces%20highly%20photorealistic%20synthetic%20data%20while%0Amaintaining%20the%20scalability%20and%20cost-efficiency%20of%20simulation.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20framework%20by%20training%20manipulation%20policies%20within%0ASplatSim%7Dand%20deploying%20them%20in%20the%20real%20world%20in%20a%20zero-shot%20manner%2C%20achieving%0Aan%20average%20success%20rate%20of%2086.25%25%2C%20compared%20to%2097.5%25%20for%20policies%20trained%20on%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatSim%253A%2520Zero-Shot%2520Sim2Real%2520Transfer%2520of%2520RGB%2520Manipulation%2520Policies%2520Using%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DMohammad%2520Nomaan%2520Qureshi%2520and%2520Sparsh%2520Garg%2520and%2520Francisco%2520Yandun%2520and%2520David%2520Held%2520and%2520George%2520Kantor%2520and%2520Abhishesh%2520Silwal%26entry.1292438233%3D%2520%2520Sim2Real%2520transfer%252C%2520particularly%2520for%2520manipulation%2520policies%2520relying%2520on%2520RGB%250Aimages%252C%2520remains%2520a%2520critical%2520challenge%2520in%2520robotics%2520due%2520to%2520the%2520significant%2520domain%250Ashift%2520between%2520synthetic%2520and%2520real-world%2520visual%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASplatSim%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520Gaussian%2520Splatting%2520as%2520the%2520primary%250Arendering%2520primitive%2520to%2520reduce%2520the%2520Sim2Real%2520gap%2520for%2520RGB-based%2520manipulation%250Apolicies.%2520By%2520replacing%2520traditional%2520mesh%2520representations%2520with%2520Gaussian%2520Splats%2520in%250Asimulators%252C%2520SplatSim%2520produces%2520highly%2520photorealistic%2520synthetic%2520data%2520while%250Amaintaining%2520the%2520scalability%2520and%2520cost-efficiency%2520of%2520simulation.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520framework%2520by%2520training%2520manipulation%2520policies%2520within%250ASplatSim%257Dand%2520deploying%2520them%2520in%2520the%2520real%2520world%2520in%2520a%2520zero-shot%2520manner%252C%2520achieving%250Aan%2520average%2520success%2520rate%2520of%252086.25%2525%252C%2520compared%2520to%252097.5%2525%2520for%2520policies%2520trained%2520on%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatSim%3A%20Zero-Shot%20Sim2Real%20Transfer%20of%20RGB%20Manipulation%20Policies%20Using%0A%20%20Gaussian%20Splatting&entry.906535625=Mohammad%20Nomaan%20Qureshi%20and%20Sparsh%20Garg%20and%20Francisco%20Yandun%20and%20David%20Held%20and%20George%20Kantor%20and%20Abhishesh%20Silwal&entry.1292438233=%20%20Sim2Real%20transfer%2C%20particularly%20for%20manipulation%20policies%20relying%20on%20RGB%0Aimages%2C%20remains%20a%20critical%20challenge%20in%20robotics%20due%20to%20the%20significant%20domain%0Ashift%20between%20synthetic%20and%20real-world%20visual%20data.%20In%20this%20paper%2C%20we%20propose%0ASplatSim%2C%20a%20novel%20framework%20that%20leverages%20Gaussian%20Splatting%20as%20the%20primary%0Arendering%20primitive%20to%20reduce%20the%20Sim2Real%20gap%20for%20RGB-based%20manipulation%0Apolicies.%20By%20replacing%20traditional%20mesh%20representations%20with%20Gaussian%20Splats%20in%0Asimulators%2C%20SplatSim%20produces%20highly%20photorealistic%20synthetic%20data%20while%0Amaintaining%20the%20scalability%20and%20cost-efficiency%20of%20simulation.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20framework%20by%20training%20manipulation%20policies%20within%0ASplatSim%7Dand%20deploying%20them%20in%20the%20real%20world%20in%20a%20zero-shot%20manner%2C%20achieving%0Aan%20average%20success%20rate%20of%2086.25%25%2C%20compared%20to%2097.5%25%20for%20policies%20trained%20on%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10161v1&entry.124074799=Read"},
{"title": "CBMAP: Clustering-based manifold approximation and projection for\n  dimensionality reduction", "author": "Berat Dogan", "abstract": "  Dimensionality reduction methods are employed to decrease data\ndimensionality, either to enhance machine learning performance or to facilitate\ndata visualization in two or three-dimensional spaces. These methods typically\nfall into two categories: feature selection and feature transformation. Feature\nselection retains significant features, while feature transformation projects\ndata into a lower-dimensional space, with linear and nonlinear methods. While\nnonlinear methods excel in preserving local structures and capturing nonlinear\nrelationships, they may struggle with interpreting global structures and can be\ncomputationally intensive. Recent algorithms, such as the t-SNE, UMAP, TriMap,\nand PaCMAP prioritize preserving local structures, often at the expense of\naccurately representing global structures, leading to clusters being spread out\nmore in lower-dimensional spaces. Moreover, these methods heavily rely on\nhyperparameters, making their results sensitive to parameter settings. To\naddress these limitations, this study introduces a clustering-based approach,\nnamely CBMAP (Clustering-Based Manifold Approximation and Projection), for\ndimensionality reduction. CBMAP aims to preserve both global and local\nstructures, ensuring that clusters in lower-dimensional spaces closely resemble\nthose in high-dimensional spaces. Experimental evaluations on benchmark\ndatasets demonstrate CBMAP's efficacy, offering speed, scalability, and minimal\nreliance on hyperparameters. Importantly, CBMAP enables low-dimensional\nprojection of test data, addressing a critical need in machine learning\napplications. CBMAP is made freely available at\nhttps://github.com/doganlab/cbmap and can be installed from the Python Package\nDirectory (PyPI) software repository with the command pip install cbmap.\n", "link": "http://arxiv.org/abs/2404.17940v2", "date": "2024-09-16", "relevancy": 2.5572, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBMAP%3A%20Clustering-based%20manifold%20approximation%20and%20projection%20for%0A%20%20dimensionality%20reduction&body=Title%3A%20CBMAP%3A%20Clustering-based%20manifold%20approximation%20and%20projection%20for%0A%20%20dimensionality%20reduction%0AAuthor%3A%20Berat%20Dogan%0AAbstract%3A%20%20%20Dimensionality%20reduction%20methods%20are%20employed%20to%20decrease%20data%0Adimensionality%2C%20either%20to%20enhance%20machine%20learning%20performance%20or%20to%20facilitate%0Adata%20visualization%20in%20two%20or%20three-dimensional%20spaces.%20These%20methods%20typically%0Afall%20into%20two%20categories%3A%20feature%20selection%20and%20feature%20transformation.%20Feature%0Aselection%20retains%20significant%20features%2C%20while%20feature%20transformation%20projects%0Adata%20into%20a%20lower-dimensional%20space%2C%20with%20linear%20and%20nonlinear%20methods.%20While%0Anonlinear%20methods%20excel%20in%20preserving%20local%20structures%20and%20capturing%20nonlinear%0Arelationships%2C%20they%20may%20struggle%20with%20interpreting%20global%20structures%20and%20can%20be%0Acomputationally%20intensive.%20Recent%20algorithms%2C%20such%20as%20the%20t-SNE%2C%20UMAP%2C%20TriMap%2C%0Aand%20PaCMAP%20prioritize%20preserving%20local%20structures%2C%20often%20at%20the%20expense%20of%0Aaccurately%20representing%20global%20structures%2C%20leading%20to%20clusters%20being%20spread%20out%0Amore%20in%20lower-dimensional%20spaces.%20Moreover%2C%20these%20methods%20heavily%20rely%20on%0Ahyperparameters%2C%20making%20their%20results%20sensitive%20to%20parameter%20settings.%20To%0Aaddress%20these%20limitations%2C%20this%20study%20introduces%20a%20clustering-based%20approach%2C%0Anamely%20CBMAP%20%28Clustering-Based%20Manifold%20Approximation%20and%20Projection%29%2C%20for%0Adimensionality%20reduction.%20CBMAP%20aims%20to%20preserve%20both%20global%20and%20local%0Astructures%2C%20ensuring%20that%20clusters%20in%20lower-dimensional%20spaces%20closely%20resemble%0Athose%20in%20high-dimensional%20spaces.%20Experimental%20evaluations%20on%20benchmark%0Adatasets%20demonstrate%20CBMAP%27s%20efficacy%2C%20offering%20speed%2C%20scalability%2C%20and%20minimal%0Areliance%20on%20hyperparameters.%20Importantly%2C%20CBMAP%20enables%20low-dimensional%0Aprojection%20of%20test%20data%2C%20addressing%20a%20critical%20need%20in%20machine%20learning%0Aapplications.%20CBMAP%20is%20made%20freely%20available%20at%0Ahttps%3A//github.com/doganlab/cbmap%20and%20can%20be%20installed%20from%20the%20Python%20Package%0ADirectory%20%28PyPI%29%20software%20repository%20with%20the%20command%20pip%20install%20cbmap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBMAP%253A%2520Clustering-based%2520manifold%2520approximation%2520and%2520projection%2520for%250A%2520%2520dimensionality%2520reduction%26entry.906535625%3DBerat%2520Dogan%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520methods%2520are%2520employed%2520to%2520decrease%2520data%250Adimensionality%252C%2520either%2520to%2520enhance%2520machine%2520learning%2520performance%2520or%2520to%2520facilitate%250Adata%2520visualization%2520in%2520two%2520or%2520three-dimensional%2520spaces.%2520These%2520methods%2520typically%250Afall%2520into%2520two%2520categories%253A%2520feature%2520selection%2520and%2520feature%2520transformation.%2520Feature%250Aselection%2520retains%2520significant%2520features%252C%2520while%2520feature%2520transformation%2520projects%250Adata%2520into%2520a%2520lower-dimensional%2520space%252C%2520with%2520linear%2520and%2520nonlinear%2520methods.%2520While%250Anonlinear%2520methods%2520excel%2520in%2520preserving%2520local%2520structures%2520and%2520capturing%2520nonlinear%250Arelationships%252C%2520they%2520may%2520struggle%2520with%2520interpreting%2520global%2520structures%2520and%2520can%2520be%250Acomputationally%2520intensive.%2520Recent%2520algorithms%252C%2520such%2520as%2520the%2520t-SNE%252C%2520UMAP%252C%2520TriMap%252C%250Aand%2520PaCMAP%2520prioritize%2520preserving%2520local%2520structures%252C%2520often%2520at%2520the%2520expense%2520of%250Aaccurately%2520representing%2520global%2520structures%252C%2520leading%2520to%2520clusters%2520being%2520spread%2520out%250Amore%2520in%2520lower-dimensional%2520spaces.%2520Moreover%252C%2520these%2520methods%2520heavily%2520rely%2520on%250Ahyperparameters%252C%2520making%2520their%2520results%2520sensitive%2520to%2520parameter%2520settings.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520study%2520introduces%2520a%2520clustering-based%2520approach%252C%250Anamely%2520CBMAP%2520%2528Clustering-Based%2520Manifold%2520Approximation%2520and%2520Projection%2529%252C%2520for%250Adimensionality%2520reduction.%2520CBMAP%2520aims%2520to%2520preserve%2520both%2520global%2520and%2520local%250Astructures%252C%2520ensuring%2520that%2520clusters%2520in%2520lower-dimensional%2520spaces%2520closely%2520resemble%250Athose%2520in%2520high-dimensional%2520spaces.%2520Experimental%2520evaluations%2520on%2520benchmark%250Adatasets%2520demonstrate%2520CBMAP%2527s%2520efficacy%252C%2520offering%2520speed%252C%2520scalability%252C%2520and%2520minimal%250Areliance%2520on%2520hyperparameters.%2520Importantly%252C%2520CBMAP%2520enables%2520low-dimensional%250Aprojection%2520of%2520test%2520data%252C%2520addressing%2520a%2520critical%2520need%2520in%2520machine%2520learning%250Aapplications.%2520CBMAP%2520is%2520made%2520freely%2520available%2520at%250Ahttps%253A//github.com/doganlab/cbmap%2520and%2520can%2520be%2520installed%2520from%2520the%2520Python%2520Package%250ADirectory%2520%2528PyPI%2529%2520software%2520repository%2520with%2520the%2520command%2520pip%2520install%2520cbmap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBMAP%3A%20Clustering-based%20manifold%20approximation%20and%20projection%20for%0A%20%20dimensionality%20reduction&entry.906535625=Berat%20Dogan&entry.1292438233=%20%20Dimensionality%20reduction%20methods%20are%20employed%20to%20decrease%20data%0Adimensionality%2C%20either%20to%20enhance%20machine%20learning%20performance%20or%20to%20facilitate%0Adata%20visualization%20in%20two%20or%20three-dimensional%20spaces.%20These%20methods%20typically%0Afall%20into%20two%20categories%3A%20feature%20selection%20and%20feature%20transformation.%20Feature%0Aselection%20retains%20significant%20features%2C%20while%20feature%20transformation%20projects%0Adata%20into%20a%20lower-dimensional%20space%2C%20with%20linear%20and%20nonlinear%20methods.%20While%0Anonlinear%20methods%20excel%20in%20preserving%20local%20structures%20and%20capturing%20nonlinear%0Arelationships%2C%20they%20may%20struggle%20with%20interpreting%20global%20structures%20and%20can%20be%0Acomputationally%20intensive.%20Recent%20algorithms%2C%20such%20as%20the%20t-SNE%2C%20UMAP%2C%20TriMap%2C%0Aand%20PaCMAP%20prioritize%20preserving%20local%20structures%2C%20often%20at%20the%20expense%20of%0Aaccurately%20representing%20global%20structures%2C%20leading%20to%20clusters%20being%20spread%20out%0Amore%20in%20lower-dimensional%20spaces.%20Moreover%2C%20these%20methods%20heavily%20rely%20on%0Ahyperparameters%2C%20making%20their%20results%20sensitive%20to%20parameter%20settings.%20To%0Aaddress%20these%20limitations%2C%20this%20study%20introduces%20a%20clustering-based%20approach%2C%0Anamely%20CBMAP%20%28Clustering-Based%20Manifold%20Approximation%20and%20Projection%29%2C%20for%0Adimensionality%20reduction.%20CBMAP%20aims%20to%20preserve%20both%20global%20and%20local%0Astructures%2C%20ensuring%20that%20clusters%20in%20lower-dimensional%20spaces%20closely%20resemble%0Athose%20in%20high-dimensional%20spaces.%20Experimental%20evaluations%20on%20benchmark%0Adatasets%20demonstrate%20CBMAP%27s%20efficacy%2C%20offering%20speed%2C%20scalability%2C%20and%20minimal%0Areliance%20on%20hyperparameters.%20Importantly%2C%20CBMAP%20enables%20low-dimensional%0Aprojection%20of%20test%20data%2C%20addressing%20a%20critical%20need%20in%20machine%20learning%0Aapplications.%20CBMAP%20is%20made%20freely%20available%20at%0Ahttps%3A//github.com/doganlab/cbmap%20and%20can%20be%20installed%20from%20the%20Python%20Package%0ADirectory%20%28PyPI%29%20software%20repository%20with%20the%20command%20pip%20install%20cbmap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17940v2&entry.124074799=Read"},
{"title": "LiLoc: Lifelong Localization using Adaptive Submap Joining and\n  Egocentric Factor Graph", "author": "Yixin Fang and Yanyan Li and Kun Qian and Federico Tombari and Yue Wang and Gim Hee Lee", "abstract": "  This paper proposes a versatile graph-based lifelong localization framework,\nLiLoc, which enhances its timeliness by maintaining a single central session\nwhile improves the accuracy through multi-modal factors between the central and\nsubsidiary sessions. First, an adaptive submap joining strategy is employed to\ngenerate prior submaps (keyframes and poses) for the central session, and to\nprovide priors for subsidiaries when constraints are needed for robust\nlocalization. Next, a coarse-to-fine pose initialization for subsidiary\nsessions is performed using vertical recognition and ICP refinement in the\nglobal coordinate frame. To elevate the accuracy of subsequent localization, we\npropose an egocentric factor graph (EFG) module that integrates the IMU\npreintegration, LiDAR odometry and scan match factors in a joint optimization\nmanner. Specifically, the scan match factors are constructed by a novel\npropagation model that efficiently distributes the prior constrains as edges to\nthe relevant prior pose nodes, weighted by noises based on keyframe\nregistration errors. Additionally, the framework supports flexible switching\nbetween two modes: relocalization (RLM) and incremental localization (ILM)\nbased on the proposed overlap-based mechanism to select or update the prior\nsubmaps from central session. The proposed LiLoc is tested on public and custom\ndatasets, demonstrating accurate localization performance against\nstate-of-the-art methods. Our codes will be publicly available on\nhttps://github.com/Yixin-F/LiLoc.\n", "link": "http://arxiv.org/abs/2409.10172v1", "date": "2024-09-16", "relevancy": 2.5442, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6817}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6289}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiLoc%3A%20Lifelong%20Localization%20using%20Adaptive%20Submap%20Joining%20and%0A%20%20Egocentric%20Factor%20Graph&body=Title%3A%20LiLoc%3A%20Lifelong%20Localization%20using%20Adaptive%20Submap%20Joining%20and%0A%20%20Egocentric%20Factor%20Graph%0AAuthor%3A%20Yixin%20Fang%20and%20Yanyan%20Li%20and%20Kun%20Qian%20and%20Federico%20Tombari%20and%20Yue%20Wang%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20versatile%20graph-based%20lifelong%20localization%20framework%2C%0ALiLoc%2C%20which%20enhances%20its%20timeliness%20by%20maintaining%20a%20single%20central%20session%0Awhile%20improves%20the%20accuracy%20through%20multi-modal%20factors%20between%20the%20central%20and%0Asubsidiary%20sessions.%20First%2C%20an%20adaptive%20submap%20joining%20strategy%20is%20employed%20to%0Agenerate%20prior%20submaps%20%28keyframes%20and%20poses%29%20for%20the%20central%20session%2C%20and%20to%0Aprovide%20priors%20for%20subsidiaries%20when%20constraints%20are%20needed%20for%20robust%0Alocalization.%20Next%2C%20a%20coarse-to-fine%20pose%20initialization%20for%20subsidiary%0Asessions%20is%20performed%20using%20vertical%20recognition%20and%20ICP%20refinement%20in%20the%0Aglobal%20coordinate%20frame.%20To%20elevate%20the%20accuracy%20of%20subsequent%20localization%2C%20we%0Apropose%20an%20egocentric%20factor%20graph%20%28EFG%29%20module%20that%20integrates%20the%20IMU%0Apreintegration%2C%20LiDAR%20odometry%20and%20scan%20match%20factors%20in%20a%20joint%20optimization%0Amanner.%20Specifically%2C%20the%20scan%20match%20factors%20are%20constructed%20by%20a%20novel%0Apropagation%20model%20that%20efficiently%20distributes%20the%20prior%20constrains%20as%20edges%20to%0Athe%20relevant%20prior%20pose%20nodes%2C%20weighted%20by%20noises%20based%20on%20keyframe%0Aregistration%20errors.%20Additionally%2C%20the%20framework%20supports%20flexible%20switching%0Abetween%20two%20modes%3A%20relocalization%20%28RLM%29%20and%20incremental%20localization%20%28ILM%29%0Abased%20on%20the%20proposed%20overlap-based%20mechanism%20to%20select%20or%20update%20the%20prior%0Asubmaps%20from%20central%20session.%20The%20proposed%20LiLoc%20is%20tested%20on%20public%20and%20custom%0Adatasets%2C%20demonstrating%20accurate%20localization%20performance%20against%0Astate-of-the-art%20methods.%20Our%20codes%20will%20be%20publicly%20available%20on%0Ahttps%3A//github.com/Yixin-F/LiLoc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiLoc%253A%2520Lifelong%2520Localization%2520using%2520Adaptive%2520Submap%2520Joining%2520and%250A%2520%2520Egocentric%2520Factor%2520Graph%26entry.906535625%3DYixin%2520Fang%2520and%2520Yanyan%2520Li%2520and%2520Kun%2520Qian%2520and%2520Federico%2520Tombari%2520and%2520Yue%2520Wang%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520versatile%2520graph-based%2520lifelong%2520localization%2520framework%252C%250ALiLoc%252C%2520which%2520enhances%2520its%2520timeliness%2520by%2520maintaining%2520a%2520single%2520central%2520session%250Awhile%2520improves%2520the%2520accuracy%2520through%2520multi-modal%2520factors%2520between%2520the%2520central%2520and%250Asubsidiary%2520sessions.%2520First%252C%2520an%2520adaptive%2520submap%2520joining%2520strategy%2520is%2520employed%2520to%250Agenerate%2520prior%2520submaps%2520%2528keyframes%2520and%2520poses%2529%2520for%2520the%2520central%2520session%252C%2520and%2520to%250Aprovide%2520priors%2520for%2520subsidiaries%2520when%2520constraints%2520are%2520needed%2520for%2520robust%250Alocalization.%2520Next%252C%2520a%2520coarse-to-fine%2520pose%2520initialization%2520for%2520subsidiary%250Asessions%2520is%2520performed%2520using%2520vertical%2520recognition%2520and%2520ICP%2520refinement%2520in%2520the%250Aglobal%2520coordinate%2520frame.%2520To%2520elevate%2520the%2520accuracy%2520of%2520subsequent%2520localization%252C%2520we%250Apropose%2520an%2520egocentric%2520factor%2520graph%2520%2528EFG%2529%2520module%2520that%2520integrates%2520the%2520IMU%250Apreintegration%252C%2520LiDAR%2520odometry%2520and%2520scan%2520match%2520factors%2520in%2520a%2520joint%2520optimization%250Amanner.%2520Specifically%252C%2520the%2520scan%2520match%2520factors%2520are%2520constructed%2520by%2520a%2520novel%250Apropagation%2520model%2520that%2520efficiently%2520distributes%2520the%2520prior%2520constrains%2520as%2520edges%2520to%250Athe%2520relevant%2520prior%2520pose%2520nodes%252C%2520weighted%2520by%2520noises%2520based%2520on%2520keyframe%250Aregistration%2520errors.%2520Additionally%252C%2520the%2520framework%2520supports%2520flexible%2520switching%250Abetween%2520two%2520modes%253A%2520relocalization%2520%2528RLM%2529%2520and%2520incremental%2520localization%2520%2528ILM%2529%250Abased%2520on%2520the%2520proposed%2520overlap-based%2520mechanism%2520to%2520select%2520or%2520update%2520the%2520prior%250Asubmaps%2520from%2520central%2520session.%2520The%2520proposed%2520LiLoc%2520is%2520tested%2520on%2520public%2520and%2520custom%250Adatasets%252C%2520demonstrating%2520accurate%2520localization%2520performance%2520against%250Astate-of-the-art%2520methods.%2520Our%2520codes%2520will%2520be%2520publicly%2520available%2520on%250Ahttps%253A//github.com/Yixin-F/LiLoc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiLoc%3A%20Lifelong%20Localization%20using%20Adaptive%20Submap%20Joining%20and%0A%20%20Egocentric%20Factor%20Graph&entry.906535625=Yixin%20Fang%20and%20Yanyan%20Li%20and%20Kun%20Qian%20and%20Federico%20Tombari%20and%20Yue%20Wang%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20This%20paper%20proposes%20a%20versatile%20graph-based%20lifelong%20localization%20framework%2C%0ALiLoc%2C%20which%20enhances%20its%20timeliness%20by%20maintaining%20a%20single%20central%20session%0Awhile%20improves%20the%20accuracy%20through%20multi-modal%20factors%20between%20the%20central%20and%0Asubsidiary%20sessions.%20First%2C%20an%20adaptive%20submap%20joining%20strategy%20is%20employed%20to%0Agenerate%20prior%20submaps%20%28keyframes%20and%20poses%29%20for%20the%20central%20session%2C%20and%20to%0Aprovide%20priors%20for%20subsidiaries%20when%20constraints%20are%20needed%20for%20robust%0Alocalization.%20Next%2C%20a%20coarse-to-fine%20pose%20initialization%20for%20subsidiary%0Asessions%20is%20performed%20using%20vertical%20recognition%20and%20ICP%20refinement%20in%20the%0Aglobal%20coordinate%20frame.%20To%20elevate%20the%20accuracy%20of%20subsequent%20localization%2C%20we%0Apropose%20an%20egocentric%20factor%20graph%20%28EFG%29%20module%20that%20integrates%20the%20IMU%0Apreintegration%2C%20LiDAR%20odometry%20and%20scan%20match%20factors%20in%20a%20joint%20optimization%0Amanner.%20Specifically%2C%20the%20scan%20match%20factors%20are%20constructed%20by%20a%20novel%0Apropagation%20model%20that%20efficiently%20distributes%20the%20prior%20constrains%20as%20edges%20to%0Athe%20relevant%20prior%20pose%20nodes%2C%20weighted%20by%20noises%20based%20on%20keyframe%0Aregistration%20errors.%20Additionally%2C%20the%20framework%20supports%20flexible%20switching%0Abetween%20two%20modes%3A%20relocalization%20%28RLM%29%20and%20incremental%20localization%20%28ILM%29%0Abased%20on%20the%20proposed%20overlap-based%20mechanism%20to%20select%20or%20update%20the%20prior%0Asubmaps%20from%20central%20session.%20The%20proposed%20LiLoc%20is%20tested%20on%20public%20and%20custom%0Adatasets%2C%20demonstrating%20accurate%20localization%20performance%20against%0Astate-of-the-art%20methods.%20Our%20codes%20will%20be%20publicly%20available%20on%0Ahttps%3A//github.com/Yixin-F/LiLoc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10172v1&entry.124074799=Read"},
{"title": "EasyControl: Transfer ControlNet to Video Diffusion for Controllable\n  Generation and Interpolation", "author": "Cong Wang and Jiaxi Gu and Panwen Hu and Haoyu Zhao and Yuanfan Guo and Jianhua Han and Hang Xu and Xiaodan Liang", "abstract": "  Following the advancements in text-guided image generation technology\nexemplified by Stable Diffusion, video generation is gaining increased\nattention in the academic community. However, relying solely on text guidance\nfor video generation has serious limitations, as videos contain much richer\ncontent than images, especially in terms of motion. This information can hardly\nbe adequately described with plain text. Fortunately, in computer vision,\nvarious visual representations can serve as additional control signals to guide\ngeneration. With the help of these signals, video generation can be controlled\nin finer detail, allowing for greater flexibility for different applications.\nIntegrating various controls, however, is nontrivial. In this paper, we propose\na universal framework called EasyControl. By propagating and injecting\ncondition features through condition adapters, our method enables users to\ncontrol video generation with a single condition map. With our framework,\nvarious conditions including raw pixels, depth, HED, etc., can be integrated\ninto different Unet-based pre-trained video diffusion models at a low practical\ncost. We conduct comprehensive experiments on public datasets, and both\nquantitative and qualitative results indicate that our method outperforms\nstate-of-the-art methods. EasyControl significantly improves various evaluation\nmetrics across multiple validation datasets compared to previous works.\nSpecifically, for the sketch-to-video generation task, EasyControl achieves an\nimprovement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared\nwith VideoComposer. For fidelity, our model demonstrates powerful image\nretention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared\nto other image-to-video models.\n", "link": "http://arxiv.org/abs/2408.13005v2", "date": "2024-09-16", "relevancy": 2.5252, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.654}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation&body=Title%3A%20EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation%0AAuthor%3A%20Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Haoyu%20Zhao%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Following%20the%20advancements%20in%20text-guided%20image%20generation%20technology%0Aexemplified%20by%20Stable%20Diffusion%2C%20video%20generation%20is%20gaining%20increased%0Aattention%20in%20the%20academic%20community.%20However%2C%20relying%20solely%20on%20text%20guidance%0Afor%20video%20generation%20has%20serious%20limitations%2C%20as%20videos%20contain%20much%20richer%0Acontent%20than%20images%2C%20especially%20in%20terms%20of%20motion.%20This%20information%20can%20hardly%0Abe%20adequately%20described%20with%20plain%20text.%20Fortunately%2C%20in%20computer%20vision%2C%0Avarious%20visual%20representations%20can%20serve%20as%20additional%20control%20signals%20to%20guide%0Ageneration.%20With%20the%20help%20of%20these%20signals%2C%20video%20generation%20can%20be%20controlled%0Ain%20finer%20detail%2C%20allowing%20for%20greater%20flexibility%20for%20different%20applications.%0AIntegrating%20various%20controls%2C%20however%2C%20is%20nontrivial.%20In%20this%20paper%2C%20we%20propose%0Aa%20universal%20framework%20called%20EasyControl.%20By%20propagating%20and%20injecting%0Acondition%20features%20through%20condition%20adapters%2C%20our%20method%20enables%20users%20to%0Acontrol%20video%20generation%20with%20a%20single%20condition%20map.%20With%20our%20framework%2C%0Avarious%20conditions%20including%20raw%20pixels%2C%20depth%2C%20HED%2C%20etc.%2C%20can%20be%20integrated%0Ainto%20different%20Unet-based%20pre-trained%20video%20diffusion%20models%20at%20a%20low%20practical%0Acost.%20We%20conduct%20comprehensive%20experiments%20on%20public%20datasets%2C%20and%20both%0Aquantitative%20and%20qualitative%20results%20indicate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods.%20EasyControl%20significantly%20improves%20various%20evaluation%0Ametrics%20across%20multiple%20validation%20datasets%20compared%20to%20previous%20works.%0ASpecifically%2C%20for%20the%20sketch-to-video%20generation%20task%2C%20EasyControl%20achieves%20an%0Aimprovement%20of%20152.0%20on%20FVD%20and%2019.9%20on%20IS%2C%20respectively%2C%20in%20UCF101%20compared%0Awith%20VideoComposer.%20For%20fidelity%2C%20our%20model%20demonstrates%20powerful%20image%0Aretention%20ability%2C%20resulting%20in%20high%20FVD%20and%20IS%20in%20UCF101%20and%20MSR-VTT%20compared%0Ato%20other%20image-to-video%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyControl%253A%2520Transfer%2520ControlNet%2520to%2520Video%2520Diffusion%2520for%2520Controllable%250A%2520%2520Generation%2520and%2520Interpolation%26entry.906535625%3DCong%2520Wang%2520and%2520Jiaxi%2520Gu%2520and%2520Panwen%2520Hu%2520and%2520Haoyu%2520Zhao%2520and%2520Yuanfan%2520Guo%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Following%2520the%2520advancements%2520in%2520text-guided%2520image%2520generation%2520technology%250Aexemplified%2520by%2520Stable%2520Diffusion%252C%2520video%2520generation%2520is%2520gaining%2520increased%250Aattention%2520in%2520the%2520academic%2520community.%2520However%252C%2520relying%2520solely%2520on%2520text%2520guidance%250Afor%2520video%2520generation%2520has%2520serious%2520limitations%252C%2520as%2520videos%2520contain%2520much%2520richer%250Acontent%2520than%2520images%252C%2520especially%2520in%2520terms%2520of%2520motion.%2520This%2520information%2520can%2520hardly%250Abe%2520adequately%2520described%2520with%2520plain%2520text.%2520Fortunately%252C%2520in%2520computer%2520vision%252C%250Avarious%2520visual%2520representations%2520can%2520serve%2520as%2520additional%2520control%2520signals%2520to%2520guide%250Ageneration.%2520With%2520the%2520help%2520of%2520these%2520signals%252C%2520video%2520generation%2520can%2520be%2520controlled%250Ain%2520finer%2520detail%252C%2520allowing%2520for%2520greater%2520flexibility%2520for%2520different%2520applications.%250AIntegrating%2520various%2520controls%252C%2520however%252C%2520is%2520nontrivial.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520universal%2520framework%2520called%2520EasyControl.%2520By%2520propagating%2520and%2520injecting%250Acondition%2520features%2520through%2520condition%2520adapters%252C%2520our%2520method%2520enables%2520users%2520to%250Acontrol%2520video%2520generation%2520with%2520a%2520single%2520condition%2520map.%2520With%2520our%2520framework%252C%250Avarious%2520conditions%2520including%2520raw%2520pixels%252C%2520depth%252C%2520HED%252C%2520etc.%252C%2520can%2520be%2520integrated%250Ainto%2520different%2520Unet-based%2520pre-trained%2520video%2520diffusion%2520models%2520at%2520a%2520low%2520practical%250Acost.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520public%2520datasets%252C%2520and%2520both%250Aquantitative%2520and%2520qualitative%2520results%2520indicate%2520that%2520our%2520method%2520outperforms%250Astate-of-the-art%2520methods.%2520EasyControl%2520significantly%2520improves%2520various%2520evaluation%250Ametrics%2520across%2520multiple%2520validation%2520datasets%2520compared%2520to%2520previous%2520works.%250ASpecifically%252C%2520for%2520the%2520sketch-to-video%2520generation%2520task%252C%2520EasyControl%2520achieves%2520an%250Aimprovement%2520of%2520152.0%2520on%2520FVD%2520and%252019.9%2520on%2520IS%252C%2520respectively%252C%2520in%2520UCF101%2520compared%250Awith%2520VideoComposer.%2520For%2520fidelity%252C%2520our%2520model%2520demonstrates%2520powerful%2520image%250Aretention%2520ability%252C%2520resulting%2520in%2520high%2520FVD%2520and%2520IS%2520in%2520UCF101%2520and%2520MSR-VTT%2520compared%250Ato%2520other%2520image-to-video%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyControl%3A%20Transfer%20ControlNet%20to%20Video%20Diffusion%20for%20Controllable%0A%20%20Generation%20and%20Interpolation&entry.906535625=Cong%20Wang%20and%20Jiaxi%20Gu%20and%20Panwen%20Hu%20and%20Haoyu%20Zhao%20and%20Yuanfan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Following%20the%20advancements%20in%20text-guided%20image%20generation%20technology%0Aexemplified%20by%20Stable%20Diffusion%2C%20video%20generation%20is%20gaining%20increased%0Aattention%20in%20the%20academic%20community.%20However%2C%20relying%20solely%20on%20text%20guidance%0Afor%20video%20generation%20has%20serious%20limitations%2C%20as%20videos%20contain%20much%20richer%0Acontent%20than%20images%2C%20especially%20in%20terms%20of%20motion.%20This%20information%20can%20hardly%0Abe%20adequately%20described%20with%20plain%20text.%20Fortunately%2C%20in%20computer%20vision%2C%0Avarious%20visual%20representations%20can%20serve%20as%20additional%20control%20signals%20to%20guide%0Ageneration.%20With%20the%20help%20of%20these%20signals%2C%20video%20generation%20can%20be%20controlled%0Ain%20finer%20detail%2C%20allowing%20for%20greater%20flexibility%20for%20different%20applications.%0AIntegrating%20various%20controls%2C%20however%2C%20is%20nontrivial.%20In%20this%20paper%2C%20we%20propose%0Aa%20universal%20framework%20called%20EasyControl.%20By%20propagating%20and%20injecting%0Acondition%20features%20through%20condition%20adapters%2C%20our%20method%20enables%20users%20to%0Acontrol%20video%20generation%20with%20a%20single%20condition%20map.%20With%20our%20framework%2C%0Avarious%20conditions%20including%20raw%20pixels%2C%20depth%2C%20HED%2C%20etc.%2C%20can%20be%20integrated%0Ainto%20different%20Unet-based%20pre-trained%20video%20diffusion%20models%20at%20a%20low%20practical%0Acost.%20We%20conduct%20comprehensive%20experiments%20on%20public%20datasets%2C%20and%20both%0Aquantitative%20and%20qualitative%20results%20indicate%20that%20our%20method%20outperforms%0Astate-of-the-art%20methods.%20EasyControl%20significantly%20improves%20various%20evaluation%0Ametrics%20across%20multiple%20validation%20datasets%20compared%20to%20previous%20works.%0ASpecifically%2C%20for%20the%20sketch-to-video%20generation%20task%2C%20EasyControl%20achieves%20an%0Aimprovement%20of%20152.0%20on%20FVD%20and%2019.9%20on%20IS%2C%20respectively%2C%20in%20UCF101%20compared%0Awith%20VideoComposer.%20For%20fidelity%2C%20our%20model%20demonstrates%20powerful%20image%0Aretention%20ability%2C%20resulting%20in%20high%20FVD%20and%20IS%20in%20UCF101%20and%20MSR-VTT%20compared%0Ato%20other%20image-to-video%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13005v2&entry.124074799=Read"},
{"title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?", "author": "Atsuki Yamaguchi and Aline Villavicencio and Nikolaos Aletras", "abstract": "  Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.\n", "link": "http://arxiv.org/abs/2406.11477v2", "date": "2024-09-16", "relevancy": 2.5153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Can%20We%20Effectively%20Expand%20the%20Vocabulary%20of%20LLMs%20with%200.01GB%20of%0A%20%20Target%20Language%20Text%3F&body=Title%3A%20How%20Can%20We%20Effectively%20Expand%20the%20Vocabulary%20of%20LLMs%20with%200.01GB%20of%0A%20%20Target%20Language%20Text%3F%0AAuthor%3A%20Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20many%0Alanguages%20beyond%20English.%20Yet%2C%20LLMs%20require%20more%20inference%20steps%20when%0Agenerating%20non-English%20text%20due%20to%20their%20reliance%20on%20English-centric%20tokenizers%0Aand%20vocabulary%2C%20resulting%20in%20higher%20usage%20costs%20to%20non-English%20speakers.%0AVocabulary%20expansion%20with%20target%20language%20tokens%20is%20a%20widely%20used%20cross-lingual%0Avocabulary%20adaptation%20approach%20to%20remedy%20this%20issue.%20Despite%20its%20effectiveness%0Ain%20inference%20speedup%2C%20previous%20work%20on%20vocabulary%20expansion%20has%20focused%20on%0Ahigh-resource%20settings%20assuming%20access%20to%20a%20substantial%20amount%20of%20target%0Alanguage%20data%20to%20effectively%20initialize%20the%20embeddings%20of%20the%20new%20tokens%20and%0Aadapt%20the%20LLM%20to%20the%20target%20language.%20However%2C%20vocabulary%20expansion%20in%0Alow-resource%20settings%20has%20yet%20to%20be%20explored.%20In%20this%20paper%2C%20we%20investigate%0Avocabulary%20expansion%20in%20low-resource%20settings%20by%20considering%20embedding%0Ainitialization%20methods%20and%20continual%20pre-training%20strategies.%20Through%20extensive%0Aexperiments%20across%20typologically%20diverse%20languages%2C%20tasks%20and%20models%2C%20we%0Aestablish%20a%20set%20of%20strategies%20to%20perform%20vocabulary%20expansion%20for%20faster%0Ainference%2C%20maintaining%20competitive%20downstream%20performance%20to%20baselines%20with%0Aonly%2030K%20sentences%20%28%24%5Csim%240.01GB%20text%20data%29%20from%20the%20target%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Can%2520We%2520Effectively%2520Expand%2520the%2520Vocabulary%2520of%2520LLMs%2520with%25200.01GB%2520of%250A%2520%2520Target%2520Language%2520Text%253F%26entry.906535625%3DAtsuki%2520Yamaguchi%2520and%2520Aline%2520Villavicencio%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520many%250Alanguages%2520beyond%2520English.%2520Yet%252C%2520LLMs%2520require%2520more%2520inference%2520steps%2520when%250Agenerating%2520non-English%2520text%2520due%2520to%2520their%2520reliance%2520on%2520English-centric%2520tokenizers%250Aand%2520vocabulary%252C%2520resulting%2520in%2520higher%2520usage%2520costs%2520to%2520non-English%2520speakers.%250AVocabulary%2520expansion%2520with%2520target%2520language%2520tokens%2520is%2520a%2520widely%2520used%2520cross-lingual%250Avocabulary%2520adaptation%2520approach%2520to%2520remedy%2520this%2520issue.%2520Despite%2520its%2520effectiveness%250Ain%2520inference%2520speedup%252C%2520previous%2520work%2520on%2520vocabulary%2520expansion%2520has%2520focused%2520on%250Ahigh-resource%2520settings%2520assuming%2520access%2520to%2520a%2520substantial%2520amount%2520of%2520target%250Alanguage%2520data%2520to%2520effectively%2520initialize%2520the%2520embeddings%2520of%2520the%2520new%2520tokens%2520and%250Aadapt%2520the%2520LLM%2520to%2520the%2520target%2520language.%2520However%252C%2520vocabulary%2520expansion%2520in%250Alow-resource%2520settings%2520has%2520yet%2520to%2520be%2520explored.%2520In%2520this%2520paper%252C%2520we%2520investigate%250Avocabulary%2520expansion%2520in%2520low-resource%2520settings%2520by%2520considering%2520embedding%250Ainitialization%2520methods%2520and%2520continual%2520pre-training%2520strategies.%2520Through%2520extensive%250Aexperiments%2520across%2520typologically%2520diverse%2520languages%252C%2520tasks%2520and%2520models%252C%2520we%250Aestablish%2520a%2520set%2520of%2520strategies%2520to%2520perform%2520vocabulary%2520expansion%2520for%2520faster%250Ainference%252C%2520maintaining%2520competitive%2520downstream%2520performance%2520to%2520baselines%2520with%250Aonly%252030K%2520sentences%2520%2528%2524%255Csim%25240.01GB%2520text%2520data%2529%2520from%2520the%2520target%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Can%20We%20Effectively%20Expand%20the%20Vocabulary%20of%20LLMs%20with%200.01GB%20of%0A%20%20Target%20Language%20Text%3F&entry.906535625=Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20many%0Alanguages%20beyond%20English.%20Yet%2C%20LLMs%20require%20more%20inference%20steps%20when%0Agenerating%20non-English%20text%20due%20to%20their%20reliance%20on%20English-centric%20tokenizers%0Aand%20vocabulary%2C%20resulting%20in%20higher%20usage%20costs%20to%20non-English%20speakers.%0AVocabulary%20expansion%20with%20target%20language%20tokens%20is%20a%20widely%20used%20cross-lingual%0Avocabulary%20adaptation%20approach%20to%20remedy%20this%20issue.%20Despite%20its%20effectiveness%0Ain%20inference%20speedup%2C%20previous%20work%20on%20vocabulary%20expansion%20has%20focused%20on%0Ahigh-resource%20settings%20assuming%20access%20to%20a%20substantial%20amount%20of%20target%0Alanguage%20data%20to%20effectively%20initialize%20the%20embeddings%20of%20the%20new%20tokens%20and%0Aadapt%20the%20LLM%20to%20the%20target%20language.%20However%2C%20vocabulary%20expansion%20in%0Alow-resource%20settings%20has%20yet%20to%20be%20explored.%20In%20this%20paper%2C%20we%20investigate%0Avocabulary%20expansion%20in%20low-resource%20settings%20by%20considering%20embedding%0Ainitialization%20methods%20and%20continual%20pre-training%20strategies.%20Through%20extensive%0Aexperiments%20across%20typologically%20diverse%20languages%2C%20tasks%20and%20models%2C%20we%0Aestablish%20a%20set%20of%20strategies%20to%20perform%20vocabulary%20expansion%20for%20faster%0Ainference%2C%20maintaining%20competitive%20downstream%20performance%20to%20baselines%20with%0Aonly%2030K%20sentences%20%28%24%5Csim%240.01GB%20text%20data%29%20from%20the%20target%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11477v2&entry.124074799=Read"},
{"title": "Contrastive Learning for Character Detection in Ancient Greek Papyri", "author": "Vedasri Nakka and Andreas Fischer and Rolf Ingold and Lars Vogtlin", "abstract": "  This thesis investigates the effectiveness of SimCLR, a contrastive learning\ntechnique, in Greek letter recognition, focusing on the impact of various\naugmentation techniques. We pretrain the SimCLR backbone using the Alpub\ndataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset\n(finetuning dataset) to compare SimCLR's performance against traditional\nbaseline models, which use cross-entropy and triplet loss functions.\nAdditionally, we explore the role of different data augmentation strategies,\nessential for the SimCLR training process. Methodologically, we examine three\nprimary approaches: (1) a baseline model using cross-entropy loss, (2) a\ntriplet embedding model with a classification layer, and (3) a SimCLR\npretrained model with a classification layer. Initially, we train the baseline,\ntriplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50\nnetworks with the ICDAR dataset. From these, the top four augmentations are\nselected using a statistical t-test. Pretraining of SimCLR is conducted on the\nAlpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss\nmodel undergoes a similar process, being pretrained on the top four\naugmentations before fine-tuning on ICDAR. Our experiments show that SimCLR\ndoes not outperform the baselines in letter recognition tasks. The baseline\nmodel with cross-entropy loss demonstrates better performance than both SimCLR\nand the triplet loss model. This study provides a detailed evaluation of\ncontrastive learning for letter recognition, highlighting SimCLR's limitations\nwhile emphasizing the strengths of traditional supervised learning models in\nthis task. We believe SimCLR's cropping strategies may cause a semantic shift\nin the input image, reducing training effectiveness despite the large\npretraining dataset. Our code is available at\nhttps://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.\n", "link": "http://arxiv.org/abs/2409.10156v1", "date": "2024-09-16", "relevancy": 2.5018, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20for%20Character%20Detection%20in%20Ancient%20Greek%20Papyri&body=Title%3A%20Contrastive%20Learning%20for%20Character%20Detection%20in%20Ancient%20Greek%20Papyri%0AAuthor%3A%20Vedasri%20Nakka%20and%20Andreas%20Fischer%20and%20Rolf%20Ingold%20and%20Lars%20Vogtlin%0AAbstract%3A%20%20%20This%20thesis%20investigates%20the%20effectiveness%20of%20SimCLR%2C%20a%20contrastive%20learning%0Atechnique%2C%20in%20Greek%20letter%20recognition%2C%20focusing%20on%20the%20impact%20of%20various%0Aaugmentation%20techniques.%20We%20pretrain%20the%20SimCLR%20backbone%20using%20the%20Alpub%0Adataset%20%28pretraining%20dataset%29%20and%20fine-tune%20it%20on%20a%20smaller%20ICDAR%20dataset%0A%28finetuning%20dataset%29%20to%20compare%20SimCLR%27s%20performance%20against%20traditional%0Abaseline%20models%2C%20which%20use%20cross-entropy%20and%20triplet%20loss%20functions.%0AAdditionally%2C%20we%20explore%20the%20role%20of%20different%20data%20augmentation%20strategies%2C%0Aessential%20for%20the%20SimCLR%20training%20process.%20Methodologically%2C%20we%20examine%20three%0Aprimary%20approaches%3A%20%281%29%20a%20baseline%20model%20using%20cross-entropy%20loss%2C%20%282%29%20a%0Atriplet%20embedding%20model%20with%20a%20classification%20layer%2C%20and%20%283%29%20a%20SimCLR%0Apretrained%20model%20with%20a%20classification%20layer.%20Initially%2C%20we%20train%20the%20baseline%2C%0Atriplet%2C%20and%20SimCLR%20models%20using%2093%20augmentations%20on%20ResNet-18%20and%20ResNet-50%0Anetworks%20with%20the%20ICDAR%20dataset.%20From%20these%2C%20the%20top%20four%20augmentations%20are%0Aselected%20using%20a%20statistical%20t-test.%20Pretraining%20of%20SimCLR%20is%20conducted%20on%20the%0AAlpub%20dataset%2C%20followed%20by%20fine-tuning%20on%20the%20ICDAR%20dataset.%20The%20triplet%20loss%0Amodel%20undergoes%20a%20similar%20process%2C%20being%20pretrained%20on%20the%20top%20four%0Aaugmentations%20before%20fine-tuning%20on%20ICDAR.%20Our%20experiments%20show%20that%20SimCLR%0Adoes%20not%20outperform%20the%20baselines%20in%20letter%20recognition%20tasks.%20The%20baseline%0Amodel%20with%20cross-entropy%20loss%20demonstrates%20better%20performance%20than%20both%20SimCLR%0Aand%20the%20triplet%20loss%20model.%20This%20study%20provides%20a%20detailed%20evaluation%20of%0Acontrastive%20learning%20for%20letter%20recognition%2C%20highlighting%20SimCLR%27s%20limitations%0Awhile%20emphasizing%20the%20strengths%20of%20traditional%20supervised%20learning%20models%20in%0Athis%20task.%20We%20believe%20SimCLR%27s%20cropping%20strategies%20may%20cause%20a%20semantic%20shift%0Ain%20the%20input%20image%2C%20reducing%20training%20effectiveness%20despite%20the%20large%0Apretraining%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520for%2520Character%2520Detection%2520in%2520Ancient%2520Greek%2520Papyri%26entry.906535625%3DVedasri%2520Nakka%2520and%2520Andreas%2520Fischer%2520and%2520Rolf%2520Ingold%2520and%2520Lars%2520Vogtlin%26entry.1292438233%3D%2520%2520This%2520thesis%2520investigates%2520the%2520effectiveness%2520of%2520SimCLR%252C%2520a%2520contrastive%2520learning%250Atechnique%252C%2520in%2520Greek%2520letter%2520recognition%252C%2520focusing%2520on%2520the%2520impact%2520of%2520various%250Aaugmentation%2520techniques.%2520We%2520pretrain%2520the%2520SimCLR%2520backbone%2520using%2520the%2520Alpub%250Adataset%2520%2528pretraining%2520dataset%2529%2520and%2520fine-tune%2520it%2520on%2520a%2520smaller%2520ICDAR%2520dataset%250A%2528finetuning%2520dataset%2529%2520to%2520compare%2520SimCLR%2527s%2520performance%2520against%2520traditional%250Abaseline%2520models%252C%2520which%2520use%2520cross-entropy%2520and%2520triplet%2520loss%2520functions.%250AAdditionally%252C%2520we%2520explore%2520the%2520role%2520of%2520different%2520data%2520augmentation%2520strategies%252C%250Aessential%2520for%2520the%2520SimCLR%2520training%2520process.%2520Methodologically%252C%2520we%2520examine%2520three%250Aprimary%2520approaches%253A%2520%25281%2529%2520a%2520baseline%2520model%2520using%2520cross-entropy%2520loss%252C%2520%25282%2529%2520a%250Atriplet%2520embedding%2520model%2520with%2520a%2520classification%2520layer%252C%2520and%2520%25283%2529%2520a%2520SimCLR%250Apretrained%2520model%2520with%2520a%2520classification%2520layer.%2520Initially%252C%2520we%2520train%2520the%2520baseline%252C%250Atriplet%252C%2520and%2520SimCLR%2520models%2520using%252093%2520augmentations%2520on%2520ResNet-18%2520and%2520ResNet-50%250Anetworks%2520with%2520the%2520ICDAR%2520dataset.%2520From%2520these%252C%2520the%2520top%2520four%2520augmentations%2520are%250Aselected%2520using%2520a%2520statistical%2520t-test.%2520Pretraining%2520of%2520SimCLR%2520is%2520conducted%2520on%2520the%250AAlpub%2520dataset%252C%2520followed%2520by%2520fine-tuning%2520on%2520the%2520ICDAR%2520dataset.%2520The%2520triplet%2520loss%250Amodel%2520undergoes%2520a%2520similar%2520process%252C%2520being%2520pretrained%2520on%2520the%2520top%2520four%250Aaugmentations%2520before%2520fine-tuning%2520on%2520ICDAR.%2520Our%2520experiments%2520show%2520that%2520SimCLR%250Adoes%2520not%2520outperform%2520the%2520baselines%2520in%2520letter%2520recognition%2520tasks.%2520The%2520baseline%250Amodel%2520with%2520cross-entropy%2520loss%2520demonstrates%2520better%2520performance%2520than%2520both%2520SimCLR%250Aand%2520the%2520triplet%2520loss%2520model.%2520This%2520study%2520provides%2520a%2520detailed%2520evaluation%2520of%250Acontrastive%2520learning%2520for%2520letter%2520recognition%252C%2520highlighting%2520SimCLR%2527s%2520limitations%250Awhile%2520emphasizing%2520the%2520strengths%2520of%2520traditional%2520supervised%2520learning%2520models%2520in%250Athis%2520task.%2520We%2520believe%2520SimCLR%2527s%2520cropping%2520strategies%2520may%2520cause%2520a%2520semantic%2520shift%250Ain%2520the%2520input%2520image%252C%2520reducing%2520training%2520effectiveness%2520despite%2520the%2520large%250Apretraining%2520dataset.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20for%20Character%20Detection%20in%20Ancient%20Greek%20Papyri&entry.906535625=Vedasri%20Nakka%20and%20Andreas%20Fischer%20and%20Rolf%20Ingold%20and%20Lars%20Vogtlin&entry.1292438233=%20%20This%20thesis%20investigates%20the%20effectiveness%20of%20SimCLR%2C%20a%20contrastive%20learning%0Atechnique%2C%20in%20Greek%20letter%20recognition%2C%20focusing%20on%20the%20impact%20of%20various%0Aaugmentation%20techniques.%20We%20pretrain%20the%20SimCLR%20backbone%20using%20the%20Alpub%0Adataset%20%28pretraining%20dataset%29%20and%20fine-tune%20it%20on%20a%20smaller%20ICDAR%20dataset%0A%28finetuning%20dataset%29%20to%20compare%20SimCLR%27s%20performance%20against%20traditional%0Abaseline%20models%2C%20which%20use%20cross-entropy%20and%20triplet%20loss%20functions.%0AAdditionally%2C%20we%20explore%20the%20role%20of%20different%20data%20augmentation%20strategies%2C%0Aessential%20for%20the%20SimCLR%20training%20process.%20Methodologically%2C%20we%20examine%20three%0Aprimary%20approaches%3A%20%281%29%20a%20baseline%20model%20using%20cross-entropy%20loss%2C%20%282%29%20a%0Atriplet%20embedding%20model%20with%20a%20classification%20layer%2C%20and%20%283%29%20a%20SimCLR%0Apretrained%20model%20with%20a%20classification%20layer.%20Initially%2C%20we%20train%20the%20baseline%2C%0Atriplet%2C%20and%20SimCLR%20models%20using%2093%20augmentations%20on%20ResNet-18%20and%20ResNet-50%0Anetworks%20with%20the%20ICDAR%20dataset.%20From%20these%2C%20the%20top%20four%20augmentations%20are%0Aselected%20using%20a%20statistical%20t-test.%20Pretraining%20of%20SimCLR%20is%20conducted%20on%20the%0AAlpub%20dataset%2C%20followed%20by%20fine-tuning%20on%20the%20ICDAR%20dataset.%20The%20triplet%20loss%0Amodel%20undergoes%20a%20similar%20process%2C%20being%20pretrained%20on%20the%20top%20four%0Aaugmentations%20before%20fine-tuning%20on%20ICDAR.%20Our%20experiments%20show%20that%20SimCLR%0Adoes%20not%20outperform%20the%20baselines%20in%20letter%20recognition%20tasks.%20The%20baseline%0Amodel%20with%20cross-entropy%20loss%20demonstrates%20better%20performance%20than%20both%20SimCLR%0Aand%20the%20triplet%20loss%20model.%20This%20study%20provides%20a%20detailed%20evaluation%20of%0Acontrastive%20learning%20for%20letter%20recognition%2C%20highlighting%20SimCLR%27s%20limitations%0Awhile%20emphasizing%20the%20strengths%20of%20traditional%20supervised%20learning%20models%20in%0Athis%20task.%20We%20believe%20SimCLR%27s%20cropping%20strategies%20may%20cause%20a%20semantic%20shift%0Ain%20the%20input%20image%2C%20reducing%20training%20effectiveness%20despite%20the%20large%0Apretraining%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10156v1&entry.124074799=Read"},
{"title": "HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping\n  Using Vision-Language Models", "author": "Vineet Bhat and Prashanth Krishnamurthy and Ramesh Karri and Farshad Khorrami", "abstract": "  Robots interacting with humans through natural language can unlock numerous\napplications such as Referring Grasp Synthesis (RGS). Given a text query, RGS\ndetermines a stable grasp pose to manipulate the referred object in the robot's\nworkspace. RGS comprises two steps: visual grounding and grasp pose estimation.\nRecent studies leverage powerful Vision-Language Models (VLMs) for visually\ngrounding free-flowing natural language in real-world robotic execution.\nHowever, comparisons in complex, cluttered environments with multiple instances\nof the same object are lacking. This paper introduces HiFi-CS, featuring\nhierarchical application of Featurewise Linear Modulation (FiLM) to fuse image\nand text embeddings, enhancing visual grounding for complex attribute rich text\nqueries encountered in robotic grasping. Visual grounding associates an object\nin 2D/3D space with natural language input and is studied in two scenarios:\nClosed and Open Vocabulary. HiFi-CS features a lightweight decoder combined\nwith a frozen VLM and outperforms competitive baselines in closed vocabulary\nsettings while being 100x smaller in size. Our model can effectively guide\nopen-set object detectors like GroundedSAM to enhance open-vocabulary\nperformance. We validate our approach through real-world RGS experiments using\na 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop\nscenes. We include our codebase in the supplementary material.\n", "link": "http://arxiv.org/abs/2409.10419v1", "date": "2024-09-16", "relevancy": 2.4979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFi-CS%3A%20Towards%20Open%20Vocabulary%20Visual%20Grounding%20For%20Robotic%20Grasping%0A%20%20Using%20Vision-Language%20Models&body=Title%3A%20HiFi-CS%3A%20Towards%20Open%20Vocabulary%20Visual%20Grounding%20For%20Robotic%20Grasping%0A%20%20Using%20Vision-Language%20Models%0AAuthor%3A%20Vineet%20Bhat%20and%20Prashanth%20Krishnamurthy%20and%20Ramesh%20Karri%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20Robots%20interacting%20with%20humans%20through%20natural%20language%20can%20unlock%20numerous%0Aapplications%20such%20as%20Referring%20Grasp%20Synthesis%20%28RGS%29.%20Given%20a%20text%20query%2C%20RGS%0Adetermines%20a%20stable%20grasp%20pose%20to%20manipulate%20the%20referred%20object%20in%20the%20robot%27s%0Aworkspace.%20RGS%20comprises%20two%20steps%3A%20visual%20grounding%20and%20grasp%20pose%20estimation.%0ARecent%20studies%20leverage%20powerful%20Vision-Language%20Models%20%28VLMs%29%20for%20visually%0Agrounding%20free-flowing%20natural%20language%20in%20real-world%20robotic%20execution.%0AHowever%2C%20comparisons%20in%20complex%2C%20cluttered%20environments%20with%20multiple%20instances%0Aof%20the%20same%20object%20are%20lacking.%20This%20paper%20introduces%20HiFi-CS%2C%20featuring%0Ahierarchical%20application%20of%20Featurewise%20Linear%20Modulation%20%28FiLM%29%20to%20fuse%20image%0Aand%20text%20embeddings%2C%20enhancing%20visual%20grounding%20for%20complex%20attribute%20rich%20text%0Aqueries%20encountered%20in%20robotic%20grasping.%20Visual%20grounding%20associates%20an%20object%0Ain%202D/3D%20space%20with%20natural%20language%20input%20and%20is%20studied%20in%20two%20scenarios%3A%0AClosed%20and%20Open%20Vocabulary.%20HiFi-CS%20features%20a%20lightweight%20decoder%20combined%0Awith%20a%20frozen%20VLM%20and%20outperforms%20competitive%20baselines%20in%20closed%20vocabulary%0Asettings%20while%20being%20100x%20smaller%20in%20size.%20Our%20model%20can%20effectively%20guide%0Aopen-set%20object%20detectors%20like%20GroundedSAM%20to%20enhance%20open-vocabulary%0Aperformance.%20We%20validate%20our%20approach%20through%20real-world%20RGS%20experiments%20using%0Aa%207-DOF%20robotic%20arm%2C%20achieving%2090.33%5C%25%20visual%20grounding%20accuracy%20in%2015%20tabletop%0Ascenes.%20We%20include%20our%20codebase%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFi-CS%253A%2520Towards%2520Open%2520Vocabulary%2520Visual%2520Grounding%2520For%2520Robotic%2520Grasping%250A%2520%2520Using%2520Vision-Language%2520Models%26entry.906535625%3DVineet%2520Bhat%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Ramesh%2520Karri%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520Robots%2520interacting%2520with%2520humans%2520through%2520natural%2520language%2520can%2520unlock%2520numerous%250Aapplications%2520such%2520as%2520Referring%2520Grasp%2520Synthesis%2520%2528RGS%2529.%2520Given%2520a%2520text%2520query%252C%2520RGS%250Adetermines%2520a%2520stable%2520grasp%2520pose%2520to%2520manipulate%2520the%2520referred%2520object%2520in%2520the%2520robot%2527s%250Aworkspace.%2520RGS%2520comprises%2520two%2520steps%253A%2520visual%2520grounding%2520and%2520grasp%2520pose%2520estimation.%250ARecent%2520studies%2520leverage%2520powerful%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520visually%250Agrounding%2520free-flowing%2520natural%2520language%2520in%2520real-world%2520robotic%2520execution.%250AHowever%252C%2520comparisons%2520in%2520complex%252C%2520cluttered%2520environments%2520with%2520multiple%2520instances%250Aof%2520the%2520same%2520object%2520are%2520lacking.%2520This%2520paper%2520introduces%2520HiFi-CS%252C%2520featuring%250Ahierarchical%2520application%2520of%2520Featurewise%2520Linear%2520Modulation%2520%2528FiLM%2529%2520to%2520fuse%2520image%250Aand%2520text%2520embeddings%252C%2520enhancing%2520visual%2520grounding%2520for%2520complex%2520attribute%2520rich%2520text%250Aqueries%2520encountered%2520in%2520robotic%2520grasping.%2520Visual%2520grounding%2520associates%2520an%2520object%250Ain%25202D/3D%2520space%2520with%2520natural%2520language%2520input%2520and%2520is%2520studied%2520in%2520two%2520scenarios%253A%250AClosed%2520and%2520Open%2520Vocabulary.%2520HiFi-CS%2520features%2520a%2520lightweight%2520decoder%2520combined%250Awith%2520a%2520frozen%2520VLM%2520and%2520outperforms%2520competitive%2520baselines%2520in%2520closed%2520vocabulary%250Asettings%2520while%2520being%2520100x%2520smaller%2520in%2520size.%2520Our%2520model%2520can%2520effectively%2520guide%250Aopen-set%2520object%2520detectors%2520like%2520GroundedSAM%2520to%2520enhance%2520open-vocabulary%250Aperformance.%2520We%2520validate%2520our%2520approach%2520through%2520real-world%2520RGS%2520experiments%2520using%250Aa%25207-DOF%2520robotic%2520arm%252C%2520achieving%252090.33%255C%2525%2520visual%2520grounding%2520accuracy%2520in%252015%2520tabletop%250Ascenes.%2520We%2520include%2520our%2520codebase%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-CS%3A%20Towards%20Open%20Vocabulary%20Visual%20Grounding%20For%20Robotic%20Grasping%0A%20%20Using%20Vision-Language%20Models&entry.906535625=Vineet%20Bhat%20and%20Prashanth%20Krishnamurthy%20and%20Ramesh%20Karri%20and%20Farshad%20Khorrami&entry.1292438233=%20%20Robots%20interacting%20with%20humans%20through%20natural%20language%20can%20unlock%20numerous%0Aapplications%20such%20as%20Referring%20Grasp%20Synthesis%20%28RGS%29.%20Given%20a%20text%20query%2C%20RGS%0Adetermines%20a%20stable%20grasp%20pose%20to%20manipulate%20the%20referred%20object%20in%20the%20robot%27s%0Aworkspace.%20RGS%20comprises%20two%20steps%3A%20visual%20grounding%20and%20grasp%20pose%20estimation.%0ARecent%20studies%20leverage%20powerful%20Vision-Language%20Models%20%28VLMs%29%20for%20visually%0Agrounding%20free-flowing%20natural%20language%20in%20real-world%20robotic%20execution.%0AHowever%2C%20comparisons%20in%20complex%2C%20cluttered%20environments%20with%20multiple%20instances%0Aof%20the%20same%20object%20are%20lacking.%20This%20paper%20introduces%20HiFi-CS%2C%20featuring%0Ahierarchical%20application%20of%20Featurewise%20Linear%20Modulation%20%28FiLM%29%20to%20fuse%20image%0Aand%20text%20embeddings%2C%20enhancing%20visual%20grounding%20for%20complex%20attribute%20rich%20text%0Aqueries%20encountered%20in%20robotic%20grasping.%20Visual%20grounding%20associates%20an%20object%0Ain%202D/3D%20space%20with%20natural%20language%20input%20and%20is%20studied%20in%20two%20scenarios%3A%0AClosed%20and%20Open%20Vocabulary.%20HiFi-CS%20features%20a%20lightweight%20decoder%20combined%0Awith%20a%20frozen%20VLM%20and%20outperforms%20competitive%20baselines%20in%20closed%20vocabulary%0Asettings%20while%20being%20100x%20smaller%20in%20size.%20Our%20model%20can%20effectively%20guide%0Aopen-set%20object%20detectors%20like%20GroundedSAM%20to%20enhance%20open-vocabulary%0Aperformance.%20We%20validate%20our%20approach%20through%20real-world%20RGS%20experiments%20using%0Aa%207-DOF%20robotic%20arm%2C%20achieving%2090.33%5C%25%20visual%20grounding%20accuracy%20in%2015%20tabletop%0Ascenes.%20We%20include%20our%20codebase%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10419v1&entry.124074799=Read"},
{"title": "Signed Graph Autoencoder for Explainable and Polarization-Aware Network\n  Embeddings", "author": "Nikolaos Nakis and Chrysoula Kosma and Giannis Nikolentzos and Michalis Chatzianastasis and Iakovos Evdaimon and Michalis Vazirgiannis", "abstract": "  Autoencoders based on Graph Neural Networks (GNNs) have garnered significant\nattention in recent years for their ability to extract informative latent\nrepresentations, characterizing the structure of complex topologies, such as\ngraphs. Despite the prevalence of Graph Autoencoders, there has been limited\nfocus on developing and evaluating explainable neural-based graph generative\nmodels specifically designed for signed networks. To address this gap, we\npropose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE\nextracts node-level representations that express node memberships over distinct\nextreme profiles, referred to as archetypes, within the network. This is\nachieved by projecting the graph onto a learned polytope, which governs its\npolarization. The framework employs a recently proposed likelihood for\nanalyzing signed networks based on the Skellam distribution, combined with\nrelational archetypal analysis and GNNs. Our experimental evaluation\ndemonstrates the SGAAEs' capability to successfully infer node memberships over\nthe different underlying latent structures while extracting competing\ncommunities formed through the participation of the opposing views in the\nnetwork. Additionally, we introduce the 2-level network polarization problem\nand show how SGAAE is able to characterize such a setting. The proposed model\nachieves high performance in different tasks of signed link prediction across\nfour real-world datasets, outperforming several baseline models.\n", "link": "http://arxiv.org/abs/2409.10452v1", "date": "2024-09-16", "relevancy": 2.4849, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5091}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signed%20Graph%20Autoencoder%20for%20Explainable%20and%20Polarization-Aware%20Network%0A%20%20Embeddings&body=Title%3A%20Signed%20Graph%20Autoencoder%20for%20Explainable%20and%20Polarization-Aware%20Network%0A%20%20Embeddings%0AAuthor%3A%20Nikolaos%20Nakis%20and%20Chrysoula%20Kosma%20and%20Giannis%20Nikolentzos%20and%20Michalis%20Chatzianastasis%20and%20Iakovos%20Evdaimon%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Autoencoders%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20garnered%20significant%0Aattention%20in%20recent%20years%20for%20their%20ability%20to%20extract%20informative%20latent%0Arepresentations%2C%20characterizing%20the%20structure%20of%20complex%20topologies%2C%20such%20as%0Agraphs.%20Despite%20the%20prevalence%20of%20Graph%20Autoencoders%2C%20there%20has%20been%20limited%0Afocus%20on%20developing%20and%20evaluating%20explainable%20neural-based%20graph%20generative%0Amodels%20specifically%20designed%20for%20signed%20networks.%20To%20address%20this%20gap%2C%20we%0Apropose%20the%20Signed%20Graph%20Archetypal%20Autoencoder%20%28SGAAE%29%20framework.%20SGAAE%0Aextracts%20node-level%20representations%20that%20express%20node%20memberships%20over%20distinct%0Aextreme%20profiles%2C%20referred%20to%20as%20archetypes%2C%20within%20the%20network.%20This%20is%0Aachieved%20by%20projecting%20the%20graph%20onto%20a%20learned%20polytope%2C%20which%20governs%20its%0Apolarization.%20The%20framework%20employs%20a%20recently%20proposed%20likelihood%20for%0Aanalyzing%20signed%20networks%20based%20on%20the%20Skellam%20distribution%2C%20combined%20with%0Arelational%20archetypal%20analysis%20and%20GNNs.%20Our%20experimental%20evaluation%0Ademonstrates%20the%20SGAAEs%27%20capability%20to%20successfully%20infer%20node%20memberships%20over%0Athe%20different%20underlying%20latent%20structures%20while%20extracting%20competing%0Acommunities%20formed%20through%20the%20participation%20of%20the%20opposing%20views%20in%20the%0Anetwork.%20Additionally%2C%20we%20introduce%20the%202-level%20network%20polarization%20problem%0Aand%20show%20how%20SGAAE%20is%20able%20to%20characterize%20such%20a%20setting.%20The%20proposed%20model%0Aachieves%20high%20performance%20in%20different%20tasks%20of%20signed%20link%20prediction%20across%0Afour%20real-world%20datasets%2C%20outperforming%20several%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigned%2520Graph%2520Autoencoder%2520for%2520Explainable%2520and%2520Polarization-Aware%2520Network%250A%2520%2520Embeddings%26entry.906535625%3DNikolaos%2520Nakis%2520and%2520Chrysoula%2520Kosma%2520and%2520Giannis%2520Nikolentzos%2520and%2520Michalis%2520Chatzianastasis%2520and%2520Iakovos%2520Evdaimon%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Autoencoders%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520garnered%2520significant%250Aattention%2520in%2520recent%2520years%2520for%2520their%2520ability%2520to%2520extract%2520informative%2520latent%250Arepresentations%252C%2520characterizing%2520the%2520structure%2520of%2520complex%2520topologies%252C%2520such%2520as%250Agraphs.%2520Despite%2520the%2520prevalence%2520of%2520Graph%2520Autoencoders%252C%2520there%2520has%2520been%2520limited%250Afocus%2520on%2520developing%2520and%2520evaluating%2520explainable%2520neural-based%2520graph%2520generative%250Amodels%2520specifically%2520designed%2520for%2520signed%2520networks.%2520To%2520address%2520this%2520gap%252C%2520we%250Apropose%2520the%2520Signed%2520Graph%2520Archetypal%2520Autoencoder%2520%2528SGAAE%2529%2520framework.%2520SGAAE%250Aextracts%2520node-level%2520representations%2520that%2520express%2520node%2520memberships%2520over%2520distinct%250Aextreme%2520profiles%252C%2520referred%2520to%2520as%2520archetypes%252C%2520within%2520the%2520network.%2520This%2520is%250Aachieved%2520by%2520projecting%2520the%2520graph%2520onto%2520a%2520learned%2520polytope%252C%2520which%2520governs%2520its%250Apolarization.%2520The%2520framework%2520employs%2520a%2520recently%2520proposed%2520likelihood%2520for%250Aanalyzing%2520signed%2520networks%2520based%2520on%2520the%2520Skellam%2520distribution%252C%2520combined%2520with%250Arelational%2520archetypal%2520analysis%2520and%2520GNNs.%2520Our%2520experimental%2520evaluation%250Ademonstrates%2520the%2520SGAAEs%2527%2520capability%2520to%2520successfully%2520infer%2520node%2520memberships%2520over%250Athe%2520different%2520underlying%2520latent%2520structures%2520while%2520extracting%2520competing%250Acommunities%2520formed%2520through%2520the%2520participation%2520of%2520the%2520opposing%2520views%2520in%2520the%250Anetwork.%2520Additionally%252C%2520we%2520introduce%2520the%25202-level%2520network%2520polarization%2520problem%250Aand%2520show%2520how%2520SGAAE%2520is%2520able%2520to%2520characterize%2520such%2520a%2520setting.%2520The%2520proposed%2520model%250Aachieves%2520high%2520performance%2520in%2520different%2520tasks%2520of%2520signed%2520link%2520prediction%2520across%250Afour%2520real-world%2520datasets%252C%2520outperforming%2520several%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signed%20Graph%20Autoencoder%20for%20Explainable%20and%20Polarization-Aware%20Network%0A%20%20Embeddings&entry.906535625=Nikolaos%20Nakis%20and%20Chrysoula%20Kosma%20and%20Giannis%20Nikolentzos%20and%20Michalis%20Chatzianastasis%20and%20Iakovos%20Evdaimon%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Autoencoders%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20garnered%20significant%0Aattention%20in%20recent%20years%20for%20their%20ability%20to%20extract%20informative%20latent%0Arepresentations%2C%20characterizing%20the%20structure%20of%20complex%20topologies%2C%20such%20as%0Agraphs.%20Despite%20the%20prevalence%20of%20Graph%20Autoencoders%2C%20there%20has%20been%20limited%0Afocus%20on%20developing%20and%20evaluating%20explainable%20neural-based%20graph%20generative%0Amodels%20specifically%20designed%20for%20signed%20networks.%20To%20address%20this%20gap%2C%20we%0Apropose%20the%20Signed%20Graph%20Archetypal%20Autoencoder%20%28SGAAE%29%20framework.%20SGAAE%0Aextracts%20node-level%20representations%20that%20express%20node%20memberships%20over%20distinct%0Aextreme%20profiles%2C%20referred%20to%20as%20archetypes%2C%20within%20the%20network.%20This%20is%0Aachieved%20by%20projecting%20the%20graph%20onto%20a%20learned%20polytope%2C%20which%20governs%20its%0Apolarization.%20The%20framework%20employs%20a%20recently%20proposed%20likelihood%20for%0Aanalyzing%20signed%20networks%20based%20on%20the%20Skellam%20distribution%2C%20combined%20with%0Arelational%20archetypal%20analysis%20and%20GNNs.%20Our%20experimental%20evaluation%0Ademonstrates%20the%20SGAAEs%27%20capability%20to%20successfully%20infer%20node%20memberships%20over%0Athe%20different%20underlying%20latent%20structures%20while%20extracting%20competing%0Acommunities%20formed%20through%20the%20participation%20of%20the%20opposing%20views%20in%20the%0Anetwork.%20Additionally%2C%20we%20introduce%20the%202-level%20network%20polarization%20problem%0Aand%20show%20how%20SGAAE%20is%20able%20to%20characterize%20such%20a%20setting.%20The%20proposed%20model%0Aachieves%20high%20performance%20in%20different%20tasks%20of%20signed%20link%20prediction%20across%0Afour%20real-world%20datasets%2C%20outperforming%20several%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10452v1&entry.124074799=Read"},
{"title": "ExelMap: Explainable Element-based HD-Map Change Detection and Update", "author": "Lena Wild and Ludvig Ericson and Rafael Valencia and Patric Jensfelt", "abstract": "  Acquisition and maintenance are central problems in deploying high-definition\n(HD) maps for autonomous driving, with two lines of research prevalent in\ncurrent literature: Online HD map generation and HD map change detection.\nHowever, the generated map's quality is currently insufficient for safe\ndeployment, and many change detection approaches fail to precisely localize and\nextract the changed map elements, hence lacking explainability and hindering a\npotential fleet-based cooperative HD map update. In this paper, we propose the\nnovel task of explainable element-based HD map change detection and update. In\nextending recent approaches that use online mapping techniques informed with an\noutdated map prior for HD map updating, we present ExelMap, an explainable\nelement-based map updating strategy that specifically identifies changed map\nelements. In this context, we discuss how currently used metrics fail to\ncapture change detection performance, while allowing for unfair comparison\nbetween prior-less and prior-informed map generation methods. Finally, we\npresent an experimental study on real-world changes related to pedestrian\ncrossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,\nthis is the first comprehensive problem investigation of real-world end-to-end\nelement-based HD map change detection and update, and ExelMap the first\nproposed solution.\n", "link": "http://arxiv.org/abs/2409.10178v1", "date": "2024-09-16", "relevancy": 2.4762, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExelMap%3A%20Explainable%20Element-based%20HD-Map%20Change%20Detection%20and%20Update&body=Title%3A%20ExelMap%3A%20Explainable%20Element-based%20HD-Map%20Change%20Detection%20and%20Update%0AAuthor%3A%20Lena%20Wild%20and%20Ludvig%20Ericson%20and%20Rafael%20Valencia%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20Acquisition%20and%20maintenance%20are%20central%20problems%20in%20deploying%20high-definition%0A%28HD%29%20maps%20for%20autonomous%20driving%2C%20with%20two%20lines%20of%20research%20prevalent%20in%0Acurrent%20literature%3A%20Online%20HD%20map%20generation%20and%20HD%20map%20change%20detection.%0AHowever%2C%20the%20generated%20map%27s%20quality%20is%20currently%20insufficient%20for%20safe%0Adeployment%2C%20and%20many%20change%20detection%20approaches%20fail%20to%20precisely%20localize%20and%0Aextract%20the%20changed%20map%20elements%2C%20hence%20lacking%20explainability%20and%20hindering%20a%0Apotential%20fleet-based%20cooperative%20HD%20map%20update.%20In%20this%20paper%2C%20we%20propose%20the%0Anovel%20task%20of%20explainable%20element-based%20HD%20map%20change%20detection%20and%20update.%20In%0Aextending%20recent%20approaches%20that%20use%20online%20mapping%20techniques%20informed%20with%20an%0Aoutdated%20map%20prior%20for%20HD%20map%20updating%2C%20we%20present%20ExelMap%2C%20an%20explainable%0Aelement-based%20map%20updating%20strategy%20that%20specifically%20identifies%20changed%20map%0Aelements.%20In%20this%20context%2C%20we%20discuss%20how%20currently%20used%20metrics%20fail%20to%0Acapture%20change%20detection%20performance%2C%20while%20allowing%20for%20unfair%20comparison%0Abetween%20prior-less%20and%20prior-informed%20map%20generation%20methods.%20Finally%2C%20we%0Apresent%20an%20experimental%20study%20on%20real-world%20changes%20related%20to%20pedestrian%0Acrossings%20of%20the%20Argoverse%202%20Map%20Change%20Dataset.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20comprehensive%20problem%20investigation%20of%20real-world%20end-to-end%0Aelement-based%20HD%20map%20change%20detection%20and%20update%2C%20and%20ExelMap%20the%20first%0Aproposed%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExelMap%253A%2520Explainable%2520Element-based%2520HD-Map%2520Change%2520Detection%2520and%2520Update%26entry.906535625%3DLena%2520Wild%2520and%2520Ludvig%2520Ericson%2520and%2520Rafael%2520Valencia%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520Acquisition%2520and%2520maintenance%2520are%2520central%2520problems%2520in%2520deploying%2520high-definition%250A%2528HD%2529%2520maps%2520for%2520autonomous%2520driving%252C%2520with%2520two%2520lines%2520of%2520research%2520prevalent%2520in%250Acurrent%2520literature%253A%2520Online%2520HD%2520map%2520generation%2520and%2520HD%2520map%2520change%2520detection.%250AHowever%252C%2520the%2520generated%2520map%2527s%2520quality%2520is%2520currently%2520insufficient%2520for%2520safe%250Adeployment%252C%2520and%2520many%2520change%2520detection%2520approaches%2520fail%2520to%2520precisely%2520localize%2520and%250Aextract%2520the%2520changed%2520map%2520elements%252C%2520hence%2520lacking%2520explainability%2520and%2520hindering%2520a%250Apotential%2520fleet-based%2520cooperative%2520HD%2520map%2520update.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250Anovel%2520task%2520of%2520explainable%2520element-based%2520HD%2520map%2520change%2520detection%2520and%2520update.%2520In%250Aextending%2520recent%2520approaches%2520that%2520use%2520online%2520mapping%2520techniques%2520informed%2520with%2520an%250Aoutdated%2520map%2520prior%2520for%2520HD%2520map%2520updating%252C%2520we%2520present%2520ExelMap%252C%2520an%2520explainable%250Aelement-based%2520map%2520updating%2520strategy%2520that%2520specifically%2520identifies%2520changed%2520map%250Aelements.%2520In%2520this%2520context%252C%2520we%2520discuss%2520how%2520currently%2520used%2520metrics%2520fail%2520to%250Acapture%2520change%2520detection%2520performance%252C%2520while%2520allowing%2520for%2520unfair%2520comparison%250Abetween%2520prior-less%2520and%2520prior-informed%2520map%2520generation%2520methods.%2520Finally%252C%2520we%250Apresent%2520an%2520experimental%2520study%2520on%2520real-world%2520changes%2520related%2520to%2520pedestrian%250Acrossings%2520of%2520the%2520Argoverse%25202%2520Map%2520Change%2520Dataset.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athis%2520is%2520the%2520first%2520comprehensive%2520problem%2520investigation%2520of%2520real-world%2520end-to-end%250Aelement-based%2520HD%2520map%2520change%2520detection%2520and%2520update%252C%2520and%2520ExelMap%2520the%2520first%250Aproposed%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExelMap%3A%20Explainable%20Element-based%20HD-Map%20Change%20Detection%20and%20Update&entry.906535625=Lena%20Wild%20and%20Ludvig%20Ericson%20and%20Rafael%20Valencia%20and%20Patric%20Jensfelt&entry.1292438233=%20%20Acquisition%20and%20maintenance%20are%20central%20problems%20in%20deploying%20high-definition%0A%28HD%29%20maps%20for%20autonomous%20driving%2C%20with%20two%20lines%20of%20research%20prevalent%20in%0Acurrent%20literature%3A%20Online%20HD%20map%20generation%20and%20HD%20map%20change%20detection.%0AHowever%2C%20the%20generated%20map%27s%20quality%20is%20currently%20insufficient%20for%20safe%0Adeployment%2C%20and%20many%20change%20detection%20approaches%20fail%20to%20precisely%20localize%20and%0Aextract%20the%20changed%20map%20elements%2C%20hence%20lacking%20explainability%20and%20hindering%20a%0Apotential%20fleet-based%20cooperative%20HD%20map%20update.%20In%20this%20paper%2C%20we%20propose%20the%0Anovel%20task%20of%20explainable%20element-based%20HD%20map%20change%20detection%20and%20update.%20In%0Aextending%20recent%20approaches%20that%20use%20online%20mapping%20techniques%20informed%20with%20an%0Aoutdated%20map%20prior%20for%20HD%20map%20updating%2C%20we%20present%20ExelMap%2C%20an%20explainable%0Aelement-based%20map%20updating%20strategy%20that%20specifically%20identifies%20changed%20map%0Aelements.%20In%20this%20context%2C%20we%20discuss%20how%20currently%20used%20metrics%20fail%20to%0Acapture%20change%20detection%20performance%2C%20while%20allowing%20for%20unfair%20comparison%0Abetween%20prior-less%20and%20prior-informed%20map%20generation%20methods.%20Finally%2C%20we%0Apresent%20an%20experimental%20study%20on%20real-world%20changes%20related%20to%20pedestrian%0Acrossings%20of%20the%20Argoverse%202%20Map%20Change%20Dataset.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20is%20the%20first%20comprehensive%20problem%20investigation%20of%20real-world%20end-to-end%0Aelement-based%20HD%20map%20change%20detection%20and%20update%2C%20and%20ExelMap%20the%20first%0Aproposed%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10178v1&entry.124074799=Read"},
{"title": "NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural\n  Networks", "author": "Fergal Stapleton and Edgar Galv\u00e1n", "abstract": "  Evolutionary Algorithms (EAs) play a crucial role in the architectural\nconfiguration and training of Artificial Deep Neural Networks (DNNs), a process\nknown as neuroevolution. However, neuroevolution is hindered by its inherent\ncomputational expense, requiring multiple generations, a large population, and\nnumerous epochs. The most computationally intensive aspect lies in evaluating\nthe fitness function of a single candidate solution. To address this challenge,\nwe employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have\nbeen proposed in neuroevolution, none have been applied to truly large DNNs due\nto issues like intractable information usage. In this work, drawing inspiration\nfrom Genetic Programming semantics, we use phenotypic distance vectors,\noutputted from DNNs, alongside Kriging Partial Least Squares (KPLS), an\napproach that is effective in handling these large vectors, making them\nsuitable for search. Our proposed approach, named Neuro-Linear Genetic\nProgramming surrogate model (NeuroLGP-SM), efficiently and accurately estimates\nDNN fitness without the need for complete evaluations. NeuroLGP-SM demonstrates\ncompetitive or superior results compared to 12 other methods, including\nNeuroLGP without SM, convolutional neural networks, support vector machines,\nand autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% more\nenergy-efficient than its NeuroLGP counterpart. This efficiency advantage adds\nto the overall appeal of our proposed NeuroLGP-SM in optimising the\nconfiguration of large DNNs.\n", "link": "http://arxiv.org/abs/2404.08786v4", "date": "2024-09-16", "relevancy": 2.4482, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4995}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4852}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n%0AAbstract%3A%20%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08786v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroLGP-SM%253A%2520Scalable%2520Surrogate-Assisted%2520Neuroevolution%2520for%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DFergal%2520Stapleton%2520and%2520Edgar%2520Galv%25C3%25A1n%26entry.1292438233%3D%2520%2520Evolutionary%2520Algorithms%2520%2528EAs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520architectural%250Aconfiguration%2520and%2520training%2520of%2520Artificial%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520a%2520process%250Aknown%2520as%2520neuroevolution.%2520However%252C%2520neuroevolution%2520is%2520hindered%2520by%2520its%2520inherent%250Acomputational%2520expense%252C%2520requiring%2520multiple%2520generations%252C%2520a%2520large%2520population%252C%2520and%250Anumerous%2520epochs.%2520The%2520most%2520computationally%2520intensive%2520aspect%2520lies%2520in%2520evaluating%250Athe%2520fitness%2520function%2520of%2520a%2520single%2520candidate%2520solution.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520employ%2520Surrogate-assisted%2520EAs%2520%2528SAEAs%2529.%2520While%2520a%2520few%2520SAEAs%2520approaches%2520have%250Abeen%2520proposed%2520in%2520neuroevolution%252C%2520none%2520have%2520been%2520applied%2520to%2520truly%2520large%2520DNNs%2520due%250Ato%2520issues%2520like%2520intractable%2520information%2520usage.%2520In%2520this%2520work%252C%2520drawing%2520inspiration%250Afrom%2520Genetic%2520Programming%2520semantics%252C%2520we%2520use%2520phenotypic%2520distance%2520vectors%252C%250Aoutputted%2520from%2520DNNs%252C%2520alongside%2520Kriging%2520Partial%2520Least%2520Squares%2520%2528KPLS%2529%252C%2520an%250Aapproach%2520that%2520is%2520effective%2520in%2520handling%2520these%2520large%2520vectors%252C%2520making%2520them%250Asuitable%2520for%2520search.%2520Our%2520proposed%2520approach%252C%2520named%2520Neuro-Linear%2520Genetic%250AProgramming%2520surrogate%2520model%2520%2528NeuroLGP-SM%2529%252C%2520efficiently%2520and%2520accurately%2520estimates%250ADNN%2520fitness%2520without%2520the%2520need%2520for%2520complete%2520evaluations.%2520NeuroLGP-SM%2520demonstrates%250Acompetitive%2520or%2520superior%2520results%2520compared%2520to%252012%2520other%2520methods%252C%2520including%250ANeuroLGP%2520without%2520SM%252C%2520convolutional%2520neural%2520networks%252C%2520support%2520vector%2520machines%252C%250Aand%2520autoencoders.%2520Additionally%252C%2520it%2520is%2520worth%2520noting%2520that%2520NeuroLGP-SM%2520is%252025%2525%2520more%250Aenergy-efficient%2520than%2520its%2520NeuroLGP%2520counterpart.%2520This%2520efficiency%2520advantage%2520adds%250Ato%2520the%2520overall%2520appeal%2520of%2520our%2520proposed%2520NeuroLGP-SM%2520in%2520optimising%2520the%250Aconfiguration%2520of%2520large%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08786v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&entry.906535625=Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n&entry.1292438233=%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08786v4&entry.124074799=Read"},
{"title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos", "author": "Fuchen Long and Zhaofan Qiu and Ting Yao and Tao Mei", "abstract": "  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.\n", "link": "http://arxiv.org/abs/2401.01256v2", "date": "2024-09-16", "relevancy": 2.4165, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6176}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoStudio%3A%20Generating%20Consistent-Content%20and%20Multi-Scene%20Videos&body=Title%3A%20VideoStudio%3A%20Generating%20Consistent-Content%20and%20Multi-Scene%20Videos%0AAuthor%3A%20Fuchen%20Long%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Tao%20Mei%0AAbstract%3A%20%20%20The%20recent%20innovations%20and%20breakthroughs%20in%20diffusion%20models%20have%0Asignificantly%20expanded%20the%20possibilities%20of%20generating%20high-quality%20videos%20for%0Athe%20given%20prompts.%20Most%20existing%20works%20tackle%20the%20single-scene%20scenario%20with%0Aonly%20one%20video%20event%20occurring%20in%20a%20single%20background.%20Extending%20to%20generate%0Amulti-scene%20videos%20nevertheless%20is%20not%20trivial%20and%20necessitates%20to%20nicely%0Amanage%20the%20logic%20in%20between%20while%20preserving%20the%20consistent%20visual%20appearance%0Aof%20key%20content%20across%20video%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20namely%20VideoStudio%2C%20for%20consistent-content%20and%20multi-scene%20video%0Ageneration.%20Technically%2C%20VideoStudio%20leverages%20Large%20Language%20Models%20%28LLM%29%20to%0Aconvert%20the%20input%20prompt%20into%20comprehensive%20multi-scene%20script%20that%20benefits%0Afrom%20the%20logical%20knowledge%20learnt%20by%20LLM.%20The%20script%20for%20each%20scene%20includes%20a%0Aprompt%20describing%20the%20event%2C%20the%20foreground/background%20entities%2C%20as%20well%20as%0Acamera%20movement.%20VideoStudio%20identifies%20the%20common%20entities%20throughout%20the%0Ascript%20and%20asks%20LLM%20to%20detail%20each%20entity.%20The%20resultant%20entity%20description%20is%0Athen%20fed%20into%20a%20text-to-image%20model%20to%20generate%20a%20reference%20image%20for%20each%0Aentity.%20Finally%2C%20VideoStudio%20outputs%20a%20multi-scene%20video%20by%20generating%20each%0Ascene%20video%20via%20a%20diffusion%20process%20that%20takes%20the%20reference%20images%2C%20the%0Adescriptive%20prompt%20of%20the%20event%20and%20camera%20movement%20into%20account.%20The%20diffusion%0Amodel%20incorporates%20the%20reference%20images%20as%20the%20condition%20and%20alignment%20to%0Astrengthen%20the%20content%20consistency%20of%20multi-scene%20videos.%20Extensive%20experiments%0Ademonstrate%20that%20VideoStudio%20outperforms%20the%20SOTA%20video%20generation%20models%20in%0Aterms%20of%20visual%20quality%2C%20content%20consistency%2C%20and%20user%20preference.%20Source%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/FuchenUSTC/VideoStudio%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoStudio%253A%2520Generating%2520Consistent-Content%2520and%2520Multi-Scene%2520Videos%26entry.906535625%3DFuchen%2520Long%2520and%2520Zhaofan%2520Qiu%2520and%2520Ting%2520Yao%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520The%2520recent%2520innovations%2520and%2520breakthroughs%2520in%2520diffusion%2520models%2520have%250Asignificantly%2520expanded%2520the%2520possibilities%2520of%2520generating%2520high-quality%2520videos%2520for%250Athe%2520given%2520prompts.%2520Most%2520existing%2520works%2520tackle%2520the%2520single-scene%2520scenario%2520with%250Aonly%2520one%2520video%2520event%2520occurring%2520in%2520a%2520single%2520background.%2520Extending%2520to%2520generate%250Amulti-scene%2520videos%2520nevertheless%2520is%2520not%2520trivial%2520and%2520necessitates%2520to%2520nicely%250Amanage%2520the%2520logic%2520in%2520between%2520while%2520preserving%2520the%2520consistent%2520visual%2520appearance%250Aof%2520key%2520content%2520across%2520video%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aframework%252C%2520namely%2520VideoStudio%252C%2520for%2520consistent-content%2520and%2520multi-scene%2520video%250Ageneration.%2520Technically%252C%2520VideoStudio%2520leverages%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520to%250Aconvert%2520the%2520input%2520prompt%2520into%2520comprehensive%2520multi-scene%2520script%2520that%2520benefits%250Afrom%2520the%2520logical%2520knowledge%2520learnt%2520by%2520LLM.%2520The%2520script%2520for%2520each%2520scene%2520includes%2520a%250Aprompt%2520describing%2520the%2520event%252C%2520the%2520foreground/background%2520entities%252C%2520as%2520well%2520as%250Acamera%2520movement.%2520VideoStudio%2520identifies%2520the%2520common%2520entities%2520throughout%2520the%250Ascript%2520and%2520asks%2520LLM%2520to%2520detail%2520each%2520entity.%2520The%2520resultant%2520entity%2520description%2520is%250Athen%2520fed%2520into%2520a%2520text-to-image%2520model%2520to%2520generate%2520a%2520reference%2520image%2520for%2520each%250Aentity.%2520Finally%252C%2520VideoStudio%2520outputs%2520a%2520multi-scene%2520video%2520by%2520generating%2520each%250Ascene%2520video%2520via%2520a%2520diffusion%2520process%2520that%2520takes%2520the%2520reference%2520images%252C%2520the%250Adescriptive%2520prompt%2520of%2520the%2520event%2520and%2520camera%2520movement%2520into%2520account.%2520The%2520diffusion%250Amodel%2520incorporates%2520the%2520reference%2520images%2520as%2520the%2520condition%2520and%2520alignment%2520to%250Astrengthen%2520the%2520content%2520consistency%2520of%2520multi-scene%2520videos.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520VideoStudio%2520outperforms%2520the%2520SOTA%2520video%2520generation%2520models%2520in%250Aterms%2520of%2520visual%2520quality%252C%2520content%2520consistency%252C%2520and%2520user%2520preference.%2520Source%2520code%250Ais%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/FuchenUSTC/VideoStudio%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoStudio%3A%20Generating%20Consistent-Content%20and%20Multi-Scene%20Videos&entry.906535625=Fuchen%20Long%20and%20Zhaofan%20Qiu%20and%20Ting%20Yao%20and%20Tao%20Mei&entry.1292438233=%20%20The%20recent%20innovations%20and%20breakthroughs%20in%20diffusion%20models%20have%0Asignificantly%20expanded%20the%20possibilities%20of%20generating%20high-quality%20videos%20for%0Athe%20given%20prompts.%20Most%20existing%20works%20tackle%20the%20single-scene%20scenario%20with%0Aonly%20one%20video%20event%20occurring%20in%20a%20single%20background.%20Extending%20to%20generate%0Amulti-scene%20videos%20nevertheless%20is%20not%20trivial%20and%20necessitates%20to%20nicely%0Amanage%20the%20logic%20in%20between%20while%20preserving%20the%20consistent%20visual%20appearance%0Aof%20key%20content%20across%20video%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20namely%20VideoStudio%2C%20for%20consistent-content%20and%20multi-scene%20video%0Ageneration.%20Technically%2C%20VideoStudio%20leverages%20Large%20Language%20Models%20%28LLM%29%20to%0Aconvert%20the%20input%20prompt%20into%20comprehensive%20multi-scene%20script%20that%20benefits%0Afrom%20the%20logical%20knowledge%20learnt%20by%20LLM.%20The%20script%20for%20each%20scene%20includes%20a%0Aprompt%20describing%20the%20event%2C%20the%20foreground/background%20entities%2C%20as%20well%20as%0Acamera%20movement.%20VideoStudio%20identifies%20the%20common%20entities%20throughout%20the%0Ascript%20and%20asks%20LLM%20to%20detail%20each%20entity.%20The%20resultant%20entity%20description%20is%0Athen%20fed%20into%20a%20text-to-image%20model%20to%20generate%20a%20reference%20image%20for%20each%0Aentity.%20Finally%2C%20VideoStudio%20outputs%20a%20multi-scene%20video%20by%20generating%20each%0Ascene%20video%20via%20a%20diffusion%20process%20that%20takes%20the%20reference%20images%2C%20the%0Adescriptive%20prompt%20of%20the%20event%20and%20camera%20movement%20into%20account.%20The%20diffusion%0Amodel%20incorporates%20the%20reference%20images%20as%20the%20condition%20and%20alignment%20to%0Astrengthen%20the%20content%20consistency%20of%20multi-scene%20videos.%20Extensive%20experiments%0Ademonstrate%20that%20VideoStudio%20outperforms%20the%20SOTA%20video%20generation%20models%20in%0Aterms%20of%20visual%20quality%2C%20content%20consistency%2C%20and%20user%20preference.%20Source%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/FuchenUSTC/VideoStudio%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01256v2&entry.124074799=Read"},
{"title": "Learning Gentle Grasping from Human-Free Force Control Demonstration", "author": "Mingxuan Li and Lunwei Zhang and Tiemin Li and Yao Jiang", "abstract": "  Humans can steadily and gently grasp unfamiliar objects based on tactile\nperception. Robots still face challenges in achieving similar performance due\nto the difficulty of learning accurate grasp-force predictions and force\ncontrol strategies that can be generalized from limited data. In this article,\nwe propose an approach for learning grasping from ideal force control\ndemonstrations, to achieve similar performance of human hands with limited data\nsize. Our approach utilizes objects with known contact characteristics to\nautomatically generate reference force curves without human demonstrations. In\naddition, we design the dual convolutional neural networks (Dual-CNN)\narchitecture which incorporating a physics-based mechanics module for learning\ntarget grasping force predictions from demonstrations. The described method can\nbe effectively applied in vision-based tactile sensors and enables gentle and\nstable grasping of objects from the ground. The described prediction model and\ngrasping strategy were validated in offline evaluations and online experiments,\nand the accuracy and generalizability were demonstrated.\n", "link": "http://arxiv.org/abs/2409.10371v1", "date": "2024-09-16", "relevancy": 2.4163, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6632}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5909}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Gentle%20Grasping%20from%20Human-Free%20Force%20Control%20Demonstration&body=Title%3A%20Learning%20Gentle%20Grasping%20from%20Human-Free%20Force%20Control%20Demonstration%0AAuthor%3A%20Mingxuan%20Li%20and%20Lunwei%20Zhang%20and%20Tiemin%20Li%20and%20Yao%20Jiang%0AAbstract%3A%20%20%20Humans%20can%20steadily%20and%20gently%20grasp%20unfamiliar%20objects%20based%20on%20tactile%0Aperception.%20Robots%20still%20face%20challenges%20in%20achieving%20similar%20performance%20due%0Ato%20the%20difficulty%20of%20learning%20accurate%20grasp-force%20predictions%20and%20force%0Acontrol%20strategies%20that%20can%20be%20generalized%20from%20limited%20data.%20In%20this%20article%2C%0Awe%20propose%20an%20approach%20for%20learning%20grasping%20from%20ideal%20force%20control%0Ademonstrations%2C%20to%20achieve%20similar%20performance%20of%20human%20hands%20with%20limited%20data%0Asize.%20Our%20approach%20utilizes%20objects%20with%20known%20contact%20characteristics%20to%0Aautomatically%20generate%20reference%20force%20curves%20without%20human%20demonstrations.%20In%0Aaddition%2C%20we%20design%20the%20dual%20convolutional%20neural%20networks%20%28Dual-CNN%29%0Aarchitecture%20which%20incorporating%20a%20physics-based%20mechanics%20module%20for%20learning%0Atarget%20grasping%20force%20predictions%20from%20demonstrations.%20The%20described%20method%20can%0Abe%20effectively%20applied%20in%20vision-based%20tactile%20sensors%20and%20enables%20gentle%20and%0Astable%20grasping%20of%20objects%20from%20the%20ground.%20The%20described%20prediction%20model%20and%0Agrasping%20strategy%20were%20validated%20in%20offline%20evaluations%20and%20online%20experiments%2C%0Aand%20the%20accuracy%20and%20generalizability%20were%20demonstrated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Gentle%2520Grasping%2520from%2520Human-Free%2520Force%2520Control%2520Demonstration%26entry.906535625%3DMingxuan%2520Li%2520and%2520Lunwei%2520Zhang%2520and%2520Tiemin%2520Li%2520and%2520Yao%2520Jiang%26entry.1292438233%3D%2520%2520Humans%2520can%2520steadily%2520and%2520gently%2520grasp%2520unfamiliar%2520objects%2520based%2520on%2520tactile%250Aperception.%2520Robots%2520still%2520face%2520challenges%2520in%2520achieving%2520similar%2520performance%2520due%250Ato%2520the%2520difficulty%2520of%2520learning%2520accurate%2520grasp-force%2520predictions%2520and%2520force%250Acontrol%2520strategies%2520that%2520can%2520be%2520generalized%2520from%2520limited%2520data.%2520In%2520this%2520article%252C%250Awe%2520propose%2520an%2520approach%2520for%2520learning%2520grasping%2520from%2520ideal%2520force%2520control%250Ademonstrations%252C%2520to%2520achieve%2520similar%2520performance%2520of%2520human%2520hands%2520with%2520limited%2520data%250Asize.%2520Our%2520approach%2520utilizes%2520objects%2520with%2520known%2520contact%2520characteristics%2520to%250Aautomatically%2520generate%2520reference%2520force%2520curves%2520without%2520human%2520demonstrations.%2520In%250Aaddition%252C%2520we%2520design%2520the%2520dual%2520convolutional%2520neural%2520networks%2520%2528Dual-CNN%2529%250Aarchitecture%2520which%2520incorporating%2520a%2520physics-based%2520mechanics%2520module%2520for%2520learning%250Atarget%2520grasping%2520force%2520predictions%2520from%2520demonstrations.%2520The%2520described%2520method%2520can%250Abe%2520effectively%2520applied%2520in%2520vision-based%2520tactile%2520sensors%2520and%2520enables%2520gentle%2520and%250Astable%2520grasping%2520of%2520objects%2520from%2520the%2520ground.%2520The%2520described%2520prediction%2520model%2520and%250Agrasping%2520strategy%2520were%2520validated%2520in%2520offline%2520evaluations%2520and%2520online%2520experiments%252C%250Aand%2520the%2520accuracy%2520and%2520generalizability%2520were%2520demonstrated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Gentle%20Grasping%20from%20Human-Free%20Force%20Control%20Demonstration&entry.906535625=Mingxuan%20Li%20and%20Lunwei%20Zhang%20and%20Tiemin%20Li%20and%20Yao%20Jiang&entry.1292438233=%20%20Humans%20can%20steadily%20and%20gently%20grasp%20unfamiliar%20objects%20based%20on%20tactile%0Aperception.%20Robots%20still%20face%20challenges%20in%20achieving%20similar%20performance%20due%0Ato%20the%20difficulty%20of%20learning%20accurate%20grasp-force%20predictions%20and%20force%0Acontrol%20strategies%20that%20can%20be%20generalized%20from%20limited%20data.%20In%20this%20article%2C%0Awe%20propose%20an%20approach%20for%20learning%20grasping%20from%20ideal%20force%20control%0Ademonstrations%2C%20to%20achieve%20similar%20performance%20of%20human%20hands%20with%20limited%20data%0Asize.%20Our%20approach%20utilizes%20objects%20with%20known%20contact%20characteristics%20to%0Aautomatically%20generate%20reference%20force%20curves%20without%20human%20demonstrations.%20In%0Aaddition%2C%20we%20design%20the%20dual%20convolutional%20neural%20networks%20%28Dual-CNN%29%0Aarchitecture%20which%20incorporating%20a%20physics-based%20mechanics%20module%20for%20learning%0Atarget%20grasping%20force%20predictions%20from%20demonstrations.%20The%20described%20method%20can%0Abe%20effectively%20applied%20in%20vision-based%20tactile%20sensors%20and%20enables%20gentle%20and%0Astable%20grasping%20of%20objects%20from%20the%20ground.%20The%20described%20prediction%20model%20and%0Agrasping%20strategy%20were%20validated%20in%20offline%20evaluations%20and%20online%20experiments%2C%0Aand%20the%20accuracy%20and%20generalizability%20were%20demonstrated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10371v1&entry.124074799=Read"},
{"title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA", "author": "Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao", "abstract": "  We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Additionally, Matryoshka\nRepresentation Learning is integrated into the training process, allowing\nflexible truncation of embedding dimensions without compromising performance.\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the\nlatest proprietary embeddings from OpenAI and Cohere on English tasks, while\nachieving superior performance compared to multilingual-e5-large-instruct\nacross all multilingual tasks.\n", "link": "http://arxiv.org/abs/2409.10173v1", "date": "2024-09-16", "relevancy": 2.4144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20jina-embeddings-v3%3A%20Multilingual%20Embeddings%20With%20Task%20LoRA&body=Title%3A%20jina-embeddings-v3%3A%20Multilingual%20Embeddings%20With%20Task%20LoRA%0AAuthor%3A%20Saba%20Sturua%20and%20Isabelle%20Mohr%20and%20Mohammad%20Kalim%20Akram%20and%20Michael%20G%C3%BCnther%20and%20Bo%20Wang%20and%20Markus%20Krimmel%20and%20Feng%20Wang%20and%20Georgios%20Mastrapas%20and%20Andreas%20Koukounas%20and%20Andreas%20Koukounas%20and%20Nan%20Wang%20and%20Han%20Xiao%0AAbstract%3A%20%20%20We%20introduce%20jina-embeddings-v3%2C%20a%20novel%20text%20embedding%20model%20with%20570%0Amillion%20parameters%2C%20achieves%20state-of-the-art%20performance%20on%20multilingual%20data%0Aand%20long-context%20retrieval%20tasks%2C%20supporting%20context%20lengths%20of%20up%20to%208192%0Atokens.%20The%20model%20includes%20a%20set%20of%20task-specific%20Low-Rank%20Adaptation%20%28LoRA%29%0Aadapters%20to%20generate%20high-quality%20embeddings%20for%20query-document%20retrieval%2C%0Aclustering%2C%20classification%2C%20and%20text%20matching.%20Additionally%2C%20Matryoshka%0ARepresentation%20Learning%20is%20integrated%20into%20the%20training%20process%2C%20allowing%0Aflexible%20truncation%20of%20embedding%20dimensions%20without%20compromising%20performance.%0AEvaluation%20on%20the%20MTEB%20benchmark%20shows%20that%20jina-embeddings-v3%20outperforms%20the%0Alatest%20proprietary%20embeddings%20from%20OpenAI%20and%20Cohere%20on%20English%20tasks%2C%20while%0Aachieving%20superior%20performance%20compared%20to%20multilingual-e5-large-instruct%0Aacross%20all%20multilingual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Djina-embeddings-v3%253A%2520Multilingual%2520Embeddings%2520With%2520Task%2520LoRA%26entry.906535625%3DSaba%2520Sturua%2520and%2520Isabelle%2520Mohr%2520and%2520Mohammad%2520Kalim%2520Akram%2520and%2520Michael%2520G%25C3%25BCnther%2520and%2520Bo%2520Wang%2520and%2520Markus%2520Krimmel%2520and%2520Feng%2520Wang%2520and%2520Georgios%2520Mastrapas%2520and%2520Andreas%2520Koukounas%2520and%2520Andreas%2520Koukounas%2520and%2520Nan%2520Wang%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520We%2520introduce%2520jina-embeddings-v3%252C%2520a%2520novel%2520text%2520embedding%2520model%2520with%2520570%250Amillion%2520parameters%252C%2520achieves%2520state-of-the-art%2520performance%2520on%2520multilingual%2520data%250Aand%2520long-context%2520retrieval%2520tasks%252C%2520supporting%2520context%2520lengths%2520of%2520up%2520to%25208192%250Atokens.%2520The%2520model%2520includes%2520a%2520set%2520of%2520task-specific%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Aadapters%2520to%2520generate%2520high-quality%2520embeddings%2520for%2520query-document%2520retrieval%252C%250Aclustering%252C%2520classification%252C%2520and%2520text%2520matching.%2520Additionally%252C%2520Matryoshka%250ARepresentation%2520Learning%2520is%2520integrated%2520into%2520the%2520training%2520process%252C%2520allowing%250Aflexible%2520truncation%2520of%2520embedding%2520dimensions%2520without%2520compromising%2520performance.%250AEvaluation%2520on%2520the%2520MTEB%2520benchmark%2520shows%2520that%2520jina-embeddings-v3%2520outperforms%2520the%250Alatest%2520proprietary%2520embeddings%2520from%2520OpenAI%2520and%2520Cohere%2520on%2520English%2520tasks%252C%2520while%250Aachieving%2520superior%2520performance%2520compared%2520to%2520multilingual-e5-large-instruct%250Aacross%2520all%2520multilingual%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=jina-embeddings-v3%3A%20Multilingual%20Embeddings%20With%20Task%20LoRA&entry.906535625=Saba%20Sturua%20and%20Isabelle%20Mohr%20and%20Mohammad%20Kalim%20Akram%20and%20Michael%20G%C3%BCnther%20and%20Bo%20Wang%20and%20Markus%20Krimmel%20and%20Feng%20Wang%20and%20Georgios%20Mastrapas%20and%20Andreas%20Koukounas%20and%20Andreas%20Koukounas%20and%20Nan%20Wang%20and%20Han%20Xiao&entry.1292438233=%20%20We%20introduce%20jina-embeddings-v3%2C%20a%20novel%20text%20embedding%20model%20with%20570%0Amillion%20parameters%2C%20achieves%20state-of-the-art%20performance%20on%20multilingual%20data%0Aand%20long-context%20retrieval%20tasks%2C%20supporting%20context%20lengths%20of%20up%20to%208192%0Atokens.%20The%20model%20includes%20a%20set%20of%20task-specific%20Low-Rank%20Adaptation%20%28LoRA%29%0Aadapters%20to%20generate%20high-quality%20embeddings%20for%20query-document%20retrieval%2C%0Aclustering%2C%20classification%2C%20and%20text%20matching.%20Additionally%2C%20Matryoshka%0ARepresentation%20Learning%20is%20integrated%20into%20the%20training%20process%2C%20allowing%0Aflexible%20truncation%20of%20embedding%20dimensions%20without%20compromising%20performance.%0AEvaluation%20on%20the%20MTEB%20benchmark%20shows%20that%20jina-embeddings-v3%20outperforms%20the%0Alatest%20proprietary%20embeddings%20from%20OpenAI%20and%20Cohere%20on%20English%20tasks%2C%20while%0Aachieving%20superior%20performance%20compared%20to%20multilingual-e5-large-instruct%0Aacross%20all%20multilingual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10173v1&entry.124074799=Read"},
{"title": "MusicLIME: Explainable Multimodal Music Understanding", "author": "Theodoros Sotirou and Vassilis Lyberatos and Orfeas Menis Mastromichalakis and Giorgos Stamou", "abstract": "  Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.\n", "link": "http://arxiv.org/abs/2409.10496v1", "date": "2024-09-16", "relevancy": 2.4132, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding&body=Title%3A%20MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding%0AAuthor%3A%20Theodoros%20Sotirou%20and%20Vassilis%20Lyberatos%20and%20Orfeas%20Menis%20Mastromichalakis%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Multimodal%20models%20are%20critical%20for%20music%20understanding%20tasks%2C%20as%20they%20capture%0Athe%20complex%20interplay%20between%20audio%20and%20lyrics.%20However%2C%20as%20these%20models%20become%0Amore%20prevalent%2C%20the%20need%20for%20explainability%20grows-understanding%20how%20these%0Asystems%20make%20decisions%20is%20vital%20for%20ensuring%20fairness%2C%20reducing%20bias%2C%20and%0Afostering%20trust.%20In%20this%20paper%2C%20we%20introduce%20MusicLIME%2C%20a%20model-agnostic%0Afeature%20importance%20explanation%20method%20designed%20for%20multimodal%20music%20models.%0AUnlike%20traditional%20unimodal%20methods%2C%20which%20analyze%20each%20modality%20separately%0Awithout%20considering%20the%20interaction%20between%20them%2C%20often%20leading%20to%20incomplete%0Aor%20misleading%20explanations%2C%20MusicLIME%20reveals%20how%20audio%20and%20lyrical%20features%0Ainteract%20and%20contribute%20to%20predictions%2C%20providing%20a%20holistic%20view%20of%20the%0Amodel%27s%20decision-making.%20Additionally%2C%20we%20enhance%20local%20explanations%20by%0Aaggregating%20them%20into%20global%20explanations%2C%20giving%20users%20a%20broader%20perspective%0Aof%20model%20behavior.%20Through%20this%20work%2C%20we%20contribute%20to%20improving%20the%0Ainterpretability%20of%20multimodal%20music%20models%2C%20empowering%20users%20to%20make%20informed%0Achoices%2C%20and%20fostering%20more%20equitable%2C%20fair%2C%20and%20transparent%20music%0Aunderstanding%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusicLIME%253A%2520Explainable%2520Multimodal%2520Music%2520Understanding%26entry.906535625%3DTheodoros%2520Sotirou%2520and%2520Vassilis%2520Lyberatos%2520and%2520Orfeas%2520Menis%2520Mastromichalakis%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Multimodal%2520models%2520are%2520critical%2520for%2520music%2520understanding%2520tasks%252C%2520as%2520they%2520capture%250Athe%2520complex%2520interplay%2520between%2520audio%2520and%2520lyrics.%2520However%252C%2520as%2520these%2520models%2520become%250Amore%2520prevalent%252C%2520the%2520need%2520for%2520explainability%2520grows-understanding%2520how%2520these%250Asystems%2520make%2520decisions%2520is%2520vital%2520for%2520ensuring%2520fairness%252C%2520reducing%2520bias%252C%2520and%250Afostering%2520trust.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MusicLIME%252C%2520a%2520model-agnostic%250Afeature%2520importance%2520explanation%2520method%2520designed%2520for%2520multimodal%2520music%2520models.%250AUnlike%2520traditional%2520unimodal%2520methods%252C%2520which%2520analyze%2520each%2520modality%2520separately%250Awithout%2520considering%2520the%2520interaction%2520between%2520them%252C%2520often%2520leading%2520to%2520incomplete%250Aor%2520misleading%2520explanations%252C%2520MusicLIME%2520reveals%2520how%2520audio%2520and%2520lyrical%2520features%250Ainteract%2520and%2520contribute%2520to%2520predictions%252C%2520providing%2520a%2520holistic%2520view%2520of%2520the%250Amodel%2527s%2520decision-making.%2520Additionally%252C%2520we%2520enhance%2520local%2520explanations%2520by%250Aaggregating%2520them%2520into%2520global%2520explanations%252C%2520giving%2520users%2520a%2520broader%2520perspective%250Aof%2520model%2520behavior.%2520Through%2520this%2520work%252C%2520we%2520contribute%2520to%2520improving%2520the%250Ainterpretability%2520of%2520multimodal%2520music%2520models%252C%2520empowering%2520users%2520to%2520make%2520informed%250Achoices%252C%2520and%2520fostering%2520more%2520equitable%252C%2520fair%252C%2520and%2520transparent%2520music%250Aunderstanding%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding&entry.906535625=Theodoros%20Sotirou%20and%20Vassilis%20Lyberatos%20and%20Orfeas%20Menis%20Mastromichalakis%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Multimodal%20models%20are%20critical%20for%20music%20understanding%20tasks%2C%20as%20they%20capture%0Athe%20complex%20interplay%20between%20audio%20and%20lyrics.%20However%2C%20as%20these%20models%20become%0Amore%20prevalent%2C%20the%20need%20for%20explainability%20grows-understanding%20how%20these%0Asystems%20make%20decisions%20is%20vital%20for%20ensuring%20fairness%2C%20reducing%20bias%2C%20and%0Afostering%20trust.%20In%20this%20paper%2C%20we%20introduce%20MusicLIME%2C%20a%20model-agnostic%0Afeature%20importance%20explanation%20method%20designed%20for%20multimodal%20music%20models.%0AUnlike%20traditional%20unimodal%20methods%2C%20which%20analyze%20each%20modality%20separately%0Awithout%20considering%20the%20interaction%20between%20them%2C%20often%20leading%20to%20incomplete%0Aor%20misleading%20explanations%2C%20MusicLIME%20reveals%20how%20audio%20and%20lyrical%20features%0Ainteract%20and%20contribute%20to%20predictions%2C%20providing%20a%20holistic%20view%20of%20the%0Amodel%27s%20decision-making.%20Additionally%2C%20we%20enhance%20local%20explanations%20by%0Aaggregating%20them%20into%20global%20explanations%2C%20giving%20users%20a%20broader%20perspective%0Aof%20model%20behavior.%20Through%20this%20work%2C%20we%20contribute%20to%20improving%20the%0Ainterpretability%20of%20multimodal%20music%20models%2C%20empowering%20users%20to%20make%20informed%0Achoices%2C%20and%20fostering%20more%20equitable%2C%20fair%2C%20and%20transparent%20music%0Aunderstanding%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10496v1&entry.124074799=Read"},
{"title": "BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting", "author": "Wugang Meng and Tianfu Wu and Huan Yin and Fumin Zhang", "abstract": "  Image-goal navigation enables a robot to reach the location where a target\nimage was captured, using visual cues for guidance. However, current methods\neither rely heavily on data and computationally expensive learning-based\napproaches or lack efficiency in complex environments due to insufficient\nexploration strategies. To address these limitations, we propose Bayesian\nEmbodied Image-goal Navigation Using Gaussian Splatting, a novel method that\nformulates ImageNav as an optimal control problem within a model predictive\ncontrol framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to\npredict future observations, enabling efficient, real-time navigation decisions\ngrounded in the robot's sensory experiences. By integrating Bayesian updates,\nour method dynamically refines the robot's strategy without requiring extensive\nprior experience or data. Our algorithm is validated through extensive\nsimulations and physical experiments, showcasing its potential for embodied\nrobot systems in visually complex scenarios.\n", "link": "http://arxiv.org/abs/2409.10216v1", "date": "2024-09-16", "relevancy": 2.4055, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEINGS%3A%20Bayesian%20Embodied%20Image-goal%20Navigation%20with%20Gaussian%20Splatting&body=Title%3A%20BEINGS%3A%20Bayesian%20Embodied%20Image-goal%20Navigation%20with%20Gaussian%20Splatting%0AAuthor%3A%20Wugang%20Meng%20and%20Tianfu%20Wu%20and%20Huan%20Yin%20and%20Fumin%20Zhang%0AAbstract%3A%20%20%20Image-goal%20navigation%20enables%20a%20robot%20to%20reach%20the%20location%20where%20a%20target%0Aimage%20was%20captured%2C%20using%20visual%20cues%20for%20guidance.%20However%2C%20current%20methods%0Aeither%20rely%20heavily%20on%20data%20and%20computationally%20expensive%20learning-based%0Aapproaches%20or%20lack%20efficiency%20in%20complex%20environments%20due%20to%20insufficient%0Aexploration%20strategies.%20To%20address%20these%20limitations%2C%20we%20propose%20Bayesian%0AEmbodied%20Image-goal%20Navigation%20Using%20Gaussian%20Splatting%2C%20a%20novel%20method%20that%0Aformulates%20ImageNav%20as%20an%20optimal%20control%20problem%20within%20a%20model%20predictive%0Acontrol%20framework.%20BEINGS%20leverages%203D%20Gaussian%20Splatting%20as%20a%20scene%20prior%20to%0Apredict%20future%20observations%2C%20enabling%20efficient%2C%20real-time%20navigation%20decisions%0Agrounded%20in%20the%20robot%27s%20sensory%20experiences.%20By%20integrating%20Bayesian%20updates%2C%0Aour%20method%20dynamically%20refines%20the%20robot%27s%20strategy%20without%20requiring%20extensive%0Aprior%20experience%20or%20data.%20Our%20algorithm%20is%20validated%20through%20extensive%0Asimulations%20and%20physical%20experiments%2C%20showcasing%20its%20potential%20for%20embodied%0Arobot%20systems%20in%20visually%20complex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEINGS%253A%2520Bayesian%2520Embodied%2520Image-goal%2520Navigation%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DWugang%2520Meng%2520and%2520Tianfu%2520Wu%2520and%2520Huan%2520Yin%2520and%2520Fumin%2520Zhang%26entry.1292438233%3D%2520%2520Image-goal%2520navigation%2520enables%2520a%2520robot%2520to%2520reach%2520the%2520location%2520where%2520a%2520target%250Aimage%2520was%2520captured%252C%2520using%2520visual%2520cues%2520for%2520guidance.%2520However%252C%2520current%2520methods%250Aeither%2520rely%2520heavily%2520on%2520data%2520and%2520computationally%2520expensive%2520learning-based%250Aapproaches%2520or%2520lack%2520efficiency%2520in%2520complex%2520environments%2520due%2520to%2520insufficient%250Aexploration%2520strategies.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Bayesian%250AEmbodied%2520Image-goal%2520Navigation%2520Using%2520Gaussian%2520Splatting%252C%2520a%2520novel%2520method%2520that%250Aformulates%2520ImageNav%2520as%2520an%2520optimal%2520control%2520problem%2520within%2520a%2520model%2520predictive%250Acontrol%2520framework.%2520BEINGS%2520leverages%25203D%2520Gaussian%2520Splatting%2520as%2520a%2520scene%2520prior%2520to%250Apredict%2520future%2520observations%252C%2520enabling%2520efficient%252C%2520real-time%2520navigation%2520decisions%250Agrounded%2520in%2520the%2520robot%2527s%2520sensory%2520experiences.%2520By%2520integrating%2520Bayesian%2520updates%252C%250Aour%2520method%2520dynamically%2520refines%2520the%2520robot%2527s%2520strategy%2520without%2520requiring%2520extensive%250Aprior%2520experience%2520or%2520data.%2520Our%2520algorithm%2520is%2520validated%2520through%2520extensive%250Asimulations%2520and%2520physical%2520experiments%252C%2520showcasing%2520its%2520potential%2520for%2520embodied%250Arobot%2520systems%2520in%2520visually%2520complex%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEINGS%3A%20Bayesian%20Embodied%20Image-goal%20Navigation%20with%20Gaussian%20Splatting&entry.906535625=Wugang%20Meng%20and%20Tianfu%20Wu%20and%20Huan%20Yin%20and%20Fumin%20Zhang&entry.1292438233=%20%20Image-goal%20navigation%20enables%20a%20robot%20to%20reach%20the%20location%20where%20a%20target%0Aimage%20was%20captured%2C%20using%20visual%20cues%20for%20guidance.%20However%2C%20current%20methods%0Aeither%20rely%20heavily%20on%20data%20and%20computationally%20expensive%20learning-based%0Aapproaches%20or%20lack%20efficiency%20in%20complex%20environments%20due%20to%20insufficient%0Aexploration%20strategies.%20To%20address%20these%20limitations%2C%20we%20propose%20Bayesian%0AEmbodied%20Image-goal%20Navigation%20Using%20Gaussian%20Splatting%2C%20a%20novel%20method%20that%0Aformulates%20ImageNav%20as%20an%20optimal%20control%20problem%20within%20a%20model%20predictive%0Acontrol%20framework.%20BEINGS%20leverages%203D%20Gaussian%20Splatting%20as%20a%20scene%20prior%20to%0Apredict%20future%20observations%2C%20enabling%20efficient%2C%20real-time%20navigation%20decisions%0Agrounded%20in%20the%20robot%27s%20sensory%20experiences.%20By%20integrating%20Bayesian%20updates%2C%0Aour%20method%20dynamically%20refines%20the%20robot%27s%20strategy%20without%20requiring%20extensive%0Aprior%20experience%20or%20data.%20Our%20algorithm%20is%20validated%20through%20extensive%0Asimulations%20and%20physical%20experiments%2C%20showcasing%20its%20potential%20for%20embodied%0Arobot%20systems%20in%20visually%20complex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10216v1&entry.124074799=Read"},
{"title": "Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning:\n  Lessons Learned", "author": "Du Yin and Jinliang Deng and Shuang Ao and Zechen Li and Hao Xue and Arian Prabowo and Renhe Jiang and Xuan Song and Flora Salim", "abstract": "  Training models on spatio-temporal (ST) data poses an open problem due to the\ncomplicated and diverse nature of the data itself, and it is challenging to\nensure the model's performance directly trained on the original ST data. While\nlimiting the variety of training data can make training easier, it can also\nlead to a lack of knowledge and information for the model, resulting in a\ndecrease in performance. To address this challenge, we presented an innovative\nparadigm that incorporates three separate forms of curriculum learning\nspecifically targeting from spatial, temporal, and quantile perspectives.\nFurthermore, our framework incorporates a stacking fusion module to combine\ndiverse information from three types of curriculum learning, resulting in a\nstrong and thorough learning process. We demonstrated the effectiveness of this\nframework with extensive empirical evaluations, highlighting its better\nperformance in addressing complex ST challenges. We provided thorough ablation\nstudies to investigate the effectiveness of our curriculum and to explain how\nit contributes to the improvement of learning efficiency on ST data.\n", "link": "http://arxiv.org/abs/2406.12709v2", "date": "2024-09-16", "relevancy": 2.4013, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4872}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4836}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned&body=Title%3A%20Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned%0AAuthor%3A%20Du%20Yin%20and%20Jinliang%20Deng%20and%20Shuang%20Ao%20and%20Zechen%20Li%20and%20Hao%20Xue%20and%20Arian%20Prabowo%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Flora%20Salim%0AAbstract%3A%20%20%20Training%20models%20on%20spatio-temporal%20%28ST%29%20data%20poses%20an%20open%20problem%20due%20to%20the%0Acomplicated%20and%20diverse%20nature%20of%20the%20data%20itself%2C%20and%20it%20is%20challenging%20to%0Aensure%20the%20model%27s%20performance%20directly%20trained%20on%20the%20original%20ST%20data.%20While%0Alimiting%20the%20variety%20of%20training%20data%20can%20make%20training%20easier%2C%20it%20can%20also%0Alead%20to%20a%20lack%20of%20knowledge%20and%20information%20for%20the%20model%2C%20resulting%20in%20a%0Adecrease%20in%20performance.%20To%20address%20this%20challenge%2C%20we%20presented%20an%20innovative%0Aparadigm%20that%20incorporates%20three%20separate%20forms%20of%20curriculum%20learning%0Aspecifically%20targeting%20from%20spatial%2C%20temporal%2C%20and%20quantile%20perspectives.%0AFurthermore%2C%20our%20framework%20incorporates%20a%20stacking%20fusion%20module%20to%20combine%0Adiverse%20information%20from%20three%20types%20of%20curriculum%20learning%2C%20resulting%20in%20a%0Astrong%20and%20thorough%20learning%20process.%20We%20demonstrated%20the%20effectiveness%20of%20this%0Aframework%20with%20extensive%20empirical%20evaluations%2C%20highlighting%20its%20better%0Aperformance%20in%20addressing%20complex%20ST%20challenges.%20We%20provided%20thorough%20ablation%0Astudies%20to%20investigate%20the%20effectiveness%20of%20our%20curriculum%20and%20to%20explain%20how%0Ait%20contributes%20to%20the%20improvement%20of%20learning%20efficiency%20on%20ST%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Spatio-temporal%2520Quantile%2520Forecasting%2520with%2520Curriculum%2520Learning%253A%250A%2520%2520Lessons%2520Learned%26entry.906535625%3DDu%2520Yin%2520and%2520Jinliang%2520Deng%2520and%2520Shuang%2520Ao%2520and%2520Zechen%2520Li%2520and%2520Hao%2520Xue%2520and%2520Arian%2520Prabowo%2520and%2520Renhe%2520Jiang%2520and%2520Xuan%2520Song%2520and%2520Flora%2520Salim%26entry.1292438233%3D%2520%2520Training%2520models%2520on%2520spatio-temporal%2520%2528ST%2529%2520data%2520poses%2520an%2520open%2520problem%2520due%2520to%2520the%250Acomplicated%2520and%2520diverse%2520nature%2520of%2520the%2520data%2520itself%252C%2520and%2520it%2520is%2520challenging%2520to%250Aensure%2520the%2520model%2527s%2520performance%2520directly%2520trained%2520on%2520the%2520original%2520ST%2520data.%2520While%250Alimiting%2520the%2520variety%2520of%2520training%2520data%2520can%2520make%2520training%2520easier%252C%2520it%2520can%2520also%250Alead%2520to%2520a%2520lack%2520of%2520knowledge%2520and%2520information%2520for%2520the%2520model%252C%2520resulting%2520in%2520a%250Adecrease%2520in%2520performance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520presented%2520an%2520innovative%250Aparadigm%2520that%2520incorporates%2520three%2520separate%2520forms%2520of%2520curriculum%2520learning%250Aspecifically%2520targeting%2520from%2520spatial%252C%2520temporal%252C%2520and%2520quantile%2520perspectives.%250AFurthermore%252C%2520our%2520framework%2520incorporates%2520a%2520stacking%2520fusion%2520module%2520to%2520combine%250Adiverse%2520information%2520from%2520three%2520types%2520of%2520curriculum%2520learning%252C%2520resulting%2520in%2520a%250Astrong%2520and%2520thorough%2520learning%2520process.%2520We%2520demonstrated%2520the%2520effectiveness%2520of%2520this%250Aframework%2520with%2520extensive%2520empirical%2520evaluations%252C%2520highlighting%2520its%2520better%250Aperformance%2520in%2520addressing%2520complex%2520ST%2520challenges.%2520We%2520provided%2520thorough%2520ablation%250Astudies%2520to%2520investigate%2520the%2520effectiveness%2520of%2520our%2520curriculum%2520and%2520to%2520explain%2520how%250Ait%2520contributes%2520to%2520the%2520improvement%2520of%2520learning%2520efficiency%2520on%2520ST%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Spatio-temporal%20Quantile%20Forecasting%20with%20Curriculum%20Learning%3A%0A%20%20Lessons%20Learned&entry.906535625=Du%20Yin%20and%20Jinliang%20Deng%20and%20Shuang%20Ao%20and%20Zechen%20Li%20and%20Hao%20Xue%20and%20Arian%20Prabowo%20and%20Renhe%20Jiang%20and%20Xuan%20Song%20and%20Flora%20Salim&entry.1292438233=%20%20Training%20models%20on%20spatio-temporal%20%28ST%29%20data%20poses%20an%20open%20problem%20due%20to%20the%0Acomplicated%20and%20diverse%20nature%20of%20the%20data%20itself%2C%20and%20it%20is%20challenging%20to%0Aensure%20the%20model%27s%20performance%20directly%20trained%20on%20the%20original%20ST%20data.%20While%0Alimiting%20the%20variety%20of%20training%20data%20can%20make%20training%20easier%2C%20it%20can%20also%0Alead%20to%20a%20lack%20of%20knowledge%20and%20information%20for%20the%20model%2C%20resulting%20in%20a%0Adecrease%20in%20performance.%20To%20address%20this%20challenge%2C%20we%20presented%20an%20innovative%0Aparadigm%20that%20incorporates%20three%20separate%20forms%20of%20curriculum%20learning%0Aspecifically%20targeting%20from%20spatial%2C%20temporal%2C%20and%20quantile%20perspectives.%0AFurthermore%2C%20our%20framework%20incorporates%20a%20stacking%20fusion%20module%20to%20combine%0Adiverse%20information%20from%20three%20types%20of%20curriculum%20learning%2C%20resulting%20in%20a%0Astrong%20and%20thorough%20learning%20process.%20We%20demonstrated%20the%20effectiveness%20of%20this%0Aframework%20with%20extensive%20empirical%20evaluations%2C%20highlighting%20its%20better%0Aperformance%20in%20addressing%20complex%20ST%20challenges.%20We%20provided%20thorough%20ablation%0Astudies%20to%20investigate%20the%20effectiveness%20of%20our%20curriculum%20and%20to%20explain%20how%0Ait%20contributes%20to%20the%20improvement%20of%20learning%20efficiency%20on%20ST%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12709v2&entry.124074799=Read"},
{"title": "RealDiff: Real-world 3D Shape Completion using Self-Supervised Diffusion\n  Models", "author": "Ba\u015fak Melis \u00d6cal and Maxim Tatarchenko and Sezer Karaoglu and Theo Gevers", "abstract": "  Point cloud completion aims to recover the complete 3D shape of an object\nfrom partial observations. While approaches relying on synthetic shape priors\nachieved promising results in this domain, their applicability and\ngeneralizability to real-world data are still limited. To tackle this problem,\nwe propose a self-supervised framework, namely RealDiff, that formulates point\ncloud completion as a conditional generation problem directly on real-world\nmeasurements. To better deal with noisy observations without resorting to\ntraining on synthetic data, we leverage additional geometric cues.\nSpecifically, RealDiff simulates a diffusion process at the missing object\nparts while conditioning the generation on the partial input to address the\nmultimodal nature of the task. We further regularize the training by matching\nobject silhouettes and depth maps, predicted by our method, with the externally\nestimated ones. Experimental results show that our method consistently\noutperforms state-of-the-art methods in real-world point cloud completion.\n", "link": "http://arxiv.org/abs/2409.10180v1", "date": "2024-09-16", "relevancy": 2.401, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6023}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6023}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealDiff%3A%20Real-world%203D%20Shape%20Completion%20using%20Self-Supervised%20Diffusion%0A%20%20Models&body=Title%3A%20RealDiff%3A%20Real-world%203D%20Shape%20Completion%20using%20Self-Supervised%20Diffusion%0A%20%20Models%0AAuthor%3A%20Ba%C5%9Fak%20Melis%20%C3%96cal%20and%20Maxim%20Tatarchenko%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20recover%20the%20complete%203D%20shape%20of%20an%20object%0Afrom%20partial%20observations.%20While%20approaches%20relying%20on%20synthetic%20shape%20priors%0Aachieved%20promising%20results%20in%20this%20domain%2C%20their%20applicability%20and%0Ageneralizability%20to%20real-world%20data%20are%20still%20limited.%20To%20tackle%20this%20problem%2C%0Awe%20propose%20a%20self-supervised%20framework%2C%20namely%20RealDiff%2C%20that%20formulates%20point%0Acloud%20completion%20as%20a%20conditional%20generation%20problem%20directly%20on%20real-world%0Ameasurements.%20To%20better%20deal%20with%20noisy%20observations%20without%20resorting%20to%0Atraining%20on%20synthetic%20data%2C%20we%20leverage%20additional%20geometric%20cues.%0ASpecifically%2C%20RealDiff%20simulates%20a%20diffusion%20process%20at%20the%20missing%20object%0Aparts%20while%20conditioning%20the%20generation%20on%20the%20partial%20input%20to%20address%20the%0Amultimodal%20nature%20of%20the%20task.%20We%20further%20regularize%20the%20training%20by%20matching%0Aobject%20silhouettes%20and%20depth%20maps%2C%20predicted%20by%20our%20method%2C%20with%20the%20externally%0Aestimated%20ones.%20Experimental%20results%20show%20that%20our%20method%20consistently%0Aoutperforms%20state-of-the-art%20methods%20in%20real-world%20point%20cloud%20completion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealDiff%253A%2520Real-world%25203D%2520Shape%2520Completion%2520using%2520Self-Supervised%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DBa%25C5%259Fak%2520Melis%2520%25C3%2596cal%2520and%2520Maxim%2520Tatarchenko%2520and%2520Sezer%2520Karaoglu%2520and%2520Theo%2520Gevers%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520aims%2520to%2520recover%2520the%2520complete%25203D%2520shape%2520of%2520an%2520object%250Afrom%2520partial%2520observations.%2520While%2520approaches%2520relying%2520on%2520synthetic%2520shape%2520priors%250Aachieved%2520promising%2520results%2520in%2520this%2520domain%252C%2520their%2520applicability%2520and%250Ageneralizability%2520to%2520real-world%2520data%2520are%2520still%2520limited.%2520To%2520tackle%2520this%2520problem%252C%250Awe%2520propose%2520a%2520self-supervised%2520framework%252C%2520namely%2520RealDiff%252C%2520that%2520formulates%2520point%250Acloud%2520completion%2520as%2520a%2520conditional%2520generation%2520problem%2520directly%2520on%2520real-world%250Ameasurements.%2520To%2520better%2520deal%2520with%2520noisy%2520observations%2520without%2520resorting%2520to%250Atraining%2520on%2520synthetic%2520data%252C%2520we%2520leverage%2520additional%2520geometric%2520cues.%250ASpecifically%252C%2520RealDiff%2520simulates%2520a%2520diffusion%2520process%2520at%2520the%2520missing%2520object%250Aparts%2520while%2520conditioning%2520the%2520generation%2520on%2520the%2520partial%2520input%2520to%2520address%2520the%250Amultimodal%2520nature%2520of%2520the%2520task.%2520We%2520further%2520regularize%2520the%2520training%2520by%2520matching%250Aobject%2520silhouettes%2520and%2520depth%2520maps%252C%2520predicted%2520by%2520our%2520method%252C%2520with%2520the%2520externally%250Aestimated%2520ones.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520real-world%2520point%2520cloud%2520completion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealDiff%3A%20Real-world%203D%20Shape%20Completion%20using%20Self-Supervised%20Diffusion%0A%20%20Models&entry.906535625=Ba%C5%9Fak%20Melis%20%C3%96cal%20and%20Maxim%20Tatarchenko%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20recover%20the%20complete%203D%20shape%20of%20an%20object%0Afrom%20partial%20observations.%20While%20approaches%20relying%20on%20synthetic%20shape%20priors%0Aachieved%20promising%20results%20in%20this%20domain%2C%20their%20applicability%20and%0Ageneralizability%20to%20real-world%20data%20are%20still%20limited.%20To%20tackle%20this%20problem%2C%0Awe%20propose%20a%20self-supervised%20framework%2C%20namely%20RealDiff%2C%20that%20formulates%20point%0Acloud%20completion%20as%20a%20conditional%20generation%20problem%20directly%20on%20real-world%0Ameasurements.%20To%20better%20deal%20with%20noisy%20observations%20without%20resorting%20to%0Atraining%20on%20synthetic%20data%2C%20we%20leverage%20additional%20geometric%20cues.%0ASpecifically%2C%20RealDiff%20simulates%20a%20diffusion%20process%20at%20the%20missing%20object%0Aparts%20while%20conditioning%20the%20generation%20on%20the%20partial%20input%20to%20address%20the%0Amultimodal%20nature%20of%20the%20task.%20We%20further%20regularize%20the%20training%20by%20matching%0Aobject%20silhouettes%20and%20depth%20maps%2C%20predicted%20by%20our%20method%2C%20with%20the%20externally%0Aestimated%20ones.%20Experimental%20results%20show%20that%20our%20method%20consistently%0Aoutperforms%20state-of-the-art%20methods%20in%20real-world%20point%20cloud%20completion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10180v1&entry.124074799=Read"},
{"title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog\n  Systems", "author": "Hitesh Tulsiani and David M. Chan and Shalini Ghosh and Garima Lalwani and Prabhat Pandey and Ankish Bansal and Sri Garimella and Ariya Rastrow and Bj\u00f6rn Hoffmeister", "abstract": "  Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.\n", "link": "http://arxiv.org/abs/2409.10515v1", "date": "2024-09-16", "relevancy": 2.387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Self-Learning%20Framework%20For%20Interactive%20Spoken%20Dialog%0A%20%20Systems&body=Title%3A%20An%20Efficient%20Self-Learning%20Framework%20For%20Interactive%20Spoken%20Dialog%0A%20%20Systems%0AAuthor%3A%20Hitesh%20Tulsiani%20and%20David%20M.%20Chan%20and%20Shalini%20Ghosh%20and%20Garima%20Lalwani%20and%20Prabhat%20Pandey%20and%20Ankish%20Bansal%20and%20Sri%20Garimella%20and%20Ariya%20Rastrow%20and%20Bj%C3%B6rn%20Hoffmeister%0AAbstract%3A%20%20%20Dialog%20systems%2C%20such%20as%20voice%20assistants%2C%20are%20expected%20to%20engage%20with%20users%0Ain%20complex%2C%20evolving%20conversations.%20Unfortunately%2C%20traditional%20automatic%20speech%0Arecognition%20%28ASR%29%20systems%20deployed%20in%20such%20applications%20are%20usually%20trained%20to%0Arecognize%20each%20turn%20independently%20and%20lack%20the%20ability%20to%20adapt%20to%20the%0Aconversational%20context%20or%20incorporate%20user%20feedback.%20In%20this%20work%2C%20we%20introduce%0Aa%20general%20framework%20for%20ASR%20in%20dialog%20systems%20that%20can%20go%20beyond%20learning%20from%0Asingle-turn%20utterances%20and%20learn%20over%20time%20how%20to%20adapt%20to%20both%20explicit%0Asupervision%20and%20implicit%20user%20feedback%20present%20in%20multi-turn%20conversations.%20We%0Aaccomplish%20that%20by%20leveraging%20advances%20in%20student-teacher%20learning%20and%0Acontext-aware%20dialog%20processing%2C%20and%20designing%20contrastive%20self-supervision%0Aapproaches%20with%20Ohm%2C%20a%20new%20online%20hard-negative%20mining%20approach.%20We%20show%20that%0Aleveraging%20our%20new%20framework%20compared%20to%20traditional%20training%20leads%20to%20relative%0AWER%20reductions%20of%20close%20to%2010%25%20in%20real-world%20dialog%20systems%2C%20and%20up%20to%2026%25%20on%0Apublic%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Self-Learning%2520Framework%2520For%2520Interactive%2520Spoken%2520Dialog%250A%2520%2520Systems%26entry.906535625%3DHitesh%2520Tulsiani%2520and%2520David%2520M.%2520Chan%2520and%2520Shalini%2520Ghosh%2520and%2520Garima%2520Lalwani%2520and%2520Prabhat%2520Pandey%2520and%2520Ankish%2520Bansal%2520and%2520Sri%2520Garimella%2520and%2520Ariya%2520Rastrow%2520and%2520Bj%25C3%25B6rn%2520Hoffmeister%26entry.1292438233%3D%2520%2520Dialog%2520systems%252C%2520such%2520as%2520voice%2520assistants%252C%2520are%2520expected%2520to%2520engage%2520with%2520users%250Ain%2520complex%252C%2520evolving%2520conversations.%2520Unfortunately%252C%2520traditional%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520systems%2520deployed%2520in%2520such%2520applications%2520are%2520usually%2520trained%2520to%250Arecognize%2520each%2520turn%2520independently%2520and%2520lack%2520the%2520ability%2520to%2520adapt%2520to%2520the%250Aconversational%2520context%2520or%2520incorporate%2520user%2520feedback.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aa%2520general%2520framework%2520for%2520ASR%2520in%2520dialog%2520systems%2520that%2520can%2520go%2520beyond%2520learning%2520from%250Asingle-turn%2520utterances%2520and%2520learn%2520over%2520time%2520how%2520to%2520adapt%2520to%2520both%2520explicit%250Asupervision%2520and%2520implicit%2520user%2520feedback%2520present%2520in%2520multi-turn%2520conversations.%2520We%250Aaccomplish%2520that%2520by%2520leveraging%2520advances%2520in%2520student-teacher%2520learning%2520and%250Acontext-aware%2520dialog%2520processing%252C%2520and%2520designing%2520contrastive%2520self-supervision%250Aapproaches%2520with%2520Ohm%252C%2520a%2520new%2520online%2520hard-negative%2520mining%2520approach.%2520We%2520show%2520that%250Aleveraging%2520our%2520new%2520framework%2520compared%2520to%2520traditional%2520training%2520leads%2520to%2520relative%250AWER%2520reductions%2520of%2520close%2520to%252010%2525%2520in%2520real-world%2520dialog%2520systems%252C%2520and%2520up%2520to%252026%2525%2520on%250Apublic%2520synthetic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Self-Learning%20Framework%20For%20Interactive%20Spoken%20Dialog%0A%20%20Systems&entry.906535625=Hitesh%20Tulsiani%20and%20David%20M.%20Chan%20and%20Shalini%20Ghosh%20and%20Garima%20Lalwani%20and%20Prabhat%20Pandey%20and%20Ankish%20Bansal%20and%20Sri%20Garimella%20and%20Ariya%20Rastrow%20and%20Bj%C3%B6rn%20Hoffmeister&entry.1292438233=%20%20Dialog%20systems%2C%20such%20as%20voice%20assistants%2C%20are%20expected%20to%20engage%20with%20users%0Ain%20complex%2C%20evolving%20conversations.%20Unfortunately%2C%20traditional%20automatic%20speech%0Arecognition%20%28ASR%29%20systems%20deployed%20in%20such%20applications%20are%20usually%20trained%20to%0Arecognize%20each%20turn%20independently%20and%20lack%20the%20ability%20to%20adapt%20to%20the%0Aconversational%20context%20or%20incorporate%20user%20feedback.%20In%20this%20work%2C%20we%20introduce%0Aa%20general%20framework%20for%20ASR%20in%20dialog%20systems%20that%20can%20go%20beyond%20learning%20from%0Asingle-turn%20utterances%20and%20learn%20over%20time%20how%20to%20adapt%20to%20both%20explicit%0Asupervision%20and%20implicit%20user%20feedback%20present%20in%20multi-turn%20conversations.%20We%0Aaccomplish%20that%20by%20leveraging%20advances%20in%20student-teacher%20learning%20and%0Acontext-aware%20dialog%20processing%2C%20and%20designing%20contrastive%20self-supervision%0Aapproaches%20with%20Ohm%2C%20a%20new%20online%20hard-negative%20mining%20approach.%20We%20show%20that%0Aleveraging%20our%20new%20framework%20compared%20to%20traditional%20training%20leads%20to%20relative%0AWER%20reductions%20of%20close%20to%2010%25%20in%20real-world%20dialog%20systems%2C%20and%20up%20to%2026%25%20on%0Apublic%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10515v1&entry.124074799=Read"},
{"title": "Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene\n  Graph for Robot Navigation", "author": "Yifan Xu and Ziming Luo and Qianwei Wang and Vineet Kamat and Carol Menassa", "abstract": "  Current open-vocabulary scene graph generation algorithms highly rely on both\n3D scene point cloud data and posed RGB-D images and thus have limited\napplications in scenarios where RGB-D images or camera poses are not readily\navailable. To solve this problem, we propose Point2Graph, a novel end-to-end\npoint cloud-based 3D open-vocabulary scene graph generation framework in which\nthe requirement of posed RGB-D image series is eliminated. This hierarchical\nframework contains room and object detection/segmentation and open-vocabulary\nclassification. For the room layer, we leverage the advantage of merging the\ngeometry-based border detection algorithm with the learning-based region\ndetection to segment rooms and create a \"Snap-Lookup\" framework for\nopen-vocabulary room classification. In addition, we create an end-to-end\npipeline for the object layer to detect and classify 3D objects based solely on\n3D point cloud data. Our evaluation results show that our framework can\noutperform the current state-of-the-art (SOTA) open-vocabulary object and room\nsegmentation and classification algorithm on widely used real-scene datasets.\n", "link": "http://arxiv.org/abs/2409.10350v1", "date": "2024-09-16", "relevancy": 2.3651, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point2Graph%3A%20An%20End-to-end%20Point%20Cloud-based%203D%20Open-Vocabulary%20Scene%0A%20%20Graph%20for%20Robot%20Navigation&body=Title%3A%20Point2Graph%3A%20An%20End-to-end%20Point%20Cloud-based%203D%20Open-Vocabulary%20Scene%0A%20%20Graph%20for%20Robot%20Navigation%0AAuthor%3A%20Yifan%20Xu%20and%20Ziming%20Luo%20and%20Qianwei%20Wang%20and%20Vineet%20Kamat%20and%20Carol%20Menassa%0AAbstract%3A%20%20%20Current%20open-vocabulary%20scene%20graph%20generation%20algorithms%20highly%20rely%20on%20both%0A3D%20scene%20point%20cloud%20data%20and%20posed%20RGB-D%20images%20and%20thus%20have%20limited%0Aapplications%20in%20scenarios%20where%20RGB-D%20images%20or%20camera%20poses%20are%20not%20readily%0Aavailable.%20To%20solve%20this%20problem%2C%20we%20propose%20Point2Graph%2C%20a%20novel%20end-to-end%0Apoint%20cloud-based%203D%20open-vocabulary%20scene%20graph%20generation%20framework%20in%20which%0Athe%20requirement%20of%20posed%20RGB-D%20image%20series%20is%20eliminated.%20This%20hierarchical%0Aframework%20contains%20room%20and%20object%20detection/segmentation%20and%20open-vocabulary%0Aclassification.%20For%20the%20room%20layer%2C%20we%20leverage%20the%20advantage%20of%20merging%20the%0Ageometry-based%20border%20detection%20algorithm%20with%20the%20learning-based%20region%0Adetection%20to%20segment%20rooms%20and%20create%20a%20%22Snap-Lookup%22%20framework%20for%0Aopen-vocabulary%20room%20classification.%20In%20addition%2C%20we%20create%20an%20end-to-end%0Apipeline%20for%20the%20object%20layer%20to%20detect%20and%20classify%203D%20objects%20based%20solely%20on%0A3D%20point%20cloud%20data.%20Our%20evaluation%20results%20show%20that%20our%20framework%20can%0Aoutperform%20the%20current%20state-of-the-art%20%28SOTA%29%20open-vocabulary%20object%20and%20room%0Asegmentation%20and%20classification%20algorithm%20on%20widely%20used%20real-scene%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint2Graph%253A%2520An%2520End-to-end%2520Point%2520Cloud-based%25203D%2520Open-Vocabulary%2520Scene%250A%2520%2520Graph%2520for%2520Robot%2520Navigation%26entry.906535625%3DYifan%2520Xu%2520and%2520Ziming%2520Luo%2520and%2520Qianwei%2520Wang%2520and%2520Vineet%2520Kamat%2520and%2520Carol%2520Menassa%26entry.1292438233%3D%2520%2520Current%2520open-vocabulary%2520scene%2520graph%2520generation%2520algorithms%2520highly%2520rely%2520on%2520both%250A3D%2520scene%2520point%2520cloud%2520data%2520and%2520posed%2520RGB-D%2520images%2520and%2520thus%2520have%2520limited%250Aapplications%2520in%2520scenarios%2520where%2520RGB-D%2520images%2520or%2520camera%2520poses%2520are%2520not%2520readily%250Aavailable.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520Point2Graph%252C%2520a%2520novel%2520end-to-end%250Apoint%2520cloud-based%25203D%2520open-vocabulary%2520scene%2520graph%2520generation%2520framework%2520in%2520which%250Athe%2520requirement%2520of%2520posed%2520RGB-D%2520image%2520series%2520is%2520eliminated.%2520This%2520hierarchical%250Aframework%2520contains%2520room%2520and%2520object%2520detection/segmentation%2520and%2520open-vocabulary%250Aclassification.%2520For%2520the%2520room%2520layer%252C%2520we%2520leverage%2520the%2520advantage%2520of%2520merging%2520the%250Ageometry-based%2520border%2520detection%2520algorithm%2520with%2520the%2520learning-based%2520region%250Adetection%2520to%2520segment%2520rooms%2520and%2520create%2520a%2520%2522Snap-Lookup%2522%2520framework%2520for%250Aopen-vocabulary%2520room%2520classification.%2520In%2520addition%252C%2520we%2520create%2520an%2520end-to-end%250Apipeline%2520for%2520the%2520object%2520layer%2520to%2520detect%2520and%2520classify%25203D%2520objects%2520based%2520solely%2520on%250A3D%2520point%2520cloud%2520data.%2520Our%2520evaluation%2520results%2520show%2520that%2520our%2520framework%2520can%250Aoutperform%2520the%2520current%2520state-of-the-art%2520%2528SOTA%2529%2520open-vocabulary%2520object%2520and%2520room%250Asegmentation%2520and%2520classification%2520algorithm%2520on%2520widely%2520used%2520real-scene%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point2Graph%3A%20An%20End-to-end%20Point%20Cloud-based%203D%20Open-Vocabulary%20Scene%0A%20%20Graph%20for%20Robot%20Navigation&entry.906535625=Yifan%20Xu%20and%20Ziming%20Luo%20and%20Qianwei%20Wang%20and%20Vineet%20Kamat%20and%20Carol%20Menassa&entry.1292438233=%20%20Current%20open-vocabulary%20scene%20graph%20generation%20algorithms%20highly%20rely%20on%20both%0A3D%20scene%20point%20cloud%20data%20and%20posed%20RGB-D%20images%20and%20thus%20have%20limited%0Aapplications%20in%20scenarios%20where%20RGB-D%20images%20or%20camera%20poses%20are%20not%20readily%0Aavailable.%20To%20solve%20this%20problem%2C%20we%20propose%20Point2Graph%2C%20a%20novel%20end-to-end%0Apoint%20cloud-based%203D%20open-vocabulary%20scene%20graph%20generation%20framework%20in%20which%0Athe%20requirement%20of%20posed%20RGB-D%20image%20series%20is%20eliminated.%20This%20hierarchical%0Aframework%20contains%20room%20and%20object%20detection/segmentation%20and%20open-vocabulary%0Aclassification.%20For%20the%20room%20layer%2C%20we%20leverage%20the%20advantage%20of%20merging%20the%0Ageometry-based%20border%20detection%20algorithm%20with%20the%20learning-based%20region%0Adetection%20to%20segment%20rooms%20and%20create%20a%20%22Snap-Lookup%22%20framework%20for%0Aopen-vocabulary%20room%20classification.%20In%20addition%2C%20we%20create%20an%20end-to-end%0Apipeline%20for%20the%20object%20layer%20to%20detect%20and%20classify%203D%20objects%20based%20solely%20on%0A3D%20point%20cloud%20data.%20Our%20evaluation%20results%20show%20that%20our%20framework%20can%0Aoutperform%20the%20current%20state-of-the-art%20%28SOTA%29%20open-vocabulary%20object%20and%20room%0Asegmentation%20and%20classification%20algorithm%20on%20widely%20used%20real-scene%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10350v1&entry.124074799=Read"},
{"title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone\n  Navigation via Scene-Aware Control Barrier Functions", "author": "Sourav Sanyal and Kaushik Roy", "abstract": "  In the rapidly evolving field of vision-language navigation (VLN), ensuring\nrobust safety mechanisms remains an open challenge. Control barrier functions\n(CBFs) are efficient tools which guarantee safety by solving an optimal control\nproblem. In this work, we consider the case of a teleoperated drone in a VLN\nsetting, and add safety features by formulating a novel scene-aware CBF using\nego-centric observations obtained through an RGB-D sensor. As a baseline, we\nimplement a vision-language understanding module which uses the contrastive\nlanguage image pretraining (CLIP) model to query about a user-specified (in\nnatural language) landmark. Using the YOLO (You Only Look Once) object\ndetector, the CLIP model is queried for verifying the cropped landmark,\ntriggering downstream navigation. To improve navigation safety of the baseline,\nwe propose ASMA -- an Adaptive Safety Margin Algorithm -- that crops the\ndrone's depth map for tracking moving object(s) to perform scene-aware CBF\nevaluation on-the-fly. By identifying potential risky observations from the\nscene, ASMA enables real-time adaptation to unpredictable environmental\nconditions, ensuring optimal safety bounds on a VLN-powered drone actions.\nUsing the robot operating system (ROS) middleware on a parrot bebop2 quadrotor\nin the gazebo environment, ASMA offers 59.4% - 61.8% increase in success rates\nwith insignificant 5.4% - 8.2% increases in trajectory lengths compared to the\nbaseline CBF-less VLN while recovering from unsafe situations.\n", "link": "http://arxiv.org/abs/2409.10283v1", "date": "2024-09-16", "relevancy": 2.35, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5964}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5815}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASMA%3A%20An%20Adaptive%20Safety%20Margin%20Algorithm%20for%20Vision-Language%20Drone%0A%20%20Navigation%20via%20Scene-Aware%20Control%20Barrier%20Functions&body=Title%3A%20ASMA%3A%20An%20Adaptive%20Safety%20Margin%20Algorithm%20for%20Vision-Language%20Drone%0A%20%20Navigation%20via%20Scene-Aware%20Control%20Barrier%20Functions%0AAuthor%3A%20Sourav%20Sanyal%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20vision-language%20navigation%20%28VLN%29%2C%20ensuring%0Arobust%20safety%20mechanisms%20remains%20an%20open%20challenge.%20Control%20barrier%20functions%0A%28CBFs%29%20are%20efficient%20tools%20which%20guarantee%20safety%20by%20solving%20an%20optimal%20control%0Aproblem.%20In%20this%20work%2C%20we%20consider%20the%20case%20of%20a%20teleoperated%20drone%20in%20a%20VLN%0Asetting%2C%20and%20add%20safety%20features%20by%20formulating%20a%20novel%20scene-aware%20CBF%20using%0Aego-centric%20observations%20obtained%20through%20an%20RGB-D%20sensor.%20As%20a%20baseline%2C%20we%0Aimplement%20a%20vision-language%20understanding%20module%20which%20uses%20the%20contrastive%0Alanguage%20image%20pretraining%20%28CLIP%29%20model%20to%20query%20about%20a%20user-specified%20%28in%0Anatural%20language%29%20landmark.%20Using%20the%20YOLO%20%28You%20Only%20Look%20Once%29%20object%0Adetector%2C%20the%20CLIP%20model%20is%20queried%20for%20verifying%20the%20cropped%20landmark%2C%0Atriggering%20downstream%20navigation.%20To%20improve%20navigation%20safety%20of%20the%20baseline%2C%0Awe%20propose%20ASMA%20--%20an%20Adaptive%20Safety%20Margin%20Algorithm%20--%20that%20crops%20the%0Adrone%27s%20depth%20map%20for%20tracking%20moving%20object%28s%29%20to%20perform%20scene-aware%20CBF%0Aevaluation%20on-the-fly.%20By%20identifying%20potential%20risky%20observations%20from%20the%0Ascene%2C%20ASMA%20enables%20real-time%20adaptation%20to%20unpredictable%20environmental%0Aconditions%2C%20ensuring%20optimal%20safety%20bounds%20on%20a%20VLN-powered%20drone%20actions.%0AUsing%20the%20robot%20operating%20system%20%28ROS%29%20middleware%20on%20a%20parrot%20bebop2%20quadrotor%0Ain%20the%20gazebo%20environment%2C%20ASMA%20offers%2059.4%25%20-%2061.8%25%20increase%20in%20success%20rates%0Awith%20insignificant%205.4%25%20-%208.2%25%20increases%20in%20trajectory%20lengths%20compared%20to%20the%0Abaseline%20CBF-less%20VLN%20while%20recovering%20from%20unsafe%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASMA%253A%2520An%2520Adaptive%2520Safety%2520Margin%2520Algorithm%2520for%2520Vision-Language%2520Drone%250A%2520%2520Navigation%2520via%2520Scene-Aware%2520Control%2520Barrier%2520Functions%26entry.906535625%3DSourav%2520Sanyal%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520vision-language%2520navigation%2520%2528VLN%2529%252C%2520ensuring%250Arobust%2520safety%2520mechanisms%2520remains%2520an%2520open%2520challenge.%2520Control%2520barrier%2520functions%250A%2528CBFs%2529%2520are%2520efficient%2520tools%2520which%2520guarantee%2520safety%2520by%2520solving%2520an%2520optimal%2520control%250Aproblem.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520case%2520of%2520a%2520teleoperated%2520drone%2520in%2520a%2520VLN%250Asetting%252C%2520and%2520add%2520safety%2520features%2520by%2520formulating%2520a%2520novel%2520scene-aware%2520CBF%2520using%250Aego-centric%2520observations%2520obtained%2520through%2520an%2520RGB-D%2520sensor.%2520As%2520a%2520baseline%252C%2520we%250Aimplement%2520a%2520vision-language%2520understanding%2520module%2520which%2520uses%2520the%2520contrastive%250Alanguage%2520image%2520pretraining%2520%2528CLIP%2529%2520model%2520to%2520query%2520about%2520a%2520user-specified%2520%2528in%250Anatural%2520language%2529%2520landmark.%2520Using%2520the%2520YOLO%2520%2528You%2520Only%2520Look%2520Once%2529%2520object%250Adetector%252C%2520the%2520CLIP%2520model%2520is%2520queried%2520for%2520verifying%2520the%2520cropped%2520landmark%252C%250Atriggering%2520downstream%2520navigation.%2520To%2520improve%2520navigation%2520safety%2520of%2520the%2520baseline%252C%250Awe%2520propose%2520ASMA%2520--%2520an%2520Adaptive%2520Safety%2520Margin%2520Algorithm%2520--%2520that%2520crops%2520the%250Adrone%2527s%2520depth%2520map%2520for%2520tracking%2520moving%2520object%2528s%2529%2520to%2520perform%2520scene-aware%2520CBF%250Aevaluation%2520on-the-fly.%2520By%2520identifying%2520potential%2520risky%2520observations%2520from%2520the%250Ascene%252C%2520ASMA%2520enables%2520real-time%2520adaptation%2520to%2520unpredictable%2520environmental%250Aconditions%252C%2520ensuring%2520optimal%2520safety%2520bounds%2520on%2520a%2520VLN-powered%2520drone%2520actions.%250AUsing%2520the%2520robot%2520operating%2520system%2520%2528ROS%2529%2520middleware%2520on%2520a%2520parrot%2520bebop2%2520quadrotor%250Ain%2520the%2520gazebo%2520environment%252C%2520ASMA%2520offers%252059.4%2525%2520-%252061.8%2525%2520increase%2520in%2520success%2520rates%250Awith%2520insignificant%25205.4%2525%2520-%25208.2%2525%2520increases%2520in%2520trajectory%2520lengths%2520compared%2520to%2520the%250Abaseline%2520CBF-less%2520VLN%2520while%2520recovering%2520from%2520unsafe%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASMA%3A%20An%20Adaptive%20Safety%20Margin%20Algorithm%20for%20Vision-Language%20Drone%0A%20%20Navigation%20via%20Scene-Aware%20Control%20Barrier%20Functions&entry.906535625=Sourav%20Sanyal%20and%20Kaushik%20Roy&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20vision-language%20navigation%20%28VLN%29%2C%20ensuring%0Arobust%20safety%20mechanisms%20remains%20an%20open%20challenge.%20Control%20barrier%20functions%0A%28CBFs%29%20are%20efficient%20tools%20which%20guarantee%20safety%20by%20solving%20an%20optimal%20control%0Aproblem.%20In%20this%20work%2C%20we%20consider%20the%20case%20of%20a%20teleoperated%20drone%20in%20a%20VLN%0Asetting%2C%20and%20add%20safety%20features%20by%20formulating%20a%20novel%20scene-aware%20CBF%20using%0Aego-centric%20observations%20obtained%20through%20an%20RGB-D%20sensor.%20As%20a%20baseline%2C%20we%0Aimplement%20a%20vision-language%20understanding%20module%20which%20uses%20the%20contrastive%0Alanguage%20image%20pretraining%20%28CLIP%29%20model%20to%20query%20about%20a%20user-specified%20%28in%0Anatural%20language%29%20landmark.%20Using%20the%20YOLO%20%28You%20Only%20Look%20Once%29%20object%0Adetector%2C%20the%20CLIP%20model%20is%20queried%20for%20verifying%20the%20cropped%20landmark%2C%0Atriggering%20downstream%20navigation.%20To%20improve%20navigation%20safety%20of%20the%20baseline%2C%0Awe%20propose%20ASMA%20--%20an%20Adaptive%20Safety%20Margin%20Algorithm%20--%20that%20crops%20the%0Adrone%27s%20depth%20map%20for%20tracking%20moving%20object%28s%29%20to%20perform%20scene-aware%20CBF%0Aevaluation%20on-the-fly.%20By%20identifying%20potential%20risky%20observations%20from%20the%0Ascene%2C%20ASMA%20enables%20real-time%20adaptation%20to%20unpredictable%20environmental%0Aconditions%2C%20ensuring%20optimal%20safety%20bounds%20on%20a%20VLN-powered%20drone%20actions.%0AUsing%20the%20robot%20operating%20system%20%28ROS%29%20middleware%20on%20a%20parrot%20bebop2%20quadrotor%0Ain%20the%20gazebo%20environment%2C%20ASMA%20offers%2059.4%25%20-%2061.8%25%20increase%20in%20success%20rates%0Awith%20insignificant%205.4%25%20-%208.2%25%20increases%20in%20trajectory%20lengths%20compared%20to%20the%0Abaseline%20CBF-less%20VLN%20while%20recovering%20from%20unsafe%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10283v1&entry.124074799=Read"},
{"title": "Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in\n  Dynamic Scenes", "author": "Kaichen Zhou and Jia-Wang Bian and Qian Xie and Jian-Qing Zheng and Niki Trigoni and Andrew Markham", "abstract": "  Despite advancements in self-supervised monocular depth estimation,\nchallenges persist in dynamic scenarios due to the dependence on assumptions\nabout a static world. In this paper, we present Manydepth2, a Motion-Guided\nCost Volume Depth Net, to achieve precise depth estimation for both dynamic\nobjects and static backgrounds, all while maintaining computational efficiency.\nTo tackle the challenges posed by dynamic content, we incorporate optical flow\nand coarse monocular depth to create a novel static reference frame. This frame\nis then utilized to build a motion-guided cost volume in collaboration with the\ntarget frame. Additionally, to enhance the accuracy and resilience of the\nnetwork structure, we introduce an attention-based depth net architecture to\neffectively integrate information from feature maps with varying resolutions.\nCompared to methods with similar computational costs, Manydepth2 achieves a\nsignificant reduction of approximately five percent in root-mean-square error\nfor self-supervised monocular depth estimation on the KITTI-2015 dataset. The\ncode could be found: https://github.com/kaichen-z/Manydepth2\n", "link": "http://arxiv.org/abs/2312.15268v2", "date": "2024-09-16", "relevancy": 2.3368, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5926}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes&body=Title%3A%20Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes%0AAuthor%3A%20Kaichen%20Zhou%20and%20Jia-Wang%20Bian%20and%20Qian%20Xie%20and%20Jian-Qing%20Zheng%20and%20Niki%20Trigoni%20and%20Andrew%20Markham%0AAbstract%3A%20%20%20Despite%20advancements%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Achallenges%20persist%20in%20dynamic%20scenarios%20due%20to%20the%20dependence%20on%20assumptions%0Aabout%20a%20static%20world.%20In%20this%20paper%2C%20we%20present%20Manydepth2%2C%20a%20Motion-Guided%0ACost%20Volume%20Depth%20Net%2C%20to%20achieve%20precise%20depth%20estimation%20for%20both%20dynamic%0Aobjects%20and%20static%20backgrounds%2C%20all%20while%20maintaining%20computational%20efficiency.%0ATo%20tackle%20the%20challenges%20posed%20by%20dynamic%20content%2C%20we%20incorporate%20optical%20flow%0Aand%20coarse%20monocular%20depth%20to%20create%20a%20novel%20static%20reference%20frame.%20This%20frame%0Ais%20then%20utilized%20to%20build%20a%20motion-guided%20cost%20volume%20in%20collaboration%20with%20the%0Atarget%20frame.%20Additionally%2C%20to%20enhance%20the%20accuracy%20and%20resilience%20of%20the%0Anetwork%20structure%2C%20we%20introduce%20an%20attention-based%20depth%20net%20architecture%20to%0Aeffectively%20integrate%20information%20from%20feature%20maps%20with%20varying%20resolutions.%0ACompared%20to%20methods%20with%20similar%20computational%20costs%2C%20Manydepth2%20achieves%20a%0Asignificant%20reduction%20of%20approximately%20five%20percent%20in%20root-mean-square%20error%0Afor%20self-supervised%20monocular%20depth%20estimation%20on%20the%20KITTI-2015%20dataset.%20The%0Acode%20could%20be%20found%3A%20https%3A//github.com/kaichen-z/Manydepth2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManydepth2%253A%2520Motion-Aware%2520Self-Supervised%2520Monocular%2520Depth%2520Estimation%2520in%250A%2520%2520Dynamic%2520Scenes%26entry.906535625%3DKaichen%2520Zhou%2520and%2520Jia-Wang%2520Bian%2520and%2520Qian%2520Xie%2520and%2520Jian-Qing%2520Zheng%2520and%2520Niki%2520Trigoni%2520and%2520Andrew%2520Markham%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520self-supervised%2520monocular%2520depth%2520estimation%252C%250Achallenges%2520persist%2520in%2520dynamic%2520scenarios%2520due%2520to%2520the%2520dependence%2520on%2520assumptions%250Aabout%2520a%2520static%2520world.%2520In%2520this%2520paper%252C%2520we%2520present%2520Manydepth2%252C%2520a%2520Motion-Guided%250ACost%2520Volume%2520Depth%2520Net%252C%2520to%2520achieve%2520precise%2520depth%2520estimation%2520for%2520both%2520dynamic%250Aobjects%2520and%2520static%2520backgrounds%252C%2520all%2520while%2520maintaining%2520computational%2520efficiency.%250ATo%2520tackle%2520the%2520challenges%2520posed%2520by%2520dynamic%2520content%252C%2520we%2520incorporate%2520optical%2520flow%250Aand%2520coarse%2520monocular%2520depth%2520to%2520create%2520a%2520novel%2520static%2520reference%2520frame.%2520This%2520frame%250Ais%2520then%2520utilized%2520to%2520build%2520a%2520motion-guided%2520cost%2520volume%2520in%2520collaboration%2520with%2520the%250Atarget%2520frame.%2520Additionally%252C%2520to%2520enhance%2520the%2520accuracy%2520and%2520resilience%2520of%2520the%250Anetwork%2520structure%252C%2520we%2520introduce%2520an%2520attention-based%2520depth%2520net%2520architecture%2520to%250Aeffectively%2520integrate%2520information%2520from%2520feature%2520maps%2520with%2520varying%2520resolutions.%250ACompared%2520to%2520methods%2520with%2520similar%2520computational%2520costs%252C%2520Manydepth2%2520achieves%2520a%250Asignificant%2520reduction%2520of%2520approximately%2520five%2520percent%2520in%2520root-mean-square%2520error%250Afor%2520self-supervised%2520monocular%2520depth%2520estimation%2520on%2520the%2520KITTI-2015%2520dataset.%2520The%250Acode%2520could%2520be%2520found%253A%2520https%253A//github.com/kaichen-z/Manydepth2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manydepth2%3A%20Motion-Aware%20Self-Supervised%20Monocular%20Depth%20Estimation%20in%0A%20%20Dynamic%20Scenes&entry.906535625=Kaichen%20Zhou%20and%20Jia-Wang%20Bian%20and%20Qian%20Xie%20and%20Jian-Qing%20Zheng%20and%20Niki%20Trigoni%20and%20Andrew%20Markham&entry.1292438233=%20%20Despite%20advancements%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Achallenges%20persist%20in%20dynamic%20scenarios%20due%20to%20the%20dependence%20on%20assumptions%0Aabout%20a%20static%20world.%20In%20this%20paper%2C%20we%20present%20Manydepth2%2C%20a%20Motion-Guided%0ACost%20Volume%20Depth%20Net%2C%20to%20achieve%20precise%20depth%20estimation%20for%20both%20dynamic%0Aobjects%20and%20static%20backgrounds%2C%20all%20while%20maintaining%20computational%20efficiency.%0ATo%20tackle%20the%20challenges%20posed%20by%20dynamic%20content%2C%20we%20incorporate%20optical%20flow%0Aand%20coarse%20monocular%20depth%20to%20create%20a%20novel%20static%20reference%20frame.%20This%20frame%0Ais%20then%20utilized%20to%20build%20a%20motion-guided%20cost%20volume%20in%20collaboration%20with%20the%0Atarget%20frame.%20Additionally%2C%20to%20enhance%20the%20accuracy%20and%20resilience%20of%20the%0Anetwork%20structure%2C%20we%20introduce%20an%20attention-based%20depth%20net%20architecture%20to%0Aeffectively%20integrate%20information%20from%20feature%20maps%20with%20varying%20resolutions.%0ACompared%20to%20methods%20with%20similar%20computational%20costs%2C%20Manydepth2%20achieves%20a%0Asignificant%20reduction%20of%20approximately%20five%20percent%20in%20root-mean-square%20error%0Afor%20self-supervised%20monocular%20depth%20estimation%20on%20the%20KITTI-2015%20dataset.%20The%0Acode%20could%20be%20found%3A%20https%3A//github.com/kaichen-z/Manydepth2%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15268v2&entry.124074799=Read"},
{"title": "Efficient Network Embedding by Approximate Equitable Partitions", "author": "Giuseppe Squillace and Mirco Tribastone and Max Tschaikowski and Andrea Vandin", "abstract": "  Structural network embedding is a crucial step in enabling effective\ndownstream tasks for complex systems that aims to project a network into a\nlower-dimensional space while preserving similarities among nodes. We introduce\na simple and efficient embedding technique based on approximate variants of\nequitable partitions. The approximation consists in introducing a user-tunable\ntolerance parameter relaxing the otherwise strict condition for exact equitable\npartitions that can be hardly found in real-world networks. We exploit a\nrelationship between equitable partitions and equivalence relations for Markov\nchains and ordinary differential equations to develop a partition refinement\nalgorithm for computing an approximate equitable partition in polynomial time.\nWe compare our method against state-of-the-art embedding techniques on\nbenchmark networks. We report comparable -- when not superior -- performance\nfor visualization, classification, and regression tasks at a cost between one\nand three orders of magnitude smaller using a prototype implementation,\nenabling the embedding of large-scale networks which could not be efficiently\nhandled by most of the competing techniques.\n", "link": "http://arxiv.org/abs/2409.10160v1", "date": "2024-09-16", "relevancy": 2.2944, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4553}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Network%20Embedding%20by%20Approximate%20Equitable%20Partitions&body=Title%3A%20Efficient%20Network%20Embedding%20by%20Approximate%20Equitable%20Partitions%0AAuthor%3A%20Giuseppe%20Squillace%20and%20Mirco%20Tribastone%20and%20Max%20Tschaikowski%20and%20Andrea%20Vandin%0AAbstract%3A%20%20%20Structural%20network%20embedding%20is%20a%20crucial%20step%20in%20enabling%20effective%0Adownstream%20tasks%20for%20complex%20systems%20that%20aims%20to%20project%20a%20network%20into%20a%0Alower-dimensional%20space%20while%20preserving%20similarities%20among%20nodes.%20We%20introduce%0Aa%20simple%20and%20efficient%20embedding%20technique%20based%20on%20approximate%20variants%20of%0Aequitable%20partitions.%20The%20approximation%20consists%20in%20introducing%20a%20user-tunable%0Atolerance%20parameter%20relaxing%20the%20otherwise%20strict%20condition%20for%20exact%20equitable%0Apartitions%20that%20can%20be%20hardly%20found%20in%20real-world%20networks.%20We%20exploit%20a%0Arelationship%20between%20equitable%20partitions%20and%20equivalence%20relations%20for%20Markov%0Achains%20and%20ordinary%20differential%20equations%20to%20develop%20a%20partition%20refinement%0Aalgorithm%20for%20computing%20an%20approximate%20equitable%20partition%20in%20polynomial%20time.%0AWe%20compare%20our%20method%20against%20state-of-the-art%20embedding%20techniques%20on%0Abenchmark%20networks.%20We%20report%20comparable%20--%20when%20not%20superior%20--%20performance%0Afor%20visualization%2C%20classification%2C%20and%20regression%20tasks%20at%20a%20cost%20between%20one%0Aand%20three%20orders%20of%20magnitude%20smaller%20using%20a%20prototype%20implementation%2C%0Aenabling%20the%20embedding%20of%20large-scale%20networks%20which%20could%20not%20be%20efficiently%0Ahandled%20by%20most%20of%20the%20competing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Network%2520Embedding%2520by%2520Approximate%2520Equitable%2520Partitions%26entry.906535625%3DGiuseppe%2520Squillace%2520and%2520Mirco%2520Tribastone%2520and%2520Max%2520Tschaikowski%2520and%2520Andrea%2520Vandin%26entry.1292438233%3D%2520%2520Structural%2520network%2520embedding%2520is%2520a%2520crucial%2520step%2520in%2520enabling%2520effective%250Adownstream%2520tasks%2520for%2520complex%2520systems%2520that%2520aims%2520to%2520project%2520a%2520network%2520into%2520a%250Alower-dimensional%2520space%2520while%2520preserving%2520similarities%2520among%2520nodes.%2520We%2520introduce%250Aa%2520simple%2520and%2520efficient%2520embedding%2520technique%2520based%2520on%2520approximate%2520variants%2520of%250Aequitable%2520partitions.%2520The%2520approximation%2520consists%2520in%2520introducing%2520a%2520user-tunable%250Atolerance%2520parameter%2520relaxing%2520the%2520otherwise%2520strict%2520condition%2520for%2520exact%2520equitable%250Apartitions%2520that%2520can%2520be%2520hardly%2520found%2520in%2520real-world%2520networks.%2520We%2520exploit%2520a%250Arelationship%2520between%2520equitable%2520partitions%2520and%2520equivalence%2520relations%2520for%2520Markov%250Achains%2520and%2520ordinary%2520differential%2520equations%2520to%2520develop%2520a%2520partition%2520refinement%250Aalgorithm%2520for%2520computing%2520an%2520approximate%2520equitable%2520partition%2520in%2520polynomial%2520time.%250AWe%2520compare%2520our%2520method%2520against%2520state-of-the-art%2520embedding%2520techniques%2520on%250Abenchmark%2520networks.%2520We%2520report%2520comparable%2520--%2520when%2520not%2520superior%2520--%2520performance%250Afor%2520visualization%252C%2520classification%252C%2520and%2520regression%2520tasks%2520at%2520a%2520cost%2520between%2520one%250Aand%2520three%2520orders%2520of%2520magnitude%2520smaller%2520using%2520a%2520prototype%2520implementation%252C%250Aenabling%2520the%2520embedding%2520of%2520large-scale%2520networks%2520which%2520could%2520not%2520be%2520efficiently%250Ahandled%2520by%2520most%2520of%2520the%2520competing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Network%20Embedding%20by%20Approximate%20Equitable%20Partitions&entry.906535625=Giuseppe%20Squillace%20and%20Mirco%20Tribastone%20and%20Max%20Tschaikowski%20and%20Andrea%20Vandin&entry.1292438233=%20%20Structural%20network%20embedding%20is%20a%20crucial%20step%20in%20enabling%20effective%0Adownstream%20tasks%20for%20complex%20systems%20that%20aims%20to%20project%20a%20network%20into%20a%0Alower-dimensional%20space%20while%20preserving%20similarities%20among%20nodes.%20We%20introduce%0Aa%20simple%20and%20efficient%20embedding%20technique%20based%20on%20approximate%20variants%20of%0Aequitable%20partitions.%20The%20approximation%20consists%20in%20introducing%20a%20user-tunable%0Atolerance%20parameter%20relaxing%20the%20otherwise%20strict%20condition%20for%20exact%20equitable%0Apartitions%20that%20can%20be%20hardly%20found%20in%20real-world%20networks.%20We%20exploit%20a%0Arelationship%20between%20equitable%20partitions%20and%20equivalence%20relations%20for%20Markov%0Achains%20and%20ordinary%20differential%20equations%20to%20develop%20a%20partition%20refinement%0Aalgorithm%20for%20computing%20an%20approximate%20equitable%20partition%20in%20polynomial%20time.%0AWe%20compare%20our%20method%20against%20state-of-the-art%20embedding%20techniques%20on%0Abenchmark%20networks.%20We%20report%20comparable%20--%20when%20not%20superior%20--%20performance%0Afor%20visualization%2C%20classification%2C%20and%20regression%20tasks%20at%20a%20cost%20between%20one%0Aand%20three%20orders%20of%20magnitude%20smaller%20using%20a%20prototype%20implementation%2C%0Aenabling%20the%20embedding%20of%20large-scale%20networks%20which%20could%20not%20be%20efficiently%0Ahandled%20by%20most%20of%20the%20competing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10160v1&entry.124074799=Read"},
{"title": "Garment Attribute Manipulation with Multi-level Attention", "author": "Vittorio Casula and Lorenzo Berlincioni and Luca Cultrera and Federico Becattini and Chiara Pero and Carmen Bisogni and Marco Bertini and Alberto Del Bimbo", "abstract": "  In the rapidly evolving field of online fashion shopping, the need for more\npersonalized and interactive image retrieval systems has become paramount.\nExisting methods often struggle with precisely manipulating specific garment\nattributes without inadvertently affecting others. To address this challenge,\nwe propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a\nnovel framework that integrates attribute-disentangled representations with a\nmulti-stage attention-based architecture. GAMMA enables targeted manipulation\nof fashion image attributes, allowing users to refine their searches with high\naccuracy. By leveraging a dual-encoder Transformer and memory block, our model\nachieves state-of-the-art performance on popular datasets like Shopping100k and\nDeepFashion.\n", "link": "http://arxiv.org/abs/2409.10206v1", "date": "2024-09-16", "relevancy": 2.2839, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6137}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Garment%20Attribute%20Manipulation%20with%20Multi-level%20Attention&body=Title%3A%20Garment%20Attribute%20Manipulation%20with%20Multi-level%20Attention%0AAuthor%3A%20Vittorio%20Casula%20and%20Lorenzo%20Berlincioni%20and%20Luca%20Cultrera%20and%20Federico%20Becattini%20and%20Chiara%20Pero%20and%20Carmen%20Bisogni%20and%20Marco%20Bertini%20and%20Alberto%20Del%20Bimbo%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20online%20fashion%20shopping%2C%20the%20need%20for%20more%0Apersonalized%20and%20interactive%20image%20retrieval%20systems%20has%20become%20paramount.%0AExisting%20methods%20often%20struggle%20with%20precisely%20manipulating%20specific%20garment%0Aattributes%20without%20inadvertently%20affecting%20others.%20To%20address%20this%20challenge%2C%0Awe%20propose%20GAMMA%20%28Garment%20Attribute%20Manipulation%20with%20Multi-level%20Attention%29%2C%20a%0Anovel%20framework%20that%20integrates%20attribute-disentangled%20representations%20with%20a%0Amulti-stage%20attention-based%20architecture.%20GAMMA%20enables%20targeted%20manipulation%0Aof%20fashion%20image%20attributes%2C%20allowing%20users%20to%20refine%20their%20searches%20with%20high%0Aaccuracy.%20By%20leveraging%20a%20dual-encoder%20Transformer%20and%20memory%20block%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20on%20popular%20datasets%20like%20Shopping100k%20and%0ADeepFashion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarment%2520Attribute%2520Manipulation%2520with%2520Multi-level%2520Attention%26entry.906535625%3DVittorio%2520Casula%2520and%2520Lorenzo%2520Berlincioni%2520and%2520Luca%2520Cultrera%2520and%2520Federico%2520Becattini%2520and%2520Chiara%2520Pero%2520and%2520Carmen%2520Bisogni%2520and%2520Marco%2520Bertini%2520and%2520Alberto%2520Del%2520Bimbo%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520online%2520fashion%2520shopping%252C%2520the%2520need%2520for%2520more%250Apersonalized%2520and%2520interactive%2520image%2520retrieval%2520systems%2520has%2520become%2520paramount.%250AExisting%2520methods%2520often%2520struggle%2520with%2520precisely%2520manipulating%2520specific%2520garment%250Aattributes%2520without%2520inadvertently%2520affecting%2520others.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520GAMMA%2520%2528Garment%2520Attribute%2520Manipulation%2520with%2520Multi-level%2520Attention%2529%252C%2520a%250Anovel%2520framework%2520that%2520integrates%2520attribute-disentangled%2520representations%2520with%2520a%250Amulti-stage%2520attention-based%2520architecture.%2520GAMMA%2520enables%2520targeted%2520manipulation%250Aof%2520fashion%2520image%2520attributes%252C%2520allowing%2520users%2520to%2520refine%2520their%2520searches%2520with%2520high%250Aaccuracy.%2520By%2520leveraging%2520a%2520dual-encoder%2520Transformer%2520and%2520memory%2520block%252C%2520our%2520model%250Aachieves%2520state-of-the-art%2520performance%2520on%2520popular%2520datasets%2520like%2520Shopping100k%2520and%250ADeepFashion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Garment%20Attribute%20Manipulation%20with%20Multi-level%20Attention&entry.906535625=Vittorio%20Casula%20and%20Lorenzo%20Berlincioni%20and%20Luca%20Cultrera%20and%20Federico%20Becattini%20and%20Chiara%20Pero%20and%20Carmen%20Bisogni%20and%20Marco%20Bertini%20and%20Alberto%20Del%20Bimbo&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20online%20fashion%20shopping%2C%20the%20need%20for%20more%0Apersonalized%20and%20interactive%20image%20retrieval%20systems%20has%20become%20paramount.%0AExisting%20methods%20often%20struggle%20with%20precisely%20manipulating%20specific%20garment%0Aattributes%20without%20inadvertently%20affecting%20others.%20To%20address%20this%20challenge%2C%0Awe%20propose%20GAMMA%20%28Garment%20Attribute%20Manipulation%20with%20Multi-level%20Attention%29%2C%20a%0Anovel%20framework%20that%20integrates%20attribute-disentangled%20representations%20with%20a%0Amulti-stage%20attention-based%20architecture.%20GAMMA%20enables%20targeted%20manipulation%0Aof%20fashion%20image%20attributes%2C%20allowing%20users%20to%20refine%20their%20searches%20with%20high%0Aaccuracy.%20By%20leveraging%20a%20dual-encoder%20Transformer%20and%20memory%20block%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20on%20popular%20datasets%20like%20Shopping100k%20and%0ADeepFashion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10206v1&entry.124074799=Read"},
{"title": "CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using\n  a Single Camera", "author": "Jingpei Lu and Zekai Liang and Tristin Xie and Florian Ritcher and Shan Lin and Sainan Liu and Michael C. Yip", "abstract": "  Camera-to-robot calibration is crucial for vision-based robot control and\nrequires effort to make it accurate. Recent advancements in markerless pose\nestimation methods have eliminated the need for time-consuming physical setups\nfor camera-to-robot calibration. While the existing markerless pose estimation\nmethods have demonstrated impressive accuracy without the need for cumbersome\nsetups, they rely on the assumption that all the robot joints are visible\nwithin the camera's field of view. However, in practice, robots usually move in\nand out of view, and some portion of the robot may stay out-of-frame during the\nwhole manipulation task due to real-world constraints, leading to a lack of\nsufficient visual features and subsequent failure of these approaches. To\naddress this challenge and enhance the applicability to vision-based robot\ncontrol, we propose a novel framework capable of estimating the robot pose with\npartially visible robot manipulators. Our approach leverages the\nVision-Language Models for fine-grained robot components detection, and\nintegrates it into a keypoint-based pose estimation network, which enables more\nrobust performance in varied operational conditions. The framework is evaluated\non both public robot datasets and self-collected partial-view datasets to\ndemonstrate our robustness and generalizability. As a result, this method is\neffective for robot pose estimation in a wider range of real-world manipulation\nscenarios.\n", "link": "http://arxiv.org/abs/2409.10441v1", "date": "2024-09-16", "relevancy": 2.2715, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5876}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5723}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CtRNet-X%3A%20Camera-to-Robot%20Pose%20Estimation%20in%20Real-world%20Conditions%20Using%0A%20%20a%20Single%20Camera&body=Title%3A%20CtRNet-X%3A%20Camera-to-Robot%20Pose%20Estimation%20in%20Real-world%20Conditions%20Using%0A%20%20a%20Single%20Camera%0AAuthor%3A%20Jingpei%20Lu%20and%20Zekai%20Liang%20and%20Tristin%20Xie%20and%20Florian%20Ritcher%20and%20Shan%20Lin%20and%20Sainan%20Liu%20and%20Michael%20C.%20Yip%0AAbstract%3A%20%20%20Camera-to-robot%20calibration%20is%20crucial%20for%20vision-based%20robot%20control%20and%0Arequires%20effort%20to%20make%20it%20accurate.%20Recent%20advancements%20in%20markerless%20pose%0Aestimation%20methods%20have%20eliminated%20the%20need%20for%20time-consuming%20physical%20setups%0Afor%20camera-to-robot%20calibration.%20While%20the%20existing%20markerless%20pose%20estimation%0Amethods%20have%20demonstrated%20impressive%20accuracy%20without%20the%20need%20for%20cumbersome%0Asetups%2C%20they%20rely%20on%20the%20assumption%20that%20all%20the%20robot%20joints%20are%20visible%0Awithin%20the%20camera%27s%20field%20of%20view.%20However%2C%20in%20practice%2C%20robots%20usually%20move%20in%0Aand%20out%20of%20view%2C%20and%20some%20portion%20of%20the%20robot%20may%20stay%20out-of-frame%20during%20the%0Awhole%20manipulation%20task%20due%20to%20real-world%20constraints%2C%20leading%20to%20a%20lack%20of%0Asufficient%20visual%20features%20and%20subsequent%20failure%20of%20these%20approaches.%20To%0Aaddress%20this%20challenge%20and%20enhance%20the%20applicability%20to%20vision-based%20robot%0Acontrol%2C%20we%20propose%20a%20novel%20framework%20capable%20of%20estimating%20the%20robot%20pose%20with%0Apartially%20visible%20robot%20manipulators.%20Our%20approach%20leverages%20the%0AVision-Language%20Models%20for%20fine-grained%20robot%20components%20detection%2C%20and%0Aintegrates%20it%20into%20a%20keypoint-based%20pose%20estimation%20network%2C%20which%20enables%20more%0Arobust%20performance%20in%20varied%20operational%20conditions.%20The%20framework%20is%20evaluated%0Aon%20both%20public%20robot%20datasets%20and%20self-collected%20partial-view%20datasets%20to%0Ademonstrate%20our%20robustness%20and%20generalizability.%20As%20a%20result%2C%20this%20method%20is%0Aeffective%20for%20robot%20pose%20estimation%20in%20a%20wider%20range%20of%20real-world%20manipulation%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCtRNet-X%253A%2520Camera-to-Robot%2520Pose%2520Estimation%2520in%2520Real-world%2520Conditions%2520Using%250A%2520%2520a%2520Single%2520Camera%26entry.906535625%3DJingpei%2520Lu%2520and%2520Zekai%2520Liang%2520and%2520Tristin%2520Xie%2520and%2520Florian%2520Ritcher%2520and%2520Shan%2520Lin%2520and%2520Sainan%2520Liu%2520and%2520Michael%2520C.%2520Yip%26entry.1292438233%3D%2520%2520Camera-to-robot%2520calibration%2520is%2520crucial%2520for%2520vision-based%2520robot%2520control%2520and%250Arequires%2520effort%2520to%2520make%2520it%2520accurate.%2520Recent%2520advancements%2520in%2520markerless%2520pose%250Aestimation%2520methods%2520have%2520eliminated%2520the%2520need%2520for%2520time-consuming%2520physical%2520setups%250Afor%2520camera-to-robot%2520calibration.%2520While%2520the%2520existing%2520markerless%2520pose%2520estimation%250Amethods%2520have%2520demonstrated%2520impressive%2520accuracy%2520without%2520the%2520need%2520for%2520cumbersome%250Asetups%252C%2520they%2520rely%2520on%2520the%2520assumption%2520that%2520all%2520the%2520robot%2520joints%2520are%2520visible%250Awithin%2520the%2520camera%2527s%2520field%2520of%2520view.%2520However%252C%2520in%2520practice%252C%2520robots%2520usually%2520move%2520in%250Aand%2520out%2520of%2520view%252C%2520and%2520some%2520portion%2520of%2520the%2520robot%2520may%2520stay%2520out-of-frame%2520during%2520the%250Awhole%2520manipulation%2520task%2520due%2520to%2520real-world%2520constraints%252C%2520leading%2520to%2520a%2520lack%2520of%250Asufficient%2520visual%2520features%2520and%2520subsequent%2520failure%2520of%2520these%2520approaches.%2520To%250Aaddress%2520this%2520challenge%2520and%2520enhance%2520the%2520applicability%2520to%2520vision-based%2520robot%250Acontrol%252C%2520we%2520propose%2520a%2520novel%2520framework%2520capable%2520of%2520estimating%2520the%2520robot%2520pose%2520with%250Apartially%2520visible%2520robot%2520manipulators.%2520Our%2520approach%2520leverages%2520the%250AVision-Language%2520Models%2520for%2520fine-grained%2520robot%2520components%2520detection%252C%2520and%250Aintegrates%2520it%2520into%2520a%2520keypoint-based%2520pose%2520estimation%2520network%252C%2520which%2520enables%2520more%250Arobust%2520performance%2520in%2520varied%2520operational%2520conditions.%2520The%2520framework%2520is%2520evaluated%250Aon%2520both%2520public%2520robot%2520datasets%2520and%2520self-collected%2520partial-view%2520datasets%2520to%250Ademonstrate%2520our%2520robustness%2520and%2520generalizability.%2520As%2520a%2520result%252C%2520this%2520method%2520is%250Aeffective%2520for%2520robot%2520pose%2520estimation%2520in%2520a%2520wider%2520range%2520of%2520real-world%2520manipulation%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CtRNet-X%3A%20Camera-to-Robot%20Pose%20Estimation%20in%20Real-world%20Conditions%20Using%0A%20%20a%20Single%20Camera&entry.906535625=Jingpei%20Lu%20and%20Zekai%20Liang%20and%20Tristin%20Xie%20and%20Florian%20Ritcher%20and%20Shan%20Lin%20and%20Sainan%20Liu%20and%20Michael%20C.%20Yip&entry.1292438233=%20%20Camera-to-robot%20calibration%20is%20crucial%20for%20vision-based%20robot%20control%20and%0Arequires%20effort%20to%20make%20it%20accurate.%20Recent%20advancements%20in%20markerless%20pose%0Aestimation%20methods%20have%20eliminated%20the%20need%20for%20time-consuming%20physical%20setups%0Afor%20camera-to-robot%20calibration.%20While%20the%20existing%20markerless%20pose%20estimation%0Amethods%20have%20demonstrated%20impressive%20accuracy%20without%20the%20need%20for%20cumbersome%0Asetups%2C%20they%20rely%20on%20the%20assumption%20that%20all%20the%20robot%20joints%20are%20visible%0Awithin%20the%20camera%27s%20field%20of%20view.%20However%2C%20in%20practice%2C%20robots%20usually%20move%20in%0Aand%20out%20of%20view%2C%20and%20some%20portion%20of%20the%20robot%20may%20stay%20out-of-frame%20during%20the%0Awhole%20manipulation%20task%20due%20to%20real-world%20constraints%2C%20leading%20to%20a%20lack%20of%0Asufficient%20visual%20features%20and%20subsequent%20failure%20of%20these%20approaches.%20To%0Aaddress%20this%20challenge%20and%20enhance%20the%20applicability%20to%20vision-based%20robot%0Acontrol%2C%20we%20propose%20a%20novel%20framework%20capable%20of%20estimating%20the%20robot%20pose%20with%0Apartially%20visible%20robot%20manipulators.%20Our%20approach%20leverages%20the%0AVision-Language%20Models%20for%20fine-grained%20robot%20components%20detection%2C%20and%0Aintegrates%20it%20into%20a%20keypoint-based%20pose%20estimation%20network%2C%20which%20enables%20more%0Arobust%20performance%20in%20varied%20operational%20conditions.%20The%20framework%20is%20evaluated%0Aon%20both%20public%20robot%20datasets%20and%20self-collected%20partial-view%20datasets%20to%0Ademonstrate%20our%20robustness%20and%20generalizability.%20As%20a%20result%2C%20this%20method%20is%0Aeffective%20for%20robot%20pose%20estimation%20in%20a%20wider%20range%20of%20real-world%20manipulation%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10441v1&entry.124074799=Read"},
{"title": "Quantifying and Learning Static vs. Dynamic Information in Deep\n  Spatiotemporal Networks", "author": "Matthew Kowal and Mennatullah Siam and Md Amirul Islam and Neil D. B. Bruce and Richard P. Wildes and Konstantinos G. Derpanis", "abstract": "  There is limited understanding of the information captured by deep\nspatiotemporal models in their intermediate representations. For example, while\nevidence suggests that action recognition algorithms are heavily influenced by\nvisual appearance in single frames, no quantitative methodology exists for\nevaluating such static bias in the latent representation compared to bias\ntoward dynamics. We tackle this challenge by proposing an approach for\nquantifying the static and dynamic biases of any spatiotemporal model, and\napply our approach to three tasks, action recognition, automatic video object\nsegmentation (AVOS) and video instance segmentation (VIS). Our key findings\nare: (i) Most examined models are biased toward static information. (ii) Some\ndatasets that are assumed to be biased toward dynamics are actually biased\ntoward static information. (iii) Individual channels in an architecture can be\nbiased toward static, dynamic or a combination of the two. (iv) Most models\nconverge to their culminating biases in the first half of training. We then\nexplore how these biases affect performance on dynamically biased datasets. For\naction recognition, we propose StaticDropout, a semantically guided dropout\nthat debiases a model from static information toward dynamics. For AVOS, we\ndesign a better combination of fusion and cross connection layers compared with\nprevious architectures.\n", "link": "http://arxiv.org/abs/2211.01783v2", "date": "2024-09-16", "relevancy": 2.2647, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5689}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.565}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20and%20Learning%20Static%20vs.%20Dynamic%20Information%20in%20Deep%0A%20%20Spatiotemporal%20Networks&body=Title%3A%20Quantifying%20and%20Learning%20Static%20vs.%20Dynamic%20Information%20in%20Deep%0A%20%20Spatiotemporal%20Networks%0AAuthor%3A%20Matthew%20Kowal%20and%20Mennatullah%20Siam%20and%20Md%20Amirul%20Islam%20and%20Neil%20D.%20B.%20Bruce%20and%20Richard%20P.%20Wildes%20and%20Konstantinos%20G.%20Derpanis%0AAbstract%3A%20%20%20There%20is%20limited%20understanding%20of%20the%20information%20captured%20by%20deep%0Aspatiotemporal%20models%20in%20their%20intermediate%20representations.%20For%20example%2C%20while%0Aevidence%20suggests%20that%20action%20recognition%20algorithms%20are%20heavily%20influenced%20by%0Avisual%20appearance%20in%20single%20frames%2C%20no%20quantitative%20methodology%20exists%20for%0Aevaluating%20such%20static%20bias%20in%20the%20latent%20representation%20compared%20to%20bias%0Atoward%20dynamics.%20We%20tackle%20this%20challenge%20by%20proposing%20an%20approach%20for%0Aquantifying%20the%20static%20and%20dynamic%20biases%20of%20any%20spatiotemporal%20model%2C%20and%0Aapply%20our%20approach%20to%20three%20tasks%2C%20action%20recognition%2C%20automatic%20video%20object%0Asegmentation%20%28AVOS%29%20and%20video%20instance%20segmentation%20%28VIS%29.%20Our%20key%20findings%0Aare%3A%20%28i%29%20Most%20examined%20models%20are%20biased%20toward%20static%20information.%20%28ii%29%20Some%0Adatasets%20that%20are%20assumed%20to%20be%20biased%20toward%20dynamics%20are%20actually%20biased%0Atoward%20static%20information.%20%28iii%29%20Individual%20channels%20in%20an%20architecture%20can%20be%0Abiased%20toward%20static%2C%20dynamic%20or%20a%20combination%20of%20the%20two.%20%28iv%29%20Most%20models%0Aconverge%20to%20their%20culminating%20biases%20in%20the%20first%20half%20of%20training.%20We%20then%0Aexplore%20how%20these%20biases%20affect%20performance%20on%20dynamically%20biased%20datasets.%20For%0Aaction%20recognition%2C%20we%20propose%20StaticDropout%2C%20a%20semantically%20guided%20dropout%0Athat%20debiases%20a%20model%20from%20static%20information%20toward%20dynamics.%20For%20AVOS%2C%20we%0Adesign%20a%20better%20combination%20of%20fusion%20and%20cross%20connection%20layers%20compared%20with%0Aprevious%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.01783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520and%2520Learning%2520Static%2520vs.%2520Dynamic%2520Information%2520in%2520Deep%250A%2520%2520Spatiotemporal%2520Networks%26entry.906535625%3DMatthew%2520Kowal%2520and%2520Mennatullah%2520Siam%2520and%2520Md%2520Amirul%2520Islam%2520and%2520Neil%2520D.%2520B.%2520Bruce%2520and%2520Richard%2520P.%2520Wildes%2520and%2520Konstantinos%2520G.%2520Derpanis%26entry.1292438233%3D%2520%2520There%2520is%2520limited%2520understanding%2520of%2520the%2520information%2520captured%2520by%2520deep%250Aspatiotemporal%2520models%2520in%2520their%2520intermediate%2520representations.%2520For%2520example%252C%2520while%250Aevidence%2520suggests%2520that%2520action%2520recognition%2520algorithms%2520are%2520heavily%2520influenced%2520by%250Avisual%2520appearance%2520in%2520single%2520frames%252C%2520no%2520quantitative%2520methodology%2520exists%2520for%250Aevaluating%2520such%2520static%2520bias%2520in%2520the%2520latent%2520representation%2520compared%2520to%2520bias%250Atoward%2520dynamics.%2520We%2520tackle%2520this%2520challenge%2520by%2520proposing%2520an%2520approach%2520for%250Aquantifying%2520the%2520static%2520and%2520dynamic%2520biases%2520of%2520any%2520spatiotemporal%2520model%252C%2520and%250Aapply%2520our%2520approach%2520to%2520three%2520tasks%252C%2520action%2520recognition%252C%2520automatic%2520video%2520object%250Asegmentation%2520%2528AVOS%2529%2520and%2520video%2520instance%2520segmentation%2520%2528VIS%2529.%2520Our%2520key%2520findings%250Aare%253A%2520%2528i%2529%2520Most%2520examined%2520models%2520are%2520biased%2520toward%2520static%2520information.%2520%2528ii%2529%2520Some%250Adatasets%2520that%2520are%2520assumed%2520to%2520be%2520biased%2520toward%2520dynamics%2520are%2520actually%2520biased%250Atoward%2520static%2520information.%2520%2528iii%2529%2520Individual%2520channels%2520in%2520an%2520architecture%2520can%2520be%250Abiased%2520toward%2520static%252C%2520dynamic%2520or%2520a%2520combination%2520of%2520the%2520two.%2520%2528iv%2529%2520Most%2520models%250Aconverge%2520to%2520their%2520culminating%2520biases%2520in%2520the%2520first%2520half%2520of%2520training.%2520We%2520then%250Aexplore%2520how%2520these%2520biases%2520affect%2520performance%2520on%2520dynamically%2520biased%2520datasets.%2520For%250Aaction%2520recognition%252C%2520we%2520propose%2520StaticDropout%252C%2520a%2520semantically%2520guided%2520dropout%250Athat%2520debiases%2520a%2520model%2520from%2520static%2520information%2520toward%2520dynamics.%2520For%2520AVOS%252C%2520we%250Adesign%2520a%2520better%2520combination%2520of%2520fusion%2520and%2520cross%2520connection%2520layers%2520compared%2520with%250Aprevious%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.01783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20and%20Learning%20Static%20vs.%20Dynamic%20Information%20in%20Deep%0A%20%20Spatiotemporal%20Networks&entry.906535625=Matthew%20Kowal%20and%20Mennatullah%20Siam%20and%20Md%20Amirul%20Islam%20and%20Neil%20D.%20B.%20Bruce%20and%20Richard%20P.%20Wildes%20and%20Konstantinos%20G.%20Derpanis&entry.1292438233=%20%20There%20is%20limited%20understanding%20of%20the%20information%20captured%20by%20deep%0Aspatiotemporal%20models%20in%20their%20intermediate%20representations.%20For%20example%2C%20while%0Aevidence%20suggests%20that%20action%20recognition%20algorithms%20are%20heavily%20influenced%20by%0Avisual%20appearance%20in%20single%20frames%2C%20no%20quantitative%20methodology%20exists%20for%0Aevaluating%20such%20static%20bias%20in%20the%20latent%20representation%20compared%20to%20bias%0Atoward%20dynamics.%20We%20tackle%20this%20challenge%20by%20proposing%20an%20approach%20for%0Aquantifying%20the%20static%20and%20dynamic%20biases%20of%20any%20spatiotemporal%20model%2C%20and%0Aapply%20our%20approach%20to%20three%20tasks%2C%20action%20recognition%2C%20automatic%20video%20object%0Asegmentation%20%28AVOS%29%20and%20video%20instance%20segmentation%20%28VIS%29.%20Our%20key%20findings%0Aare%3A%20%28i%29%20Most%20examined%20models%20are%20biased%20toward%20static%20information.%20%28ii%29%20Some%0Adatasets%20that%20are%20assumed%20to%20be%20biased%20toward%20dynamics%20are%20actually%20biased%0Atoward%20static%20information.%20%28iii%29%20Individual%20channels%20in%20an%20architecture%20can%20be%0Abiased%20toward%20static%2C%20dynamic%20or%20a%20combination%20of%20the%20two.%20%28iv%29%20Most%20models%0Aconverge%20to%20their%20culminating%20biases%20in%20the%20first%20half%20of%20training.%20We%20then%0Aexplore%20how%20these%20biases%20affect%20performance%20on%20dynamically%20biased%20datasets.%20For%0Aaction%20recognition%2C%20we%20propose%20StaticDropout%2C%20a%20semantically%20guided%20dropout%0Athat%20debiases%20a%20model%20from%20static%20information%20toward%20dynamics.%20For%20AVOS%2C%20we%0Adesign%20a%20better%20combination%20of%20fusion%20and%20cross%20connection%20layers%20compared%20with%0Aprevious%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.01783v2&entry.124074799=Read"},
{"title": "Global Uncertainty-Aware Planning for Magnetic Anomaly-Based Navigation", "author": "Aditya Penumarti and Jane Shin", "abstract": "  Navigating and localizing in partially observable, stochastic environments\nwith magnetic anomalies presents significant challenges, especially when\nbalancing the accuracy of state estimation and the stability of localization.\nTraditional approaches often struggle to maintain performance due to limited\nlocalization updates and dynamic conditions. This paper introduces a\nmulti-objective global path planner for magnetic anomaly navigation (MagNav),\nwhich leverages entropy maps to assess spatial frequency variations in magnetic\nfields and identify high-information areas. The system generates paths toward\nthese regions by employing a potential field planner, enhancing active\nlocalization. Hardware experiments demonstrate that the proposed method\nsignificantly improves localization stability and accuracy compared to existing\nactive localization techniques. The results underscore the effectiveness of\nthis method in reducing localization uncertainty and highlight its adaptability\nto various gradient-based navigation maps, including topographical and\nunderwater depth-based environments.\n", "link": "http://arxiv.org/abs/2409.10366v1", "date": "2024-09-16", "relevancy": 2.2614, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6264}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5325}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Uncertainty-Aware%20Planning%20for%20Magnetic%20Anomaly-Based%20Navigation&body=Title%3A%20Global%20Uncertainty-Aware%20Planning%20for%20Magnetic%20Anomaly-Based%20Navigation%0AAuthor%3A%20Aditya%20Penumarti%20and%20Jane%20Shin%0AAbstract%3A%20%20%20Navigating%20and%20localizing%20in%20partially%20observable%2C%20stochastic%20environments%0Awith%20magnetic%20anomalies%20presents%20significant%20challenges%2C%20especially%20when%0Abalancing%20the%20accuracy%20of%20state%20estimation%20and%20the%20stability%20of%20localization.%0ATraditional%20approaches%20often%20struggle%20to%20maintain%20performance%20due%20to%20limited%0Alocalization%20updates%20and%20dynamic%20conditions.%20This%20paper%20introduces%20a%0Amulti-objective%20global%20path%20planner%20for%20magnetic%20anomaly%20navigation%20%28MagNav%29%2C%0Awhich%20leverages%20entropy%20maps%20to%20assess%20spatial%20frequency%20variations%20in%20magnetic%0Afields%20and%20identify%20high-information%20areas.%20The%20system%20generates%20paths%20toward%0Athese%20regions%20by%20employing%20a%20potential%20field%20planner%2C%20enhancing%20active%0Alocalization.%20Hardware%20experiments%20demonstrate%20that%20the%20proposed%20method%0Asignificantly%20improves%20localization%20stability%20and%20accuracy%20compared%20to%20existing%0Aactive%20localization%20techniques.%20The%20results%20underscore%20the%20effectiveness%20of%0Athis%20method%20in%20reducing%20localization%20uncertainty%20and%20highlight%20its%20adaptability%0Ato%20various%20gradient-based%20navigation%20maps%2C%20including%20topographical%20and%0Aunderwater%20depth-based%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Uncertainty-Aware%2520Planning%2520for%2520Magnetic%2520Anomaly-Based%2520Navigation%26entry.906535625%3DAditya%2520Penumarti%2520and%2520Jane%2520Shin%26entry.1292438233%3D%2520%2520Navigating%2520and%2520localizing%2520in%2520partially%2520observable%252C%2520stochastic%2520environments%250Awith%2520magnetic%2520anomalies%2520presents%2520significant%2520challenges%252C%2520especially%2520when%250Abalancing%2520the%2520accuracy%2520of%2520state%2520estimation%2520and%2520the%2520stability%2520of%2520localization.%250ATraditional%2520approaches%2520often%2520struggle%2520to%2520maintain%2520performance%2520due%2520to%2520limited%250Alocalization%2520updates%2520and%2520dynamic%2520conditions.%2520This%2520paper%2520introduces%2520a%250Amulti-objective%2520global%2520path%2520planner%2520for%2520magnetic%2520anomaly%2520navigation%2520%2528MagNav%2529%252C%250Awhich%2520leverages%2520entropy%2520maps%2520to%2520assess%2520spatial%2520frequency%2520variations%2520in%2520magnetic%250Afields%2520and%2520identify%2520high-information%2520areas.%2520The%2520system%2520generates%2520paths%2520toward%250Athese%2520regions%2520by%2520employing%2520a%2520potential%2520field%2520planner%252C%2520enhancing%2520active%250Alocalization.%2520Hardware%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%250Asignificantly%2520improves%2520localization%2520stability%2520and%2520accuracy%2520compared%2520to%2520existing%250Aactive%2520localization%2520techniques.%2520The%2520results%2520underscore%2520the%2520effectiveness%2520of%250Athis%2520method%2520in%2520reducing%2520localization%2520uncertainty%2520and%2520highlight%2520its%2520adaptability%250Ato%2520various%2520gradient-based%2520navigation%2520maps%252C%2520including%2520topographical%2520and%250Aunderwater%2520depth-based%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Uncertainty-Aware%20Planning%20for%20Magnetic%20Anomaly-Based%20Navigation&entry.906535625=Aditya%20Penumarti%20and%20Jane%20Shin&entry.1292438233=%20%20Navigating%20and%20localizing%20in%20partially%20observable%2C%20stochastic%20environments%0Awith%20magnetic%20anomalies%20presents%20significant%20challenges%2C%20especially%20when%0Abalancing%20the%20accuracy%20of%20state%20estimation%20and%20the%20stability%20of%20localization.%0ATraditional%20approaches%20often%20struggle%20to%20maintain%20performance%20due%20to%20limited%0Alocalization%20updates%20and%20dynamic%20conditions.%20This%20paper%20introduces%20a%0Amulti-objective%20global%20path%20planner%20for%20magnetic%20anomaly%20navigation%20%28MagNav%29%2C%0Awhich%20leverages%20entropy%20maps%20to%20assess%20spatial%20frequency%20variations%20in%20magnetic%0Afields%20and%20identify%20high-information%20areas.%20The%20system%20generates%20paths%20toward%0Athese%20regions%20by%20employing%20a%20potential%20field%20planner%2C%20enhancing%20active%0Alocalization.%20Hardware%20experiments%20demonstrate%20that%20the%20proposed%20method%0Asignificantly%20improves%20localization%20stability%20and%20accuracy%20compared%20to%20existing%0Aactive%20localization%20techniques.%20The%20results%20underscore%20the%20effectiveness%20of%0Athis%20method%20in%20reducing%20localization%20uncertainty%20and%20highlight%20its%20adaptability%0Ato%20various%20gradient-based%20navigation%20maps%2C%20including%20topographical%20and%0Aunderwater%20depth-based%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10366v1&entry.124074799=Read"},
{"title": "Digital Twins Meet the Koopman Operator: Data-Driven Learning for Robust\n  Autonomy", "author": "Chinmay Vilas Samak and Tanmay Vilas Samak and Ajinkya Joglekar and Umesh Vaidya and Venkat Krovi", "abstract": "  Contrary to on-road autonomous navigation, off-road autonomy is complicated\nby various factors ranging from sensing challenges to terrain variability. In\nsuch a milieu, data-driven approaches have been commonly employed to capture\nintricate vehicle-environment interactions effectively. However, the success of\ndata-driven methods depends crucially on the quality and quantity of data,\nwhich can be compromised by large variability in off-road environments. To\naddress these concerns, we present a novel workflow to recreate the exact\nvehicle and its target operating conditions digitally for domain-specific data\ngeneration. This enables us to effectively model off-road vehicle dynamics from\nsimulation data using the Koopman operator theory, and employ the obtained\nmodels for local motion planning and optimal vehicle control. The capabilities\nof the proposed methodology are demonstrated through an autonomous navigation\nproblem of a 1:5 scale vehicle, where a terrain-informed planner is employed\nfor global mission planning. Results indicate a substantial improvement in\noff-road navigation performance with the proposed algorithm (5.84x) and\nunderscore the efficacy of digital twinning in terms of improving the sample\nefficiency (3.2x) and reducing the sim2real gap (5.2%).\n", "link": "http://arxiv.org/abs/2409.10347v1", "date": "2024-09-16", "relevancy": 2.2558, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5716}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twins%20Meet%20the%20Koopman%20Operator%3A%20Data-Driven%20Learning%20for%20Robust%0A%20%20Autonomy&body=Title%3A%20Digital%20Twins%20Meet%20the%20Koopman%20Operator%3A%20Data-Driven%20Learning%20for%20Robust%0A%20%20Autonomy%0AAuthor%3A%20Chinmay%20Vilas%20Samak%20and%20Tanmay%20Vilas%20Samak%20and%20Ajinkya%20Joglekar%20and%20Umesh%20Vaidya%20and%20Venkat%20Krovi%0AAbstract%3A%20%20%20Contrary%20to%20on-road%20autonomous%20navigation%2C%20off-road%20autonomy%20is%20complicated%0Aby%20various%20factors%20ranging%20from%20sensing%20challenges%20to%20terrain%20variability.%20In%0Asuch%20a%20milieu%2C%20data-driven%20approaches%20have%20been%20commonly%20employed%20to%20capture%0Aintricate%20vehicle-environment%20interactions%20effectively.%20However%2C%20the%20success%20of%0Adata-driven%20methods%20depends%20crucially%20on%20the%20quality%20and%20quantity%20of%20data%2C%0Awhich%20can%20be%20compromised%20by%20large%20variability%20in%20off-road%20environments.%20To%0Aaddress%20these%20concerns%2C%20we%20present%20a%20novel%20workflow%20to%20recreate%20the%20exact%0Avehicle%20and%20its%20target%20operating%20conditions%20digitally%20for%20domain-specific%20data%0Ageneration.%20This%20enables%20us%20to%20effectively%20model%20off-road%20vehicle%20dynamics%20from%0Asimulation%20data%20using%20the%20Koopman%20operator%20theory%2C%20and%20employ%20the%20obtained%0Amodels%20for%20local%20motion%20planning%20and%20optimal%20vehicle%20control.%20The%20capabilities%0Aof%20the%20proposed%20methodology%20are%20demonstrated%20through%20an%20autonomous%20navigation%0Aproblem%20of%20a%201%3A5%20scale%20vehicle%2C%20where%20a%20terrain-informed%20planner%20is%20employed%0Afor%20global%20mission%20planning.%20Results%20indicate%20a%20substantial%20improvement%20in%0Aoff-road%20navigation%20performance%20with%20the%20proposed%20algorithm%20%285.84x%29%20and%0Aunderscore%20the%20efficacy%20of%20digital%20twinning%20in%20terms%20of%20improving%20the%20sample%0Aefficiency%20%283.2x%29%20and%20reducing%20the%20sim2real%20gap%20%285.2%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twins%2520Meet%2520the%2520Koopman%2520Operator%253A%2520Data-Driven%2520Learning%2520for%2520Robust%250A%2520%2520Autonomy%26entry.906535625%3DChinmay%2520Vilas%2520Samak%2520and%2520Tanmay%2520Vilas%2520Samak%2520and%2520Ajinkya%2520Joglekar%2520and%2520Umesh%2520Vaidya%2520and%2520Venkat%2520Krovi%26entry.1292438233%3D%2520%2520Contrary%2520to%2520on-road%2520autonomous%2520navigation%252C%2520off-road%2520autonomy%2520is%2520complicated%250Aby%2520various%2520factors%2520ranging%2520from%2520sensing%2520challenges%2520to%2520terrain%2520variability.%2520In%250Asuch%2520a%2520milieu%252C%2520data-driven%2520approaches%2520have%2520been%2520commonly%2520employed%2520to%2520capture%250Aintricate%2520vehicle-environment%2520interactions%2520effectively.%2520However%252C%2520the%2520success%2520of%250Adata-driven%2520methods%2520depends%2520crucially%2520on%2520the%2520quality%2520and%2520quantity%2520of%2520data%252C%250Awhich%2520can%2520be%2520compromised%2520by%2520large%2520variability%2520in%2520off-road%2520environments.%2520To%250Aaddress%2520these%2520concerns%252C%2520we%2520present%2520a%2520novel%2520workflow%2520to%2520recreate%2520the%2520exact%250Avehicle%2520and%2520its%2520target%2520operating%2520conditions%2520digitally%2520for%2520domain-specific%2520data%250Ageneration.%2520This%2520enables%2520us%2520to%2520effectively%2520model%2520off-road%2520vehicle%2520dynamics%2520from%250Asimulation%2520data%2520using%2520the%2520Koopman%2520operator%2520theory%252C%2520and%2520employ%2520the%2520obtained%250Amodels%2520for%2520local%2520motion%2520planning%2520and%2520optimal%2520vehicle%2520control.%2520The%2520capabilities%250Aof%2520the%2520proposed%2520methodology%2520are%2520demonstrated%2520through%2520an%2520autonomous%2520navigation%250Aproblem%2520of%2520a%25201%253A5%2520scale%2520vehicle%252C%2520where%2520a%2520terrain-informed%2520planner%2520is%2520employed%250Afor%2520global%2520mission%2520planning.%2520Results%2520indicate%2520a%2520substantial%2520improvement%2520in%250Aoff-road%2520navigation%2520performance%2520with%2520the%2520proposed%2520algorithm%2520%25285.84x%2529%2520and%250Aunderscore%2520the%2520efficacy%2520of%2520digital%2520twinning%2520in%2520terms%2520of%2520improving%2520the%2520sample%250Aefficiency%2520%25283.2x%2529%2520and%2520reducing%2520the%2520sim2real%2520gap%2520%25285.2%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twins%20Meet%20the%20Koopman%20Operator%3A%20Data-Driven%20Learning%20for%20Robust%0A%20%20Autonomy&entry.906535625=Chinmay%20Vilas%20Samak%20and%20Tanmay%20Vilas%20Samak%20and%20Ajinkya%20Joglekar%20and%20Umesh%20Vaidya%20and%20Venkat%20Krovi&entry.1292438233=%20%20Contrary%20to%20on-road%20autonomous%20navigation%2C%20off-road%20autonomy%20is%20complicated%0Aby%20various%20factors%20ranging%20from%20sensing%20challenges%20to%20terrain%20variability.%20In%0Asuch%20a%20milieu%2C%20data-driven%20approaches%20have%20been%20commonly%20employed%20to%20capture%0Aintricate%20vehicle-environment%20interactions%20effectively.%20However%2C%20the%20success%20of%0Adata-driven%20methods%20depends%20crucially%20on%20the%20quality%20and%20quantity%20of%20data%2C%0Awhich%20can%20be%20compromised%20by%20large%20variability%20in%20off-road%20environments.%20To%0Aaddress%20these%20concerns%2C%20we%20present%20a%20novel%20workflow%20to%20recreate%20the%20exact%0Avehicle%20and%20its%20target%20operating%20conditions%20digitally%20for%20domain-specific%20data%0Ageneration.%20This%20enables%20us%20to%20effectively%20model%20off-road%20vehicle%20dynamics%20from%0Asimulation%20data%20using%20the%20Koopman%20operator%20theory%2C%20and%20employ%20the%20obtained%0Amodels%20for%20local%20motion%20planning%20and%20optimal%20vehicle%20control.%20The%20capabilities%0Aof%20the%20proposed%20methodology%20are%20demonstrated%20through%20an%20autonomous%20navigation%0Aproblem%20of%20a%201%3A5%20scale%20vehicle%2C%20where%20a%20terrain-informed%20planner%20is%20employed%0Afor%20global%20mission%20planning.%20Results%20indicate%20a%20substantial%20improvement%20in%0Aoff-road%20navigation%20performance%20with%20the%20proposed%20algorithm%20%285.84x%29%20and%0Aunderscore%20the%20efficacy%20of%20digital%20twinning%20in%20terms%20of%20improving%20the%20sample%0Aefficiency%20%283.2x%29%20and%20reducing%20the%20sim2real%20gap%20%285.2%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10347v1&entry.124074799=Read"},
{"title": "Self-Updating Vehicle Monitoring Framework Employing Distributed\n  Acoustic Sensing towards Real-World Settings", "author": "Xi Wang and Xin Liu and Songming Zhu and Zhanwen Li and Lina Gao", "abstract": "  The recent emergence of Distributed Acoustic Sensing (DAS) technology has\nfacilitated the effective capture of traffic-induced seismic data. The\ntraffic-induced seismic wave is a prominent contributor to urban vibrations and\ncontain crucial information to advance urban exploration and governance.\nHowever, identifying vehicular movements within massive noisy data poses a\nsignificant challenge. In this study, we introduce a real-time semi-supervised\nvehicle monitoring framework tailored to urban settings. It requires only a\nsmall fraction of manual labels for initial training and exploits unlabeled\ndata for model improvement. Additionally, the framework can autonomously adapt\nto newly collected unlabeled data. Before DAS data undergo object detection as\ntwo-dimensional images to preserve spatial information, we leveraged\ncomprehensive one-dimensional signal preprocessing to mitigate noise.\nFurthermore, we propose a novel prior loss that incorporates the shapes of\nvehicular traces to track a single vehicle with varying speeds. To evaluate our\nmodel, we conducted experiments with seismic data from the Stanford 2 DAS\nArray. The results showed that our model outperformed the baseline model\nEfficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in\nboth accuracy and robustness. With only 35 labeled images, our model surpassed\nYOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient\nTeacher. We conducted comparative experiments with multiple update strategies\nfor self-updating and identified an optimal approach. This approach surpasses\nthe performance of non-overfitting training conducted with all data in a single\npass.\n", "link": "http://arxiv.org/abs/2409.10259v1", "date": "2024-09-16", "relevancy": 2.2484, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Updating%20Vehicle%20Monitoring%20Framework%20Employing%20Distributed%0A%20%20Acoustic%20Sensing%20towards%20Real-World%20Settings&body=Title%3A%20Self-Updating%20Vehicle%20Monitoring%20Framework%20Employing%20Distributed%0A%20%20Acoustic%20Sensing%20towards%20Real-World%20Settings%0AAuthor%3A%20Xi%20Wang%20and%20Xin%20Liu%20and%20Songming%20Zhu%20and%20Zhanwen%20Li%20and%20Lina%20Gao%0AAbstract%3A%20%20%20The%20recent%20emergence%20of%20Distributed%20Acoustic%20Sensing%20%28DAS%29%20technology%20has%0Afacilitated%20the%20effective%20capture%20of%20traffic-induced%20seismic%20data.%20The%0Atraffic-induced%20seismic%20wave%20is%20a%20prominent%20contributor%20to%20urban%20vibrations%20and%0Acontain%20crucial%20information%20to%20advance%20urban%20exploration%20and%20governance.%0AHowever%2C%20identifying%20vehicular%20movements%20within%20massive%20noisy%20data%20poses%20a%0Asignificant%20challenge.%20In%20this%20study%2C%20we%20introduce%20a%20real-time%20semi-supervised%0Avehicle%20monitoring%20framework%20tailored%20to%20urban%20settings.%20It%20requires%20only%20a%0Asmall%20fraction%20of%20manual%20labels%20for%20initial%20training%20and%20exploits%20unlabeled%0Adata%20for%20model%20improvement.%20Additionally%2C%20the%20framework%20can%20autonomously%20adapt%0Ato%20newly%20collected%20unlabeled%20data.%20Before%20DAS%20data%20undergo%20object%20detection%20as%0Atwo-dimensional%20images%20to%20preserve%20spatial%20information%2C%20we%20leveraged%0Acomprehensive%20one-dimensional%20signal%20preprocessing%20to%20mitigate%20noise.%0AFurthermore%2C%20we%20propose%20a%20novel%20prior%20loss%20that%20incorporates%20the%20shapes%20of%0Avehicular%20traces%20to%20track%20a%20single%20vehicle%20with%20varying%20speeds.%20To%20evaluate%20our%0Amodel%2C%20we%20conducted%20experiments%20with%20seismic%20data%20from%20the%20Stanford%202%20DAS%0AArray.%20The%20results%20showed%20that%20our%20model%20outperformed%20the%20baseline%20model%0AEfficient%20Teacher%20and%20its%20supervised%20counterpart%2C%20YOLO%20%28You%20Only%20Look%20Once%29%2C%20in%0Aboth%20accuracy%20and%20robustness.%20With%20only%2035%20labeled%20images%2C%20our%20model%20surpassed%0AYOLO%27s%20mAP%200.5%3A0.95%20criterion%20by%2018%25%20and%20showed%20a%207%25%20increase%20over%20Efficient%0ATeacher.%20We%20conducted%20comparative%20experiments%20with%20multiple%20update%20strategies%0Afor%20self-updating%20and%20identified%20an%20optimal%20approach.%20This%20approach%20surpasses%0Athe%20performance%20of%20non-overfitting%20training%20conducted%20with%20all%20data%20in%20a%20single%0Apass.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Updating%2520Vehicle%2520Monitoring%2520Framework%2520Employing%2520Distributed%250A%2520%2520Acoustic%2520Sensing%2520towards%2520Real-World%2520Settings%26entry.906535625%3DXi%2520Wang%2520and%2520Xin%2520Liu%2520and%2520Songming%2520Zhu%2520and%2520Zhanwen%2520Li%2520and%2520Lina%2520Gao%26entry.1292438233%3D%2520%2520The%2520recent%2520emergence%2520of%2520Distributed%2520Acoustic%2520Sensing%2520%2528DAS%2529%2520technology%2520has%250Afacilitated%2520the%2520effective%2520capture%2520of%2520traffic-induced%2520seismic%2520data.%2520The%250Atraffic-induced%2520seismic%2520wave%2520is%2520a%2520prominent%2520contributor%2520to%2520urban%2520vibrations%2520and%250Acontain%2520crucial%2520information%2520to%2520advance%2520urban%2520exploration%2520and%2520governance.%250AHowever%252C%2520identifying%2520vehicular%2520movements%2520within%2520massive%2520noisy%2520data%2520poses%2520a%250Asignificant%2520challenge.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520real-time%2520semi-supervised%250Avehicle%2520monitoring%2520framework%2520tailored%2520to%2520urban%2520settings.%2520It%2520requires%2520only%2520a%250Asmall%2520fraction%2520of%2520manual%2520labels%2520for%2520initial%2520training%2520and%2520exploits%2520unlabeled%250Adata%2520for%2520model%2520improvement.%2520Additionally%252C%2520the%2520framework%2520can%2520autonomously%2520adapt%250Ato%2520newly%2520collected%2520unlabeled%2520data.%2520Before%2520DAS%2520data%2520undergo%2520object%2520detection%2520as%250Atwo-dimensional%2520images%2520to%2520preserve%2520spatial%2520information%252C%2520we%2520leveraged%250Acomprehensive%2520one-dimensional%2520signal%2520preprocessing%2520to%2520mitigate%2520noise.%250AFurthermore%252C%2520we%2520propose%2520a%2520novel%2520prior%2520loss%2520that%2520incorporates%2520the%2520shapes%2520of%250Avehicular%2520traces%2520to%2520track%2520a%2520single%2520vehicle%2520with%2520varying%2520speeds.%2520To%2520evaluate%2520our%250Amodel%252C%2520we%2520conducted%2520experiments%2520with%2520seismic%2520data%2520from%2520the%2520Stanford%25202%2520DAS%250AArray.%2520The%2520results%2520showed%2520that%2520our%2520model%2520outperformed%2520the%2520baseline%2520model%250AEfficient%2520Teacher%2520and%2520its%2520supervised%2520counterpart%252C%2520YOLO%2520%2528You%2520Only%2520Look%2520Once%2529%252C%2520in%250Aboth%2520accuracy%2520and%2520robustness.%2520With%2520only%252035%2520labeled%2520images%252C%2520our%2520model%2520surpassed%250AYOLO%2527s%2520mAP%25200.5%253A0.95%2520criterion%2520by%252018%2525%2520and%2520showed%2520a%25207%2525%2520increase%2520over%2520Efficient%250ATeacher.%2520We%2520conducted%2520comparative%2520experiments%2520with%2520multiple%2520update%2520strategies%250Afor%2520self-updating%2520and%2520identified%2520an%2520optimal%2520approach.%2520This%2520approach%2520surpasses%250Athe%2520performance%2520of%2520non-overfitting%2520training%2520conducted%2520with%2520all%2520data%2520in%2520a%2520single%250Apass.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Updating%20Vehicle%20Monitoring%20Framework%20Employing%20Distributed%0A%20%20Acoustic%20Sensing%20towards%20Real-World%20Settings&entry.906535625=Xi%20Wang%20and%20Xin%20Liu%20and%20Songming%20Zhu%20and%20Zhanwen%20Li%20and%20Lina%20Gao&entry.1292438233=%20%20The%20recent%20emergence%20of%20Distributed%20Acoustic%20Sensing%20%28DAS%29%20technology%20has%0Afacilitated%20the%20effective%20capture%20of%20traffic-induced%20seismic%20data.%20The%0Atraffic-induced%20seismic%20wave%20is%20a%20prominent%20contributor%20to%20urban%20vibrations%20and%0Acontain%20crucial%20information%20to%20advance%20urban%20exploration%20and%20governance.%0AHowever%2C%20identifying%20vehicular%20movements%20within%20massive%20noisy%20data%20poses%20a%0Asignificant%20challenge.%20In%20this%20study%2C%20we%20introduce%20a%20real-time%20semi-supervised%0Avehicle%20monitoring%20framework%20tailored%20to%20urban%20settings.%20It%20requires%20only%20a%0Asmall%20fraction%20of%20manual%20labels%20for%20initial%20training%20and%20exploits%20unlabeled%0Adata%20for%20model%20improvement.%20Additionally%2C%20the%20framework%20can%20autonomously%20adapt%0Ato%20newly%20collected%20unlabeled%20data.%20Before%20DAS%20data%20undergo%20object%20detection%20as%0Atwo-dimensional%20images%20to%20preserve%20spatial%20information%2C%20we%20leveraged%0Acomprehensive%20one-dimensional%20signal%20preprocessing%20to%20mitigate%20noise.%0AFurthermore%2C%20we%20propose%20a%20novel%20prior%20loss%20that%20incorporates%20the%20shapes%20of%0Avehicular%20traces%20to%20track%20a%20single%20vehicle%20with%20varying%20speeds.%20To%20evaluate%20our%0Amodel%2C%20we%20conducted%20experiments%20with%20seismic%20data%20from%20the%20Stanford%202%20DAS%0AArray.%20The%20results%20showed%20that%20our%20model%20outperformed%20the%20baseline%20model%0AEfficient%20Teacher%20and%20its%20supervised%20counterpart%2C%20YOLO%20%28You%20Only%20Look%20Once%29%2C%20in%0Aboth%20accuracy%20and%20robustness.%20With%20only%2035%20labeled%20images%2C%20our%20model%20surpassed%0AYOLO%27s%20mAP%200.5%3A0.95%20criterion%20by%2018%25%20and%20showed%20a%207%25%20increase%20over%20Efficient%0ATeacher.%20We%20conducted%20comparative%20experiments%20with%20multiple%20update%20strategies%0Afor%20self-updating%20and%20identified%20an%20optimal%20approach.%20This%20approach%20surpasses%0Athe%20performance%20of%20non-overfitting%20training%20conducted%20with%20all%20data%20in%20a%20single%0Apass.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10259v1&entry.124074799=Read"},
{"title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control", "author": "Elahe Delavari and John Moore and Junho Hong and Jaerock Kwon", "abstract": "  This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.\n", "link": "http://arxiv.org/abs/2407.07684v2", "date": "2024-09-16", "relevancy": 2.2482, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6049}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5825}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control&body=Title%3A%20Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control%0AAuthor%3A%20Elahe%20Delavari%20and%20John%20Moore%20and%20Junho%20Hong%20and%20Jaerock%20Kwon%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20Autonomous%20Vehicle%20%28AV%29%20control%0Athrough%20the%20application%20of%20active%20inference%2C%20a%20theory%20derived%20from%20neuroscience%0Athat%20conceptualizes%20the%20brain%20as%20a%20predictive%20machine.%20Traditional%20autonomous%0Adriving%20systems%20rely%20heavily%20on%20Modular%20Pipelines%2C%20Imitation%20Learning%2C%20or%0AReinforcement%20Learning%2C%20each%20with%20inherent%20limitations%20in%20adaptability%2C%0Ageneralization%2C%20and%20computational%20efficiency.%20Active%20inference%20addresses%20these%0Achallenges%20by%20minimizing%20prediction%20error%20%28termed%20%22surprise%22%29%20through%20a%20dynamic%0Amodel%20that%20balances%20perception%20and%20action.%20Our%20method%20integrates%20active%0Ainference%20with%20deep%20learning%20to%20manage%20lateral%20control%20in%20AVs%2C%20enabling%20them%20to%0Aperform%20lane%20following%20maneuvers%20within%20a%20simulated%20urban%20environment.%20We%0Ademonstrate%20that%20our%20model%2C%20despite%20its%20simplicity%2C%20effectively%20learns%20and%0Ageneralizes%20from%20limited%20data%20without%20extensive%20retraining%2C%20significantly%0Areducing%20computational%20demands.%20The%20proposed%20approach%20not%20only%20enhances%20the%0Aadaptability%20and%20performance%20of%20AVs%20in%20dynamic%20scenarios%20but%20also%20aligns%0Aclosely%20with%20human-like%20driving%20behavior%2C%20leveraging%20a%20generative%20model%20to%0Apredict%20and%20adapt%20to%20environmental%20changes.%20Results%20from%20extensive%20experiments%0Ain%20the%20CARLA%20simulator%20show%20promising%20outcomes%2C%20outperforming%20traditional%0Amethods%20in%20terms%20of%20adaptability%20and%20efficiency%2C%20thereby%20advancing%20the%0Apotential%20of%20active%20inference%20in%20real-world%20autonomous%20driving%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-Like%2520Driving%253A%2520Active%2520Inference%2520in%2520Autonomous%2520Vehicle%250A%2520%2520Control%26entry.906535625%3DElahe%2520Delavari%2520and%2520John%2520Moore%2520and%2520Junho%2520Hong%2520and%2520Jaerock%2520Kwon%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520control%250Athrough%2520the%2520application%2520of%2520active%2520inference%252C%2520a%2520theory%2520derived%2520from%2520neuroscience%250Athat%2520conceptualizes%2520the%2520brain%2520as%2520a%2520predictive%2520machine.%2520Traditional%2520autonomous%250Adriving%2520systems%2520rely%2520heavily%2520on%2520Modular%2520Pipelines%252C%2520Imitation%2520Learning%252C%2520or%250AReinforcement%2520Learning%252C%2520each%2520with%2520inherent%2520limitations%2520in%2520adaptability%252C%250Ageneralization%252C%2520and%2520computational%2520efficiency.%2520Active%2520inference%2520addresses%2520these%250Achallenges%2520by%2520minimizing%2520prediction%2520error%2520%2528termed%2520%2522surprise%2522%2529%2520through%2520a%2520dynamic%250Amodel%2520that%2520balances%2520perception%2520and%2520action.%2520Our%2520method%2520integrates%2520active%250Ainference%2520with%2520deep%2520learning%2520to%2520manage%2520lateral%2520control%2520in%2520AVs%252C%2520enabling%2520them%2520to%250Aperform%2520lane%2520following%2520maneuvers%2520within%2520a%2520simulated%2520urban%2520environment.%2520We%250Ademonstrate%2520that%2520our%2520model%252C%2520despite%2520its%2520simplicity%252C%2520effectively%2520learns%2520and%250Ageneralizes%2520from%2520limited%2520data%2520without%2520extensive%2520retraining%252C%2520significantly%250Areducing%2520computational%2520demands.%2520The%2520proposed%2520approach%2520not%2520only%2520enhances%2520the%250Aadaptability%2520and%2520performance%2520of%2520AVs%2520in%2520dynamic%2520scenarios%2520but%2520also%2520aligns%250Aclosely%2520with%2520human-like%2520driving%2520behavior%252C%2520leveraging%2520a%2520generative%2520model%2520to%250Apredict%2520and%2520adapt%2520to%2520environmental%2520changes.%2520Results%2520from%2520extensive%2520experiments%250Ain%2520the%2520CARLA%2520simulator%2520show%2520promising%2520outcomes%252C%2520outperforming%2520traditional%250Amethods%2520in%2520terms%2520of%2520adaptability%2520and%2520efficiency%252C%2520thereby%2520advancing%2520the%250Apotential%2520of%2520active%2520inference%2520in%2520real-world%2520autonomous%2520driving%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-Like%20Driving%3A%20Active%20Inference%20in%20Autonomous%20Vehicle%0A%20%20Control&entry.906535625=Elahe%20Delavari%20and%20John%20Moore%20and%20Junho%20Hong%20and%20Jaerock%20Kwon&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20Autonomous%20Vehicle%20%28AV%29%20control%0Athrough%20the%20application%20of%20active%20inference%2C%20a%20theory%20derived%20from%20neuroscience%0Athat%20conceptualizes%20the%20brain%20as%20a%20predictive%20machine.%20Traditional%20autonomous%0Adriving%20systems%20rely%20heavily%20on%20Modular%20Pipelines%2C%20Imitation%20Learning%2C%20or%0AReinforcement%20Learning%2C%20each%20with%20inherent%20limitations%20in%20adaptability%2C%0Ageneralization%2C%20and%20computational%20efficiency.%20Active%20inference%20addresses%20these%0Achallenges%20by%20minimizing%20prediction%20error%20%28termed%20%22surprise%22%29%20through%20a%20dynamic%0Amodel%20that%20balances%20perception%20and%20action.%20Our%20method%20integrates%20active%0Ainference%20with%20deep%20learning%20to%20manage%20lateral%20control%20in%20AVs%2C%20enabling%20them%20to%0Aperform%20lane%20following%20maneuvers%20within%20a%20simulated%20urban%20environment.%20We%0Ademonstrate%20that%20our%20model%2C%20despite%20its%20simplicity%2C%20effectively%20learns%20and%0Ageneralizes%20from%20limited%20data%20without%20extensive%20retraining%2C%20significantly%0Areducing%20computational%20demands.%20The%20proposed%20approach%20not%20only%20enhances%20the%0Aadaptability%20and%20performance%20of%20AVs%20in%20dynamic%20scenarios%20but%20also%20aligns%0Aclosely%20with%20human-like%20driving%20behavior%2C%20leveraging%20a%20generative%20model%20to%0Apredict%20and%20adapt%20to%20environmental%20changes.%20Results%20from%20extensive%20experiments%0Ain%20the%20CARLA%20simulator%20show%20promising%20outcomes%2C%20outperforming%20traditional%0Amethods%20in%20terms%20of%20adaptability%20and%20efficiency%2C%20thereby%20advancing%20the%0Apotential%20of%20active%20inference%20in%20real-world%20autonomous%20driving%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07684v2&entry.124074799=Read"},
{"title": "Hierarchical Graph Pooling Based on Minimum Description Length", "author": "Jan von Pichowski and Christopher Bl\u00f6cker and Ingo Scholtes", "abstract": "  Graph pooling is an essential part of deep graph representation learning. We\nintroduce MapEqPool, a principled pooling operator that takes the inherent\nhierarchical structure of real-world graphs into account. MapEqPool builds on\nthe map equation, an information-theoretic objective function for community\ndetection based on the minimum description length principle which naturally\nimplements Occam's razor and balances between model complexity and fit. We\ndemonstrate MapEqPool's competitive performance with an empirical comparison\nagainst various baselines across standard graph classification datasets.\n", "link": "http://arxiv.org/abs/2409.10263v1", "date": "2024-09-16", "relevancy": 2.2463, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4418}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Graph%20Pooling%20Based%20on%20Minimum%20Description%20Length&body=Title%3A%20Hierarchical%20Graph%20Pooling%20Based%20on%20Minimum%20Description%20Length%0AAuthor%3A%20Jan%20von%20Pichowski%20and%20Christopher%20Bl%C3%B6cker%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20Graph%20pooling%20is%20an%20essential%20part%20of%20deep%20graph%20representation%20learning.%20We%0Aintroduce%20MapEqPool%2C%20a%20principled%20pooling%20operator%20that%20takes%20the%20inherent%0Ahierarchical%20structure%20of%20real-world%20graphs%20into%20account.%20MapEqPool%20builds%20on%0Athe%20map%20equation%2C%20an%20information-theoretic%20objective%20function%20for%20community%0Adetection%20based%20on%20the%20minimum%20description%20length%20principle%20which%20naturally%0Aimplements%20Occam%27s%20razor%20and%20balances%20between%20model%20complexity%20and%20fit.%20We%0Ademonstrate%20MapEqPool%27s%20competitive%20performance%20with%20an%20empirical%20comparison%0Aagainst%20various%20baselines%20across%20standard%20graph%20classification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Graph%2520Pooling%2520Based%2520on%2520Minimum%2520Description%2520Length%26entry.906535625%3DJan%2520von%2520Pichowski%2520and%2520Christopher%2520Bl%25C3%25B6cker%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520Graph%2520pooling%2520is%2520an%2520essential%2520part%2520of%2520deep%2520graph%2520representation%2520learning.%2520We%250Aintroduce%2520MapEqPool%252C%2520a%2520principled%2520pooling%2520operator%2520that%2520takes%2520the%2520inherent%250Ahierarchical%2520structure%2520of%2520real-world%2520graphs%2520into%2520account.%2520MapEqPool%2520builds%2520on%250Athe%2520map%2520equation%252C%2520an%2520information-theoretic%2520objective%2520function%2520for%2520community%250Adetection%2520based%2520on%2520the%2520minimum%2520description%2520length%2520principle%2520which%2520naturally%250Aimplements%2520Occam%2527s%2520razor%2520and%2520balances%2520between%2520model%2520complexity%2520and%2520fit.%2520We%250Ademonstrate%2520MapEqPool%2527s%2520competitive%2520performance%2520with%2520an%2520empirical%2520comparison%250Aagainst%2520various%2520baselines%2520across%2520standard%2520graph%2520classification%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Graph%20Pooling%20Based%20on%20Minimum%20Description%20Length&entry.906535625=Jan%20von%20Pichowski%20and%20Christopher%20Bl%C3%B6cker%20and%20Ingo%20Scholtes&entry.1292438233=%20%20Graph%20pooling%20is%20an%20essential%20part%20of%20deep%20graph%20representation%20learning.%20We%0Aintroduce%20MapEqPool%2C%20a%20principled%20pooling%20operator%20that%20takes%20the%20inherent%0Ahierarchical%20structure%20of%20real-world%20graphs%20into%20account.%20MapEqPool%20builds%20on%0Athe%20map%20equation%2C%20an%20information-theoretic%20objective%20function%20for%20community%0Adetection%20based%20on%20the%20minimum%20description%20length%20principle%20which%20naturally%0Aimplements%20Occam%27s%20razor%20and%20balances%20between%20model%20complexity%20and%20fit.%20We%0Ademonstrate%20MapEqPool%27s%20competitive%20performance%20with%20an%20empirical%20comparison%0Aagainst%20various%20baselines%20across%20standard%20graph%20classification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10263v1&entry.124074799=Read"},
{"title": "SMAFormer: Synergistic Multi-Attention Transformer for Medical Image\n  Segmentation", "author": "Fuchen Zheng and Xuhang Chen and Weihuang Liu and Haolun Li and Yingtie Lei and Jiahui He and Chi-Man Pun and Shounjun Zhou", "abstract": "  In medical image segmentation, specialized computer vision techniques,\nnotably transformers grounded in attention mechanisms and residual networks\nemploying skip connections, have been instrumental in advancing performance.\nNonetheless, previous models often falter when segmenting small, irregularly\nshaped tumors. To this end, we introduce SMAFormer, an efficient,\nTransformer-based architecture that fuses multiple attention mechanisms for\nenhanced segmentation of small tumors and organs. SMAFormer can capture both\nlocal and global features for medical image segmentation. The architecture\ncomprises two pivotal components. First, a Synergistic Multi-Attention (SMA)\nTransformer block is proposed, which has the benefits of Pixel Attention,\nChannel Attention, and Spatial Attention for feature enrichment. Second,\naddressing the challenge of information loss incurred during attention\nmechanism transitions and feature fusion, we design a Feature Fusion Modulator.\nThis module bolsters the integration between the channel and spatial attention\nby mitigating reshaping-induced information attrition. To evaluate our method,\nwe conduct extensive experiments on various medical image segmentation tasks,\nincluding multi-organ, liver tumor, and bladder tumor segmentation, achieving\nstate-of-the-art results. Code and models are available at:\n\\url{https://github.com/CXH-Research/SMAFormer}.\n", "link": "http://arxiv.org/abs/2409.00346v2", "date": "2024-09-16", "relevancy": 2.2437, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5754}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMAFormer%3A%20Synergistic%20Multi-Attention%20Transformer%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20SMAFormer%3A%20Synergistic%20Multi-Attention%20Transformer%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Fuchen%20Zheng%20and%20Xuhang%20Chen%20and%20Weihuang%20Liu%20and%20Haolun%20Li%20and%20Yingtie%20Lei%20and%20Jiahui%20He%20and%20Chi-Man%20Pun%20and%20Shounjun%20Zhou%0AAbstract%3A%20%20%20In%20medical%20image%20segmentation%2C%20specialized%20computer%20vision%20techniques%2C%0Anotably%20transformers%20grounded%20in%20attention%20mechanisms%20and%20residual%20networks%0Aemploying%20skip%20connections%2C%20have%20been%20instrumental%20in%20advancing%20performance.%0ANonetheless%2C%20previous%20models%20often%20falter%20when%20segmenting%20small%2C%20irregularly%0Ashaped%20tumors.%20To%20this%20end%2C%20we%20introduce%20SMAFormer%2C%20an%20efficient%2C%0ATransformer-based%20architecture%20that%20fuses%20multiple%20attention%20mechanisms%20for%0Aenhanced%20segmentation%20of%20small%20tumors%20and%20organs.%20SMAFormer%20can%20capture%20both%0Alocal%20and%20global%20features%20for%20medical%20image%20segmentation.%20The%20architecture%0Acomprises%20two%20pivotal%20components.%20First%2C%20a%20Synergistic%20Multi-Attention%20%28SMA%29%0ATransformer%20block%20is%20proposed%2C%20which%20has%20the%20benefits%20of%20Pixel%20Attention%2C%0AChannel%20Attention%2C%20and%20Spatial%20Attention%20for%20feature%20enrichment.%20Second%2C%0Aaddressing%20the%20challenge%20of%20information%20loss%20incurred%20during%20attention%0Amechanism%20transitions%20and%20feature%20fusion%2C%20we%20design%20a%20Feature%20Fusion%20Modulator.%0AThis%20module%20bolsters%20the%20integration%20between%20the%20channel%20and%20spatial%20attention%0Aby%20mitigating%20reshaping-induced%20information%20attrition.%20To%20evaluate%20our%20method%2C%0Awe%20conduct%20extensive%20experiments%20on%20various%20medical%20image%20segmentation%20tasks%2C%0Aincluding%20multi-organ%2C%20liver%20tumor%2C%20and%20bladder%20tumor%20segmentation%2C%20achieving%0Astate-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/CXH-Research/SMAFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMAFormer%253A%2520Synergistic%2520Multi-Attention%2520Transformer%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DFuchen%2520Zheng%2520and%2520Xuhang%2520Chen%2520and%2520Weihuang%2520Liu%2520and%2520Haolun%2520Li%2520and%2520Yingtie%2520Lei%2520and%2520Jiahui%2520He%2520and%2520Chi-Man%2520Pun%2520and%2520Shounjun%2520Zhou%26entry.1292438233%3D%2520%2520In%2520medical%2520image%2520segmentation%252C%2520specialized%2520computer%2520vision%2520techniques%252C%250Anotably%2520transformers%2520grounded%2520in%2520attention%2520mechanisms%2520and%2520residual%2520networks%250Aemploying%2520skip%2520connections%252C%2520have%2520been%2520instrumental%2520in%2520advancing%2520performance.%250ANonetheless%252C%2520previous%2520models%2520often%2520falter%2520when%2520segmenting%2520small%252C%2520irregularly%250Ashaped%2520tumors.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SMAFormer%252C%2520an%2520efficient%252C%250ATransformer-based%2520architecture%2520that%2520fuses%2520multiple%2520attention%2520mechanisms%2520for%250Aenhanced%2520segmentation%2520of%2520small%2520tumors%2520and%2520organs.%2520SMAFormer%2520can%2520capture%2520both%250Alocal%2520and%2520global%2520features%2520for%2520medical%2520image%2520segmentation.%2520The%2520architecture%250Acomprises%2520two%2520pivotal%2520components.%2520First%252C%2520a%2520Synergistic%2520Multi-Attention%2520%2528SMA%2529%250ATransformer%2520block%2520is%2520proposed%252C%2520which%2520has%2520the%2520benefits%2520of%2520Pixel%2520Attention%252C%250AChannel%2520Attention%252C%2520and%2520Spatial%2520Attention%2520for%2520feature%2520enrichment.%2520Second%252C%250Aaddressing%2520the%2520challenge%2520of%2520information%2520loss%2520incurred%2520during%2520attention%250Amechanism%2520transitions%2520and%2520feature%2520fusion%252C%2520we%2520design%2520a%2520Feature%2520Fusion%2520Modulator.%250AThis%2520module%2520bolsters%2520the%2520integration%2520between%2520the%2520channel%2520and%2520spatial%2520attention%250Aby%2520mitigating%2520reshaping-induced%2520information%2520attrition.%2520To%2520evaluate%2520our%2520method%252C%250Awe%2520conduct%2520extensive%2520experiments%2520on%2520various%2520medical%2520image%2520segmentation%2520tasks%252C%250Aincluding%2520multi-organ%252C%2520liver%2520tumor%252C%2520and%2520bladder%2520tumor%2520segmentation%252C%2520achieving%250Astate-of-the-art%2520results.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/CXH-Research/SMAFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMAFormer%3A%20Synergistic%20Multi-Attention%20Transformer%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Fuchen%20Zheng%20and%20Xuhang%20Chen%20and%20Weihuang%20Liu%20and%20Haolun%20Li%20and%20Yingtie%20Lei%20and%20Jiahui%20He%20and%20Chi-Man%20Pun%20and%20Shounjun%20Zhou&entry.1292438233=%20%20In%20medical%20image%20segmentation%2C%20specialized%20computer%20vision%20techniques%2C%0Anotably%20transformers%20grounded%20in%20attention%20mechanisms%20and%20residual%20networks%0Aemploying%20skip%20connections%2C%20have%20been%20instrumental%20in%20advancing%20performance.%0ANonetheless%2C%20previous%20models%20often%20falter%20when%20segmenting%20small%2C%20irregularly%0Ashaped%20tumors.%20To%20this%20end%2C%20we%20introduce%20SMAFormer%2C%20an%20efficient%2C%0ATransformer-based%20architecture%20that%20fuses%20multiple%20attention%20mechanisms%20for%0Aenhanced%20segmentation%20of%20small%20tumors%20and%20organs.%20SMAFormer%20can%20capture%20both%0Alocal%20and%20global%20features%20for%20medical%20image%20segmentation.%20The%20architecture%0Acomprises%20two%20pivotal%20components.%20First%2C%20a%20Synergistic%20Multi-Attention%20%28SMA%29%0ATransformer%20block%20is%20proposed%2C%20which%20has%20the%20benefits%20of%20Pixel%20Attention%2C%0AChannel%20Attention%2C%20and%20Spatial%20Attention%20for%20feature%20enrichment.%20Second%2C%0Aaddressing%20the%20challenge%20of%20information%20loss%20incurred%20during%20attention%0Amechanism%20transitions%20and%20feature%20fusion%2C%20we%20design%20a%20Feature%20Fusion%20Modulator.%0AThis%20module%20bolsters%20the%20integration%20between%20the%20channel%20and%20spatial%20attention%0Aby%20mitigating%20reshaping-induced%20information%20attrition.%20To%20evaluate%20our%20method%2C%0Awe%20conduct%20extensive%20experiments%20on%20various%20medical%20image%20segmentation%20tasks%2C%0Aincluding%20multi-organ%2C%20liver%20tumor%2C%20and%20bladder%20tumor%20segmentation%2C%20achieving%0Astate-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/CXH-Research/SMAFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00346v2&entry.124074799=Read"},
{"title": "Kinetic and Kinematic Sensors-free Approach for Estimation of Continuous\n  Force and Gesture in sEMG Prosthetic Hands", "author": "Gang Liu and Zhenxiang Wang and Chuanmei Xi and Ziyang He and Shanshan Guo and Rui Zhang and Dezhong Yao", "abstract": "  Regression-based sEMG prosthetic hands are widely used for their ability to\nprovide continuous kinetic and kinematic parameters. However, establishing\nthese models requires complex sensors systems to collect corresponding kinetic\nand kinematic data in synchronization with sEMG, which is cumbersome and\nuser-unfriendly. This paper proposes a kinetic and kinematic sensors-free\napproach for controlling sEMG prosthetic hands, enabling continuous decoding\nand execution of three hand movements: individual finger flexion/extension,\nmultiple finger flexion/extension, and fist opening/closing. This approach\nutilizes only two data points (-1 and 1), representing maximal finger flexion\nforce label and extension force label respectively, and their corresponding\nsEMG data to establish a near-linear model based on sEMG data and labels. The\nmodel's output labels values are used to control the direction and magnitude of\nfingers forces, enabling the estimation of continuous gestures. To validate\nthis approach, we conducted offline and online experiments using four models:\nDendritic Net (DD), Linear Net (LN), Multi-Layer Perceptron (MLP), and\nConvolutional Neural Network (CNN). The offline analysis assessed each model's\nability to classify finger force direction and interpolate intermediate force\nvalues, while online experiments evaluated real-time control performance in\ncontrolling gestures and accurately adjusting forces. Our results demonstrate\nthat the DD and LN models provide excellent real-time control of finger forces\nand gestures, highlighting the practical potential of this sensors-free\napproach for prosthetic applications. This study significantly reduces the\ncomplexity of collecting kinetic and kinematic parameters in sEMG-based\nregression prosthetics, thus enhancing the usability and convenience of\nprosthetic hands.\n", "link": "http://arxiv.org/abs/2407.00014v2", "date": "2024-09-16", "relevancy": 2.2346, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6128}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5568}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinetic%20and%20Kinematic%20Sensors-free%20Approach%20for%20Estimation%20of%20Continuous%0A%20%20Force%20and%20Gesture%20in%20sEMG%20Prosthetic%20Hands&body=Title%3A%20Kinetic%20and%20Kinematic%20Sensors-free%20Approach%20for%20Estimation%20of%20Continuous%0A%20%20Force%20and%20Gesture%20in%20sEMG%20Prosthetic%20Hands%0AAuthor%3A%20Gang%20Liu%20and%20Zhenxiang%20Wang%20and%20Chuanmei%20Xi%20and%20Ziyang%20He%20and%20Shanshan%20Guo%20and%20Rui%20Zhang%20and%20Dezhong%20Yao%0AAbstract%3A%20%20%20Regression-based%20sEMG%20prosthetic%20hands%20are%20widely%20used%20for%20their%20ability%20to%0Aprovide%20continuous%20kinetic%20and%20kinematic%20parameters.%20However%2C%20establishing%0Athese%20models%20requires%20complex%20sensors%20systems%20to%20collect%20corresponding%20kinetic%0Aand%20kinematic%20data%20in%20synchronization%20with%20sEMG%2C%20which%20is%20cumbersome%20and%0Auser-unfriendly.%20This%20paper%20proposes%20a%20kinetic%20and%20kinematic%20sensors-free%0Aapproach%20for%20controlling%20sEMG%20prosthetic%20hands%2C%20enabling%20continuous%20decoding%0Aand%20execution%20of%20three%20hand%20movements%3A%20individual%20finger%20flexion/extension%2C%0Amultiple%20finger%20flexion/extension%2C%20and%20fist%20opening/closing.%20This%20approach%0Autilizes%20only%20two%20data%20points%20%28-1%20and%201%29%2C%20representing%20maximal%20finger%20flexion%0Aforce%20label%20and%20extension%20force%20label%20respectively%2C%20and%20their%20corresponding%0AsEMG%20data%20to%20establish%20a%20near-linear%20model%20based%20on%20sEMG%20data%20and%20labels.%20The%0Amodel%27s%20output%20labels%20values%20are%20used%20to%20control%20the%20direction%20and%20magnitude%20of%0Afingers%20forces%2C%20enabling%20the%20estimation%20of%20continuous%20gestures.%20To%20validate%0Athis%20approach%2C%20we%20conducted%20offline%20and%20online%20experiments%20using%20four%20models%3A%0ADendritic%20Net%20%28DD%29%2C%20Linear%20Net%20%28LN%29%2C%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20and%0AConvolutional%20Neural%20Network%20%28CNN%29.%20The%20offline%20analysis%20assessed%20each%20model%27s%0Aability%20to%20classify%20finger%20force%20direction%20and%20interpolate%20intermediate%20force%0Avalues%2C%20while%20online%20experiments%20evaluated%20real-time%20control%20performance%20in%0Acontrolling%20gestures%20and%20accurately%20adjusting%20forces.%20Our%20results%20demonstrate%0Athat%20the%20DD%20and%20LN%20models%20provide%20excellent%20real-time%20control%20of%20finger%20forces%0Aand%20gestures%2C%20highlighting%20the%20practical%20potential%20of%20this%20sensors-free%0Aapproach%20for%20prosthetic%20applications.%20This%20study%20significantly%20reduces%20the%0Acomplexity%20of%20collecting%20kinetic%20and%20kinematic%20parameters%20in%20sEMG-based%0Aregression%20prosthetics%2C%20thus%20enhancing%20the%20usability%20and%20convenience%20of%0Aprosthetic%20hands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinetic%2520and%2520Kinematic%2520Sensors-free%2520Approach%2520for%2520Estimation%2520of%2520Continuous%250A%2520%2520Force%2520and%2520Gesture%2520in%2520sEMG%2520Prosthetic%2520Hands%26entry.906535625%3DGang%2520Liu%2520and%2520Zhenxiang%2520Wang%2520and%2520Chuanmei%2520Xi%2520and%2520Ziyang%2520He%2520and%2520Shanshan%2520Guo%2520and%2520Rui%2520Zhang%2520and%2520Dezhong%2520Yao%26entry.1292438233%3D%2520%2520Regression-based%2520sEMG%2520prosthetic%2520hands%2520are%2520widely%2520used%2520for%2520their%2520ability%2520to%250Aprovide%2520continuous%2520kinetic%2520and%2520kinematic%2520parameters.%2520However%252C%2520establishing%250Athese%2520models%2520requires%2520complex%2520sensors%2520systems%2520to%2520collect%2520corresponding%2520kinetic%250Aand%2520kinematic%2520data%2520in%2520synchronization%2520with%2520sEMG%252C%2520which%2520is%2520cumbersome%2520and%250Auser-unfriendly.%2520This%2520paper%2520proposes%2520a%2520kinetic%2520and%2520kinematic%2520sensors-free%250Aapproach%2520for%2520controlling%2520sEMG%2520prosthetic%2520hands%252C%2520enabling%2520continuous%2520decoding%250Aand%2520execution%2520of%2520three%2520hand%2520movements%253A%2520individual%2520finger%2520flexion/extension%252C%250Amultiple%2520finger%2520flexion/extension%252C%2520and%2520fist%2520opening/closing.%2520This%2520approach%250Autilizes%2520only%2520two%2520data%2520points%2520%2528-1%2520and%25201%2529%252C%2520representing%2520maximal%2520finger%2520flexion%250Aforce%2520label%2520and%2520extension%2520force%2520label%2520respectively%252C%2520and%2520their%2520corresponding%250AsEMG%2520data%2520to%2520establish%2520a%2520near-linear%2520model%2520based%2520on%2520sEMG%2520data%2520and%2520labels.%2520The%250Amodel%2527s%2520output%2520labels%2520values%2520are%2520used%2520to%2520control%2520the%2520direction%2520and%2520magnitude%2520of%250Afingers%2520forces%252C%2520enabling%2520the%2520estimation%2520of%2520continuous%2520gestures.%2520To%2520validate%250Athis%2520approach%252C%2520we%2520conducted%2520offline%2520and%2520online%2520experiments%2520using%2520four%2520models%253A%250ADendritic%2520Net%2520%2528DD%2529%252C%2520Linear%2520Net%2520%2528LN%2529%252C%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%252C%2520and%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529.%2520The%2520offline%2520analysis%2520assessed%2520each%2520model%2527s%250Aability%2520to%2520classify%2520finger%2520force%2520direction%2520and%2520interpolate%2520intermediate%2520force%250Avalues%252C%2520while%2520online%2520experiments%2520evaluated%2520real-time%2520control%2520performance%2520in%250Acontrolling%2520gestures%2520and%2520accurately%2520adjusting%2520forces.%2520Our%2520results%2520demonstrate%250Athat%2520the%2520DD%2520and%2520LN%2520models%2520provide%2520excellent%2520real-time%2520control%2520of%2520finger%2520forces%250Aand%2520gestures%252C%2520highlighting%2520the%2520practical%2520potential%2520of%2520this%2520sensors-free%250Aapproach%2520for%2520prosthetic%2520applications.%2520This%2520study%2520significantly%2520reduces%2520the%250Acomplexity%2520of%2520collecting%2520kinetic%2520and%2520kinematic%2520parameters%2520in%2520sEMG-based%250Aregression%2520prosthetics%252C%2520thus%2520enhancing%2520the%2520usability%2520and%2520convenience%2520of%250Aprosthetic%2520hands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinetic%20and%20Kinematic%20Sensors-free%20Approach%20for%20Estimation%20of%20Continuous%0A%20%20Force%20and%20Gesture%20in%20sEMG%20Prosthetic%20Hands&entry.906535625=Gang%20Liu%20and%20Zhenxiang%20Wang%20and%20Chuanmei%20Xi%20and%20Ziyang%20He%20and%20Shanshan%20Guo%20and%20Rui%20Zhang%20and%20Dezhong%20Yao&entry.1292438233=%20%20Regression-based%20sEMG%20prosthetic%20hands%20are%20widely%20used%20for%20their%20ability%20to%0Aprovide%20continuous%20kinetic%20and%20kinematic%20parameters.%20However%2C%20establishing%0Athese%20models%20requires%20complex%20sensors%20systems%20to%20collect%20corresponding%20kinetic%0Aand%20kinematic%20data%20in%20synchronization%20with%20sEMG%2C%20which%20is%20cumbersome%20and%0Auser-unfriendly.%20This%20paper%20proposes%20a%20kinetic%20and%20kinematic%20sensors-free%0Aapproach%20for%20controlling%20sEMG%20prosthetic%20hands%2C%20enabling%20continuous%20decoding%0Aand%20execution%20of%20three%20hand%20movements%3A%20individual%20finger%20flexion/extension%2C%0Amultiple%20finger%20flexion/extension%2C%20and%20fist%20opening/closing.%20This%20approach%0Autilizes%20only%20two%20data%20points%20%28-1%20and%201%29%2C%20representing%20maximal%20finger%20flexion%0Aforce%20label%20and%20extension%20force%20label%20respectively%2C%20and%20their%20corresponding%0AsEMG%20data%20to%20establish%20a%20near-linear%20model%20based%20on%20sEMG%20data%20and%20labels.%20The%0Amodel%27s%20output%20labels%20values%20are%20used%20to%20control%20the%20direction%20and%20magnitude%20of%0Afingers%20forces%2C%20enabling%20the%20estimation%20of%20continuous%20gestures.%20To%20validate%0Athis%20approach%2C%20we%20conducted%20offline%20and%20online%20experiments%20using%20four%20models%3A%0ADendritic%20Net%20%28DD%29%2C%20Linear%20Net%20%28LN%29%2C%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20and%0AConvolutional%20Neural%20Network%20%28CNN%29.%20The%20offline%20analysis%20assessed%20each%20model%27s%0Aability%20to%20classify%20finger%20force%20direction%20and%20interpolate%20intermediate%20force%0Avalues%2C%20while%20online%20experiments%20evaluated%20real-time%20control%20performance%20in%0Acontrolling%20gestures%20and%20accurately%20adjusting%20forces.%20Our%20results%20demonstrate%0Athat%20the%20DD%20and%20LN%20models%20provide%20excellent%20real-time%20control%20of%20finger%20forces%0Aand%20gestures%2C%20highlighting%20the%20practical%20potential%20of%20this%20sensors-free%0Aapproach%20for%20prosthetic%20applications.%20This%20study%20significantly%20reduces%20the%0Acomplexity%20of%20collecting%20kinetic%20and%20kinematic%20parameters%20in%20sEMG-based%0Aregression%20prosthetics%2C%20thus%20enhancing%20the%20usability%20and%20convenience%20of%0Aprosthetic%20hands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00014v2&entry.124074799=Read"},
{"title": "SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely\n  Incomplete Depth Maps", "author": "Jakub Gregorek and Lazaros Nalpantidis", "abstract": "  Even if the depth maps captured by RGB-D sensors deployed in real\nenvironments are often characterized by large areas missing valid depth\nmeasurements, the vast majority of depth completion methods still assumes depth\nvalues covering all areas of the scene. To address this limitation, we\nintroduce SteeredMarigold, a training-free, zero-shot depth completion method\ncapable of producing metric dense depth, even for largely incomplete depth\nmaps. SteeredMarigold achieves this by using the available sparse depth points\nas conditions to steer a denoising diffusion probabilistic model. Our method\noutperforms relevant top-performing methods on the NYUv2 dataset, in tests\nwhere no depth was provided for a large area, achieving state-of-art\nperformance and exhibiting remarkable robustness against depth map\nincompleteness. Our code will be publicly available.\n", "link": "http://arxiv.org/abs/2409.10202v1", "date": "2024-09-16", "relevancy": 2.2176, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5619}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SteeredMarigold%3A%20Steering%20Diffusion%20Towards%20Depth%20Completion%20of%20Largely%0A%20%20Incomplete%20Depth%20Maps&body=Title%3A%20SteeredMarigold%3A%20Steering%20Diffusion%20Towards%20Depth%20Completion%20of%20Largely%0A%20%20Incomplete%20Depth%20Maps%0AAuthor%3A%20Jakub%20Gregorek%20and%20Lazaros%20Nalpantidis%0AAbstract%3A%20%20%20Even%20if%20the%20depth%20maps%20captured%20by%20RGB-D%20sensors%20deployed%20in%20real%0Aenvironments%20are%20often%20characterized%20by%20large%20areas%20missing%20valid%20depth%0Ameasurements%2C%20the%20vast%20majority%20of%20depth%20completion%20methods%20still%20assumes%20depth%0Avalues%20covering%20all%20areas%20of%20the%20scene.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20SteeredMarigold%2C%20a%20training-free%2C%20zero-shot%20depth%20completion%20method%0Acapable%20of%20producing%20metric%20dense%20depth%2C%20even%20for%20largely%20incomplete%20depth%0Amaps.%20SteeredMarigold%20achieves%20this%20by%20using%20the%20available%20sparse%20depth%20points%0Aas%20conditions%20to%20steer%20a%20denoising%20diffusion%20probabilistic%20model.%20Our%20method%0Aoutperforms%20relevant%20top-performing%20methods%20on%20the%20NYUv2%20dataset%2C%20in%20tests%0Awhere%20no%20depth%20was%20provided%20for%20a%20large%20area%2C%20achieving%20state-of-art%0Aperformance%20and%20exhibiting%20remarkable%20robustness%20against%20depth%20map%0Aincompleteness.%20Our%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteeredMarigold%253A%2520Steering%2520Diffusion%2520Towards%2520Depth%2520Completion%2520of%2520Largely%250A%2520%2520Incomplete%2520Depth%2520Maps%26entry.906535625%3DJakub%2520Gregorek%2520and%2520Lazaros%2520Nalpantidis%26entry.1292438233%3D%2520%2520Even%2520if%2520the%2520depth%2520maps%2520captured%2520by%2520RGB-D%2520sensors%2520deployed%2520in%2520real%250Aenvironments%2520are%2520often%2520characterized%2520by%2520large%2520areas%2520missing%2520valid%2520depth%250Ameasurements%252C%2520the%2520vast%2520majority%2520of%2520depth%2520completion%2520methods%2520still%2520assumes%2520depth%250Avalues%2520covering%2520all%2520areas%2520of%2520the%2520scene.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520SteeredMarigold%252C%2520a%2520training-free%252C%2520zero-shot%2520depth%2520completion%2520method%250Acapable%2520of%2520producing%2520metric%2520dense%2520depth%252C%2520even%2520for%2520largely%2520incomplete%2520depth%250Amaps.%2520SteeredMarigold%2520achieves%2520this%2520by%2520using%2520the%2520available%2520sparse%2520depth%2520points%250Aas%2520conditions%2520to%2520steer%2520a%2520denoising%2520diffusion%2520probabilistic%2520model.%2520Our%2520method%250Aoutperforms%2520relevant%2520top-performing%2520methods%2520on%2520the%2520NYUv2%2520dataset%252C%2520in%2520tests%250Awhere%2520no%2520depth%2520was%2520provided%2520for%2520a%2520large%2520area%252C%2520achieving%2520state-of-art%250Aperformance%2520and%2520exhibiting%2520remarkable%2520robustness%2520against%2520depth%2520map%250Aincompleteness.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteeredMarigold%3A%20Steering%20Diffusion%20Towards%20Depth%20Completion%20of%20Largely%0A%20%20Incomplete%20Depth%20Maps&entry.906535625=Jakub%20Gregorek%20and%20Lazaros%20Nalpantidis&entry.1292438233=%20%20Even%20if%20the%20depth%20maps%20captured%20by%20RGB-D%20sensors%20deployed%20in%20real%0Aenvironments%20are%20often%20characterized%20by%20large%20areas%20missing%20valid%20depth%0Ameasurements%2C%20the%20vast%20majority%20of%20depth%20completion%20methods%20still%20assumes%20depth%0Avalues%20covering%20all%20areas%20of%20the%20scene.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20SteeredMarigold%2C%20a%20training-free%2C%20zero-shot%20depth%20completion%20method%0Acapable%20of%20producing%20metric%20dense%20depth%2C%20even%20for%20largely%20incomplete%20depth%0Amaps.%20SteeredMarigold%20achieves%20this%20by%20using%20the%20available%20sparse%20depth%20points%0Aas%20conditions%20to%20steer%20a%20denoising%20diffusion%20probabilistic%20model.%20Our%20method%0Aoutperforms%20relevant%20top-performing%20methods%20on%20the%20NYUv2%20dataset%2C%20in%20tests%0Awhere%20no%20depth%20was%20provided%20for%20a%20large%20area%2C%20achieving%20state-of-art%0Aperformance%20and%20exhibiting%20remarkable%20robustness%20against%20depth%20map%0Aincompleteness.%20Our%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10202v1&entry.124074799=Read"},
{"title": "BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic\n  Segmentation of Urban Remote Sensing Images", "author": "Wentao Wang and Xili Wang", "abstract": "  Large-scale semantic segmentation networks often achieve high performance,\nwhile their application can be challenging when faced with limited sample sizes\nand computational resources. In scenarios with restricted network size and\ncomputational complexity, models encounter significant challenges in capturing\nlong-range dependencies and recovering detailed information in images. We\npropose a lightweight bilateral semantic segmentation network called bilateral\nattention fusion network (BAFNet) to efficiently segment high-resolution urban\nremote sensing images. The model consists of two paths, namely dependency path\nand remote-local path. The dependency path utilizes large kernel attention to\nacquire long-range dependencies in the image. Besides, multi-scale local\nattention and efficient remote attention are designed to construct remote-local\npath. Finally, a feature aggregation module is designed to effectively utilize\nthe different features of the two paths. Our proposed method was tested on\npublic high-resolution urban remote sensing datasets Vaihingen and Potsdam,\nwith mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic\nsegmentation model, BAFNet not only outperforms advanced lightweight models in\naccuracy but also demonstrates comparable performance to non-lightweight\nstate-of-the-art methods on two datasets, despite a tenfold variance in\nfloating-point operations and a fifteenfold difference in network parameters.\n", "link": "http://arxiv.org/abs/2409.10269v1", "date": "2024-09-16", "relevancy": 2.2086, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5532}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAFNet%3A%20Bilateral%20Attention%20Fusion%20Network%20for%20Lightweight%20Semantic%0A%20%20Segmentation%20of%20Urban%20Remote%20Sensing%20Images&body=Title%3A%20BAFNet%3A%20Bilateral%20Attention%20Fusion%20Network%20for%20Lightweight%20Semantic%0A%20%20Segmentation%20of%20Urban%20Remote%20Sensing%20Images%0AAuthor%3A%20Wentao%20Wang%20and%20Xili%20Wang%0AAbstract%3A%20%20%20Large-scale%20semantic%20segmentation%20networks%20often%20achieve%20high%20performance%2C%0Awhile%20their%20application%20can%20be%20challenging%20when%20faced%20with%20limited%20sample%20sizes%0Aand%20computational%20resources.%20In%20scenarios%20with%20restricted%20network%20size%20and%0Acomputational%20complexity%2C%20models%20encounter%20significant%20challenges%20in%20capturing%0Along-range%20dependencies%20and%20recovering%20detailed%20information%20in%20images.%20We%0Apropose%20a%20lightweight%20bilateral%20semantic%20segmentation%20network%20called%20bilateral%0Aattention%20fusion%20network%20%28BAFNet%29%20to%20efficiently%20segment%20high-resolution%20urban%0Aremote%20sensing%20images.%20The%20model%20consists%20of%20two%20paths%2C%20namely%20dependency%20path%0Aand%20remote-local%20path.%20The%20dependency%20path%20utilizes%20large%20kernel%20attention%20to%0Aacquire%20long-range%20dependencies%20in%20the%20image.%20Besides%2C%20multi-scale%20local%0Aattention%20and%20efficient%20remote%20attention%20are%20designed%20to%20construct%20remote-local%0Apath.%20Finally%2C%20a%20feature%20aggregation%20module%20is%20designed%20to%20effectively%20utilize%0Athe%20different%20features%20of%20the%20two%20paths.%20Our%20proposed%20method%20was%20tested%20on%0Apublic%20high-resolution%20urban%20remote%20sensing%20datasets%20Vaihingen%20and%20Potsdam%2C%0Awith%20mIoU%20reaching%2083.20%25%20and%2086.53%25%2C%20respectively.%20As%20a%20lightweight%20semantic%0Asegmentation%20model%2C%20BAFNet%20not%20only%20outperforms%20advanced%20lightweight%20models%20in%0Aaccuracy%20but%20also%20demonstrates%20comparable%20performance%20to%20non-lightweight%0Astate-of-the-art%20methods%20on%20two%20datasets%2C%20despite%20a%20tenfold%20variance%20in%0Afloating-point%20operations%20and%20a%20fifteenfold%20difference%20in%20network%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAFNet%253A%2520Bilateral%2520Attention%2520Fusion%2520Network%2520for%2520Lightweight%2520Semantic%250A%2520%2520Segmentation%2520of%2520Urban%2520Remote%2520Sensing%2520Images%26entry.906535625%3DWentao%2520Wang%2520and%2520Xili%2520Wang%26entry.1292438233%3D%2520%2520Large-scale%2520semantic%2520segmentation%2520networks%2520often%2520achieve%2520high%2520performance%252C%250Awhile%2520their%2520application%2520can%2520be%2520challenging%2520when%2520faced%2520with%2520limited%2520sample%2520sizes%250Aand%2520computational%2520resources.%2520In%2520scenarios%2520with%2520restricted%2520network%2520size%2520and%250Acomputational%2520complexity%252C%2520models%2520encounter%2520significant%2520challenges%2520in%2520capturing%250Along-range%2520dependencies%2520and%2520recovering%2520detailed%2520information%2520in%2520images.%2520We%250Apropose%2520a%2520lightweight%2520bilateral%2520semantic%2520segmentation%2520network%2520called%2520bilateral%250Aattention%2520fusion%2520network%2520%2528BAFNet%2529%2520to%2520efficiently%2520segment%2520high-resolution%2520urban%250Aremote%2520sensing%2520images.%2520The%2520model%2520consists%2520of%2520two%2520paths%252C%2520namely%2520dependency%2520path%250Aand%2520remote-local%2520path.%2520The%2520dependency%2520path%2520utilizes%2520large%2520kernel%2520attention%2520to%250Aacquire%2520long-range%2520dependencies%2520in%2520the%2520image.%2520Besides%252C%2520multi-scale%2520local%250Aattention%2520and%2520efficient%2520remote%2520attention%2520are%2520designed%2520to%2520construct%2520remote-local%250Apath.%2520Finally%252C%2520a%2520feature%2520aggregation%2520module%2520is%2520designed%2520to%2520effectively%2520utilize%250Athe%2520different%2520features%2520of%2520the%2520two%2520paths.%2520Our%2520proposed%2520method%2520was%2520tested%2520on%250Apublic%2520high-resolution%2520urban%2520remote%2520sensing%2520datasets%2520Vaihingen%2520and%2520Potsdam%252C%250Awith%2520mIoU%2520reaching%252083.20%2525%2520and%252086.53%2525%252C%2520respectively.%2520As%2520a%2520lightweight%2520semantic%250Asegmentation%2520model%252C%2520BAFNet%2520not%2520only%2520outperforms%2520advanced%2520lightweight%2520models%2520in%250Aaccuracy%2520but%2520also%2520demonstrates%2520comparable%2520performance%2520to%2520non-lightweight%250Astate-of-the-art%2520methods%2520on%2520two%2520datasets%252C%2520despite%2520a%2520tenfold%2520variance%2520in%250Afloating-point%2520operations%2520and%2520a%2520fifteenfold%2520difference%2520in%2520network%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAFNet%3A%20Bilateral%20Attention%20Fusion%20Network%20for%20Lightweight%20Semantic%0A%20%20Segmentation%20of%20Urban%20Remote%20Sensing%20Images&entry.906535625=Wentao%20Wang%20and%20Xili%20Wang&entry.1292438233=%20%20Large-scale%20semantic%20segmentation%20networks%20often%20achieve%20high%20performance%2C%0Awhile%20their%20application%20can%20be%20challenging%20when%20faced%20with%20limited%20sample%20sizes%0Aand%20computational%20resources.%20In%20scenarios%20with%20restricted%20network%20size%20and%0Acomputational%20complexity%2C%20models%20encounter%20significant%20challenges%20in%20capturing%0Along-range%20dependencies%20and%20recovering%20detailed%20information%20in%20images.%20We%0Apropose%20a%20lightweight%20bilateral%20semantic%20segmentation%20network%20called%20bilateral%0Aattention%20fusion%20network%20%28BAFNet%29%20to%20efficiently%20segment%20high-resolution%20urban%0Aremote%20sensing%20images.%20The%20model%20consists%20of%20two%20paths%2C%20namely%20dependency%20path%0Aand%20remote-local%20path.%20The%20dependency%20path%20utilizes%20large%20kernel%20attention%20to%0Aacquire%20long-range%20dependencies%20in%20the%20image.%20Besides%2C%20multi-scale%20local%0Aattention%20and%20efficient%20remote%20attention%20are%20designed%20to%20construct%20remote-local%0Apath.%20Finally%2C%20a%20feature%20aggregation%20module%20is%20designed%20to%20effectively%20utilize%0Athe%20different%20features%20of%20the%20two%20paths.%20Our%20proposed%20method%20was%20tested%20on%0Apublic%20high-resolution%20urban%20remote%20sensing%20datasets%20Vaihingen%20and%20Potsdam%2C%0Awith%20mIoU%20reaching%2083.20%25%20and%2086.53%25%2C%20respectively.%20As%20a%20lightweight%20semantic%0Asegmentation%20model%2C%20BAFNet%20not%20only%20outperforms%20advanced%20lightweight%20models%20in%0Aaccuracy%20but%20also%20demonstrates%20comparable%20performance%20to%20non-lightweight%0Astate-of-the-art%20methods%20on%20two%20datasets%2C%20despite%20a%20tenfold%20variance%20in%0Afloating-point%20operations%20and%20a%20fifteenfold%20difference%20in%20network%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10269v1&entry.124074799=Read"},
{"title": "Radar Teach and Repeat: Architecture and Initial Field Testing", "author": "Xinyuan Qiao and Alexander Krawciw and Sven Lilge and Timothy D. Barfoot", "abstract": "  Frequency-modulated continuous-wave (FMCW) scanning radar has emerged as an\nalternative to spinning LiDAR for state estimation on mobile robots. Radar's\nlonger wavelength is less affected by small particulates, providing operational\nadvantages in challenging environments such as dust, smoke, and fog. This paper\npresents Radar Teach and Repeat (RT&R): a full-stack radar system for long-term\noff-road robot autonomy. RT&R can drive routes reliably in off-road cluttered\nareas without any GPS. We benchmark the radar system's closed-loop\npath-tracking performance and compare it to its 3D LiDAR counterpart. 11.8 km\nof autonomous driving was completed without interventions using only radar and\ngyro for navigation. RT&R was evaluated on different routes with progressively\nless structured scene geometry. RT&R achieved lateral path-tracking root mean\nsquared errors (RMSE) of 5.6 cm, 7.5 cm, and 12.1 cm as the routes became more\nchallenging. On the robot we used for testing, these RMSE values are less than\nhalf of the width of one tire (24 cm). These same routes have worst-case errors\nof 21.7 cm, 24.0 cm, and 43.8 cm. We conclude that radar is a viable\nalternative to LiDAR for long-term autonomy in challenging off-road scenarios.\nThe implementation of RT&R is open-source and available at:\nhttps://github.com/utiasASRL/vtr3.\n", "link": "http://arxiv.org/abs/2409.10491v1", "date": "2024-09-16", "relevancy": 2.2025, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radar%20Teach%20and%20Repeat%3A%20Architecture%20and%20Initial%20Field%20Testing&body=Title%3A%20Radar%20Teach%20and%20Repeat%3A%20Architecture%20and%20Initial%20Field%20Testing%0AAuthor%3A%20Xinyuan%20Qiao%20and%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Frequency-modulated%20continuous-wave%20%28FMCW%29%20scanning%20radar%20has%20emerged%20as%20an%0Aalternative%20to%20spinning%20LiDAR%20for%20state%20estimation%20on%20mobile%20robots.%20Radar%27s%0Alonger%20wavelength%20is%20less%20affected%20by%20small%20particulates%2C%20providing%20operational%0Aadvantages%20in%20challenging%20environments%20such%20as%20dust%2C%20smoke%2C%20and%20fog.%20This%20paper%0Apresents%20Radar%20Teach%20and%20Repeat%20%28RT%26R%29%3A%20a%20full-stack%20radar%20system%20for%20long-term%0Aoff-road%20robot%20autonomy.%20RT%26R%20can%20drive%20routes%20reliably%20in%20off-road%20cluttered%0Aareas%20without%20any%20GPS.%20We%20benchmark%20the%20radar%20system%27s%20closed-loop%0Apath-tracking%20performance%20and%20compare%20it%20to%20its%203D%20LiDAR%20counterpart.%2011.8%20km%0Aof%20autonomous%20driving%20was%20completed%20without%20interventions%20using%20only%20radar%20and%0Agyro%20for%20navigation.%20RT%26R%20was%20evaluated%20on%20different%20routes%20with%20progressively%0Aless%20structured%20scene%20geometry.%20RT%26R%20achieved%20lateral%20path-tracking%20root%20mean%0Asquared%20errors%20%28RMSE%29%20of%205.6%20cm%2C%207.5%20cm%2C%20and%2012.1%20cm%20as%20the%20routes%20became%20more%0Achallenging.%20On%20the%20robot%20we%20used%20for%20testing%2C%20these%20RMSE%20values%20are%20less%20than%0Ahalf%20of%20the%20width%20of%20one%20tire%20%2824%20cm%29.%20These%20same%20routes%20have%20worst-case%20errors%0Aof%2021.7%20cm%2C%2024.0%20cm%2C%20and%2043.8%20cm.%20We%20conclude%20that%20radar%20is%20a%20viable%0Aalternative%20to%20LiDAR%20for%20long-term%20autonomy%20in%20challenging%20off-road%20scenarios.%0AThe%20implementation%20of%20RT%26R%20is%20open-source%20and%20available%20at%3A%0Ahttps%3A//github.com/utiasASRL/vtr3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadar%2520Teach%2520and%2520Repeat%253A%2520Architecture%2520and%2520Initial%2520Field%2520Testing%26entry.906535625%3DXinyuan%2520Qiao%2520and%2520Alexander%2520Krawciw%2520and%2520Sven%2520Lilge%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Frequency-modulated%2520continuous-wave%2520%2528FMCW%2529%2520scanning%2520radar%2520has%2520emerged%2520as%2520an%250Aalternative%2520to%2520spinning%2520LiDAR%2520for%2520state%2520estimation%2520on%2520mobile%2520robots.%2520Radar%2527s%250Alonger%2520wavelength%2520is%2520less%2520affected%2520by%2520small%2520particulates%252C%2520providing%2520operational%250Aadvantages%2520in%2520challenging%2520environments%2520such%2520as%2520dust%252C%2520smoke%252C%2520and%2520fog.%2520This%2520paper%250Apresents%2520Radar%2520Teach%2520and%2520Repeat%2520%2528RT%2526R%2529%253A%2520a%2520full-stack%2520radar%2520system%2520for%2520long-term%250Aoff-road%2520robot%2520autonomy.%2520RT%2526R%2520can%2520drive%2520routes%2520reliably%2520in%2520off-road%2520cluttered%250Aareas%2520without%2520any%2520GPS.%2520We%2520benchmark%2520the%2520radar%2520system%2527s%2520closed-loop%250Apath-tracking%2520performance%2520and%2520compare%2520it%2520to%2520its%25203D%2520LiDAR%2520counterpart.%252011.8%2520km%250Aof%2520autonomous%2520driving%2520was%2520completed%2520without%2520interventions%2520using%2520only%2520radar%2520and%250Agyro%2520for%2520navigation.%2520RT%2526R%2520was%2520evaluated%2520on%2520different%2520routes%2520with%2520progressively%250Aless%2520structured%2520scene%2520geometry.%2520RT%2526R%2520achieved%2520lateral%2520path-tracking%2520root%2520mean%250Asquared%2520errors%2520%2528RMSE%2529%2520of%25205.6%2520cm%252C%25207.5%2520cm%252C%2520and%252012.1%2520cm%2520as%2520the%2520routes%2520became%2520more%250Achallenging.%2520On%2520the%2520robot%2520we%2520used%2520for%2520testing%252C%2520these%2520RMSE%2520values%2520are%2520less%2520than%250Ahalf%2520of%2520the%2520width%2520of%2520one%2520tire%2520%252824%2520cm%2529.%2520These%2520same%2520routes%2520have%2520worst-case%2520errors%250Aof%252021.7%2520cm%252C%252024.0%2520cm%252C%2520and%252043.8%2520cm.%2520We%2520conclude%2520that%2520radar%2520is%2520a%2520viable%250Aalternative%2520to%2520LiDAR%2520for%2520long-term%2520autonomy%2520in%2520challenging%2520off-road%2520scenarios.%250AThe%2520implementation%2520of%2520RT%2526R%2520is%2520open-source%2520and%2520available%2520at%253A%250Ahttps%253A//github.com/utiasASRL/vtr3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radar%20Teach%20and%20Repeat%3A%20Architecture%20and%20Initial%20Field%20Testing&entry.906535625=Xinyuan%20Qiao%20and%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Frequency-modulated%20continuous-wave%20%28FMCW%29%20scanning%20radar%20has%20emerged%20as%20an%0Aalternative%20to%20spinning%20LiDAR%20for%20state%20estimation%20on%20mobile%20robots.%20Radar%27s%0Alonger%20wavelength%20is%20less%20affected%20by%20small%20particulates%2C%20providing%20operational%0Aadvantages%20in%20challenging%20environments%20such%20as%20dust%2C%20smoke%2C%20and%20fog.%20This%20paper%0Apresents%20Radar%20Teach%20and%20Repeat%20%28RT%26R%29%3A%20a%20full-stack%20radar%20system%20for%20long-term%0Aoff-road%20robot%20autonomy.%20RT%26R%20can%20drive%20routes%20reliably%20in%20off-road%20cluttered%0Aareas%20without%20any%20GPS.%20We%20benchmark%20the%20radar%20system%27s%20closed-loop%0Apath-tracking%20performance%20and%20compare%20it%20to%20its%203D%20LiDAR%20counterpart.%2011.8%20km%0Aof%20autonomous%20driving%20was%20completed%20without%20interventions%20using%20only%20radar%20and%0Agyro%20for%20navigation.%20RT%26R%20was%20evaluated%20on%20different%20routes%20with%20progressively%0Aless%20structured%20scene%20geometry.%20RT%26R%20achieved%20lateral%20path-tracking%20root%20mean%0Asquared%20errors%20%28RMSE%29%20of%205.6%20cm%2C%207.5%20cm%2C%20and%2012.1%20cm%20as%20the%20routes%20became%20more%0Achallenging.%20On%20the%20robot%20we%20used%20for%20testing%2C%20these%20RMSE%20values%20are%20less%20than%0Ahalf%20of%20the%20width%20of%20one%20tire%20%2824%20cm%29.%20These%20same%20routes%20have%20worst-case%20errors%0Aof%2021.7%20cm%2C%2024.0%20cm%2C%20and%2043.8%20cm.%20We%20conclude%20that%20radar%20is%20a%20viable%0Aalternative%20to%20LiDAR%20for%20long-term%20autonomy%20in%20challenging%20off-road%20scenarios.%0AThe%20implementation%20of%20RT%26R%20is%20open-source%20and%20available%20at%3A%0Ahttps%3A//github.com/utiasASRL/vtr3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10491v1&entry.124074799=Read"},
{"title": "Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot\n  Segmentation", "author": "Hanbo Bi and Yingchao Feng and Wenhui Diao and Peijin Wang and Yongqiang Mao and Kun Fu and Hongqi Wang and Xian Sun", "abstract": "  For more efficient generalization to unseen domains (classes), most Few-shot\nSegmentation (FSS) would directly exploit pre-trained encoders and only\nfine-tune the decoder, especially in the current era of large models. However,\nsuch fixed feature encoders tend to be class-agnostic, inevitably activating\nobjects that are irrelevant to the target class. In contrast, humans can\neffortlessly focus on specific objects in the line of sight. This paper mimics\nthe visual perception pattern of human beings and proposes a novel and powerful\nprompt-driven scheme, called ``Prompt and Transfer\" (PAT), which constructs a\ndynamic class-aware prompting paradigm to tune the encoder for focusing on the\ninterested object (target class) in the current task. Three key points are\nelaborated to enhance the prompting: 1) Cross-modal linguistic information is\nintroduced to initialize prompts for each task. 2) Semantic Prompt Transfer\n(SPT) that precisely transfers the class-specific semantics within the images\nto prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to\nadaptively generate different but complementary part prompts for different\nindividuals. Surprisingly, PAT achieves competitive performance on 4 different\ntasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote\nsensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new\nstate-of-the-arts on 11 benchmarks.\n", "link": "http://arxiv.org/abs/2409.10389v1", "date": "2024-09-16", "relevancy": 2.1898, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5544}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-and-Transfer%3A%20Dynamic%20Class-aware%20Enhancement%20for%20Few-shot%0A%20%20Segmentation&body=Title%3A%20Prompt-and-Transfer%3A%20Dynamic%20Class-aware%20Enhancement%20for%20Few-shot%0A%20%20Segmentation%0AAuthor%3A%20Hanbo%20Bi%20and%20Yingchao%20Feng%20and%20Wenhui%20Diao%20and%20Peijin%20Wang%20and%20Yongqiang%20Mao%20and%20Kun%20Fu%20and%20Hongqi%20Wang%20and%20Xian%20Sun%0AAbstract%3A%20%20%20For%20more%20efficient%20generalization%20to%20unseen%20domains%20%28classes%29%2C%20most%20Few-shot%0ASegmentation%20%28FSS%29%20would%20directly%20exploit%20pre-trained%20encoders%20and%20only%0Afine-tune%20the%20decoder%2C%20especially%20in%20the%20current%20era%20of%20large%20models.%20However%2C%0Asuch%20fixed%20feature%20encoders%20tend%20to%20be%20class-agnostic%2C%20inevitably%20activating%0Aobjects%20that%20are%20irrelevant%20to%20the%20target%20class.%20In%20contrast%2C%20humans%20can%0Aeffortlessly%20focus%20on%20specific%20objects%20in%20the%20line%20of%20sight.%20This%20paper%20mimics%0Athe%20visual%20perception%20pattern%20of%20human%20beings%20and%20proposes%20a%20novel%20and%20powerful%0Aprompt-driven%20scheme%2C%20called%20%60%60Prompt%20and%20Transfer%22%20%28PAT%29%2C%20which%20constructs%20a%0Adynamic%20class-aware%20prompting%20paradigm%20to%20tune%20the%20encoder%20for%20focusing%20on%20the%0Ainterested%20object%20%28target%20class%29%20in%20the%20current%20task.%20Three%20key%20points%20are%0Aelaborated%20to%20enhance%20the%20prompting%3A%201%29%20Cross-modal%20linguistic%20information%20is%0Aintroduced%20to%20initialize%20prompts%20for%20each%20task.%202%29%20Semantic%20Prompt%20Transfer%0A%28SPT%29%20that%20precisely%20transfers%20the%20class-specific%20semantics%20within%20the%20images%0Ato%20prompts.%203%29%20Part%20Mask%20Generator%20%28PMG%29%20that%20works%20in%20conjunction%20with%20SPT%20to%0Aadaptively%20generate%20different%20but%20complementary%20part%20prompts%20for%20different%0Aindividuals.%20Surprisingly%2C%20PAT%20achieves%20competitive%20performance%20on%204%20different%0Atasks%20including%20standard%20FSS%2C%20Cross-domain%20FSS%20%28e.g.%2C%20CV%2C%20medical%2C%20and%20remote%0Asensing%20domains%29%2C%20Weak-label%20FSS%2C%20and%20Zero-shot%20Segmentation%2C%20setting%20new%0Astate-of-the-arts%20on%2011%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-and-Transfer%253A%2520Dynamic%2520Class-aware%2520Enhancement%2520for%2520Few-shot%250A%2520%2520Segmentation%26entry.906535625%3DHanbo%2520Bi%2520and%2520Yingchao%2520Feng%2520and%2520Wenhui%2520Diao%2520and%2520Peijin%2520Wang%2520and%2520Yongqiang%2520Mao%2520and%2520Kun%2520Fu%2520and%2520Hongqi%2520Wang%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520For%2520more%2520efficient%2520generalization%2520to%2520unseen%2520domains%2520%2528classes%2529%252C%2520most%2520Few-shot%250ASegmentation%2520%2528FSS%2529%2520would%2520directly%2520exploit%2520pre-trained%2520encoders%2520and%2520only%250Afine-tune%2520the%2520decoder%252C%2520especially%2520in%2520the%2520current%2520era%2520of%2520large%2520models.%2520However%252C%250Asuch%2520fixed%2520feature%2520encoders%2520tend%2520to%2520be%2520class-agnostic%252C%2520inevitably%2520activating%250Aobjects%2520that%2520are%2520irrelevant%2520to%2520the%2520target%2520class.%2520In%2520contrast%252C%2520humans%2520can%250Aeffortlessly%2520focus%2520on%2520specific%2520objects%2520in%2520the%2520line%2520of%2520sight.%2520This%2520paper%2520mimics%250Athe%2520visual%2520perception%2520pattern%2520of%2520human%2520beings%2520and%2520proposes%2520a%2520novel%2520and%2520powerful%250Aprompt-driven%2520scheme%252C%2520called%2520%2560%2560Prompt%2520and%2520Transfer%2522%2520%2528PAT%2529%252C%2520which%2520constructs%2520a%250Adynamic%2520class-aware%2520prompting%2520paradigm%2520to%2520tune%2520the%2520encoder%2520for%2520focusing%2520on%2520the%250Ainterested%2520object%2520%2528target%2520class%2529%2520in%2520the%2520current%2520task.%2520Three%2520key%2520points%2520are%250Aelaborated%2520to%2520enhance%2520the%2520prompting%253A%25201%2529%2520Cross-modal%2520linguistic%2520information%2520is%250Aintroduced%2520to%2520initialize%2520prompts%2520for%2520each%2520task.%25202%2529%2520Semantic%2520Prompt%2520Transfer%250A%2528SPT%2529%2520that%2520precisely%2520transfers%2520the%2520class-specific%2520semantics%2520within%2520the%2520images%250Ato%2520prompts.%25203%2529%2520Part%2520Mask%2520Generator%2520%2528PMG%2529%2520that%2520works%2520in%2520conjunction%2520with%2520SPT%2520to%250Aadaptively%2520generate%2520different%2520but%2520complementary%2520part%2520prompts%2520for%2520different%250Aindividuals.%2520Surprisingly%252C%2520PAT%2520achieves%2520competitive%2520performance%2520on%25204%2520different%250Atasks%2520including%2520standard%2520FSS%252C%2520Cross-domain%2520FSS%2520%2528e.g.%252C%2520CV%252C%2520medical%252C%2520and%2520remote%250Asensing%2520domains%2529%252C%2520Weak-label%2520FSS%252C%2520and%2520Zero-shot%2520Segmentation%252C%2520setting%2520new%250Astate-of-the-arts%2520on%252011%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-and-Transfer%3A%20Dynamic%20Class-aware%20Enhancement%20for%20Few-shot%0A%20%20Segmentation&entry.906535625=Hanbo%20Bi%20and%20Yingchao%20Feng%20and%20Wenhui%20Diao%20and%20Peijin%20Wang%20and%20Yongqiang%20Mao%20and%20Kun%20Fu%20and%20Hongqi%20Wang%20and%20Xian%20Sun&entry.1292438233=%20%20For%20more%20efficient%20generalization%20to%20unseen%20domains%20%28classes%29%2C%20most%20Few-shot%0ASegmentation%20%28FSS%29%20would%20directly%20exploit%20pre-trained%20encoders%20and%20only%0Afine-tune%20the%20decoder%2C%20especially%20in%20the%20current%20era%20of%20large%20models.%20However%2C%0Asuch%20fixed%20feature%20encoders%20tend%20to%20be%20class-agnostic%2C%20inevitably%20activating%0Aobjects%20that%20are%20irrelevant%20to%20the%20target%20class.%20In%20contrast%2C%20humans%20can%0Aeffortlessly%20focus%20on%20specific%20objects%20in%20the%20line%20of%20sight.%20This%20paper%20mimics%0Athe%20visual%20perception%20pattern%20of%20human%20beings%20and%20proposes%20a%20novel%20and%20powerful%0Aprompt-driven%20scheme%2C%20called%20%60%60Prompt%20and%20Transfer%22%20%28PAT%29%2C%20which%20constructs%20a%0Adynamic%20class-aware%20prompting%20paradigm%20to%20tune%20the%20encoder%20for%20focusing%20on%20the%0Ainterested%20object%20%28target%20class%29%20in%20the%20current%20task.%20Three%20key%20points%20are%0Aelaborated%20to%20enhance%20the%20prompting%3A%201%29%20Cross-modal%20linguistic%20information%20is%0Aintroduced%20to%20initialize%20prompts%20for%20each%20task.%202%29%20Semantic%20Prompt%20Transfer%0A%28SPT%29%20that%20precisely%20transfers%20the%20class-specific%20semantics%20within%20the%20images%0Ato%20prompts.%203%29%20Part%20Mask%20Generator%20%28PMG%29%20that%20works%20in%20conjunction%20with%20SPT%20to%0Aadaptively%20generate%20different%20but%20complementary%20part%20prompts%20for%20different%0Aindividuals.%20Surprisingly%2C%20PAT%20achieves%20competitive%20performance%20on%204%20different%0Atasks%20including%20standard%20FSS%2C%20Cross-domain%20FSS%20%28e.g.%2C%20CV%2C%20medical%2C%20and%20remote%0Asensing%20domains%29%2C%20Weak-label%20FSS%2C%20and%20Zero-shot%20Segmentation%2C%20setting%20new%0Astate-of-the-arts%20on%2011%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10389v1&entry.124074799=Read"},
{"title": "Parameterized Approximation for Robust Clustering in Discrete Geometric\n  Spaces", "author": "Fateme Abbasi and Sandip Banerjee and Jaros\u0142aw Byrka and Parinya Chalermsook and Ameet Gadekar and Kamyar Khodamoradi and D\u00e1niel Marx and Roohani Sharma and Joachim Spoerhase", "abstract": "  We consider the well-studied Robust $(k, z)$-Clustering problem, which\ngeneralizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a\nconstant $z\\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$\nweighted points in a metric space $(M,\\delta)$ and a positive integer $k$.\nFurther, each point belongs to one (or more) of the $m$ many different groups\n$S_1,S_2,\\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that\n$\\max_{i \\in [m]} \\sum_{p \\in S_i} w(p) \\delta(p,X)^z$ is minimized.\n  This problem arises in the domains of robust optimization [Anthony, Goyal,\nGupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For\npolynomial time computation, an approximation factor of $O(\\log m/\\log\\log m)$\nis known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible\ncomplexity assumption even in the line metrics. For FPT time, there is a\n$(3^z+\\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal,\nJaiswal, Inf. Proc. Letters, 2023].\n  Motivated by the tight lower bounds for general discrete metrics, we focus on\n\\emph{geometric} spaces such as the (discrete) high-dimensional Euclidean\nsetting and metrics of low doubling dimension, which play an important role in\ndata analysis applications. First, for a universal constant $\\eta_0 >0.0006$,\nwe devise a $3^z(1-\\eta_{0})$-factor FPT approximation algorithm for discrete\nhigh-dimensional Euclidean spaces thereby bypassing the lower bound for general\nmetrics. We complement this result by showing that even the special case of\n$k$-Center in dimension $\\Theta(\\log n)$ is $(\\sqrt{3/2}- o(1))$-hard to\napproximate for FPT algorithms. Finally, we complete the FPT approximation\nlandscape by designing an FPT $(1+\\epsilon)$-approximation scheme (EPAS) for\nthe metric of sub-logarithmic doubling dimension.\n", "link": "http://arxiv.org/abs/2305.07316v2", "date": "2024-09-16", "relevancy": 2.1887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4368}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameterized%20Approximation%20for%20Robust%20Clustering%20in%20Discrete%20Geometric%0A%20%20Spaces&body=Title%3A%20Parameterized%20Approximation%20for%20Robust%20Clustering%20in%20Discrete%20Geometric%0A%20%20Spaces%0AAuthor%3A%20Fateme%20Abbasi%20and%20Sandip%20Banerjee%20and%20Jaros%C5%82aw%20Byrka%20and%20Parinya%20Chalermsook%20and%20Ameet%20Gadekar%20and%20Kamyar%20Khodamoradi%20and%20D%C3%A1niel%20Marx%20and%20Roohani%20Sharma%20and%20Joachim%20Spoerhase%0AAbstract%3A%20%20%20We%20consider%20the%20well-studied%20Robust%20%24%28k%2C%20z%29%24-Clustering%20problem%2C%20which%0Ageneralizes%20the%20classic%20%24k%24-Median%2C%20%24k%24-Means%2C%20and%20%24k%24-Center%20problems.%20Given%20a%0Aconstant%20%24z%5Cge%201%24%2C%20the%20input%20to%20Robust%20%24%28k%2C%20z%29%24-Clustering%20is%20a%20set%20%24P%24%20of%20%24n%24%0Aweighted%20points%20in%20a%20metric%20space%20%24%28M%2C%5Cdelta%29%24%20and%20a%20positive%20integer%20%24k%24.%0AFurther%2C%20each%20point%20belongs%20to%20one%20%28or%20more%29%20of%20the%20%24m%24%20many%20different%20groups%0A%24S_1%2CS_2%2C%5Cldots%2CS_m%24.%20Our%20goal%20is%20to%20find%20a%20set%20%24X%24%20of%20%24k%24%20centers%20such%20that%0A%24%5Cmax_%7Bi%20%5Cin%20%5Bm%5D%7D%20%5Csum_%7Bp%20%5Cin%20S_i%7D%20w%28p%29%20%5Cdelta%28p%2CX%29%5Ez%24%20is%20minimized.%0A%20%20This%20problem%20arises%20in%20the%20domains%20of%20robust%20optimization%20%5BAnthony%2C%20Goyal%2C%0AGupta%2C%20Nagarajan%2C%20Math.%20Oper.%20Res.%202010%5D%20and%20in%20algorithmic%20fairness.%20For%0Apolynomial%20time%20computation%2C%20an%20approximation%20factor%20of%20%24O%28%5Clog%20m/%5Clog%5Clog%20m%29%24%0Ais%20known%20%5BMakarychev%2C%20Vakilian%2C%20COLT%20%242021%24%5D%2C%20which%20is%20tight%20under%20a%20plausible%0Acomplexity%20assumption%20even%20in%20the%20line%20metrics.%20For%20FPT%20time%2C%20there%20is%20a%0A%24%283%5Ez%2B%5Cepsilon%29%24-approximation%20algorithm%2C%20which%20is%20tight%20under%20GAP-ETH%20%5BGoyal%2C%0AJaiswal%2C%20Inf.%20Proc.%20Letters%2C%202023%5D.%0A%20%20Motivated%20by%20the%20tight%20lower%20bounds%20for%20general%20discrete%20metrics%2C%20we%20focus%20on%0A%5Cemph%7Bgeometric%7D%20spaces%20such%20as%20the%20%28discrete%29%20high-dimensional%20Euclidean%0Asetting%20and%20metrics%20of%20low%20doubling%20dimension%2C%20which%20play%20an%20important%20role%20in%0Adata%20analysis%20applications.%20First%2C%20for%20a%20universal%20constant%20%24%5Ceta_0%20%3E0.0006%24%2C%0Awe%20devise%20a%20%243%5Ez%281-%5Ceta_%7B0%7D%29%24-factor%20FPT%20approximation%20algorithm%20for%20discrete%0Ahigh-dimensional%20Euclidean%20spaces%20thereby%20bypassing%20the%20lower%20bound%20for%20general%0Ametrics.%20We%20complement%20this%20result%20by%20showing%20that%20even%20the%20special%20case%20of%0A%24k%24-Center%20in%20dimension%20%24%5CTheta%28%5Clog%20n%29%24%20is%20%24%28%5Csqrt%7B3/2%7D-%20o%281%29%29%24-hard%20to%0Aapproximate%20for%20FPT%20algorithms.%20Finally%2C%20we%20complete%20the%20FPT%20approximation%0Alandscape%20by%20designing%20an%20FPT%20%24%281%2B%5Cepsilon%29%24-approximation%20scheme%20%28EPAS%29%20for%0Athe%20metric%20of%20sub-logarithmic%20doubling%20dimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameterized%2520Approximation%2520for%2520Robust%2520Clustering%2520in%2520Discrete%2520Geometric%250A%2520%2520Spaces%26entry.906535625%3DFateme%2520Abbasi%2520and%2520Sandip%2520Banerjee%2520and%2520Jaros%25C5%2582aw%2520Byrka%2520and%2520Parinya%2520Chalermsook%2520and%2520Ameet%2520Gadekar%2520and%2520Kamyar%2520Khodamoradi%2520and%2520D%25C3%25A1niel%2520Marx%2520and%2520Roohani%2520Sharma%2520and%2520Joachim%2520Spoerhase%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520well-studied%2520Robust%2520%2524%2528k%252C%2520z%2529%2524-Clustering%2520problem%252C%2520which%250Ageneralizes%2520the%2520classic%2520%2524k%2524-Median%252C%2520%2524k%2524-Means%252C%2520and%2520%2524k%2524-Center%2520problems.%2520Given%2520a%250Aconstant%2520%2524z%255Cge%25201%2524%252C%2520the%2520input%2520to%2520Robust%2520%2524%2528k%252C%2520z%2529%2524-Clustering%2520is%2520a%2520set%2520%2524P%2524%2520of%2520%2524n%2524%250Aweighted%2520points%2520in%2520a%2520metric%2520space%2520%2524%2528M%252C%255Cdelta%2529%2524%2520and%2520a%2520positive%2520integer%2520%2524k%2524.%250AFurther%252C%2520each%2520point%2520belongs%2520to%2520one%2520%2528or%2520more%2529%2520of%2520the%2520%2524m%2524%2520many%2520different%2520groups%250A%2524S_1%252CS_2%252C%255Cldots%252CS_m%2524.%2520Our%2520goal%2520is%2520to%2520find%2520a%2520set%2520%2524X%2524%2520of%2520%2524k%2524%2520centers%2520such%2520that%250A%2524%255Cmax_%257Bi%2520%255Cin%2520%255Bm%255D%257D%2520%255Csum_%257Bp%2520%255Cin%2520S_i%257D%2520w%2528p%2529%2520%255Cdelta%2528p%252CX%2529%255Ez%2524%2520is%2520minimized.%250A%2520%2520This%2520problem%2520arises%2520in%2520the%2520domains%2520of%2520robust%2520optimization%2520%255BAnthony%252C%2520Goyal%252C%250AGupta%252C%2520Nagarajan%252C%2520Math.%2520Oper.%2520Res.%25202010%255D%2520and%2520in%2520algorithmic%2520fairness.%2520For%250Apolynomial%2520time%2520computation%252C%2520an%2520approximation%2520factor%2520of%2520%2524O%2528%255Clog%2520m/%255Clog%255Clog%2520m%2529%2524%250Ais%2520known%2520%255BMakarychev%252C%2520Vakilian%252C%2520COLT%2520%25242021%2524%255D%252C%2520which%2520is%2520tight%2520under%2520a%2520plausible%250Acomplexity%2520assumption%2520even%2520in%2520the%2520line%2520metrics.%2520For%2520FPT%2520time%252C%2520there%2520is%2520a%250A%2524%25283%255Ez%252B%255Cepsilon%2529%2524-approximation%2520algorithm%252C%2520which%2520is%2520tight%2520under%2520GAP-ETH%2520%255BGoyal%252C%250AJaiswal%252C%2520Inf.%2520Proc.%2520Letters%252C%25202023%255D.%250A%2520%2520Motivated%2520by%2520the%2520tight%2520lower%2520bounds%2520for%2520general%2520discrete%2520metrics%252C%2520we%2520focus%2520on%250A%255Cemph%257Bgeometric%257D%2520spaces%2520such%2520as%2520the%2520%2528discrete%2529%2520high-dimensional%2520Euclidean%250Asetting%2520and%2520metrics%2520of%2520low%2520doubling%2520dimension%252C%2520which%2520play%2520an%2520important%2520role%2520in%250Adata%2520analysis%2520applications.%2520First%252C%2520for%2520a%2520universal%2520constant%2520%2524%255Ceta_0%2520%253E0.0006%2524%252C%250Awe%2520devise%2520a%2520%25243%255Ez%25281-%255Ceta_%257B0%257D%2529%2524-factor%2520FPT%2520approximation%2520algorithm%2520for%2520discrete%250Ahigh-dimensional%2520Euclidean%2520spaces%2520thereby%2520bypassing%2520the%2520lower%2520bound%2520for%2520general%250Ametrics.%2520We%2520complement%2520this%2520result%2520by%2520showing%2520that%2520even%2520the%2520special%2520case%2520of%250A%2524k%2524-Center%2520in%2520dimension%2520%2524%255CTheta%2528%255Clog%2520n%2529%2524%2520is%2520%2524%2528%255Csqrt%257B3/2%257D-%2520o%25281%2529%2529%2524-hard%2520to%250Aapproximate%2520for%2520FPT%2520algorithms.%2520Finally%252C%2520we%2520complete%2520the%2520FPT%2520approximation%250Alandscape%2520by%2520designing%2520an%2520FPT%2520%2524%25281%252B%255Cepsilon%2529%2524-approximation%2520scheme%2520%2528EPAS%2529%2520for%250Athe%2520metric%2520of%2520sub-logarithmic%2520doubling%2520dimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameterized%20Approximation%20for%20Robust%20Clustering%20in%20Discrete%20Geometric%0A%20%20Spaces&entry.906535625=Fateme%20Abbasi%20and%20Sandip%20Banerjee%20and%20Jaros%C5%82aw%20Byrka%20and%20Parinya%20Chalermsook%20and%20Ameet%20Gadekar%20and%20Kamyar%20Khodamoradi%20and%20D%C3%A1niel%20Marx%20and%20Roohani%20Sharma%20and%20Joachim%20Spoerhase&entry.1292438233=%20%20We%20consider%20the%20well-studied%20Robust%20%24%28k%2C%20z%29%24-Clustering%20problem%2C%20which%0Ageneralizes%20the%20classic%20%24k%24-Median%2C%20%24k%24-Means%2C%20and%20%24k%24-Center%20problems.%20Given%20a%0Aconstant%20%24z%5Cge%201%24%2C%20the%20input%20to%20Robust%20%24%28k%2C%20z%29%24-Clustering%20is%20a%20set%20%24P%24%20of%20%24n%24%0Aweighted%20points%20in%20a%20metric%20space%20%24%28M%2C%5Cdelta%29%24%20and%20a%20positive%20integer%20%24k%24.%0AFurther%2C%20each%20point%20belongs%20to%20one%20%28or%20more%29%20of%20the%20%24m%24%20many%20different%20groups%0A%24S_1%2CS_2%2C%5Cldots%2CS_m%24.%20Our%20goal%20is%20to%20find%20a%20set%20%24X%24%20of%20%24k%24%20centers%20such%20that%0A%24%5Cmax_%7Bi%20%5Cin%20%5Bm%5D%7D%20%5Csum_%7Bp%20%5Cin%20S_i%7D%20w%28p%29%20%5Cdelta%28p%2CX%29%5Ez%24%20is%20minimized.%0A%20%20This%20problem%20arises%20in%20the%20domains%20of%20robust%20optimization%20%5BAnthony%2C%20Goyal%2C%0AGupta%2C%20Nagarajan%2C%20Math.%20Oper.%20Res.%202010%5D%20and%20in%20algorithmic%20fairness.%20For%0Apolynomial%20time%20computation%2C%20an%20approximation%20factor%20of%20%24O%28%5Clog%20m/%5Clog%5Clog%20m%29%24%0Ais%20known%20%5BMakarychev%2C%20Vakilian%2C%20COLT%20%242021%24%5D%2C%20which%20is%20tight%20under%20a%20plausible%0Acomplexity%20assumption%20even%20in%20the%20line%20metrics.%20For%20FPT%20time%2C%20there%20is%20a%0A%24%283%5Ez%2B%5Cepsilon%29%24-approximation%20algorithm%2C%20which%20is%20tight%20under%20GAP-ETH%20%5BGoyal%2C%0AJaiswal%2C%20Inf.%20Proc.%20Letters%2C%202023%5D.%0A%20%20Motivated%20by%20the%20tight%20lower%20bounds%20for%20general%20discrete%20metrics%2C%20we%20focus%20on%0A%5Cemph%7Bgeometric%7D%20spaces%20such%20as%20the%20%28discrete%29%20high-dimensional%20Euclidean%0Asetting%20and%20metrics%20of%20low%20doubling%20dimension%2C%20which%20play%20an%20important%20role%20in%0Adata%20analysis%20applications.%20First%2C%20for%20a%20universal%20constant%20%24%5Ceta_0%20%3E0.0006%24%2C%0Awe%20devise%20a%20%243%5Ez%281-%5Ceta_%7B0%7D%29%24-factor%20FPT%20approximation%20algorithm%20for%20discrete%0Ahigh-dimensional%20Euclidean%20spaces%20thereby%20bypassing%20the%20lower%20bound%20for%20general%0Ametrics.%20We%20complement%20this%20result%20by%20showing%20that%20even%20the%20special%20case%20of%0A%24k%24-Center%20in%20dimension%20%24%5CTheta%28%5Clog%20n%29%24%20is%20%24%28%5Csqrt%7B3/2%7D-%20o%281%29%29%24-hard%20to%0Aapproximate%20for%20FPT%20algorithms.%20Finally%2C%20we%20complete%20the%20FPT%20approximation%0Alandscape%20by%20designing%20an%20FPT%20%24%281%2B%5Cepsilon%29%24-approximation%20scheme%20%28EPAS%29%20for%0Athe%20metric%20of%20sub-logarithmic%20doubling%20dimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07316v2&entry.124074799=Read"},
{"title": "Safe and Real-Time Consistent Planning for Autonomous Vehicles in\n  Partially Observed Environments via Parallel Consensus Optimization", "author": "Lei Zheng and Rui Yang and Minzhe Zheng and Michael Yu Wang and Jun Ma", "abstract": "  Ensuring safety and driving consistency is a significant challenge for\nautonomous vehicles operating in partially observed environments. This work\nintroduces a consistent parallel trajectory optimization (CPTO) approach to\nenable safe and consistent driving in dense obstacle environments with\nperception uncertainties. Utilizing discrete-time barrier function theory, we\ndevelop a consensus safety barrier module that ensures reliable safety coverage\nwithin the spatiotemporal trajectory space across potential obstacle\nconfigurations. Following this, a bi-convex parallel trajectory optimization\nproblem is derived that facilitates decomposition into a series of\nlow-dimensional quadratic programming problems to accelerate computation. By\nleveraging the consensus alternating direction method of multipliers (ADMM) for\nparallel optimization, each generated candidate trajectory corresponds to a\npossible environment configuration while sharing a common consensus trajectory\nsegment. This ensures driving safety and consistency when executing the\nconsensus trajectory segment for the ego vehicle in real time. We validate our\nCPTO framework through extensive comparisons with state-of-the-art baselines\nacross multiple driving tasks in partially observable environments. Our results\ndemonstrate improved safety and consistency using both synthetic and real-world\ntraffic datasets.\n", "link": "http://arxiv.org/abs/2409.10310v1", "date": "2024-09-16", "relevancy": 2.1615, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5421}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Real-Time%20Consistent%20Planning%20for%20Autonomous%20Vehicles%20in%0A%20%20Partially%20Observed%20Environments%20via%20Parallel%20Consensus%20Optimization&body=Title%3A%20Safe%20and%20Real-Time%20Consistent%20Planning%20for%20Autonomous%20Vehicles%20in%0A%20%20Partially%20Observed%20Environments%20via%20Parallel%20Consensus%20Optimization%0AAuthor%3A%20Lei%20Zheng%20and%20Rui%20Yang%20and%20Minzhe%20Zheng%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Ensuring%20safety%20and%20driving%20consistency%20is%20a%20significant%20challenge%20for%0Aautonomous%20vehicles%20operating%20in%20partially%20observed%20environments.%20This%20work%0Aintroduces%20a%20consistent%20parallel%20trajectory%20optimization%20%28CPTO%29%20approach%20to%0Aenable%20safe%20and%20consistent%20driving%20in%20dense%20obstacle%20environments%20with%0Aperception%20uncertainties.%20Utilizing%20discrete-time%20barrier%20function%20theory%2C%20we%0Adevelop%20a%20consensus%20safety%20barrier%20module%20that%20ensures%20reliable%20safety%20coverage%0Awithin%20the%20spatiotemporal%20trajectory%20space%20across%20potential%20obstacle%0Aconfigurations.%20Following%20this%2C%20a%20bi-convex%20parallel%20trajectory%20optimization%0Aproblem%20is%20derived%20that%20facilitates%20decomposition%20into%20a%20series%20of%0Alow-dimensional%20quadratic%20programming%20problems%20to%20accelerate%20computation.%20By%0Aleveraging%20the%20consensus%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%0Aparallel%20optimization%2C%20each%20generated%20candidate%20trajectory%20corresponds%20to%20a%0Apossible%20environment%20configuration%20while%20sharing%20a%20common%20consensus%20trajectory%0Asegment.%20This%20ensures%20driving%20safety%20and%20consistency%20when%20executing%20the%0Aconsensus%20trajectory%20segment%20for%20the%20ego%20vehicle%20in%20real%20time.%20We%20validate%20our%0ACPTO%20framework%20through%20extensive%20comparisons%20with%20state-of-the-art%20baselines%0Aacross%20multiple%20driving%20tasks%20in%20partially%20observable%20environments.%20Our%20results%0Ademonstrate%20improved%20safety%20and%20consistency%20using%20both%20synthetic%20and%20real-world%0Atraffic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Real-Time%2520Consistent%2520Planning%2520for%2520Autonomous%2520Vehicles%2520in%250A%2520%2520Partially%2520Observed%2520Environments%2520via%2520Parallel%2520Consensus%2520Optimization%26entry.906535625%3DLei%2520Zheng%2520and%2520Rui%2520Yang%2520and%2520Minzhe%2520Zheng%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520and%2520driving%2520consistency%2520is%2520a%2520significant%2520challenge%2520for%250Aautonomous%2520vehicles%2520operating%2520in%2520partially%2520observed%2520environments.%2520This%2520work%250Aintroduces%2520a%2520consistent%2520parallel%2520trajectory%2520optimization%2520%2528CPTO%2529%2520approach%2520to%250Aenable%2520safe%2520and%2520consistent%2520driving%2520in%2520dense%2520obstacle%2520environments%2520with%250Aperception%2520uncertainties.%2520Utilizing%2520discrete-time%2520barrier%2520function%2520theory%252C%2520we%250Adevelop%2520a%2520consensus%2520safety%2520barrier%2520module%2520that%2520ensures%2520reliable%2520safety%2520coverage%250Awithin%2520the%2520spatiotemporal%2520trajectory%2520space%2520across%2520potential%2520obstacle%250Aconfigurations.%2520Following%2520this%252C%2520a%2520bi-convex%2520parallel%2520trajectory%2520optimization%250Aproblem%2520is%2520derived%2520that%2520facilitates%2520decomposition%2520into%2520a%2520series%2520of%250Alow-dimensional%2520quadratic%2520programming%2520problems%2520to%2520accelerate%2520computation.%2520By%250Aleveraging%2520the%2520consensus%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520for%250Aparallel%2520optimization%252C%2520each%2520generated%2520candidate%2520trajectory%2520corresponds%2520to%2520a%250Apossible%2520environment%2520configuration%2520while%2520sharing%2520a%2520common%2520consensus%2520trajectory%250Asegment.%2520This%2520ensures%2520driving%2520safety%2520and%2520consistency%2520when%2520executing%2520the%250Aconsensus%2520trajectory%2520segment%2520for%2520the%2520ego%2520vehicle%2520in%2520real%2520time.%2520We%2520validate%2520our%250ACPTO%2520framework%2520through%2520extensive%2520comparisons%2520with%2520state-of-the-art%2520baselines%250Aacross%2520multiple%2520driving%2520tasks%2520in%2520partially%2520observable%2520environments.%2520Our%2520results%250Ademonstrate%2520improved%2520safety%2520and%2520consistency%2520using%2520both%2520synthetic%2520and%2520real-world%250Atraffic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Real-Time%20Consistent%20Planning%20for%20Autonomous%20Vehicles%20in%0A%20%20Partially%20Observed%20Environments%20via%20Parallel%20Consensus%20Optimization&entry.906535625=Lei%20Zheng%20and%20Rui%20Yang%20and%20Minzhe%20Zheng%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20Ensuring%20safety%20and%20driving%20consistency%20is%20a%20significant%20challenge%20for%0Aautonomous%20vehicles%20operating%20in%20partially%20observed%20environments.%20This%20work%0Aintroduces%20a%20consistent%20parallel%20trajectory%20optimization%20%28CPTO%29%20approach%20to%0Aenable%20safe%20and%20consistent%20driving%20in%20dense%20obstacle%20environments%20with%0Aperception%20uncertainties.%20Utilizing%20discrete-time%20barrier%20function%20theory%2C%20we%0Adevelop%20a%20consensus%20safety%20barrier%20module%20that%20ensures%20reliable%20safety%20coverage%0Awithin%20the%20spatiotemporal%20trajectory%20space%20across%20potential%20obstacle%0Aconfigurations.%20Following%20this%2C%20a%20bi-convex%20parallel%20trajectory%20optimization%0Aproblem%20is%20derived%20that%20facilitates%20decomposition%20into%20a%20series%20of%0Alow-dimensional%20quadratic%20programming%20problems%20to%20accelerate%20computation.%20By%0Aleveraging%20the%20consensus%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%0Aparallel%20optimization%2C%20each%20generated%20candidate%20trajectory%20corresponds%20to%20a%0Apossible%20environment%20configuration%20while%20sharing%20a%20common%20consensus%20trajectory%0Asegment.%20This%20ensures%20driving%20safety%20and%20consistency%20when%20executing%20the%0Aconsensus%20trajectory%20segment%20for%20the%20ego%20vehicle%20in%20real%20time.%20We%20validate%20our%0ACPTO%20framework%20through%20extensive%20comparisons%20with%20state-of-the-art%20baselines%0Aacross%20multiple%20driving%20tasks%20in%20partially%20observable%20environments.%20Our%20results%0Ademonstrate%20improved%20safety%20and%20consistency%20using%20both%20synthetic%20and%20real-world%0Atraffic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10310v1&entry.124074799=Read"},
{"title": "PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture", "author": "Qiang Zheng and Chao Zhang and Jian Sun", "abstract": "  In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy.\n", "link": "http://arxiv.org/abs/2408.05508v2", "date": "2024-09-16", "relevancy": 2.1612, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointMT%3A%20Efficient%20Point%20Cloud%20Analysis%20with%20Hybrid%20MLP-Transformer%0A%20%20Architecture&body=Title%3A%20PointMT%3A%20Efficient%20Point%20Cloud%20Analysis%20with%20Hybrid%20MLP-Transformer%0A%20%20Architecture%0AAuthor%3A%20Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20In%20recent%20years%2C%20point%20cloud%20analysis%20methods%20based%20on%20the%20Transformer%0Aarchitecture%20have%20made%20significant%20progress%2C%20particularly%20in%20the%20context%20of%0Amultimedia%20applications%20such%20as%203D%20modeling%2C%20virtual%20reality%2C%20and%20autonomous%0Asystems.%20However%2C%20the%20high%20computational%20resource%20demands%20of%20the%20Transformer%0Aarchitecture%20hinder%20its%20scalability%2C%20real-time%20processing%20capabilities%2C%20and%0Adeployment%20on%20mobile%20devices%20and%20other%20platforms%20with%20limited%20computational%0Aresources.%20This%20limitation%20remains%20a%20significant%20obstacle%20to%20its%20practical%0Aapplication%20in%20scenarios%20requiring%20on-device%20intelligence%20and%20multimedia%0Aprocessing.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20point%20cloud%0Aanalysis%20architecture%2C%20%5Ctextbf%7BPoint%7D%20%5Ctextbf%7BM%7DLP-%5Ctextbf%7BT%7Dransformer%0A%28PointMT%29.%20This%20study%20tackles%20the%20quadratic%20complexity%20of%20the%20self-attention%0Amechanism%20by%20introducing%20a%20linear%20complexity%20local%20attention%20mechanism%20for%0Aeffective%20feature%20aggregation.%20Additionally%2C%20to%20counter%20the%20Transformer%27s%20focus%0Aon%20token%20differences%20while%20neglecting%20channel%20differences%2C%20we%20introduce%20a%0Aparameter-free%20channel%20temperature%20adaptation%20mechanism%20that%20adaptively%20adjusts%0Athe%20attention%20weight%20distribution%20in%20each%20channel%2C%20enhancing%20the%20precision%20of%0Afeature%20aggregation.%20To%20improve%20the%20Transformer%27s%20slow%20convergence%20speed%20due%20to%0Athe%20limited%20scale%20of%20point%20cloud%20datasets%2C%20we%20propose%20an%20MLP-Transformer%20hybrid%0Amodule%2C%20which%20significantly%20enhances%20the%20model%27s%20convergence%20speed.%0AFurthermore%2C%20to%20boost%20the%20feature%20representation%20capability%20of%20point%20tokens%2C%20we%0Arefine%20the%20classification%20head%2C%20enabling%20point%20tokens%20to%20directly%20participate%0Ain%20prediction.%20Experimental%20results%20on%20multiple%20evaluation%20benchmarks%0Ademonstrate%20that%20PointMT%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods%20while%20maintaining%20an%20optimal%20balance%20between%20performance%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointMT%253A%2520Efficient%2520Point%2520Cloud%2520Analysis%2520with%2520Hybrid%2520MLP-Transformer%250A%2520%2520Architecture%26entry.906535625%3DQiang%2520Zheng%2520and%2520Chao%2520Zhang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520point%2520cloud%2520analysis%2520methods%2520based%2520on%2520the%2520Transformer%250Aarchitecture%2520have%2520made%2520significant%2520progress%252C%2520particularly%2520in%2520the%2520context%2520of%250Amultimedia%2520applications%2520such%2520as%25203D%2520modeling%252C%2520virtual%2520reality%252C%2520and%2520autonomous%250Asystems.%2520However%252C%2520the%2520high%2520computational%2520resource%2520demands%2520of%2520the%2520Transformer%250Aarchitecture%2520hinder%2520its%2520scalability%252C%2520real-time%2520processing%2520capabilities%252C%2520and%250Adeployment%2520on%2520mobile%2520devices%2520and%2520other%2520platforms%2520with%2520limited%2520computational%250Aresources.%2520This%2520limitation%2520remains%2520a%2520significant%2520obstacle%2520to%2520its%2520practical%250Aapplication%2520in%2520scenarios%2520requiring%2520on-device%2520intelligence%2520and%2520multimedia%250Aprocessing.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520efficient%2520point%2520cloud%250Aanalysis%2520architecture%252C%2520%255Ctextbf%257BPoint%257D%2520%255Ctextbf%257BM%257DLP-%255Ctextbf%257BT%257Dransformer%250A%2528PointMT%2529.%2520This%2520study%2520tackles%2520the%2520quadratic%2520complexity%2520of%2520the%2520self-attention%250Amechanism%2520by%2520introducing%2520a%2520linear%2520complexity%2520local%2520attention%2520mechanism%2520for%250Aeffective%2520feature%2520aggregation.%2520Additionally%252C%2520to%2520counter%2520the%2520Transformer%2527s%2520focus%250Aon%2520token%2520differences%2520while%2520neglecting%2520channel%2520differences%252C%2520we%2520introduce%2520a%250Aparameter-free%2520channel%2520temperature%2520adaptation%2520mechanism%2520that%2520adaptively%2520adjusts%250Athe%2520attention%2520weight%2520distribution%2520in%2520each%2520channel%252C%2520enhancing%2520the%2520precision%2520of%250Afeature%2520aggregation.%2520To%2520improve%2520the%2520Transformer%2527s%2520slow%2520convergence%2520speed%2520due%2520to%250Athe%2520limited%2520scale%2520of%2520point%2520cloud%2520datasets%252C%2520we%2520propose%2520an%2520MLP-Transformer%2520hybrid%250Amodule%252C%2520which%2520significantly%2520enhances%2520the%2520model%2527s%2520convergence%2520speed.%250AFurthermore%252C%2520to%2520boost%2520the%2520feature%2520representation%2520capability%2520of%2520point%2520tokens%252C%2520we%250Arefine%2520the%2520classification%2520head%252C%2520enabling%2520point%2520tokens%2520to%2520directly%2520participate%250Ain%2520prediction.%2520Experimental%2520results%2520on%2520multiple%2520evaluation%2520benchmarks%250Ademonstrate%2520that%2520PointMT%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%250Amethods%2520while%2520maintaining%2520an%2520optimal%2520balance%2520between%2520performance%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointMT%3A%20Efficient%20Point%20Cloud%20Analysis%20with%20Hybrid%20MLP-Transformer%0A%20%20Architecture&entry.906535625=Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun&entry.1292438233=%20%20In%20recent%20years%2C%20point%20cloud%20analysis%20methods%20based%20on%20the%20Transformer%0Aarchitecture%20have%20made%20significant%20progress%2C%20particularly%20in%20the%20context%20of%0Amultimedia%20applications%20such%20as%203D%20modeling%2C%20virtual%20reality%2C%20and%20autonomous%0Asystems.%20However%2C%20the%20high%20computational%20resource%20demands%20of%20the%20Transformer%0Aarchitecture%20hinder%20its%20scalability%2C%20real-time%20processing%20capabilities%2C%20and%0Adeployment%20on%20mobile%20devices%20and%20other%20platforms%20with%20limited%20computational%0Aresources.%20This%20limitation%20remains%20a%20significant%20obstacle%20to%20its%20practical%0Aapplication%20in%20scenarios%20requiring%20on-device%20intelligence%20and%20multimedia%0Aprocessing.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20efficient%20point%20cloud%0Aanalysis%20architecture%2C%20%5Ctextbf%7BPoint%7D%20%5Ctextbf%7BM%7DLP-%5Ctextbf%7BT%7Dransformer%0A%28PointMT%29.%20This%20study%20tackles%20the%20quadratic%20complexity%20of%20the%20self-attention%0Amechanism%20by%20introducing%20a%20linear%20complexity%20local%20attention%20mechanism%20for%0Aeffective%20feature%20aggregation.%20Additionally%2C%20to%20counter%20the%20Transformer%27s%20focus%0Aon%20token%20differences%20while%20neglecting%20channel%20differences%2C%20we%20introduce%20a%0Aparameter-free%20channel%20temperature%20adaptation%20mechanism%20that%20adaptively%20adjusts%0Athe%20attention%20weight%20distribution%20in%20each%20channel%2C%20enhancing%20the%20precision%20of%0Afeature%20aggregation.%20To%20improve%20the%20Transformer%27s%20slow%20convergence%20speed%20due%20to%0Athe%20limited%20scale%20of%20point%20cloud%20datasets%2C%20we%20propose%20an%20MLP-Transformer%20hybrid%0Amodule%2C%20which%20significantly%20enhances%20the%20model%27s%20convergence%20speed.%0AFurthermore%2C%20to%20boost%20the%20feature%20representation%20capability%20of%20point%20tokens%2C%20we%0Arefine%20the%20classification%20head%2C%20enabling%20point%20tokens%20to%20directly%20participate%0Ain%20prediction.%20Experimental%20results%20on%20multiple%20evaluation%20benchmarks%0Ademonstrate%20that%20PointMT%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amethods%20while%20maintaining%20an%20optimal%20balance%20between%20performance%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05508v2&entry.124074799=Read"},
{"title": "Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor\n  Coastal Lagoon Ecosystem in the South Western Mediterranean", "author": "Yu Ye and Aurora Gonz\u00e1lez-Vidal and Alejandro Cisterna-Garc\u00eda and Angel P\u00e9rez-Ruzafa and Miguel A. Zamora Izquierdo and Antonio F. Skarmeta", "abstract": "  Coastal marine ecosystems face mounting pressures from anthropogenic\nactivities and climate change, necessitating advanced monitoring and modeling\napproaches for effective management. This paper pioneers the development of a\nMarine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon\nEcosystem in the Region of Murcia. The platform leverages Artificial\nIntelligence to emulate complex hydrological and ecological models,\nfacilitating the simulation of what-if scenarios to predict ecosystem responses\nto various stressors. We integrate diverse datasets from public sources to\nconstruct a comprehensive digital representation of the lagoon's dynamics. The\nplatform's modular design enables real-time stakeholder engagement and informed\ndecision-making in marine management. Our work contributes to the ongoing\ndiscourse on advancing marine science through innovative digital twin\ntechnologies.\n", "link": "http://arxiv.org/abs/2409.10134v1", "date": "2024-09-16", "relevancy": 2.1494, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4304}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Towards%20a%20Marine%20Digital%20Twin%20Platform%3A%20Modeling%20the%20Mar%20Menor%0A%20%20Coastal%20Lagoon%20Ecosystem%20in%20the%20South%20Western%20Mediterranean&body=Title%3A%20Advancing%20Towards%20a%20Marine%20Digital%20Twin%20Platform%3A%20Modeling%20the%20Mar%20Menor%0A%20%20Coastal%20Lagoon%20Ecosystem%20in%20the%20South%20Western%20Mediterranean%0AAuthor%3A%20Yu%20Ye%20and%20Aurora%20Gonz%C3%A1lez-Vidal%20and%20Alejandro%20Cisterna-Garc%C3%ADa%20and%20Angel%20P%C3%A9rez-Ruzafa%20and%20Miguel%20A.%20Zamora%20Izquierdo%20and%20Antonio%20F.%20Skarmeta%0AAbstract%3A%20%20%20Coastal%20marine%20ecosystems%20face%20mounting%20pressures%20from%20anthropogenic%0Aactivities%20and%20climate%20change%2C%20necessitating%20advanced%20monitoring%20and%20modeling%0Aapproaches%20for%20effective%20management.%20This%20paper%20pioneers%20the%20development%20of%20a%0AMarine%20Digital%20Twin%20Platform%20aimed%20at%20modeling%20the%20Mar%20Menor%20Coastal%20Lagoon%0AEcosystem%20in%20the%20Region%20of%20Murcia.%20The%20platform%20leverages%20Artificial%0AIntelligence%20to%20emulate%20complex%20hydrological%20and%20ecological%20models%2C%0Afacilitating%20the%20simulation%20of%20what-if%20scenarios%20to%20predict%20ecosystem%20responses%0Ato%20various%20stressors.%20We%20integrate%20diverse%20datasets%20from%20public%20sources%20to%0Aconstruct%20a%20comprehensive%20digital%20representation%20of%20the%20lagoon%27s%20dynamics.%20The%0Aplatform%27s%20modular%20design%20enables%20real-time%20stakeholder%20engagement%20and%20informed%0Adecision-making%20in%20marine%20management.%20Our%20work%20contributes%20to%20the%20ongoing%0Adiscourse%20on%20advancing%20marine%20science%20through%20innovative%20digital%20twin%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Towards%2520a%2520Marine%2520Digital%2520Twin%2520Platform%253A%2520Modeling%2520the%2520Mar%2520Menor%250A%2520%2520Coastal%2520Lagoon%2520Ecosystem%2520in%2520the%2520South%2520Western%2520Mediterranean%26entry.906535625%3DYu%2520Ye%2520and%2520Aurora%2520Gonz%25C3%25A1lez-Vidal%2520and%2520Alejandro%2520Cisterna-Garc%25C3%25ADa%2520and%2520Angel%2520P%25C3%25A9rez-Ruzafa%2520and%2520Miguel%2520A.%2520Zamora%2520Izquierdo%2520and%2520Antonio%2520F.%2520Skarmeta%26entry.1292438233%3D%2520%2520Coastal%2520marine%2520ecosystems%2520face%2520mounting%2520pressures%2520from%2520anthropogenic%250Aactivities%2520and%2520climate%2520change%252C%2520necessitating%2520advanced%2520monitoring%2520and%2520modeling%250Aapproaches%2520for%2520effective%2520management.%2520This%2520paper%2520pioneers%2520the%2520development%2520of%2520a%250AMarine%2520Digital%2520Twin%2520Platform%2520aimed%2520at%2520modeling%2520the%2520Mar%2520Menor%2520Coastal%2520Lagoon%250AEcosystem%2520in%2520the%2520Region%2520of%2520Murcia.%2520The%2520platform%2520leverages%2520Artificial%250AIntelligence%2520to%2520emulate%2520complex%2520hydrological%2520and%2520ecological%2520models%252C%250Afacilitating%2520the%2520simulation%2520of%2520what-if%2520scenarios%2520to%2520predict%2520ecosystem%2520responses%250Ato%2520various%2520stressors.%2520We%2520integrate%2520diverse%2520datasets%2520from%2520public%2520sources%2520to%250Aconstruct%2520a%2520comprehensive%2520digital%2520representation%2520of%2520the%2520lagoon%2527s%2520dynamics.%2520The%250Aplatform%2527s%2520modular%2520design%2520enables%2520real-time%2520stakeholder%2520engagement%2520and%2520informed%250Adecision-making%2520in%2520marine%2520management.%2520Our%2520work%2520contributes%2520to%2520the%2520ongoing%250Adiscourse%2520on%2520advancing%2520marine%2520science%2520through%2520innovative%2520digital%2520twin%250Atechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Towards%20a%20Marine%20Digital%20Twin%20Platform%3A%20Modeling%20the%20Mar%20Menor%0A%20%20Coastal%20Lagoon%20Ecosystem%20in%20the%20South%20Western%20Mediterranean&entry.906535625=Yu%20Ye%20and%20Aurora%20Gonz%C3%A1lez-Vidal%20and%20Alejandro%20Cisterna-Garc%C3%ADa%20and%20Angel%20P%C3%A9rez-Ruzafa%20and%20Miguel%20A.%20Zamora%20Izquierdo%20and%20Antonio%20F.%20Skarmeta&entry.1292438233=%20%20Coastal%20marine%20ecosystems%20face%20mounting%20pressures%20from%20anthropogenic%0Aactivities%20and%20climate%20change%2C%20necessitating%20advanced%20monitoring%20and%20modeling%0Aapproaches%20for%20effective%20management.%20This%20paper%20pioneers%20the%20development%20of%20a%0AMarine%20Digital%20Twin%20Platform%20aimed%20at%20modeling%20the%20Mar%20Menor%20Coastal%20Lagoon%0AEcosystem%20in%20the%20Region%20of%20Murcia.%20The%20platform%20leverages%20Artificial%0AIntelligence%20to%20emulate%20complex%20hydrological%20and%20ecological%20models%2C%0Afacilitating%20the%20simulation%20of%20what-if%20scenarios%20to%20predict%20ecosystem%20responses%0Ato%20various%20stressors.%20We%20integrate%20diverse%20datasets%20from%20public%20sources%20to%0Aconstruct%20a%20comprehensive%20digital%20representation%20of%20the%20lagoon%27s%20dynamics.%20The%0Aplatform%27s%20modular%20design%20enables%20real-time%20stakeholder%20engagement%20and%20informed%0Adecision-making%20in%20marine%20management.%20Our%20work%20contributes%20to%20the%20ongoing%0Adiscourse%20on%20advancing%20marine%20science%20through%20innovative%20digital%20twin%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10134v1&entry.124074799=Read"},
{"title": "Safe and Stable Closed-Loop Learning for Neural-Network-Supported Model\n  Predictive Control", "author": "Sebastian Hirt and Maik Pfefferkorn and Rolf Findeisen", "abstract": "  Safe learning of control policies remains challenging, both in optimal\ncontrol and reinforcement learning. In this article, we consider safe learning\nof parametrized predictive controllers that operate with incomplete information\nabout the underlying process. To this end, we employ Bayesian optimization for\nlearning the best parameters from closed-loop data. Our method focuses on the\nsystem's overall long-term performance in closed-loop while keeping it safe and\nstable. Specifically, we parametrize the stage cost function of an MPC using a\nfeedforward neural network. This allows for a high degree of flexibility,\nenabling the system to achieve a better closed-loop performance with respect to\na superordinate measure. However, this flexibility also necessitates safety\nmeasures, especially with respect to closed-loop stability. To this end, we\nexplicitly incorporated stability information in the\nBayesian-optimization-based learning procedure, thereby achieving rigorous\nprobabilistic safety guarantees. The proposed approach is illustrated using a\nnumeric example.\n", "link": "http://arxiv.org/abs/2409.10171v1", "date": "2024-09-16", "relevancy": 2.1426, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20and%20Stable%20Closed-Loop%20Learning%20for%20Neural-Network-Supported%20Model%0A%20%20Predictive%20Control&body=Title%3A%20Safe%20and%20Stable%20Closed-Loop%20Learning%20for%20Neural-Network-Supported%20Model%0A%20%20Predictive%20Control%0AAuthor%3A%20Sebastian%20Hirt%20and%20Maik%20Pfefferkorn%20and%20Rolf%20Findeisen%0AAbstract%3A%20%20%20Safe%20learning%20of%20control%20policies%20remains%20challenging%2C%20both%20in%20optimal%0Acontrol%20and%20reinforcement%20learning.%20In%20this%20article%2C%20we%20consider%20safe%20learning%0Aof%20parametrized%20predictive%20controllers%20that%20operate%20with%20incomplete%20information%0Aabout%20the%20underlying%20process.%20To%20this%20end%2C%20we%20employ%20Bayesian%20optimization%20for%0Alearning%20the%20best%20parameters%20from%20closed-loop%20data.%20Our%20method%20focuses%20on%20the%0Asystem%27s%20overall%20long-term%20performance%20in%20closed-loop%20while%20keeping%20it%20safe%20and%0Astable.%20Specifically%2C%20we%20parametrize%20the%20stage%20cost%20function%20of%20an%20MPC%20using%20a%0Afeedforward%20neural%20network.%20This%20allows%20for%20a%20high%20degree%20of%20flexibility%2C%0Aenabling%20the%20system%20to%20achieve%20a%20better%20closed-loop%20performance%20with%20respect%20to%0Aa%20superordinate%20measure.%20However%2C%20this%20flexibility%20also%20necessitates%20safety%0Ameasures%2C%20especially%20with%20respect%20to%20closed-loop%20stability.%20To%20this%20end%2C%20we%0Aexplicitly%20incorporated%20stability%20information%20in%20the%0ABayesian-optimization-based%20learning%20procedure%2C%20thereby%20achieving%20rigorous%0Aprobabilistic%20safety%20guarantees.%20The%20proposed%20approach%20is%20illustrated%20using%20a%0Anumeric%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520and%2520Stable%2520Closed-Loop%2520Learning%2520for%2520Neural-Network-Supported%2520Model%250A%2520%2520Predictive%2520Control%26entry.906535625%3DSebastian%2520Hirt%2520and%2520Maik%2520Pfefferkorn%2520and%2520Rolf%2520Findeisen%26entry.1292438233%3D%2520%2520Safe%2520learning%2520of%2520control%2520policies%2520remains%2520challenging%252C%2520both%2520in%2520optimal%250Acontrol%2520and%2520reinforcement%2520learning.%2520In%2520this%2520article%252C%2520we%2520consider%2520safe%2520learning%250Aof%2520parametrized%2520predictive%2520controllers%2520that%2520operate%2520with%2520incomplete%2520information%250Aabout%2520the%2520underlying%2520process.%2520To%2520this%2520end%252C%2520we%2520employ%2520Bayesian%2520optimization%2520for%250Alearning%2520the%2520best%2520parameters%2520from%2520closed-loop%2520data.%2520Our%2520method%2520focuses%2520on%2520the%250Asystem%2527s%2520overall%2520long-term%2520performance%2520in%2520closed-loop%2520while%2520keeping%2520it%2520safe%2520and%250Astable.%2520Specifically%252C%2520we%2520parametrize%2520the%2520stage%2520cost%2520function%2520of%2520an%2520MPC%2520using%2520a%250Afeedforward%2520neural%2520network.%2520This%2520allows%2520for%2520a%2520high%2520degree%2520of%2520flexibility%252C%250Aenabling%2520the%2520system%2520to%2520achieve%2520a%2520better%2520closed-loop%2520performance%2520with%2520respect%2520to%250Aa%2520superordinate%2520measure.%2520However%252C%2520this%2520flexibility%2520also%2520necessitates%2520safety%250Ameasures%252C%2520especially%2520with%2520respect%2520to%2520closed-loop%2520stability.%2520To%2520this%2520end%252C%2520we%250Aexplicitly%2520incorporated%2520stability%2520information%2520in%2520the%250ABayesian-optimization-based%2520learning%2520procedure%252C%2520thereby%2520achieving%2520rigorous%250Aprobabilistic%2520safety%2520guarantees.%2520The%2520proposed%2520approach%2520is%2520illustrated%2520using%2520a%250Anumeric%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20and%20Stable%20Closed-Loop%20Learning%20for%20Neural-Network-Supported%20Model%0A%20%20Predictive%20Control&entry.906535625=Sebastian%20Hirt%20and%20Maik%20Pfefferkorn%20and%20Rolf%20Findeisen&entry.1292438233=%20%20Safe%20learning%20of%20control%20policies%20remains%20challenging%2C%20both%20in%20optimal%0Acontrol%20and%20reinforcement%20learning.%20In%20this%20article%2C%20we%20consider%20safe%20learning%0Aof%20parametrized%20predictive%20controllers%20that%20operate%20with%20incomplete%20information%0Aabout%20the%20underlying%20process.%20To%20this%20end%2C%20we%20employ%20Bayesian%20optimization%20for%0Alearning%20the%20best%20parameters%20from%20closed-loop%20data.%20Our%20method%20focuses%20on%20the%0Asystem%27s%20overall%20long-term%20performance%20in%20closed-loop%20while%20keeping%20it%20safe%20and%0Astable.%20Specifically%2C%20we%20parametrize%20the%20stage%20cost%20function%20of%20an%20MPC%20using%20a%0Afeedforward%20neural%20network.%20This%20allows%20for%20a%20high%20degree%20of%20flexibility%2C%0Aenabling%20the%20system%20to%20achieve%20a%20better%20closed-loop%20performance%20with%20respect%20to%0Aa%20superordinate%20measure.%20However%2C%20this%20flexibility%20also%20necessitates%20safety%0Ameasures%2C%20especially%20with%20respect%20to%20closed-loop%20stability.%20To%20this%20end%2C%20we%0Aexplicitly%20incorporated%20stability%20information%20in%20the%0ABayesian-optimization-based%20learning%20procedure%2C%20thereby%20achieving%20rigorous%0Aprobabilistic%20safety%20guarantees.%20The%20proposed%20approach%20is%20illustrated%20using%20a%0Anumeric%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10171v1&entry.124074799=Read"},
{"title": "Efficient Point Cloud Classification via Offline Distillation Framework\n  and Negative-Weight Self-Distillation Technique", "author": "Qiang Zheng and Chao Zhang and Jian Sun", "abstract": "  The rapid advancement in point cloud processing technologies has\nsignificantly increased the demand for efficient and compact models that\nachieve high-accuracy classification. Knowledge distillation has emerged as a\npotent model compression technique. However, traditional KD often requires\nextensive computational resources for forward inference of large teacher\nmodels, thereby reducing training efficiency for student models and increasing\nresource demands. To address these challenges, we introduce an innovative\noffline recording strategy that avoids the simultaneous loading of both teacher\nand student models, thereby reducing hardware demands. This approach feeds a\nmultitude of augmented samples into the teacher model, recording both the data\naugmentation parameters and the corresponding logit outputs. By applying\nshape-level augmentation operations such as random scaling and translation,\nwhile excluding point-level operations like random jittering, the size of the\nrecords is significantly reduced. Additionally, to mitigate the issue of small\nstudent model over-imitating the teacher model's outputs and converging to\nsuboptimal solutions, we incorporate a negative-weight self-distillation\nstrategy. Experimental results demonstrate that the proposed distillation\nstrategy enables the student model to achieve performance comparable to\nstate-of-the-art models while maintaining lower parameter count. This approach\nstrikes an optimal balance between performance and complexity. This study\nhighlights the potential of our method to optimize knowledge distillation for\npoint cloud classification tasks, particularly in resource-constrained\nenvironments, providing a novel solution for efficient point cloud analysis.\n", "link": "http://arxiv.org/abs/2409.02020v2", "date": "2024-09-16", "relevancy": 2.1332, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.54}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5378}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Point%20Cloud%20Classification%20via%20Offline%20Distillation%20Framework%0A%20%20and%20Negative-Weight%20Self-Distillation%20Technique&body=Title%3A%20Efficient%20Point%20Cloud%20Classification%20via%20Offline%20Distillation%20Framework%0A%20%20and%20Negative-Weight%20Self-Distillation%20Technique%0AAuthor%3A%20Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20point%20cloud%20processing%20technologies%20has%0Asignificantly%20increased%20the%20demand%20for%20efficient%20and%20compact%20models%20that%0Aachieve%20high-accuracy%20classification.%20Knowledge%20distillation%20has%20emerged%20as%20a%0Apotent%20model%20compression%20technique.%20However%2C%20traditional%20KD%20often%20requires%0Aextensive%20computational%20resources%20for%20forward%20inference%20of%20large%20teacher%0Amodels%2C%20thereby%20reducing%20training%20efficiency%20for%20student%20models%20and%20increasing%0Aresource%20demands.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20innovative%0Aoffline%20recording%20strategy%20that%20avoids%20the%20simultaneous%20loading%20of%20both%20teacher%0Aand%20student%20models%2C%20thereby%20reducing%20hardware%20demands.%20This%20approach%20feeds%20a%0Amultitude%20of%20augmented%20samples%20into%20the%20teacher%20model%2C%20recording%20both%20the%20data%0Aaugmentation%20parameters%20and%20the%20corresponding%20logit%20outputs.%20By%20applying%0Ashape-level%20augmentation%20operations%20such%20as%20random%20scaling%20and%20translation%2C%0Awhile%20excluding%20point-level%20operations%20like%20random%20jittering%2C%20the%20size%20of%20the%0Arecords%20is%20significantly%20reduced.%20Additionally%2C%20to%20mitigate%20the%20issue%20of%20small%0Astudent%20model%20over-imitating%20the%20teacher%20model%27s%20outputs%20and%20converging%20to%0Asuboptimal%20solutions%2C%20we%20incorporate%20a%20negative-weight%20self-distillation%0Astrategy.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20distillation%0Astrategy%20enables%20the%20student%20model%20to%20achieve%20performance%20comparable%20to%0Astate-of-the-art%20models%20while%20maintaining%20lower%20parameter%20count.%20This%20approach%0Astrikes%20an%20optimal%20balance%20between%20performance%20and%20complexity.%20This%20study%0Ahighlights%20the%20potential%20of%20our%20method%20to%20optimize%20knowledge%20distillation%20for%0Apoint%20cloud%20classification%20tasks%2C%20particularly%20in%20resource-constrained%0Aenvironments%2C%20providing%20a%20novel%20solution%20for%20efficient%20point%20cloud%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Point%2520Cloud%2520Classification%2520via%2520Offline%2520Distillation%2520Framework%250A%2520%2520and%2520Negative-Weight%2520Self-Distillation%2520Technique%26entry.906535625%3DQiang%2520Zheng%2520and%2520Chao%2520Zhang%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520point%2520cloud%2520processing%2520technologies%2520has%250Asignificantly%2520increased%2520the%2520demand%2520for%2520efficient%2520and%2520compact%2520models%2520that%250Aachieve%2520high-accuracy%2520classification.%2520Knowledge%2520distillation%2520has%2520emerged%2520as%2520a%250Apotent%2520model%2520compression%2520technique.%2520However%252C%2520traditional%2520KD%2520often%2520requires%250Aextensive%2520computational%2520resources%2520for%2520forward%2520inference%2520of%2520large%2520teacher%250Amodels%252C%2520thereby%2520reducing%2520training%2520efficiency%2520for%2520student%2520models%2520and%2520increasing%250Aresource%2520demands.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520an%2520innovative%250Aoffline%2520recording%2520strategy%2520that%2520avoids%2520the%2520simultaneous%2520loading%2520of%2520both%2520teacher%250Aand%2520student%2520models%252C%2520thereby%2520reducing%2520hardware%2520demands.%2520This%2520approach%2520feeds%2520a%250Amultitude%2520of%2520augmented%2520samples%2520into%2520the%2520teacher%2520model%252C%2520recording%2520both%2520the%2520data%250Aaugmentation%2520parameters%2520and%2520the%2520corresponding%2520logit%2520outputs.%2520By%2520applying%250Ashape-level%2520augmentation%2520operations%2520such%2520as%2520random%2520scaling%2520and%2520translation%252C%250Awhile%2520excluding%2520point-level%2520operations%2520like%2520random%2520jittering%252C%2520the%2520size%2520of%2520the%250Arecords%2520is%2520significantly%2520reduced.%2520Additionally%252C%2520to%2520mitigate%2520the%2520issue%2520of%2520small%250Astudent%2520model%2520over-imitating%2520the%2520teacher%2520model%2527s%2520outputs%2520and%2520converging%2520to%250Asuboptimal%2520solutions%252C%2520we%2520incorporate%2520a%2520negative-weight%2520self-distillation%250Astrategy.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520distillation%250Astrategy%2520enables%2520the%2520student%2520model%2520to%2520achieve%2520performance%2520comparable%2520to%250Astate-of-the-art%2520models%2520while%2520maintaining%2520lower%2520parameter%2520count.%2520This%2520approach%250Astrikes%2520an%2520optimal%2520balance%2520between%2520performance%2520and%2520complexity.%2520This%2520study%250Ahighlights%2520the%2520potential%2520of%2520our%2520method%2520to%2520optimize%2520knowledge%2520distillation%2520for%250Apoint%2520cloud%2520classification%2520tasks%252C%2520particularly%2520in%2520resource-constrained%250Aenvironments%252C%2520providing%2520a%2520novel%2520solution%2520for%2520efficient%2520point%2520cloud%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Point%20Cloud%20Classification%20via%20Offline%20Distillation%20Framework%0A%20%20and%20Negative-Weight%20Self-Distillation%20Technique&entry.906535625=Qiang%20Zheng%20and%20Chao%20Zhang%20and%20Jian%20Sun&entry.1292438233=%20%20The%20rapid%20advancement%20in%20point%20cloud%20processing%20technologies%20has%0Asignificantly%20increased%20the%20demand%20for%20efficient%20and%20compact%20models%20that%0Aachieve%20high-accuracy%20classification.%20Knowledge%20distillation%20has%20emerged%20as%20a%0Apotent%20model%20compression%20technique.%20However%2C%20traditional%20KD%20often%20requires%0Aextensive%20computational%20resources%20for%20forward%20inference%20of%20large%20teacher%0Amodels%2C%20thereby%20reducing%20training%20efficiency%20for%20student%20models%20and%20increasing%0Aresource%20demands.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20innovative%0Aoffline%20recording%20strategy%20that%20avoids%20the%20simultaneous%20loading%20of%20both%20teacher%0Aand%20student%20models%2C%20thereby%20reducing%20hardware%20demands.%20This%20approach%20feeds%20a%0Amultitude%20of%20augmented%20samples%20into%20the%20teacher%20model%2C%20recording%20both%20the%20data%0Aaugmentation%20parameters%20and%20the%20corresponding%20logit%20outputs.%20By%20applying%0Ashape-level%20augmentation%20operations%20such%20as%20random%20scaling%20and%20translation%2C%0Awhile%20excluding%20point-level%20operations%20like%20random%20jittering%2C%20the%20size%20of%20the%0Arecords%20is%20significantly%20reduced.%20Additionally%2C%20to%20mitigate%20the%20issue%20of%20small%0Astudent%20model%20over-imitating%20the%20teacher%20model%27s%20outputs%20and%20converging%20to%0Asuboptimal%20solutions%2C%20we%20incorporate%20a%20negative-weight%20self-distillation%0Astrategy.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20distillation%0Astrategy%20enables%20the%20student%20model%20to%20achieve%20performance%20comparable%20to%0Astate-of-the-art%20models%20while%20maintaining%20lower%20parameter%20count.%20This%20approach%0Astrikes%20an%20optimal%20balance%20between%20performance%20and%20complexity.%20This%20study%0Ahighlights%20the%20potential%20of%20our%20method%20to%20optimize%20knowledge%20distillation%20for%0Apoint%20cloud%20classification%20tasks%2C%20particularly%20in%20resource-constrained%0Aenvironments%2C%20providing%20a%20novel%20solution%20for%20efficient%20point%20cloud%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02020v2&entry.124074799=Read"},
{"title": "RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing\n  using Residual Policy Learning", "author": "Raphael Trumpp and Ehsan Javanmardi and Jin Nakazato and Manabu Tsukada and Marco Caccamo", "abstract": "  The interactive decision-making in multi-agent autonomous racing offers\ninsights valuable beyond the domain of self-driving cars. Mapless online path\nplanning is particularly of practical appeal but poses a challenge for safely\novertaking opponents due to the limited planning horizon. To address this, we\nintroduce RaceMOP, a novel method for mapless online path planning designed for\nmulti-agent racing of F1TENTH cars. Unlike classical planners that rely on\npredefined racing lines, RaceMOP operates without a map, utilizing only local\nobservations to execute high-speed overtaking maneuvers. Our approach combines\nan artificial potential field method as a base policy with residual policy\nlearning to enable long-horizon planning. We advance the field by introducing a\nnovel approach for policy fusion with the residual policy directly in\nprobability space. Extensive experiments on twelve simulated racetracks\nvalidate that RaceMOP is capable of long-horizon decision-making with robust\ncollision avoidance during overtaking maneuvers. RaceMOP demonstrates superior\nhandling over existing mapless planners and generalizes to unknown racetracks,\naffirming its potential for broader applications in robotics. Our code is\navailable at http://github.com/raphajaner/racemop.\n", "link": "http://arxiv.org/abs/2403.07129v2", "date": "2024-09-16", "relevancy": 2.1219, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5644}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5381}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaceMOP%3A%20Mapless%20Online%20Path%20Planning%20for%20Multi-Agent%20Autonomous%20Racing%0A%20%20using%20Residual%20Policy%20Learning&body=Title%3A%20RaceMOP%3A%20Mapless%20Online%20Path%20Planning%20for%20Multi-Agent%20Autonomous%20Racing%0A%20%20using%20Residual%20Policy%20Learning%0AAuthor%3A%20Raphael%20Trumpp%20and%20Ehsan%20Javanmardi%20and%20Jin%20Nakazato%20and%20Manabu%20Tsukada%20and%20Marco%20Caccamo%0AAbstract%3A%20%20%20The%20interactive%20decision-making%20in%20multi-agent%20autonomous%20racing%20offers%0Ainsights%20valuable%20beyond%20the%20domain%20of%20self-driving%20cars.%20Mapless%20online%20path%0Aplanning%20is%20particularly%20of%20practical%20appeal%20but%20poses%20a%20challenge%20for%20safely%0Aovertaking%20opponents%20due%20to%20the%20limited%20planning%20horizon.%20To%20address%20this%2C%20we%0Aintroduce%20RaceMOP%2C%20a%20novel%20method%20for%20mapless%20online%20path%20planning%20designed%20for%0Amulti-agent%20racing%20of%20F1TENTH%20cars.%20Unlike%20classical%20planners%20that%20rely%20on%0Apredefined%20racing%20lines%2C%20RaceMOP%20operates%20without%20a%20map%2C%20utilizing%20only%20local%0Aobservations%20to%20execute%20high-speed%20overtaking%20maneuvers.%20Our%20approach%20combines%0Aan%20artificial%20potential%20field%20method%20as%20a%20base%20policy%20with%20residual%20policy%0Alearning%20to%20enable%20long-horizon%20planning.%20We%20advance%20the%20field%20by%20introducing%20a%0Anovel%20approach%20for%20policy%20fusion%20with%20the%20residual%20policy%20directly%20in%0Aprobability%20space.%20Extensive%20experiments%20on%20twelve%20simulated%20racetracks%0Avalidate%20that%20RaceMOP%20is%20capable%20of%20long-horizon%20decision-making%20with%20robust%0Acollision%20avoidance%20during%20overtaking%20maneuvers.%20RaceMOP%20demonstrates%20superior%0Ahandling%20over%20existing%20mapless%20planners%20and%20generalizes%20to%20unknown%20racetracks%2C%0Aaffirming%20its%20potential%20for%20broader%20applications%20in%20robotics.%20Our%20code%20is%0Aavailable%20at%20http%3A//github.com/raphajaner/racemop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaceMOP%253A%2520Mapless%2520Online%2520Path%2520Planning%2520for%2520Multi-Agent%2520Autonomous%2520Racing%250A%2520%2520using%2520Residual%2520Policy%2520Learning%26entry.906535625%3DRaphael%2520Trumpp%2520and%2520Ehsan%2520Javanmardi%2520and%2520Jin%2520Nakazato%2520and%2520Manabu%2520Tsukada%2520and%2520Marco%2520Caccamo%26entry.1292438233%3D%2520%2520The%2520interactive%2520decision-making%2520in%2520multi-agent%2520autonomous%2520racing%2520offers%250Ainsights%2520valuable%2520beyond%2520the%2520domain%2520of%2520self-driving%2520cars.%2520Mapless%2520online%2520path%250Aplanning%2520is%2520particularly%2520of%2520practical%2520appeal%2520but%2520poses%2520a%2520challenge%2520for%2520safely%250Aovertaking%2520opponents%2520due%2520to%2520the%2520limited%2520planning%2520horizon.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520RaceMOP%252C%2520a%2520novel%2520method%2520for%2520mapless%2520online%2520path%2520planning%2520designed%2520for%250Amulti-agent%2520racing%2520of%2520F1TENTH%2520cars.%2520Unlike%2520classical%2520planners%2520that%2520rely%2520on%250Apredefined%2520racing%2520lines%252C%2520RaceMOP%2520operates%2520without%2520a%2520map%252C%2520utilizing%2520only%2520local%250Aobservations%2520to%2520execute%2520high-speed%2520overtaking%2520maneuvers.%2520Our%2520approach%2520combines%250Aan%2520artificial%2520potential%2520field%2520method%2520as%2520a%2520base%2520policy%2520with%2520residual%2520policy%250Alearning%2520to%2520enable%2520long-horizon%2520planning.%2520We%2520advance%2520the%2520field%2520by%2520introducing%2520a%250Anovel%2520approach%2520for%2520policy%2520fusion%2520with%2520the%2520residual%2520policy%2520directly%2520in%250Aprobability%2520space.%2520Extensive%2520experiments%2520on%2520twelve%2520simulated%2520racetracks%250Avalidate%2520that%2520RaceMOP%2520is%2520capable%2520of%2520long-horizon%2520decision-making%2520with%2520robust%250Acollision%2520avoidance%2520during%2520overtaking%2520maneuvers.%2520RaceMOP%2520demonstrates%2520superior%250Ahandling%2520over%2520existing%2520mapless%2520planners%2520and%2520generalizes%2520to%2520unknown%2520racetracks%252C%250Aaffirming%2520its%2520potential%2520for%2520broader%2520applications%2520in%2520robotics.%2520Our%2520code%2520is%250Aavailable%2520at%2520http%253A//github.com/raphajaner/racemop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaceMOP%3A%20Mapless%20Online%20Path%20Planning%20for%20Multi-Agent%20Autonomous%20Racing%0A%20%20using%20Residual%20Policy%20Learning&entry.906535625=Raphael%20Trumpp%20and%20Ehsan%20Javanmardi%20and%20Jin%20Nakazato%20and%20Manabu%20Tsukada%20and%20Marco%20Caccamo&entry.1292438233=%20%20The%20interactive%20decision-making%20in%20multi-agent%20autonomous%20racing%20offers%0Ainsights%20valuable%20beyond%20the%20domain%20of%20self-driving%20cars.%20Mapless%20online%20path%0Aplanning%20is%20particularly%20of%20practical%20appeal%20but%20poses%20a%20challenge%20for%20safely%0Aovertaking%20opponents%20due%20to%20the%20limited%20planning%20horizon.%20To%20address%20this%2C%20we%0Aintroduce%20RaceMOP%2C%20a%20novel%20method%20for%20mapless%20online%20path%20planning%20designed%20for%0Amulti-agent%20racing%20of%20F1TENTH%20cars.%20Unlike%20classical%20planners%20that%20rely%20on%0Apredefined%20racing%20lines%2C%20RaceMOP%20operates%20without%20a%20map%2C%20utilizing%20only%20local%0Aobservations%20to%20execute%20high-speed%20overtaking%20maneuvers.%20Our%20approach%20combines%0Aan%20artificial%20potential%20field%20method%20as%20a%20base%20policy%20with%20residual%20policy%0Alearning%20to%20enable%20long-horizon%20planning.%20We%20advance%20the%20field%20by%20introducing%20a%0Anovel%20approach%20for%20policy%20fusion%20with%20the%20residual%20policy%20directly%20in%0Aprobability%20space.%20Extensive%20experiments%20on%20twelve%20simulated%20racetracks%0Avalidate%20that%20RaceMOP%20is%20capable%20of%20long-horizon%20decision-making%20with%20robust%0Acollision%20avoidance%20during%20overtaking%20maneuvers.%20RaceMOP%20demonstrates%20superior%0Ahandling%20over%20existing%20mapless%20planners%20and%20generalizes%20to%20unknown%20racetracks%2C%0Aaffirming%20its%20potential%20for%20broader%20applications%20in%20robotics.%20Our%20code%20is%0Aavailable%20at%20http%3A//github.com/raphajaner/racemop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07129v2&entry.124074799=Read"},
{"title": "On Semidefinite Relaxations for Matrix-Weighted State-Estimation\n  Problems in Robotics", "author": "Connor Holmes and Frederike D\u00fcmbgen and Timothy D Barfoot", "abstract": "  In recent years, there has been remarkable progress in the development of\nso-called certifiable perception methods, which leverage semidefinite, convex\nrelaxations to find global optima of perception problems in robotics. However,\nmany of these relaxations rely on simplifying assumptions that facilitate the\nproblem formulation, such as an isotropic measurement noise distribution. In\nthis paper, we explore the tightness of the semidefinite relaxations of\nmatrix-weighted (anisotropic) state-estimation problems and reveal the\nlimitations lurking therein: matrix-weighted factors can cause convex\nrelaxations to lose tightness. In particular, we show that the semidefinite\nrelaxations of localization problems with matrix weights may be tight only for\nlow noise levels. To better understand this issue, we introduce a theoretical\nconnection between the posterior uncertainty of the state estimate and the\ncertificate matrix obtained via convex relaxation. With this connection in\nmind, we empirically explore the factors that contribute to this loss of\ntightness and demonstrate that redundant constraints can be used to regain it.\nAs a second technical contribution of this paper, we show that the\nstate-of-the-art relaxation of scalar-weighted SLAM cannot be used when matrix\nweights are considered. We provide an alternate formulation and show that its\nSDP relaxation is not tight (even for very low noise levels) unless specific\nredundant constraints are used. We demonstrate the tightness of our\nformulations on both simulated and real-world data.\n", "link": "http://arxiv.org/abs/2308.07275v3", "date": "2024-09-16", "relevancy": 2.115, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5551}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Semidefinite%20Relaxations%20for%20Matrix-Weighted%20State-Estimation%0A%20%20Problems%20in%20Robotics&body=Title%3A%20On%20Semidefinite%20Relaxations%20for%20Matrix-Weighted%20State-Estimation%0A%20%20Problems%20in%20Robotics%0AAuthor%3A%20Connor%20Holmes%20and%20Frederike%20D%C3%BCmbgen%20and%20Timothy%20D%20Barfoot%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20remarkable%20progress%20in%20the%20development%20of%0Aso-called%20certifiable%20perception%20methods%2C%20which%20leverage%20semidefinite%2C%20convex%0Arelaxations%20to%20find%20global%20optima%20of%20perception%20problems%20in%20robotics.%20However%2C%0Amany%20of%20these%20relaxations%20rely%20on%20simplifying%20assumptions%20that%20facilitate%20the%0Aproblem%20formulation%2C%20such%20as%20an%20isotropic%20measurement%20noise%20distribution.%20In%0Athis%20paper%2C%20we%20explore%20the%20tightness%20of%20the%20semidefinite%20relaxations%20of%0Amatrix-weighted%20%28anisotropic%29%20state-estimation%20problems%20and%20reveal%20the%0Alimitations%20lurking%20therein%3A%20matrix-weighted%20factors%20can%20cause%20convex%0Arelaxations%20to%20lose%20tightness.%20In%20particular%2C%20we%20show%20that%20the%20semidefinite%0Arelaxations%20of%20localization%20problems%20with%20matrix%20weights%20may%20be%20tight%20only%20for%0Alow%20noise%20levels.%20To%20better%20understand%20this%20issue%2C%20we%20introduce%20a%20theoretical%0Aconnection%20between%20the%20posterior%20uncertainty%20of%20the%20state%20estimate%20and%20the%0Acertificate%20matrix%20obtained%20via%20convex%20relaxation.%20With%20this%20connection%20in%0Amind%2C%20we%20empirically%20explore%20the%20factors%20that%20contribute%20to%20this%20loss%20of%0Atightness%20and%20demonstrate%20that%20redundant%20constraints%20can%20be%20used%20to%20regain%20it.%0AAs%20a%20second%20technical%20contribution%20of%20this%20paper%2C%20we%20show%20that%20the%0Astate-of-the-art%20relaxation%20of%20scalar-weighted%20SLAM%20cannot%20be%20used%20when%20matrix%0Aweights%20are%20considered.%20We%20provide%20an%20alternate%20formulation%20and%20show%20that%20its%0ASDP%20relaxation%20is%20not%20tight%20%28even%20for%20very%20low%20noise%20levels%29%20unless%20specific%0Aredundant%20constraints%20are%20used.%20We%20demonstrate%20the%20tightness%20of%20our%0Aformulations%20on%20both%20simulated%20and%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07275v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Semidefinite%2520Relaxations%2520for%2520Matrix-Weighted%2520State-Estimation%250A%2520%2520Problems%2520in%2520Robotics%26entry.906535625%3DConnor%2520Holmes%2520and%2520Frederike%2520D%25C3%25BCmbgen%2520and%2520Timothy%2520D%2520Barfoot%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520remarkable%2520progress%2520in%2520the%2520development%2520of%250Aso-called%2520certifiable%2520perception%2520methods%252C%2520which%2520leverage%2520semidefinite%252C%2520convex%250Arelaxations%2520to%2520find%2520global%2520optima%2520of%2520perception%2520problems%2520in%2520robotics.%2520However%252C%250Amany%2520of%2520these%2520relaxations%2520rely%2520on%2520simplifying%2520assumptions%2520that%2520facilitate%2520the%250Aproblem%2520formulation%252C%2520such%2520as%2520an%2520isotropic%2520measurement%2520noise%2520distribution.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520the%2520tightness%2520of%2520the%2520semidefinite%2520relaxations%2520of%250Amatrix-weighted%2520%2528anisotropic%2529%2520state-estimation%2520problems%2520and%2520reveal%2520the%250Alimitations%2520lurking%2520therein%253A%2520matrix-weighted%2520factors%2520can%2520cause%2520convex%250Arelaxations%2520to%2520lose%2520tightness.%2520In%2520particular%252C%2520we%2520show%2520that%2520the%2520semidefinite%250Arelaxations%2520of%2520localization%2520problems%2520with%2520matrix%2520weights%2520may%2520be%2520tight%2520only%2520for%250Alow%2520noise%2520levels.%2520To%2520better%2520understand%2520this%2520issue%252C%2520we%2520introduce%2520a%2520theoretical%250Aconnection%2520between%2520the%2520posterior%2520uncertainty%2520of%2520the%2520state%2520estimate%2520and%2520the%250Acertificate%2520matrix%2520obtained%2520via%2520convex%2520relaxation.%2520With%2520this%2520connection%2520in%250Amind%252C%2520we%2520empirically%2520explore%2520the%2520factors%2520that%2520contribute%2520to%2520this%2520loss%2520of%250Atightness%2520and%2520demonstrate%2520that%2520redundant%2520constraints%2520can%2520be%2520used%2520to%2520regain%2520it.%250AAs%2520a%2520second%2520technical%2520contribution%2520of%2520this%2520paper%252C%2520we%2520show%2520that%2520the%250Astate-of-the-art%2520relaxation%2520of%2520scalar-weighted%2520SLAM%2520cannot%2520be%2520used%2520when%2520matrix%250Aweights%2520are%2520considered.%2520We%2520provide%2520an%2520alternate%2520formulation%2520and%2520show%2520that%2520its%250ASDP%2520relaxation%2520is%2520not%2520tight%2520%2528even%2520for%2520very%2520low%2520noise%2520levels%2529%2520unless%2520specific%250Aredundant%2520constraints%2520are%2520used.%2520We%2520demonstrate%2520the%2520tightness%2520of%2520our%250Aformulations%2520on%2520both%2520simulated%2520and%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07275v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Semidefinite%20Relaxations%20for%20Matrix-Weighted%20State-Estimation%0A%20%20Problems%20in%20Robotics&entry.906535625=Connor%20Holmes%20and%20Frederike%20D%C3%BCmbgen%20and%20Timothy%20D%20Barfoot&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20remarkable%20progress%20in%20the%20development%20of%0Aso-called%20certifiable%20perception%20methods%2C%20which%20leverage%20semidefinite%2C%20convex%0Arelaxations%20to%20find%20global%20optima%20of%20perception%20problems%20in%20robotics.%20However%2C%0Amany%20of%20these%20relaxations%20rely%20on%20simplifying%20assumptions%20that%20facilitate%20the%0Aproblem%20formulation%2C%20such%20as%20an%20isotropic%20measurement%20noise%20distribution.%20In%0Athis%20paper%2C%20we%20explore%20the%20tightness%20of%20the%20semidefinite%20relaxations%20of%0Amatrix-weighted%20%28anisotropic%29%20state-estimation%20problems%20and%20reveal%20the%0Alimitations%20lurking%20therein%3A%20matrix-weighted%20factors%20can%20cause%20convex%0Arelaxations%20to%20lose%20tightness.%20In%20particular%2C%20we%20show%20that%20the%20semidefinite%0Arelaxations%20of%20localization%20problems%20with%20matrix%20weights%20may%20be%20tight%20only%20for%0Alow%20noise%20levels.%20To%20better%20understand%20this%20issue%2C%20we%20introduce%20a%20theoretical%0Aconnection%20between%20the%20posterior%20uncertainty%20of%20the%20state%20estimate%20and%20the%0Acertificate%20matrix%20obtained%20via%20convex%20relaxation.%20With%20this%20connection%20in%0Amind%2C%20we%20empirically%20explore%20the%20factors%20that%20contribute%20to%20this%20loss%20of%0Atightness%20and%20demonstrate%20that%20redundant%20constraints%20can%20be%20used%20to%20regain%20it.%0AAs%20a%20second%20technical%20contribution%20of%20this%20paper%2C%20we%20show%20that%20the%0Astate-of-the-art%20relaxation%20of%20scalar-weighted%20SLAM%20cannot%20be%20used%20when%20matrix%0Aweights%20are%20considered.%20We%20provide%20an%20alternate%20formulation%20and%20show%20that%20its%0ASDP%20relaxation%20is%20not%20tight%20%28even%20for%20very%20low%20noise%20levels%29%20unless%20specific%0Aredundant%20constraints%20are%20used.%20We%20demonstrate%20the%20tightness%20of%20our%0Aformulations%20on%20both%20simulated%20and%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07275v3&entry.124074799=Read"},
{"title": "Synchronization-Based Cooperative Distributed Model Predictive Control", "author": "Julius Beerwerth and Maximilian Kloock and Bassam Alrifaee", "abstract": "  Distributed control algorithms are known to reduce overall computation time\ncompared to centralized control algorithms. However, they can result in\ninconsistent solutions leading to the violation of safety-critical constraints.\nInconsistent solutions can arise when two or more agents compute concurrently\nwhile making predictions on each others control actions. To address this issue,\nwe propose an iterative algorithm called Synchronization-Based Cooperative\nDistributed Model Predictive Control, which we presented in [1]. The algorithm\nconsists of two steps: 1. computing the optimal control inputs for each agent\nand 2. synchronizing the predicted states across all agents. We demonstrate the\nefficacy of our algorithm in the control of multiple small-scale vehicles in\nour Cyber-Physical Mobility Lab.\n", "link": "http://arxiv.org/abs/2409.10215v1", "date": "2024-09-16", "relevancy": 2.1, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5301}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronization-Based%20Cooperative%20Distributed%20Model%20Predictive%20Control&body=Title%3A%20Synchronization-Based%20Cooperative%20Distributed%20Model%20Predictive%20Control%0AAuthor%3A%20Julius%20Beerwerth%20and%20Maximilian%20Kloock%20and%20Bassam%20Alrifaee%0AAbstract%3A%20%20%20Distributed%20control%20algorithms%20are%20known%20to%20reduce%20overall%20computation%20time%0Acompared%20to%20centralized%20control%20algorithms.%20However%2C%20they%20can%20result%20in%0Ainconsistent%20solutions%20leading%20to%20the%20violation%20of%20safety-critical%20constraints.%0AInconsistent%20solutions%20can%20arise%20when%20two%20or%20more%20agents%20compute%20concurrently%0Awhile%20making%20predictions%20on%20each%20others%20control%20actions.%20To%20address%20this%20issue%2C%0Awe%20propose%20an%20iterative%20algorithm%20called%20Synchronization-Based%20Cooperative%0ADistributed%20Model%20Predictive%20Control%2C%20which%20we%20presented%20in%20%5B1%5D.%20The%20algorithm%0Aconsists%20of%20two%20steps%3A%201.%20computing%20the%20optimal%20control%20inputs%20for%20each%20agent%0Aand%202.%20synchronizing%20the%20predicted%20states%20across%20all%20agents.%20We%20demonstrate%20the%0Aefficacy%20of%20our%20algorithm%20in%20the%20control%20of%20multiple%20small-scale%20vehicles%20in%0Aour%20Cyber-Physical%20Mobility%20Lab.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronization-Based%2520Cooperative%2520Distributed%2520Model%2520Predictive%2520Control%26entry.906535625%3DJulius%2520Beerwerth%2520and%2520Maximilian%2520Kloock%2520and%2520Bassam%2520Alrifaee%26entry.1292438233%3D%2520%2520Distributed%2520control%2520algorithms%2520are%2520known%2520to%2520reduce%2520overall%2520computation%2520time%250Acompared%2520to%2520centralized%2520control%2520algorithms.%2520However%252C%2520they%2520can%2520result%2520in%250Ainconsistent%2520solutions%2520leading%2520to%2520the%2520violation%2520of%2520safety-critical%2520constraints.%250AInconsistent%2520solutions%2520can%2520arise%2520when%2520two%2520or%2520more%2520agents%2520compute%2520concurrently%250Awhile%2520making%2520predictions%2520on%2520each%2520others%2520control%2520actions.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520an%2520iterative%2520algorithm%2520called%2520Synchronization-Based%2520Cooperative%250ADistributed%2520Model%2520Predictive%2520Control%252C%2520which%2520we%2520presented%2520in%2520%255B1%255D.%2520The%2520algorithm%250Aconsists%2520of%2520two%2520steps%253A%25201.%2520computing%2520the%2520optimal%2520control%2520inputs%2520for%2520each%2520agent%250Aand%25202.%2520synchronizing%2520the%2520predicted%2520states%2520across%2520all%2520agents.%2520We%2520demonstrate%2520the%250Aefficacy%2520of%2520our%2520algorithm%2520in%2520the%2520control%2520of%2520multiple%2520small-scale%2520vehicles%2520in%250Aour%2520Cyber-Physical%2520Mobility%2520Lab.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronization-Based%20Cooperative%20Distributed%20Model%20Predictive%20Control&entry.906535625=Julius%20Beerwerth%20and%20Maximilian%20Kloock%20and%20Bassam%20Alrifaee&entry.1292438233=%20%20Distributed%20control%20algorithms%20are%20known%20to%20reduce%20overall%20computation%20time%0Acompared%20to%20centralized%20control%20algorithms.%20However%2C%20they%20can%20result%20in%0Ainconsistent%20solutions%20leading%20to%20the%20violation%20of%20safety-critical%20constraints.%0AInconsistent%20solutions%20can%20arise%20when%20two%20or%20more%20agents%20compute%20concurrently%0Awhile%20making%20predictions%20on%20each%20others%20control%20actions.%20To%20address%20this%20issue%2C%0Awe%20propose%20an%20iterative%20algorithm%20called%20Synchronization-Based%20Cooperative%0ADistributed%20Model%20Predictive%20Control%2C%20which%20we%20presented%20in%20%5B1%5D.%20The%20algorithm%0Aconsists%20of%20two%20steps%3A%201.%20computing%20the%20optimal%20control%20inputs%20for%20each%20agent%0Aand%202.%20synchronizing%20the%20predicted%20states%20across%20all%20agents.%20We%20demonstrate%20the%0Aefficacy%20of%20our%20algorithm%20in%20the%20control%20of%20multiple%20small-scale%20vehicles%20in%0Aour%20Cyber-Physical%20Mobility%20Lab.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10215v1&entry.124074799=Read"},
{"title": "AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal\n  Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from\n  Whole-Body PET/CT Images", "author": "Shadab Ahamed", "abstract": "  Automated segmentation of cancerous lesions in PET/CT scans is a crucial\nfirst step in quantitative image analysis. However, training deep learning\nmodels for segmentation with high accuracy is particularly challenging due to\nthe variations in lesion size, shape, and radiotracer uptake. These lesions can\nappear in different parts of the body, often near healthy organs that also\nexhibit considerable uptake, making the task even more complex. As a result,\ncreating an effective segmentation model for routine PET/CT image analysis is\nchallenging. In this study, we utilized a 3D Residual UNet model and employed\nthe Generalized Dice Focal Loss function to train the model on the AutoPET\nChallenge 2024 dataset. We conducted a 5-fold cross-validation and used an\naverage ensembling technique using the models from the five folds. In the\npreliminary test phase for Task-1, the average ensemble achieved a mean Dice\nSimilarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of\n10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about\nthe algorithm can be found on our GitHub repository:\nhttps://github.com/ahxmeds/autosegnet2024.git. The training code has been\nshared via the repository: https://github.com/ahxmeds/autopet2024.git.\n", "link": "http://arxiv.org/abs/2409.10151v1", "date": "2024-09-16", "relevancy": 2.0998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5157}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPET%20Challenge%20III%3A%20Testing%20the%20Robustness%20of%20Generalized%20Dice%20Focal%0A%20%20Loss%20trained%203D%20Residual%20UNet%20for%20FDG%20and%20PSMA%20Lesion%20Segmentation%20from%0A%20%20Whole-Body%20PET/CT%20Images&body=Title%3A%20AutoPET%20Challenge%20III%3A%20Testing%20the%20Robustness%20of%20Generalized%20Dice%20Focal%0A%20%20Loss%20trained%203D%20Residual%20UNet%20for%20FDG%20and%20PSMA%20Lesion%20Segmentation%20from%0A%20%20Whole-Body%20PET/CT%20Images%0AAuthor%3A%20Shadab%20Ahamed%0AAbstract%3A%20%20%20Automated%20segmentation%20of%20cancerous%20lesions%20in%20PET/CT%20scans%20is%20a%20crucial%0Afirst%20step%20in%20quantitative%20image%20analysis.%20However%2C%20training%20deep%20learning%0Amodels%20for%20segmentation%20with%20high%20accuracy%20is%20particularly%20challenging%20due%20to%0Athe%20variations%20in%20lesion%20size%2C%20shape%2C%20and%20radiotracer%20uptake.%20These%20lesions%20can%0Aappear%20in%20different%20parts%20of%20the%20body%2C%20often%20near%20healthy%20organs%20that%20also%0Aexhibit%20considerable%20uptake%2C%20making%20the%20task%20even%20more%20complex.%20As%20a%20result%2C%0Acreating%20an%20effective%20segmentation%20model%20for%20routine%20PET/CT%20image%20analysis%20is%0Achallenging.%20In%20this%20study%2C%20we%20utilized%20a%203D%20Residual%20UNet%20model%20and%20employed%0Athe%20Generalized%20Dice%20Focal%20Loss%20function%20to%20train%20the%20model%20on%20the%20AutoPET%0AChallenge%202024%20dataset.%20We%20conducted%20a%205-fold%20cross-validation%20and%20used%20an%0Aaverage%20ensembling%20technique%20using%20the%20models%20from%20the%20five%20folds.%20In%20the%0Apreliminary%20test%20phase%20for%20Task-1%2C%20the%20average%20ensemble%20achieved%20a%20mean%20Dice%0ASimilarity%20Coefficient%20%28DSC%29%20of%200.6687%2C%20mean%20false%20negative%20volume%20%28FNV%29%20of%0A10.9522%20ml%20and%20mean%20false%20positive%20volume%20%28FPV%29%202.9684%20ml.%20More%20details%20about%0Athe%20algorithm%20can%20be%20found%20on%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/ahxmeds/autosegnet2024.git.%20The%20training%20code%20has%20been%0Ashared%20via%20the%20repository%3A%20https%3A//github.com/ahxmeds/autopet2024.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPET%2520Challenge%2520III%253A%2520Testing%2520the%2520Robustness%2520of%2520Generalized%2520Dice%2520Focal%250A%2520%2520Loss%2520trained%25203D%2520Residual%2520UNet%2520for%2520FDG%2520and%2520PSMA%2520Lesion%2520Segmentation%2520from%250A%2520%2520Whole-Body%2520PET/CT%2520Images%26entry.906535625%3DShadab%2520Ahamed%26entry.1292438233%3D%2520%2520Automated%2520segmentation%2520of%2520cancerous%2520lesions%2520in%2520PET/CT%2520scans%2520is%2520a%2520crucial%250Afirst%2520step%2520in%2520quantitative%2520image%2520analysis.%2520However%252C%2520training%2520deep%2520learning%250Amodels%2520for%2520segmentation%2520with%2520high%2520accuracy%2520is%2520particularly%2520challenging%2520due%2520to%250Athe%2520variations%2520in%2520lesion%2520size%252C%2520shape%252C%2520and%2520radiotracer%2520uptake.%2520These%2520lesions%2520can%250Aappear%2520in%2520different%2520parts%2520of%2520the%2520body%252C%2520often%2520near%2520healthy%2520organs%2520that%2520also%250Aexhibit%2520considerable%2520uptake%252C%2520making%2520the%2520task%2520even%2520more%2520complex.%2520As%2520a%2520result%252C%250Acreating%2520an%2520effective%2520segmentation%2520model%2520for%2520routine%2520PET/CT%2520image%2520analysis%2520is%250Achallenging.%2520In%2520this%2520study%252C%2520we%2520utilized%2520a%25203D%2520Residual%2520UNet%2520model%2520and%2520employed%250Athe%2520Generalized%2520Dice%2520Focal%2520Loss%2520function%2520to%2520train%2520the%2520model%2520on%2520the%2520AutoPET%250AChallenge%25202024%2520dataset.%2520We%2520conducted%2520a%25205-fold%2520cross-validation%2520and%2520used%2520an%250Aaverage%2520ensembling%2520technique%2520using%2520the%2520models%2520from%2520the%2520five%2520folds.%2520In%2520the%250Apreliminary%2520test%2520phase%2520for%2520Task-1%252C%2520the%2520average%2520ensemble%2520achieved%2520a%2520mean%2520Dice%250ASimilarity%2520Coefficient%2520%2528DSC%2529%2520of%25200.6687%252C%2520mean%2520false%2520negative%2520volume%2520%2528FNV%2529%2520of%250A10.9522%2520ml%2520and%2520mean%2520false%2520positive%2520volume%2520%2528FPV%2529%25202.9684%2520ml.%2520More%2520details%2520about%250Athe%2520algorithm%2520can%2520be%2520found%2520on%2520our%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/ahxmeds/autosegnet2024.git.%2520The%2520training%2520code%2520has%2520been%250Ashared%2520via%2520the%2520repository%253A%2520https%253A//github.com/ahxmeds/autopet2024.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPET%20Challenge%20III%3A%20Testing%20the%20Robustness%20of%20Generalized%20Dice%20Focal%0A%20%20Loss%20trained%203D%20Residual%20UNet%20for%20FDG%20and%20PSMA%20Lesion%20Segmentation%20from%0A%20%20Whole-Body%20PET/CT%20Images&entry.906535625=Shadab%20Ahamed&entry.1292438233=%20%20Automated%20segmentation%20of%20cancerous%20lesions%20in%20PET/CT%20scans%20is%20a%20crucial%0Afirst%20step%20in%20quantitative%20image%20analysis.%20However%2C%20training%20deep%20learning%0Amodels%20for%20segmentation%20with%20high%20accuracy%20is%20particularly%20challenging%20due%20to%0Athe%20variations%20in%20lesion%20size%2C%20shape%2C%20and%20radiotracer%20uptake.%20These%20lesions%20can%0Aappear%20in%20different%20parts%20of%20the%20body%2C%20often%20near%20healthy%20organs%20that%20also%0Aexhibit%20considerable%20uptake%2C%20making%20the%20task%20even%20more%20complex.%20As%20a%20result%2C%0Acreating%20an%20effective%20segmentation%20model%20for%20routine%20PET/CT%20image%20analysis%20is%0Achallenging.%20In%20this%20study%2C%20we%20utilized%20a%203D%20Residual%20UNet%20model%20and%20employed%0Athe%20Generalized%20Dice%20Focal%20Loss%20function%20to%20train%20the%20model%20on%20the%20AutoPET%0AChallenge%202024%20dataset.%20We%20conducted%20a%205-fold%20cross-validation%20and%20used%20an%0Aaverage%20ensembling%20technique%20using%20the%20models%20from%20the%20five%20folds.%20In%20the%0Apreliminary%20test%20phase%20for%20Task-1%2C%20the%20average%20ensemble%20achieved%20a%20mean%20Dice%0ASimilarity%20Coefficient%20%28DSC%29%20of%200.6687%2C%20mean%20false%20negative%20volume%20%28FNV%29%20of%0A10.9522%20ml%20and%20mean%20false%20positive%20volume%20%28FPV%29%202.9684%20ml.%20More%20details%20about%0Athe%20algorithm%20can%20be%20found%20on%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/ahxmeds/autosegnet2024.git.%20The%20training%20code%20has%20been%0Ashared%20via%20the%20repository%3A%20https%3A//github.com/ahxmeds/autopet2024.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10151v1&entry.124074799=Read"},
{"title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models", "author": "Namjoon Suh and Guang Cheng", "abstract": "  In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics.\n", "link": "http://arxiv.org/abs/2401.07187v3", "date": "2024-09-16", "relevancy": 2.0925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5196}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Statistical%20Theory%20of%20Deep%20Learning%3A%20Approximation%2C%20Training%0A%20%20Dynamics%2C%20and%20Generative%20Models&body=Title%3A%20A%20Survey%20on%20Statistical%20Theory%20of%20Deep%20Learning%3A%20Approximation%2C%20Training%0A%20%20Dynamics%2C%20and%20Generative%20Models%0AAuthor%3A%20Namjoon%20Suh%20and%20Guang%20Cheng%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20review%20the%20literature%20on%20statistical%20theories%20of%20neural%0Anetworks%20from%20three%20perspectives%3A%20approximation%2C%20training%20dynamics%20and%0Agenerative%20models.%20In%20the%20first%20part%2C%20results%20on%20excess%20risks%20for%20neural%0Anetworks%20are%20reviewed%20in%20the%20nonparametric%20framework%20of%20regression%20%28and%0Aclassification%20in%20Appendix~%7B%5Ccolor%7Bblue%7DB%7D%29.%20These%20results%20rely%20on%20explicit%0Aconstructions%20of%20neural%20networks%2C%20leading%20to%20fast%20convergence%20rates%20of%20excess%0Arisks.%20Nonetheless%2C%20their%20underlying%20analysis%20only%20applies%20to%20the%20global%0Aminimizer%20in%20the%20highly%20non-convex%20landscape%20of%20deep%20neural%20networks.%20This%0Amotivates%20us%20to%20review%20the%20training%20dynamics%20of%20neural%20networks%20in%20the%20second%0Apart.%20Specifically%2C%20we%20review%20papers%20that%20attempt%20to%20answer%20%60%60how%20the%20neural%0Anetwork%20trained%20via%20gradient-based%20methods%20finds%20the%20solution%20that%20can%0Ageneralize%20well%20on%20unseen%20data.%27%27%20In%20particular%2C%20two%20well-known%20paradigms%20are%0Areviewed%3A%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20paradigm%2C%20and%20Mean-Field%20%28MF%29%0Aparadigm.%20Last%20but%20not%20least%2C%20we%20review%20the%20most%20recent%20theoretical%0Aadvancements%20in%20generative%20models%20including%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20diffusion%20models%2C%20and%20in-context%20learning%20%28ICL%29%20in%20the%20Large%20Language%0AModels%20%28LLMs%29%20from%20two%20perpsectives%20reviewed%20previously%2C%20i.e.%2C%20approximation%0Aand%20training%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07187v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Statistical%2520Theory%2520of%2520Deep%2520Learning%253A%2520Approximation%252C%2520Training%250A%2520%2520Dynamics%252C%2520and%2520Generative%2520Models%26entry.906535625%3DNamjoon%2520Suh%2520and%2520Guang%2520Cheng%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520review%2520the%2520literature%2520on%2520statistical%2520theories%2520of%2520neural%250Anetworks%2520from%2520three%2520perspectives%253A%2520approximation%252C%2520training%2520dynamics%2520and%250Agenerative%2520models.%2520In%2520the%2520first%2520part%252C%2520results%2520on%2520excess%2520risks%2520for%2520neural%250Anetworks%2520are%2520reviewed%2520in%2520the%2520nonparametric%2520framework%2520of%2520regression%2520%2528and%250Aclassification%2520in%2520Appendix~%257B%255Ccolor%257Bblue%257DB%257D%2529.%2520These%2520results%2520rely%2520on%2520explicit%250Aconstructions%2520of%2520neural%2520networks%252C%2520leading%2520to%2520fast%2520convergence%2520rates%2520of%2520excess%250Arisks.%2520Nonetheless%252C%2520their%2520underlying%2520analysis%2520only%2520applies%2520to%2520the%2520global%250Aminimizer%2520in%2520the%2520highly%2520non-convex%2520landscape%2520of%2520deep%2520neural%2520networks.%2520This%250Amotivates%2520us%2520to%2520review%2520the%2520training%2520dynamics%2520of%2520neural%2520networks%2520in%2520the%2520second%250Apart.%2520Specifically%252C%2520we%2520review%2520papers%2520that%2520attempt%2520to%2520answer%2520%2560%2560how%2520the%2520neural%250Anetwork%2520trained%2520via%2520gradient-based%2520methods%2520finds%2520the%2520solution%2520that%2520can%250Ageneralize%2520well%2520on%2520unseen%2520data.%2527%2527%2520In%2520particular%252C%2520two%2520well-known%2520paradigms%2520are%250Areviewed%253A%2520the%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520paradigm%252C%2520and%2520Mean-Field%2520%2528MF%2529%250Aparadigm.%2520Last%2520but%2520not%2520least%252C%2520we%2520review%2520the%2520most%2520recent%2520theoretical%250Aadvancements%2520in%2520generative%2520models%2520including%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%252C%2520diffusion%2520models%252C%2520and%2520in-context%2520learning%2520%2528ICL%2529%2520in%2520the%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520from%2520two%2520perpsectives%2520reviewed%2520previously%252C%2520i.e.%252C%2520approximation%250Aand%2520training%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07187v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Statistical%20Theory%20of%20Deep%20Learning%3A%20Approximation%2C%20Training%0A%20%20Dynamics%2C%20and%20Generative%20Models&entry.906535625=Namjoon%20Suh%20and%20Guang%20Cheng&entry.1292438233=%20%20In%20this%20article%2C%20we%20review%20the%20literature%20on%20statistical%20theories%20of%20neural%0Anetworks%20from%20three%20perspectives%3A%20approximation%2C%20training%20dynamics%20and%0Agenerative%20models.%20In%20the%20first%20part%2C%20results%20on%20excess%20risks%20for%20neural%0Anetworks%20are%20reviewed%20in%20the%20nonparametric%20framework%20of%20regression%20%28and%0Aclassification%20in%20Appendix~%7B%5Ccolor%7Bblue%7DB%7D%29.%20These%20results%20rely%20on%20explicit%0Aconstructions%20of%20neural%20networks%2C%20leading%20to%20fast%20convergence%20rates%20of%20excess%0Arisks.%20Nonetheless%2C%20their%20underlying%20analysis%20only%20applies%20to%20the%20global%0Aminimizer%20in%20the%20highly%20non-convex%20landscape%20of%20deep%20neural%20networks.%20This%0Amotivates%20us%20to%20review%20the%20training%20dynamics%20of%20neural%20networks%20in%20the%20second%0Apart.%20Specifically%2C%20we%20review%20papers%20that%20attempt%20to%20answer%20%60%60how%20the%20neural%0Anetwork%20trained%20via%20gradient-based%20methods%20finds%20the%20solution%20that%20can%0Ageneralize%20well%20on%20unseen%20data.%27%27%20In%20particular%2C%20two%20well-known%20paradigms%20are%0Areviewed%3A%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20paradigm%2C%20and%20Mean-Field%20%28MF%29%0Aparadigm.%20Last%20but%20not%20least%2C%20we%20review%20the%20most%20recent%20theoretical%0Aadvancements%20in%20generative%20models%20including%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20diffusion%20models%2C%20and%20in-context%20learning%20%28ICL%29%20in%20the%20Large%20Language%0AModels%20%28LLMs%29%20from%20two%20perpsectives%20reviewed%20previously%2C%20i.e.%2C%20approximation%0Aand%20training%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07187v3&entry.124074799=Read"},
{"title": "2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?", "author": "T\u00e9o Guichoux and Laure Soulier and Nicolas Obin and Catherine Pelachaud", "abstract": "  Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.\n", "link": "http://arxiv.org/abs/2409.10357v1", "date": "2024-09-16", "relevancy": 2.0923, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F&body=Title%3A%202D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F%0AAuthor%3A%20T%C3%A9o%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud%0AAbstract%3A%20%20%20Co-speech%20gestures%20are%20fundamental%20for%20communication.%20The%20advent%20of%20recent%0Adeep%20learning%20techniques%20has%20facilitated%20the%20creation%20of%20lifelike%2C%20synchronous%0Aco-speech%20gestures%20for%20Embodied%20Conversational%20Agents.%20%22In-the-wild%22%20datasets%2C%0Aaggregating%20video%20content%20from%20platforms%20like%20YouTube%20via%20human%20pose%20detection%0Atechnologies%2C%20provide%20a%20feasible%20solution%20by%20offering%202D%20skeletal%20sequences%0Aaligned%20with%20speech.%20Concurrent%20developments%20in%20lifting%20models%20enable%20the%0Aconversion%20of%20these%202D%20sequences%20into%203D%20gesture%20databases.%20However%2C%20it%20is%0Aimportant%20to%20note%20that%20the%203D%20poses%20estimated%20from%20the%202D%20extracted%20poses%20are%2C%0Ain%20essence%2C%20approximations%20of%20the%20ground-truth%2C%20which%20remains%20in%20the%202D%20domain.%0AThis%20distinction%20raises%20questions%20about%20the%20impact%20of%20gesture%20representation%0Adimensionality%20on%20the%20quality%20of%20generated%20motions%20-%20a%20topic%20that%2C%20to%20our%0Aknowledge%2C%20remains%20largely%20unexplored.%20Our%20study%20examines%20the%20effect%20of%20using%0Aeither%202D%20or%203D%20joint%20coordinates%20as%20training%20data%20on%20the%20performance%20of%0Aspeech-to-gesture%20deep%20generative%20models.%20We%20employ%20a%20lifting%20model%20for%0Aconverting%20generated%202D%20pose%20sequences%20into%203D%20and%20assess%20how%20gestures%20created%0Adirectly%20in%203D%20stack%20up%20against%20those%20initially%20generated%20in%202D%20and%20then%0Aconverted%20to%203D.%20We%20perform%20an%20objective%20evaluation%20using%20widely%20used%20metrics%0Ain%20the%20gesture%20generation%20field%20as%20well%20as%20a%20user%20study%20to%20qualitatively%0Aevaluate%20the%20different%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2D%2520or%2520not%25202D%253A%2520How%2520Does%2520the%2520Dimensionality%2520of%2520Gesture%2520Representation%250A%2520%2520Affect%25203D%2520Co-Speech%2520Gesture%2520Generation%253F%26entry.906535625%3DT%25C3%25A9o%2520Guichoux%2520and%2520Laure%2520Soulier%2520and%2520Nicolas%2520Obin%2520and%2520Catherine%2520Pelachaud%26entry.1292438233%3D%2520%2520Co-speech%2520gestures%2520are%2520fundamental%2520for%2520communication.%2520The%2520advent%2520of%2520recent%250Adeep%2520learning%2520techniques%2520has%2520facilitated%2520the%2520creation%2520of%2520lifelike%252C%2520synchronous%250Aco-speech%2520gestures%2520for%2520Embodied%2520Conversational%2520Agents.%2520%2522In-the-wild%2522%2520datasets%252C%250Aaggregating%2520video%2520content%2520from%2520platforms%2520like%2520YouTube%2520via%2520human%2520pose%2520detection%250Atechnologies%252C%2520provide%2520a%2520feasible%2520solution%2520by%2520offering%25202D%2520skeletal%2520sequences%250Aaligned%2520with%2520speech.%2520Concurrent%2520developments%2520in%2520lifting%2520models%2520enable%2520the%250Aconversion%2520of%2520these%25202D%2520sequences%2520into%25203D%2520gesture%2520databases.%2520However%252C%2520it%2520is%250Aimportant%2520to%2520note%2520that%2520the%25203D%2520poses%2520estimated%2520from%2520the%25202D%2520extracted%2520poses%2520are%252C%250Ain%2520essence%252C%2520approximations%2520of%2520the%2520ground-truth%252C%2520which%2520remains%2520in%2520the%25202D%2520domain.%250AThis%2520distinction%2520raises%2520questions%2520about%2520the%2520impact%2520of%2520gesture%2520representation%250Adimensionality%2520on%2520the%2520quality%2520of%2520generated%2520motions%2520-%2520a%2520topic%2520that%252C%2520to%2520our%250Aknowledge%252C%2520remains%2520largely%2520unexplored.%2520Our%2520study%2520examines%2520the%2520effect%2520of%2520using%250Aeither%25202D%2520or%25203D%2520joint%2520coordinates%2520as%2520training%2520data%2520on%2520the%2520performance%2520of%250Aspeech-to-gesture%2520deep%2520generative%2520models.%2520We%2520employ%2520a%2520lifting%2520model%2520for%250Aconverting%2520generated%25202D%2520pose%2520sequences%2520into%25203D%2520and%2520assess%2520how%2520gestures%2520created%250Adirectly%2520in%25203D%2520stack%2520up%2520against%2520those%2520initially%2520generated%2520in%25202D%2520and%2520then%250Aconverted%2520to%25203D.%2520We%2520perform%2520an%2520objective%2520evaluation%2520using%2520widely%2520used%2520metrics%250Ain%2520the%2520gesture%2520generation%2520field%2520as%2520well%2520as%2520a%2520user%2520study%2520to%2520qualitatively%250Aevaluate%2520the%2520different%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2D%20or%20not%202D%3A%20How%20Does%20the%20Dimensionality%20of%20Gesture%20Representation%0A%20%20Affect%203D%20Co-Speech%20Gesture%20Generation%3F&entry.906535625=T%C3%A9o%20Guichoux%20and%20Laure%20Soulier%20and%20Nicolas%20Obin%20and%20Catherine%20Pelachaud&entry.1292438233=%20%20Co-speech%20gestures%20are%20fundamental%20for%20communication.%20The%20advent%20of%20recent%0Adeep%20learning%20techniques%20has%20facilitated%20the%20creation%20of%20lifelike%2C%20synchronous%0Aco-speech%20gestures%20for%20Embodied%20Conversational%20Agents.%20%22In-the-wild%22%20datasets%2C%0Aaggregating%20video%20content%20from%20platforms%20like%20YouTube%20via%20human%20pose%20detection%0Atechnologies%2C%20provide%20a%20feasible%20solution%20by%20offering%202D%20skeletal%20sequences%0Aaligned%20with%20speech.%20Concurrent%20developments%20in%20lifting%20models%20enable%20the%0Aconversion%20of%20these%202D%20sequences%20into%203D%20gesture%20databases.%20However%2C%20it%20is%0Aimportant%20to%20note%20that%20the%203D%20poses%20estimated%20from%20the%202D%20extracted%20poses%20are%2C%0Ain%20essence%2C%20approximations%20of%20the%20ground-truth%2C%20which%20remains%20in%20the%202D%20domain.%0AThis%20distinction%20raises%20questions%20about%20the%20impact%20of%20gesture%20representation%0Adimensionality%20on%20the%20quality%20of%20generated%20motions%20-%20a%20topic%20that%2C%20to%20our%0Aknowledge%2C%20remains%20largely%20unexplored.%20Our%20study%20examines%20the%20effect%20of%20using%0Aeither%202D%20or%203D%20joint%20coordinates%20as%20training%20data%20on%20the%20performance%20of%0Aspeech-to-gesture%20deep%20generative%20models.%20We%20employ%20a%20lifting%20model%20for%0Aconverting%20generated%202D%20pose%20sequences%20into%203D%20and%20assess%20how%20gestures%20created%0Adirectly%20in%203D%20stack%20up%20against%20those%20initially%20generated%20in%202D%20and%20then%0Aconverted%20to%203D.%20We%20perform%20an%20objective%20evaluation%20using%20widely%20used%20metrics%0Ain%20the%20gesture%20generation%20field%20as%20well%20as%20a%20user%20study%20to%20qualitatively%0Aevaluate%20the%20different%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10357v1&entry.124074799=Read"},
{"title": "InfoDisent: Explainability of Image Classification Models by Information\n  Disentanglement", "author": "\u0141ukasz Struski and Jacek Tabor", "abstract": "  Understanding the decisions made by image classification networks is a\ncritical area of research in deep learning. This task is traditionally divided\ninto two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc\nmethods, such as GradCam, aim to interpret the decisions of pre-trained models\nby identifying regions of the image where the network focuses its attention.\nHowever, these methods provide only a high-level overview, making it difficult\nto fully understand the network's decision-making process. Conversely,\nintrinsic methods, like prototypical parts models, offer a more detailed\nunderstanding of network predictions but are constrained by specific\narchitectures, training methods, and datasets.\n  In this paper, we introduce InfoDisent, a hybrid model that combines the\nadvantages of both approaches. By utilizing an information bottleneck,\nInfoDisent disentangles the information in the final layer of a pre-trained\ndeep network, enabling the breakdown of classification decisions into basic,\nunderstandable atomic components. Unlike standard prototypical parts\napproaches, InfoDisent can interpret the decisions of pre-trained\nclassification networks and be used for making classification decisions,\nsimilar to intrinsic models. We validate the effectiveness of InfoDisent on\nbenchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford\nDogs for both convolutional and transformer backbones.\n", "link": "http://arxiv.org/abs/2409.10329v1", "date": "2024-09-16", "relevancy": 2.0909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoDisent%3A%20Explainability%20of%20Image%20Classification%20Models%20by%20Information%0A%20%20Disentanglement&body=Title%3A%20InfoDisent%3A%20Explainability%20of%20Image%20Classification%20Models%20by%20Information%0A%20%20Disentanglement%0AAuthor%3A%20%C5%81ukasz%20Struski%20and%20Jacek%20Tabor%0AAbstract%3A%20%20%20Understanding%20the%20decisions%20made%20by%20image%20classification%20networks%20is%20a%0Acritical%20area%20of%20research%20in%20deep%20learning.%20This%20task%20is%20traditionally%20divided%0Ainto%20two%20distinct%20approaches%3A%20post-hoc%20methods%20and%20intrinsic%20methods.%20Post-hoc%0Amethods%2C%20such%20as%20GradCam%2C%20aim%20to%20interpret%20the%20decisions%20of%20pre-trained%20models%0Aby%20identifying%20regions%20of%20the%20image%20where%20the%20network%20focuses%20its%20attention.%0AHowever%2C%20these%20methods%20provide%20only%20a%20high-level%20overview%2C%20making%20it%20difficult%0Ato%20fully%20understand%20the%20network%27s%20decision-making%20process.%20Conversely%2C%0Aintrinsic%20methods%2C%20like%20prototypical%20parts%20models%2C%20offer%20a%20more%20detailed%0Aunderstanding%20of%20network%20predictions%20but%20are%20constrained%20by%20specific%0Aarchitectures%2C%20training%20methods%2C%20and%20datasets.%0A%20%20In%20this%20paper%2C%20we%20introduce%20InfoDisent%2C%20a%20hybrid%20model%20that%20combines%20the%0Aadvantages%20of%20both%20approaches.%20By%20utilizing%20an%20information%20bottleneck%2C%0AInfoDisent%20disentangles%20the%20information%20in%20the%20final%20layer%20of%20a%20pre-trained%0Adeep%20network%2C%20enabling%20the%20breakdown%20of%20classification%20decisions%20into%20basic%2C%0Aunderstandable%20atomic%20components.%20Unlike%20standard%20prototypical%20parts%0Aapproaches%2C%20InfoDisent%20can%20interpret%20the%20decisions%20of%20pre-trained%0Aclassification%20networks%20and%20be%20used%20for%20making%20classification%20decisions%2C%0Asimilar%20to%20intrinsic%20models.%20We%20validate%20the%20effectiveness%20of%20InfoDisent%20on%0Abenchmark%20datasets%20such%20as%20ImageNet%2C%20CUB-200-2011%2C%20Stanford%20Cars%2C%20and%20Stanford%0ADogs%20for%20both%20convolutional%20and%20transformer%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoDisent%253A%2520Explainability%2520of%2520Image%2520Classification%2520Models%2520by%2520Information%250A%2520%2520Disentanglement%26entry.906535625%3D%25C5%2581ukasz%2520Struski%2520and%2520Jacek%2520Tabor%26entry.1292438233%3D%2520%2520Understanding%2520the%2520decisions%2520made%2520by%2520image%2520classification%2520networks%2520is%2520a%250Acritical%2520area%2520of%2520research%2520in%2520deep%2520learning.%2520This%2520task%2520is%2520traditionally%2520divided%250Ainto%2520two%2520distinct%2520approaches%253A%2520post-hoc%2520methods%2520and%2520intrinsic%2520methods.%2520Post-hoc%250Amethods%252C%2520such%2520as%2520GradCam%252C%2520aim%2520to%2520interpret%2520the%2520decisions%2520of%2520pre-trained%2520models%250Aby%2520identifying%2520regions%2520of%2520the%2520image%2520where%2520the%2520network%2520focuses%2520its%2520attention.%250AHowever%252C%2520these%2520methods%2520provide%2520only%2520a%2520high-level%2520overview%252C%2520making%2520it%2520difficult%250Ato%2520fully%2520understand%2520the%2520network%2527s%2520decision-making%2520process.%2520Conversely%252C%250Aintrinsic%2520methods%252C%2520like%2520prototypical%2520parts%2520models%252C%2520offer%2520a%2520more%2520detailed%250Aunderstanding%2520of%2520network%2520predictions%2520but%2520are%2520constrained%2520by%2520specific%250Aarchitectures%252C%2520training%2520methods%252C%2520and%2520datasets.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520InfoDisent%252C%2520a%2520hybrid%2520model%2520that%2520combines%2520the%250Aadvantages%2520of%2520both%2520approaches.%2520By%2520utilizing%2520an%2520information%2520bottleneck%252C%250AInfoDisent%2520disentangles%2520the%2520information%2520in%2520the%2520final%2520layer%2520of%2520a%2520pre-trained%250Adeep%2520network%252C%2520enabling%2520the%2520breakdown%2520of%2520classification%2520decisions%2520into%2520basic%252C%250Aunderstandable%2520atomic%2520components.%2520Unlike%2520standard%2520prototypical%2520parts%250Aapproaches%252C%2520InfoDisent%2520can%2520interpret%2520the%2520decisions%2520of%2520pre-trained%250Aclassification%2520networks%2520and%2520be%2520used%2520for%2520making%2520classification%2520decisions%252C%250Asimilar%2520to%2520intrinsic%2520models.%2520We%2520validate%2520the%2520effectiveness%2520of%2520InfoDisent%2520on%250Abenchmark%2520datasets%2520such%2520as%2520ImageNet%252C%2520CUB-200-2011%252C%2520Stanford%2520Cars%252C%2520and%2520Stanford%250ADogs%2520for%2520both%2520convolutional%2520and%2520transformer%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoDisent%3A%20Explainability%20of%20Image%20Classification%20Models%20by%20Information%0A%20%20Disentanglement&entry.906535625=%C5%81ukasz%20Struski%20and%20Jacek%20Tabor&entry.1292438233=%20%20Understanding%20the%20decisions%20made%20by%20image%20classification%20networks%20is%20a%0Acritical%20area%20of%20research%20in%20deep%20learning.%20This%20task%20is%20traditionally%20divided%0Ainto%20two%20distinct%20approaches%3A%20post-hoc%20methods%20and%20intrinsic%20methods.%20Post-hoc%0Amethods%2C%20such%20as%20GradCam%2C%20aim%20to%20interpret%20the%20decisions%20of%20pre-trained%20models%0Aby%20identifying%20regions%20of%20the%20image%20where%20the%20network%20focuses%20its%20attention.%0AHowever%2C%20these%20methods%20provide%20only%20a%20high-level%20overview%2C%20making%20it%20difficult%0Ato%20fully%20understand%20the%20network%27s%20decision-making%20process.%20Conversely%2C%0Aintrinsic%20methods%2C%20like%20prototypical%20parts%20models%2C%20offer%20a%20more%20detailed%0Aunderstanding%20of%20network%20predictions%20but%20are%20constrained%20by%20specific%0Aarchitectures%2C%20training%20methods%2C%20and%20datasets.%0A%20%20In%20this%20paper%2C%20we%20introduce%20InfoDisent%2C%20a%20hybrid%20model%20that%20combines%20the%0Aadvantages%20of%20both%20approaches.%20By%20utilizing%20an%20information%20bottleneck%2C%0AInfoDisent%20disentangles%20the%20information%20in%20the%20final%20layer%20of%20a%20pre-trained%0Adeep%20network%2C%20enabling%20the%20breakdown%20of%20classification%20decisions%20into%20basic%2C%0Aunderstandable%20atomic%20components.%20Unlike%20standard%20prototypical%20parts%0Aapproaches%2C%20InfoDisent%20can%20interpret%20the%20decisions%20of%20pre-trained%0Aclassification%20networks%20and%20be%20used%20for%20making%20classification%20decisions%2C%0Asimilar%20to%20intrinsic%20models.%20We%20validate%20the%20effectiveness%20of%20InfoDisent%20on%0Abenchmark%20datasets%20such%20as%20ImageNet%2C%20CUB-200-2011%2C%20Stanford%20Cars%2C%20and%20Stanford%0ADogs%20for%20both%20convolutional%20and%20transformer%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10329v1&entry.124074799=Read"},
{"title": "Enhancing Image Classification in Small and Unbalanced Datasets through\n  Synthetic Data Augmentation", "author": "Neil De La Fuente and Mireia Maj\u00f3 and Irina Luzko and Henry C\u00f3rdova and Gloria Fern\u00e1ndez-Esparrach and Jorge Bernal", "abstract": "  Accurate and robust medical image classification is a challenging task,\nespecially in application domains where available annotated datasets are small\nand present high imbalance between target classes. Considering that data\nacquisition is not always feasible, especially for underrepresented classes,\nour approach introduces a novel synthetic augmentation strategy using\nclass-specific Variational Autoencoders (VAEs) and latent space interpolation\nto improve discrimination capabilities.\n  By generating realistic, varied synthetic data that fills feature space gaps,\nwe address issues of data scarcity and class imbalance. The method presented in\nthis paper relies on the interpolation of latent representations within each\nclass, thus enriching the training set and improving the model's\ngeneralizability and diagnostic accuracy. The proposed strategy was tested in a\nsmall dataset of 321 images created to train and validate an automatic method\nfor assessing the quality of cleanliness of esophagogastroduodenoscopy images.\nBy combining real and synthetic data, an increase of over 18\\% in the accuracy\nof the most challenging underrepresented class was observed. The proposed\nstrategy not only benefited the underrepresented class but also led to a\ngeneral improvement in other metrics, including a 6\\% increase in global\naccuracy and precision.\n", "link": "http://arxiv.org/abs/2409.10286v1", "date": "2024-09-16", "relevancy": 2.0866, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5342}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5146}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Image%20Classification%20in%20Small%20and%20Unbalanced%20Datasets%20through%0A%20%20Synthetic%20Data%20Augmentation&body=Title%3A%20Enhancing%20Image%20Classification%20in%20Small%20and%20Unbalanced%20Datasets%20through%0A%20%20Synthetic%20Data%20Augmentation%0AAuthor%3A%20Neil%20De%20La%20Fuente%20and%20Mireia%20Maj%C3%B3%20and%20Irina%20Luzko%20and%20Henry%20C%C3%B3rdova%20and%20Gloria%20Fern%C3%A1ndez-Esparrach%20and%20Jorge%20Bernal%0AAbstract%3A%20%20%20Accurate%20and%20robust%20medical%20image%20classification%20is%20a%20challenging%20task%2C%0Aespecially%20in%20application%20domains%20where%20available%20annotated%20datasets%20are%20small%0Aand%20present%20high%20imbalance%20between%20target%20classes.%20Considering%20that%20data%0Aacquisition%20is%20not%20always%20feasible%2C%20especially%20for%20underrepresented%20classes%2C%0Aour%20approach%20introduces%20a%20novel%20synthetic%20augmentation%20strategy%20using%0Aclass-specific%20Variational%20Autoencoders%20%28VAEs%29%20and%20latent%20space%20interpolation%0Ato%20improve%20discrimination%20capabilities.%0A%20%20By%20generating%20realistic%2C%20varied%20synthetic%20data%20that%20fills%20feature%20space%20gaps%2C%0Awe%20address%20issues%20of%20data%20scarcity%20and%20class%20imbalance.%20The%20method%20presented%20in%0Athis%20paper%20relies%20on%20the%20interpolation%20of%20latent%20representations%20within%20each%0Aclass%2C%20thus%20enriching%20the%20training%20set%20and%20improving%20the%20model%27s%0Ageneralizability%20and%20diagnostic%20accuracy.%20The%20proposed%20strategy%20was%20tested%20in%20a%0Asmall%20dataset%20of%20321%20images%20created%20to%20train%20and%20validate%20an%20automatic%20method%0Afor%20assessing%20the%20quality%20of%20cleanliness%20of%20esophagogastroduodenoscopy%20images.%0ABy%20combining%20real%20and%20synthetic%20data%2C%20an%20increase%20of%20over%2018%5C%25%20in%20the%20accuracy%0Aof%20the%20most%20challenging%20underrepresented%20class%20was%20observed.%20The%20proposed%0Astrategy%20not%20only%20benefited%20the%20underrepresented%20class%20but%20also%20led%20to%20a%0Ageneral%20improvement%20in%20other%20metrics%2C%20including%20a%206%5C%25%20increase%20in%20global%0Aaccuracy%20and%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Image%2520Classification%2520in%2520Small%2520and%2520Unbalanced%2520Datasets%2520through%250A%2520%2520Synthetic%2520Data%2520Augmentation%26entry.906535625%3DNeil%2520De%2520La%2520Fuente%2520and%2520Mireia%2520Maj%25C3%25B3%2520and%2520Irina%2520Luzko%2520and%2520Henry%2520C%25C3%25B3rdova%2520and%2520Gloria%2520Fern%25C3%25A1ndez-Esparrach%2520and%2520Jorge%2520Bernal%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520medical%2520image%2520classification%2520is%2520a%2520challenging%2520task%252C%250Aespecially%2520in%2520application%2520domains%2520where%2520available%2520annotated%2520datasets%2520are%2520small%250Aand%2520present%2520high%2520imbalance%2520between%2520target%2520classes.%2520Considering%2520that%2520data%250Aacquisition%2520is%2520not%2520always%2520feasible%252C%2520especially%2520for%2520underrepresented%2520classes%252C%250Aour%2520approach%2520introduces%2520a%2520novel%2520synthetic%2520augmentation%2520strategy%2520using%250Aclass-specific%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520and%2520latent%2520space%2520interpolation%250Ato%2520improve%2520discrimination%2520capabilities.%250A%2520%2520By%2520generating%2520realistic%252C%2520varied%2520synthetic%2520data%2520that%2520fills%2520feature%2520space%2520gaps%252C%250Awe%2520address%2520issues%2520of%2520data%2520scarcity%2520and%2520class%2520imbalance.%2520The%2520method%2520presented%2520in%250Athis%2520paper%2520relies%2520on%2520the%2520interpolation%2520of%2520latent%2520representations%2520within%2520each%250Aclass%252C%2520thus%2520enriching%2520the%2520training%2520set%2520and%2520improving%2520the%2520model%2527s%250Ageneralizability%2520and%2520diagnostic%2520accuracy.%2520The%2520proposed%2520strategy%2520was%2520tested%2520in%2520a%250Asmall%2520dataset%2520of%2520321%2520images%2520created%2520to%2520train%2520and%2520validate%2520an%2520automatic%2520method%250Afor%2520assessing%2520the%2520quality%2520of%2520cleanliness%2520of%2520esophagogastroduodenoscopy%2520images.%250ABy%2520combining%2520real%2520and%2520synthetic%2520data%252C%2520an%2520increase%2520of%2520over%252018%255C%2525%2520in%2520the%2520accuracy%250Aof%2520the%2520most%2520challenging%2520underrepresented%2520class%2520was%2520observed.%2520The%2520proposed%250Astrategy%2520not%2520only%2520benefited%2520the%2520underrepresented%2520class%2520but%2520also%2520led%2520to%2520a%250Ageneral%2520improvement%2520in%2520other%2520metrics%252C%2520including%2520a%25206%255C%2525%2520increase%2520in%2520global%250Aaccuracy%2520and%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Image%20Classification%20in%20Small%20and%20Unbalanced%20Datasets%20through%0A%20%20Synthetic%20Data%20Augmentation&entry.906535625=Neil%20De%20La%20Fuente%20and%20Mireia%20Maj%C3%B3%20and%20Irina%20Luzko%20and%20Henry%20C%C3%B3rdova%20and%20Gloria%20Fern%C3%A1ndez-Esparrach%20and%20Jorge%20Bernal&entry.1292438233=%20%20Accurate%20and%20robust%20medical%20image%20classification%20is%20a%20challenging%20task%2C%0Aespecially%20in%20application%20domains%20where%20available%20annotated%20datasets%20are%20small%0Aand%20present%20high%20imbalance%20between%20target%20classes.%20Considering%20that%20data%0Aacquisition%20is%20not%20always%20feasible%2C%20especially%20for%20underrepresented%20classes%2C%0Aour%20approach%20introduces%20a%20novel%20synthetic%20augmentation%20strategy%20using%0Aclass-specific%20Variational%20Autoencoders%20%28VAEs%29%20and%20latent%20space%20interpolation%0Ato%20improve%20discrimination%20capabilities.%0A%20%20By%20generating%20realistic%2C%20varied%20synthetic%20data%20that%20fills%20feature%20space%20gaps%2C%0Awe%20address%20issues%20of%20data%20scarcity%20and%20class%20imbalance.%20The%20method%20presented%20in%0Athis%20paper%20relies%20on%20the%20interpolation%20of%20latent%20representations%20within%20each%0Aclass%2C%20thus%20enriching%20the%20training%20set%20and%20improving%20the%20model%27s%0Ageneralizability%20and%20diagnostic%20accuracy.%20The%20proposed%20strategy%20was%20tested%20in%20a%0Asmall%20dataset%20of%20321%20images%20created%20to%20train%20and%20validate%20an%20automatic%20method%0Afor%20assessing%20the%20quality%20of%20cleanliness%20of%20esophagogastroduodenoscopy%20images.%0ABy%20combining%20real%20and%20synthetic%20data%2C%20an%20increase%20of%20over%2018%5C%25%20in%20the%20accuracy%0Aof%20the%20most%20challenging%20underrepresented%20class%20was%20observed.%20The%20proposed%0Astrategy%20not%20only%20benefited%20the%20underrepresented%20class%20but%20also%20led%20to%20a%0Ageneral%20improvement%20in%20other%20metrics%2C%20including%20a%206%5C%25%20increase%20in%20global%0Aaccuracy%20and%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10286v1&entry.124074799=Read"},
{"title": "Towards Evaluating the Robustness of Visual State Space Models", "author": "Hashmat Shadab Malik and Fahad Shamshad and Muzammal Naseer and Karthik Nandakumar and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Vision State Space Models (VSSMs), a novel architecture that combines the\nstrengths of recurrent neural networks and latent variable models, have\ndemonstrated remarkable performance in visual perception tasks by efficiently\ncapturing long-range dependencies and modeling complex visual dynamics.\nHowever, their robustness under natural and adversarial perturbations remains a\ncritical concern. In this work, we present a comprehensive evaluation of VSSMs'\nrobustness under various perturbation scenarios, including occlusions, image\nstructure, common corruptions, and adversarial attacks, and compare their\nperformance to well-established architectures such as transformers and\nConvolutional Neural Networks. Furthermore, we investigate the resilience of\nVSSMs to object-background compositional changes on sophisticated benchmarks\ndesigned to test model performance in complex visual scenes. We also assess\ntheir robustness on object detection and segmentation tasks using corrupted\ndatasets that mimic real-world scenarios. To gain a deeper understanding of\nVSSMs' adversarial robustness, we conduct a frequency-based analysis of\nadversarial attacks, evaluating their performance against low-frequency and\nhigh-frequency perturbations. Our findings highlight the strengths and\nlimitations of VSSMs in handling complex visual corruptions, offering valuable\ninsights for future research. Our code and models will be available at\nhttps://github.com/HashmatShadab/MambaRobustness.\n", "link": "http://arxiv.org/abs/2406.09407v2", "date": "2024-09-16", "relevancy": 2.0862, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Evaluating%20the%20Robustness%20of%20Visual%20State%20Space%20Models&body=Title%3A%20Towards%20Evaluating%20the%20Robustness%20of%20Visual%20State%20Space%20Models%0AAuthor%3A%20Hashmat%20Shadab%20Malik%20and%20Fahad%20Shamshad%20and%20Muzammal%20Naseer%20and%20Karthik%20Nandakumar%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Vision%20State%20Space%20Models%20%28VSSMs%29%2C%20a%20novel%20architecture%20that%20combines%20the%0Astrengths%20of%20recurrent%20neural%20networks%20and%20latent%20variable%20models%2C%20have%0Ademonstrated%20remarkable%20performance%20in%20visual%20perception%20tasks%20by%20efficiently%0Acapturing%20long-range%20dependencies%20and%20modeling%20complex%20visual%20dynamics.%0AHowever%2C%20their%20robustness%20under%20natural%20and%20adversarial%20perturbations%20remains%20a%0Acritical%20concern.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20VSSMs%27%0Arobustness%20under%20various%20perturbation%20scenarios%2C%20including%20occlusions%2C%20image%0Astructure%2C%20common%20corruptions%2C%20and%20adversarial%20attacks%2C%20and%20compare%20their%0Aperformance%20to%20well-established%20architectures%20such%20as%20transformers%20and%0AConvolutional%20Neural%20Networks.%20Furthermore%2C%20we%20investigate%20the%20resilience%20of%0AVSSMs%20to%20object-background%20compositional%20changes%20on%20sophisticated%20benchmarks%0Adesigned%20to%20test%20model%20performance%20in%20complex%20visual%20scenes.%20We%20also%20assess%0Atheir%20robustness%20on%20object%20detection%20and%20segmentation%20tasks%20using%20corrupted%0Adatasets%20that%20mimic%20real-world%20scenarios.%20To%20gain%20a%20deeper%20understanding%20of%0AVSSMs%27%20adversarial%20robustness%2C%20we%20conduct%20a%20frequency-based%20analysis%20of%0Aadversarial%20attacks%2C%20evaluating%20their%20performance%20against%20low-frequency%20and%0Ahigh-frequency%20perturbations.%20Our%20findings%20highlight%20the%20strengths%20and%0Alimitations%20of%20VSSMs%20in%20handling%20complex%20visual%20corruptions%2C%20offering%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/HashmatShadab/MambaRobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Evaluating%2520the%2520Robustness%2520of%2520Visual%2520State%2520Space%2520Models%26entry.906535625%3DHashmat%2520Shadab%2520Malik%2520and%2520Fahad%2520Shamshad%2520and%2520Muzammal%2520Naseer%2520and%2520Karthik%2520Nandakumar%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Vision%2520State%2520Space%2520Models%2520%2528VSSMs%2529%252C%2520a%2520novel%2520architecture%2520that%2520combines%2520the%250Astrengths%2520of%2520recurrent%2520neural%2520networks%2520and%2520latent%2520variable%2520models%252C%2520have%250Ademonstrated%2520remarkable%2520performance%2520in%2520visual%2520perception%2520tasks%2520by%2520efficiently%250Acapturing%2520long-range%2520dependencies%2520and%2520modeling%2520complex%2520visual%2520dynamics.%250AHowever%252C%2520their%2520robustness%2520under%2520natural%2520and%2520adversarial%2520perturbations%2520remains%2520a%250Acritical%2520concern.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%2520evaluation%2520of%2520VSSMs%2527%250Arobustness%2520under%2520various%2520perturbation%2520scenarios%252C%2520including%2520occlusions%252C%2520image%250Astructure%252C%2520common%2520corruptions%252C%2520and%2520adversarial%2520attacks%252C%2520and%2520compare%2520their%250Aperformance%2520to%2520well-established%2520architectures%2520such%2520as%2520transformers%2520and%250AConvolutional%2520Neural%2520Networks.%2520Furthermore%252C%2520we%2520investigate%2520the%2520resilience%2520of%250AVSSMs%2520to%2520object-background%2520compositional%2520changes%2520on%2520sophisticated%2520benchmarks%250Adesigned%2520to%2520test%2520model%2520performance%2520in%2520complex%2520visual%2520scenes.%2520We%2520also%2520assess%250Atheir%2520robustness%2520on%2520object%2520detection%2520and%2520segmentation%2520tasks%2520using%2520corrupted%250Adatasets%2520that%2520mimic%2520real-world%2520scenarios.%2520To%2520gain%2520a%2520deeper%2520understanding%2520of%250AVSSMs%2527%2520adversarial%2520robustness%252C%2520we%2520conduct%2520a%2520frequency-based%2520analysis%2520of%250Aadversarial%2520attacks%252C%2520evaluating%2520their%2520performance%2520against%2520low-frequency%2520and%250Ahigh-frequency%2520perturbations.%2520Our%2520findings%2520highlight%2520the%2520strengths%2520and%250Alimitations%2520of%2520VSSMs%2520in%2520handling%2520complex%2520visual%2520corruptions%252C%2520offering%2520valuable%250Ainsights%2520for%2520future%2520research.%2520Our%2520code%2520and%2520models%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/HashmatShadab/MambaRobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Evaluating%20the%20Robustness%20of%20Visual%20State%20Space%20Models&entry.906535625=Hashmat%20Shadab%20Malik%20and%20Fahad%20Shamshad%20and%20Muzammal%20Naseer%20and%20Karthik%20Nandakumar%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Vision%20State%20Space%20Models%20%28VSSMs%29%2C%20a%20novel%20architecture%20that%20combines%20the%0Astrengths%20of%20recurrent%20neural%20networks%20and%20latent%20variable%20models%2C%20have%0Ademonstrated%20remarkable%20performance%20in%20visual%20perception%20tasks%20by%20efficiently%0Acapturing%20long-range%20dependencies%20and%20modeling%20complex%20visual%20dynamics.%0AHowever%2C%20their%20robustness%20under%20natural%20and%20adversarial%20perturbations%20remains%20a%0Acritical%20concern.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20VSSMs%27%0Arobustness%20under%20various%20perturbation%20scenarios%2C%20including%20occlusions%2C%20image%0Astructure%2C%20common%20corruptions%2C%20and%20adversarial%20attacks%2C%20and%20compare%20their%0Aperformance%20to%20well-established%20architectures%20such%20as%20transformers%20and%0AConvolutional%20Neural%20Networks.%20Furthermore%2C%20we%20investigate%20the%20resilience%20of%0AVSSMs%20to%20object-background%20compositional%20changes%20on%20sophisticated%20benchmarks%0Adesigned%20to%20test%20model%20performance%20in%20complex%20visual%20scenes.%20We%20also%20assess%0Atheir%20robustness%20on%20object%20detection%20and%20segmentation%20tasks%20using%20corrupted%0Adatasets%20that%20mimic%20real-world%20scenarios.%20To%20gain%20a%20deeper%20understanding%20of%0AVSSMs%27%20adversarial%20robustness%2C%20we%20conduct%20a%20frequency-based%20analysis%20of%0Aadversarial%20attacks%2C%20evaluating%20their%20performance%20against%20low-frequency%20and%0Ahigh-frequency%20perturbations.%20Our%20findings%20highlight%20the%20strengths%20and%0Alimitations%20of%20VSSMs%20in%20handling%20complex%20visual%20corruptions%2C%20offering%20valuable%0Ainsights%20for%20future%20research.%20Our%20code%20and%20models%20will%20be%20available%20at%0Ahttps%3A//github.com/HashmatShadab/MambaRobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09407v2&entry.124074799=Read"},
{"title": "Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models", "author": "Weihao Ye and Qiong Wu and Wenhao Lin and Yiyi Zhou", "abstract": "  Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.\n", "link": "http://arxiv.org/abs/2409.10197v1", "date": "2024-09-16", "relevancy": 2.0841, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fit%20and%20Prune%3A%20Fast%20and%20Training-free%20Visual%20Token%20Pruning%20for%0A%20%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20Fit%20and%20Prune%3A%20Fast%20and%20Training-free%20Visual%20Token%20Pruning%20for%0A%20%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Weihao%20Ye%20and%20Qiong%20Wu%20and%20Wenhao%20Lin%20and%20Yiyi%20Zhou%0AAbstract%3A%20%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%28MLLMs%29%20often%20use%20large%0Aimage%20tokens%20to%20compensate%20the%20visual%20shortcoming%20of%20MLLMs%2C%20which%20not%20only%0Aexhibits%20obvious%20redundancy%20but%20also%20greatly%20exacerbates%20the%20already%20high%0Acomputation.%20Token%20pruning%20is%20an%20effective%20solution%20for%20speeding%20up%20MLLMs%2C%20but%0Awhen%20and%20how%20to%20drop%20tokens%20still%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20training-free%20approach%20for%20the%20effective%20visual%20token%0Apruning%20of%20MLLMs%2C%20termed%20FitPrune%2C%20which%20can%20quickly%20produce%20a%20complete%20pruning%0Arecipe%20for%20MLLMs%20according%20to%20a%20pre-defined%20budget.%20Specifically%2C%20FitPrune%0Aconsiders%20token%20pruning%20as%20a%20statistical%20problem%20of%20MLLM%20and%20its%20objective%20is%0Ato%20find%20out%20an%20optimal%20pruning%20scheme%20that%20can%20minimize%20the%20divergence%20of%20the%0Aattention%20distributions%20before%20and%20after%20pruning.%20In%20practice%2C%20FitPrune%20can%20be%0Aquickly%20accomplished%20based%20on%20the%20attention%20statistics%20from%20a%20small%20batch%20of%0Ainference%20data%2C%20avoiding%20the%20expensive%20trials%20of%20MLLMs.%20According%20to%20the%0Apruning%20recipe%2C%20an%20MLLM%20can%20directly%20remove%20the%20redundant%20visual%20tokens%20of%0Adifferent%20examples%20during%20inference.%20To%20validate%20FitPrune%2C%20we%20apply%20it%20to%20a%20set%0Aof%20recent%20MLLMs%2C%20including%20LLaVA-1.5%2C%20LLaVA-HR%20and%20LLaVA-NEXT%2C%20and%20conduct%0Aextensive%20experiments%20on%20a%20set%20of%20benchmarks.%20The%20experimental%20results%20show%0Athat%20our%20FitPrune%20can%20not%20only%20reduce%20the%20computational%20complexity%20to%20a%20large%0Aextent%2C%20while%20retaining%20high%20performance%2C%20e.g.%2C%20-54.9%25%20FLOPs%20for%20LLaVA-NEXT%0Awith%20only%200.5%25%20accuracy%20drop.%20Notably%2C%20the%20pruning%20recipe%20can%20be%20obtained%20in%0Aabout%205%20minutes.%20Our%20code%20is%20available%20at%20https%3A//github.com/ywh187/FitPrune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFit%2520and%2520Prune%253A%2520Fast%2520and%2520Training-free%2520Visual%2520Token%2520Pruning%2520for%250A%2520%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DWeihao%2520Ye%2520and%2520Qiong%2520Wu%2520and%2520Wenhao%2520Lin%2520and%2520Yiyi%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2528MLLMs%2529%2520often%2520use%2520large%250Aimage%2520tokens%2520to%2520compensate%2520the%2520visual%2520shortcoming%2520of%2520MLLMs%252C%2520which%2520not%2520only%250Aexhibits%2520obvious%2520redundancy%2520but%2520also%2520greatly%2520exacerbates%2520the%2520already%2520high%250Acomputation.%2520Token%2520pruning%2520is%2520an%2520effective%2520solution%2520for%2520speeding%2520up%2520MLLMs%252C%2520but%250Awhen%2520and%2520how%2520to%2520drop%2520tokens%2520still%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520and%2520training-free%2520approach%2520for%2520the%2520effective%2520visual%2520token%250Apruning%2520of%2520MLLMs%252C%2520termed%2520FitPrune%252C%2520which%2520can%2520quickly%2520produce%2520a%2520complete%2520pruning%250Arecipe%2520for%2520MLLMs%2520according%2520to%2520a%2520pre-defined%2520budget.%2520Specifically%252C%2520FitPrune%250Aconsiders%2520token%2520pruning%2520as%2520a%2520statistical%2520problem%2520of%2520MLLM%2520and%2520its%2520objective%2520is%250Ato%2520find%2520out%2520an%2520optimal%2520pruning%2520scheme%2520that%2520can%2520minimize%2520the%2520divergence%2520of%2520the%250Aattention%2520distributions%2520before%2520and%2520after%2520pruning.%2520In%2520practice%252C%2520FitPrune%2520can%2520be%250Aquickly%2520accomplished%2520based%2520on%2520the%2520attention%2520statistics%2520from%2520a%2520small%2520batch%2520of%250Ainference%2520data%252C%2520avoiding%2520the%2520expensive%2520trials%2520of%2520MLLMs.%2520According%2520to%2520the%250Apruning%2520recipe%252C%2520an%2520MLLM%2520can%2520directly%2520remove%2520the%2520redundant%2520visual%2520tokens%2520of%250Adifferent%2520examples%2520during%2520inference.%2520To%2520validate%2520FitPrune%252C%2520we%2520apply%2520it%2520to%2520a%2520set%250Aof%2520recent%2520MLLMs%252C%2520including%2520LLaVA-1.5%252C%2520LLaVA-HR%2520and%2520LLaVA-NEXT%252C%2520and%2520conduct%250Aextensive%2520experiments%2520on%2520a%2520set%2520of%2520benchmarks.%2520The%2520experimental%2520results%2520show%250Athat%2520our%2520FitPrune%2520can%2520not%2520only%2520reduce%2520the%2520computational%2520complexity%2520to%2520a%2520large%250Aextent%252C%2520while%2520retaining%2520high%2520performance%252C%2520e.g.%252C%2520-54.9%2525%2520FLOPs%2520for%2520LLaVA-NEXT%250Awith%2520only%25200.5%2525%2520accuracy%2520drop.%2520Notably%252C%2520the%2520pruning%2520recipe%2520can%2520be%2520obtained%2520in%250Aabout%25205%2520minutes.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ywh187/FitPrune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fit%20and%20Prune%3A%20Fast%20and%20Training-free%20Visual%20Token%20Pruning%20for%0A%20%20Multi-modal%20Large%20Language%20Models&entry.906535625=Weihao%20Ye%20and%20Qiong%20Wu%20and%20Wenhao%20Lin%20and%20Yiyi%20Zhou&entry.1292438233=%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%28MLLMs%29%20often%20use%20large%0Aimage%20tokens%20to%20compensate%20the%20visual%20shortcoming%20of%20MLLMs%2C%20which%20not%20only%0Aexhibits%20obvious%20redundancy%20but%20also%20greatly%20exacerbates%20the%20already%20high%0Acomputation.%20Token%20pruning%20is%20an%20effective%20solution%20for%20speeding%20up%20MLLMs%2C%20but%0Awhen%20and%20how%20to%20drop%20tokens%20still%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20training-free%20approach%20for%20the%20effective%20visual%20token%0Apruning%20of%20MLLMs%2C%20termed%20FitPrune%2C%20which%20can%20quickly%20produce%20a%20complete%20pruning%0Arecipe%20for%20MLLMs%20according%20to%20a%20pre-defined%20budget.%20Specifically%2C%20FitPrune%0Aconsiders%20token%20pruning%20as%20a%20statistical%20problem%20of%20MLLM%20and%20its%20objective%20is%0Ato%20find%20out%20an%20optimal%20pruning%20scheme%20that%20can%20minimize%20the%20divergence%20of%20the%0Aattention%20distributions%20before%20and%20after%20pruning.%20In%20practice%2C%20FitPrune%20can%20be%0Aquickly%20accomplished%20based%20on%20the%20attention%20statistics%20from%20a%20small%20batch%20of%0Ainference%20data%2C%20avoiding%20the%20expensive%20trials%20of%20MLLMs.%20According%20to%20the%0Apruning%20recipe%2C%20an%20MLLM%20can%20directly%20remove%20the%20redundant%20visual%20tokens%20of%0Adifferent%20examples%20during%20inference.%20To%20validate%20FitPrune%2C%20we%20apply%20it%20to%20a%20set%0Aof%20recent%20MLLMs%2C%20including%20LLaVA-1.5%2C%20LLaVA-HR%20and%20LLaVA-NEXT%2C%20and%20conduct%0Aextensive%20experiments%20on%20a%20set%20of%20benchmarks.%20The%20experimental%20results%20show%0Athat%20our%20FitPrune%20can%20not%20only%20reduce%20the%20computational%20complexity%20to%20a%20large%0Aextent%2C%20while%20retaining%20high%20performance%2C%20e.g.%2C%20-54.9%25%20FLOPs%20for%20LLaVA-NEXT%0Awith%20only%200.5%25%20accuracy%20drop.%20Notably%2C%20the%20pruning%20recipe%20can%20be%20obtained%20in%0Aabout%205%20minutes.%20Our%20code%20is%20available%20at%20https%3A//github.com/ywh187/FitPrune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10197v1&entry.124074799=Read"},
{"title": "CLIPCleaner: Cleaning Noisy Labels with CLIP", "author": "Chen Feng and Georgios Tzimiropoulos and Ioannis Patras", "abstract": "  Learning with Noisy labels (LNL) poses a significant challenge for the\nMachine Learning community. Some of the most widely used approaches that select\nas clean samples for which the model itself (the in-training model) has high\nconfidence, e.g., `small loss', can suffer from the so called\n`self-confirmation' bias. This bias arises because the in-training model, is at\nleast partially trained on the noisy labels. Furthermore, in the classification\ncase, an additional challenge arises because some of the label noise is between\nclasses that are visually very similar (`hard noise'). This paper addresses\nthese challenges by proposing a method (\\textit{CLIPCleaner}) that leverages\nCLIP, a powerful Vision-Language (VL) model for constructing a zero-shot\nclassifier for efficient, offline, clean sample selection. This has the\nadvantage that the sample selection is decoupled from the in-training model and\nthat the sample selection is aware of the semantic and visual similarities\nbetween the classes due to the way that CLIP is trained. We provide theoretical\njustifications and empirical evidence to demonstrate the advantages of CLIP for\nLNL compared to conventional pre-trained models. Compared to current methods\nthat combine iterative sample selection with various techniques,\n\\textit{CLIPCleaner} offers a simple, single-step approach that achieves\ncompetitive or superior performance on benchmark datasets. To the best of our\nknowledge, this is the first time a VL model has been used for sample selection\nto address the problem of Learning with Noisy Labels (LNL), highlighting their\npotential in the domain.\n", "link": "http://arxiv.org/abs/2408.10012v2", "date": "2024-09-16", "relevancy": 2.0836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5154}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP&body=Title%3A%20CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP%0AAuthor%3A%20Chen%20Feng%20and%20Georgios%20Tzimiropoulos%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20Learning%20with%20Noisy%20labels%20%28LNL%29%20poses%20a%20significant%20challenge%20for%20the%0AMachine%20Learning%20community.%20Some%20of%20the%20most%20widely%20used%20approaches%20that%20select%0Aas%20clean%20samples%20for%20which%20the%20model%20itself%20%28the%20in-training%20model%29%20has%20high%0Aconfidence%2C%20e.g.%2C%20%60small%20loss%27%2C%20can%20suffer%20from%20the%20so%20called%0A%60self-confirmation%27%20bias.%20This%20bias%20arises%20because%20the%20in-training%20model%2C%20is%20at%0Aleast%20partially%20trained%20on%20the%20noisy%20labels.%20Furthermore%2C%20in%20the%20classification%0Acase%2C%20an%20additional%20challenge%20arises%20because%20some%20of%20the%20label%20noise%20is%20between%0Aclasses%20that%20are%20visually%20very%20similar%20%28%60hard%20noise%27%29.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20a%20method%20%28%5Ctextit%7BCLIPCleaner%7D%29%20that%20leverages%0ACLIP%2C%20a%20powerful%20Vision-Language%20%28VL%29%20model%20for%20constructing%20a%20zero-shot%0Aclassifier%20for%20efficient%2C%20offline%2C%20clean%20sample%20selection.%20This%20has%20the%0Aadvantage%20that%20the%20sample%20selection%20is%20decoupled%20from%20the%20in-training%20model%20and%0Athat%20the%20sample%20selection%20is%20aware%20of%20the%20semantic%20and%20visual%20similarities%0Abetween%20the%20classes%20due%20to%20the%20way%20that%20CLIP%20is%20trained.%20We%20provide%20theoretical%0Ajustifications%20and%20empirical%20evidence%20to%20demonstrate%20the%20advantages%20of%20CLIP%20for%0ALNL%20compared%20to%20conventional%20pre-trained%20models.%20Compared%20to%20current%20methods%0Athat%20combine%20iterative%20sample%20selection%20with%20various%20techniques%2C%0A%5Ctextit%7BCLIPCleaner%7D%20offers%20a%20simple%2C%20single-step%20approach%20that%20achieves%0Acompetitive%20or%20superior%20performance%20on%20benchmark%20datasets.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20time%20a%20VL%20model%20has%20been%20used%20for%20sample%20selection%0Ato%20address%20the%20problem%20of%20Learning%20with%20Noisy%20Labels%20%28LNL%29%2C%20highlighting%20their%0Apotential%20in%20the%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPCleaner%253A%2520Cleaning%2520Noisy%2520Labels%2520with%2520CLIP%26entry.906535625%3DChen%2520Feng%2520and%2520Georgios%2520Tzimiropoulos%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520Learning%2520with%2520Noisy%2520labels%2520%2528LNL%2529%2520poses%2520a%2520significant%2520challenge%2520for%2520the%250AMachine%2520Learning%2520community.%2520Some%2520of%2520the%2520most%2520widely%2520used%2520approaches%2520that%2520select%250Aas%2520clean%2520samples%2520for%2520which%2520the%2520model%2520itself%2520%2528the%2520in-training%2520model%2529%2520has%2520high%250Aconfidence%252C%2520e.g.%252C%2520%2560small%2520loss%2527%252C%2520can%2520suffer%2520from%2520the%2520so%2520called%250A%2560self-confirmation%2527%2520bias.%2520This%2520bias%2520arises%2520because%2520the%2520in-training%2520model%252C%2520is%2520at%250Aleast%2520partially%2520trained%2520on%2520the%2520noisy%2520labels.%2520Furthermore%252C%2520in%2520the%2520classification%250Acase%252C%2520an%2520additional%2520challenge%2520arises%2520because%2520some%2520of%2520the%2520label%2520noise%2520is%2520between%250Aclasses%2520that%2520are%2520visually%2520very%2520similar%2520%2528%2560hard%2520noise%2527%2529.%2520This%2520paper%2520addresses%250Athese%2520challenges%2520by%2520proposing%2520a%2520method%2520%2528%255Ctextit%257BCLIPCleaner%257D%2529%2520that%2520leverages%250ACLIP%252C%2520a%2520powerful%2520Vision-Language%2520%2528VL%2529%2520model%2520for%2520constructing%2520a%2520zero-shot%250Aclassifier%2520for%2520efficient%252C%2520offline%252C%2520clean%2520sample%2520selection.%2520This%2520has%2520the%250Aadvantage%2520that%2520the%2520sample%2520selection%2520is%2520decoupled%2520from%2520the%2520in-training%2520model%2520and%250Athat%2520the%2520sample%2520selection%2520is%2520aware%2520of%2520the%2520semantic%2520and%2520visual%2520similarities%250Abetween%2520the%2520classes%2520due%2520to%2520the%2520way%2520that%2520CLIP%2520is%2520trained.%2520We%2520provide%2520theoretical%250Ajustifications%2520and%2520empirical%2520evidence%2520to%2520demonstrate%2520the%2520advantages%2520of%2520CLIP%2520for%250ALNL%2520compared%2520to%2520conventional%2520pre-trained%2520models.%2520Compared%2520to%2520current%2520methods%250Athat%2520combine%2520iterative%2520sample%2520selection%2520with%2520various%2520techniques%252C%250A%255Ctextit%257BCLIPCleaner%257D%2520offers%2520a%2520simple%252C%2520single-step%2520approach%2520that%2520achieves%250Acompetitive%2520or%2520superior%2520performance%2520on%2520benchmark%2520datasets.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520time%2520a%2520VL%2520model%2520has%2520been%2520used%2520for%2520sample%2520selection%250Ato%2520address%2520the%2520problem%2520of%2520Learning%2520with%2520Noisy%2520Labels%2520%2528LNL%2529%252C%2520highlighting%2520their%250Apotential%2520in%2520the%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPCleaner%3A%20Cleaning%20Noisy%20Labels%20with%20CLIP&entry.906535625=Chen%20Feng%20and%20Georgios%20Tzimiropoulos%20and%20Ioannis%20Patras&entry.1292438233=%20%20Learning%20with%20Noisy%20labels%20%28LNL%29%20poses%20a%20significant%20challenge%20for%20the%0AMachine%20Learning%20community.%20Some%20of%20the%20most%20widely%20used%20approaches%20that%20select%0Aas%20clean%20samples%20for%20which%20the%20model%20itself%20%28the%20in-training%20model%29%20has%20high%0Aconfidence%2C%20e.g.%2C%20%60small%20loss%27%2C%20can%20suffer%20from%20the%20so%20called%0A%60self-confirmation%27%20bias.%20This%20bias%20arises%20because%20the%20in-training%20model%2C%20is%20at%0Aleast%20partially%20trained%20on%20the%20noisy%20labels.%20Furthermore%2C%20in%20the%20classification%0Acase%2C%20an%20additional%20challenge%20arises%20because%20some%20of%20the%20label%20noise%20is%20between%0Aclasses%20that%20are%20visually%20very%20similar%20%28%60hard%20noise%27%29.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20a%20method%20%28%5Ctextit%7BCLIPCleaner%7D%29%20that%20leverages%0ACLIP%2C%20a%20powerful%20Vision-Language%20%28VL%29%20model%20for%20constructing%20a%20zero-shot%0Aclassifier%20for%20efficient%2C%20offline%2C%20clean%20sample%20selection.%20This%20has%20the%0Aadvantage%20that%20the%20sample%20selection%20is%20decoupled%20from%20the%20in-training%20model%20and%0Athat%20the%20sample%20selection%20is%20aware%20of%20the%20semantic%20and%20visual%20similarities%0Abetween%20the%20classes%20due%20to%20the%20way%20that%20CLIP%20is%20trained.%20We%20provide%20theoretical%0Ajustifications%20and%20empirical%20evidence%20to%20demonstrate%20the%20advantages%20of%20CLIP%20for%0ALNL%20compared%20to%20conventional%20pre-trained%20models.%20Compared%20to%20current%20methods%0Athat%20combine%20iterative%20sample%20selection%20with%20various%20techniques%2C%0A%5Ctextit%7BCLIPCleaner%7D%20offers%20a%20simple%2C%20single-step%20approach%20that%20achieves%0Acompetitive%20or%20superior%20performance%20on%20benchmark%20datasets.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20time%20a%20VL%20model%20has%20been%20used%20for%20sample%20selection%0Ato%20address%20the%20problem%20of%20Learning%20with%20Noisy%20Labels%20%28LNL%29%2C%20highlighting%20their%0Apotential%20in%20the%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10012v2&entry.124074799=Read"},
{"title": "Hedging Is Not All You Need: A Simple Baseline for Online Learning Under\n  Haphazard Inputs", "author": "Himanshu Buckchash and Momojit Biswas and Rohit Agarwal and Dilip K. Prasad", "abstract": "  Handling haphazard streaming data, such as data from edge devices, presents a\nchallenging problem. Over time, the incoming data becomes inconsistent, with\nmissing, faulty, or new inputs reappearing. Therefore, it requires models that\nare reliable. Recent methods to solve this problem depend on a hedging-based\nsolution and require specialized elements like auxiliary dropouts, forked\narchitectures, and intricate network design. We observed that hedging can be\nreduced to a special case of weighted residual connection; this motivated us to\napproximate it with plain self-attention. In this work, we propose HapNet, a\nsimple baseline that is scalable, does not require online backpropagation, and\nis adaptable to varying input types. All present methods are restricted to\nscaling with a fixed window; however, we introduce a more complex problem of\nscaling with a variable window where the data becomes positionally\nuncorrelated, and cannot be addressed by present methods. We demonstrate that a\nvariant of the proposed approach can work even for this complex scenario. We\nextensively evaluated the proposed approach on five benchmarks and found\ncompetitive performance.\n", "link": "http://arxiv.org/abs/2409.10242v1", "date": "2024-09-16", "relevancy": 2.0784, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs&body=Title%3A%20Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs%0AAuthor%3A%20Himanshu%20Buckchash%20and%20Momojit%20Biswas%20and%20Rohit%20Agarwal%20and%20Dilip%20K.%20Prasad%0AAbstract%3A%20%20%20Handling%20haphazard%20streaming%20data%2C%20such%20as%20data%20from%20edge%20devices%2C%20presents%20a%0Achallenging%20problem.%20Over%20time%2C%20the%20incoming%20data%20becomes%20inconsistent%2C%20with%0Amissing%2C%20faulty%2C%20or%20new%20inputs%20reappearing.%20Therefore%2C%20it%20requires%20models%20that%0Aare%20reliable.%20Recent%20methods%20to%20solve%20this%20problem%20depend%20on%20a%20hedging-based%0Asolution%20and%20require%20specialized%20elements%20like%20auxiliary%20dropouts%2C%20forked%0Aarchitectures%2C%20and%20intricate%20network%20design.%20We%20observed%20that%20hedging%20can%20be%0Areduced%20to%20a%20special%20case%20of%20weighted%20residual%20connection%3B%20this%20motivated%20us%20to%0Aapproximate%20it%20with%20plain%20self-attention.%20In%20this%20work%2C%20we%20propose%20HapNet%2C%20a%0Asimple%20baseline%20that%20is%20scalable%2C%20does%20not%20require%20online%20backpropagation%2C%20and%0Ais%20adaptable%20to%20varying%20input%20types.%20All%20present%20methods%20are%20restricted%20to%0Ascaling%20with%20a%20fixed%20window%3B%20however%2C%20we%20introduce%20a%20more%20complex%20problem%20of%0Ascaling%20with%20a%20variable%20window%20where%20the%20data%20becomes%20positionally%0Auncorrelated%2C%20and%20cannot%20be%20addressed%20by%20present%20methods.%20We%20demonstrate%20that%20a%0Avariant%20of%20the%20proposed%20approach%20can%20work%20even%20for%20this%20complex%20scenario.%20We%0Aextensively%20evaluated%20the%20proposed%20approach%20on%20five%20benchmarks%20and%20found%0Acompetitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHedging%2520Is%2520Not%2520All%2520You%2520Need%253A%2520A%2520Simple%2520Baseline%2520for%2520Online%2520Learning%2520Under%250A%2520%2520Haphazard%2520Inputs%26entry.906535625%3DHimanshu%2520Buckchash%2520and%2520Momojit%2520Biswas%2520and%2520Rohit%2520Agarwal%2520and%2520Dilip%2520K.%2520Prasad%26entry.1292438233%3D%2520%2520Handling%2520haphazard%2520streaming%2520data%252C%2520such%2520as%2520data%2520from%2520edge%2520devices%252C%2520presents%2520a%250Achallenging%2520problem.%2520Over%2520time%252C%2520the%2520incoming%2520data%2520becomes%2520inconsistent%252C%2520with%250Amissing%252C%2520faulty%252C%2520or%2520new%2520inputs%2520reappearing.%2520Therefore%252C%2520it%2520requires%2520models%2520that%250Aare%2520reliable.%2520Recent%2520methods%2520to%2520solve%2520this%2520problem%2520depend%2520on%2520a%2520hedging-based%250Asolution%2520and%2520require%2520specialized%2520elements%2520like%2520auxiliary%2520dropouts%252C%2520forked%250Aarchitectures%252C%2520and%2520intricate%2520network%2520design.%2520We%2520observed%2520that%2520hedging%2520can%2520be%250Areduced%2520to%2520a%2520special%2520case%2520of%2520weighted%2520residual%2520connection%253B%2520this%2520motivated%2520us%2520to%250Aapproximate%2520it%2520with%2520plain%2520self-attention.%2520In%2520this%2520work%252C%2520we%2520propose%2520HapNet%252C%2520a%250Asimple%2520baseline%2520that%2520is%2520scalable%252C%2520does%2520not%2520require%2520online%2520backpropagation%252C%2520and%250Ais%2520adaptable%2520to%2520varying%2520input%2520types.%2520All%2520present%2520methods%2520are%2520restricted%2520to%250Ascaling%2520with%2520a%2520fixed%2520window%253B%2520however%252C%2520we%2520introduce%2520a%2520more%2520complex%2520problem%2520of%250Ascaling%2520with%2520a%2520variable%2520window%2520where%2520the%2520data%2520becomes%2520positionally%250Auncorrelated%252C%2520and%2520cannot%2520be%2520addressed%2520by%2520present%2520methods.%2520We%2520demonstrate%2520that%2520a%250Avariant%2520of%2520the%2520proposed%2520approach%2520can%2520work%2520even%2520for%2520this%2520complex%2520scenario.%2520We%250Aextensively%2520evaluated%2520the%2520proposed%2520approach%2520on%2520five%2520benchmarks%2520and%2520found%250Acompetitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs&entry.906535625=Himanshu%20Buckchash%20and%20Momojit%20Biswas%20and%20Rohit%20Agarwal%20and%20Dilip%20K.%20Prasad&entry.1292438233=%20%20Handling%20haphazard%20streaming%20data%2C%20such%20as%20data%20from%20edge%20devices%2C%20presents%20a%0Achallenging%20problem.%20Over%20time%2C%20the%20incoming%20data%20becomes%20inconsistent%2C%20with%0Amissing%2C%20faulty%2C%20or%20new%20inputs%20reappearing.%20Therefore%2C%20it%20requires%20models%20that%0Aare%20reliable.%20Recent%20methods%20to%20solve%20this%20problem%20depend%20on%20a%20hedging-based%0Asolution%20and%20require%20specialized%20elements%20like%20auxiliary%20dropouts%2C%20forked%0Aarchitectures%2C%20and%20intricate%20network%20design.%20We%20observed%20that%20hedging%20can%20be%0Areduced%20to%20a%20special%20case%20of%20weighted%20residual%20connection%3B%20this%20motivated%20us%20to%0Aapproximate%20it%20with%20plain%20self-attention.%20In%20this%20work%2C%20we%20propose%20HapNet%2C%20a%0Asimple%20baseline%20that%20is%20scalable%2C%20does%20not%20require%20online%20backpropagation%2C%20and%0Ais%20adaptable%20to%20varying%20input%20types.%20All%20present%20methods%20are%20restricted%20to%0Ascaling%20with%20a%20fixed%20window%3B%20however%2C%20we%20introduce%20a%20more%20complex%20problem%20of%0Ascaling%20with%20a%20variable%20window%20where%20the%20data%20becomes%20positionally%0Auncorrelated%2C%20and%20cannot%20be%20addressed%20by%20present%20methods.%20We%20demonstrate%20that%20a%0Avariant%20of%20the%20proposed%20approach%20can%20work%20even%20for%20this%20complex%20scenario.%20We%0Aextensively%20evaluated%20the%20proposed%20approach%20on%20five%20benchmarks%20and%20found%0Acompetitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10242v1&entry.124074799=Read"},
{"title": "The 20 questions game to distinguish large language models", "author": "Gurvan Richardeau and Erwan Le Merrer and Camilla Penzo and Gilles Tredan", "abstract": "  In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.\n", "link": "http://arxiv.org/abs/2409.10338v1", "date": "2024-09-16", "relevancy": 2.0775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.529}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%2020%20questions%20game%20to%20distinguish%20large%20language%20models&body=Title%3A%20The%2020%20questions%20game%20to%20distinguish%20large%20language%20models%0AAuthor%3A%20Gurvan%20Richardeau%20and%20Erwan%20Le%20Merrer%20and%20Camilla%20Penzo%20and%20Gilles%20Tredan%0AAbstract%3A%20%20%20In%20a%20parallel%20with%20the%2020%20questions%20game%2C%20we%20present%20a%20method%20to%20determine%0Awhether%20two%20large%20language%20models%20%28LLMs%29%2C%20placed%20in%20a%20black-box%20context%2C%20are%0Athe%20same%20or%20not.%20The%20goal%20is%20to%20use%20a%20small%20set%20of%20%28benign%29%20binary%20questions%2C%0Atypically%20under%2020.%20We%20formalize%20the%20problem%20and%20first%20establish%20a%20baseline%0Ausing%20a%20random%20selection%20of%20questions%20from%20known%20benchmark%20datasets%2C%20achieving%0Aan%20accuracy%20of%20nearly%20100%25%20within%2020%20questions.%20After%20showing%20optimal%20bounds%0Afor%20this%20problem%2C%20we%20introduce%20two%20effective%20questioning%20heuristics%20able%20to%0Adiscriminate%2022%20LLMs%20by%20using%20half%20as%20many%20questions%20for%20the%20same%20task.%20These%0Amethods%20offer%20significant%20advantages%20in%20terms%20of%20stealth%20and%20are%20thus%20of%0Ainterest%20to%20auditors%20or%20copyright%20owners%20facing%20suspicions%20of%20model%20leaks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%252020%2520questions%2520game%2520to%2520distinguish%2520large%2520language%2520models%26entry.906535625%3DGurvan%2520Richardeau%2520and%2520Erwan%2520Le%2520Merrer%2520and%2520Camilla%2520Penzo%2520and%2520Gilles%2520Tredan%26entry.1292438233%3D%2520%2520In%2520a%2520parallel%2520with%2520the%252020%2520questions%2520game%252C%2520we%2520present%2520a%2520method%2520to%2520determine%250Awhether%2520two%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520placed%2520in%2520a%2520black-box%2520context%252C%2520are%250Athe%2520same%2520or%2520not.%2520The%2520goal%2520is%2520to%2520use%2520a%2520small%2520set%2520of%2520%2528benign%2529%2520binary%2520questions%252C%250Atypically%2520under%252020.%2520We%2520formalize%2520the%2520problem%2520and%2520first%2520establish%2520a%2520baseline%250Ausing%2520a%2520random%2520selection%2520of%2520questions%2520from%2520known%2520benchmark%2520datasets%252C%2520achieving%250Aan%2520accuracy%2520of%2520nearly%2520100%2525%2520within%252020%2520questions.%2520After%2520showing%2520optimal%2520bounds%250Afor%2520this%2520problem%252C%2520we%2520introduce%2520two%2520effective%2520questioning%2520heuristics%2520able%2520to%250Adiscriminate%252022%2520LLMs%2520by%2520using%2520half%2520as%2520many%2520questions%2520for%2520the%2520same%2520task.%2520These%250Amethods%2520offer%2520significant%2520advantages%2520in%2520terms%2520of%2520stealth%2520and%2520are%2520thus%2520of%250Ainterest%2520to%2520auditors%2520or%2520copyright%2520owners%2520facing%2520suspicions%2520of%2520model%2520leaks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%2020%20questions%20game%20to%20distinguish%20large%20language%20models&entry.906535625=Gurvan%20Richardeau%20and%20Erwan%20Le%20Merrer%20and%20Camilla%20Penzo%20and%20Gilles%20Tredan&entry.1292438233=%20%20In%20a%20parallel%20with%20the%2020%20questions%20game%2C%20we%20present%20a%20method%20to%20determine%0Awhether%20two%20large%20language%20models%20%28LLMs%29%2C%20placed%20in%20a%20black-box%20context%2C%20are%0Athe%20same%20or%20not.%20The%20goal%20is%20to%20use%20a%20small%20set%20of%20%28benign%29%20binary%20questions%2C%0Atypically%20under%2020.%20We%20formalize%20the%20problem%20and%20first%20establish%20a%20baseline%0Ausing%20a%20random%20selection%20of%20questions%20from%20known%20benchmark%20datasets%2C%20achieving%0Aan%20accuracy%20of%20nearly%20100%25%20within%2020%20questions.%20After%20showing%20optimal%20bounds%0Afor%20this%20problem%2C%20we%20introduce%20two%20effective%20questioning%20heuristics%20able%20to%0Adiscriminate%2022%20LLMs%20by%20using%20half%20as%20many%20questions%20for%20the%20same%20task.%20These%0Amethods%20offer%20significant%20advantages%20in%20terms%20of%20stealth%20and%20are%20thus%20of%0Ainterest%20to%20auditors%20or%20copyright%20owners%20facing%20suspicions%20of%20model%20leaks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10338v1&entry.124074799=Read"},
{"title": "MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation", "author": "Jing Xu", "abstract": "  Although transformer is preferred in natural language processing, some\nstudies has only been applied to the field of medical imaging in recent years.\nFor its long-term dependency, the transformer is expected to contribute to\nunconventional convolution neural net conquer their inherent spatial induction\nbias. The lately suggested transformer-based segmentation method only uses the\ntransformer as an auxiliary module to help encode the global context into a\nconvolutional representation. How to optimally integrate self-attention with\nconvolution has not been investigated in depth. To solve the problem, this\npaper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentation\nmodel on account of the bond of self-attention and convolution. MS-Twins can\nbetter capture semantic and fine-grained information by combining different\nscales and cascading features. Compared with the existing network structure,\nMS-Twins has made progress on the previous method based on the transformer of\ntwo in common use data sets, Synapse and ACDC. In particular, the performance\nof MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,\nthe best entirely convoluted medical image segmentation network, the\nperformance of MS-Twins on Synapse and ACDC still has a bit advantage.\n", "link": "http://arxiv.org/abs/2312.07128v5", "date": "2024-09-16", "relevancy": 2.077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5166}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Jing%20Xu%0AAbstract%3A%20%20%20Although%20transformer%20is%20preferred%20in%20natural%20language%20processing%2C%20some%0Astudies%20has%20only%20been%20applied%20to%20the%20field%20of%20medical%20imaging%20in%20recent%20years.%0AFor%20its%20long-term%20dependency%2C%20the%20transformer%20is%20expected%20to%20contribute%20to%0Aunconventional%20convolution%20neural%20net%20conquer%20their%20inherent%20spatial%20induction%0Abias.%20The%20lately%20suggested%20transformer-based%20segmentation%20method%20only%20uses%20the%0Atransformer%20as%20an%20auxiliary%20module%20to%20help%20encode%20the%20global%20context%20into%20a%0Aconvolutional%20representation.%20How%20to%20optimally%20integrate%20self-attention%20with%0Aconvolution%20has%20not%20been%20investigated%20in%20depth.%20To%20solve%20the%20problem%2C%20this%0Apaper%20proposes%20MS-Twins%20%28Multi-Scale%20Twins%29%2C%20which%20is%20a%20powerful%20segmentation%0Amodel%20on%20account%20of%20the%20bond%20of%20self-attention%20and%20convolution.%20MS-Twins%20can%0Abetter%20capture%20semantic%20and%20fine-grained%20information%20by%20combining%20different%0Ascales%20and%20cascading%20features.%20Compared%20with%20the%20existing%20network%20structure%2C%0AMS-Twins%20has%20made%20progress%20on%20the%20previous%20method%20based%20on%20the%20transformer%20of%0Atwo%20in%20common%20use%20data%20sets%2C%20Synapse%20and%20ACDC.%20In%20particular%2C%20the%20performance%0Aof%20MS-Twins%20on%20Synapse%20is%208%25%20higher%20than%20SwinUNet.%20Even%20compared%20with%20nnUNet%2C%0Athe%20best%20entirely%20convoluted%20medical%20image%20segmentation%20network%2C%20the%0Aperformance%20of%20MS-Twins%20on%20Synapse%20and%20ACDC%20still%20has%20a%20bit%20advantage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07128v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Twins%253A%2520Multi-Scale%2520Deep%2520Self-Attention%2520Networks%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DJing%2520Xu%26entry.1292438233%3D%2520%2520Although%2520transformer%2520is%2520preferred%2520in%2520natural%2520language%2520processing%252C%2520some%250Astudies%2520has%2520only%2520been%2520applied%2520to%2520the%2520field%2520of%2520medical%2520imaging%2520in%2520recent%2520years.%250AFor%2520its%2520long-term%2520dependency%252C%2520the%2520transformer%2520is%2520expected%2520to%2520contribute%2520to%250Aunconventional%2520convolution%2520neural%2520net%2520conquer%2520their%2520inherent%2520spatial%2520induction%250Abias.%2520The%2520lately%2520suggested%2520transformer-based%2520segmentation%2520method%2520only%2520uses%2520the%250Atransformer%2520as%2520an%2520auxiliary%2520module%2520to%2520help%2520encode%2520the%2520global%2520context%2520into%2520a%250Aconvolutional%2520representation.%2520How%2520to%2520optimally%2520integrate%2520self-attention%2520with%250Aconvolution%2520has%2520not%2520been%2520investigated%2520in%2520depth.%2520To%2520solve%2520the%2520problem%252C%2520this%250Apaper%2520proposes%2520MS-Twins%2520%2528Multi-Scale%2520Twins%2529%252C%2520which%2520is%2520a%2520powerful%2520segmentation%250Amodel%2520on%2520account%2520of%2520the%2520bond%2520of%2520self-attention%2520and%2520convolution.%2520MS-Twins%2520can%250Abetter%2520capture%2520semantic%2520and%2520fine-grained%2520information%2520by%2520combining%2520different%250Ascales%2520and%2520cascading%2520features.%2520Compared%2520with%2520the%2520existing%2520network%2520structure%252C%250AMS-Twins%2520has%2520made%2520progress%2520on%2520the%2520previous%2520method%2520based%2520on%2520the%2520transformer%2520of%250Atwo%2520in%2520common%2520use%2520data%2520sets%252C%2520Synapse%2520and%2520ACDC.%2520In%2520particular%252C%2520the%2520performance%250Aof%2520MS-Twins%2520on%2520Synapse%2520is%25208%2525%2520higher%2520than%2520SwinUNet.%2520Even%2520compared%2520with%2520nnUNet%252C%250Athe%2520best%2520entirely%2520convoluted%2520medical%2520image%2520segmentation%2520network%252C%2520the%250Aperformance%2520of%2520MS-Twins%2520on%2520Synapse%2520and%2520ACDC%2520still%2520has%2520a%2520bit%2520advantage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07128v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Jing%20Xu&entry.1292438233=%20%20Although%20transformer%20is%20preferred%20in%20natural%20language%20processing%2C%20some%0Astudies%20has%20only%20been%20applied%20to%20the%20field%20of%20medical%20imaging%20in%20recent%20years.%0AFor%20its%20long-term%20dependency%2C%20the%20transformer%20is%20expected%20to%20contribute%20to%0Aunconventional%20convolution%20neural%20net%20conquer%20their%20inherent%20spatial%20induction%0Abias.%20The%20lately%20suggested%20transformer-based%20segmentation%20method%20only%20uses%20the%0Atransformer%20as%20an%20auxiliary%20module%20to%20help%20encode%20the%20global%20context%20into%20a%0Aconvolutional%20representation.%20How%20to%20optimally%20integrate%20self-attention%20with%0Aconvolution%20has%20not%20been%20investigated%20in%20depth.%20To%20solve%20the%20problem%2C%20this%0Apaper%20proposes%20MS-Twins%20%28Multi-Scale%20Twins%29%2C%20which%20is%20a%20powerful%20segmentation%0Amodel%20on%20account%20of%20the%20bond%20of%20self-attention%20and%20convolution.%20MS-Twins%20can%0Abetter%20capture%20semantic%20and%20fine-grained%20information%20by%20combining%20different%0Ascales%20and%20cascading%20features.%20Compared%20with%20the%20existing%20network%20structure%2C%0AMS-Twins%20has%20made%20progress%20on%20the%20previous%20method%20based%20on%20the%20transformer%20of%0Atwo%20in%20common%20use%20data%20sets%2C%20Synapse%20and%20ACDC.%20In%20particular%2C%20the%20performance%0Aof%20MS-Twins%20on%20Synapse%20is%208%25%20higher%20than%20SwinUNet.%20Even%20compared%20with%20nnUNet%2C%0Athe%20best%20entirely%20convoluted%20medical%20image%20segmentation%20network%2C%20the%0Aperformance%20of%20MS-Twins%20on%20Synapse%20and%20ACDC%20still%20has%20a%20bit%20advantage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07128v5&entry.124074799=Read"},
{"title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles", "author": "Kulin Shah and Nishanth Dikkala and Xin Wang and Rina Panigrahy", "abstract": "  Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.\n", "link": "http://arxiv.org/abs/2409.10502v1", "date": "2024-09-16", "relevancy": 2.0602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Language%20Modeling%20Can%20Elicit%20Search%20and%20Reasoning%20Capabilities%20on%0A%20%20Logic%20Puzzles&body=Title%3A%20Causal%20Language%20Modeling%20Can%20Elicit%20Search%20and%20Reasoning%20Capabilities%20on%0A%20%20Logic%20Puzzles%0AAuthor%3A%20Kulin%20Shah%20and%20Nishanth%20Dikkala%20and%20Xin%20Wang%20and%20Rina%20Panigrahy%0AAbstract%3A%20%20%20Causal%20language%20modeling%20using%20the%20Transformer%20architecture%20has%20yielded%0Aremarkable%20capabilities%20in%20Large%20Language%20Models%20%28LLMs%29%20over%20the%20last%20few%0Ayears.%20However%2C%20the%20extent%20to%20which%20fundamental%20search%20and%20reasoning%0Acapabilities%20emerged%20within%20LLMs%20remains%20a%20topic%20of%20ongoing%20debate.%20In%20this%0Awork%2C%20we%20study%20if%20causal%20language%20modeling%20can%20learn%20a%20complex%20task%20such%20as%0Asolving%20Sudoku%20puzzles.%20To%20solve%20a%20Sudoku%2C%20the%20model%20is%20first%20required%20to%0Asearch%20over%20all%20empty%20cells%20of%20the%20puzzle%20to%20decide%20on%20a%20cell%20to%20fill%20and%20then%0Aapply%20an%20appropriate%20strategy%20to%20fill%20the%20decided%20cell.%20Sometimes%2C%20the%0Aapplication%20of%20a%20strategy%20only%20results%20in%20thinning%20down%20the%20possible%20values%20in%0Aa%20cell%20rather%20than%20concluding%20the%20exact%20value%20of%20the%20cell.%20In%20such%20cases%2C%0Amultiple%20strategies%20are%20applied%20one%20after%20the%20other%20to%20fill%20a%20single%20cell.%20We%0Aobserve%20that%20Transformer%20models%20trained%20on%20this%20synthetic%20task%20can%20indeed%20learn%0Ato%20solve%20Sudokus%20%28our%20model%20solves%20%2494.21%5C%25%24%20of%20the%20puzzles%20fully%20correctly%29%0Awhen%20trained%20on%20a%20logical%20sequence%20of%20steps%20taken%20by%20a%20solver.%20We%20find%20that%0Atraining%20Transformers%20with%20the%20logical%20sequence%20of%20steps%20is%20necessary%20and%0Awithout%20such%20training%2C%20they%20fail%20to%20learn%20Sudoku.%20We%20also%20extend%20our%20analysis%0Ato%20Zebra%20puzzles%20%28known%20as%20Einstein%20puzzles%29%20and%20show%20that%20the%20model%20solves%0A%2492.04%20%5C%25%24%20of%20the%20puzzles%20fully%20correctly.%20In%20addition%2C%20we%20study%20the%20internal%0Arepresentations%20of%20the%20trained%20Transformer%20and%20find%20that%20through%20linear%0Aprobing%2C%20we%20can%20decode%20information%20about%20the%20set%20of%20possible%20values%20in%20any%0Agiven%20cell%20from%20them%2C%20pointing%20to%20the%20presence%20of%20a%20strong%20reasoning%20engine%0Aimplicit%20in%20the%20Transformer%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Language%2520Modeling%2520Can%2520Elicit%2520Search%2520and%2520Reasoning%2520Capabilities%2520on%250A%2520%2520Logic%2520Puzzles%26entry.906535625%3DKulin%2520Shah%2520and%2520Nishanth%2520Dikkala%2520and%2520Xin%2520Wang%2520and%2520Rina%2520Panigrahy%26entry.1292438233%3D%2520%2520Causal%2520language%2520modeling%2520using%2520the%2520Transformer%2520architecture%2520has%2520yielded%250Aremarkable%2520capabilities%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520over%2520the%2520last%2520few%250Ayears.%2520However%252C%2520the%2520extent%2520to%2520which%2520fundamental%2520search%2520and%2520reasoning%250Acapabilities%2520emerged%2520within%2520LLMs%2520remains%2520a%2520topic%2520of%2520ongoing%2520debate.%2520In%2520this%250Awork%252C%2520we%2520study%2520if%2520causal%2520language%2520modeling%2520can%2520learn%2520a%2520complex%2520task%2520such%2520as%250Asolving%2520Sudoku%2520puzzles.%2520To%2520solve%2520a%2520Sudoku%252C%2520the%2520model%2520is%2520first%2520required%2520to%250Asearch%2520over%2520all%2520empty%2520cells%2520of%2520the%2520puzzle%2520to%2520decide%2520on%2520a%2520cell%2520to%2520fill%2520and%2520then%250Aapply%2520an%2520appropriate%2520strategy%2520to%2520fill%2520the%2520decided%2520cell.%2520Sometimes%252C%2520the%250Aapplication%2520of%2520a%2520strategy%2520only%2520results%2520in%2520thinning%2520down%2520the%2520possible%2520values%2520in%250Aa%2520cell%2520rather%2520than%2520concluding%2520the%2520exact%2520value%2520of%2520the%2520cell.%2520In%2520such%2520cases%252C%250Amultiple%2520strategies%2520are%2520applied%2520one%2520after%2520the%2520other%2520to%2520fill%2520a%2520single%2520cell.%2520We%250Aobserve%2520that%2520Transformer%2520models%2520trained%2520on%2520this%2520synthetic%2520task%2520can%2520indeed%2520learn%250Ato%2520solve%2520Sudokus%2520%2528our%2520model%2520solves%2520%252494.21%255C%2525%2524%2520of%2520the%2520puzzles%2520fully%2520correctly%2529%250Awhen%2520trained%2520on%2520a%2520logical%2520sequence%2520of%2520steps%2520taken%2520by%2520a%2520solver.%2520We%2520find%2520that%250Atraining%2520Transformers%2520with%2520the%2520logical%2520sequence%2520of%2520steps%2520is%2520necessary%2520and%250Awithout%2520such%2520training%252C%2520they%2520fail%2520to%2520learn%2520Sudoku.%2520We%2520also%2520extend%2520our%2520analysis%250Ato%2520Zebra%2520puzzles%2520%2528known%2520as%2520Einstein%2520puzzles%2529%2520and%2520show%2520that%2520the%2520model%2520solves%250A%252492.04%2520%255C%2525%2524%2520of%2520the%2520puzzles%2520fully%2520correctly.%2520In%2520addition%252C%2520we%2520study%2520the%2520internal%250Arepresentations%2520of%2520the%2520trained%2520Transformer%2520and%2520find%2520that%2520through%2520linear%250Aprobing%252C%2520we%2520can%2520decode%2520information%2520about%2520the%2520set%2520of%2520possible%2520values%2520in%2520any%250Agiven%2520cell%2520from%2520them%252C%2520pointing%2520to%2520the%2520presence%2520of%2520a%2520strong%2520reasoning%2520engine%250Aimplicit%2520in%2520the%2520Transformer%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Language%20Modeling%20Can%20Elicit%20Search%20and%20Reasoning%20Capabilities%20on%0A%20%20Logic%20Puzzles&entry.906535625=Kulin%20Shah%20and%20Nishanth%20Dikkala%20and%20Xin%20Wang%20and%20Rina%20Panigrahy&entry.1292438233=%20%20Causal%20language%20modeling%20using%20the%20Transformer%20architecture%20has%20yielded%0Aremarkable%20capabilities%20in%20Large%20Language%20Models%20%28LLMs%29%20over%20the%20last%20few%0Ayears.%20However%2C%20the%20extent%20to%20which%20fundamental%20search%20and%20reasoning%0Acapabilities%20emerged%20within%20LLMs%20remains%20a%20topic%20of%20ongoing%20debate.%20In%20this%0Awork%2C%20we%20study%20if%20causal%20language%20modeling%20can%20learn%20a%20complex%20task%20such%20as%0Asolving%20Sudoku%20puzzles.%20To%20solve%20a%20Sudoku%2C%20the%20model%20is%20first%20required%20to%0Asearch%20over%20all%20empty%20cells%20of%20the%20puzzle%20to%20decide%20on%20a%20cell%20to%20fill%20and%20then%0Aapply%20an%20appropriate%20strategy%20to%20fill%20the%20decided%20cell.%20Sometimes%2C%20the%0Aapplication%20of%20a%20strategy%20only%20results%20in%20thinning%20down%20the%20possible%20values%20in%0Aa%20cell%20rather%20than%20concluding%20the%20exact%20value%20of%20the%20cell.%20In%20such%20cases%2C%0Amultiple%20strategies%20are%20applied%20one%20after%20the%20other%20to%20fill%20a%20single%20cell.%20We%0Aobserve%20that%20Transformer%20models%20trained%20on%20this%20synthetic%20task%20can%20indeed%20learn%0Ato%20solve%20Sudokus%20%28our%20model%20solves%20%2494.21%5C%25%24%20of%20the%20puzzles%20fully%20correctly%29%0Awhen%20trained%20on%20a%20logical%20sequence%20of%20steps%20taken%20by%20a%20solver.%20We%20find%20that%0Atraining%20Transformers%20with%20the%20logical%20sequence%20of%20steps%20is%20necessary%20and%0Awithout%20such%20training%2C%20they%20fail%20to%20learn%20Sudoku.%20We%20also%20extend%20our%20analysis%0Ato%20Zebra%20puzzles%20%28known%20as%20Einstein%20puzzles%29%20and%20show%20that%20the%20model%20solves%0A%2492.04%20%5C%25%24%20of%20the%20puzzles%20fully%20correctly.%20In%20addition%2C%20we%20study%20the%20internal%0Arepresentations%20of%20the%20trained%20Transformer%20and%20find%20that%20through%20linear%0Aprobing%2C%20we%20can%20decode%20information%20about%20the%20set%20of%20possible%20values%20in%20any%0Agiven%20cell%20from%20them%2C%20pointing%20to%20the%20presence%20of%20a%20strong%20reasoning%20engine%0Aimplicit%20in%20the%20Transformer%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10502v1&entry.124074799=Read"},
{"title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge", "author": "Hamed Babaei Giglou and Jennifer D'Souza and S\u00f6ren Auer", "abstract": "  This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.\n", "link": "http://arxiv.org/abs/2409.10146v1", "date": "2024-09-16", "relevancy": 2.0449, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs4OL%202024%20Overview%3A%20The%201st%20Large%20Language%20Models%20for%20Ontology%0A%20%20Learning%20Challenge&body=Title%3A%20LLMs4OL%202024%20Overview%3A%20The%201st%20Large%20Language%20Models%20for%20Ontology%0A%20%20Learning%20Challenge%0AAuthor%3A%20Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20S%C3%B6ren%20Auer%0AAbstract%3A%20%20%20This%20paper%20outlines%20the%20LLMs4OL%202024%2C%20the%20first%20edition%20of%20the%20Large%20Language%0AModels%20for%20Ontology%20Learning%20Challenge.%20LLMs4OL%20is%20a%20community%20development%0Ainitiative%20collocated%20with%20the%2023rd%20International%20Semantic%20Web%20Conference%0A%28ISWC%29%20to%20explore%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20Ontology%0ALearning%20%28OL%29%2C%20a%20vital%20process%20for%20enhancing%20the%20web%20with%20structured%20knowledge%0Ato%20improve%20interoperability.%20By%20leveraging%20LLMs%2C%20the%20challenge%20aims%20to%20advance%0Aunderstanding%20and%20innovation%20in%20OL%2C%20aligning%20with%20the%20goals%20of%20the%20Semantic%20Web%0Ato%20create%20a%20more%20intelligent%20and%20user-friendly%20web.%20In%20this%20paper%2C%20we%20give%20an%0Aoverview%20of%20the%202024%20edition%20of%20the%20LLMs4OL%20challenge%20and%20summarize%20the%0Acontributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs4OL%25202024%2520Overview%253A%2520The%25201st%2520Large%2520Language%2520Models%2520for%2520Ontology%250A%2520%2520Learning%2520Challenge%26entry.906535625%3DHamed%2520Babaei%2520Giglou%2520and%2520Jennifer%2520D%2527Souza%2520and%2520S%25C3%25B6ren%2520Auer%26entry.1292438233%3D%2520%2520This%2520paper%2520outlines%2520the%2520LLMs4OL%25202024%252C%2520the%2520first%2520edition%2520of%2520the%2520Large%2520Language%250AModels%2520for%2520Ontology%2520Learning%2520Challenge.%2520LLMs4OL%2520is%2520a%2520community%2520development%250Ainitiative%2520collocated%2520with%2520the%252023rd%2520International%2520Semantic%2520Web%2520Conference%250A%2528ISWC%2529%2520to%2520explore%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520Ontology%250ALearning%2520%2528OL%2529%252C%2520a%2520vital%2520process%2520for%2520enhancing%2520the%2520web%2520with%2520structured%2520knowledge%250Ato%2520improve%2520interoperability.%2520By%2520leveraging%2520LLMs%252C%2520the%2520challenge%2520aims%2520to%2520advance%250Aunderstanding%2520and%2520innovation%2520in%2520OL%252C%2520aligning%2520with%2520the%2520goals%2520of%2520the%2520Semantic%2520Web%250Ato%2520create%2520a%2520more%2520intelligent%2520and%2520user-friendly%2520web.%2520In%2520this%2520paper%252C%2520we%2520give%2520an%250Aoverview%2520of%2520the%25202024%2520edition%2520of%2520the%2520LLMs4OL%2520challenge%2520and%2520summarize%2520the%250Acontributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs4OL%202024%20Overview%3A%20The%201st%20Large%20Language%20Models%20for%20Ontology%0A%20%20Learning%20Challenge&entry.906535625=Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20S%C3%B6ren%20Auer&entry.1292438233=%20%20This%20paper%20outlines%20the%20LLMs4OL%202024%2C%20the%20first%20edition%20of%20the%20Large%20Language%0AModels%20for%20Ontology%20Learning%20Challenge.%20LLMs4OL%20is%20a%20community%20development%0Ainitiative%20collocated%20with%20the%2023rd%20International%20Semantic%20Web%20Conference%0A%28ISWC%29%20to%20explore%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20Ontology%0ALearning%20%28OL%29%2C%20a%20vital%20process%20for%20enhancing%20the%20web%20with%20structured%20knowledge%0Ato%20improve%20interoperability.%20By%20leveraging%20LLMs%2C%20the%20challenge%20aims%20to%20advance%0Aunderstanding%20and%20innovation%20in%20OL%2C%20aligning%20with%20the%20goals%20of%20the%20Semantic%20Web%0Ato%20create%20a%20more%20intelligent%20and%20user-friendly%20web.%20In%20this%20paper%2C%20we%20give%20an%0Aoverview%20of%20the%202024%20edition%20of%20the%20LLMs4OL%20challenge%20and%20summarize%20the%0Acontributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10146v1&entry.124074799=Read"},
{"title": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots", "author": "Hongming Zhang and Xiaoman Pan and Hongwei Wang and Kaixin Ma and Wenhao Yu and Dong Yu", "abstract": "  We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.\n", "link": "http://arxiv.org/abs/2409.10277v1", "date": "2024-09-16", "relevancy": 2.0415, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5148}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Kernel%3A%20An%20Open-source%20Agent%20System%20towards%20Generalist%0A%20%20Autopilots&body=Title%3A%20Cognitive%20Kernel%3A%20An%20Open-source%20Agent%20System%20towards%20Generalist%0A%20%20Autopilots%0AAuthor%3A%20Hongming%20Zhang%20and%20Xiaoman%20Pan%20and%20Hongwei%20Wang%20and%20Kaixin%20Ma%20and%20Wenhao%20Yu%20and%20Dong%20Yu%0AAbstract%3A%20%20%20We%20introduce%20Cognitive%20Kernel%2C%20an%20open-source%20agent%20system%20towards%20the%20goal%0Aof%20generalist%20autopilots.%20Unlike%20copilot%20systems%2C%20which%20primarily%20rely%20on%20users%0Ato%20provide%20essential%20state%20information%20%28e.g.%2C%20task%20descriptions%29%20and%20assist%0Ausers%20by%20answering%20questions%20or%20auto-completing%20contents%2C%20autopilot%20systems%0Amust%20complete%20tasks%20from%20start%20to%20finish%20independently%2C%20which%20requires%20the%0Asystem%20to%20acquire%20the%20state%20information%20from%20the%20environments%20actively.%20To%0Aachieve%20this%2C%20an%20autopilot%20system%20should%20be%20capable%20of%20understanding%20user%0Aintents%2C%20actively%20gathering%20necessary%20information%20from%20various%20real-world%0Asources%2C%20and%20making%20wise%20decisions.%20Cognitive%20Kernel%20adopts%20a%20model-centric%0Adesign.%20In%20our%20implementation%2C%20the%20central%20policy%20model%20%28a%20fine-tuned%20LLM%29%0Ainitiates%20interactions%20with%20the%20environment%20using%20a%20combination%20of%20atomic%0Aactions%2C%20such%20as%20opening%20files%2C%20clicking%20buttons%2C%20saving%20intermediate%20results%0Ato%20memory%2C%20or%20calling%20the%20LLM%20itself.%20This%20differs%20from%20the%20widely%20used%0Aenvironment-centric%20design%2C%20where%20a%20task-specific%20environment%20with%20predefined%0Aactions%20is%20fixed%2C%20and%20the%20policy%20model%20is%20limited%20to%20selecting%20the%20correct%0Aaction%20from%20a%20given%20set%20of%20options.%20Our%20design%20facilitates%20seamless%20information%0Aflow%20across%20various%20sources%20and%20provides%20greater%20flexibility.%20We%20evaluate%20our%0Asystem%20in%20three%20use%20cases%3A%20real-time%20information%20management%2C%20private%0Ainformation%20management%2C%20and%20long-term%20memory%20management.%20The%20results%0Ademonstrate%20that%20Cognitive%20Kernel%20achieves%20better%20or%20comparable%20performance%20to%0Aother%20closed-source%20systems%20in%20these%20scenarios.%20Cognitive%20Kernel%20is%20fully%0Adockerized%2C%20ensuring%20everyone%20can%20deploy%20it%20privately%20and%20securely.%20We%0Aopen-source%20the%20system%20and%20the%20backbone%20model%20to%20encourage%20further%20research%20on%0ALLM-driven%20autopilot%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Kernel%253A%2520An%2520Open-source%2520Agent%2520System%2520towards%2520Generalist%250A%2520%2520Autopilots%26entry.906535625%3DHongming%2520Zhang%2520and%2520Xiaoman%2520Pan%2520and%2520Hongwei%2520Wang%2520and%2520Kaixin%2520Ma%2520and%2520Wenhao%2520Yu%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cognitive%2520Kernel%252C%2520an%2520open-source%2520agent%2520system%2520towards%2520the%2520goal%250Aof%2520generalist%2520autopilots.%2520Unlike%2520copilot%2520systems%252C%2520which%2520primarily%2520rely%2520on%2520users%250Ato%2520provide%2520essential%2520state%2520information%2520%2528e.g.%252C%2520task%2520descriptions%2529%2520and%2520assist%250Ausers%2520by%2520answering%2520questions%2520or%2520auto-completing%2520contents%252C%2520autopilot%2520systems%250Amust%2520complete%2520tasks%2520from%2520start%2520to%2520finish%2520independently%252C%2520which%2520requires%2520the%250Asystem%2520to%2520acquire%2520the%2520state%2520information%2520from%2520the%2520environments%2520actively.%2520To%250Aachieve%2520this%252C%2520an%2520autopilot%2520system%2520should%2520be%2520capable%2520of%2520understanding%2520user%250Aintents%252C%2520actively%2520gathering%2520necessary%2520information%2520from%2520various%2520real-world%250Asources%252C%2520and%2520making%2520wise%2520decisions.%2520Cognitive%2520Kernel%2520adopts%2520a%2520model-centric%250Adesign.%2520In%2520our%2520implementation%252C%2520the%2520central%2520policy%2520model%2520%2528a%2520fine-tuned%2520LLM%2529%250Ainitiates%2520interactions%2520with%2520the%2520environment%2520using%2520a%2520combination%2520of%2520atomic%250Aactions%252C%2520such%2520as%2520opening%2520files%252C%2520clicking%2520buttons%252C%2520saving%2520intermediate%2520results%250Ato%2520memory%252C%2520or%2520calling%2520the%2520LLM%2520itself.%2520This%2520differs%2520from%2520the%2520widely%2520used%250Aenvironment-centric%2520design%252C%2520where%2520a%2520task-specific%2520environment%2520with%2520predefined%250Aactions%2520is%2520fixed%252C%2520and%2520the%2520policy%2520model%2520is%2520limited%2520to%2520selecting%2520the%2520correct%250Aaction%2520from%2520a%2520given%2520set%2520of%2520options.%2520Our%2520design%2520facilitates%2520seamless%2520information%250Aflow%2520across%2520various%2520sources%2520and%2520provides%2520greater%2520flexibility.%2520We%2520evaluate%2520our%250Asystem%2520in%2520three%2520use%2520cases%253A%2520real-time%2520information%2520management%252C%2520private%250Ainformation%2520management%252C%2520and%2520long-term%2520memory%2520management.%2520The%2520results%250Ademonstrate%2520that%2520Cognitive%2520Kernel%2520achieves%2520better%2520or%2520comparable%2520performance%2520to%250Aother%2520closed-source%2520systems%2520in%2520these%2520scenarios.%2520Cognitive%2520Kernel%2520is%2520fully%250Adockerized%252C%2520ensuring%2520everyone%2520can%2520deploy%2520it%2520privately%2520and%2520securely.%2520We%250Aopen-source%2520the%2520system%2520and%2520the%2520backbone%2520model%2520to%2520encourage%2520further%2520research%2520on%250ALLM-driven%2520autopilot%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Kernel%3A%20An%20Open-source%20Agent%20System%20towards%20Generalist%0A%20%20Autopilots&entry.906535625=Hongming%20Zhang%20and%20Xiaoman%20Pan%20and%20Hongwei%20Wang%20and%20Kaixin%20Ma%20and%20Wenhao%20Yu%20and%20Dong%20Yu&entry.1292438233=%20%20We%20introduce%20Cognitive%20Kernel%2C%20an%20open-source%20agent%20system%20towards%20the%20goal%0Aof%20generalist%20autopilots.%20Unlike%20copilot%20systems%2C%20which%20primarily%20rely%20on%20users%0Ato%20provide%20essential%20state%20information%20%28e.g.%2C%20task%20descriptions%29%20and%20assist%0Ausers%20by%20answering%20questions%20or%20auto-completing%20contents%2C%20autopilot%20systems%0Amust%20complete%20tasks%20from%20start%20to%20finish%20independently%2C%20which%20requires%20the%0Asystem%20to%20acquire%20the%20state%20information%20from%20the%20environments%20actively.%20To%0Aachieve%20this%2C%20an%20autopilot%20system%20should%20be%20capable%20of%20understanding%20user%0Aintents%2C%20actively%20gathering%20necessary%20information%20from%20various%20real-world%0Asources%2C%20and%20making%20wise%20decisions.%20Cognitive%20Kernel%20adopts%20a%20model-centric%0Adesign.%20In%20our%20implementation%2C%20the%20central%20policy%20model%20%28a%20fine-tuned%20LLM%29%0Ainitiates%20interactions%20with%20the%20environment%20using%20a%20combination%20of%20atomic%0Aactions%2C%20such%20as%20opening%20files%2C%20clicking%20buttons%2C%20saving%20intermediate%20results%0Ato%20memory%2C%20or%20calling%20the%20LLM%20itself.%20This%20differs%20from%20the%20widely%20used%0Aenvironment-centric%20design%2C%20where%20a%20task-specific%20environment%20with%20predefined%0Aactions%20is%20fixed%2C%20and%20the%20policy%20model%20is%20limited%20to%20selecting%20the%20correct%0Aaction%20from%20a%20given%20set%20of%20options.%20Our%20design%20facilitates%20seamless%20information%0Aflow%20across%20various%20sources%20and%20provides%20greater%20flexibility.%20We%20evaluate%20our%0Asystem%20in%20three%20use%20cases%3A%20real-time%20information%20management%2C%20private%0Ainformation%20management%2C%20and%20long-term%20memory%20management.%20The%20results%0Ademonstrate%20that%20Cognitive%20Kernel%20achieves%20better%20or%20comparable%20performance%20to%0Aother%20closed-source%20systems%20in%20these%20scenarios.%20Cognitive%20Kernel%20is%20fully%0Adockerized%2C%20ensuring%20everyone%20can%20deploy%20it%20privately%20and%20securely.%20We%0Aopen-source%20the%20system%20and%20the%20backbone%20model%20to%20encourage%20further%20research%20on%0ALLM-driven%20autopilot%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10277v1&entry.124074799=Read"},
{"title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval", "author": "Di Liu and Meng Chen and Baotong Lu and Huiqiang Jiang and Zhenhua Han and Qianxi Zhang and Qi Chen and Chengruidong Zhang and Bailu Ding and Kai Zhang and Chen Chen and Fan Yang and Yuqing Yang and Lili Qiu", "abstract": "  Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).\n", "link": "http://arxiv.org/abs/2409.10516v1", "date": "2024-09-16", "relevancy": 2.032, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5453}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RetrievalAttention%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Vector%0A%20%20Retrieval&body=Title%3A%20RetrievalAttention%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Vector%0A%20%20Retrieval%0AAuthor%3A%20Di%20Liu%20and%20Meng%20Chen%20and%20Baotong%20Lu%20and%20Huiqiang%20Jiang%20and%20Zhenhua%20Han%20and%20Qianxi%20Zhang%20and%20Qi%20Chen%20and%20Chengruidong%20Zhang%20and%20Bailu%20Ding%20and%20Kai%20Zhang%20and%20Chen%20Chen%20and%20Fan%20Yang%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20Transformer-based%20large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20important%0Ain%20various%20domains.%20However%2C%20the%20quadratic%20time%20complexity%20of%20attention%0Aoperation%20poses%20a%20significant%20challenge%20for%20scaling%20to%20longer%20contexts%20due%20to%0Athe%20extremely%20high%20inference%20latency%20and%20GPU%20memory%20consumption%20for%20caching%0Akey-value%20%28KV%29%20vectors.%20This%20paper%20proposes%20RetrievalAttention%2C%20a%20training-free%0Aapproach%20to%20accelerate%20attention%20computation.%20To%20leverage%20the%20dynamic%20sparse%0Aproperty%20of%20attention%2C%20RetrievalAttention%20builds%20approximate%20nearest%20neighbor%0Asearch%20%28ANNS%29%20indexes%20upon%20KV%20vectors%20in%20CPU%20memory%20and%20retrieves%20the%20most%0Arelevant%20ones%20via%20vector%20search%20during%20generation.%20Due%20to%20the%0Aout-of-distribution%20%28OOD%29%20between%20query%20vectors%20and%20key%20vectors%2C%20off-the-shelf%0AANNS%20indexes%20still%20need%20to%20scan%20O%28N%29%20%28usually%2030%25%20of%20all%20keys%29%20data%20for%0Aaccurate%20retrieval%2C%20which%20fails%20to%20exploit%20the%20high%20sparsity.%0ARetrievalAttention%20first%20identifies%20the%20OOD%20challenge%20of%20ANNS-based%20attention%2C%0Aand%20addresses%20it%20via%20an%20attention-aware%20vector%20search%20algorithm%20that%20can%20adapt%0Ato%20queries%20and%20only%20access%201--3%25%20of%20data%2C%20thus%20achieving%20a%20sub-linear%20time%0Acomplexity.%20RetrievalAttention%20greatly%20reduces%20the%20inference%20cost%20of%0Along-context%20LLM%20with%20much%20lower%20GPU%20memory%20requirements%20while%20maintaining%20the%0Amodel%20accuracy.%20Especially%2C%20RetrievalAttention%20only%20needs%2016GB%20GPU%20memory%20for%0Aserving%20128K%20tokens%20in%20LLMs%20with%208B%20parameters%2C%20which%20is%20capable%20of%20generating%0Aone%20token%20in%200.188%20seconds%20on%20a%20single%20NVIDIA%20RTX4090%20%2824GB%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrievalAttention%253A%2520Accelerating%2520Long-Context%2520LLM%2520Inference%2520via%2520Vector%250A%2520%2520Retrieval%26entry.906535625%3DDi%2520Liu%2520and%2520Meng%2520Chen%2520and%2520Baotong%2520Lu%2520and%2520Huiqiang%2520Jiang%2520and%2520Zhenhua%2520Han%2520and%2520Qianxi%2520Zhang%2520and%2520Qi%2520Chen%2520and%2520Chengruidong%2520Zhang%2520and%2520Bailu%2520Ding%2520and%2520Kai%2520Zhang%2520and%2520Chen%2520Chen%2520and%2520Fan%2520Yang%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520increasingly%2520important%250Ain%2520various%2520domains.%2520However%252C%2520the%2520quadratic%2520time%2520complexity%2520of%2520attention%250Aoperation%2520poses%2520a%2520significant%2520challenge%2520for%2520scaling%2520to%2520longer%2520contexts%2520due%2520to%250Athe%2520extremely%2520high%2520inference%2520latency%2520and%2520GPU%2520memory%2520consumption%2520for%2520caching%250Akey-value%2520%2528KV%2529%2520vectors.%2520This%2520paper%2520proposes%2520RetrievalAttention%252C%2520a%2520training-free%250Aapproach%2520to%2520accelerate%2520attention%2520computation.%2520To%2520leverage%2520the%2520dynamic%2520sparse%250Aproperty%2520of%2520attention%252C%2520RetrievalAttention%2520builds%2520approximate%2520nearest%2520neighbor%250Asearch%2520%2528ANNS%2529%2520indexes%2520upon%2520KV%2520vectors%2520in%2520CPU%2520memory%2520and%2520retrieves%2520the%2520most%250Arelevant%2520ones%2520via%2520vector%2520search%2520during%2520generation.%2520Due%2520to%2520the%250Aout-of-distribution%2520%2528OOD%2529%2520between%2520query%2520vectors%2520and%2520key%2520vectors%252C%2520off-the-shelf%250AANNS%2520indexes%2520still%2520need%2520to%2520scan%2520O%2528N%2529%2520%2528usually%252030%2525%2520of%2520all%2520keys%2529%2520data%2520for%250Aaccurate%2520retrieval%252C%2520which%2520fails%2520to%2520exploit%2520the%2520high%2520sparsity.%250ARetrievalAttention%2520first%2520identifies%2520the%2520OOD%2520challenge%2520of%2520ANNS-based%2520attention%252C%250Aand%2520addresses%2520it%2520via%2520an%2520attention-aware%2520vector%2520search%2520algorithm%2520that%2520can%2520adapt%250Ato%2520queries%2520and%2520only%2520access%25201--3%2525%2520of%2520data%252C%2520thus%2520achieving%2520a%2520sub-linear%2520time%250Acomplexity.%2520RetrievalAttention%2520greatly%2520reduces%2520the%2520inference%2520cost%2520of%250Along-context%2520LLM%2520with%2520much%2520lower%2520GPU%2520memory%2520requirements%2520while%2520maintaining%2520the%250Amodel%2520accuracy.%2520Especially%252C%2520RetrievalAttention%2520only%2520needs%252016GB%2520GPU%2520memory%2520for%250Aserving%2520128K%2520tokens%2520in%2520LLMs%2520with%25208B%2520parameters%252C%2520which%2520is%2520capable%2520of%2520generating%250Aone%2520token%2520in%25200.188%2520seconds%2520on%2520a%2520single%2520NVIDIA%2520RTX4090%2520%252824GB%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RetrievalAttention%3A%20Accelerating%20Long-Context%20LLM%20Inference%20via%20Vector%0A%20%20Retrieval&entry.906535625=Di%20Liu%20and%20Meng%20Chen%20and%20Baotong%20Lu%20and%20Huiqiang%20Jiang%20and%20Zhenhua%20Han%20and%20Qianxi%20Zhang%20and%20Qi%20Chen%20and%20Chengruidong%20Zhang%20and%20Bailu%20Ding%20and%20Kai%20Zhang%20and%20Chen%20Chen%20and%20Fan%20Yang%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20Transformer-based%20large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20important%0Ain%20various%20domains.%20However%2C%20the%20quadratic%20time%20complexity%20of%20attention%0Aoperation%20poses%20a%20significant%20challenge%20for%20scaling%20to%20longer%20contexts%20due%20to%0Athe%20extremely%20high%20inference%20latency%20and%20GPU%20memory%20consumption%20for%20caching%0Akey-value%20%28KV%29%20vectors.%20This%20paper%20proposes%20RetrievalAttention%2C%20a%20training-free%0Aapproach%20to%20accelerate%20attention%20computation.%20To%20leverage%20the%20dynamic%20sparse%0Aproperty%20of%20attention%2C%20RetrievalAttention%20builds%20approximate%20nearest%20neighbor%0Asearch%20%28ANNS%29%20indexes%20upon%20KV%20vectors%20in%20CPU%20memory%20and%20retrieves%20the%20most%0Arelevant%20ones%20via%20vector%20search%20during%20generation.%20Due%20to%20the%0Aout-of-distribution%20%28OOD%29%20between%20query%20vectors%20and%20key%20vectors%2C%20off-the-shelf%0AANNS%20indexes%20still%20need%20to%20scan%20O%28N%29%20%28usually%2030%25%20of%20all%20keys%29%20data%20for%0Aaccurate%20retrieval%2C%20which%20fails%20to%20exploit%20the%20high%20sparsity.%0ARetrievalAttention%20first%20identifies%20the%20OOD%20challenge%20of%20ANNS-based%20attention%2C%0Aand%20addresses%20it%20via%20an%20attention-aware%20vector%20search%20algorithm%20that%20can%20adapt%0Ato%20queries%20and%20only%20access%201--3%25%20of%20data%2C%20thus%20achieving%20a%20sub-linear%20time%0Acomplexity.%20RetrievalAttention%20greatly%20reduces%20the%20inference%20cost%20of%0Along-context%20LLM%20with%20much%20lower%20GPU%20memory%20requirements%20while%20maintaining%20the%0Amodel%20accuracy.%20Especially%2C%20RetrievalAttention%20only%20needs%2016GB%20GPU%20memory%20for%0Aserving%20128K%20tokens%20in%20LLMs%20with%208B%20parameters%2C%20which%20is%20capable%20of%20generating%0Aone%20token%20in%200.188%20seconds%20on%20a%20single%20NVIDIA%20RTX4090%20%2824GB%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10516v1&entry.124074799=Read"},
{"title": "TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based\n  Clustering", "author": "Rasoul Jafari Gohari and Laya Aliahmadipour and Ezat Valipour", "abstract": "  The world of Machine Learning (ML) has witnessed rapid changes in terms of\nnew models and ways to process users data. The majority of work that has been\ndone is focused on Deep Learning (DL) based approaches. However, with the\nemergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there\nis growing interest in exploring alternative approaches that may offer unique\nadvantages in certain domains or applications. One of these domains is\nFederated Learning (FL), in which users privacy is of utmost importance. Due to\nits novelty, FL has seen a surge in the incorporation of personalization\ntechniques to enhance model accuracy while maintaining user privacy under\npersonalized conditions. In this work, we propose a novel approach dubbed TPFL:\nTsetlin-Personalized Federated Learning, in which models are grouped into\nclusters based on their confidence towards a specific class. In this way,\nclustering can benefit from two key advantages. Firstly, clients share only\nwhat they are confident about, resulting in the elimination of wrongful weight\naggregation among clients whose data for a specific class may have not been\nenough during the training. This phenomenon is prevalent when the data are\nnon-Independent and Identically Distributed (non-IID). Secondly, by sharing\nonly weights towards a specific class, communication cost is substantially\nreduced, making TPLF efficient in terms of both accuracy and communication\ncost. The results of TPFL demonstrated the highest accuracy on three different\ndatasets; namely MNIST, FashionMNIST and FEMNIST.\n", "link": "http://arxiv.org/abs/2409.10392v1", "date": "2024-09-16", "relevancy": 2.0256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPFL%3A%20Tsetlin-Personalized%20Federated%20Learning%20with%20Confidence-Based%0A%20%20Clustering&body=Title%3A%20TPFL%3A%20Tsetlin-Personalized%20Federated%20Learning%20with%20Confidence-Based%0A%20%20Clustering%0AAuthor%3A%20Rasoul%20Jafari%20Gohari%20and%20Laya%20Aliahmadipour%20and%20Ezat%20Valipour%0AAbstract%3A%20%20%20The%20world%20of%20Machine%20Learning%20%28ML%29%20has%20witnessed%20rapid%20changes%20in%20terms%20of%0Anew%20models%20and%20ways%20to%20process%20users%20data.%20The%20majority%20of%20work%20that%20has%20been%0Adone%20is%20focused%20on%20Deep%20Learning%20%28DL%29%20based%20approaches.%20However%2C%20with%20the%0Aemergence%20of%20new%20algorithms%20such%20as%20the%20Tsetlin%20Machine%20%28TM%29%20algorithm%2C%20there%0Ais%20growing%20interest%20in%20exploring%20alternative%20approaches%20that%20may%20offer%20unique%0Aadvantages%20in%20certain%20domains%20or%20applications.%20One%20of%20these%20domains%20is%0AFederated%20Learning%20%28FL%29%2C%20in%20which%20users%20privacy%20is%20of%20utmost%20importance.%20Due%20to%0Aits%20novelty%2C%20FL%20has%20seen%20a%20surge%20in%20the%20incorporation%20of%20personalization%0Atechniques%20to%20enhance%20model%20accuracy%20while%20maintaining%20user%20privacy%20under%0Apersonalized%20conditions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20dubbed%20TPFL%3A%0ATsetlin-Personalized%20Federated%20Learning%2C%20in%20which%20models%20are%20grouped%20into%0Aclusters%20based%20on%20their%20confidence%20towards%20a%20specific%20class.%20In%20this%20way%2C%0Aclustering%20can%20benefit%20from%20two%20key%20advantages.%20Firstly%2C%20clients%20share%20only%0Awhat%20they%20are%20confident%20about%2C%20resulting%20in%20the%20elimination%20of%20wrongful%20weight%0Aaggregation%20among%20clients%20whose%20data%20for%20a%20specific%20class%20may%20have%20not%20been%0Aenough%20during%20the%20training.%20This%20phenomenon%20is%20prevalent%20when%20the%20data%20are%0Anon-Independent%20and%20Identically%20Distributed%20%28non-IID%29.%20Secondly%2C%20by%20sharing%0Aonly%20weights%20towards%20a%20specific%20class%2C%20communication%20cost%20is%20substantially%0Areduced%2C%20making%20TPLF%20efficient%20in%20terms%20of%20both%20accuracy%20and%20communication%0Acost.%20The%20results%20of%20TPFL%20demonstrated%20the%20highest%20accuracy%20on%20three%20different%0Adatasets%3B%20namely%20MNIST%2C%20FashionMNIST%20and%20FEMNIST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPFL%253A%2520Tsetlin-Personalized%2520Federated%2520Learning%2520with%2520Confidence-Based%250A%2520%2520Clustering%26entry.906535625%3DRasoul%2520Jafari%2520Gohari%2520and%2520Laya%2520Aliahmadipour%2520and%2520Ezat%2520Valipour%26entry.1292438233%3D%2520%2520The%2520world%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520has%2520witnessed%2520rapid%2520changes%2520in%2520terms%2520of%250Anew%2520models%2520and%2520ways%2520to%2520process%2520users%2520data.%2520The%2520majority%2520of%2520work%2520that%2520has%2520been%250Adone%2520is%2520focused%2520on%2520Deep%2520Learning%2520%2528DL%2529%2520based%2520approaches.%2520However%252C%2520with%2520the%250Aemergence%2520of%2520new%2520algorithms%2520such%2520as%2520the%2520Tsetlin%2520Machine%2520%2528TM%2529%2520algorithm%252C%2520there%250Ais%2520growing%2520interest%2520in%2520exploring%2520alternative%2520approaches%2520that%2520may%2520offer%2520unique%250Aadvantages%2520in%2520certain%2520domains%2520or%2520applications.%2520One%2520of%2520these%2520domains%2520is%250AFederated%2520Learning%2520%2528FL%2529%252C%2520in%2520which%2520users%2520privacy%2520is%2520of%2520utmost%2520importance.%2520Due%2520to%250Aits%2520novelty%252C%2520FL%2520has%2520seen%2520a%2520surge%2520in%2520the%2520incorporation%2520of%2520personalization%250Atechniques%2520to%2520enhance%2520model%2520accuracy%2520while%2520maintaining%2520user%2520privacy%2520under%250Apersonalized%2520conditions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520dubbed%2520TPFL%253A%250ATsetlin-Personalized%2520Federated%2520Learning%252C%2520in%2520which%2520models%2520are%2520grouped%2520into%250Aclusters%2520based%2520on%2520their%2520confidence%2520towards%2520a%2520specific%2520class.%2520In%2520this%2520way%252C%250Aclustering%2520can%2520benefit%2520from%2520two%2520key%2520advantages.%2520Firstly%252C%2520clients%2520share%2520only%250Awhat%2520they%2520are%2520confident%2520about%252C%2520resulting%2520in%2520the%2520elimination%2520of%2520wrongful%2520weight%250Aaggregation%2520among%2520clients%2520whose%2520data%2520for%2520a%2520specific%2520class%2520may%2520have%2520not%2520been%250Aenough%2520during%2520the%2520training.%2520This%2520phenomenon%2520is%2520prevalent%2520when%2520the%2520data%2520are%250Anon-Independent%2520and%2520Identically%2520Distributed%2520%2528non-IID%2529.%2520Secondly%252C%2520by%2520sharing%250Aonly%2520weights%2520towards%2520a%2520specific%2520class%252C%2520communication%2520cost%2520is%2520substantially%250Areduced%252C%2520making%2520TPLF%2520efficient%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520communication%250Acost.%2520The%2520results%2520of%2520TPFL%2520demonstrated%2520the%2520highest%2520accuracy%2520on%2520three%2520different%250Adatasets%253B%2520namely%2520MNIST%252C%2520FashionMNIST%2520and%2520FEMNIST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPFL%3A%20Tsetlin-Personalized%20Federated%20Learning%20with%20Confidence-Based%0A%20%20Clustering&entry.906535625=Rasoul%20Jafari%20Gohari%20and%20Laya%20Aliahmadipour%20and%20Ezat%20Valipour&entry.1292438233=%20%20The%20world%20of%20Machine%20Learning%20%28ML%29%20has%20witnessed%20rapid%20changes%20in%20terms%20of%0Anew%20models%20and%20ways%20to%20process%20users%20data.%20The%20majority%20of%20work%20that%20has%20been%0Adone%20is%20focused%20on%20Deep%20Learning%20%28DL%29%20based%20approaches.%20However%2C%20with%20the%0Aemergence%20of%20new%20algorithms%20such%20as%20the%20Tsetlin%20Machine%20%28TM%29%20algorithm%2C%20there%0Ais%20growing%20interest%20in%20exploring%20alternative%20approaches%20that%20may%20offer%20unique%0Aadvantages%20in%20certain%20domains%20or%20applications.%20One%20of%20these%20domains%20is%0AFederated%20Learning%20%28FL%29%2C%20in%20which%20users%20privacy%20is%20of%20utmost%20importance.%20Due%20to%0Aits%20novelty%2C%20FL%20has%20seen%20a%20surge%20in%20the%20incorporation%20of%20personalization%0Atechniques%20to%20enhance%20model%20accuracy%20while%20maintaining%20user%20privacy%20under%0Apersonalized%20conditions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20dubbed%20TPFL%3A%0ATsetlin-Personalized%20Federated%20Learning%2C%20in%20which%20models%20are%20grouped%20into%0Aclusters%20based%20on%20their%20confidence%20towards%20a%20specific%20class.%20In%20this%20way%2C%0Aclustering%20can%20benefit%20from%20two%20key%20advantages.%20Firstly%2C%20clients%20share%20only%0Awhat%20they%20are%20confident%20about%2C%20resulting%20in%20the%20elimination%20of%20wrongful%20weight%0Aaggregation%20among%20clients%20whose%20data%20for%20a%20specific%20class%20may%20have%20not%20been%0Aenough%20during%20the%20training.%20This%20phenomenon%20is%20prevalent%20when%20the%20data%20are%0Anon-Independent%20and%20Identically%20Distributed%20%28non-IID%29.%20Secondly%2C%20by%20sharing%0Aonly%20weights%20towards%20a%20specific%20class%2C%20communication%20cost%20is%20substantially%0Areduced%2C%20making%20TPLF%20efficient%20in%20terms%20of%20both%20accuracy%20and%20communication%0Acost.%20The%20results%20of%20TPFL%20demonstrated%20the%20highest%20accuracy%20on%20three%20different%0Adatasets%3B%20namely%20MNIST%2C%20FashionMNIST%20and%20FEMNIST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10392v1&entry.124074799=Read"},
{"title": "MGSA: Multi-granularity Graph Structure Attention for Knowledge\n  Graph-to-Text Generation", "author": "Shanshan Wang and Chun Zhang and Ning Zhang", "abstract": "  The Knowledge Graph-to-Text Generation task aims to convert structured\nknowledge graphs into coherent and human-readable natural language text. Recent\nefforts in this field have focused on enhancing pre-trained language models\n(PLMs) by incorporating graph structure information to capture the intricate\nstructure details of knowledge graphs. However, most of these approaches tend\nto capture only single-granularity structure information, concentrating either\non the relationships between entities within the original graph or on the\nrelationships between words within the same entity or across different\nentities. This narrow focus results in a significant limitation: models that\nconcentrate solely on entity-level structure fail to capture the nuanced\nsemantic relationships between words, while those that focus only on word-level\nstructure overlook the broader relationships between original entire entities.\nTo overcome these limitations, this paper introduces the Multi-granularity\nGraph Structure Attention (MGSA), which is based on PLMs. The encoder of the\nmodel architecture features an entity-level structure encoding module, a\nword-level structure encoding module, and an aggregation module that\nsynthesizes information from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture both entity-level\nand word-level structure information, providing a more comprehensive\nunderstanding of the knowledge graph's structure information, thereby\nsignificantly improving the quality of the generated text. We conducted\nextensive evaluations of the MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative, where it consistently\noutperformed models that rely solely on single-granularity structure\ninformation, demonstrating the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2409.10294v1", "date": "2024-09-16", "relevancy": 2.0242, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5163}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5013}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGSA%3A%20Multi-granularity%20Graph%20Structure%20Attention%20for%20Knowledge%0A%20%20Graph-to-Text%20Generation&body=Title%3A%20MGSA%3A%20Multi-granularity%20Graph%20Structure%20Attention%20for%20Knowledge%0A%20%20Graph-to-Text%20Generation%0AAuthor%3A%20Shanshan%20Wang%20and%20Chun%20Zhang%20and%20Ning%20Zhang%0AAbstract%3A%20%20%20The%20Knowledge%20Graph-to-Text%20Generation%20task%20aims%20to%20convert%20structured%0Aknowledge%20graphs%20into%20coherent%20and%20human-readable%20natural%20language%20text.%20Recent%0Aefforts%20in%20this%20field%20have%20focused%20on%20enhancing%20pre-trained%20language%20models%0A%28PLMs%29%20by%20incorporating%20graph%20structure%20information%20to%20capture%20the%20intricate%0Astructure%20details%20of%20knowledge%20graphs.%20However%2C%20most%20of%20these%20approaches%20tend%0Ato%20capture%20only%20single-granularity%20structure%20information%2C%20concentrating%20either%0Aon%20the%20relationships%20between%20entities%20within%20the%20original%20graph%20or%20on%20the%0Arelationships%20between%20words%20within%20the%20same%20entity%20or%20across%20different%0Aentities.%20This%20narrow%20focus%20results%20in%20a%20significant%20limitation%3A%20models%20that%0Aconcentrate%20solely%20on%20entity-level%20structure%20fail%20to%20capture%20the%20nuanced%0Asemantic%20relationships%20between%20words%2C%20while%20those%20that%20focus%20only%20on%20word-level%0Astructure%20overlook%20the%20broader%20relationships%20between%20original%20entire%20entities.%0ATo%20overcome%20these%20limitations%2C%20this%20paper%20introduces%20the%20Multi-granularity%0AGraph%20Structure%20Attention%20%28MGSA%29%2C%20which%20is%20based%20on%20PLMs.%20The%20encoder%20of%20the%0Amodel%20architecture%20features%20an%20entity-level%20structure%20encoding%20module%2C%20a%0Aword-level%20structure%20encoding%20module%2C%20and%20an%20aggregation%20module%20that%0Asynthesizes%20information%20from%20both%20structure.%20This%20multi-granularity%20structure%0Aencoding%20approach%20allows%20the%20model%20to%20simultaneously%20capture%20both%20entity-level%0Aand%20word-level%20structure%20information%2C%20providing%20a%20more%20comprehensive%0Aunderstanding%20of%20the%20knowledge%20graph%27s%20structure%20information%2C%20thereby%0Asignificantly%20improving%20the%20quality%20of%20the%20generated%20text.%20We%20conducted%0Aextensive%20evaluations%20of%20the%20MGSA%20model%20using%20two%20widely%20recognized%20KG-to-Text%0AGeneration%20benchmark%20datasets%2C%20WebNLG%20and%20EventNarrative%2C%20where%20it%20consistently%0Aoutperformed%20models%20that%20rely%20solely%20on%20single-granularity%20structure%0Ainformation%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGSA%253A%2520Multi-granularity%2520Graph%2520Structure%2520Attention%2520for%2520Knowledge%250A%2520%2520Graph-to-Text%2520Generation%26entry.906535625%3DShanshan%2520Wang%2520and%2520Chun%2520Zhang%2520and%2520Ning%2520Zhang%26entry.1292438233%3D%2520%2520The%2520Knowledge%2520Graph-to-Text%2520Generation%2520task%2520aims%2520to%2520convert%2520structured%250Aknowledge%2520graphs%2520into%2520coherent%2520and%2520human-readable%2520natural%2520language%2520text.%2520Recent%250Aefforts%2520in%2520this%2520field%2520have%2520focused%2520on%2520enhancing%2520pre-trained%2520language%2520models%250A%2528PLMs%2529%2520by%2520incorporating%2520graph%2520structure%2520information%2520to%2520capture%2520the%2520intricate%250Astructure%2520details%2520of%2520knowledge%2520graphs.%2520However%252C%2520most%2520of%2520these%2520approaches%2520tend%250Ato%2520capture%2520only%2520single-granularity%2520structure%2520information%252C%2520concentrating%2520either%250Aon%2520the%2520relationships%2520between%2520entities%2520within%2520the%2520original%2520graph%2520or%2520on%2520the%250Arelationships%2520between%2520words%2520within%2520the%2520same%2520entity%2520or%2520across%2520different%250Aentities.%2520This%2520narrow%2520focus%2520results%2520in%2520a%2520significant%2520limitation%253A%2520models%2520that%250Aconcentrate%2520solely%2520on%2520entity-level%2520structure%2520fail%2520to%2520capture%2520the%2520nuanced%250Asemantic%2520relationships%2520between%2520words%252C%2520while%2520those%2520that%2520focus%2520only%2520on%2520word-level%250Astructure%2520overlook%2520the%2520broader%2520relationships%2520between%2520original%2520entire%2520entities.%250ATo%2520overcome%2520these%2520limitations%252C%2520this%2520paper%2520introduces%2520the%2520Multi-granularity%250AGraph%2520Structure%2520Attention%2520%2528MGSA%2529%252C%2520which%2520is%2520based%2520on%2520PLMs.%2520The%2520encoder%2520of%2520the%250Amodel%2520architecture%2520features%2520an%2520entity-level%2520structure%2520encoding%2520module%252C%2520a%250Aword-level%2520structure%2520encoding%2520module%252C%2520and%2520an%2520aggregation%2520module%2520that%250Asynthesizes%2520information%2520from%2520both%2520structure.%2520This%2520multi-granularity%2520structure%250Aencoding%2520approach%2520allows%2520the%2520model%2520to%2520simultaneously%2520capture%2520both%2520entity-level%250Aand%2520word-level%2520structure%2520information%252C%2520providing%2520a%2520more%2520comprehensive%250Aunderstanding%2520of%2520the%2520knowledge%2520graph%2527s%2520structure%2520information%252C%2520thereby%250Asignificantly%2520improving%2520the%2520quality%2520of%2520the%2520generated%2520text.%2520We%2520conducted%250Aextensive%2520evaluations%2520of%2520the%2520MGSA%2520model%2520using%2520two%2520widely%2520recognized%2520KG-to-Text%250AGeneration%2520benchmark%2520datasets%252C%2520WebNLG%2520and%2520EventNarrative%252C%2520where%2520it%2520consistently%250Aoutperformed%2520models%2520that%2520rely%2520solely%2520on%2520single-granularity%2520structure%250Ainformation%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGSA%3A%20Multi-granularity%20Graph%20Structure%20Attention%20for%20Knowledge%0A%20%20Graph-to-Text%20Generation&entry.906535625=Shanshan%20Wang%20and%20Chun%20Zhang%20and%20Ning%20Zhang&entry.1292438233=%20%20The%20Knowledge%20Graph-to-Text%20Generation%20task%20aims%20to%20convert%20structured%0Aknowledge%20graphs%20into%20coherent%20and%20human-readable%20natural%20language%20text.%20Recent%0Aefforts%20in%20this%20field%20have%20focused%20on%20enhancing%20pre-trained%20language%20models%0A%28PLMs%29%20by%20incorporating%20graph%20structure%20information%20to%20capture%20the%20intricate%0Astructure%20details%20of%20knowledge%20graphs.%20However%2C%20most%20of%20these%20approaches%20tend%0Ato%20capture%20only%20single-granularity%20structure%20information%2C%20concentrating%20either%0Aon%20the%20relationships%20between%20entities%20within%20the%20original%20graph%20or%20on%20the%0Arelationships%20between%20words%20within%20the%20same%20entity%20or%20across%20different%0Aentities.%20This%20narrow%20focus%20results%20in%20a%20significant%20limitation%3A%20models%20that%0Aconcentrate%20solely%20on%20entity-level%20structure%20fail%20to%20capture%20the%20nuanced%0Asemantic%20relationships%20between%20words%2C%20while%20those%20that%20focus%20only%20on%20word-level%0Astructure%20overlook%20the%20broader%20relationships%20between%20original%20entire%20entities.%0ATo%20overcome%20these%20limitations%2C%20this%20paper%20introduces%20the%20Multi-granularity%0AGraph%20Structure%20Attention%20%28MGSA%29%2C%20which%20is%20based%20on%20PLMs.%20The%20encoder%20of%20the%0Amodel%20architecture%20features%20an%20entity-level%20structure%20encoding%20module%2C%20a%0Aword-level%20structure%20encoding%20module%2C%20and%20an%20aggregation%20module%20that%0Asynthesizes%20information%20from%20both%20structure.%20This%20multi-granularity%20structure%0Aencoding%20approach%20allows%20the%20model%20to%20simultaneously%20capture%20both%20entity-level%0Aand%20word-level%20structure%20information%2C%20providing%20a%20more%20comprehensive%0Aunderstanding%20of%20the%20knowledge%20graph%27s%20structure%20information%2C%20thereby%0Asignificantly%20improving%20the%20quality%20of%20the%20generated%20text.%20We%20conducted%0Aextensive%20evaluations%20of%20the%20MGSA%20model%20using%20two%20widely%20recognized%20KG-to-Text%0AGeneration%20benchmark%20datasets%2C%20WebNLG%20and%20EventNarrative%2C%20where%20it%20consistently%0Aoutperformed%20models%20that%20rely%20solely%20on%20single-granularity%20structure%0Ainformation%2C%20demonstrating%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10294v1&entry.124074799=Read"},
{"title": "Relative Positioning for Aerial Robot Path Planning in GPS Denied\n  Environment", "author": "Farzad Sanati", "abstract": "  One of the most useful applications of intelligent aerial robots sometimes\ncalled Unmanned Aerial Vehicles (UAV) in Australia is known to be in bushfire\nmonitoring and prediction operations. A swarm of autonomous drones/UAVs\nprogrammed to work in real-time observing the fire parameters using their\nonboard sensors would be valuable in reducing the life-threatening impact of\nthat fire. However autonomous UAVs face serious challenges in their positioning\nand navigation in critical bushfire conditions such as remoteness and severe\nweather conditions where GPS signals could also be unreliable. This paper\ntackles one of the most important factors in autonomous UAV navigation, namely\nInitial Positioning sometimes called Localisation. The solution provided by\nthis paper will enable a team of autonomous UAVs to establish a relative\nposition to their base of operation to be able to commence a team search and\nreconnaissance in a bushfire-affected area and find their way back to their\nbase without the help of GPS signals.\n", "link": "http://arxiv.org/abs/2409.10193v1", "date": "2024-09-16", "relevancy": 2.0207, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Positioning%20for%20Aerial%20Robot%20Path%20Planning%20in%20GPS%20Denied%0A%20%20Environment&body=Title%3A%20Relative%20Positioning%20for%20Aerial%20Robot%20Path%20Planning%20in%20GPS%20Denied%0A%20%20Environment%0AAuthor%3A%20Farzad%20Sanati%0AAbstract%3A%20%20%20One%20of%20the%20most%20useful%20applications%20of%20intelligent%20aerial%20robots%20sometimes%0Acalled%20Unmanned%20Aerial%20Vehicles%20%28UAV%29%20in%20Australia%20is%20known%20to%20be%20in%20bushfire%0Amonitoring%20and%20prediction%20operations.%20A%20swarm%20of%20autonomous%20drones/UAVs%0Aprogrammed%20to%20work%20in%20real-time%20observing%20the%20fire%20parameters%20using%20their%0Aonboard%20sensors%20would%20be%20valuable%20in%20reducing%20the%20life-threatening%20impact%20of%0Athat%20fire.%20However%20autonomous%20UAVs%20face%20serious%20challenges%20in%20their%20positioning%0Aand%20navigation%20in%20critical%20bushfire%20conditions%20such%20as%20remoteness%20and%20severe%0Aweather%20conditions%20where%20GPS%20signals%20could%20also%20be%20unreliable.%20This%20paper%0Atackles%20one%20of%20the%20most%20important%20factors%20in%20autonomous%20UAV%20navigation%2C%20namely%0AInitial%20Positioning%20sometimes%20called%20Localisation.%20The%20solution%20provided%20by%0Athis%20paper%20will%20enable%20a%20team%20of%20autonomous%20UAVs%20to%20establish%20a%20relative%0Aposition%20to%20their%20base%20of%20operation%20to%20be%20able%20to%20commence%20a%20team%20search%20and%0Areconnaissance%20in%20a%20bushfire-affected%20area%20and%20find%20their%20way%20back%20to%20their%0Abase%20without%20the%20help%20of%20GPS%20signals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Positioning%2520for%2520Aerial%2520Robot%2520Path%2520Planning%2520in%2520GPS%2520Denied%250A%2520%2520Environment%26entry.906535625%3DFarzad%2520Sanati%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520useful%2520applications%2520of%2520intelligent%2520aerial%2520robots%2520sometimes%250Acalled%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAV%2529%2520in%2520Australia%2520is%2520known%2520to%2520be%2520in%2520bushfire%250Amonitoring%2520and%2520prediction%2520operations.%2520A%2520swarm%2520of%2520autonomous%2520drones/UAVs%250Aprogrammed%2520to%2520work%2520in%2520real-time%2520observing%2520the%2520fire%2520parameters%2520using%2520their%250Aonboard%2520sensors%2520would%2520be%2520valuable%2520in%2520reducing%2520the%2520life-threatening%2520impact%2520of%250Athat%2520fire.%2520However%2520autonomous%2520UAVs%2520face%2520serious%2520challenges%2520in%2520their%2520positioning%250Aand%2520navigation%2520in%2520critical%2520bushfire%2520conditions%2520such%2520as%2520remoteness%2520and%2520severe%250Aweather%2520conditions%2520where%2520GPS%2520signals%2520could%2520also%2520be%2520unreliable.%2520This%2520paper%250Atackles%2520one%2520of%2520the%2520most%2520important%2520factors%2520in%2520autonomous%2520UAV%2520navigation%252C%2520namely%250AInitial%2520Positioning%2520sometimes%2520called%2520Localisation.%2520The%2520solution%2520provided%2520by%250Athis%2520paper%2520will%2520enable%2520a%2520team%2520of%2520autonomous%2520UAVs%2520to%2520establish%2520a%2520relative%250Aposition%2520to%2520their%2520base%2520of%2520operation%2520to%2520be%2520able%2520to%2520commence%2520a%2520team%2520search%2520and%250Areconnaissance%2520in%2520a%2520bushfire-affected%2520area%2520and%2520find%2520their%2520way%2520back%2520to%2520their%250Abase%2520without%2520the%2520help%2520of%2520GPS%2520signals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Positioning%20for%20Aerial%20Robot%20Path%20Planning%20in%20GPS%20Denied%0A%20%20Environment&entry.906535625=Farzad%20Sanati&entry.1292438233=%20%20One%20of%20the%20most%20useful%20applications%20of%20intelligent%20aerial%20robots%20sometimes%0Acalled%20Unmanned%20Aerial%20Vehicles%20%28UAV%29%20in%20Australia%20is%20known%20to%20be%20in%20bushfire%0Amonitoring%20and%20prediction%20operations.%20A%20swarm%20of%20autonomous%20drones/UAVs%0Aprogrammed%20to%20work%20in%20real-time%20observing%20the%20fire%20parameters%20using%20their%0Aonboard%20sensors%20would%20be%20valuable%20in%20reducing%20the%20life-threatening%20impact%20of%0Athat%20fire.%20However%20autonomous%20UAVs%20face%20serious%20challenges%20in%20their%20positioning%0Aand%20navigation%20in%20critical%20bushfire%20conditions%20such%20as%20remoteness%20and%20severe%0Aweather%20conditions%20where%20GPS%20signals%20could%20also%20be%20unreliable.%20This%20paper%0Atackles%20one%20of%20the%20most%20important%20factors%20in%20autonomous%20UAV%20navigation%2C%20namely%0AInitial%20Positioning%20sometimes%20called%20Localisation.%20The%20solution%20provided%20by%0Athis%20paper%20will%20enable%20a%20team%20of%20autonomous%20UAVs%20to%20establish%20a%20relative%0Aposition%20to%20their%20base%20of%20operation%20to%20be%20able%20to%20commence%20a%20team%20search%20and%0Areconnaissance%20in%20a%20bushfire-affected%20area%20and%20find%20their%20way%20back%20to%20their%0Abase%20without%20the%20help%20of%20GPS%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10193v1&entry.124074799=Read"},
{"title": "Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube\n  Search for COVID-19 Misinformation between the United States and South Africa", "author": "Hayoung Jung and Prerna Juneja and Tanushree Mitra", "abstract": "  Despite being an integral tool for finding health-related information online,\nYouTube has faced criticism for disseminating COVID-19 misinformation globally\nto its users. Yet, prior audit studies have predominantly investigated YouTube\nwithin the Global North contexts, often overlooking the Global South. To\naddress this gap, we conducted a comprehensive 10-day geolocation-based audit\non YouTube to compare the prevalence of COVID-19 misinformation in search\nresults between the United States (US) and South Africa (SA), the countries\nheavily affected by the pandemic in the Global North and the Global South,\nrespectively. For each country, we selected 3 geolocations and placed\nsock-puppets, or bots emulating \"real\" users, that collected search results for\n48 search queries sorted by 4 search filters for 10 days, yielding a dataset of\n915K results. We found that 31.55% of the top-10 search results contained\nCOVID-19 misinformation. Among the top-10 search results, bots in SA faced\nsignificantly more misinformative search results than their US counterparts.\nOverall, our study highlights the contrasting algorithmic behaviors of YouTube\nsearch between two countries, underscoring the need for the platform to\nregulate algorithmic behavior consistently across different regions of the\nGlobe.\n", "link": "http://arxiv.org/abs/2409.10168v1", "date": "2024-09-16", "relevancy": 1.1461, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3993}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3613}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Behaviors%20Across%20Regions%3A%20A%20Geolocation%20Audit%20of%20YouTube%0A%20%20Search%20for%20COVID-19%20Misinformation%20between%20the%20United%20States%20and%20South%20Africa&body=Title%3A%20Algorithmic%20Behaviors%20Across%20Regions%3A%20A%20Geolocation%20Audit%20of%20YouTube%0A%20%20Search%20for%20COVID-19%20Misinformation%20between%20the%20United%20States%20and%20South%20Africa%0AAuthor%3A%20Hayoung%20Jung%20and%20Prerna%20Juneja%20and%20Tanushree%20Mitra%0AAbstract%3A%20%20%20Despite%20being%20an%20integral%20tool%20for%20finding%20health-related%20information%20online%2C%0AYouTube%20has%20faced%20criticism%20for%20disseminating%20COVID-19%20misinformation%20globally%0Ato%20its%20users.%20Yet%2C%20prior%20audit%20studies%20have%20predominantly%20investigated%20YouTube%0Awithin%20the%20Global%20North%20contexts%2C%20often%20overlooking%20the%20Global%20South.%20To%0Aaddress%20this%20gap%2C%20we%20conducted%20a%20comprehensive%2010-day%20geolocation-based%20audit%0Aon%20YouTube%20to%20compare%20the%20prevalence%20of%20COVID-19%20misinformation%20in%20search%0Aresults%20between%20the%20United%20States%20%28US%29%20and%20South%20Africa%20%28SA%29%2C%20the%20countries%0Aheavily%20affected%20by%20the%20pandemic%20in%20the%20Global%20North%20and%20the%20Global%20South%2C%0Arespectively.%20For%20each%20country%2C%20we%20selected%203%20geolocations%20and%20placed%0Asock-puppets%2C%20or%20bots%20emulating%20%22real%22%20users%2C%20that%20collected%20search%20results%20for%0A48%20search%20queries%20sorted%20by%204%20search%20filters%20for%2010%20days%2C%20yielding%20a%20dataset%20of%0A915K%20results.%20We%20found%20that%2031.55%25%20of%20the%20top-10%20search%20results%20contained%0ACOVID-19%20misinformation.%20Among%20the%20top-10%20search%20results%2C%20bots%20in%20SA%20faced%0Asignificantly%20more%20misinformative%20search%20results%20than%20their%20US%20counterparts.%0AOverall%2C%20our%20study%20highlights%20the%20contrasting%20algorithmic%20behaviors%20of%20YouTube%0Asearch%20between%20two%20countries%2C%20underscoring%20the%20need%20for%20the%20platform%20to%0Aregulate%20algorithmic%20behavior%20consistently%20across%20different%20regions%20of%20the%0AGlobe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Behaviors%2520Across%2520Regions%253A%2520A%2520Geolocation%2520Audit%2520of%2520YouTube%250A%2520%2520Search%2520for%2520COVID-19%2520Misinformation%2520between%2520the%2520United%2520States%2520and%2520South%2520Africa%26entry.906535625%3DHayoung%2520Jung%2520and%2520Prerna%2520Juneja%2520and%2520Tanushree%2520Mitra%26entry.1292438233%3D%2520%2520Despite%2520being%2520an%2520integral%2520tool%2520for%2520finding%2520health-related%2520information%2520online%252C%250AYouTube%2520has%2520faced%2520criticism%2520for%2520disseminating%2520COVID-19%2520misinformation%2520globally%250Ato%2520its%2520users.%2520Yet%252C%2520prior%2520audit%2520studies%2520have%2520predominantly%2520investigated%2520YouTube%250Awithin%2520the%2520Global%2520North%2520contexts%252C%2520often%2520overlooking%2520the%2520Global%2520South.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520conducted%2520a%2520comprehensive%252010-day%2520geolocation-based%2520audit%250Aon%2520YouTube%2520to%2520compare%2520the%2520prevalence%2520of%2520COVID-19%2520misinformation%2520in%2520search%250Aresults%2520between%2520the%2520United%2520States%2520%2528US%2529%2520and%2520South%2520Africa%2520%2528SA%2529%252C%2520the%2520countries%250Aheavily%2520affected%2520by%2520the%2520pandemic%2520in%2520the%2520Global%2520North%2520and%2520the%2520Global%2520South%252C%250Arespectively.%2520For%2520each%2520country%252C%2520we%2520selected%25203%2520geolocations%2520and%2520placed%250Asock-puppets%252C%2520or%2520bots%2520emulating%2520%2522real%2522%2520users%252C%2520that%2520collected%2520search%2520results%2520for%250A48%2520search%2520queries%2520sorted%2520by%25204%2520search%2520filters%2520for%252010%2520days%252C%2520yielding%2520a%2520dataset%2520of%250A915K%2520results.%2520We%2520found%2520that%252031.55%2525%2520of%2520the%2520top-10%2520search%2520results%2520contained%250ACOVID-19%2520misinformation.%2520Among%2520the%2520top-10%2520search%2520results%252C%2520bots%2520in%2520SA%2520faced%250Asignificantly%2520more%2520misinformative%2520search%2520results%2520than%2520their%2520US%2520counterparts.%250AOverall%252C%2520our%2520study%2520highlights%2520the%2520contrasting%2520algorithmic%2520behaviors%2520of%2520YouTube%250Asearch%2520between%2520two%2520countries%252C%2520underscoring%2520the%2520need%2520for%2520the%2520platform%2520to%250Aregulate%2520algorithmic%2520behavior%2520consistently%2520across%2520different%2520regions%2520of%2520the%250AGlobe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Behaviors%20Across%20Regions%3A%20A%20Geolocation%20Audit%20of%20YouTube%0A%20%20Search%20for%20COVID-19%20Misinformation%20between%20the%20United%20States%20and%20South%20Africa&entry.906535625=Hayoung%20Jung%20and%20Prerna%20Juneja%20and%20Tanushree%20Mitra&entry.1292438233=%20%20Despite%20being%20an%20integral%20tool%20for%20finding%20health-related%20information%20online%2C%0AYouTube%20has%20faced%20criticism%20for%20disseminating%20COVID-19%20misinformation%20globally%0Ato%20its%20users.%20Yet%2C%20prior%20audit%20studies%20have%20predominantly%20investigated%20YouTube%0Awithin%20the%20Global%20North%20contexts%2C%20often%20overlooking%20the%20Global%20South.%20To%0Aaddress%20this%20gap%2C%20we%20conducted%20a%20comprehensive%2010-day%20geolocation-based%20audit%0Aon%20YouTube%20to%20compare%20the%20prevalence%20of%20COVID-19%20misinformation%20in%20search%0Aresults%20between%20the%20United%20States%20%28US%29%20and%20South%20Africa%20%28SA%29%2C%20the%20countries%0Aheavily%20affected%20by%20the%20pandemic%20in%20the%20Global%20North%20and%20the%20Global%20South%2C%0Arespectively.%20For%20each%20country%2C%20we%20selected%203%20geolocations%20and%20placed%0Asock-puppets%2C%20or%20bots%20emulating%20%22real%22%20users%2C%20that%20collected%20search%20results%20for%0A48%20search%20queries%20sorted%20by%204%20search%20filters%20for%2010%20days%2C%20yielding%20a%20dataset%20of%0A915K%20results.%20We%20found%20that%2031.55%25%20of%20the%20top-10%20search%20results%20contained%0ACOVID-19%20misinformation.%20Among%20the%20top-10%20search%20results%2C%20bots%20in%20SA%20faced%0Asignificantly%20more%20misinformative%20search%20results%20than%20their%20US%20counterparts.%0AOverall%2C%20our%20study%20highlights%20the%20contrasting%20algorithmic%20behaviors%20of%20YouTube%0Asearch%20between%20two%20countries%2C%20underscoring%20the%20need%20for%20the%20platform%20to%0Aregulate%20algorithmic%20behavior%20consistently%20across%20different%20regions%20of%20the%0AGlobe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10168v1&entry.124074799=Read"},
{"title": "On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex\n  Optimization", "author": "Guy Kornowski and Swati Padmanabhan and Ohad Shamir", "abstract": "  We study the oracle complexity of nonsmooth nonconvex optimization, with the\nalgorithm assumed to have access only to local function information. It has\nbeen shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmooth\nLipschitz functions satisfying certain regularity and strictness conditions,\nperturbed gradient descent converges to local minimizers asymptotically.\nMotivated by this result and by other recent algorithmic advances in nonconvex\nnonsmooth optimization concerning Goldstein stationarity, we consider the\nquestion of obtaining a non-asymptotic rate of convergence to local minima for\nthis problem class.\n  We provide the following negative answer to this question: Local algorithms\nacting on regular Lipschitz functions cannot, in the worst case, provide\nmeaningful local guarantees in terms of function value in sub-exponential time,\neven when all near-stationary points are global minima. This sharply contrasts\nwith the smooth setting, for which it is well-known that standard gradient\nmethods can do so in a dimension-independent rate. Our result complements the\nrich body of work in the theoretical computer science literature that provide\nhardness results conditional on conjectures such as $\\mathsf{P}\\neq\\mathsf{NP}$\nor cryptographic assumptions, in that ours holds unconditional of any such\nassumptions.\n", "link": "http://arxiv.org/abs/2409.10323v1", "date": "2024-09-16", "relevancy": 1.7003, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4347}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Hardness%20of%20Meaningful%20Local%20Guarantees%20in%20Nonsmooth%20Nonconvex%0A%20%20Optimization&body=Title%3A%20On%20the%20Hardness%20of%20Meaningful%20Local%20Guarantees%20in%20Nonsmooth%20Nonconvex%0A%20%20Optimization%0AAuthor%3A%20Guy%20Kornowski%20and%20Swati%20Padmanabhan%20and%20Ohad%20Shamir%0AAbstract%3A%20%20%20We%20study%20the%20oracle%20complexity%20of%20nonsmooth%20nonconvex%20optimization%2C%20with%20the%0Aalgorithm%20assumed%20to%20have%20access%20only%20to%20local%20function%20information.%20It%20has%0Abeen%20shown%20by%20Davis%2C%20Drusvyatskiy%2C%20and%20Jiang%20%282023%29%20that%20for%20nonsmooth%0ALipschitz%20functions%20satisfying%20certain%20regularity%20and%20strictness%20conditions%2C%0Aperturbed%20gradient%20descent%20converges%20to%20local%20minimizers%20asymptotically.%0AMotivated%20by%20this%20result%20and%20by%20other%20recent%20algorithmic%20advances%20in%20nonconvex%0Anonsmooth%20optimization%20concerning%20Goldstein%20stationarity%2C%20we%20consider%20the%0Aquestion%20of%20obtaining%20a%20non-asymptotic%20rate%20of%20convergence%20to%20local%20minima%20for%0Athis%20problem%20class.%0A%20%20We%20provide%20the%20following%20negative%20answer%20to%20this%20question%3A%20Local%20algorithms%0Aacting%20on%20regular%20Lipschitz%20functions%20cannot%2C%20in%20the%20worst%20case%2C%20provide%0Ameaningful%20local%20guarantees%20in%20terms%20of%20function%20value%20in%20sub-exponential%20time%2C%0Aeven%20when%20all%20near-stationary%20points%20are%20global%20minima.%20This%20sharply%20contrasts%0Awith%20the%20smooth%20setting%2C%20for%20which%20it%20is%20well-known%20that%20standard%20gradient%0Amethods%20can%20do%20so%20in%20a%20dimension-independent%20rate.%20Our%20result%20complements%20the%0Arich%20body%20of%20work%20in%20the%20theoretical%20computer%20science%20literature%20that%20provide%0Ahardness%20results%20conditional%20on%20conjectures%20such%20as%20%24%5Cmathsf%7BP%7D%5Cneq%5Cmathsf%7BNP%7D%24%0Aor%20cryptographic%20assumptions%2C%20in%20that%20ours%20holds%20unconditional%20of%20any%20such%0Aassumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Hardness%2520of%2520Meaningful%2520Local%2520Guarantees%2520in%2520Nonsmooth%2520Nonconvex%250A%2520%2520Optimization%26entry.906535625%3DGuy%2520Kornowski%2520and%2520Swati%2520Padmanabhan%2520and%2520Ohad%2520Shamir%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520oracle%2520complexity%2520of%2520nonsmooth%2520nonconvex%2520optimization%252C%2520with%2520the%250Aalgorithm%2520assumed%2520to%2520have%2520access%2520only%2520to%2520local%2520function%2520information.%2520It%2520has%250Abeen%2520shown%2520by%2520Davis%252C%2520Drusvyatskiy%252C%2520and%2520Jiang%2520%25282023%2529%2520that%2520for%2520nonsmooth%250ALipschitz%2520functions%2520satisfying%2520certain%2520regularity%2520and%2520strictness%2520conditions%252C%250Aperturbed%2520gradient%2520descent%2520converges%2520to%2520local%2520minimizers%2520asymptotically.%250AMotivated%2520by%2520this%2520result%2520and%2520by%2520other%2520recent%2520algorithmic%2520advances%2520in%2520nonconvex%250Anonsmooth%2520optimization%2520concerning%2520Goldstein%2520stationarity%252C%2520we%2520consider%2520the%250Aquestion%2520of%2520obtaining%2520a%2520non-asymptotic%2520rate%2520of%2520convergence%2520to%2520local%2520minima%2520for%250Athis%2520problem%2520class.%250A%2520%2520We%2520provide%2520the%2520following%2520negative%2520answer%2520to%2520this%2520question%253A%2520Local%2520algorithms%250Aacting%2520on%2520regular%2520Lipschitz%2520functions%2520cannot%252C%2520in%2520the%2520worst%2520case%252C%2520provide%250Ameaningful%2520local%2520guarantees%2520in%2520terms%2520of%2520function%2520value%2520in%2520sub-exponential%2520time%252C%250Aeven%2520when%2520all%2520near-stationary%2520points%2520are%2520global%2520minima.%2520This%2520sharply%2520contrasts%250Awith%2520the%2520smooth%2520setting%252C%2520for%2520which%2520it%2520is%2520well-known%2520that%2520standard%2520gradient%250Amethods%2520can%2520do%2520so%2520in%2520a%2520dimension-independent%2520rate.%2520Our%2520result%2520complements%2520the%250Arich%2520body%2520of%2520work%2520in%2520the%2520theoretical%2520computer%2520science%2520literature%2520that%2520provide%250Ahardness%2520results%2520conditional%2520on%2520conjectures%2520such%2520as%2520%2524%255Cmathsf%257BP%257D%255Cneq%255Cmathsf%257BNP%257D%2524%250Aor%2520cryptographic%2520assumptions%252C%2520in%2520that%2520ours%2520holds%2520unconditional%2520of%2520any%2520such%250Aassumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Hardness%20of%20Meaningful%20Local%20Guarantees%20in%20Nonsmooth%20Nonconvex%0A%20%20Optimization&entry.906535625=Guy%20Kornowski%20and%20Swati%20Padmanabhan%20and%20Ohad%20Shamir&entry.1292438233=%20%20We%20study%20the%20oracle%20complexity%20of%20nonsmooth%20nonconvex%20optimization%2C%20with%20the%0Aalgorithm%20assumed%20to%20have%20access%20only%20to%20local%20function%20information.%20It%20has%0Abeen%20shown%20by%20Davis%2C%20Drusvyatskiy%2C%20and%20Jiang%20%282023%29%20that%20for%20nonsmooth%0ALipschitz%20functions%20satisfying%20certain%20regularity%20and%20strictness%20conditions%2C%0Aperturbed%20gradient%20descent%20converges%20to%20local%20minimizers%20asymptotically.%0AMotivated%20by%20this%20result%20and%20by%20other%20recent%20algorithmic%20advances%20in%20nonconvex%0Anonsmooth%20optimization%20concerning%20Goldstein%20stationarity%2C%20we%20consider%20the%0Aquestion%20of%20obtaining%20a%20non-asymptotic%20rate%20of%20convergence%20to%20local%20minima%20for%0Athis%20problem%20class.%0A%20%20We%20provide%20the%20following%20negative%20answer%20to%20this%20question%3A%20Local%20algorithms%0Aacting%20on%20regular%20Lipschitz%20functions%20cannot%2C%20in%20the%20worst%20case%2C%20provide%0Ameaningful%20local%20guarantees%20in%20terms%20of%20function%20value%20in%20sub-exponential%20time%2C%0Aeven%20when%20all%20near-stationary%20points%20are%20global%20minima.%20This%20sharply%20contrasts%0Awith%20the%20smooth%20setting%2C%20for%20which%20it%20is%20well-known%20that%20standard%20gradient%0Amethods%20can%20do%20so%20in%20a%20dimension-independent%20rate.%20Our%20result%20complements%20the%0Arich%20body%20of%20work%20in%20the%20theoretical%20computer%20science%20literature%20that%20provide%0Ahardness%20results%20conditional%20on%20conjectures%20such%20as%20%24%5Cmathsf%7BP%7D%5Cneq%5Cmathsf%7BNP%7D%24%0Aor%20cryptographic%20assumptions%2C%20in%20that%20ours%20holds%20unconditional%20of%20any%20such%0Aassumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10323v1&entry.124074799=Read"},
{"title": "Simultaneous Position-and-Stiffness Control of Underactuated\n  Antagonistic Tendon-Driven Continuum Robots", "author": "Bowen Yi and Yeman Fan and Dikai Liu and Jose Guadalupe Romero", "abstract": "  Continuum robots have gained widespread popularity due to their inherent\ncompliance and flexibility, particularly their adjustable levels of stiffness\nfor various application scenarios. Despite efforts to dynamic modeling and\ncontrol synthesis over the past decade, few studies have incorporated stiffness\nregulation into their feedback control design; however, this is one of the\ninitial motivations to develop continuum robots. This paper addresses the\ncrucial challenge of controlling both the position and stiffness of\nunderactuated continuum robots actuated by antagonistic tendons. We begin by\npresenting a rigid-link dynamical model that can analyze the open-loop\nstiffening of tendon-driven continuum robots. Based on this model, we propose a\nnovel passivity-based position-and-stiffness controller that adheres to the\nnon-negative tension constraint. Comprehensive experiments on our continuum\nrobot validate the theoretical results and demonstrate the efficacy and\nprecision of this approach.\n", "link": "http://arxiv.org/abs/2306.03865v3", "date": "2024-09-16", "relevancy": 1.4411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5081}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4783}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Position-and-Stiffness%20Control%20of%20Underactuated%0A%20%20Antagonistic%20Tendon-Driven%20Continuum%20Robots&body=Title%3A%20Simultaneous%20Position-and-Stiffness%20Control%20of%20Underactuated%0A%20%20Antagonistic%20Tendon-Driven%20Continuum%20Robots%0AAuthor%3A%20Bowen%20Yi%20and%20Yeman%20Fan%20and%20Dikai%20Liu%20and%20Jose%20Guadalupe%20Romero%0AAbstract%3A%20%20%20Continuum%20robots%20have%20gained%20widespread%20popularity%20due%20to%20their%20inherent%0Acompliance%20and%20flexibility%2C%20particularly%20their%20adjustable%20levels%20of%20stiffness%0Afor%20various%20application%20scenarios.%20Despite%20efforts%20to%20dynamic%20modeling%20and%0Acontrol%20synthesis%20over%20the%20past%20decade%2C%20few%20studies%20have%20incorporated%20stiffness%0Aregulation%20into%20their%20feedback%20control%20design%3B%20however%2C%20this%20is%20one%20of%20the%0Ainitial%20motivations%20to%20develop%20continuum%20robots.%20This%20paper%20addresses%20the%0Acrucial%20challenge%20of%20controlling%20both%20the%20position%20and%20stiffness%20of%0Aunderactuated%20continuum%20robots%20actuated%20by%20antagonistic%20tendons.%20We%20begin%20by%0Apresenting%20a%20rigid-link%20dynamical%20model%20that%20can%20analyze%20the%20open-loop%0Astiffening%20of%20tendon-driven%20continuum%20robots.%20Based%20on%20this%20model%2C%20we%20propose%20a%0Anovel%20passivity-based%20position-and-stiffness%20controller%20that%20adheres%20to%20the%0Anon-negative%20tension%20constraint.%20Comprehensive%20experiments%20on%20our%20continuum%0Arobot%20validate%20the%20theoretical%20results%20and%20demonstrate%20the%20efficacy%20and%0Aprecision%20of%20this%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.03865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Position-and-Stiffness%2520Control%2520of%2520Underactuated%250A%2520%2520Antagonistic%2520Tendon-Driven%2520Continuum%2520Robots%26entry.906535625%3DBowen%2520Yi%2520and%2520Yeman%2520Fan%2520and%2520Dikai%2520Liu%2520and%2520Jose%2520Guadalupe%2520Romero%26entry.1292438233%3D%2520%2520Continuum%2520robots%2520have%2520gained%2520widespread%2520popularity%2520due%2520to%2520their%2520inherent%250Acompliance%2520and%2520flexibility%252C%2520particularly%2520their%2520adjustable%2520levels%2520of%2520stiffness%250Afor%2520various%2520application%2520scenarios.%2520Despite%2520efforts%2520to%2520dynamic%2520modeling%2520and%250Acontrol%2520synthesis%2520over%2520the%2520past%2520decade%252C%2520few%2520studies%2520have%2520incorporated%2520stiffness%250Aregulation%2520into%2520their%2520feedback%2520control%2520design%253B%2520however%252C%2520this%2520is%2520one%2520of%2520the%250Ainitial%2520motivations%2520to%2520develop%2520continuum%2520robots.%2520This%2520paper%2520addresses%2520the%250Acrucial%2520challenge%2520of%2520controlling%2520both%2520the%2520position%2520and%2520stiffness%2520of%250Aunderactuated%2520continuum%2520robots%2520actuated%2520by%2520antagonistic%2520tendons.%2520We%2520begin%2520by%250Apresenting%2520a%2520rigid-link%2520dynamical%2520model%2520that%2520can%2520analyze%2520the%2520open-loop%250Astiffening%2520of%2520tendon-driven%2520continuum%2520robots.%2520Based%2520on%2520this%2520model%252C%2520we%2520propose%2520a%250Anovel%2520passivity-based%2520position-and-stiffness%2520controller%2520that%2520adheres%2520to%2520the%250Anon-negative%2520tension%2520constraint.%2520Comprehensive%2520experiments%2520on%2520our%2520continuum%250Arobot%2520validate%2520the%2520theoretical%2520results%2520and%2520demonstrate%2520the%2520efficacy%2520and%250Aprecision%2520of%2520this%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.03865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Position-and-Stiffness%20Control%20of%20Underactuated%0A%20%20Antagonistic%20Tendon-Driven%20Continuum%20Robots&entry.906535625=Bowen%20Yi%20and%20Yeman%20Fan%20and%20Dikai%20Liu%20and%20Jose%20Guadalupe%20Romero&entry.1292438233=%20%20Continuum%20robots%20have%20gained%20widespread%20popularity%20due%20to%20their%20inherent%0Acompliance%20and%20flexibility%2C%20particularly%20their%20adjustable%20levels%20of%20stiffness%0Afor%20various%20application%20scenarios.%20Despite%20efforts%20to%20dynamic%20modeling%20and%0Acontrol%20synthesis%20over%20the%20past%20decade%2C%20few%20studies%20have%20incorporated%20stiffness%0Aregulation%20into%20their%20feedback%20control%20design%3B%20however%2C%20this%20is%20one%20of%20the%0Ainitial%20motivations%20to%20develop%20continuum%20robots.%20This%20paper%20addresses%20the%0Acrucial%20challenge%20of%20controlling%20both%20the%20position%20and%20stiffness%20of%0Aunderactuated%20continuum%20robots%20actuated%20by%20antagonistic%20tendons.%20We%20begin%20by%0Apresenting%20a%20rigid-link%20dynamical%20model%20that%20can%20analyze%20the%20open-loop%0Astiffening%20of%20tendon-driven%20continuum%20robots.%20Based%20on%20this%20model%2C%20we%20propose%20a%0Anovel%20passivity-based%20position-and-stiffness%20controller%20that%20adheres%20to%20the%0Anon-negative%20tension%20constraint.%20Comprehensive%20experiments%20on%20our%20continuum%0Arobot%20validate%20the%20theoretical%20results%20and%20demonstrate%20the%20efficacy%20and%0Aprecision%20of%20this%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.03865v3&entry.124074799=Read"},
{"title": "NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous\n  Perception, Reasoning, and Planning in Complex UAV Search Missions", "author": "Zhixi Cai and Cristian Rojas Cardenas and Kevin Leo and Chenyuan Zhang and Kal Backman and Hanbing Li and Boying Li and Mahsa Ghorbanali and Stavya Datta and Lizhen Qu and Julian Gutierrez Santiago and Alexey Ignatiev and Yuan-Fang Li and Mor Vered and Peter J Stuckey and Maria Garcia de la Banda and Hamid Rezatofighi", "abstract": "  This paper addresses the problem of autonomous UAV search missions, where a\nUAV must locate specific Entities of Interest (EOIs) within a time limit, based\non brief descriptions in large, hazard-prone environments with keep-out zones.\nThe UAV must perceive, reason, and make decisions with limited and uncertain\ninformation. We propose NEUSIS, a compositional neuro-symbolic system designed\nfor interpretable UAV search and navigation in realistic scenarios. NEUSIS\nintegrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to\nprocess raw sensory inputs, maintains a probabilistic world model for\nenvironment representation, and uses a hierarchical planning component (SNaC)\nfor efficient path planning. Experimental results from simulated urban search\nmissions using AirSim and Unreal Engine show that NEUSIS outperforms a\nstate-of-the-art (SOTA) vision-language model and a SOTA search planning model\nin success rate, search efficiency, and 3D localization. These results\ndemonstrate the effectiveness of our compositional neuro-symbolic approach in\nhandling complex, real-world scenarios, making it a promising solution for\nautonomous UAV systems in search missions.\n", "link": "http://arxiv.org/abs/2409.10196v1", "date": "2024-09-16", "relevancy": 1.7124, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.549}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEUSIS%3A%20A%20Compositional%20Neuro-Symbolic%20Framework%20for%20Autonomous%0A%20%20Perception%2C%20Reasoning%2C%20and%20Planning%20in%20Complex%20UAV%20Search%20Missions&body=Title%3A%20NEUSIS%3A%20A%20Compositional%20Neuro-Symbolic%20Framework%20for%20Autonomous%0A%20%20Perception%2C%20Reasoning%2C%20and%20Planning%20in%20Complex%20UAV%20Search%20Missions%0AAuthor%3A%20Zhixi%20Cai%20and%20Cristian%20Rojas%20Cardenas%20and%20Kevin%20Leo%20and%20Chenyuan%20Zhang%20and%20Kal%20Backman%20and%20Hanbing%20Li%20and%20Boying%20Li%20and%20Mahsa%20Ghorbanali%20and%20Stavya%20Datta%20and%20Lizhen%20Qu%20and%20Julian%20Gutierrez%20Santiago%20and%20Alexey%20Ignatiev%20and%20Yuan-Fang%20Li%20and%20Mor%20Vered%20and%20Peter%20J%20Stuckey%20and%20Maria%20Garcia%20de%20la%20Banda%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20autonomous%20UAV%20search%20missions%2C%20where%20a%0AUAV%20must%20locate%20specific%20Entities%20of%20Interest%20%28EOIs%29%20within%20a%20time%20limit%2C%20based%0Aon%20brief%20descriptions%20in%20large%2C%20hazard-prone%20environments%20with%20keep-out%20zones.%0AThe%20UAV%20must%20perceive%2C%20reason%2C%20and%20make%20decisions%20with%20limited%20and%20uncertain%0Ainformation.%20We%20propose%20NEUSIS%2C%20a%20compositional%20neuro-symbolic%20system%20designed%0Afor%20interpretable%20UAV%20search%20and%20navigation%20in%20realistic%20scenarios.%20NEUSIS%0Aintegrates%20neuro-symbolic%20visual%20perception%2C%20reasoning%2C%20and%20grounding%20%28GRiD%29%20to%0Aprocess%20raw%20sensory%20inputs%2C%20maintains%20a%20probabilistic%20world%20model%20for%0Aenvironment%20representation%2C%20and%20uses%20a%20hierarchical%20planning%20component%20%28SNaC%29%0Afor%20efficient%20path%20planning.%20Experimental%20results%20from%20simulated%20urban%20search%0Amissions%20using%20AirSim%20and%20Unreal%20Engine%20show%20that%20NEUSIS%20outperforms%20a%0Astate-of-the-art%20%28SOTA%29%20vision-language%20model%20and%20a%20SOTA%20search%20planning%20model%0Ain%20success%20rate%2C%20search%20efficiency%2C%20and%203D%20localization.%20These%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20compositional%20neuro-symbolic%20approach%20in%0Ahandling%20complex%2C%20real-world%20scenarios%2C%20making%20it%20a%20promising%20solution%20for%0Aautonomous%20UAV%20systems%20in%20search%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEUSIS%253A%2520A%2520Compositional%2520Neuro-Symbolic%2520Framework%2520for%2520Autonomous%250A%2520%2520Perception%252C%2520Reasoning%252C%2520and%2520Planning%2520in%2520Complex%2520UAV%2520Search%2520Missions%26entry.906535625%3DZhixi%2520Cai%2520and%2520Cristian%2520Rojas%2520Cardenas%2520and%2520Kevin%2520Leo%2520and%2520Chenyuan%2520Zhang%2520and%2520Kal%2520Backman%2520and%2520Hanbing%2520Li%2520and%2520Boying%2520Li%2520and%2520Mahsa%2520Ghorbanali%2520and%2520Stavya%2520Datta%2520and%2520Lizhen%2520Qu%2520and%2520Julian%2520Gutierrez%2520Santiago%2520and%2520Alexey%2520Ignatiev%2520and%2520Yuan-Fang%2520Li%2520and%2520Mor%2520Vered%2520and%2520Peter%2520J%2520Stuckey%2520and%2520Maria%2520Garcia%2520de%2520la%2520Banda%2520and%2520Hamid%2520Rezatofighi%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520autonomous%2520UAV%2520search%2520missions%252C%2520where%2520a%250AUAV%2520must%2520locate%2520specific%2520Entities%2520of%2520Interest%2520%2528EOIs%2529%2520within%2520a%2520time%2520limit%252C%2520based%250Aon%2520brief%2520descriptions%2520in%2520large%252C%2520hazard-prone%2520environments%2520with%2520keep-out%2520zones.%250AThe%2520UAV%2520must%2520perceive%252C%2520reason%252C%2520and%2520make%2520decisions%2520with%2520limited%2520and%2520uncertain%250Ainformation.%2520We%2520propose%2520NEUSIS%252C%2520a%2520compositional%2520neuro-symbolic%2520system%2520designed%250Afor%2520interpretable%2520UAV%2520search%2520and%2520navigation%2520in%2520realistic%2520scenarios.%2520NEUSIS%250Aintegrates%2520neuro-symbolic%2520visual%2520perception%252C%2520reasoning%252C%2520and%2520grounding%2520%2528GRiD%2529%2520to%250Aprocess%2520raw%2520sensory%2520inputs%252C%2520maintains%2520a%2520probabilistic%2520world%2520model%2520for%250Aenvironment%2520representation%252C%2520and%2520uses%2520a%2520hierarchical%2520planning%2520component%2520%2528SNaC%2529%250Afor%2520efficient%2520path%2520planning.%2520Experimental%2520results%2520from%2520simulated%2520urban%2520search%250Amissions%2520using%2520AirSim%2520and%2520Unreal%2520Engine%2520show%2520that%2520NEUSIS%2520outperforms%2520a%250Astate-of-the-art%2520%2528SOTA%2529%2520vision-language%2520model%2520and%2520a%2520SOTA%2520search%2520planning%2520model%250Ain%2520success%2520rate%252C%2520search%2520efficiency%252C%2520and%25203D%2520localization.%2520These%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520compositional%2520neuro-symbolic%2520approach%2520in%250Ahandling%2520complex%252C%2520real-world%2520scenarios%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%250Aautonomous%2520UAV%2520systems%2520in%2520search%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEUSIS%3A%20A%20Compositional%20Neuro-Symbolic%20Framework%20for%20Autonomous%0A%20%20Perception%2C%20Reasoning%2C%20and%20Planning%20in%20Complex%20UAV%20Search%20Missions&entry.906535625=Zhixi%20Cai%20and%20Cristian%20Rojas%20Cardenas%20and%20Kevin%20Leo%20and%20Chenyuan%20Zhang%20and%20Kal%20Backman%20and%20Hanbing%20Li%20and%20Boying%20Li%20and%20Mahsa%20Ghorbanali%20and%20Stavya%20Datta%20and%20Lizhen%20Qu%20and%20Julian%20Gutierrez%20Santiago%20and%20Alexey%20Ignatiev%20and%20Yuan-Fang%20Li%20and%20Mor%20Vered%20and%20Peter%20J%20Stuckey%20and%20Maria%20Garcia%20de%20la%20Banda%20and%20Hamid%20Rezatofighi&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20autonomous%20UAV%20search%20missions%2C%20where%20a%0AUAV%20must%20locate%20specific%20Entities%20of%20Interest%20%28EOIs%29%20within%20a%20time%20limit%2C%20based%0Aon%20brief%20descriptions%20in%20large%2C%20hazard-prone%20environments%20with%20keep-out%20zones.%0AThe%20UAV%20must%20perceive%2C%20reason%2C%20and%20make%20decisions%20with%20limited%20and%20uncertain%0Ainformation.%20We%20propose%20NEUSIS%2C%20a%20compositional%20neuro-symbolic%20system%20designed%0Afor%20interpretable%20UAV%20search%20and%20navigation%20in%20realistic%20scenarios.%20NEUSIS%0Aintegrates%20neuro-symbolic%20visual%20perception%2C%20reasoning%2C%20and%20grounding%20%28GRiD%29%20to%0Aprocess%20raw%20sensory%20inputs%2C%20maintains%20a%20probabilistic%20world%20model%20for%0Aenvironment%20representation%2C%20and%20uses%20a%20hierarchical%20planning%20component%20%28SNaC%29%0Afor%20efficient%20path%20planning.%20Experimental%20results%20from%20simulated%20urban%20search%0Amissions%20using%20AirSim%20and%20Unreal%20Engine%20show%20that%20NEUSIS%20outperforms%20a%0Astate-of-the-art%20%28SOTA%29%20vision-language%20model%20and%20a%20SOTA%20search%20planning%20model%0Ain%20success%20rate%2C%20search%20efficiency%2C%20and%203D%20localization.%20These%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20compositional%20neuro-symbolic%20approach%20in%0Ahandling%20complex%2C%20real-world%20scenarios%2C%20making%20it%20a%20promising%20solution%20for%0Aautonomous%20UAV%20systems%20in%20search%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10196v1&entry.124074799=Read"},
{"title": "Local Methods with Adaptivity via Scaling", "author": "Savelii Chezhegov and Sergey Skorik and Nikolas Khachaturov and Danil Shalagin and Aram Avetisyan and Martin Tak\u00e1\u010d and Yaroslav Kholodov and Aleksandr Beznosikov", "abstract": "  The rapid development of machine learning and deep learning has introduced\nincreasingly complex optimization challenges that must be addressed. Indeed,\ntraining modern, advanced models has become difficult to implement without\nleveraging multiple computing nodes in a distributed environment. Distributed\noptimization is also fundamental to emerging fields such as federated learning.\nSpecifically, there is a need to organize the training process to minimize the\ntime lost due to communication. A widely used and extensively researched\ntechnique to mitigate the communication bottleneck involves performing local\ntraining before communication. This approach is the focus of our paper.\nConcurrently, adaptive methods that incorporate scaling, notably led by Adam,\nhave gained significant popularity in recent years. Therefore, this paper aims\nto merge the local training technique with the adaptive approach to develop\nefficient distributed learning methods. We consider the classical Local SGD\nmethod and enhance it with a scaling feature. A crucial aspect is that the\nscaling is described generically, allowing us to analyze various approaches,\nincluding Adam, RMSProp, and OASIS, in a unified manner. In addition to\ntheoretical analysis, we validate the performance of our methods in practice by\ntraining a neural network.\n", "link": "http://arxiv.org/abs/2406.00846v3", "date": "2024-09-16", "relevancy": 1.9577, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Methods%20with%20Adaptivity%20via%20Scaling&body=Title%3A%20Local%20Methods%20with%20Adaptivity%20via%20Scaling%0AAuthor%3A%20Savelii%20Chezhegov%20and%20Sergey%20Skorik%20and%20Nikolas%20Khachaturov%20and%20Danil%20Shalagin%20and%20Aram%20Avetisyan%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Yaroslav%20Kholodov%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20machine%20learning%20and%20deep%20learning%20has%20introduced%0Aincreasingly%20complex%20optimization%20challenges%20that%20must%20be%20addressed.%20Indeed%2C%0Atraining%20modern%2C%20advanced%20models%20has%20become%20difficult%20to%20implement%20without%0Aleveraging%20multiple%20computing%20nodes%20in%20a%20distributed%20environment.%20Distributed%0Aoptimization%20is%20also%20fundamental%20to%20emerging%20fields%20such%20as%20federated%20learning.%0ASpecifically%2C%20there%20is%20a%20need%20to%20organize%20the%20training%20process%20to%20minimize%20the%0Atime%20lost%20due%20to%20communication.%20A%20widely%20used%20and%20extensively%20researched%0Atechnique%20to%20mitigate%20the%20communication%20bottleneck%20involves%20performing%20local%0Atraining%20before%20communication.%20This%20approach%20is%20the%20focus%20of%20our%20paper.%0AConcurrently%2C%20adaptive%20methods%20that%20incorporate%20scaling%2C%20notably%20led%20by%20Adam%2C%0Ahave%20gained%20significant%20popularity%20in%20recent%20years.%20Therefore%2C%20this%20paper%20aims%0Ato%20merge%20the%20local%20training%20technique%20with%20the%20adaptive%20approach%20to%20develop%0Aefficient%20distributed%20learning%20methods.%20We%20consider%20the%20classical%20Local%20SGD%0Amethod%20and%20enhance%20it%20with%20a%20scaling%20feature.%20A%20crucial%20aspect%20is%20that%20the%0Ascaling%20is%20described%20generically%2C%20allowing%20us%20to%20analyze%20various%20approaches%2C%0Aincluding%20Adam%2C%20RMSProp%2C%20and%20OASIS%2C%20in%20a%20unified%20manner.%20In%20addition%20to%0Atheoretical%20analysis%2C%20we%20validate%20the%20performance%20of%20our%20methods%20in%20practice%20by%0Atraining%20a%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00846v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Methods%2520with%2520Adaptivity%2520via%2520Scaling%26entry.906535625%3DSavelii%2520Chezhegov%2520and%2520Sergey%2520Skorik%2520and%2520Nikolas%2520Khachaturov%2520and%2520Danil%2520Shalagin%2520and%2520Aram%2520Avetisyan%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%2520and%2520Yaroslav%2520Kholodov%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520machine%2520learning%2520and%2520deep%2520learning%2520has%2520introduced%250Aincreasingly%2520complex%2520optimization%2520challenges%2520that%2520must%2520be%2520addressed.%2520Indeed%252C%250Atraining%2520modern%252C%2520advanced%2520models%2520has%2520become%2520difficult%2520to%2520implement%2520without%250Aleveraging%2520multiple%2520computing%2520nodes%2520in%2520a%2520distributed%2520environment.%2520Distributed%250Aoptimization%2520is%2520also%2520fundamental%2520to%2520emerging%2520fields%2520such%2520as%2520federated%2520learning.%250ASpecifically%252C%2520there%2520is%2520a%2520need%2520to%2520organize%2520the%2520training%2520process%2520to%2520minimize%2520the%250Atime%2520lost%2520due%2520to%2520communication.%2520A%2520widely%2520used%2520and%2520extensively%2520researched%250Atechnique%2520to%2520mitigate%2520the%2520communication%2520bottleneck%2520involves%2520performing%2520local%250Atraining%2520before%2520communication.%2520This%2520approach%2520is%2520the%2520focus%2520of%2520our%2520paper.%250AConcurrently%252C%2520adaptive%2520methods%2520that%2520incorporate%2520scaling%252C%2520notably%2520led%2520by%2520Adam%252C%250Ahave%2520gained%2520significant%2520popularity%2520in%2520recent%2520years.%2520Therefore%252C%2520this%2520paper%2520aims%250Ato%2520merge%2520the%2520local%2520training%2520technique%2520with%2520the%2520adaptive%2520approach%2520to%2520develop%250Aefficient%2520distributed%2520learning%2520methods.%2520We%2520consider%2520the%2520classical%2520Local%2520SGD%250Amethod%2520and%2520enhance%2520it%2520with%2520a%2520scaling%2520feature.%2520A%2520crucial%2520aspect%2520is%2520that%2520the%250Ascaling%2520is%2520described%2520generically%252C%2520allowing%2520us%2520to%2520analyze%2520various%2520approaches%252C%250Aincluding%2520Adam%252C%2520RMSProp%252C%2520and%2520OASIS%252C%2520in%2520a%2520unified%2520manner.%2520In%2520addition%2520to%250Atheoretical%2520analysis%252C%2520we%2520validate%2520the%2520performance%2520of%2520our%2520methods%2520in%2520practice%2520by%250Atraining%2520a%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00846v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Methods%20with%20Adaptivity%20via%20Scaling&entry.906535625=Savelii%20Chezhegov%20and%20Sergey%20Skorik%20and%20Nikolas%20Khachaturov%20and%20Danil%20Shalagin%20and%20Aram%20Avetisyan%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Yaroslav%20Kholodov%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20The%20rapid%20development%20of%20machine%20learning%20and%20deep%20learning%20has%20introduced%0Aincreasingly%20complex%20optimization%20challenges%20that%20must%20be%20addressed.%20Indeed%2C%0Atraining%20modern%2C%20advanced%20models%20has%20become%20difficult%20to%20implement%20without%0Aleveraging%20multiple%20computing%20nodes%20in%20a%20distributed%20environment.%20Distributed%0Aoptimization%20is%20also%20fundamental%20to%20emerging%20fields%20such%20as%20federated%20learning.%0ASpecifically%2C%20there%20is%20a%20need%20to%20organize%20the%20training%20process%20to%20minimize%20the%0Atime%20lost%20due%20to%20communication.%20A%20widely%20used%20and%20extensively%20researched%0Atechnique%20to%20mitigate%20the%20communication%20bottleneck%20involves%20performing%20local%0Atraining%20before%20communication.%20This%20approach%20is%20the%20focus%20of%20our%20paper.%0AConcurrently%2C%20adaptive%20methods%20that%20incorporate%20scaling%2C%20notably%20led%20by%20Adam%2C%0Ahave%20gained%20significant%20popularity%20in%20recent%20years.%20Therefore%2C%20this%20paper%20aims%0Ato%20merge%20the%20local%20training%20technique%20with%20the%20adaptive%20approach%20to%20develop%0Aefficient%20distributed%20learning%20methods.%20We%20consider%20the%20classical%20Local%20SGD%0Amethod%20and%20enhance%20it%20with%20a%20scaling%20feature.%20A%20crucial%20aspect%20is%20that%20the%0Ascaling%20is%20described%20generically%2C%20allowing%20us%20to%20analyze%20various%20approaches%2C%0Aincluding%20Adam%2C%20RMSProp%2C%20and%20OASIS%2C%20in%20a%20unified%20manner.%20In%20addition%20to%0Atheoretical%20analysis%2C%20we%20validate%20the%20performance%20of%20our%20methods%20in%20practice%20by%0Atraining%20a%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00846v3&entry.124074799=Read"},
{"title": "Causal Learning in Biomedical Applications: A Benchmark", "author": "Petr Ry\u0161av\u00fd and Xiaoyu He and Jakub Mare\u010dek", "abstract": "  Learning causal relationships between a set of variables is a challenging\nproblem in computer science. Many existing artificial benchmark datasets are\nbased on sampling from causal models and thus contain residual information that\nthe ${R} ^2$-sortability can identify. Here, we present a benchmark for methods\nin causal learning using time series. The presented dataset is not\n${R}^2$-sortable and is based on a real-world scenario of the Krebs cycle that\nis used in cells to release energy. We provide four scenarios of learning,\nincluding short and long time series, and provide guidance so that testing is\nunified between possible users.\n", "link": "http://arxiv.org/abs/2406.15189v2", "date": "2024-09-16", "relevancy": 1.2833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4272}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Learning%20in%20Biomedical%20Applications%3A%20A%20Benchmark&body=Title%3A%20Causal%20Learning%20in%20Biomedical%20Applications%3A%20A%20Benchmark%0AAuthor%3A%20Petr%20Ry%C5%A1av%C3%BD%20and%20Xiaoyu%20He%20and%20Jakub%20Mare%C4%8Dek%0AAbstract%3A%20%20%20Learning%20causal%20relationships%20between%20a%20set%20of%20variables%20is%20a%20challenging%0Aproblem%20in%20computer%20science.%20Many%20existing%20artificial%20benchmark%20datasets%20are%0Abased%20on%20sampling%20from%20causal%20models%20and%20thus%20contain%20residual%20information%20that%0Athe%20%24%7BR%7D%20%5E2%24-sortability%20can%20identify.%20Here%2C%20we%20present%20a%20benchmark%20for%20methods%0Ain%20causal%20learning%20using%20time%20series.%20The%20presented%20dataset%20is%20not%0A%24%7BR%7D%5E2%24-sortable%20and%20is%20based%20on%20a%20real-world%20scenario%20of%20the%20Krebs%20cycle%20that%0Ais%20used%20in%20cells%20to%20release%20energy.%20We%20provide%20four%20scenarios%20of%20learning%2C%0Aincluding%20short%20and%20long%20time%20series%2C%20and%20provide%20guidance%20so%20that%20testing%20is%0Aunified%20between%20possible%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Learning%2520in%2520Biomedical%2520Applications%253A%2520A%2520Benchmark%26entry.906535625%3DPetr%2520Ry%25C5%25A1av%25C3%25BD%2520and%2520Xiaoyu%2520He%2520and%2520Jakub%2520Mare%25C4%258Dek%26entry.1292438233%3D%2520%2520Learning%2520causal%2520relationships%2520between%2520a%2520set%2520of%2520variables%2520is%2520a%2520challenging%250Aproblem%2520in%2520computer%2520science.%2520Many%2520existing%2520artificial%2520benchmark%2520datasets%2520are%250Abased%2520on%2520sampling%2520from%2520causal%2520models%2520and%2520thus%2520contain%2520residual%2520information%2520that%250Athe%2520%2524%257BR%257D%2520%255E2%2524-sortability%2520can%2520identify.%2520Here%252C%2520we%2520present%2520a%2520benchmark%2520for%2520methods%250Ain%2520causal%2520learning%2520using%2520time%2520series.%2520The%2520presented%2520dataset%2520is%2520not%250A%2524%257BR%257D%255E2%2524-sortable%2520and%2520is%2520based%2520on%2520a%2520real-world%2520scenario%2520of%2520the%2520Krebs%2520cycle%2520that%250Ais%2520used%2520in%2520cells%2520to%2520release%2520energy.%2520We%2520provide%2520four%2520scenarios%2520of%2520learning%252C%250Aincluding%2520short%2520and%2520long%2520time%2520series%252C%2520and%2520provide%2520guidance%2520so%2520that%2520testing%2520is%250Aunified%2520between%2520possible%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Learning%20in%20Biomedical%20Applications%3A%20A%20Benchmark&entry.906535625=Petr%20Ry%C5%A1av%C3%BD%20and%20Xiaoyu%20He%20and%20Jakub%20Mare%C4%8Dek&entry.1292438233=%20%20Learning%20causal%20relationships%20between%20a%20set%20of%20variables%20is%20a%20challenging%0Aproblem%20in%20computer%20science.%20Many%20existing%20artificial%20benchmark%20datasets%20are%0Abased%20on%20sampling%20from%20causal%20models%20and%20thus%20contain%20residual%20information%20that%0Athe%20%24%7BR%7D%20%5E2%24-sortability%20can%20identify.%20Here%2C%20we%20present%20a%20benchmark%20for%20methods%0Ain%20causal%20learning%20using%20time%20series.%20The%20presented%20dataset%20is%20not%0A%24%7BR%7D%5E2%24-sortable%20and%20is%20based%20on%20a%20real-world%20scenario%20of%20the%20Krebs%20cycle%20that%0Ais%20used%20in%20cells%20to%20release%20energy.%20We%20provide%20four%20scenarios%20of%20learning%2C%0Aincluding%20short%20and%20long%20time%20series%2C%20and%20provide%20guidance%20so%20that%20testing%20is%0Aunified%20between%20possible%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15189v2&entry.124074799=Read"},
{"title": "Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces", "author": "Luca Pernigo and Rohan Sen and Davide Baroli", "abstract": "  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n", "link": "http://arxiv.org/abs/2408.04405v3", "date": "2024-09-16", "relevancy": 1.1459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4413}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces&body=Title%3A%20Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces%0AAuthor%3A%20Luca%20Pernigo%20and%20Rohan%20Sen%20and%20Davide%20Baroli%0AAbstract%3A%20%20%20Accurate%20energy%20demand%20forecasting%20is%20crucial%20for%20sustainable%20and%20resilient%0Aenergy%20development.%20To%20meet%20the%20Net%20Zero%20Representative%20Concentration%20Pathways%0A%28RCP%29%20%244.5%24%20scenario%20in%20the%20DACH%20countries%2C%20increased%20renewable%20energy%0Aproduction%2C%20energy%20storage%2C%20and%20reduced%20commercial%20building%20consumption%20are%0Aneeded.%20This%20scenario%27s%20success%20depends%20on%20hydroelectric%20capacity%20and%20climatic%0Afactors.%20Informed%20decisions%20require%20quantifying%20uncertainty%20in%20forecasts.%20This%0Astudy%20explores%20a%20non-parametric%20method%20based%20on%20%5Cemph%7Breproducing%20kernel%0AHilbert%20spaces%20%28RKHS%29%7D%2C%20known%20as%20kernel%20quantile%20regression%2C%20for%20energy%0Aprediction.%20Our%20experiments%20demonstrate%20its%20reliability%20and%20sharpness%2C%20and%20we%0Abenchmark%20it%20against%20state-of-the-art%20methods%20in%20load%20and%20price%20forecasting%20for%0Athe%20DACH%20region.%20We%20offer%20our%20implementation%20in%20conjunction%20with%20additional%0Ascripts%20to%20ensure%20the%20reproducibility%20of%20our%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04405v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520energy%2520forecasting%2520through%2520quantile%2520regression%2520in%250A%2520%2520reproducing%2520kernel%2520Hilbert%2520spaces%26entry.906535625%3DLuca%2520Pernigo%2520and%2520Rohan%2520Sen%2520and%2520Davide%2520Baroli%26entry.1292438233%3D%2520%2520Accurate%2520energy%2520demand%2520forecasting%2520is%2520crucial%2520for%2520sustainable%2520and%2520resilient%250Aenergy%2520development.%2520To%2520meet%2520the%2520Net%2520Zero%2520Representative%2520Concentration%2520Pathways%250A%2528RCP%2529%2520%25244.5%2524%2520scenario%2520in%2520the%2520DACH%2520countries%252C%2520increased%2520renewable%2520energy%250Aproduction%252C%2520energy%2520storage%252C%2520and%2520reduced%2520commercial%2520building%2520consumption%2520are%250Aneeded.%2520This%2520scenario%2527s%2520success%2520depends%2520on%2520hydroelectric%2520capacity%2520and%2520climatic%250Afactors.%2520Informed%2520decisions%2520require%2520quantifying%2520uncertainty%2520in%2520forecasts.%2520This%250Astudy%2520explores%2520a%2520non-parametric%2520method%2520based%2520on%2520%255Cemph%257Breproducing%2520kernel%250AHilbert%2520spaces%2520%2528RKHS%2529%257D%252C%2520known%2520as%2520kernel%2520quantile%2520regression%252C%2520for%2520energy%250Aprediction.%2520Our%2520experiments%2520demonstrate%2520its%2520reliability%2520and%2520sharpness%252C%2520and%2520we%250Abenchmark%2520it%2520against%2520state-of-the-art%2520methods%2520in%2520load%2520and%2520price%2520forecasting%2520for%250Athe%2520DACH%2520region.%2520We%2520offer%2520our%2520implementation%2520in%2520conjunction%2520with%2520additional%250Ascripts%2520to%2520ensure%2520the%2520reproducibility%2520of%2520our%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04405v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces&entry.906535625=Luca%20Pernigo%20and%20Rohan%20Sen%20and%20Davide%20Baroli&entry.1292438233=%20%20Accurate%20energy%20demand%20forecasting%20is%20crucial%20for%20sustainable%20and%20resilient%0Aenergy%20development.%20To%20meet%20the%20Net%20Zero%20Representative%20Concentration%20Pathways%0A%28RCP%29%20%244.5%24%20scenario%20in%20the%20DACH%20countries%2C%20increased%20renewable%20energy%0Aproduction%2C%20energy%20storage%2C%20and%20reduced%20commercial%20building%20consumption%20are%0Aneeded.%20This%20scenario%27s%20success%20depends%20on%20hydroelectric%20capacity%20and%20climatic%0Afactors.%20Informed%20decisions%20require%20quantifying%20uncertainty%20in%20forecasts.%20This%0Astudy%20explores%20a%20non-parametric%20method%20based%20on%20%5Cemph%7Breproducing%20kernel%0AHilbert%20spaces%20%28RKHS%29%7D%2C%20known%20as%20kernel%20quantile%20regression%2C%20for%20energy%0Aprediction.%20Our%20experiments%20demonstrate%20its%20reliability%20and%20sharpness%2C%20and%20we%0Abenchmark%20it%20against%20state-of-the-art%20methods%20in%20load%20and%20price%20forecasting%20for%0Athe%20DACH%20region.%20We%20offer%20our%20implementation%20in%20conjunction%20with%20additional%0Ascripts%20to%20ensure%20the%20reproducibility%20of%20our%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04405v3&entry.124074799=Read"},
{"title": "The Role of Deep Learning Regularizations on Actors in Offline RL", "author": "Denis Tarasov and Anja Surina and Caglar Gulcehre", "abstract": "  Deep learning regularization techniques, such as dropout, layer\nnormalization, or weight decay, are widely adopted in the construction of\nmodern artificial neural networks, often resulting in more robust training\nprocesses and improved generalization capabilities. However, in the domain of\nReinforcement Learning (RL), the application of these techniques has been\nlimited, usually applied to value function estimators, and may result in\ndetrimental effects. This issue is even more pronounced in offline RL settings,\nwhich bear greater similarity to supervised learning but have received less\nattention. Recent work in continuous offline RL has demonstrated that while we\ncan build sufficiently powerful critic networks, the generalization of actor\nnetworks remains a bottleneck. In this study, we empirically show that applying\nstandard regularization techniques to actor networks in offline RL actor-critic\nalgorithms yields improvements of 6% on average across two algorithms and three\ndifferent continuous D4RL domains.\n", "link": "http://arxiv.org/abs/2409.07606v2", "date": "2024-09-16", "relevancy": 1.8043, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Deep%20Learning%20Regularizations%20on%20Actors%20in%20Offline%20RL&body=Title%3A%20The%20Role%20of%20Deep%20Learning%20Regularizations%20on%20Actors%20in%20Offline%20RL%0AAuthor%3A%20Denis%20Tarasov%20and%20Anja%20Surina%20and%20Caglar%20Gulcehre%0AAbstract%3A%20%20%20Deep%20learning%20regularization%20techniques%2C%20such%20as%20dropout%2C%20layer%0Anormalization%2C%20or%20weight%20decay%2C%20are%20widely%20adopted%20in%20the%20construction%20of%0Amodern%20artificial%20neural%20networks%2C%20often%20resulting%20in%20more%20robust%20training%0Aprocesses%20and%20improved%20generalization%20capabilities.%20However%2C%20in%20the%20domain%20of%0AReinforcement%20Learning%20%28RL%29%2C%20the%20application%20of%20these%20techniques%20has%20been%0Alimited%2C%20usually%20applied%20to%20value%20function%20estimators%2C%20and%20may%20result%20in%0Adetrimental%20effects.%20This%20issue%20is%20even%20more%20pronounced%20in%20offline%20RL%20settings%2C%0Awhich%20bear%20greater%20similarity%20to%20supervised%20learning%20but%20have%20received%20less%0Aattention.%20Recent%20work%20in%20continuous%20offline%20RL%20has%20demonstrated%20that%20while%20we%0Acan%20build%20sufficiently%20powerful%20critic%20networks%2C%20the%20generalization%20of%20actor%0Anetworks%20remains%20a%20bottleneck.%20In%20this%20study%2C%20we%20empirically%20show%20that%20applying%0Astandard%20regularization%20techniques%20to%20actor%20networks%20in%20offline%20RL%20actor-critic%0Aalgorithms%20yields%20improvements%20of%206%25%20on%20average%20across%20two%20algorithms%20and%20three%0Adifferent%20continuous%20D4RL%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Deep%2520Learning%2520Regularizations%2520on%2520Actors%2520in%2520Offline%2520RL%26entry.906535625%3DDenis%2520Tarasov%2520and%2520Anja%2520Surina%2520and%2520Caglar%2520Gulcehre%26entry.1292438233%3D%2520%2520Deep%2520learning%2520regularization%2520techniques%252C%2520such%2520as%2520dropout%252C%2520layer%250Anormalization%252C%2520or%2520weight%2520decay%252C%2520are%2520widely%2520adopted%2520in%2520the%2520construction%2520of%250Amodern%2520artificial%2520neural%2520networks%252C%2520often%2520resulting%2520in%2520more%2520robust%2520training%250Aprocesses%2520and%2520improved%2520generalization%2520capabilities.%2520However%252C%2520in%2520the%2520domain%2520of%250AReinforcement%2520Learning%2520%2528RL%2529%252C%2520the%2520application%2520of%2520these%2520techniques%2520has%2520been%250Alimited%252C%2520usually%2520applied%2520to%2520value%2520function%2520estimators%252C%2520and%2520may%2520result%2520in%250Adetrimental%2520effects.%2520This%2520issue%2520is%2520even%2520more%2520pronounced%2520in%2520offline%2520RL%2520settings%252C%250Awhich%2520bear%2520greater%2520similarity%2520to%2520supervised%2520learning%2520but%2520have%2520received%2520less%250Aattention.%2520Recent%2520work%2520in%2520continuous%2520offline%2520RL%2520has%2520demonstrated%2520that%2520while%2520we%250Acan%2520build%2520sufficiently%2520powerful%2520critic%2520networks%252C%2520the%2520generalization%2520of%2520actor%250Anetworks%2520remains%2520a%2520bottleneck.%2520In%2520this%2520study%252C%2520we%2520empirically%2520show%2520that%2520applying%250Astandard%2520regularization%2520techniques%2520to%2520actor%2520networks%2520in%2520offline%2520RL%2520actor-critic%250Aalgorithms%2520yields%2520improvements%2520of%25206%2525%2520on%2520average%2520across%2520two%2520algorithms%2520and%2520three%250Adifferent%2520continuous%2520D4RL%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Deep%20Learning%20Regularizations%20on%20Actors%20in%20Offline%20RL&entry.906535625=Denis%20Tarasov%20and%20Anja%20Surina%20and%20Caglar%20Gulcehre&entry.1292438233=%20%20Deep%20learning%20regularization%20techniques%2C%20such%20as%20dropout%2C%20layer%0Anormalization%2C%20or%20weight%20decay%2C%20are%20widely%20adopted%20in%20the%20construction%20of%0Amodern%20artificial%20neural%20networks%2C%20often%20resulting%20in%20more%20robust%20training%0Aprocesses%20and%20improved%20generalization%20capabilities.%20However%2C%20in%20the%20domain%20of%0AReinforcement%20Learning%20%28RL%29%2C%20the%20application%20of%20these%20techniques%20has%20been%0Alimited%2C%20usually%20applied%20to%20value%20function%20estimators%2C%20and%20may%20result%20in%0Adetrimental%20effects.%20This%20issue%20is%20even%20more%20pronounced%20in%20offline%20RL%20settings%2C%0Awhich%20bear%20greater%20similarity%20to%20supervised%20learning%20but%20have%20received%20less%0Aattention.%20Recent%20work%20in%20continuous%20offline%20RL%20has%20demonstrated%20that%20while%20we%0Acan%20build%20sufficiently%20powerful%20critic%20networks%2C%20the%20generalization%20of%20actor%0Anetworks%20remains%20a%20bottleneck.%20In%20this%20study%2C%20we%20empirically%20show%20that%20applying%0Astandard%20regularization%20techniques%20to%20actor%20networks%20in%20offline%20RL%20actor-critic%0Aalgorithms%20yields%20improvements%20of%206%25%20on%20average%20across%20two%20algorithms%20and%20three%0Adifferent%20continuous%20D4RL%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07606v2&entry.124074799=Read"},
{"title": "My part is bigger than yours -- assessment within a group of peers", "author": "Konrad Ku\u0142akowski and Jacek Szybowski", "abstract": "  A project (e.g., writing a collaborative research paper) is often a group\neffort. At the end, each contributor identifies their contribution, often\nverbally. The reward, however, is very frequently financial. It leads to the\nquestion of what (percentage) share in the creation of the paper is due to\nindividual authors. Different authors may have various opinions on the matter;\neven worse, their opinions may have different relevance. In this paper, we\npresent simple models that allow aggregation of experts' views, linking the\npriority of his preference directly to the assessment made by other experts. In\nthis approach, the more significant the contribution of a given expert, the\ngreater the importance of his opinion. The presented method can be considered\nan attempt to find consensus among peers involved in the same project. Hence,\nits applications may go beyond the proposed study example of writing a\nscientific paper.\n", "link": "http://arxiv.org/abs/2407.01843v2", "date": "2024-09-16", "relevancy": 1.0255, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.3619}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20My%20part%20is%20bigger%20than%20yours%20--%20assessment%20within%20a%20group%20of%20peers&body=Title%3A%20My%20part%20is%20bigger%20than%20yours%20--%20assessment%20within%20a%20group%20of%20peers%0AAuthor%3A%20Konrad%20Ku%C5%82akowski%20and%20Jacek%20Szybowski%0AAbstract%3A%20%20%20A%20project%20%28e.g.%2C%20writing%20a%20collaborative%20research%20paper%29%20is%20often%20a%20group%0Aeffort.%20At%20the%20end%2C%20each%20contributor%20identifies%20their%20contribution%2C%20often%0Averbally.%20The%20reward%2C%20however%2C%20is%20very%20frequently%20financial.%20It%20leads%20to%20the%0Aquestion%20of%20what%20%28percentage%29%20share%20in%20the%20creation%20of%20the%20paper%20is%20due%20to%0Aindividual%20authors.%20Different%20authors%20may%20have%20various%20opinions%20on%20the%20matter%3B%0Aeven%20worse%2C%20their%20opinions%20may%20have%20different%20relevance.%20In%20this%20paper%2C%20we%0Apresent%20simple%20models%20that%20allow%20aggregation%20of%20experts%27%20views%2C%20linking%20the%0Apriority%20of%20his%20preference%20directly%20to%20the%20assessment%20made%20by%20other%20experts.%20In%0Athis%20approach%2C%20the%20more%20significant%20the%20contribution%20of%20a%20given%20expert%2C%20the%0Agreater%20the%20importance%20of%20his%20opinion.%20The%20presented%20method%20can%20be%20considered%0Aan%20attempt%20to%20find%20consensus%20among%20peers%20involved%20in%20the%20same%20project.%20Hence%2C%0Aits%20applications%20may%20go%20beyond%20the%20proposed%20study%20example%20of%20writing%20a%0Ascientific%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMy%2520part%2520is%2520bigger%2520than%2520yours%2520--%2520assessment%2520within%2520a%2520group%2520of%2520peers%26entry.906535625%3DKonrad%2520Ku%25C5%2582akowski%2520and%2520Jacek%2520Szybowski%26entry.1292438233%3D%2520%2520A%2520project%2520%2528e.g.%252C%2520writing%2520a%2520collaborative%2520research%2520paper%2529%2520is%2520often%2520a%2520group%250Aeffort.%2520At%2520the%2520end%252C%2520each%2520contributor%2520identifies%2520their%2520contribution%252C%2520often%250Averbally.%2520The%2520reward%252C%2520however%252C%2520is%2520very%2520frequently%2520financial.%2520It%2520leads%2520to%2520the%250Aquestion%2520of%2520what%2520%2528percentage%2529%2520share%2520in%2520the%2520creation%2520of%2520the%2520paper%2520is%2520due%2520to%250Aindividual%2520authors.%2520Different%2520authors%2520may%2520have%2520various%2520opinions%2520on%2520the%2520matter%253B%250Aeven%2520worse%252C%2520their%2520opinions%2520may%2520have%2520different%2520relevance.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520simple%2520models%2520that%2520allow%2520aggregation%2520of%2520experts%2527%2520views%252C%2520linking%2520the%250Apriority%2520of%2520his%2520preference%2520directly%2520to%2520the%2520assessment%2520made%2520by%2520other%2520experts.%2520In%250Athis%2520approach%252C%2520the%2520more%2520significant%2520the%2520contribution%2520of%2520a%2520given%2520expert%252C%2520the%250Agreater%2520the%2520importance%2520of%2520his%2520opinion.%2520The%2520presented%2520method%2520can%2520be%2520considered%250Aan%2520attempt%2520to%2520find%2520consensus%2520among%2520peers%2520involved%2520in%2520the%2520same%2520project.%2520Hence%252C%250Aits%2520applications%2520may%2520go%2520beyond%2520the%2520proposed%2520study%2520example%2520of%2520writing%2520a%250Ascientific%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=My%20part%20is%20bigger%20than%20yours%20--%20assessment%20within%20a%20group%20of%20peers&entry.906535625=Konrad%20Ku%C5%82akowski%20and%20Jacek%20Szybowski&entry.1292438233=%20%20A%20project%20%28e.g.%2C%20writing%20a%20collaborative%20research%20paper%29%20is%20often%20a%20group%0Aeffort.%20At%20the%20end%2C%20each%20contributor%20identifies%20their%20contribution%2C%20often%0Averbally.%20The%20reward%2C%20however%2C%20is%20very%20frequently%20financial.%20It%20leads%20to%20the%0Aquestion%20of%20what%20%28percentage%29%20share%20in%20the%20creation%20of%20the%20paper%20is%20due%20to%0Aindividual%20authors.%20Different%20authors%20may%20have%20various%20opinions%20on%20the%20matter%3B%0Aeven%20worse%2C%20their%20opinions%20may%20have%20different%20relevance.%20In%20this%20paper%2C%20we%0Apresent%20simple%20models%20that%20allow%20aggregation%20of%20experts%27%20views%2C%20linking%20the%0Apriority%20of%20his%20preference%20directly%20to%20the%20assessment%20made%20by%20other%20experts.%20In%0Athis%20approach%2C%20the%20more%20significant%20the%20contribution%20of%20a%20given%20expert%2C%20the%0Agreater%20the%20importance%20of%20his%20opinion.%20The%20presented%20method%20can%20be%20considered%0Aan%20attempt%20to%20find%20consensus%20among%20peers%20involved%20in%20the%20same%20project.%20Hence%2C%0Aits%20applications%20may%20go%20beyond%20the%20proposed%20study%20example%20of%20writing%20a%0Ascientific%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01843v2&entry.124074799=Read"},
{"title": "Voice control interface for surgical robot assistants", "author": "Ana Davila and Jacinto Colan and Yasuhisa Hasegawa", "abstract": "  Traditional control interfaces for robotic-assisted minimally invasive\nsurgery impose a significant cognitive load on surgeons. To improve surgical\nefficiency, surgeon-robot collaboration capabilities, and reduce surgeon\nburden, we present a novel voice control interface for surgical robotic\nassistants. Our system integrates Whisper, state-of-the-art speech recognition,\nwithin the ROS framework to enable real-time interpretation and execution of\nvoice commands for surgical manipulator control. The proposed system consists\nof a speech recognition module, an action mapping module, and a robot control\nmodule. Experimental results demonstrate the system's high accuracy and\ninference speed, and demonstrates its feasibility for surgical applications in\na tissue triangulation task. Future work will focus on further improving its\nrobustness and clinical applicability.\n", "link": "http://arxiv.org/abs/2409.10225v1", "date": "2024-09-16", "relevancy": 1.5635, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5431}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voice%20control%20interface%20for%20surgical%20robot%20assistants&body=Title%3A%20Voice%20control%20interface%20for%20surgical%20robot%20assistants%0AAuthor%3A%20Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20%20%20Traditional%20control%20interfaces%20for%20robotic-assisted%20minimally%20invasive%0Asurgery%20impose%20a%20significant%20cognitive%20load%20on%20surgeons.%20To%20improve%20surgical%0Aefficiency%2C%20surgeon-robot%20collaboration%20capabilities%2C%20and%20reduce%20surgeon%0Aburden%2C%20we%20present%20a%20novel%20voice%20control%20interface%20for%20surgical%20robotic%0Aassistants.%20Our%20system%20integrates%20Whisper%2C%20state-of-the-art%20speech%20recognition%2C%0Awithin%20the%20ROS%20framework%20to%20enable%20real-time%20interpretation%20and%20execution%20of%0Avoice%20commands%20for%20surgical%20manipulator%20control.%20The%20proposed%20system%20consists%0Aof%20a%20speech%20recognition%20module%2C%20an%20action%20mapping%20module%2C%20and%20a%20robot%20control%0Amodule.%20Experimental%20results%20demonstrate%20the%20system%27s%20high%20accuracy%20and%0Ainference%20speed%2C%20and%20demonstrates%20its%20feasibility%20for%20surgical%20applications%20in%0Aa%20tissue%20triangulation%20task.%20Future%20work%20will%20focus%20on%20further%20improving%20its%0Arobustness%20and%20clinical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoice%2520control%2520interface%2520for%2520surgical%2520robot%2520assistants%26entry.906535625%3DAna%2520Davila%2520and%2520Jacinto%2520Colan%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3D%2520%2520Traditional%2520control%2520interfaces%2520for%2520robotic-assisted%2520minimally%2520invasive%250Asurgery%2520impose%2520a%2520significant%2520cognitive%2520load%2520on%2520surgeons.%2520To%2520improve%2520surgical%250Aefficiency%252C%2520surgeon-robot%2520collaboration%2520capabilities%252C%2520and%2520reduce%2520surgeon%250Aburden%252C%2520we%2520present%2520a%2520novel%2520voice%2520control%2520interface%2520for%2520surgical%2520robotic%250Aassistants.%2520Our%2520system%2520integrates%2520Whisper%252C%2520state-of-the-art%2520speech%2520recognition%252C%250Awithin%2520the%2520ROS%2520framework%2520to%2520enable%2520real-time%2520interpretation%2520and%2520execution%2520of%250Avoice%2520commands%2520for%2520surgical%2520manipulator%2520control.%2520The%2520proposed%2520system%2520consists%250Aof%2520a%2520speech%2520recognition%2520module%252C%2520an%2520action%2520mapping%2520module%252C%2520and%2520a%2520robot%2520control%250Amodule.%2520Experimental%2520results%2520demonstrate%2520the%2520system%2527s%2520high%2520accuracy%2520and%250Ainference%2520speed%252C%2520and%2520demonstrates%2520its%2520feasibility%2520for%2520surgical%2520applications%2520in%250Aa%2520tissue%2520triangulation%2520task.%2520Future%2520work%2520will%2520focus%2520on%2520further%2520improving%2520its%250Arobustness%2520and%2520clinical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voice%20control%20interface%20for%20surgical%20robot%20assistants&entry.906535625=Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa&entry.1292438233=%20%20Traditional%20control%20interfaces%20for%20robotic-assisted%20minimally%20invasive%0Asurgery%20impose%20a%20significant%20cognitive%20load%20on%20surgeons.%20To%20improve%20surgical%0Aefficiency%2C%20surgeon-robot%20collaboration%20capabilities%2C%20and%20reduce%20surgeon%0Aburden%2C%20we%20present%20a%20novel%20voice%20control%20interface%20for%20surgical%20robotic%0Aassistants.%20Our%20system%20integrates%20Whisper%2C%20state-of-the-art%20speech%20recognition%2C%0Awithin%20the%20ROS%20framework%20to%20enable%20real-time%20interpretation%20and%20execution%20of%0Avoice%20commands%20for%20surgical%20manipulator%20control.%20The%20proposed%20system%20consists%0Aof%20a%20speech%20recognition%20module%2C%20an%20action%20mapping%20module%2C%20and%20a%20robot%20control%0Amodule.%20Experimental%20results%20demonstrate%20the%20system%27s%20high%20accuracy%20and%0Ainference%20speed%2C%20and%20demonstrates%20its%20feasibility%20for%20surgical%20applications%20in%0Aa%20tissue%20triangulation%20task.%20Future%20work%20will%20focus%20on%20further%20improving%20its%0Arobustness%20and%20clinical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10225v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


