<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251225.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study", "author": "Zhengyu Hu and Jianxun Lian and Zheyuan Xiao and Seraphina Zhang and Tianfu Wang and Nicholas Jing Yuan and Xing Xie and Hui Xiong", "abstract": "Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.", "link": "http://arxiv.org/abs/2506.13464v3", "date": "2025-12-26", "relevancy": 2.9285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5979}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%20and%20Empirical%20Study&body=Title%3A%20Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%20and%20Empirical%20Study%0AAuthor%3A%20Zhengyu%20Hu%20and%20Jianxun%20Lian%20and%20Zheyuan%20Xiao%20and%20Seraphina%20Zhang%20and%20Tianfu%20Wang%20and%20Nicholas%20Jing%20Yuan%20and%20Xing%20Xie%20and%20Hui%20Xiong%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%20tasks%20such%20as%20mathematics%2C%20coding%2C%20and%20reasoning%2C%20yet%20their%20learning%20ability%2C%20which%20is%20crucial%20for%20adapting%20to%20dynamic%20environments%20and%20acquiring%20new%20knowledge%2C%20remains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%20framework%20inspired%20by%20cognitive%20psychology%20and%20education.%20Specifically%2C%20we%20decompose%20general%20learning%20ability%20into%20three%20distinct%2C%20complementary%20dimensions%3A%20Learning%20from%20Instructor%20%28acquiring%20knowledge%20via%20explicit%20guidance%29%2C%20Learning%20from%20Concept%20%28internalizing%20abstract%20structures%20and%20generalizing%20to%20new%20contexts%29%2C%20and%20Learning%20from%20Experience%20%28adapting%20through%20accumulated%20exploration%20and%20feedback%29.%20We%20conduct%20a%20comprehensive%20empirical%20study%20across%20the%20three%20learning%20dimensions%20and%20identify%20several%20insightful%20findings%2C%20such%20as%20%28i%29%20interaction%20improves%20learning%3B%20%28ii%29%20conceptual%20understanding%20is%20scale-emergent%20and%20benefits%20larger%20models%3B%20and%20%28iii%29%20LLMs%20are%20effective%20few-shot%20learners%20but%20not%20many-shot%20learners.%20Based%20on%20our%20framework%20and%20empirical%20findings%2C%20we%20introduce%20a%20benchmark%20that%20provides%20a%20unified%20and%20realistic%20evaluation%20of%20LLMs%27%20general%20learning%20abilities%20across%20three%20learning%20cognition%20dimensions.%20It%20enables%20diagnostic%20insights%20and%20supports%20evaluation%20and%20development%20of%20more%20adaptive%20and%20human-like%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13464v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Learning%2520Mind%2520of%2520Language%2520Models%253A%2520A%2520Cognitive%2520Framework%2520and%2520Empirical%2520Study%26entry.906535625%3DZhengyu%2520Hu%2520and%2520Jianxun%2520Lian%2520and%2520Zheyuan%2520Xiao%2520and%2520Seraphina%2520Zhang%2520and%2520Tianfu%2520Wang%2520and%2520Nicholas%2520Jing%2520Yuan%2520and%2520Xing%2520Xie%2520and%2520Hui%2520Xiong%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520across%2520tasks%2520such%2520as%2520mathematics%252C%2520coding%252C%2520and%2520reasoning%252C%2520yet%2520their%2520learning%2520ability%252C%2520which%2520is%2520crucial%2520for%2520adapting%2520to%2520dynamic%2520environments%2520and%2520acquiring%2520new%2520knowledge%252C%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520framework%2520inspired%2520by%2520cognitive%2520psychology%2520and%2520education.%2520Specifically%252C%2520we%2520decompose%2520general%2520learning%2520ability%2520into%2520three%2520distinct%252C%2520complementary%2520dimensions%253A%2520Learning%2520from%2520Instructor%2520%2528acquiring%2520knowledge%2520via%2520explicit%2520guidance%2529%252C%2520Learning%2520from%2520Concept%2520%2528internalizing%2520abstract%2520structures%2520and%2520generalizing%2520to%2520new%2520contexts%2529%252C%2520and%2520Learning%2520from%2520Experience%2520%2528adapting%2520through%2520accumulated%2520exploration%2520and%2520feedback%2529.%2520We%2520conduct%2520a%2520comprehensive%2520empirical%2520study%2520across%2520the%2520three%2520learning%2520dimensions%2520and%2520identify%2520several%2520insightful%2520findings%252C%2520such%2520as%2520%2528i%2529%2520interaction%2520improves%2520learning%253B%2520%2528ii%2529%2520conceptual%2520understanding%2520is%2520scale-emergent%2520and%2520benefits%2520larger%2520models%253B%2520and%2520%2528iii%2529%2520LLMs%2520are%2520effective%2520few-shot%2520learners%2520but%2520not%2520many-shot%2520learners.%2520Based%2520on%2520our%2520framework%2520and%2520empirical%2520findings%252C%2520we%2520introduce%2520a%2520benchmark%2520that%2520provides%2520a%2520unified%2520and%2520realistic%2520evaluation%2520of%2520LLMs%2527%2520general%2520learning%2520abilities%2520across%2520three%2520learning%2520cognition%2520dimensions.%2520It%2520enables%2520diagnostic%2520insights%2520and%2520supports%2520evaluation%2520and%2520development%2520of%2520more%2520adaptive%2520and%2520human-like%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13464v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Learning%20Mind%20of%20Language%20Models%3A%20A%20Cognitive%20Framework%20and%20Empirical%20Study&entry.906535625=Zhengyu%20Hu%20and%20Jianxun%20Lian%20and%20Zheyuan%20Xiao%20and%20Seraphina%20Zhang%20and%20Tianfu%20Wang%20and%20Nicholas%20Jing%20Yuan%20and%20Xing%20Xie%20and%20Hui%20Xiong&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20across%20tasks%20such%20as%20mathematics%2C%20coding%2C%20and%20reasoning%2C%20yet%20their%20learning%20ability%2C%20which%20is%20crucial%20for%20adapting%20to%20dynamic%20environments%20and%20acquiring%20new%20knowledge%2C%20remains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20introducing%20a%20framework%20inspired%20by%20cognitive%20psychology%20and%20education.%20Specifically%2C%20we%20decompose%20general%20learning%20ability%20into%20three%20distinct%2C%20complementary%20dimensions%3A%20Learning%20from%20Instructor%20%28acquiring%20knowledge%20via%20explicit%20guidance%29%2C%20Learning%20from%20Concept%20%28internalizing%20abstract%20structures%20and%20generalizing%20to%20new%20contexts%29%2C%20and%20Learning%20from%20Experience%20%28adapting%20through%20accumulated%20exploration%20and%20feedback%29.%20We%20conduct%20a%20comprehensive%20empirical%20study%20across%20the%20three%20learning%20dimensions%20and%20identify%20several%20insightful%20findings%2C%20such%20as%20%28i%29%20interaction%20improves%20learning%3B%20%28ii%29%20conceptual%20understanding%20is%20scale-emergent%20and%20benefits%20larger%20models%3B%20and%20%28iii%29%20LLMs%20are%20effective%20few-shot%20learners%20but%20not%20many-shot%20learners.%20Based%20on%20our%20framework%20and%20empirical%20findings%2C%20we%20introduce%20a%20benchmark%20that%20provides%20a%20unified%20and%20realistic%20evaluation%20of%20LLMs%27%20general%20learning%20abilities%20across%20three%20learning%20cognition%20dimensions.%20It%20enables%20diagnostic%20insights%20and%20supports%20evaluation%20and%20development%20of%20more%20adaptive%20and%20human-like%20models.&entry.1838667208=http%3A//arxiv.org/abs/2506.13464v3&entry.124074799=Read"},
{"title": "fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding", "author": "Yuxiang Wei and Yanteng Zhang and Xi Xiao and Chengxuan Qian and Tianyang Wang and Vince D. Calhoun", "abstract": "Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.", "link": "http://arxiv.org/abs/2511.21760v2", "date": "2025-12-26", "relevancy": 2.8629, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20fMRI-LM%3A%20Towards%20a%20Universal%20Foundation%20Model%20for%20Language-Aligned%20fMRI%20Understanding&body=Title%3A%20fMRI-LM%3A%20Towards%20a%20Universal%20Foundation%20Model%20for%20Language-Aligned%20fMRI%20Understanding%0AAuthor%3A%20Yuxiang%20Wei%20and%20Yanteng%20Zhang%20and%20Xi%20Xiao%20and%20Chengxuan%20Qian%20and%20Tianyang%20Wang%20and%20Vince%20D.%20Calhoun%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20enabled%20unified%20reasoning%20across%20images%2C%20audio%2C%20and%20video%2C%20but%20extending%20such%20capability%20to%20brain%20imaging%20remains%20largely%20unexplored.%20Bridging%20this%20gap%20is%20essential%20to%20link%20neural%20activity%20with%20semantic%20cognition%20and%20to%20develop%20cross-modal%20brain%20representations.%20To%20this%20end%2C%20we%20present%20fMRI-LM%2C%20a%20foundational%20model%20that%20bridges%20functional%20MRI%20%28fMRI%29%20and%20language%20through%20a%20three-stage%20framework.%20In%20Stage%201%2C%20we%20learn%20a%20neural%20tokenizer%20that%20maps%20fMRI%20into%20discrete%20tokens%20embedded%20in%20a%20language-consistent%20space.%20In%20Stage%202%2C%20a%20pretrained%20LLM%20is%20adapted%20to%20jointly%20model%20fMRI%20tokens%20and%20text%2C%20treating%20brain%20activity%20as%20a%20sequence%20that%20can%20be%20temporally%20predicted%20and%20linguistically%20described.%20To%20overcome%20the%20lack%20of%20natural%20fMRI-text%20pairs%2C%20we%20construct%20a%20large%20descriptive%20corpus%20that%20translates%20diverse%20imaging-based%20features%20into%20structured%20textual%20descriptors%2C%20capturing%20the%20low-level%20organization%20of%20fMRI%20signals.%20In%20Stage%203%2C%20we%20perform%20multi-task%2C%20multi-paradigm%20instruction%20tuning%20to%20endow%20fMRI-LM%20with%20high-level%20semantic%20understanding%2C%20supporting%20diverse%20downstream%20applications.%20Across%20various%20benchmarks%2C%20fMRI-LM%20achieves%20strong%20zero-shot%20and%20few-shot%20performance%2C%20and%20adapts%20efficiently%20with%20parameter-efficient%20tuning%20%28LoRA%29%2C%20establishing%20a%20scalable%20pathway%20toward%20a%20language-aligned%2C%20universal%20model%20for%20structural%20and%20semantic%20understanding%20of%20fMRI.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DfMRI-LM%253A%2520Towards%2520a%2520Universal%2520Foundation%2520Model%2520for%2520Language-Aligned%2520fMRI%2520Understanding%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Yanteng%2520Zhang%2520and%2520Xi%2520Xiao%2520and%2520Chengxuan%2520Qian%2520and%2520Tianyang%2520Wang%2520and%2520Vince%2520D.%2520Calhoun%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520unified%2520reasoning%2520across%2520images%252C%2520audio%252C%2520and%2520video%252C%2520but%2520extending%2520such%2520capability%2520to%2520brain%2520imaging%2520remains%2520largely%2520unexplored.%2520Bridging%2520this%2520gap%2520is%2520essential%2520to%2520link%2520neural%2520activity%2520with%2520semantic%2520cognition%2520and%2520to%2520develop%2520cross-modal%2520brain%2520representations.%2520To%2520this%2520end%252C%2520we%2520present%2520fMRI-LM%252C%2520a%2520foundational%2520model%2520that%2520bridges%2520functional%2520MRI%2520%2528fMRI%2529%2520and%2520language%2520through%2520a%2520three-stage%2520framework.%2520In%2520Stage%25201%252C%2520we%2520learn%2520a%2520neural%2520tokenizer%2520that%2520maps%2520fMRI%2520into%2520discrete%2520tokens%2520embedded%2520in%2520a%2520language-consistent%2520space.%2520In%2520Stage%25202%252C%2520a%2520pretrained%2520LLM%2520is%2520adapted%2520to%2520jointly%2520model%2520fMRI%2520tokens%2520and%2520text%252C%2520treating%2520brain%2520activity%2520as%2520a%2520sequence%2520that%2520can%2520be%2520temporally%2520predicted%2520and%2520linguistically%2520described.%2520To%2520overcome%2520the%2520lack%2520of%2520natural%2520fMRI-text%2520pairs%252C%2520we%2520construct%2520a%2520large%2520descriptive%2520corpus%2520that%2520translates%2520diverse%2520imaging-based%2520features%2520into%2520structured%2520textual%2520descriptors%252C%2520capturing%2520the%2520low-level%2520organization%2520of%2520fMRI%2520signals.%2520In%2520Stage%25203%252C%2520we%2520perform%2520multi-task%252C%2520multi-paradigm%2520instruction%2520tuning%2520to%2520endow%2520fMRI-LM%2520with%2520high-level%2520semantic%2520understanding%252C%2520supporting%2520diverse%2520downstream%2520applications.%2520Across%2520various%2520benchmarks%252C%2520fMRI-LM%2520achieves%2520strong%2520zero-shot%2520and%2520few-shot%2520performance%252C%2520and%2520adapts%2520efficiently%2520with%2520parameter-efficient%2520tuning%2520%2528LoRA%2529%252C%2520establishing%2520a%2520scalable%2520pathway%2520toward%2520a%2520language-aligned%252C%2520universal%2520model%2520for%2520structural%2520and%2520semantic%2520understanding%2520of%2520fMRI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=fMRI-LM%3A%20Towards%20a%20Universal%20Foundation%20Model%20for%20Language-Aligned%20fMRI%20Understanding&entry.906535625=Yuxiang%20Wei%20and%20Yanteng%20Zhang%20and%20Xi%20Xiao%20and%20Chengxuan%20Qian%20and%20Tianyang%20Wang%20and%20Vince%20D.%20Calhoun&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20enabled%20unified%20reasoning%20across%20images%2C%20audio%2C%20and%20video%2C%20but%20extending%20such%20capability%20to%20brain%20imaging%20remains%20largely%20unexplored.%20Bridging%20this%20gap%20is%20essential%20to%20link%20neural%20activity%20with%20semantic%20cognition%20and%20to%20develop%20cross-modal%20brain%20representations.%20To%20this%20end%2C%20we%20present%20fMRI-LM%2C%20a%20foundational%20model%20that%20bridges%20functional%20MRI%20%28fMRI%29%20and%20language%20through%20a%20three-stage%20framework.%20In%20Stage%201%2C%20we%20learn%20a%20neural%20tokenizer%20that%20maps%20fMRI%20into%20discrete%20tokens%20embedded%20in%20a%20language-consistent%20space.%20In%20Stage%202%2C%20a%20pretrained%20LLM%20is%20adapted%20to%20jointly%20model%20fMRI%20tokens%20and%20text%2C%20treating%20brain%20activity%20as%20a%20sequence%20that%20can%20be%20temporally%20predicted%20and%20linguistically%20described.%20To%20overcome%20the%20lack%20of%20natural%20fMRI-text%20pairs%2C%20we%20construct%20a%20large%20descriptive%20corpus%20that%20translates%20diverse%20imaging-based%20features%20into%20structured%20textual%20descriptors%2C%20capturing%20the%20low-level%20organization%20of%20fMRI%20signals.%20In%20Stage%203%2C%20we%20perform%20multi-task%2C%20multi-paradigm%20instruction%20tuning%20to%20endow%20fMRI-LM%20with%20high-level%20semantic%20understanding%2C%20supporting%20diverse%20downstream%20applications.%20Across%20various%20benchmarks%2C%20fMRI-LM%20achieves%20strong%20zero-shot%20and%20few-shot%20performance%2C%20and%20adapts%20efficiently%20with%20parameter-efficient%20tuning%20%28LoRA%29%2C%20establishing%20a%20scalable%20pathway%20toward%20a%20language-aligned%2C%20universal%20model%20for%20structural%20and%20semantic%20understanding%20of%20fMRI.&entry.1838667208=http%3A//arxiv.org/abs/2511.21760v2&entry.124074799=Read"},
{"title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning", "author": "Shuoshuo Zhang and Yizhen Zhang and Jingjing Fu and Lei Song and Jiang Bian and Yujiu Yang and Rui Wang", "abstract": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.", "link": "http://arxiv.org/abs/2512.22120v1", "date": "2025-12-26", "relevancy": 2.8592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20Less%2C%20See%20Right%3A%20Bi-directional%20Perceptual%20Shaping%20For%20Multimodal%20Reasoning&body=Title%3A%20See%20Less%2C%20See%20Right%3A%20Bi-directional%20Perceptual%20Shaping%20For%20Multimodal%20Reasoning%0AAuthor%3A%20Shuoshuo%20Zhang%20and%20Yizhen%20Zhang%20and%20Jingjing%20Fu%20and%20Lei%20Song%20and%20Jiang%20Bian%20and%20Yujiu%20Yang%20and%20Rui%20Wang%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20often%20benefit%20from%20intermediate%20visual%20cues%2C%20either%20injected%20via%20external%20tools%20or%20generated%20as%20latent%20visual%20tokens%20during%20reasoning%2C%20but%20these%20mechanisms%20still%20overlook%20fine-grained%20visual%20evidence%20%28e.g.%2C%20polylines%20in%20charts%29%2C%20generalize%20poorly%20across%20domains%2C%20and%20incur%20high%20inference-time%20cost.%20In%20this%20paper%2C%20we%20propose%20Bi-directional%20Perceptual%20Shaping%20%28BiPS%29%2C%20which%20transforms%20question-conditioned%20masked%20views%20into%20bidirectional%20where-to-look%20signals%20that%20shape%20perception%20during%20training.%20BiPS%20first%20applies%20a%20KL-consistency%20constraint%20between%20the%20original%20image%20and%20an%20evidence-preserving%20view%20that%20keeps%20only%20question-relevant%20regions%2C%20encouraging%20coarse%20but%20complete%20coverage%20of%20supporting%20pixels.%20It%20then%20applies%20a%20KL-separation%20constraint%20between%20the%20original%20and%20an%20evidence-ablated%20view%20where%20critical%20pixels%20are%20masked%20so%20the%20image%20no%20longer%20supports%20the%20original%20answer%2C%20discouraging%20text-only%20shortcuts%20%28i.e.%2C%20answering%20from%20text%20alone%29%20and%20enforcing%20fine-grained%20visual%20reliance.%20Across%20eight%20benchmarks%2C%20BiPS%20boosts%20Qwen2.5-VL-7B%20by%208.2%25%20on%20average%20and%20shows%20strong%20out-of-domain%20generalization%20to%20unseen%20datasets%20and%20image%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520Less%252C%2520See%2520Right%253A%2520Bi-directional%2520Perceptual%2520Shaping%2520For%2520Multimodal%2520Reasoning%26entry.906535625%3DShuoshuo%2520Zhang%2520and%2520Yizhen%2520Zhang%2520and%2520Jingjing%2520Fu%2520and%2520Lei%2520Song%2520and%2520Jiang%2520Bian%2520and%2520Yujiu%2520Yang%2520and%2520Rui%2520Wang%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520benefit%2520from%2520intermediate%2520visual%2520cues%252C%2520either%2520injected%2520via%2520external%2520tools%2520or%2520generated%2520as%2520latent%2520visual%2520tokens%2520during%2520reasoning%252C%2520but%2520these%2520mechanisms%2520still%2520overlook%2520fine-grained%2520visual%2520evidence%2520%2528e.g.%252C%2520polylines%2520in%2520charts%2529%252C%2520generalize%2520poorly%2520across%2520domains%252C%2520and%2520incur%2520high%2520inference-time%2520cost.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Bi-directional%2520Perceptual%2520Shaping%2520%2528BiPS%2529%252C%2520which%2520transforms%2520question-conditioned%2520masked%2520views%2520into%2520bidirectional%2520where-to-look%2520signals%2520that%2520shape%2520perception%2520during%2520training.%2520BiPS%2520first%2520applies%2520a%2520KL-consistency%2520constraint%2520between%2520the%2520original%2520image%2520and%2520an%2520evidence-preserving%2520view%2520that%2520keeps%2520only%2520question-relevant%2520regions%252C%2520encouraging%2520coarse%2520but%2520complete%2520coverage%2520of%2520supporting%2520pixels.%2520It%2520then%2520applies%2520a%2520KL-separation%2520constraint%2520between%2520the%2520original%2520and%2520an%2520evidence-ablated%2520view%2520where%2520critical%2520pixels%2520are%2520masked%2520so%2520the%2520image%2520no%2520longer%2520supports%2520the%2520original%2520answer%252C%2520discouraging%2520text-only%2520shortcuts%2520%2528i.e.%252C%2520answering%2520from%2520text%2520alone%2529%2520and%2520enforcing%2520fine-grained%2520visual%2520reliance.%2520Across%2520eight%2520benchmarks%252C%2520BiPS%2520boosts%2520Qwen2.5-VL-7B%2520by%25208.2%2525%2520on%2520average%2520and%2520shows%2520strong%2520out-of-domain%2520generalization%2520to%2520unseen%2520datasets%2520and%2520image%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20Less%2C%20See%20Right%3A%20Bi-directional%20Perceptual%20Shaping%20For%20Multimodal%20Reasoning&entry.906535625=Shuoshuo%20Zhang%20and%20Yizhen%20Zhang%20and%20Jingjing%20Fu%20and%20Lei%20Song%20and%20Jiang%20Bian%20and%20Yujiu%20Yang%20and%20Rui%20Wang&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20often%20benefit%20from%20intermediate%20visual%20cues%2C%20either%20injected%20via%20external%20tools%20or%20generated%20as%20latent%20visual%20tokens%20during%20reasoning%2C%20but%20these%20mechanisms%20still%20overlook%20fine-grained%20visual%20evidence%20%28e.g.%2C%20polylines%20in%20charts%29%2C%20generalize%20poorly%20across%20domains%2C%20and%20incur%20high%20inference-time%20cost.%20In%20this%20paper%2C%20we%20propose%20Bi-directional%20Perceptual%20Shaping%20%28BiPS%29%2C%20which%20transforms%20question-conditioned%20masked%20views%20into%20bidirectional%20where-to-look%20signals%20that%20shape%20perception%20during%20training.%20BiPS%20first%20applies%20a%20KL-consistency%20constraint%20between%20the%20original%20image%20and%20an%20evidence-preserving%20view%20that%20keeps%20only%20question-relevant%20regions%2C%20encouraging%20coarse%20but%20complete%20coverage%20of%20supporting%20pixels.%20It%20then%20applies%20a%20KL-separation%20constraint%20between%20the%20original%20and%20an%20evidence-ablated%20view%20where%20critical%20pixels%20are%20masked%20so%20the%20image%20no%20longer%20supports%20the%20original%20answer%2C%20discouraging%20text-only%20shortcuts%20%28i.e.%2C%20answering%20from%20text%20alone%29%20and%20enforcing%20fine-grained%20visual%20reliance.%20Across%20eight%20benchmarks%2C%20BiPS%20boosts%20Qwen2.5-VL-7B%20by%208.2%25%20on%20average%20and%20shows%20strong%20out-of-domain%20generalization%20to%20unseen%20datasets%20and%20image%20types.&entry.1838667208=http%3A//arxiv.org/abs/2512.22120v1&entry.124074799=Read"},
{"title": "Transformer Driven Visual Servoing for Fabric Texture Matching Using Dual-Arm Manipulator", "author": "Fuyuki Tokuda and Akira Seino and Akinari Kobayashi and Kai Tang and Kazuhiro Kosuge", "abstract": "In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.", "link": "http://arxiv.org/abs/2511.21203v4", "date": "2025-12-26", "relevancy": 2.8429, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6083}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5582}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Driven%20Visual%20Servoing%20for%20Fabric%20Texture%20Matching%20Using%20Dual-Arm%20Manipulator&body=Title%3A%20Transformer%20Driven%20Visual%20Servoing%20for%20Fabric%20Texture%20Matching%20Using%20Dual-Arm%20Manipulator%0AAuthor%3A%20Fuyuki%20Tokuda%20and%20Akira%20Seino%20and%20Akinari%20Kobayashi%20and%20Kai%20Tang%20and%20Kazuhiro%20Kosuge%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20align%20and%20place%20a%20fabric%20piece%20on%20top%20of%20another%20using%20a%20dual-arm%20manipulator%20and%20a%20grayscale%20camera%2C%20so%20that%20their%20surface%20textures%20are%20accurately%20matched.%20We%20propose%20a%20novel%20control%20scheme%20that%20combines%20Transformer-driven%20visual%20servoing%20with%20dualarm%20impedance%20control.%20This%20approach%20enables%20the%20system%20to%20simultaneously%20control%20the%20pose%20of%20the%20fabric%20piece%20and%20place%20it%20onto%20the%20underlying%20one%20while%20applying%20tension%20to%20keep%20the%20fabric%20piece%20flat.%20Our%20transformer-based%20network%20incorporates%20pretrained%20backbones%20and%20a%20newly%20introduced%20Difference%20Extraction%20Attention%20Module%20%28DEAM%29%2C%20which%20significantly%20enhances%20pose%20difference%20prediction%20accuracy.%20Trained%20entirely%20on%20synthetic%20images%20generated%20using%20rendering%20software%2C%20the%20network%20enables%20zero-shot%20deployment%20in%20real-world%20scenarios%20without%20requiring%20prior%20training%20on%20specific%20fabric%20textures.%20Real-world%20experiments%20demonstrate%20that%20the%20proposed%20system%20accurately%20aligns%20fabric%20pieces%20with%20different%20textures.%0ALink%3A%20http%3A//arxiv.org/abs/2511.21203v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Driven%2520Visual%2520Servoing%2520for%2520Fabric%2520Texture%2520Matching%2520Using%2520Dual-Arm%2520Manipulator%26entry.906535625%3DFuyuki%2520Tokuda%2520and%2520Akira%2520Seino%2520and%2520Akinari%2520Kobayashi%2520and%2520Kai%2520Tang%2520and%2520Kazuhiro%2520Kosuge%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520to%2520align%2520and%2520place%2520a%2520fabric%2520piece%2520on%2520top%2520of%2520another%2520using%2520a%2520dual-arm%2520manipulator%2520and%2520a%2520grayscale%2520camera%252C%2520so%2520that%2520their%2520surface%2520textures%2520are%2520accurately%2520matched.%2520We%2520propose%2520a%2520novel%2520control%2520scheme%2520that%2520combines%2520Transformer-driven%2520visual%2520servoing%2520with%2520dualarm%2520impedance%2520control.%2520This%2520approach%2520enables%2520the%2520system%2520to%2520simultaneously%2520control%2520the%2520pose%2520of%2520the%2520fabric%2520piece%2520and%2520place%2520it%2520onto%2520the%2520underlying%2520one%2520while%2520applying%2520tension%2520to%2520keep%2520the%2520fabric%2520piece%2520flat.%2520Our%2520transformer-based%2520network%2520incorporates%2520pretrained%2520backbones%2520and%2520a%2520newly%2520introduced%2520Difference%2520Extraction%2520Attention%2520Module%2520%2528DEAM%2529%252C%2520which%2520significantly%2520enhances%2520pose%2520difference%2520prediction%2520accuracy.%2520Trained%2520entirely%2520on%2520synthetic%2520images%2520generated%2520using%2520rendering%2520software%252C%2520the%2520network%2520enables%2520zero-shot%2520deployment%2520in%2520real-world%2520scenarios%2520without%2520requiring%2520prior%2520training%2520on%2520specific%2520fabric%2520textures.%2520Real-world%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520system%2520accurately%2520aligns%2520fabric%2520pieces%2520with%2520different%2520textures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.21203v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Driven%20Visual%20Servoing%20for%20Fabric%20Texture%20Matching%20Using%20Dual-Arm%20Manipulator&entry.906535625=Fuyuki%20Tokuda%20and%20Akira%20Seino%20and%20Akinari%20Kobayashi%20and%20Kai%20Tang%20and%20Kazuhiro%20Kosuge&entry.1292438233=In%20this%20paper%2C%20we%20propose%20a%20method%20to%20align%20and%20place%20a%20fabric%20piece%20on%20top%20of%20another%20using%20a%20dual-arm%20manipulator%20and%20a%20grayscale%20camera%2C%20so%20that%20their%20surface%20textures%20are%20accurately%20matched.%20We%20propose%20a%20novel%20control%20scheme%20that%20combines%20Transformer-driven%20visual%20servoing%20with%20dualarm%20impedance%20control.%20This%20approach%20enables%20the%20system%20to%20simultaneously%20control%20the%20pose%20of%20the%20fabric%20piece%20and%20place%20it%20onto%20the%20underlying%20one%20while%20applying%20tension%20to%20keep%20the%20fabric%20piece%20flat.%20Our%20transformer-based%20network%20incorporates%20pretrained%20backbones%20and%20a%20newly%20introduced%20Difference%20Extraction%20Attention%20Module%20%28DEAM%29%2C%20which%20significantly%20enhances%20pose%20difference%20prediction%20accuracy.%20Trained%20entirely%20on%20synthetic%20images%20generated%20using%20rendering%20software%2C%20the%20network%20enables%20zero-shot%20deployment%20in%20real-world%20scenarios%20without%20requiring%20prior%20training%20on%20specific%20fabric%20textures.%20Real-world%20experiments%20demonstrate%20that%20the%20proposed%20system%20accurately%20aligns%20fabric%20pieces%20with%20different%20textures.&entry.1838667208=http%3A//arxiv.org/abs/2511.21203v4&entry.124074799=Read"},
{"title": "LVLM-Aided Alignment of Task-Specific Vision Models", "author": "Alexander Koebler and Lukas Kuhn and Ingo Thon and Florian Buettner", "abstract": "In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.", "link": "http://arxiv.org/abs/2512.21985v1", "date": "2025-12-26", "relevancy": 2.8244, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVLM-Aided%20Alignment%20of%20Task-Specific%20Vision%20Models&body=Title%3A%20LVLM-Aided%20Alignment%20of%20Task-Specific%20Vision%20Models%0AAuthor%3A%20Alexander%20Koebler%20and%20Lukas%20Kuhn%20and%20Ingo%20Thon%20and%20Florian%20Buettner%0AAbstract%3A%20In%20high-stakes%20domains%2C%20small%20task-specific%20vision%20models%20are%20crucial%20due%20to%20their%20low%20computational%20requirements%20and%20the%20availability%20of%20numerous%20methods%20to%20explain%20their%20results.%20However%2C%20these%20explanations%20often%20reveal%20that%20the%20models%20do%20not%20align%20well%20with%20human%20domain%20knowledge%2C%20relying%20instead%20on%20spurious%20correlations.%20This%20might%20result%20in%20brittle%20behavior%20once%20deployed%20in%20the%20real-world.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20and%20efficient%20method%20for%20aligning%20small%20task-specific%20vision%20models%20with%20human%20domain%20knowledge%20by%20leveraging%20the%20generalization%20capabilities%20of%20a%20Large%20Vision%20Language%20Model%20%28LVLM%29.%20Our%20LVLM-Aided%20Visual%20Alignment%20%28LVLM-VA%29%20method%20provides%20a%20bidirectional%20interface%20that%20translates%20model%20behavior%20into%20natural%20language%20and%20maps%20human%20class-level%20specifications%20to%20image-level%20critiques%2C%20enabling%20effective%20interaction%20between%20domain%20experts%20and%20the%20model.%20Our%20method%20demonstrates%20substantial%20improvement%20in%20aligning%20model%20behavior%20with%20human%20specifications%2C%20as%20validated%20on%20both%20synthetic%20and%20real-world%20datasets.%20We%20show%20that%20it%20effectively%20reduces%20the%20model%27s%20dependence%20on%20spurious%20features%20and%20on%20group-specific%20biases%2C%20without%20requiring%20fine-grained%20feedback.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVLM-Aided%2520Alignment%2520of%2520Task-Specific%2520Vision%2520Models%26entry.906535625%3DAlexander%2520Koebler%2520and%2520Lukas%2520Kuhn%2520and%2520Ingo%2520Thon%2520and%2520Florian%2520Buettner%26entry.1292438233%3DIn%2520high-stakes%2520domains%252C%2520small%2520task-specific%2520vision%2520models%2520are%2520crucial%2520due%2520to%2520their%2520low%2520computational%2520requirements%2520and%2520the%2520availability%2520of%2520numerous%2520methods%2520to%2520explain%2520their%2520results.%2520However%252C%2520these%2520explanations%2520often%2520reveal%2520that%2520the%2520models%2520do%2520not%2520align%2520well%2520with%2520human%2520domain%2520knowledge%252C%2520relying%2520instead%2520on%2520spurious%2520correlations.%2520This%2520might%2520result%2520in%2520brittle%2520behavior%2520once%2520deployed%2520in%2520the%2520real-world.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520and%2520efficient%2520method%2520for%2520aligning%2520small%2520task-specific%2520vision%2520models%2520with%2520human%2520domain%2520knowledge%2520by%2520leveraging%2520the%2520generalization%2520capabilities%2520of%2520a%2520Large%2520Vision%2520Language%2520Model%2520%2528LVLM%2529.%2520Our%2520LVLM-Aided%2520Visual%2520Alignment%2520%2528LVLM-VA%2529%2520method%2520provides%2520a%2520bidirectional%2520interface%2520that%2520translates%2520model%2520behavior%2520into%2520natural%2520language%2520and%2520maps%2520human%2520class-level%2520specifications%2520to%2520image-level%2520critiques%252C%2520enabling%2520effective%2520interaction%2520between%2520domain%2520experts%2520and%2520the%2520model.%2520Our%2520method%2520demonstrates%2520substantial%2520improvement%2520in%2520aligning%2520model%2520behavior%2520with%2520human%2520specifications%252C%2520as%2520validated%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets.%2520We%2520show%2520that%2520it%2520effectively%2520reduces%2520the%2520model%2527s%2520dependence%2520on%2520spurious%2520features%2520and%2520on%2520group-specific%2520biases%252C%2520without%2520requiring%2520fine-grained%2520feedback.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVLM-Aided%20Alignment%20of%20Task-Specific%20Vision%20Models&entry.906535625=Alexander%20Koebler%20and%20Lukas%20Kuhn%20and%20Ingo%20Thon%20and%20Florian%20Buettner&entry.1292438233=In%20high-stakes%20domains%2C%20small%20task-specific%20vision%20models%20are%20crucial%20due%20to%20their%20low%20computational%20requirements%20and%20the%20availability%20of%20numerous%20methods%20to%20explain%20their%20results.%20However%2C%20these%20explanations%20often%20reveal%20that%20the%20models%20do%20not%20align%20well%20with%20human%20domain%20knowledge%2C%20relying%20instead%20on%20spurious%20correlations.%20This%20might%20result%20in%20brittle%20behavior%20once%20deployed%20in%20the%20real-world.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20novel%20and%20efficient%20method%20for%20aligning%20small%20task-specific%20vision%20models%20with%20human%20domain%20knowledge%20by%20leveraging%20the%20generalization%20capabilities%20of%20a%20Large%20Vision%20Language%20Model%20%28LVLM%29.%20Our%20LVLM-Aided%20Visual%20Alignment%20%28LVLM-VA%29%20method%20provides%20a%20bidirectional%20interface%20that%20translates%20model%20behavior%20into%20natural%20language%20and%20maps%20human%20class-level%20specifications%20to%20image-level%20critiques%2C%20enabling%20effective%20interaction%20between%20domain%20experts%20and%20the%20model.%20Our%20method%20demonstrates%20substantial%20improvement%20in%20aligning%20model%20behavior%20with%20human%20specifications%2C%20as%20validated%20on%20both%20synthetic%20and%20real-world%20datasets.%20We%20show%20that%20it%20effectively%20reduces%20the%20model%27s%20dependence%20on%20spurious%20features%20and%20on%20group-specific%20biases%2C%20without%20requiring%20fine-grained%20feedback.&entry.1838667208=http%3A//arxiv.org/abs/2512.21985v1&entry.124074799=Read"},
{"title": "A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation", "author": "Qi Lai and JunYan Li and Qiang Cai and Lei Wang and Tao Yan and XiaoKun Liang", "abstract": "Real-time instance segmentation for spinal endoscopy is important for identifying and protecting critical anatomy during surgery, but it is difficult because of the narrow field of view, specular highlights, smoke/bleeding, unclear boundaries, and large scale changes. Deployment is also constrained by limited surgical hardware, so the model must balance accuracy and speed and remain stable under small-batch (even batch-1) training. We propose LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head. The backbone uses a C2f-Pro module that combines RepViT-style re-parameterized convolution (RVB) with efficient multi-scale attention (EMA), enabling multi-branch training while collapsing into a single fast path for inference. The neck improves cross-scale consistency and boundary detail using Scale-Sequence Feature Fusion (SSFF) and Triple Feature Encoding (TFE), which strengthens high-resolution features. The head adopts a Lightweight Multi-task Shared Head (LMSH) with shared convolutions and GroupNorm to reduce parameters and support batch-1 stability. We also release the clinically reviewed PELD dataset (61 patients, 610 images) with instance masks for adipose tissue, bone, ligamentum flavum, and nerve. Experiments show that LMSF-A is highly competitive (or even better than) in all evaluation metrics and much lighter than most instance segmentation methods requiring only 1.8M parameters and 8.8 GFLOPs, and it generalizes well to a public teeth benchmark. Code and dataset: https://github.com/hhwmortal/PELD-Instance-segmentation.", "link": "http://arxiv.org/abs/2512.21984v1", "date": "2025-12-26", "relevancy": 2.7807, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5577}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Multi-Scale%20Attention%20Framework%20for%20Real-Time%20Spinal%20Endoscopic%20Instance%20Segmentation&body=Title%3A%20A%20Lightweight%20Multi-Scale%20Attention%20Framework%20for%20Real-Time%20Spinal%20Endoscopic%20Instance%20Segmentation%0AAuthor%3A%20Qi%20Lai%20and%20JunYan%20Li%20and%20Qiang%20Cai%20and%20Lei%20Wang%20and%20Tao%20Yan%20and%20XiaoKun%20Liang%0AAbstract%3A%20Real-time%20instance%20segmentation%20for%20spinal%20endoscopy%20is%20important%20for%20identifying%20and%20protecting%20critical%20anatomy%20during%20surgery%2C%20but%20it%20is%20difficult%20because%20of%20the%20narrow%20field%20of%20view%2C%20specular%20highlights%2C%20smoke/bleeding%2C%20unclear%20boundaries%2C%20and%20large%20scale%20changes.%20Deployment%20is%20also%20constrained%20by%20limited%20surgical%20hardware%2C%20so%20the%20model%20must%20balance%20accuracy%20and%20speed%20and%20remain%20stable%20under%20small-batch%20%28even%20batch-1%29%20training.%20We%20propose%20LMSF-A%2C%20a%20lightweight%20multi-scale%20attention%20framework%20co-designed%20across%20backbone%2C%20neck%2C%20and%20head.%20The%20backbone%20uses%20a%20C2f-Pro%20module%20that%20combines%20RepViT-style%20re-parameterized%20convolution%20%28RVB%29%20with%20efficient%20multi-scale%20attention%20%28EMA%29%2C%20enabling%20multi-branch%20training%20while%20collapsing%20into%20a%20single%20fast%20path%20for%20inference.%20The%20neck%20improves%20cross-scale%20consistency%20and%20boundary%20detail%20using%20Scale-Sequence%20Feature%20Fusion%20%28SSFF%29%20and%20Triple%20Feature%20Encoding%20%28TFE%29%2C%20which%20strengthens%20high-resolution%20features.%20The%20head%20adopts%20a%20Lightweight%20Multi-task%20Shared%20Head%20%28LMSH%29%20with%20shared%20convolutions%20and%20GroupNorm%20to%20reduce%20parameters%20and%20support%20batch-1%20stability.%20We%20also%20release%20the%20clinically%20reviewed%20PELD%20dataset%20%2861%20patients%2C%20610%20images%29%20with%20instance%20masks%20for%20adipose%20tissue%2C%20bone%2C%20ligamentum%20flavum%2C%20and%20nerve.%20Experiments%20show%20that%20LMSF-A%20is%20highly%20competitive%20%28or%20even%20better%20than%29%20in%20all%20evaluation%20metrics%20and%20much%20lighter%20than%20most%20instance%20segmentation%20methods%20requiring%20only%201.8M%20parameters%20and%208.8%20GFLOPs%2C%20and%20it%20generalizes%20well%20to%20a%20public%20teeth%20benchmark.%20Code%20and%20dataset%3A%20https%3A//github.com/hhwmortal/PELD-Instance-segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Multi-Scale%2520Attention%2520Framework%2520for%2520Real-Time%2520Spinal%2520Endoscopic%2520Instance%2520Segmentation%26entry.906535625%3DQi%2520Lai%2520and%2520JunYan%2520Li%2520and%2520Qiang%2520Cai%2520and%2520Lei%2520Wang%2520and%2520Tao%2520Yan%2520and%2520XiaoKun%2520Liang%26entry.1292438233%3DReal-time%2520instance%2520segmentation%2520for%2520spinal%2520endoscopy%2520is%2520important%2520for%2520identifying%2520and%2520protecting%2520critical%2520anatomy%2520during%2520surgery%252C%2520but%2520it%2520is%2520difficult%2520because%2520of%2520the%2520narrow%2520field%2520of%2520view%252C%2520specular%2520highlights%252C%2520smoke/bleeding%252C%2520unclear%2520boundaries%252C%2520and%2520large%2520scale%2520changes.%2520Deployment%2520is%2520also%2520constrained%2520by%2520limited%2520surgical%2520hardware%252C%2520so%2520the%2520model%2520must%2520balance%2520accuracy%2520and%2520speed%2520and%2520remain%2520stable%2520under%2520small-batch%2520%2528even%2520batch-1%2529%2520training.%2520We%2520propose%2520LMSF-A%252C%2520a%2520lightweight%2520multi-scale%2520attention%2520framework%2520co-designed%2520across%2520backbone%252C%2520neck%252C%2520and%2520head.%2520The%2520backbone%2520uses%2520a%2520C2f-Pro%2520module%2520that%2520combines%2520RepViT-style%2520re-parameterized%2520convolution%2520%2528RVB%2529%2520with%2520efficient%2520multi-scale%2520attention%2520%2528EMA%2529%252C%2520enabling%2520multi-branch%2520training%2520while%2520collapsing%2520into%2520a%2520single%2520fast%2520path%2520for%2520inference.%2520The%2520neck%2520improves%2520cross-scale%2520consistency%2520and%2520boundary%2520detail%2520using%2520Scale-Sequence%2520Feature%2520Fusion%2520%2528SSFF%2529%2520and%2520Triple%2520Feature%2520Encoding%2520%2528TFE%2529%252C%2520which%2520strengthens%2520high-resolution%2520features.%2520The%2520head%2520adopts%2520a%2520Lightweight%2520Multi-task%2520Shared%2520Head%2520%2528LMSH%2529%2520with%2520shared%2520convolutions%2520and%2520GroupNorm%2520to%2520reduce%2520parameters%2520and%2520support%2520batch-1%2520stability.%2520We%2520also%2520release%2520the%2520clinically%2520reviewed%2520PELD%2520dataset%2520%252861%2520patients%252C%2520610%2520images%2529%2520with%2520instance%2520masks%2520for%2520adipose%2520tissue%252C%2520bone%252C%2520ligamentum%2520flavum%252C%2520and%2520nerve.%2520Experiments%2520show%2520that%2520LMSF-A%2520is%2520highly%2520competitive%2520%2528or%2520even%2520better%2520than%2529%2520in%2520all%2520evaluation%2520metrics%2520and%2520much%2520lighter%2520than%2520most%2520instance%2520segmentation%2520methods%2520requiring%2520only%25201.8M%2520parameters%2520and%25208.8%2520GFLOPs%252C%2520and%2520it%2520generalizes%2520well%2520to%2520a%2520public%2520teeth%2520benchmark.%2520Code%2520and%2520dataset%253A%2520https%253A//github.com/hhwmortal/PELD-Instance-segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Multi-Scale%20Attention%20Framework%20for%20Real-Time%20Spinal%20Endoscopic%20Instance%20Segmentation&entry.906535625=Qi%20Lai%20and%20JunYan%20Li%20and%20Qiang%20Cai%20and%20Lei%20Wang%20and%20Tao%20Yan%20and%20XiaoKun%20Liang&entry.1292438233=Real-time%20instance%20segmentation%20for%20spinal%20endoscopy%20is%20important%20for%20identifying%20and%20protecting%20critical%20anatomy%20during%20surgery%2C%20but%20it%20is%20difficult%20because%20of%20the%20narrow%20field%20of%20view%2C%20specular%20highlights%2C%20smoke/bleeding%2C%20unclear%20boundaries%2C%20and%20large%20scale%20changes.%20Deployment%20is%20also%20constrained%20by%20limited%20surgical%20hardware%2C%20so%20the%20model%20must%20balance%20accuracy%20and%20speed%20and%20remain%20stable%20under%20small-batch%20%28even%20batch-1%29%20training.%20We%20propose%20LMSF-A%2C%20a%20lightweight%20multi-scale%20attention%20framework%20co-designed%20across%20backbone%2C%20neck%2C%20and%20head.%20The%20backbone%20uses%20a%20C2f-Pro%20module%20that%20combines%20RepViT-style%20re-parameterized%20convolution%20%28RVB%29%20with%20efficient%20multi-scale%20attention%20%28EMA%29%2C%20enabling%20multi-branch%20training%20while%20collapsing%20into%20a%20single%20fast%20path%20for%20inference.%20The%20neck%20improves%20cross-scale%20consistency%20and%20boundary%20detail%20using%20Scale-Sequence%20Feature%20Fusion%20%28SSFF%29%20and%20Triple%20Feature%20Encoding%20%28TFE%29%2C%20which%20strengthens%20high-resolution%20features.%20The%20head%20adopts%20a%20Lightweight%20Multi-task%20Shared%20Head%20%28LMSH%29%20with%20shared%20convolutions%20and%20GroupNorm%20to%20reduce%20parameters%20and%20support%20batch-1%20stability.%20We%20also%20release%20the%20clinically%20reviewed%20PELD%20dataset%20%2861%20patients%2C%20610%20images%29%20with%20instance%20masks%20for%20adipose%20tissue%2C%20bone%2C%20ligamentum%20flavum%2C%20and%20nerve.%20Experiments%20show%20that%20LMSF-A%20is%20highly%20competitive%20%28or%20even%20better%20than%29%20in%20all%20evaluation%20metrics%20and%20much%20lighter%20than%20most%20instance%20segmentation%20methods%20requiring%20only%201.8M%20parameters%20and%208.8%20GFLOPs%2C%20and%20it%20generalizes%20well%20to%20a%20public%20teeth%20benchmark.%20Code%20and%20dataset%3A%20https%3A//github.com/hhwmortal/PELD-Instance-segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2512.21984v1&entry.124074799=Read"},
{"title": "Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification", "author": "Theo Di Piazza and Carole Lazarus and Olivier Nempont and Loic Boussel", "abstract": "The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload. Multi-label classification of Three-Dimensional (3D) CT scans is a challenging task due to the volumetric nature of the data and the variety of anomalies to be detected. Existing deep learning methods based on Convolutional Neural Networks (CNNs) struggle to capture long-range dependencies effectively, while Vision Transformers require extensive pre-training, posing challenges for practical use. Additionally, these existing methods do not explicitly model the radiologist's navigational behavior while scrolling through CT scan slices, which requires both global context understanding and local detail awareness. In this study, we present CT-Scroll, a novel global-local attention model specifically designed to emulate the scrolling behavior of radiologists during the analysis of 3D CT scans. Our approach is evaluated on two public datasets, demonstrating its efficacy through comprehensive experiments and an ablation study that highlights the contribution of each model component.", "link": "http://arxiv.org/abs/2503.20652v6", "date": "2025-12-26", "relevancy": 2.7778, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5465}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification&body=Title%3A%20Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification%0AAuthor%3A%20Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel%0AAbstract%3A%20The%20rapid%20increase%20in%20the%20number%20of%20Computed%20Tomography%20%28CT%29%20scan%20examinations%20has%20created%20an%20urgent%20need%20for%20automated%20tools%2C%20such%20as%20organ%20segmentation%2C%20anomaly%20classification%2C%20and%20report%20generation%2C%20to%20assist%20radiologists%20with%20their%20growing%20workload.%20Multi-label%20classification%20of%20Three-Dimensional%20%283D%29%20CT%20scans%20is%20a%20challenging%20task%20due%20to%20the%20volumetric%20nature%20of%20the%20data%20and%20the%20variety%20of%20anomalies%20to%20be%20detected.%20Existing%20deep%20learning%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%20to%20capture%20long-range%20dependencies%20effectively%2C%20while%20Vision%20Transformers%20require%20extensive%20pre-training%2C%20posing%20challenges%20for%20practical%20use.%20Additionally%2C%20these%20existing%20methods%20do%20not%20explicitly%20model%20the%20radiologist%27s%20navigational%20behavior%20while%20scrolling%20through%20CT%20scan%20slices%2C%20which%20requires%20both%20global%20context%20understanding%20and%20local%20detail%20awareness.%20In%20this%20study%2C%20we%20present%20CT-Scroll%2C%20a%20novel%20global-local%20attention%20model%20specifically%20designed%20to%20emulate%20the%20scrolling%20behavior%20of%20radiologists%20during%20the%20analysis%20of%203D%20CT%20scans.%20Our%20approach%20is%20evaluated%20on%20two%20public%20datasets%2C%20demonstrating%20its%20efficacy%20through%20comprehensive%20experiments%20and%20an%20ablation%20study%20that%20highlights%20the%20contribution%20of%20each%20model%20component.%0ALink%3A%20http%3A//arxiv.org/abs/2503.20652v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitating%2520Radiological%2520Scrolling%253A%2520A%2520Global-Local%2520Attention%2520Model%2520for%25203D%2520Chest%2520CT%2520Volumes%2520Multi-Label%2520Anomaly%2520Classification%26entry.906535625%3DTheo%2520Di%2520Piazza%2520and%2520Carole%2520Lazarus%2520and%2520Olivier%2520Nempont%2520and%2520Loic%2520Boussel%26entry.1292438233%3DThe%2520rapid%2520increase%2520in%2520the%2520number%2520of%2520Computed%2520Tomography%2520%2528CT%2529%2520scan%2520examinations%2520has%2520created%2520an%2520urgent%2520need%2520for%2520automated%2520tools%252C%2520such%2520as%2520organ%2520segmentation%252C%2520anomaly%2520classification%252C%2520and%2520report%2520generation%252C%2520to%2520assist%2520radiologists%2520with%2520their%2520growing%2520workload.%2520Multi-label%2520classification%2520of%2520Three-Dimensional%2520%25283D%2529%2520CT%2520scans%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520volumetric%2520nature%2520of%2520the%2520data%2520and%2520the%2520variety%2520of%2520anomalies%2520to%2520be%2520detected.%2520Existing%2520deep%2520learning%2520methods%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520struggle%2520to%2520capture%2520long-range%2520dependencies%2520effectively%252C%2520while%2520Vision%2520Transformers%2520require%2520extensive%2520pre-training%252C%2520posing%2520challenges%2520for%2520practical%2520use.%2520Additionally%252C%2520these%2520existing%2520methods%2520do%2520not%2520explicitly%2520model%2520the%2520radiologist%2527s%2520navigational%2520behavior%2520while%2520scrolling%2520through%2520CT%2520scan%2520slices%252C%2520which%2520requires%2520both%2520global%2520context%2520understanding%2520and%2520local%2520detail%2520awareness.%2520In%2520this%2520study%252C%2520we%2520present%2520CT-Scroll%252C%2520a%2520novel%2520global-local%2520attention%2520model%2520specifically%2520designed%2520to%2520emulate%2520the%2520scrolling%2520behavior%2520of%2520radiologists%2520during%2520the%2520analysis%2520of%25203D%2520CT%2520scans.%2520Our%2520approach%2520is%2520evaluated%2520on%2520two%2520public%2520datasets%252C%2520demonstrating%2520its%2520efficacy%2520through%2520comprehensive%2520experiments%2520and%2520an%2520ablation%2520study%2520that%2520highlights%2520the%2520contribution%2520of%2520each%2520model%2520component.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20652v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitating%20Radiological%20Scrolling%3A%20A%20Global-Local%20Attention%20Model%20for%203D%20Chest%20CT%20Volumes%20Multi-Label%20Anomaly%20Classification&entry.906535625=Theo%20Di%20Piazza%20and%20Carole%20Lazarus%20and%20Olivier%20Nempont%20and%20Loic%20Boussel&entry.1292438233=The%20rapid%20increase%20in%20the%20number%20of%20Computed%20Tomography%20%28CT%29%20scan%20examinations%20has%20created%20an%20urgent%20need%20for%20automated%20tools%2C%20such%20as%20organ%20segmentation%2C%20anomaly%20classification%2C%20and%20report%20generation%2C%20to%20assist%20radiologists%20with%20their%20growing%20workload.%20Multi-label%20classification%20of%20Three-Dimensional%20%283D%29%20CT%20scans%20is%20a%20challenging%20task%20due%20to%20the%20volumetric%20nature%20of%20the%20data%20and%20the%20variety%20of%20anomalies%20to%20be%20detected.%20Existing%20deep%20learning%20methods%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20struggle%20to%20capture%20long-range%20dependencies%20effectively%2C%20while%20Vision%20Transformers%20require%20extensive%20pre-training%2C%20posing%20challenges%20for%20practical%20use.%20Additionally%2C%20these%20existing%20methods%20do%20not%20explicitly%20model%20the%20radiologist%27s%20navigational%20behavior%20while%20scrolling%20through%20CT%20scan%20slices%2C%20which%20requires%20both%20global%20context%20understanding%20and%20local%20detail%20awareness.%20In%20this%20study%2C%20we%20present%20CT-Scroll%2C%20a%20novel%20global-local%20attention%20model%20specifically%20designed%20to%20emulate%20the%20scrolling%20behavior%20of%20radiologists%20during%20the%20analysis%20of%203D%20CT%20scans.%20Our%20approach%20is%20evaluated%20on%20two%20public%20datasets%2C%20demonstrating%20its%20efficacy%20through%20comprehensive%20experiments%20and%20an%20ablation%20study%20that%20highlights%20the%20contribution%20of%20each%20model%20component.&entry.1838667208=http%3A//arxiv.org/abs/2503.20652v6&entry.124074799=Read"},
{"title": "Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy", "author": "Liansheng Wang and Xinke Zhang and Chenhui Li and Dongjiao He and Yihan Pan and Jianjun Yi", "abstract": "LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous systems, yet its deployment on resource-constrained platforms remains challenging due to computational and memory limitations. We propose Super-LIO, a robust LIO system that demands both high performance and accuracy, ideal for applications such as aerial robots and mobile autonomous systems. At the core of Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that limits each voxel to eight fused subvoxels, enabling strict point density control and incremental denoising during map updates. This design enables a simple yet efficient and accurate map structure, which can be easily integrated into existing LIO frameworks. Additionally, Super-LIO designs a heuristic-guided KNN strategy (HKNN) that accelerates the correspondence search by leveraging spatial locality, further reducing runtime overhead. We evaluated the proposed system using four publicly available datasets and several self-collected datasets, totaling more than 30 sequences. Extensive testing on both X86 and ARM platforms confirms that Super-LIO offers superior efficiency and robustness, while maintaining competitive accuracy. Super-LIO processes each frame approximately 73% faster than SOTA, while consuming less CPU resources. The system is fully open-source and plug-and-play compatible with a wide range of LiDAR sensors and platforms. The implementation is available at: https://github.com/Liansheng-Wang/Super-LIO.git", "link": "http://arxiv.org/abs/2509.05723v2", "date": "2025-12-26", "relevancy": 2.6983, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5676}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5329}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-LIO%3A%20A%20Robust%20and%20Efficient%20LiDAR-Inertial%20Odometry%20System%20with%20a%20Compact%20Mapping%20Strategy&body=Title%3A%20Super-LIO%3A%20A%20Robust%20and%20Efficient%20LiDAR-Inertial%20Odometry%20System%20with%20a%20Compact%20Mapping%20Strategy%0AAuthor%3A%20Liansheng%20Wang%20and%20Xinke%20Zhang%20and%20Chenhui%20Li%20and%20Dongjiao%20He%20and%20Yihan%20Pan%20and%20Jianjun%20Yi%0AAbstract%3A%20LiDAR-Inertial%20Odometry%20%28LIO%29%20is%20a%20foundational%20technique%20for%20autonomous%20systems%2C%20yet%20its%20deployment%20on%20resource-constrained%20platforms%20remains%20challenging%20due%20to%20computational%20and%20memory%20limitations.%20We%20propose%20Super-LIO%2C%20a%20robust%20LIO%20system%20that%20demands%20both%20high%20performance%20and%20accuracy%2C%20ideal%20for%20applications%20such%20as%20aerial%20robots%20and%20mobile%20autonomous%20systems.%20At%20the%20core%20of%20Super-LIO%20is%20a%20compact%20octo-voxel-based%20map%20structure%2C%20termed%20OctVox%2C%20that%20limits%20each%20voxel%20to%20eight%20fused%20subvoxels%2C%20enabling%20strict%20point%20density%20control%20and%20incremental%20denoising%20during%20map%20updates.%20This%20design%20enables%20a%20simple%20yet%20efficient%20and%20accurate%20map%20structure%2C%20which%20can%20be%20easily%20integrated%20into%20existing%20LIO%20frameworks.%20Additionally%2C%20Super-LIO%20designs%20a%20heuristic-guided%20KNN%20strategy%20%28HKNN%29%20that%20accelerates%20the%20correspondence%20search%20by%20leveraging%20spatial%20locality%2C%20further%20reducing%20runtime%20overhead.%20We%20evaluated%20the%20proposed%20system%20using%20four%20publicly%20available%20datasets%20and%20several%20self-collected%20datasets%2C%20totaling%20more%20than%2030%20sequences.%20Extensive%20testing%20on%20both%20X86%20and%20ARM%20platforms%20confirms%20that%20Super-LIO%20offers%20superior%20efficiency%20and%20robustness%2C%20while%20maintaining%20competitive%20accuracy.%20Super-LIO%20processes%20each%20frame%20approximately%2073%25%20faster%20than%20SOTA%2C%20while%20consuming%20less%20CPU%20resources.%20The%20system%20is%20fully%20open-source%20and%20plug-and-play%20compatible%20with%20a%20wide%20range%20of%20LiDAR%20sensors%20and%20platforms.%20The%20implementation%20is%20available%20at%3A%20https%3A//github.com/Liansheng-Wang/Super-LIO.git%0ALink%3A%20http%3A//arxiv.org/abs/2509.05723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-LIO%253A%2520A%2520Robust%2520and%2520Efficient%2520LiDAR-Inertial%2520Odometry%2520System%2520with%2520a%2520Compact%2520Mapping%2520Strategy%26entry.906535625%3DLiansheng%2520Wang%2520and%2520Xinke%2520Zhang%2520and%2520Chenhui%2520Li%2520and%2520Dongjiao%2520He%2520and%2520Yihan%2520Pan%2520and%2520Jianjun%2520Yi%26entry.1292438233%3DLiDAR-Inertial%2520Odometry%2520%2528LIO%2529%2520is%2520a%2520foundational%2520technique%2520for%2520autonomous%2520systems%252C%2520yet%2520its%2520deployment%2520on%2520resource-constrained%2520platforms%2520remains%2520challenging%2520due%2520to%2520computational%2520and%2520memory%2520limitations.%2520We%2520propose%2520Super-LIO%252C%2520a%2520robust%2520LIO%2520system%2520that%2520demands%2520both%2520high%2520performance%2520and%2520accuracy%252C%2520ideal%2520for%2520applications%2520such%2520as%2520aerial%2520robots%2520and%2520mobile%2520autonomous%2520systems.%2520At%2520the%2520core%2520of%2520Super-LIO%2520is%2520a%2520compact%2520octo-voxel-based%2520map%2520structure%252C%2520termed%2520OctVox%252C%2520that%2520limits%2520each%2520voxel%2520to%2520eight%2520fused%2520subvoxels%252C%2520enabling%2520strict%2520point%2520density%2520control%2520and%2520incremental%2520denoising%2520during%2520map%2520updates.%2520This%2520design%2520enables%2520a%2520simple%2520yet%2520efficient%2520and%2520accurate%2520map%2520structure%252C%2520which%2520can%2520be%2520easily%2520integrated%2520into%2520existing%2520LIO%2520frameworks.%2520Additionally%252C%2520Super-LIO%2520designs%2520a%2520heuristic-guided%2520KNN%2520strategy%2520%2528HKNN%2529%2520that%2520accelerates%2520the%2520correspondence%2520search%2520by%2520leveraging%2520spatial%2520locality%252C%2520further%2520reducing%2520runtime%2520overhead.%2520We%2520evaluated%2520the%2520proposed%2520system%2520using%2520four%2520publicly%2520available%2520datasets%2520and%2520several%2520self-collected%2520datasets%252C%2520totaling%2520more%2520than%252030%2520sequences.%2520Extensive%2520testing%2520on%2520both%2520X86%2520and%2520ARM%2520platforms%2520confirms%2520that%2520Super-LIO%2520offers%2520superior%2520efficiency%2520and%2520robustness%252C%2520while%2520maintaining%2520competitive%2520accuracy.%2520Super-LIO%2520processes%2520each%2520frame%2520approximately%252073%2525%2520faster%2520than%2520SOTA%252C%2520while%2520consuming%2520less%2520CPU%2520resources.%2520The%2520system%2520is%2520fully%2520open-source%2520and%2520plug-and-play%2520compatible%2520with%2520a%2520wide%2520range%2520of%2520LiDAR%2520sensors%2520and%2520platforms.%2520The%2520implementation%2520is%2520available%2520at%253A%2520https%253A//github.com/Liansheng-Wang/Super-LIO.git%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-LIO%3A%20A%20Robust%20and%20Efficient%20LiDAR-Inertial%20Odometry%20System%20with%20a%20Compact%20Mapping%20Strategy&entry.906535625=Liansheng%20Wang%20and%20Xinke%20Zhang%20and%20Chenhui%20Li%20and%20Dongjiao%20He%20and%20Yihan%20Pan%20and%20Jianjun%20Yi&entry.1292438233=LiDAR-Inertial%20Odometry%20%28LIO%29%20is%20a%20foundational%20technique%20for%20autonomous%20systems%2C%20yet%20its%20deployment%20on%20resource-constrained%20platforms%20remains%20challenging%20due%20to%20computational%20and%20memory%20limitations.%20We%20propose%20Super-LIO%2C%20a%20robust%20LIO%20system%20that%20demands%20both%20high%20performance%20and%20accuracy%2C%20ideal%20for%20applications%20such%20as%20aerial%20robots%20and%20mobile%20autonomous%20systems.%20At%20the%20core%20of%20Super-LIO%20is%20a%20compact%20octo-voxel-based%20map%20structure%2C%20termed%20OctVox%2C%20that%20limits%20each%20voxel%20to%20eight%20fused%20subvoxels%2C%20enabling%20strict%20point%20density%20control%20and%20incremental%20denoising%20during%20map%20updates.%20This%20design%20enables%20a%20simple%20yet%20efficient%20and%20accurate%20map%20structure%2C%20which%20can%20be%20easily%20integrated%20into%20existing%20LIO%20frameworks.%20Additionally%2C%20Super-LIO%20designs%20a%20heuristic-guided%20KNN%20strategy%20%28HKNN%29%20that%20accelerates%20the%20correspondence%20search%20by%20leveraging%20spatial%20locality%2C%20further%20reducing%20runtime%20overhead.%20We%20evaluated%20the%20proposed%20system%20using%20four%20publicly%20available%20datasets%20and%20several%20self-collected%20datasets%2C%20totaling%20more%20than%2030%20sequences.%20Extensive%20testing%20on%20both%20X86%20and%20ARM%20platforms%20confirms%20that%20Super-LIO%20offers%20superior%20efficiency%20and%20robustness%2C%20while%20maintaining%20competitive%20accuracy.%20Super-LIO%20processes%20each%20frame%20approximately%2073%25%20faster%20than%20SOTA%2C%20while%20consuming%20less%20CPU%20resources.%20The%20system%20is%20fully%20open-source%20and%20plug-and-play%20compatible%20with%20a%20wide%20range%20of%20LiDAR%20sensors%20and%20platforms.%20The%20implementation%20is%20available%20at%3A%20https%3A//github.com/Liansheng-Wang/Super-LIO.git&entry.1838667208=http%3A//arxiv.org/abs/2509.05723v2&entry.124074799=Read"},
{"title": "Patch-Discontinuity Mining for Generalized Deepfake Detection", "author": "Huanhuan Yuan and Yang Ping and Zhengqin Xu and Junyi Cao and Shuai Jia and Chao Ma", "abstract": "The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.", "link": "http://arxiv.org/abs/2512.22027v1", "date": "2025-12-26", "relevancy": 2.6533, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.554}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5204}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Discontinuity%20Mining%20for%20Generalized%20Deepfake%20Detection&body=Title%3A%20Patch-Discontinuity%20Mining%20for%20Generalized%20Deepfake%20Detection%0AAuthor%3A%20Huanhuan%20Yuan%20and%20Yang%20Ping%20and%20Zhengqin%20Xu%20and%20Junyi%20Cao%20and%20Shuai%20Jia%20and%20Chao%20Ma%0AAbstract%3A%20The%20rapid%20advancement%20of%20generative%20artificial%20intelligence%20has%20enabled%20the%20creation%20of%20highly%20realistic%20fake%20facial%20images%2C%20posing%20serious%20threats%20to%20personal%20privacy%20and%20the%20integrity%20of%20online%20information.%20Existing%20deepfake%20detection%20methods%20often%20rely%20on%20handcrafted%20forensic%20cues%20and%20complex%20architectures%2C%20achieving%20strong%20performance%20in%20intra-domain%20settings%20but%20suffering%20significant%20degradation%20when%20confronted%20with%20unseen%20forgery%20patterns.%20In%20this%20paper%2C%20we%20propose%20GenDF%2C%20a%20simple%20yet%20effective%20framework%20that%20transfers%20a%20powerful%20large-scale%20vision%20model%20to%20the%20deepfake%20detection%20task%20with%20a%20compact%20and%20neat%20network%20design.%20GenDF%20incorporates%20deepfake-specific%20representation%20learning%20to%20capture%20discriminative%20patterns%20between%20real%20and%20fake%20facial%20images%2C%20feature%20space%20redistribution%20to%20mitigate%20distribution%20mismatch%2C%20and%20a%20classification-invariant%20feature%20augmentation%20strategy%20to%20enhance%20generalization%20without%20introducing%20additional%20trainable%20parameters.%20Extensive%20experiments%20demonstrate%20that%20GenDF%20achieves%20state-of-the-art%20generalization%20performance%20in%20cross-domain%20and%20cross-manipulation%20settings%20while%20requiring%20only%200.28M%20trainable%20parameters%2C%20validating%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Discontinuity%2520Mining%2520for%2520Generalized%2520Deepfake%2520Detection%26entry.906535625%3DHuanhuan%2520Yuan%2520and%2520Yang%2520Ping%2520and%2520Zhengqin%2520Xu%2520and%2520Junyi%2520Cao%2520and%2520Shuai%2520Jia%2520and%2520Chao%2520Ma%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520generative%2520artificial%2520intelligence%2520has%2520enabled%2520the%2520creation%2520of%2520highly%2520realistic%2520fake%2520facial%2520images%252C%2520posing%2520serious%2520threats%2520to%2520personal%2520privacy%2520and%2520the%2520integrity%2520of%2520online%2520information.%2520Existing%2520deepfake%2520detection%2520methods%2520often%2520rely%2520on%2520handcrafted%2520forensic%2520cues%2520and%2520complex%2520architectures%252C%2520achieving%2520strong%2520performance%2520in%2520intra-domain%2520settings%2520but%2520suffering%2520significant%2520degradation%2520when%2520confronted%2520with%2520unseen%2520forgery%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GenDF%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520that%2520transfers%2520a%2520powerful%2520large-scale%2520vision%2520model%2520to%2520the%2520deepfake%2520detection%2520task%2520with%2520a%2520compact%2520and%2520neat%2520network%2520design.%2520GenDF%2520incorporates%2520deepfake-specific%2520representation%2520learning%2520to%2520capture%2520discriminative%2520patterns%2520between%2520real%2520and%2520fake%2520facial%2520images%252C%2520feature%2520space%2520redistribution%2520to%2520mitigate%2520distribution%2520mismatch%252C%2520and%2520a%2520classification-invariant%2520feature%2520augmentation%2520strategy%2520to%2520enhance%2520generalization%2520without%2520introducing%2520additional%2520trainable%2520parameters.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GenDF%2520achieves%2520state-of-the-art%2520generalization%2520performance%2520in%2520cross-domain%2520and%2520cross-manipulation%2520settings%2520while%2520requiring%2520only%25200.28M%2520trainable%2520parameters%252C%2520validating%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Discontinuity%20Mining%20for%20Generalized%20Deepfake%20Detection&entry.906535625=Huanhuan%20Yuan%20and%20Yang%20Ping%20and%20Zhengqin%20Xu%20and%20Junyi%20Cao%20and%20Shuai%20Jia%20and%20Chao%20Ma&entry.1292438233=The%20rapid%20advancement%20of%20generative%20artificial%20intelligence%20has%20enabled%20the%20creation%20of%20highly%20realistic%20fake%20facial%20images%2C%20posing%20serious%20threats%20to%20personal%20privacy%20and%20the%20integrity%20of%20online%20information.%20Existing%20deepfake%20detection%20methods%20often%20rely%20on%20handcrafted%20forensic%20cues%20and%20complex%20architectures%2C%20achieving%20strong%20performance%20in%20intra-domain%20settings%20but%20suffering%20significant%20degradation%20when%20confronted%20with%20unseen%20forgery%20patterns.%20In%20this%20paper%2C%20we%20propose%20GenDF%2C%20a%20simple%20yet%20effective%20framework%20that%20transfers%20a%20powerful%20large-scale%20vision%20model%20to%20the%20deepfake%20detection%20task%20with%20a%20compact%20and%20neat%20network%20design.%20GenDF%20incorporates%20deepfake-specific%20representation%20learning%20to%20capture%20discriminative%20patterns%20between%20real%20and%20fake%20facial%20images%2C%20feature%20space%20redistribution%20to%20mitigate%20distribution%20mismatch%2C%20and%20a%20classification-invariant%20feature%20augmentation%20strategy%20to%20enhance%20generalization%20without%20introducing%20additional%20trainable%20parameters.%20Extensive%20experiments%20demonstrate%20that%20GenDF%20achieves%20state-of-the-art%20generalization%20performance%20in%20cross-domain%20and%20cross-manipulation%20settings%20while%20requiring%20only%200.28M%20trainable%20parameters%2C%20validating%20the%20effectiveness%20and%20efficiency%20of%20the%20proposed%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.22027v1&entry.124074799=Read"},
{"title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "author": "Xiaofeng Mao and Zhen Li and Chuanhao Li and Xiaojie Xu and Kaining Ying and Tong He and Jiangmiao Pang and Yu Qiao and Kaipeng Zhang", "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "link": "http://arxiv.org/abs/2512.22096v1", "date": "2025-12-26", "relevancy": 2.5746, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6775}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6728}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yume-1.5%3A%20A%20Text-Controlled%20Interactive%20World%20Generation%20Model&body=Title%3A%20Yume-1.5%3A%20A%20Text-Controlled%20Interactive%20World%20Generation%20Model%0AAuthor%3A%20Xiaofeng%20Mao%20and%20Zhen%20Li%20and%20Chuanhao%20Li%20and%20Xiaojie%20Xu%20and%20Kaining%20Ying%20and%20Tong%20He%20and%20Jiangmiao%20Pang%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang%0AAbstract%3A%20Recent%20approaches%20have%20demonstrated%20the%20promise%20of%20using%20diffusion%20models%20to%20generate%20interactive%20and%20explorable%20worlds.%20However%2C%20most%20of%20these%20methods%20face%20critical%20challenges%20such%20as%20excessively%20large%20parameter%20sizes%2C%20reliance%20on%20lengthy%20inference%20steps%2C%20and%20rapidly%20growing%20historical%20context%2C%20which%20severely%20limit%20real-time%20performance%20and%20lack%20text-controlled%20generation%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Cmethod%2C%20a%20novel%20framework%20designed%20to%20generate%20realistic%2C%20interactive%2C%20and%20continuous%20worlds%20from%20a%20single%20image%20or%20text%20prompt.%20%5Cmethod%20achieves%20this%20through%20a%20carefully%20designed%20framework%20that%20supports%20keyboard-based%20exploration%20of%20the%20generated%20worlds.%20The%20framework%20comprises%20three%20core%20components%3A%20%281%29%20a%20long-video%20generation%20framework%20integrating%20unified%20context%20compression%20with%20linear%20attention%3B%20%282%29%20a%20real-time%20streaming%20acceleration%20strategy%20powered%20by%20bidirectional%20attention%20distillation%20and%20an%20enhanced%20text%20embedding%20scheme%3B%20%283%29%20a%20text-controlled%20method%20for%20generating%20world%20events.%20We%20have%20provided%20the%20codebase%20in%20the%20supplementary%20material.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYume-1.5%253A%2520A%2520Text-Controlled%2520Interactive%2520World%2520Generation%2520Model%26entry.906535625%3DXiaofeng%2520Mao%2520and%2520Zhen%2520Li%2520and%2520Chuanhao%2520Li%2520and%2520Xiaojie%2520Xu%2520and%2520Kaining%2520Ying%2520and%2520Tong%2520He%2520and%2520Jiangmiao%2520Pang%2520and%2520Yu%2520Qiao%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3DRecent%2520approaches%2520have%2520demonstrated%2520the%2520promise%2520of%2520using%2520diffusion%2520models%2520to%2520generate%2520interactive%2520and%2520explorable%2520worlds.%2520However%252C%2520most%2520of%2520these%2520methods%2520face%2520critical%2520challenges%2520such%2520as%2520excessively%2520large%2520parameter%2520sizes%252C%2520reliance%2520on%2520lengthy%2520inference%2520steps%252C%2520and%2520rapidly%2520growing%2520historical%2520context%252C%2520which%2520severely%2520limit%2520real-time%2520performance%2520and%2520lack%2520text-controlled%2520generation%2520capabilities.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Cmethod%252C%2520a%2520novel%2520framework%2520designed%2520to%2520generate%2520realistic%252C%2520interactive%252C%2520and%2520continuous%2520worlds%2520from%2520a%2520single%2520image%2520or%2520text%2520prompt.%2520%255Cmethod%2520achieves%2520this%2520through%2520a%2520carefully%2520designed%2520framework%2520that%2520supports%2520keyboard-based%2520exploration%2520of%2520the%2520generated%2520worlds.%2520The%2520framework%2520comprises%2520three%2520core%2520components%253A%2520%25281%2529%2520a%2520long-video%2520generation%2520framework%2520integrating%2520unified%2520context%2520compression%2520with%2520linear%2520attention%253B%2520%25282%2529%2520a%2520real-time%2520streaming%2520acceleration%2520strategy%2520powered%2520by%2520bidirectional%2520attention%2520distillation%2520and%2520an%2520enhanced%2520text%2520embedding%2520scheme%253B%2520%25283%2529%2520a%2520text-controlled%2520method%2520for%2520generating%2520world%2520events.%2520We%2520have%2520provided%2520the%2520codebase%2520in%2520the%2520supplementary%2520material.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yume-1.5%3A%20A%20Text-Controlled%20Interactive%20World%20Generation%20Model&entry.906535625=Xiaofeng%20Mao%20and%20Zhen%20Li%20and%20Chuanhao%20Li%20and%20Xiaojie%20Xu%20and%20Kaining%20Ying%20and%20Tong%20He%20and%20Jiangmiao%20Pang%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang&entry.1292438233=Recent%20approaches%20have%20demonstrated%20the%20promise%20of%20using%20diffusion%20models%20to%20generate%20interactive%20and%20explorable%20worlds.%20However%2C%20most%20of%20these%20methods%20face%20critical%20challenges%20such%20as%20excessively%20large%20parameter%20sizes%2C%20reliance%20on%20lengthy%20inference%20steps%2C%20and%20rapidly%20growing%20historical%20context%2C%20which%20severely%20limit%20real-time%20performance%20and%20lack%20text-controlled%20generation%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Cmethod%2C%20a%20novel%20framework%20designed%20to%20generate%20realistic%2C%20interactive%2C%20and%20continuous%20worlds%20from%20a%20single%20image%20or%20text%20prompt.%20%5Cmethod%20achieves%20this%20through%20a%20carefully%20designed%20framework%20that%20supports%20keyboard-based%20exploration%20of%20the%20generated%20worlds.%20The%20framework%20comprises%20three%20core%20components%3A%20%281%29%20a%20long-video%20generation%20framework%20integrating%20unified%20context%20compression%20with%20linear%20attention%3B%20%282%29%20a%20real-time%20streaming%20acceleration%20strategy%20powered%20by%20bidirectional%20attention%20distillation%20and%20an%20enhanced%20text%20embedding%20scheme%3B%20%283%29%20a%20text-controlled%20method%20for%20generating%20world%20events.%20We%20have%20provided%20the%20codebase%20in%20the%20supplementary%20material.&entry.1838667208=http%3A//arxiv.org/abs/2512.22096v1&entry.124074799=Read"},
{"title": "Rewards-based image analysis in microscopy", "author": "Kamyar Barakati and Yu Liu and Utkarsh Pratiush and Boris N. Slautin and Sergei V. Kalinin", "abstract": "Imaging and hyperspectral data analysis is central to progress across biology, medicine, chemistry, and physics. The core challenge lies in converting high-resolution or high-dimensional datasets into interpretable representations that enable insight into the underlying physical or chemical properties of a system. Traditional analysis relies on expert-designed, multistep workflows, such as denoising, feature extraction, clustering, dimensionality reduction, and physics-based deconvolution, or on machine learning (ML) methods that accelerate individual steps. Both approaches, however, typically demand significant human intervention, including hyperparameter tuning and data labeling. Achieving the next level of autonomy in scientific imaging requires designing effective reward-based workflows that guide algorithms toward best data representation for human or automated decision-making. Here, we discuss recent advances in reward-based workflows for image analysis, which capture key elements of human reasoning and exhibit strong transferability across various tasks. We highlight how reward-driven approaches enable a shift from supervised black-box models toward explainable, unsupervised optimization on the examples of Scanning Probe and Electron Microscopies. Such reward-based frameworks are promising for a broad range of applications, including classification, regression, structure-property mapping, and general hyperspectral data processing.", "link": "http://arxiv.org/abs/2502.18522v2", "date": "2025-12-26", "relevancy": 2.5354, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rewards-based%20image%20analysis%20in%20microscopy&body=Title%3A%20Rewards-based%20image%20analysis%20in%20microscopy%0AAuthor%3A%20Kamyar%20Barakati%20and%20Yu%20Liu%20and%20Utkarsh%20Pratiush%20and%20Boris%20N.%20Slautin%20and%20Sergei%20V.%20Kalinin%0AAbstract%3A%20Imaging%20and%20hyperspectral%20data%20analysis%20is%20central%20to%20progress%20across%20biology%2C%20medicine%2C%20chemistry%2C%20and%20physics.%20The%20core%20challenge%20lies%20in%20converting%20high-resolution%20or%20high-dimensional%20datasets%20into%20interpretable%20representations%20that%20enable%20insight%20into%20the%20underlying%20physical%20or%20chemical%20properties%20of%20a%20system.%20Traditional%20analysis%20relies%20on%20expert-designed%2C%20multistep%20workflows%2C%20such%20as%20denoising%2C%20feature%20extraction%2C%20clustering%2C%20dimensionality%20reduction%2C%20and%20physics-based%20deconvolution%2C%20or%20on%20machine%20learning%20%28ML%29%20methods%20that%20accelerate%20individual%20steps.%20Both%20approaches%2C%20however%2C%20typically%20demand%20significant%20human%20intervention%2C%20including%20hyperparameter%20tuning%20and%20data%20labeling.%20Achieving%20the%20next%20level%20of%20autonomy%20in%20scientific%20imaging%20requires%20designing%20effective%20reward-based%20workflows%20that%20guide%20algorithms%20toward%20best%20data%20representation%20for%20human%20or%20automated%20decision-making.%20Here%2C%20we%20discuss%20recent%20advances%20in%20reward-based%20workflows%20for%20image%20analysis%2C%20which%20capture%20key%20elements%20of%20human%20reasoning%20and%20exhibit%20strong%20transferability%20across%20various%20tasks.%20We%20highlight%20how%20reward-driven%20approaches%20enable%20a%20shift%20from%20supervised%20black-box%20models%20toward%20explainable%2C%20unsupervised%20optimization%20on%20the%20examples%20of%20Scanning%20Probe%20and%20Electron%20Microscopies.%20Such%20reward-based%20frameworks%20are%20promising%20for%20a%20broad%20range%20of%20applications%2C%20including%20classification%2C%20regression%2C%20structure-property%20mapping%2C%20and%20general%20hyperspectral%20data%20processing.%0ALink%3A%20http%3A//arxiv.org/abs/2502.18522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewards-based%2520image%2520analysis%2520in%2520microscopy%26entry.906535625%3DKamyar%2520Barakati%2520and%2520Yu%2520Liu%2520and%2520Utkarsh%2520Pratiush%2520and%2520Boris%2520N.%2520Slautin%2520and%2520Sergei%2520V.%2520Kalinin%26entry.1292438233%3DImaging%2520and%2520hyperspectral%2520data%2520analysis%2520is%2520central%2520to%2520progress%2520across%2520biology%252C%2520medicine%252C%2520chemistry%252C%2520and%2520physics.%2520The%2520core%2520challenge%2520lies%2520in%2520converting%2520high-resolution%2520or%2520high-dimensional%2520datasets%2520into%2520interpretable%2520representations%2520that%2520enable%2520insight%2520into%2520the%2520underlying%2520physical%2520or%2520chemical%2520properties%2520of%2520a%2520system.%2520Traditional%2520analysis%2520relies%2520on%2520expert-designed%252C%2520multistep%2520workflows%252C%2520such%2520as%2520denoising%252C%2520feature%2520extraction%252C%2520clustering%252C%2520dimensionality%2520reduction%252C%2520and%2520physics-based%2520deconvolution%252C%2520or%2520on%2520machine%2520learning%2520%2528ML%2529%2520methods%2520that%2520accelerate%2520individual%2520steps.%2520Both%2520approaches%252C%2520however%252C%2520typically%2520demand%2520significant%2520human%2520intervention%252C%2520including%2520hyperparameter%2520tuning%2520and%2520data%2520labeling.%2520Achieving%2520the%2520next%2520level%2520of%2520autonomy%2520in%2520scientific%2520imaging%2520requires%2520designing%2520effective%2520reward-based%2520workflows%2520that%2520guide%2520algorithms%2520toward%2520best%2520data%2520representation%2520for%2520human%2520or%2520automated%2520decision-making.%2520Here%252C%2520we%2520discuss%2520recent%2520advances%2520in%2520reward-based%2520workflows%2520for%2520image%2520analysis%252C%2520which%2520capture%2520key%2520elements%2520of%2520human%2520reasoning%2520and%2520exhibit%2520strong%2520transferability%2520across%2520various%2520tasks.%2520We%2520highlight%2520how%2520reward-driven%2520approaches%2520enable%2520a%2520shift%2520from%2520supervised%2520black-box%2520models%2520toward%2520explainable%252C%2520unsupervised%2520optimization%2520on%2520the%2520examples%2520of%2520Scanning%2520Probe%2520and%2520Electron%2520Microscopies.%2520Such%2520reward-based%2520frameworks%2520are%2520promising%2520for%2520a%2520broad%2520range%2520of%2520applications%252C%2520including%2520classification%252C%2520regression%252C%2520structure-property%2520mapping%252C%2520and%2520general%2520hyperspectral%2520data%2520processing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rewards-based%20image%20analysis%20in%20microscopy&entry.906535625=Kamyar%20Barakati%20and%20Yu%20Liu%20and%20Utkarsh%20Pratiush%20and%20Boris%20N.%20Slautin%20and%20Sergei%20V.%20Kalinin&entry.1292438233=Imaging%20and%20hyperspectral%20data%20analysis%20is%20central%20to%20progress%20across%20biology%2C%20medicine%2C%20chemistry%2C%20and%20physics.%20The%20core%20challenge%20lies%20in%20converting%20high-resolution%20or%20high-dimensional%20datasets%20into%20interpretable%20representations%20that%20enable%20insight%20into%20the%20underlying%20physical%20or%20chemical%20properties%20of%20a%20system.%20Traditional%20analysis%20relies%20on%20expert-designed%2C%20multistep%20workflows%2C%20such%20as%20denoising%2C%20feature%20extraction%2C%20clustering%2C%20dimensionality%20reduction%2C%20and%20physics-based%20deconvolution%2C%20or%20on%20machine%20learning%20%28ML%29%20methods%20that%20accelerate%20individual%20steps.%20Both%20approaches%2C%20however%2C%20typically%20demand%20significant%20human%20intervention%2C%20including%20hyperparameter%20tuning%20and%20data%20labeling.%20Achieving%20the%20next%20level%20of%20autonomy%20in%20scientific%20imaging%20requires%20designing%20effective%20reward-based%20workflows%20that%20guide%20algorithms%20toward%20best%20data%20representation%20for%20human%20or%20automated%20decision-making.%20Here%2C%20we%20discuss%20recent%20advances%20in%20reward-based%20workflows%20for%20image%20analysis%2C%20which%20capture%20key%20elements%20of%20human%20reasoning%20and%20exhibit%20strong%20transferability%20across%20various%20tasks.%20We%20highlight%20how%20reward-driven%20approaches%20enable%20a%20shift%20from%20supervised%20black-box%20models%20toward%20explainable%2C%20unsupervised%20optimization%20on%20the%20examples%20of%20Scanning%20Probe%20and%20Electron%20Microscopies.%20Such%20reward-based%20frameworks%20are%20promising%20for%20a%20broad%20range%20of%20applications%2C%20including%20classification%2C%20regression%2C%20structure-property%20mapping%2C%20and%20general%20hyperspectral%20data%20processing.&entry.1838667208=http%3A//arxiv.org/abs/2502.18522v2&entry.124074799=Read"},
{"title": "Modeling high dimensional point clouds with the spherical cluster model", "author": "Fr\u00e9d\u00e9ric Cazals and Antoine Commaret and Louis Goldenberg", "abstract": "A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\\em spherical cluster model} (SC) approximates a finite point set $P\\subset \\mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $\u03b7\\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $\u03b7=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.\n  First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $\u03b7$, and for high dimensional datasets (say $d>100$) whatever the value of $\u03b7$. Second, the center of the SC model behave as a parameterized high-dimensional median.\n  The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper.", "link": "http://arxiv.org/abs/2512.21960v1", "date": "2025-12-26", "relevancy": 2.48, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4986}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4986}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20high%20dimensional%20point%20clouds%20with%20the%20spherical%20cluster%20model&body=Title%3A%20Modeling%20high%20dimensional%20point%20clouds%20with%20the%20spherical%20cluster%20model%0AAuthor%3A%20Fr%C3%A9d%C3%A9ric%20Cazals%20and%20Antoine%20Commaret%20and%20Louis%20Goldenberg%0AAbstract%3A%20A%20parametric%20cluster%20model%20is%20a%20statistical%20model%20providing%20geometric%20insights%20onto%20the%20points%20defining%20a%20cluster.%20The%20%7B%5Cem%20spherical%20cluster%20model%7D%20%28SC%29%20approximates%20a%20finite%20point%20set%20%24P%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20by%20a%20sphere%20%24S%28c%2Cr%29%24%20as%20follows.%20Taking%20%24r%24%20as%20a%20fraction%20%24%CE%B7%5Cin%280%2C1%29%24%20%28hyper-parameter%29%20of%20the%20std%20deviation%20of%20distances%20between%20the%20center%20%24c%24%20and%20the%20data%20points%2C%20the%20cost%20of%20the%20SC%20model%20is%20the%20sum%20over%20all%20data%20points%20lying%20outside%20the%20sphere%20%24S%24%20of%20their%20power%20distance%20with%20respect%20to%20%24S%24.%20The%20center%20%24c%24%20of%20the%20SC%20model%20is%20the%20point%20minimizing%20this%20cost.%20Note%20that%20%24%CE%B7%3D0%24%20yields%20the%20celebrated%20center%20of%20mass%20used%20in%20KMeans%20clustering.%20We%20make%20three%20contributions.%0A%20%20First%2C%20we%20show%20fitting%20a%20spherical%20cluster%20yields%20a%20strictly%20convex%20but%20not%20smooth%20combinatorial%20optimization%20problem.%20Second%2C%20we%20present%20an%20exact%20solver%20using%20the%20Clarke%20gradient%20on%20a%20suitable%20stratified%20cell%20complex%20defined%20from%20an%20arrangement%20of%20hyper-spheres.%20Finally%2C%20we%20present%20experiments%20on%20a%20variety%20of%20datasets%20ranging%20in%20dimension%20from%20%24d%3D9%24%20to%20%24d%3D10%2C000%24%2C%20with%20two%20main%20observations.%20First%2C%20the%20exact%20algorithm%20is%20orders%20of%20magnitude%20faster%20than%20BFGS%20based%20heuristics%20for%20datasets%20of%20small/intermediate%20dimension%20and%20small%20values%20of%20%24%CE%B7%24%2C%20and%20for%20high%20dimensional%20datasets%20%28say%20%24d%3E100%24%29%20whatever%20the%20value%20of%20%24%CE%B7%24.%20Second%2C%20the%20center%20of%20the%20SC%20model%20behave%20as%20a%20parameterized%20high-dimensional%20median.%0A%20%20The%20SC%20model%20is%20of%20direct%20interest%20for%20high%20dimensional%20multivariate%20data%20analysis%2C%20and%20the%20application%20to%20the%20design%20of%20mixtures%20of%20SC%20will%20be%20reported%20in%20a%20companion%20paper.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520high%2520dimensional%2520point%2520clouds%2520with%2520the%2520spherical%2520cluster%2520model%26entry.906535625%3DFr%25C3%25A9d%25C3%25A9ric%2520Cazals%2520and%2520Antoine%2520Commaret%2520and%2520Louis%2520Goldenberg%26entry.1292438233%3DA%2520parametric%2520cluster%2520model%2520is%2520a%2520statistical%2520model%2520providing%2520geometric%2520insights%2520onto%2520the%2520points%2520defining%2520a%2520cluster.%2520The%2520%257B%255Cem%2520spherical%2520cluster%2520model%257D%2520%2528SC%2529%2520approximates%2520a%2520finite%2520point%2520set%2520%2524P%255Csubset%2520%255Cmathbb%257BR%257D%255Ed%2524%2520by%2520a%2520sphere%2520%2524S%2528c%252Cr%2529%2524%2520as%2520follows.%2520Taking%2520%2524r%2524%2520as%2520a%2520fraction%2520%2524%25CE%25B7%255Cin%25280%252C1%2529%2524%2520%2528hyper-parameter%2529%2520of%2520the%2520std%2520deviation%2520of%2520distances%2520between%2520the%2520center%2520%2524c%2524%2520and%2520the%2520data%2520points%252C%2520the%2520cost%2520of%2520the%2520SC%2520model%2520is%2520the%2520sum%2520over%2520all%2520data%2520points%2520lying%2520outside%2520the%2520sphere%2520%2524S%2524%2520of%2520their%2520power%2520distance%2520with%2520respect%2520to%2520%2524S%2524.%2520The%2520center%2520%2524c%2524%2520of%2520the%2520SC%2520model%2520is%2520the%2520point%2520minimizing%2520this%2520cost.%2520Note%2520that%2520%2524%25CE%25B7%253D0%2524%2520yields%2520the%2520celebrated%2520center%2520of%2520mass%2520used%2520in%2520KMeans%2520clustering.%2520We%2520make%2520three%2520contributions.%250A%2520%2520First%252C%2520we%2520show%2520fitting%2520a%2520spherical%2520cluster%2520yields%2520a%2520strictly%2520convex%2520but%2520not%2520smooth%2520combinatorial%2520optimization%2520problem.%2520Second%252C%2520we%2520present%2520an%2520exact%2520solver%2520using%2520the%2520Clarke%2520gradient%2520on%2520a%2520suitable%2520stratified%2520cell%2520complex%2520defined%2520from%2520an%2520arrangement%2520of%2520hyper-spheres.%2520Finally%252C%2520we%2520present%2520experiments%2520on%2520a%2520variety%2520of%2520datasets%2520ranging%2520in%2520dimension%2520from%2520%2524d%253D9%2524%2520to%2520%2524d%253D10%252C000%2524%252C%2520with%2520two%2520main%2520observations.%2520First%252C%2520the%2520exact%2520algorithm%2520is%2520orders%2520of%2520magnitude%2520faster%2520than%2520BFGS%2520based%2520heuristics%2520for%2520datasets%2520of%2520small/intermediate%2520dimension%2520and%2520small%2520values%2520of%2520%2524%25CE%25B7%2524%252C%2520and%2520for%2520high%2520dimensional%2520datasets%2520%2528say%2520%2524d%253E100%2524%2529%2520whatever%2520the%2520value%2520of%2520%2524%25CE%25B7%2524.%2520Second%252C%2520the%2520center%2520of%2520the%2520SC%2520model%2520behave%2520as%2520a%2520parameterized%2520high-dimensional%2520median.%250A%2520%2520The%2520SC%2520model%2520is%2520of%2520direct%2520interest%2520for%2520high%2520dimensional%2520multivariate%2520data%2520analysis%252C%2520and%2520the%2520application%2520to%2520the%2520design%2520of%2520mixtures%2520of%2520SC%2520will%2520be%2520reported%2520in%2520a%2520companion%2520paper.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20high%20dimensional%20point%20clouds%20with%20the%20spherical%20cluster%20model&entry.906535625=Fr%C3%A9d%C3%A9ric%20Cazals%20and%20Antoine%20Commaret%20and%20Louis%20Goldenberg&entry.1292438233=A%20parametric%20cluster%20model%20is%20a%20statistical%20model%20providing%20geometric%20insights%20onto%20the%20points%20defining%20a%20cluster.%20The%20%7B%5Cem%20spherical%20cluster%20model%7D%20%28SC%29%20approximates%20a%20finite%20point%20set%20%24P%5Csubset%20%5Cmathbb%7BR%7D%5Ed%24%20by%20a%20sphere%20%24S%28c%2Cr%29%24%20as%20follows.%20Taking%20%24r%24%20as%20a%20fraction%20%24%CE%B7%5Cin%280%2C1%29%24%20%28hyper-parameter%29%20of%20the%20std%20deviation%20of%20distances%20between%20the%20center%20%24c%24%20and%20the%20data%20points%2C%20the%20cost%20of%20the%20SC%20model%20is%20the%20sum%20over%20all%20data%20points%20lying%20outside%20the%20sphere%20%24S%24%20of%20their%20power%20distance%20with%20respect%20to%20%24S%24.%20The%20center%20%24c%24%20of%20the%20SC%20model%20is%20the%20point%20minimizing%20this%20cost.%20Note%20that%20%24%CE%B7%3D0%24%20yields%20the%20celebrated%20center%20of%20mass%20used%20in%20KMeans%20clustering.%20We%20make%20three%20contributions.%0A%20%20First%2C%20we%20show%20fitting%20a%20spherical%20cluster%20yields%20a%20strictly%20convex%20but%20not%20smooth%20combinatorial%20optimization%20problem.%20Second%2C%20we%20present%20an%20exact%20solver%20using%20the%20Clarke%20gradient%20on%20a%20suitable%20stratified%20cell%20complex%20defined%20from%20an%20arrangement%20of%20hyper-spheres.%20Finally%2C%20we%20present%20experiments%20on%20a%20variety%20of%20datasets%20ranging%20in%20dimension%20from%20%24d%3D9%24%20to%20%24d%3D10%2C000%24%2C%20with%20two%20main%20observations.%20First%2C%20the%20exact%20algorithm%20is%20orders%20of%20magnitude%20faster%20than%20BFGS%20based%20heuristics%20for%20datasets%20of%20small/intermediate%20dimension%20and%20small%20values%20of%20%24%CE%B7%24%2C%20and%20for%20high%20dimensional%20datasets%20%28say%20%24d%3E100%24%29%20whatever%20the%20value%20of%20%24%CE%B7%24.%20Second%2C%20the%20center%20of%20the%20SC%20model%20behave%20as%20a%20parameterized%20high-dimensional%20median.%0A%20%20The%20SC%20model%20is%20of%20direct%20interest%20for%20high%20dimensional%20multivariate%20data%20analysis%2C%20and%20the%20application%20to%20the%20design%20of%20mixtures%20of%20SC%20will%20be%20reported%20in%20a%20companion%20paper.&entry.1838667208=http%3A//arxiv.org/abs/2512.21960v1&entry.124074799=Read"},
{"title": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification", "author": "Mykola Kuz and Ihor Lazarovych and Mykola Kozlenko and Mykola Pikuliak and Andrii Kvasniuk", "abstract": "This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.", "link": "http://arxiv.org/abs/2512.18475v2", "date": "2025-12-26", "relevancy": 2.4555, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20a%20hybrid%20LSTM-CNN-Attention%20model%20for%20text-based%20web%20content%20classification&body=Title%3A%20Research%20on%20a%20hybrid%20LSTM-CNN-Attention%20model%20for%20text-based%20web%20content%20classification%0AAuthor%3A%20Mykola%20Kuz%20and%20Ihor%20Lazarovych%20and%20Mykola%20Kozlenko%20and%20Mykola%20Pikuliak%20and%20Andrii%20Kvasniuk%0AAbstract%3A%20This%20study%20presents%20a%20hybrid%20deep%20learning%20architecture%20that%20integrates%20LSTM%2C%20CNN%2C%20and%20an%20Attention%20mechanism%20to%20enhance%20the%20classification%20of%20web%20content%20based%20on%20text.%20Pretrained%20GloVe%20embeddings%20are%20used%20to%20represent%20words%20as%20dense%20vectors%20that%20preserve%20semantic%20similarity.%20The%20CNN%20layer%20extracts%20local%20n-gram%20patterns%20and%20lexical%20features%2C%20while%20the%20LSTM%20layer%20models%20long-range%20dependencies%20and%20sequential%20structure.%20The%20integrated%20Attention%20mechanism%20enables%20the%20model%20to%20focus%20selectively%20on%20the%20most%20informative%20parts%20of%20the%20input%20sequence.%20A%205-fold%20cross-validation%20setup%20was%20used%20to%20assess%20the%20robustness%20and%20generalizability%20of%20the%20proposed%20solution.%20Experimental%20results%20show%20that%20the%20hybrid%20LSTM-CNN-Attention%20model%20achieved%20outstanding%20performance%2C%20with%20an%20accuracy%20of%200.98%2C%20precision%20of%200.94%2C%20recall%20of%200.92%2C%20and%20F1-score%20of%200.93.%20These%20results%20surpass%20the%20performance%20of%20baseline%20models%20based%20solely%20on%20CNNs%2C%20LSTMs%2C%20or%20transformer-based%20classifiers%20such%20as%20BERT.%20The%20combination%20of%20neural%20network%20components%20enabled%20the%20model%20to%20effectively%20capture%20both%20fine-grained%20text%20structures%20and%20broader%20semantic%20context.%20Furthermore%2C%20the%20use%20of%20GloVe%20embeddings%20provided%20an%20efficient%20and%20effective%20representation%20of%20textual%20data%2C%20making%20the%20model%20suitable%20for%20integration%20into%20systems%20with%20real-time%20or%20near-real-time%20requirements.%20The%20proposed%20hybrid%20architecture%20demonstrates%20high%20effectiveness%20in%20text-based%20web%20content%20classification%2C%20particularly%20in%20tasks%20requiring%20both%20syntactic%20feature%20extraction%20and%20semantic%20interpretation.%20By%20combining%20presented%20mechanisms%2C%20the%20model%20addresses%20the%20limitations%20of%20individual%20architectures%20and%20achieves%20improved%20generalization.%20These%20findings%20support%20the%20broader%20use%20of%20hybrid%20deep%20learning%20approaches%20in%20NLP%20applications%2C%20especially%20where%20complex%2C%20unstructured%20textual%20data%20must%20be%20processed%20and%20classified%20with%20high%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18475v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520a%2520hybrid%2520LSTM-CNN-Attention%2520model%2520for%2520text-based%2520web%2520content%2520classification%26entry.906535625%3DMykola%2520Kuz%2520and%2520Ihor%2520Lazarovych%2520and%2520Mykola%2520Kozlenko%2520and%2520Mykola%2520Pikuliak%2520and%2520Andrii%2520Kvasniuk%26entry.1292438233%3DThis%2520study%2520presents%2520a%2520hybrid%2520deep%2520learning%2520architecture%2520that%2520integrates%2520LSTM%252C%2520CNN%252C%2520and%2520an%2520Attention%2520mechanism%2520to%2520enhance%2520the%2520classification%2520of%2520web%2520content%2520based%2520on%2520text.%2520Pretrained%2520GloVe%2520embeddings%2520are%2520used%2520to%2520represent%2520words%2520as%2520dense%2520vectors%2520that%2520preserve%2520semantic%2520similarity.%2520The%2520CNN%2520layer%2520extracts%2520local%2520n-gram%2520patterns%2520and%2520lexical%2520features%252C%2520while%2520the%2520LSTM%2520layer%2520models%2520long-range%2520dependencies%2520and%2520sequential%2520structure.%2520The%2520integrated%2520Attention%2520mechanism%2520enables%2520the%2520model%2520to%2520focus%2520selectively%2520on%2520the%2520most%2520informative%2520parts%2520of%2520the%2520input%2520sequence.%2520A%25205-fold%2520cross-validation%2520setup%2520was%2520used%2520to%2520assess%2520the%2520robustness%2520and%2520generalizability%2520of%2520the%2520proposed%2520solution.%2520Experimental%2520results%2520show%2520that%2520the%2520hybrid%2520LSTM-CNN-Attention%2520model%2520achieved%2520outstanding%2520performance%252C%2520with%2520an%2520accuracy%2520of%25200.98%252C%2520precision%2520of%25200.94%252C%2520recall%2520of%25200.92%252C%2520and%2520F1-score%2520of%25200.93.%2520These%2520results%2520surpass%2520the%2520performance%2520of%2520baseline%2520models%2520based%2520solely%2520on%2520CNNs%252C%2520LSTMs%252C%2520or%2520transformer-based%2520classifiers%2520such%2520as%2520BERT.%2520The%2520combination%2520of%2520neural%2520network%2520components%2520enabled%2520the%2520model%2520to%2520effectively%2520capture%2520both%2520fine-grained%2520text%2520structures%2520and%2520broader%2520semantic%2520context.%2520Furthermore%252C%2520the%2520use%2520of%2520GloVe%2520embeddings%2520provided%2520an%2520efficient%2520and%2520effective%2520representation%2520of%2520textual%2520data%252C%2520making%2520the%2520model%2520suitable%2520for%2520integration%2520into%2520systems%2520with%2520real-time%2520or%2520near-real-time%2520requirements.%2520The%2520proposed%2520hybrid%2520architecture%2520demonstrates%2520high%2520effectiveness%2520in%2520text-based%2520web%2520content%2520classification%252C%2520particularly%2520in%2520tasks%2520requiring%2520both%2520syntactic%2520feature%2520extraction%2520and%2520semantic%2520interpretation.%2520By%2520combining%2520presented%2520mechanisms%252C%2520the%2520model%2520addresses%2520the%2520limitations%2520of%2520individual%2520architectures%2520and%2520achieves%2520improved%2520generalization.%2520These%2520findings%2520support%2520the%2520broader%2520use%2520of%2520hybrid%2520deep%2520learning%2520approaches%2520in%2520NLP%2520applications%252C%2520especially%2520where%2520complex%252C%2520unstructured%2520textual%2520data%2520must%2520be%2520processed%2520and%2520classified%2520with%2520high%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18475v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20a%20hybrid%20LSTM-CNN-Attention%20model%20for%20text-based%20web%20content%20classification&entry.906535625=Mykola%20Kuz%20and%20Ihor%20Lazarovych%20and%20Mykola%20Kozlenko%20and%20Mykola%20Pikuliak%20and%20Andrii%20Kvasniuk&entry.1292438233=This%20study%20presents%20a%20hybrid%20deep%20learning%20architecture%20that%20integrates%20LSTM%2C%20CNN%2C%20and%20an%20Attention%20mechanism%20to%20enhance%20the%20classification%20of%20web%20content%20based%20on%20text.%20Pretrained%20GloVe%20embeddings%20are%20used%20to%20represent%20words%20as%20dense%20vectors%20that%20preserve%20semantic%20similarity.%20The%20CNN%20layer%20extracts%20local%20n-gram%20patterns%20and%20lexical%20features%2C%20while%20the%20LSTM%20layer%20models%20long-range%20dependencies%20and%20sequential%20structure.%20The%20integrated%20Attention%20mechanism%20enables%20the%20model%20to%20focus%20selectively%20on%20the%20most%20informative%20parts%20of%20the%20input%20sequence.%20A%205-fold%20cross-validation%20setup%20was%20used%20to%20assess%20the%20robustness%20and%20generalizability%20of%20the%20proposed%20solution.%20Experimental%20results%20show%20that%20the%20hybrid%20LSTM-CNN-Attention%20model%20achieved%20outstanding%20performance%2C%20with%20an%20accuracy%20of%200.98%2C%20precision%20of%200.94%2C%20recall%20of%200.92%2C%20and%20F1-score%20of%200.93.%20These%20results%20surpass%20the%20performance%20of%20baseline%20models%20based%20solely%20on%20CNNs%2C%20LSTMs%2C%20or%20transformer-based%20classifiers%20such%20as%20BERT.%20The%20combination%20of%20neural%20network%20components%20enabled%20the%20model%20to%20effectively%20capture%20both%20fine-grained%20text%20structures%20and%20broader%20semantic%20context.%20Furthermore%2C%20the%20use%20of%20GloVe%20embeddings%20provided%20an%20efficient%20and%20effective%20representation%20of%20textual%20data%2C%20making%20the%20model%20suitable%20for%20integration%20into%20systems%20with%20real-time%20or%20near-real-time%20requirements.%20The%20proposed%20hybrid%20architecture%20demonstrates%20high%20effectiveness%20in%20text-based%20web%20content%20classification%2C%20particularly%20in%20tasks%20requiring%20both%20syntactic%20feature%20extraction%20and%20semantic%20interpretation.%20By%20combining%20presented%20mechanisms%2C%20the%20model%20addresses%20the%20limitations%20of%20individual%20architectures%20and%20achieves%20improved%20generalization.%20These%20findings%20support%20the%20broader%20use%20of%20hybrid%20deep%20learning%20approaches%20in%20NLP%20applications%2C%20especially%20where%20complex%2C%20unstructured%20textual%20data%20must%20be%20processed%20and%20classified%20with%20high%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2512.18475v2&entry.124074799=Read"},
{"title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars", "author": "Zhiyao Sun and Ziqiao Peng and Yifeng Ma and Yi Chen and Zhengguang Zhou and Zixiang Zhou and Guozhen Zhang and Youliang Zhang and Yuan Zhou and Qinglin Lu and Yong-Jin Liu", "abstract": "Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .", "link": "http://arxiv.org/abs/2512.22065v1", "date": "2025-12-26", "relevancy": 2.4357, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6388}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6171}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamAvatar%3A%20Streaming%20Diffusion%20Models%20for%20Real-Time%20Interactive%20Human%20Avatars&body=Title%3A%20StreamAvatar%3A%20Streaming%20Diffusion%20Models%20for%20Real-Time%20Interactive%20Human%20Avatars%0AAuthor%3A%20Zhiyao%20Sun%20and%20Ziqiao%20Peng%20and%20Yifeng%20Ma%20and%20Yi%20Chen%20and%20Zhengguang%20Zhou%20and%20Zixiang%20Zhou%20and%20Guozhen%20Zhang%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Yong-Jin%20Liu%0AAbstract%3A%20Real-time%2C%20streaming%20interactive%20avatars%20represent%20a%20critical%20yet%20challenging%20goal%20in%20digital%20human%20research.%20Although%20diffusion-based%20human%20avatar%20generation%20methods%20achieve%20remarkable%20success%2C%20their%20non-causal%20architecture%20and%20high%20computational%20costs%20make%20them%20unsuitable%20for%20streaming.%20Moreover%2C%20existing%20interactive%20approaches%20are%20typically%20limited%20to%20head-and-shoulder%20region%2C%20limiting%20their%20ability%20to%20produce%20gestures%20and%20body%20motions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20two-stage%20autoregressive%20adaptation%20and%20acceleration%20framework%20that%20applies%20autoregressive%20distillation%20and%20adversarial%20refinement%20to%20adapt%20a%20high-fidelity%20human%20video%20diffusion%20model%20for%20real-time%2C%20interactive%20streaming.%20To%20ensure%20long-term%20stability%20and%20consistency%2C%20we%20introduce%20three%20key%20components%3A%20a%20Reference%20Sink%2C%20a%20Reference-Anchored%20Positional%20Re-encoding%20%28RAPR%29%20strategy%2C%20and%20a%20Consistency-Aware%20Discriminator.%20Building%20on%20this%20framework%2C%20we%20develop%20a%20one-shot%2C%20interactive%2C%20human%20avatar%20model%20capable%20of%20generating%20both%20natural%20talking%20and%20listening%20behaviors%20with%20coherent%20gestures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%20surpassing%20existing%20approaches%20in%20generation%20quality%2C%20real-time%20efficiency%2C%20and%20interaction%20naturalness.%20Project%20page%3A%20https%3A//streamavatar.github.io%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamAvatar%253A%2520Streaming%2520Diffusion%2520Models%2520for%2520Real-Time%2520Interactive%2520Human%2520Avatars%26entry.906535625%3DZhiyao%2520Sun%2520and%2520Ziqiao%2520Peng%2520and%2520Yifeng%2520Ma%2520and%2520Yi%2520Chen%2520and%2520Zhengguang%2520Zhou%2520and%2520Zixiang%2520Zhou%2520and%2520Guozhen%2520Zhang%2520and%2520Youliang%2520Zhang%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3DReal-time%252C%2520streaming%2520interactive%2520avatars%2520represent%2520a%2520critical%2520yet%2520challenging%2520goal%2520in%2520digital%2520human%2520research.%2520Although%2520diffusion-based%2520human%2520avatar%2520generation%2520methods%2520achieve%2520remarkable%2520success%252C%2520their%2520non-causal%2520architecture%2520and%2520high%2520computational%2520costs%2520make%2520them%2520unsuitable%2520for%2520streaming.%2520Moreover%252C%2520existing%2520interactive%2520approaches%2520are%2520typically%2520limited%2520to%2520head-and-shoulder%2520region%252C%2520limiting%2520their%2520ability%2520to%2520produce%2520gestures%2520and%2520body%2520motions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520two-stage%2520autoregressive%2520adaptation%2520and%2520acceleration%2520framework%2520that%2520applies%2520autoregressive%2520distillation%2520and%2520adversarial%2520refinement%2520to%2520adapt%2520a%2520high-fidelity%2520human%2520video%2520diffusion%2520model%2520for%2520real-time%252C%2520interactive%2520streaming.%2520To%2520ensure%2520long-term%2520stability%2520and%2520consistency%252C%2520we%2520introduce%2520three%2520key%2520components%253A%2520a%2520Reference%2520Sink%252C%2520a%2520Reference-Anchored%2520Positional%2520Re-encoding%2520%2528RAPR%2529%2520strategy%252C%2520and%2520a%2520Consistency-Aware%2520Discriminator.%2520Building%2520on%2520this%2520framework%252C%2520we%2520develop%2520a%2520one-shot%252C%2520interactive%252C%2520human%2520avatar%2520model%2520capable%2520of%2520generating%2520both%2520natural%2520talking%2520and%2520listening%2520behaviors%2520with%2520coherent%2520gestures.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%2520surpassing%2520existing%2520approaches%2520in%2520generation%2520quality%252C%2520real-time%2520efficiency%252C%2520and%2520interaction%2520naturalness.%2520Project%2520page%253A%2520https%253A//streamavatar.github.io%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamAvatar%3A%20Streaming%20Diffusion%20Models%20for%20Real-Time%20Interactive%20Human%20Avatars&entry.906535625=Zhiyao%20Sun%20and%20Ziqiao%20Peng%20and%20Yifeng%20Ma%20and%20Yi%20Chen%20and%20Zhengguang%20Zhou%20and%20Zixiang%20Zhou%20and%20Guozhen%20Zhang%20and%20Youliang%20Zhang%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Yong-Jin%20Liu&entry.1292438233=Real-time%2C%20streaming%20interactive%20avatars%20represent%20a%20critical%20yet%20challenging%20goal%20in%20digital%20human%20research.%20Although%20diffusion-based%20human%20avatar%20generation%20methods%20achieve%20remarkable%20success%2C%20their%20non-causal%20architecture%20and%20high%20computational%20costs%20make%20them%20unsuitable%20for%20streaming.%20Moreover%2C%20existing%20interactive%20approaches%20are%20typically%20limited%20to%20head-and-shoulder%20region%2C%20limiting%20their%20ability%20to%20produce%20gestures%20and%20body%20motions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20two-stage%20autoregressive%20adaptation%20and%20acceleration%20framework%20that%20applies%20autoregressive%20distillation%20and%20adversarial%20refinement%20to%20adapt%20a%20high-fidelity%20human%20video%20diffusion%20model%20for%20real-time%2C%20interactive%20streaming.%20To%20ensure%20long-term%20stability%20and%20consistency%2C%20we%20introduce%20three%20key%20components%3A%20a%20Reference%20Sink%2C%20a%20Reference-Anchored%20Positional%20Re-encoding%20%28RAPR%29%20strategy%2C%20and%20a%20Consistency-Aware%20Discriminator.%20Building%20on%20this%20framework%2C%20we%20develop%20a%20one-shot%2C%20interactive%2C%20human%20avatar%20model%20capable%20of%20generating%20both%20natural%20talking%20and%20listening%20behaviors%20with%20coherent%20gestures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%2C%20surpassing%20existing%20approaches%20in%20generation%20quality%2C%20real-time%20efficiency%2C%20and%20interaction%20naturalness.%20Project%20page%3A%20https%3A//streamavatar.github.io%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.22065v1&entry.124074799=Read"},
{"title": "Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach", "author": "Yanmeng Wang and Wenkai Ji and Jian Zhou and Fu Xiao and Tsung-Hui Chang", "abstract": "Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients. Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards. However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks. This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL. Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity. Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling. Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures.", "link": "http://arxiv.org/abs/2502.17260v4", "date": "2025-12-26", "relevancy": 2.4161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Federated%20Learning%20in%20Unreliable%20Wireless%20Networks%3A%20A%20Client%20Selection%20Approach&body=Title%3A%20Robust%20Federated%20Learning%20in%20Unreliable%20Wireless%20Networks%3A%20A%20Client%20Selection%20Approach%0AAuthor%3A%20Yanmeng%20Wang%20and%20Wenkai%20Ji%20and%20Jian%20Zhou%20and%20Fu%20Xiao%20and%20Tsung-Hui%20Chang%0AAbstract%3A%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20distributed%20learning%20paradigm%20for%20training%20deep%20neural%20networks%20%28DNNs%29%20at%20the%20wireless%20edge%2C%20but%20its%20performance%20can%20be%20severely%20hindered%20by%20unreliable%20wireless%20transmission%20and%20inherent%20data%20heterogeneity%20among%20clients.%20Existing%20solutions%20primarily%20address%20these%20challenges%20by%20incorporating%20wireless%20resource%20optimization%20strategies%2C%20often%20focusing%20on%20uplink%20resource%20allocation%20across%20clients%20under%20the%20assumption%20of%20homogeneous%20client-server%20network%20standards.%20However%2C%20these%20approaches%20overlooked%20the%20fact%20that%20mobile%20clients%20may%20connect%20to%20the%20server%20via%20diverse%20network%20standards%20%28e.g.%2C%204G%2C%205G%2C%20Wi-Fi%29%20with%20customized%20configurations%2C%20limiting%20the%20flexibility%20of%20server-side%20modifications%20and%20restricting%20applicability%20in%20real-world%20commercial%20networks.%20This%20paper%20presents%20a%20novel%20theoretical%20analysis%20about%20how%20transmission%20failures%20in%20unreliable%20networks%20distort%20the%20effective%20label%20distributions%20of%20local%20samples%2C%20causing%20deviations%20from%20the%20global%20data%20distribution%20and%20introducing%20convergence%20bias%20in%20FL.%20Our%20analysis%20reveals%20that%20a%20carefully%20designed%20client%20selection%20strategy%20can%20mitigate%20biases%20induced%20by%20network%20unreliability%20and%20data%20heterogeneity.%20Motivated%20by%20this%20insight%2C%20we%20propose%20FedCote%2C%20a%20client%20selection%20approach%20that%20optimizes%20client%20selection%20probabilities%20without%20relying%20on%20wireless%20resource%20scheduling.%20Experimental%20results%20demonstrate%20the%20robustness%20of%20FedCote%20in%20DNN-based%20classification%20tasks%20under%20unreliable%20networks%20with%20frequent%20transmission%20failures.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17260v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Federated%2520Learning%2520in%2520Unreliable%2520Wireless%2520Networks%253A%2520A%2520Client%2520Selection%2520Approach%26entry.906535625%3DYanmeng%2520Wang%2520and%2520Wenkai%2520Ji%2520and%2520Jian%2520Zhou%2520and%2520Fu%2520Xiao%2520and%2520Tsung-Hui%2520Chang%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520distributed%2520learning%2520paradigm%2520for%2520training%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520at%2520the%2520wireless%2520edge%252C%2520but%2520its%2520performance%2520can%2520be%2520severely%2520hindered%2520by%2520unreliable%2520wireless%2520transmission%2520and%2520inherent%2520data%2520heterogeneity%2520among%2520clients.%2520Existing%2520solutions%2520primarily%2520address%2520these%2520challenges%2520by%2520incorporating%2520wireless%2520resource%2520optimization%2520strategies%252C%2520often%2520focusing%2520on%2520uplink%2520resource%2520allocation%2520across%2520clients%2520under%2520the%2520assumption%2520of%2520homogeneous%2520client-server%2520network%2520standards.%2520However%252C%2520these%2520approaches%2520overlooked%2520the%2520fact%2520that%2520mobile%2520clients%2520may%2520connect%2520to%2520the%2520server%2520via%2520diverse%2520network%2520standards%2520%2528e.g.%252C%25204G%252C%25205G%252C%2520Wi-Fi%2529%2520with%2520customized%2520configurations%252C%2520limiting%2520the%2520flexibility%2520of%2520server-side%2520modifications%2520and%2520restricting%2520applicability%2520in%2520real-world%2520commercial%2520networks.%2520This%2520paper%2520presents%2520a%2520novel%2520theoretical%2520analysis%2520about%2520how%2520transmission%2520failures%2520in%2520unreliable%2520networks%2520distort%2520the%2520effective%2520label%2520distributions%2520of%2520local%2520samples%252C%2520causing%2520deviations%2520from%2520the%2520global%2520data%2520distribution%2520and%2520introducing%2520convergence%2520bias%2520in%2520FL.%2520Our%2520analysis%2520reveals%2520that%2520a%2520carefully%2520designed%2520client%2520selection%2520strategy%2520can%2520mitigate%2520biases%2520induced%2520by%2520network%2520unreliability%2520and%2520data%2520heterogeneity.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520FedCote%252C%2520a%2520client%2520selection%2520approach%2520that%2520optimizes%2520client%2520selection%2520probabilities%2520without%2520relying%2520on%2520wireless%2520resource%2520scheduling.%2520Experimental%2520results%2520demonstrate%2520the%2520robustness%2520of%2520FedCote%2520in%2520DNN-based%2520classification%2520tasks%2520under%2520unreliable%2520networks%2520with%2520frequent%2520transmission%2520failures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17260v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Learning%20in%20Unreliable%20Wireless%20Networks%3A%20A%20Client%20Selection%20Approach&entry.906535625=Yanmeng%20Wang%20and%20Wenkai%20Ji%20and%20Jian%20Zhou%20and%20Fu%20Xiao%20and%20Tsung-Hui%20Chang&entry.1292438233=Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20distributed%20learning%20paradigm%20for%20training%20deep%20neural%20networks%20%28DNNs%29%20at%20the%20wireless%20edge%2C%20but%20its%20performance%20can%20be%20severely%20hindered%20by%20unreliable%20wireless%20transmission%20and%20inherent%20data%20heterogeneity%20among%20clients.%20Existing%20solutions%20primarily%20address%20these%20challenges%20by%20incorporating%20wireless%20resource%20optimization%20strategies%2C%20often%20focusing%20on%20uplink%20resource%20allocation%20across%20clients%20under%20the%20assumption%20of%20homogeneous%20client-server%20network%20standards.%20However%2C%20these%20approaches%20overlooked%20the%20fact%20that%20mobile%20clients%20may%20connect%20to%20the%20server%20via%20diverse%20network%20standards%20%28e.g.%2C%204G%2C%205G%2C%20Wi-Fi%29%20with%20customized%20configurations%2C%20limiting%20the%20flexibility%20of%20server-side%20modifications%20and%20restricting%20applicability%20in%20real-world%20commercial%20networks.%20This%20paper%20presents%20a%20novel%20theoretical%20analysis%20about%20how%20transmission%20failures%20in%20unreliable%20networks%20distort%20the%20effective%20label%20distributions%20of%20local%20samples%2C%20causing%20deviations%20from%20the%20global%20data%20distribution%20and%20introducing%20convergence%20bias%20in%20FL.%20Our%20analysis%20reveals%20that%20a%20carefully%20designed%20client%20selection%20strategy%20can%20mitigate%20biases%20induced%20by%20network%20unreliability%20and%20data%20heterogeneity.%20Motivated%20by%20this%20insight%2C%20we%20propose%20FedCote%2C%20a%20client%20selection%20approach%20that%20optimizes%20client%20selection%20probabilities%20without%20relying%20on%20wireless%20resource%20scheduling.%20Experimental%20results%20demonstrate%20the%20robustness%20of%20FedCote%20in%20DNN-based%20classification%20tasks%20under%20unreliable%20networks%20with%20frequent%20transmission%20failures.&entry.1838667208=http%3A//arxiv.org/abs/2502.17260v4&entry.124074799=Read"},
{"title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision", "author": "Shengliang Deng and Mi Yan and Yixin Zheng and Jiayi Su and Wenhao Zhang and Xiaoguang Zhao and Heming Cui and Zhizheng Zhang and He Wang", "abstract": "Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.", "link": "http://arxiv.org/abs/2512.21970v1", "date": "2025-12-26", "relevancy": 2.3758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6046}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StereoVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Stereo%20Vision&body=Title%3A%20StereoVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Stereo%20Vision%0AAuthor%3A%20Shengliang%20Deng%20and%20Mi%20Yan%20and%20Yixin%20Zheng%20and%20Jiayi%20Su%20and%20Wenhao%20Zhang%20and%20Xiaoguang%20Zhao%20and%20Heming%20Cui%20and%20Zhizheng%20Zhang%20and%20He%20Wang%0AAbstract%3A%20Stereo%20cameras%20closely%20mimic%20human%20binocular%20vision%2C%20providing%20rich%20spatial%20cues%20critical%20for%20precise%20robotic%20manipulation.%20Despite%20their%20advantage%2C%20the%20adoption%20of%20stereo%20vision%20in%20vision-language-action%20models%20%28VLAs%29%20remains%20underexplored.%20In%20this%20work%2C%20we%20present%20StereoVLA%2C%20a%20VLA%20model%20that%20leverages%20rich%20geometric%20cues%20from%20stereo%20vision.%20We%20propose%20a%20novel%20Geometric-Semantic%20Feature%20Extraction%20module%20that%20utilizes%20vision%20foundation%20models%20to%20extract%20and%20fuse%20two%20key%20features%3A%201%29%20geometric%20features%20from%20subtle%20stereo-view%20differences%20for%20spatial%20perception%3B%202%29%20semantic-rich%20features%20from%20the%20monocular%20view%20for%20instruction%20following.%20Additionally%2C%20we%20propose%20an%20auxiliary%20Interaction-Region%20Depth%20Estimation%20task%20to%20further%20enhance%20spatial%20perception%20and%20accelerate%20model%20convergence.%20Extensive%20experiments%20show%20that%20our%20approach%20outperforms%20baselines%20by%20a%20large%20margin%20in%20diverse%20tasks%20under%20the%20stereo%20setting%20and%20demonstrates%20strong%20robustness%20to%20camera%20pose%20variations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereoVLA%253A%2520Enhancing%2520Vision-Language-Action%2520Models%2520with%2520Stereo%2520Vision%26entry.906535625%3DShengliang%2520Deng%2520and%2520Mi%2520Yan%2520and%2520Yixin%2520Zheng%2520and%2520Jiayi%2520Su%2520and%2520Wenhao%2520Zhang%2520and%2520Xiaoguang%2520Zhao%2520and%2520Heming%2520Cui%2520and%2520Zhizheng%2520Zhang%2520and%2520He%2520Wang%26entry.1292438233%3DStereo%2520cameras%2520closely%2520mimic%2520human%2520binocular%2520vision%252C%2520providing%2520rich%2520spatial%2520cues%2520critical%2520for%2520precise%2520robotic%2520manipulation.%2520Despite%2520their%2520advantage%252C%2520the%2520adoption%2520of%2520stereo%2520vision%2520in%2520vision-language-action%2520models%2520%2528VLAs%2529%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520present%2520StereoVLA%252C%2520a%2520VLA%2520model%2520that%2520leverages%2520rich%2520geometric%2520cues%2520from%2520stereo%2520vision.%2520We%2520propose%2520a%2520novel%2520Geometric-Semantic%2520Feature%2520Extraction%2520module%2520that%2520utilizes%2520vision%2520foundation%2520models%2520to%2520extract%2520and%2520fuse%2520two%2520key%2520features%253A%25201%2529%2520geometric%2520features%2520from%2520subtle%2520stereo-view%2520differences%2520for%2520spatial%2520perception%253B%25202%2529%2520semantic-rich%2520features%2520from%2520the%2520monocular%2520view%2520for%2520instruction%2520following.%2520Additionally%252C%2520we%2520propose%2520an%2520auxiliary%2520Interaction-Region%2520Depth%2520Estimation%2520task%2520to%2520further%2520enhance%2520spatial%2520perception%2520and%2520accelerate%2520model%2520convergence.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520outperforms%2520baselines%2520by%2520a%2520large%2520margin%2520in%2520diverse%2520tasks%2520under%2520the%2520stereo%2520setting%2520and%2520demonstrates%2520strong%2520robustness%2520to%2520camera%2520pose%2520variations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StereoVLA%3A%20Enhancing%20Vision-Language-Action%20Models%20with%20Stereo%20Vision&entry.906535625=Shengliang%20Deng%20and%20Mi%20Yan%20and%20Yixin%20Zheng%20and%20Jiayi%20Su%20and%20Wenhao%20Zhang%20and%20Xiaoguang%20Zhao%20and%20Heming%20Cui%20and%20Zhizheng%20Zhang%20and%20He%20Wang&entry.1292438233=Stereo%20cameras%20closely%20mimic%20human%20binocular%20vision%2C%20providing%20rich%20spatial%20cues%20critical%20for%20precise%20robotic%20manipulation.%20Despite%20their%20advantage%2C%20the%20adoption%20of%20stereo%20vision%20in%20vision-language-action%20models%20%28VLAs%29%20remains%20underexplored.%20In%20this%20work%2C%20we%20present%20StereoVLA%2C%20a%20VLA%20model%20that%20leverages%20rich%20geometric%20cues%20from%20stereo%20vision.%20We%20propose%20a%20novel%20Geometric-Semantic%20Feature%20Extraction%20module%20that%20utilizes%20vision%20foundation%20models%20to%20extract%20and%20fuse%20two%20key%20features%3A%201%29%20geometric%20features%20from%20subtle%20stereo-view%20differences%20for%20spatial%20perception%3B%202%29%20semantic-rich%20features%20from%20the%20monocular%20view%20for%20instruction%20following.%20Additionally%2C%20we%20propose%20an%20auxiliary%20Interaction-Region%20Depth%20Estimation%20task%20to%20further%20enhance%20spatial%20perception%20and%20accelerate%20model%20convergence.%20Extensive%20experiments%20show%20that%20our%20approach%20outperforms%20baselines%20by%20a%20large%20margin%20in%20diverse%20tasks%20under%20the%20stereo%20setting%20and%20demonstrates%20strong%20robustness%20to%20camera%20pose%20variations.&entry.1838667208=http%3A//arxiv.org/abs/2512.21970v1&entry.124074799=Read"},
{"title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning", "author": "Wenbin Li and Shangge Liu and Borui Kang and Yiyang Chen and KaXuan Lew and Yang Chen and Yinghuan Shi and Lei Wang and Yang Gao and Jiebo Luo", "abstract": "A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \\href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.", "link": "http://arxiv.org/abs/2512.22029v1", "date": "2025-12-26", "relevancy": 2.3569, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LibContinual%3A%20A%20Comprehensive%20Library%20towards%20Realistic%20Continual%20Learning&body=Title%3A%20LibContinual%3A%20A%20Comprehensive%20Library%20towards%20Realistic%20Continual%20Learning%0AAuthor%3A%20Wenbin%20Li%20and%20Shangge%20Liu%20and%20Borui%20Kang%20and%20Yiyang%20Chen%20and%20KaXuan%20Lew%20and%20Yang%20Chen%20and%20Yinghuan%20Shi%20and%20Lei%20Wang%20and%20Yang%20Gao%20and%20Jiebo%20Luo%0AAbstract%3A%20A%20fundamental%20challenge%20in%20Continual%20Learning%20%28CL%29%20is%20catastrophic%20forgetting%2C%20where%20adapting%20to%20new%20tasks%20degrades%20the%20performance%20on%20previous%20ones.%20While%20the%20field%20has%20evolved%20with%20diverse%20methods%2C%20this%20rapid%20surge%20in%20diverse%20methodologies%20has%20culminated%20in%20a%20fragmented%20research%20landscape.%20The%20lack%20of%20a%20unified%20framework%2C%20including%20inconsistent%20implementations%2C%20conflicting%20dependencies%2C%20and%20varying%20evaluation%20protocols%2C%20makes%20fair%20comparison%20and%20reproducible%20research%20increasingly%20difficult.%20To%20address%20this%20challenge%2C%20we%20propose%20LibContinual%2C%20a%20comprehensive%20and%20reproducible%20library%20designed%20to%20serve%20as%20a%20foundational%20platform%20for%20realistic%20CL.%20Built%20upon%20a%20high-cohesion%2C%20low-coupling%20modular%20architecture%2C%20LibContinual%20integrates%2019%20representative%20algorithms%20across%20five%20major%20methodological%20categories%2C%20providing%20a%20standardized%20execution%20environment.%20Meanwhile%2C%20leveraging%20this%20unified%20framework%2C%20we%20systematically%20identify%20and%20investigate%20three%20implicit%20assumptions%20prevalent%20in%20mainstream%20evaluation%3A%20%281%29%20offline%20data%20accessibility%2C%20%282%29%20unregulated%20memory%20resources%2C%20and%20%283%29%20intra-task%20semantic%20homogeneity.%20We%20argue%20that%20these%20assumptions%20often%20overestimate%20the%20real-world%20applicability%20of%20CL%20methods.%20Through%20our%20comprehensive%20analysis%20using%20strict%20online%20CL%20settings%2C%20a%20novel%20unified%20memory%20budget%20protocol%2C%20and%20a%20proposed%20category-randomized%20setting%2C%20we%20reveal%20significant%20performance%20drops%20in%20many%20representative%20CL%20methods%20when%20subjected%20to%20these%20real-world%20constraints.%20Our%20study%20underscores%20the%20necessity%20of%20resource-aware%20and%20semantically%20robust%20CL%20strategies%2C%20and%20offers%20LibContinual%20as%20a%20foundational%20toolkit%20for%20future%20research%20in%20realistic%20continual%20learning.%20The%20source%20code%20is%20available%20from%20%5Chref%7Bhttps%3A//github.com/RL-VIG/LibContinual%7D%7Bhttps%3A//github.com/RL-VIG/LibContinual%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLibContinual%253A%2520A%2520Comprehensive%2520Library%2520towards%2520Realistic%2520Continual%2520Learning%26entry.906535625%3DWenbin%2520Li%2520and%2520Shangge%2520Liu%2520and%2520Borui%2520Kang%2520and%2520Yiyang%2520Chen%2520and%2520KaXuan%2520Lew%2520and%2520Yang%2520Chen%2520and%2520Yinghuan%2520Shi%2520and%2520Lei%2520Wang%2520and%2520Yang%2520Gao%2520and%2520Jiebo%2520Luo%26entry.1292438233%3DA%2520fundamental%2520challenge%2520in%2520Continual%2520Learning%2520%2528CL%2529%2520is%2520catastrophic%2520forgetting%252C%2520where%2520adapting%2520to%2520new%2520tasks%2520degrades%2520the%2520performance%2520on%2520previous%2520ones.%2520While%2520the%2520field%2520has%2520evolved%2520with%2520diverse%2520methods%252C%2520this%2520rapid%2520surge%2520in%2520diverse%2520methodologies%2520has%2520culminated%2520in%2520a%2520fragmented%2520research%2520landscape.%2520The%2520lack%2520of%2520a%2520unified%2520framework%252C%2520including%2520inconsistent%2520implementations%252C%2520conflicting%2520dependencies%252C%2520and%2520varying%2520evaluation%2520protocols%252C%2520makes%2520fair%2520comparison%2520and%2520reproducible%2520research%2520increasingly%2520difficult.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520LibContinual%252C%2520a%2520comprehensive%2520and%2520reproducible%2520library%2520designed%2520to%2520serve%2520as%2520a%2520foundational%2520platform%2520for%2520realistic%2520CL.%2520Built%2520upon%2520a%2520high-cohesion%252C%2520low-coupling%2520modular%2520architecture%252C%2520LibContinual%2520integrates%252019%2520representative%2520algorithms%2520across%2520five%2520major%2520methodological%2520categories%252C%2520providing%2520a%2520standardized%2520execution%2520environment.%2520Meanwhile%252C%2520leveraging%2520this%2520unified%2520framework%252C%2520we%2520systematically%2520identify%2520and%2520investigate%2520three%2520implicit%2520assumptions%2520prevalent%2520in%2520mainstream%2520evaluation%253A%2520%25281%2529%2520offline%2520data%2520accessibility%252C%2520%25282%2529%2520unregulated%2520memory%2520resources%252C%2520and%2520%25283%2529%2520intra-task%2520semantic%2520homogeneity.%2520We%2520argue%2520that%2520these%2520assumptions%2520often%2520overestimate%2520the%2520real-world%2520applicability%2520of%2520CL%2520methods.%2520Through%2520our%2520comprehensive%2520analysis%2520using%2520strict%2520online%2520CL%2520settings%252C%2520a%2520novel%2520unified%2520memory%2520budget%2520protocol%252C%2520and%2520a%2520proposed%2520category-randomized%2520setting%252C%2520we%2520reveal%2520significant%2520performance%2520drops%2520in%2520many%2520representative%2520CL%2520methods%2520when%2520subjected%2520to%2520these%2520real-world%2520constraints.%2520Our%2520study%2520underscores%2520the%2520necessity%2520of%2520resource-aware%2520and%2520semantically%2520robust%2520CL%2520strategies%252C%2520and%2520offers%2520LibContinual%2520as%2520a%2520foundational%2520toolkit%2520for%2520future%2520research%2520in%2520realistic%2520continual%2520learning.%2520The%2520source%2520code%2520is%2520available%2520from%2520%255Chref%257Bhttps%253A//github.com/RL-VIG/LibContinual%257D%257Bhttps%253A//github.com/RL-VIG/LibContinual%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LibContinual%3A%20A%20Comprehensive%20Library%20towards%20Realistic%20Continual%20Learning&entry.906535625=Wenbin%20Li%20and%20Shangge%20Liu%20and%20Borui%20Kang%20and%20Yiyang%20Chen%20and%20KaXuan%20Lew%20and%20Yang%20Chen%20and%20Yinghuan%20Shi%20and%20Lei%20Wang%20and%20Yang%20Gao%20and%20Jiebo%20Luo&entry.1292438233=A%20fundamental%20challenge%20in%20Continual%20Learning%20%28CL%29%20is%20catastrophic%20forgetting%2C%20where%20adapting%20to%20new%20tasks%20degrades%20the%20performance%20on%20previous%20ones.%20While%20the%20field%20has%20evolved%20with%20diverse%20methods%2C%20this%20rapid%20surge%20in%20diverse%20methodologies%20has%20culminated%20in%20a%20fragmented%20research%20landscape.%20The%20lack%20of%20a%20unified%20framework%2C%20including%20inconsistent%20implementations%2C%20conflicting%20dependencies%2C%20and%20varying%20evaluation%20protocols%2C%20makes%20fair%20comparison%20and%20reproducible%20research%20increasingly%20difficult.%20To%20address%20this%20challenge%2C%20we%20propose%20LibContinual%2C%20a%20comprehensive%20and%20reproducible%20library%20designed%20to%20serve%20as%20a%20foundational%20platform%20for%20realistic%20CL.%20Built%20upon%20a%20high-cohesion%2C%20low-coupling%20modular%20architecture%2C%20LibContinual%20integrates%2019%20representative%20algorithms%20across%20five%20major%20methodological%20categories%2C%20providing%20a%20standardized%20execution%20environment.%20Meanwhile%2C%20leveraging%20this%20unified%20framework%2C%20we%20systematically%20identify%20and%20investigate%20three%20implicit%20assumptions%20prevalent%20in%20mainstream%20evaluation%3A%20%281%29%20offline%20data%20accessibility%2C%20%282%29%20unregulated%20memory%20resources%2C%20and%20%283%29%20intra-task%20semantic%20homogeneity.%20We%20argue%20that%20these%20assumptions%20often%20overestimate%20the%20real-world%20applicability%20of%20CL%20methods.%20Through%20our%20comprehensive%20analysis%20using%20strict%20online%20CL%20settings%2C%20a%20novel%20unified%20memory%20budget%20protocol%2C%20and%20a%20proposed%20category-randomized%20setting%2C%20we%20reveal%20significant%20performance%20drops%20in%20many%20representative%20CL%20methods%20when%20subjected%20to%20these%20real-world%20constraints.%20Our%20study%20underscores%20the%20necessity%20of%20resource-aware%20and%20semantically%20robust%20CL%20strategies%2C%20and%20offers%20LibContinual%20as%20a%20foundational%20toolkit%20for%20future%20research%20in%20realistic%20continual%20learning.%20The%20source%20code%20is%20available%20from%20%5Chref%7Bhttps%3A//github.com/RL-VIG/LibContinual%7D%7Bhttps%3A//github.com/RL-VIG/LibContinual%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.22029v1&entry.124074799=Read"},
{"title": "Learning Association via Track-Detection Matching for Multi-Object Tracking", "author": "Momir Ad\u017eemovi\u0107", "abstract": "Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \\href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.", "link": "http://arxiv.org/abs/2512.22105v1", "date": "2025-12-26", "relevancy": 2.3109, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Association%20via%20Track-Detection%20Matching%20for%20Multi-Object%20Tracking&body=Title%3A%20Learning%20Association%20via%20Track-Detection%20Matching%20for%20Multi-Object%20Tracking%0AAuthor%3A%20Momir%20Ad%C5%BEemovi%C4%87%0AAbstract%3A%20Multi-object%20tracking%20aims%20to%20maintain%20object%20identities%20over%20time%20by%20associating%20detections%20across%20video%20frames.%20Two%20dominant%20paradigms%20exist%20in%20literature%3A%20tracking-by-detection%20methods%2C%20which%20are%20computationally%20efficient%20but%20rely%20on%20handcrafted%20association%20heuristics%2C%20and%20end-to-end%20approaches%2C%20which%20learn%20association%20from%20data%20at%20the%20cost%20of%20higher%20computational%20complexity.%20We%20propose%20Track-Detection%20Link%20Prediction%20%28TDLP%29%2C%20a%20tracking-by-detection%20method%20that%20performs%20per-frame%20association%20via%20link%20prediction%20between%20tracks%20and%20detections%2C%20i.e.%2C%20by%20predicting%20the%20correct%20continuation%20of%20each%20track%20at%20every%20frame.%20TDLP%20is%20architecturally%20designed%20primarily%20for%20geometric%20features%20such%20as%20bounding%20boxes%2C%20while%20optionally%20incorporating%20additional%20cues%2C%20including%20pose%20and%20appearance.%20Unlike%20heuristic-based%20methods%2C%20TDLP%20learns%20association%20directly%20from%20data%20without%20handcrafted%20rules%2C%20while%20remaining%20modular%20and%20computationally%20efficient%20compared%20to%20end-to-end%20trackers.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20TDLP%20consistently%20surpasses%20state-of-the-art%20performance%20across%20both%20tracking-by-detection%20and%20end-to-end%20methods.%20Finally%2C%20we%20provide%20a%20detailed%20analysis%20comparing%20link%20prediction%20with%20metric%20learning-based%20association%20and%20show%20that%20link%20prediction%20is%20more%20effective%2C%20particularly%20when%20handling%20heterogeneous%20features%20such%20as%20detection%20bounding%20boxes.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Robotmurlock/TDLP%7D%7Bhttps%3A//github.com/Robotmurlock/TDLP%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Association%2520via%2520Track-Detection%2520Matching%2520for%2520Multi-Object%2520Tracking%26entry.906535625%3DMomir%2520Ad%25C5%25BEemovi%25C4%2587%26entry.1292438233%3DMulti-object%2520tracking%2520aims%2520to%2520maintain%2520object%2520identities%2520over%2520time%2520by%2520associating%2520detections%2520across%2520video%2520frames.%2520Two%2520dominant%2520paradigms%2520exist%2520in%2520literature%253A%2520tracking-by-detection%2520methods%252C%2520which%2520are%2520computationally%2520efficient%2520but%2520rely%2520on%2520handcrafted%2520association%2520heuristics%252C%2520and%2520end-to-end%2520approaches%252C%2520which%2520learn%2520association%2520from%2520data%2520at%2520the%2520cost%2520of%2520higher%2520computational%2520complexity.%2520We%2520propose%2520Track-Detection%2520Link%2520Prediction%2520%2528TDLP%2529%252C%2520a%2520tracking-by-detection%2520method%2520that%2520performs%2520per-frame%2520association%2520via%2520link%2520prediction%2520between%2520tracks%2520and%2520detections%252C%2520i.e.%252C%2520by%2520predicting%2520the%2520correct%2520continuation%2520of%2520each%2520track%2520at%2520every%2520frame.%2520TDLP%2520is%2520architecturally%2520designed%2520primarily%2520for%2520geometric%2520features%2520such%2520as%2520bounding%2520boxes%252C%2520while%2520optionally%2520incorporating%2520additional%2520cues%252C%2520including%2520pose%2520and%2520appearance.%2520Unlike%2520heuristic-based%2520methods%252C%2520TDLP%2520learns%2520association%2520directly%2520from%2520data%2520without%2520handcrafted%2520rules%252C%2520while%2520remaining%2520modular%2520and%2520computationally%2520efficient%2520compared%2520to%2520end-to-end%2520trackers.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520TDLP%2520consistently%2520surpasses%2520state-of-the-art%2520performance%2520across%2520both%2520tracking-by-detection%2520and%2520end-to-end%2520methods.%2520Finally%252C%2520we%2520provide%2520a%2520detailed%2520analysis%2520comparing%2520link%2520prediction%2520with%2520metric%2520learning-based%2520association%2520and%2520show%2520that%2520link%2520prediction%2520is%2520more%2520effective%252C%2520particularly%2520when%2520handling%2520heterogeneous%2520features%2520such%2520as%2520detection%2520bounding%2520boxes.%2520Our%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Robotmurlock/TDLP%257D%257Bhttps%253A//github.com/Robotmurlock/TDLP%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Association%20via%20Track-Detection%20Matching%20for%20Multi-Object%20Tracking&entry.906535625=Momir%20Ad%C5%BEemovi%C4%87&entry.1292438233=Multi-object%20tracking%20aims%20to%20maintain%20object%20identities%20over%20time%20by%20associating%20detections%20across%20video%20frames.%20Two%20dominant%20paradigms%20exist%20in%20literature%3A%20tracking-by-detection%20methods%2C%20which%20are%20computationally%20efficient%20but%20rely%20on%20handcrafted%20association%20heuristics%2C%20and%20end-to-end%20approaches%2C%20which%20learn%20association%20from%20data%20at%20the%20cost%20of%20higher%20computational%20complexity.%20We%20propose%20Track-Detection%20Link%20Prediction%20%28TDLP%29%2C%20a%20tracking-by-detection%20method%20that%20performs%20per-frame%20association%20via%20link%20prediction%20between%20tracks%20and%20detections%2C%20i.e.%2C%20by%20predicting%20the%20correct%20continuation%20of%20each%20track%20at%20every%20frame.%20TDLP%20is%20architecturally%20designed%20primarily%20for%20geometric%20features%20such%20as%20bounding%20boxes%2C%20while%20optionally%20incorporating%20additional%20cues%2C%20including%20pose%20and%20appearance.%20Unlike%20heuristic-based%20methods%2C%20TDLP%20learns%20association%20directly%20from%20data%20without%20handcrafted%20rules%2C%20while%20remaining%20modular%20and%20computationally%20efficient%20compared%20to%20end-to-end%20trackers.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20TDLP%20consistently%20surpasses%20state-of-the-art%20performance%20across%20both%20tracking-by-detection%20and%20end-to-end%20methods.%20Finally%2C%20we%20provide%20a%20detailed%20analysis%20comparing%20link%20prediction%20with%20metric%20learning-based%20association%20and%20show%20that%20link%20prediction%20is%20more%20effective%2C%20particularly%20when%20handling%20heterogeneous%20features%20such%20as%20detection%20bounding%20boxes.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Robotmurlock/TDLP%7D%7Bhttps%3A//github.com/Robotmurlock/TDLP%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.22105v1&entry.124074799=Read"},
{"title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration", "author": "Wen Jiang and Li Wang and Kangyao Huang and Wei Fan and Jinyuan Liu and Shaoyu Liu and Hongwei Duan and Bin Xu and Xiangyang Ji", "abstract": "Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN. LongFly proposes a history-aware spatiotemporal modeling strategy that transforms fragmented and redundant historical data into structured, compact, and expressive representations. First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations. Then, the spatiotemporal trajectory encoding module is introduced to capture the temporal dynamics and spatial structure of UAV trajectories. Finally, to integrate existing spatiotemporal context with current observations, we design the prompt-guided multimodal integration module to support time-based reasoning and robust waypoint prediction. Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\\% in success rate and 6.33\\% in success weighted by path length, consistently across both seen and unseen environments.", "link": "http://arxiv.org/abs/2512.22010v1", "date": "2025-12-26", "relevancy": 2.3074, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6091}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongFly%3A%20Long-Horizon%20UAV%20Vision-and-Language%20Navigation%20with%20Spatiotemporal%20Context%20Integration&body=Title%3A%20LongFly%3A%20Long-Horizon%20UAV%20Vision-and-Language%20Navigation%20with%20Spatiotemporal%20Context%20Integration%0AAuthor%3A%20Wen%20Jiang%20and%20Li%20Wang%20and%20Kangyao%20Huang%20and%20Wei%20Fan%20and%20Jinyuan%20Liu%20and%20Shaoyu%20Liu%20and%20Hongwei%20Duan%20and%20Bin%20Xu%20and%20Xiangyang%20Ji%0AAbstract%3A%20Unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20crucial%20tools%20for%20post-disaster%20search%20and%20rescue%2C%20facing%20challenges%20such%20as%20high%20information%20density%2C%20rapid%20changes%20in%20viewpoint%2C%20and%20dynamic%20structures%2C%20especially%20in%20long-horizon%20navigation.%20However%2C%20current%20UAV%20vision-and-language%20navigation%28VLN%29%20methods%20struggle%20to%20model%20long-horizon%20spatiotemporal%20context%20in%20complex%20environments%2C%20resulting%20in%20inaccurate%20semantic%20alignment%20and%20unstable%20path%20planning.%20To%20this%20end%2C%20we%20propose%20LongFly%2C%20a%20spatiotemporal%20context%20modeling%20framework%20for%20long-horizon%20UAV%20VLN.%20LongFly%20proposes%20a%20history-aware%20spatiotemporal%20modeling%20strategy%20that%20transforms%20fragmented%20and%20redundant%20historical%20data%20into%20structured%2C%20compact%2C%20and%20expressive%20representations.%20First%2C%20we%20propose%20the%20slot-based%20historical%20image%20compression%20module%2C%20which%20dynamically%20distills%20multi-view%20historical%20observations%20into%20fixed-length%20contextual%20representations.%20Then%2C%20the%20spatiotemporal%20trajectory%20encoding%20module%20is%20introduced%20to%20capture%20the%20temporal%20dynamics%20and%20spatial%20structure%20of%20UAV%20trajectories.%20Finally%2C%20to%20integrate%20existing%20spatiotemporal%20context%20with%20current%20observations%2C%20we%20design%20the%20prompt-guided%20multimodal%20integration%20module%20to%20support%20time-based%20reasoning%20and%20robust%20waypoint%20prediction.%20Experimental%20results%20demonstrate%20that%20LongFly%20outperforms%20state-of-the-art%20UAV%20VLN%20baselines%20by%207.89%5C%25%20in%20success%20rate%20and%206.33%5C%25%20in%20success%20weighted%20by%20path%20length%2C%20consistently%20across%20both%20seen%20and%20unseen%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongFly%253A%2520Long-Horizon%2520UAV%2520Vision-and-Language%2520Navigation%2520with%2520Spatiotemporal%2520Context%2520Integration%26entry.906535625%3DWen%2520Jiang%2520and%2520Li%2520Wang%2520and%2520Kangyao%2520Huang%2520and%2520Wei%2520Fan%2520and%2520Jinyuan%2520Liu%2520and%2520Shaoyu%2520Liu%2520and%2520Hongwei%2520Duan%2520and%2520Bin%2520Xu%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3DUnmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520are%2520crucial%2520tools%2520for%2520post-disaster%2520search%2520and%2520rescue%252C%2520facing%2520challenges%2520such%2520as%2520high%2520information%2520density%252C%2520rapid%2520changes%2520in%2520viewpoint%252C%2520and%2520dynamic%2520structures%252C%2520especially%2520in%2520long-horizon%2520navigation.%2520However%252C%2520current%2520UAV%2520vision-and-language%2520navigation%2528VLN%2529%2520methods%2520struggle%2520to%2520model%2520long-horizon%2520spatiotemporal%2520context%2520in%2520complex%2520environments%252C%2520resulting%2520in%2520inaccurate%2520semantic%2520alignment%2520and%2520unstable%2520path%2520planning.%2520To%2520this%2520end%252C%2520we%2520propose%2520LongFly%252C%2520a%2520spatiotemporal%2520context%2520modeling%2520framework%2520for%2520long-horizon%2520UAV%2520VLN.%2520LongFly%2520proposes%2520a%2520history-aware%2520spatiotemporal%2520modeling%2520strategy%2520that%2520transforms%2520fragmented%2520and%2520redundant%2520historical%2520data%2520into%2520structured%252C%2520compact%252C%2520and%2520expressive%2520representations.%2520First%252C%2520we%2520propose%2520the%2520slot-based%2520historical%2520image%2520compression%2520module%252C%2520which%2520dynamically%2520distills%2520multi-view%2520historical%2520observations%2520into%2520fixed-length%2520contextual%2520representations.%2520Then%252C%2520the%2520spatiotemporal%2520trajectory%2520encoding%2520module%2520is%2520introduced%2520to%2520capture%2520the%2520temporal%2520dynamics%2520and%2520spatial%2520structure%2520of%2520UAV%2520trajectories.%2520Finally%252C%2520to%2520integrate%2520existing%2520spatiotemporal%2520context%2520with%2520current%2520observations%252C%2520we%2520design%2520the%2520prompt-guided%2520multimodal%2520integration%2520module%2520to%2520support%2520time-based%2520reasoning%2520and%2520robust%2520waypoint%2520prediction.%2520Experimental%2520results%2520demonstrate%2520that%2520LongFly%2520outperforms%2520state-of-the-art%2520UAV%2520VLN%2520baselines%2520by%25207.89%255C%2525%2520in%2520success%2520rate%2520and%25206.33%255C%2525%2520in%2520success%2520weighted%2520by%2520path%2520length%252C%2520consistently%2520across%2520both%2520seen%2520and%2520unseen%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongFly%3A%20Long-Horizon%20UAV%20Vision-and-Language%20Navigation%20with%20Spatiotemporal%20Context%20Integration&entry.906535625=Wen%20Jiang%20and%20Li%20Wang%20and%20Kangyao%20Huang%20and%20Wei%20Fan%20and%20Jinyuan%20Liu%20and%20Shaoyu%20Liu%20and%20Hongwei%20Duan%20and%20Bin%20Xu%20and%20Xiangyang%20Ji&entry.1292438233=Unmanned%20aerial%20vehicles%20%28UAVs%29%20are%20crucial%20tools%20for%20post-disaster%20search%20and%20rescue%2C%20facing%20challenges%20such%20as%20high%20information%20density%2C%20rapid%20changes%20in%20viewpoint%2C%20and%20dynamic%20structures%2C%20especially%20in%20long-horizon%20navigation.%20However%2C%20current%20UAV%20vision-and-language%20navigation%28VLN%29%20methods%20struggle%20to%20model%20long-horizon%20spatiotemporal%20context%20in%20complex%20environments%2C%20resulting%20in%20inaccurate%20semantic%20alignment%20and%20unstable%20path%20planning.%20To%20this%20end%2C%20we%20propose%20LongFly%2C%20a%20spatiotemporal%20context%20modeling%20framework%20for%20long-horizon%20UAV%20VLN.%20LongFly%20proposes%20a%20history-aware%20spatiotemporal%20modeling%20strategy%20that%20transforms%20fragmented%20and%20redundant%20historical%20data%20into%20structured%2C%20compact%2C%20and%20expressive%20representations.%20First%2C%20we%20propose%20the%20slot-based%20historical%20image%20compression%20module%2C%20which%20dynamically%20distills%20multi-view%20historical%20observations%20into%20fixed-length%20contextual%20representations.%20Then%2C%20the%20spatiotemporal%20trajectory%20encoding%20module%20is%20introduced%20to%20capture%20the%20temporal%20dynamics%20and%20spatial%20structure%20of%20UAV%20trajectories.%20Finally%2C%20to%20integrate%20existing%20spatiotemporal%20context%20with%20current%20observations%2C%20we%20design%20the%20prompt-guided%20multimodal%20integration%20module%20to%20support%20time-based%20reasoning%20and%20robust%20waypoint%20prediction.%20Experimental%20results%20demonstrate%20that%20LongFly%20outperforms%20state-of-the-art%20UAV%20VLN%20baselines%20by%207.89%5C%25%20in%20success%20rate%20and%206.33%5C%25%20in%20success%20weighted%20by%20path%20length%2C%20consistently%20across%20both%20seen%20and%20unseen%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.22010v1&entry.124074799=Read"},
{"title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models", "author": "Senyu Fei and Siyin Wang and Junhao Shi and Zihao Dai and Jikun Cai and Pengfang Qian and Li Ji and Xinzhe He and Shiduo Zhang and Zhaoye Fei and Jinlan Fu and Jingjing Gong and Xipeng Qiu", "abstract": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.", "link": "http://arxiv.org/abs/2510.13626v3", "date": "2025-12-26", "relevancy": 2.2797, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%20Models&body=Title%3A%20LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%20Models%0AAuthor%3A%20Senyu%20Fei%20and%20Siyin%20Wang%20and%20Junhao%20Shi%20and%20Zihao%20Dai%20and%20Jikun%20Cai%20and%20Pengfang%20Qian%20and%20Li%20Ji%20and%20Xinzhe%20He%20and%20Shiduo%20Zhang%20and%20Zhaoye%20Fei%20and%20Jinlan%20Fu%20and%20Jingjing%20Gong%20and%20Xipeng%20Qiu%0AAbstract%3A%20Visual-Language-Action%20%28VLA%29%20models%20report%20impressive%20success%20rates%20on%20robotic%20manipulation%20benchmarks%2C%20yet%20these%20results%20may%20mask%20fundamental%20weaknesses%20in%20robustness.%20We%20perform%20a%20systematic%20vulnerability%20analysis%20by%20introducing%20controlled%20perturbations%20across%20seven%20dimensions%3A%20objects%20layout%2C%20camera%20viewpoints%2C%20robot%20initial%20states%2C%20language%20instructions%2C%20light%20conditions%2C%20background%20textures%20and%20sensor%20noise.%20We%20comprehensively%20analyzed%20multiple%20state-of-the-art%20models%20and%20revealed%20consistent%20brittleness%20beneath%20apparent%20competence.%20Our%20analysis%20exposes%20critical%20weaknesses%3A%20models%20exhibit%20extreme%20sensitivity%20to%20perturbation%20factors%2C%20including%20camera%20viewpoints%20and%20robot%20initial%20states%2C%20with%20performance%20dropping%20from%2095%25%20to%20below%2030%25%20under%20modest%20perturbations.%20Surprisingly%2C%20models%20are%20largely%20insensitive%20to%20language%20variations%2C%20with%20further%20experiments%20revealing%20that%20models%20tend%20to%20ignore%20language%20instructions%20completely.%20Our%20findings%20challenge%20the%20assumption%20that%20high%20benchmark%20scores%20equate%20to%20true%20competency%20and%20highlight%20the%20need%20for%20evaluation%20practices%20that%20assess%20reliability%20under%20realistic%20variation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13626v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIBERO-Plus%253A%2520In-depth%2520Robustness%2520Analysis%2520of%2520Vision-Language-Action%2520Models%26entry.906535625%3DSenyu%2520Fei%2520and%2520Siyin%2520Wang%2520and%2520Junhao%2520Shi%2520and%2520Zihao%2520Dai%2520and%2520Jikun%2520Cai%2520and%2520Pengfang%2520Qian%2520and%2520Li%2520Ji%2520and%2520Xinzhe%2520He%2520and%2520Shiduo%2520Zhang%2520and%2520Zhaoye%2520Fei%2520and%2520Jinlan%2520Fu%2520and%2520Jingjing%2520Gong%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DVisual-Language-Action%2520%2528VLA%2529%2520models%2520report%2520impressive%2520success%2520rates%2520on%2520robotic%2520manipulation%2520benchmarks%252C%2520yet%2520these%2520results%2520may%2520mask%2520fundamental%2520weaknesses%2520in%2520robustness.%2520We%2520perform%2520a%2520systematic%2520vulnerability%2520analysis%2520by%2520introducing%2520controlled%2520perturbations%2520across%2520seven%2520dimensions%253A%2520objects%2520layout%252C%2520camera%2520viewpoints%252C%2520robot%2520initial%2520states%252C%2520language%2520instructions%252C%2520light%2520conditions%252C%2520background%2520textures%2520and%2520sensor%2520noise.%2520We%2520comprehensively%2520analyzed%2520multiple%2520state-of-the-art%2520models%2520and%2520revealed%2520consistent%2520brittleness%2520beneath%2520apparent%2520competence.%2520Our%2520analysis%2520exposes%2520critical%2520weaknesses%253A%2520models%2520exhibit%2520extreme%2520sensitivity%2520to%2520perturbation%2520factors%252C%2520including%2520camera%2520viewpoints%2520and%2520robot%2520initial%2520states%252C%2520with%2520performance%2520dropping%2520from%252095%2525%2520to%2520below%252030%2525%2520under%2520modest%2520perturbations.%2520Surprisingly%252C%2520models%2520are%2520largely%2520insensitive%2520to%2520language%2520variations%252C%2520with%2520further%2520experiments%2520revealing%2520that%2520models%2520tend%2520to%2520ignore%2520language%2520instructions%2520completely.%2520Our%2520findings%2520challenge%2520the%2520assumption%2520that%2520high%2520benchmark%2520scores%2520equate%2520to%2520true%2520competency%2520and%2520highlight%2520the%2520need%2520for%2520evaluation%2520practices%2520that%2520assess%2520reliability%2520under%2520realistic%2520variation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13626v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIBERO-Plus%3A%20In-depth%20Robustness%20Analysis%20of%20Vision-Language-Action%20Models&entry.906535625=Senyu%20Fei%20and%20Siyin%20Wang%20and%20Junhao%20Shi%20and%20Zihao%20Dai%20and%20Jikun%20Cai%20and%20Pengfang%20Qian%20and%20Li%20Ji%20and%20Xinzhe%20He%20and%20Shiduo%20Zhang%20and%20Zhaoye%20Fei%20and%20Jinlan%20Fu%20and%20Jingjing%20Gong%20and%20Xipeng%20Qiu&entry.1292438233=Visual-Language-Action%20%28VLA%29%20models%20report%20impressive%20success%20rates%20on%20robotic%20manipulation%20benchmarks%2C%20yet%20these%20results%20may%20mask%20fundamental%20weaknesses%20in%20robustness.%20We%20perform%20a%20systematic%20vulnerability%20analysis%20by%20introducing%20controlled%20perturbations%20across%20seven%20dimensions%3A%20objects%20layout%2C%20camera%20viewpoints%2C%20robot%20initial%20states%2C%20language%20instructions%2C%20light%20conditions%2C%20background%20textures%20and%20sensor%20noise.%20We%20comprehensively%20analyzed%20multiple%20state-of-the-art%20models%20and%20revealed%20consistent%20brittleness%20beneath%20apparent%20competence.%20Our%20analysis%20exposes%20critical%20weaknesses%3A%20models%20exhibit%20extreme%20sensitivity%20to%20perturbation%20factors%2C%20including%20camera%20viewpoints%20and%20robot%20initial%20states%2C%20with%20performance%20dropping%20from%2095%25%20to%20below%2030%25%20under%20modest%20perturbations.%20Surprisingly%2C%20models%20are%20largely%20insensitive%20to%20language%20variations%2C%20with%20further%20experiments%20revealing%20that%20models%20tend%20to%20ignore%20language%20instructions%20completely.%20Our%20findings%20challenge%20the%20assumption%20that%20high%20benchmark%20scores%20equate%20to%20true%20competency%20and%20highlight%20the%20need%20for%20evaluation%20practices%20that%20assess%20reliability%20under%20realistic%20variation.&entry.1838667208=http%3A//arxiv.org/abs/2510.13626v3&entry.124074799=Read"},
{"title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching", "author": "Xiangwen Zhang and Xiaowei Dai and Runnan Chen and Xiaoming Chen and Zeke Zexi Hu", "abstract": "Creating physically realistic content in VR often requires complex modeling tools or predefined 3D models, textures, and animations, which present significant barriers for non-expert users. In this paper, we propose SketchPlay, a novel VR interaction framework that transforms humans' air-drawn sketches and gestures into dynamic, physically realistic scenes, making content creation intuitive and playful like drawing. Specifically, sketches capture the structure and spatial arrangement of objects and scenes, while gestures convey physical cues such as velocity, direction, and force that define movement and behavior. By combining these complementary forms of input, SketchPlay captures both the structure and dynamics of user-created content, enabling the generation of a wide range of complex physical phenomena, such as rigid body motion, elastic deformation, and cloth dynamics. Experimental results demonstrate that, compared to traditional text-driven methods, SketchPlay offers significant advantages in expressiveness, and user experience. By providing an intuitive and engaging creation process, SketchPlay lowers the entry barrier for non-expert users and shows strong potential for applications in education, art, and immersive storytelling.", "link": "http://arxiv.org/abs/2512.22016v1", "date": "2025-12-26", "relevancy": 2.2746, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5872}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5622}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SketchPlay%3A%20Intuitive%20Creation%20of%20Physically%20Realistic%20VR%20Content%20with%20Gesture-Driven%20Sketching&body=Title%3A%20SketchPlay%3A%20Intuitive%20Creation%20of%20Physically%20Realistic%20VR%20Content%20with%20Gesture-Driven%20Sketching%0AAuthor%3A%20Xiangwen%20Zhang%20and%20Xiaowei%20Dai%20and%20Runnan%20Chen%20and%20Xiaoming%20Chen%20and%20Zeke%20Zexi%20Hu%0AAbstract%3A%20Creating%20physically%20realistic%20content%20in%20VR%20often%20requires%20complex%20modeling%20tools%20or%20predefined%203D%20models%2C%20textures%2C%20and%20animations%2C%20which%20present%20significant%20barriers%20for%20non-expert%20users.%20In%20this%20paper%2C%20we%20propose%20SketchPlay%2C%20a%20novel%20VR%20interaction%20framework%20that%20transforms%20humans%27%20air-drawn%20sketches%20and%20gestures%20into%20dynamic%2C%20physically%20realistic%20scenes%2C%20making%20content%20creation%20intuitive%20and%20playful%20like%20drawing.%20Specifically%2C%20sketches%20capture%20the%20structure%20and%20spatial%20arrangement%20of%20objects%20and%20scenes%2C%20while%20gestures%20convey%20physical%20cues%20such%20as%20velocity%2C%20direction%2C%20and%20force%20that%20define%20movement%20and%20behavior.%20By%20combining%20these%20complementary%20forms%20of%20input%2C%20SketchPlay%20captures%20both%20the%20structure%20and%20dynamics%20of%20user-created%20content%2C%20enabling%20the%20generation%20of%20a%20wide%20range%20of%20complex%20physical%20phenomena%2C%20such%20as%20rigid%20body%20motion%2C%20elastic%20deformation%2C%20and%20cloth%20dynamics.%20Experimental%20results%20demonstrate%20that%2C%20compared%20to%20traditional%20text-driven%20methods%2C%20SketchPlay%20offers%20significant%20advantages%20in%20expressiveness%2C%20and%20user%20experience.%20By%20providing%20an%20intuitive%20and%20engaging%20creation%20process%2C%20SketchPlay%20lowers%20the%20entry%20barrier%20for%20non-expert%20users%20and%20shows%20strong%20potential%20for%20applications%20in%20education%2C%20art%2C%20and%20immersive%20storytelling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketchPlay%253A%2520Intuitive%2520Creation%2520of%2520Physically%2520Realistic%2520VR%2520Content%2520with%2520Gesture-Driven%2520Sketching%26entry.906535625%3DXiangwen%2520Zhang%2520and%2520Xiaowei%2520Dai%2520and%2520Runnan%2520Chen%2520and%2520Xiaoming%2520Chen%2520and%2520Zeke%2520Zexi%2520Hu%26entry.1292438233%3DCreating%2520physically%2520realistic%2520content%2520in%2520VR%2520often%2520requires%2520complex%2520modeling%2520tools%2520or%2520predefined%25203D%2520models%252C%2520textures%252C%2520and%2520animations%252C%2520which%2520present%2520significant%2520barriers%2520for%2520non-expert%2520users.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SketchPlay%252C%2520a%2520novel%2520VR%2520interaction%2520framework%2520that%2520transforms%2520humans%2527%2520air-drawn%2520sketches%2520and%2520gestures%2520into%2520dynamic%252C%2520physically%2520realistic%2520scenes%252C%2520making%2520content%2520creation%2520intuitive%2520and%2520playful%2520like%2520drawing.%2520Specifically%252C%2520sketches%2520capture%2520the%2520structure%2520and%2520spatial%2520arrangement%2520of%2520objects%2520and%2520scenes%252C%2520while%2520gestures%2520convey%2520physical%2520cues%2520such%2520as%2520velocity%252C%2520direction%252C%2520and%2520force%2520that%2520define%2520movement%2520and%2520behavior.%2520By%2520combining%2520these%2520complementary%2520forms%2520of%2520input%252C%2520SketchPlay%2520captures%2520both%2520the%2520structure%2520and%2520dynamics%2520of%2520user-created%2520content%252C%2520enabling%2520the%2520generation%2520of%2520a%2520wide%2520range%2520of%2520complex%2520physical%2520phenomena%252C%2520such%2520as%2520rigid%2520body%2520motion%252C%2520elastic%2520deformation%252C%2520and%2520cloth%2520dynamics.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520compared%2520to%2520traditional%2520text-driven%2520methods%252C%2520SketchPlay%2520offers%2520significant%2520advantages%2520in%2520expressiveness%252C%2520and%2520user%2520experience.%2520By%2520providing%2520an%2520intuitive%2520and%2520engaging%2520creation%2520process%252C%2520SketchPlay%2520lowers%2520the%2520entry%2520barrier%2520for%2520non-expert%2520users%2520and%2520shows%2520strong%2520potential%2520for%2520applications%2520in%2520education%252C%2520art%252C%2520and%2520immersive%2520storytelling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SketchPlay%3A%20Intuitive%20Creation%20of%20Physically%20Realistic%20VR%20Content%20with%20Gesture-Driven%20Sketching&entry.906535625=Xiangwen%20Zhang%20and%20Xiaowei%20Dai%20and%20Runnan%20Chen%20and%20Xiaoming%20Chen%20and%20Zeke%20Zexi%20Hu&entry.1292438233=Creating%20physically%20realistic%20content%20in%20VR%20often%20requires%20complex%20modeling%20tools%20or%20predefined%203D%20models%2C%20textures%2C%20and%20animations%2C%20which%20present%20significant%20barriers%20for%20non-expert%20users.%20In%20this%20paper%2C%20we%20propose%20SketchPlay%2C%20a%20novel%20VR%20interaction%20framework%20that%20transforms%20humans%27%20air-drawn%20sketches%20and%20gestures%20into%20dynamic%2C%20physically%20realistic%20scenes%2C%20making%20content%20creation%20intuitive%20and%20playful%20like%20drawing.%20Specifically%2C%20sketches%20capture%20the%20structure%20and%20spatial%20arrangement%20of%20objects%20and%20scenes%2C%20while%20gestures%20convey%20physical%20cues%20such%20as%20velocity%2C%20direction%2C%20and%20force%20that%20define%20movement%20and%20behavior.%20By%20combining%20these%20complementary%20forms%20of%20input%2C%20SketchPlay%20captures%20both%20the%20structure%20and%20dynamics%20of%20user-created%20content%2C%20enabling%20the%20generation%20of%20a%20wide%20range%20of%20complex%20physical%20phenomena%2C%20such%20as%20rigid%20body%20motion%2C%20elastic%20deformation%2C%20and%20cloth%20dynamics.%20Experimental%20results%20demonstrate%20that%2C%20compared%20to%20traditional%20text-driven%20methods%2C%20SketchPlay%20offers%20significant%20advantages%20in%20expressiveness%2C%20and%20user%20experience.%20By%20providing%20an%20intuitive%20and%20engaging%20creation%20process%2C%20SketchPlay%20lowers%20the%20entry%20barrier%20for%20non-expert%20users%20and%20shows%20strong%20potential%20for%20applications%20in%20education%2C%20art%2C%20and%20immersive%20storytelling.&entry.1838667208=http%3A//arxiv.org/abs/2512.22016v1&entry.124074799=Read"},
{"title": "HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness", "author": "Chengyu Tian and Wenbin Pei", "abstract": "Robustness in complex systems is of significant engineering and economic importance. However, conventional attack-based a posteriori robustness assessments incur prohibitive computational overhead. Recently, deep learning methods, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), have been widely employed as surrogates for rapid robustness prediction. Nevertheless, these methods neglect the complex higher-order correlations prevalent in real-world systems, which are naturally modeled as hypergraphs. Although Hypergraph Neural Networks (HGNNs) have been widely adopted for hypergraph learning, their topological expressive power has not yet reached the theoretical upper bound. To address this limitation, inspired by Graph Isomorphism Networks, this paper proposes a hypergraph-level Hypergraph Isomorphism Network framework. Theoretically, this approach is proven to possess an expressive power strictly equivalent to the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experimental results demonstrate that while maintaining superior efficiency in training and prediction, the proposed method not only outperforms existing graph-based models but also significantly surpasses conventional HGNNs in tasks that prioritize topological structure representation.", "link": "http://arxiv.org/abs/2512.22014v1", "date": "2025-12-26", "relevancy": 2.2579, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HWL-HIN%3A%20A%20Hypergraph-Level%20Hypergraph%20Isomorphism%20Network%20as%20Powerful%20as%20the%20Hypergraph%20Weisfeiler-Lehman%20Test%20with%20Application%20to%20Higher-Order%20Network%20Robustness&body=Title%3A%20HWL-HIN%3A%20A%20Hypergraph-Level%20Hypergraph%20Isomorphism%20Network%20as%20Powerful%20as%20the%20Hypergraph%20Weisfeiler-Lehman%20Test%20with%20Application%20to%20Higher-Order%20Network%20Robustness%0AAuthor%3A%20Chengyu%20Tian%20and%20Wenbin%20Pei%0AAbstract%3A%20Robustness%20in%20complex%20systems%20is%20of%20significant%20engineering%20and%20economic%20importance.%20However%2C%20conventional%20attack-based%20a%20posteriori%20robustness%20assessments%20incur%20prohibitive%20computational%20overhead.%20Recently%2C%20deep%20learning%20methods%2C%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20have%20been%20widely%20employed%20as%20surrogates%20for%20rapid%20robustness%20prediction.%20Nevertheless%2C%20these%20methods%20neglect%20the%20complex%20higher-order%20correlations%20prevalent%20in%20real-world%20systems%2C%20which%20are%20naturally%20modeled%20as%20hypergraphs.%20Although%20Hypergraph%20Neural%20Networks%20%28HGNNs%29%20have%20been%20widely%20adopted%20for%20hypergraph%20learning%2C%20their%20topological%20expressive%20power%20has%20not%20yet%20reached%20the%20theoretical%20upper%20bound.%20To%20address%20this%20limitation%2C%20inspired%20by%20Graph%20Isomorphism%20Networks%2C%20this%20paper%20proposes%20a%20hypergraph-level%20Hypergraph%20Isomorphism%20Network%20framework.%20Theoretically%2C%20this%20approach%20is%20proven%20to%20possess%20an%20expressive%20power%20strictly%20equivalent%20to%20the%20Hypergraph%20Weisfeiler-Lehman%20test%20and%20is%20applied%20to%20predict%20hypergraph%20robustness.%20Experimental%20results%20demonstrate%20that%20while%20maintaining%20superior%20efficiency%20in%20training%20and%20prediction%2C%20the%20proposed%20method%20not%20only%20outperforms%20existing%20graph-based%20models%20but%20also%20significantly%20surpasses%20conventional%20HGNNs%20in%20tasks%20that%20prioritize%20topological%20structure%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHWL-HIN%253A%2520A%2520Hypergraph-Level%2520Hypergraph%2520Isomorphism%2520Network%2520as%2520Powerful%2520as%2520the%2520Hypergraph%2520Weisfeiler-Lehman%2520Test%2520with%2520Application%2520to%2520Higher-Order%2520Network%2520Robustness%26entry.906535625%3DChengyu%2520Tian%2520and%2520Wenbin%2520Pei%26entry.1292438233%3DRobustness%2520in%2520complex%2520systems%2520is%2520of%2520significant%2520engineering%2520and%2520economic%2520importance.%2520However%252C%2520conventional%2520attack-based%2520a%2520posteriori%2520robustness%2520assessments%2520incur%2520prohibitive%2520computational%2520overhead.%2520Recently%252C%2520deep%2520learning%2520methods%252C%2520such%2520as%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520have%2520been%2520widely%2520employed%2520as%2520surrogates%2520for%2520rapid%2520robustness%2520prediction.%2520Nevertheless%252C%2520these%2520methods%2520neglect%2520the%2520complex%2520higher-order%2520correlations%2520prevalent%2520in%2520real-world%2520systems%252C%2520which%2520are%2520naturally%2520modeled%2520as%2520hypergraphs.%2520Although%2520Hypergraph%2520Neural%2520Networks%2520%2528HGNNs%2529%2520have%2520been%2520widely%2520adopted%2520for%2520hypergraph%2520learning%252C%2520their%2520topological%2520expressive%2520power%2520has%2520not%2520yet%2520reached%2520the%2520theoretical%2520upper%2520bound.%2520To%2520address%2520this%2520limitation%252C%2520inspired%2520by%2520Graph%2520Isomorphism%2520Networks%252C%2520this%2520paper%2520proposes%2520a%2520hypergraph-level%2520Hypergraph%2520Isomorphism%2520Network%2520framework.%2520Theoretically%252C%2520this%2520approach%2520is%2520proven%2520to%2520possess%2520an%2520expressive%2520power%2520strictly%2520equivalent%2520to%2520the%2520Hypergraph%2520Weisfeiler-Lehman%2520test%2520and%2520is%2520applied%2520to%2520predict%2520hypergraph%2520robustness.%2520Experimental%2520results%2520demonstrate%2520that%2520while%2520maintaining%2520superior%2520efficiency%2520in%2520training%2520and%2520prediction%252C%2520the%2520proposed%2520method%2520not%2520only%2520outperforms%2520existing%2520graph-based%2520models%2520but%2520also%2520significantly%2520surpasses%2520conventional%2520HGNNs%2520in%2520tasks%2520that%2520prioritize%2520topological%2520structure%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HWL-HIN%3A%20A%20Hypergraph-Level%20Hypergraph%20Isomorphism%20Network%20as%20Powerful%20as%20the%20Hypergraph%20Weisfeiler-Lehman%20Test%20with%20Application%20to%20Higher-Order%20Network%20Robustness&entry.906535625=Chengyu%20Tian%20and%20Wenbin%20Pei&entry.1292438233=Robustness%20in%20complex%20systems%20is%20of%20significant%20engineering%20and%20economic%20importance.%20However%2C%20conventional%20attack-based%20a%20posteriori%20robustness%20assessments%20incur%20prohibitive%20computational%20overhead.%20Recently%2C%20deep%20learning%20methods%2C%20such%20as%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20have%20been%20widely%20employed%20as%20surrogates%20for%20rapid%20robustness%20prediction.%20Nevertheless%2C%20these%20methods%20neglect%20the%20complex%20higher-order%20correlations%20prevalent%20in%20real-world%20systems%2C%20which%20are%20naturally%20modeled%20as%20hypergraphs.%20Although%20Hypergraph%20Neural%20Networks%20%28HGNNs%29%20have%20been%20widely%20adopted%20for%20hypergraph%20learning%2C%20their%20topological%20expressive%20power%20has%20not%20yet%20reached%20the%20theoretical%20upper%20bound.%20To%20address%20this%20limitation%2C%20inspired%20by%20Graph%20Isomorphism%20Networks%2C%20this%20paper%20proposes%20a%20hypergraph-level%20Hypergraph%20Isomorphism%20Network%20framework.%20Theoretically%2C%20this%20approach%20is%20proven%20to%20possess%20an%20expressive%20power%20strictly%20equivalent%20to%20the%20Hypergraph%20Weisfeiler-Lehman%20test%20and%20is%20applied%20to%20predict%20hypergraph%20robustness.%20Experimental%20results%20demonstrate%20that%20while%20maintaining%20superior%20efficiency%20in%20training%20and%20prediction%2C%20the%20proposed%20method%20not%20only%20outperforms%20existing%20graph-based%20models%20but%20also%20significantly%20surpasses%20conventional%20HGNNs%20in%20tasks%20that%20prioritize%20topological%20structure%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2512.22014v1&entry.124074799=Read"},
{"title": "Efficient Curvature-aware Graph Network", "author": "Chaoqun Fei and Tinglve Zhou and Tianyong Hao and Yangyang Li", "abstract": "Graph curvature provides geometric priors for Graph Neural Networks (GNNs), enhancing their ability to model complex graph structures, particularly in terms of structural awareness, robustness, and theoretical interpretability. Among existing methods, Ollivier-Ricci curvature has been extensively studied due to its strong geometric interpretability, effectively characterizing the local geometric distribution between nodes. However, its prohibitively high computational complexity limits its applicability to large-scale graph datasets. To address this challenge, we propose a novel graph curvature measure--Effective Resistance Curvature--which quantifies the ease of message passing along graph edges using the effective resistance between node pairs, instead of the optimal transport distance. This method significantly outperforms Ollivier-Ricci curvature in computational efficiency while preserving comparable geometric expressiveness. Theoretically, we prove the low computational complexity of effective resistance curvature and establish its substitutability for Ollivier-Ricci curvature. Furthermore, extensive experiments on diverse GNN tasks demonstrate that our method achieves competitive performance with Ollivier-Ricci curvature while drastically reducing computational overhead.", "link": "http://arxiv.org/abs/2511.01443v2", "date": "2025-12-26", "relevancy": 2.252, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4603}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4457}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Curvature-aware%20Graph%20Network&body=Title%3A%20Efficient%20Curvature-aware%20Graph%20Network%0AAuthor%3A%20Chaoqun%20Fei%20and%20Tinglve%20Zhou%20and%20Tianyong%20Hao%20and%20Yangyang%20Li%0AAbstract%3A%20Graph%20curvature%20provides%20geometric%20priors%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20enhancing%20their%20ability%20to%20model%20complex%20graph%20structures%2C%20particularly%20in%20terms%20of%20structural%20awareness%2C%20robustness%2C%20and%20theoretical%20interpretability.%20Among%20existing%20methods%2C%20Ollivier-Ricci%20curvature%20has%20been%20extensively%20studied%20due%20to%20its%20strong%20geometric%20interpretability%2C%20effectively%20characterizing%20the%20local%20geometric%20distribution%20between%20nodes.%20However%2C%20its%20prohibitively%20high%20computational%20complexity%20limits%20its%20applicability%20to%20large-scale%20graph%20datasets.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20graph%20curvature%20measure--Effective%20Resistance%20Curvature--which%20quantifies%20the%20ease%20of%20message%20passing%20along%20graph%20edges%20using%20the%20effective%20resistance%20between%20node%20pairs%2C%20instead%20of%20the%20optimal%20transport%20distance.%20This%20method%20significantly%20outperforms%20Ollivier-Ricci%20curvature%20in%20computational%20efficiency%20while%20preserving%20comparable%20geometric%20expressiveness.%20Theoretically%2C%20we%20prove%20the%20low%20computational%20complexity%20of%20effective%20resistance%20curvature%20and%20establish%20its%20substitutability%20for%20Ollivier-Ricci%20curvature.%20Furthermore%2C%20extensive%20experiments%20on%20diverse%20GNN%20tasks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20with%20Ollivier-Ricci%20curvature%20while%20drastically%20reducing%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Curvature-aware%2520Graph%2520Network%26entry.906535625%3DChaoqun%2520Fei%2520and%2520Tinglve%2520Zhou%2520and%2520Tianyong%2520Hao%2520and%2520Yangyang%2520Li%26entry.1292438233%3DGraph%2520curvature%2520provides%2520geometric%2520priors%2520for%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520enhancing%2520their%2520ability%2520to%2520model%2520complex%2520graph%2520structures%252C%2520particularly%2520in%2520terms%2520of%2520structural%2520awareness%252C%2520robustness%252C%2520and%2520theoretical%2520interpretability.%2520Among%2520existing%2520methods%252C%2520Ollivier-Ricci%2520curvature%2520has%2520been%2520extensively%2520studied%2520due%2520to%2520its%2520strong%2520geometric%2520interpretability%252C%2520effectively%2520characterizing%2520the%2520local%2520geometric%2520distribution%2520between%2520nodes.%2520However%252C%2520its%2520prohibitively%2520high%2520computational%2520complexity%2520limits%2520its%2520applicability%2520to%2520large-scale%2520graph%2520datasets.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520graph%2520curvature%2520measure--Effective%2520Resistance%2520Curvature--which%2520quantifies%2520the%2520ease%2520of%2520message%2520passing%2520along%2520graph%2520edges%2520using%2520the%2520effective%2520resistance%2520between%2520node%2520pairs%252C%2520instead%2520of%2520the%2520optimal%2520transport%2520distance.%2520This%2520method%2520significantly%2520outperforms%2520Ollivier-Ricci%2520curvature%2520in%2520computational%2520efficiency%2520while%2520preserving%2520comparable%2520geometric%2520expressiveness.%2520Theoretically%252C%2520we%2520prove%2520the%2520low%2520computational%2520complexity%2520of%2520effective%2520resistance%2520curvature%2520and%2520establish%2520its%2520substitutability%2520for%2520Ollivier-Ricci%2520curvature.%2520Furthermore%252C%2520extensive%2520experiments%2520on%2520diverse%2520GNN%2520tasks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%2520with%2520Ollivier-Ricci%2520curvature%2520while%2520drastically%2520reducing%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Curvature-aware%20Graph%20Network&entry.906535625=Chaoqun%20Fei%20and%20Tinglve%20Zhou%20and%20Tianyong%20Hao%20and%20Yangyang%20Li&entry.1292438233=Graph%20curvature%20provides%20geometric%20priors%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20enhancing%20their%20ability%20to%20model%20complex%20graph%20structures%2C%20particularly%20in%20terms%20of%20structural%20awareness%2C%20robustness%2C%20and%20theoretical%20interpretability.%20Among%20existing%20methods%2C%20Ollivier-Ricci%20curvature%20has%20been%20extensively%20studied%20due%20to%20its%20strong%20geometric%20interpretability%2C%20effectively%20characterizing%20the%20local%20geometric%20distribution%20between%20nodes.%20However%2C%20its%20prohibitively%20high%20computational%20complexity%20limits%20its%20applicability%20to%20large-scale%20graph%20datasets.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20graph%20curvature%20measure--Effective%20Resistance%20Curvature--which%20quantifies%20the%20ease%20of%20message%20passing%20along%20graph%20edges%20using%20the%20effective%20resistance%20between%20node%20pairs%2C%20instead%20of%20the%20optimal%20transport%20distance.%20This%20method%20significantly%20outperforms%20Ollivier-Ricci%20curvature%20in%20computational%20efficiency%20while%20preserving%20comparable%20geometric%20expressiveness.%20Theoretically%2C%20we%20prove%20the%20low%20computational%20complexity%20of%20effective%20resistance%20curvature%20and%20establish%20its%20substitutability%20for%20Ollivier-Ricci%20curvature.%20Furthermore%2C%20extensive%20experiments%20on%20diverse%20GNN%20tasks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20with%20Ollivier-Ricci%20curvature%20while%20drastically%20reducing%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2511.01443v2&entry.124074799=Read"},
{"title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "author": "S M A Sharif and Abdur Rehman and Fayaz Ali Dharejo and Radu Timofte and Rizwan Ali Naqvi", "abstract": "Real-world images often suffer from spatially diverse degradations such as haze, rain, snow, and low-light, significantly impacting visual quality and downstream vision tasks. Existing all-in-one restoration (AIR) approaches either depend on external text prompts or embed hand-crafted architectural priors (e.g., frequency heuristics); both impose discrete, brittle assumptions that weaken generalization to unseen or mixed degradations. To address this limitation, we propose to reframe AIR as learned latent prior inference, where degradation-aware representations are automatically inferred from the input without explicit task cues. Based on latent priors, we formulate AIR as a structured reasoning paradigm: (1) which features to route (adaptive feature selection), (2) where to restore (spatial localization), and (3) what to restore (degradation semantics). We design a lightweight decoding module that efficiently leverages these latent encoded cues for spatially-adaptive restoration. Extensive experiments across six common degradation tasks, five compound settings, and previously unseen degradations demonstrate that our method outperforms state-of-the-art (SOTA) approaches, achieving an average PSNR improvement of 1.68 dB while being three times more efficient.", "link": "http://arxiv.org/abs/2509.17792v3", "date": "2025-12-26", "relevancy": 2.2109, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5717}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&body=Title%3A%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding%0AAuthor%3A%20S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi%0AAbstract%3A%20Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%20haze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%20downstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%20either%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%20priors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%20that%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%20limitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%20degradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%20without%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%20structured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%20selection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%20restore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%20efficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%20restoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%20compound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%20PSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17792v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation-Aware%2520All-in-One%2520Image%2520Restoration%2520via%2520Latent%2520Prior%2520Encoding%26entry.906535625%3DS%2520M%2520A%2520Sharif%2520and%2520Abdur%2520Rehman%2520and%2520Fayaz%2520Ali%2520Dharejo%2520and%2520Radu%2520Timofte%2520and%2520Rizwan%2520Ali%2520Naqvi%26entry.1292438233%3DReal-world%2520images%2520often%2520suffer%2520from%2520spatially%2520diverse%2520degradations%2520such%2520as%2520haze%252C%2520rain%252C%2520snow%252C%2520and%2520low-light%252C%2520significantly%2520impacting%2520visual%2520quality%2520and%2520downstream%2520vision%2520tasks.%2520Existing%2520all-in-one%2520restoration%2520%2528AIR%2529%2520approaches%2520either%2520depend%2520on%2520external%2520text%2520prompts%2520or%2520embed%2520hand-crafted%2520architectural%2520priors%2520%2528e.g.%252C%2520frequency%2520heuristics%2529%253B%2520both%2520impose%2520discrete%252C%2520brittle%2520assumptions%2520that%2520weaken%2520generalization%2520to%2520unseen%2520or%2520mixed%2520degradations.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520to%2520reframe%2520AIR%2520as%2520learned%2520latent%2520prior%2520inference%252C%2520where%2520degradation-aware%2520representations%2520are%2520automatically%2520inferred%2520from%2520the%2520input%2520without%2520explicit%2520task%2520cues.%2520Based%2520on%2520latent%2520priors%252C%2520we%2520formulate%2520AIR%2520as%2520a%2520structured%2520reasoning%2520paradigm%253A%2520%25281%2529%2520which%2520features%2520to%2520route%2520%2528adaptive%2520feature%2520selection%2529%252C%2520%25282%2529%2520where%2520to%2520restore%2520%2528spatial%2520localization%2529%252C%2520and%2520%25283%2529%2520what%2520to%2520restore%2520%2528degradation%2520semantics%2529.%2520We%2520design%2520a%2520lightweight%2520decoding%2520module%2520that%2520efficiently%2520leverages%2520these%2520latent%2520encoded%2520cues%2520for%2520spatially-adaptive%2520restoration.%2520Extensive%2520experiments%2520across%2520six%2520common%2520degradation%2520tasks%252C%2520five%2520compound%2520settings%252C%2520and%2520previously%2520unseen%2520degradations%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520approaches%252C%2520achieving%2520an%2520average%2520PSNR%2520improvement%2520of%25201.68%2520dB%2520while%2520being%2520three%2520times%2520more%2520efficient.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17792v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&entry.906535625=S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi&entry.1292438233=Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%20haze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%20downstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%20either%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%20priors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%20that%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%20limitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%20degradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%20without%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%20structured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%20selection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%20restore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%20efficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%20restoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%20compound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%20PSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.&entry.1838667208=http%3A//arxiv.org/abs/2509.17792v3&entry.124074799=Read"},
{"title": "Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation", "author": "Xintong Duan and Yutong He and Fahim Tajwar and Ruslan Salakhutdinov and J. Zico Kolter and Jeff Schneider", "abstract": "Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cloning or rely on complex concurrent training of multiple networks under the actor-critic framework. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method achieves single-step sampling while generating higher-reward action trajectories through decoupled training and noise-free reward signals. Empirical evaluations on the Gym MuJoCo, FrankaKitchen, and long horizon planning benchmarks demonstrate that our approach can achieve a 9.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.", "link": "http://arxiv.org/abs/2506.07822v2", "date": "2025-12-26", "relevancy": 2.0701, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.525}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Diffusion%20Planners%20in%20Offline%20RL%20via%20Reward-Aware%20Consistency%20Trajectory%20Distillation&body=Title%3A%20Accelerating%20Diffusion%20Planners%20in%20Offline%20RL%20via%20Reward-Aware%20Consistency%20Trajectory%20Distillation%0AAuthor%3A%20Xintong%20Duan%20and%20Yutong%20He%20and%20Fahim%20Tajwar%20and%20Ruslan%20Salakhutdinov%20and%20J.%20Zico%20Kolter%20and%20Jeff%20Schneider%0AAbstract%3A%20Although%20diffusion%20models%20have%20achieved%20strong%20results%20in%20decision-making%20tasks%2C%20their%20slow%20inference%20speed%20remains%20a%20key%20limitation.%20While%20consistency%20models%20offer%20a%20potential%20solution%2C%20existing%20applications%20to%20decision-making%20either%20struggle%20with%20suboptimal%20demonstrations%20under%20behavior%20cloning%20or%20rely%20on%20complex%20concurrent%20training%20of%20multiple%20networks%20under%20the%20actor-critic%20framework.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20to%20consistency%20distillation%20for%20offline%20reinforcement%20learning%20that%20directly%20incorporates%20reward%20optimization%20into%20the%20distillation%20process.%20Our%20method%20achieves%20single-step%20sampling%20while%20generating%20higher-reward%20action%20trajectories%20through%20decoupled%20training%20and%20noise-free%20reward%20signals.%20Empirical%20evaluations%20on%20the%20Gym%20MuJoCo%2C%20FrankaKitchen%2C%20and%20long%20horizon%20planning%20benchmarks%20demonstrate%20that%20our%20approach%20can%20achieve%20a%209.7%25%20improvement%20over%20previous%20state-of-the-art%20while%20offering%20up%20to%20142x%20speedup%20over%20diffusion%20counterparts%20in%20inference%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2506.07822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Diffusion%2520Planners%2520in%2520Offline%2520RL%2520via%2520Reward-Aware%2520Consistency%2520Trajectory%2520Distillation%26entry.906535625%3DXintong%2520Duan%2520and%2520Yutong%2520He%2520and%2520Fahim%2520Tajwar%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520J.%2520Zico%2520Kolter%2520and%2520Jeff%2520Schneider%26entry.1292438233%3DAlthough%2520diffusion%2520models%2520have%2520achieved%2520strong%2520results%2520in%2520decision-making%2520tasks%252C%2520their%2520slow%2520inference%2520speed%2520remains%2520a%2520key%2520limitation.%2520While%2520consistency%2520models%2520offer%2520a%2520potential%2520solution%252C%2520existing%2520applications%2520to%2520decision-making%2520either%2520struggle%2520with%2520suboptimal%2520demonstrations%2520under%2520behavior%2520cloning%2520or%2520rely%2520on%2520complex%2520concurrent%2520training%2520of%2520multiple%2520networks%2520under%2520the%2520actor-critic%2520framework.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520consistency%2520distillation%2520for%2520offline%2520reinforcement%2520learning%2520that%2520directly%2520incorporates%2520reward%2520optimization%2520into%2520the%2520distillation%2520process.%2520Our%2520method%2520achieves%2520single-step%2520sampling%2520while%2520generating%2520higher-reward%2520action%2520trajectories%2520through%2520decoupled%2520training%2520and%2520noise-free%2520reward%2520signals.%2520Empirical%2520evaluations%2520on%2520the%2520Gym%2520MuJoCo%252C%2520FrankaKitchen%252C%2520and%2520long%2520horizon%2520planning%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520a%25209.7%2525%2520improvement%2520over%2520previous%2520state-of-the-art%2520while%2520offering%2520up%2520to%2520142x%2520speedup%2520over%2520diffusion%2520counterparts%2520in%2520inference%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Diffusion%20Planners%20in%20Offline%20RL%20via%20Reward-Aware%20Consistency%20Trajectory%20Distillation&entry.906535625=Xintong%20Duan%20and%20Yutong%20He%20and%20Fahim%20Tajwar%20and%20Ruslan%20Salakhutdinov%20and%20J.%20Zico%20Kolter%20and%20Jeff%20Schneider&entry.1292438233=Although%20diffusion%20models%20have%20achieved%20strong%20results%20in%20decision-making%20tasks%2C%20their%20slow%20inference%20speed%20remains%20a%20key%20limitation.%20While%20consistency%20models%20offer%20a%20potential%20solution%2C%20existing%20applications%20to%20decision-making%20either%20struggle%20with%20suboptimal%20demonstrations%20under%20behavior%20cloning%20or%20rely%20on%20complex%20concurrent%20training%20of%20multiple%20networks%20under%20the%20actor-critic%20framework.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20to%20consistency%20distillation%20for%20offline%20reinforcement%20learning%20that%20directly%20incorporates%20reward%20optimization%20into%20the%20distillation%20process.%20Our%20method%20achieves%20single-step%20sampling%20while%20generating%20higher-reward%20action%20trajectories%20through%20decoupled%20training%20and%20noise-free%20reward%20signals.%20Empirical%20evaluations%20on%20the%20Gym%20MuJoCo%2C%20FrankaKitchen%2C%20and%20long%20horizon%20planning%20benchmarks%20demonstrate%20that%20our%20approach%20can%20achieve%20a%209.7%25%20improvement%20over%20previous%20state-of-the-art%20while%20offering%20up%20to%20142x%20speedup%20over%20diffusion%20counterparts%20in%20inference%20time.&entry.1838667208=http%3A//arxiv.org/abs/2506.07822v2&entry.124074799=Read"},
{"title": "From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation", "author": "Nagham Osman and Vittorio Lembo and Giovanni Bottegoni and Laura Toni", "abstract": "Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$\u03b2$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.", "link": "http://arxiv.org/abs/2512.22031v1", "date": "2025-12-26", "relevancy": 2.0651, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5177}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20In%20Silico%20to%20In%20Vitro%3A%20Evaluating%20Molecule%20Generative%20Models%20for%20Hit%20Generation&body=Title%3A%20From%20In%20Silico%20to%20In%20Vitro%3A%20Evaluating%20Molecule%20Generative%20Models%20for%20Hit%20Generation%0AAuthor%3A%20Nagham%20Osman%20and%20Vittorio%20Lembo%20and%20Giovanni%20Bottegoni%20and%20Laura%20Toni%0AAbstract%3A%20Hit%20identification%20is%20a%20critical%20yet%20resource-intensive%20step%20in%20the%20drug%20discovery%20pipeline%2C%20traditionally%20relying%20on%20high-throughput%20screening%20of%20large%20compound%20libraries.%20Despite%20advancements%20in%20virtual%20screening%2C%20these%20methods%20remain%20time-consuming%20and%20costly.%20Recent%20progress%20in%20deep%20learning%20has%20enabled%20the%20development%20of%20generative%20models%20capable%20of%20learning%20complex%20molecular%20representations%20and%20generating%20novel%20compounds%20de%20novo.%20However%2C%20using%20ML%20to%20replace%20the%20entire%20drug-discovery%20pipeline%20is%20highly%20challenging.%20In%20this%20work%2C%20we%20rather%20investigate%20whether%20generative%20models%20can%20replace%20one%20step%20of%20the%20pipeline%3A%20hit-like%20molecule%20generation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20explicitly%20frame%20hit-like%20molecule%20generation%20as%20a%20standalone%20task%20and%20empirically%20test%20whether%20generative%20models%20can%20directly%20support%20this%20stage%20of%20the%20drug%20discovery%20pipeline.%20Specifically%2C%20we%20investigate%20if%20such%20models%20can%20be%20trained%20to%20generate%20hit-like%20molecules%2C%20enabling%20direct%20incorporation%20into%2C%20or%20even%20substitution%20of%2C%20traditional%20hit%20identification%20workflows.%20We%20propose%20an%20evaluation%20framework%20tailored%20to%20this%20task%2C%20integrating%20physicochemical%2C%20structural%2C%20and%20bioactivity-related%20criteria%20within%20a%20multi-stage%20filtering%20pipeline%20that%20defines%20the%20hit-like%20chemical%20space.%20Two%20autoregressive%20and%20one%20diffusion-based%20generative%20models%20were%20benchmarked%20across%20various%20datasets%20and%20training%20settings%2C%20with%20outputs%20assessed%20using%20standard%20metrics%20and%20target-specific%20docking%20scores.%20Our%20results%20show%20that%20these%20models%20can%20generate%20valid%2C%20diverse%2C%20and%20biologically%20relevant%20compounds%20across%20multiple%20targets%2C%20with%20a%20few%20selected%20GSK-3%24%CE%B2%24%20hits%20synthesized%20and%20confirmed%20active%20in%20vitro.%20We%20also%20identify%20key%20limitations%20in%20current%20evaluation%20metrics%20and%20available%20training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520In%2520Silico%2520to%2520In%2520Vitro%253A%2520Evaluating%2520Molecule%2520Generative%2520Models%2520for%2520Hit%2520Generation%26entry.906535625%3DNagham%2520Osman%2520and%2520Vittorio%2520Lembo%2520and%2520Giovanni%2520Bottegoni%2520and%2520Laura%2520Toni%26entry.1292438233%3DHit%2520identification%2520is%2520a%2520critical%2520yet%2520resource-intensive%2520step%2520in%2520the%2520drug%2520discovery%2520pipeline%252C%2520traditionally%2520relying%2520on%2520high-throughput%2520screening%2520of%2520large%2520compound%2520libraries.%2520Despite%2520advancements%2520in%2520virtual%2520screening%252C%2520these%2520methods%2520remain%2520time-consuming%2520and%2520costly.%2520Recent%2520progress%2520in%2520deep%2520learning%2520has%2520enabled%2520the%2520development%2520of%2520generative%2520models%2520capable%2520of%2520learning%2520complex%2520molecular%2520representations%2520and%2520generating%2520novel%2520compounds%2520de%2520novo.%2520However%252C%2520using%2520ML%2520to%2520replace%2520the%2520entire%2520drug-discovery%2520pipeline%2520is%2520highly%2520challenging.%2520In%2520this%2520work%252C%2520we%2520rather%2520investigate%2520whether%2520generative%2520models%2520can%2520replace%2520one%2520step%2520of%2520the%2520pipeline%253A%2520hit-like%2520molecule%2520generation.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520explicitly%2520frame%2520hit-like%2520molecule%2520generation%2520as%2520a%2520standalone%2520task%2520and%2520empirically%2520test%2520whether%2520generative%2520models%2520can%2520directly%2520support%2520this%2520stage%2520of%2520the%2520drug%2520discovery%2520pipeline.%2520Specifically%252C%2520we%2520investigate%2520if%2520such%2520models%2520can%2520be%2520trained%2520to%2520generate%2520hit-like%2520molecules%252C%2520enabling%2520direct%2520incorporation%2520into%252C%2520or%2520even%2520substitution%2520of%252C%2520traditional%2520hit%2520identification%2520workflows.%2520We%2520propose%2520an%2520evaluation%2520framework%2520tailored%2520to%2520this%2520task%252C%2520integrating%2520physicochemical%252C%2520structural%252C%2520and%2520bioactivity-related%2520criteria%2520within%2520a%2520multi-stage%2520filtering%2520pipeline%2520that%2520defines%2520the%2520hit-like%2520chemical%2520space.%2520Two%2520autoregressive%2520and%2520one%2520diffusion-based%2520generative%2520models%2520were%2520benchmarked%2520across%2520various%2520datasets%2520and%2520training%2520settings%252C%2520with%2520outputs%2520assessed%2520using%2520standard%2520metrics%2520and%2520target-specific%2520docking%2520scores.%2520Our%2520results%2520show%2520that%2520these%2520models%2520can%2520generate%2520valid%252C%2520diverse%252C%2520and%2520biologically%2520relevant%2520compounds%2520across%2520multiple%2520targets%252C%2520with%2520a%2520few%2520selected%2520GSK-3%2524%25CE%25B2%2524%2520hits%2520synthesized%2520and%2520confirmed%2520active%2520in%2520vitro.%2520We%2520also%2520identify%2520key%2520limitations%2520in%2520current%2520evaluation%2520metrics%2520and%2520available%2520training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20In%20Silico%20to%20In%20Vitro%3A%20Evaluating%20Molecule%20Generative%20Models%20for%20Hit%20Generation&entry.906535625=Nagham%20Osman%20and%20Vittorio%20Lembo%20and%20Giovanni%20Bottegoni%20and%20Laura%20Toni&entry.1292438233=Hit%20identification%20is%20a%20critical%20yet%20resource-intensive%20step%20in%20the%20drug%20discovery%20pipeline%2C%20traditionally%20relying%20on%20high-throughput%20screening%20of%20large%20compound%20libraries.%20Despite%20advancements%20in%20virtual%20screening%2C%20these%20methods%20remain%20time-consuming%20and%20costly.%20Recent%20progress%20in%20deep%20learning%20has%20enabled%20the%20development%20of%20generative%20models%20capable%20of%20learning%20complex%20molecular%20representations%20and%20generating%20novel%20compounds%20de%20novo.%20However%2C%20using%20ML%20to%20replace%20the%20entire%20drug-discovery%20pipeline%20is%20highly%20challenging.%20In%20this%20work%2C%20we%20rather%20investigate%20whether%20generative%20models%20can%20replace%20one%20step%20of%20the%20pipeline%3A%20hit-like%20molecule%20generation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20explicitly%20frame%20hit-like%20molecule%20generation%20as%20a%20standalone%20task%20and%20empirically%20test%20whether%20generative%20models%20can%20directly%20support%20this%20stage%20of%20the%20drug%20discovery%20pipeline.%20Specifically%2C%20we%20investigate%20if%20such%20models%20can%20be%20trained%20to%20generate%20hit-like%20molecules%2C%20enabling%20direct%20incorporation%20into%2C%20or%20even%20substitution%20of%2C%20traditional%20hit%20identification%20workflows.%20We%20propose%20an%20evaluation%20framework%20tailored%20to%20this%20task%2C%20integrating%20physicochemical%2C%20structural%2C%20and%20bioactivity-related%20criteria%20within%20a%20multi-stage%20filtering%20pipeline%20that%20defines%20the%20hit-like%20chemical%20space.%20Two%20autoregressive%20and%20one%20diffusion-based%20generative%20models%20were%20benchmarked%20across%20various%20datasets%20and%20training%20settings%2C%20with%20outputs%20assessed%20using%20standard%20metrics%20and%20target-specific%20docking%20scores.%20Our%20results%20show%20that%20these%20models%20can%20generate%20valid%2C%20diverse%2C%20and%20biologically%20relevant%20compounds%20across%20multiple%20targets%2C%20with%20a%20few%20selected%20GSK-3%24%CE%B2%24%20hits%20synthesized%20and%20confirmed%20active%20in%20vitro.%20We%20also%20identify%20key%20limitations%20in%20current%20evaluation%20metrics%20and%20available%20training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.22031v1&entry.124074799=Read"},
{"title": "Scaling Adversarial Training via Data Selection", "author": "Youran Ye and Dejin Wang and Ajinkya Bhandare", "abstract": "Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose \\emph{Selective Adversarial Training}, which perturbs only a subset of critical samples in each minibatch. Specifically, we introduce two principled selection criteria: (1) margin-based sampling, which prioritizes samples near the decision boundary, and (2) gradient-matching sampling, which selects samples whose gradients align with the dominant batch optimization direction. Adversarial examples are generated only for the selected subset, while the remaining samples are trained cleanly using a mixed objective. Experiments on MNIST and CIFAR-10 show that the proposed methods achieve robustness comparable to, or even exceeding, full PGD adversarial training, while reducing adversarial computation by up to $50\\%$, demonstrating that informed sample selection is sufficient for scalable adversarial robustness.", "link": "http://arxiv.org/abs/2512.22069v1", "date": "2025-12-26", "relevancy": 2.0644, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5551}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4891}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Adversarial%20Training%20via%20Data%20Selection&body=Title%3A%20Scaling%20Adversarial%20Training%20via%20Data%20Selection%0AAuthor%3A%20Youran%20Ye%20and%20Dejin%20Wang%20and%20Ajinkya%20Bhandare%0AAbstract%3A%20Projected%20Gradient%20Descent%20%28PGD%29%20is%20a%20strong%20and%20widely%20used%20first-order%20adversarial%20attack%2C%20yet%20its%20computational%20cost%20scales%20poorly%2C%20as%20all%20training%20samples%20undergo%20identical%20iterative%20inner-loop%20optimization%20despite%20contributing%20unequally%20to%20robustness.%20Motivated%20by%20this%20inefficiency%2C%20we%20propose%20%5Cemph%7BSelective%20Adversarial%20Training%7D%2C%20which%20perturbs%20only%20a%20subset%20of%20critical%20samples%20in%20each%20minibatch.%20Specifically%2C%20we%20introduce%20two%20principled%20selection%20criteria%3A%20%281%29%20margin-based%20sampling%2C%20which%20prioritizes%20samples%20near%20the%20decision%20boundary%2C%20and%20%282%29%20gradient-matching%20sampling%2C%20which%20selects%20samples%20whose%20gradients%20align%20with%20the%20dominant%20batch%20optimization%20direction.%20Adversarial%20examples%20are%20generated%20only%20for%20the%20selected%20subset%2C%20while%20the%20remaining%20samples%20are%20trained%20cleanly%20using%20a%20mixed%20objective.%20Experiments%20on%20MNIST%20and%20CIFAR-10%20show%20that%20the%20proposed%20methods%20achieve%20robustness%20comparable%20to%2C%20or%20even%20exceeding%2C%20full%20PGD%20adversarial%20training%2C%20while%20reducing%20adversarial%20computation%20by%20up%20to%20%2450%5C%25%24%2C%20demonstrating%20that%20informed%20sample%20selection%20is%20sufficient%20for%20scalable%20adversarial%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Adversarial%2520Training%2520via%2520Data%2520Selection%26entry.906535625%3DYouran%2520Ye%2520and%2520Dejin%2520Wang%2520and%2520Ajinkya%2520Bhandare%26entry.1292438233%3DProjected%2520Gradient%2520Descent%2520%2528PGD%2529%2520is%2520a%2520strong%2520and%2520widely%2520used%2520first-order%2520adversarial%2520attack%252C%2520yet%2520its%2520computational%2520cost%2520scales%2520poorly%252C%2520as%2520all%2520training%2520samples%2520undergo%2520identical%2520iterative%2520inner-loop%2520optimization%2520despite%2520contributing%2520unequally%2520to%2520robustness.%2520Motivated%2520by%2520this%2520inefficiency%252C%2520we%2520propose%2520%255Cemph%257BSelective%2520Adversarial%2520Training%257D%252C%2520which%2520perturbs%2520only%2520a%2520subset%2520of%2520critical%2520samples%2520in%2520each%2520minibatch.%2520Specifically%252C%2520we%2520introduce%2520two%2520principled%2520selection%2520criteria%253A%2520%25281%2529%2520margin-based%2520sampling%252C%2520which%2520prioritizes%2520samples%2520near%2520the%2520decision%2520boundary%252C%2520and%2520%25282%2529%2520gradient-matching%2520sampling%252C%2520which%2520selects%2520samples%2520whose%2520gradients%2520align%2520with%2520the%2520dominant%2520batch%2520optimization%2520direction.%2520Adversarial%2520examples%2520are%2520generated%2520only%2520for%2520the%2520selected%2520subset%252C%2520while%2520the%2520remaining%2520samples%2520are%2520trained%2520cleanly%2520using%2520a%2520mixed%2520objective.%2520Experiments%2520on%2520MNIST%2520and%2520CIFAR-10%2520show%2520that%2520the%2520proposed%2520methods%2520achieve%2520robustness%2520comparable%2520to%252C%2520or%2520even%2520exceeding%252C%2520full%2520PGD%2520adversarial%2520training%252C%2520while%2520reducing%2520adversarial%2520computation%2520by%2520up%2520to%2520%252450%255C%2525%2524%252C%2520demonstrating%2520that%2520informed%2520sample%2520selection%2520is%2520sufficient%2520for%2520scalable%2520adversarial%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Adversarial%20Training%20via%20Data%20Selection&entry.906535625=Youran%20Ye%20and%20Dejin%20Wang%20and%20Ajinkya%20Bhandare&entry.1292438233=Projected%20Gradient%20Descent%20%28PGD%29%20is%20a%20strong%20and%20widely%20used%20first-order%20adversarial%20attack%2C%20yet%20its%20computational%20cost%20scales%20poorly%2C%20as%20all%20training%20samples%20undergo%20identical%20iterative%20inner-loop%20optimization%20despite%20contributing%20unequally%20to%20robustness.%20Motivated%20by%20this%20inefficiency%2C%20we%20propose%20%5Cemph%7BSelective%20Adversarial%20Training%7D%2C%20which%20perturbs%20only%20a%20subset%20of%20critical%20samples%20in%20each%20minibatch.%20Specifically%2C%20we%20introduce%20two%20principled%20selection%20criteria%3A%20%281%29%20margin-based%20sampling%2C%20which%20prioritizes%20samples%20near%20the%20decision%20boundary%2C%20and%20%282%29%20gradient-matching%20sampling%2C%20which%20selects%20samples%20whose%20gradients%20align%20with%20the%20dominant%20batch%20optimization%20direction.%20Adversarial%20examples%20are%20generated%20only%20for%20the%20selected%20subset%2C%20while%20the%20remaining%20samples%20are%20trained%20cleanly%20using%20a%20mixed%20objective.%20Experiments%20on%20MNIST%20and%20CIFAR-10%20show%20that%20the%20proposed%20methods%20achieve%20robustness%20comparable%20to%2C%20or%20even%20exceeding%2C%20full%20PGD%20adversarial%20training%2C%20while%20reducing%20adversarial%20computation%20by%20up%20to%20%2450%5C%25%24%2C%20demonstrating%20that%20informed%20sample%20selection%20is%20sufficient%20for%20scalable%20adversarial%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2512.22069v1&entry.124074799=Read"},
{"title": "Accelerating Training Speed of Tiny Recursive Models with Curriculum Guided Adaptive Recursion", "author": "Kaleem Ullah Qasim and Jiashu Zhang", "abstract": "Background: Recursive reasoning models achieve strong performance through iterative refinement, allowing small networks to match large language models. However, training is computationally expensive, often requiring 36 GPU-hours for Sudoku extreme. Existing models use fixed recursion depth and uniform supervision weighting, leading to inefficient training. Objectives: We propose CGAR (Curriculum-Guided Adaptive Recursion), applying curriculum learning to architectural depth. CGAR introduces Progressive Depth Curriculum (PDC) to dynamically adjust recursion depth and Hierarchical Supervision Weighting (HSW) to apply exponentially decaying importance to supervision steps. Methods: PDC implements a three-stage schedule transitioning from shallow (2, 1) to full depth (6, 3) configurations, providing 41.4% FLOPs reduction. HSW applies exponential decay to supervision steps, achieving 40% gradient variance reduction and accelerated convergence. Results: On Sudoku-Extreme, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours) with only a 0.63% accuracy drop (86.65% to 86.02%). PDC alone achieves 2.26x speedup with 85.47% accuracy, showing a Pareto improvement in efficiency and quality. HSW provides 1.61x speedup. CGAR-trained models show superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Conclusions: CGAR enables efficient training of recursive models on modest hardware. By treating depth as a scheduled parameter, it achieves substantial savings and prevents overfitting, making these models practical for neurosymbolic AI and program synthesis. https://github.com/Kaleemullahqasim/CGAR and huggingface.co/Kaleemullah/trm-cgar-sudoku.", "link": "http://arxiv.org/abs/2511.08653v3", "date": "2025-12-26", "relevancy": 2.0575, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5187}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Training%20Speed%20of%20Tiny%20Recursive%20Models%20with%20Curriculum%20Guided%20Adaptive%20Recursion&body=Title%3A%20Accelerating%20Training%20Speed%20of%20Tiny%20Recursive%20Models%20with%20Curriculum%20Guided%20Adaptive%20Recursion%0AAuthor%3A%20Kaleem%20Ullah%20Qasim%20and%20Jiashu%20Zhang%0AAbstract%3A%20Background%3A%20Recursive%20reasoning%20models%20achieve%20strong%20performance%20through%20iterative%20refinement%2C%20allowing%20small%20networks%20to%20match%20large%20language%20models.%20However%2C%20training%20is%20computationally%20expensive%2C%20often%20requiring%2036%20GPU-hours%20for%20Sudoku%20extreme.%20Existing%20models%20use%20fixed%20recursion%20depth%20and%20uniform%20supervision%20weighting%2C%20leading%20to%20inefficient%20training.%20Objectives%3A%20We%20propose%20CGAR%20%28Curriculum-Guided%20Adaptive%20Recursion%29%2C%20applying%20curriculum%20learning%20to%20architectural%20depth.%20CGAR%20introduces%20Progressive%20Depth%20Curriculum%20%28PDC%29%20to%20dynamically%20adjust%20recursion%20depth%20and%20Hierarchical%20Supervision%20Weighting%20%28HSW%29%20to%20apply%20exponentially%20decaying%20importance%20to%20supervision%20steps.%20Methods%3A%20PDC%20implements%20a%20three-stage%20schedule%20transitioning%20from%20shallow%20%282%2C%201%29%20to%20full%20depth%20%286%2C%203%29%20configurations%2C%20providing%2041.4%25%20FLOPs%20reduction.%20HSW%20applies%20exponential%20decay%20to%20supervision%20steps%2C%20achieving%2040%25%20gradient%20variance%20reduction%20and%20accelerated%20convergence.%20Results%3A%20On%20Sudoku-Extreme%2C%20CGAR%20achieves%201.71x%20training%20speedup%20%2810.93%20to%206.38%20hours%29%20with%20only%20a%200.63%25%20accuracy%20drop%20%2886.65%25%20to%2086.02%25%29.%20PDC%20alone%20achieves%202.26x%20speedup%20with%2085.47%25%20accuracy%2C%20showing%20a%20Pareto%20improvement%20in%20efficiency%20and%20quality.%20HSW%20provides%201.61x%20speedup.%20CGAR-trained%20models%20show%20superior%20inference%20efficiency%20with%20100%25%20halting%20accuracy%20and%2011%25%20fewer%20reasoning%20steps.%20Conclusions%3A%20CGAR%20enables%20efficient%20training%20of%20recursive%20models%20on%20modest%20hardware.%20By%20treating%20depth%20as%20a%20scheduled%20parameter%2C%20it%20achieves%20substantial%20savings%20and%20prevents%20overfitting%2C%20making%20these%20models%20practical%20for%20neurosymbolic%20AI%20and%20program%20synthesis.%20https%3A//github.com/Kaleemullahqasim/CGAR%20and%20huggingface.co/Kaleemullah/trm-cgar-sudoku.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08653v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Training%2520Speed%2520of%2520Tiny%2520Recursive%2520Models%2520with%2520Curriculum%2520Guided%2520Adaptive%2520Recursion%26entry.906535625%3DKaleem%2520Ullah%2520Qasim%2520and%2520Jiashu%2520Zhang%26entry.1292438233%3DBackground%253A%2520Recursive%2520reasoning%2520models%2520achieve%2520strong%2520performance%2520through%2520iterative%2520refinement%252C%2520allowing%2520small%2520networks%2520to%2520match%2520large%2520language%2520models.%2520However%252C%2520training%2520is%2520computationally%2520expensive%252C%2520often%2520requiring%252036%2520GPU-hours%2520for%2520Sudoku%2520extreme.%2520Existing%2520models%2520use%2520fixed%2520recursion%2520depth%2520and%2520uniform%2520supervision%2520weighting%252C%2520leading%2520to%2520inefficient%2520training.%2520Objectives%253A%2520We%2520propose%2520CGAR%2520%2528Curriculum-Guided%2520Adaptive%2520Recursion%2529%252C%2520applying%2520curriculum%2520learning%2520to%2520architectural%2520depth.%2520CGAR%2520introduces%2520Progressive%2520Depth%2520Curriculum%2520%2528PDC%2529%2520to%2520dynamically%2520adjust%2520recursion%2520depth%2520and%2520Hierarchical%2520Supervision%2520Weighting%2520%2528HSW%2529%2520to%2520apply%2520exponentially%2520decaying%2520importance%2520to%2520supervision%2520steps.%2520Methods%253A%2520PDC%2520implements%2520a%2520three-stage%2520schedule%2520transitioning%2520from%2520shallow%2520%25282%252C%25201%2529%2520to%2520full%2520depth%2520%25286%252C%25203%2529%2520configurations%252C%2520providing%252041.4%2525%2520FLOPs%2520reduction.%2520HSW%2520applies%2520exponential%2520decay%2520to%2520supervision%2520steps%252C%2520achieving%252040%2525%2520gradient%2520variance%2520reduction%2520and%2520accelerated%2520convergence.%2520Results%253A%2520On%2520Sudoku-Extreme%252C%2520CGAR%2520achieves%25201.71x%2520training%2520speedup%2520%252810.93%2520to%25206.38%2520hours%2529%2520with%2520only%2520a%25200.63%2525%2520accuracy%2520drop%2520%252886.65%2525%2520to%252086.02%2525%2529.%2520PDC%2520alone%2520achieves%25202.26x%2520speedup%2520with%252085.47%2525%2520accuracy%252C%2520showing%2520a%2520Pareto%2520improvement%2520in%2520efficiency%2520and%2520quality.%2520HSW%2520provides%25201.61x%2520speedup.%2520CGAR-trained%2520models%2520show%2520superior%2520inference%2520efficiency%2520with%2520100%2525%2520halting%2520accuracy%2520and%252011%2525%2520fewer%2520reasoning%2520steps.%2520Conclusions%253A%2520CGAR%2520enables%2520efficient%2520training%2520of%2520recursive%2520models%2520on%2520modest%2520hardware.%2520By%2520treating%2520depth%2520as%2520a%2520scheduled%2520parameter%252C%2520it%2520achieves%2520substantial%2520savings%2520and%2520prevents%2520overfitting%252C%2520making%2520these%2520models%2520practical%2520for%2520neurosymbolic%2520AI%2520and%2520program%2520synthesis.%2520https%253A//github.com/Kaleemullahqasim/CGAR%2520and%2520huggingface.co/Kaleemullah/trm-cgar-sudoku.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08653v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Training%20Speed%20of%20Tiny%20Recursive%20Models%20with%20Curriculum%20Guided%20Adaptive%20Recursion&entry.906535625=Kaleem%20Ullah%20Qasim%20and%20Jiashu%20Zhang&entry.1292438233=Background%3A%20Recursive%20reasoning%20models%20achieve%20strong%20performance%20through%20iterative%20refinement%2C%20allowing%20small%20networks%20to%20match%20large%20language%20models.%20However%2C%20training%20is%20computationally%20expensive%2C%20often%20requiring%2036%20GPU-hours%20for%20Sudoku%20extreme.%20Existing%20models%20use%20fixed%20recursion%20depth%20and%20uniform%20supervision%20weighting%2C%20leading%20to%20inefficient%20training.%20Objectives%3A%20We%20propose%20CGAR%20%28Curriculum-Guided%20Adaptive%20Recursion%29%2C%20applying%20curriculum%20learning%20to%20architectural%20depth.%20CGAR%20introduces%20Progressive%20Depth%20Curriculum%20%28PDC%29%20to%20dynamically%20adjust%20recursion%20depth%20and%20Hierarchical%20Supervision%20Weighting%20%28HSW%29%20to%20apply%20exponentially%20decaying%20importance%20to%20supervision%20steps.%20Methods%3A%20PDC%20implements%20a%20three-stage%20schedule%20transitioning%20from%20shallow%20%282%2C%201%29%20to%20full%20depth%20%286%2C%203%29%20configurations%2C%20providing%2041.4%25%20FLOPs%20reduction.%20HSW%20applies%20exponential%20decay%20to%20supervision%20steps%2C%20achieving%2040%25%20gradient%20variance%20reduction%20and%20accelerated%20convergence.%20Results%3A%20On%20Sudoku-Extreme%2C%20CGAR%20achieves%201.71x%20training%20speedup%20%2810.93%20to%206.38%20hours%29%20with%20only%20a%200.63%25%20accuracy%20drop%20%2886.65%25%20to%2086.02%25%29.%20PDC%20alone%20achieves%202.26x%20speedup%20with%2085.47%25%20accuracy%2C%20showing%20a%20Pareto%20improvement%20in%20efficiency%20and%20quality.%20HSW%20provides%201.61x%20speedup.%20CGAR-trained%20models%20show%20superior%20inference%20efficiency%20with%20100%25%20halting%20accuracy%20and%2011%25%20fewer%20reasoning%20steps.%20Conclusions%3A%20CGAR%20enables%20efficient%20training%20of%20recursive%20models%20on%20modest%20hardware.%20By%20treating%20depth%20as%20a%20scheduled%20parameter%2C%20it%20achieves%20substantial%20savings%20and%20prevents%20overfitting%2C%20making%20these%20models%20practical%20for%20neurosymbolic%20AI%20and%20program%20synthesis.%20https%3A//github.com/Kaleemullahqasim/CGAR%20and%20huggingface.co/Kaleemullah/trm-cgar-sudoku.&entry.1838667208=http%3A//arxiv.org/abs/2511.08653v3&entry.124074799=Read"},
{"title": "Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing", "author": "Wesley S. Leite and Rodrigo C. de Lamare and Yuriy Zakharov and Wei Liu and Martin Haardt", "abstract": "In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspaces, while preserving the signal subspace span. We also derive the bounds that guarantees identifiability, by limiting the values that can be assumed by the compression parameter. Simulations with sparse geometries reveal significant performance improvements and complexity savings relative to the fixed-window coarray MUSIC method.", "link": "http://arxiv.org/abs/2512.22024v1", "date": "2025-12-26", "relevancy": 2.0477, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4282}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direction%20Finding%20with%20Sparse%20Arrays%20Based%20on%20Variable%20Window%20Size%20Spatial%20Smoothing&body=Title%3A%20Direction%20Finding%20with%20Sparse%20Arrays%20Based%20on%20Variable%20Window%20Size%20Spatial%20Smoothing%0AAuthor%3A%20Wesley%20S.%20Leite%20and%20Rodrigo%20C.%20de%20Lamare%20and%20Yuriy%20Zakharov%20and%20Wei%20Liu%20and%20Martin%20Haardt%0AAbstract%3A%20In%20this%20work%2C%20we%20introduce%20a%20variable%20window%20size%20%28VWS%29%20spatial%20smoothing%20framework%20that%20enhances%20coarray-based%20direction%20of%20arrival%20%28DOA%29%20estimation%20for%20sparse%20linear%20arrays.%20By%20compressing%20the%20smoothing%20aperture%2C%20the%20proposed%20VWS%20Coarray%20MUSIC%20%28VWS-CA-MUSIC%29%20and%20VWS%20Coarray%20root-MUSIC%20%28VWS-CA-rMUSIC%29%20algorithms%20replace%20part%20of%20the%20perturbed%20rank-one%20outer%20products%20in%20the%20smoothed%20coarray%20data%20with%20unperturbed%20low-rank%20additional%20terms%2C%20increasing%20the%20separation%20between%20signal%20and%20noise%20subspaces%2C%20while%20preserving%20the%20signal%20subspace%20span.%20We%20also%20derive%20the%20bounds%20that%20guarantees%20identifiability%2C%20by%20limiting%20the%20values%20that%20can%20be%20assumed%20by%20the%20compression%20parameter.%20Simulations%20with%20sparse%20geometries%20reveal%20significant%20performance%20improvements%20and%20complexity%20savings%20relative%20to%20the%20fixed-window%20coarray%20MUSIC%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirection%2520Finding%2520with%2520Sparse%2520Arrays%2520Based%2520on%2520Variable%2520Window%2520Size%2520Spatial%2520Smoothing%26entry.906535625%3DWesley%2520S.%2520Leite%2520and%2520Rodrigo%2520C.%2520de%2520Lamare%2520and%2520Yuriy%2520Zakharov%2520and%2520Wei%2520Liu%2520and%2520Martin%2520Haardt%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520variable%2520window%2520size%2520%2528VWS%2529%2520spatial%2520smoothing%2520framework%2520that%2520enhances%2520coarray-based%2520direction%2520of%2520arrival%2520%2528DOA%2529%2520estimation%2520for%2520sparse%2520linear%2520arrays.%2520By%2520compressing%2520the%2520smoothing%2520aperture%252C%2520the%2520proposed%2520VWS%2520Coarray%2520MUSIC%2520%2528VWS-CA-MUSIC%2529%2520and%2520VWS%2520Coarray%2520root-MUSIC%2520%2528VWS-CA-rMUSIC%2529%2520algorithms%2520replace%2520part%2520of%2520the%2520perturbed%2520rank-one%2520outer%2520products%2520in%2520the%2520smoothed%2520coarray%2520data%2520with%2520unperturbed%2520low-rank%2520additional%2520terms%252C%2520increasing%2520the%2520separation%2520between%2520signal%2520and%2520noise%2520subspaces%252C%2520while%2520preserving%2520the%2520signal%2520subspace%2520span.%2520We%2520also%2520derive%2520the%2520bounds%2520that%2520guarantees%2520identifiability%252C%2520by%2520limiting%2520the%2520values%2520that%2520can%2520be%2520assumed%2520by%2520the%2520compression%2520parameter.%2520Simulations%2520with%2520sparse%2520geometries%2520reveal%2520significant%2520performance%2520improvements%2520and%2520complexity%2520savings%2520relative%2520to%2520the%2520fixed-window%2520coarray%2520MUSIC%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direction%20Finding%20with%20Sparse%20Arrays%20Based%20on%20Variable%20Window%20Size%20Spatial%20Smoothing&entry.906535625=Wesley%20S.%20Leite%20and%20Rodrigo%20C.%20de%20Lamare%20and%20Yuriy%20Zakharov%20and%20Wei%20Liu%20and%20Martin%20Haardt&entry.1292438233=In%20this%20work%2C%20we%20introduce%20a%20variable%20window%20size%20%28VWS%29%20spatial%20smoothing%20framework%20that%20enhances%20coarray-based%20direction%20of%20arrival%20%28DOA%29%20estimation%20for%20sparse%20linear%20arrays.%20By%20compressing%20the%20smoothing%20aperture%2C%20the%20proposed%20VWS%20Coarray%20MUSIC%20%28VWS-CA-MUSIC%29%20and%20VWS%20Coarray%20root-MUSIC%20%28VWS-CA-rMUSIC%29%20algorithms%20replace%20part%20of%20the%20perturbed%20rank-one%20outer%20products%20in%20the%20smoothed%20coarray%20data%20with%20unperturbed%20low-rank%20additional%20terms%2C%20increasing%20the%20separation%20between%20signal%20and%20noise%20subspaces%2C%20while%20preserving%20the%20signal%20subspace%20span.%20We%20also%20derive%20the%20bounds%20that%20guarantees%20identifiability%2C%20by%20limiting%20the%20values%20that%20can%20be%20assumed%20by%20the%20compression%20parameter.%20Simulations%20with%20sparse%20geometries%20reveal%20significant%20performance%20improvements%20and%20complexity%20savings%20relative%20to%20the%20fixed-window%20coarray%20MUSIC%20method.&entry.1838667208=http%3A//arxiv.org/abs/2512.22024v1&entry.124074799=Read"},
{"title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models", "author": "Zongmin Zhang and Zhen Sun and Yifan Liao and Wenhan Dong and Xinlei He and Xingshuo Han and Shengmin Xu and Xinyi Huang", "abstract": "Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.", "link": "http://arxiv.org/abs/2512.22046v1", "date": "2025-12-26", "relevancy": 2.0372, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5103}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Attacks%20on%20Prompt-Driven%20Video%20Segmentation%20Foundation%20Models&body=Title%3A%20Backdoor%20Attacks%20on%20Prompt-Driven%20Video%20Segmentation%20Foundation%20Models%0AAuthor%3A%20Zongmin%20Zhang%20and%20Zhen%20Sun%20and%20Yifan%20Liao%20and%20Wenhan%20Dong%20and%20Xinlei%20He%20and%20Xingshuo%20Han%20and%20Shengmin%20Xu%20and%20Xinyi%20Huang%0AAbstract%3A%20Prompt-driven%20Video%20Segmentation%20Foundation%20Models%20%28VSFMs%29%20such%20as%20SAM2%20are%20increasingly%20deployed%20in%20applications%20like%20autonomous%20driving%20and%20digital%20pathology%2C%20raising%20concerns%20about%20backdoor%20threats.%20Surprisingly%2C%20we%20find%20that%20directly%20transferring%20classic%20backdoor%20attacks%20%28e.g.%2C%20BadNet%29%20to%20VSFMs%20is%20almost%20ineffective%2C%20with%20ASR%20below%205%5C%25.%20To%20understand%20this%2C%20we%20study%20encoder%20gradients%20and%20attention%20maps%20and%20observe%20that%20conventional%20training%20keeps%20gradients%20for%20clean%20and%20triggered%20samples%20largely%20aligned%2C%20while%20attention%20still%20focuses%20on%20the%20true%20object%2C%20preventing%20the%20encoder%20from%20learning%20a%20distinct%20trigger-related%20representation.%20To%20address%20this%20challenge%2C%20we%20propose%20BadVSFM%2C%20the%20first%20backdoor%20framework%20tailored%20to%20prompt-driven%20VSFMs.%20BadVSFM%20uses%20a%20two-stage%20strategy%3A%20%281%29%20steer%20the%20image%20encoder%20so%20triggered%20frames%20map%20to%20a%20designated%20target%20embedding%20while%20clean%20frames%20remain%20aligned%20with%20a%20clean%20reference%20encoder%3B%20%282%29%20train%20the%20mask%20decoder%20so%20that%2C%20across%20prompt%20types%2C%20triggered%20frame-prompt%20pairs%20produce%20a%20shared%20target%20mask%2C%20while%20clean%20outputs%20stay%20close%20to%20a%20reference%20decoder.%20Extensive%20experiments%20on%20two%20datasets%20and%20five%20VSFMs%20show%20that%20BadVSFM%20achieves%20strong%2C%20controllable%20backdoor%20effects%20under%20diverse%20triggers%20and%20prompts%20while%20preserving%20clean%20segmentation%20quality.%20Ablations%20over%20losses%2C%20stages%2C%20targets%2C%20trigger%20settings%2C%20and%20poisoning%20rates%20demonstrate%20robustness%20to%20reasonable%20hyperparameter%20changes%20and%20confirm%20the%20necessity%20of%20the%20two-stage%20design.%20Finally%2C%20gradient-conflict%20analysis%20and%20attention%20visualizations%20show%20that%20BadVSFM%20separates%20triggered%20and%20clean%20representations%20and%20shifts%20attention%20to%20trigger%20regions%2C%20while%20four%20representative%20defenses%20remain%20largely%20ineffective%2C%20revealing%20an%20underexplored%20vulnerability%20in%20current%20VSFMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Attacks%2520on%2520Prompt-Driven%2520Video%2520Segmentation%2520Foundation%2520Models%26entry.906535625%3DZongmin%2520Zhang%2520and%2520Zhen%2520Sun%2520and%2520Yifan%2520Liao%2520and%2520Wenhan%2520Dong%2520and%2520Xinlei%2520He%2520and%2520Xingshuo%2520Han%2520and%2520Shengmin%2520Xu%2520and%2520Xinyi%2520Huang%26entry.1292438233%3DPrompt-driven%2520Video%2520Segmentation%2520Foundation%2520Models%2520%2528VSFMs%2529%2520such%2520as%2520SAM2%2520are%2520increasingly%2520deployed%2520in%2520applications%2520like%2520autonomous%2520driving%2520and%2520digital%2520pathology%252C%2520raising%2520concerns%2520about%2520backdoor%2520threats.%2520Surprisingly%252C%2520we%2520find%2520that%2520directly%2520transferring%2520classic%2520backdoor%2520attacks%2520%2528e.g.%252C%2520BadNet%2529%2520to%2520VSFMs%2520is%2520almost%2520ineffective%252C%2520with%2520ASR%2520below%25205%255C%2525.%2520To%2520understand%2520this%252C%2520we%2520study%2520encoder%2520gradients%2520and%2520attention%2520maps%2520and%2520observe%2520that%2520conventional%2520training%2520keeps%2520gradients%2520for%2520clean%2520and%2520triggered%2520samples%2520largely%2520aligned%252C%2520while%2520attention%2520still%2520focuses%2520on%2520the%2520true%2520object%252C%2520preventing%2520the%2520encoder%2520from%2520learning%2520a%2520distinct%2520trigger-related%2520representation.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520BadVSFM%252C%2520the%2520first%2520backdoor%2520framework%2520tailored%2520to%2520prompt-driven%2520VSFMs.%2520BadVSFM%2520uses%2520a%2520two-stage%2520strategy%253A%2520%25281%2529%2520steer%2520the%2520image%2520encoder%2520so%2520triggered%2520frames%2520map%2520to%2520a%2520designated%2520target%2520embedding%2520while%2520clean%2520frames%2520remain%2520aligned%2520with%2520a%2520clean%2520reference%2520encoder%253B%2520%25282%2529%2520train%2520the%2520mask%2520decoder%2520so%2520that%252C%2520across%2520prompt%2520types%252C%2520triggered%2520frame-prompt%2520pairs%2520produce%2520a%2520shared%2520target%2520mask%252C%2520while%2520clean%2520outputs%2520stay%2520close%2520to%2520a%2520reference%2520decoder.%2520Extensive%2520experiments%2520on%2520two%2520datasets%2520and%2520five%2520VSFMs%2520show%2520that%2520BadVSFM%2520achieves%2520strong%252C%2520controllable%2520backdoor%2520effects%2520under%2520diverse%2520triggers%2520and%2520prompts%2520while%2520preserving%2520clean%2520segmentation%2520quality.%2520Ablations%2520over%2520losses%252C%2520stages%252C%2520targets%252C%2520trigger%2520settings%252C%2520and%2520poisoning%2520rates%2520demonstrate%2520robustness%2520to%2520reasonable%2520hyperparameter%2520changes%2520and%2520confirm%2520the%2520necessity%2520of%2520the%2520two-stage%2520design.%2520Finally%252C%2520gradient-conflict%2520analysis%2520and%2520attention%2520visualizations%2520show%2520that%2520BadVSFM%2520separates%2520triggered%2520and%2520clean%2520representations%2520and%2520shifts%2520attention%2520to%2520trigger%2520regions%252C%2520while%2520four%2520representative%2520defenses%2520remain%2520largely%2520ineffective%252C%2520revealing%2520an%2520underexplored%2520vulnerability%2520in%2520current%2520VSFMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Attacks%20on%20Prompt-Driven%20Video%20Segmentation%20Foundation%20Models&entry.906535625=Zongmin%20Zhang%20and%20Zhen%20Sun%20and%20Yifan%20Liao%20and%20Wenhan%20Dong%20and%20Xinlei%20He%20and%20Xingshuo%20Han%20and%20Shengmin%20Xu%20and%20Xinyi%20Huang&entry.1292438233=Prompt-driven%20Video%20Segmentation%20Foundation%20Models%20%28VSFMs%29%20such%20as%20SAM2%20are%20increasingly%20deployed%20in%20applications%20like%20autonomous%20driving%20and%20digital%20pathology%2C%20raising%20concerns%20about%20backdoor%20threats.%20Surprisingly%2C%20we%20find%20that%20directly%20transferring%20classic%20backdoor%20attacks%20%28e.g.%2C%20BadNet%29%20to%20VSFMs%20is%20almost%20ineffective%2C%20with%20ASR%20below%205%5C%25.%20To%20understand%20this%2C%20we%20study%20encoder%20gradients%20and%20attention%20maps%20and%20observe%20that%20conventional%20training%20keeps%20gradients%20for%20clean%20and%20triggered%20samples%20largely%20aligned%2C%20while%20attention%20still%20focuses%20on%20the%20true%20object%2C%20preventing%20the%20encoder%20from%20learning%20a%20distinct%20trigger-related%20representation.%20To%20address%20this%20challenge%2C%20we%20propose%20BadVSFM%2C%20the%20first%20backdoor%20framework%20tailored%20to%20prompt-driven%20VSFMs.%20BadVSFM%20uses%20a%20two-stage%20strategy%3A%20%281%29%20steer%20the%20image%20encoder%20so%20triggered%20frames%20map%20to%20a%20designated%20target%20embedding%20while%20clean%20frames%20remain%20aligned%20with%20a%20clean%20reference%20encoder%3B%20%282%29%20train%20the%20mask%20decoder%20so%20that%2C%20across%20prompt%20types%2C%20triggered%20frame-prompt%20pairs%20produce%20a%20shared%20target%20mask%2C%20while%20clean%20outputs%20stay%20close%20to%20a%20reference%20decoder.%20Extensive%20experiments%20on%20two%20datasets%20and%20five%20VSFMs%20show%20that%20BadVSFM%20achieves%20strong%2C%20controllable%20backdoor%20effects%20under%20diverse%20triggers%20and%20prompts%20while%20preserving%20clean%20segmentation%20quality.%20Ablations%20over%20losses%2C%20stages%2C%20targets%2C%20trigger%20settings%2C%20and%20poisoning%20rates%20demonstrate%20robustness%20to%20reasonable%20hyperparameter%20changes%20and%20confirm%20the%20necessity%20of%20the%20two-stage%20design.%20Finally%2C%20gradient-conflict%20analysis%20and%20attention%20visualizations%20show%20that%20BadVSFM%20separates%20triggered%20and%20clean%20representations%20and%20shifts%20attention%20to%20trigger%20regions%2C%20while%20four%20representative%20defenses%20remain%20largely%20ineffective%2C%20revealing%20an%20underexplored%20vulnerability%20in%20current%20VSFMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.22046v1&entry.124074799=Read"},
{"title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model", "author": "Zhiyi Duan and Xiangren Wang and Hongyu Yuan and Qianli Xing", "abstract": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression. In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED. To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process. The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information. Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA. AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.", "link": "http://arxiv.org/abs/2512.20548v2", "date": "2025-12-26", "relevancy": 2.0355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4991}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model&body=Title%3A%20Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model%0AAuthor%3A%20Zhiyi%20Duan%20and%20Xiangren%20Wang%20and%20Hongyu%20Yuan%20and%20Qianli%20Xing%0AAbstract%3A%20Teachers%27%20emotional%20states%20are%20critical%20in%20educational%20scenarios%2C%20profoundly%20impacting%20teaching%20efficacy%2C%20student%20engagement%2C%20and%20learning%20achievements.%20However%2C%20existing%20studies%20often%20fail%20to%20accurately%20capture%20teachers%27%20emotions%20due%20to%20the%20performative%20nature%20and%20overlook%20the%20critical%20impact%20of%20instructional%20information%20on%20emotional%20expression.%20In%20this%20paper%2C%20we%20systematically%20investigate%20teacher%20sentiment%20analysis%20by%20building%20both%20the%20dataset%20and%20the%20model%20accordingly.%20We%20construct%20the%20first%20large-scale%20teacher%20multimodal%20sentiment%20analysis%20dataset%2C%20T-MED.%20To%20ensure%20labeling%20accuracy%20and%20efficiency%2C%20we%20employ%20a%20human-machine%20collaborative%20labeling%20process.%20The%20T-MED%20dataset%20includes%2014%2C938%20instances%20of%20teacher%20emotional%20data%20from%20250%20real%20classrooms%20across%2011%20subjects%20ranging%20from%20K-12%20to%20higher%20education%2C%20integrating%20multimodal%20text%2C%20audio%2C%20video%2C%20and%20instructional%20information.%20Furthermore%2C%20we%20propose%20a%20novel%20asymmetric%20attention-based%20multimodal%20teacher%20sentiment%20analysis%20model%2C%20AAM-TSA.%20AAM-TSA%20introduces%20an%20asymmetric%20attention%20mechanism%20and%20hierarchical%20gating%20unit%20to%20enable%20differentiated%20cross-modal%20feature%20fusion%20and%20precise%20emotional%20classification.%20Experimental%20results%20demonstrate%20that%20AAM-TSA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%20and%20interpretability%20on%20the%20T-MED%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20548v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multimodal%2520Teacher%2520Sentiment%2520Analysis%253AThe%2520Large-Scale%2520T-MED%2520Dataset%2520%2526%2520The%2520Effective%2520AAM-TSA%2520Model%26entry.906535625%3DZhiyi%2520Duan%2520and%2520Xiangren%2520Wang%2520and%2520Hongyu%2520Yuan%2520and%2520Qianli%2520Xing%26entry.1292438233%3DTeachers%2527%2520emotional%2520states%2520are%2520critical%2520in%2520educational%2520scenarios%252C%2520profoundly%2520impacting%2520teaching%2520efficacy%252C%2520student%2520engagement%252C%2520and%2520learning%2520achievements.%2520However%252C%2520existing%2520studies%2520often%2520fail%2520to%2520accurately%2520capture%2520teachers%2527%2520emotions%2520due%2520to%2520the%2520performative%2520nature%2520and%2520overlook%2520the%2520critical%2520impact%2520of%2520instructional%2520information%2520on%2520emotional%2520expression.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520teacher%2520sentiment%2520analysis%2520by%2520building%2520both%2520the%2520dataset%2520and%2520the%2520model%2520accordingly.%2520We%2520construct%2520the%2520first%2520large-scale%2520teacher%2520multimodal%2520sentiment%2520analysis%2520dataset%252C%2520T-MED.%2520To%2520ensure%2520labeling%2520accuracy%2520and%2520efficiency%252C%2520we%2520employ%2520a%2520human-machine%2520collaborative%2520labeling%2520process.%2520The%2520T-MED%2520dataset%2520includes%252014%252C938%2520instances%2520of%2520teacher%2520emotional%2520data%2520from%2520250%2520real%2520classrooms%2520across%252011%2520subjects%2520ranging%2520from%2520K-12%2520to%2520higher%2520education%252C%2520integrating%2520multimodal%2520text%252C%2520audio%252C%2520video%252C%2520and%2520instructional%2520information.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520asymmetric%2520attention-based%2520multimodal%2520teacher%2520sentiment%2520analysis%2520model%252C%2520AAM-TSA.%2520AAM-TSA%2520introduces%2520an%2520asymmetric%2520attention%2520mechanism%2520and%2520hierarchical%2520gating%2520unit%2520to%2520enable%2520differentiated%2520cross-modal%2520feature%2520fusion%2520and%2520precise%2520emotional%2520classification.%2520Experimental%2520results%2520demonstrate%2520that%2520AAM-TSA%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520accuracy%2520and%2520interpretability%2520on%2520the%2520T-MED%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20548v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model&entry.906535625=Zhiyi%20Duan%20and%20Xiangren%20Wang%20and%20Hongyu%20Yuan%20and%20Qianli%20Xing&entry.1292438233=Teachers%27%20emotional%20states%20are%20critical%20in%20educational%20scenarios%2C%20profoundly%20impacting%20teaching%20efficacy%2C%20student%20engagement%2C%20and%20learning%20achievements.%20However%2C%20existing%20studies%20often%20fail%20to%20accurately%20capture%20teachers%27%20emotions%20due%20to%20the%20performative%20nature%20and%20overlook%20the%20critical%20impact%20of%20instructional%20information%20on%20emotional%20expression.%20In%20this%20paper%2C%20we%20systematically%20investigate%20teacher%20sentiment%20analysis%20by%20building%20both%20the%20dataset%20and%20the%20model%20accordingly.%20We%20construct%20the%20first%20large-scale%20teacher%20multimodal%20sentiment%20analysis%20dataset%2C%20T-MED.%20To%20ensure%20labeling%20accuracy%20and%20efficiency%2C%20we%20employ%20a%20human-machine%20collaborative%20labeling%20process.%20The%20T-MED%20dataset%20includes%2014%2C938%20instances%20of%20teacher%20emotional%20data%20from%20250%20real%20classrooms%20across%2011%20subjects%20ranging%20from%20K-12%20to%20higher%20education%2C%20integrating%20multimodal%20text%2C%20audio%2C%20video%2C%20and%20instructional%20information.%20Furthermore%2C%20we%20propose%20a%20novel%20asymmetric%20attention-based%20multimodal%20teacher%20sentiment%20analysis%20model%2C%20AAM-TSA.%20AAM-TSA%20introduces%20an%20asymmetric%20attention%20mechanism%20and%20hierarchical%20gating%20unit%20to%20enable%20differentiated%20cross-modal%20feature%20fusion%20and%20precise%20emotional%20classification.%20Experimental%20results%20demonstrate%20that%20AAM-TSA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%20and%20interpretability%20on%20the%20T-MED%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2512.20548v2&entry.124074799=Read"},
{"title": "Universal Reasoning Model", "author": "Zitian Gao and Lynx Chen and Yihao Xiao and He Xing and Ran Tao and Haoming Luo and Joey Zhou and Bryan Dai", "abstract": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/UbiquantAI/URM.", "link": "http://arxiv.org/abs/2512.14693v3", "date": "2025-12-26", "relevancy": 2.0317, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5366}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4924}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Reasoning%20Model&body=Title%3A%20Universal%20Reasoning%20Model%0AAuthor%3A%20Zitian%20Gao%20and%20Lynx%20Chen%20and%20Yihao%20Xiao%20and%20He%20Xing%20and%20Ran%20Tao%20and%20Haoming%20Luo%20and%20Joey%20Zhou%20and%20Bryan%20Dai%0AAbstract%3A%20Universal%20transformers%20%28UTs%29%20have%20been%20widely%20used%20for%20complex%20reasoning%20tasks%20such%20as%20ARC-AGI%20and%20Sudoku%2C%20yet%20the%20specific%20sources%20of%20their%20performance%20gains%20remain%20underexplored.%20In%20this%20work%2C%20we%20systematically%20analyze%20UTs%20variants%20and%20show%20that%20improvements%20on%20ARC-AGI%20primarily%20arise%20from%20the%20recurrent%20inductive%20bias%20and%20strong%20nonlinear%20components%20of%20Transformer%2C%20rather%20than%20from%20elaborate%20architectural%20designs.%20Motivated%20by%20this%20finding%2C%20we%20propose%20the%20Universal%20Reasoning%20Model%20%28URM%29%2C%20which%20enhances%20the%20UT%20with%20short%20convolution%20and%20truncated%20backpropagation.%20Our%20approach%20substantially%20improves%20reasoning%20performance%2C%20achieving%20state-of-the-art%2053.8%25%20pass%401%20on%20ARC-AGI%201%20and%2016.0%25%20pass%401%20on%20ARC-AGI%202.%20Our%20code%20is%20avaliable%20at%20https%3A//github.com/UbiquantAI/URM.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14693v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Reasoning%2520Model%26entry.906535625%3DZitian%2520Gao%2520and%2520Lynx%2520Chen%2520and%2520Yihao%2520Xiao%2520and%2520He%2520Xing%2520and%2520Ran%2520Tao%2520and%2520Haoming%2520Luo%2520and%2520Joey%2520Zhou%2520and%2520Bryan%2520Dai%26entry.1292438233%3DUniversal%2520transformers%2520%2528UTs%2529%2520have%2520been%2520widely%2520used%2520for%2520complex%2520reasoning%2520tasks%2520such%2520as%2520ARC-AGI%2520and%2520Sudoku%252C%2520yet%2520the%2520specific%2520sources%2520of%2520their%2520performance%2520gains%2520remain%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520systematically%2520analyze%2520UTs%2520variants%2520and%2520show%2520that%2520improvements%2520on%2520ARC-AGI%2520primarily%2520arise%2520from%2520the%2520recurrent%2520inductive%2520bias%2520and%2520strong%2520nonlinear%2520components%2520of%2520Transformer%252C%2520rather%2520than%2520from%2520elaborate%2520architectural%2520designs.%2520Motivated%2520by%2520this%2520finding%252C%2520we%2520propose%2520the%2520Universal%2520Reasoning%2520Model%2520%2528URM%2529%252C%2520which%2520enhances%2520the%2520UT%2520with%2520short%2520convolution%2520and%2520truncated%2520backpropagation.%2520Our%2520approach%2520substantially%2520improves%2520reasoning%2520performance%252C%2520achieving%2520state-of-the-art%252053.8%2525%2520pass%25401%2520on%2520ARC-AGI%25201%2520and%252016.0%2525%2520pass%25401%2520on%2520ARC-AGI%25202.%2520Our%2520code%2520is%2520avaliable%2520at%2520https%253A//github.com/UbiquantAI/URM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14693v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Reasoning%20Model&entry.906535625=Zitian%20Gao%20and%20Lynx%20Chen%20and%20Yihao%20Xiao%20and%20He%20Xing%20and%20Ran%20Tao%20and%20Haoming%20Luo%20and%20Joey%20Zhou%20and%20Bryan%20Dai&entry.1292438233=Universal%20transformers%20%28UTs%29%20have%20been%20widely%20used%20for%20complex%20reasoning%20tasks%20such%20as%20ARC-AGI%20and%20Sudoku%2C%20yet%20the%20specific%20sources%20of%20their%20performance%20gains%20remain%20underexplored.%20In%20this%20work%2C%20we%20systematically%20analyze%20UTs%20variants%20and%20show%20that%20improvements%20on%20ARC-AGI%20primarily%20arise%20from%20the%20recurrent%20inductive%20bias%20and%20strong%20nonlinear%20components%20of%20Transformer%2C%20rather%20than%20from%20elaborate%20architectural%20designs.%20Motivated%20by%20this%20finding%2C%20we%20propose%20the%20Universal%20Reasoning%20Model%20%28URM%29%2C%20which%20enhances%20the%20UT%20with%20short%20convolution%20and%20truncated%20backpropagation.%20Our%20approach%20substantially%20improves%20reasoning%20performance%2C%20achieving%20state-of-the-art%2053.8%25%20pass%401%20on%20ARC-AGI%201%20and%2016.0%25%20pass%401%20on%20ARC-AGI%202.%20Our%20code%20is%20avaliable%20at%20https%3A//github.com/UbiquantAI/URM.&entry.1838667208=http%3A//arxiv.org/abs/2512.14693v3&entry.124074799=Read"},
{"title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law", "author": "Chiwun Yang", "abstract": "The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.\n  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $\u0398(\\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.", "link": "http://arxiv.org/abs/2512.22088v1", "date": "2025-12-26", "relevancy": 2.0136, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6068}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4883}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Learning%20Dynamics%20and%20Generalization%20in%20Transformers%20Scaling%20Law&body=Title%3A%20Unifying%20Learning%20Dynamics%20and%20Generalization%20in%20Transformers%20Scaling%20Law%0AAuthor%3A%20Chiwun%20Yang%0AAbstract%3A%20The%20scaling%20law%2C%20a%20cornerstone%20of%20Large%20Language%20Model%20%28LLM%29%20development%2C%20predicts%20improvements%20in%20model%20performance%20with%20increasing%20computational%20resources.%20Yet%2C%20while%20empirically%20validated%2C%20its%20theoretical%20underpinnings%20remain%20poorly%20understood.%20This%20work%20formalizes%20the%20learning%20dynamics%20of%20transformer-based%20language%20models%20as%20an%20ordinary%20differential%20equation%20%28ODE%29%20system%2C%20then%20approximates%20this%20process%20to%20kernel%20behaviors.%20Departing%20from%20prior%20toy-model%20analyses%2C%20we%20rigorously%20analyze%20stochastic%20gradient%20descent%20%28SGD%29%20training%20for%20multi-layer%20transformers%20on%20sequence-to-sequence%20data%20with%20arbitrary%20data%20distribution%2C%20closely%20mirroring%20real-world%20conditions.%20Our%20analysis%20characterizes%20the%20convergence%20of%20generalization%20error%20to%20the%20irreducible%20risk%20as%20computational%20resources%20scale%20with%20data%2C%20especially%20during%20the%20optimization%20process.%0A%20%20We%20establish%20a%20theoretical%20upper%20bound%20on%20excess%20risk%20characterized%20by%20a%20distinct%20phase%20transition.%20In%20the%20initial%20optimization%20phase%2C%20the%20excess%20risk%20decays%20exponentially%20relative%20to%20the%20computational%20cost%20%24%7B%5Csf%20C%7D%24.%20However%2C%20once%20a%20specific%20resource%20allocation%20threshold%20is%20crossed%2C%20the%20system%20enters%20a%20statistical%20phase%2C%20where%20the%20generalization%20error%20follows%20a%20power-law%20decay%20of%20%24%CE%98%28%5Cmathsf%7BC%7D%5E%7B-1/6%7D%29%24.%20Beyond%20this%20unified%20framework%2C%20our%20theory%20derives%20isolated%20scaling%20laws%20for%20model%20size%2C%20training%20time%2C%20and%20dataset%20size%2C%20elucidating%20how%20each%20variable%20independently%20governs%20the%20upper%20bounds%20of%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Learning%2520Dynamics%2520and%2520Generalization%2520in%2520Transformers%2520Scaling%2520Law%26entry.906535625%3DChiwun%2520Yang%26entry.1292438233%3DThe%2520scaling%2520law%252C%2520a%2520cornerstone%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520development%252C%2520predicts%2520improvements%2520in%2520model%2520performance%2520with%2520increasing%2520computational%2520resources.%2520Yet%252C%2520while%2520empirically%2520validated%252C%2520its%2520theoretical%2520underpinnings%2520remain%2520poorly%2520understood.%2520This%2520work%2520formalizes%2520the%2520learning%2520dynamics%2520of%2520transformer-based%2520language%2520models%2520as%2520an%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%2520system%252C%2520then%2520approximates%2520this%2520process%2520to%2520kernel%2520behaviors.%2520Departing%2520from%2520prior%2520toy-model%2520analyses%252C%2520we%2520rigorously%2520analyze%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520training%2520for%2520multi-layer%2520transformers%2520on%2520sequence-to-sequence%2520data%2520with%2520arbitrary%2520data%2520distribution%252C%2520closely%2520mirroring%2520real-world%2520conditions.%2520Our%2520analysis%2520characterizes%2520the%2520convergence%2520of%2520generalization%2520error%2520to%2520the%2520irreducible%2520risk%2520as%2520computational%2520resources%2520scale%2520with%2520data%252C%2520especially%2520during%2520the%2520optimization%2520process.%250A%2520%2520We%2520establish%2520a%2520theoretical%2520upper%2520bound%2520on%2520excess%2520risk%2520characterized%2520by%2520a%2520distinct%2520phase%2520transition.%2520In%2520the%2520initial%2520optimization%2520phase%252C%2520the%2520excess%2520risk%2520decays%2520exponentially%2520relative%2520to%2520the%2520computational%2520cost%2520%2524%257B%255Csf%2520C%257D%2524.%2520However%252C%2520once%2520a%2520specific%2520resource%2520allocation%2520threshold%2520is%2520crossed%252C%2520the%2520system%2520enters%2520a%2520statistical%2520phase%252C%2520where%2520the%2520generalization%2520error%2520follows%2520a%2520power-law%2520decay%2520of%2520%2524%25CE%2598%2528%255Cmathsf%257BC%257D%255E%257B-1/6%257D%2529%2524.%2520Beyond%2520this%2520unified%2520framework%252C%2520our%2520theory%2520derives%2520isolated%2520scaling%2520laws%2520for%2520model%2520size%252C%2520training%2520time%252C%2520and%2520dataset%2520size%252C%2520elucidating%2520how%2520each%2520variable%2520independently%2520governs%2520the%2520upper%2520bounds%2520of%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Learning%20Dynamics%20and%20Generalization%20in%20Transformers%20Scaling%20Law&entry.906535625=Chiwun%20Yang&entry.1292438233=The%20scaling%20law%2C%20a%20cornerstone%20of%20Large%20Language%20Model%20%28LLM%29%20development%2C%20predicts%20improvements%20in%20model%20performance%20with%20increasing%20computational%20resources.%20Yet%2C%20while%20empirically%20validated%2C%20its%20theoretical%20underpinnings%20remain%20poorly%20understood.%20This%20work%20formalizes%20the%20learning%20dynamics%20of%20transformer-based%20language%20models%20as%20an%20ordinary%20differential%20equation%20%28ODE%29%20system%2C%20then%20approximates%20this%20process%20to%20kernel%20behaviors.%20Departing%20from%20prior%20toy-model%20analyses%2C%20we%20rigorously%20analyze%20stochastic%20gradient%20descent%20%28SGD%29%20training%20for%20multi-layer%20transformers%20on%20sequence-to-sequence%20data%20with%20arbitrary%20data%20distribution%2C%20closely%20mirroring%20real-world%20conditions.%20Our%20analysis%20characterizes%20the%20convergence%20of%20generalization%20error%20to%20the%20irreducible%20risk%20as%20computational%20resources%20scale%20with%20data%2C%20especially%20during%20the%20optimization%20process.%0A%20%20We%20establish%20a%20theoretical%20upper%20bound%20on%20excess%20risk%20characterized%20by%20a%20distinct%20phase%20transition.%20In%20the%20initial%20optimization%20phase%2C%20the%20excess%20risk%20decays%20exponentially%20relative%20to%20the%20computational%20cost%20%24%7B%5Csf%20C%7D%24.%20However%2C%20once%20a%20specific%20resource%20allocation%20threshold%20is%20crossed%2C%20the%20system%20enters%20a%20statistical%20phase%2C%20where%20the%20generalization%20error%20follows%20a%20power-law%20decay%20of%20%24%CE%98%28%5Cmathsf%7BC%7D%5E%7B-1/6%7D%29%24.%20Beyond%20this%20unified%20framework%2C%20our%20theory%20derives%20isolated%20scaling%20laws%20for%20model%20size%2C%20training%20time%2C%20and%20dataset%20size%2C%20elucidating%20how%20each%20variable%20independently%20governs%20the%20upper%20bounds%20of%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2512.22088v1&entry.124074799=Read"},
{"title": "When Unsupervised Domain Adaptation meets One-class Anomaly Detection: Addressing the Two-fold Unsupervised Curse by Leveraging Anomaly Scarcity", "author": "Nesryne Mejri and Enjie Ghorbel and Anis Kacem and Pavel Chernakov and Niki Foteinopoulou and Djamila Aouada", "abstract": "This paper introduces the first fully unsupervised domain adaptation (UDA) framework for unsupervised anomaly detection (UAD). The performance of UAD techniques degrades significantly in the presence of a domain shift, difficult to avoid in a real-world setting. While UDA has contributed to solving this issue in binary and multi-class classification, such a strategy is ill-posed in UAD. This might be explained by the unsupervised nature of the two tasks, namely, domain adaptation and anomaly detection. Herein, we first formulate this problem that we call the two-fold unsupervised curse. Then, we propose a pioneering solution to this curse, considered intractable so far, by assuming that anomalies are rare. Specifically, we leverage clustering techniques to identify a dominant cluster in the target feature space. Posed as the normal cluster, the latter is aligned with the source normal features. Concretely, given a one-class source set and an unlabeled target set composed mostly of normal data and some anomalies, we fit the source features within a hypersphere while jointly aligning them with the features of the dominant cluster from the target set. The paper provides extensive experiments and analysis on common adaptation benchmarks for anomaly detection, demonstrating the relevance of both the newly introduced paradigm and the proposed approach. The code will be made publicly available.", "link": "http://arxiv.org/abs/2502.21022v3", "date": "2025-12-26", "relevancy": 1.9765, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5089}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4866}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Unsupervised%20Domain%20Adaptation%20meets%20One-class%20Anomaly%20Detection%3A%20Addressing%20the%20Two-fold%20Unsupervised%20Curse%20by%20Leveraging%20Anomaly%20Scarcity&body=Title%3A%20When%20Unsupervised%20Domain%20Adaptation%20meets%20One-class%20Anomaly%20Detection%3A%20Addressing%20the%20Two-fold%20Unsupervised%20Curse%20by%20Leveraging%20Anomaly%20Scarcity%0AAuthor%3A%20Nesryne%20Mejri%20and%20Enjie%20Ghorbel%20and%20Anis%20Kacem%20and%20Pavel%20Chernakov%20and%20Niki%20Foteinopoulou%20and%20Djamila%20Aouada%0AAbstract%3A%20This%20paper%20introduces%20the%20first%20fully%20unsupervised%20domain%20adaptation%20%28UDA%29%20framework%20for%20unsupervised%20anomaly%20detection%20%28UAD%29.%20The%20performance%20of%20UAD%20techniques%20degrades%20significantly%20in%20the%20presence%20of%20a%20domain%20shift%2C%20difficult%20to%20avoid%20in%20a%20real-world%20setting.%20While%20UDA%20has%20contributed%20to%20solving%20this%20issue%20in%20binary%20and%20multi-class%20classification%2C%20such%20a%20strategy%20is%20ill-posed%20in%20UAD.%20This%20might%20be%20explained%20by%20the%20unsupervised%20nature%20of%20the%20two%20tasks%2C%20namely%2C%20domain%20adaptation%20and%20anomaly%20detection.%20Herein%2C%20we%20first%20formulate%20this%20problem%20that%20we%20call%20the%20two-fold%20unsupervised%20curse.%20Then%2C%20we%20propose%20a%20pioneering%20solution%20to%20this%20curse%2C%20considered%20intractable%20so%20far%2C%20by%20assuming%20that%20anomalies%20are%20rare.%20Specifically%2C%20we%20leverage%20clustering%20techniques%20to%20identify%20a%20dominant%20cluster%20in%20the%20target%20feature%20space.%20Posed%20as%20the%20normal%20cluster%2C%20the%20latter%20is%20aligned%20with%20the%20source%20normal%20features.%20Concretely%2C%20given%20a%20one-class%20source%20set%20and%20an%20unlabeled%20target%20set%20composed%20mostly%20of%20normal%20data%20and%20some%20anomalies%2C%20we%20fit%20the%20source%20features%20within%20a%20hypersphere%20while%20jointly%20aligning%20them%20with%20the%20features%20of%20the%20dominant%20cluster%20from%20the%20target%20set.%20The%20paper%20provides%20extensive%20experiments%20and%20analysis%20on%20common%20adaptation%20benchmarks%20for%20anomaly%20detection%2C%20demonstrating%20the%20relevance%20of%20both%20the%20newly%20introduced%20paradigm%20and%20the%20proposed%20approach.%20The%20code%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2502.21022v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Unsupervised%2520Domain%2520Adaptation%2520meets%2520One-class%2520Anomaly%2520Detection%253A%2520Addressing%2520the%2520Two-fold%2520Unsupervised%2520Curse%2520by%2520Leveraging%2520Anomaly%2520Scarcity%26entry.906535625%3DNesryne%2520Mejri%2520and%2520Enjie%2520Ghorbel%2520and%2520Anis%2520Kacem%2520and%2520Pavel%2520Chernakov%2520and%2520Niki%2520Foteinopoulou%2520and%2520Djamila%2520Aouada%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520first%2520fully%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520framework%2520for%2520unsupervised%2520anomaly%2520detection%2520%2528UAD%2529.%2520The%2520performance%2520of%2520UAD%2520techniques%2520degrades%2520significantly%2520in%2520the%2520presence%2520of%2520a%2520domain%2520shift%252C%2520difficult%2520to%2520avoid%2520in%2520a%2520real-world%2520setting.%2520While%2520UDA%2520has%2520contributed%2520to%2520solving%2520this%2520issue%2520in%2520binary%2520and%2520multi-class%2520classification%252C%2520such%2520a%2520strategy%2520is%2520ill-posed%2520in%2520UAD.%2520This%2520might%2520be%2520explained%2520by%2520the%2520unsupervised%2520nature%2520of%2520the%2520two%2520tasks%252C%2520namely%252C%2520domain%2520adaptation%2520and%2520anomaly%2520detection.%2520Herein%252C%2520we%2520first%2520formulate%2520this%2520problem%2520that%2520we%2520call%2520the%2520two-fold%2520unsupervised%2520curse.%2520Then%252C%2520we%2520propose%2520a%2520pioneering%2520solution%2520to%2520this%2520curse%252C%2520considered%2520intractable%2520so%2520far%252C%2520by%2520assuming%2520that%2520anomalies%2520are%2520rare.%2520Specifically%252C%2520we%2520leverage%2520clustering%2520techniques%2520to%2520identify%2520a%2520dominant%2520cluster%2520in%2520the%2520target%2520feature%2520space.%2520Posed%2520as%2520the%2520normal%2520cluster%252C%2520the%2520latter%2520is%2520aligned%2520with%2520the%2520source%2520normal%2520features.%2520Concretely%252C%2520given%2520a%2520one-class%2520source%2520set%2520and%2520an%2520unlabeled%2520target%2520set%2520composed%2520mostly%2520of%2520normal%2520data%2520and%2520some%2520anomalies%252C%2520we%2520fit%2520the%2520source%2520features%2520within%2520a%2520hypersphere%2520while%2520jointly%2520aligning%2520them%2520with%2520the%2520features%2520of%2520the%2520dominant%2520cluster%2520from%2520the%2520target%2520set.%2520The%2520paper%2520provides%2520extensive%2520experiments%2520and%2520analysis%2520on%2520common%2520adaptation%2520benchmarks%2520for%2520anomaly%2520detection%252C%2520demonstrating%2520the%2520relevance%2520of%2520both%2520the%2520newly%2520introduced%2520paradigm%2520and%2520the%2520proposed%2520approach.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.21022v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Unsupervised%20Domain%20Adaptation%20meets%20One-class%20Anomaly%20Detection%3A%20Addressing%20the%20Two-fold%20Unsupervised%20Curse%20by%20Leveraging%20Anomaly%20Scarcity&entry.906535625=Nesryne%20Mejri%20and%20Enjie%20Ghorbel%20and%20Anis%20Kacem%20and%20Pavel%20Chernakov%20and%20Niki%20Foteinopoulou%20and%20Djamila%20Aouada&entry.1292438233=This%20paper%20introduces%20the%20first%20fully%20unsupervised%20domain%20adaptation%20%28UDA%29%20framework%20for%20unsupervised%20anomaly%20detection%20%28UAD%29.%20The%20performance%20of%20UAD%20techniques%20degrades%20significantly%20in%20the%20presence%20of%20a%20domain%20shift%2C%20difficult%20to%20avoid%20in%20a%20real-world%20setting.%20While%20UDA%20has%20contributed%20to%20solving%20this%20issue%20in%20binary%20and%20multi-class%20classification%2C%20such%20a%20strategy%20is%20ill-posed%20in%20UAD.%20This%20might%20be%20explained%20by%20the%20unsupervised%20nature%20of%20the%20two%20tasks%2C%20namely%2C%20domain%20adaptation%20and%20anomaly%20detection.%20Herein%2C%20we%20first%20formulate%20this%20problem%20that%20we%20call%20the%20two-fold%20unsupervised%20curse.%20Then%2C%20we%20propose%20a%20pioneering%20solution%20to%20this%20curse%2C%20considered%20intractable%20so%20far%2C%20by%20assuming%20that%20anomalies%20are%20rare.%20Specifically%2C%20we%20leverage%20clustering%20techniques%20to%20identify%20a%20dominant%20cluster%20in%20the%20target%20feature%20space.%20Posed%20as%20the%20normal%20cluster%2C%20the%20latter%20is%20aligned%20with%20the%20source%20normal%20features.%20Concretely%2C%20given%20a%20one-class%20source%20set%20and%20an%20unlabeled%20target%20set%20composed%20mostly%20of%20normal%20data%20and%20some%20anomalies%2C%20we%20fit%20the%20source%20features%20within%20a%20hypersphere%20while%20jointly%20aligning%20them%20with%20the%20features%20of%20the%20dominant%20cluster%20from%20the%20target%20set.%20The%20paper%20provides%20extensive%20experiments%20and%20analysis%20on%20common%20adaptation%20benchmarks%20for%20anomaly%20detection%2C%20demonstrating%20the%20relevance%20of%20both%20the%20newly%20introduced%20paradigm%20and%20the%20proposed%20approach.%20The%20code%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2502.21022v3&entry.124074799=Read"},
{"title": "An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction", "author": "Yifan Lei and Jiahua Luo and Tingyu Jiang and Bo Zhang and Lifeng Wang and Dapeng Liu and Zhaoren Wu and Haijie Gu and Huan Yu and Jie Jiang", "abstract": "In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.", "link": "http://arxiv.org/abs/2511.22460v2", "date": "2025-12-26", "relevancy": 1.9673, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4809}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Embedding%20Based%20Ad%20Retrieval%20with%20GPU-Powered%20Feature%20Interaction&body=Title%3A%20An%20Efficient%20Embedding%20Based%20Ad%20Retrieval%20with%20GPU-Powered%20Feature%20Interaction%0AAuthor%3A%20Yifan%20Lei%20and%20Jiahua%20Luo%20and%20Tingyu%20Jiang%20and%20Bo%20Zhang%20and%20Lifeng%20Wang%20and%20Dapeng%20Liu%20and%20Zhaoren%20Wu%20and%20Haijie%20Gu%20and%20Huan%20Yu%20and%20Jie%20Jiang%0AAbstract%3A%20In%20large-scale%20advertising%20recommendation%20systems%2C%20retrieval%20serves%20as%20a%20critical%20component%2C%20aiming%20to%20efficiently%20select%20a%20subset%20of%20candidate%20ads%20relevant%20to%20user%20behaviors%20from%20a%20massive%20ad%20inventory%20for%20subsequent%20ranking%20and%20recommendation.%20The%20Embedding-Based%20Retrieval%20%28EBR%29%20methods%20modeled%20by%20the%20dual-tower%20network%20are%20widely%20used%20in%20the%20industry%20to%20maintain%20both%20retrieval%20efficiency%20and%20accuracy.%20However%2C%20the%20dual-tower%20model%20has%20significant%20limitations%3A%20the%20embeddings%20of%20users%20and%20ads%20interact%20only%20at%20the%20final%20inner%20product%20computation%2C%20resulting%20in%20insufficient%20feature%20interaction%20capabilities.%20Although%20DNN-based%20models%20with%20both%20user%20and%20ad%20as%20input%20features%2C%20allowing%20for%20early-stage%20interaction%20between%20these%20features%2C%20are%20introduced%20in%20the%20ranking%20stage%20to%20mitigate%20this%20issue%2C%20they%20are%20computationally%20infeasible%20for%20the%20retrieval%20stage.%20To%20bridge%20this%20gap%2C%20this%20paper%20proposes%20an%20efficient%20GPU-based%20feature%20interaction%20for%20the%20dual-tower%20network%20to%20significantly%20improve%20retrieval%20accuracy%20while%20substantially%20reducing%20computational%20costs.%20Specifically%2C%20we%20introduce%20a%20novel%20compressed%20inverted%20list%20designed%20for%20GPU%20acceleration%2C%20enabling%20efficient%20feature%20interaction%20computation%20at%20scale.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20in%20the%20industry%20to%20successfully%20implement%20Wide%20and%20Deep%20in%20a%20retrieval%20system.%20We%20apply%20this%20model%20to%20the%20real-world%20business%20scenarios%20in%20Tencent%20Advertising%2C%20and%20experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%20offline%20evaluation%20and%20has%20been%20successfully%20deployed%20to%20Tencent%27s%20advertising%20recommendation%20system%2C%20delivering%20significant%20online%20performance%20gains.%20This%20improvement%20not%20only%20validates%20the%20effectiveness%20of%20the%20proposed%20method%2C%20but%20also%20provides%20new%20practical%20guidance%20for%20optimizing%20large-scale%20ad%20retrieval%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Embedding%2520Based%2520Ad%2520Retrieval%2520with%2520GPU-Powered%2520Feature%2520Interaction%26entry.906535625%3DYifan%2520Lei%2520and%2520Jiahua%2520Luo%2520and%2520Tingyu%2520Jiang%2520and%2520Bo%2520Zhang%2520and%2520Lifeng%2520Wang%2520and%2520Dapeng%2520Liu%2520and%2520Zhaoren%2520Wu%2520and%2520Haijie%2520Gu%2520and%2520Huan%2520Yu%2520and%2520Jie%2520Jiang%26entry.1292438233%3DIn%2520large-scale%2520advertising%2520recommendation%2520systems%252C%2520retrieval%2520serves%2520as%2520a%2520critical%2520component%252C%2520aiming%2520to%2520efficiently%2520select%2520a%2520subset%2520of%2520candidate%2520ads%2520relevant%2520to%2520user%2520behaviors%2520from%2520a%2520massive%2520ad%2520inventory%2520for%2520subsequent%2520ranking%2520and%2520recommendation.%2520The%2520Embedding-Based%2520Retrieval%2520%2528EBR%2529%2520methods%2520modeled%2520by%2520the%2520dual-tower%2520network%2520are%2520widely%2520used%2520in%2520the%2520industry%2520to%2520maintain%2520both%2520retrieval%2520efficiency%2520and%2520accuracy.%2520However%252C%2520the%2520dual-tower%2520model%2520has%2520significant%2520limitations%253A%2520the%2520embeddings%2520of%2520users%2520and%2520ads%2520interact%2520only%2520at%2520the%2520final%2520inner%2520product%2520computation%252C%2520resulting%2520in%2520insufficient%2520feature%2520interaction%2520capabilities.%2520Although%2520DNN-based%2520models%2520with%2520both%2520user%2520and%2520ad%2520as%2520input%2520features%252C%2520allowing%2520for%2520early-stage%2520interaction%2520between%2520these%2520features%252C%2520are%2520introduced%2520in%2520the%2520ranking%2520stage%2520to%2520mitigate%2520this%2520issue%252C%2520they%2520are%2520computationally%2520infeasible%2520for%2520the%2520retrieval%2520stage.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520proposes%2520an%2520efficient%2520GPU-based%2520feature%2520interaction%2520for%2520the%2520dual-tower%2520network%2520to%2520significantly%2520improve%2520retrieval%2520accuracy%2520while%2520substantially%2520reducing%2520computational%2520costs.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520compressed%2520inverted%2520list%2520designed%2520for%2520GPU%2520acceleration%252C%2520enabling%2520efficient%2520feature%2520interaction%2520computation%2520at%2520scale.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520framework%2520in%2520the%2520industry%2520to%2520successfully%2520implement%2520Wide%2520and%2520Deep%2520in%2520a%2520retrieval%2520system.%2520We%2520apply%2520this%2520model%2520to%2520the%2520real-world%2520business%2520scenarios%2520in%2520Tencent%2520Advertising%252C%2520and%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520in%2520offline%2520evaluation%2520and%2520has%2520been%2520successfully%2520deployed%2520to%2520Tencent%2527s%2520advertising%2520recommendation%2520system%252C%2520delivering%2520significant%2520online%2520performance%2520gains.%2520This%2520improvement%2520not%2520only%2520validates%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520but%2520also%2520provides%2520new%2520practical%2520guidance%2520for%2520optimizing%2520large-scale%2520ad%2520retrieval%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Embedding%20Based%20Ad%20Retrieval%20with%20GPU-Powered%20Feature%20Interaction&entry.906535625=Yifan%20Lei%20and%20Jiahua%20Luo%20and%20Tingyu%20Jiang%20and%20Bo%20Zhang%20and%20Lifeng%20Wang%20and%20Dapeng%20Liu%20and%20Zhaoren%20Wu%20and%20Haijie%20Gu%20and%20Huan%20Yu%20and%20Jie%20Jiang&entry.1292438233=In%20large-scale%20advertising%20recommendation%20systems%2C%20retrieval%20serves%20as%20a%20critical%20component%2C%20aiming%20to%20efficiently%20select%20a%20subset%20of%20candidate%20ads%20relevant%20to%20user%20behaviors%20from%20a%20massive%20ad%20inventory%20for%20subsequent%20ranking%20and%20recommendation.%20The%20Embedding-Based%20Retrieval%20%28EBR%29%20methods%20modeled%20by%20the%20dual-tower%20network%20are%20widely%20used%20in%20the%20industry%20to%20maintain%20both%20retrieval%20efficiency%20and%20accuracy.%20However%2C%20the%20dual-tower%20model%20has%20significant%20limitations%3A%20the%20embeddings%20of%20users%20and%20ads%20interact%20only%20at%20the%20final%20inner%20product%20computation%2C%20resulting%20in%20insufficient%20feature%20interaction%20capabilities.%20Although%20DNN-based%20models%20with%20both%20user%20and%20ad%20as%20input%20features%2C%20allowing%20for%20early-stage%20interaction%20between%20these%20features%2C%20are%20introduced%20in%20the%20ranking%20stage%20to%20mitigate%20this%20issue%2C%20they%20are%20computationally%20infeasible%20for%20the%20retrieval%20stage.%20To%20bridge%20this%20gap%2C%20this%20paper%20proposes%20an%20efficient%20GPU-based%20feature%20interaction%20for%20the%20dual-tower%20network%20to%20significantly%20improve%20retrieval%20accuracy%20while%20substantially%20reducing%20computational%20costs.%20Specifically%2C%20we%20introduce%20a%20novel%20compressed%20inverted%20list%20designed%20for%20GPU%20acceleration%2C%20enabling%20efficient%20feature%20interaction%20computation%20at%20scale.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20in%20the%20industry%20to%20successfully%20implement%20Wide%20and%20Deep%20in%20a%20retrieval%20system.%20We%20apply%20this%20model%20to%20the%20real-world%20business%20scenarios%20in%20Tencent%20Advertising%2C%20and%20experimental%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%20approaches%20in%20offline%20evaluation%20and%20has%20been%20successfully%20deployed%20to%20Tencent%27s%20advertising%20recommendation%20system%2C%20delivering%20significant%20online%20performance%20gains.%20This%20improvement%20not%20only%20validates%20the%20effectiveness%20of%20the%20proposed%20method%2C%20but%20also%20provides%20new%20practical%20guidance%20for%20optimizing%20large-scale%20ad%20retrieval%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.22460v2&entry.124074799=Read"},
{"title": "APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation", "author": "Ravin Kumar", "abstract": "We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both optimization-efficient and elegant. The proposed neuron follows the functional form $y = \\sum_{i=1}^{n} ((\u03b1_i + \\tanh(\u03b2_i x_i)) \\cdot \u03b3_i x_i) + \u03b4$, where all parameters $\u03b1_i$, $\u03b2_i$, $\u03b3_i$, and $\u03b4$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to $96.69\\%$ test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and training efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it. Source code is available at https://github.com/mr-ravin/aptx_neuron.", "link": "http://arxiv.org/abs/2507.14270v6", "date": "2025-12-26", "relevancy": 1.9672, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5059}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4905}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APTx%20Neuron%3A%20A%20Unified%20Trainable%20Neuron%20Architecture%20Integrating%20Activation%20and%20Computation&body=Title%3A%20APTx%20Neuron%3A%20A%20Unified%20Trainable%20Neuron%20Architecture%20Integrating%20Activation%20and%20Computation%0AAuthor%3A%20Ravin%20Kumar%0AAbstract%3A%20We%20propose%20the%20APTx%20Neuron%2C%20a%20novel%2C%20unified%20neural%20computation%20unit%20that%20integrates%20non-linear%20activation%20and%20linear%20transformation%20into%20a%20single%20trainable%20expression.%20The%20APTx%20Neuron%20is%20derived%20from%20the%20APTx%20activation%20function%2C%20thereby%20eliminating%20the%20need%20for%20separate%20activation%20layers%20and%20making%20the%20architecture%20both%20optimization-efficient%20and%20elegant.%20The%20proposed%20neuron%20follows%20the%20functional%20form%20%24y%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%28%28%CE%B1_i%20%2B%20%5Ctanh%28%CE%B2_i%20x_i%29%29%20%5Ccdot%20%CE%B3_i%20x_i%29%20%2B%20%CE%B4%24%2C%20where%20all%20parameters%20%24%CE%B1_i%24%2C%20%24%CE%B2_i%24%2C%20%24%CE%B3_i%24%2C%20and%20%24%CE%B4%24%20are%20trainable.%20We%20validate%20our%20APTx%20Neuron-based%20architecture%20on%20the%20MNIST%20dataset%2C%20achieving%20up%20to%20%2496.69%5C%25%24%20test%20accuracy%20within%2011%20epochs%20using%20approximately%20332K%20trainable%20parameters.%20The%20results%20highlight%20the%20superior%20expressiveness%20and%20training%20efficiency%20of%20the%20APTx%20Neuron%20compared%20to%20traditional%20neurons%2C%20pointing%20toward%20a%20new%20paradigm%20in%20unified%20neuron%20design%20and%20the%20architectures%20built%20upon%20it.%20Source%20code%20is%20available%20at%20https%3A//github.com/mr-ravin/aptx_neuron.%0ALink%3A%20http%3A//arxiv.org/abs/2507.14270v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPTx%2520Neuron%253A%2520A%2520Unified%2520Trainable%2520Neuron%2520Architecture%2520Integrating%2520Activation%2520and%2520Computation%26entry.906535625%3DRavin%2520Kumar%26entry.1292438233%3DWe%2520propose%2520the%2520APTx%2520Neuron%252C%2520a%2520novel%252C%2520unified%2520neural%2520computation%2520unit%2520that%2520integrates%2520non-linear%2520activation%2520and%2520linear%2520transformation%2520into%2520a%2520single%2520trainable%2520expression.%2520The%2520APTx%2520Neuron%2520is%2520derived%2520from%2520the%2520APTx%2520activation%2520function%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520separate%2520activation%2520layers%2520and%2520making%2520the%2520architecture%2520both%2520optimization-efficient%2520and%2520elegant.%2520The%2520proposed%2520neuron%2520follows%2520the%2520functional%2520form%2520%2524y%2520%253D%2520%255Csum_%257Bi%253D1%257D%255E%257Bn%257D%2520%2528%2528%25CE%25B1_i%2520%252B%2520%255Ctanh%2528%25CE%25B2_i%2520x_i%2529%2529%2520%255Ccdot%2520%25CE%25B3_i%2520x_i%2529%2520%252B%2520%25CE%25B4%2524%252C%2520where%2520all%2520parameters%2520%2524%25CE%25B1_i%2524%252C%2520%2524%25CE%25B2_i%2524%252C%2520%2524%25CE%25B3_i%2524%252C%2520and%2520%2524%25CE%25B4%2524%2520are%2520trainable.%2520We%2520validate%2520our%2520APTx%2520Neuron-based%2520architecture%2520on%2520the%2520MNIST%2520dataset%252C%2520achieving%2520up%2520to%2520%252496.69%255C%2525%2524%2520test%2520accuracy%2520within%252011%2520epochs%2520using%2520approximately%2520332K%2520trainable%2520parameters.%2520The%2520results%2520highlight%2520the%2520superior%2520expressiveness%2520and%2520training%2520efficiency%2520of%2520the%2520APTx%2520Neuron%2520compared%2520to%2520traditional%2520neurons%252C%2520pointing%2520toward%2520a%2520new%2520paradigm%2520in%2520unified%2520neuron%2520design%2520and%2520the%2520architectures%2520built%2520upon%2520it.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//github.com/mr-ravin/aptx_neuron.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14270v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APTx%20Neuron%3A%20A%20Unified%20Trainable%20Neuron%20Architecture%20Integrating%20Activation%20and%20Computation&entry.906535625=Ravin%20Kumar&entry.1292438233=We%20propose%20the%20APTx%20Neuron%2C%20a%20novel%2C%20unified%20neural%20computation%20unit%20that%20integrates%20non-linear%20activation%20and%20linear%20transformation%20into%20a%20single%20trainable%20expression.%20The%20APTx%20Neuron%20is%20derived%20from%20the%20APTx%20activation%20function%2C%20thereby%20eliminating%20the%20need%20for%20separate%20activation%20layers%20and%20making%20the%20architecture%20both%20optimization-efficient%20and%20elegant.%20The%20proposed%20neuron%20follows%20the%20functional%20form%20%24y%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%28%28%CE%B1_i%20%2B%20%5Ctanh%28%CE%B2_i%20x_i%29%29%20%5Ccdot%20%CE%B3_i%20x_i%29%20%2B%20%CE%B4%24%2C%20where%20all%20parameters%20%24%CE%B1_i%24%2C%20%24%CE%B2_i%24%2C%20%24%CE%B3_i%24%2C%20and%20%24%CE%B4%24%20are%20trainable.%20We%20validate%20our%20APTx%20Neuron-based%20architecture%20on%20the%20MNIST%20dataset%2C%20achieving%20up%20to%20%2496.69%5C%25%24%20test%20accuracy%20within%2011%20epochs%20using%20approximately%20332K%20trainable%20parameters.%20The%20results%20highlight%20the%20superior%20expressiveness%20and%20training%20efficiency%20of%20the%20APTx%20Neuron%20compared%20to%20traditional%20neurons%2C%20pointing%20toward%20a%20new%20paradigm%20in%20unified%20neuron%20design%20and%20the%20architectures%20built%20upon%20it.%20Source%20code%20is%20available%20at%20https%3A//github.com/mr-ravin/aptx_neuron.&entry.1838667208=http%3A//arxiv.org/abs/2507.14270v6&entry.124074799=Read"},
{"title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling", "author": "Yuxi Liu and Renjia Deng and Yutong He and Xue Wang and Tao Yao and Kun Yuan", "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.", "link": "http://arxiv.org/abs/2511.00056v2", "date": "2025-12-26", "relevancy": 1.9599, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.484}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MISA%3A%20Memory-Efficient%20LLMs%20Optimization%20with%20Module-wise%20Importance%20Sampling&body=Title%3A%20MISA%3A%20Memory-Efficient%20LLMs%20Optimization%20with%20Module-wise%20Importance%20Sampling%0AAuthor%3A%20Yuxi%20Liu%20and%20Renjia%20Deng%20and%20Yutong%20He%20and%20Xue%20Wang%20and%20Tao%20Yao%20and%20Kun%20Yuan%0AAbstract%3A%20The%20substantial%20memory%20demands%20of%20pre-training%20and%20fine-tuning%20large%20language%20models%20%28LLMs%29%20require%20memory-efficient%20optimization%20algorithms.%20One%20promising%20approach%20is%20layer-wise%20optimization%2C%20which%20treats%20each%20transformer%20block%20as%20a%20single%20layer%20and%20optimizes%20it%20sequentially%2C%20while%20freezing%20the%20other%20layers%20to%20save%20optimizer%20states%20and%20activations.%20Although%20effective%2C%20these%20methods%20ignore%20the%20varying%20importance%20of%20the%20modules%20within%20each%20layer%2C%20leading%20to%20suboptimal%20performance.%20Moreover%2C%20layer-wise%20sampling%20provides%20only%20limited%20memory%20savings%2C%20as%20at%20least%20one%20full%20layer%20must%20remain%20active%20during%20optimization.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Module-wise%20Importance%20SAmpling%20%28MISA%29%2C%20a%20novel%20method%20that%20divides%20each%20layer%20into%20smaller%20modules%20and%20assigns%20importance%20scores%20to%20each%20module.%20MISA%20uses%20a%20weighted%20random%20sampling%20mechanism%20to%20activate%20modules%2C%20provably%20reducing%20gradient%20variance%20compared%20to%20layer-wise%20sampling.%20Additionally%2C%20we%20establish%20an%20%5C%28%5Cmathcal%7BO%7D%281/%5Csqrt%7BK%7D%29%5C%29%20convergence%20rate%20under%20non-convex%20and%20stochastic%20conditions%2C%20where%20%24K%24%20is%20the%20total%20number%20of%20block%20updates%2C%20and%20provide%20a%20detailed%20memory%20analysis%20showcasing%20MISA%27s%20superiority%20over%20existing%20baseline%20methods.%20Experiments%20on%20diverse%20learning%20tasks%20validate%20the%20effectiveness%20of%20MISA.%20Source%20code%20is%20available%20at%20https%3A//github.com/pkumelon/MISA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00056v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMISA%253A%2520Memory-Efficient%2520LLMs%2520Optimization%2520with%2520Module-wise%2520Importance%2520Sampling%26entry.906535625%3DYuxi%2520Liu%2520and%2520Renjia%2520Deng%2520and%2520Yutong%2520He%2520and%2520Xue%2520Wang%2520and%2520Tao%2520Yao%2520and%2520Kun%2520Yuan%26entry.1292438233%3DThe%2520substantial%2520memory%2520demands%2520of%2520pre-training%2520and%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520require%2520memory-efficient%2520optimization%2520algorithms.%2520One%2520promising%2520approach%2520is%2520layer-wise%2520optimization%252C%2520which%2520treats%2520each%2520transformer%2520block%2520as%2520a%2520single%2520layer%2520and%2520optimizes%2520it%2520sequentially%252C%2520while%2520freezing%2520the%2520other%2520layers%2520to%2520save%2520optimizer%2520states%2520and%2520activations.%2520Although%2520effective%252C%2520these%2520methods%2520ignore%2520the%2520varying%2520importance%2520of%2520the%2520modules%2520within%2520each%2520layer%252C%2520leading%2520to%2520suboptimal%2520performance.%2520Moreover%252C%2520layer-wise%2520sampling%2520provides%2520only%2520limited%2520memory%2520savings%252C%2520as%2520at%2520least%2520one%2520full%2520layer%2520must%2520remain%2520active%2520during%2520optimization.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Module-wise%2520Importance%2520SAmpling%2520%2528MISA%2529%252C%2520a%2520novel%2520method%2520that%2520divides%2520each%2520layer%2520into%2520smaller%2520modules%2520and%2520assigns%2520importance%2520scores%2520to%2520each%2520module.%2520MISA%2520uses%2520a%2520weighted%2520random%2520sampling%2520mechanism%2520to%2520activate%2520modules%252C%2520provably%2520reducing%2520gradient%2520variance%2520compared%2520to%2520layer-wise%2520sampling.%2520Additionally%252C%2520we%2520establish%2520an%2520%255C%2528%255Cmathcal%257BO%257D%25281/%255Csqrt%257BK%257D%2529%255C%2529%2520convergence%2520rate%2520under%2520non-convex%2520and%2520stochastic%2520conditions%252C%2520where%2520%2524K%2524%2520is%2520the%2520total%2520number%2520of%2520block%2520updates%252C%2520and%2520provide%2520a%2520detailed%2520memory%2520analysis%2520showcasing%2520MISA%2527s%2520superiority%2520over%2520existing%2520baseline%2520methods.%2520Experiments%2520on%2520diverse%2520learning%2520tasks%2520validate%2520the%2520effectiveness%2520of%2520MISA.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//github.com/pkumelon/MISA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00056v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MISA%3A%20Memory-Efficient%20LLMs%20Optimization%20with%20Module-wise%20Importance%20Sampling&entry.906535625=Yuxi%20Liu%20and%20Renjia%20Deng%20and%20Yutong%20He%20and%20Xue%20Wang%20and%20Tao%20Yao%20and%20Kun%20Yuan&entry.1292438233=The%20substantial%20memory%20demands%20of%20pre-training%20and%20fine-tuning%20large%20language%20models%20%28LLMs%29%20require%20memory-efficient%20optimization%20algorithms.%20One%20promising%20approach%20is%20layer-wise%20optimization%2C%20which%20treats%20each%20transformer%20block%20as%20a%20single%20layer%20and%20optimizes%20it%20sequentially%2C%20while%20freezing%20the%20other%20layers%20to%20save%20optimizer%20states%20and%20activations.%20Although%20effective%2C%20these%20methods%20ignore%20the%20varying%20importance%20of%20the%20modules%20within%20each%20layer%2C%20leading%20to%20suboptimal%20performance.%20Moreover%2C%20layer-wise%20sampling%20provides%20only%20limited%20memory%20savings%2C%20as%20at%20least%20one%20full%20layer%20must%20remain%20active%20during%20optimization.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Module-wise%20Importance%20SAmpling%20%28MISA%29%2C%20a%20novel%20method%20that%20divides%20each%20layer%20into%20smaller%20modules%20and%20assigns%20importance%20scores%20to%20each%20module.%20MISA%20uses%20a%20weighted%20random%20sampling%20mechanism%20to%20activate%20modules%2C%20provably%20reducing%20gradient%20variance%20compared%20to%20layer-wise%20sampling.%20Additionally%2C%20we%20establish%20an%20%5C%28%5Cmathcal%7BO%7D%281/%5Csqrt%7BK%7D%29%5C%29%20convergence%20rate%20under%20non-convex%20and%20stochastic%20conditions%2C%20where%20%24K%24%20is%20the%20total%20number%20of%20block%20updates%2C%20and%20provide%20a%20detailed%20memory%20analysis%20showcasing%20MISA%27s%20superiority%20over%20existing%20baseline%20methods.%20Experiments%20on%20diverse%20learning%20tasks%20validate%20the%20effectiveness%20of%20MISA.%20Source%20code%20is%20available%20at%20https%3A//github.com/pkumelon/MISA.&entry.1838667208=http%3A//arxiv.org/abs/2511.00056v2&entry.124074799=Read"},
{"title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis", "author": "Duygu Altinok", "abstract": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.", "link": "http://arxiv.org/abs/2512.22100v1", "date": "2025-12-26", "relevancy": 1.9398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20TrGLUE%20and%20SentiTurca%3A%20A%20Comprehensive%20Benchmark%20for%20Turkish%20General%20Language%20Understanding%20and%20Sentiment%20Analysis&body=Title%3A%20Introducing%20TrGLUE%20and%20SentiTurca%3A%20A%20Comprehensive%20Benchmark%20for%20Turkish%20General%20Language%20Understanding%20and%20Sentiment%20Analysis%0AAuthor%3A%20Duygu%20Altinok%0AAbstract%3A%20Evaluating%20the%20performance%20of%20various%20model%20architectures%2C%20such%20as%20transformers%2C%20large%20language%20models%20%28LLMs%29%2C%20and%20other%20NLP%20systems%2C%20requires%20comprehensive%20benchmarks%20that%20measure%20performance%20across%20multiple%20dimensions.%20Among%20these%2C%20the%20evaluation%20of%20natural%20language%20understanding%20%28NLU%29%20is%20particularly%20critical%20as%20it%20serves%20as%20a%20fundamental%20criterion%20for%20assessing%20model%20capabilities.%20Thus%2C%20it%20is%20essential%20to%20establish%20benchmarks%20that%20enable%20thorough%20evaluation%20and%20analysis%20of%20NLU%20abilities%20from%20diverse%20perspectives.%20While%20the%20GLUE%20benchmark%20has%20set%20a%20standard%20for%20evaluating%20English%20NLU%2C%20similar%20benchmarks%20have%20been%20developed%20for%20other%20languages%2C%20such%20as%20CLUE%20for%20Chinese%2C%20FLUE%20for%20French%2C%20and%20JGLUE%20for%20Japanese.%20However%2C%20no%20comparable%20benchmark%20currently%20exists%20for%20the%20Turkish%20language.%20To%20address%20this%20gap%2C%20we%20introduce%20TrGLUE%2C%20a%20comprehensive%20benchmark%20encompassing%20a%20variety%20of%20NLU%20tasks%20for%20Turkish.%20In%20addition%2C%20we%20present%20SentiTurca%2C%20a%20specialized%20benchmark%20for%20sentiment%20analysis.%20To%20support%20researchers%2C%20we%20also%20provide%20fine-tuning%20and%20evaluation%20code%20for%20transformer-based%20models%2C%20facilitating%20the%20effective%20use%20of%20these%20benchmarks.%20TrGLUE%20comprises%20Turkish-native%20corpora%20curated%20to%20mirror%20the%20domains%20and%20task%20formulations%20of%20GLUE-style%20evaluations%2C%20with%20labels%20obtained%20through%20a%20semi-automated%20pipeline%20that%20combines%20strong%20LLM-based%20annotation%2C%20cross-model%20agreement%20checks%2C%20and%20subsequent%20human%20validation.%20This%20design%20prioritizes%20linguistic%20naturalness%2C%20minimizes%20direct%20translation%20artifacts%2C%20and%20yields%20a%20scalable%2C%20reproducible%20workflow.%20With%20TrGLUE%2C%20our%20goal%20is%20to%20establish%20a%20robust%20evaluation%20framework%20for%20Turkish%20NLU%2C%20empower%20researchers%20with%20valuable%20resources%2C%20and%20provide%20insights%20into%20generating%20high-quality%20semi-automated%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520TrGLUE%2520and%2520SentiTurca%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Turkish%2520General%2520Language%2520Understanding%2520and%2520Sentiment%2520Analysis%26entry.906535625%3DDuygu%2520Altinok%26entry.1292438233%3DEvaluating%2520the%2520performance%2520of%2520various%2520model%2520architectures%252C%2520such%2520as%2520transformers%252C%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520and%2520other%2520NLP%2520systems%252C%2520requires%2520comprehensive%2520benchmarks%2520that%2520measure%2520performance%2520across%2520multiple%2520dimensions.%2520Among%2520these%252C%2520the%2520evaluation%2520of%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520is%2520particularly%2520critical%2520as%2520it%2520serves%2520as%2520a%2520fundamental%2520criterion%2520for%2520assessing%2520model%2520capabilities.%2520Thus%252C%2520it%2520is%2520essential%2520to%2520establish%2520benchmarks%2520that%2520enable%2520thorough%2520evaluation%2520and%2520analysis%2520of%2520NLU%2520abilities%2520from%2520diverse%2520perspectives.%2520While%2520the%2520GLUE%2520benchmark%2520has%2520set%2520a%2520standard%2520for%2520evaluating%2520English%2520NLU%252C%2520similar%2520benchmarks%2520have%2520been%2520developed%2520for%2520other%2520languages%252C%2520such%2520as%2520CLUE%2520for%2520Chinese%252C%2520FLUE%2520for%2520French%252C%2520and%2520JGLUE%2520for%2520Japanese.%2520However%252C%2520no%2520comparable%2520benchmark%2520currently%2520exists%2520for%2520the%2520Turkish%2520language.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520TrGLUE%252C%2520a%2520comprehensive%2520benchmark%2520encompassing%2520a%2520variety%2520of%2520NLU%2520tasks%2520for%2520Turkish.%2520In%2520addition%252C%2520we%2520present%2520SentiTurca%252C%2520a%2520specialized%2520benchmark%2520for%2520sentiment%2520analysis.%2520To%2520support%2520researchers%252C%2520we%2520also%2520provide%2520fine-tuning%2520and%2520evaluation%2520code%2520for%2520transformer-based%2520models%252C%2520facilitating%2520the%2520effective%2520use%2520of%2520these%2520benchmarks.%2520TrGLUE%2520comprises%2520Turkish-native%2520corpora%2520curated%2520to%2520mirror%2520the%2520domains%2520and%2520task%2520formulations%2520of%2520GLUE-style%2520evaluations%252C%2520with%2520labels%2520obtained%2520through%2520a%2520semi-automated%2520pipeline%2520that%2520combines%2520strong%2520LLM-based%2520annotation%252C%2520cross-model%2520agreement%2520checks%252C%2520and%2520subsequent%2520human%2520validation.%2520This%2520design%2520prioritizes%2520linguistic%2520naturalness%252C%2520minimizes%2520direct%2520translation%2520artifacts%252C%2520and%2520yields%2520a%2520scalable%252C%2520reproducible%2520workflow.%2520With%2520TrGLUE%252C%2520our%2520goal%2520is%2520to%2520establish%2520a%2520robust%2520evaluation%2520framework%2520for%2520Turkish%2520NLU%252C%2520empower%2520researchers%2520with%2520valuable%2520resources%252C%2520and%2520provide%2520insights%2520into%2520generating%2520high-quality%2520semi-automated%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20TrGLUE%20and%20SentiTurca%3A%20A%20Comprehensive%20Benchmark%20for%20Turkish%20General%20Language%20Understanding%20and%20Sentiment%20Analysis&entry.906535625=Duygu%20Altinok&entry.1292438233=Evaluating%20the%20performance%20of%20various%20model%20architectures%2C%20such%20as%20transformers%2C%20large%20language%20models%20%28LLMs%29%2C%20and%20other%20NLP%20systems%2C%20requires%20comprehensive%20benchmarks%20that%20measure%20performance%20across%20multiple%20dimensions.%20Among%20these%2C%20the%20evaluation%20of%20natural%20language%20understanding%20%28NLU%29%20is%20particularly%20critical%20as%20it%20serves%20as%20a%20fundamental%20criterion%20for%20assessing%20model%20capabilities.%20Thus%2C%20it%20is%20essential%20to%20establish%20benchmarks%20that%20enable%20thorough%20evaluation%20and%20analysis%20of%20NLU%20abilities%20from%20diverse%20perspectives.%20While%20the%20GLUE%20benchmark%20has%20set%20a%20standard%20for%20evaluating%20English%20NLU%2C%20similar%20benchmarks%20have%20been%20developed%20for%20other%20languages%2C%20such%20as%20CLUE%20for%20Chinese%2C%20FLUE%20for%20French%2C%20and%20JGLUE%20for%20Japanese.%20However%2C%20no%20comparable%20benchmark%20currently%20exists%20for%20the%20Turkish%20language.%20To%20address%20this%20gap%2C%20we%20introduce%20TrGLUE%2C%20a%20comprehensive%20benchmark%20encompassing%20a%20variety%20of%20NLU%20tasks%20for%20Turkish.%20In%20addition%2C%20we%20present%20SentiTurca%2C%20a%20specialized%20benchmark%20for%20sentiment%20analysis.%20To%20support%20researchers%2C%20we%20also%20provide%20fine-tuning%20and%20evaluation%20code%20for%20transformer-based%20models%2C%20facilitating%20the%20effective%20use%20of%20these%20benchmarks.%20TrGLUE%20comprises%20Turkish-native%20corpora%20curated%20to%20mirror%20the%20domains%20and%20task%20formulations%20of%20GLUE-style%20evaluations%2C%20with%20labels%20obtained%20through%20a%20semi-automated%20pipeline%20that%20combines%20strong%20LLM-based%20annotation%2C%20cross-model%20agreement%20checks%2C%20and%20subsequent%20human%20validation.%20This%20design%20prioritizes%20linguistic%20naturalness%2C%20minimizes%20direct%20translation%20artifacts%2C%20and%20yields%20a%20scalable%2C%20reproducible%20workflow.%20With%20TrGLUE%2C%20our%20goal%20is%20to%20establish%20a%20robust%20evaluation%20framework%20for%20Turkish%20NLU%2C%20empower%20researchers%20with%20valuable%20resources%2C%20and%20provide%20insights%20into%20generating%20high-quality%20semi-automated%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.22100v1&entry.124074799=Read"},
{"title": "GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction", "author": "Daeyoung Kim", "abstract": "Age Related Macular Degeneration(AMD) has been one of the most leading causes of permanent vision impairment in ophthalmology. Though treatments, such as anti VEGF drugs or photodynamic therapies, were developed to slow down the degenerative process of AMD, there is still no specific cure to reverse vision loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD or AMD itself within the patient retina in early stages is a crucial task to reduce the possibility of vision impairment. Apart from traditional approaches, deep learning based methods, especially attention mechanism based CNNs and GradCAM based XAI analysis on OCT scans, exhibited successful performance in distinguishing AMD retina from normal retinas, making it possible to use AI driven models to aid medical diagnosis and analysis by ophthalmologists regarding AMD. However, though having significant success, previous works mostly focused on prediction performance itself, not pathologies or underlying causal mechanisms of AMD, which can prohibit intervention analysis on specific factors or even lead to less reliable decisions. Thus, this paper introduces a novel causal AMD analysis model: GCVAMD, which incorporates a modified CausalVAE approach that can extract latent causal factors from only raw OCT images. By considering causality in AMD detection, GCVAMD enables causal inference such as treatment simulation or intervention analysis regarding major risk factors: drusen and neovascularization, while returning informative latent causal features that can enhance downstream tasks. Results show that through GCVAMD, drusen status and neovascularization status can be identified with AMD causal mechanisms in GCVAMD latent spaces, which can in turn be used for various tasks from AMD detection(classification) to intervention analysis.", "link": "http://arxiv.org/abs/2510.02781v3", "date": "2025-12-26", "relevancy": 1.9391, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5024}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.489}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCVAMD%3A%20A%20Modified%20CausalVAE%20Model%20for%20Causal%20Age-related%20Macular%20Degeneration%20Risk%20Factor%20Detection%20and%20Prediction&body=Title%3A%20GCVAMD%3A%20A%20Modified%20CausalVAE%20Model%20for%20Causal%20Age-related%20Macular%20Degeneration%20Risk%20Factor%20Detection%20and%20Prediction%0AAuthor%3A%20Daeyoung%20Kim%0AAbstract%3A%20Age%20Related%20Macular%20Degeneration%28AMD%29%20has%20been%20one%20of%20the%20most%20leading%20causes%20of%20permanent%20vision%20impairment%20in%20ophthalmology.%20Though%20treatments%2C%20such%20as%20anti%20VEGF%20drugs%20or%20photodynamic%20therapies%2C%20were%20developed%20to%20slow%20down%20the%20degenerative%20process%20of%20AMD%2C%20there%20is%20still%20no%20specific%20cure%20to%20reverse%20vision%20loss%20caused%20by%20AMD.%20Thus%2C%20for%20AMD%2C%20detecting%20existence%20of%20risk%20factors%20of%20AMD%20or%20AMD%20itself%20within%20the%20patient%20retina%20in%20early%20stages%20is%20a%20crucial%20task%20to%20reduce%20the%20possibility%20of%20vision%20impairment.%20Apart%20from%20traditional%20approaches%2C%20deep%20learning%20based%20methods%2C%20especially%20attention%20mechanism%20based%20CNNs%20and%20GradCAM%20based%20XAI%20analysis%20on%20OCT%20scans%2C%20exhibited%20successful%20performance%20in%20distinguishing%20AMD%20retina%20from%20normal%20retinas%2C%20making%20it%20possible%20to%20use%20AI%20driven%20models%20to%20aid%20medical%20diagnosis%20and%20analysis%20by%20ophthalmologists%20regarding%20AMD.%20However%2C%20though%20having%20significant%20success%2C%20previous%20works%20mostly%20focused%20on%20prediction%20performance%20itself%2C%20not%20pathologies%20or%20underlying%20causal%20mechanisms%20of%20AMD%2C%20which%20can%20prohibit%20intervention%20analysis%20on%20specific%20factors%20or%20even%20lead%20to%20less%20reliable%20decisions.%20Thus%2C%20this%20paper%20introduces%20a%20novel%20causal%20AMD%20analysis%20model%3A%20GCVAMD%2C%20which%20incorporates%20a%20modified%20CausalVAE%20approach%20that%20can%20extract%20latent%20causal%20factors%20from%20only%20raw%20OCT%20images.%20By%20considering%20causality%20in%20AMD%20detection%2C%20GCVAMD%20enables%20causal%20inference%20such%20as%20treatment%20simulation%20or%20intervention%20analysis%20regarding%20major%20risk%20factors%3A%20drusen%20and%20neovascularization%2C%20while%20returning%20informative%20latent%20causal%20features%20that%20can%20enhance%20downstream%20tasks.%20Results%20show%20that%20through%20GCVAMD%2C%20drusen%20status%20and%20neovascularization%20status%20can%20be%20identified%20with%20AMD%20causal%20mechanisms%20in%20GCVAMD%20latent%20spaces%2C%20which%20can%20in%20turn%20be%20used%20for%20various%20tasks%20from%20AMD%20detection%28classification%29%20to%20intervention%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCVAMD%253A%2520A%2520Modified%2520CausalVAE%2520Model%2520for%2520Causal%2520Age-related%2520Macular%2520Degeneration%2520Risk%2520Factor%2520Detection%2520and%2520Prediction%26entry.906535625%3DDaeyoung%2520Kim%26entry.1292438233%3DAge%2520Related%2520Macular%2520Degeneration%2528AMD%2529%2520has%2520been%2520one%2520of%2520the%2520most%2520leading%2520causes%2520of%2520permanent%2520vision%2520impairment%2520in%2520ophthalmology.%2520Though%2520treatments%252C%2520such%2520as%2520anti%2520VEGF%2520drugs%2520or%2520photodynamic%2520therapies%252C%2520were%2520developed%2520to%2520slow%2520down%2520the%2520degenerative%2520process%2520of%2520AMD%252C%2520there%2520is%2520still%2520no%2520specific%2520cure%2520to%2520reverse%2520vision%2520loss%2520caused%2520by%2520AMD.%2520Thus%252C%2520for%2520AMD%252C%2520detecting%2520existence%2520of%2520risk%2520factors%2520of%2520AMD%2520or%2520AMD%2520itself%2520within%2520the%2520patient%2520retina%2520in%2520early%2520stages%2520is%2520a%2520crucial%2520task%2520to%2520reduce%2520the%2520possibility%2520of%2520vision%2520impairment.%2520Apart%2520from%2520traditional%2520approaches%252C%2520deep%2520learning%2520based%2520methods%252C%2520especially%2520attention%2520mechanism%2520based%2520CNNs%2520and%2520GradCAM%2520based%2520XAI%2520analysis%2520on%2520OCT%2520scans%252C%2520exhibited%2520successful%2520performance%2520in%2520distinguishing%2520AMD%2520retina%2520from%2520normal%2520retinas%252C%2520making%2520it%2520possible%2520to%2520use%2520AI%2520driven%2520models%2520to%2520aid%2520medical%2520diagnosis%2520and%2520analysis%2520by%2520ophthalmologists%2520regarding%2520AMD.%2520However%252C%2520though%2520having%2520significant%2520success%252C%2520previous%2520works%2520mostly%2520focused%2520on%2520prediction%2520performance%2520itself%252C%2520not%2520pathologies%2520or%2520underlying%2520causal%2520mechanisms%2520of%2520AMD%252C%2520which%2520can%2520prohibit%2520intervention%2520analysis%2520on%2520specific%2520factors%2520or%2520even%2520lead%2520to%2520less%2520reliable%2520decisions.%2520Thus%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520causal%2520AMD%2520analysis%2520model%253A%2520GCVAMD%252C%2520which%2520incorporates%2520a%2520modified%2520CausalVAE%2520approach%2520that%2520can%2520extract%2520latent%2520causal%2520factors%2520from%2520only%2520raw%2520OCT%2520images.%2520By%2520considering%2520causality%2520in%2520AMD%2520detection%252C%2520GCVAMD%2520enables%2520causal%2520inference%2520such%2520as%2520treatment%2520simulation%2520or%2520intervention%2520analysis%2520regarding%2520major%2520risk%2520factors%253A%2520drusen%2520and%2520neovascularization%252C%2520while%2520returning%2520informative%2520latent%2520causal%2520features%2520that%2520can%2520enhance%2520downstream%2520tasks.%2520Results%2520show%2520that%2520through%2520GCVAMD%252C%2520drusen%2520status%2520and%2520neovascularization%2520status%2520can%2520be%2520identified%2520with%2520AMD%2520causal%2520mechanisms%2520in%2520GCVAMD%2520latent%2520spaces%252C%2520which%2520can%2520in%2520turn%2520be%2520used%2520for%2520various%2520tasks%2520from%2520AMD%2520detection%2528classification%2529%2520to%2520intervention%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCVAMD%3A%20A%20Modified%20CausalVAE%20Model%20for%20Causal%20Age-related%20Macular%20Degeneration%20Risk%20Factor%20Detection%20and%20Prediction&entry.906535625=Daeyoung%20Kim&entry.1292438233=Age%20Related%20Macular%20Degeneration%28AMD%29%20has%20been%20one%20of%20the%20most%20leading%20causes%20of%20permanent%20vision%20impairment%20in%20ophthalmology.%20Though%20treatments%2C%20such%20as%20anti%20VEGF%20drugs%20or%20photodynamic%20therapies%2C%20were%20developed%20to%20slow%20down%20the%20degenerative%20process%20of%20AMD%2C%20there%20is%20still%20no%20specific%20cure%20to%20reverse%20vision%20loss%20caused%20by%20AMD.%20Thus%2C%20for%20AMD%2C%20detecting%20existence%20of%20risk%20factors%20of%20AMD%20or%20AMD%20itself%20within%20the%20patient%20retina%20in%20early%20stages%20is%20a%20crucial%20task%20to%20reduce%20the%20possibility%20of%20vision%20impairment.%20Apart%20from%20traditional%20approaches%2C%20deep%20learning%20based%20methods%2C%20especially%20attention%20mechanism%20based%20CNNs%20and%20GradCAM%20based%20XAI%20analysis%20on%20OCT%20scans%2C%20exhibited%20successful%20performance%20in%20distinguishing%20AMD%20retina%20from%20normal%20retinas%2C%20making%20it%20possible%20to%20use%20AI%20driven%20models%20to%20aid%20medical%20diagnosis%20and%20analysis%20by%20ophthalmologists%20regarding%20AMD.%20However%2C%20though%20having%20significant%20success%2C%20previous%20works%20mostly%20focused%20on%20prediction%20performance%20itself%2C%20not%20pathologies%20or%20underlying%20causal%20mechanisms%20of%20AMD%2C%20which%20can%20prohibit%20intervention%20analysis%20on%20specific%20factors%20or%20even%20lead%20to%20less%20reliable%20decisions.%20Thus%2C%20this%20paper%20introduces%20a%20novel%20causal%20AMD%20analysis%20model%3A%20GCVAMD%2C%20which%20incorporates%20a%20modified%20CausalVAE%20approach%20that%20can%20extract%20latent%20causal%20factors%20from%20only%20raw%20OCT%20images.%20By%20considering%20causality%20in%20AMD%20detection%2C%20GCVAMD%20enables%20causal%20inference%20such%20as%20treatment%20simulation%20or%20intervention%20analysis%20regarding%20major%20risk%20factors%3A%20drusen%20and%20neovascularization%2C%20while%20returning%20informative%20latent%20causal%20features%20that%20can%20enhance%20downstream%20tasks.%20Results%20show%20that%20through%20GCVAMD%2C%20drusen%20status%20and%20neovascularization%20status%20can%20be%20identified%20with%20AMD%20causal%20mechanisms%20in%20GCVAMD%20latent%20spaces%2C%20which%20can%20in%20turn%20be%20used%20for%20various%20tasks%20from%20AMD%20detection%28classification%29%20to%20intervention%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2510.02781v3&entry.124074799=Read"},
{"title": "Periodic Asynchrony: An Effective Method for Accelerating Reinforcement Learning for Large Language Models", "author": "Jian Lu", "abstract": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.", "link": "http://arxiv.org/abs/2511.18871v3", "date": "2025-12-26", "relevancy": 1.9366, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4925}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4916}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Periodic%20Asynchrony%3A%20An%20Effective%20Method%20for%20Accelerating%20Reinforcement%20Learning%20for%20Large%20Language%20Models&body=Title%3A%20Periodic%20Asynchrony%3A%20An%20Effective%20Method%20for%20Accelerating%20Reinforcement%20Learning%20for%20Large%20Language%20Models%0AAuthor%3A%20Jian%20Lu%0AAbstract%3A%20Since%20the%20introduction%20of%20the%20GRPO%20algorithm%2C%20reinforcement%20learning%20%28RL%29%20has%20attracted%20increasing%20attention%2C%20with%20growing%20efforts%20to%20reproduce%20and%20apply%20it.%20However%2C%20training%20efficiency%20remains%20a%20critical%20challenge.%20In%20mainstream%20RL%20frameworks%2C%20inference%20and%20training%20are%20typically%20deployed%20on%20the%20same%20devices.%20While%20this%20approach%20reduces%20costs%20through%20resource%20consolidation%2C%20its%20synchronous%20execution%20imposes%20a%20computational%20coupling%20that%20prevents%20concurrent%20inference%20and%20training.%20In%20this%20study%2C%20we%20are%20returning%20to%20the%20strategy%20of%20separating%20inference%20and%20training%20deployment%2C%20and%20by%20introducing%20improvements%20in%20the%20data%20loader%2C%20we%20transform%20the%20conventional%20synchronous%20architecture%20into%20a%20periodically%20asynchronous%20framework%2C%20which%20allows%20for%20demand-driven%2C%20independent%2C%20and%20elastic%20scaling%20of%20each%20component%2C%20while%20the%20accuracy%20of%20the%20algorithm%20remains%20completely%20equivalent%20to%20the%20synchronization%20method%2C%20with%20both%20belonging%20to%20the%20on-policy%20strategy.%20It%20is%20worth%20emphasizing%20that%20we%20apply%20a%20unified%20tri-model%20architecture%20in%20the%20training%20phase%2C%20and%20we%20also%20proposed%20a%20shared-prompt%20attention%20mask%20to%20reduce%20repetitive%20computation.%20In%20practice%2C%20these%20works%20have%20achieved%20at%20least%20a%20threefold%20overall%20performance%20improvement%20in%20RL%20training%20on%20NPU%20platforms%2C%20indicating%20its%20potential%20for%20widespread%20application.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18871v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeriodic%2520Asynchrony%253A%2520An%2520Effective%2520Method%2520for%2520Accelerating%2520Reinforcement%2520Learning%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJian%2520Lu%26entry.1292438233%3DSince%2520the%2520introduction%2520of%2520the%2520GRPO%2520algorithm%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520attracted%2520increasing%2520attention%252C%2520with%2520growing%2520efforts%2520to%2520reproduce%2520and%2520apply%2520it.%2520However%252C%2520training%2520efficiency%2520remains%2520a%2520critical%2520challenge.%2520In%2520mainstream%2520RL%2520frameworks%252C%2520inference%2520and%2520training%2520are%2520typically%2520deployed%2520on%2520the%2520same%2520devices.%2520While%2520this%2520approach%2520reduces%2520costs%2520through%2520resource%2520consolidation%252C%2520its%2520synchronous%2520execution%2520imposes%2520a%2520computational%2520coupling%2520that%2520prevents%2520concurrent%2520inference%2520and%2520training.%2520In%2520this%2520study%252C%2520we%2520are%2520returning%2520to%2520the%2520strategy%2520of%2520separating%2520inference%2520and%2520training%2520deployment%252C%2520and%2520by%2520introducing%2520improvements%2520in%2520the%2520data%2520loader%252C%2520we%2520transform%2520the%2520conventional%2520synchronous%2520architecture%2520into%2520a%2520periodically%2520asynchronous%2520framework%252C%2520which%2520allows%2520for%2520demand-driven%252C%2520independent%252C%2520and%2520elastic%2520scaling%2520of%2520each%2520component%252C%2520while%2520the%2520accuracy%2520of%2520the%2520algorithm%2520remains%2520completely%2520equivalent%2520to%2520the%2520synchronization%2520method%252C%2520with%2520both%2520belonging%2520to%2520the%2520on-policy%2520strategy.%2520It%2520is%2520worth%2520emphasizing%2520that%2520we%2520apply%2520a%2520unified%2520tri-model%2520architecture%2520in%2520the%2520training%2520phase%252C%2520and%2520we%2520also%2520proposed%2520a%2520shared-prompt%2520attention%2520mask%2520to%2520reduce%2520repetitive%2520computation.%2520In%2520practice%252C%2520these%2520works%2520have%2520achieved%2520at%2520least%2520a%2520threefold%2520overall%2520performance%2520improvement%2520in%2520RL%2520training%2520on%2520NPU%2520platforms%252C%2520indicating%2520its%2520potential%2520for%2520widespread%2520application.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18871v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Periodic%20Asynchrony%3A%20An%20Effective%20Method%20for%20Accelerating%20Reinforcement%20Learning%20for%20Large%20Language%20Models&entry.906535625=Jian%20Lu&entry.1292438233=Since%20the%20introduction%20of%20the%20GRPO%20algorithm%2C%20reinforcement%20learning%20%28RL%29%20has%20attracted%20increasing%20attention%2C%20with%20growing%20efforts%20to%20reproduce%20and%20apply%20it.%20However%2C%20training%20efficiency%20remains%20a%20critical%20challenge.%20In%20mainstream%20RL%20frameworks%2C%20inference%20and%20training%20are%20typically%20deployed%20on%20the%20same%20devices.%20While%20this%20approach%20reduces%20costs%20through%20resource%20consolidation%2C%20its%20synchronous%20execution%20imposes%20a%20computational%20coupling%20that%20prevents%20concurrent%20inference%20and%20training.%20In%20this%20study%2C%20we%20are%20returning%20to%20the%20strategy%20of%20separating%20inference%20and%20training%20deployment%2C%20and%20by%20introducing%20improvements%20in%20the%20data%20loader%2C%20we%20transform%20the%20conventional%20synchronous%20architecture%20into%20a%20periodically%20asynchronous%20framework%2C%20which%20allows%20for%20demand-driven%2C%20independent%2C%20and%20elastic%20scaling%20of%20each%20component%2C%20while%20the%20accuracy%20of%20the%20algorithm%20remains%20completely%20equivalent%20to%20the%20synchronization%20method%2C%20with%20both%20belonging%20to%20the%20on-policy%20strategy.%20It%20is%20worth%20emphasizing%20that%20we%20apply%20a%20unified%20tri-model%20architecture%20in%20the%20training%20phase%2C%20and%20we%20also%20proposed%20a%20shared-prompt%20attention%20mask%20to%20reduce%20repetitive%20computation.%20In%20practice%2C%20these%20works%20have%20achieved%20at%20least%20a%20threefold%20overall%20performance%20improvement%20in%20RL%20training%20on%20NPU%20platforms%2C%20indicating%20its%20potential%20for%20widespread%20application.&entry.1838667208=http%3A//arxiv.org/abs/2511.18871v3&entry.124074799=Read"},
{"title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "author": "Hanzhang Zhou and Xu Zhang and Panrong Tong and Jianan Zhang and Liangyu Chen and Quyu Kong and Chenglin Cai and Chen Liu and Yue Wang and Jingren Zhou and Steven Hoi", "abstract": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "link": "http://arxiv.org/abs/2512.22047v1", "date": "2025-12-26", "relevancy": 1.9172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4885}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4783}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAI-UI%20Technical%20Report%3A%20Real-World%20Centric%20Foundation%20GUI%20Agents&body=Title%3A%20MAI-UI%20Technical%20Report%3A%20Real-World%20Centric%20Foundation%20GUI%20Agents%0AAuthor%3A%20Hanzhang%20Zhou%20and%20Xu%20Zhang%20and%20Panrong%20Tong%20and%20Jianan%20Zhang%20and%20Liangyu%20Chen%20and%20Quyu%20Kong%20and%20Chenglin%20Cai%20and%20Chen%20Liu%20and%20Yue%20Wang%20and%20Jingren%20Zhou%20and%20Steven%20Hoi%0AAbstract%3A%20The%20development%20of%20GUI%20agents%20could%20revolutionize%20the%20next%20generation%20of%20human-computer%20interaction.%20Motivated%20by%20this%20vision%2C%20we%20present%20MAI-UI%2C%20a%20family%20of%20foundation%20GUI%20agents%20spanning%20the%20full%20spectrum%20of%20sizes%2C%20including%202B%2C%208B%2C%2032B%2C%20and%20235B-A22B%20variants.%20We%20identify%20four%20key%20challenges%20to%20realistic%20deployment%3A%20the%20lack%20of%20native%20agent-user%20interaction%2C%20the%20limits%20of%20UI-only%20operation%2C%20the%20absence%20of%20a%20practical%20deployment%20architecture%2C%20and%20brittleness%20in%20dynamic%20environments.%20MAI-UI%20addresses%20these%20issues%20with%20a%20unified%20methodology%3A%20a%20self-evolving%20data%20pipeline%20that%20expands%20the%20navigation%20data%20to%20include%20user%20interaction%20and%20MCP%20tool%20calls%2C%20a%20native%20device-cloud%20collaboration%20system%20routes%20execution%20by%20task%20state%2C%20and%20an%20online%20RL%20framework%20with%20advanced%20optimizations%20to%20scale%20parallel%20environments%20and%20context%20length.%20MAI-UI%20establishes%20new%20state-of-the-art%20across%20GUI%20grounding%20and%20mobile%20navigation.%20On%20grounding%20benchmarks%2C%20it%20reaches%2073.5%25%20on%20ScreenSpot-Pro%2C%2091.3%25%20on%20MMBench%20GUI%20L2%2C%2070.9%25%20on%20OSWorld-G%2C%20and%2049.2%25%20on%20UI-Vision%2C%20surpassing%20Gemini-3-Pro%20and%20Seed1.8%20on%20ScreenSpot-Pro.%20On%20mobile%20GUI%20navigation%2C%20it%20sets%20a%20new%20SOTA%20of%2076.7%25%20on%20AndroidWorld%2C%20surpassing%20UI-Tars-2%2C%20Gemini-2.5-Pro%20and%20Seed1.8.%20On%20MobileWorld%2C%20MAI-UI%20obtains%2041.7%25%20success%20rate%2C%20significantly%20outperforming%20end-to-end%20GUI%20models%20and%20competitive%20with%20Gemini-3-Pro%20based%20agentic%20frameworks.%20Our%20online%20RL%20experiments%20show%20significant%20gains%20from%20scaling%20parallel%20environments%20from%2032%20to%20512%20%28%2B5.2%20points%29%20and%20increasing%20environment%20step%20budget%20from%2015%20to%2050%20%28%2B4.3%20points%29.%20Finally%2C%20the%20native%20device-cloud%20collaboration%20system%20improves%20on-device%20performance%20by%2033%25%2C%20reduces%20cloud%20model%20calls%20by%20over%2040%25%2C%20and%20preserves%20user%20privacy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAI-UI%2520Technical%2520Report%253A%2520Real-World%2520Centric%2520Foundation%2520GUI%2520Agents%26entry.906535625%3DHanzhang%2520Zhou%2520and%2520Xu%2520Zhang%2520and%2520Panrong%2520Tong%2520and%2520Jianan%2520Zhang%2520and%2520Liangyu%2520Chen%2520and%2520Quyu%2520Kong%2520and%2520Chenglin%2520Cai%2520and%2520Chen%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Jingren%2520Zhou%2520and%2520Steven%2520Hoi%26entry.1292438233%3DThe%2520development%2520of%2520GUI%2520agents%2520could%2520revolutionize%2520the%2520next%2520generation%2520of%2520human-computer%2520interaction.%2520Motivated%2520by%2520this%2520vision%252C%2520we%2520present%2520MAI-UI%252C%2520a%2520family%2520of%2520foundation%2520GUI%2520agents%2520spanning%2520the%2520full%2520spectrum%2520of%2520sizes%252C%2520including%25202B%252C%25208B%252C%252032B%252C%2520and%2520235B-A22B%2520variants.%2520We%2520identify%2520four%2520key%2520challenges%2520to%2520realistic%2520deployment%253A%2520the%2520lack%2520of%2520native%2520agent-user%2520interaction%252C%2520the%2520limits%2520of%2520UI-only%2520operation%252C%2520the%2520absence%2520of%2520a%2520practical%2520deployment%2520architecture%252C%2520and%2520brittleness%2520in%2520dynamic%2520environments.%2520MAI-UI%2520addresses%2520these%2520issues%2520with%2520a%2520unified%2520methodology%253A%2520a%2520self-evolving%2520data%2520pipeline%2520that%2520expands%2520the%2520navigation%2520data%2520to%2520include%2520user%2520interaction%2520and%2520MCP%2520tool%2520calls%252C%2520a%2520native%2520device-cloud%2520collaboration%2520system%2520routes%2520execution%2520by%2520task%2520state%252C%2520and%2520an%2520online%2520RL%2520framework%2520with%2520advanced%2520optimizations%2520to%2520scale%2520parallel%2520environments%2520and%2520context%2520length.%2520MAI-UI%2520establishes%2520new%2520state-of-the-art%2520across%2520GUI%2520grounding%2520and%2520mobile%2520navigation.%2520On%2520grounding%2520benchmarks%252C%2520it%2520reaches%252073.5%2525%2520on%2520ScreenSpot-Pro%252C%252091.3%2525%2520on%2520MMBench%2520GUI%2520L2%252C%252070.9%2525%2520on%2520OSWorld-G%252C%2520and%252049.2%2525%2520on%2520UI-Vision%252C%2520surpassing%2520Gemini-3-Pro%2520and%2520Seed1.8%2520on%2520ScreenSpot-Pro.%2520On%2520mobile%2520GUI%2520navigation%252C%2520it%2520sets%2520a%2520new%2520SOTA%2520of%252076.7%2525%2520on%2520AndroidWorld%252C%2520surpassing%2520UI-Tars-2%252C%2520Gemini-2.5-Pro%2520and%2520Seed1.8.%2520On%2520MobileWorld%252C%2520MAI-UI%2520obtains%252041.7%2525%2520success%2520rate%252C%2520significantly%2520outperforming%2520end-to-end%2520GUI%2520models%2520and%2520competitive%2520with%2520Gemini-3-Pro%2520based%2520agentic%2520frameworks.%2520Our%2520online%2520RL%2520experiments%2520show%2520significant%2520gains%2520from%2520scaling%2520parallel%2520environments%2520from%252032%2520to%2520512%2520%2528%252B5.2%2520points%2529%2520and%2520increasing%2520environment%2520step%2520budget%2520from%252015%2520to%252050%2520%2528%252B4.3%2520points%2529.%2520Finally%252C%2520the%2520native%2520device-cloud%2520collaboration%2520system%2520improves%2520on-device%2520performance%2520by%252033%2525%252C%2520reduces%2520cloud%2520model%2520calls%2520by%2520over%252040%2525%252C%2520and%2520preserves%2520user%2520privacy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAI-UI%20Technical%20Report%3A%20Real-World%20Centric%20Foundation%20GUI%20Agents&entry.906535625=Hanzhang%20Zhou%20and%20Xu%20Zhang%20and%20Panrong%20Tong%20and%20Jianan%20Zhang%20and%20Liangyu%20Chen%20and%20Quyu%20Kong%20and%20Chenglin%20Cai%20and%20Chen%20Liu%20and%20Yue%20Wang%20and%20Jingren%20Zhou%20and%20Steven%20Hoi&entry.1292438233=The%20development%20of%20GUI%20agents%20could%20revolutionize%20the%20next%20generation%20of%20human-computer%20interaction.%20Motivated%20by%20this%20vision%2C%20we%20present%20MAI-UI%2C%20a%20family%20of%20foundation%20GUI%20agents%20spanning%20the%20full%20spectrum%20of%20sizes%2C%20including%202B%2C%208B%2C%2032B%2C%20and%20235B-A22B%20variants.%20We%20identify%20four%20key%20challenges%20to%20realistic%20deployment%3A%20the%20lack%20of%20native%20agent-user%20interaction%2C%20the%20limits%20of%20UI-only%20operation%2C%20the%20absence%20of%20a%20practical%20deployment%20architecture%2C%20and%20brittleness%20in%20dynamic%20environments.%20MAI-UI%20addresses%20these%20issues%20with%20a%20unified%20methodology%3A%20a%20self-evolving%20data%20pipeline%20that%20expands%20the%20navigation%20data%20to%20include%20user%20interaction%20and%20MCP%20tool%20calls%2C%20a%20native%20device-cloud%20collaboration%20system%20routes%20execution%20by%20task%20state%2C%20and%20an%20online%20RL%20framework%20with%20advanced%20optimizations%20to%20scale%20parallel%20environments%20and%20context%20length.%20MAI-UI%20establishes%20new%20state-of-the-art%20across%20GUI%20grounding%20and%20mobile%20navigation.%20On%20grounding%20benchmarks%2C%20it%20reaches%2073.5%25%20on%20ScreenSpot-Pro%2C%2091.3%25%20on%20MMBench%20GUI%20L2%2C%2070.9%25%20on%20OSWorld-G%2C%20and%2049.2%25%20on%20UI-Vision%2C%20surpassing%20Gemini-3-Pro%20and%20Seed1.8%20on%20ScreenSpot-Pro.%20On%20mobile%20GUI%20navigation%2C%20it%20sets%20a%20new%20SOTA%20of%2076.7%25%20on%20AndroidWorld%2C%20surpassing%20UI-Tars-2%2C%20Gemini-2.5-Pro%20and%20Seed1.8.%20On%20MobileWorld%2C%20MAI-UI%20obtains%2041.7%25%20success%20rate%2C%20significantly%20outperforming%20end-to-end%20GUI%20models%20and%20competitive%20with%20Gemini-3-Pro%20based%20agentic%20frameworks.%20Our%20online%20RL%20experiments%20show%20significant%20gains%20from%20scaling%20parallel%20environments%20from%2032%20to%20512%20%28%2B5.2%20points%29%20and%20increasing%20environment%20step%20budget%20from%2015%20to%2050%20%28%2B4.3%20points%29.%20Finally%2C%20the%20native%20device-cloud%20collaboration%20system%20improves%20on-device%20performance%20by%2033%25%2C%20reduces%20cloud%20model%20calls%20by%20over%2040%25%2C%20and%20preserves%20user%20privacy.&entry.1838667208=http%3A//arxiv.org/abs/2512.22047v1&entry.124074799=Read"},
{"title": "A new machine learning framework for occupational accidents forecasting with safety inspections integration", "author": "Aho Yapi and Pierre Latouche and Arnaud Guillin and Yan Bailly", "abstract": "We propose a generic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments to better inform decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Unlike the other approaches, the long short-term memory (LSTM) network outperforms the other approaches and detects the upcoming high-risk periods with a balanced accuracy of 0.86, confirming the robustness of our methodology and demonstrating that a binary time series model can anticipate these critical periods based on safety inspections. The proposed methodology converts routine safety inspection data into clear weekly risk scores, detecting the periods when accidents are most likely. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.", "link": "http://arxiv.org/abs/2507.00089v2", "date": "2025-12-26", "relevancy": 1.9049, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4813}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20machine%20learning%20framework%20for%20occupational%20accidents%20forecasting%20with%20safety%20inspections%20integration&body=Title%3A%20A%20new%20machine%20learning%20framework%20for%20occupational%20accidents%20forecasting%20with%20safety%20inspections%20integration%0AAuthor%3A%20Aho%20Yapi%20and%20Pierre%20Latouche%20and%20Arnaud%20Guillin%20and%20Yan%20Bailly%0AAbstract%3A%20We%20propose%20a%20generic%20framework%20for%20short-term%20occupational%20accident%20forecasting%20that%20leverages%20safety%20inspections%20and%20models%20accident%20occurrences%20as%20binary%20time%20series.%20The%20approach%20generates%20daily%20predictions%2C%20which%20are%20then%20aggregated%20into%20weekly%20safety%20assessments%20to%20better%20inform%20decision%20making.%20To%20ensure%20the%20reliability%20and%20operational%20applicability%20of%20the%20forecasts%2C%20we%20apply%20a%20sliding-window%20cross-validation%20procedure%20specifically%20designed%20for%20time%20series%20data%2C%20combined%20with%20an%20evaluation%20based%20on%20aggregated%20period-level%20metrics.%20Several%20machine%20learning%20algorithms%2C%20including%20logistic%20regression%2C%20tree-based%20models%2C%20and%20neural%20networks%2C%20are%20trained%20and%20systematically%20compared%20within%20this%20framework.%20Unlike%20the%20other%20approaches%2C%20the%20long%20short-term%20memory%20%28LSTM%29%20network%20outperforms%20the%20other%20approaches%20and%20detects%20the%20upcoming%20high-risk%20periods%20with%20a%20balanced%20accuracy%20of%200.86%2C%20confirming%20the%20robustness%20of%20our%20methodology%20and%20demonstrating%20that%20a%20binary%20time%20series%20model%20can%20anticipate%20these%20critical%20periods%20based%20on%20safety%20inspections.%20The%20proposed%20methodology%20converts%20routine%20safety%20inspection%20data%20into%20clear%20weekly%20risk%20scores%2C%20detecting%20the%20periods%20when%20accidents%20are%20most%20likely.%20Decision-makers%20can%20integrate%20these%20scores%20into%20their%20planning%20tools%20to%20classify%20inspection%20priorities%2C%20schedule%20targeted%20interventions%2C%20and%20funnel%20resources%20to%20the%20sites%20or%20shifts%20classified%20as%20highest%20risk%2C%20stepping%20in%20before%20incidents%20occur%20and%20getting%20the%20greatest%20return%20on%20safety%20investments.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520machine%2520learning%2520framework%2520for%2520occupational%2520accidents%2520forecasting%2520with%2520safety%2520inspections%2520integration%26entry.906535625%3DAho%2520Yapi%2520and%2520Pierre%2520Latouche%2520and%2520Arnaud%2520Guillin%2520and%2520Yan%2520Bailly%26entry.1292438233%3DWe%2520propose%2520a%2520generic%2520framework%2520for%2520short-term%2520occupational%2520accident%2520forecasting%2520that%2520leverages%2520safety%2520inspections%2520and%2520models%2520accident%2520occurrences%2520as%2520binary%2520time%2520series.%2520The%2520approach%2520generates%2520daily%2520predictions%252C%2520which%2520are%2520then%2520aggregated%2520into%2520weekly%2520safety%2520assessments%2520to%2520better%2520inform%2520decision%2520making.%2520To%2520ensure%2520the%2520reliability%2520and%2520operational%2520applicability%2520of%2520the%2520forecasts%252C%2520we%2520apply%2520a%2520sliding-window%2520cross-validation%2520procedure%2520specifically%2520designed%2520for%2520time%2520series%2520data%252C%2520combined%2520with%2520an%2520evaluation%2520based%2520on%2520aggregated%2520period-level%2520metrics.%2520Several%2520machine%2520learning%2520algorithms%252C%2520including%2520logistic%2520regression%252C%2520tree-based%2520models%252C%2520and%2520neural%2520networks%252C%2520are%2520trained%2520and%2520systematically%2520compared%2520within%2520this%2520framework.%2520Unlike%2520the%2520other%2520approaches%252C%2520the%2520long%2520short-term%2520memory%2520%2528LSTM%2529%2520network%2520outperforms%2520the%2520other%2520approaches%2520and%2520detects%2520the%2520upcoming%2520high-risk%2520periods%2520with%2520a%2520balanced%2520accuracy%2520of%25200.86%252C%2520confirming%2520the%2520robustness%2520of%2520our%2520methodology%2520and%2520demonstrating%2520that%2520a%2520binary%2520time%2520series%2520model%2520can%2520anticipate%2520these%2520critical%2520periods%2520based%2520on%2520safety%2520inspections.%2520The%2520proposed%2520methodology%2520converts%2520routine%2520safety%2520inspection%2520data%2520into%2520clear%2520weekly%2520risk%2520scores%252C%2520detecting%2520the%2520periods%2520when%2520accidents%2520are%2520most%2520likely.%2520Decision-makers%2520can%2520integrate%2520these%2520scores%2520into%2520their%2520planning%2520tools%2520to%2520classify%2520inspection%2520priorities%252C%2520schedule%2520targeted%2520interventions%252C%2520and%2520funnel%2520resources%2520to%2520the%2520sites%2520or%2520shifts%2520classified%2520as%2520highest%2520risk%252C%2520stepping%2520in%2520before%2520incidents%2520occur%2520and%2520getting%2520the%2520greatest%2520return%2520on%2520safety%2520investments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20machine%20learning%20framework%20for%20occupational%20accidents%20forecasting%20with%20safety%20inspections%20integration&entry.906535625=Aho%20Yapi%20and%20Pierre%20Latouche%20and%20Arnaud%20Guillin%20and%20Yan%20Bailly&entry.1292438233=We%20propose%20a%20generic%20framework%20for%20short-term%20occupational%20accident%20forecasting%20that%20leverages%20safety%20inspections%20and%20models%20accident%20occurrences%20as%20binary%20time%20series.%20The%20approach%20generates%20daily%20predictions%2C%20which%20are%20then%20aggregated%20into%20weekly%20safety%20assessments%20to%20better%20inform%20decision%20making.%20To%20ensure%20the%20reliability%20and%20operational%20applicability%20of%20the%20forecasts%2C%20we%20apply%20a%20sliding-window%20cross-validation%20procedure%20specifically%20designed%20for%20time%20series%20data%2C%20combined%20with%20an%20evaluation%20based%20on%20aggregated%20period-level%20metrics.%20Several%20machine%20learning%20algorithms%2C%20including%20logistic%20regression%2C%20tree-based%20models%2C%20and%20neural%20networks%2C%20are%20trained%20and%20systematically%20compared%20within%20this%20framework.%20Unlike%20the%20other%20approaches%2C%20the%20long%20short-term%20memory%20%28LSTM%29%20network%20outperforms%20the%20other%20approaches%20and%20detects%20the%20upcoming%20high-risk%20periods%20with%20a%20balanced%20accuracy%20of%200.86%2C%20confirming%20the%20robustness%20of%20our%20methodology%20and%20demonstrating%20that%20a%20binary%20time%20series%20model%20can%20anticipate%20these%20critical%20periods%20based%20on%20safety%20inspections.%20The%20proposed%20methodology%20converts%20routine%20safety%20inspection%20data%20into%20clear%20weekly%20risk%20scores%2C%20detecting%20the%20periods%20when%20accidents%20are%20most%20likely.%20Decision-makers%20can%20integrate%20these%20scores%20into%20their%20planning%20tools%20to%20classify%20inspection%20priorities%2C%20schedule%20targeted%20interventions%2C%20and%20funnel%20resources%20to%20the%20sites%20or%20shifts%20classified%20as%20highest%20risk%2C%20stepping%20in%20before%20incidents%20occur%20and%20getting%20the%20greatest%20return%20on%20safety%20investments.&entry.1838667208=http%3A//arxiv.org/abs/2507.00089v2&entry.124074799=Read"},
{"title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "author": "Grgur Kova\u010d and J\u00e9r\u00e9my Perez and R\u00e9my Portelas and Peter Ford Dominey and Pierre-Yves Oudeyer", "abstract": "Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data. Such loops were shown to lead to distribution shifts - models misrepresenting the true underlying distributions of human data (also called model collapse). However, how human data properties affect such shifts remains poorly understood. In this paper, we provide the first empirical examination of the effect of such properties on the outcome of recursive training. We first confirm that using different human datasets leads to distribution shifts of different magnitudes. Through exhaustive manipulation of dataset properties combined with regression analyses, we then identify a set of properties predicting distribution shift magnitudes. Lexical diversity is found to amplify these shifts, while semantic diversity and data quality mitigate them. Furthermore, we find that these influences are highly modular: data scrapped from a given internet domain has little influence on the content generated for another domain. Finally, experiments on political bias reveal that human data properties affect whether the initial bias will be amplified or reduced. Overall, our results portray a novel view, where different parts of internet may undergo different types of distribution shift.", "link": "http://arxiv.org/abs/2504.03814v6", "date": "2025-12-26", "relevancy": 1.8658, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Training%20Loops%20in%20LLMs%3A%20How%20training%20data%20properties%20modulate%20distribution%20shift%20in%20generated%20data%3F&body=Title%3A%20Recursive%20Training%20Loops%20in%20LLMs%3A%20How%20training%20data%20properties%20modulate%20distribution%20shift%20in%20generated%20data%3F%0AAuthor%3A%20Grgur%20Kova%C4%8D%20and%20J%C3%A9r%C3%A9my%20Perez%20and%20R%C3%A9my%20Portelas%20and%20Peter%20Ford%20Dominey%20and%20Pierre-Yves%20Oudeyer%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20in%20the%20creation%20of%20online%20content%2C%20creating%20feedback%20loops%20as%20subsequent%20generations%20of%20models%20will%20be%20trained%20on%20this%20synthetic%20data.%20Such%20loops%20were%20shown%20to%20lead%20to%20distribution%20shifts%20-%20models%20misrepresenting%20the%20true%20underlying%20distributions%20of%20human%20data%20%28also%20called%20model%20collapse%29.%20However%2C%20how%20human%20data%20properties%20affect%20such%20shifts%20remains%20poorly%20understood.%20In%20this%20paper%2C%20we%20provide%20the%20first%20empirical%20examination%20of%20the%20effect%20of%20such%20properties%20on%20the%20outcome%20of%20recursive%20training.%20We%20first%20confirm%20that%20using%20different%20human%20datasets%20leads%20to%20distribution%20shifts%20of%20different%20magnitudes.%20Through%20exhaustive%20manipulation%20of%20dataset%20properties%20combined%20with%20regression%20analyses%2C%20we%20then%20identify%20a%20set%20of%20properties%20predicting%20distribution%20shift%20magnitudes.%20Lexical%20diversity%20is%20found%20to%20amplify%20these%20shifts%2C%20while%20semantic%20diversity%20and%20data%20quality%20mitigate%20them.%20Furthermore%2C%20we%20find%20that%20these%20influences%20are%20highly%20modular%3A%20data%20scrapped%20from%20a%20given%20internet%20domain%20has%20little%20influence%20on%20the%20content%20generated%20for%20another%20domain.%20Finally%2C%20experiments%20on%20political%20bias%20reveal%20that%20human%20data%20properties%20affect%20whether%20the%20initial%20bias%20will%20be%20amplified%20or%20reduced.%20Overall%2C%20our%20results%20portray%20a%20novel%20view%2C%20where%20different%20parts%20of%20internet%20may%20undergo%20different%20types%20of%20distribution%20shift.%0ALink%3A%20http%3A//arxiv.org/abs/2504.03814v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Training%2520Loops%2520in%2520LLMs%253A%2520How%2520training%2520data%2520properties%2520modulate%2520distribution%2520shift%2520in%2520generated%2520data%253F%26entry.906535625%3DGrgur%2520Kova%25C4%258D%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Perez%2520and%2520R%25C3%25A9my%2520Portelas%2520and%2520Peter%2520Ford%2520Dominey%2520and%2520Pierre-Yves%2520Oudeyer%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520the%2520creation%2520of%2520online%2520content%252C%2520creating%2520feedback%2520loops%2520as%2520subsequent%2520generations%2520of%2520models%2520will%2520be%2520trained%2520on%2520this%2520synthetic%2520data.%2520Such%2520loops%2520were%2520shown%2520to%2520lead%2520to%2520distribution%2520shifts%2520-%2520models%2520misrepresenting%2520the%2520true%2520underlying%2520distributions%2520of%2520human%2520data%2520%2528also%2520called%2520model%2520collapse%2529.%2520However%252C%2520how%2520human%2520data%2520properties%2520affect%2520such%2520shifts%2520remains%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%2520first%2520empirical%2520examination%2520of%2520the%2520effect%2520of%2520such%2520properties%2520on%2520the%2520outcome%2520of%2520recursive%2520training.%2520We%2520first%2520confirm%2520that%2520using%2520different%2520human%2520datasets%2520leads%2520to%2520distribution%2520shifts%2520of%2520different%2520magnitudes.%2520Through%2520exhaustive%2520manipulation%2520of%2520dataset%2520properties%2520combined%2520with%2520regression%2520analyses%252C%2520we%2520then%2520identify%2520a%2520set%2520of%2520properties%2520predicting%2520distribution%2520shift%2520magnitudes.%2520Lexical%2520diversity%2520is%2520found%2520to%2520amplify%2520these%2520shifts%252C%2520while%2520semantic%2520diversity%2520and%2520data%2520quality%2520mitigate%2520them.%2520Furthermore%252C%2520we%2520find%2520that%2520these%2520influences%2520are%2520highly%2520modular%253A%2520data%2520scrapped%2520from%2520a%2520given%2520internet%2520domain%2520has%2520little%2520influence%2520on%2520the%2520content%2520generated%2520for%2520another%2520domain.%2520Finally%252C%2520experiments%2520on%2520political%2520bias%2520reveal%2520that%2520human%2520data%2520properties%2520affect%2520whether%2520the%2520initial%2520bias%2520will%2520be%2520amplified%2520or%2520reduced.%2520Overall%252C%2520our%2520results%2520portray%2520a%2520novel%2520view%252C%2520where%2520different%2520parts%2520of%2520internet%2520may%2520undergo%2520different%2520types%2520of%2520distribution%2520shift.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03814v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Training%20Loops%20in%20LLMs%3A%20How%20training%20data%20properties%20modulate%20distribution%20shift%20in%20generated%20data%3F&entry.906535625=Grgur%20Kova%C4%8D%20and%20J%C3%A9r%C3%A9my%20Perez%20and%20R%C3%A9my%20Portelas%20and%20Peter%20Ford%20Dominey%20and%20Pierre-Yves%20Oudeyer&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20in%20the%20creation%20of%20online%20content%2C%20creating%20feedback%20loops%20as%20subsequent%20generations%20of%20models%20will%20be%20trained%20on%20this%20synthetic%20data.%20Such%20loops%20were%20shown%20to%20lead%20to%20distribution%20shifts%20-%20models%20misrepresenting%20the%20true%20underlying%20distributions%20of%20human%20data%20%28also%20called%20model%20collapse%29.%20However%2C%20how%20human%20data%20properties%20affect%20such%20shifts%20remains%20poorly%20understood.%20In%20this%20paper%2C%20we%20provide%20the%20first%20empirical%20examination%20of%20the%20effect%20of%20such%20properties%20on%20the%20outcome%20of%20recursive%20training.%20We%20first%20confirm%20that%20using%20different%20human%20datasets%20leads%20to%20distribution%20shifts%20of%20different%20magnitudes.%20Through%20exhaustive%20manipulation%20of%20dataset%20properties%20combined%20with%20regression%20analyses%2C%20we%20then%20identify%20a%20set%20of%20properties%20predicting%20distribution%20shift%20magnitudes.%20Lexical%20diversity%20is%20found%20to%20amplify%20these%20shifts%2C%20while%20semantic%20diversity%20and%20data%20quality%20mitigate%20them.%20Furthermore%2C%20we%20find%20that%20these%20influences%20are%20highly%20modular%3A%20data%20scrapped%20from%20a%20given%20internet%20domain%20has%20little%20influence%20on%20the%20content%20generated%20for%20another%20domain.%20Finally%2C%20experiments%20on%20political%20bias%20reveal%20that%20human%20data%20properties%20affect%20whether%20the%20initial%20bias%20will%20be%20amplified%20or%20reduced.%20Overall%2C%20our%20results%20portray%20a%20novel%20view%2C%20where%20different%20parts%20of%20internet%20may%20undergo%20different%20types%20of%20distribution%20shift.&entry.1838667208=http%3A//arxiv.org/abs/2504.03814v6&entry.124074799=Read"},
{"title": "Sparse Hyperparametric Itakura-Saito Nonnegative Matrix Factorization via Bi-Level Optimization", "author": "Laura Selicato and Flavia Esposito and Andersen Ang and Nicoletta Del Buono and Rafal Zdunek", "abstract": "The selection of penalty hyperparameters is a critical aspect in Nonnegative Matrix Factorization (NMF), since these values control the trade-off between reconstruction accuracy and adherence to desired constraints. In this work, we focus on an NMF problem involving the Itakura-Saito (IS) divergence, which is particularly effective for extracting low spectral density components from spectrograms of mixed signals, and benefits from the introduction of sparsity constraints. We propose a new algorithm called SHINBO, which introduces a bi-level optimization framework to automatically and adaptively tune the row-dependent penalty hyperparameters, enhancing the ability of IS-NMF to isolate sparse, periodic signals in noisy environments. Experimental results demonstrate that SHINBO achieves accurate spectral decompositions and demonstrates superior performance in both synthetic and real-world applications. In the latter case, SHINBO is particularly useful for noninvasive vibration-based fault detection in rolling bearings, where the desired signal components often reside in high-frequency subbands but are obscured by stronger, spectrally broader noise. By addressing the critical issue of hyperparameter selection, SHINBO improves the state-of-the-art in signal recovery for complex, noise-dominated environments.", "link": "http://arxiv.org/abs/2502.17123v3", "date": "2025-12-26", "relevancy": 1.8538, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Hyperparametric%20Itakura-Saito%20Nonnegative%20Matrix%20Factorization%20via%20Bi-Level%20Optimization&body=Title%3A%20Sparse%20Hyperparametric%20Itakura-Saito%20Nonnegative%20Matrix%20Factorization%20via%20Bi-Level%20Optimization%0AAuthor%3A%20Laura%20Selicato%20and%20Flavia%20Esposito%20and%20Andersen%20Ang%20and%20Nicoletta%20Del%20Buono%20and%20Rafal%20Zdunek%0AAbstract%3A%20The%20selection%20of%20penalty%20hyperparameters%20is%20a%20critical%20aspect%20in%20Nonnegative%20Matrix%20Factorization%20%28NMF%29%2C%20since%20these%20values%20control%20the%20trade-off%20between%20reconstruction%20accuracy%20and%20adherence%20to%20desired%20constraints.%20In%20this%20work%2C%20we%20focus%20on%20an%20NMF%20problem%20involving%20the%20Itakura-Saito%20%28IS%29%20divergence%2C%20which%20is%20particularly%20effective%20for%20extracting%20low%20spectral%20density%20components%20from%20spectrograms%20of%20mixed%20signals%2C%20and%20benefits%20from%20the%20introduction%20of%20sparsity%20constraints.%20We%20propose%20a%20new%20algorithm%20called%20SHINBO%2C%20which%20introduces%20a%20bi-level%20optimization%20framework%20to%20automatically%20and%20adaptively%20tune%20the%20row-dependent%20penalty%20hyperparameters%2C%20enhancing%20the%20ability%20of%20IS-NMF%20to%20isolate%20sparse%2C%20periodic%20signals%20in%20noisy%20environments.%20Experimental%20results%20demonstrate%20that%20SHINBO%20achieves%20accurate%20spectral%20decompositions%20and%20demonstrates%20superior%20performance%20in%20both%20synthetic%20and%20real-world%20applications.%20In%20the%20latter%20case%2C%20SHINBO%20is%20particularly%20useful%20for%20noninvasive%20vibration-based%20fault%20detection%20in%20rolling%20bearings%2C%20where%20the%20desired%20signal%20components%20often%20reside%20in%20high-frequency%20subbands%20but%20are%20obscured%20by%20stronger%2C%20spectrally%20broader%20noise.%20By%20addressing%20the%20critical%20issue%20of%20hyperparameter%20selection%2C%20SHINBO%20improves%20the%20state-of-the-art%20in%20signal%20recovery%20for%20complex%2C%20noise-dominated%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17123v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Hyperparametric%2520Itakura-Saito%2520Nonnegative%2520Matrix%2520Factorization%2520via%2520Bi-Level%2520Optimization%26entry.906535625%3DLaura%2520Selicato%2520and%2520Flavia%2520Esposito%2520and%2520Andersen%2520Ang%2520and%2520Nicoletta%2520Del%2520Buono%2520and%2520Rafal%2520Zdunek%26entry.1292438233%3DThe%2520selection%2520of%2520penalty%2520hyperparameters%2520is%2520a%2520critical%2520aspect%2520in%2520Nonnegative%2520Matrix%2520Factorization%2520%2528NMF%2529%252C%2520since%2520these%2520values%2520control%2520the%2520trade-off%2520between%2520reconstruction%2520accuracy%2520and%2520adherence%2520to%2520desired%2520constraints.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520an%2520NMF%2520problem%2520involving%2520the%2520Itakura-Saito%2520%2528IS%2529%2520divergence%252C%2520which%2520is%2520particularly%2520effective%2520for%2520extracting%2520low%2520spectral%2520density%2520components%2520from%2520spectrograms%2520of%2520mixed%2520signals%252C%2520and%2520benefits%2520from%2520the%2520introduction%2520of%2520sparsity%2520constraints.%2520We%2520propose%2520a%2520new%2520algorithm%2520called%2520SHINBO%252C%2520which%2520introduces%2520a%2520bi-level%2520optimization%2520framework%2520to%2520automatically%2520and%2520adaptively%2520tune%2520the%2520row-dependent%2520penalty%2520hyperparameters%252C%2520enhancing%2520the%2520ability%2520of%2520IS-NMF%2520to%2520isolate%2520sparse%252C%2520periodic%2520signals%2520in%2520noisy%2520environments.%2520Experimental%2520results%2520demonstrate%2520that%2520SHINBO%2520achieves%2520accurate%2520spectral%2520decompositions%2520and%2520demonstrates%2520superior%2520performance%2520in%2520both%2520synthetic%2520and%2520real-world%2520applications.%2520In%2520the%2520latter%2520case%252C%2520SHINBO%2520is%2520particularly%2520useful%2520for%2520noninvasive%2520vibration-based%2520fault%2520detection%2520in%2520rolling%2520bearings%252C%2520where%2520the%2520desired%2520signal%2520components%2520often%2520reside%2520in%2520high-frequency%2520subbands%2520but%2520are%2520obscured%2520by%2520stronger%252C%2520spectrally%2520broader%2520noise.%2520By%2520addressing%2520the%2520critical%2520issue%2520of%2520hyperparameter%2520selection%252C%2520SHINBO%2520improves%2520the%2520state-of-the-art%2520in%2520signal%2520recovery%2520for%2520complex%252C%2520noise-dominated%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17123v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Hyperparametric%20Itakura-Saito%20Nonnegative%20Matrix%20Factorization%20via%20Bi-Level%20Optimization&entry.906535625=Laura%20Selicato%20and%20Flavia%20Esposito%20and%20Andersen%20Ang%20and%20Nicoletta%20Del%20Buono%20and%20Rafal%20Zdunek&entry.1292438233=The%20selection%20of%20penalty%20hyperparameters%20is%20a%20critical%20aspect%20in%20Nonnegative%20Matrix%20Factorization%20%28NMF%29%2C%20since%20these%20values%20control%20the%20trade-off%20between%20reconstruction%20accuracy%20and%20adherence%20to%20desired%20constraints.%20In%20this%20work%2C%20we%20focus%20on%20an%20NMF%20problem%20involving%20the%20Itakura-Saito%20%28IS%29%20divergence%2C%20which%20is%20particularly%20effective%20for%20extracting%20low%20spectral%20density%20components%20from%20spectrograms%20of%20mixed%20signals%2C%20and%20benefits%20from%20the%20introduction%20of%20sparsity%20constraints.%20We%20propose%20a%20new%20algorithm%20called%20SHINBO%2C%20which%20introduces%20a%20bi-level%20optimization%20framework%20to%20automatically%20and%20adaptively%20tune%20the%20row-dependent%20penalty%20hyperparameters%2C%20enhancing%20the%20ability%20of%20IS-NMF%20to%20isolate%20sparse%2C%20periodic%20signals%20in%20noisy%20environments.%20Experimental%20results%20demonstrate%20that%20SHINBO%20achieves%20accurate%20spectral%20decompositions%20and%20demonstrates%20superior%20performance%20in%20both%20synthetic%20and%20real-world%20applications.%20In%20the%20latter%20case%2C%20SHINBO%20is%20particularly%20useful%20for%20noninvasive%20vibration-based%20fault%20detection%20in%20rolling%20bearings%2C%20where%20the%20desired%20signal%20components%20often%20reside%20in%20high-frequency%20subbands%20but%20are%20obscured%20by%20stronger%2C%20spectrally%20broader%20noise.%20By%20addressing%20the%20critical%20issue%20of%20hyperparameter%20selection%2C%20SHINBO%20improves%20the%20state-of-the-art%20in%20signal%20recovery%20for%20complex%2C%20noise-dominated%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2502.17123v3&entry.124074799=Read"},
{"title": "DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction", "author": "Aicha Boutorh and Soumia Bouyahiaoui and Sara Belhadj and Nour El Yakine Guendouz and Manel Kara Laouar", "abstract": "Predicting the binding affinity between antigens and antibodies is fundamental to drug discovery and vaccine development. Traditional computational approaches often rely on experimentally determined 3D structures, which are scarce and computationally expensive to obtain. This paper introduces DuaDeep-SeqAffinity, a novel sequence-only deep learning framework that predicts affinity scores solely from their amino acid sequences using a dual-stream hybrid architecture. Our approach leverages pre-trained ESM-2 protein language model embeddings, combining 1D Convolutional Neural Networks (CNNs) for local motif detection with Transformer encoders for global contextual representation. A subsequent fusion module integrates these multi-faceted features, which are then passed to a fully connected network for final score regression. Experimental results demonstrate that DuaDeep-SeqAffinity significantly outperforms individual architectural components and existing state-of-the-art (SOTA) methods. DuaDeep achieved a superior Pearson correlation of 0.688, an R^2 of 0.460, and a Root Mean Square Error (RMSE) of 0.737, surpassing single-branch variants ESM-CNN and ESM-Transformer. Notably, the model achieved an Area Under the Curve (AUC) of 0.890, outperforming sequence-only benchmarks and even surpassing structure-sequence hybrid models. These findings prove that high-fidelity sequence embeddings can capture essential binding patterns typically reserved for structural modeling. By eliminating the reliance on 3D structures, DuaDeep-SeqAffinity provides a highly scalable and efficient solution for high-throughput screening of vast sequence libraries, significantly accelerating the therapeutic discovery pipeline.", "link": "http://arxiv.org/abs/2512.22007v1", "date": "2025-12-26", "relevancy": 1.8282, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuaDeep-SeqAffinity%3A%20Dual-Stream%20Deep%20Learning%20Framework%20for%20Sequence-Only%20Antigen-Antibody%20Affinity%20Prediction&body=Title%3A%20DuaDeep-SeqAffinity%3A%20Dual-Stream%20Deep%20Learning%20Framework%20for%20Sequence-Only%20Antigen-Antibody%20Affinity%20Prediction%0AAuthor%3A%20Aicha%20Boutorh%20and%20Soumia%20Bouyahiaoui%20and%20Sara%20Belhadj%20and%20Nour%20El%20Yakine%20Guendouz%20and%20Manel%20Kara%20Laouar%0AAbstract%3A%20Predicting%20the%20binding%20affinity%20between%20antigens%20and%20antibodies%20is%20fundamental%20to%20drug%20discovery%20and%20vaccine%20development.%20Traditional%20computational%20approaches%20often%20rely%20on%20experimentally%20determined%203D%20structures%2C%20which%20are%20scarce%20and%20computationally%20expensive%20to%20obtain.%20This%20paper%20introduces%20DuaDeep-SeqAffinity%2C%20a%20novel%20sequence-only%20deep%20learning%20framework%20that%20predicts%20affinity%20scores%20solely%20from%20their%20amino%20acid%20sequences%20using%20a%20dual-stream%20hybrid%20architecture.%20Our%20approach%20leverages%20pre-trained%20ESM-2%20protein%20language%20model%20embeddings%2C%20combining%201D%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%20local%20motif%20detection%20with%20Transformer%20encoders%20for%20global%20contextual%20representation.%20A%20subsequent%20fusion%20module%20integrates%20these%20multi-faceted%20features%2C%20which%20are%20then%20passed%20to%20a%20fully%20connected%20network%20for%20final%20score%20regression.%20Experimental%20results%20demonstrate%20that%20DuaDeep-SeqAffinity%20significantly%20outperforms%20individual%20architectural%20components%20and%20existing%20state-of-the-art%20%28SOTA%29%20methods.%20DuaDeep%20achieved%20a%20superior%20Pearson%20correlation%20of%200.688%2C%20an%20R%5E2%20of%200.460%2C%20and%20a%20Root%20Mean%20Square%20Error%20%28RMSE%29%20of%200.737%2C%20surpassing%20single-branch%20variants%20ESM-CNN%20and%20ESM-Transformer.%20Notably%2C%20the%20model%20achieved%20an%20Area%20Under%20the%20Curve%20%28AUC%29%20of%200.890%2C%20outperforming%20sequence-only%20benchmarks%20and%20even%20surpassing%20structure-sequence%20hybrid%20models.%20These%20findings%20prove%20that%20high-fidelity%20sequence%20embeddings%20can%20capture%20essential%20binding%20patterns%20typically%20reserved%20for%20structural%20modeling.%20By%20eliminating%20the%20reliance%20on%203D%20structures%2C%20DuaDeep-SeqAffinity%20provides%20a%20highly%20scalable%20and%20efficient%20solution%20for%20high-throughput%20screening%20of%20vast%20sequence%20libraries%2C%20significantly%20accelerating%20the%20therapeutic%20discovery%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuaDeep-SeqAffinity%253A%2520Dual-Stream%2520Deep%2520Learning%2520Framework%2520for%2520Sequence-Only%2520Antigen-Antibody%2520Affinity%2520Prediction%26entry.906535625%3DAicha%2520Boutorh%2520and%2520Soumia%2520Bouyahiaoui%2520and%2520Sara%2520Belhadj%2520and%2520Nour%2520El%2520Yakine%2520Guendouz%2520and%2520Manel%2520Kara%2520Laouar%26entry.1292438233%3DPredicting%2520the%2520binding%2520affinity%2520between%2520antigens%2520and%2520antibodies%2520is%2520fundamental%2520to%2520drug%2520discovery%2520and%2520vaccine%2520development.%2520Traditional%2520computational%2520approaches%2520often%2520rely%2520on%2520experimentally%2520determined%25203D%2520structures%252C%2520which%2520are%2520scarce%2520and%2520computationally%2520expensive%2520to%2520obtain.%2520This%2520paper%2520introduces%2520DuaDeep-SeqAffinity%252C%2520a%2520novel%2520sequence-only%2520deep%2520learning%2520framework%2520that%2520predicts%2520affinity%2520scores%2520solely%2520from%2520their%2520amino%2520acid%2520sequences%2520using%2520a%2520dual-stream%2520hybrid%2520architecture.%2520Our%2520approach%2520leverages%2520pre-trained%2520ESM-2%2520protein%2520language%2520model%2520embeddings%252C%2520combining%25201D%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520for%2520local%2520motif%2520detection%2520with%2520Transformer%2520encoders%2520for%2520global%2520contextual%2520representation.%2520A%2520subsequent%2520fusion%2520module%2520integrates%2520these%2520multi-faceted%2520features%252C%2520which%2520are%2520then%2520passed%2520to%2520a%2520fully%2520connected%2520network%2520for%2520final%2520score%2520regression.%2520Experimental%2520results%2520demonstrate%2520that%2520DuaDeep-SeqAffinity%2520significantly%2520outperforms%2520individual%2520architectural%2520components%2520and%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520DuaDeep%2520achieved%2520a%2520superior%2520Pearson%2520correlation%2520of%25200.688%252C%2520an%2520R%255E2%2520of%25200.460%252C%2520and%2520a%2520Root%2520Mean%2520Square%2520Error%2520%2528RMSE%2529%2520of%25200.737%252C%2520surpassing%2520single-branch%2520variants%2520ESM-CNN%2520and%2520ESM-Transformer.%2520Notably%252C%2520the%2520model%2520achieved%2520an%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520of%25200.890%252C%2520outperforming%2520sequence-only%2520benchmarks%2520and%2520even%2520surpassing%2520structure-sequence%2520hybrid%2520models.%2520These%2520findings%2520prove%2520that%2520high-fidelity%2520sequence%2520embeddings%2520can%2520capture%2520essential%2520binding%2520patterns%2520typically%2520reserved%2520for%2520structural%2520modeling.%2520By%2520eliminating%2520the%2520reliance%2520on%25203D%2520structures%252C%2520DuaDeep-SeqAffinity%2520provides%2520a%2520highly%2520scalable%2520and%2520efficient%2520solution%2520for%2520high-throughput%2520screening%2520of%2520vast%2520sequence%2520libraries%252C%2520significantly%2520accelerating%2520the%2520therapeutic%2520discovery%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuaDeep-SeqAffinity%3A%20Dual-Stream%20Deep%20Learning%20Framework%20for%20Sequence-Only%20Antigen-Antibody%20Affinity%20Prediction&entry.906535625=Aicha%20Boutorh%20and%20Soumia%20Bouyahiaoui%20and%20Sara%20Belhadj%20and%20Nour%20El%20Yakine%20Guendouz%20and%20Manel%20Kara%20Laouar&entry.1292438233=Predicting%20the%20binding%20affinity%20between%20antigens%20and%20antibodies%20is%20fundamental%20to%20drug%20discovery%20and%20vaccine%20development.%20Traditional%20computational%20approaches%20often%20rely%20on%20experimentally%20determined%203D%20structures%2C%20which%20are%20scarce%20and%20computationally%20expensive%20to%20obtain.%20This%20paper%20introduces%20DuaDeep-SeqAffinity%2C%20a%20novel%20sequence-only%20deep%20learning%20framework%20that%20predicts%20affinity%20scores%20solely%20from%20their%20amino%20acid%20sequences%20using%20a%20dual-stream%20hybrid%20architecture.%20Our%20approach%20leverages%20pre-trained%20ESM-2%20protein%20language%20model%20embeddings%2C%20combining%201D%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%20local%20motif%20detection%20with%20Transformer%20encoders%20for%20global%20contextual%20representation.%20A%20subsequent%20fusion%20module%20integrates%20these%20multi-faceted%20features%2C%20which%20are%20then%20passed%20to%20a%20fully%20connected%20network%20for%20final%20score%20regression.%20Experimental%20results%20demonstrate%20that%20DuaDeep-SeqAffinity%20significantly%20outperforms%20individual%20architectural%20components%20and%20existing%20state-of-the-art%20%28SOTA%29%20methods.%20DuaDeep%20achieved%20a%20superior%20Pearson%20correlation%20of%200.688%2C%20an%20R%5E2%20of%200.460%2C%20and%20a%20Root%20Mean%20Square%20Error%20%28RMSE%29%20of%200.737%2C%20surpassing%20single-branch%20variants%20ESM-CNN%20and%20ESM-Transformer.%20Notably%2C%20the%20model%20achieved%20an%20Area%20Under%20the%20Curve%20%28AUC%29%20of%200.890%2C%20outperforming%20sequence-only%20benchmarks%20and%20even%20surpassing%20structure-sequence%20hybrid%20models.%20These%20findings%20prove%20that%20high-fidelity%20sequence%20embeddings%20can%20capture%20essential%20binding%20patterns%20typically%20reserved%20for%20structural%20modeling.%20By%20eliminating%20the%20reliance%20on%203D%20structures%2C%20DuaDeep-SeqAffinity%20provides%20a%20highly%20scalable%20and%20efficient%20solution%20for%20high-throughput%20screening%20of%20vast%20sequence%20libraries%2C%20significantly%20accelerating%20the%20therapeutic%20discovery%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2512.22007v1&entry.124074799=Read"},
{"title": "Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles", "author": "Hanwen Zhang and Ruichen Zhang and Wei Zhang and Dusit Niyato and Yonggang Wen and Chunyan Miao", "abstract": "The energy optimization and demand side management (DSM) of Internet of Things (IoT)-enabled microgrids are being transformed by generative artificial intelligence, such as large language models (LLMs). This paper explores the integration of LLMs into energy management, and emphasizes their roles in automating the optimization of DSM strategies with Internet of Electric Vehicles (IoEV) as a representative example of the Internet of Vehicles (IoV). We investigate challenges and solutions associated with DSM and explore the new opportunities presented by leveraging LLMs. Then, we propose an innovative solution that enhances LLMs with retrieval-augmented generation for automatic problem formulation, code generation, and customizing optimization. The results demonstrate the effectiveness of our proposed solution in charging scheduling and optimization for electric vehicles, and highlight our solution's significant advancements in energy efficiency and user adaptability. This work shows LLMs' potential in energy optimization of the IoT-enabled microgrids and promotes intelligent DSM solutions.", "link": "http://arxiv.org/abs/2501.15544v5", "date": "2025-12-26", "relevancy": 1.8195, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4569}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4543}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Generative%20Artificial%20Intelligence%20and%20Large%20Language%20Models%20for%20Demand%20Side%20Management%20with%20Internet%20of%20Electric%20Vehicles&body=Title%3A%20Advancing%20Generative%20Artificial%20Intelligence%20and%20Large%20Language%20Models%20for%20Demand%20Side%20Management%20with%20Internet%20of%20Electric%20Vehicles%0AAuthor%3A%20Hanwen%20Zhang%20and%20Ruichen%20Zhang%20and%20Wei%20Zhang%20and%20Dusit%20Niyato%20and%20Yonggang%20Wen%20and%20Chunyan%20Miao%0AAbstract%3A%20The%20energy%20optimization%20and%20demand%20side%20management%20%28DSM%29%20of%20Internet%20of%20Things%20%28IoT%29-enabled%20microgrids%20are%20being%20transformed%20by%20generative%20artificial%20intelligence%2C%20such%20as%20large%20language%20models%20%28LLMs%29.%20This%20paper%20explores%20the%20integration%20of%20LLMs%20into%20energy%20management%2C%20and%20emphasizes%20their%20roles%20in%20automating%20the%20optimization%20of%20DSM%20strategies%20with%20Internet%20of%20Electric%20Vehicles%20%28IoEV%29%20as%20a%20representative%20example%20of%20the%20Internet%20of%20Vehicles%20%28IoV%29.%20We%20investigate%20challenges%20and%20solutions%20associated%20with%20DSM%20and%20explore%20the%20new%20opportunities%20presented%20by%20leveraging%20LLMs.%20Then%2C%20we%20propose%20an%20innovative%20solution%20that%20enhances%20LLMs%20with%20retrieval-augmented%20generation%20for%20automatic%20problem%20formulation%2C%20code%20generation%2C%20and%20customizing%20optimization.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20solution%20in%20charging%20scheduling%20and%20optimization%20for%20electric%20vehicles%2C%20and%20highlight%20our%20solution%27s%20significant%20advancements%20in%20energy%20efficiency%20and%20user%20adaptability.%20This%20work%20shows%20LLMs%27%20potential%20in%20energy%20optimization%20of%20the%20IoT-enabled%20microgrids%20and%20promotes%20intelligent%20DSM%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2501.15544v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Generative%2520Artificial%2520Intelligence%2520and%2520Large%2520Language%2520Models%2520for%2520Demand%2520Side%2520Management%2520with%2520Internet%2520of%2520Electric%2520Vehicles%26entry.906535625%3DHanwen%2520Zhang%2520and%2520Ruichen%2520Zhang%2520and%2520Wei%2520Zhang%2520and%2520Dusit%2520Niyato%2520and%2520Yonggang%2520Wen%2520and%2520Chunyan%2520Miao%26entry.1292438233%3DThe%2520energy%2520optimization%2520and%2520demand%2520side%2520management%2520%2528DSM%2529%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529-enabled%2520microgrids%2520are%2520being%2520transformed%2520by%2520generative%2520artificial%2520intelligence%252C%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520paper%2520explores%2520the%2520integration%2520of%2520LLMs%2520into%2520energy%2520management%252C%2520and%2520emphasizes%2520their%2520roles%2520in%2520automating%2520the%2520optimization%2520of%2520DSM%2520strategies%2520with%2520Internet%2520of%2520Electric%2520Vehicles%2520%2528IoEV%2529%2520as%2520a%2520representative%2520example%2520of%2520the%2520Internet%2520of%2520Vehicles%2520%2528IoV%2529.%2520We%2520investigate%2520challenges%2520and%2520solutions%2520associated%2520with%2520DSM%2520and%2520explore%2520the%2520new%2520opportunities%2520presented%2520by%2520leveraging%2520LLMs.%2520Then%252C%2520we%2520propose%2520an%2520innovative%2520solution%2520that%2520enhances%2520LLMs%2520with%2520retrieval-augmented%2520generation%2520for%2520automatic%2520problem%2520formulation%252C%2520code%2520generation%252C%2520and%2520customizing%2520optimization.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520solution%2520in%2520charging%2520scheduling%2520and%2520optimization%2520for%2520electric%2520vehicles%252C%2520and%2520highlight%2520our%2520solution%2527s%2520significant%2520advancements%2520in%2520energy%2520efficiency%2520and%2520user%2520adaptability.%2520This%2520work%2520shows%2520LLMs%2527%2520potential%2520in%2520energy%2520optimization%2520of%2520the%2520IoT-enabled%2520microgrids%2520and%2520promotes%2520intelligent%2520DSM%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15544v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Generative%20Artificial%20Intelligence%20and%20Large%20Language%20Models%20for%20Demand%20Side%20Management%20with%20Internet%20of%20Electric%20Vehicles&entry.906535625=Hanwen%20Zhang%20and%20Ruichen%20Zhang%20and%20Wei%20Zhang%20and%20Dusit%20Niyato%20and%20Yonggang%20Wen%20and%20Chunyan%20Miao&entry.1292438233=The%20energy%20optimization%20and%20demand%20side%20management%20%28DSM%29%20of%20Internet%20of%20Things%20%28IoT%29-enabled%20microgrids%20are%20being%20transformed%20by%20generative%20artificial%20intelligence%2C%20such%20as%20large%20language%20models%20%28LLMs%29.%20This%20paper%20explores%20the%20integration%20of%20LLMs%20into%20energy%20management%2C%20and%20emphasizes%20their%20roles%20in%20automating%20the%20optimization%20of%20DSM%20strategies%20with%20Internet%20of%20Electric%20Vehicles%20%28IoEV%29%20as%20a%20representative%20example%20of%20the%20Internet%20of%20Vehicles%20%28IoV%29.%20We%20investigate%20challenges%20and%20solutions%20associated%20with%20DSM%20and%20explore%20the%20new%20opportunities%20presented%20by%20leveraging%20LLMs.%20Then%2C%20we%20propose%20an%20innovative%20solution%20that%20enhances%20LLMs%20with%20retrieval-augmented%20generation%20for%20automatic%20problem%20formulation%2C%20code%20generation%2C%20and%20customizing%20optimization.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20solution%20in%20charging%20scheduling%20and%20optimization%20for%20electric%20vehicles%2C%20and%20highlight%20our%20solution%27s%20significant%20advancements%20in%20energy%20efficiency%20and%20user%20adaptability.%20This%20work%20shows%20LLMs%27%20potential%20in%20energy%20optimization%20of%20the%20IoT-enabled%20microgrids%20and%20promotes%20intelligent%20DSM%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2501.15544v5&entry.124074799=Read"},
{"title": "Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks", "author": "Zubair Shah and Noaman Khan", "abstract": "Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.", "link": "http://arxiv.org/abs/2512.22106v1", "date": "2025-12-26", "relevancy": 1.7921, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4505}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20as%20a%20Game%3A%20Equilibrium-Driven%20Sparsification%20of%20Neural%20Networks&body=Title%3A%20Pruning%20as%20a%20Game%3A%20Equilibrium-Driven%20Sparsification%20of%20Neural%20Networks%0AAuthor%3A%20Zubair%20Shah%20and%20Noaman%20Khan%0AAbstract%3A%20Neural%20network%20pruning%20is%20widely%20used%20to%20reduce%20model%20size%20and%20computational%20cost.%20Yet%2C%20most%20existing%20methods%20treat%20sparsity%20as%20an%20externally%20imposed%20constraint%2C%20enforced%20through%20heuristic%20importance%20scores%20or%20training-time%20regularization.%20In%20this%20work%2C%20we%20propose%20a%20fundamentally%20different%20perspective%3A%20pruning%20as%20an%20equilibrium%20outcome%20of%20strategic%20interaction%20among%20model%20components.%20We%20model%20parameter%20groups%20such%20as%20weights%2C%20neurons%2C%20or%20filters%20as%20players%20in%20a%20continuous%20non-cooperative%20game%2C%20where%20each%20player%20selects%20its%20level%20of%20participation%20in%20the%20network%20to%20balance%20contribution%20against%20redundancy%20and%20competition.%20Within%20this%20formulation%2C%20sparsity%20emerges%20naturally%20when%20continued%20participation%20becomes%20a%20dominated%20strategy%20at%20equilibrium.%20We%20analyze%20the%20resulting%20game%20and%20show%20that%20dominated%20players%20collapse%20to%20zero%20participation%20under%20mild%20conditions%2C%20providing%20a%20principled%20explanation%20for%20pruning%20behavior.%20Building%20on%20this%20insight%2C%20we%20derive%20a%20simple%20equilibrium-driven%20pruning%20algorithm%20that%20jointly%20updates%20network%20parameters%20and%20participation%20variables%20without%20relying%20on%20explicit%20importance%20scores.%20This%20work%20focuses%20on%20establishing%20a%20principled%20formulation%20and%20empirical%20validation%20of%20pruning%20as%20an%20equilibrium%20phenomenon%2C%20rather%20than%20exhaustive%20architectural%20or%20large-scale%20benchmarking.%20Experiments%20on%20standard%20benchmarks%20demonstrate%20that%20the%20proposed%20approach%20achieves%20competitive%20sparsity-accuracy%20trade-offs%20while%20offering%20an%20interpretable%2C%20theory-grounded%20alternative%20to%20existing%20pruning%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520as%2520a%2520Game%253A%2520Equilibrium-Driven%2520Sparsification%2520of%2520Neural%2520Networks%26entry.906535625%3DZubair%2520Shah%2520and%2520Noaman%2520Khan%26entry.1292438233%3DNeural%2520network%2520pruning%2520is%2520widely%2520used%2520to%2520reduce%2520model%2520size%2520and%2520computational%2520cost.%2520Yet%252C%2520most%2520existing%2520methods%2520treat%2520sparsity%2520as%2520an%2520externally%2520imposed%2520constraint%252C%2520enforced%2520through%2520heuristic%2520importance%2520scores%2520or%2520training-time%2520regularization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fundamentally%2520different%2520perspective%253A%2520pruning%2520as%2520an%2520equilibrium%2520outcome%2520of%2520strategic%2520interaction%2520among%2520model%2520components.%2520We%2520model%2520parameter%2520groups%2520such%2520as%2520weights%252C%2520neurons%252C%2520or%2520filters%2520as%2520players%2520in%2520a%2520continuous%2520non-cooperative%2520game%252C%2520where%2520each%2520player%2520selects%2520its%2520level%2520of%2520participation%2520in%2520the%2520network%2520to%2520balance%2520contribution%2520against%2520redundancy%2520and%2520competition.%2520Within%2520this%2520formulation%252C%2520sparsity%2520emerges%2520naturally%2520when%2520continued%2520participation%2520becomes%2520a%2520dominated%2520strategy%2520at%2520equilibrium.%2520We%2520analyze%2520the%2520resulting%2520game%2520and%2520show%2520that%2520dominated%2520players%2520collapse%2520to%2520zero%2520participation%2520under%2520mild%2520conditions%252C%2520providing%2520a%2520principled%2520explanation%2520for%2520pruning%2520behavior.%2520Building%2520on%2520this%2520insight%252C%2520we%2520derive%2520a%2520simple%2520equilibrium-driven%2520pruning%2520algorithm%2520that%2520jointly%2520updates%2520network%2520parameters%2520and%2520participation%2520variables%2520without%2520relying%2520on%2520explicit%2520importance%2520scores.%2520This%2520work%2520focuses%2520on%2520establishing%2520a%2520principled%2520formulation%2520and%2520empirical%2520validation%2520of%2520pruning%2520as%2520an%2520equilibrium%2520phenomenon%252C%2520rather%2520than%2520exhaustive%2520architectural%2520or%2520large-scale%2520benchmarking.%2520Experiments%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520achieves%2520competitive%2520sparsity-accuracy%2520trade-offs%2520while%2520offering%2520an%2520interpretable%252C%2520theory-grounded%2520alternative%2520to%2520existing%2520pruning%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20as%20a%20Game%3A%20Equilibrium-Driven%20Sparsification%20of%20Neural%20Networks&entry.906535625=Zubair%20Shah%20and%20Noaman%20Khan&entry.1292438233=Neural%20network%20pruning%20is%20widely%20used%20to%20reduce%20model%20size%20and%20computational%20cost.%20Yet%2C%20most%20existing%20methods%20treat%20sparsity%20as%20an%20externally%20imposed%20constraint%2C%20enforced%20through%20heuristic%20importance%20scores%20or%20training-time%20regularization.%20In%20this%20work%2C%20we%20propose%20a%20fundamentally%20different%20perspective%3A%20pruning%20as%20an%20equilibrium%20outcome%20of%20strategic%20interaction%20among%20model%20components.%20We%20model%20parameter%20groups%20such%20as%20weights%2C%20neurons%2C%20or%20filters%20as%20players%20in%20a%20continuous%20non-cooperative%20game%2C%20where%20each%20player%20selects%20its%20level%20of%20participation%20in%20the%20network%20to%20balance%20contribution%20against%20redundancy%20and%20competition.%20Within%20this%20formulation%2C%20sparsity%20emerges%20naturally%20when%20continued%20participation%20becomes%20a%20dominated%20strategy%20at%20equilibrium.%20We%20analyze%20the%20resulting%20game%20and%20show%20that%20dominated%20players%20collapse%20to%20zero%20participation%20under%20mild%20conditions%2C%20providing%20a%20principled%20explanation%20for%20pruning%20behavior.%20Building%20on%20this%20insight%2C%20we%20derive%20a%20simple%20equilibrium-driven%20pruning%20algorithm%20that%20jointly%20updates%20network%20parameters%20and%20participation%20variables%20without%20relying%20on%20explicit%20importance%20scores.%20This%20work%20focuses%20on%20establishing%20a%20principled%20formulation%20and%20empirical%20validation%20of%20pruning%20as%20an%20equilibrium%20phenomenon%2C%20rather%20than%20exhaustive%20architectural%20or%20large-scale%20benchmarking.%20Experiments%20on%20standard%20benchmarks%20demonstrate%20that%20the%20proposed%20approach%20achieves%20competitive%20sparsity-accuracy%20trade-offs%20while%20offering%20an%20interpretable%2C%20theory-grounded%20alternative%20to%20existing%20pruning%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.22106v1&entry.124074799=Read"},
{"title": "A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models", "author": "John M. Mango and Ronald Katende", "abstract": "We consider the problem of restoring linear conservation laws in data-driven linear dynamical models. Given a learned operator $\\widehat{A}$ and a full-rank constraint matrix $C$ encoding one or more invariants, we show that the matrix closest to $\\widehat{A}$ in the Frobenius norm and satisfying $C^\\top A = 0$ is the orthogonal projection $A^\\star = \\widehat{A} - C(C^\\top C)^{-1}C^\\top \\widehat{A}$. This correction is uniquely defined, low rank and fully determined by the violation $C^\\top \\widehat{A}$. In the single-invariant case it reduces to a rank-one update. We prove that $A^\\star$ enforces exact conservation while minimally perturbing the dynamics, and we verify these properties numerically on a Markov-type example. The projection provides an elementary and general mechanism for embedding exact invariants into any learned linear model.", "link": "http://arxiv.org/abs/2512.22084v1", "date": "2025-12-26", "relevancy": 1.7868, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4515}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4482}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Frobenius-Optimal%20Projection%20for%20Enforcing%20Linear%20Conservation%20in%20Learned%20Dynamical%20Models&body=Title%3A%20A%20Frobenius-Optimal%20Projection%20for%20Enforcing%20Linear%20Conservation%20in%20Learned%20Dynamical%20Models%0AAuthor%3A%20John%20M.%20Mango%20and%20Ronald%20Katende%0AAbstract%3A%20We%20consider%20the%20problem%20of%20restoring%20linear%20conservation%20laws%20in%20data-driven%20linear%20dynamical%20models.%20Given%20a%20learned%20operator%20%24%5Cwidehat%7BA%7D%24%20and%20a%20full-rank%20constraint%20matrix%20%24C%24%20encoding%20one%20or%20more%20invariants%2C%20we%20show%20that%20the%20matrix%20closest%20to%20%24%5Cwidehat%7BA%7D%24%20in%20the%20Frobenius%20norm%20and%20satisfying%20%24C%5E%5Ctop%20A%20%3D%200%24%20is%20the%20orthogonal%20projection%20%24A%5E%5Cstar%20%3D%20%5Cwidehat%7BA%7D%20-%20C%28C%5E%5Ctop%20C%29%5E%7B-1%7DC%5E%5Ctop%20%5Cwidehat%7BA%7D%24.%20This%20correction%20is%20uniquely%20defined%2C%20low%20rank%20and%20fully%20determined%20by%20the%20violation%20%24C%5E%5Ctop%20%5Cwidehat%7BA%7D%24.%20In%20the%20single-invariant%20case%20it%20reduces%20to%20a%20rank-one%20update.%20We%20prove%20that%20%24A%5E%5Cstar%24%20enforces%20exact%20conservation%20while%20minimally%20perturbing%20the%20dynamics%2C%20and%20we%20verify%20these%20properties%20numerically%20on%20a%20Markov-type%20example.%20The%20projection%20provides%20an%20elementary%20and%20general%20mechanism%20for%20embedding%20exact%20invariants%20into%20any%20learned%20linear%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Frobenius-Optimal%2520Projection%2520for%2520Enforcing%2520Linear%2520Conservation%2520in%2520Learned%2520Dynamical%2520Models%26entry.906535625%3DJohn%2520M.%2520Mango%2520and%2520Ronald%2520Katende%26entry.1292438233%3DWe%2520consider%2520the%2520problem%2520of%2520restoring%2520linear%2520conservation%2520laws%2520in%2520data-driven%2520linear%2520dynamical%2520models.%2520Given%2520a%2520learned%2520operator%2520%2524%255Cwidehat%257BA%257D%2524%2520and%2520a%2520full-rank%2520constraint%2520matrix%2520%2524C%2524%2520encoding%2520one%2520or%2520more%2520invariants%252C%2520we%2520show%2520that%2520the%2520matrix%2520closest%2520to%2520%2524%255Cwidehat%257BA%257D%2524%2520in%2520the%2520Frobenius%2520norm%2520and%2520satisfying%2520%2524C%255E%255Ctop%2520A%2520%253D%25200%2524%2520is%2520the%2520orthogonal%2520projection%2520%2524A%255E%255Cstar%2520%253D%2520%255Cwidehat%257BA%257D%2520-%2520C%2528C%255E%255Ctop%2520C%2529%255E%257B-1%257DC%255E%255Ctop%2520%255Cwidehat%257BA%257D%2524.%2520This%2520correction%2520is%2520uniquely%2520defined%252C%2520low%2520rank%2520and%2520fully%2520determined%2520by%2520the%2520violation%2520%2524C%255E%255Ctop%2520%255Cwidehat%257BA%257D%2524.%2520In%2520the%2520single-invariant%2520case%2520it%2520reduces%2520to%2520a%2520rank-one%2520update.%2520We%2520prove%2520that%2520%2524A%255E%255Cstar%2524%2520enforces%2520exact%2520conservation%2520while%2520minimally%2520perturbing%2520the%2520dynamics%252C%2520and%2520we%2520verify%2520these%2520properties%2520numerically%2520on%2520a%2520Markov-type%2520example.%2520The%2520projection%2520provides%2520an%2520elementary%2520and%2520general%2520mechanism%2520for%2520embedding%2520exact%2520invariants%2520into%2520any%2520learned%2520linear%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Frobenius-Optimal%20Projection%20for%20Enforcing%20Linear%20Conservation%20in%20Learned%20Dynamical%20Models&entry.906535625=John%20M.%20Mango%20and%20Ronald%20Katende&entry.1292438233=We%20consider%20the%20problem%20of%20restoring%20linear%20conservation%20laws%20in%20data-driven%20linear%20dynamical%20models.%20Given%20a%20learned%20operator%20%24%5Cwidehat%7BA%7D%24%20and%20a%20full-rank%20constraint%20matrix%20%24C%24%20encoding%20one%20or%20more%20invariants%2C%20we%20show%20that%20the%20matrix%20closest%20to%20%24%5Cwidehat%7BA%7D%24%20in%20the%20Frobenius%20norm%20and%20satisfying%20%24C%5E%5Ctop%20A%20%3D%200%24%20is%20the%20orthogonal%20projection%20%24A%5E%5Cstar%20%3D%20%5Cwidehat%7BA%7D%20-%20C%28C%5E%5Ctop%20C%29%5E%7B-1%7DC%5E%5Ctop%20%5Cwidehat%7BA%7D%24.%20This%20correction%20is%20uniquely%20defined%2C%20low%20rank%20and%20fully%20determined%20by%20the%20violation%20%24C%5E%5Ctop%20%5Cwidehat%7BA%7D%24.%20In%20the%20single-invariant%20case%20it%20reduces%20to%20a%20rank-one%20update.%20We%20prove%20that%20%24A%5E%5Cstar%24%20enforces%20exact%20conservation%20while%20minimally%20perturbing%20the%20dynamics%2C%20and%20we%20verify%20these%20properties%20numerically%20on%20a%20Markov-type%20example.%20The%20projection%20provides%20an%20elementary%20and%20general%20mechanism%20for%20embedding%20exact%20invariants%20into%20any%20learned%20linear%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.22084v1&entry.124074799=Read"},
{"title": "Communication-Efficient and Differentially Private Vertical Federated Learning with Zeroth-Order Optimization", "author": "Jianing Zhang and Evan Chen and Dong-Jun Han and Chaoyue Liu and Christopher G. Brinton", "abstract": "Vertical Federated Learning (VFL) enables collaborative model training across feature-partitioned devices, yet its reliance on device-server information exchange introduces significant communication overhead and privacy risks. Downlink communication from the server to devices in VFL exposes gradient-related signals of the global loss that can be leveraged in inference attacks. Existing privacy-preserving VFL approaches that inject differential privacy (DP) noise on the downlink have the natural repercussion of degraded gradient quality, slowed convergence, and excessive communication rounds. In this work, we propose DPZV, a communication-efficient and differentially private ZO-VFL framework with tunable privacy guarantees. Based on zeroth-order (ZO) optimization, DPZV injects calibrated scalar-valued DP noise on the downlink, significantly reducing variance amplification while providing equivalent protection against targeted inference attacks. Through rigorous theoretical analysis, we establish convergence guarantees comparable to first-order DP-SGD, despite relying solely on ZO estimators, and prove that DPZV satisfies $(\u03b5, \u03b4)$-DP. Extensive experiments demonstrate that DPZV consistently achieves a superior privacy-utility tradeoff and requires fewer communication rounds than existing DP-VFL baselines under strict privacy constraints ($\u03b5\\leq 10$).", "link": "http://arxiv.org/abs/2502.20565v3", "date": "2025-12-26", "relevancy": 1.7639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4627}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4398}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20and%20Differentially%20Private%20Vertical%20Federated%20Learning%20with%20Zeroth-Order%20Optimization&body=Title%3A%20Communication-Efficient%20and%20Differentially%20Private%20Vertical%20Federated%20Learning%20with%20Zeroth-Order%20Optimization%0AAuthor%3A%20Jianing%20Zhang%20and%20Evan%20Chen%20and%20Dong-Jun%20Han%20and%20Chaoyue%20Liu%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20model%20training%20across%20feature-partitioned%20devices%2C%20yet%20its%20reliance%20on%20device-server%20information%20exchange%20introduces%20significant%20communication%20overhead%20and%20privacy%20risks.%20Downlink%20communication%20from%20the%20server%20to%20devices%20in%20VFL%20exposes%20gradient-related%20signals%20of%20the%20global%20loss%20that%20can%20be%20leveraged%20in%20inference%20attacks.%20Existing%20privacy-preserving%20VFL%20approaches%20that%20inject%20differential%20privacy%20%28DP%29%20noise%20on%20the%20downlink%20have%20the%20natural%20repercussion%20of%20degraded%20gradient%20quality%2C%20slowed%20convergence%2C%20and%20excessive%20communication%20rounds.%20In%20this%20work%2C%20we%20propose%20DPZV%2C%20a%20communication-efficient%20and%20differentially%20private%20ZO-VFL%20framework%20with%20tunable%20privacy%20guarantees.%20Based%20on%20zeroth-order%20%28ZO%29%20optimization%2C%20DPZV%20injects%20calibrated%20scalar-valued%20DP%20noise%20on%20the%20downlink%2C%20significantly%20reducing%20variance%20amplification%20while%20providing%20equivalent%20protection%20against%20targeted%20inference%20attacks.%20Through%20rigorous%20theoretical%20analysis%2C%20we%20establish%20convergence%20guarantees%20comparable%20to%20first-order%20DP-SGD%2C%20despite%20relying%20solely%20on%20ZO%20estimators%2C%20and%20prove%20that%20DPZV%20satisfies%20%24%28%CE%B5%2C%20%CE%B4%29%24-DP.%20Extensive%20experiments%20demonstrate%20that%20DPZV%20consistently%20achieves%20a%20superior%20privacy-utility%20tradeoff%20and%20requires%20fewer%20communication%20rounds%20than%20existing%20DP-VFL%20baselines%20under%20strict%20privacy%20constraints%20%28%24%CE%B5%5Cleq%2010%24%29.%0ALink%3A%20http%3A//arxiv.org/abs/2502.20565v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520and%2520Differentially%2520Private%2520Vertical%2520Federated%2520Learning%2520with%2520Zeroth-Order%2520Optimization%26entry.906535625%3DJianing%2520Zhang%2520and%2520Evan%2520Chen%2520and%2520Dong-Jun%2520Han%2520and%2520Chaoyue%2520Liu%2520and%2520Christopher%2520G.%2520Brinton%26entry.1292438233%3DVertical%2520Federated%2520Learning%2520%2528VFL%2529%2520enables%2520collaborative%2520model%2520training%2520across%2520feature-partitioned%2520devices%252C%2520yet%2520its%2520reliance%2520on%2520device-server%2520information%2520exchange%2520introduces%2520significant%2520communication%2520overhead%2520and%2520privacy%2520risks.%2520Downlink%2520communication%2520from%2520the%2520server%2520to%2520devices%2520in%2520VFL%2520exposes%2520gradient-related%2520signals%2520of%2520the%2520global%2520loss%2520that%2520can%2520be%2520leveraged%2520in%2520inference%2520attacks.%2520Existing%2520privacy-preserving%2520VFL%2520approaches%2520that%2520inject%2520differential%2520privacy%2520%2528DP%2529%2520noise%2520on%2520the%2520downlink%2520have%2520the%2520natural%2520repercussion%2520of%2520degraded%2520gradient%2520quality%252C%2520slowed%2520convergence%252C%2520and%2520excessive%2520communication%2520rounds.%2520In%2520this%2520work%252C%2520we%2520propose%2520DPZV%252C%2520a%2520communication-efficient%2520and%2520differentially%2520private%2520ZO-VFL%2520framework%2520with%2520tunable%2520privacy%2520guarantees.%2520Based%2520on%2520zeroth-order%2520%2528ZO%2529%2520optimization%252C%2520DPZV%2520injects%2520calibrated%2520scalar-valued%2520DP%2520noise%2520on%2520the%2520downlink%252C%2520significantly%2520reducing%2520variance%2520amplification%2520while%2520providing%2520equivalent%2520protection%2520against%2520targeted%2520inference%2520attacks.%2520Through%2520rigorous%2520theoretical%2520analysis%252C%2520we%2520establish%2520convergence%2520guarantees%2520comparable%2520to%2520first-order%2520DP-SGD%252C%2520despite%2520relying%2520solely%2520on%2520ZO%2520estimators%252C%2520and%2520prove%2520that%2520DPZV%2520satisfies%2520%2524%2528%25CE%25B5%252C%2520%25CE%25B4%2529%2524-DP.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DPZV%2520consistently%2520achieves%2520a%2520superior%2520privacy-utility%2520tradeoff%2520and%2520requires%2520fewer%2520communication%2520rounds%2520than%2520existing%2520DP-VFL%2520baselines%2520under%2520strict%2520privacy%2520constraints%2520%2528%2524%25CE%25B5%255Cleq%252010%2524%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20565v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20and%20Differentially%20Private%20Vertical%20Federated%20Learning%20with%20Zeroth-Order%20Optimization&entry.906535625=Jianing%20Zhang%20and%20Evan%20Chen%20and%20Dong-Jun%20Han%20and%20Chaoyue%20Liu%20and%20Christopher%20G.%20Brinton&entry.1292438233=Vertical%20Federated%20Learning%20%28VFL%29%20enables%20collaborative%20model%20training%20across%20feature-partitioned%20devices%2C%20yet%20its%20reliance%20on%20device-server%20information%20exchange%20introduces%20significant%20communication%20overhead%20and%20privacy%20risks.%20Downlink%20communication%20from%20the%20server%20to%20devices%20in%20VFL%20exposes%20gradient-related%20signals%20of%20the%20global%20loss%20that%20can%20be%20leveraged%20in%20inference%20attacks.%20Existing%20privacy-preserving%20VFL%20approaches%20that%20inject%20differential%20privacy%20%28DP%29%20noise%20on%20the%20downlink%20have%20the%20natural%20repercussion%20of%20degraded%20gradient%20quality%2C%20slowed%20convergence%2C%20and%20excessive%20communication%20rounds.%20In%20this%20work%2C%20we%20propose%20DPZV%2C%20a%20communication-efficient%20and%20differentially%20private%20ZO-VFL%20framework%20with%20tunable%20privacy%20guarantees.%20Based%20on%20zeroth-order%20%28ZO%29%20optimization%2C%20DPZV%20injects%20calibrated%20scalar-valued%20DP%20noise%20on%20the%20downlink%2C%20significantly%20reducing%20variance%20amplification%20while%20providing%20equivalent%20protection%20against%20targeted%20inference%20attacks.%20Through%20rigorous%20theoretical%20analysis%2C%20we%20establish%20convergence%20guarantees%20comparable%20to%20first-order%20DP-SGD%2C%20despite%20relying%20solely%20on%20ZO%20estimators%2C%20and%20prove%20that%20DPZV%20satisfies%20%24%28%CE%B5%2C%20%CE%B4%29%24-DP.%20Extensive%20experiments%20demonstrate%20that%20DPZV%20consistently%20achieves%20a%20superior%20privacy-utility%20tradeoff%20and%20requires%20fewer%20communication%20rounds%20than%20existing%20DP-VFL%20baselines%20under%20strict%20privacy%20constraints%20%28%24%CE%B5%5Cleq%2010%24%29.&entry.1838667208=http%3A//arxiv.org/abs/2502.20565v3&entry.124074799=Read"},
{"title": "Cost-aware Stopping for Bayesian Optimization", "author": "Qian Xie and Linda Cai and Alexander Terenin and Peter I. Frazier and Ziv Scully", "abstract": "In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions in a cost-aware manner is an important but underexplored practical consideration. A natural performance metric for this purpose is the cost-adjusted simple regret, which captures the trade-off between solution quality and cumulative evaluation cost. While several heuristic or adaptive stopping rules have been proposed, they lack guarantees ensuring stopping before incurring excessive function evaluation costs. We propose a principled cost-aware stopping rule for Bayesian optimization that adapts to varying evaluation costs without heuristic tuning. Our rule is grounded in a theoretical connection to state-of-the-art cost-aware acquisition functions, namely the Pandora's Box Gittins Index (PBGI) and log expected improvement per cost (LogEIPC). We prove a theoretical guarantee bounding the expected cost-adjusted simple regret incurred by our stopping rule when paired with either acquisition function. Across synthetic and empirical tasks, including hyperparameter optimization and neural architecture size search, pairing our stopping rule with PBGI or LogEIPC usually matches or outperforms other acquisition-function--stopping-rule pairs in terms of cost-adjusted simple regret.", "link": "http://arxiv.org/abs/2507.12453v4", "date": "2025-12-26", "relevancy": 1.7299, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4276}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization&body=Title%3A%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization%0AAuthor%3A%20Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully%0AAbstract%3A%20In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%20of%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%20functions%20in%20a%20cost-aware%20manner%20is%20an%20important%20but%20underexplored%20practical%20consideration.%20A%20natural%20performance%20metric%20for%20this%20purpose%20is%20the%20cost-adjusted%20simple%20regret%2C%20which%20captures%20the%20trade-off%20between%20solution%20quality%20and%20cumulative%20evaluation%20cost.%20While%20several%20heuristic%20or%20adaptive%20stopping%20rules%20have%20been%20proposed%2C%20they%20lack%20guarantees%20ensuring%20stopping%20before%20incurring%20excessive%20function%20evaluation%20costs.%20We%20propose%20a%20principled%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%20adapts%20to%20varying%20evaluation%20costs%20without%20heuristic%20tuning.%20Our%20rule%20is%20grounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%20functions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%20improvement%20per%20cost%20%28LogEIPC%29.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%20cost-adjusted%20simple%20regret%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20either%20acquisition%20function.%20Across%20synthetic%20and%20empirical%20tasks%2C%20including%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20pairing%20our%20stopping%20rule%20with%20PBGI%20or%20LogEIPC%20usually%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%20pairs%20in%20terms%20of%20cost-adjusted%20simple%20regret.%0ALink%3A%20http%3A//arxiv.org/abs/2507.12453v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-aware%2520Stopping%2520for%2520Bayesian%2520Optimization%26entry.906535625%3DQian%2520Xie%2520and%2520Linda%2520Cai%2520and%2520Alexander%2520Terenin%2520and%2520Peter%2520I.%2520Frazier%2520and%2520Ziv%2520Scully%26entry.1292438233%3DIn%2520automated%2520machine%2520learning%252C%2520scientific%2520discovery%252C%2520and%2520other%2520applications%2520of%2520Bayesian%2520optimization%252C%2520deciding%2520when%2520to%2520stop%2520evaluating%2520expensive%2520black-box%2520functions%2520in%2520a%2520cost-aware%2520manner%2520is%2520an%2520important%2520but%2520underexplored%2520practical%2520consideration.%2520A%2520natural%2520performance%2520metric%2520for%2520this%2520purpose%2520is%2520the%2520cost-adjusted%2520simple%2520regret%252C%2520which%2520captures%2520the%2520trade-off%2520between%2520solution%2520quality%2520and%2520cumulative%2520evaluation%2520cost.%2520While%2520several%2520heuristic%2520or%2520adaptive%2520stopping%2520rules%2520have%2520been%2520proposed%252C%2520they%2520lack%2520guarantees%2520ensuring%2520stopping%2520before%2520incurring%2520excessive%2520function%2520evaluation%2520costs.%2520We%2520propose%2520a%2520principled%2520cost-aware%2520stopping%2520rule%2520for%2520Bayesian%2520optimization%2520that%2520adapts%2520to%2520varying%2520evaluation%2520costs%2520without%2520heuristic%2520tuning.%2520Our%2520rule%2520is%2520grounded%2520in%2520a%2520theoretical%2520connection%2520to%2520state-of-the-art%2520cost-aware%2520acquisition%2520functions%252C%2520namely%2520the%2520Pandora%2527s%2520Box%2520Gittins%2520Index%2520%2528PBGI%2529%2520and%2520log%2520expected%2520improvement%2520per%2520cost%2520%2528LogEIPC%2529.%2520We%2520prove%2520a%2520theoretical%2520guarantee%2520bounding%2520the%2520expected%2520cost-adjusted%2520simple%2520regret%2520incurred%2520by%2520our%2520stopping%2520rule%2520when%2520paired%2520with%2520either%2520acquisition%2520function.%2520Across%2520synthetic%2520and%2520empirical%2520tasks%252C%2520including%2520hyperparameter%2520optimization%2520and%2520neural%2520architecture%2520size%2520search%252C%2520pairing%2520our%2520stopping%2520rule%2520with%2520PBGI%2520or%2520LogEIPC%2520usually%2520matches%2520or%2520outperforms%2520other%2520acquisition-function--stopping-rule%2520pairs%2520in%2520terms%2520of%2520cost-adjusted%2520simple%2520regret.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12453v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-aware%20Stopping%20for%20Bayesian%20Optimization&entry.906535625=Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully&entry.1292438233=In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%20of%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%20functions%20in%20a%20cost-aware%20manner%20is%20an%20important%20but%20underexplored%20practical%20consideration.%20A%20natural%20performance%20metric%20for%20this%20purpose%20is%20the%20cost-adjusted%20simple%20regret%2C%20which%20captures%20the%20trade-off%20between%20solution%20quality%20and%20cumulative%20evaluation%20cost.%20While%20several%20heuristic%20or%20adaptive%20stopping%20rules%20have%20been%20proposed%2C%20they%20lack%20guarantees%20ensuring%20stopping%20before%20incurring%20excessive%20function%20evaluation%20costs.%20We%20propose%20a%20principled%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%20adapts%20to%20varying%20evaluation%20costs%20without%20heuristic%20tuning.%20Our%20rule%20is%20grounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%20functions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%20improvement%20per%20cost%20%28LogEIPC%29.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%20cost-adjusted%20simple%20regret%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20either%20acquisition%20function.%20Across%20synthetic%20and%20empirical%20tasks%2C%20including%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20pairing%20our%20stopping%20rule%20with%20PBGI%20or%20LogEIPC%20usually%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%20pairs%20in%20terms%20of%20cost-adjusted%20simple%20regret.&entry.1838667208=http%3A//arxiv.org/abs/2507.12453v4&entry.124074799=Read"},
{"title": "Meta-Learning-Based Handover Management in NextG O-RAN", "author": "Michail Kalntis and George Iosifidis and Jos\u00e9 Su\u00e1rez-Varela and Andra Lutu and Fernando A. Kuipers", "abstract": "While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.", "link": "http://arxiv.org/abs/2512.22022v1", "date": "2025-12-26", "relevancy": 1.7105, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.417}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning-Based%20Handover%20Management%20in%20NextG%20O-RAN&body=Title%3A%20Meta-Learning-Based%20Handover%20Management%20in%20NextG%20O-RAN%0AAuthor%3A%20Michail%20Kalntis%20and%20George%20Iosifidis%20and%20Jos%C3%A9%20Su%C3%A1rez-Varela%20and%20Andra%20Lutu%20and%20Fernando%20A.%20Kuipers%0AAbstract%3A%20While%20traditional%20handovers%20%28THOs%29%20have%20served%20as%20a%20backbone%20for%20mobile%20connectivity%2C%20they%20increasingly%20suffer%20from%20failures%20and%20delays%2C%20especially%20in%20dense%20deployments%20and%20high-frequency%20bands.%20To%20address%20these%20limitations%2C%203GPP%20introduced%20Conditional%20Handovers%20%28CHOs%29%20that%20enable%20proactive%20cell%20reservations%20and%20user-driven%20execution.%20However%2C%20both%20handover%20%28HO%29%20types%20present%20intricate%20trade-offs%20in%20signaling%2C%20resource%20usage%2C%20and%20reliability.%20This%20paper%20presents%20unique%2C%20countrywide%20mobility%20management%20datasets%20from%20a%20top-tier%20mobile%20network%20operator%20%28MNO%29%20that%20offer%20fresh%20insights%20into%20these%20issues%20and%20call%20for%20adaptive%20and%20robust%20HO%20control%20in%20next-generation%20networks.%20Motivated%20by%20these%20findings%2C%20we%20propose%20CONTRA%2C%20a%20framework%20that%2C%20for%20the%20first%20time%2C%20jointly%20optimizes%20THOs%20and%20CHOs%20within%20the%20O-RAN%20architecture.%20We%20study%20two%20variants%20of%20CONTRA%3A%20one%20where%20users%20are%20a%20priori%20assigned%20to%20one%20of%20the%20HO%20types%2C%20reflecting%20distinct%20service%20or%20user-specific%20requirements%2C%20as%20well%20as%20a%20more%20dynamic%20formulation%20where%20the%20controller%20decides%20on-the-fly%20the%20HO%20type%2C%20based%20on%20system%20conditions%20and%20needs.%20To%20this%20end%2C%20it%20relies%20on%20a%20practical%20meta-learning%20algorithm%20that%20adapts%20to%20runtime%20observations%20and%20guarantees%20performance%20comparable%20to%20an%20oracle%20with%20perfect%20future%20information%20%28universal%20no-regret%29.%20CONTRA%20is%20specifically%20designed%20for%20near-real-time%20deployment%20as%20an%20O-RAN%20xApp%20and%20aligns%20with%20the%206G%20goals%20of%20flexible%20and%20intelligent%20control.%20Extensive%20evaluations%20leveraging%20crowdsourced%20datasets%20show%20that%20CONTRA%20improves%20user%20throughput%20and%20reduces%20both%20THO%20and%20CHO%20switching%20costs%2C%20outperforming%203GPP-compliant%20and%20Reinforcement%20Learning%20%28RL%29%20baselines%20in%20dynamic%20and%20real-world%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning-Based%2520Handover%2520Management%2520in%2520NextG%2520O-RAN%26entry.906535625%3DMichail%2520Kalntis%2520and%2520George%2520Iosifidis%2520and%2520Jos%25C3%25A9%2520Su%25C3%25A1rez-Varela%2520and%2520Andra%2520Lutu%2520and%2520Fernando%2520A.%2520Kuipers%26entry.1292438233%3DWhile%2520traditional%2520handovers%2520%2528THOs%2529%2520have%2520served%2520as%2520a%2520backbone%2520for%2520mobile%2520connectivity%252C%2520they%2520increasingly%2520suffer%2520from%2520failures%2520and%2520delays%252C%2520especially%2520in%2520dense%2520deployments%2520and%2520high-frequency%2520bands.%2520To%2520address%2520these%2520limitations%252C%25203GPP%2520introduced%2520Conditional%2520Handovers%2520%2528CHOs%2529%2520that%2520enable%2520proactive%2520cell%2520reservations%2520and%2520user-driven%2520execution.%2520However%252C%2520both%2520handover%2520%2528HO%2529%2520types%2520present%2520intricate%2520trade-offs%2520in%2520signaling%252C%2520resource%2520usage%252C%2520and%2520reliability.%2520This%2520paper%2520presents%2520unique%252C%2520countrywide%2520mobility%2520management%2520datasets%2520from%2520a%2520top-tier%2520mobile%2520network%2520operator%2520%2528MNO%2529%2520that%2520offer%2520fresh%2520insights%2520into%2520these%2520issues%2520and%2520call%2520for%2520adaptive%2520and%2520robust%2520HO%2520control%2520in%2520next-generation%2520networks.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520CONTRA%252C%2520a%2520framework%2520that%252C%2520for%2520the%2520first%2520time%252C%2520jointly%2520optimizes%2520THOs%2520and%2520CHOs%2520within%2520the%2520O-RAN%2520architecture.%2520We%2520study%2520two%2520variants%2520of%2520CONTRA%253A%2520one%2520where%2520users%2520are%2520a%2520priori%2520assigned%2520to%2520one%2520of%2520the%2520HO%2520types%252C%2520reflecting%2520distinct%2520service%2520or%2520user-specific%2520requirements%252C%2520as%2520well%2520as%2520a%2520more%2520dynamic%2520formulation%2520where%2520the%2520controller%2520decides%2520on-the-fly%2520the%2520HO%2520type%252C%2520based%2520on%2520system%2520conditions%2520and%2520needs.%2520To%2520this%2520end%252C%2520it%2520relies%2520on%2520a%2520practical%2520meta-learning%2520algorithm%2520that%2520adapts%2520to%2520runtime%2520observations%2520and%2520guarantees%2520performance%2520comparable%2520to%2520an%2520oracle%2520with%2520perfect%2520future%2520information%2520%2528universal%2520no-regret%2529.%2520CONTRA%2520is%2520specifically%2520designed%2520for%2520near-real-time%2520deployment%2520as%2520an%2520O-RAN%2520xApp%2520and%2520aligns%2520with%2520the%25206G%2520goals%2520of%2520flexible%2520and%2520intelligent%2520control.%2520Extensive%2520evaluations%2520leveraging%2520crowdsourced%2520datasets%2520show%2520that%2520CONTRA%2520improves%2520user%2520throughput%2520and%2520reduces%2520both%2520THO%2520and%2520CHO%2520switching%2520costs%252C%2520outperforming%25203GPP-compliant%2520and%2520Reinforcement%2520Learning%2520%2528RL%2529%2520baselines%2520in%2520dynamic%2520and%2520real-world%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning-Based%20Handover%20Management%20in%20NextG%20O-RAN&entry.906535625=Michail%20Kalntis%20and%20George%20Iosifidis%20and%20Jos%C3%A9%20Su%C3%A1rez-Varela%20and%20Andra%20Lutu%20and%20Fernando%20A.%20Kuipers&entry.1292438233=While%20traditional%20handovers%20%28THOs%29%20have%20served%20as%20a%20backbone%20for%20mobile%20connectivity%2C%20they%20increasingly%20suffer%20from%20failures%20and%20delays%2C%20especially%20in%20dense%20deployments%20and%20high-frequency%20bands.%20To%20address%20these%20limitations%2C%203GPP%20introduced%20Conditional%20Handovers%20%28CHOs%29%20that%20enable%20proactive%20cell%20reservations%20and%20user-driven%20execution.%20However%2C%20both%20handover%20%28HO%29%20types%20present%20intricate%20trade-offs%20in%20signaling%2C%20resource%20usage%2C%20and%20reliability.%20This%20paper%20presents%20unique%2C%20countrywide%20mobility%20management%20datasets%20from%20a%20top-tier%20mobile%20network%20operator%20%28MNO%29%20that%20offer%20fresh%20insights%20into%20these%20issues%20and%20call%20for%20adaptive%20and%20robust%20HO%20control%20in%20next-generation%20networks.%20Motivated%20by%20these%20findings%2C%20we%20propose%20CONTRA%2C%20a%20framework%20that%2C%20for%20the%20first%20time%2C%20jointly%20optimizes%20THOs%20and%20CHOs%20within%20the%20O-RAN%20architecture.%20We%20study%20two%20variants%20of%20CONTRA%3A%20one%20where%20users%20are%20a%20priori%20assigned%20to%20one%20of%20the%20HO%20types%2C%20reflecting%20distinct%20service%20or%20user-specific%20requirements%2C%20as%20well%20as%20a%20more%20dynamic%20formulation%20where%20the%20controller%20decides%20on-the-fly%20the%20HO%20type%2C%20based%20on%20system%20conditions%20and%20needs.%20To%20this%20end%2C%20it%20relies%20on%20a%20practical%20meta-learning%20algorithm%20that%20adapts%20to%20runtime%20observations%20and%20guarantees%20performance%20comparable%20to%20an%20oracle%20with%20perfect%20future%20information%20%28universal%20no-regret%29.%20CONTRA%20is%20specifically%20designed%20for%20near-real-time%20deployment%20as%20an%20O-RAN%20xApp%20and%20aligns%20with%20the%206G%20goals%20of%20flexible%20and%20intelligent%20control.%20Extensive%20evaluations%20leveraging%20crowdsourced%20datasets%20show%20that%20CONTRA%20improves%20user%20throughput%20and%20reduces%20both%20THO%20and%20CHO%20switching%20costs%2C%20outperforming%203GPP-compliant%20and%20Reinforcement%20Learning%20%28RL%29%20baselines%20in%20dynamic%20and%20real-world%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.22022v1&entry.124074799=Read"},
{"title": "Real-Time Streamable Generative Speech Restoration with Flow Matching", "author": "Simon Welker and Bunlong Lay and Maris Hillemann and Tal Peer and Timo Gerkmann", "abstract": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream$.$FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream$.$FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream$.$FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.", "link": "http://arxiv.org/abs/2512.19442v2", "date": "2025-12-26", "relevancy": 1.71, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6159}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5707}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Streamable%20Generative%20Speech%20Restoration%20with%20Flow%20Matching&body=Title%3A%20Real-Time%20Streamable%20Generative%20Speech%20Restoration%20with%20Flow%20Matching%0AAuthor%3A%20Simon%20Welker%20and%20Bunlong%20Lay%20and%20Maris%20Hillemann%20and%20Tal%20Peer%20and%20Timo%20Gerkmann%0AAbstract%3A%20Diffusion-based%20generative%20models%20have%20greatly%20impacted%20the%20speech%20processing%20field%20in%20recent%20years%2C%20exhibiting%20high%20speech%20naturalness%20and%20spawning%20a%20new%20research%20direction.%20Their%20application%20in%20real-time%20communication%20is%2C%20however%2C%20still%20lagging%20behind%20due%20to%20their%20computation-heavy%20nature%20involving%20multiple%20calls%20of%20large%20DNNs.%0A%20%20Here%2C%20we%20present%20Stream%24.%24FM%2C%20a%20frame-causal%20flow-based%20generative%20model%20with%20an%20algorithmic%20latency%20of%2032%20milliseconds%20%28ms%29%20and%20a%20total%20latency%20of%2048%20ms%2C%20paving%20the%20way%20for%20generative%20speech%20processing%20in%20real-time%20communication.%20We%20propose%20a%20buffered%20streaming%20inference%20scheme%20and%20an%20optimized%20DNN%20architecture%2C%20show%20how%20learned%20few-step%20numerical%20solvers%20can%20boost%20output%20quality%20at%20a%20fixed%20compute%20budget%2C%20explore%20model%20weight%20compression%20to%20find%20favorable%20points%20along%20a%20compute/quality%20tradeoff%2C%20and%20contribute%20a%20model%20variant%20with%2024%20ms%20total%20latency%20for%20the%20speech%20enhancement%20task.%0A%20%20Our%20work%20looks%20beyond%20theoretical%20latencies%2C%20showing%20that%20high-quality%20streaming%20generative%20speech%20processing%20can%20be%20realized%20on%20consumer%20GPUs%20available%20today.%20Stream%24.%24FM%20can%20solve%20a%20variety%20of%20speech%20processing%20tasks%20in%20a%20streaming%20fashion%3A%20speech%20enhancement%2C%20dereverberation%2C%20codec%20post-filtering%2C%20bandwidth%20extension%2C%20STFT%20phase%20retrieval%2C%20and%20Mel%20vocoding.%20As%20we%20verify%20through%20comprehensive%20evaluations%20and%20a%20MUSHRA%20listening%20test%2C%20Stream%24.%24FM%20establishes%20a%20state-of-the-art%20for%20generative%20streaming%20speech%20restoration%2C%20exhibits%20only%20a%20reasonable%20reduction%20in%20quality%20compared%20to%20a%20non-streaming%20variant%2C%20and%20outperforms%20our%20recent%20work%20%28Diffusion%20Buffer%29%20on%20generative%20streaming%20speech%20enhancement%20while%20operating%20at%20a%20lower%20latency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Streamable%2520Generative%2520Speech%2520Restoration%2520with%2520Flow%2520Matching%26entry.906535625%3DSimon%2520Welker%2520and%2520Bunlong%2520Lay%2520and%2520Maris%2520Hillemann%2520and%2520Tal%2520Peer%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3DDiffusion-based%2520generative%2520models%2520have%2520greatly%2520impacted%2520the%2520speech%2520processing%2520field%2520in%2520recent%2520years%252C%2520exhibiting%2520high%2520speech%2520naturalness%2520and%2520spawning%2520a%2520new%2520research%2520direction.%2520Their%2520application%2520in%2520real-time%2520communication%2520is%252C%2520however%252C%2520still%2520lagging%2520behind%2520due%2520to%2520their%2520computation-heavy%2520nature%2520involving%2520multiple%2520calls%2520of%2520large%2520DNNs.%250A%2520%2520Here%252C%2520we%2520present%2520Stream%2524.%2524FM%252C%2520a%2520frame-causal%2520flow-based%2520generative%2520model%2520with%2520an%2520algorithmic%2520latency%2520of%252032%2520milliseconds%2520%2528ms%2529%2520and%2520a%2520total%2520latency%2520of%252048%2520ms%252C%2520paving%2520the%2520way%2520for%2520generative%2520speech%2520processing%2520in%2520real-time%2520communication.%2520We%2520propose%2520a%2520buffered%2520streaming%2520inference%2520scheme%2520and%2520an%2520optimized%2520DNN%2520architecture%252C%2520show%2520how%2520learned%2520few-step%2520numerical%2520solvers%2520can%2520boost%2520output%2520quality%2520at%2520a%2520fixed%2520compute%2520budget%252C%2520explore%2520model%2520weight%2520compression%2520to%2520find%2520favorable%2520points%2520along%2520a%2520compute/quality%2520tradeoff%252C%2520and%2520contribute%2520a%2520model%2520variant%2520with%252024%2520ms%2520total%2520latency%2520for%2520the%2520speech%2520enhancement%2520task.%250A%2520%2520Our%2520work%2520looks%2520beyond%2520theoretical%2520latencies%252C%2520showing%2520that%2520high-quality%2520streaming%2520generative%2520speech%2520processing%2520can%2520be%2520realized%2520on%2520consumer%2520GPUs%2520available%2520today.%2520Stream%2524.%2524FM%2520can%2520solve%2520a%2520variety%2520of%2520speech%2520processing%2520tasks%2520in%2520a%2520streaming%2520fashion%253A%2520speech%2520enhancement%252C%2520dereverberation%252C%2520codec%2520post-filtering%252C%2520bandwidth%2520extension%252C%2520STFT%2520phase%2520retrieval%252C%2520and%2520Mel%2520vocoding.%2520As%2520we%2520verify%2520through%2520comprehensive%2520evaluations%2520and%2520a%2520MUSHRA%2520listening%2520test%252C%2520Stream%2524.%2524FM%2520establishes%2520a%2520state-of-the-art%2520for%2520generative%2520streaming%2520speech%2520restoration%252C%2520exhibits%2520only%2520a%2520reasonable%2520reduction%2520in%2520quality%2520compared%2520to%2520a%2520non-streaming%2520variant%252C%2520and%2520outperforms%2520our%2520recent%2520work%2520%2528Diffusion%2520Buffer%2529%2520on%2520generative%2520streaming%2520speech%2520enhancement%2520while%2520operating%2520at%2520a%2520lower%2520latency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Streamable%20Generative%20Speech%20Restoration%20with%20Flow%20Matching&entry.906535625=Simon%20Welker%20and%20Bunlong%20Lay%20and%20Maris%20Hillemann%20and%20Tal%20Peer%20and%20Timo%20Gerkmann&entry.1292438233=Diffusion-based%20generative%20models%20have%20greatly%20impacted%20the%20speech%20processing%20field%20in%20recent%20years%2C%20exhibiting%20high%20speech%20naturalness%20and%20spawning%20a%20new%20research%20direction.%20Their%20application%20in%20real-time%20communication%20is%2C%20however%2C%20still%20lagging%20behind%20due%20to%20their%20computation-heavy%20nature%20involving%20multiple%20calls%20of%20large%20DNNs.%0A%20%20Here%2C%20we%20present%20Stream%24.%24FM%2C%20a%20frame-causal%20flow-based%20generative%20model%20with%20an%20algorithmic%20latency%20of%2032%20milliseconds%20%28ms%29%20and%20a%20total%20latency%20of%2048%20ms%2C%20paving%20the%20way%20for%20generative%20speech%20processing%20in%20real-time%20communication.%20We%20propose%20a%20buffered%20streaming%20inference%20scheme%20and%20an%20optimized%20DNN%20architecture%2C%20show%20how%20learned%20few-step%20numerical%20solvers%20can%20boost%20output%20quality%20at%20a%20fixed%20compute%20budget%2C%20explore%20model%20weight%20compression%20to%20find%20favorable%20points%20along%20a%20compute/quality%20tradeoff%2C%20and%20contribute%20a%20model%20variant%20with%2024%20ms%20total%20latency%20for%20the%20speech%20enhancement%20task.%0A%20%20Our%20work%20looks%20beyond%20theoretical%20latencies%2C%20showing%20that%20high-quality%20streaming%20generative%20speech%20processing%20can%20be%20realized%20on%20consumer%20GPUs%20available%20today.%20Stream%24.%24FM%20can%20solve%20a%20variety%20of%20speech%20processing%20tasks%20in%20a%20streaming%20fashion%3A%20speech%20enhancement%2C%20dereverberation%2C%20codec%20post-filtering%2C%20bandwidth%20extension%2C%20STFT%20phase%20retrieval%2C%20and%20Mel%20vocoding.%20As%20we%20verify%20through%20comprehensive%20evaluations%20and%20a%20MUSHRA%20listening%20test%2C%20Stream%24.%24FM%20establishes%20a%20state-of-the-art%20for%20generative%20streaming%20speech%20restoration%2C%20exhibits%20only%20a%20reasonable%20reduction%20in%20quality%20compared%20to%20a%20non-streaming%20variant%2C%20and%20outperforms%20our%20recent%20work%20%28Diffusion%20Buffer%29%20on%20generative%20streaming%20speech%20enhancement%20while%20operating%20at%20a%20lower%20latency.&entry.1838667208=http%3A//arxiv.org/abs/2512.19442v2&entry.124074799=Read"},
{"title": "RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring", "author": "Zhuoyu Wu and Wenhui Ou and Qiawei Zheng and Jiayan Yang and Quanjun Wang and Wenqi Fang and Zheng Wang and Yongkui Yang and Heshan Li", "abstract": "Motion blur caused by camera or object movement severely degrades image quality and poses challenges for real-time applications such as autonomous driving, UAV perception, and medical imaging. In this paper, a lightweight U-shaped network tailored for real-time deblurring is presented and named RT-Focuser. To balance speed and accuracy, we design three key components: Lightweight Deblurring Block (LD) for edge-aware feature extraction, Multi-Level Integrated Aggregation module (MLIA) for encoder integration, and Cross-source Fusion Block (X-Fuse) for progressive decoder refinement. Trained on a single blurred input, RT-Focuser achieves 30.67 dB PSNR with only 5.85M parameters and 15.76 GMACs. It runs 6ms per frame on GPU and mobile, exceeds 140 FPS on both, showing strong potential for deployment on the edge. The official code and usage are available on: https://github.com/ReaganWu/RT-Focuser.", "link": "http://arxiv.org/abs/2512.21975v1", "date": "2025-12-26", "relevancy": 1.6919, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5845}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5595}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-Focuser%3A%20A%20Real-Time%20Lightweight%20Model%20for%20Edge-side%20Image%20Deblurring&body=Title%3A%20RT-Focuser%3A%20A%20Real-Time%20Lightweight%20Model%20for%20Edge-side%20Image%20Deblurring%0AAuthor%3A%20Zhuoyu%20Wu%20and%20Wenhui%20Ou%20and%20Qiawei%20Zheng%20and%20Jiayan%20Yang%20and%20Quanjun%20Wang%20and%20Wenqi%20Fang%20and%20Zheng%20Wang%20and%20Yongkui%20Yang%20and%20Heshan%20Li%0AAbstract%3A%20Motion%20blur%20caused%20by%20camera%20or%20object%20movement%20severely%20degrades%20image%20quality%20and%20poses%20challenges%20for%20real-time%20applications%20such%20as%20autonomous%20driving%2C%20UAV%20perception%2C%20and%20medical%20imaging.%20In%20this%20paper%2C%20a%20lightweight%20U-shaped%20network%20tailored%20for%20real-time%20deblurring%20is%20presented%20and%20named%20RT-Focuser.%20To%20balance%20speed%20and%20accuracy%2C%20we%20design%20three%20key%20components%3A%20Lightweight%20Deblurring%20Block%20%28LD%29%20for%20edge-aware%20feature%20extraction%2C%20Multi-Level%20Integrated%20Aggregation%20module%20%28MLIA%29%20for%20encoder%20integration%2C%20and%20Cross-source%20Fusion%20Block%20%28X-Fuse%29%20for%20progressive%20decoder%20refinement.%20Trained%20on%20a%20single%20blurred%20input%2C%20RT-Focuser%20achieves%2030.67%20dB%20PSNR%20with%20only%205.85M%20parameters%20and%2015.76%20GMACs.%20It%20runs%206ms%20per%20frame%20on%20GPU%20and%20mobile%2C%20exceeds%20140%20FPS%20on%20both%2C%20showing%20strong%20potential%20for%20deployment%20on%20the%20edge.%20The%20official%20code%20and%20usage%20are%20available%20on%3A%20https%3A//github.com/ReaganWu/RT-Focuser.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-Focuser%253A%2520A%2520Real-Time%2520Lightweight%2520Model%2520for%2520Edge-side%2520Image%2520Deblurring%26entry.906535625%3DZhuoyu%2520Wu%2520and%2520Wenhui%2520Ou%2520and%2520Qiawei%2520Zheng%2520and%2520Jiayan%2520Yang%2520and%2520Quanjun%2520Wang%2520and%2520Wenqi%2520Fang%2520and%2520Zheng%2520Wang%2520and%2520Yongkui%2520Yang%2520and%2520Heshan%2520Li%26entry.1292438233%3DMotion%2520blur%2520caused%2520by%2520camera%2520or%2520object%2520movement%2520severely%2520degrades%2520image%2520quality%2520and%2520poses%2520challenges%2520for%2520real-time%2520applications%2520such%2520as%2520autonomous%2520driving%252C%2520UAV%2520perception%252C%2520and%2520medical%2520imaging.%2520In%2520this%2520paper%252C%2520a%2520lightweight%2520U-shaped%2520network%2520tailored%2520for%2520real-time%2520deblurring%2520is%2520presented%2520and%2520named%2520RT-Focuser.%2520To%2520balance%2520speed%2520and%2520accuracy%252C%2520we%2520design%2520three%2520key%2520components%253A%2520Lightweight%2520Deblurring%2520Block%2520%2528LD%2529%2520for%2520edge-aware%2520feature%2520extraction%252C%2520Multi-Level%2520Integrated%2520Aggregation%2520module%2520%2528MLIA%2529%2520for%2520encoder%2520integration%252C%2520and%2520Cross-source%2520Fusion%2520Block%2520%2528X-Fuse%2529%2520for%2520progressive%2520decoder%2520refinement.%2520Trained%2520on%2520a%2520single%2520blurred%2520input%252C%2520RT-Focuser%2520achieves%252030.67%2520dB%2520PSNR%2520with%2520only%25205.85M%2520parameters%2520and%252015.76%2520GMACs.%2520It%2520runs%25206ms%2520per%2520frame%2520on%2520GPU%2520and%2520mobile%252C%2520exceeds%2520140%2520FPS%2520on%2520both%252C%2520showing%2520strong%2520potential%2520for%2520deployment%2520on%2520the%2520edge.%2520The%2520official%2520code%2520and%2520usage%2520are%2520available%2520on%253A%2520https%253A//github.com/ReaganWu/RT-Focuser.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-Focuser%3A%20A%20Real-Time%20Lightweight%20Model%20for%20Edge-side%20Image%20Deblurring&entry.906535625=Zhuoyu%20Wu%20and%20Wenhui%20Ou%20and%20Qiawei%20Zheng%20and%20Jiayan%20Yang%20and%20Quanjun%20Wang%20and%20Wenqi%20Fang%20and%20Zheng%20Wang%20and%20Yongkui%20Yang%20and%20Heshan%20Li&entry.1292438233=Motion%20blur%20caused%20by%20camera%20or%20object%20movement%20severely%20degrades%20image%20quality%20and%20poses%20challenges%20for%20real-time%20applications%20such%20as%20autonomous%20driving%2C%20UAV%20perception%2C%20and%20medical%20imaging.%20In%20this%20paper%2C%20a%20lightweight%20U-shaped%20network%20tailored%20for%20real-time%20deblurring%20is%20presented%20and%20named%20RT-Focuser.%20To%20balance%20speed%20and%20accuracy%2C%20we%20design%20three%20key%20components%3A%20Lightweight%20Deblurring%20Block%20%28LD%29%20for%20edge-aware%20feature%20extraction%2C%20Multi-Level%20Integrated%20Aggregation%20module%20%28MLIA%29%20for%20encoder%20integration%2C%20and%20Cross-source%20Fusion%20Block%20%28X-Fuse%29%20for%20progressive%20decoder%20refinement.%20Trained%20on%20a%20single%20blurred%20input%2C%20RT-Focuser%20achieves%2030.67%20dB%20PSNR%20with%20only%205.85M%20parameters%20and%2015.76%20GMACs.%20It%20runs%206ms%20per%20frame%20on%20GPU%20and%20mobile%2C%20exceeds%20140%20FPS%20on%20both%2C%20showing%20strong%20potential%20for%20deployment%20on%20the%20edge.%20The%20official%20code%20and%20usage%20are%20available%20on%3A%20https%3A//github.com/ReaganWu/RT-Focuser.&entry.1838667208=http%3A//arxiv.org/abs/2512.21975v1&entry.124074799=Read"},
{"title": "Why Smooth Stability Assumptions Fail for ReLU Learning", "author": "Ronald Katende", "abstract": "Stability analyses of modern learning systems are frequently derived under smoothness assumptions that are violated by ReLU-type nonlinearities. In this note, we isolate a minimal obstruction by showing that no uniform smoothness-based stability proxy such as gradient Lipschitzness or Hessian control can hold globally for ReLU networks, even in simple settings where training trajectories appear empirically stable. We give a concrete counterexample demonstrating the failure of classical stability bounds and identify a minimal generalized derivative condition under which stability statements can be meaningfully restored. The result clarifies why smooth approximations of ReLU can be misleading and motivates nonsmooth-aware stability frameworks.", "link": "http://arxiv.org/abs/2512.22055v1", "date": "2025-12-26", "relevancy": 1.6664, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Smooth%20Stability%20Assumptions%20Fail%20for%20ReLU%20Learning&body=Title%3A%20Why%20Smooth%20Stability%20Assumptions%20Fail%20for%20ReLU%20Learning%0AAuthor%3A%20Ronald%20Katende%0AAbstract%3A%20Stability%20analyses%20of%20modern%20learning%20systems%20are%20frequently%20derived%20under%20smoothness%20assumptions%20that%20are%20violated%20by%20ReLU-type%20nonlinearities.%20In%20this%20note%2C%20we%20isolate%20a%20minimal%20obstruction%20by%20showing%20that%20no%20uniform%20smoothness-based%20stability%20proxy%20such%20as%20gradient%20Lipschitzness%20or%20Hessian%20control%20can%20hold%20globally%20for%20ReLU%20networks%2C%20even%20in%20simple%20settings%20where%20training%20trajectories%20appear%20empirically%20stable.%20We%20give%20a%20concrete%20counterexample%20demonstrating%20the%20failure%20of%20classical%20stability%20bounds%20and%20identify%20a%20minimal%20generalized%20derivative%20condition%20under%20which%20stability%20statements%20can%20be%20meaningfully%20restored.%20The%20result%20clarifies%20why%20smooth%20approximations%20of%20ReLU%20can%20be%20misleading%20and%20motivates%20nonsmooth-aware%20stability%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Smooth%2520Stability%2520Assumptions%2520Fail%2520for%2520ReLU%2520Learning%26entry.906535625%3DRonald%2520Katende%26entry.1292438233%3DStability%2520analyses%2520of%2520modern%2520learning%2520systems%2520are%2520frequently%2520derived%2520under%2520smoothness%2520assumptions%2520that%2520are%2520violated%2520by%2520ReLU-type%2520nonlinearities.%2520In%2520this%2520note%252C%2520we%2520isolate%2520a%2520minimal%2520obstruction%2520by%2520showing%2520that%2520no%2520uniform%2520smoothness-based%2520stability%2520proxy%2520such%2520as%2520gradient%2520Lipschitzness%2520or%2520Hessian%2520control%2520can%2520hold%2520globally%2520for%2520ReLU%2520networks%252C%2520even%2520in%2520simple%2520settings%2520where%2520training%2520trajectories%2520appear%2520empirically%2520stable.%2520We%2520give%2520a%2520concrete%2520counterexample%2520demonstrating%2520the%2520failure%2520of%2520classical%2520stability%2520bounds%2520and%2520identify%2520a%2520minimal%2520generalized%2520derivative%2520condition%2520under%2520which%2520stability%2520statements%2520can%2520be%2520meaningfully%2520restored.%2520The%2520result%2520clarifies%2520why%2520smooth%2520approximations%2520of%2520ReLU%2520can%2520be%2520misleading%2520and%2520motivates%2520nonsmooth-aware%2520stability%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Smooth%20Stability%20Assumptions%20Fail%20for%20ReLU%20Learning&entry.906535625=Ronald%20Katende&entry.1292438233=Stability%20analyses%20of%20modern%20learning%20systems%20are%20frequently%20derived%20under%20smoothness%20assumptions%20that%20are%20violated%20by%20ReLU-type%20nonlinearities.%20In%20this%20note%2C%20we%20isolate%20a%20minimal%20obstruction%20by%20showing%20that%20no%20uniform%20smoothness-based%20stability%20proxy%20such%20as%20gradient%20Lipschitzness%20or%20Hessian%20control%20can%20hold%20globally%20for%20ReLU%20networks%2C%20even%20in%20simple%20settings%20where%20training%20trajectories%20appear%20empirically%20stable.%20We%20give%20a%20concrete%20counterexample%20demonstrating%20the%20failure%20of%20classical%20stability%20bounds%20and%20identify%20a%20minimal%20generalized%20derivative%20condition%20under%20which%20stability%20statements%20can%20be%20meaningfully%20restored.%20The%20result%20clarifies%20why%20smooth%20approximations%20of%20ReLU%20can%20be%20misleading%20and%20motivates%20nonsmooth-aware%20stability%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2512.22055v1&entry.124074799=Read"},
{"title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models", "author": "Dunyuan XU and Xikai Yang and Yaoqian Li and Juzheng Miao and Jinpeng Li and Pheng-Ann Heng", "abstract": "Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.", "link": "http://arxiv.org/abs/2512.21964v1", "date": "2025-12-26", "relevancy": 1.6478, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5978}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5357}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceive%20and%20Calibrate%3A%20Analyzing%20and%20Enhancing%20Robustness%20of%20Medical%20Multi-Modal%20Large%20Language%20Models&body=Title%3A%20Perceive%20and%20Calibrate%3A%20Analyzing%20and%20Enhancing%20Robustness%20of%20Medical%20Multi-Modal%20Large%20Language%20Models%0AAuthor%3A%20Dunyuan%20XU%20and%20Xikai%20Yang%20and%20Yaoqian%20Li%20and%20Juzheng%20Miao%20and%20Jinpeng%20Li%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20Medical%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20clinical%20performance.%20However%2C%20their%20sensitivity%20to%20real-world%20input%20perturbations%2C%20such%20as%20imaging%20artifacts%20and%20textual%20errors%2C%20critically%20undermines%20their%20clinical%20applicability.%20Systematic%20analysis%20of%20such%20noise%20impact%20on%20medical%20MLLMs%20remains%20largely%20unexplored.%20Furthermore%2C%20while%20several%20works%20have%20investigated%20the%20MLLMs%27%20robustness%20in%20general%20domains%2C%20they%20primarily%20focus%20on%20text%20modality%20and%20rely%20on%20costly%20fine-tuning.%20They%20are%20inadequate%20to%20address%20the%20complex%20noise%20patterns%20and%20fulfill%20the%20strict%20safety%20standards%20in%20medicine.%20To%20bridge%20this%20gap%2C%20this%20work%20systematically%20analyzes%20the%20impact%20of%20various%20perturbations%20on%20medical%20MLLMs%20across%20both%20visual%20and%20textual%20modalities.%20Building%20on%20our%20findings%2C%20we%20introduce%20a%20training-free%20Inherent-enhanced%20Multi-modal%20Calibration%20%28IMC%29%20framework%20that%20leverages%20MLLMs%27%20inherent%20denoising%20capabilities%20following%20the%20perceive-and-calibrate%20principle%20for%20cross-modal%20robustness%20enhancement.%20For%20the%20visual%20modality%2C%20we%20propose%20a%20Perturbation-aware%20Denoising%20Calibration%20%28PDC%29%20which%20leverages%20MLLMs%27%20own%20vision%20encoder%20to%20identify%20noise%20patterns%20and%20perform%20prototype-guided%20feature%20calibration.%20For%20text%20denoising%2C%20we%20design%20a%20Self-instantiated%20Multi-agent%20System%20%28SMS%29%20that%20exploits%20the%20MLLMs%27%20self-assessment%20capabilities%20to%20refine%20noisy%20text%20through%20a%20cooperative%20hierarchy%20of%20agents.%20We%20construct%20a%20benchmark%20containing%2011%20types%20of%20noise%20across%20both%20image%20and%20text%20modalities%20on%202%20datasets.%20Experimental%20results%20demonstrate%20our%20method%20achieves%20the%20state-of-the-art%20performance%20across%20multiple%20modalities%2C%20showing%20potential%20to%20enhance%20MLLMs%27%20robustness%20in%20real%20clinical%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceive%2520and%2520Calibrate%253A%2520Analyzing%2520and%2520Enhancing%2520Robustness%2520of%2520Medical%2520Multi-Modal%2520Large%2520Language%2520Models%26entry.906535625%3DDunyuan%2520XU%2520and%2520Xikai%2520Yang%2520and%2520Yaoqian%2520Li%2520and%2520Juzheng%2520Miao%2520and%2520Jinpeng%2520Li%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3DMedical%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520promising%2520clinical%2520performance.%2520However%252C%2520their%2520sensitivity%2520to%2520real-world%2520input%2520perturbations%252C%2520such%2520as%2520imaging%2520artifacts%2520and%2520textual%2520errors%252C%2520critically%2520undermines%2520their%2520clinical%2520applicability.%2520Systematic%2520analysis%2520of%2520such%2520noise%2520impact%2520on%2520medical%2520MLLMs%2520remains%2520largely%2520unexplored.%2520Furthermore%252C%2520while%2520several%2520works%2520have%2520investigated%2520the%2520MLLMs%2527%2520robustness%2520in%2520general%2520domains%252C%2520they%2520primarily%2520focus%2520on%2520text%2520modality%2520and%2520rely%2520on%2520costly%2520fine-tuning.%2520They%2520are%2520inadequate%2520to%2520address%2520the%2520complex%2520noise%2520patterns%2520and%2520fulfill%2520the%2520strict%2520safety%2520standards%2520in%2520medicine.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520work%2520systematically%2520analyzes%2520the%2520impact%2520of%2520various%2520perturbations%2520on%2520medical%2520MLLMs%2520across%2520both%2520visual%2520and%2520textual%2520modalities.%2520Building%2520on%2520our%2520findings%252C%2520we%2520introduce%2520a%2520training-free%2520Inherent-enhanced%2520Multi-modal%2520Calibration%2520%2528IMC%2529%2520framework%2520that%2520leverages%2520MLLMs%2527%2520inherent%2520denoising%2520capabilities%2520following%2520the%2520perceive-and-calibrate%2520principle%2520for%2520cross-modal%2520robustness%2520enhancement.%2520For%2520the%2520visual%2520modality%252C%2520we%2520propose%2520a%2520Perturbation-aware%2520Denoising%2520Calibration%2520%2528PDC%2529%2520which%2520leverages%2520MLLMs%2527%2520own%2520vision%2520encoder%2520to%2520identify%2520noise%2520patterns%2520and%2520perform%2520prototype-guided%2520feature%2520calibration.%2520For%2520text%2520denoising%252C%2520we%2520design%2520a%2520Self-instantiated%2520Multi-agent%2520System%2520%2528SMS%2529%2520that%2520exploits%2520the%2520MLLMs%2527%2520self-assessment%2520capabilities%2520to%2520refine%2520noisy%2520text%2520through%2520a%2520cooperative%2520hierarchy%2520of%2520agents.%2520We%2520construct%2520a%2520benchmark%2520containing%252011%2520types%2520of%2520noise%2520across%2520both%2520image%2520and%2520text%2520modalities%2520on%25202%2520datasets.%2520Experimental%2520results%2520demonstrate%2520our%2520method%2520achieves%2520the%2520state-of-the-art%2520performance%2520across%2520multiple%2520modalities%252C%2520showing%2520potential%2520to%2520enhance%2520MLLMs%2527%2520robustness%2520in%2520real%2520clinical%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceive%20and%20Calibrate%3A%20Analyzing%20and%20Enhancing%20Robustness%20of%20Medical%20Multi-Modal%20Large%20Language%20Models&entry.906535625=Dunyuan%20XU%20and%20Xikai%20Yang%20and%20Yaoqian%20Li%20and%20Juzheng%20Miao%20and%20Jinpeng%20Li%20and%20Pheng-Ann%20Heng&entry.1292438233=Medical%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20clinical%20performance.%20However%2C%20their%20sensitivity%20to%20real-world%20input%20perturbations%2C%20such%20as%20imaging%20artifacts%20and%20textual%20errors%2C%20critically%20undermines%20their%20clinical%20applicability.%20Systematic%20analysis%20of%20such%20noise%20impact%20on%20medical%20MLLMs%20remains%20largely%20unexplored.%20Furthermore%2C%20while%20several%20works%20have%20investigated%20the%20MLLMs%27%20robustness%20in%20general%20domains%2C%20they%20primarily%20focus%20on%20text%20modality%20and%20rely%20on%20costly%20fine-tuning.%20They%20are%20inadequate%20to%20address%20the%20complex%20noise%20patterns%20and%20fulfill%20the%20strict%20safety%20standards%20in%20medicine.%20To%20bridge%20this%20gap%2C%20this%20work%20systematically%20analyzes%20the%20impact%20of%20various%20perturbations%20on%20medical%20MLLMs%20across%20both%20visual%20and%20textual%20modalities.%20Building%20on%20our%20findings%2C%20we%20introduce%20a%20training-free%20Inherent-enhanced%20Multi-modal%20Calibration%20%28IMC%29%20framework%20that%20leverages%20MLLMs%27%20inherent%20denoising%20capabilities%20following%20the%20perceive-and-calibrate%20principle%20for%20cross-modal%20robustness%20enhancement.%20For%20the%20visual%20modality%2C%20we%20propose%20a%20Perturbation-aware%20Denoising%20Calibration%20%28PDC%29%20which%20leverages%20MLLMs%27%20own%20vision%20encoder%20to%20identify%20noise%20patterns%20and%20perform%20prototype-guided%20feature%20calibration.%20For%20text%20denoising%2C%20we%20design%20a%20Self-instantiated%20Multi-agent%20System%20%28SMS%29%20that%20exploits%20the%20MLLMs%27%20self-assessment%20capabilities%20to%20refine%20noisy%20text%20through%20a%20cooperative%20hierarchy%20of%20agents.%20We%20construct%20a%20benchmark%20containing%2011%20types%20of%20noise%20across%20both%20image%20and%20text%20modalities%20on%202%20datasets.%20Experimental%20results%20demonstrate%20our%20method%20achieves%20the%20state-of-the-art%20performance%20across%20multiple%20modalities%2C%20showing%20potential%20to%20enhance%20MLLMs%27%20robustness%20in%20real%20clinical%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.21964v1&entry.124074799=Read"},
{"title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications", "author": "Shengkun Cui and Rahul Krishna and Saurabh Jha and Ravishankar K. Iyer", "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.", "link": "http://arxiv.org/abs/2512.22113v1", "date": "2025-12-26", "relevancy": 1.6443, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Structured%20Graph%20Traversal%20for%20Root%20Cause%20Analysis%20of%20Code-related%20Incidents%20in%20Cloud%20Applications&body=Title%3A%20Agentic%20Structured%20Graph%20Traversal%20for%20Root%20Cause%20Analysis%20of%20Code-related%20Incidents%20in%20Cloud%20Applications%0AAuthor%3A%20Shengkun%20Cui%20and%20Rahul%20Krishna%20and%20Saurabh%20Jha%20and%20Ravishankar%20K.%20Iyer%0AAbstract%3A%20Cloud%20incidents%20pose%20major%20operational%20challenges%20in%20production%2C%20with%20unresolved%20production%20cloud%20incidents%20cost%20on%20average%20over%20%242M%20per%20hour.%20Prior%20research%20identifies%20code-%20and%20configuration-related%20issues%20as%20the%20predominant%20category%20of%20root%20causes%20in%20cloud%20incidents.%20This%20paper%20introduces%20PRAXIS%2C%20an%20orchestrator%20that%20manages%20and%20deploys%20an%20agentic%20workflow%20for%20diagnosing%20code-%20and%20configuration-caused%20cloud%20incidents.%20PRAXIS%20employs%20an%20LLM-driven%20structured%20traversal%20over%20two%20types%20of%20graph%3A%20%281%29%20a%20service%20dependency%20graph%20%28SDG%29%20that%20captures%20microservice-level%20dependencies%3B%20and%20%282%29%20a%20hammock-block%20program%20dependence%20graph%20%28PDG%29%20that%20captures%20code-level%20dependencies%20for%20each%20microservice.%20Together%2C%20these%20graphs%20encode%20microservice-%20and%20code-level%20dependencies%20and%20the%20LLM%20acts%20as%20a%20traversal%20policy%20over%20these%20graphs%2C%20moving%20between%20services%20and%20code%20dependencies%20to%20localize%20and%20explain%20failures.%20Compared%20to%20state-of-the-art%20ReAct%20baselines%2C%20PRAXIS%20improves%20RCA%20accuracy%20by%20up%20to%203.1x%20while%20reducing%20token%20consumption%20by%203.8x.%20PRAXIS%20is%20demonstrated%20on%20a%20set%20of%2030%20comprehensive%20real-world%20incidents%20that%20is%20being%20compiled%20into%20an%20RCA%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Structured%2520Graph%2520Traversal%2520for%2520Root%2520Cause%2520Analysis%2520of%2520Code-related%2520Incidents%2520in%2520Cloud%2520Applications%26entry.906535625%3DShengkun%2520Cui%2520and%2520Rahul%2520Krishna%2520and%2520Saurabh%2520Jha%2520and%2520Ravishankar%2520K.%2520Iyer%26entry.1292438233%3DCloud%2520incidents%2520pose%2520major%2520operational%2520challenges%2520in%2520production%252C%2520with%2520unresolved%2520production%2520cloud%2520incidents%2520cost%2520on%2520average%2520over%2520%25242M%2520per%2520hour.%2520Prior%2520research%2520identifies%2520code-%2520and%2520configuration-related%2520issues%2520as%2520the%2520predominant%2520category%2520of%2520root%2520causes%2520in%2520cloud%2520incidents.%2520This%2520paper%2520introduces%2520PRAXIS%252C%2520an%2520orchestrator%2520that%2520manages%2520and%2520deploys%2520an%2520agentic%2520workflow%2520for%2520diagnosing%2520code-%2520and%2520configuration-caused%2520cloud%2520incidents.%2520PRAXIS%2520employs%2520an%2520LLM-driven%2520structured%2520traversal%2520over%2520two%2520types%2520of%2520graph%253A%2520%25281%2529%2520a%2520service%2520dependency%2520graph%2520%2528SDG%2529%2520that%2520captures%2520microservice-level%2520dependencies%253B%2520and%2520%25282%2529%2520a%2520hammock-block%2520program%2520dependence%2520graph%2520%2528PDG%2529%2520that%2520captures%2520code-level%2520dependencies%2520for%2520each%2520microservice.%2520Together%252C%2520these%2520graphs%2520encode%2520microservice-%2520and%2520code-level%2520dependencies%2520and%2520the%2520LLM%2520acts%2520as%2520a%2520traversal%2520policy%2520over%2520these%2520graphs%252C%2520moving%2520between%2520services%2520and%2520code%2520dependencies%2520to%2520localize%2520and%2520explain%2520failures.%2520Compared%2520to%2520state-of-the-art%2520ReAct%2520baselines%252C%2520PRAXIS%2520improves%2520RCA%2520accuracy%2520by%2520up%2520to%25203.1x%2520while%2520reducing%2520token%2520consumption%2520by%25203.8x.%2520PRAXIS%2520is%2520demonstrated%2520on%2520a%2520set%2520of%252030%2520comprehensive%2520real-world%2520incidents%2520that%2520is%2520being%2520compiled%2520into%2520an%2520RCA%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Structured%20Graph%20Traversal%20for%20Root%20Cause%20Analysis%20of%20Code-related%20Incidents%20in%20Cloud%20Applications&entry.906535625=Shengkun%20Cui%20and%20Rahul%20Krishna%20and%20Saurabh%20Jha%20and%20Ravishankar%20K.%20Iyer&entry.1292438233=Cloud%20incidents%20pose%20major%20operational%20challenges%20in%20production%2C%20with%20unresolved%20production%20cloud%20incidents%20cost%20on%20average%20over%20%242M%20per%20hour.%20Prior%20research%20identifies%20code-%20and%20configuration-related%20issues%20as%20the%20predominant%20category%20of%20root%20causes%20in%20cloud%20incidents.%20This%20paper%20introduces%20PRAXIS%2C%20an%20orchestrator%20that%20manages%20and%20deploys%20an%20agentic%20workflow%20for%20diagnosing%20code-%20and%20configuration-caused%20cloud%20incidents.%20PRAXIS%20employs%20an%20LLM-driven%20structured%20traversal%20over%20two%20types%20of%20graph%3A%20%281%29%20a%20service%20dependency%20graph%20%28SDG%29%20that%20captures%20microservice-level%20dependencies%3B%20and%20%282%29%20a%20hammock-block%20program%20dependence%20graph%20%28PDG%29%20that%20captures%20code-level%20dependencies%20for%20each%20microservice.%20Together%2C%20these%20graphs%20encode%20microservice-%20and%20code-level%20dependencies%20and%20the%20LLM%20acts%20as%20a%20traversal%20policy%20over%20these%20graphs%2C%20moving%20between%20services%20and%20code%20dependencies%20to%20localize%20and%20explain%20failures.%20Compared%20to%20state-of-the-art%20ReAct%20baselines%2C%20PRAXIS%20improves%20RCA%20accuracy%20by%20up%20to%203.1x%20while%20reducing%20token%20consumption%20by%203.8x.%20PRAXIS%20is%20demonstrated%20on%20a%20set%20of%2030%20comprehensive%20real-world%20incidents%20that%20is%20being%20compiled%20into%20an%20RCA%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2512.22113v1&entry.124074799=Read"},
{"title": "RoboCade: Gamifying Robot Data Collection", "author": "Suvir Mirchandani and Mia Tang and Jiafei Duan and Jubayer Ibn Hamid and Michael Cho and Dorsa Sadigh", "abstract": "Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.", "link": "http://arxiv.org/abs/2512.21235v2", "date": "2025-12-26", "relevancy": 1.6244, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboCade%3A%20Gamifying%20Robot%20Data%20Collection&body=Title%3A%20RoboCade%3A%20Gamifying%20Robot%20Data%20Collection%0AAuthor%3A%20Suvir%20Mirchandani%20and%20Mia%20Tang%20and%20Jiafei%20Duan%20and%20Jubayer%20Ibn%20Hamid%20and%20Michael%20Cho%20and%20Dorsa%20Sadigh%0AAbstract%3A%20Imitation%20learning%20from%20human%20demonstrations%20has%20become%20a%20dominant%20approach%20for%20training%20autonomous%20robot%20policies.%20However%2C%20collecting%20demonstration%20datasets%20is%20costly%3A%20it%20often%20requires%20access%20to%20robots%20and%20needs%20sustained%20effort%20in%20a%20tedious%2C%20long%20process.%20These%20factors%20limit%20the%20scale%20of%20data%20available%20for%20training%20policies.%20We%20aim%20to%20address%20this%20scalability%20challenge%20by%20involving%20a%20broader%20audience%20in%20a%20gamified%20data%20collection%20experience%20that%20is%20both%20accessible%20and%20motivating.%20Specifically%2C%20we%20develop%20a%20gamified%20remote%20teleoperation%20platform%2C%20RoboCade%2C%20to%20engage%20general%20users%20in%20collecting%20data%20that%20is%20beneficial%20for%20downstream%20policy%20training.%20To%20do%20this%2C%20we%20embed%20gamification%20strategies%20into%20the%20design%20of%20the%20system%20interface%20and%20data%20collection%20tasks.%20In%20the%20system%20interface%2C%20we%20include%20components%20such%20as%20visual%20feedback%2C%20sound%20effects%2C%20goal%20visualizations%2C%20progress%20bars%2C%20leaderboards%2C%20and%20badges.%20We%20additionally%20propose%20principles%20for%20constructing%20gamified%20tasks%20that%20have%20overlapping%20structure%20with%20useful%20downstream%20target%20tasks.%20We%20instantiate%20RoboCade%20on%20three%20manipulation%20tasks%20--%20including%20spatial%20arrangement%2C%20scanning%2C%20and%20insertion.%20To%20illustrate%20the%20viability%20of%20gamified%20robot%20data%20collection%2C%20we%20collect%20a%20demonstration%20dataset%20through%20our%20platform%2C%20and%20show%20that%20co-training%20robot%20policies%20with%20this%20data%20can%20improve%20success%20rate%20on%20non-gamified%20target%20tasks%20%28%2B16-56%25%29.%20Further%2C%20we%20conduct%20a%20user%20study%20to%20validate%20that%20novice%20users%20find%20the%20gamified%20platform%20significantly%20more%20enjoyable%20than%20a%20standard%20non-gamified%20platform%20%28%2B24%25%29.%20These%20results%20highlight%20the%20promise%20of%20gamified%20data%20collection%20as%20a%20scalable%2C%20accessible%2C%20and%20engaging%20method%20for%20collecting%20demonstration%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboCade%253A%2520Gamifying%2520Robot%2520Data%2520Collection%26entry.906535625%3DSuvir%2520Mirchandani%2520and%2520Mia%2520Tang%2520and%2520Jiafei%2520Duan%2520and%2520Jubayer%2520Ibn%2520Hamid%2520and%2520Michael%2520Cho%2520and%2520Dorsa%2520Sadigh%26entry.1292438233%3DImitation%2520learning%2520from%2520human%2520demonstrations%2520has%2520become%2520a%2520dominant%2520approach%2520for%2520training%2520autonomous%2520robot%2520policies.%2520However%252C%2520collecting%2520demonstration%2520datasets%2520is%2520costly%253A%2520it%2520often%2520requires%2520access%2520to%2520robots%2520and%2520needs%2520sustained%2520effort%2520in%2520a%2520tedious%252C%2520long%2520process.%2520These%2520factors%2520limit%2520the%2520scale%2520of%2520data%2520available%2520for%2520training%2520policies.%2520We%2520aim%2520to%2520address%2520this%2520scalability%2520challenge%2520by%2520involving%2520a%2520broader%2520audience%2520in%2520a%2520gamified%2520data%2520collection%2520experience%2520that%2520is%2520both%2520accessible%2520and%2520motivating.%2520Specifically%252C%2520we%2520develop%2520a%2520gamified%2520remote%2520teleoperation%2520platform%252C%2520RoboCade%252C%2520to%2520engage%2520general%2520users%2520in%2520collecting%2520data%2520that%2520is%2520beneficial%2520for%2520downstream%2520policy%2520training.%2520To%2520do%2520this%252C%2520we%2520embed%2520gamification%2520strategies%2520into%2520the%2520design%2520of%2520the%2520system%2520interface%2520and%2520data%2520collection%2520tasks.%2520In%2520the%2520system%2520interface%252C%2520we%2520include%2520components%2520such%2520as%2520visual%2520feedback%252C%2520sound%2520effects%252C%2520goal%2520visualizations%252C%2520progress%2520bars%252C%2520leaderboards%252C%2520and%2520badges.%2520We%2520additionally%2520propose%2520principles%2520for%2520constructing%2520gamified%2520tasks%2520that%2520have%2520overlapping%2520structure%2520with%2520useful%2520downstream%2520target%2520tasks.%2520We%2520instantiate%2520RoboCade%2520on%2520three%2520manipulation%2520tasks%2520--%2520including%2520spatial%2520arrangement%252C%2520scanning%252C%2520and%2520insertion.%2520To%2520illustrate%2520the%2520viability%2520of%2520gamified%2520robot%2520data%2520collection%252C%2520we%2520collect%2520a%2520demonstration%2520dataset%2520through%2520our%2520platform%252C%2520and%2520show%2520that%2520co-training%2520robot%2520policies%2520with%2520this%2520data%2520can%2520improve%2520success%2520rate%2520on%2520non-gamified%2520target%2520tasks%2520%2528%252B16-56%2525%2529.%2520Further%252C%2520we%2520conduct%2520a%2520user%2520study%2520to%2520validate%2520that%2520novice%2520users%2520find%2520the%2520gamified%2520platform%2520significantly%2520more%2520enjoyable%2520than%2520a%2520standard%2520non-gamified%2520platform%2520%2528%252B24%2525%2529.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520gamified%2520data%2520collection%2520as%2520a%2520scalable%252C%2520accessible%252C%2520and%2520engaging%2520method%2520for%2520collecting%2520demonstration%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboCade%3A%20Gamifying%20Robot%20Data%20Collection&entry.906535625=Suvir%20Mirchandani%20and%20Mia%20Tang%20and%20Jiafei%20Duan%20and%20Jubayer%20Ibn%20Hamid%20and%20Michael%20Cho%20and%20Dorsa%20Sadigh&entry.1292438233=Imitation%20learning%20from%20human%20demonstrations%20has%20become%20a%20dominant%20approach%20for%20training%20autonomous%20robot%20policies.%20However%2C%20collecting%20demonstration%20datasets%20is%20costly%3A%20it%20often%20requires%20access%20to%20robots%20and%20needs%20sustained%20effort%20in%20a%20tedious%2C%20long%20process.%20These%20factors%20limit%20the%20scale%20of%20data%20available%20for%20training%20policies.%20We%20aim%20to%20address%20this%20scalability%20challenge%20by%20involving%20a%20broader%20audience%20in%20a%20gamified%20data%20collection%20experience%20that%20is%20both%20accessible%20and%20motivating.%20Specifically%2C%20we%20develop%20a%20gamified%20remote%20teleoperation%20platform%2C%20RoboCade%2C%20to%20engage%20general%20users%20in%20collecting%20data%20that%20is%20beneficial%20for%20downstream%20policy%20training.%20To%20do%20this%2C%20we%20embed%20gamification%20strategies%20into%20the%20design%20of%20the%20system%20interface%20and%20data%20collection%20tasks.%20In%20the%20system%20interface%2C%20we%20include%20components%20such%20as%20visual%20feedback%2C%20sound%20effects%2C%20goal%20visualizations%2C%20progress%20bars%2C%20leaderboards%2C%20and%20badges.%20We%20additionally%20propose%20principles%20for%20constructing%20gamified%20tasks%20that%20have%20overlapping%20structure%20with%20useful%20downstream%20target%20tasks.%20We%20instantiate%20RoboCade%20on%20three%20manipulation%20tasks%20--%20including%20spatial%20arrangement%2C%20scanning%2C%20and%20insertion.%20To%20illustrate%20the%20viability%20of%20gamified%20robot%20data%20collection%2C%20we%20collect%20a%20demonstration%20dataset%20through%20our%20platform%2C%20and%20show%20that%20co-training%20robot%20policies%20with%20this%20data%20can%20improve%20success%20rate%20on%20non-gamified%20target%20tasks%20%28%2B16-56%25%29.%20Further%2C%20we%20conduct%20a%20user%20study%20to%20validate%20that%20novice%20users%20find%20the%20gamified%20platform%20significantly%20more%20enjoyable%20than%20a%20standard%20non-gamified%20platform%20%28%2B24%25%29.%20These%20results%20highlight%20the%20promise%20of%20gamified%20data%20collection%20as%20a%20scalable%2C%20accessible%2C%20and%20engaging%20method%20for%20collecting%20demonstration%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.21235v2&entry.124074799=Read"},
{"title": "Explainable Multimodal Regression via Information Decomposition", "author": "Zhaozhao Ma and Shujian Yu", "abstract": "Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.", "link": "http://arxiv.org/abs/2512.22102v1", "date": "2025-12-26", "relevancy": 1.5982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5739}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5255}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Multimodal%20Regression%20via%20Information%20Decomposition&body=Title%3A%20Explainable%20Multimodal%20Regression%20via%20Information%20Decomposition%0AAuthor%3A%20Zhaozhao%20Ma%20and%20Shujian%20Yu%0AAbstract%3A%20Multimodal%20regression%20aims%20to%20predict%20a%20continuous%20target%20from%20heterogeneous%20input%20sources%20and%20typically%20relies%20on%20fusion%20strategies%20such%20as%20early%20or%20late%20fusion.%20However%2C%20existing%20methods%20lack%20principled%20tools%20to%20disentangle%20and%20quantify%20the%20individual%20contributions%20of%20each%20modality%20and%20their%20interactions%2C%20limiting%20the%20interpretability%20of%20multimodal%20fusion.%20We%20propose%20a%20novel%20multimodal%20regression%20framework%20grounded%20in%20Partial%20Information%20Decomposition%20%28PID%29%2C%20which%20decomposes%20modality-specific%20representations%20into%20unique%2C%20redundant%2C%20and%20synergistic%20components.%20The%20basic%20PID%20framework%20is%20inherently%20underdetermined.%20To%20resolve%20this%2C%20we%20introduce%20inductive%20bias%20by%20enforcing%20Gaussianity%20in%20the%20joint%20distribution%20of%20latent%20representations%20and%20the%20transformed%20response%20variable%20%28after%20inverse%20normal%20transformation%29%2C%20thereby%20enabling%20analytical%20computation%20of%20the%20PID%20terms.%20Additionally%2C%20we%20derive%20a%20closed-form%20conditional%20independence%20regularizer%20to%20promote%20the%20isolation%20of%20unique%20information%20within%20each%20modality.%20Experiments%20on%20six%20real-world%20datasets%2C%20including%20a%20case%20study%20on%20large-scale%20brain%20age%20prediction%20from%20multimodal%20neuroimaging%20data%2C%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20methods%20in%20both%20predictive%20accuracy%20and%20interpretability%2C%20while%20also%20enabling%20informed%20modality%20selection%20for%20efficient%20inference.%20Implementation%20is%20available%20at%20https%3A//github.com/zhaozhaoma/PIDReg.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Multimodal%2520Regression%2520via%2520Information%2520Decomposition%26entry.906535625%3DZhaozhao%2520Ma%2520and%2520Shujian%2520Yu%26entry.1292438233%3DMultimodal%2520regression%2520aims%2520to%2520predict%2520a%2520continuous%2520target%2520from%2520heterogeneous%2520input%2520sources%2520and%2520typically%2520relies%2520on%2520fusion%2520strategies%2520such%2520as%2520early%2520or%2520late%2520fusion.%2520However%252C%2520existing%2520methods%2520lack%2520principled%2520tools%2520to%2520disentangle%2520and%2520quantify%2520the%2520individual%2520contributions%2520of%2520each%2520modality%2520and%2520their%2520interactions%252C%2520limiting%2520the%2520interpretability%2520of%2520multimodal%2520fusion.%2520We%2520propose%2520a%2520novel%2520multimodal%2520regression%2520framework%2520grounded%2520in%2520Partial%2520Information%2520Decomposition%2520%2528PID%2529%252C%2520which%2520decomposes%2520modality-specific%2520representations%2520into%2520unique%252C%2520redundant%252C%2520and%2520synergistic%2520components.%2520The%2520basic%2520PID%2520framework%2520is%2520inherently%2520underdetermined.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520inductive%2520bias%2520by%2520enforcing%2520Gaussianity%2520in%2520the%2520joint%2520distribution%2520of%2520latent%2520representations%2520and%2520the%2520transformed%2520response%2520variable%2520%2528after%2520inverse%2520normal%2520transformation%2529%252C%2520thereby%2520enabling%2520analytical%2520computation%2520of%2520the%2520PID%2520terms.%2520Additionally%252C%2520we%2520derive%2520a%2520closed-form%2520conditional%2520independence%2520regularizer%2520to%2520promote%2520the%2520isolation%2520of%2520unique%2520information%2520within%2520each%2520modality.%2520Experiments%2520on%2520six%2520real-world%2520datasets%252C%2520including%2520a%2520case%2520study%2520on%2520large-scale%2520brain%2520age%2520prediction%2520from%2520multimodal%2520neuroimaging%2520data%252C%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520predictive%2520accuracy%2520and%2520interpretability%252C%2520while%2520also%2520enabling%2520informed%2520modality%2520selection%2520for%2520efficient%2520inference.%2520Implementation%2520is%2520available%2520at%2520https%253A//github.com/zhaozhaoma/PIDReg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Multimodal%20Regression%20via%20Information%20Decomposition&entry.906535625=Zhaozhao%20Ma%20and%20Shujian%20Yu&entry.1292438233=Multimodal%20regression%20aims%20to%20predict%20a%20continuous%20target%20from%20heterogeneous%20input%20sources%20and%20typically%20relies%20on%20fusion%20strategies%20such%20as%20early%20or%20late%20fusion.%20However%2C%20existing%20methods%20lack%20principled%20tools%20to%20disentangle%20and%20quantify%20the%20individual%20contributions%20of%20each%20modality%20and%20their%20interactions%2C%20limiting%20the%20interpretability%20of%20multimodal%20fusion.%20We%20propose%20a%20novel%20multimodal%20regression%20framework%20grounded%20in%20Partial%20Information%20Decomposition%20%28PID%29%2C%20which%20decomposes%20modality-specific%20representations%20into%20unique%2C%20redundant%2C%20and%20synergistic%20components.%20The%20basic%20PID%20framework%20is%20inherently%20underdetermined.%20To%20resolve%20this%2C%20we%20introduce%20inductive%20bias%20by%20enforcing%20Gaussianity%20in%20the%20joint%20distribution%20of%20latent%20representations%20and%20the%20transformed%20response%20variable%20%28after%20inverse%20normal%20transformation%29%2C%20thereby%20enabling%20analytical%20computation%20of%20the%20PID%20terms.%20Additionally%2C%20we%20derive%20a%20closed-form%20conditional%20independence%20regularizer%20to%20promote%20the%20isolation%20of%20unique%20information%20within%20each%20modality.%20Experiments%20on%20six%20real-world%20datasets%2C%20including%20a%20case%20study%20on%20large-scale%20brain%20age%20prediction%20from%20multimodal%20neuroimaging%20data%2C%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%20methods%20in%20both%20predictive%20accuracy%20and%20interpretability%2C%20while%20also%20enabling%20informed%20modality%20selection%20for%20efficient%20inference.%20Implementation%20is%20available%20at%20https%3A//github.com/zhaozhaoma/PIDReg.&entry.1838667208=http%3A//arxiv.org/abs/2512.22102v1&entry.124074799=Read"},
{"title": "Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning", "author": "Haiyang Zheng and Nan Pu and Wenjing Li and Teng Long and Nicu Sebe and Zhun Zhong", "abstract": "The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.", "link": "http://arxiv.org/abs/2512.12667v2", "date": "2025-12-26", "relevancy": 1.578, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5386}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-World%20Deepfake%20Attribution%20via%20Confidence-Aware%20Asymmetric%20Learning&body=Title%3A%20Open-World%20Deepfake%20Attribution%20via%20Confidence-Aware%20Asymmetric%20Learning%0AAuthor%3A%20Haiyang%20Zheng%20and%20Nan%20Pu%20and%20Wenjing%20Li%20and%20Teng%20Long%20and%20Nicu%20Sebe%20and%20Zhun%20Zhong%0AAbstract%3A%20The%20proliferation%20of%20synthetic%20facial%20imagery%20has%20intensified%20the%20need%20for%20robust%20Open-World%20DeepFake%20Attribution%20%28OW-DFA%29%2C%20which%20aims%20to%20attribute%20both%20known%20and%20unknown%20forgeries%20using%20labeled%20data%20for%20known%20types%20and%20unlabeled%20data%20containing%20a%20mixture%20of%20known%20and%20novel%20types.%20However%2C%20existing%20OW-DFA%20methods%20face%20two%20critical%20limitations%3A%201%29%20A%20confidence%20skew%20that%20leads%20to%20unreliable%20pseudo-labels%20for%20novel%20forgeries%2C%20resulting%20in%20biased%20training.%202%29%20An%20unrealistic%20assumption%20that%20the%20number%20of%20unknown%20forgery%20types%20is%20known%20%2Aa%20priori%2A.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Confidence-Aware%20Asymmetric%20Learning%20%28CAL%29%20framework%2C%20which%20adaptively%20balances%20model%20confidence%20across%20known%20and%20novel%20forgery%20types.%20CAL%20mainly%20consists%20of%20two%20components%3A%20Confidence-Aware%20Consistency%20Regularization%20%28CCR%29%20and%20Asymmetric%20Confidence%20Reinforcement%20%28ACR%29.%20CCR%20mitigates%20pseudo-label%20bias%20by%20dynamically%20scaling%20sample%20losses%20based%20on%20normalized%20confidence%2C%20gradually%20shifting%20the%20training%20focus%20from%20high-%20to%20low-confidence%20samples.%20ACR%20complements%20this%20by%20separately%20calibrating%20confidence%20for%20known%20and%20novel%20classes%20through%20selective%20learning%20on%20high-confidence%20samples%2C%20guided%20by%20their%20confidence%20gap.%20Together%2C%20CCR%20and%20ACR%20form%20a%20mutually%20reinforcing%20loop%20that%20significantly%20improves%20the%20model%27s%20OW-DFA%20performance.%20Moreover%2C%20we%20introduce%20a%20Dynamic%20Prototype%20Pruning%20%28DPP%29%20strategy%20that%20automatically%20estimates%20the%20number%20of%20novel%20forgery%20types%20in%20a%20coarse-to-fine%20manner%2C%20removing%20the%20need%20for%20unrealistic%20prior%20assumptions%20and%20enhancing%20the%20scalability%20of%20our%20methods%20to%20real-world%20OW-DFA%20scenarios.%20Extensive%20experiments%20on%20the%20standard%20OW-DFA%20benchmark%20and%20a%20newly%20extended%20benchmark%20incorporating%20advanced%20manipulations%20demonstrate%20that%20CAL%20consistently%20outperforms%20previous%20methods%2C%20achieving%20new%20state-of-the-art%20performance%20on%20both%20known%20and%20novel%20forgery%20attribution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-World%2520Deepfake%2520Attribution%2520via%2520Confidence-Aware%2520Asymmetric%2520Learning%26entry.906535625%3DHaiyang%2520Zheng%2520and%2520Nan%2520Pu%2520and%2520Wenjing%2520Li%2520and%2520Teng%2520Long%2520and%2520Nicu%2520Sebe%2520and%2520Zhun%2520Zhong%26entry.1292438233%3DThe%2520proliferation%2520of%2520synthetic%2520facial%2520imagery%2520has%2520intensified%2520the%2520need%2520for%2520robust%2520Open-World%2520DeepFake%2520Attribution%2520%2528OW-DFA%2529%252C%2520which%2520aims%2520to%2520attribute%2520both%2520known%2520and%2520unknown%2520forgeries%2520using%2520labeled%2520data%2520for%2520known%2520types%2520and%2520unlabeled%2520data%2520containing%2520a%2520mixture%2520of%2520known%2520and%2520novel%2520types.%2520However%252C%2520existing%2520OW-DFA%2520methods%2520face%2520two%2520critical%2520limitations%253A%25201%2529%2520A%2520confidence%2520skew%2520that%2520leads%2520to%2520unreliable%2520pseudo-labels%2520for%2520novel%2520forgeries%252C%2520resulting%2520in%2520biased%2520training.%25202%2529%2520An%2520unrealistic%2520assumption%2520that%2520the%2520number%2520of%2520unknown%2520forgery%2520types%2520is%2520known%2520%252Aa%2520priori%252A.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Confidence-Aware%2520Asymmetric%2520Learning%2520%2528CAL%2529%2520framework%252C%2520which%2520adaptively%2520balances%2520model%2520confidence%2520across%2520known%2520and%2520novel%2520forgery%2520types.%2520CAL%2520mainly%2520consists%2520of%2520two%2520components%253A%2520Confidence-Aware%2520Consistency%2520Regularization%2520%2528CCR%2529%2520and%2520Asymmetric%2520Confidence%2520Reinforcement%2520%2528ACR%2529.%2520CCR%2520mitigates%2520pseudo-label%2520bias%2520by%2520dynamically%2520scaling%2520sample%2520losses%2520based%2520on%2520normalized%2520confidence%252C%2520gradually%2520shifting%2520the%2520training%2520focus%2520from%2520high-%2520to%2520low-confidence%2520samples.%2520ACR%2520complements%2520this%2520by%2520separately%2520calibrating%2520confidence%2520for%2520known%2520and%2520novel%2520classes%2520through%2520selective%2520learning%2520on%2520high-confidence%2520samples%252C%2520guided%2520by%2520their%2520confidence%2520gap.%2520Together%252C%2520CCR%2520and%2520ACR%2520form%2520a%2520mutually%2520reinforcing%2520loop%2520that%2520significantly%2520improves%2520the%2520model%2527s%2520OW-DFA%2520performance.%2520Moreover%252C%2520we%2520introduce%2520a%2520Dynamic%2520Prototype%2520Pruning%2520%2528DPP%2529%2520strategy%2520that%2520automatically%2520estimates%2520the%2520number%2520of%2520novel%2520forgery%2520types%2520in%2520a%2520coarse-to-fine%2520manner%252C%2520removing%2520the%2520need%2520for%2520unrealistic%2520prior%2520assumptions%2520and%2520enhancing%2520the%2520scalability%2520of%2520our%2520methods%2520to%2520real-world%2520OW-DFA%2520scenarios.%2520Extensive%2520experiments%2520on%2520the%2520standard%2520OW-DFA%2520benchmark%2520and%2520a%2520newly%2520extended%2520benchmark%2520incorporating%2520advanced%2520manipulations%2520demonstrate%2520that%2520CAL%2520consistently%2520outperforms%2520previous%2520methods%252C%2520achieving%2520new%2520state-of-the-art%2520performance%2520on%2520both%2520known%2520and%2520novel%2520forgery%2520attribution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-World%20Deepfake%20Attribution%20via%20Confidence-Aware%20Asymmetric%20Learning&entry.906535625=Haiyang%20Zheng%20and%20Nan%20Pu%20and%20Wenjing%20Li%20and%20Teng%20Long%20and%20Nicu%20Sebe%20and%20Zhun%20Zhong&entry.1292438233=The%20proliferation%20of%20synthetic%20facial%20imagery%20has%20intensified%20the%20need%20for%20robust%20Open-World%20DeepFake%20Attribution%20%28OW-DFA%29%2C%20which%20aims%20to%20attribute%20both%20known%20and%20unknown%20forgeries%20using%20labeled%20data%20for%20known%20types%20and%20unlabeled%20data%20containing%20a%20mixture%20of%20known%20and%20novel%20types.%20However%2C%20existing%20OW-DFA%20methods%20face%20two%20critical%20limitations%3A%201%29%20A%20confidence%20skew%20that%20leads%20to%20unreliable%20pseudo-labels%20for%20novel%20forgeries%2C%20resulting%20in%20biased%20training.%202%29%20An%20unrealistic%20assumption%20that%20the%20number%20of%20unknown%20forgery%20types%20is%20known%20%2Aa%20priori%2A.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Confidence-Aware%20Asymmetric%20Learning%20%28CAL%29%20framework%2C%20which%20adaptively%20balances%20model%20confidence%20across%20known%20and%20novel%20forgery%20types.%20CAL%20mainly%20consists%20of%20two%20components%3A%20Confidence-Aware%20Consistency%20Regularization%20%28CCR%29%20and%20Asymmetric%20Confidence%20Reinforcement%20%28ACR%29.%20CCR%20mitigates%20pseudo-label%20bias%20by%20dynamically%20scaling%20sample%20losses%20based%20on%20normalized%20confidence%2C%20gradually%20shifting%20the%20training%20focus%20from%20high-%20to%20low-confidence%20samples.%20ACR%20complements%20this%20by%20separately%20calibrating%20confidence%20for%20known%20and%20novel%20classes%20through%20selective%20learning%20on%20high-confidence%20samples%2C%20guided%20by%20their%20confidence%20gap.%20Together%2C%20CCR%20and%20ACR%20form%20a%20mutually%20reinforcing%20loop%20that%20significantly%20improves%20the%20model%27s%20OW-DFA%20performance.%20Moreover%2C%20we%20introduce%20a%20Dynamic%20Prototype%20Pruning%20%28DPP%29%20strategy%20that%20automatically%20estimates%20the%20number%20of%20novel%20forgery%20types%20in%20a%20coarse-to-fine%20manner%2C%20removing%20the%20need%20for%20unrealistic%20prior%20assumptions%20and%20enhancing%20the%20scalability%20of%20our%20methods%20to%20real-world%20OW-DFA%20scenarios.%20Extensive%20experiments%20on%20the%20standard%20OW-DFA%20benchmark%20and%20a%20newly%20extended%20benchmark%20incorporating%20advanced%20manipulations%20demonstrate%20that%20CAL%20consistently%20outperforms%20previous%20methods%2C%20achieving%20new%20state-of-the-art%20performance%20on%20both%20known%20and%20novel%20forgery%20attribution.&entry.1838667208=http%3A//arxiv.org/abs/2512.12667v2&entry.124074799=Read"},
{"title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs", "author": "Jiayu Hu and Beibei Li and Jiangwei Xia and Yanjun Qin and Bing Ji and Zhongshi He", "abstract": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \\textbf{A}ctivate-\\textbf{L}ocate-\\textbf{E}dit \\textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.", "link": "http://arxiv.org/abs/2512.21999v1", "date": "2025-12-26", "relevancy": 1.5507, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5309}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Closer%21%20An%20Adversarial%20Parametric%20Editing%20Framework%20for%20Hallucination%20Mitigation%20in%20VLMs&body=Title%3A%20Look%20Closer%21%20An%20Adversarial%20Parametric%20Editing%20Framework%20for%20Hallucination%20Mitigation%20in%20VLMs%0AAuthor%3A%20Jiayu%20Hu%20and%20Beibei%20Li%20and%20Jiangwei%20Xia%20and%20Yanjun%20Qin%20and%20Bing%20Ji%20and%20Zhongshi%20He%0AAbstract%3A%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20garnered%20increasing%20attention%20in%20the%20AI%20community%20due%20to%20their%20promising%20practical%20applications%2C%20they%20exhibit%20persistent%20hallucination%20issues%2C%20generating%20outputs%20misaligned%20with%20visual%20inputs.%20Recent%20studies%20attribute%20these%20hallucinations%20to%20VLMs%27%20over-reliance%20on%20linguistic%20priors%20and%20insufficient%20visual%20feature%20integration%2C%20proposing%20heuristic%20decoding%20calibration%20strategies%20to%20mitigate%20them.%20However%2C%20the%20non-trainable%20nature%20of%20these%20strategies%20inherently%20limits%20their%20optimization%20potential.%20To%20this%20end%2C%20we%20propose%20an%20adversarial%20parametric%20editing%20framework%20for%20Hallucination%20mitigation%20in%20VLMs%2C%20which%20follows%20an%20%5Ctextbf%7BA%7Dctivate-%5Ctextbf%7BL%7Docate-%5Ctextbf%7BE%7Ddit%20%5Ctextbf%7BA%7Ddversarially%20paradigm.%20Specifically%2C%20we%20first%20construct%20an%20activation%20dataset%20that%20comprises%20grounded%20responses%20%28positive%20samples%20attentively%20anchored%20in%20visual%20features%29%20and%20hallucinatory%20responses%20%28negative%20samples%20reflecting%20LLM%20prior%20bias%20and%20internal%20knowledge%20artifacts%29.%20Next%2C%20we%20identify%20critical%20hallucination-prone%20parameter%20clusters%20by%20analyzing%20differential%20hidden%20states%20of%20response%20pairs.%20Then%2C%20these%20clusters%20are%20fine-tuned%20using%20prompts%20injected%20with%20adversarial%20tuned%20prefixes%20that%20are%20optimized%20to%20maximize%20visual%20neglect%2C%20thereby%20forcing%20the%20model%20to%20prioritize%20visual%20evidence%20over%20inherent%20parametric%20biases.%20Evaluations%20on%20both%20generative%20and%20discriminative%20VLM%20tasks%20demonstrate%20the%20significant%20effectiveness%20of%20ALEAHallu%20in%20alleviating%20hallucinations.%20Our%20code%20is%20available%20at%20https%3A//github.com/hujiayu1223/ALEAHallu.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Closer%2521%2520An%2520Adversarial%2520Parametric%2520Editing%2520Framework%2520for%2520Hallucination%2520Mitigation%2520in%2520VLMs%26entry.906535625%3DJiayu%2520Hu%2520and%2520Beibei%2520Li%2520and%2520Jiangwei%2520Xia%2520and%2520Yanjun%2520Qin%2520and%2520Bing%2520Ji%2520and%2520Zhongshi%2520He%26entry.1292438233%3DWhile%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520garnered%2520increasing%2520attention%2520in%2520the%2520AI%2520community%2520due%2520to%2520their%2520promising%2520practical%2520applications%252C%2520they%2520exhibit%2520persistent%2520hallucination%2520issues%252C%2520generating%2520outputs%2520misaligned%2520with%2520visual%2520inputs.%2520Recent%2520studies%2520attribute%2520these%2520hallucinations%2520to%2520VLMs%2527%2520over-reliance%2520on%2520linguistic%2520priors%2520and%2520insufficient%2520visual%2520feature%2520integration%252C%2520proposing%2520heuristic%2520decoding%2520calibration%2520strategies%2520to%2520mitigate%2520them.%2520However%252C%2520the%2520non-trainable%2520nature%2520of%2520these%2520strategies%2520inherently%2520limits%2520their%2520optimization%2520potential.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520adversarial%2520parametric%2520editing%2520framework%2520for%2520Hallucination%2520mitigation%2520in%2520VLMs%252C%2520which%2520follows%2520an%2520%255Ctextbf%257BA%257Dctivate-%255Ctextbf%257BL%257Docate-%255Ctextbf%257BE%257Ddit%2520%255Ctextbf%257BA%257Ddversarially%2520paradigm.%2520Specifically%252C%2520we%2520first%2520construct%2520an%2520activation%2520dataset%2520that%2520comprises%2520grounded%2520responses%2520%2528positive%2520samples%2520attentively%2520anchored%2520in%2520visual%2520features%2529%2520and%2520hallucinatory%2520responses%2520%2528negative%2520samples%2520reflecting%2520LLM%2520prior%2520bias%2520and%2520internal%2520knowledge%2520artifacts%2529.%2520Next%252C%2520we%2520identify%2520critical%2520hallucination-prone%2520parameter%2520clusters%2520by%2520analyzing%2520differential%2520hidden%2520states%2520of%2520response%2520pairs.%2520Then%252C%2520these%2520clusters%2520are%2520fine-tuned%2520using%2520prompts%2520injected%2520with%2520adversarial%2520tuned%2520prefixes%2520that%2520are%2520optimized%2520to%2520maximize%2520visual%2520neglect%252C%2520thereby%2520forcing%2520the%2520model%2520to%2520prioritize%2520visual%2520evidence%2520over%2520inherent%2520parametric%2520biases.%2520Evaluations%2520on%2520both%2520generative%2520and%2520discriminative%2520VLM%2520tasks%2520demonstrate%2520the%2520significant%2520effectiveness%2520of%2520ALEAHallu%2520in%2520alleviating%2520hallucinations.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hujiayu1223/ALEAHallu.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Closer%21%20An%20Adversarial%20Parametric%20Editing%20Framework%20for%20Hallucination%20Mitigation%20in%20VLMs&entry.906535625=Jiayu%20Hu%20and%20Beibei%20Li%20and%20Jiangwei%20Xia%20and%20Yanjun%20Qin%20and%20Bing%20Ji%20and%20Zhongshi%20He&entry.1292438233=While%20Vision-Language%20Models%20%28VLMs%29%20have%20garnered%20increasing%20attention%20in%20the%20AI%20community%20due%20to%20their%20promising%20practical%20applications%2C%20they%20exhibit%20persistent%20hallucination%20issues%2C%20generating%20outputs%20misaligned%20with%20visual%20inputs.%20Recent%20studies%20attribute%20these%20hallucinations%20to%20VLMs%27%20over-reliance%20on%20linguistic%20priors%20and%20insufficient%20visual%20feature%20integration%2C%20proposing%20heuristic%20decoding%20calibration%20strategies%20to%20mitigate%20them.%20However%2C%20the%20non-trainable%20nature%20of%20these%20strategies%20inherently%20limits%20their%20optimization%20potential.%20To%20this%20end%2C%20we%20propose%20an%20adversarial%20parametric%20editing%20framework%20for%20Hallucination%20mitigation%20in%20VLMs%2C%20which%20follows%20an%20%5Ctextbf%7BA%7Dctivate-%5Ctextbf%7BL%7Docate-%5Ctextbf%7BE%7Ddit%20%5Ctextbf%7BA%7Ddversarially%20paradigm.%20Specifically%2C%20we%20first%20construct%20an%20activation%20dataset%20that%20comprises%20grounded%20responses%20%28positive%20samples%20attentively%20anchored%20in%20visual%20features%29%20and%20hallucinatory%20responses%20%28negative%20samples%20reflecting%20LLM%20prior%20bias%20and%20internal%20knowledge%20artifacts%29.%20Next%2C%20we%20identify%20critical%20hallucination-prone%20parameter%20clusters%20by%20analyzing%20differential%20hidden%20states%20of%20response%20pairs.%20Then%2C%20these%20clusters%20are%20fine-tuned%20using%20prompts%20injected%20with%20adversarial%20tuned%20prefixes%20that%20are%20optimized%20to%20maximize%20visual%20neglect%2C%20thereby%20forcing%20the%20model%20to%20prioritize%20visual%20evidence%20over%20inherent%20parametric%20biases.%20Evaluations%20on%20both%20generative%20and%20discriminative%20VLM%20tasks%20demonstrate%20the%20significant%20effectiveness%20of%20ALEAHallu%20in%20alleviating%20hallucinations.%20Our%20code%20is%20available%20at%20https%3A//github.com/hujiayu1223/ALEAHallu.&entry.1838667208=http%3A//arxiv.org/abs/2512.21999v1&entry.124074799=Read"},
{"title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception", "author": "Sarthak Mehrotra and Sairam V C Rebbapragada and Mani Hemanth Reddy Bonthu and Vineeth N Balasubramanian", "abstract": "Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.", "link": "http://arxiv.org/abs/2512.22009v1", "date": "2025-12-26", "relevancy": 1.5396, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5222}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5141}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iSHIFT%3A%20Lightweight%20Slow-Fast%20GUI%20Agent%20with%20Adaptive%20Perception&body=Title%3A%20iSHIFT%3A%20Lightweight%20Slow-Fast%20GUI%20Agent%20with%20Adaptive%20Perception%0AAuthor%3A%20Sarthak%20Mehrotra%20and%20Sairam%20V%20C%20Rebbapragada%20and%20Mani%20Hemanth%20Reddy%20Bonthu%20and%20Vineeth%20N%20Balasubramanian%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20strong%20potential%20for%20interpreting%20and%20interacting%20with%20complex%2C%20pixel-rich%20Graphical%20User%20Interface%20%28GUI%29%20environments.%20However%2C%20building%20agents%20that%20are%20both%20efficient%20for%20high-level%20tasks%20and%20precise%20for%20fine-grained%20interactions%20remains%20challenging.%20GUI%20agents%20must%20perform%20routine%20actions%20efficiently%20while%20also%20handling%20tasks%20that%20demand%20exact%20visual%20grounding%2C%20yet%20existing%20approaches%20struggle%20when%20accuracy%20depends%20on%20identifying%20specific%20interface%20elements.%20These%20MLLMs%20also%20remain%20large%20and%20cannot%20adapt%20their%20reasoning%20depth%20to%20the%20task%20at%20hand.%20In%20this%20work%2C%20we%20introduce%20iSHIFT%3A%20Implicit%20Slow-fast%20Hybrid%20Inference%20with%20Flexible%20Tokens%2C%20a%20lightweight%20agent%20that%20integrates%20latent%20thinking%20%28implicit%20chain-of-thought%29%20with%20a%20perception%20control%20module.%20iSHIFT%20enables%20an%20MLLM%20to%20switch%20between%20a%20slow%20mode%2C%20which%20leverages%20detailed%20visual%20grounding%20for%20high%20precision%20and%20a%20fast%20mode%20that%20uses%20global%20cues%20for%20efficiency.%20Special%20perception%20tokens%20guide%20attention%20to%20relevant%20screen%20regions%2C%20allowing%20the%20model%20to%20decide%20both%20how%20to%20reason%20and%20where%20to%20focus.%20Despite%20its%20compact%202.5B%20size%2C%20iSHIFT%20matches%20state-of-the-art%20performance%20on%20multiple%20benchmark%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiSHIFT%253A%2520Lightweight%2520Slow-Fast%2520GUI%2520Agent%2520with%2520Adaptive%2520Perception%26entry.906535625%3DSarthak%2520Mehrotra%2520and%2520Sairam%2520V%2520C%2520Rebbapragada%2520and%2520Mani%2520Hemanth%2520Reddy%2520Bonthu%2520and%2520Vineeth%2520N%2520Balasubramanian%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520show%2520strong%2520potential%2520for%2520interpreting%2520and%2520interacting%2520with%2520complex%252C%2520pixel-rich%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520environments.%2520However%252C%2520building%2520agents%2520that%2520are%2520both%2520efficient%2520for%2520high-level%2520tasks%2520and%2520precise%2520for%2520fine-grained%2520interactions%2520remains%2520challenging.%2520GUI%2520agents%2520must%2520perform%2520routine%2520actions%2520efficiently%2520while%2520also%2520handling%2520tasks%2520that%2520demand%2520exact%2520visual%2520grounding%252C%2520yet%2520existing%2520approaches%2520struggle%2520when%2520accuracy%2520depends%2520on%2520identifying%2520specific%2520interface%2520elements.%2520These%2520MLLMs%2520also%2520remain%2520large%2520and%2520cannot%2520adapt%2520their%2520reasoning%2520depth%2520to%2520the%2520task%2520at%2520hand.%2520In%2520this%2520work%252C%2520we%2520introduce%2520iSHIFT%253A%2520Implicit%2520Slow-fast%2520Hybrid%2520Inference%2520with%2520Flexible%2520Tokens%252C%2520a%2520lightweight%2520agent%2520that%2520integrates%2520latent%2520thinking%2520%2528implicit%2520chain-of-thought%2529%2520with%2520a%2520perception%2520control%2520module.%2520iSHIFT%2520enables%2520an%2520MLLM%2520to%2520switch%2520between%2520a%2520slow%2520mode%252C%2520which%2520leverages%2520detailed%2520visual%2520grounding%2520for%2520high%2520precision%2520and%2520a%2520fast%2520mode%2520that%2520uses%2520global%2520cues%2520for%2520efficiency.%2520Special%2520perception%2520tokens%2520guide%2520attention%2520to%2520relevant%2520screen%2520regions%252C%2520allowing%2520the%2520model%2520to%2520decide%2520both%2520how%2520to%2520reason%2520and%2520where%2520to%2520focus.%2520Despite%2520its%2520compact%25202.5B%2520size%252C%2520iSHIFT%2520matches%2520state-of-the-art%2520performance%2520on%2520multiple%2520benchmark%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iSHIFT%3A%20Lightweight%20Slow-Fast%20GUI%20Agent%20with%20Adaptive%20Perception&entry.906535625=Sarthak%20Mehrotra%20and%20Sairam%20V%20C%20Rebbapragada%20and%20Mani%20Hemanth%20Reddy%20Bonthu%20and%20Vineeth%20N%20Balasubramanian&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20strong%20potential%20for%20interpreting%20and%20interacting%20with%20complex%2C%20pixel-rich%20Graphical%20User%20Interface%20%28GUI%29%20environments.%20However%2C%20building%20agents%20that%20are%20both%20efficient%20for%20high-level%20tasks%20and%20precise%20for%20fine-grained%20interactions%20remains%20challenging.%20GUI%20agents%20must%20perform%20routine%20actions%20efficiently%20while%20also%20handling%20tasks%20that%20demand%20exact%20visual%20grounding%2C%20yet%20existing%20approaches%20struggle%20when%20accuracy%20depends%20on%20identifying%20specific%20interface%20elements.%20These%20MLLMs%20also%20remain%20large%20and%20cannot%20adapt%20their%20reasoning%20depth%20to%20the%20task%20at%20hand.%20In%20this%20work%2C%20we%20introduce%20iSHIFT%3A%20Implicit%20Slow-fast%20Hybrid%20Inference%20with%20Flexible%20Tokens%2C%20a%20lightweight%20agent%20that%20integrates%20latent%20thinking%20%28implicit%20chain-of-thought%29%20with%20a%20perception%20control%20module.%20iSHIFT%20enables%20an%20MLLM%20to%20switch%20between%20a%20slow%20mode%2C%20which%20leverages%20detailed%20visual%20grounding%20for%20high%20precision%20and%20a%20fast%20mode%20that%20uses%20global%20cues%20for%20efficiency.%20Special%20perception%20tokens%20guide%20attention%20to%20relevant%20screen%20regions%2C%20allowing%20the%20model%20to%20decide%20both%20how%20to%20reason%20and%20where%20to%20focus.%20Despite%20its%20compact%202.5B%20size%2C%20iSHIFT%20matches%20state-of-the-art%20performance%20on%20multiple%20benchmark%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.22009v1&entry.124074799=Read"},
{"title": "Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation", "author": "Kei Saito", "abstract": "Current AI systems exhibit a fundamental limitation: they resolve ambiguity prematurely. This premature semantic collapse--collapsing multiple valid interpretations into single outputs--stems from classical identity assumptions in neural architectures. We propose Non-Resolution Reasoning (NRR), treating ambiguity retention as a valid reasoning mode. NRR introduces three principles: (1) Non-Identity ($A \\neq A$)--the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$)--entities share partial overlap without being identical; (3) Non-Resolution--conflicting interpretations coexist without forced convergence. We formalize these through Multi-Vector Embeddings, Non-Collapsing Attention, and Contextual Identity Tracking (CIT). Functional verification via Turn 1 Entropy measurement shows NRR-lite maintains high entropy ($H = 0.63$) at ambiguous turns while standard architectures collapse early ($H = 0.10$), demonstrating that NRR preserves interpretive flexibility until context arrives. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "link": "http://arxiv.org/abs/2512.13478v5", "date": "2025-12-26", "relevancy": 1.5375, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Resolution%20Reasoning%20%28NRR%29%3A%20A%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation&body=Title%3A%20Non-Resolution%20Reasoning%20%28NRR%29%3A%20A%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation%0AAuthor%3A%20Kei%20Saito%0AAbstract%3A%20Current%20AI%20systems%20exhibit%20a%20fundamental%20limitation%3A%20they%20resolve%20ambiguity%20prematurely.%20This%20premature%20semantic%20collapse--collapsing%20multiple%20valid%20interpretations%20into%20single%20outputs--stems%20from%20classical%20identity%20assumptions%20in%20neural%20architectures.%20We%20propose%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20treating%20ambiguity%20retention%20as%20a%20valid%20reasoning%20mode.%20NRR%20introduces%20three%20principles%3A%20%281%29%20Non-Identity%20%28%24A%20%5Cneq%20A%24%29--the%20same%20symbol%20refers%20to%20different%20entities%20across%20contexts%3B%20%282%29%20Approximate%20Identity%20%28%24A%20%5Capprox%20A%24%29--entities%20share%20partial%20overlap%20without%20being%20identical%3B%20%283%29%20Non-Resolution--conflicting%20interpretations%20coexist%20without%20forced%20convergence.%20We%20formalize%20these%20through%20Multi-Vector%20Embeddings%2C%20Non-Collapsing%20Attention%2C%20and%20Contextual%20Identity%20Tracking%20%28CIT%29.%20Functional%20verification%20via%20Turn%201%20Entropy%20measurement%20shows%20NRR-lite%20maintains%20high%20entropy%20%28%24H%20%3D%200.63%24%29%20at%20ambiguous%20turns%20while%20standard%20architectures%20collapse%20early%20%28%24H%20%3D%200.10%24%29%2C%20demonstrating%20that%20NRR%20preserves%20interpretive%20flexibility%20until%20context%20arrives.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13478v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Resolution%2520Reasoning%2520%2528NRR%2529%253A%2520A%2520Computational%2520Framework%2520for%2520Contextual%2520Identity%2520and%2520Ambiguity%2520Preservation%26entry.906535625%3DKei%2520Saito%26entry.1292438233%3DCurrent%2520AI%2520systems%2520exhibit%2520a%2520fundamental%2520limitation%253A%2520they%2520resolve%2520ambiguity%2520prematurely.%2520This%2520premature%2520semantic%2520collapse--collapsing%2520multiple%2520valid%2520interpretations%2520into%2520single%2520outputs--stems%2520from%2520classical%2520identity%2520assumptions%2520in%2520neural%2520architectures.%2520We%2520propose%2520Non-Resolution%2520Reasoning%2520%2528NRR%2529%252C%2520treating%2520ambiguity%2520retention%2520as%2520a%2520valid%2520reasoning%2520mode.%2520NRR%2520introduces%2520three%2520principles%253A%2520%25281%2529%2520Non-Identity%2520%2528%2524A%2520%255Cneq%2520A%2524%2529--the%2520same%2520symbol%2520refers%2520to%2520different%2520entities%2520across%2520contexts%253B%2520%25282%2529%2520Approximate%2520Identity%2520%2528%2524A%2520%255Capprox%2520A%2524%2529--entities%2520share%2520partial%2520overlap%2520without%2520being%2520identical%253B%2520%25283%2529%2520Non-Resolution--conflicting%2520interpretations%2520coexist%2520without%2520forced%2520convergence.%2520We%2520formalize%2520these%2520through%2520Multi-Vector%2520Embeddings%252C%2520Non-Collapsing%2520Attention%252C%2520and%2520Contextual%2520Identity%2520Tracking%2520%2528CIT%2529.%2520Functional%2520verification%2520via%2520Turn%25201%2520Entropy%2520measurement%2520shows%2520NRR-lite%2520maintains%2520high%2520entropy%2520%2528%2524H%2520%253D%25200.63%2524%2529%2520at%2520ambiguous%2520turns%2520while%2520standard%2520architectures%2520collapse%2520early%2520%2528%2524H%2520%253D%25200.10%2524%2529%252C%2520demonstrating%2520that%2520NRR%2520preserves%2520interpretive%2520flexibility%2520until%2520context%2520arrives.%2520The%2520question%2520is%2520not%2520whether%2520AI%2520should%2520resolve%2520ambiguity%252C%2520but%2520when%252C%2520how%252C%2520and%2520under%2520whose%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13478v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Resolution%20Reasoning%20%28NRR%29%3A%20A%20Computational%20Framework%20for%20Contextual%20Identity%20and%20Ambiguity%20Preservation&entry.906535625=Kei%20Saito&entry.1292438233=Current%20AI%20systems%20exhibit%20a%20fundamental%20limitation%3A%20they%20resolve%20ambiguity%20prematurely.%20This%20premature%20semantic%20collapse--collapsing%20multiple%20valid%20interpretations%20into%20single%20outputs--stems%20from%20classical%20identity%20assumptions%20in%20neural%20architectures.%20We%20propose%20Non-Resolution%20Reasoning%20%28NRR%29%2C%20treating%20ambiguity%20retention%20as%20a%20valid%20reasoning%20mode.%20NRR%20introduces%20three%20principles%3A%20%281%29%20Non-Identity%20%28%24A%20%5Cneq%20A%24%29--the%20same%20symbol%20refers%20to%20different%20entities%20across%20contexts%3B%20%282%29%20Approximate%20Identity%20%28%24A%20%5Capprox%20A%24%29--entities%20share%20partial%20overlap%20without%20being%20identical%3B%20%283%29%20Non-Resolution--conflicting%20interpretations%20coexist%20without%20forced%20convergence.%20We%20formalize%20these%20through%20Multi-Vector%20Embeddings%2C%20Non-Collapsing%20Attention%2C%20and%20Contextual%20Identity%20Tracking%20%28CIT%29.%20Functional%20verification%20via%20Turn%201%20Entropy%20measurement%20shows%20NRR-lite%20maintains%20high%20entropy%20%28%24H%20%3D%200.63%24%29%20at%20ambiguous%20turns%20while%20standard%20architectures%20collapse%20early%20%28%24H%20%3D%200.10%24%29%2C%20demonstrating%20that%20NRR%20preserves%20interpretive%20flexibility%20until%20context%20arrives.%20The%20question%20is%20not%20whether%20AI%20should%20resolve%20ambiguity%2C%20but%20when%2C%20how%2C%20and%20under%20whose%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.13478v5&entry.124074799=Read"},
{"title": "CP-Agent: Agentic Constraint Programming", "author": "Stefan Szeider", "abstract": "Translating natural language into formal constraint models requires expertise in the problem domain and modeling frameworks. To investigate whether constraint modeling benefits from agentic workflows, we introduce CP-Agent, a Python coding agent using the ReAct framework with a persistent IPython kernel. Domain knowledge is provided through a project prompt of under 50 lines. The agent iteratively executes code, observes the solver's feedback, and refines models based on the execution results.\n  We evaluate CP-Agent on CP-Bench's 101 constraint programming problems. We clarified the benchmark to address systematic ambiguities in problem specifications and errors in ground-truth models. On the clarified benchmark, CP-Agent solves all 101 problems. Ablation studies indicate that minimal guidance outperforms detailed procedural scaffolding, and that explicit task management tools have mixed effects on focused modeling tasks.", "link": "http://arxiv.org/abs/2508.07468v2", "date": "2025-12-26", "relevancy": 1.4774, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5185}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4967}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Agent%3A%20Agentic%20Constraint%20Programming&body=Title%3A%20CP-Agent%3A%20Agentic%20Constraint%20Programming%0AAuthor%3A%20Stefan%20Szeider%0AAbstract%3A%20Translating%20natural%20language%20into%20formal%20constraint%20models%20requires%20expertise%20in%20the%20problem%20domain%20and%20modeling%20frameworks.%20To%20investigate%20whether%20constraint%20modeling%20benefits%20from%20agentic%20workflows%2C%20we%20introduce%20CP-Agent%2C%20a%20Python%20coding%20agent%20using%20the%20ReAct%20framework%20with%20a%20persistent%20IPython%20kernel.%20Domain%20knowledge%20is%20provided%20through%20a%20project%20prompt%20of%20under%2050%20lines.%20The%20agent%20iteratively%20executes%20code%2C%20observes%20the%20solver%27s%20feedback%2C%20and%20refines%20models%20based%20on%20the%20execution%20results.%0A%20%20We%20evaluate%20CP-Agent%20on%20CP-Bench%27s%20101%20constraint%20programming%20problems.%20We%20clarified%20the%20benchmark%20to%20address%20systematic%20ambiguities%20in%20problem%20specifications%20and%20errors%20in%20ground-truth%20models.%20On%20the%20clarified%20benchmark%2C%20CP-Agent%20solves%20all%20101%20problems.%20Ablation%20studies%20indicate%20that%20minimal%20guidance%20outperforms%20detailed%20procedural%20scaffolding%2C%20and%20that%20explicit%20task%20management%20tools%20have%20mixed%20effects%20on%20focused%20modeling%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Agent%253A%2520Agentic%2520Constraint%2520Programming%26entry.906535625%3DStefan%2520Szeider%26entry.1292438233%3DTranslating%2520natural%2520language%2520into%2520formal%2520constraint%2520models%2520requires%2520expertise%2520in%2520the%2520problem%2520domain%2520and%2520modeling%2520frameworks.%2520To%2520investigate%2520whether%2520constraint%2520modeling%2520benefits%2520from%2520agentic%2520workflows%252C%2520we%2520introduce%2520CP-Agent%252C%2520a%2520Python%2520coding%2520agent%2520using%2520the%2520ReAct%2520framework%2520with%2520a%2520persistent%2520IPython%2520kernel.%2520Domain%2520knowledge%2520is%2520provided%2520through%2520a%2520project%2520prompt%2520of%2520under%252050%2520lines.%2520The%2520agent%2520iteratively%2520executes%2520code%252C%2520observes%2520the%2520solver%2527s%2520feedback%252C%2520and%2520refines%2520models%2520based%2520on%2520the%2520execution%2520results.%250A%2520%2520We%2520evaluate%2520CP-Agent%2520on%2520CP-Bench%2527s%2520101%2520constraint%2520programming%2520problems.%2520We%2520clarified%2520the%2520benchmark%2520to%2520address%2520systematic%2520ambiguities%2520in%2520problem%2520specifications%2520and%2520errors%2520in%2520ground-truth%2520models.%2520On%2520the%2520clarified%2520benchmark%252C%2520CP-Agent%2520solves%2520all%2520101%2520problems.%2520Ablation%2520studies%2520indicate%2520that%2520minimal%2520guidance%2520outperforms%2520detailed%2520procedural%2520scaffolding%252C%2520and%2520that%2520explicit%2520task%2520management%2520tools%2520have%2520mixed%2520effects%2520on%2520focused%2520modeling%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Agent%3A%20Agentic%20Constraint%20Programming&entry.906535625=Stefan%20Szeider&entry.1292438233=Translating%20natural%20language%20into%20formal%20constraint%20models%20requires%20expertise%20in%20the%20problem%20domain%20and%20modeling%20frameworks.%20To%20investigate%20whether%20constraint%20modeling%20benefits%20from%20agentic%20workflows%2C%20we%20introduce%20CP-Agent%2C%20a%20Python%20coding%20agent%20using%20the%20ReAct%20framework%20with%20a%20persistent%20IPython%20kernel.%20Domain%20knowledge%20is%20provided%20through%20a%20project%20prompt%20of%20under%2050%20lines.%20The%20agent%20iteratively%20executes%20code%2C%20observes%20the%20solver%27s%20feedback%2C%20and%20refines%20models%20based%20on%20the%20execution%20results.%0A%20%20We%20evaluate%20CP-Agent%20on%20CP-Bench%27s%20101%20constraint%20programming%20problems.%20We%20clarified%20the%20benchmark%20to%20address%20systematic%20ambiguities%20in%20problem%20specifications%20and%20errors%20in%20ground-truth%20models.%20On%20the%20clarified%20benchmark%2C%20CP-Agent%20solves%20all%20101%20problems.%20Ablation%20studies%20indicate%20that%20minimal%20guidance%20outperforms%20detailed%20procedural%20scaffolding%2C%20and%20that%20explicit%20task%20management%20tools%20have%20mixed%20effects%20on%20focused%20modeling%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2508.07468v2&entry.124074799=Read"},
{"title": "A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism", "author": "Yu Xue and Pengcheng Jiang and Chenchen Zhu and MengChu Zhou and Mohamed Wahib and Moncef Gabbouj", "abstract": "Neural architecture search (NAS) has emerged as a powerful paradigm that enables researchers to automatically explore vast search spaces and discover efficient neural networks. However, NAS suffers from a critical bottleneck, i.e. the evaluation of numerous architectures during the search process demands substantial computing resources and time. In order to improve the efficiency of NAS, a series of methods have been proposed to reduce the evaluation time of neural architectures. However, they are not efficient enough and still only focus on the accuracy of architectures. Beyond classification accuracy, real-world applications increasingly demand more efficient and compact network architectures that balance multiple performance criteria. To address these challenges, we propose the SMEMNAS, a pairwise comparison relation-assisted multi-objective evolutionary algorithm based on a multi-population mechanism. In the SMEMNAS, a surrogate model is constructed based on pairwise comparison relations to predict the accuracy ranking of architectures, rather than the absolute accuracy. Moreover, two populations cooperate with each other in the search process, i.e. a main population that guides the evolutionary process, while a vice population that enhances search diversity. Our method aims to discover high-performance models that simultaneously optimize multiple objectives. We conduct comprehensive experiments on CIFAR-10, CIFAR-100 and ImageNet datasets to validate the effectiveness of our approach. With only a single GPU searching for 0.17 days, competitive architectures can be found by SMEMNAS which achieves 78.91% accuracy with the MAdds of 570M on the ImageNet. This work makes a significant advancement in the field of NAS.", "link": "http://arxiv.org/abs/2407.15600v3", "date": "2025-12-26", "relevancy": 1.4718, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4949}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pairwise%20Comparison%20Relation-assisted%20Multi-objective%20Evolutionary%20Neural%20Architecture%20Search%20Method%20with%20Multi-population%20Mechanism&body=Title%3A%20A%20Pairwise%20Comparison%20Relation-assisted%20Multi-objective%20Evolutionary%20Neural%20Architecture%20Search%20Method%20with%20Multi-population%20Mechanism%0AAuthor%3A%20Yu%20Xue%20and%20Pengcheng%20Jiang%20and%20Chenchen%20Zhu%20and%20MengChu%20Zhou%20and%20Mohamed%20Wahib%20and%20Moncef%20Gabbouj%0AAbstract%3A%20Neural%20architecture%20search%20%28NAS%29%20has%20emerged%20as%20a%20powerful%20paradigm%20that%20enables%20researchers%20to%20automatically%20explore%20vast%20search%20spaces%20and%20discover%20efficient%20neural%20networks.%20However%2C%20NAS%20suffers%20from%20a%20critical%20bottleneck%2C%20i.e.%20the%20evaluation%20of%20numerous%20architectures%20during%20the%20search%20process%20demands%20substantial%20computing%20resources%20and%20time.%20In%20order%20to%20improve%20the%20efficiency%20of%20NAS%2C%20a%20series%20of%20methods%20have%20been%20proposed%20to%20reduce%20the%20evaluation%20time%20of%20neural%20architectures.%20However%2C%20they%20are%20not%20efficient%20enough%20and%20still%20only%20focus%20on%20the%20accuracy%20of%20architectures.%20Beyond%20classification%20accuracy%2C%20real-world%20applications%20increasingly%20demand%20more%20efficient%20and%20compact%20network%20architectures%20that%20balance%20multiple%20performance%20criteria.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20SMEMNAS%2C%20a%20pairwise%20comparison%20relation-assisted%20multi-objective%20evolutionary%20algorithm%20based%20on%20a%20multi-population%20mechanism.%20In%20the%20SMEMNAS%2C%20a%20surrogate%20model%20is%20constructed%20based%20on%20pairwise%20comparison%20relations%20to%20predict%20the%20accuracy%20ranking%20of%20architectures%2C%20rather%20than%20the%20absolute%20accuracy.%20Moreover%2C%20two%20populations%20cooperate%20with%20each%20other%20in%20the%20search%20process%2C%20i.e.%20a%20main%20population%20that%20guides%20the%20evolutionary%20process%2C%20while%20a%20vice%20population%20that%20enhances%20search%20diversity.%20Our%20method%20aims%20to%20discover%20high-performance%20models%20that%20simultaneously%20optimize%20multiple%20objectives.%20We%20conduct%20comprehensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%20datasets%20to%20validate%20the%20effectiveness%20of%20our%20approach.%20With%20only%20a%20single%20GPU%20searching%20for%200.17%20days%2C%20competitive%20architectures%20can%20be%20found%20by%20SMEMNAS%20which%20achieves%2078.91%25%20accuracy%20with%20the%20MAdds%20of%20570M%20on%20the%20ImageNet.%20This%20work%20makes%20a%20significant%20advancement%20in%20the%20field%20of%20NAS.%0ALink%3A%20http%3A//arxiv.org/abs/2407.15600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pairwise%2520Comparison%2520Relation-assisted%2520Multi-objective%2520Evolutionary%2520Neural%2520Architecture%2520Search%2520Method%2520with%2520Multi-population%2520Mechanism%26entry.906535625%3DYu%2520Xue%2520and%2520Pengcheng%2520Jiang%2520and%2520Chenchen%2520Zhu%2520and%2520MengChu%2520Zhou%2520and%2520Mohamed%2520Wahib%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3DNeural%2520architecture%2520search%2520%2528NAS%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520that%2520enables%2520researchers%2520to%2520automatically%2520explore%2520vast%2520search%2520spaces%2520and%2520discover%2520efficient%2520neural%2520networks.%2520However%252C%2520NAS%2520suffers%2520from%2520a%2520critical%2520bottleneck%252C%2520i.e.%2520the%2520evaluation%2520of%2520numerous%2520architectures%2520during%2520the%2520search%2520process%2520demands%2520substantial%2520computing%2520resources%2520and%2520time.%2520In%2520order%2520to%2520improve%2520the%2520efficiency%2520of%2520NAS%252C%2520a%2520series%2520of%2520methods%2520have%2520been%2520proposed%2520to%2520reduce%2520the%2520evaluation%2520time%2520of%2520neural%2520architectures.%2520However%252C%2520they%2520are%2520not%2520efficient%2520enough%2520and%2520still%2520only%2520focus%2520on%2520the%2520accuracy%2520of%2520architectures.%2520Beyond%2520classification%2520accuracy%252C%2520real-world%2520applications%2520increasingly%2520demand%2520more%2520efficient%2520and%2520compact%2520network%2520architectures%2520that%2520balance%2520multiple%2520performance%2520criteria.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520SMEMNAS%252C%2520a%2520pairwise%2520comparison%2520relation-assisted%2520multi-objective%2520evolutionary%2520algorithm%2520based%2520on%2520a%2520multi-population%2520mechanism.%2520In%2520the%2520SMEMNAS%252C%2520a%2520surrogate%2520model%2520is%2520constructed%2520based%2520on%2520pairwise%2520comparison%2520relations%2520to%2520predict%2520the%2520accuracy%2520ranking%2520of%2520architectures%252C%2520rather%2520than%2520the%2520absolute%2520accuracy.%2520Moreover%252C%2520two%2520populations%2520cooperate%2520with%2520each%2520other%2520in%2520the%2520search%2520process%252C%2520i.e.%2520a%2520main%2520population%2520that%2520guides%2520the%2520evolutionary%2520process%252C%2520while%2520a%2520vice%2520population%2520that%2520enhances%2520search%2520diversity.%2520Our%2520method%2520aims%2520to%2520discover%2520high-performance%2520models%2520that%2520simultaneously%2520optimize%2520multiple%2520objectives.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%2520and%2520ImageNet%2520datasets%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520With%2520only%2520a%2520single%2520GPU%2520searching%2520for%25200.17%2520days%252C%2520competitive%2520architectures%2520can%2520be%2520found%2520by%2520SMEMNAS%2520which%2520achieves%252078.91%2525%2520accuracy%2520with%2520the%2520MAdds%2520of%2520570M%2520on%2520the%2520ImageNet.%2520This%2520work%2520makes%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%2520NAS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pairwise%20Comparison%20Relation-assisted%20Multi-objective%20Evolutionary%20Neural%20Architecture%20Search%20Method%20with%20Multi-population%20Mechanism&entry.906535625=Yu%20Xue%20and%20Pengcheng%20Jiang%20and%20Chenchen%20Zhu%20and%20MengChu%20Zhou%20and%20Mohamed%20Wahib%20and%20Moncef%20Gabbouj&entry.1292438233=Neural%20architecture%20search%20%28NAS%29%20has%20emerged%20as%20a%20powerful%20paradigm%20that%20enables%20researchers%20to%20automatically%20explore%20vast%20search%20spaces%20and%20discover%20efficient%20neural%20networks.%20However%2C%20NAS%20suffers%20from%20a%20critical%20bottleneck%2C%20i.e.%20the%20evaluation%20of%20numerous%20architectures%20during%20the%20search%20process%20demands%20substantial%20computing%20resources%20and%20time.%20In%20order%20to%20improve%20the%20efficiency%20of%20NAS%2C%20a%20series%20of%20methods%20have%20been%20proposed%20to%20reduce%20the%20evaluation%20time%20of%20neural%20architectures.%20However%2C%20they%20are%20not%20efficient%20enough%20and%20still%20only%20focus%20on%20the%20accuracy%20of%20architectures.%20Beyond%20classification%20accuracy%2C%20real-world%20applications%20increasingly%20demand%20more%20efficient%20and%20compact%20network%20architectures%20that%20balance%20multiple%20performance%20criteria.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20SMEMNAS%2C%20a%20pairwise%20comparison%20relation-assisted%20multi-objective%20evolutionary%20algorithm%20based%20on%20a%20multi-population%20mechanism.%20In%20the%20SMEMNAS%2C%20a%20surrogate%20model%20is%20constructed%20based%20on%20pairwise%20comparison%20relations%20to%20predict%20the%20accuracy%20ranking%20of%20architectures%2C%20rather%20than%20the%20absolute%20accuracy.%20Moreover%2C%20two%20populations%20cooperate%20with%20each%20other%20in%20the%20search%20process%2C%20i.e.%20a%20main%20population%20that%20guides%20the%20evolutionary%20process%2C%20while%20a%20vice%20population%20that%20enhances%20search%20diversity.%20Our%20method%20aims%20to%20discover%20high-performance%20models%20that%20simultaneously%20optimize%20multiple%20objectives.%20We%20conduct%20comprehensive%20experiments%20on%20CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%20datasets%20to%20validate%20the%20effectiveness%20of%20our%20approach.%20With%20only%20a%20single%20GPU%20searching%20for%200.17%20days%2C%20competitive%20architectures%20can%20be%20found%20by%20SMEMNAS%20which%20achieves%2078.91%25%20accuracy%20with%20the%20MAdds%20of%20570M%20on%20the%20ImageNet.%20This%20work%20makes%20a%20significant%20advancement%20in%20the%20field%20of%20NAS.&entry.1838667208=http%3A//arxiv.org/abs/2407.15600v3&entry.124074799=Read"},
{"title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "author": "Cong Qi and Hanzhang Fang and Tianxing Hu and Siqi Jiang and Wei Zhi", "abstract": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.", "link": "http://arxiv.org/abs/2504.16956v2", "date": "2025-12-26", "relevancy": 1.4645, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Mamba%20for%20Single-Cell%20Data%3A%20Efficient%20Context%20Learning%20with%20Biological%20Fidelity&body=Title%3A%20Bidirectional%20Mamba%20for%20Single-Cell%20Data%3A%20Efficient%20Context%20Learning%20with%20Biological%20Fidelity%0AAuthor%3A%20Cong%20Qi%20and%20Hanzhang%20Fang%20and%20Tianxing%20Hu%20and%20Siqi%20Jiang%20and%20Wei%20Zhi%0AAbstract%3A%20Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20enables%20high-resolution%20analysis%20of%20cellular%20heterogeneity%2C%20but%20its%20complexity%2C%20which%20is%20marked%20by%20high%20dimensionality%2C%20sparsity%2C%20and%20batch%20effects%2C%20which%20poses%20major%20computational%20challenges.%20Transformer-based%20models%20have%20made%20significant%20advances%20in%20this%20domain%20but%20are%20often%20limited%20by%20their%20quadratic%20complexity%20and%20suboptimal%20handling%20of%20long-range%20dependencies.%20In%20this%20work%2C%20we%20introduce%20GeneMamba%2C%20a%20scalable%20and%20efficient%20foundation%20model%20for%20single-cell%20transcriptomics%20built%20on%20state%20space%20modeling.%20Leveraging%20the%20Bi-Mamba%20architecture%2C%20GeneMamba%20captures%20bidirectional%20gene%20context%20with%20linear-time%20complexity%2C%20offering%20substantial%20computational%20gains%20over%20transformer%20baselines.%20The%20model%20is%20pretrained%20on%20nearly%2030%20million%20cells%20and%20incorporates%20biologically%20informed%20objectives%2C%20including%20pathway-aware%20contrastive%20loss%20and%20rank-based%20gene%20encoding.%20We%20evaluate%20GeneMamba%20across%20diverse%20tasks%2C%20including%20multi-batch%20integration%2C%20cell%20type%20annotation%2C%20and%20gene-gene%20correlation%2C%20demonstrating%20strong%20performance%2C%20interpretability%2C%20and%20robustness.%20These%20results%20position%20GeneMamba%20as%20a%20practical%20and%20powerful%20alternative%20to%20transformer-based%20methods%2C%20advancing%20the%20development%20of%20biologically%20grounded%2C%20scalable%20tools%20for%20large-scale%20single-cell%20data%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2504.16956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Mamba%2520for%2520Single-Cell%2520Data%253A%2520Efficient%2520Context%2520Learning%2520with%2520Biological%2520Fidelity%26entry.906535625%3DCong%2520Qi%2520and%2520Hanzhang%2520Fang%2520and%2520Tianxing%2520Hu%2520and%2520Siqi%2520Jiang%2520and%2520Wei%2520Zhi%26entry.1292438233%3DSingle-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520enables%2520high-resolution%2520analysis%2520of%2520cellular%2520heterogeneity%252C%2520but%2520its%2520complexity%252C%2520which%2520is%2520marked%2520by%2520high%2520dimensionality%252C%2520sparsity%252C%2520and%2520batch%2520effects%252C%2520which%2520poses%2520major%2520computational%2520challenges.%2520Transformer-based%2520models%2520have%2520made%2520significant%2520advances%2520in%2520this%2520domain%2520but%2520are%2520often%2520limited%2520by%2520their%2520quadratic%2520complexity%2520and%2520suboptimal%2520handling%2520of%2520long-range%2520dependencies.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GeneMamba%252C%2520a%2520scalable%2520and%2520efficient%2520foundation%2520model%2520for%2520single-cell%2520transcriptomics%2520built%2520on%2520state%2520space%2520modeling.%2520Leveraging%2520the%2520Bi-Mamba%2520architecture%252C%2520GeneMamba%2520captures%2520bidirectional%2520gene%2520context%2520with%2520linear-time%2520complexity%252C%2520offering%2520substantial%2520computational%2520gains%2520over%2520transformer%2520baselines.%2520The%2520model%2520is%2520pretrained%2520on%2520nearly%252030%2520million%2520cells%2520and%2520incorporates%2520biologically%2520informed%2520objectives%252C%2520including%2520pathway-aware%2520contrastive%2520loss%2520and%2520rank-based%2520gene%2520encoding.%2520We%2520evaluate%2520GeneMamba%2520across%2520diverse%2520tasks%252C%2520including%2520multi-batch%2520integration%252C%2520cell%2520type%2520annotation%252C%2520and%2520gene-gene%2520correlation%252C%2520demonstrating%2520strong%2520performance%252C%2520interpretability%252C%2520and%2520robustness.%2520These%2520results%2520position%2520GeneMamba%2520as%2520a%2520practical%2520and%2520powerful%2520alternative%2520to%2520transformer-based%2520methods%252C%2520advancing%2520the%2520development%2520of%2520biologically%2520grounded%252C%2520scalable%2520tools%2520for%2520large-scale%2520single-cell%2520data%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Mamba%20for%20Single-Cell%20Data%3A%20Efficient%20Context%20Learning%20with%20Biological%20Fidelity&entry.906535625=Cong%20Qi%20and%20Hanzhang%20Fang%20and%20Tianxing%20Hu%20and%20Siqi%20Jiang%20and%20Wei%20Zhi&entry.1292438233=Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20enables%20high-resolution%20analysis%20of%20cellular%20heterogeneity%2C%20but%20its%20complexity%2C%20which%20is%20marked%20by%20high%20dimensionality%2C%20sparsity%2C%20and%20batch%20effects%2C%20which%20poses%20major%20computational%20challenges.%20Transformer-based%20models%20have%20made%20significant%20advances%20in%20this%20domain%20but%20are%20often%20limited%20by%20their%20quadratic%20complexity%20and%20suboptimal%20handling%20of%20long-range%20dependencies.%20In%20this%20work%2C%20we%20introduce%20GeneMamba%2C%20a%20scalable%20and%20efficient%20foundation%20model%20for%20single-cell%20transcriptomics%20built%20on%20state%20space%20modeling.%20Leveraging%20the%20Bi-Mamba%20architecture%2C%20GeneMamba%20captures%20bidirectional%20gene%20context%20with%20linear-time%20complexity%2C%20offering%20substantial%20computational%20gains%20over%20transformer%20baselines.%20The%20model%20is%20pretrained%20on%20nearly%2030%20million%20cells%20and%20incorporates%20biologically%20informed%20objectives%2C%20including%20pathway-aware%20contrastive%20loss%20and%20rank-based%20gene%20encoding.%20We%20evaluate%20GeneMamba%20across%20diverse%20tasks%2C%20including%20multi-batch%20integration%2C%20cell%20type%20annotation%2C%20and%20gene-gene%20correlation%2C%20demonstrating%20strong%20performance%2C%20interpretability%2C%20and%20robustness.%20These%20results%20position%20GeneMamba%20as%20a%20practical%20and%20powerful%20alternative%20to%20transformer-based%20methods%2C%20advancing%20the%20development%20of%20biologically%20grounded%2C%20scalable%20tools%20for%20large-scale%20single-cell%20data%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2504.16956v2&entry.124074799=Read"},
{"title": "Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation", "author": "Saksham Gupta and Sarthak Mishra and Arshad Ayub and Kamran Farooque and Spandan Roy and Babita Gupta", "abstract": "Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications. Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina. This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness. BRIS integrates a four-way steerable fiberoptic bronchoscope, an independent endotracheal tube advancement mechanism, and a camera-augmented mouthpiece compatible with standard clinical workflows. A learning-enabled closed-loop control framework leverages real-time shape sensing to map joystick inputs to distal bronchoscope tip motion in Cartesian space, providing stable and intuitive teleoperation under tendon nonlinearities and airway contact. Monocular endoscopic depth estimation is used to classify airway regions and provide interpretable, anatomy-aware guidance for safe tube positioning relative to the carina. The system is validated on high-fidelity airway mannequins under standard and difficult airway configurations, demonstrating reliable navigation and controlled tube placement. These results highlight BRIS as a step toward safer, more consistent, and clinically compatible robotic airway management.", "link": "http://arxiv.org/abs/2512.21983v1", "date": "2025-12-26", "relevancy": 1.4606, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4975}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bab_Sak%20Robotic%20Intubation%20System%20%28BRIS%29%3A%20A%20Learning-Enabled%20Control%20Framework%20for%20Safe%20Fiberoptic%20Endotracheal%20Intubation&body=Title%3A%20Bab_Sak%20Robotic%20Intubation%20System%20%28BRIS%29%3A%20A%20Learning-Enabled%20Control%20Framework%20for%20Safe%20Fiberoptic%20Endotracheal%20Intubation%0AAuthor%3A%20Saksham%20Gupta%20and%20Sarthak%20Mishra%20and%20Arshad%20Ayub%20and%20Kamran%20Farooque%20and%20Spandan%20Roy%20and%20Babita%20Gupta%0AAbstract%3A%20Endotracheal%20intubation%20is%20a%20critical%20yet%20technically%20demanding%20procedure%2C%20with%20failure%20or%20improper%20tube%20placement%20leading%20to%20severe%20complications.%20Existing%20robotic%20and%20teleoperated%20intubation%20systems%20primarily%20focus%20on%20airway%20navigation%20and%20do%20not%20provide%20integrated%20control%20of%20endotracheal%20tube%20advancement%20or%20objective%20verification%20of%20tube%20depth%20relative%20to%20the%20carina.%20This%20paper%20presents%20the%20Robotic%20Intubation%20System%20%28BRIS%29%2C%20a%20compact%2C%20human-in-the-loop%20platform%20designed%20to%20assist%20fiberoptic-guided%20intubation%20while%20enabling%20real-time%2C%20objective%20depth%20awareness.%20BRIS%20integrates%20a%20four-way%20steerable%20fiberoptic%20bronchoscope%2C%20an%20independent%20endotracheal%20tube%20advancement%20mechanism%2C%20and%20a%20camera-augmented%20mouthpiece%20compatible%20with%20standard%20clinical%20workflows.%20A%20learning-enabled%20closed-loop%20control%20framework%20leverages%20real-time%20shape%20sensing%20to%20map%20joystick%20inputs%20to%20distal%20bronchoscope%20tip%20motion%20in%20Cartesian%20space%2C%20providing%20stable%20and%20intuitive%20teleoperation%20under%20tendon%20nonlinearities%20and%20airway%20contact.%20Monocular%20endoscopic%20depth%20estimation%20is%20used%20to%20classify%20airway%20regions%20and%20provide%20interpretable%2C%20anatomy-aware%20guidance%20for%20safe%20tube%20positioning%20relative%20to%20the%20carina.%20The%20system%20is%20validated%20on%20high-fidelity%20airway%20mannequins%20under%20standard%20and%20difficult%20airway%20configurations%2C%20demonstrating%20reliable%20navigation%20and%20controlled%20tube%20placement.%20These%20results%20highlight%20BRIS%20as%20a%20step%20toward%20safer%2C%20more%20consistent%2C%20and%20clinically%20compatible%20robotic%20airway%20management.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBab_Sak%2520Robotic%2520Intubation%2520System%2520%2528BRIS%2529%253A%2520A%2520Learning-Enabled%2520Control%2520Framework%2520for%2520Safe%2520Fiberoptic%2520Endotracheal%2520Intubation%26entry.906535625%3DSaksham%2520Gupta%2520and%2520Sarthak%2520Mishra%2520and%2520Arshad%2520Ayub%2520and%2520Kamran%2520Farooque%2520and%2520Spandan%2520Roy%2520and%2520Babita%2520Gupta%26entry.1292438233%3DEndotracheal%2520intubation%2520is%2520a%2520critical%2520yet%2520technically%2520demanding%2520procedure%252C%2520with%2520failure%2520or%2520improper%2520tube%2520placement%2520leading%2520to%2520severe%2520complications.%2520Existing%2520robotic%2520and%2520teleoperated%2520intubation%2520systems%2520primarily%2520focus%2520on%2520airway%2520navigation%2520and%2520do%2520not%2520provide%2520integrated%2520control%2520of%2520endotracheal%2520tube%2520advancement%2520or%2520objective%2520verification%2520of%2520tube%2520depth%2520relative%2520to%2520the%2520carina.%2520This%2520paper%2520presents%2520the%2520Robotic%2520Intubation%2520System%2520%2528BRIS%2529%252C%2520a%2520compact%252C%2520human-in-the-loop%2520platform%2520designed%2520to%2520assist%2520fiberoptic-guided%2520intubation%2520while%2520enabling%2520real-time%252C%2520objective%2520depth%2520awareness.%2520BRIS%2520integrates%2520a%2520four-way%2520steerable%2520fiberoptic%2520bronchoscope%252C%2520an%2520independent%2520endotracheal%2520tube%2520advancement%2520mechanism%252C%2520and%2520a%2520camera-augmented%2520mouthpiece%2520compatible%2520with%2520standard%2520clinical%2520workflows.%2520A%2520learning-enabled%2520closed-loop%2520control%2520framework%2520leverages%2520real-time%2520shape%2520sensing%2520to%2520map%2520joystick%2520inputs%2520to%2520distal%2520bronchoscope%2520tip%2520motion%2520in%2520Cartesian%2520space%252C%2520providing%2520stable%2520and%2520intuitive%2520teleoperation%2520under%2520tendon%2520nonlinearities%2520and%2520airway%2520contact.%2520Monocular%2520endoscopic%2520depth%2520estimation%2520is%2520used%2520to%2520classify%2520airway%2520regions%2520and%2520provide%2520interpretable%252C%2520anatomy-aware%2520guidance%2520for%2520safe%2520tube%2520positioning%2520relative%2520to%2520the%2520carina.%2520The%2520system%2520is%2520validated%2520on%2520high-fidelity%2520airway%2520mannequins%2520under%2520standard%2520and%2520difficult%2520airway%2520configurations%252C%2520demonstrating%2520reliable%2520navigation%2520and%2520controlled%2520tube%2520placement.%2520These%2520results%2520highlight%2520BRIS%2520as%2520a%2520step%2520toward%2520safer%252C%2520more%2520consistent%252C%2520and%2520clinically%2520compatible%2520robotic%2520airway%2520management.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bab_Sak%20Robotic%20Intubation%20System%20%28BRIS%29%3A%20A%20Learning-Enabled%20Control%20Framework%20for%20Safe%20Fiberoptic%20Endotracheal%20Intubation&entry.906535625=Saksham%20Gupta%20and%20Sarthak%20Mishra%20and%20Arshad%20Ayub%20and%20Kamran%20Farooque%20and%20Spandan%20Roy%20and%20Babita%20Gupta&entry.1292438233=Endotracheal%20intubation%20is%20a%20critical%20yet%20technically%20demanding%20procedure%2C%20with%20failure%20or%20improper%20tube%20placement%20leading%20to%20severe%20complications.%20Existing%20robotic%20and%20teleoperated%20intubation%20systems%20primarily%20focus%20on%20airway%20navigation%20and%20do%20not%20provide%20integrated%20control%20of%20endotracheal%20tube%20advancement%20or%20objective%20verification%20of%20tube%20depth%20relative%20to%20the%20carina.%20This%20paper%20presents%20the%20Robotic%20Intubation%20System%20%28BRIS%29%2C%20a%20compact%2C%20human-in-the-loop%20platform%20designed%20to%20assist%20fiberoptic-guided%20intubation%20while%20enabling%20real-time%2C%20objective%20depth%20awareness.%20BRIS%20integrates%20a%20four-way%20steerable%20fiberoptic%20bronchoscope%2C%20an%20independent%20endotracheal%20tube%20advancement%20mechanism%2C%20and%20a%20camera-augmented%20mouthpiece%20compatible%20with%20standard%20clinical%20workflows.%20A%20learning-enabled%20closed-loop%20control%20framework%20leverages%20real-time%20shape%20sensing%20to%20map%20joystick%20inputs%20to%20distal%20bronchoscope%20tip%20motion%20in%20Cartesian%20space%2C%20providing%20stable%20and%20intuitive%20teleoperation%20under%20tendon%20nonlinearities%20and%20airway%20contact.%20Monocular%20endoscopic%20depth%20estimation%20is%20used%20to%20classify%20airway%20regions%20and%20provide%20interpretable%2C%20anatomy-aware%20guidance%20for%20safe%20tube%20positioning%20relative%20to%20the%20carina.%20The%20system%20is%20validated%20on%20high-fidelity%20airway%20mannequins%20under%20standard%20and%20difficult%20airway%20configurations%2C%20demonstrating%20reliable%20navigation%20and%20controlled%20tube%20placement.%20These%20results%20highlight%20BRIS%20as%20a%20step%20toward%20safer%2C%20more%20consistent%2C%20and%20clinically%20compatible%20robotic%20airway%20management.&entry.1838667208=http%3A//arxiv.org/abs/2512.21983v1&entry.124074799=Read"},
{"title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting", "author": "Shuyu Gan and Renxiang Wang and James Mooney and Dongyeop Kang", "abstract": "Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.", "link": "http://arxiv.org/abs/2512.22101v1", "date": "2025-12-26", "relevancy": 1.459, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4904}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4827}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A2P-Vis%3A%20an%20Analyzer-to-Presenter%20Agentic%20Pipeline%20for%20Visual%20Insights%20Generation%20and%20Reporting&body=Title%3A%20A2P-Vis%3A%20an%20Analyzer-to-Presenter%20Agentic%20Pipeline%20for%20Visual%20Insights%20Generation%20and%20Reporting%0AAuthor%3A%20Shuyu%20Gan%20and%20Renxiang%20Wang%20and%20James%20Mooney%20and%20Dongyeop%20Kang%0AAbstract%3A%20Automating%20end-to-end%20data%20science%20pipeline%20with%20AI%20agents%20still%20stalls%20on%20two%20gaps%3A%20generating%20insightful%2C%20diverse%20visual%20evidence%20and%20assembling%20it%20into%20a%20coherent%2C%20professional%20report.%20We%20present%20A2P-Vis%2C%20a%20two-part%2C%20multi-agent%20pipeline%20that%20turns%20raw%20datasets%20into%20a%20high-quality%20data-visualization%20report.%20The%20Data%20Analyzer%20orchestrates%20profiling%2C%20proposes%20diverse%20visualization%20directions%2C%20generates%20and%20executes%20plotting%20code%2C%20filters%20low-quality%20figures%20with%20a%20legibility%20checker%2C%20and%20elicits%20candidate%20insights%20that%20are%20automatically%20scored%20for%20depth%2C%20correctness%2C%20specificity%2C%20depth%20and%20actionability.%20The%20Presenter%20then%20orders%20topics%2C%20composes%20chart-grounded%20narratives%20from%20the%20top-ranked%20insights%2C%20writes%20justified%20transitions%2C%20and%20revises%20the%20document%20for%20clarity%20and%20consistency%2C%20yielding%20a%20coherent%2C%20publication-ready%20report.%20Together%2C%20these%20agents%20convert%20raw%20data%20into%20curated%20materials%20%28charts%20%2B%20vetted%20insights%29%20and%20into%20a%20readable%20narrative%20without%20manual%20glue%20work.%20We%20claim%20that%20by%20coupling%20a%20quality-assured%20Analyzer%20with%20a%20narrative%20Presenter%2C%20A2P-Vis%20operationalizes%20co-analysis%20end-to-end%2C%20improving%20the%20real-world%20usefulness%20of%20automated%20data%20analysis%20for%20practitioners.%20For%20the%20complete%20dataset%20report%2C%20please%20see%3A%20https%3A//www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA2P-Vis%253A%2520an%2520Analyzer-to-Presenter%2520Agentic%2520Pipeline%2520for%2520Visual%2520Insights%2520Generation%2520and%2520Reporting%26entry.906535625%3DShuyu%2520Gan%2520and%2520Renxiang%2520Wang%2520and%2520James%2520Mooney%2520and%2520Dongyeop%2520Kang%26entry.1292438233%3DAutomating%2520end-to-end%2520data%2520science%2520pipeline%2520with%2520AI%2520agents%2520still%2520stalls%2520on%2520two%2520gaps%253A%2520generating%2520insightful%252C%2520diverse%2520visual%2520evidence%2520and%2520assembling%2520it%2520into%2520a%2520coherent%252C%2520professional%2520report.%2520We%2520present%2520A2P-Vis%252C%2520a%2520two-part%252C%2520multi-agent%2520pipeline%2520that%2520turns%2520raw%2520datasets%2520into%2520a%2520high-quality%2520data-visualization%2520report.%2520The%2520Data%2520Analyzer%2520orchestrates%2520profiling%252C%2520proposes%2520diverse%2520visualization%2520directions%252C%2520generates%2520and%2520executes%2520plotting%2520code%252C%2520filters%2520low-quality%2520figures%2520with%2520a%2520legibility%2520checker%252C%2520and%2520elicits%2520candidate%2520insights%2520that%2520are%2520automatically%2520scored%2520for%2520depth%252C%2520correctness%252C%2520specificity%252C%2520depth%2520and%2520actionability.%2520The%2520Presenter%2520then%2520orders%2520topics%252C%2520composes%2520chart-grounded%2520narratives%2520from%2520the%2520top-ranked%2520insights%252C%2520writes%2520justified%2520transitions%252C%2520and%2520revises%2520the%2520document%2520for%2520clarity%2520and%2520consistency%252C%2520yielding%2520a%2520coherent%252C%2520publication-ready%2520report.%2520Together%252C%2520these%2520agents%2520convert%2520raw%2520data%2520into%2520curated%2520materials%2520%2528charts%2520%252B%2520vetted%2520insights%2529%2520and%2520into%2520a%2520readable%2520narrative%2520without%2520manual%2520glue%2520work.%2520We%2520claim%2520that%2520by%2520coupling%2520a%2520quality-assured%2520Analyzer%2520with%2520a%2520narrative%2520Presenter%252C%2520A2P-Vis%2520operationalizes%2520co-analysis%2520end-to-end%252C%2520improving%2520the%2520real-world%2520usefulness%2520of%2520automated%2520data%2520analysis%2520for%2520practitioners.%2520For%2520the%2520complete%2520dataset%2520report%252C%2520please%2520see%253A%2520https%253A//www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2P-Vis%3A%20an%20Analyzer-to-Presenter%20Agentic%20Pipeline%20for%20Visual%20Insights%20Generation%20and%20Reporting&entry.906535625=Shuyu%20Gan%20and%20Renxiang%20Wang%20and%20James%20Mooney%20and%20Dongyeop%20Kang&entry.1292438233=Automating%20end-to-end%20data%20science%20pipeline%20with%20AI%20agents%20still%20stalls%20on%20two%20gaps%3A%20generating%20insightful%2C%20diverse%20visual%20evidence%20and%20assembling%20it%20into%20a%20coherent%2C%20professional%20report.%20We%20present%20A2P-Vis%2C%20a%20two-part%2C%20multi-agent%20pipeline%20that%20turns%20raw%20datasets%20into%20a%20high-quality%20data-visualization%20report.%20The%20Data%20Analyzer%20orchestrates%20profiling%2C%20proposes%20diverse%20visualization%20directions%2C%20generates%20and%20executes%20plotting%20code%2C%20filters%20low-quality%20figures%20with%20a%20legibility%20checker%2C%20and%20elicits%20candidate%20insights%20that%20are%20automatically%20scored%20for%20depth%2C%20correctness%2C%20specificity%2C%20depth%20and%20actionability.%20The%20Presenter%20then%20orders%20topics%2C%20composes%20chart-grounded%20narratives%20from%20the%20top-ranked%20insights%2C%20writes%20justified%20transitions%2C%20and%20revises%20the%20document%20for%20clarity%20and%20consistency%2C%20yielding%20a%20coherent%2C%20publication-ready%20report.%20Together%2C%20these%20agents%20convert%20raw%20data%20into%20curated%20materials%20%28charts%20%2B%20vetted%20insights%29%20and%20into%20a%20readable%20narrative%20without%20manual%20glue%20work.%20We%20claim%20that%20by%20coupling%20a%20quality-assured%20Analyzer%20with%20a%20narrative%20Presenter%2C%20A2P-Vis%20operationalizes%20co-analysis%20end-to-end%2C%20improving%20the%20real-world%20usefulness%20of%20automated%20data%20analysis%20for%20practitioners.%20For%20the%20complete%20dataset%20report%2C%20please%20see%3A%20https%3A//www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.&entry.1838667208=http%3A//arxiv.org/abs/2512.22101v1&entry.124074799=Read"},
{"title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "author": "Shashi Kant Gupta and Arijeet Pramanik and Jerrin John Thomas and Regina Schwind and Lauren Wiener and Avi Raju and Jeremy Kornbluth and Yanshan Wang and Zhaohui Su and Hrituraj Singh", "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "link": "http://arxiv.org/abs/2512.19864v2", "date": "2025-12-26", "relevancy": 1.4451, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4933}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARMON-E%3A%20Hierarchical%20Agentic%20Reasoning%20for%20Multimodal%20Oncology%20Notes%20to%20Extract%20Structured%20Data&body=Title%3A%20HARMON-E%3A%20Hierarchical%20Agentic%20Reasoning%20for%20Multimodal%20Oncology%20Notes%20to%20Extract%20Structured%20Data%0AAuthor%3A%20Shashi%20Kant%20Gupta%20and%20Arijeet%20Pramanik%20and%20Jerrin%20John%20Thomas%20and%20Regina%20Schwind%20and%20Lauren%20Wiener%20and%20Avi%20Raju%20and%20Jeremy%20Kornbluth%20and%20Yanshan%20Wang%20and%20Zhaohui%20Su%20and%20Hrituraj%20Singh%0AAbstract%3A%20Unstructured%20notes%20within%20the%20electronic%20health%20record%20%28EHR%29%20contain%20rich%20clinical%20information%20vital%20for%20cancer%20treatment%20decision%20making%20and%20research%2C%20yet%20reliably%20extracting%20structured%20oncology%20data%20remains%20challenging%20due%20to%20extensive%20variability%2C%20specialized%20terminology%2C%20and%20inconsistent%20document%20formats.%20Manual%20abstraction%2C%20although%20accurate%2C%20is%20prohibitively%20costly%20and%20unscalable.%20Existing%20automated%20approaches%20typically%20address%20narrow%20scenarios%20-%20either%20using%20synthetic%20datasets%2C%20restricting%20focus%20to%20document-level%20extraction%2C%20or%20isolating%20specific%20clinical%20variables%20%28e.g.%2C%20staging%2C%20biomarkers%2C%20histology%29%20-%20and%20do%20not%20adequately%20handle%20patient-level%20synthesis%20across%20the%20large%20number%20of%20clinical%20documents%20containing%20contradictory%20information.%20In%20this%20study%2C%20we%20propose%20an%20agentic%20framework%20that%20systematically%20decomposes%20complex%20oncology%20data%20extraction%20into%20modular%2C%20adaptive%20tasks.%20Specifically%2C%20we%20use%20large%20language%20models%20%28LLMs%29%20as%20reasoning%20agents%2C%20equipped%20with%20context-sensitive%20retrieval%20and%20iterative%20synthesis%20capabilities%2C%20to%20exhaustively%20and%20comprehensively%20extract%20structured%20clinical%20variables%20from%20real-world%20oncology%20notes.%20Evaluated%20on%20a%20large-scale%20dataset%20of%20over%20400%2C000%20unstructured%20clinical%20notes%20and%20scanned%20PDF%20reports%20spanning%202%2C250%20cancer%20patients%2C%20our%20method%20achieves%20an%20average%20F1-score%20of%200.93%2C%20with%20100%20out%20of%20103%20oncology-specific%20clinical%20variables%20exceeding%200.85%2C%20and%20critical%20variables%20%28e.g.%2C%20biomarkers%20and%20medications%29%20surpassing%200.95.%20Moreover%2C%20integration%20of%20the%20agentic%20system%20into%20a%20data%20curation%20workflow%20resulted%20in%200.94%20direct%20manual%20approval%20rate%2C%20significantly%20reducing%20annotation%20costs.%20To%20our%20knowledge%2C%20this%20constitutes%20the%20first%20exhaustive%2C%20end-to-end%20application%20of%20LLM-based%20agents%20for%20structured%20oncology%20data%20extraction%20at%20scale%0ALink%3A%20http%3A//arxiv.org/abs/2512.19864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARMON-E%253A%2520Hierarchical%2520Agentic%2520Reasoning%2520for%2520Multimodal%2520Oncology%2520Notes%2520to%2520Extract%2520Structured%2520Data%26entry.906535625%3DShashi%2520Kant%2520Gupta%2520and%2520Arijeet%2520Pramanik%2520and%2520Jerrin%2520John%2520Thomas%2520and%2520Regina%2520Schwind%2520and%2520Lauren%2520Wiener%2520and%2520Avi%2520Raju%2520and%2520Jeremy%2520Kornbluth%2520and%2520Yanshan%2520Wang%2520and%2520Zhaohui%2520Su%2520and%2520Hrituraj%2520Singh%26entry.1292438233%3DUnstructured%2520notes%2520within%2520the%2520electronic%2520health%2520record%2520%2528EHR%2529%2520contain%2520rich%2520clinical%2520information%2520vital%2520for%2520cancer%2520treatment%2520decision%2520making%2520and%2520research%252C%2520yet%2520reliably%2520extracting%2520structured%2520oncology%2520data%2520remains%2520challenging%2520due%2520to%2520extensive%2520variability%252C%2520specialized%2520terminology%252C%2520and%2520inconsistent%2520document%2520formats.%2520Manual%2520abstraction%252C%2520although%2520accurate%252C%2520is%2520prohibitively%2520costly%2520and%2520unscalable.%2520Existing%2520automated%2520approaches%2520typically%2520address%2520narrow%2520scenarios%2520-%2520either%2520using%2520synthetic%2520datasets%252C%2520restricting%2520focus%2520to%2520document-level%2520extraction%252C%2520or%2520isolating%2520specific%2520clinical%2520variables%2520%2528e.g.%252C%2520staging%252C%2520biomarkers%252C%2520histology%2529%2520-%2520and%2520do%2520not%2520adequately%2520handle%2520patient-level%2520synthesis%2520across%2520the%2520large%2520number%2520of%2520clinical%2520documents%2520containing%2520contradictory%2520information.%2520In%2520this%2520study%252C%2520we%2520propose%2520an%2520agentic%2520framework%2520that%2520systematically%2520decomposes%2520complex%2520oncology%2520data%2520extraction%2520into%2520modular%252C%2520adaptive%2520tasks.%2520Specifically%252C%2520we%2520use%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520reasoning%2520agents%252C%2520equipped%2520with%2520context-sensitive%2520retrieval%2520and%2520iterative%2520synthesis%2520capabilities%252C%2520to%2520exhaustively%2520and%2520comprehensively%2520extract%2520structured%2520clinical%2520variables%2520from%2520real-world%2520oncology%2520notes.%2520Evaluated%2520on%2520a%2520large-scale%2520dataset%2520of%2520over%2520400%252C000%2520unstructured%2520clinical%2520notes%2520and%2520scanned%2520PDF%2520reports%2520spanning%25202%252C250%2520cancer%2520patients%252C%2520our%2520method%2520achieves%2520an%2520average%2520F1-score%2520of%25200.93%252C%2520with%2520100%2520out%2520of%2520103%2520oncology-specific%2520clinical%2520variables%2520exceeding%25200.85%252C%2520and%2520critical%2520variables%2520%2528e.g.%252C%2520biomarkers%2520and%2520medications%2529%2520surpassing%25200.95.%2520Moreover%252C%2520integration%2520of%2520the%2520agentic%2520system%2520into%2520a%2520data%2520curation%2520workflow%2520resulted%2520in%25200.94%2520direct%2520manual%2520approval%2520rate%252C%2520significantly%2520reducing%2520annotation%2520costs.%2520To%2520our%2520knowledge%252C%2520this%2520constitutes%2520the%2520first%2520exhaustive%252C%2520end-to-end%2520application%2520of%2520LLM-based%2520agents%2520for%2520structured%2520oncology%2520data%2520extraction%2520at%2520scale%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARMON-E%3A%20Hierarchical%20Agentic%20Reasoning%20for%20Multimodal%20Oncology%20Notes%20to%20Extract%20Structured%20Data&entry.906535625=Shashi%20Kant%20Gupta%20and%20Arijeet%20Pramanik%20and%20Jerrin%20John%20Thomas%20and%20Regina%20Schwind%20and%20Lauren%20Wiener%20and%20Avi%20Raju%20and%20Jeremy%20Kornbluth%20and%20Yanshan%20Wang%20and%20Zhaohui%20Su%20and%20Hrituraj%20Singh&entry.1292438233=Unstructured%20notes%20within%20the%20electronic%20health%20record%20%28EHR%29%20contain%20rich%20clinical%20information%20vital%20for%20cancer%20treatment%20decision%20making%20and%20research%2C%20yet%20reliably%20extracting%20structured%20oncology%20data%20remains%20challenging%20due%20to%20extensive%20variability%2C%20specialized%20terminology%2C%20and%20inconsistent%20document%20formats.%20Manual%20abstraction%2C%20although%20accurate%2C%20is%20prohibitively%20costly%20and%20unscalable.%20Existing%20automated%20approaches%20typically%20address%20narrow%20scenarios%20-%20either%20using%20synthetic%20datasets%2C%20restricting%20focus%20to%20document-level%20extraction%2C%20or%20isolating%20specific%20clinical%20variables%20%28e.g.%2C%20staging%2C%20biomarkers%2C%20histology%29%20-%20and%20do%20not%20adequately%20handle%20patient-level%20synthesis%20across%20the%20large%20number%20of%20clinical%20documents%20containing%20contradictory%20information.%20In%20this%20study%2C%20we%20propose%20an%20agentic%20framework%20that%20systematically%20decomposes%20complex%20oncology%20data%20extraction%20into%20modular%2C%20adaptive%20tasks.%20Specifically%2C%20we%20use%20large%20language%20models%20%28LLMs%29%20as%20reasoning%20agents%2C%20equipped%20with%20context-sensitive%20retrieval%20and%20iterative%20synthesis%20capabilities%2C%20to%20exhaustively%20and%20comprehensively%20extract%20structured%20clinical%20variables%20from%20real-world%20oncology%20notes.%20Evaluated%20on%20a%20large-scale%20dataset%20of%20over%20400%2C000%20unstructured%20clinical%20notes%20and%20scanned%20PDF%20reports%20spanning%202%2C250%20cancer%20patients%2C%20our%20method%20achieves%20an%20average%20F1-score%20of%200.93%2C%20with%20100%20out%20of%20103%20oncology-specific%20clinical%20variables%20exceeding%200.85%2C%20and%20critical%20variables%20%28e.g.%2C%20biomarkers%20and%20medications%29%20surpassing%200.95.%20Moreover%2C%20integration%20of%20the%20agentic%20system%20into%20a%20data%20curation%20workflow%20resulted%20in%200.94%20direct%20manual%20approval%20rate%2C%20significantly%20reducing%20annotation%20costs.%20To%20our%20knowledge%2C%20this%20constitutes%20the%20first%20exhaustive%2C%20end-to-end%20application%20of%20LLM-based%20agents%20for%20structured%20oncology%20data%20extraction%20at%20scale&entry.1838667208=http%3A//arxiv.org/abs/2512.19864v2&entry.124074799=Read"},
{"title": "Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow", "author": "Kristiyan Sakalyan and Alessandro Palma and Filippo Guerranti and Fabian J. Theis and Stephan G\u00fcnnemann", "abstract": "Understanding the evolution of cellular microenvironments in spatiotemporal data is essential for deciphering tissue development and disease progression. While experimental techniques like spatial transcriptomics now enable high-resolution mapping of tissue organization across space and time, current methods that model cellular evolution operate at the single-cell level, overlooking the coordinated development of cellular states in a tissue. We introduce NicheFlow, a flow-based generative model that infers the temporal trajectory of cellular microenvironments across sequential spatial slides. By representing local cell neighborhoods as point clouds, NicheFlow jointly models the evolution of cell states and spatial coordinates using optimal transport and Variational Flow Matching. Our approach successfully recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.", "link": "http://arxiv.org/abs/2511.00977v2", "date": "2025-12-26", "relevancy": 1.4252, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.486}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4747}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Microenvironment%20Trajectories%20on%20Spatial%20Transcriptomics%20with%20NicheFlow&body=Title%3A%20Modeling%20Microenvironment%20Trajectories%20on%20Spatial%20Transcriptomics%20with%20NicheFlow%0AAuthor%3A%20Kristiyan%20Sakalyan%20and%20Alessandro%20Palma%20and%20Filippo%20Guerranti%20and%20Fabian%20J.%20Theis%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20Understanding%20the%20evolution%20of%20cellular%20microenvironments%20in%20spatiotemporal%20data%20is%20essential%20for%20deciphering%20tissue%20development%20and%20disease%20progression.%20While%20experimental%20techniques%20like%20spatial%20transcriptomics%20now%20enable%20high-resolution%20mapping%20of%20tissue%20organization%20across%20space%20and%20time%2C%20current%20methods%20that%20model%20cellular%20evolution%20operate%20at%20the%20single-cell%20level%2C%20overlooking%20the%20coordinated%20development%20of%20cellular%20states%20in%20a%20tissue.%20We%20introduce%20NicheFlow%2C%20a%20flow-based%20generative%20model%20that%20infers%20the%20temporal%20trajectory%20of%20cellular%20microenvironments%20across%20sequential%20spatial%20slides.%20By%20representing%20local%20cell%20neighborhoods%20as%20point%20clouds%2C%20NicheFlow%20jointly%20models%20the%20evolution%20of%20cell%20states%20and%20spatial%20coordinates%20using%20optimal%20transport%20and%20Variational%20Flow%20Matching.%20Our%20approach%20successfully%20recovers%20both%20global%20spatial%20architecture%20and%20local%20microenvironment%20composition%20across%20diverse%20spatiotemporal%20datasets%2C%20from%20embryonic%20to%20brain%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Microenvironment%2520Trajectories%2520on%2520Spatial%2520Transcriptomics%2520with%2520NicheFlow%26entry.906535625%3DKristiyan%2520Sakalyan%2520and%2520Alessandro%2520Palma%2520and%2520Filippo%2520Guerranti%2520and%2520Fabian%2520J.%2520Theis%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3DUnderstanding%2520the%2520evolution%2520of%2520cellular%2520microenvironments%2520in%2520spatiotemporal%2520data%2520is%2520essential%2520for%2520deciphering%2520tissue%2520development%2520and%2520disease%2520progression.%2520While%2520experimental%2520techniques%2520like%2520spatial%2520transcriptomics%2520now%2520enable%2520high-resolution%2520mapping%2520of%2520tissue%2520organization%2520across%2520space%2520and%2520time%252C%2520current%2520methods%2520that%2520model%2520cellular%2520evolution%2520operate%2520at%2520the%2520single-cell%2520level%252C%2520overlooking%2520the%2520coordinated%2520development%2520of%2520cellular%2520states%2520in%2520a%2520tissue.%2520We%2520introduce%2520NicheFlow%252C%2520a%2520flow-based%2520generative%2520model%2520that%2520infers%2520the%2520temporal%2520trajectory%2520of%2520cellular%2520microenvironments%2520across%2520sequential%2520spatial%2520slides.%2520By%2520representing%2520local%2520cell%2520neighborhoods%2520as%2520point%2520clouds%252C%2520NicheFlow%2520jointly%2520models%2520the%2520evolution%2520of%2520cell%2520states%2520and%2520spatial%2520coordinates%2520using%2520optimal%2520transport%2520and%2520Variational%2520Flow%2520Matching.%2520Our%2520approach%2520successfully%2520recovers%2520both%2520global%2520spatial%2520architecture%2520and%2520local%2520microenvironment%2520composition%2520across%2520diverse%2520spatiotemporal%2520datasets%252C%2520from%2520embryonic%2520to%2520brain%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Microenvironment%20Trajectories%20on%20Spatial%20Transcriptomics%20with%20NicheFlow&entry.906535625=Kristiyan%20Sakalyan%20and%20Alessandro%20Palma%20and%20Filippo%20Guerranti%20and%20Fabian%20J.%20Theis%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=Understanding%20the%20evolution%20of%20cellular%20microenvironments%20in%20spatiotemporal%20data%20is%20essential%20for%20deciphering%20tissue%20development%20and%20disease%20progression.%20While%20experimental%20techniques%20like%20spatial%20transcriptomics%20now%20enable%20high-resolution%20mapping%20of%20tissue%20organization%20across%20space%20and%20time%2C%20current%20methods%20that%20model%20cellular%20evolution%20operate%20at%20the%20single-cell%20level%2C%20overlooking%20the%20coordinated%20development%20of%20cellular%20states%20in%20a%20tissue.%20We%20introduce%20NicheFlow%2C%20a%20flow-based%20generative%20model%20that%20infers%20the%20temporal%20trajectory%20of%20cellular%20microenvironments%20across%20sequential%20spatial%20slides.%20By%20representing%20local%20cell%20neighborhoods%20as%20point%20clouds%2C%20NicheFlow%20jointly%20models%20the%20evolution%20of%20cell%20states%20and%20spatial%20coordinates%20using%20optimal%20transport%20and%20Variational%20Flow%20Matching.%20Our%20approach%20successfully%20recovers%20both%20global%20spatial%20architecture%20and%20local%20microenvironment%20composition%20across%20diverse%20spatiotemporal%20datasets%2C%20from%20embryonic%20to%20brain%20development.&entry.1838667208=http%3A//arxiv.org/abs/2511.00977v2&entry.124074799=Read"},
{"title": "M2RU: Memristive Minion Recurrent Unit for On-Chip Continual Learning at the Edge", "author": "Abdullah M. Zyarah and Dhireesha Kudithipudi", "abstract": "Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.", "link": "http://arxiv.org/abs/2512.17299v2", "date": "2025-12-26", "relevancy": 1.4153, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5105}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M2RU%3A%20Memristive%20Minion%20Recurrent%20Unit%20for%20On-Chip%20Continual%20Learning%20at%20the%20Edge&body=Title%3A%20M2RU%3A%20Memristive%20Minion%20Recurrent%20Unit%20for%20On-Chip%20Continual%20Learning%20at%20the%20Edge%0AAuthor%3A%20Abdullah%20M.%20Zyarah%20and%20Dhireesha%20Kudithipudi%0AAbstract%3A%20Continual%20learning%20on%20edge%20platforms%20remains%20challenging%20because%20recurrent%20networks%20depend%20on%20energy-intensive%20training%20procedures%20and%20frequent%20data%20movement%20that%20are%20impractical%20for%20embedded%20deployments.%20This%20work%20introduces%20M2RU%2C%20a%20mixed-signal%20architecture%20that%20implements%20the%20minion%20recurrent%20unit%20for%20efficient%20temporal%20processing%20with%20on-chip%20continual%20learning.%20The%20architecture%20integrates%20weighted-bit%20streaming%2C%20which%20enables%20multi-bit%20digital%20inputs%20to%20be%20processed%20in%20crossbars%20without%20high-resolution%20conversion%2C%20and%20an%20experience%20replay%20mechanism%20that%20stabilizes%20learning%20under%20domain%20shifts.%20M2RU%20achieves%2015%20GOPS%20at%2048.62%20mW%2C%20corresponding%20to%20312%20GOPS%20per%20watt%2C%20and%20maintains%20accuracy%20within%205%20percent%20of%20software%20baselines%20on%20sequential%20MNIST%20and%20CIFAR-10%20tasks.%20Compared%20with%20a%20CMOS%20digital%20design%2C%20the%20accelerator%20provides%2029X%20improvement%20in%20energy%20efficiency.%20Device-aware%20analysis%20shows%20an%20expected%20operational%20lifetime%20of%2012.2%20years%20under%20continual%20learning%20workloads.%20These%20results%20establish%20M2RU%20as%20a%20scalable%20and%20energy-efficient%20platform%20for%20real-time%20adaptation%20in%20edge-level%20temporal%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM2RU%253A%2520Memristive%2520Minion%2520Recurrent%2520Unit%2520for%2520On-Chip%2520Continual%2520Learning%2520at%2520the%2520Edge%26entry.906535625%3DAbdullah%2520M.%2520Zyarah%2520and%2520Dhireesha%2520Kudithipudi%26entry.1292438233%3DContinual%2520learning%2520on%2520edge%2520platforms%2520remains%2520challenging%2520because%2520recurrent%2520networks%2520depend%2520on%2520energy-intensive%2520training%2520procedures%2520and%2520frequent%2520data%2520movement%2520that%2520are%2520impractical%2520for%2520embedded%2520deployments.%2520This%2520work%2520introduces%2520M2RU%252C%2520a%2520mixed-signal%2520architecture%2520that%2520implements%2520the%2520minion%2520recurrent%2520unit%2520for%2520efficient%2520temporal%2520processing%2520with%2520on-chip%2520continual%2520learning.%2520The%2520architecture%2520integrates%2520weighted-bit%2520streaming%252C%2520which%2520enables%2520multi-bit%2520digital%2520inputs%2520to%2520be%2520processed%2520in%2520crossbars%2520without%2520high-resolution%2520conversion%252C%2520and%2520an%2520experience%2520replay%2520mechanism%2520that%2520stabilizes%2520learning%2520under%2520domain%2520shifts.%2520M2RU%2520achieves%252015%2520GOPS%2520at%252048.62%2520mW%252C%2520corresponding%2520to%2520312%2520GOPS%2520per%2520watt%252C%2520and%2520maintains%2520accuracy%2520within%25205%2520percent%2520of%2520software%2520baselines%2520on%2520sequential%2520MNIST%2520and%2520CIFAR-10%2520tasks.%2520Compared%2520with%2520a%2520CMOS%2520digital%2520design%252C%2520the%2520accelerator%2520provides%252029X%2520improvement%2520in%2520energy%2520efficiency.%2520Device-aware%2520analysis%2520shows%2520an%2520expected%2520operational%2520lifetime%2520of%252012.2%2520years%2520under%2520continual%2520learning%2520workloads.%2520These%2520results%2520establish%2520M2RU%2520as%2520a%2520scalable%2520and%2520energy-efficient%2520platform%2520for%2520real-time%2520adaptation%2520in%2520edge-level%2520temporal%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M2RU%3A%20Memristive%20Minion%20Recurrent%20Unit%20for%20On-Chip%20Continual%20Learning%20at%20the%20Edge&entry.906535625=Abdullah%20M.%20Zyarah%20and%20Dhireesha%20Kudithipudi&entry.1292438233=Continual%20learning%20on%20edge%20platforms%20remains%20challenging%20because%20recurrent%20networks%20depend%20on%20energy-intensive%20training%20procedures%20and%20frequent%20data%20movement%20that%20are%20impractical%20for%20embedded%20deployments.%20This%20work%20introduces%20M2RU%2C%20a%20mixed-signal%20architecture%20that%20implements%20the%20minion%20recurrent%20unit%20for%20efficient%20temporal%20processing%20with%20on-chip%20continual%20learning.%20The%20architecture%20integrates%20weighted-bit%20streaming%2C%20which%20enables%20multi-bit%20digital%20inputs%20to%20be%20processed%20in%20crossbars%20without%20high-resolution%20conversion%2C%20and%20an%20experience%20replay%20mechanism%20that%20stabilizes%20learning%20under%20domain%20shifts.%20M2RU%20achieves%2015%20GOPS%20at%2048.62%20mW%2C%20corresponding%20to%20312%20GOPS%20per%20watt%2C%20and%20maintains%20accuracy%20within%205%20percent%20of%20software%20baselines%20on%20sequential%20MNIST%20and%20CIFAR-10%20tasks.%20Compared%20with%20a%20CMOS%20digital%20design%2C%20the%20accelerator%20provides%2029X%20improvement%20in%20energy%20efficiency.%20Device-aware%20analysis%20shows%20an%20expected%20operational%20lifetime%20of%2012.2%20years%20under%20continual%20learning%20workloads.%20These%20results%20establish%20M2RU%20as%20a%20scalable%20and%20energy-efficient%20platform%20for%20real-time%20adaptation%20in%20edge-level%20temporal%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.17299v2&entry.124074799=Read"},
{"title": "AI-Enhanced Real-Time Wi-Fi Sensing Through Single Transceiver Pair", "author": "Yuxuan Liu and Chiya Zhang and Yifeng Yuan and Chunlong He and Weizheng Zhang and Gaojie Chen", "abstract": "The advancement of next-generation Wi-Fi technology heavily relies on sensing capabilities, which play a pivotal role in enabling sophisticated applications. In response to the growing demand for large-scale deployments, contemporary Wi-Fi sensing systems strive to achieve high-precision perception while maintaining minimal bandwidth consumption and antenna count requirements. Remarkably, various AI-driven perception technologies have demonstrated the ability to surpass the traditional resolution limitations imposed by radar theory. However, the theoretical underpinnings of this phenomenon have not been thoroughly investigated in existing research. In this study, we found that under hardware-constrained conditions, the performance gains brought by AI to Wi-Fi sensing systems primarily originate from two aspects: prior information and temporal correlation. Prior information enables the AI to generate plausible details based on vague input, while temporal correlation helps reduce the upper bound of sensing error. Building on these insights, we developed a real-time, AI-based Wi-Fi sensing and visualization system using a single transceiver pair, and designed experiments focusing on human pose estimation and indoor localization. The system operates in real time on commodity hardware, and experimental results confirm our theoretical findings.", "link": "http://arxiv.org/abs/2511.02845v2", "date": "2025-12-26", "relevancy": 1.4067, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Enhanced%20Real-Time%20Wi-Fi%20Sensing%20Through%20Single%20Transceiver%20Pair&body=Title%3A%20AI-Enhanced%20Real-Time%20Wi-Fi%20Sensing%20Through%20Single%20Transceiver%20Pair%0AAuthor%3A%20Yuxuan%20Liu%20and%20Chiya%20Zhang%20and%20Yifeng%20Yuan%20and%20Chunlong%20He%20and%20Weizheng%20Zhang%20and%20Gaojie%20Chen%0AAbstract%3A%20The%20advancement%20of%20next-generation%20Wi-Fi%20technology%20heavily%20relies%20on%20sensing%20capabilities%2C%20which%20play%20a%20pivotal%20role%20in%20enabling%20sophisticated%20applications.%20In%20response%20to%20the%20growing%20demand%20for%20large-scale%20deployments%2C%20contemporary%20Wi-Fi%20sensing%20systems%20strive%20to%20achieve%20high-precision%20perception%20while%20maintaining%20minimal%20bandwidth%20consumption%20and%20antenna%20count%20requirements.%20Remarkably%2C%20various%20AI-driven%20perception%20technologies%20have%20demonstrated%20the%20ability%20to%20surpass%20the%20traditional%20resolution%20limitations%20imposed%20by%20radar%20theory.%20However%2C%20the%20theoretical%20underpinnings%20of%20this%20phenomenon%20have%20not%20been%20thoroughly%20investigated%20in%20existing%20research.%20In%20this%20study%2C%20we%20found%20that%20under%20hardware-constrained%20conditions%2C%20the%20performance%20gains%20brought%20by%20AI%20to%20Wi-Fi%20sensing%20systems%20primarily%20originate%20from%20two%20aspects%3A%20prior%20information%20and%20temporal%20correlation.%20Prior%20information%20enables%20the%20AI%20to%20generate%20plausible%20details%20based%20on%20vague%20input%2C%20while%20temporal%20correlation%20helps%20reduce%20the%20upper%20bound%20of%20sensing%20error.%20Building%20on%20these%20insights%2C%20we%20developed%20a%20real-time%2C%20AI-based%20Wi-Fi%20sensing%20and%20visualization%20system%20using%20a%20single%20transceiver%20pair%2C%20and%20designed%20experiments%20focusing%20on%20human%20pose%20estimation%20and%20indoor%20localization.%20The%20system%20operates%20in%20real%20time%20on%20commodity%20hardware%2C%20and%20experimental%20results%20confirm%20our%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Enhanced%2520Real-Time%2520Wi-Fi%2520Sensing%2520Through%2520Single%2520Transceiver%2520Pair%26entry.906535625%3DYuxuan%2520Liu%2520and%2520Chiya%2520Zhang%2520and%2520Yifeng%2520Yuan%2520and%2520Chunlong%2520He%2520and%2520Weizheng%2520Zhang%2520and%2520Gaojie%2520Chen%26entry.1292438233%3DThe%2520advancement%2520of%2520next-generation%2520Wi-Fi%2520technology%2520heavily%2520relies%2520on%2520sensing%2520capabilities%252C%2520which%2520play%2520a%2520pivotal%2520role%2520in%2520enabling%2520sophisticated%2520applications.%2520In%2520response%2520to%2520the%2520growing%2520demand%2520for%2520large-scale%2520deployments%252C%2520contemporary%2520Wi-Fi%2520sensing%2520systems%2520strive%2520to%2520achieve%2520high-precision%2520perception%2520while%2520maintaining%2520minimal%2520bandwidth%2520consumption%2520and%2520antenna%2520count%2520requirements.%2520Remarkably%252C%2520various%2520AI-driven%2520perception%2520technologies%2520have%2520demonstrated%2520the%2520ability%2520to%2520surpass%2520the%2520traditional%2520resolution%2520limitations%2520imposed%2520by%2520radar%2520theory.%2520However%252C%2520the%2520theoretical%2520underpinnings%2520of%2520this%2520phenomenon%2520have%2520not%2520been%2520thoroughly%2520investigated%2520in%2520existing%2520research.%2520In%2520this%2520study%252C%2520we%2520found%2520that%2520under%2520hardware-constrained%2520conditions%252C%2520the%2520performance%2520gains%2520brought%2520by%2520AI%2520to%2520Wi-Fi%2520sensing%2520systems%2520primarily%2520originate%2520from%2520two%2520aspects%253A%2520prior%2520information%2520and%2520temporal%2520correlation.%2520Prior%2520information%2520enables%2520the%2520AI%2520to%2520generate%2520plausible%2520details%2520based%2520on%2520vague%2520input%252C%2520while%2520temporal%2520correlation%2520helps%2520reduce%2520the%2520upper%2520bound%2520of%2520sensing%2520error.%2520Building%2520on%2520these%2520insights%252C%2520we%2520developed%2520a%2520real-time%252C%2520AI-based%2520Wi-Fi%2520sensing%2520and%2520visualization%2520system%2520using%2520a%2520single%2520transceiver%2520pair%252C%2520and%2520designed%2520experiments%2520focusing%2520on%2520human%2520pose%2520estimation%2520and%2520indoor%2520localization.%2520The%2520system%2520operates%2520in%2520real%2520time%2520on%2520commodity%2520hardware%252C%2520and%2520experimental%2520results%2520confirm%2520our%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Enhanced%20Real-Time%20Wi-Fi%20Sensing%20Through%20Single%20Transceiver%20Pair&entry.906535625=Yuxuan%20Liu%20and%20Chiya%20Zhang%20and%20Yifeng%20Yuan%20and%20Chunlong%20He%20and%20Weizheng%20Zhang%20and%20Gaojie%20Chen&entry.1292438233=The%20advancement%20of%20next-generation%20Wi-Fi%20technology%20heavily%20relies%20on%20sensing%20capabilities%2C%20which%20play%20a%20pivotal%20role%20in%20enabling%20sophisticated%20applications.%20In%20response%20to%20the%20growing%20demand%20for%20large-scale%20deployments%2C%20contemporary%20Wi-Fi%20sensing%20systems%20strive%20to%20achieve%20high-precision%20perception%20while%20maintaining%20minimal%20bandwidth%20consumption%20and%20antenna%20count%20requirements.%20Remarkably%2C%20various%20AI-driven%20perception%20technologies%20have%20demonstrated%20the%20ability%20to%20surpass%20the%20traditional%20resolution%20limitations%20imposed%20by%20radar%20theory.%20However%2C%20the%20theoretical%20underpinnings%20of%20this%20phenomenon%20have%20not%20been%20thoroughly%20investigated%20in%20existing%20research.%20In%20this%20study%2C%20we%20found%20that%20under%20hardware-constrained%20conditions%2C%20the%20performance%20gains%20brought%20by%20AI%20to%20Wi-Fi%20sensing%20systems%20primarily%20originate%20from%20two%20aspects%3A%20prior%20information%20and%20temporal%20correlation.%20Prior%20information%20enables%20the%20AI%20to%20generate%20plausible%20details%20based%20on%20vague%20input%2C%20while%20temporal%20correlation%20helps%20reduce%20the%20upper%20bound%20of%20sensing%20error.%20Building%20on%20these%20insights%2C%20we%20developed%20a%20real-time%2C%20AI-based%20Wi-Fi%20sensing%20and%20visualization%20system%20using%20a%20single%20transceiver%20pair%2C%20and%20designed%20experiments%20focusing%20on%20human%20pose%20estimation%20and%20indoor%20localization.%20The%20system%20operates%20in%20real%20time%20on%20commodity%20hardware%2C%20and%20experimental%20results%20confirm%20our%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2511.02845v2&entry.124074799=Read"},
{"title": "Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission", "author": "Sergio Hernandez and Christophe Peucheret and Francesco Da Ros and Darko Zibar", "abstract": "Directly modulated lasers (DMLs) are an attractive technology for short-reach intensity modulation and direct detection communication systems. However, their complex nonlinear dynamics make the modeling and optimization of DML-based systems challenging. In this paper, we study the end-to-end optimization of DML-based systems based on a data-driven surrogate model trained on experimental data. The end-to-end optimization includes the pulse shaping and equalizer filters, the bias current and the modulation radio-frequency (RF) power applied to the laser. The performance of the end-to-end optimization scheme is tested on the experimental setup and compared to 4 different benchmark schemes based on linear and nonlinear receiver-side equalization. The results show that the proposed end-to-end scheme is able to deliver better performance throughout the studied symbol rates and transmission distances while employing lower modulation RF power, fewer filter taps and utilizing a smaller signal bandwidth.", "link": "http://arxiv.org/abs/2508.19910v2", "date": "2025-12-26", "relevancy": 1.3767, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4812}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4562}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experimental%20End-to-End%20Optimization%20of%20Directly%20Modulated%20Laser-based%20IM/DD%20Transmission&body=Title%3A%20Experimental%20End-to-End%20Optimization%20of%20Directly%20Modulated%20Laser-based%20IM/DD%20Transmission%0AAuthor%3A%20Sergio%20Hernandez%20and%20Christophe%20Peucheret%20and%20Francesco%20Da%20Ros%20and%20Darko%20Zibar%0AAbstract%3A%20Directly%20modulated%20lasers%20%28DMLs%29%20are%20an%20attractive%20technology%20for%20short-reach%20intensity%20modulation%20and%20direct%20detection%20communication%20systems.%20However%2C%20their%20complex%20nonlinear%20dynamics%20make%20the%20modeling%20and%20optimization%20of%20DML-based%20systems%20challenging.%20In%20this%20paper%2C%20we%20study%20the%20end-to-end%20optimization%20of%20DML-based%20systems%20based%20on%20a%20data-driven%20surrogate%20model%20trained%20on%20experimental%20data.%20The%20end-to-end%20optimization%20includes%20the%20pulse%20shaping%20and%20equalizer%20filters%2C%20the%20bias%20current%20and%20the%20modulation%20radio-frequency%20%28RF%29%20power%20applied%20to%20the%20laser.%20The%20performance%20of%20the%20end-to-end%20optimization%20scheme%20is%20tested%20on%20the%20experimental%20setup%20and%20compared%20to%204%20different%20benchmark%20schemes%20based%20on%20linear%20and%20nonlinear%20receiver-side%20equalization.%20The%20results%20show%20that%20the%20proposed%20end-to-end%20scheme%20is%20able%20to%20deliver%20better%20performance%20throughout%20the%20studied%20symbol%20rates%20and%20transmission%20distances%20while%20employing%20lower%20modulation%20RF%20power%2C%20fewer%20filter%20taps%20and%20utilizing%20a%20smaller%20signal%20bandwidth.%0ALink%3A%20http%3A//arxiv.org/abs/2508.19910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperimental%2520End-to-End%2520Optimization%2520of%2520Directly%2520Modulated%2520Laser-based%2520IM/DD%2520Transmission%26entry.906535625%3DSergio%2520Hernandez%2520and%2520Christophe%2520Peucheret%2520and%2520Francesco%2520Da%2520Ros%2520and%2520Darko%2520Zibar%26entry.1292438233%3DDirectly%2520modulated%2520lasers%2520%2528DMLs%2529%2520are%2520an%2520attractive%2520technology%2520for%2520short-reach%2520intensity%2520modulation%2520and%2520direct%2520detection%2520communication%2520systems.%2520However%252C%2520their%2520complex%2520nonlinear%2520dynamics%2520make%2520the%2520modeling%2520and%2520optimization%2520of%2520DML-based%2520systems%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520end-to-end%2520optimization%2520of%2520DML-based%2520systems%2520based%2520on%2520a%2520data-driven%2520surrogate%2520model%2520trained%2520on%2520experimental%2520data.%2520The%2520end-to-end%2520optimization%2520includes%2520the%2520pulse%2520shaping%2520and%2520equalizer%2520filters%252C%2520the%2520bias%2520current%2520and%2520the%2520modulation%2520radio-frequency%2520%2528RF%2529%2520power%2520applied%2520to%2520the%2520laser.%2520The%2520performance%2520of%2520the%2520end-to-end%2520optimization%2520scheme%2520is%2520tested%2520on%2520the%2520experimental%2520setup%2520and%2520compared%2520to%25204%2520different%2520benchmark%2520schemes%2520based%2520on%2520linear%2520and%2520nonlinear%2520receiver-side%2520equalization.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520end-to-end%2520scheme%2520is%2520able%2520to%2520deliver%2520better%2520performance%2520throughout%2520the%2520studied%2520symbol%2520rates%2520and%2520transmission%2520distances%2520while%2520employing%2520lower%2520modulation%2520RF%2520power%252C%2520fewer%2520filter%2520taps%2520and%2520utilizing%2520a%2520smaller%2520signal%2520bandwidth.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experimental%20End-to-End%20Optimization%20of%20Directly%20Modulated%20Laser-based%20IM/DD%20Transmission&entry.906535625=Sergio%20Hernandez%20and%20Christophe%20Peucheret%20and%20Francesco%20Da%20Ros%20and%20Darko%20Zibar&entry.1292438233=Directly%20modulated%20lasers%20%28DMLs%29%20are%20an%20attractive%20technology%20for%20short-reach%20intensity%20modulation%20and%20direct%20detection%20communication%20systems.%20However%2C%20their%20complex%20nonlinear%20dynamics%20make%20the%20modeling%20and%20optimization%20of%20DML-based%20systems%20challenging.%20In%20this%20paper%2C%20we%20study%20the%20end-to-end%20optimization%20of%20DML-based%20systems%20based%20on%20a%20data-driven%20surrogate%20model%20trained%20on%20experimental%20data.%20The%20end-to-end%20optimization%20includes%20the%20pulse%20shaping%20and%20equalizer%20filters%2C%20the%20bias%20current%20and%20the%20modulation%20radio-frequency%20%28RF%29%20power%20applied%20to%20the%20laser.%20The%20performance%20of%20the%20end-to-end%20optimization%20scheme%20is%20tested%20on%20the%20experimental%20setup%20and%20compared%20to%204%20different%20benchmark%20schemes%20based%20on%20linear%20and%20nonlinear%20receiver-side%20equalization.%20The%20results%20show%20that%20the%20proposed%20end-to-end%20scheme%20is%20able%20to%20deliver%20better%20performance%20throughout%20the%20studied%20symbol%20rates%20and%20transmission%20distances%20while%20employing%20lower%20modulation%20RF%20power%2C%20fewer%20filter%20taps%20and%20utilizing%20a%20smaller%20signal%20bandwidth.&entry.1838667208=http%3A//arxiv.org/abs/2508.19910v2&entry.124074799=Read"},
{"title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico", "author": "Mart\u00ed Medina-Hern\u00e1ndez and Janos Kert\u00e9sz and Mih\u00e1ly Fazekas", "abstract": "Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.", "link": "http://arxiv.org/abs/2512.19491v2", "date": "2025-12-26", "relevancy": 1.329, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4569}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20sanctioned%20government%20suppliers%3A%20A%20machine%20learning%20and%20network%20science%20approach%20to%20detecting%20fraud%20and%20corruption%20in%20Mexico&body=Title%3A%20Learning%20from%20sanctioned%20government%20suppliers%3A%20A%20machine%20learning%20and%20network%20science%20approach%20to%20detecting%20fraud%20and%20corruption%20in%20Mexico%0AAuthor%3A%20Mart%C3%AD%20Medina-Hern%C3%A1ndez%20and%20Janos%20Kert%C3%A9sz%20and%20Mih%C3%A1ly%20Fazekas%0AAbstract%3A%20Detecting%20fraud%20and%20corruption%20in%20public%20procurement%20remains%20a%20major%20challenge%20for%20governments%20worldwide.%20Most%20research%20to-date%20builds%20on%20domain-knowledge-based%20corruption%20risk%20indicators%20of%20individual%20contract-level%20features%20and%20some%20also%20analyzes%20contracting%20network%20patterns.%20A%20critical%20barrier%20for%20supervised%20machine%20learning%20is%20the%20absence%20of%20confirmed%20non-corrupt%2C%20negative%2C%20examples%2C%20which%20makes%20conventional%20machine%20learning%20inappropriate%20for%20this%20task.%20Using%20publicly%20available%20data%20on%20federally%20funded%20procurement%20in%20Mexico%20and%20company%20sanction%20records%2C%20this%20study%20implements%20positive-unlabeled%20%28PU%29%20learning%20algorithms%20that%20integrate%20domain-knowledge-based%20red%20flags%20with%20network-derived%20features%20to%20identify%20likely%20corrupt%20and%20fraudulent%20contracts.%20The%20best-performing%20PU%20model%20on%20average%20captures%2032%20percent%20more%20known%20positives%20and%20performs%20on%20average%202.3%20times%20better%20than%20random%20guessing%2C%20substantially%20outperforming%20approaches%20based%20solely%20on%20traditional%20red%20flags.%20The%20analysis%20of%20the%20Shapley%20Additive%20Explanations%20reveals%20that%20network-derived%20features%2C%20particularly%20those%20associated%20with%20contracts%20in%20the%20network%20core%20or%20suppliers%20with%20high%20eigenvector%20centrality%2C%20are%20the%20most%20important.%20Traditional%20red%20flags%20further%20enhance%20model%20performance%20in%20line%20with%20expectations%2C%20albeit%20mainly%20for%20contracts%20awarded%20through%20competitive%20tenders.%20This%20methodology%20can%20support%20law%20enforcement%20in%20Mexico%2C%20and%20it%20can%20be%20adapted%20to%20other%20national%20contexts%20too.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520sanctioned%2520government%2520suppliers%253A%2520A%2520machine%2520learning%2520and%2520network%2520science%2520approach%2520to%2520detecting%2520fraud%2520and%2520corruption%2520in%2520Mexico%26entry.906535625%3DMart%25C3%25AD%2520Medina-Hern%25C3%25A1ndez%2520and%2520Janos%2520Kert%25C3%25A9sz%2520and%2520Mih%25C3%25A1ly%2520Fazekas%26entry.1292438233%3DDetecting%2520fraud%2520and%2520corruption%2520in%2520public%2520procurement%2520remains%2520a%2520major%2520challenge%2520for%2520governments%2520worldwide.%2520Most%2520research%2520to-date%2520builds%2520on%2520domain-knowledge-based%2520corruption%2520risk%2520indicators%2520of%2520individual%2520contract-level%2520features%2520and%2520some%2520also%2520analyzes%2520contracting%2520network%2520patterns.%2520A%2520critical%2520barrier%2520for%2520supervised%2520machine%2520learning%2520is%2520the%2520absence%2520of%2520confirmed%2520non-corrupt%252C%2520negative%252C%2520examples%252C%2520which%2520makes%2520conventional%2520machine%2520learning%2520inappropriate%2520for%2520this%2520task.%2520Using%2520publicly%2520available%2520data%2520on%2520federally%2520funded%2520procurement%2520in%2520Mexico%2520and%2520company%2520sanction%2520records%252C%2520this%2520study%2520implements%2520positive-unlabeled%2520%2528PU%2529%2520learning%2520algorithms%2520that%2520integrate%2520domain-knowledge-based%2520red%2520flags%2520with%2520network-derived%2520features%2520to%2520identify%2520likely%2520corrupt%2520and%2520fraudulent%2520contracts.%2520The%2520best-performing%2520PU%2520model%2520on%2520average%2520captures%252032%2520percent%2520more%2520known%2520positives%2520and%2520performs%2520on%2520average%25202.3%2520times%2520better%2520than%2520random%2520guessing%252C%2520substantially%2520outperforming%2520approaches%2520based%2520solely%2520on%2520traditional%2520red%2520flags.%2520The%2520analysis%2520of%2520the%2520Shapley%2520Additive%2520Explanations%2520reveals%2520that%2520network-derived%2520features%252C%2520particularly%2520those%2520associated%2520with%2520contracts%2520in%2520the%2520network%2520core%2520or%2520suppliers%2520with%2520high%2520eigenvector%2520centrality%252C%2520are%2520the%2520most%2520important.%2520Traditional%2520red%2520flags%2520further%2520enhance%2520model%2520performance%2520in%2520line%2520with%2520expectations%252C%2520albeit%2520mainly%2520for%2520contracts%2520awarded%2520through%2520competitive%2520tenders.%2520This%2520methodology%2520can%2520support%2520law%2520enforcement%2520in%2520Mexico%252C%2520and%2520it%2520can%2520be%2520adapted%2520to%2520other%2520national%2520contexts%2520too.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20sanctioned%20government%20suppliers%3A%20A%20machine%20learning%20and%20network%20science%20approach%20to%20detecting%20fraud%20and%20corruption%20in%20Mexico&entry.906535625=Mart%C3%AD%20Medina-Hern%C3%A1ndez%20and%20Janos%20Kert%C3%A9sz%20and%20Mih%C3%A1ly%20Fazekas&entry.1292438233=Detecting%20fraud%20and%20corruption%20in%20public%20procurement%20remains%20a%20major%20challenge%20for%20governments%20worldwide.%20Most%20research%20to-date%20builds%20on%20domain-knowledge-based%20corruption%20risk%20indicators%20of%20individual%20contract-level%20features%20and%20some%20also%20analyzes%20contracting%20network%20patterns.%20A%20critical%20barrier%20for%20supervised%20machine%20learning%20is%20the%20absence%20of%20confirmed%20non-corrupt%2C%20negative%2C%20examples%2C%20which%20makes%20conventional%20machine%20learning%20inappropriate%20for%20this%20task.%20Using%20publicly%20available%20data%20on%20federally%20funded%20procurement%20in%20Mexico%20and%20company%20sanction%20records%2C%20this%20study%20implements%20positive-unlabeled%20%28PU%29%20learning%20algorithms%20that%20integrate%20domain-knowledge-based%20red%20flags%20with%20network-derived%20features%20to%20identify%20likely%20corrupt%20and%20fraudulent%20contracts.%20The%20best-performing%20PU%20model%20on%20average%20captures%2032%20percent%20more%20known%20positives%20and%20performs%20on%20average%202.3%20times%20better%20than%20random%20guessing%2C%20substantially%20outperforming%20approaches%20based%20solely%20on%20traditional%20red%20flags.%20The%20analysis%20of%20the%20Shapley%20Additive%20Explanations%20reveals%20that%20network-derived%20features%2C%20particularly%20those%20associated%20with%20contracts%20in%20the%20network%20core%20or%20suppliers%20with%20high%20eigenvector%20centrality%2C%20are%20the%20most%20important.%20Traditional%20red%20flags%20further%20enhance%20model%20performance%20in%20line%20with%20expectations%2C%20albeit%20mainly%20for%20contracts%20awarded%20through%20competitive%20tenders.%20This%20methodology%20can%20support%20law%20enforcement%20in%20Mexico%2C%20and%20it%20can%20be%20adapted%20to%20other%20national%20contexts%20too.&entry.1838667208=http%3A//arxiv.org/abs/2512.19491v2&entry.124074799=Read"},
{"title": "The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology", "author": "Sungwoo Kang", "abstract": "Smartphone-based tele-dermatology assumes that colorimetric calibration ensures clinical reliability, yet this remains untested for underrepresented skin phototypes. We investigated whether standard calibration translates to reliable clinical biomarkers using 43,425 images from 965 Korean subjects (Fitzpatrick III-IV) across DSLR, tablet, and smartphone devices. While Linear Color Correction Matrix (CCM) normalization reduced color error by 67-77% -- achieving near-clinical accuracy (Delta E < 2.3) -- this success did not translate to biomarker reliability.\n  We identify a phenomenon termed \"color-clinical decoupling\": despite perceptual accuracy, the Individual Typology Angle (ITA) showed poor inter-device agreement (ICC = 0.40), while the Melanin Index achieved good agreement (ICC = 0.77). This decoupling is driven by the ITA formula's sensitivity to b* channel noise and is further compounded by anatomical variance. Facial region accounts for 25.2% of color variance -- 3.6x greater than device effects (7.0%) -- challenging the efficacy of single-patch calibration. Our results demonstrate that current colorimetric standards are insufficient for clinical-grade biomarker extraction, necessitating region-aware protocols for mobile dermatology.", "link": "http://arxiv.org/abs/2512.21988v1", "date": "2025-12-26", "relevancy": 1.2632, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4434}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.44}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Color-Clinical%20Decoupling%3A%20Why%20Perceptual%20Calibration%20Fails%20Clinical%20Biomarkers%20in%20Smartphone%20Dermatology&body=Title%3A%20The%20Color-Clinical%20Decoupling%3A%20Why%20Perceptual%20Calibration%20Fails%20Clinical%20Biomarkers%20in%20Smartphone%20Dermatology%0AAuthor%3A%20Sungwoo%20Kang%0AAbstract%3A%20Smartphone-based%20tele-dermatology%20assumes%20that%20colorimetric%20calibration%20ensures%20clinical%20reliability%2C%20yet%20this%20remains%20untested%20for%20underrepresented%20skin%20phototypes.%20We%20investigated%20whether%20standard%20calibration%20translates%20to%20reliable%20clinical%20biomarkers%20using%2043%2C425%20images%20from%20965%20Korean%20subjects%20%28Fitzpatrick%20III-IV%29%20across%20DSLR%2C%20tablet%2C%20and%20smartphone%20devices.%20While%20Linear%20Color%20Correction%20Matrix%20%28CCM%29%20normalization%20reduced%20color%20error%20by%2067-77%25%20--%20achieving%20near-clinical%20accuracy%20%28Delta%20E%20%3C%202.3%29%20--%20this%20success%20did%20not%20translate%20to%20biomarker%20reliability.%0A%20%20We%20identify%20a%20phenomenon%20termed%20%22color-clinical%20decoupling%22%3A%20despite%20perceptual%20accuracy%2C%20the%20Individual%20Typology%20Angle%20%28ITA%29%20showed%20poor%20inter-device%20agreement%20%28ICC%20%3D%200.40%29%2C%20while%20the%20Melanin%20Index%20achieved%20good%20agreement%20%28ICC%20%3D%200.77%29.%20This%20decoupling%20is%20driven%20by%20the%20ITA%20formula%27s%20sensitivity%20to%20b%2A%20channel%20noise%20and%20is%20further%20compounded%20by%20anatomical%20variance.%20Facial%20region%20accounts%20for%2025.2%25%20of%20color%20variance%20--%203.6x%20greater%20than%20device%20effects%20%287.0%25%29%20--%20challenging%20the%20efficacy%20of%20single-patch%20calibration.%20Our%20results%20demonstrate%20that%20current%20colorimetric%20standards%20are%20insufficient%20for%20clinical-grade%20biomarker%20extraction%2C%20necessitating%20region-aware%20protocols%20for%20mobile%20dermatology.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Color-Clinical%2520Decoupling%253A%2520Why%2520Perceptual%2520Calibration%2520Fails%2520Clinical%2520Biomarkers%2520in%2520Smartphone%2520Dermatology%26entry.906535625%3DSungwoo%2520Kang%26entry.1292438233%3DSmartphone-based%2520tele-dermatology%2520assumes%2520that%2520colorimetric%2520calibration%2520ensures%2520clinical%2520reliability%252C%2520yet%2520this%2520remains%2520untested%2520for%2520underrepresented%2520skin%2520phototypes.%2520We%2520investigated%2520whether%2520standard%2520calibration%2520translates%2520to%2520reliable%2520clinical%2520biomarkers%2520using%252043%252C425%2520images%2520from%2520965%2520Korean%2520subjects%2520%2528Fitzpatrick%2520III-IV%2529%2520across%2520DSLR%252C%2520tablet%252C%2520and%2520smartphone%2520devices.%2520While%2520Linear%2520Color%2520Correction%2520Matrix%2520%2528CCM%2529%2520normalization%2520reduced%2520color%2520error%2520by%252067-77%2525%2520--%2520achieving%2520near-clinical%2520accuracy%2520%2528Delta%2520E%2520%253C%25202.3%2529%2520--%2520this%2520success%2520did%2520not%2520translate%2520to%2520biomarker%2520reliability.%250A%2520%2520We%2520identify%2520a%2520phenomenon%2520termed%2520%2522color-clinical%2520decoupling%2522%253A%2520despite%2520perceptual%2520accuracy%252C%2520the%2520Individual%2520Typology%2520Angle%2520%2528ITA%2529%2520showed%2520poor%2520inter-device%2520agreement%2520%2528ICC%2520%253D%25200.40%2529%252C%2520while%2520the%2520Melanin%2520Index%2520achieved%2520good%2520agreement%2520%2528ICC%2520%253D%25200.77%2529.%2520This%2520decoupling%2520is%2520driven%2520by%2520the%2520ITA%2520formula%2527s%2520sensitivity%2520to%2520b%252A%2520channel%2520noise%2520and%2520is%2520further%2520compounded%2520by%2520anatomical%2520variance.%2520Facial%2520region%2520accounts%2520for%252025.2%2525%2520of%2520color%2520variance%2520--%25203.6x%2520greater%2520than%2520device%2520effects%2520%25287.0%2525%2529%2520--%2520challenging%2520the%2520efficacy%2520of%2520single-patch%2520calibration.%2520Our%2520results%2520demonstrate%2520that%2520current%2520colorimetric%2520standards%2520are%2520insufficient%2520for%2520clinical-grade%2520biomarker%2520extraction%252C%2520necessitating%2520region-aware%2520protocols%2520for%2520mobile%2520dermatology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Color-Clinical%20Decoupling%3A%20Why%20Perceptual%20Calibration%20Fails%20Clinical%20Biomarkers%20in%20Smartphone%20Dermatology&entry.906535625=Sungwoo%20Kang&entry.1292438233=Smartphone-based%20tele-dermatology%20assumes%20that%20colorimetric%20calibration%20ensures%20clinical%20reliability%2C%20yet%20this%20remains%20untested%20for%20underrepresented%20skin%20phototypes.%20We%20investigated%20whether%20standard%20calibration%20translates%20to%20reliable%20clinical%20biomarkers%20using%2043%2C425%20images%20from%20965%20Korean%20subjects%20%28Fitzpatrick%20III-IV%29%20across%20DSLR%2C%20tablet%2C%20and%20smartphone%20devices.%20While%20Linear%20Color%20Correction%20Matrix%20%28CCM%29%20normalization%20reduced%20color%20error%20by%2067-77%25%20--%20achieving%20near-clinical%20accuracy%20%28Delta%20E%20%3C%202.3%29%20--%20this%20success%20did%20not%20translate%20to%20biomarker%20reliability.%0A%20%20We%20identify%20a%20phenomenon%20termed%20%22color-clinical%20decoupling%22%3A%20despite%20perceptual%20accuracy%2C%20the%20Individual%20Typology%20Angle%20%28ITA%29%20showed%20poor%20inter-device%20agreement%20%28ICC%20%3D%200.40%29%2C%20while%20the%20Melanin%20Index%20achieved%20good%20agreement%20%28ICC%20%3D%200.77%29.%20This%20decoupling%20is%20driven%20by%20the%20ITA%20formula%27s%20sensitivity%20to%20b%2A%20channel%20noise%20and%20is%20further%20compounded%20by%20anatomical%20variance.%20Facial%20region%20accounts%20for%2025.2%25%20of%20color%20variance%20--%203.6x%20greater%20than%20device%20effects%20%287.0%25%29%20--%20challenging%20the%20efficacy%20of%20single-patch%20calibration.%20Our%20results%20demonstrate%20that%20current%20colorimetric%20standards%20are%20insufficient%20for%20clinical-grade%20biomarker%20extraction%2C%20necessitating%20region-aware%20protocols%20for%20mobile%20dermatology.&entry.1838667208=http%3A//arxiv.org/abs/2512.21988v1&entry.124074799=Read"},
{"title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling", "author": "Hannah Atmer and Yuan Yao and Thiemo Voigt and Stefanos Kaxiras", "abstract": "Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.", "link": "http://arxiv.org/abs/2512.22066v1", "date": "2025-12-26", "relevancy": 1.252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4342}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prefill%20vs.%20Decode%20Bottlenecks%3A%20SRAM-Frequency%20Tradeoffs%20and%20the%20Memory-Bandwidth%20Ceiling&body=Title%3A%20Prefill%20vs.%20Decode%20Bottlenecks%3A%20SRAM-Frequency%20Tradeoffs%20and%20the%20Memory-Bandwidth%20Ceiling%0AAuthor%3A%20Hannah%20Atmer%20and%20Yuan%20Yao%20and%20Thiemo%20Voigt%20and%20Stefanos%20Kaxiras%0AAbstract%3A%20Energy%20consumption%20dictates%20the%20cost%20and%20environmental%20impact%20of%20deploying%20Large%20Language%20Models.%20This%20paper%20investigates%20the%20impact%20of%20on-chip%20SRAM%20size%20and%20operating%20frequency%20on%20the%20energy%20efficiency%20and%20performance%20of%20LLM%20inference%2C%20focusing%20on%20the%20distinct%20behaviors%20of%20the%20compute-bound%20prefill%20and%20memory-bound%20decode%20phases.%20Our%20simulation%20methodology%20combines%20OpenRAM%20for%20energy%20modeling%2C%20LLMCompass%20for%20latency%20simulation%2C%20and%20ScaleSIM%20for%20systolic%20array%20operational%20intensity.%20Our%20findings%20show%20that%20total%20energy%20use%20is%20predominantly%20determined%20by%20SRAM%20size%20in%20both%20phases%2C%20with%20larger%20buffers%20significantly%20increasing%20static%20energy%20due%20to%20leakage%2C%20which%20is%20not%20offset%20by%20corresponding%20latency%20benefits.%20We%20quantitatively%20explore%20the%20memory-bandwidth%20bottleneck%2C%20demonstrating%20that%20while%20high%20operating%20frequencies%20reduce%20prefill%20latency%2C%20their%20positive%20impact%20on%20memory-bound%20decode%20latency%20is%20capped%20by%20the%20external%20memory%20bandwidth.%20Counter-intuitively%2C%20high%20compute%20frequency%20can%20reduce%20total%20energy%20by%20reducing%20execution%20time%20and%20consequently%20decreasing%20static%20energy%20consumption%20more%20than%20the%20resulting%20dynamic%20power%20increase.%20We%20identify%20an%20optimal%20hardware%20configuration%20for%20the%20simulated%20workload%3A%20high%20operating%20frequencies%20%281200MHz-1400MHz%29%20and%20a%20small%20local%20buffer%20size%20of%2032KB%20to%2064KB.%20This%20combination%20achieves%20the%20best%20energy-delay%20product%2C%20balancing%20low%20latency%20with%20high%20energy%20efficiency.%20Furthermore%2C%20we%20demonstrate%20how%20memory%20bandwidth%20acts%20as%20a%20performance%20ceiling%2C%20and%20that%20increasing%20compute%20frequency%20only%20yields%20performance%20gains%20up%20to%20the%20point%20where%20the%20workload%20becomes%20memory-bound.%20This%20analysis%20provides%20concrete%20architectural%20insights%20for%20designing%20energy-efficient%20LLM%20accelerators%2C%20especially%20for%20datacenters%20aiming%20to%20minimize%20their%20energy%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefill%2520vs.%2520Decode%2520Bottlenecks%253A%2520SRAM-Frequency%2520Tradeoffs%2520and%2520the%2520Memory-Bandwidth%2520Ceiling%26entry.906535625%3DHannah%2520Atmer%2520and%2520Yuan%2520Yao%2520and%2520Thiemo%2520Voigt%2520and%2520Stefanos%2520Kaxiras%26entry.1292438233%3DEnergy%2520consumption%2520dictates%2520the%2520cost%2520and%2520environmental%2520impact%2520of%2520deploying%2520Large%2520Language%2520Models.%2520This%2520paper%2520investigates%2520the%2520impact%2520of%2520on-chip%2520SRAM%2520size%2520and%2520operating%2520frequency%2520on%2520the%2520energy%2520efficiency%2520and%2520performance%2520of%2520LLM%2520inference%252C%2520focusing%2520on%2520the%2520distinct%2520behaviors%2520of%2520the%2520compute-bound%2520prefill%2520and%2520memory-bound%2520decode%2520phases.%2520Our%2520simulation%2520methodology%2520combines%2520OpenRAM%2520for%2520energy%2520modeling%252C%2520LLMCompass%2520for%2520latency%2520simulation%252C%2520and%2520ScaleSIM%2520for%2520systolic%2520array%2520operational%2520intensity.%2520Our%2520findings%2520show%2520that%2520total%2520energy%2520use%2520is%2520predominantly%2520determined%2520by%2520SRAM%2520size%2520in%2520both%2520phases%252C%2520with%2520larger%2520buffers%2520significantly%2520increasing%2520static%2520energy%2520due%2520to%2520leakage%252C%2520which%2520is%2520not%2520offset%2520by%2520corresponding%2520latency%2520benefits.%2520We%2520quantitatively%2520explore%2520the%2520memory-bandwidth%2520bottleneck%252C%2520demonstrating%2520that%2520while%2520high%2520operating%2520frequencies%2520reduce%2520prefill%2520latency%252C%2520their%2520positive%2520impact%2520on%2520memory-bound%2520decode%2520latency%2520is%2520capped%2520by%2520the%2520external%2520memory%2520bandwidth.%2520Counter-intuitively%252C%2520high%2520compute%2520frequency%2520can%2520reduce%2520total%2520energy%2520by%2520reducing%2520execution%2520time%2520and%2520consequently%2520decreasing%2520static%2520energy%2520consumption%2520more%2520than%2520the%2520resulting%2520dynamic%2520power%2520increase.%2520We%2520identify%2520an%2520optimal%2520hardware%2520configuration%2520for%2520the%2520simulated%2520workload%253A%2520high%2520operating%2520frequencies%2520%25281200MHz-1400MHz%2529%2520and%2520a%2520small%2520local%2520buffer%2520size%2520of%252032KB%2520to%252064KB.%2520This%2520combination%2520achieves%2520the%2520best%2520energy-delay%2520product%252C%2520balancing%2520low%2520latency%2520with%2520high%2520energy%2520efficiency.%2520Furthermore%252C%2520we%2520demonstrate%2520how%2520memory%2520bandwidth%2520acts%2520as%2520a%2520performance%2520ceiling%252C%2520and%2520that%2520increasing%2520compute%2520frequency%2520only%2520yields%2520performance%2520gains%2520up%2520to%2520the%2520point%2520where%2520the%2520workload%2520becomes%2520memory-bound.%2520This%2520analysis%2520provides%2520concrete%2520architectural%2520insights%2520for%2520designing%2520energy-efficient%2520LLM%2520accelerators%252C%2520especially%2520for%2520datacenters%2520aiming%2520to%2520minimize%2520their%2520energy%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefill%20vs.%20Decode%20Bottlenecks%3A%20SRAM-Frequency%20Tradeoffs%20and%20the%20Memory-Bandwidth%20Ceiling&entry.906535625=Hannah%20Atmer%20and%20Yuan%20Yao%20and%20Thiemo%20Voigt%20and%20Stefanos%20Kaxiras&entry.1292438233=Energy%20consumption%20dictates%20the%20cost%20and%20environmental%20impact%20of%20deploying%20Large%20Language%20Models.%20This%20paper%20investigates%20the%20impact%20of%20on-chip%20SRAM%20size%20and%20operating%20frequency%20on%20the%20energy%20efficiency%20and%20performance%20of%20LLM%20inference%2C%20focusing%20on%20the%20distinct%20behaviors%20of%20the%20compute-bound%20prefill%20and%20memory-bound%20decode%20phases.%20Our%20simulation%20methodology%20combines%20OpenRAM%20for%20energy%20modeling%2C%20LLMCompass%20for%20latency%20simulation%2C%20and%20ScaleSIM%20for%20systolic%20array%20operational%20intensity.%20Our%20findings%20show%20that%20total%20energy%20use%20is%20predominantly%20determined%20by%20SRAM%20size%20in%20both%20phases%2C%20with%20larger%20buffers%20significantly%20increasing%20static%20energy%20due%20to%20leakage%2C%20which%20is%20not%20offset%20by%20corresponding%20latency%20benefits.%20We%20quantitatively%20explore%20the%20memory-bandwidth%20bottleneck%2C%20demonstrating%20that%20while%20high%20operating%20frequencies%20reduce%20prefill%20latency%2C%20their%20positive%20impact%20on%20memory-bound%20decode%20latency%20is%20capped%20by%20the%20external%20memory%20bandwidth.%20Counter-intuitively%2C%20high%20compute%20frequency%20can%20reduce%20total%20energy%20by%20reducing%20execution%20time%20and%20consequently%20decreasing%20static%20energy%20consumption%20more%20than%20the%20resulting%20dynamic%20power%20increase.%20We%20identify%20an%20optimal%20hardware%20configuration%20for%20the%20simulated%20workload%3A%20high%20operating%20frequencies%20%281200MHz-1400MHz%29%20and%20a%20small%20local%20buffer%20size%20of%2032KB%20to%2064KB.%20This%20combination%20achieves%20the%20best%20energy-delay%20product%2C%20balancing%20low%20latency%20with%20high%20energy%20efficiency.%20Furthermore%2C%20we%20demonstrate%20how%20memory%20bandwidth%20acts%20as%20a%20performance%20ceiling%2C%20and%20that%20increasing%20compute%20frequency%20only%20yields%20performance%20gains%20up%20to%20the%20point%20where%20the%20workload%20becomes%20memory-bound.%20This%20analysis%20provides%20concrete%20architectural%20insights%20for%20designing%20energy-efficient%20LLM%20accelerators%2C%20especially%20for%20datacenters%20aiming%20to%20minimize%20their%20energy%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2512.22066v1&entry.124074799=Read"},
{"title": "Bias-variance decompositions: the exclusive privilege of Bregman divergences", "author": "Tom Heskes", "abstract": "Bias-variance decompositions are widely used to understand the generalization performance of machine learning models. While the squared error loss permits a straightforward decomposition, other loss functions - such as zero-one loss or $L_1$ loss - either fail to sum bias and variance to the expected loss or rely on definitions that lack the essential properties of meaningful bias and variance. Recent research has shown that clean decompositions can be achieved for the broader class of Bregman divergences, with the cross-entropy loss as a special case. However, the necessary and sufficient conditions for these decompositions remain an open question.\n  In this paper, we address this question by studying continuous, nonnegative loss functions that satisfy the identity of indiscernibles (zero loss if and only if the two arguments are identical), under mild regularity conditions. We prove that so-called $g$-Bregman or rho-tau divergences are the only such loss functions that have a clean bias-variance decomposition. A $g$-Bregman divergence can be transformed into a standard Bregman divergence through an invertible change of variables. This makes the squared Mahalanobis distance, up to such a variable transformation, the only symmetric loss function with a clean bias-variance decomposition. Consequently, common metrics such as $0$-$1$ and $L_1$ losses cannot admit a clean bias-variance decomposition, explaining why previous attempts have failed. We also examine the impact of relaxing the restrictions on the loss functions and how this affects our results.", "link": "http://arxiv.org/abs/2501.18581v3", "date": "2025-12-26", "relevancy": 1.2333, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4102}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias-variance%20decompositions%3A%20the%20exclusive%20privilege%20of%20Bregman%20divergences&body=Title%3A%20Bias-variance%20decompositions%3A%20the%20exclusive%20privilege%20of%20Bregman%20divergences%0AAuthor%3A%20Tom%20Heskes%0AAbstract%3A%20Bias-variance%20decompositions%20are%20widely%20used%20to%20understand%20the%20generalization%20performance%20of%20machine%20learning%20models.%20While%20the%20squared%20error%20loss%20permits%20a%20straightforward%20decomposition%2C%20other%20loss%20functions%20-%20such%20as%20zero-one%20loss%20or%20%24L_1%24%20loss%20-%20either%20fail%20to%20sum%20bias%20and%20variance%20to%20the%20expected%20loss%20or%20rely%20on%20definitions%20that%20lack%20the%20essential%20properties%20of%20meaningful%20bias%20and%20variance.%20Recent%20research%20has%20shown%20that%20clean%20decompositions%20can%20be%20achieved%20for%20the%20broader%20class%20of%20Bregman%20divergences%2C%20with%20the%20cross-entropy%20loss%20as%20a%20special%20case.%20However%2C%20the%20necessary%20and%20sufficient%20conditions%20for%20these%20decompositions%20remain%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20studying%20continuous%2C%20nonnegative%20loss%20functions%20that%20satisfy%20the%20identity%20of%20indiscernibles%20%28zero%20loss%20if%20and%20only%20if%20the%20two%20arguments%20are%20identical%29%2C%20under%20mild%20regularity%20conditions.%20We%20prove%20that%20so-called%20%24g%24-Bregman%20or%20rho-tau%20divergences%20are%20the%20only%20such%20loss%20functions%20that%20have%20a%20clean%20bias-variance%20decomposition.%20A%20%24g%24-Bregman%20divergence%20can%20be%20transformed%20into%20a%20standard%20Bregman%20divergence%20through%20an%20invertible%20change%20of%20variables.%20This%20makes%20the%20squared%20Mahalanobis%20distance%2C%20up%20to%20such%20a%20variable%20transformation%2C%20the%20only%20symmetric%20loss%20function%20with%20a%20clean%20bias-variance%20decomposition.%20Consequently%2C%20common%20metrics%20such%20as%20%240%24-%241%24%20and%20%24L_1%24%20losses%20cannot%20admit%20a%20clean%20bias-variance%20decomposition%2C%20explaining%20why%20previous%20attempts%20have%20failed.%20We%20also%20examine%20the%20impact%20of%20relaxing%20the%20restrictions%20on%20the%20loss%20functions%20and%20how%20this%20affects%20our%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2501.18581v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias-variance%2520decompositions%253A%2520the%2520exclusive%2520privilege%2520of%2520Bregman%2520divergences%26entry.906535625%3DTom%2520Heskes%26entry.1292438233%3DBias-variance%2520decompositions%2520are%2520widely%2520used%2520to%2520understand%2520the%2520generalization%2520performance%2520of%2520machine%2520learning%2520models.%2520While%2520the%2520squared%2520error%2520loss%2520permits%2520a%2520straightforward%2520decomposition%252C%2520other%2520loss%2520functions%2520-%2520such%2520as%2520zero-one%2520loss%2520or%2520%2524L_1%2524%2520loss%2520-%2520either%2520fail%2520to%2520sum%2520bias%2520and%2520variance%2520to%2520the%2520expected%2520loss%2520or%2520rely%2520on%2520definitions%2520that%2520lack%2520the%2520essential%2520properties%2520of%2520meaningful%2520bias%2520and%2520variance.%2520Recent%2520research%2520has%2520shown%2520that%2520clean%2520decompositions%2520can%2520be%2520achieved%2520for%2520the%2520broader%2520class%2520of%2520Bregman%2520divergences%252C%2520with%2520the%2520cross-entropy%2520loss%2520as%2520a%2520special%2520case.%2520However%252C%2520the%2520necessary%2520and%2520sufficient%2520conditions%2520for%2520these%2520decompositions%2520remain%2520an%2520open%2520question.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520question%2520by%2520studying%2520continuous%252C%2520nonnegative%2520loss%2520functions%2520that%2520satisfy%2520the%2520identity%2520of%2520indiscernibles%2520%2528zero%2520loss%2520if%2520and%2520only%2520if%2520the%2520two%2520arguments%2520are%2520identical%2529%252C%2520under%2520mild%2520regularity%2520conditions.%2520We%2520prove%2520that%2520so-called%2520%2524g%2524-Bregman%2520or%2520rho-tau%2520divergences%2520are%2520the%2520only%2520such%2520loss%2520functions%2520that%2520have%2520a%2520clean%2520bias-variance%2520decomposition.%2520A%2520%2524g%2524-Bregman%2520divergence%2520can%2520be%2520transformed%2520into%2520a%2520standard%2520Bregman%2520divergence%2520through%2520an%2520invertible%2520change%2520of%2520variables.%2520This%2520makes%2520the%2520squared%2520Mahalanobis%2520distance%252C%2520up%2520to%2520such%2520a%2520variable%2520transformation%252C%2520the%2520only%2520symmetric%2520loss%2520function%2520with%2520a%2520clean%2520bias-variance%2520decomposition.%2520Consequently%252C%2520common%2520metrics%2520such%2520as%2520%25240%2524-%25241%2524%2520and%2520%2524L_1%2524%2520losses%2520cannot%2520admit%2520a%2520clean%2520bias-variance%2520decomposition%252C%2520explaining%2520why%2520previous%2520attempts%2520have%2520failed.%2520We%2520also%2520examine%2520the%2520impact%2520of%2520relaxing%2520the%2520restrictions%2520on%2520the%2520loss%2520functions%2520and%2520how%2520this%2520affects%2520our%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18581v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias-variance%20decompositions%3A%20the%20exclusive%20privilege%20of%20Bregman%20divergences&entry.906535625=Tom%20Heskes&entry.1292438233=Bias-variance%20decompositions%20are%20widely%20used%20to%20understand%20the%20generalization%20performance%20of%20machine%20learning%20models.%20While%20the%20squared%20error%20loss%20permits%20a%20straightforward%20decomposition%2C%20other%20loss%20functions%20-%20such%20as%20zero-one%20loss%20or%20%24L_1%24%20loss%20-%20either%20fail%20to%20sum%20bias%20and%20variance%20to%20the%20expected%20loss%20or%20rely%20on%20definitions%20that%20lack%20the%20essential%20properties%20of%20meaningful%20bias%20and%20variance.%20Recent%20research%20has%20shown%20that%20clean%20decompositions%20can%20be%20achieved%20for%20the%20broader%20class%20of%20Bregman%20divergences%2C%20with%20the%20cross-entropy%20loss%20as%20a%20special%20case.%20However%2C%20the%20necessary%20and%20sufficient%20conditions%20for%20these%20decompositions%20remain%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20studying%20continuous%2C%20nonnegative%20loss%20functions%20that%20satisfy%20the%20identity%20of%20indiscernibles%20%28zero%20loss%20if%20and%20only%20if%20the%20two%20arguments%20are%20identical%29%2C%20under%20mild%20regularity%20conditions.%20We%20prove%20that%20so-called%20%24g%24-Bregman%20or%20rho-tau%20divergences%20are%20the%20only%20such%20loss%20functions%20that%20have%20a%20clean%20bias-variance%20decomposition.%20A%20%24g%24-Bregman%20divergence%20can%20be%20transformed%20into%20a%20standard%20Bregman%20divergence%20through%20an%20invertible%20change%20of%20variables.%20This%20makes%20the%20squared%20Mahalanobis%20distance%2C%20up%20to%20such%20a%20variable%20transformation%2C%20the%20only%20symmetric%20loss%20function%20with%20a%20clean%20bias-variance%20decomposition.%20Consequently%2C%20common%20metrics%20such%20as%20%240%24-%241%24%20and%20%24L_1%24%20losses%20cannot%20admit%20a%20clean%20bias-variance%20decomposition%2C%20explaining%20why%20previous%20attempts%20have%20failed.%20We%20also%20examine%20the%20impact%20of%20relaxing%20the%20restrictions%20on%20the%20loss%20functions%20and%20how%20this%20affects%20our%20results.&entry.1838667208=http%3A//arxiv.org/abs/2501.18581v3&entry.124074799=Read"},
{"title": "ProEdit: Inversion-based Editing From Prompts Done Right", "author": "Zhi Ouyang and Dian Zheng and Xiao-Ming Wu and Jian-Jian Jiang and Kun-Yu Lin and Jingke Meng and Wei-Shi Zheng", "abstract": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.", "link": "http://arxiv.org/abs/2512.22118v1", "date": "2025-12-26", "relevancy": 1.1086, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5562}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5554}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProEdit%3A%20Inversion-based%20Editing%20From%20Prompts%20Done%20Right&body=Title%3A%20ProEdit%3A%20Inversion-based%20Editing%20From%20Prompts%20Done%20Right%0AAuthor%3A%20Zhi%20Ouyang%20and%20Dian%20Zheng%20and%20Xiao-Ming%20Wu%20and%20Jian-Jian%20Jiang%20and%20Kun-Yu%20Lin%20and%20Jingke%20Meng%20and%20Wei-Shi%20Zheng%0AAbstract%3A%20Inversion-based%20visual%20editing%20provides%20an%20effective%20and%20training-free%20way%20to%20edit%20an%20image%20or%20a%20video%20based%20on%20user%20instructions.%20Existing%20methods%20typically%20inject%20source%20image%20information%20during%20the%20sampling%20process%20to%20maintain%20editing%20consistency.%20However%2C%20this%20sampling%20strategy%20overly%20relies%20on%20source%20information%2C%20which%20negatively%20affects%20the%20edits%20in%20the%20target%20image%20%28e.g.%2C%20failing%20to%20change%20the%20subject%27s%20atributes%20like%20pose%2C%20number%2C%20or%20color%20as%20instructed%29.%20In%20this%20work%2C%20we%20propose%20ProEdit%20to%20address%20this%20issue%20both%20in%20the%20attention%20and%20the%20latent%20aspects.%20In%20the%20attention%20aspect%2C%20we%20introduce%20KV-mix%2C%20which%20mixes%20KV%20features%20of%20the%20source%20and%20the%20target%20in%20the%20edited%20region%2C%20mitigating%20the%20influence%20of%20the%20source%20image%20on%20the%20editing%20region%20while%20maintaining%20background%20consistency.%20In%20the%20latent%20aspect%2C%20we%20propose%20Latents-Shift%2C%20which%20perturbs%20the%20edited%20region%20of%20the%20source%20latent%2C%20eliminating%20the%20influence%20of%20the%20inverted%20latent%20on%20the%20sampling.%20Extensive%20experiments%20on%20several%20image%20and%20video%20editing%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20SOTA%20performance.%20In%20addition%2C%20our%20design%20is%20plug-and-play%2C%20which%20can%20be%20seamlessly%20integrated%20into%20existing%20inversion%20and%20editing%20methods%2C%20such%20as%20RF-Solver%2C%20FireFlow%20and%20UniEdit.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProEdit%253A%2520Inversion-based%2520Editing%2520From%2520Prompts%2520Done%2520Right%26entry.906535625%3DZhi%2520Ouyang%2520and%2520Dian%2520Zheng%2520and%2520Xiao-Ming%2520Wu%2520and%2520Jian-Jian%2520Jiang%2520and%2520Kun-Yu%2520Lin%2520and%2520Jingke%2520Meng%2520and%2520Wei-Shi%2520Zheng%26entry.1292438233%3DInversion-based%2520visual%2520editing%2520provides%2520an%2520effective%2520and%2520training-free%2520way%2520to%2520edit%2520an%2520image%2520or%2520a%2520video%2520based%2520on%2520user%2520instructions.%2520Existing%2520methods%2520typically%2520inject%2520source%2520image%2520information%2520during%2520the%2520sampling%2520process%2520to%2520maintain%2520editing%2520consistency.%2520However%252C%2520this%2520sampling%2520strategy%2520overly%2520relies%2520on%2520source%2520information%252C%2520which%2520negatively%2520affects%2520the%2520edits%2520in%2520the%2520target%2520image%2520%2528e.g.%252C%2520failing%2520to%2520change%2520the%2520subject%2527s%2520atributes%2520like%2520pose%252C%2520number%252C%2520or%2520color%2520as%2520instructed%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520ProEdit%2520to%2520address%2520this%2520issue%2520both%2520in%2520the%2520attention%2520and%2520the%2520latent%2520aspects.%2520In%2520the%2520attention%2520aspect%252C%2520we%2520introduce%2520KV-mix%252C%2520which%2520mixes%2520KV%2520features%2520of%2520the%2520source%2520and%2520the%2520target%2520in%2520the%2520edited%2520region%252C%2520mitigating%2520the%2520influence%2520of%2520the%2520source%2520image%2520on%2520the%2520editing%2520region%2520while%2520maintaining%2520background%2520consistency.%2520In%2520the%2520latent%2520aspect%252C%2520we%2520propose%2520Latents-Shift%252C%2520which%2520perturbs%2520the%2520edited%2520region%2520of%2520the%2520source%2520latent%252C%2520eliminating%2520the%2520influence%2520of%2520the%2520inverted%2520latent%2520on%2520the%2520sampling.%2520Extensive%2520experiments%2520on%2520several%2520image%2520and%2520video%2520editing%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520SOTA%2520performance.%2520In%2520addition%252C%2520our%2520design%2520is%2520plug-and-play%252C%2520which%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520inversion%2520and%2520editing%2520methods%252C%2520such%2520as%2520RF-Solver%252C%2520FireFlow%2520and%2520UniEdit.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProEdit%3A%20Inversion-based%20Editing%20From%20Prompts%20Done%20Right&entry.906535625=Zhi%20Ouyang%20and%20Dian%20Zheng%20and%20Xiao-Ming%20Wu%20and%20Jian-Jian%20Jiang%20and%20Kun-Yu%20Lin%20and%20Jingke%20Meng%20and%20Wei-Shi%20Zheng&entry.1292438233=Inversion-based%20visual%20editing%20provides%20an%20effective%20and%20training-free%20way%20to%20edit%20an%20image%20or%20a%20video%20based%20on%20user%20instructions.%20Existing%20methods%20typically%20inject%20source%20image%20information%20during%20the%20sampling%20process%20to%20maintain%20editing%20consistency.%20However%2C%20this%20sampling%20strategy%20overly%20relies%20on%20source%20information%2C%20which%20negatively%20affects%20the%20edits%20in%20the%20target%20image%20%28e.g.%2C%20failing%20to%20change%20the%20subject%27s%20atributes%20like%20pose%2C%20number%2C%20or%20color%20as%20instructed%29.%20In%20this%20work%2C%20we%20propose%20ProEdit%20to%20address%20this%20issue%20both%20in%20the%20attention%20and%20the%20latent%20aspects.%20In%20the%20attention%20aspect%2C%20we%20introduce%20KV-mix%2C%20which%20mixes%20KV%20features%20of%20the%20source%20and%20the%20target%20in%20the%20edited%20region%2C%20mitigating%20the%20influence%20of%20the%20source%20image%20on%20the%20editing%20region%20while%20maintaining%20background%20consistency.%20In%20the%20latent%20aspect%2C%20we%20propose%20Latents-Shift%2C%20which%20perturbs%20the%20edited%20region%20of%20the%20source%20latent%2C%20eliminating%20the%20influence%20of%20the%20inverted%20latent%20on%20the%20sampling.%20Extensive%20experiments%20on%20several%20image%20and%20video%20editing%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20SOTA%20performance.%20In%20addition%2C%20our%20design%20is%20plug-and-play%2C%20which%20can%20be%20seamlessly%20integrated%20into%20existing%20inversion%20and%20editing%20methods%2C%20such%20as%20RF-Solver%2C%20FireFlow%20and%20UniEdit.&entry.1838667208=http%3A//arxiv.org/abs/2512.22118v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


