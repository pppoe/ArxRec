<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240609.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\n  of Photo-Realistic Free-Viewpoint Videos", "author": "Jiakai Sun and Han Jiao and Guangyuan Li and Zhanjie Zhang and Lei Zhao and Wei Xing", "abstract": "  Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\nfrom multi-view videos remains a challenging endeavor. Despite the remarkable\nadvancements achieved by current neural rendering techniques, these methods\ngenerally require complete video sequences for offline training and are not\ncapable of real-time rendering. To address these constraints, we introduce\n3DGStream, a method designed for efficient FVV streaming of real-world dynamic\nscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\nseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\nGaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of\ndirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\nCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame. Furthermore, we\npropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\nscenes. Experiments demonstrate that 3DGStream achieves competitive performance\nin terms of rendering speed, image quality, training time, and model storage\nwhen compared with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.01444v3", "date": "2024-06-07", "relevancy": 3.2383, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.677}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.653}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos&body=Title%3A%203DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos%0AAuthor%3A%20Jiakai%20Sun%20and%20Han%20Jiao%20and%20Guangyuan%20Li%20and%20Zhanjie%20Zhang%20and%20Lei%20Zhao%20and%20Wei%20Xing%0AAbstract%3A%20%20%20Constructing%20photo-realistic%20Free-Viewpoint%20Videos%20%28FVVs%29%20of%20dynamic%20scenes%0Afrom%20multi-view%20videos%20remains%20a%20challenging%20endeavor.%20Despite%20the%20remarkable%0Aadvancements%20achieved%20by%20current%20neural%20rendering%20techniques%2C%20these%20methods%0Agenerally%20require%20complete%20video%20sequences%20for%20offline%20training%20and%20are%20not%0Acapable%20of%20real-time%20rendering.%20To%20address%20these%20constraints%2C%20we%20introduce%0A3DGStream%2C%20a%20method%20designed%20for%20efficient%20FVV%20streaming%20of%20real-world%20dynamic%0Ascenes.%20Our%20method%20achieves%20fast%20on-the-fly%20per-frame%20reconstruction%20within%2012%0Aseconds%20and%20real-time%20rendering%20at%20200%20FPS.%20Specifically%2C%20we%20utilize%203D%0AGaussians%20%283DGs%29%20to%20represent%20the%20scene.%20Instead%20of%20the%20na%5C%22ive%20approach%20of%0Adirectly%20optimizing%203DGs%20per-frame%2C%20we%20employ%20a%20compact%20Neural%20Transformation%0ACache%20%28NTC%29%20to%20model%20the%20translations%20and%20rotations%20of%203DGs%2C%20markedly%20reducing%0Athe%20training%20time%20and%20storage%20required%20for%20each%20FVV%20frame.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%203DG%20addition%20strategy%20to%20handle%20emerging%20objects%20in%20dynamic%0Ascenes.%20Experiments%20demonstrate%20that%203DGStream%20achieves%20competitive%20performance%0Ain%20terms%20of%20rendering%20speed%2C%20image%20quality%2C%20training%20time%2C%20and%20model%20storage%0Awhen%20compared%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01444v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGStream%253A%2520On-the-Fly%2520Training%2520of%25203D%2520Gaussians%2520for%2520Efficient%2520Streaming%250A%2520%2520of%2520Photo-Realistic%2520Free-Viewpoint%2520Videos%26entry.906535625%3DJiakai%2520Sun%2520and%2520Han%2520Jiao%2520and%2520Guangyuan%2520Li%2520and%2520Zhanjie%2520Zhang%2520and%2520Lei%2520Zhao%2520and%2520Wei%2520Xing%26entry.1292438233%3D%2520%2520Constructing%2520photo-realistic%2520Free-Viewpoint%2520Videos%2520%2528FVVs%2529%2520of%2520dynamic%2520scenes%250Afrom%2520multi-view%2520videos%2520remains%2520a%2520challenging%2520endeavor.%2520Despite%2520the%2520remarkable%250Aadvancements%2520achieved%2520by%2520current%2520neural%2520rendering%2520techniques%252C%2520these%2520methods%250Agenerally%2520require%2520complete%2520video%2520sequences%2520for%2520offline%2520training%2520and%2520are%2520not%250Acapable%2520of%2520real-time%2520rendering.%2520To%2520address%2520these%2520constraints%252C%2520we%2520introduce%250A3DGStream%252C%2520a%2520method%2520designed%2520for%2520efficient%2520FVV%2520streaming%2520of%2520real-world%2520dynamic%250Ascenes.%2520Our%2520method%2520achieves%2520fast%2520on-the-fly%2520per-frame%2520reconstruction%2520within%252012%250Aseconds%2520and%2520real-time%2520rendering%2520at%2520200%2520FPS.%2520Specifically%252C%2520we%2520utilize%25203D%250AGaussians%2520%25283DGs%2529%2520to%2520represent%2520the%2520scene.%2520Instead%2520of%2520the%2520na%255C%2522ive%2520approach%2520of%250Adirectly%2520optimizing%25203DGs%2520per-frame%252C%2520we%2520employ%2520a%2520compact%2520Neural%2520Transformation%250ACache%2520%2528NTC%2529%2520to%2520model%2520the%2520translations%2520and%2520rotations%2520of%25203DGs%252C%2520markedly%2520reducing%250Athe%2520training%2520time%2520and%2520storage%2520required%2520for%2520each%2520FVV%2520frame.%2520Furthermore%252C%2520we%250Apropose%2520an%2520adaptive%25203DG%2520addition%2520strategy%2520to%2520handle%2520emerging%2520objects%2520in%2520dynamic%250Ascenes.%2520Experiments%2520demonstrate%2520that%25203DGStream%2520achieves%2520competitive%2520performance%250Ain%2520terms%2520of%2520rendering%2520speed%252C%2520image%2520quality%252C%2520training%2520time%252C%2520and%2520model%2520storage%250Awhen%2520compared%2520with%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01444v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos&entry.906535625=Jiakai%20Sun%20and%20Han%20Jiao%20and%20Guangyuan%20Li%20and%20Zhanjie%20Zhang%20and%20Lei%20Zhao%20and%20Wei%20Xing&entry.1292438233=%20%20Constructing%20photo-realistic%20Free-Viewpoint%20Videos%20%28FVVs%29%20of%20dynamic%20scenes%0Afrom%20multi-view%20videos%20remains%20a%20challenging%20endeavor.%20Despite%20the%20remarkable%0Aadvancements%20achieved%20by%20current%20neural%20rendering%20techniques%2C%20these%20methods%0Agenerally%20require%20complete%20video%20sequences%20for%20offline%20training%20and%20are%20not%0Acapable%20of%20real-time%20rendering.%20To%20address%20these%20constraints%2C%20we%20introduce%0A3DGStream%2C%20a%20method%20designed%20for%20efficient%20FVV%20streaming%20of%20real-world%20dynamic%0Ascenes.%20Our%20method%20achieves%20fast%20on-the-fly%20per-frame%20reconstruction%20within%2012%0Aseconds%20and%20real-time%20rendering%20at%20200%20FPS.%20Specifically%2C%20we%20utilize%203D%0AGaussians%20%283DGs%29%20to%20represent%20the%20scene.%20Instead%20of%20the%20na%5C%22ive%20approach%20of%0Adirectly%20optimizing%203DGs%20per-frame%2C%20we%20employ%20a%20compact%20Neural%20Transformation%0ACache%20%28NTC%29%20to%20model%20the%20translations%20and%20rotations%20of%203DGs%2C%20markedly%20reducing%0Athe%20training%20time%20and%20storage%20required%20for%20each%20FVV%20frame.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%203DG%20addition%20strategy%20to%20handle%20emerging%20objects%20in%20dynamic%0Ascenes.%20Experiments%20demonstrate%20that%203DGStream%20achieves%20competitive%20performance%0Ain%20terms%20of%20rendering%20speed%2C%20image%20quality%2C%20training%20time%2C%20and%20model%20storage%0Awhen%20compared%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01444v3&entry.124074799=Read"},
{"title": "Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes", "author": "Hmrishav Bandyopadhyay and Subhadeep Koley and Ayan Das and Ayan Kumar Bhunia and Aneeshan Sain and Pinaki Nath Chowdhury and Tao Xiang and Yi-Zhe Song", "abstract": "  In this paper, we democratise 3D content creation, enabling precise\ngeneration of 3D shapes from abstract sketches while overcoming limitations\ntied to drawing skills. We introduce a novel part-level modelling and alignment\nframework that facilitates abstraction modelling and cross-modal\ncorrespondence. Leveraging the same part-level decoder, our approach seamlessly\nextends to sketch modelling by establishing correspondence between CLIPasso\nedgemaps and projected 3D part regions, eliminating the need for a dataset\npairing human sketches and 3D shapes. Additionally, our method introduces a\nseamless in-position editing process as a byproduct of cross-modal part-aligned\nmodelling. Operating in a low-dimensional implicit space, our approach\nsignificantly reduces computational demands and processing time.\n", "link": "http://arxiv.org/abs/2312.04043v2", "date": "2024-06-07", "relevancy": 3.0693, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6182}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doodle%20Your%203D%3A%20From%20Abstract%20Freehand%20Sketches%20to%20Precise%203D%20Shapes&body=Title%3A%20Doodle%20Your%203D%3A%20From%20Abstract%20Freehand%20Sketches%20to%20Precise%203D%20Shapes%0AAuthor%3A%20Hmrishav%20Bandyopadhyay%20and%20Subhadeep%20Koley%20and%20Ayan%20Das%20and%20Ayan%20Kumar%20Bhunia%20and%20Aneeshan%20Sain%20and%20Pinaki%20Nath%20Chowdhury%20and%20Tao%20Xiang%20and%20Yi-Zhe%20Song%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20democratise%203D%20content%20creation%2C%20enabling%20precise%0Ageneration%20of%203D%20shapes%20from%20abstract%20sketches%20while%20overcoming%20limitations%0Atied%20to%20drawing%20skills.%20We%20introduce%20a%20novel%20part-level%20modelling%20and%20alignment%0Aframework%20that%20facilitates%20abstraction%20modelling%20and%20cross-modal%0Acorrespondence.%20Leveraging%20the%20same%20part-level%20decoder%2C%20our%20approach%20seamlessly%0Aextends%20to%20sketch%20modelling%20by%20establishing%20correspondence%20between%20CLIPasso%0Aedgemaps%20and%20projected%203D%20part%20regions%2C%20eliminating%20the%20need%20for%20a%20dataset%0Apairing%20human%20sketches%20and%203D%20shapes.%20Additionally%2C%20our%20method%20introduces%20a%0Aseamless%20in-position%20editing%20process%20as%20a%20byproduct%20of%20cross-modal%20part-aligned%0Amodelling.%20Operating%20in%20a%20low-dimensional%20implicit%20space%2C%20our%20approach%0Asignificantly%20reduces%20computational%20demands%20and%20processing%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoodle%2520Your%25203D%253A%2520From%2520Abstract%2520Freehand%2520Sketches%2520to%2520Precise%25203D%2520Shapes%26entry.906535625%3DHmrishav%2520Bandyopadhyay%2520and%2520Subhadeep%2520Koley%2520and%2520Ayan%2520Das%2520and%2520Ayan%2520Kumar%2520Bhunia%2520and%2520Aneeshan%2520Sain%2520and%2520Pinaki%2520Nath%2520Chowdhury%2520and%2520Tao%2520Xiang%2520and%2520Yi-Zhe%2520Song%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520democratise%25203D%2520content%2520creation%252C%2520enabling%2520precise%250Ageneration%2520of%25203D%2520shapes%2520from%2520abstract%2520sketches%2520while%2520overcoming%2520limitations%250Atied%2520to%2520drawing%2520skills.%2520We%2520introduce%2520a%2520novel%2520part-level%2520modelling%2520and%2520alignment%250Aframework%2520that%2520facilitates%2520abstraction%2520modelling%2520and%2520cross-modal%250Acorrespondence.%2520Leveraging%2520the%2520same%2520part-level%2520decoder%252C%2520our%2520approach%2520seamlessly%250Aextends%2520to%2520sketch%2520modelling%2520by%2520establishing%2520correspondence%2520between%2520CLIPasso%250Aedgemaps%2520and%2520projected%25203D%2520part%2520regions%252C%2520eliminating%2520the%2520need%2520for%2520a%2520dataset%250Apairing%2520human%2520sketches%2520and%25203D%2520shapes.%2520Additionally%252C%2520our%2520method%2520introduces%2520a%250Aseamless%2520in-position%2520editing%2520process%2520as%2520a%2520byproduct%2520of%2520cross-modal%2520part-aligned%250Amodelling.%2520Operating%2520in%2520a%2520low-dimensional%2520implicit%2520space%252C%2520our%2520approach%250Asignificantly%2520reduces%2520computational%2520demands%2520and%2520processing%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doodle%20Your%203D%3A%20From%20Abstract%20Freehand%20Sketches%20to%20Precise%203D%20Shapes&entry.906535625=Hmrishav%20Bandyopadhyay%20and%20Subhadeep%20Koley%20and%20Ayan%20Das%20and%20Ayan%20Kumar%20Bhunia%20and%20Aneeshan%20Sain%20and%20Pinaki%20Nath%20Chowdhury%20and%20Tao%20Xiang%20and%20Yi-Zhe%20Song&entry.1292438233=%20%20In%20this%20paper%2C%20we%20democratise%203D%20content%20creation%2C%20enabling%20precise%0Ageneration%20of%203D%20shapes%20from%20abstract%20sketches%20while%20overcoming%20limitations%0Atied%20to%20drawing%20skills.%20We%20introduce%20a%20novel%20part-level%20modelling%20and%20alignment%0Aframework%20that%20facilitates%20abstraction%20modelling%20and%20cross-modal%0Acorrespondence.%20Leveraging%20the%20same%20part-level%20decoder%2C%20our%20approach%20seamlessly%0Aextends%20to%20sketch%20modelling%20by%20establishing%20correspondence%20between%20CLIPasso%0Aedgemaps%20and%20projected%203D%20part%20regions%2C%20eliminating%20the%20need%20for%20a%20dataset%0Apairing%20human%20sketches%20and%203D%20shapes.%20Additionally%2C%20our%20method%20introduces%20a%0Aseamless%20in-position%20editing%20process%20as%20a%20byproduct%20of%20cross-modal%20part-aligned%0Amodelling.%20Operating%20in%20a%20low-dimensional%20implicit%20space%2C%20our%20approach%0Asignificantly%20reduces%20computational%20demands%20and%20processing%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04043v2&entry.124074799=Read"},
{"title": "3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views", "author": "Xiaobiao Du and Haiyang Sun and Shuyun Wang and Zhuojie Wu and Hongwei Sheng and Jiaying Ying and Ming Lu and Tianqing Zhu and Kun Zhan and Xin Yu", "abstract": "  3D cars are commonly used in self-driving systems, virtual/augmented reality,\nand games. However, existing 3D car datasets are either synthetic or\nlow-quality, presenting a significant gap toward the high-quality real-world 3D\ncar datasets and limiting their applications in practical scenarios. In this\npaper, we propose the first large-scale 3D real car dataset, termed 3DRealCar,\noffering three distinctive features. (1) \\textbf{High-Volume}: 2,500 cars are\nmeticulously scanned by 3D scanners, obtaining car images and point clouds with\nreal-world dimensions; (2) \\textbf{High-Quality}: Each car is captured in an\naverage of 200 dense, high-resolution 360-degree RGB-D views, enabling\nhigh-fidelity 3D reconstruction; (3) \\textbf{High-Diversity}: The dataset\ncontains various cars from over 100 brands, collected under three distinct\nlighting conditions, including reflective, standard, and dark. Additionally, we\noffer detailed car parsing maps for each instance to promote research in car\nparsing tasks. Moreover, we remove background point clouds and standardize the\ncar orientation to a unified axis for the reconstruction only on cars without\nbackground and controllable rendering. We benchmark 3D reconstruction results\nwith state-of-the-art methods across each lighting condition in 3DRealCar.\nExtensive experiments demonstrate that the standard lighting condition part of\n3DRealCar can be used to produce a large number of high-quality 3D cars,\nimproving various 2D and 3D tasks related to cars. Notably, our dataset brings\ninsight into the fact that recent 3D reconstruction methods face challenges in\nreconstructing high-quality 3D cars under reflective and dark lighting\nconditions. \\textcolor{red}{\\href{https://xiaobiaodu.github.io/3drealcar/}{Our\ndataset is available here.}}\n", "link": "http://arxiv.org/abs/2406.04875v1", "date": "2024-06-07", "relevancy": 3.0489, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6191}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6191}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DRealCar%3A%20An%20In-the-wild%20RGB-D%20Car%20Dataset%20with%20360-degree%20Views&body=Title%3A%203DRealCar%3A%20An%20In-the-wild%20RGB-D%20Car%20Dataset%20with%20360-degree%20Views%0AAuthor%3A%20Xiaobiao%20Du%20and%20Haiyang%20Sun%20and%20Shuyun%20Wang%20and%20Zhuojie%20Wu%20and%20Hongwei%20Sheng%20and%20Jiaying%20Ying%20and%20Ming%20Lu%20and%20Tianqing%20Zhu%20and%20Kun%20Zhan%20and%20Xin%20Yu%0AAbstract%3A%20%20%203D%20cars%20are%20commonly%20used%20in%20self-driving%20systems%2C%20virtual/augmented%20reality%2C%0Aand%20games.%20However%2C%20existing%203D%20car%20datasets%20are%20either%20synthetic%20or%0Alow-quality%2C%20presenting%20a%20significant%20gap%20toward%20the%20high-quality%20real-world%203D%0Acar%20datasets%20and%20limiting%20their%20applications%20in%20practical%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20the%20first%20large-scale%203D%20real%20car%20dataset%2C%20termed%203DRealCar%2C%0Aoffering%20three%20distinctive%20features.%20%281%29%20%5Ctextbf%7BHigh-Volume%7D%3A%202%2C500%20cars%20are%0Ameticulously%20scanned%20by%203D%20scanners%2C%20obtaining%20car%20images%20and%20point%20clouds%20with%0Areal-world%20dimensions%3B%20%282%29%20%5Ctextbf%7BHigh-Quality%7D%3A%20Each%20car%20is%20captured%20in%20an%0Aaverage%20of%20200%20dense%2C%20high-resolution%20360-degree%20RGB-D%20views%2C%20enabling%0Ahigh-fidelity%203D%20reconstruction%3B%20%283%29%20%5Ctextbf%7BHigh-Diversity%7D%3A%20The%20dataset%0Acontains%20various%20cars%20from%20over%20100%20brands%2C%20collected%20under%20three%20distinct%0Alighting%20conditions%2C%20including%20reflective%2C%20standard%2C%20and%20dark.%20Additionally%2C%20we%0Aoffer%20detailed%20car%20parsing%20maps%20for%20each%20instance%20to%20promote%20research%20in%20car%0Aparsing%20tasks.%20Moreover%2C%20we%20remove%20background%20point%20clouds%20and%20standardize%20the%0Acar%20orientation%20to%20a%20unified%20axis%20for%20the%20reconstruction%20only%20on%20cars%20without%0Abackground%20and%20controllable%20rendering.%20We%20benchmark%203D%20reconstruction%20results%0Awith%20state-of-the-art%20methods%20across%20each%20lighting%20condition%20in%203DRealCar.%0AExtensive%20experiments%20demonstrate%20that%20the%20standard%20lighting%20condition%20part%20of%0A3DRealCar%20can%20be%20used%20to%20produce%20a%20large%20number%20of%20high-quality%203D%20cars%2C%0Aimproving%20various%202D%20and%203D%20tasks%20related%20to%20cars.%20Notably%2C%20our%20dataset%20brings%0Ainsight%20into%20the%20fact%20that%20recent%203D%20reconstruction%20methods%20face%20challenges%20in%0Areconstructing%20high-quality%203D%20cars%20under%20reflective%20and%20dark%20lighting%0Aconditions.%20%5Ctextcolor%7Bred%7D%7B%5Chref%7Bhttps%3A//xiaobiaodu.github.io/3drealcar/%7D%7BOur%0Adataset%20is%20available%20here.%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DRealCar%253A%2520An%2520In-the-wild%2520RGB-D%2520Car%2520Dataset%2520with%2520360-degree%2520Views%26entry.906535625%3DXiaobiao%2520Du%2520and%2520Haiyang%2520Sun%2520and%2520Shuyun%2520Wang%2520and%2520Zhuojie%2520Wu%2520and%2520Hongwei%2520Sheng%2520and%2520Jiaying%2520Ying%2520and%2520Ming%2520Lu%2520and%2520Tianqing%2520Zhu%2520and%2520Kun%2520Zhan%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%25203D%2520cars%2520are%2520commonly%2520used%2520in%2520self-driving%2520systems%252C%2520virtual/augmented%2520reality%252C%250Aand%2520games.%2520However%252C%2520existing%25203D%2520car%2520datasets%2520are%2520either%2520synthetic%2520or%250Alow-quality%252C%2520presenting%2520a%2520significant%2520gap%2520toward%2520the%2520high-quality%2520real-world%25203D%250Acar%2520datasets%2520and%2520limiting%2520their%2520applications%2520in%2520practical%2520scenarios.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520first%2520large-scale%25203D%2520real%2520car%2520dataset%252C%2520termed%25203DRealCar%252C%250Aoffering%2520three%2520distinctive%2520features.%2520%25281%2529%2520%255Ctextbf%257BHigh-Volume%257D%253A%25202%252C500%2520cars%2520are%250Ameticulously%2520scanned%2520by%25203D%2520scanners%252C%2520obtaining%2520car%2520images%2520and%2520point%2520clouds%2520with%250Areal-world%2520dimensions%253B%2520%25282%2529%2520%255Ctextbf%257BHigh-Quality%257D%253A%2520Each%2520car%2520is%2520captured%2520in%2520an%250Aaverage%2520of%2520200%2520dense%252C%2520high-resolution%2520360-degree%2520RGB-D%2520views%252C%2520enabling%250Ahigh-fidelity%25203D%2520reconstruction%253B%2520%25283%2529%2520%255Ctextbf%257BHigh-Diversity%257D%253A%2520The%2520dataset%250Acontains%2520various%2520cars%2520from%2520over%2520100%2520brands%252C%2520collected%2520under%2520three%2520distinct%250Alighting%2520conditions%252C%2520including%2520reflective%252C%2520standard%252C%2520and%2520dark.%2520Additionally%252C%2520we%250Aoffer%2520detailed%2520car%2520parsing%2520maps%2520for%2520each%2520instance%2520to%2520promote%2520research%2520in%2520car%250Aparsing%2520tasks.%2520Moreover%252C%2520we%2520remove%2520background%2520point%2520clouds%2520and%2520standardize%2520the%250Acar%2520orientation%2520to%2520a%2520unified%2520axis%2520for%2520the%2520reconstruction%2520only%2520on%2520cars%2520without%250Abackground%2520and%2520controllable%2520rendering.%2520We%2520benchmark%25203D%2520reconstruction%2520results%250Awith%2520state-of-the-art%2520methods%2520across%2520each%2520lighting%2520condition%2520in%25203DRealCar.%250AExtensive%2520experiments%2520demonstrate%2520that%2520the%2520standard%2520lighting%2520condition%2520part%2520of%250A3DRealCar%2520can%2520be%2520used%2520to%2520produce%2520a%2520large%2520number%2520of%2520high-quality%25203D%2520cars%252C%250Aimproving%2520various%25202D%2520and%25203D%2520tasks%2520related%2520to%2520cars.%2520Notably%252C%2520our%2520dataset%2520brings%250Ainsight%2520into%2520the%2520fact%2520that%2520recent%25203D%2520reconstruction%2520methods%2520face%2520challenges%2520in%250Areconstructing%2520high-quality%25203D%2520cars%2520under%2520reflective%2520and%2520dark%2520lighting%250Aconditions.%2520%255Ctextcolor%257Bred%257D%257B%255Chref%257Bhttps%253A//xiaobiaodu.github.io/3drealcar/%257D%257BOur%250Adataset%2520is%2520available%2520here.%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DRealCar%3A%20An%20In-the-wild%20RGB-D%20Car%20Dataset%20with%20360-degree%20Views&entry.906535625=Xiaobiao%20Du%20and%20Haiyang%20Sun%20and%20Shuyun%20Wang%20and%20Zhuojie%20Wu%20and%20Hongwei%20Sheng%20and%20Jiaying%20Ying%20and%20Ming%20Lu%20and%20Tianqing%20Zhu%20and%20Kun%20Zhan%20and%20Xin%20Yu&entry.1292438233=%20%203D%20cars%20are%20commonly%20used%20in%20self-driving%20systems%2C%20virtual/augmented%20reality%2C%0Aand%20games.%20However%2C%20existing%203D%20car%20datasets%20are%20either%20synthetic%20or%0Alow-quality%2C%20presenting%20a%20significant%20gap%20toward%20the%20high-quality%20real-world%203D%0Acar%20datasets%20and%20limiting%20their%20applications%20in%20practical%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20the%20first%20large-scale%203D%20real%20car%20dataset%2C%20termed%203DRealCar%2C%0Aoffering%20three%20distinctive%20features.%20%281%29%20%5Ctextbf%7BHigh-Volume%7D%3A%202%2C500%20cars%20are%0Ameticulously%20scanned%20by%203D%20scanners%2C%20obtaining%20car%20images%20and%20point%20clouds%20with%0Areal-world%20dimensions%3B%20%282%29%20%5Ctextbf%7BHigh-Quality%7D%3A%20Each%20car%20is%20captured%20in%20an%0Aaverage%20of%20200%20dense%2C%20high-resolution%20360-degree%20RGB-D%20views%2C%20enabling%0Ahigh-fidelity%203D%20reconstruction%3B%20%283%29%20%5Ctextbf%7BHigh-Diversity%7D%3A%20The%20dataset%0Acontains%20various%20cars%20from%20over%20100%20brands%2C%20collected%20under%20three%20distinct%0Alighting%20conditions%2C%20including%20reflective%2C%20standard%2C%20and%20dark.%20Additionally%2C%20we%0Aoffer%20detailed%20car%20parsing%20maps%20for%20each%20instance%20to%20promote%20research%20in%20car%0Aparsing%20tasks.%20Moreover%2C%20we%20remove%20background%20point%20clouds%20and%20standardize%20the%0Acar%20orientation%20to%20a%20unified%20axis%20for%20the%20reconstruction%20only%20on%20cars%20without%0Abackground%20and%20controllable%20rendering.%20We%20benchmark%203D%20reconstruction%20results%0Awith%20state-of-the-art%20methods%20across%20each%20lighting%20condition%20in%203DRealCar.%0AExtensive%20experiments%20demonstrate%20that%20the%20standard%20lighting%20condition%20part%20of%0A3DRealCar%20can%20be%20used%20to%20produce%20a%20large%20number%20of%20high-quality%203D%20cars%2C%0Aimproving%20various%202D%20and%203D%20tasks%20related%20to%20cars.%20Notably%2C%20our%20dataset%20brings%0Ainsight%20into%20the%20fact%20that%20recent%203D%20reconstruction%20methods%20face%20challenges%20in%0Areconstructing%20high-quality%203D%20cars%20under%20reflective%20and%20dark%20lighting%0Aconditions.%20%5Ctextcolor%7Bred%7D%7B%5Chref%7Bhttps%3A//xiaobiaodu.github.io/3drealcar/%7D%7BOur%0Adataset%20is%20available%20here.%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04875v1&entry.124074799=Read"},
{"title": "GenHeld: Generating and Editing Handheld Objects", "author": "Chaerin Min and Srinath Sridhar", "abstract": "  Grasping is an important human activity that has long been studied in\nrobotics, computer vision, and cognitive science. Most existing works study\ngrasping from the perspective of synthesizing hand poses conditioned on 3D or\n2D object representations. We propose GenHeld to address the inverse problem of\nsynthesizing held objects conditioned on 3D hand model or 2D image. Given a 3D\nmodel of hand, GenHeld 3D can select a plausible held object from a large\ndataset using compact object representations called object codes.The selected\nobject is then positioned and oriented to form a plausible grasp without\nchanging hand pose. If only a 2D hand image is available, GenHeld 2D can edit\nthis image to add or replace a held object. GenHeld 2D operates by combining\nthe abilities of GenHeld 3D with diffusion-based image editing. Results and\nexperiments show that we outperform baselines and can generate plausible held\nobjects in both 2D and 3D. Our experiments demonstrate that our method achieves\nhigh quality and plausibility of held object synthesis in both 3D and 2D.\n", "link": "http://arxiv.org/abs/2406.05059v1", "date": "2024-06-07", "relevancy": 3.0341, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6781}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5951}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects&body=Title%3A%20GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects%0AAuthor%3A%20Chaerin%20Min%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Grasping%20is%20an%20important%20human%20activity%20that%20has%20long%20been%20studied%20in%0Arobotics%2C%20computer%20vision%2C%20and%20cognitive%20science.%20Most%20existing%20works%20study%0Agrasping%20from%20the%20perspective%20of%20synthesizing%20hand%20poses%20conditioned%20on%203D%20or%0A2D%20object%20representations.%20We%20propose%20GenHeld%20to%20address%20the%20inverse%20problem%20of%0Asynthesizing%20held%20objects%20conditioned%20on%203D%20hand%20model%20or%202D%20image.%20Given%20a%203D%0Amodel%20of%20hand%2C%20GenHeld%203D%20can%20select%20a%20plausible%20held%20object%20from%20a%20large%0Adataset%20using%20compact%20object%20representations%20called%20object%20codes.The%20selected%0Aobject%20is%20then%20positioned%20and%20oriented%20to%20form%20a%20plausible%20grasp%20without%0Achanging%20hand%20pose.%20If%20only%20a%202D%20hand%20image%20is%20available%2C%20GenHeld%202D%20can%20edit%0Athis%20image%20to%20add%20or%20replace%20a%20held%20object.%20GenHeld%202D%20operates%20by%20combining%0Athe%20abilities%20of%20GenHeld%203D%20with%20diffusion-based%20image%20editing.%20Results%20and%0Aexperiments%20show%20that%20we%20outperform%20baselines%20and%20can%20generate%20plausible%20held%0Aobjects%20in%20both%202D%20and%203D.%20Our%20experiments%20demonstrate%20that%20our%20method%20achieves%0Ahigh%20quality%20and%20plausibility%20of%20held%20object%20synthesis%20in%20both%203D%20and%202D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenHeld%253A%2520Generating%2520and%2520Editing%2520Handheld%2520Objects%26entry.906535625%3DChaerin%2520Min%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3D%2520%2520Grasping%2520is%2520an%2520important%2520human%2520activity%2520that%2520has%2520long%2520been%2520studied%2520in%250Arobotics%252C%2520computer%2520vision%252C%2520and%2520cognitive%2520science.%2520Most%2520existing%2520works%2520study%250Agrasping%2520from%2520the%2520perspective%2520of%2520synthesizing%2520hand%2520poses%2520conditioned%2520on%25203D%2520or%250A2D%2520object%2520representations.%2520We%2520propose%2520GenHeld%2520to%2520address%2520the%2520inverse%2520problem%2520of%250Asynthesizing%2520held%2520objects%2520conditioned%2520on%25203D%2520hand%2520model%2520or%25202D%2520image.%2520Given%2520a%25203D%250Amodel%2520of%2520hand%252C%2520GenHeld%25203D%2520can%2520select%2520a%2520plausible%2520held%2520object%2520from%2520a%2520large%250Adataset%2520using%2520compact%2520object%2520representations%2520called%2520object%2520codes.The%2520selected%250Aobject%2520is%2520then%2520positioned%2520and%2520oriented%2520to%2520form%2520a%2520plausible%2520grasp%2520without%250Achanging%2520hand%2520pose.%2520If%2520only%2520a%25202D%2520hand%2520image%2520is%2520available%252C%2520GenHeld%25202D%2520can%2520edit%250Athis%2520image%2520to%2520add%2520or%2520replace%2520a%2520held%2520object.%2520GenHeld%25202D%2520operates%2520by%2520combining%250Athe%2520abilities%2520of%2520GenHeld%25203D%2520with%2520diffusion-based%2520image%2520editing.%2520Results%2520and%250Aexperiments%2520show%2520that%2520we%2520outperform%2520baselines%2520and%2520can%2520generate%2520plausible%2520held%250Aobjects%2520in%2520both%25202D%2520and%25203D.%2520Our%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Ahigh%2520quality%2520and%2520plausibility%2520of%2520held%2520object%2520synthesis%2520in%2520both%25203D%2520and%25202D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenHeld%3A%20Generating%20and%20Editing%20Handheld%20Objects&entry.906535625=Chaerin%20Min%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Grasping%20is%20an%20important%20human%20activity%20that%20has%20long%20been%20studied%20in%0Arobotics%2C%20computer%20vision%2C%20and%20cognitive%20science.%20Most%20existing%20works%20study%0Agrasping%20from%20the%20perspective%20of%20synthesizing%20hand%20poses%20conditioned%20on%203D%20or%0A2D%20object%20representations.%20We%20propose%20GenHeld%20to%20address%20the%20inverse%20problem%20of%0Asynthesizing%20held%20objects%20conditioned%20on%203D%20hand%20model%20or%202D%20image.%20Given%20a%203D%0Amodel%20of%20hand%2C%20GenHeld%203D%20can%20select%20a%20plausible%20held%20object%20from%20a%20large%0Adataset%20using%20compact%20object%20representations%20called%20object%20codes.The%20selected%0Aobject%20is%20then%20positioned%20and%20oriented%20to%20form%20a%20plausible%20grasp%20without%0Achanging%20hand%20pose.%20If%20only%20a%202D%20hand%20image%20is%20available%2C%20GenHeld%202D%20can%20edit%0Athis%20image%20to%20add%20or%20replace%20a%20held%20object.%20GenHeld%202D%20operates%20by%20combining%0Athe%20abilities%20of%20GenHeld%203D%20with%20diffusion-based%20image%20editing.%20Results%20and%0Aexperiments%20show%20that%20we%20outperform%20baselines%20and%20can%20generate%20plausible%20held%0Aobjects%20in%20both%202D%20and%203D.%20Our%20experiments%20demonstrate%20that%20our%20method%20achieves%0Ahigh%20quality%20and%20plausibility%20of%20held%20object%20synthesis%20in%20both%203D%20and%202D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05059v1&entry.124074799=Read"},
{"title": "Normal-guided Detail-Preserving Neural Implicit Functions for\n  High-Fidelity 3D Surface Reconstruction", "author": "Aarya Patel and Hamid Laga and Ojaswa Sharma", "abstract": "  Neural implicit representations have emerged as a powerful paradigm for 3D\nreconstruction. However, despite their success, existing methods fail to\ncapture fine geometric details and thin structures, especially in scenarios\nwhere only sparse RGB views of the objects of interest are available. We\nhypothesize that current methods for learning neural implicit representations\nfrom RGB or RGBD images produce 3D surfaces with missing parts and details\nbecause they only rely on 0-order differential properties, i.e. the 3D surface\npoints and their projections, as supervisory signals. Such properties, however,\ndo not capture the local 3D geometry around the points and also ignore the\ninteractions between points. This paper demonstrates that training neural\nrepresentations with first-order differential properties, i.e. surface normals,\nleads to highly accurate 3D surface reconstruction even in situations where\nonly as few as two RGB (front and back) images are available. Given multiview\nRGB images of an object of interest, we first compute the approximate surface\nnormals in the image space using the gradient of the depth maps produced using\nan off-the-shelf monocular depth estimator such as Depth Anything model. An\nimplicit surface regressor is then trained using a loss function that enforces\nthe first-order differential properties of the regressed surface to match those\nestimated from Depth Anything. Our extensive experiments on a wide range of\nreal and synthetic datasets show that the proposed method achieves an\nunprecedented level of reconstruction accuracy even when using as few as two\nRGB views. The detailed ablation study also demonstrates that normal-based\nsupervision plays a key role in this significant improvement in performance,\nenabling the 3D reconstruction of intricate geometric details and thin\nstructures that were previously challenging to capture.\n", "link": "http://arxiv.org/abs/2406.04861v1", "date": "2024-06-07", "relevancy": 2.9469, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5969}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5857}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Functions%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction&body=Title%3A%20Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Functions%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction%0AAuthor%3A%20Aarya%20Patel%20and%20Hamid%20Laga%20and%20Ojaswa%20Sharma%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Areconstruction.%20However%2C%20despite%20their%20success%2C%20existing%20methods%20fail%20to%0Acapture%20fine%20geometric%20details%20and%20thin%20structures%2C%20especially%20in%20scenarios%0Awhere%20only%20sparse%20RGB%20views%20of%20the%20objects%20of%20interest%20are%20available.%20We%0Ahypothesize%20that%20current%20methods%20for%20learning%20neural%20implicit%20representations%0Afrom%20RGB%20or%20RGBD%20images%20produce%203D%20surfaces%20with%20missing%20parts%20and%20details%0Abecause%20they%20only%20rely%20on%200-order%20differential%20properties%2C%20i.e.%20the%203D%20surface%0Apoints%20and%20their%20projections%2C%20as%20supervisory%20signals.%20Such%20properties%2C%20however%2C%0Ado%20not%20capture%20the%20local%203D%20geometry%20around%20the%20points%20and%20also%20ignore%20the%0Ainteractions%20between%20points.%20This%20paper%20demonstrates%20that%20training%20neural%0Arepresentations%20with%20first-order%20differential%20properties%2C%20i.e.%20surface%20normals%2C%0Aleads%20to%20highly%20accurate%203D%20surface%20reconstruction%20even%20in%20situations%20where%0Aonly%20as%20few%20as%20two%20RGB%20%28front%20and%20back%29%20images%20are%20available.%20Given%20multiview%0ARGB%20images%20of%20an%20object%20of%20interest%2C%20we%20first%20compute%20the%20approximate%20surface%0Anormals%20in%20the%20image%20space%20using%20the%20gradient%20of%20the%20depth%20maps%20produced%20using%0Aan%20off-the-shelf%20monocular%20depth%20estimator%20such%20as%20Depth%20Anything%20model.%20An%0Aimplicit%20surface%20regressor%20is%20then%20trained%20using%20a%20loss%20function%20that%20enforces%0Athe%20first-order%20differential%20properties%20of%20the%20regressed%20surface%20to%20match%20those%0Aestimated%20from%20Depth%20Anything.%20Our%20extensive%20experiments%20on%20a%20wide%20range%20of%0Areal%20and%20synthetic%20datasets%20show%20that%20the%20proposed%20method%20achieves%20an%0Aunprecedented%20level%20of%20reconstruction%20accuracy%20even%20when%20using%20as%20few%20as%20two%0ARGB%20views.%20The%20detailed%20ablation%20study%20also%20demonstrates%20that%20normal-based%0Asupervision%20plays%20a%20key%20role%20in%20this%20significant%20improvement%20in%20performance%2C%0Aenabling%20the%203D%20reconstruction%20of%20intricate%20geometric%20details%20and%20thin%0Astructures%20that%20were%20previously%20challenging%20to%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormal-guided%2520Detail-Preserving%2520Neural%2520Implicit%2520Functions%2520for%250A%2520%2520High-Fidelity%25203D%2520Surface%2520Reconstruction%26entry.906535625%3DAarya%2520Patel%2520and%2520Hamid%2520Laga%2520and%2520Ojaswa%2520Sharma%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%25203D%250Areconstruction.%2520However%252C%2520despite%2520their%2520success%252C%2520existing%2520methods%2520fail%2520to%250Acapture%2520fine%2520geometric%2520details%2520and%2520thin%2520structures%252C%2520especially%2520in%2520scenarios%250Awhere%2520only%2520sparse%2520RGB%2520views%2520of%2520the%2520objects%2520of%2520interest%2520are%2520available.%2520We%250Ahypothesize%2520that%2520current%2520methods%2520for%2520learning%2520neural%2520implicit%2520representations%250Afrom%2520RGB%2520or%2520RGBD%2520images%2520produce%25203D%2520surfaces%2520with%2520missing%2520parts%2520and%2520details%250Abecause%2520they%2520only%2520rely%2520on%25200-order%2520differential%2520properties%252C%2520i.e.%2520the%25203D%2520surface%250Apoints%2520and%2520their%2520projections%252C%2520as%2520supervisory%2520signals.%2520Such%2520properties%252C%2520however%252C%250Ado%2520not%2520capture%2520the%2520local%25203D%2520geometry%2520around%2520the%2520points%2520and%2520also%2520ignore%2520the%250Ainteractions%2520between%2520points.%2520This%2520paper%2520demonstrates%2520that%2520training%2520neural%250Arepresentations%2520with%2520first-order%2520differential%2520properties%252C%2520i.e.%2520surface%2520normals%252C%250Aleads%2520to%2520highly%2520accurate%25203D%2520surface%2520reconstruction%2520even%2520in%2520situations%2520where%250Aonly%2520as%2520few%2520as%2520two%2520RGB%2520%2528front%2520and%2520back%2529%2520images%2520are%2520available.%2520Given%2520multiview%250ARGB%2520images%2520of%2520an%2520object%2520of%2520interest%252C%2520we%2520first%2520compute%2520the%2520approximate%2520surface%250Anormals%2520in%2520the%2520image%2520space%2520using%2520the%2520gradient%2520of%2520the%2520depth%2520maps%2520produced%2520using%250Aan%2520off-the-shelf%2520monocular%2520depth%2520estimator%2520such%2520as%2520Depth%2520Anything%2520model.%2520An%250Aimplicit%2520surface%2520regressor%2520is%2520then%2520trained%2520using%2520a%2520loss%2520function%2520that%2520enforces%250Athe%2520first-order%2520differential%2520properties%2520of%2520the%2520regressed%2520surface%2520to%2520match%2520those%250Aestimated%2520from%2520Depth%2520Anything.%2520Our%2520extensive%2520experiments%2520on%2520a%2520wide%2520range%2520of%250Areal%2520and%2520synthetic%2520datasets%2520show%2520that%2520the%2520proposed%2520method%2520achieves%2520an%250Aunprecedented%2520level%2520of%2520reconstruction%2520accuracy%2520even%2520when%2520using%2520as%2520few%2520as%2520two%250ARGB%2520views.%2520The%2520detailed%2520ablation%2520study%2520also%2520demonstrates%2520that%2520normal-based%250Asupervision%2520plays%2520a%2520key%2520role%2520in%2520this%2520significant%2520improvement%2520in%2520performance%252C%250Aenabling%2520the%25203D%2520reconstruction%2520of%2520intricate%2520geometric%2520details%2520and%2520thin%250Astructures%2520that%2520were%2520previously%2520challenging%2520to%2520capture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normal-guided%20Detail-Preserving%20Neural%20Implicit%20Functions%20for%0A%20%20High-Fidelity%203D%20Surface%20Reconstruction&entry.906535625=Aarya%20Patel%20and%20Hamid%20Laga%20and%20Ojaswa%20Sharma&entry.1292438233=%20%20Neural%20implicit%20representations%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Areconstruction.%20However%2C%20despite%20their%20success%2C%20existing%20methods%20fail%20to%0Acapture%20fine%20geometric%20details%20and%20thin%20structures%2C%20especially%20in%20scenarios%0Awhere%20only%20sparse%20RGB%20views%20of%20the%20objects%20of%20interest%20are%20available.%20We%0Ahypothesize%20that%20current%20methods%20for%20learning%20neural%20implicit%20representations%0Afrom%20RGB%20or%20RGBD%20images%20produce%203D%20surfaces%20with%20missing%20parts%20and%20details%0Abecause%20they%20only%20rely%20on%200-order%20differential%20properties%2C%20i.e.%20the%203D%20surface%0Apoints%20and%20their%20projections%2C%20as%20supervisory%20signals.%20Such%20properties%2C%20however%2C%0Ado%20not%20capture%20the%20local%203D%20geometry%20around%20the%20points%20and%20also%20ignore%20the%0Ainteractions%20between%20points.%20This%20paper%20demonstrates%20that%20training%20neural%0Arepresentations%20with%20first-order%20differential%20properties%2C%20i.e.%20surface%20normals%2C%0Aleads%20to%20highly%20accurate%203D%20surface%20reconstruction%20even%20in%20situations%20where%0Aonly%20as%20few%20as%20two%20RGB%20%28front%20and%20back%29%20images%20are%20available.%20Given%20multiview%0ARGB%20images%20of%20an%20object%20of%20interest%2C%20we%20first%20compute%20the%20approximate%20surface%0Anormals%20in%20the%20image%20space%20using%20the%20gradient%20of%20the%20depth%20maps%20produced%20using%0Aan%20off-the-shelf%20monocular%20depth%20estimator%20such%20as%20Depth%20Anything%20model.%20An%0Aimplicit%20surface%20regressor%20is%20then%20trained%20using%20a%20loss%20function%20that%20enforces%0Athe%20first-order%20differential%20properties%20of%20the%20regressed%20surface%20to%20match%20those%0Aestimated%20from%20Depth%20Anything.%20Our%20extensive%20experiments%20on%20a%20wide%20range%20of%0Areal%20and%20synthetic%20datasets%20show%20that%20the%20proposed%20method%20achieves%20an%0Aunprecedented%20level%20of%20reconstruction%20accuracy%20even%20when%20using%20as%20few%20as%20two%0ARGB%20views.%20The%20detailed%20ablation%20study%20also%20demonstrates%20that%20normal-based%0Asupervision%20plays%20a%20key%20role%20in%20this%20significant%20improvement%20in%20performance%2C%0Aenabling%20the%203D%20reconstruction%20of%20intricate%20geometric%20details%20and%20thin%0Astructures%20that%20were%20previously%20challenging%20to%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04861v1&entry.124074799=Read"},
{"title": "GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions", "author": "Salvatore Esposito and Qingshan Xu and Kacper Kania and Charlie Hewitt and Octave Mariotti and Lohit Petikam and Julien Valentin and Arno Onken and Oisin Mac Aodha", "abstract": "  We introduce a new generative approach for synthesizing 3D geometry and\nimages from single-view collections. Most existing approaches predict\nvolumetric density to render multi-view consistent images. By employing\nvolumetric rendering using neural radiance fields, they inherit a key\nlimitation: the generated geometry is noisy and unconstrained, limiting the\nquality and utility of the output meshes. To address this issue, we propose\nGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.\nInitially, we reinterpret the volumetric density as a Signed Distance Function\n(SDF). This allows us to introduce useful priors to generate valid meshes.\nHowever, those priors prevent the generative model from learning details,\nlimiting the applicability of the method to real-world scenarios. To alleviate\nthat problem, we make the transformation learnable and constrain the rendered\ndepth map to be consistent with the zero-level set of the SDF. Through the lens\nof adversarial training, we encourage the network to produce higher fidelity\ndetails on the output meshes. For evaluation, we introduce a synthetic dataset\nof human avatars captured from 360-degree camera angles, to overcome the\nchallenges presented by real-world datasets, which often lack 3D consistency\nand do not cover all camera angles. Our experiments on multiple datasets show\nthat GeoGen produces visually and quantitatively better geometry than the\nprevious generative models based on neural radiance fields.\n", "link": "http://arxiv.org/abs/2406.04254v2", "date": "2024-06-07", "relevancy": 2.9382, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6019}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&body=Title%3A%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions%0AAuthor%3A%20Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoGen%253A%2520Geometry-Aware%2520Generative%2520Modeling%2520via%2520Signed%2520Distance%2520Functions%26entry.906535625%3DSalvatore%2520Esposito%2520and%2520Qingshan%2520Xu%2520and%2520Kacper%2520Kania%2520and%2520Charlie%2520Hewitt%2520and%2520Octave%2520Mariotti%2520and%2520Lohit%2520Petikam%2520and%2520Julien%2520Valentin%2520and%2520Arno%2520Onken%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520generative%2520approach%2520for%2520synthesizing%25203D%2520geometry%2520and%250Aimages%2520from%2520single-view%2520collections.%2520Most%2520existing%2520approaches%2520predict%250Avolumetric%2520density%2520to%2520render%2520multi-view%2520consistent%2520images.%2520By%2520employing%250Avolumetric%2520rendering%2520using%2520neural%2520radiance%2520fields%252C%2520they%2520inherit%2520a%2520key%250Alimitation%253A%2520the%2520generated%2520geometry%2520is%2520noisy%2520and%2520unconstrained%252C%2520limiting%2520the%250Aquality%2520and%2520utility%2520of%2520the%2520output%2520meshes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AGeoGen%252C%2520a%2520new%2520SDF-based%25203D%2520generative%2520model%2520trained%2520in%2520an%2520end-to-end%2520manner.%250AInitially%252C%2520we%2520reinterpret%2520the%2520volumetric%2520density%2520as%2520a%2520Signed%2520Distance%2520Function%250A%2528SDF%2529.%2520This%2520allows%2520us%2520to%2520introduce%2520useful%2520priors%2520to%2520generate%2520valid%2520meshes.%250AHowever%252C%2520those%2520priors%2520prevent%2520the%2520generative%2520model%2520from%2520learning%2520details%252C%250Alimiting%2520the%2520applicability%2520of%2520the%2520method%2520to%2520real-world%2520scenarios.%2520To%2520alleviate%250Athat%2520problem%252C%2520we%2520make%2520the%2520transformation%2520learnable%2520and%2520constrain%2520the%2520rendered%250Adepth%2520map%2520to%2520be%2520consistent%2520with%2520the%2520zero-level%2520set%2520of%2520the%2520SDF.%2520Through%2520the%2520lens%250Aof%2520adversarial%2520training%252C%2520we%2520encourage%2520the%2520network%2520to%2520produce%2520higher%2520fidelity%250Adetails%2520on%2520the%2520output%2520meshes.%2520For%2520evaluation%252C%2520we%2520introduce%2520a%2520synthetic%2520dataset%250Aof%2520human%2520avatars%2520captured%2520from%2520360-degree%2520camera%2520angles%252C%2520to%2520overcome%2520the%250Achallenges%2520presented%2520by%2520real-world%2520datasets%252C%2520which%2520often%2520lack%25203D%2520consistency%250Aand%2520do%2520not%2520cover%2520all%2520camera%2520angles.%2520Our%2520experiments%2520on%2520multiple%2520datasets%2520show%250Athat%2520GeoGen%2520produces%2520visually%2520and%2520quantitatively%2520better%2520geometry%2520than%2520the%250Aprevious%2520generative%2520models%2520based%2520on%2520neural%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&entry.906535625=Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04254v2&entry.124074799=Read"},
{"title": "Boundary Discretization and Reliable Classification Network for Temporal\n  Action Detection", "author": "Zhenying Fang and Jun Yu and Richang Hong", "abstract": "  Temporal action detection aims to recognize the action category and determine\neach action instance's starting and ending time in untrimmed videos. The mixed\nmethods have achieved remarkable performance by seamlessly merging anchor-based\nand anchor-free approaches. Nonetheless, there are still two crucial issues\nwithin the mixed framework: (1) Brute-force merging and handcrafted anchor\ndesign hinder the substantial potential and practicality of the mixed methods.\n(2) Within-category predictions show a significant abundance of false\npositives. In this paper, we propose a novel Boundary Discretization and\nReliable Classification Network (BDRC-Net) that addresses the issues above by\nintroducing boundary discretization and reliable classification modules.\nSpecifically, the boundary discretization module (BDM) elegantly merges\nanchor-based and anchor-free approaches in the form of boundary discretization,\neliminating the need for the traditional handcrafted anchor design.\nFurthermore, the reliable classification module (RCM) predicts reliable global\naction categories to reduce false positives. Extensive experiments conducted on\ndifferent benchmarks demonstrate that our proposed method achieves competitive\ndetection performance. The code will be released at\nhttps://github.com/zhenyingfang/BDRC-Net.\n", "link": "http://arxiv.org/abs/2310.06403v4", "date": "2024-06-07", "relevancy": 2.7603, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6208}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary%20Discretization%20and%20Reliable%20Classification%20Network%20for%20Temporal%0A%20%20Action%20Detection&body=Title%3A%20Boundary%20Discretization%20and%20Reliable%20Classification%20Network%20for%20Temporal%0A%20%20Action%20Detection%0AAuthor%3A%20Zhenying%20Fang%20and%20Jun%20Yu%20and%20Richang%20Hong%0AAbstract%3A%20%20%20Temporal%20action%20detection%20aims%20to%20recognize%20the%20action%20category%20and%20determine%0Aeach%20action%20instance%27s%20starting%20and%20ending%20time%20in%20untrimmed%20videos.%20The%20mixed%0Amethods%20have%20achieved%20remarkable%20performance%20by%20seamlessly%20merging%20anchor-based%0Aand%20anchor-free%20approaches.%20Nonetheless%2C%20there%20are%20still%20two%20crucial%20issues%0Awithin%20the%20mixed%20framework%3A%20%281%29%20Brute-force%20merging%20and%20handcrafted%20anchor%0Adesign%20hinder%20the%20substantial%20potential%20and%20practicality%20of%20the%20mixed%20methods.%0A%282%29%20Within-category%20predictions%20show%20a%20significant%20abundance%20of%20false%0Apositives.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Boundary%20Discretization%20and%0AReliable%20Classification%20Network%20%28BDRC-Net%29%20that%20addresses%20the%20issues%20above%20by%0Aintroducing%20boundary%20discretization%20and%20reliable%20classification%20modules.%0ASpecifically%2C%20the%20boundary%20discretization%20module%20%28BDM%29%20elegantly%20merges%0Aanchor-based%20and%20anchor-free%20approaches%20in%20the%20form%20of%20boundary%20discretization%2C%0Aeliminating%20the%20need%20for%20the%20traditional%20handcrafted%20anchor%20design.%0AFurthermore%2C%20the%20reliable%20classification%20module%20%28RCM%29%20predicts%20reliable%20global%0Aaction%20categories%20to%20reduce%20false%20positives.%20Extensive%20experiments%20conducted%20on%0Adifferent%20benchmarks%20demonstrate%20that%20our%20proposed%20method%20achieves%20competitive%0Adetection%20performance.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/zhenyingfang/BDRC-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06403v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary%2520Discretization%2520and%2520Reliable%2520Classification%2520Network%2520for%2520Temporal%250A%2520%2520Action%2520Detection%26entry.906535625%3DZhenying%2520Fang%2520and%2520Jun%2520Yu%2520and%2520Richang%2520Hong%26entry.1292438233%3D%2520%2520Temporal%2520action%2520detection%2520aims%2520to%2520recognize%2520the%2520action%2520category%2520and%2520determine%250Aeach%2520action%2520instance%2527s%2520starting%2520and%2520ending%2520time%2520in%2520untrimmed%2520videos.%2520The%2520mixed%250Amethods%2520have%2520achieved%2520remarkable%2520performance%2520by%2520seamlessly%2520merging%2520anchor-based%250Aand%2520anchor-free%2520approaches.%2520Nonetheless%252C%2520there%2520are%2520still%2520two%2520crucial%2520issues%250Awithin%2520the%2520mixed%2520framework%253A%2520%25281%2529%2520Brute-force%2520merging%2520and%2520handcrafted%2520anchor%250Adesign%2520hinder%2520the%2520substantial%2520potential%2520and%2520practicality%2520of%2520the%2520mixed%2520methods.%250A%25282%2529%2520Within-category%2520predictions%2520show%2520a%2520significant%2520abundance%2520of%2520false%250Apositives.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Boundary%2520Discretization%2520and%250AReliable%2520Classification%2520Network%2520%2528BDRC-Net%2529%2520that%2520addresses%2520the%2520issues%2520above%2520by%250Aintroducing%2520boundary%2520discretization%2520and%2520reliable%2520classification%2520modules.%250ASpecifically%252C%2520the%2520boundary%2520discretization%2520module%2520%2528BDM%2529%2520elegantly%2520merges%250Aanchor-based%2520and%2520anchor-free%2520approaches%2520in%2520the%2520form%2520of%2520boundary%2520discretization%252C%250Aeliminating%2520the%2520need%2520for%2520the%2520traditional%2520handcrafted%2520anchor%2520design.%250AFurthermore%252C%2520the%2520reliable%2520classification%2520module%2520%2528RCM%2529%2520predicts%2520reliable%2520global%250Aaction%2520categories%2520to%2520reduce%2520false%2520positives.%2520Extensive%2520experiments%2520conducted%2520on%250Adifferent%2520benchmarks%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520competitive%250Adetection%2520performance.%2520The%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/zhenyingfang/BDRC-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06403v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary%20Discretization%20and%20Reliable%20Classification%20Network%20for%20Temporal%0A%20%20Action%20Detection&entry.906535625=Zhenying%20Fang%20and%20Jun%20Yu%20and%20Richang%20Hong&entry.1292438233=%20%20Temporal%20action%20detection%20aims%20to%20recognize%20the%20action%20category%20and%20determine%0Aeach%20action%20instance%27s%20starting%20and%20ending%20time%20in%20untrimmed%20videos.%20The%20mixed%0Amethods%20have%20achieved%20remarkable%20performance%20by%20seamlessly%20merging%20anchor-based%0Aand%20anchor-free%20approaches.%20Nonetheless%2C%20there%20are%20still%20two%20crucial%20issues%0Awithin%20the%20mixed%20framework%3A%20%281%29%20Brute-force%20merging%20and%20handcrafted%20anchor%0Adesign%20hinder%20the%20substantial%20potential%20and%20practicality%20of%20the%20mixed%20methods.%0A%282%29%20Within-category%20predictions%20show%20a%20significant%20abundance%20of%20false%0Apositives.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Boundary%20Discretization%20and%0AReliable%20Classification%20Network%20%28BDRC-Net%29%20that%20addresses%20the%20issues%20above%20by%0Aintroducing%20boundary%20discretization%20and%20reliable%20classification%20modules.%0ASpecifically%2C%20the%20boundary%20discretization%20module%20%28BDM%29%20elegantly%20merges%0Aanchor-based%20and%20anchor-free%20approaches%20in%20the%20form%20of%20boundary%20discretization%2C%0Aeliminating%20the%20need%20for%20the%20traditional%20handcrafted%20anchor%20design.%0AFurthermore%2C%20the%20reliable%20classification%20module%20%28RCM%29%20predicts%20reliable%20global%0Aaction%20categories%20to%20reduce%20false%20positives.%20Extensive%20experiments%20conducted%20on%0Adifferent%20benchmarks%20demonstrate%20that%20our%20proposed%20method%20achieves%20competitive%0Adetection%20performance.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/zhenyingfang/BDRC-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06403v4&entry.124074799=Read"},
{"title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers", "author": "Takeru Miyato and Bernhard Jaeger and Max Welling and Andreas Geiger", "abstract": "  As transformers are equivariant to the permutation of input tokens, encoding\nthe positional information of tokens is necessary for many tasks. However,\nsince existing positional encoding schemes have been initially designed for NLP\ntasks, their suitability for vision tasks, which typically exhibit different\nstructural properties in their data, is questionable. We argue that existing\npositional encoding schemes are suboptimal for 3D vision tasks, as they do not\nrespect their underlying 3D geometric structure. Based on this hypothesis, we\npropose a geometry-aware attention mechanism that encodes the geometric\nstructure of tokens as relative transformation determined by the geometric\nrelationship between queries and key-value pairs. By evaluating on multiple\nnovel view synthesis (NVS) datasets in the sparse wide-baseline multi-view\nsetting, we show that our attention, called Geometric Transform Attention\n(GTA), improves learning efficiency and performance of state-of-the-art\ntransformer-based NVS models without any additional learned parameters and only\nminor computational overhead.\n", "link": "http://arxiv.org/abs/2310.10375v3", "date": "2024-06-07", "relevancy": 2.7507, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTA%3A%20A%20Geometry-Aware%20Attention%20Mechanism%20for%20Multi-View%20Transformers&body=Title%3A%20GTA%3A%20A%20Geometry-Aware%20Attention%20Mechanism%20for%20Multi-View%20Transformers%0AAuthor%3A%20Takeru%20Miyato%20and%20Bernhard%20Jaeger%20and%20Max%20Welling%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20As%20transformers%20are%20equivariant%20to%20the%20permutation%20of%20input%20tokens%2C%20encoding%0Athe%20positional%20information%20of%20tokens%20is%20necessary%20for%20many%20tasks.%20However%2C%0Asince%20existing%20positional%20encoding%20schemes%20have%20been%20initially%20designed%20for%20NLP%0Atasks%2C%20their%20suitability%20for%20vision%20tasks%2C%20which%20typically%20exhibit%20different%0Astructural%20properties%20in%20their%20data%2C%20is%20questionable.%20We%20argue%20that%20existing%0Apositional%20encoding%20schemes%20are%20suboptimal%20for%203D%20vision%20tasks%2C%20as%20they%20do%20not%0Arespect%20their%20underlying%203D%20geometric%20structure.%20Based%20on%20this%20hypothesis%2C%20we%0Apropose%20a%20geometry-aware%20attention%20mechanism%20that%20encodes%20the%20geometric%0Astructure%20of%20tokens%20as%20relative%20transformation%20determined%20by%20the%20geometric%0Arelationship%20between%20queries%20and%20key-value%20pairs.%20By%20evaluating%20on%20multiple%0Anovel%20view%20synthesis%20%28NVS%29%20datasets%20in%20the%20sparse%20wide-baseline%20multi-view%0Asetting%2C%20we%20show%20that%20our%20attention%2C%20called%20Geometric%20Transform%20Attention%0A%28GTA%29%2C%20improves%20learning%20efficiency%20and%20performance%20of%20state-of-the-art%0Atransformer-based%20NVS%20models%20without%20any%20additional%20learned%20parameters%20and%20only%0Aminor%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10375v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTA%253A%2520A%2520Geometry-Aware%2520Attention%2520Mechanism%2520for%2520Multi-View%2520Transformers%26entry.906535625%3DTakeru%2520Miyato%2520and%2520Bernhard%2520Jaeger%2520and%2520Max%2520Welling%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520As%2520transformers%2520are%2520equivariant%2520to%2520the%2520permutation%2520of%2520input%2520tokens%252C%2520encoding%250Athe%2520positional%2520information%2520of%2520tokens%2520is%2520necessary%2520for%2520many%2520tasks.%2520However%252C%250Asince%2520existing%2520positional%2520encoding%2520schemes%2520have%2520been%2520initially%2520designed%2520for%2520NLP%250Atasks%252C%2520their%2520suitability%2520for%2520vision%2520tasks%252C%2520which%2520typically%2520exhibit%2520different%250Astructural%2520properties%2520in%2520their%2520data%252C%2520is%2520questionable.%2520We%2520argue%2520that%2520existing%250Apositional%2520encoding%2520schemes%2520are%2520suboptimal%2520for%25203D%2520vision%2520tasks%252C%2520as%2520they%2520do%2520not%250Arespect%2520their%2520underlying%25203D%2520geometric%2520structure.%2520Based%2520on%2520this%2520hypothesis%252C%2520we%250Apropose%2520a%2520geometry-aware%2520attention%2520mechanism%2520that%2520encodes%2520the%2520geometric%250Astructure%2520of%2520tokens%2520as%2520relative%2520transformation%2520determined%2520by%2520the%2520geometric%250Arelationship%2520between%2520queries%2520and%2520key-value%2520pairs.%2520By%2520evaluating%2520on%2520multiple%250Anovel%2520view%2520synthesis%2520%2528NVS%2529%2520datasets%2520in%2520the%2520sparse%2520wide-baseline%2520multi-view%250Asetting%252C%2520we%2520show%2520that%2520our%2520attention%252C%2520called%2520Geometric%2520Transform%2520Attention%250A%2528GTA%2529%252C%2520improves%2520learning%2520efficiency%2520and%2520performance%2520of%2520state-of-the-art%250Atransformer-based%2520NVS%2520models%2520without%2520any%2520additional%2520learned%2520parameters%2520and%2520only%250Aminor%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10375v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTA%3A%20A%20Geometry-Aware%20Attention%20Mechanism%20for%20Multi-View%20Transformers&entry.906535625=Takeru%20Miyato%20and%20Bernhard%20Jaeger%20and%20Max%20Welling%20and%20Andreas%20Geiger&entry.1292438233=%20%20As%20transformers%20are%20equivariant%20to%20the%20permutation%20of%20input%20tokens%2C%20encoding%0Athe%20positional%20information%20of%20tokens%20is%20necessary%20for%20many%20tasks.%20However%2C%0Asince%20existing%20positional%20encoding%20schemes%20have%20been%20initially%20designed%20for%20NLP%0Atasks%2C%20their%20suitability%20for%20vision%20tasks%2C%20which%20typically%20exhibit%20different%0Astructural%20properties%20in%20their%20data%2C%20is%20questionable.%20We%20argue%20that%20existing%0Apositional%20encoding%20schemes%20are%20suboptimal%20for%203D%20vision%20tasks%2C%20as%20they%20do%20not%0Arespect%20their%20underlying%203D%20geometric%20structure.%20Based%20on%20this%20hypothesis%2C%20we%0Apropose%20a%20geometry-aware%20attention%20mechanism%20that%20encodes%20the%20geometric%0Astructure%20of%20tokens%20as%20relative%20transformation%20determined%20by%20the%20geometric%0Arelationship%20between%20queries%20and%20key-value%20pairs.%20By%20evaluating%20on%20multiple%0Anovel%20view%20synthesis%20%28NVS%29%20datasets%20in%20the%20sparse%20wide-baseline%20multi-view%0Asetting%2C%20we%20show%20that%20our%20attention%2C%20called%20Geometric%20Transform%20Attention%0A%28GTA%29%2C%20improves%20learning%20efficiency%20and%20performance%20of%20state-of-the-art%0Atransformer-based%20NVS%20models%20without%20any%20additional%20learned%20parameters%20and%20only%0Aminor%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10375v3&entry.124074799=Read"},
{"title": "Joint Spatial-Temporal Modeling and Contrastive Learning for\n  Self-supervised Heart Rate Measurement", "author": "Wei Qian and Qi Li and Kun Li and Xinke Wang and Xiao Sun and Meng Wang and Dan Guo", "abstract": "  This paper briefly introduces the solutions developed by our team, HFUT-VUT,\nfor Track 1 of self-supervised heart rate measurement in the 3rd Vision-based\nRemote Physiological Signal Sensing (RePSS) Challenge hosted at IJCAI 2024. The\ngoal is to develop a self-supervised learning algorithm for heart rate (HR)\nestimation using unlabeled facial videos. To tackle this task, we present two\nself-supervised HR estimation solutions that integrate spatial-temporal\nmodeling and contrastive learning, respectively. Specifically, we first propose\na non-end-to-end self-supervised HR measurement framework based on\nspatial-temporal modeling, which can effectively capture subtle rPPG clues and\nleverage the inherent bandwidth and periodicity characteristics of rPPG to\nconstrain the model. Meanwhile, we employ an excellent end-to-end solution\nbased on contrastive learning, aiming to generalize across different scenarios\nfrom complementary perspectives. Finally, we combine the strengths of the above\nsolutions through an ensemble strategy to generate the final predictions,\nleading to a more accurate HR estimation. As a result, our solutions achieved a\nremarkable RMSE score of 8.85277 on the test dataset, securing \\textbf{2nd\nplace} in Track 1 of the challenge.\n", "link": "http://arxiv.org/abs/2406.04942v1", "date": "2024-06-07", "relevancy": 2.6688, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5734}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5206}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Spatial-Temporal%20Modeling%20and%20Contrastive%20Learning%20for%0A%20%20Self-supervised%20Heart%20Rate%20Measurement&body=Title%3A%20Joint%20Spatial-Temporal%20Modeling%20and%20Contrastive%20Learning%20for%0A%20%20Self-supervised%20Heart%20Rate%20Measurement%0AAuthor%3A%20Wei%20Qian%20and%20Qi%20Li%20and%20Kun%20Li%20and%20Xinke%20Wang%20and%20Xiao%20Sun%20and%20Meng%20Wang%20and%20Dan%20Guo%0AAbstract%3A%20%20%20This%20paper%20briefly%20introduces%20the%20solutions%20developed%20by%20our%20team%2C%20HFUT-VUT%2C%0Afor%20Track%201%20of%20self-supervised%20heart%20rate%20measurement%20in%20the%203rd%20Vision-based%0ARemote%20Physiological%20Signal%20Sensing%20%28RePSS%29%20Challenge%20hosted%20at%20IJCAI%202024.%20The%0Agoal%20is%20to%20develop%20a%20self-supervised%20learning%20algorithm%20for%20heart%20rate%20%28HR%29%0Aestimation%20using%20unlabeled%20facial%20videos.%20To%20tackle%20this%20task%2C%20we%20present%20two%0Aself-supervised%20HR%20estimation%20solutions%20that%20integrate%20spatial-temporal%0Amodeling%20and%20contrastive%20learning%2C%20respectively.%20Specifically%2C%20we%20first%20propose%0Aa%20non-end-to-end%20self-supervised%20HR%20measurement%20framework%20based%20on%0Aspatial-temporal%20modeling%2C%20which%20can%20effectively%20capture%20subtle%20rPPG%20clues%20and%0Aleverage%20the%20inherent%20bandwidth%20and%20periodicity%20characteristics%20of%20rPPG%20to%0Aconstrain%20the%20model.%20Meanwhile%2C%20we%20employ%20an%20excellent%20end-to-end%20solution%0Abased%20on%20contrastive%20learning%2C%20aiming%20to%20generalize%20across%20different%20scenarios%0Afrom%20complementary%20perspectives.%20Finally%2C%20we%20combine%20the%20strengths%20of%20the%20above%0Asolutions%20through%20an%20ensemble%20strategy%20to%20generate%20the%20final%20predictions%2C%0Aleading%20to%20a%20more%20accurate%20HR%20estimation.%20As%20a%20result%2C%20our%20solutions%20achieved%20a%0Aremarkable%20RMSE%20score%20of%208.85277%20on%20the%20test%20dataset%2C%20securing%20%5Ctextbf%7B2nd%0Aplace%7D%20in%20Track%201%20of%20the%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Spatial-Temporal%2520Modeling%2520and%2520Contrastive%2520Learning%2520for%250A%2520%2520Self-supervised%2520Heart%2520Rate%2520Measurement%26entry.906535625%3DWei%2520Qian%2520and%2520Qi%2520Li%2520and%2520Kun%2520Li%2520and%2520Xinke%2520Wang%2520and%2520Xiao%2520Sun%2520and%2520Meng%2520Wang%2520and%2520Dan%2520Guo%26entry.1292438233%3D%2520%2520This%2520paper%2520briefly%2520introduces%2520the%2520solutions%2520developed%2520by%2520our%2520team%252C%2520HFUT-VUT%252C%250Afor%2520Track%25201%2520of%2520self-supervised%2520heart%2520rate%2520measurement%2520in%2520the%25203rd%2520Vision-based%250ARemote%2520Physiological%2520Signal%2520Sensing%2520%2528RePSS%2529%2520Challenge%2520hosted%2520at%2520IJCAI%25202024.%2520The%250Agoal%2520is%2520to%2520develop%2520a%2520self-supervised%2520learning%2520algorithm%2520for%2520heart%2520rate%2520%2528HR%2529%250Aestimation%2520using%2520unlabeled%2520facial%2520videos.%2520To%2520tackle%2520this%2520task%252C%2520we%2520present%2520two%250Aself-supervised%2520HR%2520estimation%2520solutions%2520that%2520integrate%2520spatial-temporal%250Amodeling%2520and%2520contrastive%2520learning%252C%2520respectively.%2520Specifically%252C%2520we%2520first%2520propose%250Aa%2520non-end-to-end%2520self-supervised%2520HR%2520measurement%2520framework%2520based%2520on%250Aspatial-temporal%2520modeling%252C%2520which%2520can%2520effectively%2520capture%2520subtle%2520rPPG%2520clues%2520and%250Aleverage%2520the%2520inherent%2520bandwidth%2520and%2520periodicity%2520characteristics%2520of%2520rPPG%2520to%250Aconstrain%2520the%2520model.%2520Meanwhile%252C%2520we%2520employ%2520an%2520excellent%2520end-to-end%2520solution%250Abased%2520on%2520contrastive%2520learning%252C%2520aiming%2520to%2520generalize%2520across%2520different%2520scenarios%250Afrom%2520complementary%2520perspectives.%2520Finally%252C%2520we%2520combine%2520the%2520strengths%2520of%2520the%2520above%250Asolutions%2520through%2520an%2520ensemble%2520strategy%2520to%2520generate%2520the%2520final%2520predictions%252C%250Aleading%2520to%2520a%2520more%2520accurate%2520HR%2520estimation.%2520As%2520a%2520result%252C%2520our%2520solutions%2520achieved%2520a%250Aremarkable%2520RMSE%2520score%2520of%25208.85277%2520on%2520the%2520test%2520dataset%252C%2520securing%2520%255Ctextbf%257B2nd%250Aplace%257D%2520in%2520Track%25201%2520of%2520the%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Spatial-Temporal%20Modeling%20and%20Contrastive%20Learning%20for%0A%20%20Self-supervised%20Heart%20Rate%20Measurement&entry.906535625=Wei%20Qian%20and%20Qi%20Li%20and%20Kun%20Li%20and%20Xinke%20Wang%20and%20Xiao%20Sun%20and%20Meng%20Wang%20and%20Dan%20Guo&entry.1292438233=%20%20This%20paper%20briefly%20introduces%20the%20solutions%20developed%20by%20our%20team%2C%20HFUT-VUT%2C%0Afor%20Track%201%20of%20self-supervised%20heart%20rate%20measurement%20in%20the%203rd%20Vision-based%0ARemote%20Physiological%20Signal%20Sensing%20%28RePSS%29%20Challenge%20hosted%20at%20IJCAI%202024.%20The%0Agoal%20is%20to%20develop%20a%20self-supervised%20learning%20algorithm%20for%20heart%20rate%20%28HR%29%0Aestimation%20using%20unlabeled%20facial%20videos.%20To%20tackle%20this%20task%2C%20we%20present%20two%0Aself-supervised%20HR%20estimation%20solutions%20that%20integrate%20spatial-temporal%0Amodeling%20and%20contrastive%20learning%2C%20respectively.%20Specifically%2C%20we%20first%20propose%0Aa%20non-end-to-end%20self-supervised%20HR%20measurement%20framework%20based%20on%0Aspatial-temporal%20modeling%2C%20which%20can%20effectively%20capture%20subtle%20rPPG%20clues%20and%0Aleverage%20the%20inherent%20bandwidth%20and%20periodicity%20characteristics%20of%20rPPG%20to%0Aconstrain%20the%20model.%20Meanwhile%2C%20we%20employ%20an%20excellent%20end-to-end%20solution%0Abased%20on%20contrastive%20learning%2C%20aiming%20to%20generalize%20across%20different%20scenarios%0Afrom%20complementary%20perspectives.%20Finally%2C%20we%20combine%20the%20strengths%20of%20the%20above%0Asolutions%20through%20an%20ensemble%20strategy%20to%20generate%20the%20final%20predictions%2C%0Aleading%20to%20a%20more%20accurate%20HR%20estimation.%20As%20a%20result%2C%20our%20solutions%20achieved%20a%0Aremarkable%20RMSE%20score%20of%208.85277%20on%20the%20test%20dataset%2C%20securing%20%5Ctextbf%7B2nd%0Aplace%7D%20in%20Track%201%20of%20the%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04942v1&entry.124074799=Read"},
{"title": "Image Coding for Machines with Edge Information Learning Using Segment\n  Anything", "author": "Takahiro Shindo and Kein Yamada and Taiju Watanabe and Hiroshi Watanabe", "abstract": "  Image Coding for Machines (ICM) is an image compression technique for image\nrecognition.\n  This technique is essential due to the growing demand for image recognition\nAI.\n  In this paper, we propose a method for ICM that focuses on encoding and\ndecoding only the edge information of object parts in an image, which we call\nSA-ICM.\n  This is an Learned Image Compression (LIC) model trained using edge\ninformation created by Segment Anything.\n  Our method can be used for image recognition models with various tasks.\n  SA-ICM is also robust to changes in input data, making it effective for a\nvariety of use cases.\n  Additionally, our method provides benefits from a privacy point of view, as\nit removes human facial information on the encoder's side, thus protecting\none's privacy.\n  Furthermore, this LIC model training method can be used to train Neural\nRepresentations for Videos (NeRV), which is a video compression model.\n  By training NeRV using edge information created by Segment Anything, it is\npossible to create a NeRV that is effective for image recognition (SA-NeRV).\n  Experimental results confirm the advantages of SA-ICM, presenting the best\nperformance in image compression for image recognition.\n  We also show that SA-NeRV is superior to ordinary NeRV in video compression\nfor machines.\n  Code is available at https://github.com/final-0/SA-ICM.\n", "link": "http://arxiv.org/abs/2403.04173v3", "date": "2024-06-07", "relevancy": 2.6533, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5476}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5378}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything&body=Title%3A%20Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything%0AAuthor%3A%20Takahiro%20Shindo%20and%20Kein%20Yamada%20and%20Taiju%20Watanabe%20and%20Hiroshi%20Watanabe%0AAbstract%3A%20%20%20Image%20Coding%20for%20Machines%20%28ICM%29%20is%20an%20image%20compression%20technique%20for%20image%0Arecognition.%0A%20%20This%20technique%20is%20essential%20due%20to%20the%20growing%20demand%20for%20image%20recognition%0AAI.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20ICM%20that%20focuses%20on%20encoding%20and%0Adecoding%20only%20the%20edge%20information%20of%20object%20parts%20in%20an%20image%2C%20which%20we%20call%0ASA-ICM.%0A%20%20This%20is%20an%20Learned%20Image%20Compression%20%28LIC%29%20model%20trained%20using%20edge%0Ainformation%20created%20by%20Segment%20Anything.%0A%20%20Our%20method%20can%20be%20used%20for%20image%20recognition%20models%20with%20various%20tasks.%0A%20%20SA-ICM%20is%20also%20robust%20to%20changes%20in%20input%20data%2C%20making%20it%20effective%20for%20a%0Avariety%20of%20use%20cases.%0A%20%20Additionally%2C%20our%20method%20provides%20benefits%20from%20a%20privacy%20point%20of%20view%2C%20as%0Ait%20removes%20human%20facial%20information%20on%20the%20encoder%27s%20side%2C%20thus%20protecting%0Aone%27s%20privacy.%0A%20%20Furthermore%2C%20this%20LIC%20model%20training%20method%20can%20be%20used%20to%20train%20Neural%0ARepresentations%20for%20Videos%20%28NeRV%29%2C%20which%20is%20a%20video%20compression%20model.%0A%20%20By%20training%20NeRV%20using%20edge%20information%20created%20by%20Segment%20Anything%2C%20it%20is%0Apossible%20to%20create%20a%20NeRV%20that%20is%20effective%20for%20image%20recognition%20%28SA-NeRV%29.%0A%20%20Experimental%20results%20confirm%20the%20advantages%20of%20SA-ICM%2C%20presenting%20the%20best%0Aperformance%20in%20image%20compression%20for%20image%20recognition.%0A%20%20We%20also%20show%20that%20SA-NeRV%20is%20superior%20to%20ordinary%20NeRV%20in%20video%20compression%0Afor%20machines.%0A%20%20Code%20is%20available%20at%20https%3A//github.com/final-0/SA-ICM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04173v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Coding%2520for%2520Machines%2520with%2520Edge%2520Information%2520Learning%2520Using%2520Segment%250A%2520%2520Anything%26entry.906535625%3DTakahiro%2520Shindo%2520and%2520Kein%2520Yamada%2520and%2520Taiju%2520Watanabe%2520and%2520Hiroshi%2520Watanabe%26entry.1292438233%3D%2520%2520Image%2520Coding%2520for%2520Machines%2520%2528ICM%2529%2520is%2520an%2520image%2520compression%2520technique%2520for%2520image%250Arecognition.%250A%2520%2520This%2520technique%2520is%2520essential%2520due%2520to%2520the%2520growing%2520demand%2520for%2520image%2520recognition%250AAI.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520ICM%2520that%2520focuses%2520on%2520encoding%2520and%250Adecoding%2520only%2520the%2520edge%2520information%2520of%2520object%2520parts%2520in%2520an%2520image%252C%2520which%2520we%2520call%250ASA-ICM.%250A%2520%2520This%2520is%2520an%2520Learned%2520Image%2520Compression%2520%2528LIC%2529%2520model%2520trained%2520using%2520edge%250Ainformation%2520created%2520by%2520Segment%2520Anything.%250A%2520%2520Our%2520method%2520can%2520be%2520used%2520for%2520image%2520recognition%2520models%2520with%2520various%2520tasks.%250A%2520%2520SA-ICM%2520is%2520also%2520robust%2520to%2520changes%2520in%2520input%2520data%252C%2520making%2520it%2520effective%2520for%2520a%250Avariety%2520of%2520use%2520cases.%250A%2520%2520Additionally%252C%2520our%2520method%2520provides%2520benefits%2520from%2520a%2520privacy%2520point%2520of%2520view%252C%2520as%250Ait%2520removes%2520human%2520facial%2520information%2520on%2520the%2520encoder%2527s%2520side%252C%2520thus%2520protecting%250Aone%2527s%2520privacy.%250A%2520%2520Furthermore%252C%2520this%2520LIC%2520model%2520training%2520method%2520can%2520be%2520used%2520to%2520train%2520Neural%250ARepresentations%2520for%2520Videos%2520%2528NeRV%2529%252C%2520which%2520is%2520a%2520video%2520compression%2520model.%250A%2520%2520By%2520training%2520NeRV%2520using%2520edge%2520information%2520created%2520by%2520Segment%2520Anything%252C%2520it%2520is%250Apossible%2520to%2520create%2520a%2520NeRV%2520that%2520is%2520effective%2520for%2520image%2520recognition%2520%2528SA-NeRV%2529.%250A%2520%2520Experimental%2520results%2520confirm%2520the%2520advantages%2520of%2520SA-ICM%252C%2520presenting%2520the%2520best%250Aperformance%2520in%2520image%2520compression%2520for%2520image%2520recognition.%250A%2520%2520We%2520also%2520show%2520that%2520SA-NeRV%2520is%2520superior%2520to%2520ordinary%2520NeRV%2520in%2520video%2520compression%250Afor%2520machines.%250A%2520%2520Code%2520is%2520available%2520at%2520https%253A//github.com/final-0/SA-ICM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04173v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Coding%20for%20Machines%20with%20Edge%20Information%20Learning%20Using%20Segment%0A%20%20Anything&entry.906535625=Takahiro%20Shindo%20and%20Kein%20Yamada%20and%20Taiju%20Watanabe%20and%20Hiroshi%20Watanabe&entry.1292438233=%20%20Image%20Coding%20for%20Machines%20%28ICM%29%20is%20an%20image%20compression%20technique%20for%20image%0Arecognition.%0A%20%20This%20technique%20is%20essential%20due%20to%20the%20growing%20demand%20for%20image%20recognition%0AAI.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20ICM%20that%20focuses%20on%20encoding%20and%0Adecoding%20only%20the%20edge%20information%20of%20object%20parts%20in%20an%20image%2C%20which%20we%20call%0ASA-ICM.%0A%20%20This%20is%20an%20Learned%20Image%20Compression%20%28LIC%29%20model%20trained%20using%20edge%0Ainformation%20created%20by%20Segment%20Anything.%0A%20%20Our%20method%20can%20be%20used%20for%20image%20recognition%20models%20with%20various%20tasks.%0A%20%20SA-ICM%20is%20also%20robust%20to%20changes%20in%20input%20data%2C%20making%20it%20effective%20for%20a%0Avariety%20of%20use%20cases.%0A%20%20Additionally%2C%20our%20method%20provides%20benefits%20from%20a%20privacy%20point%20of%20view%2C%20as%0Ait%20removes%20human%20facial%20information%20on%20the%20encoder%27s%20side%2C%20thus%20protecting%0Aone%27s%20privacy.%0A%20%20Furthermore%2C%20this%20LIC%20model%20training%20method%20can%20be%20used%20to%20train%20Neural%0ARepresentations%20for%20Videos%20%28NeRV%29%2C%20which%20is%20a%20video%20compression%20model.%0A%20%20By%20training%20NeRV%20using%20edge%20information%20created%20by%20Segment%20Anything%2C%20it%20is%0Apossible%20to%20create%20a%20NeRV%20that%20is%20effective%20for%20image%20recognition%20%28SA-NeRV%29.%0A%20%20Experimental%20results%20confirm%20the%20advantages%20of%20SA-ICM%2C%20presenting%20the%20best%0Aperformance%20in%20image%20compression%20for%20image%20recognition.%0A%20%20We%20also%20show%20that%20SA-NeRV%20is%20superior%20to%20ordinary%20NeRV%20in%20video%20compression%0Afor%20machines.%0A%20%20Code%20is%20available%20at%20https%3A//github.com/final-0/SA-ICM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04173v3&entry.124074799=Read"},
{"title": "Multiplane Prior Guided Few-Shot Aerial Scene Rendering", "author": "Zihan Gao and Licheng Jiao and Lingling Li and Xu Liu and Fang Liu and Puhua Chen and Yuwei Guo", "abstract": "  Neural Radiance Fields (NeRF) have been successfully applied in various\naerial scenes, yet they face challenges with sparse views due to limited\nsupervision. The acquisition of dense aerial views is often prohibitive, as\nunmanned aerial vehicles (UAVs) may encounter constraints in perspective range\nand energy constraints. In this work, we introduce Multiplane Prior guided NeRF\n(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking\na pioneering effort in this domain. Our key insight is that the intrinsic\ngeometric regularities specific to aerial imagery could be leveraged to enhance\nNeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image\n(MPI)'s behavior, we propose to guide the training process of NeRF with a\nMultiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and\nincorporates advanced image comprehension through a SwinV2 Transformer,\npre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF\noutperforms existing state-of-the-art methods applied in non-aerial contexts,\nby tripling the performance in SSIM and LPIPS even with three views available.\nWe hope our work offers insights into the development of NeRF-based\napplications in aerial scenes with limited data.\n", "link": "http://arxiv.org/abs/2406.04961v1", "date": "2024-06-07", "relevancy": 2.6458, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiplane%20Prior%20Guided%20Few-Shot%20Aerial%20Scene%20Rendering&body=Title%3A%20Multiplane%20Prior%20Guided%20Few-Shot%20Aerial%20Scene%20Rendering%0AAuthor%3A%20Zihan%20Gao%20and%20Licheng%20Jiao%20and%20Lingling%20Li%20and%20Xu%20Liu%20and%20Fang%20Liu%20and%20Puhua%20Chen%20and%20Yuwei%20Guo%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20been%20successfully%20applied%20in%20various%0Aaerial%20scenes%2C%20yet%20they%20face%20challenges%20with%20sparse%20views%20due%20to%20limited%0Asupervision.%20The%20acquisition%20of%20dense%20aerial%20views%20is%20often%20prohibitive%2C%20as%0Aunmanned%20aerial%20vehicles%20%28UAVs%29%20may%20encounter%20constraints%20in%20perspective%20range%0Aand%20energy%20constraints.%20In%20this%20work%2C%20we%20introduce%20Multiplane%20Prior%20guided%20NeRF%0A%28MPNeRF%29%2C%20a%20novel%20approach%20tailored%20for%20few-shot%20aerial%20scene%20rendering-marking%0Aa%20pioneering%20effort%20in%20this%20domain.%20Our%20key%20insight%20is%20that%20the%20intrinsic%0Ageometric%20regularities%20specific%20to%20aerial%20imagery%20could%20be%20leveraged%20to%20enhance%0ANeRF%20in%20sparse%20aerial%20scenes.%20By%20investigating%20NeRF%27s%20and%20Multiplane%20Image%0A%28MPI%29%27s%20behavior%2C%20we%20propose%20to%20guide%20the%20training%20process%20of%20NeRF%20with%20a%0AMultiplane%20Prior.%20The%20proposed%20Multiplane%20Prior%20draws%20upon%20MPI%27s%20benefits%20and%0Aincorporates%20advanced%20image%20comprehension%20through%20a%20SwinV2%20Transformer%2C%0Apre-trained%20via%20SimMIM.%20Our%20extensive%20experiments%20demonstrate%20that%20MPNeRF%0Aoutperforms%20existing%20state-of-the-art%20methods%20applied%20in%20non-aerial%20contexts%2C%0Aby%20tripling%20the%20performance%20in%20SSIM%20and%20LPIPS%20even%20with%20three%20views%20available.%0AWe%20hope%20our%20work%20offers%20insights%20into%20the%20development%20of%20NeRF-based%0Aapplications%20in%20aerial%20scenes%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiplane%2520Prior%2520Guided%2520Few-Shot%2520Aerial%2520Scene%2520Rendering%26entry.906535625%3DZihan%2520Gao%2520and%2520Licheng%2520Jiao%2520and%2520Lingling%2520Li%2520and%2520Xu%2520Liu%2520and%2520Fang%2520Liu%2520and%2520Puhua%2520Chen%2520and%2520Yuwei%2520Guo%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520been%2520successfully%2520applied%2520in%2520various%250Aaerial%2520scenes%252C%2520yet%2520they%2520face%2520challenges%2520with%2520sparse%2520views%2520due%2520to%2520limited%250Asupervision.%2520The%2520acquisition%2520of%2520dense%2520aerial%2520views%2520is%2520often%2520prohibitive%252C%2520as%250Aunmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520may%2520encounter%2520constraints%2520in%2520perspective%2520range%250Aand%2520energy%2520constraints.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Multiplane%2520Prior%2520guided%2520NeRF%250A%2528MPNeRF%2529%252C%2520a%2520novel%2520approach%2520tailored%2520for%2520few-shot%2520aerial%2520scene%2520rendering-marking%250Aa%2520pioneering%2520effort%2520in%2520this%2520domain.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520intrinsic%250Ageometric%2520regularities%2520specific%2520to%2520aerial%2520imagery%2520could%2520be%2520leveraged%2520to%2520enhance%250ANeRF%2520in%2520sparse%2520aerial%2520scenes.%2520By%2520investigating%2520NeRF%2527s%2520and%2520Multiplane%2520Image%250A%2528MPI%2529%2527s%2520behavior%252C%2520we%2520propose%2520to%2520guide%2520the%2520training%2520process%2520of%2520NeRF%2520with%2520a%250AMultiplane%2520Prior.%2520The%2520proposed%2520Multiplane%2520Prior%2520draws%2520upon%2520MPI%2527s%2520benefits%2520and%250Aincorporates%2520advanced%2520image%2520comprehension%2520through%2520a%2520SwinV2%2520Transformer%252C%250Apre-trained%2520via%2520SimMIM.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520MPNeRF%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520applied%2520in%2520non-aerial%2520contexts%252C%250Aby%2520tripling%2520the%2520performance%2520in%2520SSIM%2520and%2520LPIPS%2520even%2520with%2520three%2520views%2520available.%250AWe%2520hope%2520our%2520work%2520offers%2520insights%2520into%2520the%2520development%2520of%2520NeRF-based%250Aapplications%2520in%2520aerial%2520scenes%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiplane%20Prior%20Guided%20Few-Shot%20Aerial%20Scene%20Rendering&entry.906535625=Zihan%20Gao%20and%20Licheng%20Jiao%20and%20Lingling%20Li%20and%20Xu%20Liu%20and%20Fang%20Liu%20and%20Puhua%20Chen%20and%20Yuwei%20Guo&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20been%20successfully%20applied%20in%20various%0Aaerial%20scenes%2C%20yet%20they%20face%20challenges%20with%20sparse%20views%20due%20to%20limited%0Asupervision.%20The%20acquisition%20of%20dense%20aerial%20views%20is%20often%20prohibitive%2C%20as%0Aunmanned%20aerial%20vehicles%20%28UAVs%29%20may%20encounter%20constraints%20in%20perspective%20range%0Aand%20energy%20constraints.%20In%20this%20work%2C%20we%20introduce%20Multiplane%20Prior%20guided%20NeRF%0A%28MPNeRF%29%2C%20a%20novel%20approach%20tailored%20for%20few-shot%20aerial%20scene%20rendering-marking%0Aa%20pioneering%20effort%20in%20this%20domain.%20Our%20key%20insight%20is%20that%20the%20intrinsic%0Ageometric%20regularities%20specific%20to%20aerial%20imagery%20could%20be%20leveraged%20to%20enhance%0ANeRF%20in%20sparse%20aerial%20scenes.%20By%20investigating%20NeRF%27s%20and%20Multiplane%20Image%0A%28MPI%29%27s%20behavior%2C%20we%20propose%20to%20guide%20the%20training%20process%20of%20NeRF%20with%20a%0AMultiplane%20Prior.%20The%20proposed%20Multiplane%20Prior%20draws%20upon%20MPI%27s%20benefits%20and%0Aincorporates%20advanced%20image%20comprehension%20through%20a%20SwinV2%20Transformer%2C%0Apre-trained%20via%20SimMIM.%20Our%20extensive%20experiments%20demonstrate%20that%20MPNeRF%0Aoutperforms%20existing%20state-of-the-art%20methods%20applied%20in%20non-aerial%20contexts%2C%0Aby%20tripling%20the%20performance%20in%20SSIM%20and%20LPIPS%20even%20with%20three%20views%20available.%0AWe%20hope%20our%20work%20offers%20insights%20into%20the%20development%20of%20NeRF-based%0Aapplications%20in%20aerial%20scenes%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04961v1&entry.124074799=Read"},
{"title": "CoNo: Consistency Noise Injection for Tuning-free Long Video Diffusion", "author": "Xingrui Wang and Xin Li and Zhibo Chen", "abstract": "  Tuning-free long video diffusion has been proposed to generate\nextended-duration videos with enriched content by reusing the knowledge from\npre-trained short video diffusion model without retraining. However, most works\noverlook the fine-grained long-term video consistency modeling, resulting in\nlimited scene consistency (i.e., unreasonable object or background\ntransitions), especially with multiple text inputs. To mitigate this, we\npropose the Consistency Noise Injection, dubbed CoNo, which introduces the\n\"look-back\" mechanism to enhance the fine-grained scene transition between\ndifferent video clips, and designs the long-term consistency regularization to\neliminate the content shifts when extending video contents through noise\nprediction. In particular, the \"look-back\" mechanism breaks the noise\nscheduling process into three essential parts, where one internal noise\nprediction part is injected into two video-extending parts, intending to\nachieve a fine-grained transition between two video clips. The long-term\nconsistency regularization focuses on explicitly minimizing the pixel-wise\ndistance between the predicted noises of the extended video clip and the\noriginal one, thereby preventing abrupt scene transitions. Extensive\nexperiments have shown the effectiveness of the above strategies by performing\nlong-video generation under both single- and multi-text prompt conditions. The\nproject has been available in https://wxrui182.github.io/CoNo.github.io/.\n", "link": "http://arxiv.org/abs/2406.05082v1", "date": "2024-06-07", "relevancy": 2.5794, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6847}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6352}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoNo%3A%20Consistency%20Noise%20Injection%20for%20Tuning-free%20Long%20Video%20Diffusion&body=Title%3A%20CoNo%3A%20Consistency%20Noise%20Injection%20for%20Tuning-free%20Long%20Video%20Diffusion%0AAuthor%3A%20Xingrui%20Wang%20and%20Xin%20Li%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20Tuning-free%20long%20video%20diffusion%20has%20been%20proposed%20to%20generate%0Aextended-duration%20videos%20with%20enriched%20content%20by%20reusing%20the%20knowledge%20from%0Apre-trained%20short%20video%20diffusion%20model%20without%20retraining.%20However%2C%20most%20works%0Aoverlook%20the%20fine-grained%20long-term%20video%20consistency%20modeling%2C%20resulting%20in%0Alimited%20scene%20consistency%20%28i.e.%2C%20unreasonable%20object%20or%20background%0Atransitions%29%2C%20especially%20with%20multiple%20text%20inputs.%20To%20mitigate%20this%2C%20we%0Apropose%20the%20Consistency%20Noise%20Injection%2C%20dubbed%20CoNo%2C%20which%20introduces%20the%0A%22look-back%22%20mechanism%20to%20enhance%20the%20fine-grained%20scene%20transition%20between%0Adifferent%20video%20clips%2C%20and%20designs%20the%20long-term%20consistency%20regularization%20to%0Aeliminate%20the%20content%20shifts%20when%20extending%20video%20contents%20through%20noise%0Aprediction.%20In%20particular%2C%20the%20%22look-back%22%20mechanism%20breaks%20the%20noise%0Ascheduling%20process%20into%20three%20essential%20parts%2C%20where%20one%20internal%20noise%0Aprediction%20part%20is%20injected%20into%20two%20video-extending%20parts%2C%20intending%20to%0Aachieve%20a%20fine-grained%20transition%20between%20two%20video%20clips.%20The%20long-term%0Aconsistency%20regularization%20focuses%20on%20explicitly%20minimizing%20the%20pixel-wise%0Adistance%20between%20the%20predicted%20noises%20of%20the%20extended%20video%20clip%20and%20the%0Aoriginal%20one%2C%20thereby%20preventing%20abrupt%20scene%20transitions.%20Extensive%0Aexperiments%20have%20shown%20the%20effectiveness%20of%20the%20above%20strategies%20by%20performing%0Along-video%20generation%20under%20both%20single-%20and%20multi-text%20prompt%20conditions.%20The%0Aproject%20has%20been%20available%20in%20https%3A//wxrui182.github.io/CoNo.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoNo%253A%2520Consistency%2520Noise%2520Injection%2520for%2520Tuning-free%2520Long%2520Video%2520Diffusion%26entry.906535625%3DXingrui%2520Wang%2520and%2520Xin%2520Li%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520Tuning-free%2520long%2520video%2520diffusion%2520has%2520been%2520proposed%2520to%2520generate%250Aextended-duration%2520videos%2520with%2520enriched%2520content%2520by%2520reusing%2520the%2520knowledge%2520from%250Apre-trained%2520short%2520video%2520diffusion%2520model%2520without%2520retraining.%2520However%252C%2520most%2520works%250Aoverlook%2520the%2520fine-grained%2520long-term%2520video%2520consistency%2520modeling%252C%2520resulting%2520in%250Alimited%2520scene%2520consistency%2520%2528i.e.%252C%2520unreasonable%2520object%2520or%2520background%250Atransitions%2529%252C%2520especially%2520with%2520multiple%2520text%2520inputs.%2520To%2520mitigate%2520this%252C%2520we%250Apropose%2520the%2520Consistency%2520Noise%2520Injection%252C%2520dubbed%2520CoNo%252C%2520which%2520introduces%2520the%250A%2522look-back%2522%2520mechanism%2520to%2520enhance%2520the%2520fine-grained%2520scene%2520transition%2520between%250Adifferent%2520video%2520clips%252C%2520and%2520designs%2520the%2520long-term%2520consistency%2520regularization%2520to%250Aeliminate%2520the%2520content%2520shifts%2520when%2520extending%2520video%2520contents%2520through%2520noise%250Aprediction.%2520In%2520particular%252C%2520the%2520%2522look-back%2522%2520mechanism%2520breaks%2520the%2520noise%250Ascheduling%2520process%2520into%2520three%2520essential%2520parts%252C%2520where%2520one%2520internal%2520noise%250Aprediction%2520part%2520is%2520injected%2520into%2520two%2520video-extending%2520parts%252C%2520intending%2520to%250Aachieve%2520a%2520fine-grained%2520transition%2520between%2520two%2520video%2520clips.%2520The%2520long-term%250Aconsistency%2520regularization%2520focuses%2520on%2520explicitly%2520minimizing%2520the%2520pixel-wise%250Adistance%2520between%2520the%2520predicted%2520noises%2520of%2520the%2520extended%2520video%2520clip%2520and%2520the%250Aoriginal%2520one%252C%2520thereby%2520preventing%2520abrupt%2520scene%2520transitions.%2520Extensive%250Aexperiments%2520have%2520shown%2520the%2520effectiveness%2520of%2520the%2520above%2520strategies%2520by%2520performing%250Along-video%2520generation%2520under%2520both%2520single-%2520and%2520multi-text%2520prompt%2520conditions.%2520The%250Aproject%2520has%2520been%2520available%2520in%2520https%253A//wxrui182.github.io/CoNo.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoNo%3A%20Consistency%20Noise%20Injection%20for%20Tuning-free%20Long%20Video%20Diffusion&entry.906535625=Xingrui%20Wang%20and%20Xin%20Li%20and%20Zhibo%20Chen&entry.1292438233=%20%20Tuning-free%20long%20video%20diffusion%20has%20been%20proposed%20to%20generate%0Aextended-duration%20videos%20with%20enriched%20content%20by%20reusing%20the%20knowledge%20from%0Apre-trained%20short%20video%20diffusion%20model%20without%20retraining.%20However%2C%20most%20works%0Aoverlook%20the%20fine-grained%20long-term%20video%20consistency%20modeling%2C%20resulting%20in%0Alimited%20scene%20consistency%20%28i.e.%2C%20unreasonable%20object%20or%20background%0Atransitions%29%2C%20especially%20with%20multiple%20text%20inputs.%20To%20mitigate%20this%2C%20we%0Apropose%20the%20Consistency%20Noise%20Injection%2C%20dubbed%20CoNo%2C%20which%20introduces%20the%0A%22look-back%22%20mechanism%20to%20enhance%20the%20fine-grained%20scene%20transition%20between%0Adifferent%20video%20clips%2C%20and%20designs%20the%20long-term%20consistency%20regularization%20to%0Aeliminate%20the%20content%20shifts%20when%20extending%20video%20contents%20through%20noise%0Aprediction.%20In%20particular%2C%20the%20%22look-back%22%20mechanism%20breaks%20the%20noise%0Ascheduling%20process%20into%20three%20essential%20parts%2C%20where%20one%20internal%20noise%0Aprediction%20part%20is%20injected%20into%20two%20video-extending%20parts%2C%20intending%20to%0Aachieve%20a%20fine-grained%20transition%20between%20two%20video%20clips.%20The%20long-term%0Aconsistency%20regularization%20focuses%20on%20explicitly%20minimizing%20the%20pixel-wise%0Adistance%20between%20the%20predicted%20noises%20of%20the%20extended%20video%20clip%20and%20the%0Aoriginal%20one%2C%20thereby%20preventing%20abrupt%20scene%20transitions.%20Extensive%0Aexperiments%20have%20shown%20the%20effectiveness%20of%20the%20above%20strategies%20by%20performing%0Along-video%20generation%20under%20both%20single-%20and%20multi-text%20prompt%20conditions.%20The%0Aproject%20has%20been%20available%20in%20https%3A//wxrui182.github.io/CoNo.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05082v1&entry.124074799=Read"},
{"title": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation", "author": "Lianghan Zhu and Yanqi Bao and Jing Huo and Jing Wu and Yu-Kun Lai and Wenbin Li and Yang Gao", "abstract": "  The burgeoning field of text-based video generation (T2V) has reignited\nsignificant interest in the research of controllable video editing. Although\npre-trained T2V-based editing models have achieved efficient editing\ncapabilities, current works are still plagued by two major challenges. Firstly,\nthe inherent limitations of T2V models lead to content inconsistencies and\nmotion discontinuities between frames. Secondly, the notorious issue of\nover-editing significantly disrupts areas that are intended to remain\nunaltered. To address these challenges, our work aims to explore a robust\nvideo-based editing paradigm based on score distillation. Specifically, we\npropose an Adaptive Sliding Score Distillation strategy, which not only\nenhances the stability of T2V supervision but also incorporates both global and\nlocal video guidance to mitigate the impact of generation errors. Additionally,\nwe modify the self-attention layers during the editing process to further\npreserve the key features of the original video. Extensive experiments\ndemonstrate that these strategies enable us to effectively address the\naforementioned challenges, achieving superior editing performance compared to\nexisting state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.04888v1", "date": "2024-06-07", "relevancy": 2.5324, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6949}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6416}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation&body=Title%3A%20Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation%0AAuthor%3A%20Lianghan%20Zhu%20and%20Yanqi%20Bao%20and%20Jing%20Huo%20and%20Jing%20Wu%20and%20Yu-Kun%20Lai%20and%20Wenbin%20Li%20and%20Yang%20Gao%0AAbstract%3A%20%20%20The%20burgeoning%20field%20of%20text-based%20video%20generation%20%28T2V%29%20has%20reignited%0Asignificant%20interest%20in%20the%20research%20of%20controllable%20video%20editing.%20Although%0Apre-trained%20T2V-based%20editing%20models%20have%20achieved%20efficient%20editing%0Acapabilities%2C%20current%20works%20are%20still%20plagued%20by%20two%20major%20challenges.%20Firstly%2C%0Athe%20inherent%20limitations%20of%20T2V%20models%20lead%20to%20content%20inconsistencies%20and%0Amotion%20discontinuities%20between%20frames.%20Secondly%2C%20the%20notorious%20issue%20of%0Aover-editing%20significantly%20disrupts%20areas%20that%20are%20intended%20to%20remain%0Aunaltered.%20To%20address%20these%20challenges%2C%20our%20work%20aims%20to%20explore%20a%20robust%0Avideo-based%20editing%20paradigm%20based%20on%20score%20distillation.%20Specifically%2C%20we%0Apropose%20an%20Adaptive%20Sliding%20Score%20Distillation%20strategy%2C%20which%20not%20only%0Aenhances%20the%20stability%20of%20T2V%20supervision%20but%20also%20incorporates%20both%20global%20and%0Alocal%20video%20guidance%20to%20mitigate%20the%20impact%20of%20generation%20errors.%20Additionally%2C%0Awe%20modify%20the%20self-attention%20layers%20during%20the%20editing%20process%20to%20further%0Apreserve%20the%20key%20features%20of%20the%20original%20video.%20Extensive%20experiments%0Ademonstrate%20that%20these%20strategies%20enable%20us%20to%20effectively%20address%20the%0Aaforementioned%20challenges%2C%20achieving%20superior%20editing%20performance%20compared%20to%0Aexisting%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Video%2520Editing%2520through%2520Adaptive%2520Sliding%2520Score%2520Distillation%26entry.906535625%3DLianghan%2520Zhu%2520and%2520Yanqi%2520Bao%2520and%2520Jing%2520Huo%2520and%2520Jing%2520Wu%2520and%2520Yu-Kun%2520Lai%2520and%2520Wenbin%2520Li%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520field%2520of%2520text-based%2520video%2520generation%2520%2528T2V%2529%2520has%2520reignited%250Asignificant%2520interest%2520in%2520the%2520research%2520of%2520controllable%2520video%2520editing.%2520Although%250Apre-trained%2520T2V-based%2520editing%2520models%2520have%2520achieved%2520efficient%2520editing%250Acapabilities%252C%2520current%2520works%2520are%2520still%2520plagued%2520by%2520two%2520major%2520challenges.%2520Firstly%252C%250Athe%2520inherent%2520limitations%2520of%2520T2V%2520models%2520lead%2520to%2520content%2520inconsistencies%2520and%250Amotion%2520discontinuities%2520between%2520frames.%2520Secondly%252C%2520the%2520notorious%2520issue%2520of%250Aover-editing%2520significantly%2520disrupts%2520areas%2520that%2520are%2520intended%2520to%2520remain%250Aunaltered.%2520To%2520address%2520these%2520challenges%252C%2520our%2520work%2520aims%2520to%2520explore%2520a%2520robust%250Avideo-based%2520editing%2520paradigm%2520based%2520on%2520score%2520distillation.%2520Specifically%252C%2520we%250Apropose%2520an%2520Adaptive%2520Sliding%2520Score%2520Distillation%2520strategy%252C%2520which%2520not%2520only%250Aenhances%2520the%2520stability%2520of%2520T2V%2520supervision%2520but%2520also%2520incorporates%2520both%2520global%2520and%250Alocal%2520video%2520guidance%2520to%2520mitigate%2520the%2520impact%2520of%2520generation%2520errors.%2520Additionally%252C%250Awe%2520modify%2520the%2520self-attention%2520layers%2520during%2520the%2520editing%2520process%2520to%2520further%250Apreserve%2520the%2520key%2520features%2520of%2520the%2520original%2520video.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520these%2520strategies%2520enable%2520us%2520to%2520effectively%2520address%2520the%250Aaforementioned%2520challenges%252C%2520achieving%2520superior%2520editing%2520performance%2520compared%2520to%250Aexisting%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Video%20Editing%20through%20Adaptive%20Sliding%20Score%20Distillation&entry.906535625=Lianghan%20Zhu%20and%20Yanqi%20Bao%20and%20Jing%20Huo%20and%20Jing%20Wu%20and%20Yu-Kun%20Lai%20and%20Wenbin%20Li%20and%20Yang%20Gao&entry.1292438233=%20%20The%20burgeoning%20field%20of%20text-based%20video%20generation%20%28T2V%29%20has%20reignited%0Asignificant%20interest%20in%20the%20research%20of%20controllable%20video%20editing.%20Although%0Apre-trained%20T2V-based%20editing%20models%20have%20achieved%20efficient%20editing%0Acapabilities%2C%20current%20works%20are%20still%20plagued%20by%20two%20major%20challenges.%20Firstly%2C%0Athe%20inherent%20limitations%20of%20T2V%20models%20lead%20to%20content%20inconsistencies%20and%0Amotion%20discontinuities%20between%20frames.%20Secondly%2C%20the%20notorious%20issue%20of%0Aover-editing%20significantly%20disrupts%20areas%20that%20are%20intended%20to%20remain%0Aunaltered.%20To%20address%20these%20challenges%2C%20our%20work%20aims%20to%20explore%20a%20robust%0Avideo-based%20editing%20paradigm%20based%20on%20score%20distillation.%20Specifically%2C%20we%0Apropose%20an%20Adaptive%20Sliding%20Score%20Distillation%20strategy%2C%20which%20not%20only%0Aenhances%20the%20stability%20of%20T2V%20supervision%20but%20also%20incorporates%20both%20global%20and%0Alocal%20video%20guidance%20to%20mitigate%20the%20impact%20of%20generation%20errors.%20Additionally%2C%0Awe%20modify%20the%20self-attention%20layers%20during%20the%20editing%20process%20to%20further%0Apreserve%20the%20key%20features%20of%20the%20original%20video.%20Extensive%20experiments%0Ademonstrate%20that%20these%20strategies%20enable%20us%20to%20effectively%20address%20the%0Aaforementioned%20challenges%2C%20achieving%20superior%20editing%20performance%20compared%20to%0Aexisting%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04888v1&entry.124074799=Read"},
{"title": "GANetic Loss for Generative Adversarial Networks with a Focus on Medical\n  Applications", "author": "Shakhnaz Akhmedova and Nils K\u00f6rber", "abstract": "  Generative adversarial networks (GANs) are machine learning models that are\nused to estimate the underlying statistical structure of a given dataset and as\na result can be used for a variety of tasks such as image generation or anomaly\ndetection. Despite their initial simplicity, designing an effective loss\nfunction for training GANs remains challenging, and various loss functions have\nbeen proposed aiming to improve the performance and stability of the generative\nmodels. In this study, loss function design for GANs is presented as an\noptimization problem solved using the genetic programming (GP) approach.\nInitial experiments were carried out using small Deep Convolutional GAN (DCGAN)\nmodel and the MNIST dataset, in order to search experimentally for an improved\nloss function. The functions found were evaluated on CIFAR10, with the best\nfunction, named GANetic loss, showing exceptionally better performance and\nstability compared to the losses commonly used for GAN training. To further\nevalute its general applicability on more challenging problems, GANetic loss\nwas applied for two medical applications: image generation and anomaly\ndetection. Experiments were performed with histopathological, gastrointestinal\nor glaucoma images to evaluate the GANetic loss in medical image generation,\nresulting in improved image quality compared to the baseline models. The\nGANetic Loss used for polyp and glaucoma images showed a strong improvement in\nthe detection of anomalies. In summary, the GANetic loss function was evaluated\non multiple datasets and applications where it consistently outperforms\nalternative loss functions. Moreover, GANetic loss leads to stable training and\nreproducible results, a known weak spot of GANs.\n", "link": "http://arxiv.org/abs/2406.05023v1", "date": "2024-06-07", "relevancy": 2.513, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5122}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANetic%20Loss%20for%20Generative%20Adversarial%20Networks%20with%20a%20Focus%20on%20Medical%0A%20%20Applications&body=Title%3A%20GANetic%20Loss%20for%20Generative%20Adversarial%20Networks%20with%20a%20Focus%20on%20Medical%0A%20%20Applications%0AAuthor%3A%20Shakhnaz%20Akhmedova%20and%20Nils%20K%C3%B6rber%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20machine%20learning%20models%20that%20are%0Aused%20to%20estimate%20the%20underlying%20statistical%20structure%20of%20a%20given%20dataset%20and%20as%0Aa%20result%20can%20be%20used%20for%20a%20variety%20of%20tasks%20such%20as%20image%20generation%20or%20anomaly%0Adetection.%20Despite%20their%20initial%20simplicity%2C%20designing%20an%20effective%20loss%0Afunction%20for%20training%20GANs%20remains%20challenging%2C%20and%20various%20loss%20functions%20have%0Abeen%20proposed%20aiming%20to%20improve%20the%20performance%20and%20stability%20of%20the%20generative%0Amodels.%20In%20this%20study%2C%20loss%20function%20design%20for%20GANs%20is%20presented%20as%20an%0Aoptimization%20problem%20solved%20using%20the%20genetic%20programming%20%28GP%29%20approach.%0AInitial%20experiments%20were%20carried%20out%20using%20small%20Deep%20Convolutional%20GAN%20%28DCGAN%29%0Amodel%20and%20the%20MNIST%20dataset%2C%20in%20order%20to%20search%20experimentally%20for%20an%20improved%0Aloss%20function.%20The%20functions%20found%20were%20evaluated%20on%20CIFAR10%2C%20with%20the%20best%0Afunction%2C%20named%20GANetic%20loss%2C%20showing%20exceptionally%20better%20performance%20and%0Astability%20compared%20to%20the%20losses%20commonly%20used%20for%20GAN%20training.%20To%20further%0Aevalute%20its%20general%20applicability%20on%20more%20challenging%20problems%2C%20GANetic%20loss%0Awas%20applied%20for%20two%20medical%20applications%3A%20image%20generation%20and%20anomaly%0Adetection.%20Experiments%20were%20performed%20with%20histopathological%2C%20gastrointestinal%0Aor%20glaucoma%20images%20to%20evaluate%20the%20GANetic%20loss%20in%20medical%20image%20generation%2C%0Aresulting%20in%20improved%20image%20quality%20compared%20to%20the%20baseline%20models.%20The%0AGANetic%20Loss%20used%20for%20polyp%20and%20glaucoma%20images%20showed%20a%20strong%20improvement%20in%0Athe%20detection%20of%20anomalies.%20In%20summary%2C%20the%20GANetic%20loss%20function%20was%20evaluated%0Aon%20multiple%20datasets%20and%20applications%20where%20it%20consistently%20outperforms%0Aalternative%20loss%20functions.%20Moreover%2C%20GANetic%20loss%20leads%20to%20stable%20training%20and%0Areproducible%20results%2C%20a%20known%20weak%20spot%20of%20GANs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANetic%2520Loss%2520for%2520Generative%2520Adversarial%2520Networks%2520with%2520a%2520Focus%2520on%2520Medical%250A%2520%2520Applications%26entry.906535625%3DShakhnaz%2520Akhmedova%2520and%2520Nils%2520K%25C3%25B6rber%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520are%2520machine%2520learning%2520models%2520that%2520are%250Aused%2520to%2520estimate%2520the%2520underlying%2520statistical%2520structure%2520of%2520a%2520given%2520dataset%2520and%2520as%250Aa%2520result%2520can%2520be%2520used%2520for%2520a%2520variety%2520of%2520tasks%2520such%2520as%2520image%2520generation%2520or%2520anomaly%250Adetection.%2520Despite%2520their%2520initial%2520simplicity%252C%2520designing%2520an%2520effective%2520loss%250Afunction%2520for%2520training%2520GANs%2520remains%2520challenging%252C%2520and%2520various%2520loss%2520functions%2520have%250Abeen%2520proposed%2520aiming%2520to%2520improve%2520the%2520performance%2520and%2520stability%2520of%2520the%2520generative%250Amodels.%2520In%2520this%2520study%252C%2520loss%2520function%2520design%2520for%2520GANs%2520is%2520presented%2520as%2520an%250Aoptimization%2520problem%2520solved%2520using%2520the%2520genetic%2520programming%2520%2528GP%2529%2520approach.%250AInitial%2520experiments%2520were%2520carried%2520out%2520using%2520small%2520Deep%2520Convolutional%2520GAN%2520%2528DCGAN%2529%250Amodel%2520and%2520the%2520MNIST%2520dataset%252C%2520in%2520order%2520to%2520search%2520experimentally%2520for%2520an%2520improved%250Aloss%2520function.%2520The%2520functions%2520found%2520were%2520evaluated%2520on%2520CIFAR10%252C%2520with%2520the%2520best%250Afunction%252C%2520named%2520GANetic%2520loss%252C%2520showing%2520exceptionally%2520better%2520performance%2520and%250Astability%2520compared%2520to%2520the%2520losses%2520commonly%2520used%2520for%2520GAN%2520training.%2520To%2520further%250Aevalute%2520its%2520general%2520applicability%2520on%2520more%2520challenging%2520problems%252C%2520GANetic%2520loss%250Awas%2520applied%2520for%2520two%2520medical%2520applications%253A%2520image%2520generation%2520and%2520anomaly%250Adetection.%2520Experiments%2520were%2520performed%2520with%2520histopathological%252C%2520gastrointestinal%250Aor%2520glaucoma%2520images%2520to%2520evaluate%2520the%2520GANetic%2520loss%2520in%2520medical%2520image%2520generation%252C%250Aresulting%2520in%2520improved%2520image%2520quality%2520compared%2520to%2520the%2520baseline%2520models.%2520The%250AGANetic%2520Loss%2520used%2520for%2520polyp%2520and%2520glaucoma%2520images%2520showed%2520a%2520strong%2520improvement%2520in%250Athe%2520detection%2520of%2520anomalies.%2520In%2520summary%252C%2520the%2520GANetic%2520loss%2520function%2520was%2520evaluated%250Aon%2520multiple%2520datasets%2520and%2520applications%2520where%2520it%2520consistently%2520outperforms%250Aalternative%2520loss%2520functions.%2520Moreover%252C%2520GANetic%2520loss%2520leads%2520to%2520stable%2520training%2520and%250Areproducible%2520results%252C%2520a%2520known%2520weak%2520spot%2520of%2520GANs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANetic%20Loss%20for%20Generative%20Adversarial%20Networks%20with%20a%20Focus%20on%20Medical%0A%20%20Applications&entry.906535625=Shakhnaz%20Akhmedova%20and%20Nils%20K%C3%B6rber&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20machine%20learning%20models%20that%20are%0Aused%20to%20estimate%20the%20underlying%20statistical%20structure%20of%20a%20given%20dataset%20and%20as%0Aa%20result%20can%20be%20used%20for%20a%20variety%20of%20tasks%20such%20as%20image%20generation%20or%20anomaly%0Adetection.%20Despite%20their%20initial%20simplicity%2C%20designing%20an%20effective%20loss%0Afunction%20for%20training%20GANs%20remains%20challenging%2C%20and%20various%20loss%20functions%20have%0Abeen%20proposed%20aiming%20to%20improve%20the%20performance%20and%20stability%20of%20the%20generative%0Amodels.%20In%20this%20study%2C%20loss%20function%20design%20for%20GANs%20is%20presented%20as%20an%0Aoptimization%20problem%20solved%20using%20the%20genetic%20programming%20%28GP%29%20approach.%0AInitial%20experiments%20were%20carried%20out%20using%20small%20Deep%20Convolutional%20GAN%20%28DCGAN%29%0Amodel%20and%20the%20MNIST%20dataset%2C%20in%20order%20to%20search%20experimentally%20for%20an%20improved%0Aloss%20function.%20The%20functions%20found%20were%20evaluated%20on%20CIFAR10%2C%20with%20the%20best%0Afunction%2C%20named%20GANetic%20loss%2C%20showing%20exceptionally%20better%20performance%20and%0Astability%20compared%20to%20the%20losses%20commonly%20used%20for%20GAN%20training.%20To%20further%0Aevalute%20its%20general%20applicability%20on%20more%20challenging%20problems%2C%20GANetic%20loss%0Awas%20applied%20for%20two%20medical%20applications%3A%20image%20generation%20and%20anomaly%0Adetection.%20Experiments%20were%20performed%20with%20histopathological%2C%20gastrointestinal%0Aor%20glaucoma%20images%20to%20evaluate%20the%20GANetic%20loss%20in%20medical%20image%20generation%2C%0Aresulting%20in%20improved%20image%20quality%20compared%20to%20the%20baseline%20models.%20The%0AGANetic%20Loss%20used%20for%20polyp%20and%20glaucoma%20images%20showed%20a%20strong%20improvement%20in%0Athe%20detection%20of%20anomalies.%20In%20summary%2C%20the%20GANetic%20loss%20function%20was%20evaluated%0Aon%20multiple%20datasets%20and%20applications%20where%20it%20consistently%20outperforms%0Aalternative%20loss%20functions.%20Moreover%2C%20GANetic%20loss%20leads%20to%20stable%20training%20and%0Areproducible%20results%2C%20a%20known%20weak%20spot%20of%20GANs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05023v1&entry.124074799=Read"},
{"title": "GNNavi: Navigating the Information Flow in Large Language Models by\n  Graph Neural Network", "author": "Shuzhou Yuan and Ercong Nie and Michael F\u00e4rber and Helmut Schmid and Hinrich Sch\u00fctze", "abstract": "  Large Language Models (LLMs) exhibit strong In-Context Learning (ICL)\ncapabilities when prompts with demonstrations are used. However, fine-tuning\nstill remains crucial to further enhance their adaptability. Prompt-based\nfine-tuning proves to be an effective fine-tuning method in low-data scenarios,\nbut high demands on computing resources limit its practicality. We address this\nissue by introducing a prompt-based parameter-efficient fine-tuning (PEFT)\napproach. GNNavi leverages insights into ICL's information flow dynamics, which\nindicates that label words act in prompts as anchors for information\npropagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely\nguide the aggregation and distribution of information flow during the\nprocessing of prompts by hardwiring the desired information flow into the GNN.\nOur experiments on text classification tasks with GPT-2 and Llama2 show GNNavi\nsurpasses standard prompt-based fine-tuning methods in few-shot settings by\nupdating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT\napproaches, such as prefix tuning, LoRA and Adapter in terms of performance and\nefficiency. Our analysis reveals that GNNavi enhances information flow and\nensures a clear aggregation process.\n", "link": "http://arxiv.org/abs/2402.11709v2", "date": "2024-06-07", "relevancy": 2.5093, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5094}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNNavi%3A%20Navigating%20the%20Information%20Flow%20in%20Large%20Language%20Models%20by%0A%20%20Graph%20Neural%20Network&body=Title%3A%20GNNavi%3A%20Navigating%20the%20Information%20Flow%20in%20Large%20Language%20Models%20by%0A%20%20Graph%20Neural%20Network%0AAuthor%3A%20Shuzhou%20Yuan%20and%20Ercong%20Nie%20and%20Michael%20F%C3%A4rber%20and%20Helmut%20Schmid%20and%20Hinrich%20Sch%C3%BCtze%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20strong%20In-Context%20Learning%20%28ICL%29%0Acapabilities%20when%20prompts%20with%20demonstrations%20are%20used.%20However%2C%20fine-tuning%0Astill%20remains%20crucial%20to%20further%20enhance%20their%20adaptability.%20Prompt-based%0Afine-tuning%20proves%20to%20be%20an%20effective%20fine-tuning%20method%20in%20low-data%20scenarios%2C%0Abut%20high%20demands%20on%20computing%20resources%20limit%20its%20practicality.%20We%20address%20this%0Aissue%20by%20introducing%20a%20prompt-based%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Aapproach.%20GNNavi%20leverages%20insights%20into%20ICL%27s%20information%20flow%20dynamics%2C%20which%0Aindicates%20that%20label%20words%20act%20in%20prompts%20as%20anchors%20for%20information%0Apropagation.%20GNNavi%20employs%20a%20Graph%20Neural%20Network%20%28GNN%29%20layer%20to%20precisely%0Aguide%20the%20aggregation%20and%20distribution%20of%20information%20flow%20during%20the%0Aprocessing%20of%20prompts%20by%20hardwiring%20the%20desired%20information%20flow%20into%20the%20GNN.%0AOur%20experiments%20on%20text%20classification%20tasks%20with%20GPT-2%20and%20Llama2%20show%20GNNavi%0Asurpasses%20standard%20prompt-based%20fine-tuning%20methods%20in%20few-shot%20settings%20by%0Aupdating%20just%200.2%25%20to%200.5%25%20of%20parameters.%20We%20compare%20GNNavi%20with%20prevalent%20PEFT%0Aapproaches%2C%20such%20as%20prefix%20tuning%2C%20LoRA%20and%20Adapter%20in%20terms%20of%20performance%20and%0Aefficiency.%20Our%20analysis%20reveals%20that%20GNNavi%20enhances%20information%20flow%20and%0Aensures%20a%20clear%20aggregation%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11709v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNNavi%253A%2520Navigating%2520the%2520Information%2520Flow%2520in%2520Large%2520Language%2520Models%2520by%250A%2520%2520Graph%2520Neural%2520Network%26entry.906535625%3DShuzhou%2520Yuan%2520and%2520Ercong%2520Nie%2520and%2520Michael%2520F%25C3%25A4rber%2520and%2520Helmut%2520Schmid%2520and%2520Hinrich%2520Sch%25C3%25BCtze%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520strong%2520In-Context%2520Learning%2520%2528ICL%2529%250Acapabilities%2520when%2520prompts%2520with%2520demonstrations%2520are%2520used.%2520However%252C%2520fine-tuning%250Astill%2520remains%2520crucial%2520to%2520further%2520enhance%2520their%2520adaptability.%2520Prompt-based%250Afine-tuning%2520proves%2520to%2520be%2520an%2520effective%2520fine-tuning%2520method%2520in%2520low-data%2520scenarios%252C%250Abut%2520high%2520demands%2520on%2520computing%2520resources%2520limit%2520its%2520practicality.%2520We%2520address%2520this%250Aissue%2520by%2520introducing%2520a%2520prompt-based%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%250Aapproach.%2520GNNavi%2520leverages%2520insights%2520into%2520ICL%2527s%2520information%2520flow%2520dynamics%252C%2520which%250Aindicates%2520that%2520label%2520words%2520act%2520in%2520prompts%2520as%2520anchors%2520for%2520information%250Apropagation.%2520GNNavi%2520employs%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520layer%2520to%2520precisely%250Aguide%2520the%2520aggregation%2520and%2520distribution%2520of%2520information%2520flow%2520during%2520the%250Aprocessing%2520of%2520prompts%2520by%2520hardwiring%2520the%2520desired%2520information%2520flow%2520into%2520the%2520GNN.%250AOur%2520experiments%2520on%2520text%2520classification%2520tasks%2520with%2520GPT-2%2520and%2520Llama2%2520show%2520GNNavi%250Asurpasses%2520standard%2520prompt-based%2520fine-tuning%2520methods%2520in%2520few-shot%2520settings%2520by%250Aupdating%2520just%25200.2%2525%2520to%25200.5%2525%2520of%2520parameters.%2520We%2520compare%2520GNNavi%2520with%2520prevalent%2520PEFT%250Aapproaches%252C%2520such%2520as%2520prefix%2520tuning%252C%2520LoRA%2520and%2520Adapter%2520in%2520terms%2520of%2520performance%2520and%250Aefficiency.%2520Our%2520analysis%2520reveals%2520that%2520GNNavi%2520enhances%2520information%2520flow%2520and%250Aensures%2520a%2520clear%2520aggregation%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11709v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNNavi%3A%20Navigating%20the%20Information%20Flow%20in%20Large%20Language%20Models%20by%0A%20%20Graph%20Neural%20Network&entry.906535625=Shuzhou%20Yuan%20and%20Ercong%20Nie%20and%20Michael%20F%C3%A4rber%20and%20Helmut%20Schmid%20and%20Hinrich%20Sch%C3%BCtze&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20strong%20In-Context%20Learning%20%28ICL%29%0Acapabilities%20when%20prompts%20with%20demonstrations%20are%20used.%20However%2C%20fine-tuning%0Astill%20remains%20crucial%20to%20further%20enhance%20their%20adaptability.%20Prompt-based%0Afine-tuning%20proves%20to%20be%20an%20effective%20fine-tuning%20method%20in%20low-data%20scenarios%2C%0Abut%20high%20demands%20on%20computing%20resources%20limit%20its%20practicality.%20We%20address%20this%0Aissue%20by%20introducing%20a%20prompt-based%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Aapproach.%20GNNavi%20leverages%20insights%20into%20ICL%27s%20information%20flow%20dynamics%2C%20which%0Aindicates%20that%20label%20words%20act%20in%20prompts%20as%20anchors%20for%20information%0Apropagation.%20GNNavi%20employs%20a%20Graph%20Neural%20Network%20%28GNN%29%20layer%20to%20precisely%0Aguide%20the%20aggregation%20and%20distribution%20of%20information%20flow%20during%20the%0Aprocessing%20of%20prompts%20by%20hardwiring%20the%20desired%20information%20flow%20into%20the%20GNN.%0AOur%20experiments%20on%20text%20classification%20tasks%20with%20GPT-2%20and%20Llama2%20show%20GNNavi%0Asurpasses%20standard%20prompt-based%20fine-tuning%20methods%20in%20few-shot%20settings%20by%0Aupdating%20just%200.2%25%20to%200.5%25%20of%20parameters.%20We%20compare%20GNNavi%20with%20prevalent%20PEFT%0Aapproaches%2C%20such%20as%20prefix%20tuning%2C%20LoRA%20and%20Adapter%20in%20terms%20of%20performance%20and%0Aefficiency.%20Our%20analysis%20reveals%20that%20GNNavi%20enhances%20information%20flow%20and%0Aensures%20a%20clear%20aggregation%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11709v2&entry.124074799=Read"},
{"title": "Efficient 3D Shape Generation via Diffusion Mamba with Bidirectional\n  SSMs", "author": "Shentong Mo", "abstract": "  Recent advancements in sequence modeling have led to the development of the\nMamba architecture, noted for its selective state space approach, offering a\npromising avenue for efficient long sequence handling. However, its application\nin 3D shape generation, particularly at high resolutions, remains\nunderexplored. Traditional diffusion transformers (DiT) with self-attention\nmechanisms, despite their potential, face scalability challenges due to the\ncubic complexity of attention operations as input length increases. This\ncomplexity becomes a significant hurdle when dealing with high-resolution voxel\nsizes. To address this challenge, we introduce a novel diffusion architecture\ntailored for 3D point clouds generation-Diffusion Mamba (DiM-3D). This\narchitecture forgoes traditional attention mechanisms, instead utilizing the\ninherent efficiency of the Mamba architecture to maintain linear complexity\nwith respect to sequence length. DiM-3D is characterized by fast inference\ntimes and substantially lower computational demands, quantified in reduced\nGflops, thereby addressing the key scalability issues of prior models. Our\nempirical results on the ShapeNet benchmark demonstrate that DiM-3D achieves\nstate-of-the-art performance in generating high-fidelity and diverse 3D shapes.\nAdditionally, DiM-3D shows superior capabilities in tasks like 3D point cloud\ncompletion. This not only proves the model's scalability but also underscores\nits efficiency in generating detailed, high-resolution voxels necessary for\nadvanced 3D shape modeling, particularly excelling in environments requiring\nhigh-resolution voxel sizes. Through these findings, we illustrate the\nexceptional scalability and efficiency of the Diffusion Mamba framework in 3D\nshape generation, setting a new standard for the field and paving the way for\nfuture explorations in high-resolution 3D modeling technologies.\n", "link": "http://arxiv.org/abs/2406.05038v1", "date": "2024-06-07", "relevancy": 2.4961, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6302}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6302}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Shape%20Generation%20via%20Diffusion%20Mamba%20with%20Bidirectional%0A%20%20SSMs&body=Title%3A%20Efficient%203D%20Shape%20Generation%20via%20Diffusion%20Mamba%20with%20Bidirectional%0A%20%20SSMs%0AAuthor%3A%20Shentong%20Mo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20sequence%20modeling%20have%20led%20to%20the%20development%20of%20the%0AMamba%20architecture%2C%20noted%20for%20its%20selective%20state%20space%20approach%2C%20offering%20a%0Apromising%20avenue%20for%20efficient%20long%20sequence%20handling.%20However%2C%20its%20application%0Ain%203D%20shape%20generation%2C%20particularly%20at%20high%20resolutions%2C%20remains%0Aunderexplored.%20Traditional%20diffusion%20transformers%20%28DiT%29%20with%20self-attention%0Amechanisms%2C%20despite%20their%20potential%2C%20face%20scalability%20challenges%20due%20to%20the%0Acubic%20complexity%20of%20attention%20operations%20as%20input%20length%20increases.%20This%0Acomplexity%20becomes%20a%20significant%20hurdle%20when%20dealing%20with%20high-resolution%20voxel%0Asizes.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20diffusion%20architecture%0Atailored%20for%203D%20point%20clouds%20generation-Diffusion%20Mamba%20%28DiM-3D%29.%20This%0Aarchitecture%20forgoes%20traditional%20attention%20mechanisms%2C%20instead%20utilizing%20the%0Ainherent%20efficiency%20of%20the%20Mamba%20architecture%20to%20maintain%20linear%20complexity%0Awith%20respect%20to%20sequence%20length.%20DiM-3D%20is%20characterized%20by%20fast%20inference%0Atimes%20and%20substantially%20lower%20computational%20demands%2C%20quantified%20in%20reduced%0AGflops%2C%20thereby%20addressing%20the%20key%20scalability%20issues%20of%20prior%20models.%20Our%0Aempirical%20results%20on%20the%20ShapeNet%20benchmark%20demonstrate%20that%20DiM-3D%20achieves%0Astate-of-the-art%20performance%20in%20generating%20high-fidelity%20and%20diverse%203D%20shapes.%0AAdditionally%2C%20DiM-3D%20shows%20superior%20capabilities%20in%20tasks%20like%203D%20point%20cloud%0Acompletion.%20This%20not%20only%20proves%20the%20model%27s%20scalability%20but%20also%20underscores%0Aits%20efficiency%20in%20generating%20detailed%2C%20high-resolution%20voxels%20necessary%20for%0Aadvanced%203D%20shape%20modeling%2C%20particularly%20excelling%20in%20environments%20requiring%0Ahigh-resolution%20voxel%20sizes.%20Through%20these%20findings%2C%20we%20illustrate%20the%0Aexceptional%20scalability%20and%20efficiency%20of%20the%20Diffusion%20Mamba%20framework%20in%203D%0Ashape%20generation%2C%20setting%20a%20new%20standard%20for%20the%20field%20and%20paving%20the%20way%20for%0Afuture%20explorations%20in%20high-resolution%203D%20modeling%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Shape%2520Generation%2520via%2520Diffusion%2520Mamba%2520with%2520Bidirectional%250A%2520%2520SSMs%26entry.906535625%3DShentong%2520Mo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520sequence%2520modeling%2520have%2520led%2520to%2520the%2520development%2520of%2520the%250AMamba%2520architecture%252C%2520noted%2520for%2520its%2520selective%2520state%2520space%2520approach%252C%2520offering%2520a%250Apromising%2520avenue%2520for%2520efficient%2520long%2520sequence%2520handling.%2520However%252C%2520its%2520application%250Ain%25203D%2520shape%2520generation%252C%2520particularly%2520at%2520high%2520resolutions%252C%2520remains%250Aunderexplored.%2520Traditional%2520diffusion%2520transformers%2520%2528DiT%2529%2520with%2520self-attention%250Amechanisms%252C%2520despite%2520their%2520potential%252C%2520face%2520scalability%2520challenges%2520due%2520to%2520the%250Acubic%2520complexity%2520of%2520attention%2520operations%2520as%2520input%2520length%2520increases.%2520This%250Acomplexity%2520becomes%2520a%2520significant%2520hurdle%2520when%2520dealing%2520with%2520high-resolution%2520voxel%250Asizes.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520novel%2520diffusion%2520architecture%250Atailored%2520for%25203D%2520point%2520clouds%2520generation-Diffusion%2520Mamba%2520%2528DiM-3D%2529.%2520This%250Aarchitecture%2520forgoes%2520traditional%2520attention%2520mechanisms%252C%2520instead%2520utilizing%2520the%250Ainherent%2520efficiency%2520of%2520the%2520Mamba%2520architecture%2520to%2520maintain%2520linear%2520complexity%250Awith%2520respect%2520to%2520sequence%2520length.%2520DiM-3D%2520is%2520characterized%2520by%2520fast%2520inference%250Atimes%2520and%2520substantially%2520lower%2520computational%2520demands%252C%2520quantified%2520in%2520reduced%250AGflops%252C%2520thereby%2520addressing%2520the%2520key%2520scalability%2520issues%2520of%2520prior%2520models.%2520Our%250Aempirical%2520results%2520on%2520the%2520ShapeNet%2520benchmark%2520demonstrate%2520that%2520DiM-3D%2520achieves%250Astate-of-the-art%2520performance%2520in%2520generating%2520high-fidelity%2520and%2520diverse%25203D%2520shapes.%250AAdditionally%252C%2520DiM-3D%2520shows%2520superior%2520capabilities%2520in%2520tasks%2520like%25203D%2520point%2520cloud%250Acompletion.%2520This%2520not%2520only%2520proves%2520the%2520model%2527s%2520scalability%2520but%2520also%2520underscores%250Aits%2520efficiency%2520in%2520generating%2520detailed%252C%2520high-resolution%2520voxels%2520necessary%2520for%250Aadvanced%25203D%2520shape%2520modeling%252C%2520particularly%2520excelling%2520in%2520environments%2520requiring%250Ahigh-resolution%2520voxel%2520sizes.%2520Through%2520these%2520findings%252C%2520we%2520illustrate%2520the%250Aexceptional%2520scalability%2520and%2520efficiency%2520of%2520the%2520Diffusion%2520Mamba%2520framework%2520in%25203D%250Ashape%2520generation%252C%2520setting%2520a%2520new%2520standard%2520for%2520the%2520field%2520and%2520paving%2520the%2520way%2520for%250Afuture%2520explorations%2520in%2520high-resolution%25203D%2520modeling%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Shape%20Generation%20via%20Diffusion%20Mamba%20with%20Bidirectional%0A%20%20SSMs&entry.906535625=Shentong%20Mo&entry.1292438233=%20%20Recent%20advancements%20in%20sequence%20modeling%20have%20led%20to%20the%20development%20of%20the%0AMamba%20architecture%2C%20noted%20for%20its%20selective%20state%20space%20approach%2C%20offering%20a%0Apromising%20avenue%20for%20efficient%20long%20sequence%20handling.%20However%2C%20its%20application%0Ain%203D%20shape%20generation%2C%20particularly%20at%20high%20resolutions%2C%20remains%0Aunderexplored.%20Traditional%20diffusion%20transformers%20%28DiT%29%20with%20self-attention%0Amechanisms%2C%20despite%20their%20potential%2C%20face%20scalability%20challenges%20due%20to%20the%0Acubic%20complexity%20of%20attention%20operations%20as%20input%20length%20increases.%20This%0Acomplexity%20becomes%20a%20significant%20hurdle%20when%20dealing%20with%20high-resolution%20voxel%0Asizes.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20diffusion%20architecture%0Atailored%20for%203D%20point%20clouds%20generation-Diffusion%20Mamba%20%28DiM-3D%29.%20This%0Aarchitecture%20forgoes%20traditional%20attention%20mechanisms%2C%20instead%20utilizing%20the%0Ainherent%20efficiency%20of%20the%20Mamba%20architecture%20to%20maintain%20linear%20complexity%0Awith%20respect%20to%20sequence%20length.%20DiM-3D%20is%20characterized%20by%20fast%20inference%0Atimes%20and%20substantially%20lower%20computational%20demands%2C%20quantified%20in%20reduced%0AGflops%2C%20thereby%20addressing%20the%20key%20scalability%20issues%20of%20prior%20models.%20Our%0Aempirical%20results%20on%20the%20ShapeNet%20benchmark%20demonstrate%20that%20DiM-3D%20achieves%0Astate-of-the-art%20performance%20in%20generating%20high-fidelity%20and%20diverse%203D%20shapes.%0AAdditionally%2C%20DiM-3D%20shows%20superior%20capabilities%20in%20tasks%20like%203D%20point%20cloud%0Acompletion.%20This%20not%20only%20proves%20the%20model%27s%20scalability%20but%20also%20underscores%0Aits%20efficiency%20in%20generating%20detailed%2C%20high-resolution%20voxels%20necessary%20for%0Aadvanced%203D%20shape%20modeling%2C%20particularly%20excelling%20in%20environments%20requiring%0Ahigh-resolution%20voxel%20sizes.%20Through%20these%20findings%2C%20we%20illustrate%20the%0Aexceptional%20scalability%20and%20efficiency%20of%20the%20Diffusion%20Mamba%20framework%20in%203D%0Ashape%20generation%2C%20setting%20a%20new%20standard%20for%20the%20field%20and%20paving%20the%20way%20for%0Afuture%20explorations%20in%20high-resolution%203D%20modeling%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05038v1&entry.124074799=Read"},
{"title": "Layerwise Proximal Replay: A Proximal Point Method for Online Continual\n  Learning", "author": "Jason Yoo and Yunpeng Liu and Frank Wood and Geoff Pleiss", "abstract": "  In online continual learning, a neural network incrementally learns from a\nnon-i.i.d. data stream. Nearly all online continual learning methods employ\nexperience replay to simultaneously prevent catastrophic forgetting and\nunderfitting on past data. Our work demonstrates a limitation of this approach:\nneural networks trained with experience replay tend to have unstable\noptimization trajectories, impeding their overall accuracy. Surprisingly, these\ninstabilities persist even when the replay buffer stores all previous training\nexamples, suggesting that this issue is orthogonal to catastrophic forgetting.\nWe minimize these instabilities through a simple modification of the\noptimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances\nlearning from new and replay data while only allowing for gradual changes in\nthe hidden activation of past data. We demonstrate that LPR consistently\nimproves replay-based online continual learning methods across multiple problem\nsettings, regardless of the amount of available replay memory.\n", "link": "http://arxiv.org/abs/2402.09542v2", "date": "2024-06-07", "relevancy": 2.4412, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4923}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4895}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layerwise%20Proximal%20Replay%3A%20A%20Proximal%20Point%20Method%20for%20Online%20Continual%0A%20%20Learning&body=Title%3A%20Layerwise%20Proximal%20Replay%3A%20A%20Proximal%20Point%20Method%20for%20Online%20Continual%0A%20%20Learning%0AAuthor%3A%20Jason%20Yoo%20and%20Yunpeng%20Liu%20and%20Frank%20Wood%20and%20Geoff%20Pleiss%0AAbstract%3A%20%20%20In%20online%20continual%20learning%2C%20a%20neural%20network%20incrementally%20learns%20from%20a%0Anon-i.i.d.%20data%20stream.%20Nearly%20all%20online%20continual%20learning%20methods%20employ%0Aexperience%20replay%20to%20simultaneously%20prevent%20catastrophic%20forgetting%20and%0Aunderfitting%20on%20past%20data.%20Our%20work%20demonstrates%20a%20limitation%20of%20this%20approach%3A%0Aneural%20networks%20trained%20with%20experience%20replay%20tend%20to%20have%20unstable%0Aoptimization%20trajectories%2C%20impeding%20their%20overall%20accuracy.%20Surprisingly%2C%20these%0Ainstabilities%20persist%20even%20when%20the%20replay%20buffer%20stores%20all%20previous%20training%0Aexamples%2C%20suggesting%20that%20this%20issue%20is%20orthogonal%20to%20catastrophic%20forgetting.%0AWe%20minimize%20these%20instabilities%20through%20a%20simple%20modification%20of%20the%0Aoptimization%20geometry.%20Our%20solution%2C%20Layerwise%20Proximal%20Replay%20%28LPR%29%2C%20balances%0Alearning%20from%20new%20and%20replay%20data%20while%20only%20allowing%20for%20gradual%20changes%20in%0Athe%20hidden%20activation%20of%20past%20data.%20We%20demonstrate%20that%20LPR%20consistently%0Aimproves%20replay-based%20online%20continual%20learning%20methods%20across%20multiple%20problem%0Asettings%2C%20regardless%20of%20the%20amount%20of%20available%20replay%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerwise%2520Proximal%2520Replay%253A%2520A%2520Proximal%2520Point%2520Method%2520for%2520Online%2520Continual%250A%2520%2520Learning%26entry.906535625%3DJason%2520Yoo%2520and%2520Yunpeng%2520Liu%2520and%2520Frank%2520Wood%2520and%2520Geoff%2520Pleiss%26entry.1292438233%3D%2520%2520In%2520online%2520continual%2520learning%252C%2520a%2520neural%2520network%2520incrementally%2520learns%2520from%2520a%250Anon-i.i.d.%2520data%2520stream.%2520Nearly%2520all%2520online%2520continual%2520learning%2520methods%2520employ%250Aexperience%2520replay%2520to%2520simultaneously%2520prevent%2520catastrophic%2520forgetting%2520and%250Aunderfitting%2520on%2520past%2520data.%2520Our%2520work%2520demonstrates%2520a%2520limitation%2520of%2520this%2520approach%253A%250Aneural%2520networks%2520trained%2520with%2520experience%2520replay%2520tend%2520to%2520have%2520unstable%250Aoptimization%2520trajectories%252C%2520impeding%2520their%2520overall%2520accuracy.%2520Surprisingly%252C%2520these%250Ainstabilities%2520persist%2520even%2520when%2520the%2520replay%2520buffer%2520stores%2520all%2520previous%2520training%250Aexamples%252C%2520suggesting%2520that%2520this%2520issue%2520is%2520orthogonal%2520to%2520catastrophic%2520forgetting.%250AWe%2520minimize%2520these%2520instabilities%2520through%2520a%2520simple%2520modification%2520of%2520the%250Aoptimization%2520geometry.%2520Our%2520solution%252C%2520Layerwise%2520Proximal%2520Replay%2520%2528LPR%2529%252C%2520balances%250Alearning%2520from%2520new%2520and%2520replay%2520data%2520while%2520only%2520allowing%2520for%2520gradual%2520changes%2520in%250Athe%2520hidden%2520activation%2520of%2520past%2520data.%2520We%2520demonstrate%2520that%2520LPR%2520consistently%250Aimproves%2520replay-based%2520online%2520continual%2520learning%2520methods%2520across%2520multiple%2520problem%250Asettings%252C%2520regardless%2520of%2520the%2520amount%2520of%2520available%2520replay%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layerwise%20Proximal%20Replay%3A%20A%20Proximal%20Point%20Method%20for%20Online%20Continual%0A%20%20Learning&entry.906535625=Jason%20Yoo%20and%20Yunpeng%20Liu%20and%20Frank%20Wood%20and%20Geoff%20Pleiss&entry.1292438233=%20%20In%20online%20continual%20learning%2C%20a%20neural%20network%20incrementally%20learns%20from%20a%0Anon-i.i.d.%20data%20stream.%20Nearly%20all%20online%20continual%20learning%20methods%20employ%0Aexperience%20replay%20to%20simultaneously%20prevent%20catastrophic%20forgetting%20and%0Aunderfitting%20on%20past%20data.%20Our%20work%20demonstrates%20a%20limitation%20of%20this%20approach%3A%0Aneural%20networks%20trained%20with%20experience%20replay%20tend%20to%20have%20unstable%0Aoptimization%20trajectories%2C%20impeding%20their%20overall%20accuracy.%20Surprisingly%2C%20these%0Ainstabilities%20persist%20even%20when%20the%20replay%20buffer%20stores%20all%20previous%20training%0Aexamples%2C%20suggesting%20that%20this%20issue%20is%20orthogonal%20to%20catastrophic%20forgetting.%0AWe%20minimize%20these%20instabilities%20through%20a%20simple%20modification%20of%20the%0Aoptimization%20geometry.%20Our%20solution%2C%20Layerwise%20Proximal%20Replay%20%28LPR%29%2C%20balances%0Alearning%20from%20new%20and%20replay%20data%20while%20only%20allowing%20for%20gradual%20changes%20in%0Athe%20hidden%20activation%20of%20past%20data.%20We%20demonstrate%20that%20LPR%20consistently%0Aimproves%20replay-based%20online%20continual%20learning%20methods%20across%20multiple%20problem%0Asettings%2C%20regardless%20of%20the%20amount%20of%20available%20replay%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09542v2&entry.124074799=Read"},
{"title": "CityCraft: A Real Crafter for 3D City Generation", "author": "Jie Deng and Wenhao Chai and Junsheng Huang and Zhonghan Zhao and Qixuan Huang and Mingyan Gao and Jianshu Guo and Shengyu Hao and Wenhao Hu and Jenq-Neng Hwang and Xi Li and Gaoang Wang", "abstract": "  City scene generation has gained significant attention in autonomous driving,\nsmart city development, and traffic simulation. It helps enhance infrastructure\nplanning and monitoring solutions. Existing methods have employed a two-stage\nprocess involving city layout generation, typically using Variational\nAutoencoders (VAEs), Generative Adversarial Networks (GANs), or Transformers,\nfollowed by neural rendering. These techniques often exhibit limited diversity\nand noticeable artifacts in the rendered city scenes. The rendered scenes lack\nvariety, resembling the training images, resulting in monotonous styles.\nAdditionally, these methods lack planning capabilities, leading to less\nrealistic generated scenes. In this paper, we introduce CityCraft, an\ninnovative framework designed to enhance both the diversity and quality of\nurban scene generation. Our approach integrates three key stages: initially, a\ndiffusion transformer (DiT) model is deployed to generate diverse and\ncontrollable 2D city layouts. Subsequently, a Large Language Model(LLM) is\nutilized to strategically make land-use plans within these layouts based on\nuser prompts and language guidelines. Based on the generated layout and city\nplan, we utilize the asset retrieval module and Blender for precise asset\nplacement and scene construction. Furthermore, we contribute two new datasets\nto the field: 1)CityCraft-OSM dataset including 2D semantic layouts of urban\nareas, corresponding satellite images, and detailed annotations. 2)\nCityCraft-Buildings dataset, featuring thousands of diverse, high-quality 3D\nbuilding assets. CityCraft achieves state-of-the-art performance in generating\nrealistic 3D cities.\n", "link": "http://arxiv.org/abs/2406.04983v1", "date": "2024-06-07", "relevancy": 2.4352, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.62}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.62}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityCraft%3A%20A%20Real%20Crafter%20for%203D%20City%20Generation&body=Title%3A%20CityCraft%3A%20A%20Real%20Crafter%20for%203D%20City%20Generation%0AAuthor%3A%20Jie%20Deng%20and%20Wenhao%20Chai%20and%20Junsheng%20Huang%20and%20Zhonghan%20Zhao%20and%20Qixuan%20Huang%20and%20Mingyan%20Gao%20and%20Jianshu%20Guo%20and%20Shengyu%20Hao%20and%20Wenhao%20Hu%20and%20Jenq-Neng%20Hwang%20and%20Xi%20Li%20and%20Gaoang%20Wang%0AAbstract%3A%20%20%20City%20scene%20generation%20has%20gained%20significant%20attention%20in%20autonomous%20driving%2C%0Asmart%20city%20development%2C%20and%20traffic%20simulation.%20It%20helps%20enhance%20infrastructure%0Aplanning%20and%20monitoring%20solutions.%20Existing%20methods%20have%20employed%20a%20two-stage%0Aprocess%20involving%20city%20layout%20generation%2C%20typically%20using%20Variational%0AAutoencoders%20%28VAEs%29%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20or%20Transformers%2C%0Afollowed%20by%20neural%20rendering.%20These%20techniques%20often%20exhibit%20limited%20diversity%0Aand%20noticeable%20artifacts%20in%20the%20rendered%20city%20scenes.%20The%20rendered%20scenes%20lack%0Avariety%2C%20resembling%20the%20training%20images%2C%20resulting%20in%20monotonous%20styles.%0AAdditionally%2C%20these%20methods%20lack%20planning%20capabilities%2C%20leading%20to%20less%0Arealistic%20generated%20scenes.%20In%20this%20paper%2C%20we%20introduce%20CityCraft%2C%20an%0Ainnovative%20framework%20designed%20to%20enhance%20both%20the%20diversity%20and%20quality%20of%0Aurban%20scene%20generation.%20Our%20approach%20integrates%20three%20key%20stages%3A%20initially%2C%20a%0Adiffusion%20transformer%20%28DiT%29%20model%20is%20deployed%20to%20generate%20diverse%20and%0Acontrollable%202D%20city%20layouts.%20Subsequently%2C%20a%20Large%20Language%20Model%28LLM%29%20is%0Autilized%20to%20strategically%20make%20land-use%20plans%20within%20these%20layouts%20based%20on%0Auser%20prompts%20and%20language%20guidelines.%20Based%20on%20the%20generated%20layout%20and%20city%0Aplan%2C%20we%20utilize%20the%20asset%20retrieval%20module%20and%20Blender%20for%20precise%20asset%0Aplacement%20and%20scene%20construction.%20Furthermore%2C%20we%20contribute%20two%20new%20datasets%0Ato%20the%20field%3A%201%29CityCraft-OSM%20dataset%20including%202D%20semantic%20layouts%20of%20urban%0Aareas%2C%20corresponding%20satellite%20images%2C%20and%20detailed%20annotations.%202%29%0ACityCraft-Buildings%20dataset%2C%20featuring%20thousands%20of%20diverse%2C%20high-quality%203D%0Abuilding%20assets.%20CityCraft%20achieves%20state-of-the-art%20performance%20in%20generating%0Arealistic%203D%20cities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityCraft%253A%2520A%2520Real%2520Crafter%2520for%25203D%2520City%2520Generation%26entry.906535625%3DJie%2520Deng%2520and%2520Wenhao%2520Chai%2520and%2520Junsheng%2520Huang%2520and%2520Zhonghan%2520Zhao%2520and%2520Qixuan%2520Huang%2520and%2520Mingyan%2520Gao%2520and%2520Jianshu%2520Guo%2520and%2520Shengyu%2520Hao%2520and%2520Wenhao%2520Hu%2520and%2520Jenq-Neng%2520Hwang%2520and%2520Xi%2520Li%2520and%2520Gaoang%2520Wang%26entry.1292438233%3D%2520%2520City%2520scene%2520generation%2520has%2520gained%2520significant%2520attention%2520in%2520autonomous%2520driving%252C%250Asmart%2520city%2520development%252C%2520and%2520traffic%2520simulation.%2520It%2520helps%2520enhance%2520infrastructure%250Aplanning%2520and%2520monitoring%2520solutions.%2520Existing%2520methods%2520have%2520employed%2520a%2520two-stage%250Aprocess%2520involving%2520city%2520layout%2520generation%252C%2520typically%2520using%2520Variational%250AAutoencoders%2520%2528VAEs%2529%252C%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%2520or%2520Transformers%252C%250Afollowed%2520by%2520neural%2520rendering.%2520These%2520techniques%2520often%2520exhibit%2520limited%2520diversity%250Aand%2520noticeable%2520artifacts%2520in%2520the%2520rendered%2520city%2520scenes.%2520The%2520rendered%2520scenes%2520lack%250Avariety%252C%2520resembling%2520the%2520training%2520images%252C%2520resulting%2520in%2520monotonous%2520styles.%250AAdditionally%252C%2520these%2520methods%2520lack%2520planning%2520capabilities%252C%2520leading%2520to%2520less%250Arealistic%2520generated%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CityCraft%252C%2520an%250Ainnovative%2520framework%2520designed%2520to%2520enhance%2520both%2520the%2520diversity%2520and%2520quality%2520of%250Aurban%2520scene%2520generation.%2520Our%2520approach%2520integrates%2520three%2520key%2520stages%253A%2520initially%252C%2520a%250Adiffusion%2520transformer%2520%2528DiT%2529%2520model%2520is%2520deployed%2520to%2520generate%2520diverse%2520and%250Acontrollable%25202D%2520city%2520layouts.%2520Subsequently%252C%2520a%2520Large%2520Language%2520Model%2528LLM%2529%2520is%250Autilized%2520to%2520strategically%2520make%2520land-use%2520plans%2520within%2520these%2520layouts%2520based%2520on%250Auser%2520prompts%2520and%2520language%2520guidelines.%2520Based%2520on%2520the%2520generated%2520layout%2520and%2520city%250Aplan%252C%2520we%2520utilize%2520the%2520asset%2520retrieval%2520module%2520and%2520Blender%2520for%2520precise%2520asset%250Aplacement%2520and%2520scene%2520construction.%2520Furthermore%252C%2520we%2520contribute%2520two%2520new%2520datasets%250Ato%2520the%2520field%253A%25201%2529CityCraft-OSM%2520dataset%2520including%25202D%2520semantic%2520layouts%2520of%2520urban%250Aareas%252C%2520corresponding%2520satellite%2520images%252C%2520and%2520detailed%2520annotations.%25202%2529%250ACityCraft-Buildings%2520dataset%252C%2520featuring%2520thousands%2520of%2520diverse%252C%2520high-quality%25203D%250Abuilding%2520assets.%2520CityCraft%2520achieves%2520state-of-the-art%2520performance%2520in%2520generating%250Arealistic%25203D%2520cities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityCraft%3A%20A%20Real%20Crafter%20for%203D%20City%20Generation&entry.906535625=Jie%20Deng%20and%20Wenhao%20Chai%20and%20Junsheng%20Huang%20and%20Zhonghan%20Zhao%20and%20Qixuan%20Huang%20and%20Mingyan%20Gao%20and%20Jianshu%20Guo%20and%20Shengyu%20Hao%20and%20Wenhao%20Hu%20and%20Jenq-Neng%20Hwang%20and%20Xi%20Li%20and%20Gaoang%20Wang&entry.1292438233=%20%20City%20scene%20generation%20has%20gained%20significant%20attention%20in%20autonomous%20driving%2C%0Asmart%20city%20development%2C%20and%20traffic%20simulation.%20It%20helps%20enhance%20infrastructure%0Aplanning%20and%20monitoring%20solutions.%20Existing%20methods%20have%20employed%20a%20two-stage%0Aprocess%20involving%20city%20layout%20generation%2C%20typically%20using%20Variational%0AAutoencoders%20%28VAEs%29%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20or%20Transformers%2C%0Afollowed%20by%20neural%20rendering.%20These%20techniques%20often%20exhibit%20limited%20diversity%0Aand%20noticeable%20artifacts%20in%20the%20rendered%20city%20scenes.%20The%20rendered%20scenes%20lack%0Avariety%2C%20resembling%20the%20training%20images%2C%20resulting%20in%20monotonous%20styles.%0AAdditionally%2C%20these%20methods%20lack%20planning%20capabilities%2C%20leading%20to%20less%0Arealistic%20generated%20scenes.%20In%20this%20paper%2C%20we%20introduce%20CityCraft%2C%20an%0Ainnovative%20framework%20designed%20to%20enhance%20both%20the%20diversity%20and%20quality%20of%0Aurban%20scene%20generation.%20Our%20approach%20integrates%20three%20key%20stages%3A%20initially%2C%20a%0Adiffusion%20transformer%20%28DiT%29%20model%20is%20deployed%20to%20generate%20diverse%20and%0Acontrollable%202D%20city%20layouts.%20Subsequently%2C%20a%20Large%20Language%20Model%28LLM%29%20is%0Autilized%20to%20strategically%20make%20land-use%20plans%20within%20these%20layouts%20based%20on%0Auser%20prompts%20and%20language%20guidelines.%20Based%20on%20the%20generated%20layout%20and%20city%0Aplan%2C%20we%20utilize%20the%20asset%20retrieval%20module%20and%20Blender%20for%20precise%20asset%0Aplacement%20and%20scene%20construction.%20Furthermore%2C%20we%20contribute%20two%20new%20datasets%0Ato%20the%20field%3A%201%29CityCraft-OSM%20dataset%20including%202D%20semantic%20layouts%20of%20urban%0Aareas%2C%20corresponding%20satellite%20images%2C%20and%20detailed%20annotations.%202%29%0ACityCraft-Buildings%20dataset%2C%20featuring%20thousands%20of%20diverse%2C%20high-quality%203D%0Abuilding%20assets.%20CityCraft%20achieves%20state-of-the-art%20performance%20in%20generating%0Arealistic%203D%20cities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04983v1&entry.124074799=Read"},
{"title": "The Influencer Next Door: How Misinformation Creators Use GenAI", "author": "Amelia Hassoun and Ariel Abonizio and Katy Osborn and Cameron Wu and Beth Goldberg", "abstract": "  Advances in generative AI (GenAI) have raised concerns about detecting and\ndiscerning AI-generated content from human-generated content. Most existing\nliterature assumes a paradigm where 'expert' organized disinformation creators\nand flawed AI models deceive 'ordinary' users. Based on longitudinal\nethnographic research with misinformation creators and consumers between\n2022-2023, we instead find that GenAI supports bricolage work, where\nnon-experts increasingly use GenAI to remix, repackage, and (re)produce content\nto meet their personal needs and desires. This research yielded four key\nfindings: First, participants primarily used GenAI for creation, rather than\ntruth-seeking. Second, a spreading 'influencer millionaire' narrative drove\nparticipants to become content creators, using GenAI as a productivity tool to\ngenerate a volume of (often misinformative) content. Third, GenAI lowered the\nbarrier to entry for content creation across modalities, enticing consumers to\nbecome creators and significantly increasing existing creators' output.\nFinally, participants used Gen AI to learn and deploy marketing tactics to\nexpand engagement and monetize their content. We argue for shifting analysis\nfrom the public as consumers of AI content to bricoleurs who use GenAI\ncreatively, often without a detailed understanding of its underlying\ntechnology. We analyze how these understudied emergent uses of GenAI produce\nnew or accelerated misinformation harms, and their implications for AI\nproducts, platforms and policies.\n", "link": "http://arxiv.org/abs/2405.13554v2", "date": "2024-06-07", "relevancy": 2.4211, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4676}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Influencer%20Next%20Door%3A%20How%20Misinformation%20Creators%20Use%20GenAI&body=Title%3A%20The%20Influencer%20Next%20Door%3A%20How%20Misinformation%20Creators%20Use%20GenAI%0AAuthor%3A%20Amelia%20Hassoun%20and%20Ariel%20Abonizio%20and%20Katy%20Osborn%20and%20Cameron%20Wu%20and%20Beth%20Goldberg%0AAbstract%3A%20%20%20Advances%20in%20generative%20AI%20%28GenAI%29%20have%20raised%20concerns%20about%20detecting%20and%0Adiscerning%20AI-generated%20content%20from%20human-generated%20content.%20Most%20existing%0Aliterature%20assumes%20a%20paradigm%20where%20%27expert%27%20organized%20disinformation%20creators%0Aand%20flawed%20AI%20models%20deceive%20%27ordinary%27%20users.%20Based%20on%20longitudinal%0Aethnographic%20research%20with%20misinformation%20creators%20and%20consumers%20between%0A2022-2023%2C%20we%20instead%20find%20that%20GenAI%20supports%20bricolage%20work%2C%20where%0Anon-experts%20increasingly%20use%20GenAI%20to%20remix%2C%20repackage%2C%20and%20%28re%29produce%20content%0Ato%20meet%20their%20personal%20needs%20and%20desires.%20This%20research%20yielded%20four%20key%0Afindings%3A%20First%2C%20participants%20primarily%20used%20GenAI%20for%20creation%2C%20rather%20than%0Atruth-seeking.%20Second%2C%20a%20spreading%20%27influencer%20millionaire%27%20narrative%20drove%0Aparticipants%20to%20become%20content%20creators%2C%20using%20GenAI%20as%20a%20productivity%20tool%20to%0Agenerate%20a%20volume%20of%20%28often%20misinformative%29%20content.%20Third%2C%20GenAI%20lowered%20the%0Abarrier%20to%20entry%20for%20content%20creation%20across%20modalities%2C%20enticing%20consumers%20to%0Abecome%20creators%20and%20significantly%20increasing%20existing%20creators%27%20output.%0AFinally%2C%20participants%20used%20Gen%20AI%20to%20learn%20and%20deploy%20marketing%20tactics%20to%0Aexpand%20engagement%20and%20monetize%20their%20content.%20We%20argue%20for%20shifting%20analysis%0Afrom%20the%20public%20as%20consumers%20of%20AI%20content%20to%20bricoleurs%20who%20use%20GenAI%0Acreatively%2C%20often%20without%20a%20detailed%20understanding%20of%20its%20underlying%0Atechnology.%20We%20analyze%20how%20these%20understudied%20emergent%20uses%20of%20GenAI%20produce%0Anew%20or%20accelerated%20misinformation%20harms%2C%20and%20their%20implications%20for%20AI%0Aproducts%2C%20platforms%20and%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13554v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Influencer%2520Next%2520Door%253A%2520How%2520Misinformation%2520Creators%2520Use%2520GenAI%26entry.906535625%3DAmelia%2520Hassoun%2520and%2520Ariel%2520Abonizio%2520and%2520Katy%2520Osborn%2520and%2520Cameron%2520Wu%2520and%2520Beth%2520Goldberg%26entry.1292438233%3D%2520%2520Advances%2520in%2520generative%2520AI%2520%2528GenAI%2529%2520have%2520raised%2520concerns%2520about%2520detecting%2520and%250Adiscerning%2520AI-generated%2520content%2520from%2520human-generated%2520content.%2520Most%2520existing%250Aliterature%2520assumes%2520a%2520paradigm%2520where%2520%2527expert%2527%2520organized%2520disinformation%2520creators%250Aand%2520flawed%2520AI%2520models%2520deceive%2520%2527ordinary%2527%2520users.%2520Based%2520on%2520longitudinal%250Aethnographic%2520research%2520with%2520misinformation%2520creators%2520and%2520consumers%2520between%250A2022-2023%252C%2520we%2520instead%2520find%2520that%2520GenAI%2520supports%2520bricolage%2520work%252C%2520where%250Anon-experts%2520increasingly%2520use%2520GenAI%2520to%2520remix%252C%2520repackage%252C%2520and%2520%2528re%2529produce%2520content%250Ato%2520meet%2520their%2520personal%2520needs%2520and%2520desires.%2520This%2520research%2520yielded%2520four%2520key%250Afindings%253A%2520First%252C%2520participants%2520primarily%2520used%2520GenAI%2520for%2520creation%252C%2520rather%2520than%250Atruth-seeking.%2520Second%252C%2520a%2520spreading%2520%2527influencer%2520millionaire%2527%2520narrative%2520drove%250Aparticipants%2520to%2520become%2520content%2520creators%252C%2520using%2520GenAI%2520as%2520a%2520productivity%2520tool%2520to%250Agenerate%2520a%2520volume%2520of%2520%2528often%2520misinformative%2529%2520content.%2520Third%252C%2520GenAI%2520lowered%2520the%250Abarrier%2520to%2520entry%2520for%2520content%2520creation%2520across%2520modalities%252C%2520enticing%2520consumers%2520to%250Abecome%2520creators%2520and%2520significantly%2520increasing%2520existing%2520creators%2527%2520output.%250AFinally%252C%2520participants%2520used%2520Gen%2520AI%2520to%2520learn%2520and%2520deploy%2520marketing%2520tactics%2520to%250Aexpand%2520engagement%2520and%2520monetize%2520their%2520content.%2520We%2520argue%2520for%2520shifting%2520analysis%250Afrom%2520the%2520public%2520as%2520consumers%2520of%2520AI%2520content%2520to%2520bricoleurs%2520who%2520use%2520GenAI%250Acreatively%252C%2520often%2520without%2520a%2520detailed%2520understanding%2520of%2520its%2520underlying%250Atechnology.%2520We%2520analyze%2520how%2520these%2520understudied%2520emergent%2520uses%2520of%2520GenAI%2520produce%250Anew%2520or%2520accelerated%2520misinformation%2520harms%252C%2520and%2520their%2520implications%2520for%2520AI%250Aproducts%252C%2520platforms%2520and%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13554v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Influencer%20Next%20Door%3A%20How%20Misinformation%20Creators%20Use%20GenAI&entry.906535625=Amelia%20Hassoun%20and%20Ariel%20Abonizio%20and%20Katy%20Osborn%20and%20Cameron%20Wu%20and%20Beth%20Goldberg&entry.1292438233=%20%20Advances%20in%20generative%20AI%20%28GenAI%29%20have%20raised%20concerns%20about%20detecting%20and%0Adiscerning%20AI-generated%20content%20from%20human-generated%20content.%20Most%20existing%0Aliterature%20assumes%20a%20paradigm%20where%20%27expert%27%20organized%20disinformation%20creators%0Aand%20flawed%20AI%20models%20deceive%20%27ordinary%27%20users.%20Based%20on%20longitudinal%0Aethnographic%20research%20with%20misinformation%20creators%20and%20consumers%20between%0A2022-2023%2C%20we%20instead%20find%20that%20GenAI%20supports%20bricolage%20work%2C%20where%0Anon-experts%20increasingly%20use%20GenAI%20to%20remix%2C%20repackage%2C%20and%20%28re%29produce%20content%0Ato%20meet%20their%20personal%20needs%20and%20desires.%20This%20research%20yielded%20four%20key%0Afindings%3A%20First%2C%20participants%20primarily%20used%20GenAI%20for%20creation%2C%20rather%20than%0Atruth-seeking.%20Second%2C%20a%20spreading%20%27influencer%20millionaire%27%20narrative%20drove%0Aparticipants%20to%20become%20content%20creators%2C%20using%20GenAI%20as%20a%20productivity%20tool%20to%0Agenerate%20a%20volume%20of%20%28often%20misinformative%29%20content.%20Third%2C%20GenAI%20lowered%20the%0Abarrier%20to%20entry%20for%20content%20creation%20across%20modalities%2C%20enticing%20consumers%20to%0Abecome%20creators%20and%20significantly%20increasing%20existing%20creators%27%20output.%0AFinally%2C%20participants%20used%20Gen%20AI%20to%20learn%20and%20deploy%20marketing%20tactics%20to%0Aexpand%20engagement%20and%20monetize%20their%20content.%20We%20argue%20for%20shifting%20analysis%0Afrom%20the%20public%20as%20consumers%20of%20AI%20content%20to%20bricoleurs%20who%20use%20GenAI%0Acreatively%2C%20often%20without%20a%20detailed%20understanding%20of%20its%20underlying%0Atechnology.%20We%20analyze%20how%20these%20understudied%20emergent%20uses%20of%20GenAI%20produce%0Anew%20or%20accelerated%20misinformation%20harms%2C%20and%20their%20implications%20for%20AI%0Aproducts%2C%20platforms%20and%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13554v2&entry.124074799=Read"},
{"title": "AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image\n  Generation", "author": "Lianyu Pang and Jian Yin and Baoquan Zhao and Feize Wu and Fu Lee Wang and Qing Li and Xudong Mao", "abstract": "  Recent advances in text-to-image models have enabled high-quality\npersonalized image synthesis of user-provided concepts with flexible textual\ncontrol. In this work, we analyze the limitations of two primary techniques in\ntext-to-image personalization: Textual Inversion and DreamBooth. When\nintegrating the learned concept into new prompts, Textual Inversion tends to\noverfit the concept, while DreamBooth often overlooks it. We attribute these\nissues to the incorrect learning of the embedding alignment for the concept. We\nintroduce AttnDreamBooth, a novel approach that addresses these issues by\nseparately learning the embedding alignment, the attention map, and the subject\nidentity in different training stages. We also introduce a cross-attention map\nregularization term to enhance the learning of the attention map. Our method\ndemonstrates significant improvements in identity preservation and text\nalignment compared to the baseline methods.\n", "link": "http://arxiv.org/abs/2406.05000v1", "date": "2024-06-07", "relevancy": 2.4087, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6238}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6121}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttnDreamBooth%3A%20Towards%20Text-Aligned%20Personalized%20Text-to-Image%0A%20%20Generation&body=Title%3A%20AttnDreamBooth%3A%20Towards%20Text-Aligned%20Personalized%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Lianyu%20Pang%20and%20Jian%20Yin%20and%20Baoquan%20Zhao%20and%20Feize%20Wu%20and%20Fu%20Lee%20Wang%20and%20Qing%20Li%20and%20Xudong%20Mao%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20models%20have%20enabled%20high-quality%0Apersonalized%20image%20synthesis%20of%20user-provided%20concepts%20with%20flexible%20textual%0Acontrol.%20In%20this%20work%2C%20we%20analyze%20the%20limitations%20of%20two%20primary%20techniques%20in%0Atext-to-image%20personalization%3A%20Textual%20Inversion%20and%20DreamBooth.%20When%0Aintegrating%20the%20learned%20concept%20into%20new%20prompts%2C%20Textual%20Inversion%20tends%20to%0Aoverfit%20the%20concept%2C%20while%20DreamBooth%20often%20overlooks%20it.%20We%20attribute%20these%0Aissues%20to%20the%20incorrect%20learning%20of%20the%20embedding%20alignment%20for%20the%20concept.%20We%0Aintroduce%20AttnDreamBooth%2C%20a%20novel%20approach%20that%20addresses%20these%20issues%20by%0Aseparately%20learning%20the%20embedding%20alignment%2C%20the%20attention%20map%2C%20and%20the%20subject%0Aidentity%20in%20different%20training%20stages.%20We%20also%20introduce%20a%20cross-attention%20map%0Aregularization%20term%20to%20enhance%20the%20learning%20of%20the%20attention%20map.%20Our%20method%0Ademonstrates%20significant%20improvements%20in%20identity%20preservation%20and%20text%0Aalignment%20compared%20to%20the%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttnDreamBooth%253A%2520Towards%2520Text-Aligned%2520Personalized%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DLianyu%2520Pang%2520and%2520Jian%2520Yin%2520and%2520Baoquan%2520Zhao%2520and%2520Feize%2520Wu%2520and%2520Fu%2520Lee%2520Wang%2520and%2520Qing%2520Li%2520and%2520Xudong%2520Mao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520models%2520have%2520enabled%2520high-quality%250Apersonalized%2520image%2520synthesis%2520of%2520user-provided%2520concepts%2520with%2520flexible%2520textual%250Acontrol.%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%2520limitations%2520of%2520two%2520primary%2520techniques%2520in%250Atext-to-image%2520personalization%253A%2520Textual%2520Inversion%2520and%2520DreamBooth.%2520When%250Aintegrating%2520the%2520learned%2520concept%2520into%2520new%2520prompts%252C%2520Textual%2520Inversion%2520tends%2520to%250Aoverfit%2520the%2520concept%252C%2520while%2520DreamBooth%2520often%2520overlooks%2520it.%2520We%2520attribute%2520these%250Aissues%2520to%2520the%2520incorrect%2520learning%2520of%2520the%2520embedding%2520alignment%2520for%2520the%2520concept.%2520We%250Aintroduce%2520AttnDreamBooth%252C%2520a%2520novel%2520approach%2520that%2520addresses%2520these%2520issues%2520by%250Aseparately%2520learning%2520the%2520embedding%2520alignment%252C%2520the%2520attention%2520map%252C%2520and%2520the%2520subject%250Aidentity%2520in%2520different%2520training%2520stages.%2520We%2520also%2520introduce%2520a%2520cross-attention%2520map%250Aregularization%2520term%2520to%2520enhance%2520the%2520learning%2520of%2520the%2520attention%2520map.%2520Our%2520method%250Ademonstrates%2520significant%2520improvements%2520in%2520identity%2520preservation%2520and%2520text%250Aalignment%2520compared%2520to%2520the%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttnDreamBooth%3A%20Towards%20Text-Aligned%20Personalized%20Text-to-Image%0A%20%20Generation&entry.906535625=Lianyu%20Pang%20and%20Jian%20Yin%20and%20Baoquan%20Zhao%20and%20Feize%20Wu%20and%20Fu%20Lee%20Wang%20and%20Qing%20Li%20and%20Xudong%20Mao&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20models%20have%20enabled%20high-quality%0Apersonalized%20image%20synthesis%20of%20user-provided%20concepts%20with%20flexible%20textual%0Acontrol.%20In%20this%20work%2C%20we%20analyze%20the%20limitations%20of%20two%20primary%20techniques%20in%0Atext-to-image%20personalization%3A%20Textual%20Inversion%20and%20DreamBooth.%20When%0Aintegrating%20the%20learned%20concept%20into%20new%20prompts%2C%20Textual%20Inversion%20tends%20to%0Aoverfit%20the%20concept%2C%20while%20DreamBooth%20often%20overlooks%20it.%20We%20attribute%20these%0Aissues%20to%20the%20incorrect%20learning%20of%20the%20embedding%20alignment%20for%20the%20concept.%20We%0Aintroduce%20AttnDreamBooth%2C%20a%20novel%20approach%20that%20addresses%20these%20issues%20by%0Aseparately%20learning%20the%20embedding%20alignment%2C%20the%20attention%20map%2C%20and%20the%20subject%0Aidentity%20in%20different%20training%20stages.%20We%20also%20introduce%20a%20cross-attention%20map%0Aregularization%20term%20to%20enhance%20the%20learning%20of%20the%20attention%20map.%20Our%20method%0Ademonstrates%20significant%20improvements%20in%20identity%20preservation%20and%20text%0Aalignment%20compared%20to%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05000v1&entry.124074799=Read"},
{"title": "A Sparse Graph Formulation for Efficient Spectral Image Segmentation", "author": "Rahul Palnitkar and Jeova Farias Sales Rocha Neto", "abstract": "  Spectral Clustering is one of the most traditional methods to solve\nsegmentation problems. Based on Normalized Cuts, it aims at partitioning an\nimage using an objective function defined by a graph. Despite their\nmathematical attractiveness, spectral approaches are traditionally neglected by\nthe scientific community due to their practical issues and underperformance. In\nthis paper, we adopt a sparse graph formulation based on the inclusion of extra\nnodes to a simple grid graph. While the grid encodes the pixel spatial\ndisposition, the extra nodes account for the pixel color data. Applying the\noriginal Normalized Cuts algorithm to this graph leads to a simple and scalable\nmethod for spectral image segmentation, with an interpretable solution. Our\nexperiments also demonstrate that our proposed methodology over performs both\ntraditional and modern unsupervised algorithms for segmentation in both real\nand synthetic data.\n", "link": "http://arxiv.org/abs/2306.13166v3", "date": "2024-06-07", "relevancy": 2.4077, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sparse%20Graph%20Formulation%20for%20Efficient%20Spectral%20Image%20Segmentation&body=Title%3A%20A%20Sparse%20Graph%20Formulation%20for%20Efficient%20Spectral%20Image%20Segmentation%0AAuthor%3A%20Rahul%20Palnitkar%20and%20Jeova%20Farias%20Sales%20Rocha%20Neto%0AAbstract%3A%20%20%20Spectral%20Clustering%20is%20one%20of%20the%20most%20traditional%20methods%20to%20solve%0Asegmentation%20problems.%20Based%20on%20Normalized%20Cuts%2C%20it%20aims%20at%20partitioning%20an%0Aimage%20using%20an%20objective%20function%20defined%20by%20a%20graph.%20Despite%20their%0Amathematical%20attractiveness%2C%20spectral%20approaches%20are%20traditionally%20neglected%20by%0Athe%20scientific%20community%20due%20to%20their%20practical%20issues%20and%20underperformance.%20In%0Athis%20paper%2C%20we%20adopt%20a%20sparse%20graph%20formulation%20based%20on%20the%20inclusion%20of%20extra%0Anodes%20to%20a%20simple%20grid%20graph.%20While%20the%20grid%20encodes%20the%20pixel%20spatial%0Adisposition%2C%20the%20extra%20nodes%20account%20for%20the%20pixel%20color%20data.%20Applying%20the%0Aoriginal%20Normalized%20Cuts%20algorithm%20to%20this%20graph%20leads%20to%20a%20simple%20and%20scalable%0Amethod%20for%20spectral%20image%20segmentation%2C%20with%20an%20interpretable%20solution.%20Our%0Aexperiments%20also%20demonstrate%20that%20our%20proposed%20methodology%20over%20performs%20both%0Atraditional%20and%20modern%20unsupervised%20algorithms%20for%20segmentation%20in%20both%20real%0Aand%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13166v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sparse%2520Graph%2520Formulation%2520for%2520Efficient%2520Spectral%2520Image%2520Segmentation%26entry.906535625%3DRahul%2520Palnitkar%2520and%2520Jeova%2520Farias%2520Sales%2520Rocha%2520Neto%26entry.1292438233%3D%2520%2520Spectral%2520Clustering%2520is%2520one%2520of%2520the%2520most%2520traditional%2520methods%2520to%2520solve%250Asegmentation%2520problems.%2520Based%2520on%2520Normalized%2520Cuts%252C%2520it%2520aims%2520at%2520partitioning%2520an%250Aimage%2520using%2520an%2520objective%2520function%2520defined%2520by%2520a%2520graph.%2520Despite%2520their%250Amathematical%2520attractiveness%252C%2520spectral%2520approaches%2520are%2520traditionally%2520neglected%2520by%250Athe%2520scientific%2520community%2520due%2520to%2520their%2520practical%2520issues%2520and%2520underperformance.%2520In%250Athis%2520paper%252C%2520we%2520adopt%2520a%2520sparse%2520graph%2520formulation%2520based%2520on%2520the%2520inclusion%2520of%2520extra%250Anodes%2520to%2520a%2520simple%2520grid%2520graph.%2520While%2520the%2520grid%2520encodes%2520the%2520pixel%2520spatial%250Adisposition%252C%2520the%2520extra%2520nodes%2520account%2520for%2520the%2520pixel%2520color%2520data.%2520Applying%2520the%250Aoriginal%2520Normalized%2520Cuts%2520algorithm%2520to%2520this%2520graph%2520leads%2520to%2520a%2520simple%2520and%2520scalable%250Amethod%2520for%2520spectral%2520image%2520segmentation%252C%2520with%2520an%2520interpretable%2520solution.%2520Our%250Aexperiments%2520also%2520demonstrate%2520that%2520our%2520proposed%2520methodology%2520over%2520performs%2520both%250Atraditional%2520and%2520modern%2520unsupervised%2520algorithms%2520for%2520segmentation%2520in%2520both%2520real%250Aand%2520synthetic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13166v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sparse%20Graph%20Formulation%20for%20Efficient%20Spectral%20Image%20Segmentation&entry.906535625=Rahul%20Palnitkar%20and%20Jeova%20Farias%20Sales%20Rocha%20Neto&entry.1292438233=%20%20Spectral%20Clustering%20is%20one%20of%20the%20most%20traditional%20methods%20to%20solve%0Asegmentation%20problems.%20Based%20on%20Normalized%20Cuts%2C%20it%20aims%20at%20partitioning%20an%0Aimage%20using%20an%20objective%20function%20defined%20by%20a%20graph.%20Despite%20their%0Amathematical%20attractiveness%2C%20spectral%20approaches%20are%20traditionally%20neglected%20by%0Athe%20scientific%20community%20due%20to%20their%20practical%20issues%20and%20underperformance.%20In%0Athis%20paper%2C%20we%20adopt%20a%20sparse%20graph%20formulation%20based%20on%20the%20inclusion%20of%20extra%0Anodes%20to%20a%20simple%20grid%20graph.%20While%20the%20grid%20encodes%20the%20pixel%20spatial%0Adisposition%2C%20the%20extra%20nodes%20account%20for%20the%20pixel%20color%20data.%20Applying%20the%0Aoriginal%20Normalized%20Cuts%20algorithm%20to%20this%20graph%20leads%20to%20a%20simple%20and%20scalable%0Amethod%20for%20spectral%20image%20segmentation%2C%20with%20an%20interpretable%20solution.%20Our%0Aexperiments%20also%20demonstrate%20that%20our%20proposed%20methodology%20over%20performs%20both%0Atraditional%20and%20modern%20unsupervised%20algorithms%20for%20segmentation%20in%20both%20real%0Aand%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13166v3&entry.124074799=Read"},
{"title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior", "author": "Tanvir Mahmud and Mustafa Munir and Radu Marculescu and Diana Marculescu", "abstract": "  Video-to-video synthesis models face significant challenges, such as ensuring\nconsistent character generation across frames, maintaining smooth temporal\ntransitions, and preserving quality during fast motion. The introduction of\njoint fully cross-frame self-attention mechanisms has improved character\nconsistency, but this comes at the cost of increased computational complexity.\nThis full cross-frame self-attention mechanism also incorporates redundant\ndetails and limits the number of frames that can be jointly edited due to its\ncomputational cost. Moreover, the lack of frames in cross-frame attention\nadversely affects temporal consistency and visual quality. To address these\nlimitations, we propose a new adaptive motion-guided cross-frame attention\nmechanism that drastically reduces complexity while preserving semantic details\nand temporal consistency. Specifically, we selectively incorporate the moving\nregions of successive frames in cross-frame attention and sparsely include\nstationary regions based on optical flow sampling. This technique allows for an\nincreased number of jointly edited frames without additional computational\noverhead. For longer duration of video editing, existing methods primarily\nfocus on frame interpolation or flow-warping from jointly edited keyframes,\nwhich often results in blurry frames or reduced temporal consistency. To\nimprove this, we introduce KV-caching of jointly edited frames and reuse the\nsame KV across all intermediate frames, significantly enhancing both\nintermediate frame quality and temporal consistency. Overall, our\nmotion-sampling method enables the use of around three times more keyframes\nthan existing joint editing methods while maintaining superior prediction\nquality. Ada-VE achieves up to 4x speed-up when using fully-extended\nself-attention across 40 frames for joint editing, without compromising visual\nquality or temporal consistency.\n", "link": "http://arxiv.org/abs/2406.04873v1", "date": "2024-06-07", "relevancy": 2.4065, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6296}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6084}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ada-VE%3A%20Training-Free%20Consistent%20Video%20Editing%20Using%20Adaptive%20Motion%0A%20%20Prior&body=Title%3A%20Ada-VE%3A%20Training-Free%20Consistent%20Video%20Editing%20Using%20Adaptive%20Motion%0A%20%20Prior%0AAuthor%3A%20Tanvir%20Mahmud%20and%20Mustafa%20Munir%20and%20Radu%20Marculescu%20and%20Diana%20Marculescu%0AAbstract%3A%20%20%20Video-to-video%20synthesis%20models%20face%20significant%20challenges%2C%20such%20as%20ensuring%0Aconsistent%20character%20generation%20across%20frames%2C%20maintaining%20smooth%20temporal%0Atransitions%2C%20and%20preserving%20quality%20during%20fast%20motion.%20The%20introduction%20of%0Ajoint%20fully%20cross-frame%20self-attention%20mechanisms%20has%20improved%20character%0Aconsistency%2C%20but%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity.%0AThis%20full%20cross-frame%20self-attention%20mechanism%20also%20incorporates%20redundant%0Adetails%20and%20limits%20the%20number%20of%20frames%20that%20can%20be%20jointly%20edited%20due%20to%20its%0Acomputational%20cost.%20Moreover%2C%20the%20lack%20of%20frames%20in%20cross-frame%20attention%0Aadversely%20affects%20temporal%20consistency%20and%20visual%20quality.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20new%20adaptive%20motion-guided%20cross-frame%20attention%0Amechanism%20that%20drastically%20reduces%20complexity%20while%20preserving%20semantic%20details%0Aand%20temporal%20consistency.%20Specifically%2C%20we%20selectively%20incorporate%20the%20moving%0Aregions%20of%20successive%20frames%20in%20cross-frame%20attention%20and%20sparsely%20include%0Astationary%20regions%20based%20on%20optical%20flow%20sampling.%20This%20technique%20allows%20for%20an%0Aincreased%20number%20of%20jointly%20edited%20frames%20without%20additional%20computational%0Aoverhead.%20For%20longer%20duration%20of%20video%20editing%2C%20existing%20methods%20primarily%0Afocus%20on%20frame%20interpolation%20or%20flow-warping%20from%20jointly%20edited%20keyframes%2C%0Awhich%20often%20results%20in%20blurry%20frames%20or%20reduced%20temporal%20consistency.%20To%0Aimprove%20this%2C%20we%20introduce%20KV-caching%20of%20jointly%20edited%20frames%20and%20reuse%20the%0Asame%20KV%20across%20all%20intermediate%20frames%2C%20significantly%20enhancing%20both%0Aintermediate%20frame%20quality%20and%20temporal%20consistency.%20Overall%2C%20our%0Amotion-sampling%20method%20enables%20the%20use%20of%20around%20three%20times%20more%20keyframes%0Athan%20existing%20joint%20editing%20methods%20while%20maintaining%20superior%20prediction%0Aquality.%20Ada-VE%20achieves%20up%20to%204x%20speed-up%20when%20using%20fully-extended%0Aself-attention%20across%2040%20frames%20for%20joint%20editing%2C%20without%20compromising%20visual%0Aquality%20or%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAda-VE%253A%2520Training-Free%2520Consistent%2520Video%2520Editing%2520Using%2520Adaptive%2520Motion%250A%2520%2520Prior%26entry.906535625%3DTanvir%2520Mahmud%2520and%2520Mustafa%2520Munir%2520and%2520Radu%2520Marculescu%2520and%2520Diana%2520Marculescu%26entry.1292438233%3D%2520%2520Video-to-video%2520synthesis%2520models%2520face%2520significant%2520challenges%252C%2520such%2520as%2520ensuring%250Aconsistent%2520character%2520generation%2520across%2520frames%252C%2520maintaining%2520smooth%2520temporal%250Atransitions%252C%2520and%2520preserving%2520quality%2520during%2520fast%2520motion.%2520The%2520introduction%2520of%250Ajoint%2520fully%2520cross-frame%2520self-attention%2520mechanisms%2520has%2520improved%2520character%250Aconsistency%252C%2520but%2520this%2520comes%2520at%2520the%2520cost%2520of%2520increased%2520computational%2520complexity.%250AThis%2520full%2520cross-frame%2520self-attention%2520mechanism%2520also%2520incorporates%2520redundant%250Adetails%2520and%2520limits%2520the%2520number%2520of%2520frames%2520that%2520can%2520be%2520jointly%2520edited%2520due%2520to%2520its%250Acomputational%2520cost.%2520Moreover%252C%2520the%2520lack%2520of%2520frames%2520in%2520cross-frame%2520attention%250Aadversely%2520affects%2520temporal%2520consistency%2520and%2520visual%2520quality.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520new%2520adaptive%2520motion-guided%2520cross-frame%2520attention%250Amechanism%2520that%2520drastically%2520reduces%2520complexity%2520while%2520preserving%2520semantic%2520details%250Aand%2520temporal%2520consistency.%2520Specifically%252C%2520we%2520selectively%2520incorporate%2520the%2520moving%250Aregions%2520of%2520successive%2520frames%2520in%2520cross-frame%2520attention%2520and%2520sparsely%2520include%250Astationary%2520regions%2520based%2520on%2520optical%2520flow%2520sampling.%2520This%2520technique%2520allows%2520for%2520an%250Aincreased%2520number%2520of%2520jointly%2520edited%2520frames%2520without%2520additional%2520computational%250Aoverhead.%2520For%2520longer%2520duration%2520of%2520video%2520editing%252C%2520existing%2520methods%2520primarily%250Afocus%2520on%2520frame%2520interpolation%2520or%2520flow-warping%2520from%2520jointly%2520edited%2520keyframes%252C%250Awhich%2520often%2520results%2520in%2520blurry%2520frames%2520or%2520reduced%2520temporal%2520consistency.%2520To%250Aimprove%2520this%252C%2520we%2520introduce%2520KV-caching%2520of%2520jointly%2520edited%2520frames%2520and%2520reuse%2520the%250Asame%2520KV%2520across%2520all%2520intermediate%2520frames%252C%2520significantly%2520enhancing%2520both%250Aintermediate%2520frame%2520quality%2520and%2520temporal%2520consistency.%2520Overall%252C%2520our%250Amotion-sampling%2520method%2520enables%2520the%2520use%2520of%2520around%2520three%2520times%2520more%2520keyframes%250Athan%2520existing%2520joint%2520editing%2520methods%2520while%2520maintaining%2520superior%2520prediction%250Aquality.%2520Ada-VE%2520achieves%2520up%2520to%25204x%2520speed-up%2520when%2520using%2520fully-extended%250Aself-attention%2520across%252040%2520frames%2520for%2520joint%2520editing%252C%2520without%2520compromising%2520visual%250Aquality%2520or%2520temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ada-VE%3A%20Training-Free%20Consistent%20Video%20Editing%20Using%20Adaptive%20Motion%0A%20%20Prior&entry.906535625=Tanvir%20Mahmud%20and%20Mustafa%20Munir%20and%20Radu%20Marculescu%20and%20Diana%20Marculescu&entry.1292438233=%20%20Video-to-video%20synthesis%20models%20face%20significant%20challenges%2C%20such%20as%20ensuring%0Aconsistent%20character%20generation%20across%20frames%2C%20maintaining%20smooth%20temporal%0Atransitions%2C%20and%20preserving%20quality%20during%20fast%20motion.%20The%20introduction%20of%0Ajoint%20fully%20cross-frame%20self-attention%20mechanisms%20has%20improved%20character%0Aconsistency%2C%20but%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity.%0AThis%20full%20cross-frame%20self-attention%20mechanism%20also%20incorporates%20redundant%0Adetails%20and%20limits%20the%20number%20of%20frames%20that%20can%20be%20jointly%20edited%20due%20to%20its%0Acomputational%20cost.%20Moreover%2C%20the%20lack%20of%20frames%20in%20cross-frame%20attention%0Aadversely%20affects%20temporal%20consistency%20and%20visual%20quality.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20new%20adaptive%20motion-guided%20cross-frame%20attention%0Amechanism%20that%20drastically%20reduces%20complexity%20while%20preserving%20semantic%20details%0Aand%20temporal%20consistency.%20Specifically%2C%20we%20selectively%20incorporate%20the%20moving%0Aregions%20of%20successive%20frames%20in%20cross-frame%20attention%20and%20sparsely%20include%0Astationary%20regions%20based%20on%20optical%20flow%20sampling.%20This%20technique%20allows%20for%20an%0Aincreased%20number%20of%20jointly%20edited%20frames%20without%20additional%20computational%0Aoverhead.%20For%20longer%20duration%20of%20video%20editing%2C%20existing%20methods%20primarily%0Afocus%20on%20frame%20interpolation%20or%20flow-warping%20from%20jointly%20edited%20keyframes%2C%0Awhich%20often%20results%20in%20blurry%20frames%20or%20reduced%20temporal%20consistency.%20To%0Aimprove%20this%2C%20we%20introduce%20KV-caching%20of%20jointly%20edited%20frames%20and%20reuse%20the%0Asame%20KV%20across%20all%20intermediate%20frames%2C%20significantly%20enhancing%20both%0Aintermediate%20frame%20quality%20and%20temporal%20consistency.%20Overall%2C%20our%0Amotion-sampling%20method%20enables%20the%20use%20of%20around%20three%20times%20more%20keyframes%0Athan%20existing%20joint%20editing%20methods%20while%20maintaining%20superior%20prediction%0Aquality.%20Ada-VE%20achieves%20up%20to%204x%20speed-up%20when%20using%20fully-extended%0Aself-attention%20across%2040%20frames%20for%20joint%20editing%2C%20without%20compromising%20visual%0Aquality%20or%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04873v1&entry.124074799=Read"},
{"title": "The Expanding Scope of the Stability Gap: Unveiling its Presence in\n  Joint Incremental Learning of Homogeneous Tasks", "author": "Sandesh Kamath and Albin Soutif-Cormerais and Joost van de Weijer and Bogdan Raducanu", "abstract": "  Recent research identified a temporary performance drop on previously learned\ntasks when transitioning to a new one. This drop is called the stability gap\nand has great consequences for continual learning: it complicates the direct\nemployment of continually learning since the worse-case performance at\ntask-boundaries is dramatic, it limits its potential as an energy-efficient\ntraining paradigm, and finally, the stability drop could result in a reduced\nfinal performance of the algorithm. In this paper, we show that the stability\ngap also occurs when applying joint incremental training of homogeneous tasks.\nIn this scenario, the learner continues training on the same data distribution\nand has access to all data from previous tasks. In addition, we show that in\nthis scenario, there exists a low-loss linear path to the next minima, but that\nSGD optimization does not choose this path. We perform further analysis\nincluding a finer batch-wise analysis which could provide insights towards\npotential solution directions.\n", "link": "http://arxiv.org/abs/2406.05114v1", "date": "2024-06-07", "relevancy": 2.4047, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5022}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4706}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Expanding%20Scope%20of%20the%20Stability%20Gap%3A%20Unveiling%20its%20Presence%20in%0A%20%20Joint%20Incremental%20Learning%20of%20Homogeneous%20Tasks&body=Title%3A%20The%20Expanding%20Scope%20of%20the%20Stability%20Gap%3A%20Unveiling%20its%20Presence%20in%0A%20%20Joint%20Incremental%20Learning%20of%20Homogeneous%20Tasks%0AAuthor%3A%20Sandesh%20Kamath%20and%20Albin%20Soutif-Cormerais%20and%20Joost%20van%20de%20Weijer%20and%20Bogdan%20Raducanu%0AAbstract%3A%20%20%20Recent%20research%20identified%20a%20temporary%20performance%20drop%20on%20previously%20learned%0Atasks%20when%20transitioning%20to%20a%20new%20one.%20This%20drop%20is%20called%20the%20stability%20gap%0Aand%20has%20great%20consequences%20for%20continual%20learning%3A%20it%20complicates%20the%20direct%0Aemployment%20of%20continually%20learning%20since%20the%20worse-case%20performance%20at%0Atask-boundaries%20is%20dramatic%2C%20it%20limits%20its%20potential%20as%20an%20energy-efficient%0Atraining%20paradigm%2C%20and%20finally%2C%20the%20stability%20drop%20could%20result%20in%20a%20reduced%0Afinal%20performance%20of%20the%20algorithm.%20In%20this%20paper%2C%20we%20show%20that%20the%20stability%0Agap%20also%20occurs%20when%20applying%20joint%20incremental%20training%20of%20homogeneous%20tasks.%0AIn%20this%20scenario%2C%20the%20learner%20continues%20training%20on%20the%20same%20data%20distribution%0Aand%20has%20access%20to%20all%20data%20from%20previous%20tasks.%20In%20addition%2C%20we%20show%20that%20in%0Athis%20scenario%2C%20there%20exists%20a%20low-loss%20linear%20path%20to%20the%20next%20minima%2C%20but%20that%0ASGD%20optimization%20does%20not%20choose%20this%20path.%20We%20perform%20further%20analysis%0Aincluding%20a%20finer%20batch-wise%20analysis%20which%20could%20provide%20insights%20towards%0Apotential%20solution%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Expanding%2520Scope%2520of%2520the%2520Stability%2520Gap%253A%2520Unveiling%2520its%2520Presence%2520in%250A%2520%2520Joint%2520Incremental%2520Learning%2520of%2520Homogeneous%2520Tasks%26entry.906535625%3DSandesh%2520Kamath%2520and%2520Albin%2520Soutif-Cormerais%2520and%2520Joost%2520van%2520de%2520Weijer%2520and%2520Bogdan%2520Raducanu%26entry.1292438233%3D%2520%2520Recent%2520research%2520identified%2520a%2520temporary%2520performance%2520drop%2520on%2520previously%2520learned%250Atasks%2520when%2520transitioning%2520to%2520a%2520new%2520one.%2520This%2520drop%2520is%2520called%2520the%2520stability%2520gap%250Aand%2520has%2520great%2520consequences%2520for%2520continual%2520learning%253A%2520it%2520complicates%2520the%2520direct%250Aemployment%2520of%2520continually%2520learning%2520since%2520the%2520worse-case%2520performance%2520at%250Atask-boundaries%2520is%2520dramatic%252C%2520it%2520limits%2520its%2520potential%2520as%2520an%2520energy-efficient%250Atraining%2520paradigm%252C%2520and%2520finally%252C%2520the%2520stability%2520drop%2520could%2520result%2520in%2520a%2520reduced%250Afinal%2520performance%2520of%2520the%2520algorithm.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520stability%250Agap%2520also%2520occurs%2520when%2520applying%2520joint%2520incremental%2520training%2520of%2520homogeneous%2520tasks.%250AIn%2520this%2520scenario%252C%2520the%2520learner%2520continues%2520training%2520on%2520the%2520same%2520data%2520distribution%250Aand%2520has%2520access%2520to%2520all%2520data%2520from%2520previous%2520tasks.%2520In%2520addition%252C%2520we%2520show%2520that%2520in%250Athis%2520scenario%252C%2520there%2520exists%2520a%2520low-loss%2520linear%2520path%2520to%2520the%2520next%2520minima%252C%2520but%2520that%250ASGD%2520optimization%2520does%2520not%2520choose%2520this%2520path.%2520We%2520perform%2520further%2520analysis%250Aincluding%2520a%2520finer%2520batch-wise%2520analysis%2520which%2520could%2520provide%2520insights%2520towards%250Apotential%2520solution%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Expanding%20Scope%20of%20the%20Stability%20Gap%3A%20Unveiling%20its%20Presence%20in%0A%20%20Joint%20Incremental%20Learning%20of%20Homogeneous%20Tasks&entry.906535625=Sandesh%20Kamath%20and%20Albin%20Soutif-Cormerais%20and%20Joost%20van%20de%20Weijer%20and%20Bogdan%20Raducanu&entry.1292438233=%20%20Recent%20research%20identified%20a%20temporary%20performance%20drop%20on%20previously%20learned%0Atasks%20when%20transitioning%20to%20a%20new%20one.%20This%20drop%20is%20called%20the%20stability%20gap%0Aand%20has%20great%20consequences%20for%20continual%20learning%3A%20it%20complicates%20the%20direct%0Aemployment%20of%20continually%20learning%20since%20the%20worse-case%20performance%20at%0Atask-boundaries%20is%20dramatic%2C%20it%20limits%20its%20potential%20as%20an%20energy-efficient%0Atraining%20paradigm%2C%20and%20finally%2C%20the%20stability%20drop%20could%20result%20in%20a%20reduced%0Afinal%20performance%20of%20the%20algorithm.%20In%20this%20paper%2C%20we%20show%20that%20the%20stability%0Agap%20also%20occurs%20when%20applying%20joint%20incremental%20training%20of%20homogeneous%20tasks.%0AIn%20this%20scenario%2C%20the%20learner%20continues%20training%20on%20the%20same%20data%20distribution%0Aand%20has%20access%20to%20all%20data%20from%20previous%20tasks.%20In%20addition%2C%20we%20show%20that%20in%0Athis%20scenario%2C%20there%20exists%20a%20low-loss%20linear%20path%20to%20the%20next%20minima%2C%20but%20that%0ASGD%20optimization%20does%20not%20choose%20this%20path.%20We%20perform%20further%20analysis%0Aincluding%20a%20finer%20batch-wise%20analysis%20which%20could%20provide%20insights%20towards%0Apotential%20solution%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05114v1&entry.124074799=Read"},
{"title": "Robotic in-hand manipulation with relaxed optimization", "author": "Ali Hammoud and Valerio Belcamino and Quentin Huet and Alessandro Carf\u00ec and Mahdi Khoramshahi and Veronique Perdereau and Fulvio Mastrogiovanni", "abstract": "  Dexterous in-hand manipulation is a unique and valuable human skill requiring\nsophisticated sensorimotor interaction with the environment while respecting\nstability constraints. Satisfying these constraints with generated motions is\nessential for a robotic platform to achieve reliable in-hand manipulation\nskills. Explicitly modelling these constraints can be challenging, but they can\nbe implicitly modelled and learned through experience or human demonstrations.\nWe propose a learning and control approach based on dictionaries of motion\nprimitives generated from human demonstrations. To achieve this, we defined an\noptimization process that combines motion primitives to generate robot\nfingertip trajectories for moving an object from an initial to a desired final\npose. Based on our experiments, our approach allows a robotic hand to handle\nobjects like humans, adhering to stability constraints without requiring\nexplicit formalization. In other words, the proposed motion primitive\ndictionaries learn and implicitly embed the constraints crucial to the in-hand\nmanipulation task.\n", "link": "http://arxiv.org/abs/2406.04950v1", "date": "2024-06-07", "relevancy": 2.3952, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6299}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5961}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20in-hand%20manipulation%20with%20relaxed%20optimization&body=Title%3A%20Robotic%20in-hand%20manipulation%20with%20relaxed%20optimization%0AAuthor%3A%20Ali%20Hammoud%20and%20Valerio%20Belcamino%20and%20Quentin%20Huet%20and%20Alessandro%20Carf%C3%AC%20and%20Mahdi%20Khoramshahi%20and%20Veronique%20Perdereau%20and%20Fulvio%20Mastrogiovanni%0AAbstract%3A%20%20%20Dexterous%20in-hand%20manipulation%20is%20a%20unique%20and%20valuable%20human%20skill%20requiring%0Asophisticated%20sensorimotor%20interaction%20with%20the%20environment%20while%20respecting%0Astability%20constraints.%20Satisfying%20these%20constraints%20with%20generated%20motions%20is%0Aessential%20for%20a%20robotic%20platform%20to%20achieve%20reliable%20in-hand%20manipulation%0Askills.%20Explicitly%20modelling%20these%20constraints%20can%20be%20challenging%2C%20but%20they%20can%0Abe%20implicitly%20modelled%20and%20learned%20through%20experience%20or%20human%20demonstrations.%0AWe%20propose%20a%20learning%20and%20control%20approach%20based%20on%20dictionaries%20of%20motion%0Aprimitives%20generated%20from%20human%20demonstrations.%20To%20achieve%20this%2C%20we%20defined%20an%0Aoptimization%20process%20that%20combines%20motion%20primitives%20to%20generate%20robot%0Afingertip%20trajectories%20for%20moving%20an%20object%20from%20an%20initial%20to%20a%20desired%20final%0Apose.%20Based%20on%20our%20experiments%2C%20our%20approach%20allows%20a%20robotic%20hand%20to%20handle%0Aobjects%20like%20humans%2C%20adhering%20to%20stability%20constraints%20without%20requiring%0Aexplicit%20formalization.%20In%20other%20words%2C%20the%20proposed%20motion%20primitive%0Adictionaries%20learn%20and%20implicitly%20embed%20the%20constraints%20crucial%20to%20the%20in-hand%0Amanipulation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520in-hand%2520manipulation%2520with%2520relaxed%2520optimization%26entry.906535625%3DAli%2520Hammoud%2520and%2520Valerio%2520Belcamino%2520and%2520Quentin%2520Huet%2520and%2520Alessandro%2520Carf%25C3%25AC%2520and%2520Mahdi%2520Khoramshahi%2520and%2520Veronique%2520Perdereau%2520and%2520Fulvio%2520Mastrogiovanni%26entry.1292438233%3D%2520%2520Dexterous%2520in-hand%2520manipulation%2520is%2520a%2520unique%2520and%2520valuable%2520human%2520skill%2520requiring%250Asophisticated%2520sensorimotor%2520interaction%2520with%2520the%2520environment%2520while%2520respecting%250Astability%2520constraints.%2520Satisfying%2520these%2520constraints%2520with%2520generated%2520motions%2520is%250Aessential%2520for%2520a%2520robotic%2520platform%2520to%2520achieve%2520reliable%2520in-hand%2520manipulation%250Askills.%2520Explicitly%2520modelling%2520these%2520constraints%2520can%2520be%2520challenging%252C%2520but%2520they%2520can%250Abe%2520implicitly%2520modelled%2520and%2520learned%2520through%2520experience%2520or%2520human%2520demonstrations.%250AWe%2520propose%2520a%2520learning%2520and%2520control%2520approach%2520based%2520on%2520dictionaries%2520of%2520motion%250Aprimitives%2520generated%2520from%2520human%2520demonstrations.%2520To%2520achieve%2520this%252C%2520we%2520defined%2520an%250Aoptimization%2520process%2520that%2520combines%2520motion%2520primitives%2520to%2520generate%2520robot%250Afingertip%2520trajectories%2520for%2520moving%2520an%2520object%2520from%2520an%2520initial%2520to%2520a%2520desired%2520final%250Apose.%2520Based%2520on%2520our%2520experiments%252C%2520our%2520approach%2520allows%2520a%2520robotic%2520hand%2520to%2520handle%250Aobjects%2520like%2520humans%252C%2520adhering%2520to%2520stability%2520constraints%2520without%2520requiring%250Aexplicit%2520formalization.%2520In%2520other%2520words%252C%2520the%2520proposed%2520motion%2520primitive%250Adictionaries%2520learn%2520and%2520implicitly%2520embed%2520the%2520constraints%2520crucial%2520to%2520the%2520in-hand%250Amanipulation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20in-hand%20manipulation%20with%20relaxed%20optimization&entry.906535625=Ali%20Hammoud%20and%20Valerio%20Belcamino%20and%20Quentin%20Huet%20and%20Alessandro%20Carf%C3%AC%20and%20Mahdi%20Khoramshahi%20and%20Veronique%20Perdereau%20and%20Fulvio%20Mastrogiovanni&entry.1292438233=%20%20Dexterous%20in-hand%20manipulation%20is%20a%20unique%20and%20valuable%20human%20skill%20requiring%0Asophisticated%20sensorimotor%20interaction%20with%20the%20environment%20while%20respecting%0Astability%20constraints.%20Satisfying%20these%20constraints%20with%20generated%20motions%20is%0Aessential%20for%20a%20robotic%20platform%20to%20achieve%20reliable%20in-hand%20manipulation%0Askills.%20Explicitly%20modelling%20these%20constraints%20can%20be%20challenging%2C%20but%20they%20can%0Abe%20implicitly%20modelled%20and%20learned%20through%20experience%20or%20human%20demonstrations.%0AWe%20propose%20a%20learning%20and%20control%20approach%20based%20on%20dictionaries%20of%20motion%0Aprimitives%20generated%20from%20human%20demonstrations.%20To%20achieve%20this%2C%20we%20defined%20an%0Aoptimization%20process%20that%20combines%20motion%20primitives%20to%20generate%20robot%0Afingertip%20trajectories%20for%20moving%20an%20object%20from%20an%20initial%20to%20a%20desired%20final%0Apose.%20Based%20on%20our%20experiments%2C%20our%20approach%20allows%20a%20robotic%20hand%20to%20handle%0Aobjects%20like%20humans%2C%20adhering%20to%20stability%20constraints%20without%20requiring%0Aexplicit%20formalization.%20In%20other%20words%2C%20the%20proposed%20motion%20primitive%0Adictionaries%20learn%20and%20implicitly%20embed%20the%20constraints%20crucial%20to%20the%20in-hand%0Amanipulation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04950v1&entry.124074799=Read"},
{"title": "Multi-Granularity Language-Guided Multi-Object Tracking", "author": "Yuhao Li and Muzammal Naseer and Jiale Cao and Yu Zhu and Jinqiu Sun and Yanning Zhang and Fahad Shahbaz Khan", "abstract": "  Most existing multi-object tracking methods typically learn visual tracking\nfeatures via maximizing dis-similarities of different instances and minimizing\nsimilarities of the same instance. While such a feature learning scheme\nachieves promising performance, learning discriminative features solely based\non visual information is challenging especially in case of environmental\ninterference such as occlusion, blur and domain variance. In this work, we\nargue that multi-modal language-driven features provide complementary\ninformation to classical visual features, thereby aiding in improving the\nrobustness to such environmental interference. To this end, we propose a new\nmulti-object tracking framework, named LG-MOT, that explicitly leverages\nlanguage information at different levels of granularity (scene-and\ninstance-level) and combines it with standard visual features to obtain\ndiscriminative representations. To develop LG-MOT, we annotate existing MOT\ndatasets with scene-and instance-level language descriptions. We then encode\nboth instance-and scene-level language information into high-dimensional\nembeddings, which are utilized to guide the visual features during training. At\ninference, our LG-MOT uses the standard visual features without relying on\nannotated language descriptions. Extensive experiments on three benchmarks,\nMOT17, DanceTrack and SportsMOT, reveal the merits of the proposed\ncontributions leading to state-of-the-art performance. On the DanceTrack test\nset, our LG-MOT achieves an absolute gain of 2.2\\% in terms of target object\nassociation (IDF1 score), compared to the baseline using only visual features.\nFurther, our LG-MOT exhibits strong cross-domain generalizability. The dataset\nand code will be available at ~\\url{https://github.com/WesLee88524/LG-MOT}.\n", "link": "http://arxiv.org/abs/2406.04844v1", "date": "2024-06-07", "relevancy": 2.3893, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6051}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5987}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Granularity%20Language-Guided%20Multi-Object%20Tracking&body=Title%3A%20Multi-Granularity%20Language-Guided%20Multi-Object%20Tracking%0AAuthor%3A%20Yuhao%20Li%20and%20Muzammal%20Naseer%20and%20Jiale%20Cao%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Most%20existing%20multi-object%20tracking%20methods%20typically%20learn%20visual%20tracking%0Afeatures%20via%20maximizing%20dis-similarities%20of%20different%20instances%20and%20minimizing%0Asimilarities%20of%20the%20same%20instance.%20While%20such%20a%20feature%20learning%20scheme%0Aachieves%20promising%20performance%2C%20learning%20discriminative%20features%20solely%20based%0Aon%20visual%20information%20is%20challenging%20especially%20in%20case%20of%20environmental%0Ainterference%20such%20as%20occlusion%2C%20blur%20and%20domain%20variance.%20In%20this%20work%2C%20we%0Aargue%20that%20multi-modal%20language-driven%20features%20provide%20complementary%0Ainformation%20to%20classical%20visual%20features%2C%20thereby%20aiding%20in%20improving%20the%0Arobustness%20to%20such%20environmental%20interference.%20To%20this%20end%2C%20we%20propose%20a%20new%0Amulti-object%20tracking%20framework%2C%20named%20LG-MOT%2C%20that%20explicitly%20leverages%0Alanguage%20information%20at%20different%20levels%20of%20granularity%20%28scene-and%0Ainstance-level%29%20and%20combines%20it%20with%20standard%20visual%20features%20to%20obtain%0Adiscriminative%20representations.%20To%20develop%20LG-MOT%2C%20we%20annotate%20existing%20MOT%0Adatasets%20with%20scene-and%20instance-level%20language%20descriptions.%20We%20then%20encode%0Aboth%20instance-and%20scene-level%20language%20information%20into%20high-dimensional%0Aembeddings%2C%20which%20are%20utilized%20to%20guide%20the%20visual%20features%20during%20training.%20At%0Ainference%2C%20our%20LG-MOT%20uses%20the%20standard%20visual%20features%20without%20relying%20on%0Aannotated%20language%20descriptions.%20Extensive%20experiments%20on%20three%20benchmarks%2C%0AMOT17%2C%20DanceTrack%20and%20SportsMOT%2C%20reveal%20the%20merits%20of%20the%20proposed%0Acontributions%20leading%20to%20state-of-the-art%20performance.%20On%20the%20DanceTrack%20test%0Aset%2C%20our%20LG-MOT%20achieves%20an%20absolute%20gain%20of%202.2%5C%25%20in%20terms%20of%20target%20object%0Aassociation%20%28IDF1%20score%29%2C%20compared%20to%20the%20baseline%20using%20only%20visual%20features.%0AFurther%2C%20our%20LG-MOT%20exhibits%20strong%20cross-domain%20generalizability.%20The%20dataset%0Aand%20code%20will%20be%20available%20at%20~%5Curl%7Bhttps%3A//github.com/WesLee88524/LG-MOT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Granularity%2520Language-Guided%2520Multi-Object%2520Tracking%26entry.906535625%3DYuhao%2520Li%2520and%2520Muzammal%2520Naseer%2520and%2520Jiale%2520Cao%2520and%2520Yu%2520Zhu%2520and%2520Jinqiu%2520Sun%2520and%2520Yanning%2520Zhang%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Most%2520existing%2520multi-object%2520tracking%2520methods%2520typically%2520learn%2520visual%2520tracking%250Afeatures%2520via%2520maximizing%2520dis-similarities%2520of%2520different%2520instances%2520and%2520minimizing%250Asimilarities%2520of%2520the%2520same%2520instance.%2520While%2520such%2520a%2520feature%2520learning%2520scheme%250Aachieves%2520promising%2520performance%252C%2520learning%2520discriminative%2520features%2520solely%2520based%250Aon%2520visual%2520information%2520is%2520challenging%2520especially%2520in%2520case%2520of%2520environmental%250Ainterference%2520such%2520as%2520occlusion%252C%2520blur%2520and%2520domain%2520variance.%2520In%2520this%2520work%252C%2520we%250Aargue%2520that%2520multi-modal%2520language-driven%2520features%2520provide%2520complementary%250Ainformation%2520to%2520classical%2520visual%2520features%252C%2520thereby%2520aiding%2520in%2520improving%2520the%250Arobustness%2520to%2520such%2520environmental%2520interference.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%250Amulti-object%2520tracking%2520framework%252C%2520named%2520LG-MOT%252C%2520that%2520explicitly%2520leverages%250Alanguage%2520information%2520at%2520different%2520levels%2520of%2520granularity%2520%2528scene-and%250Ainstance-level%2529%2520and%2520combines%2520it%2520with%2520standard%2520visual%2520features%2520to%2520obtain%250Adiscriminative%2520representations.%2520To%2520develop%2520LG-MOT%252C%2520we%2520annotate%2520existing%2520MOT%250Adatasets%2520with%2520scene-and%2520instance-level%2520language%2520descriptions.%2520We%2520then%2520encode%250Aboth%2520instance-and%2520scene-level%2520language%2520information%2520into%2520high-dimensional%250Aembeddings%252C%2520which%2520are%2520utilized%2520to%2520guide%2520the%2520visual%2520features%2520during%2520training.%2520At%250Ainference%252C%2520our%2520LG-MOT%2520uses%2520the%2520standard%2520visual%2520features%2520without%2520relying%2520on%250Aannotated%2520language%2520descriptions.%2520Extensive%2520experiments%2520on%2520three%2520benchmarks%252C%250AMOT17%252C%2520DanceTrack%2520and%2520SportsMOT%252C%2520reveal%2520the%2520merits%2520of%2520the%2520proposed%250Acontributions%2520leading%2520to%2520state-of-the-art%2520performance.%2520On%2520the%2520DanceTrack%2520test%250Aset%252C%2520our%2520LG-MOT%2520achieves%2520an%2520absolute%2520gain%2520of%25202.2%255C%2525%2520in%2520terms%2520of%2520target%2520object%250Aassociation%2520%2528IDF1%2520score%2529%252C%2520compared%2520to%2520the%2520baseline%2520using%2520only%2520visual%2520features.%250AFurther%252C%2520our%2520LG-MOT%2520exhibits%2520strong%2520cross-domain%2520generalizability.%2520The%2520dataset%250Aand%2520code%2520will%2520be%2520available%2520at%2520~%255Curl%257Bhttps%253A//github.com/WesLee88524/LG-MOT%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Granularity%20Language-Guided%20Multi-Object%20Tracking&entry.906535625=Yuhao%20Li%20and%20Muzammal%20Naseer%20and%20Jiale%20Cao%20and%20Yu%20Zhu%20and%20Jinqiu%20Sun%20and%20Yanning%20Zhang%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Most%20existing%20multi-object%20tracking%20methods%20typically%20learn%20visual%20tracking%0Afeatures%20via%20maximizing%20dis-similarities%20of%20different%20instances%20and%20minimizing%0Asimilarities%20of%20the%20same%20instance.%20While%20such%20a%20feature%20learning%20scheme%0Aachieves%20promising%20performance%2C%20learning%20discriminative%20features%20solely%20based%0Aon%20visual%20information%20is%20challenging%20especially%20in%20case%20of%20environmental%0Ainterference%20such%20as%20occlusion%2C%20blur%20and%20domain%20variance.%20In%20this%20work%2C%20we%0Aargue%20that%20multi-modal%20language-driven%20features%20provide%20complementary%0Ainformation%20to%20classical%20visual%20features%2C%20thereby%20aiding%20in%20improving%20the%0Arobustness%20to%20such%20environmental%20interference.%20To%20this%20end%2C%20we%20propose%20a%20new%0Amulti-object%20tracking%20framework%2C%20named%20LG-MOT%2C%20that%20explicitly%20leverages%0Alanguage%20information%20at%20different%20levels%20of%20granularity%20%28scene-and%0Ainstance-level%29%20and%20combines%20it%20with%20standard%20visual%20features%20to%20obtain%0Adiscriminative%20representations.%20To%20develop%20LG-MOT%2C%20we%20annotate%20existing%20MOT%0Adatasets%20with%20scene-and%20instance-level%20language%20descriptions.%20We%20then%20encode%0Aboth%20instance-and%20scene-level%20language%20information%20into%20high-dimensional%0Aembeddings%2C%20which%20are%20utilized%20to%20guide%20the%20visual%20features%20during%20training.%20At%0Ainference%2C%20our%20LG-MOT%20uses%20the%20standard%20visual%20features%20without%20relying%20on%0Aannotated%20language%20descriptions.%20Extensive%20experiments%20on%20three%20benchmarks%2C%0AMOT17%2C%20DanceTrack%20and%20SportsMOT%2C%20reveal%20the%20merits%20of%20the%20proposed%0Acontributions%20leading%20to%20state-of-the-art%20performance.%20On%20the%20DanceTrack%20test%0Aset%2C%20our%20LG-MOT%20achieves%20an%20absolute%20gain%20of%202.2%5C%25%20in%20terms%20of%20target%20object%0Aassociation%20%28IDF1%20score%29%2C%20compared%20to%20the%20baseline%20using%20only%20visual%20features.%0AFurther%2C%20our%20LG-MOT%20exhibits%20strong%20cross-domain%20generalizability.%20The%20dataset%0Aand%20code%20will%20be%20available%20at%20~%5Curl%7Bhttps%3A//github.com/WesLee88524/LG-MOT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04844v1&entry.124074799=Read"},
{"title": "GENIE: Watermarking Graph Neural Networks for Link Prediction", "author": "Venkata Sai Pranav Bachina and Ankit Gangwal and Aaryan Ajay Sharma and Charu Sharma", "abstract": "  Graph Neural Networks (GNNs) have advanced the field of machine learning by\nutilizing graph-structured data, which is ubiquitous in the real world. GNNs\nhave applications in various fields, ranging from social network analysis to\ndrug discovery. GNN training is strenuous, requiring significant computational\nresources and human expertise. It makes a trained GNN an indispensable\nIntellectual Property (IP) for its owner. Recent studies have shown GNNs to be\nvulnerable to model-stealing attacks, which raises concerns over IP rights\nprotection. Watermarking has been shown to be effective at protecting the IP of\na GNN model. Existing efforts to develop a watermarking scheme for GNNs have\nonly focused on the node classification and the graph classification tasks.\n  To the best of our knowledge, we introduce the first-ever watermarking scheme\nfor GNNs tailored to the Link Prediction (LP) task. We call our proposed\nwatermarking scheme GENIE (watermarking Graph nEural Networks for lInk\nprEdiction). We design GENIE using a novel backdoor attack to create a trigger\nset for two key methods of LP: (1) node representation-based and (2)\nsubgraph-based. In GENIE, the watermark is embedded into the GNN model by\ntraining it on both the trigger set and a modified training set, resulting in a\nwatermarked GNN model. To assess a suspect model, we verify the watermark\nagainst the trigger set. We extensively evaluate GENIE across 3 model\narchitectures (i.e., SEAL, GCN, and GraphSAGE) and 7 real-world datasets.\nFurthermore, we validate the robustness of GENIE against 11 state-of-the-art\nwatermark removal techniques and 3 model extraction attacks. We also\ndemonstrate that GENIE is robust against ownership piracy attack. Our ownership\ndemonstration scheme statistically guarantees both False Positive Rate (FPR)\nand False Negative Rate (FNR) to be less than $10^{-6}$.\n", "link": "http://arxiv.org/abs/2406.04805v1", "date": "2024-06-07", "relevancy": 2.3761, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5265}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENIE%3A%20Watermarking%20Graph%20Neural%20Networks%20for%20Link%20Prediction&body=Title%3A%20GENIE%3A%20Watermarking%20Graph%20Neural%20Networks%20for%20Link%20Prediction%0AAuthor%3A%20Venkata%20Sai%20Pranav%20Bachina%20and%20Ankit%20Gangwal%20and%20Aaryan%20Ajay%20Sharma%20and%20Charu%20Sharma%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20advanced%20the%20field%20of%20machine%20learning%20by%0Autilizing%20graph-structured%20data%2C%20which%20is%20ubiquitous%20in%20the%20real%20world.%20GNNs%0Ahave%20applications%20in%20various%20fields%2C%20ranging%20from%20social%20network%20analysis%20to%0Adrug%20discovery.%20GNN%20training%20is%20strenuous%2C%20requiring%20significant%20computational%0Aresources%20and%20human%20expertise.%20It%20makes%20a%20trained%20GNN%20an%20indispensable%0AIntellectual%20Property%20%28IP%29%20for%20its%20owner.%20Recent%20studies%20have%20shown%20GNNs%20to%20be%0Avulnerable%20to%20model-stealing%20attacks%2C%20which%20raises%20concerns%20over%20IP%20rights%0Aprotection.%20Watermarking%20has%20been%20shown%20to%20be%20effective%20at%20protecting%20the%20IP%20of%0Aa%20GNN%20model.%20Existing%20efforts%20to%20develop%20a%20watermarking%20scheme%20for%20GNNs%20have%0Aonly%20focused%20on%20the%20node%20classification%20and%20the%20graph%20classification%20tasks.%0A%20%20To%20the%20best%20of%20our%20knowledge%2C%20we%20introduce%20the%20first-ever%20watermarking%20scheme%0Afor%20GNNs%20tailored%20to%20the%20Link%20Prediction%20%28LP%29%20task.%20We%20call%20our%20proposed%0Awatermarking%20scheme%20GENIE%20%28watermarking%20Graph%20nEural%20Networks%20for%20lInk%0AprEdiction%29.%20We%20design%20GENIE%20using%20a%20novel%20backdoor%20attack%20to%20create%20a%20trigger%0Aset%20for%20two%20key%20methods%20of%20LP%3A%20%281%29%20node%20representation-based%20and%20%282%29%0Asubgraph-based.%20In%20GENIE%2C%20the%20watermark%20is%20embedded%20into%20the%20GNN%20model%20by%0Atraining%20it%20on%20both%20the%20trigger%20set%20and%20a%20modified%20training%20set%2C%20resulting%20in%20a%0Awatermarked%20GNN%20model.%20To%20assess%20a%20suspect%20model%2C%20we%20verify%20the%20watermark%0Aagainst%20the%20trigger%20set.%20We%20extensively%20evaluate%20GENIE%20across%203%20model%0Aarchitectures%20%28i.e.%2C%20SEAL%2C%20GCN%2C%20and%20GraphSAGE%29%20and%207%20real-world%20datasets.%0AFurthermore%2C%20we%20validate%20the%20robustness%20of%20GENIE%20against%2011%20state-of-the-art%0Awatermark%20removal%20techniques%20and%203%20model%20extraction%20attacks.%20We%20also%0Ademonstrate%20that%20GENIE%20is%20robust%20against%20ownership%20piracy%20attack.%20Our%20ownership%0Ademonstration%20scheme%20statistically%20guarantees%20both%20False%20Positive%20Rate%20%28FPR%29%0Aand%20False%20Negative%20Rate%20%28FNR%29%20to%20be%20less%20than%20%2410%5E%7B-6%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENIE%253A%2520Watermarking%2520Graph%2520Neural%2520Networks%2520for%2520Link%2520Prediction%26entry.906535625%3DVenkata%2520Sai%2520Pranav%2520Bachina%2520and%2520Ankit%2520Gangwal%2520and%2520Aaryan%2520Ajay%2520Sharma%2520and%2520Charu%2520Sharma%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520advanced%2520the%2520field%2520of%2520machine%2520learning%2520by%250Autilizing%2520graph-structured%2520data%252C%2520which%2520is%2520ubiquitous%2520in%2520the%2520real%2520world.%2520GNNs%250Ahave%2520applications%2520in%2520various%2520fields%252C%2520ranging%2520from%2520social%2520network%2520analysis%2520to%250Adrug%2520discovery.%2520GNN%2520training%2520is%2520strenuous%252C%2520requiring%2520significant%2520computational%250Aresources%2520and%2520human%2520expertise.%2520It%2520makes%2520a%2520trained%2520GNN%2520an%2520indispensable%250AIntellectual%2520Property%2520%2528IP%2529%2520for%2520its%2520owner.%2520Recent%2520studies%2520have%2520shown%2520GNNs%2520to%2520be%250Avulnerable%2520to%2520model-stealing%2520attacks%252C%2520which%2520raises%2520concerns%2520over%2520IP%2520rights%250Aprotection.%2520Watermarking%2520has%2520been%2520shown%2520to%2520be%2520effective%2520at%2520protecting%2520the%2520IP%2520of%250Aa%2520GNN%2520model.%2520Existing%2520efforts%2520to%2520develop%2520a%2520watermarking%2520scheme%2520for%2520GNNs%2520have%250Aonly%2520focused%2520on%2520the%2520node%2520classification%2520and%2520the%2520graph%2520classification%2520tasks.%250A%2520%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520introduce%2520the%2520first-ever%2520watermarking%2520scheme%250Afor%2520GNNs%2520tailored%2520to%2520the%2520Link%2520Prediction%2520%2528LP%2529%2520task.%2520We%2520call%2520our%2520proposed%250Awatermarking%2520scheme%2520GENIE%2520%2528watermarking%2520Graph%2520nEural%2520Networks%2520for%2520lInk%250AprEdiction%2529.%2520We%2520design%2520GENIE%2520using%2520a%2520novel%2520backdoor%2520attack%2520to%2520create%2520a%2520trigger%250Aset%2520for%2520two%2520key%2520methods%2520of%2520LP%253A%2520%25281%2529%2520node%2520representation-based%2520and%2520%25282%2529%250Asubgraph-based.%2520In%2520GENIE%252C%2520the%2520watermark%2520is%2520embedded%2520into%2520the%2520GNN%2520model%2520by%250Atraining%2520it%2520on%2520both%2520the%2520trigger%2520set%2520and%2520a%2520modified%2520training%2520set%252C%2520resulting%2520in%2520a%250Awatermarked%2520GNN%2520model.%2520To%2520assess%2520a%2520suspect%2520model%252C%2520we%2520verify%2520the%2520watermark%250Aagainst%2520the%2520trigger%2520set.%2520We%2520extensively%2520evaluate%2520GENIE%2520across%25203%2520model%250Aarchitectures%2520%2528i.e.%252C%2520SEAL%252C%2520GCN%252C%2520and%2520GraphSAGE%2529%2520and%25207%2520real-world%2520datasets.%250AFurthermore%252C%2520we%2520validate%2520the%2520robustness%2520of%2520GENIE%2520against%252011%2520state-of-the-art%250Awatermark%2520removal%2520techniques%2520and%25203%2520model%2520extraction%2520attacks.%2520We%2520also%250Ademonstrate%2520that%2520GENIE%2520is%2520robust%2520against%2520ownership%2520piracy%2520attack.%2520Our%2520ownership%250Ademonstration%2520scheme%2520statistically%2520guarantees%2520both%2520False%2520Positive%2520Rate%2520%2528FPR%2529%250Aand%2520False%2520Negative%2520Rate%2520%2528FNR%2529%2520to%2520be%2520less%2520than%2520%252410%255E%257B-6%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENIE%3A%20Watermarking%20Graph%20Neural%20Networks%20for%20Link%20Prediction&entry.906535625=Venkata%20Sai%20Pranav%20Bachina%20and%20Ankit%20Gangwal%20and%20Aaryan%20Ajay%20Sharma%20and%20Charu%20Sharma&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20advanced%20the%20field%20of%20machine%20learning%20by%0Autilizing%20graph-structured%20data%2C%20which%20is%20ubiquitous%20in%20the%20real%20world.%20GNNs%0Ahave%20applications%20in%20various%20fields%2C%20ranging%20from%20social%20network%20analysis%20to%0Adrug%20discovery.%20GNN%20training%20is%20strenuous%2C%20requiring%20significant%20computational%0Aresources%20and%20human%20expertise.%20It%20makes%20a%20trained%20GNN%20an%20indispensable%0AIntellectual%20Property%20%28IP%29%20for%20its%20owner.%20Recent%20studies%20have%20shown%20GNNs%20to%20be%0Avulnerable%20to%20model-stealing%20attacks%2C%20which%20raises%20concerns%20over%20IP%20rights%0Aprotection.%20Watermarking%20has%20been%20shown%20to%20be%20effective%20at%20protecting%20the%20IP%20of%0Aa%20GNN%20model.%20Existing%20efforts%20to%20develop%20a%20watermarking%20scheme%20for%20GNNs%20have%0Aonly%20focused%20on%20the%20node%20classification%20and%20the%20graph%20classification%20tasks.%0A%20%20To%20the%20best%20of%20our%20knowledge%2C%20we%20introduce%20the%20first-ever%20watermarking%20scheme%0Afor%20GNNs%20tailored%20to%20the%20Link%20Prediction%20%28LP%29%20task.%20We%20call%20our%20proposed%0Awatermarking%20scheme%20GENIE%20%28watermarking%20Graph%20nEural%20Networks%20for%20lInk%0AprEdiction%29.%20We%20design%20GENIE%20using%20a%20novel%20backdoor%20attack%20to%20create%20a%20trigger%0Aset%20for%20two%20key%20methods%20of%20LP%3A%20%281%29%20node%20representation-based%20and%20%282%29%0Asubgraph-based.%20In%20GENIE%2C%20the%20watermark%20is%20embedded%20into%20the%20GNN%20model%20by%0Atraining%20it%20on%20both%20the%20trigger%20set%20and%20a%20modified%20training%20set%2C%20resulting%20in%20a%0Awatermarked%20GNN%20model.%20To%20assess%20a%20suspect%20model%2C%20we%20verify%20the%20watermark%0Aagainst%20the%20trigger%20set.%20We%20extensively%20evaluate%20GENIE%20across%203%20model%0Aarchitectures%20%28i.e.%2C%20SEAL%2C%20GCN%2C%20and%20GraphSAGE%29%20and%207%20real-world%20datasets.%0AFurthermore%2C%20we%20validate%20the%20robustness%20of%20GENIE%20against%2011%20state-of-the-art%0Awatermark%20removal%20techniques%20and%203%20model%20extraction%20attacks.%20We%20also%0Ademonstrate%20that%20GENIE%20is%20robust%20against%20ownership%20piracy%20attack.%20Our%20ownership%0Ademonstration%20scheme%20statistically%20guarantees%20both%20False%20Positive%20Rate%20%28FPR%29%0Aand%20False%20Negative%20Rate%20%28FNR%29%20to%20be%20less%20than%20%2410%5E%7B-6%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04805v1&entry.124074799=Read"},
{"title": "A Novel Cross-Perturbation for Single Domain Generalization", "author": "Dongjia Zhao and Lei Qi and Xiao Shi and Yinghuan Shi and Xin Geng", "abstract": "  Single domain generalization aims to enhance the ability of the model to\ngeneralize to unknown domains when trained on a single source domain. However,\nthe limited diversity in the training data hampers the learning of\ndomain-invariant features, resulting in compromised generalization performance.\nTo address this, data perturbation (augmentation) has emerged as a crucial\nmethod to increase data diversity. Nevertheless, existing perturbation methods\noften focus on either image-level or feature-level perturbations independently,\nneglecting their synergistic effects. To overcome these limitations, we propose\nCPerb, a simple yet effective cross-perturbation method. Specifically, CPerb\nutilizes both horizontal and vertical operations. Horizontally, it applies\nimage-level and feature-level perturbations to enhance the diversity of the\ntraining data, mitigating the issue of limited diversity in single-source\ndomains. Vertically, it introduces multi-route perturbation to learn\ndomain-invariant features from different perspectives of samples with the same\nsemantic category, thereby enhancing the generalization capability of the\nmodel. Additionally, we propose MixPatch, a novel feature-level perturbation\nmethod that exploits local image style information to further diversify the\ntraining data. Extensive experiments on various benchmark datasets validate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2308.00918v2", "date": "2024-06-07", "relevancy": 2.3695, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4739}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Cross-Perturbation%20for%20Single%20Domain%20Generalization&body=Title%3A%20A%20Novel%20Cross-Perturbation%20for%20Single%20Domain%20Generalization%0AAuthor%3A%20Dongjia%20Zhao%20and%20Lei%20Qi%20and%20Xiao%20Shi%20and%20Yinghuan%20Shi%20and%20Xin%20Geng%0AAbstract%3A%20%20%20Single%20domain%20generalization%20aims%20to%20enhance%20the%20ability%20of%20the%20model%20to%0Ageneralize%20to%20unknown%20domains%20when%20trained%20on%20a%20single%20source%20domain.%20However%2C%0Athe%20limited%20diversity%20in%20the%20training%20data%20hampers%20the%20learning%20of%0Adomain-invariant%20features%2C%20resulting%20in%20compromised%20generalization%20performance.%0ATo%20address%20this%2C%20data%20perturbation%20%28augmentation%29%20has%20emerged%20as%20a%20crucial%0Amethod%20to%20increase%20data%20diversity.%20Nevertheless%2C%20existing%20perturbation%20methods%0Aoften%20focus%20on%20either%20image-level%20or%20feature-level%20perturbations%20independently%2C%0Aneglecting%20their%20synergistic%20effects.%20To%20overcome%20these%20limitations%2C%20we%20propose%0ACPerb%2C%20a%20simple%20yet%20effective%20cross-perturbation%20method.%20Specifically%2C%20CPerb%0Autilizes%20both%20horizontal%20and%20vertical%20operations.%20Horizontally%2C%20it%20applies%0Aimage-level%20and%20feature-level%20perturbations%20to%20enhance%20the%20diversity%20of%20the%0Atraining%20data%2C%20mitigating%20the%20issue%20of%20limited%20diversity%20in%20single-source%0Adomains.%20Vertically%2C%20it%20introduces%20multi-route%20perturbation%20to%20learn%0Adomain-invariant%20features%20from%20different%20perspectives%20of%20samples%20with%20the%20same%0Asemantic%20category%2C%20thereby%20enhancing%20the%20generalization%20capability%20of%20the%0Amodel.%20Additionally%2C%20we%20propose%20MixPatch%2C%20a%20novel%20feature-level%20perturbation%0Amethod%20that%20exploits%20local%20image%20style%20information%20to%20further%20diversify%20the%0Atraining%20data.%20Extensive%20experiments%20on%20various%20benchmark%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Cross-Perturbation%2520for%2520Single%2520Domain%2520Generalization%26entry.906535625%3DDongjia%2520Zhao%2520and%2520Lei%2520Qi%2520and%2520Xiao%2520Shi%2520and%2520Yinghuan%2520Shi%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520Single%2520domain%2520generalization%2520aims%2520to%2520enhance%2520the%2520ability%2520of%2520the%2520model%2520to%250Ageneralize%2520to%2520unknown%2520domains%2520when%2520trained%2520on%2520a%2520single%2520source%2520domain.%2520However%252C%250Athe%2520limited%2520diversity%2520in%2520the%2520training%2520data%2520hampers%2520the%2520learning%2520of%250Adomain-invariant%2520features%252C%2520resulting%2520in%2520compromised%2520generalization%2520performance.%250ATo%2520address%2520this%252C%2520data%2520perturbation%2520%2528augmentation%2529%2520has%2520emerged%2520as%2520a%2520crucial%250Amethod%2520to%2520increase%2520data%2520diversity.%2520Nevertheless%252C%2520existing%2520perturbation%2520methods%250Aoften%2520focus%2520on%2520either%2520image-level%2520or%2520feature-level%2520perturbations%2520independently%252C%250Aneglecting%2520their%2520synergistic%2520effects.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%250ACPerb%252C%2520a%2520simple%2520yet%2520effective%2520cross-perturbation%2520method.%2520Specifically%252C%2520CPerb%250Autilizes%2520both%2520horizontal%2520and%2520vertical%2520operations.%2520Horizontally%252C%2520it%2520applies%250Aimage-level%2520and%2520feature-level%2520perturbations%2520to%2520enhance%2520the%2520diversity%2520of%2520the%250Atraining%2520data%252C%2520mitigating%2520the%2520issue%2520of%2520limited%2520diversity%2520in%2520single-source%250Adomains.%2520Vertically%252C%2520it%2520introduces%2520multi-route%2520perturbation%2520to%2520learn%250Adomain-invariant%2520features%2520from%2520different%2520perspectives%2520of%2520samples%2520with%2520the%2520same%250Asemantic%2520category%252C%2520thereby%2520enhancing%2520the%2520generalization%2520capability%2520of%2520the%250Amodel.%2520Additionally%252C%2520we%2520propose%2520MixPatch%252C%2520a%2520novel%2520feature-level%2520perturbation%250Amethod%2520that%2520exploits%2520local%2520image%2520style%2520information%2520to%2520further%2520diversify%2520the%250Atraining%2520data.%2520Extensive%2520experiments%2520on%2520various%2520benchmark%2520datasets%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.00918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Cross-Perturbation%20for%20Single%20Domain%20Generalization&entry.906535625=Dongjia%20Zhao%20and%20Lei%20Qi%20and%20Xiao%20Shi%20and%20Yinghuan%20Shi%20and%20Xin%20Geng&entry.1292438233=%20%20Single%20domain%20generalization%20aims%20to%20enhance%20the%20ability%20of%20the%20model%20to%0Ageneralize%20to%20unknown%20domains%20when%20trained%20on%20a%20single%20source%20domain.%20However%2C%0Athe%20limited%20diversity%20in%20the%20training%20data%20hampers%20the%20learning%20of%0Adomain-invariant%20features%2C%20resulting%20in%20compromised%20generalization%20performance.%0ATo%20address%20this%2C%20data%20perturbation%20%28augmentation%29%20has%20emerged%20as%20a%20crucial%0Amethod%20to%20increase%20data%20diversity.%20Nevertheless%2C%20existing%20perturbation%20methods%0Aoften%20focus%20on%20either%20image-level%20or%20feature-level%20perturbations%20independently%2C%0Aneglecting%20their%20synergistic%20effects.%20To%20overcome%20these%20limitations%2C%20we%20propose%0ACPerb%2C%20a%20simple%20yet%20effective%20cross-perturbation%20method.%20Specifically%2C%20CPerb%0Autilizes%20both%20horizontal%20and%20vertical%20operations.%20Horizontally%2C%20it%20applies%0Aimage-level%20and%20feature-level%20perturbations%20to%20enhance%20the%20diversity%20of%20the%0Atraining%20data%2C%20mitigating%20the%20issue%20of%20limited%20diversity%20in%20single-source%0Adomains.%20Vertically%2C%20it%20introduces%20multi-route%20perturbation%20to%20learn%0Adomain-invariant%20features%20from%20different%20perspectives%20of%20samples%20with%20the%20same%0Asemantic%20category%2C%20thereby%20enhancing%20the%20generalization%20capability%20of%20the%0Amodel.%20Additionally%2C%20we%20propose%20MixPatch%2C%20a%20novel%20feature-level%20perturbation%0Amethod%20that%20exploits%20local%20image%20style%20information%20to%20further%20diversify%20the%0Atraining%20data.%20Extensive%20experiments%20on%20various%20benchmark%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00918v2&entry.124074799=Read"},
{"title": "Optimizing Time Series Forecasting Architectures: A Hierarchical Neural\n  Architecture Search Approach", "author": "Difan Deng and Marius Lindauer", "abstract": "  The rapid development of time series forecasting research has brought many\ndeep learning-based modules in this field. However, despite the increasing\namount of new forecasting architectures, it is still unclear if we have\nleveraged the full potential of these existing modules within a properly\ndesigned architecture. In this work, we propose a novel hierarchical neural\narchitecture search approach for time series forecasting tasks. With the design\nof a hierarchical search space, we incorporate many architecture types designed\nfor forecasting tasks and allow for the efficient combination of different\nforecasting architecture modules. Results on long-term-time-series-forecasting\ntasks show that our approach can search for lightweight high-performing\nforecasting architectures across different forecasting tasks.\n", "link": "http://arxiv.org/abs/2406.05088v1", "date": "2024-06-07", "relevancy": 2.365, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4755}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Time%20Series%20Forecasting%20Architectures%3A%20A%20Hierarchical%20Neural%0A%20%20Architecture%20Search%20Approach&body=Title%3A%20Optimizing%20Time%20Series%20Forecasting%20Architectures%3A%20A%20Hierarchical%20Neural%0A%20%20Architecture%20Search%20Approach%0AAuthor%3A%20Difan%20Deng%20and%20Marius%20Lindauer%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20time%20series%20forecasting%20research%20has%20brought%20many%0Adeep%20learning-based%20modules%20in%20this%20field.%20However%2C%20despite%20the%20increasing%0Aamount%20of%20new%20forecasting%20architectures%2C%20it%20is%20still%20unclear%20if%20we%20have%0Aleveraged%20the%20full%20potential%20of%20these%20existing%20modules%20within%20a%20properly%0Adesigned%20architecture.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hierarchical%20neural%0Aarchitecture%20search%20approach%20for%20time%20series%20forecasting%20tasks.%20With%20the%20design%0Aof%20a%20hierarchical%20search%20space%2C%20we%20incorporate%20many%20architecture%20types%20designed%0Afor%20forecasting%20tasks%20and%20allow%20for%20the%20efficient%20combination%20of%20different%0Aforecasting%20architecture%20modules.%20Results%20on%20long-term-time-series-forecasting%0Atasks%20show%20that%20our%20approach%20can%20search%20for%20lightweight%20high-performing%0Aforecasting%20architectures%20across%20different%20forecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Time%2520Series%2520Forecasting%2520Architectures%253A%2520A%2520Hierarchical%2520Neural%250A%2520%2520Architecture%2520Search%2520Approach%26entry.906535625%3DDifan%2520Deng%2520and%2520Marius%2520Lindauer%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520time%2520series%2520forecasting%2520research%2520has%2520brought%2520many%250Adeep%2520learning-based%2520modules%2520in%2520this%2520field.%2520However%252C%2520despite%2520the%2520increasing%250Aamount%2520of%2520new%2520forecasting%2520architectures%252C%2520it%2520is%2520still%2520unclear%2520if%2520we%2520have%250Aleveraged%2520the%2520full%2520potential%2520of%2520these%2520existing%2520modules%2520within%2520a%2520properly%250Adesigned%2520architecture.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520neural%250Aarchitecture%2520search%2520approach%2520for%2520time%2520series%2520forecasting%2520tasks.%2520With%2520the%2520design%250Aof%2520a%2520hierarchical%2520search%2520space%252C%2520we%2520incorporate%2520many%2520architecture%2520types%2520designed%250Afor%2520forecasting%2520tasks%2520and%2520allow%2520for%2520the%2520efficient%2520combination%2520of%2520different%250Aforecasting%2520architecture%2520modules.%2520Results%2520on%2520long-term-time-series-forecasting%250Atasks%2520show%2520that%2520our%2520approach%2520can%2520search%2520for%2520lightweight%2520high-performing%250Aforecasting%2520architectures%2520across%2520different%2520forecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Time%20Series%20Forecasting%20Architectures%3A%20A%20Hierarchical%20Neural%0A%20%20Architecture%20Search%20Approach&entry.906535625=Difan%20Deng%20and%20Marius%20Lindauer&entry.1292438233=%20%20The%20rapid%20development%20of%20time%20series%20forecasting%20research%20has%20brought%20many%0Adeep%20learning-based%20modules%20in%20this%20field.%20However%2C%20despite%20the%20increasing%0Aamount%20of%20new%20forecasting%20architectures%2C%20it%20is%20still%20unclear%20if%20we%20have%0Aleveraged%20the%20full%20potential%20of%20these%20existing%20modules%20within%20a%20properly%0Adesigned%20architecture.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hierarchical%20neural%0Aarchitecture%20search%20approach%20for%20time%20series%20forecasting%20tasks.%20With%20the%20design%0Aof%20a%20hierarchical%20search%20space%2C%20we%20incorporate%20many%20architecture%20types%20designed%0Afor%20forecasting%20tasks%20and%20allow%20for%20the%20efficient%20combination%20of%20different%0Aforecasting%20architecture%20modules.%20Results%20on%20long-term-time-series-forecasting%0Atasks%20show%20that%20our%20approach%20can%20search%20for%20lightweight%20high-performing%0Aforecasting%20architectures%20across%20different%20forecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05088v1&entry.124074799=Read"},
{"title": "Large Generative Graph Models", "author": "Yu Wang and Ryan A. Rossi and Namyong Park and Huiyuan Chen and Nesreen K. Ahmed and Puja Trivedi and Franck Dernoncourt and Danai Koutra and Tyler Derr", "abstract": "  Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno\nare trained on a huge amount of language corpus, images, videos, and audio that\nare extremely diverse from numerous domains. This training paradigm over\ndiverse well-curated data lies at the heart of generating creative and sensible\ncontent. However, all previous graph generative models (e.g., GraphRNN, MDVAE,\nMoFlow, GDSS, and DiGress) have been trained only on one dataset each time,\nwhich cannot replicate the revolutionary success achieved by LGMs in other\nfields. To remedy this crucial gap, we propose a new class of graph generative\nmodel called Large Graph Generative Model (LGGM) that is trained on a large\ncorpus of graphs (over 5000 graphs) from 13 different domains. We empirically\ndemonstrate that the pre-trained LGGM has superior zero-shot generative\ncapability to existing graph generative models. Furthermore, our pre-trained\nLGGM can be easily fine-tuned with graphs from target domains and demonstrate\neven better performance than those directly trained from scratch, behaving as a\nsolid starting point for real-world customization. Inspired by Stable\nDiffusion, we further equip LGGM with the capability to generate graphs given\ntext prompts (Text-to-Graph), such as the description of the network name and\ndomain (i.e., \"The power-1138-bus graph represents a network of buses in a\npower distribution system.\"), and network statistics (i.e., \"The graph has a\nlow average degree, suitable for modeling social media interactions.\"). This\nText-to-Graph capability integrates the extensive world knowledge in the\nunderlying language model, offering users fine-grained control of the generated\ngraphs. We release the code, the model checkpoint, and the datasets at\nhttps://lggm-lg.github.io/.\n", "link": "http://arxiv.org/abs/2406.05109v1", "date": "2024-06-07", "relevancy": 2.339, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5929}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Generative%20Graph%20Models&body=Title%3A%20Large%20Generative%20Graph%20Models%0AAuthor%3A%20Yu%20Wang%20and%20Ryan%20A.%20Rossi%20and%20Namyong%20Park%20and%20Huiyuan%20Chen%20and%20Nesreen%20K.%20Ahmed%20and%20Puja%20Trivedi%20and%20Franck%20Dernoncourt%20and%20Danai%20Koutra%20and%20Tyler%20Derr%0AAbstract%3A%20%20%20Large%20Generative%20Models%20%28LGMs%29%20such%20as%20GPT%2C%20Stable%20Diffusion%2C%20Sora%2C%20and%20Suno%0Aare%20trained%20on%20a%20huge%20amount%20of%20language%20corpus%2C%20images%2C%20videos%2C%20and%20audio%20that%0Aare%20extremely%20diverse%20from%20numerous%20domains.%20This%20training%20paradigm%20over%0Adiverse%20well-curated%20data%20lies%20at%20the%20heart%20of%20generating%20creative%20and%20sensible%0Acontent.%20However%2C%20all%20previous%20graph%20generative%20models%20%28e.g.%2C%20GraphRNN%2C%20MDVAE%2C%0AMoFlow%2C%20GDSS%2C%20and%20DiGress%29%20have%20been%20trained%20only%20on%20one%20dataset%20each%20time%2C%0Awhich%20cannot%20replicate%20the%20revolutionary%20success%20achieved%20by%20LGMs%20in%20other%0Afields.%20To%20remedy%20this%20crucial%20gap%2C%20we%20propose%20a%20new%20class%20of%20graph%20generative%0Amodel%20called%20Large%20Graph%20Generative%20Model%20%28LGGM%29%20that%20is%20trained%20on%20a%20large%0Acorpus%20of%20graphs%20%28over%205000%20graphs%29%20from%2013%20different%20domains.%20We%20empirically%0Ademonstrate%20that%20the%20pre-trained%20LGGM%20has%20superior%20zero-shot%20generative%0Acapability%20to%20existing%20graph%20generative%20models.%20Furthermore%2C%20our%20pre-trained%0ALGGM%20can%20be%20easily%20fine-tuned%20with%20graphs%20from%20target%20domains%20and%20demonstrate%0Aeven%20better%20performance%20than%20those%20directly%20trained%20from%20scratch%2C%20behaving%20as%20a%0Asolid%20starting%20point%20for%20real-world%20customization.%20Inspired%20by%20Stable%0ADiffusion%2C%20we%20further%20equip%20LGGM%20with%20the%20capability%20to%20generate%20graphs%20given%0Atext%20prompts%20%28Text-to-Graph%29%2C%20such%20as%20the%20description%20of%20the%20network%20name%20and%0Adomain%20%28i.e.%2C%20%22The%20power-1138-bus%20graph%20represents%20a%20network%20of%20buses%20in%20a%0Apower%20distribution%20system.%22%29%2C%20and%20network%20statistics%20%28i.e.%2C%20%22The%20graph%20has%20a%0Alow%20average%20degree%2C%20suitable%20for%20modeling%20social%20media%20interactions.%22%29.%20This%0AText-to-Graph%20capability%20integrates%20the%20extensive%20world%20knowledge%20in%20the%0Aunderlying%20language%20model%2C%20offering%20users%20fine-grained%20control%20of%20the%20generated%0Agraphs.%20We%20release%20the%20code%2C%20the%20model%20checkpoint%2C%20and%20the%20datasets%20at%0Ahttps%3A//lggm-lg.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Generative%2520Graph%2520Models%26entry.906535625%3DYu%2520Wang%2520and%2520Ryan%2520A.%2520Rossi%2520and%2520Namyong%2520Park%2520and%2520Huiyuan%2520Chen%2520and%2520Nesreen%2520K.%2520Ahmed%2520and%2520Puja%2520Trivedi%2520and%2520Franck%2520Dernoncourt%2520and%2520Danai%2520Koutra%2520and%2520Tyler%2520Derr%26entry.1292438233%3D%2520%2520Large%2520Generative%2520Models%2520%2528LGMs%2529%2520such%2520as%2520GPT%252C%2520Stable%2520Diffusion%252C%2520Sora%252C%2520and%2520Suno%250Aare%2520trained%2520on%2520a%2520huge%2520amount%2520of%2520language%2520corpus%252C%2520images%252C%2520videos%252C%2520and%2520audio%2520that%250Aare%2520extremely%2520diverse%2520from%2520numerous%2520domains.%2520This%2520training%2520paradigm%2520over%250Adiverse%2520well-curated%2520data%2520lies%2520at%2520the%2520heart%2520of%2520generating%2520creative%2520and%2520sensible%250Acontent.%2520However%252C%2520all%2520previous%2520graph%2520generative%2520models%2520%2528e.g.%252C%2520GraphRNN%252C%2520MDVAE%252C%250AMoFlow%252C%2520GDSS%252C%2520and%2520DiGress%2529%2520have%2520been%2520trained%2520only%2520on%2520one%2520dataset%2520each%2520time%252C%250Awhich%2520cannot%2520replicate%2520the%2520revolutionary%2520success%2520achieved%2520by%2520LGMs%2520in%2520other%250Afields.%2520To%2520remedy%2520this%2520crucial%2520gap%252C%2520we%2520propose%2520a%2520new%2520class%2520of%2520graph%2520generative%250Amodel%2520called%2520Large%2520Graph%2520Generative%2520Model%2520%2528LGGM%2529%2520that%2520is%2520trained%2520on%2520a%2520large%250Acorpus%2520of%2520graphs%2520%2528over%25205000%2520graphs%2529%2520from%252013%2520different%2520domains.%2520We%2520empirically%250Ademonstrate%2520that%2520the%2520pre-trained%2520LGGM%2520has%2520superior%2520zero-shot%2520generative%250Acapability%2520to%2520existing%2520graph%2520generative%2520models.%2520Furthermore%252C%2520our%2520pre-trained%250ALGGM%2520can%2520be%2520easily%2520fine-tuned%2520with%2520graphs%2520from%2520target%2520domains%2520and%2520demonstrate%250Aeven%2520better%2520performance%2520than%2520those%2520directly%2520trained%2520from%2520scratch%252C%2520behaving%2520as%2520a%250Asolid%2520starting%2520point%2520for%2520real-world%2520customization.%2520Inspired%2520by%2520Stable%250ADiffusion%252C%2520we%2520further%2520equip%2520LGGM%2520with%2520the%2520capability%2520to%2520generate%2520graphs%2520given%250Atext%2520prompts%2520%2528Text-to-Graph%2529%252C%2520such%2520as%2520the%2520description%2520of%2520the%2520network%2520name%2520and%250Adomain%2520%2528i.e.%252C%2520%2522The%2520power-1138-bus%2520graph%2520represents%2520a%2520network%2520of%2520buses%2520in%2520a%250Apower%2520distribution%2520system.%2522%2529%252C%2520and%2520network%2520statistics%2520%2528i.e.%252C%2520%2522The%2520graph%2520has%2520a%250Alow%2520average%2520degree%252C%2520suitable%2520for%2520modeling%2520social%2520media%2520interactions.%2522%2529.%2520This%250AText-to-Graph%2520capability%2520integrates%2520the%2520extensive%2520world%2520knowledge%2520in%2520the%250Aunderlying%2520language%2520model%252C%2520offering%2520users%2520fine-grained%2520control%2520of%2520the%2520generated%250Agraphs.%2520We%2520release%2520the%2520code%252C%2520the%2520model%2520checkpoint%252C%2520and%2520the%2520datasets%2520at%250Ahttps%253A//lggm-lg.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Generative%20Graph%20Models&entry.906535625=Yu%20Wang%20and%20Ryan%20A.%20Rossi%20and%20Namyong%20Park%20and%20Huiyuan%20Chen%20and%20Nesreen%20K.%20Ahmed%20and%20Puja%20Trivedi%20and%20Franck%20Dernoncourt%20and%20Danai%20Koutra%20and%20Tyler%20Derr&entry.1292438233=%20%20Large%20Generative%20Models%20%28LGMs%29%20such%20as%20GPT%2C%20Stable%20Diffusion%2C%20Sora%2C%20and%20Suno%0Aare%20trained%20on%20a%20huge%20amount%20of%20language%20corpus%2C%20images%2C%20videos%2C%20and%20audio%20that%0Aare%20extremely%20diverse%20from%20numerous%20domains.%20This%20training%20paradigm%20over%0Adiverse%20well-curated%20data%20lies%20at%20the%20heart%20of%20generating%20creative%20and%20sensible%0Acontent.%20However%2C%20all%20previous%20graph%20generative%20models%20%28e.g.%2C%20GraphRNN%2C%20MDVAE%2C%0AMoFlow%2C%20GDSS%2C%20and%20DiGress%29%20have%20been%20trained%20only%20on%20one%20dataset%20each%20time%2C%0Awhich%20cannot%20replicate%20the%20revolutionary%20success%20achieved%20by%20LGMs%20in%20other%0Afields.%20To%20remedy%20this%20crucial%20gap%2C%20we%20propose%20a%20new%20class%20of%20graph%20generative%0Amodel%20called%20Large%20Graph%20Generative%20Model%20%28LGGM%29%20that%20is%20trained%20on%20a%20large%0Acorpus%20of%20graphs%20%28over%205000%20graphs%29%20from%2013%20different%20domains.%20We%20empirically%0Ademonstrate%20that%20the%20pre-trained%20LGGM%20has%20superior%20zero-shot%20generative%0Acapability%20to%20existing%20graph%20generative%20models.%20Furthermore%2C%20our%20pre-trained%0ALGGM%20can%20be%20easily%20fine-tuned%20with%20graphs%20from%20target%20domains%20and%20demonstrate%0Aeven%20better%20performance%20than%20those%20directly%20trained%20from%20scratch%2C%20behaving%20as%20a%0Asolid%20starting%20point%20for%20real-world%20customization.%20Inspired%20by%20Stable%0ADiffusion%2C%20we%20further%20equip%20LGGM%20with%20the%20capability%20to%20generate%20graphs%20given%0Atext%20prompts%20%28Text-to-Graph%29%2C%20such%20as%20the%20description%20of%20the%20network%20name%20and%0Adomain%20%28i.e.%2C%20%22The%20power-1138-bus%20graph%20represents%20a%20network%20of%20buses%20in%20a%0Apower%20distribution%20system.%22%29%2C%20and%20network%20statistics%20%28i.e.%2C%20%22The%20graph%20has%20a%0Alow%20average%20degree%2C%20suitable%20for%20modeling%20social%20media%20interactions.%22%29.%20This%0AText-to-Graph%20capability%20integrates%20the%20extensive%20world%20knowledge%20in%20the%0Aunderlying%20language%20model%2C%20offering%20users%20fine-grained%20control%20of%20the%20generated%0Agraphs.%20We%20release%20the%20code%2C%20the%20model%20checkpoint%2C%20and%20the%20datasets%20at%0Ahttps%3A//lggm-lg.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05109v1&entry.124074799=Read"},
{"title": "SpanGNN: Towards Memory-Efficient Graph Neural Networks via Spanning\n  Subgraph Training", "author": "Xizhi Gu and Hongzheng Li and Shihong Gao and Xinyan Zhang and Lei Chen and Yingxia Shao", "abstract": "  Graph Neural Networks (GNNs) have superior capability in learning graph data.\nFull-graph GNN training generally has high accuracy, however, it suffers from\nlarge peak memory usage and encounters the Out-of-Memory problem when handling\nlarge graphs. To address this memory problem, a popular solution is mini-batch\nGNN training. However, mini-batch GNN training increases the training variance\nand sacrifices the model accuracy. In this paper, we propose a new\nmemory-efficient GNN training method using spanning subgraph, called SpanGNN.\nSpanGNN trains GNN models over a sequence of spanning subgraphs, which are\nconstructed from empty structure. To overcome the excessive peak memory\nconsumption problem, SpanGNN selects a set of edges from the original graph to\nincrementally update the spanning subgraph between every epoch. To ensure the\nmodel accuracy, we introduce two types of edge sampling strategies (i.e.,\nvariance-reduced and noise-reduced), and help SpanGNN select high-quality edges\nfor the GNN learning. We conduct experiments with SpanGNN on widely used\ndatasets, demonstrating SpanGNN's advantages in the model performance and low\npeak memory usage.\n", "link": "http://arxiv.org/abs/2406.04938v1", "date": "2024-06-07", "relevancy": 2.336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4989}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4525}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpanGNN%3A%20Towards%20Memory-Efficient%20Graph%20Neural%20Networks%20via%20Spanning%0A%20%20Subgraph%20Training&body=Title%3A%20SpanGNN%3A%20Towards%20Memory-Efficient%20Graph%20Neural%20Networks%20via%20Spanning%0A%20%20Subgraph%20Training%0AAuthor%3A%20Xizhi%20Gu%20and%20Hongzheng%20Li%20and%20Shihong%20Gao%20and%20Xinyan%20Zhang%20and%20Lei%20Chen%20and%20Yingxia%20Shao%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20superior%20capability%20in%20learning%20graph%20data.%0AFull-graph%20GNN%20training%20generally%20has%20high%20accuracy%2C%20however%2C%20it%20suffers%20from%0Alarge%20peak%20memory%20usage%20and%20encounters%20the%20Out-of-Memory%20problem%20when%20handling%0Alarge%20graphs.%20To%20address%20this%20memory%20problem%2C%20a%20popular%20solution%20is%20mini-batch%0AGNN%20training.%20However%2C%20mini-batch%20GNN%20training%20increases%20the%20training%20variance%0Aand%20sacrifices%20the%20model%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%20new%0Amemory-efficient%20GNN%20training%20method%20using%20spanning%20subgraph%2C%20called%20SpanGNN.%0ASpanGNN%20trains%20GNN%20models%20over%20a%20sequence%20of%20spanning%20subgraphs%2C%20which%20are%0Aconstructed%20from%20empty%20structure.%20To%20overcome%20the%20excessive%20peak%20memory%0Aconsumption%20problem%2C%20SpanGNN%20selects%20a%20set%20of%20edges%20from%20the%20original%20graph%20to%0Aincrementally%20update%20the%20spanning%20subgraph%20between%20every%20epoch.%20To%20ensure%20the%0Amodel%20accuracy%2C%20we%20introduce%20two%20types%20of%20edge%20sampling%20strategies%20%28i.e.%2C%0Avariance-reduced%20and%20noise-reduced%29%2C%20and%20help%20SpanGNN%20select%20high-quality%20edges%0Afor%20the%20GNN%20learning.%20We%20conduct%20experiments%20with%20SpanGNN%20on%20widely%20used%0Adatasets%2C%20demonstrating%20SpanGNN%27s%20advantages%20in%20the%20model%20performance%20and%20low%0Apeak%20memory%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpanGNN%253A%2520Towards%2520Memory-Efficient%2520Graph%2520Neural%2520Networks%2520via%2520Spanning%250A%2520%2520Subgraph%2520Training%26entry.906535625%3DXizhi%2520Gu%2520and%2520Hongzheng%2520Li%2520and%2520Shihong%2520Gao%2520and%2520Xinyan%2520Zhang%2520and%2520Lei%2520Chen%2520and%2520Yingxia%2520Shao%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520superior%2520capability%2520in%2520learning%2520graph%2520data.%250AFull-graph%2520GNN%2520training%2520generally%2520has%2520high%2520accuracy%252C%2520however%252C%2520it%2520suffers%2520from%250Alarge%2520peak%2520memory%2520usage%2520and%2520encounters%2520the%2520Out-of-Memory%2520problem%2520when%2520handling%250Alarge%2520graphs.%2520To%2520address%2520this%2520memory%2520problem%252C%2520a%2520popular%2520solution%2520is%2520mini-batch%250AGNN%2520training.%2520However%252C%2520mini-batch%2520GNN%2520training%2520increases%2520the%2520training%2520variance%250Aand%2520sacrifices%2520the%2520model%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%250Amemory-efficient%2520GNN%2520training%2520method%2520using%2520spanning%2520subgraph%252C%2520called%2520SpanGNN.%250ASpanGNN%2520trains%2520GNN%2520models%2520over%2520a%2520sequence%2520of%2520spanning%2520subgraphs%252C%2520which%2520are%250Aconstructed%2520from%2520empty%2520structure.%2520To%2520overcome%2520the%2520excessive%2520peak%2520memory%250Aconsumption%2520problem%252C%2520SpanGNN%2520selects%2520a%2520set%2520of%2520edges%2520from%2520the%2520original%2520graph%2520to%250Aincrementally%2520update%2520the%2520spanning%2520subgraph%2520between%2520every%2520epoch.%2520To%2520ensure%2520the%250Amodel%2520accuracy%252C%2520we%2520introduce%2520two%2520types%2520of%2520edge%2520sampling%2520strategies%2520%2528i.e.%252C%250Avariance-reduced%2520and%2520noise-reduced%2529%252C%2520and%2520help%2520SpanGNN%2520select%2520high-quality%2520edges%250Afor%2520the%2520GNN%2520learning.%2520We%2520conduct%2520experiments%2520with%2520SpanGNN%2520on%2520widely%2520used%250Adatasets%252C%2520demonstrating%2520SpanGNN%2527s%2520advantages%2520in%2520the%2520model%2520performance%2520and%2520low%250Apeak%2520memory%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpanGNN%3A%20Towards%20Memory-Efficient%20Graph%20Neural%20Networks%20via%20Spanning%0A%20%20Subgraph%20Training&entry.906535625=Xizhi%20Gu%20and%20Hongzheng%20Li%20and%20Shihong%20Gao%20and%20Xinyan%20Zhang%20and%20Lei%20Chen%20and%20Yingxia%20Shao&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20superior%20capability%20in%20learning%20graph%20data.%0AFull-graph%20GNN%20training%20generally%20has%20high%20accuracy%2C%20however%2C%20it%20suffers%20from%0Alarge%20peak%20memory%20usage%20and%20encounters%20the%20Out-of-Memory%20problem%20when%20handling%0Alarge%20graphs.%20To%20address%20this%20memory%20problem%2C%20a%20popular%20solution%20is%20mini-batch%0AGNN%20training.%20However%2C%20mini-batch%20GNN%20training%20increases%20the%20training%20variance%0Aand%20sacrifices%20the%20model%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%20new%0Amemory-efficient%20GNN%20training%20method%20using%20spanning%20subgraph%2C%20called%20SpanGNN.%0ASpanGNN%20trains%20GNN%20models%20over%20a%20sequence%20of%20spanning%20subgraphs%2C%20which%20are%0Aconstructed%20from%20empty%20structure.%20To%20overcome%20the%20excessive%20peak%20memory%0Aconsumption%20problem%2C%20SpanGNN%20selects%20a%20set%20of%20edges%20from%20the%20original%20graph%20to%0Aincrementally%20update%20the%20spanning%20subgraph%20between%20every%20epoch.%20To%20ensure%20the%0Amodel%20accuracy%2C%20we%20introduce%20two%20types%20of%20edge%20sampling%20strategies%20%28i.e.%2C%0Avariance-reduced%20and%20noise-reduced%29%2C%20and%20help%20SpanGNN%20select%20high-quality%20edges%0Afor%20the%20GNN%20learning.%20We%20conduct%20experiments%20with%20SpanGNN%20on%20widely%20used%0Adatasets%2C%20demonstrating%20SpanGNN%27s%20advantages%20in%20the%20model%20performance%20and%20low%0Apeak%20memory%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04938v1&entry.124074799=Read"},
{"title": "Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and\n  Image Embeddings", "author": "Sahand Sharifzadeh and Christos Kaplanis and Shreya Pathak and Dharshan Kumaran and Anastasija Ilic and Jovana Mitrovic and Charles Blundell and Andrea Banino", "abstract": "  The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). In\nthis work, we investigate an approach that leverages the strengths of Large\nLanguage Models (LLMs) and image generation models to create synthetic\nimage-text pairs for efficient and effective VLM training. Our method employs a\npretrained text-to-image model to synthesize image embeddings from captions\ngenerated by an LLM. Despite the text-to-image model and VLM initially being\ntrained on the same data, our approach leverages the image generator's ability\nto create novel compositions, resulting in synthetic image embeddings that\nexpand beyond the limitations of the original dataset. Extensive experiments\ndemonstrate that our VLM, finetuned on synthetic data achieves comparable\nperformance to models trained solely on human-annotated data, while requiring\nsignificantly less data. Furthermore, we perform a set of analyses on captions\nwhich reveals that semantic diversity and balance are key aspects for better\ndownstream performance. Finally, we show that synthesizing images in the image\nembedding space is 25\\% faster than in the pixel space. We believe our work not\nonly addresses a significant challenge in VLM training but also opens up\npromising avenues for the development of self-improving multi-modal models.\n", "link": "http://arxiv.org/abs/2403.07750v2", "date": "2024-06-07", "relevancy": 2.2953, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5728}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synth%24%5E2%24%3A%20Boosting%20Visual-Language%20Models%20with%20Synthetic%20Captions%20and%0A%20%20Image%20Embeddings&body=Title%3A%20Synth%24%5E2%24%3A%20Boosting%20Visual-Language%20Models%20with%20Synthetic%20Captions%20and%0A%20%20Image%20Embeddings%0AAuthor%3A%20Sahand%20Sharifzadeh%20and%20Christos%20Kaplanis%20and%20Shreya%20Pathak%20and%20Dharshan%20Kumaran%20and%20Anastasija%20Ilic%20and%20Jovana%20Mitrovic%20and%20Charles%20Blundell%20and%20Andrea%20Banino%0AAbstract%3A%20%20%20The%20creation%20of%20high-quality%20human-labeled%20image-caption%20datasets%20presents%20a%0Asignificant%20bottleneck%20in%20the%20development%20of%20Visual-Language%20Models%20%28VLMs%29.%20In%0Athis%20work%2C%20we%20investigate%20an%20approach%20that%20leverages%20the%20strengths%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20image%20generation%20models%20to%20create%20synthetic%0Aimage-text%20pairs%20for%20efficient%20and%20effective%20VLM%20training.%20Our%20method%20employs%20a%0Apretrained%20text-to-image%20model%20to%20synthesize%20image%20embeddings%20from%20captions%0Agenerated%20by%20an%20LLM.%20Despite%20the%20text-to-image%20model%20and%20VLM%20initially%20being%0Atrained%20on%20the%20same%20data%2C%20our%20approach%20leverages%20the%20image%20generator%27s%20ability%0Ato%20create%20novel%20compositions%2C%20resulting%20in%20synthetic%20image%20embeddings%20that%0Aexpand%20beyond%20the%20limitations%20of%20the%20original%20dataset.%20Extensive%20experiments%0Ademonstrate%20that%20our%20VLM%2C%20finetuned%20on%20synthetic%20data%20achieves%20comparable%0Aperformance%20to%20models%20trained%20solely%20on%20human-annotated%20data%2C%20while%20requiring%0Asignificantly%20less%20data.%20Furthermore%2C%20we%20perform%20a%20set%20of%20analyses%20on%20captions%0Awhich%20reveals%20that%20semantic%20diversity%20and%20balance%20are%20key%20aspects%20for%20better%0Adownstream%20performance.%20Finally%2C%20we%20show%20that%20synthesizing%20images%20in%20the%20image%0Aembedding%20space%20is%2025%5C%25%20faster%20than%20in%20the%20pixel%20space.%20We%20believe%20our%20work%20not%0Aonly%20addresses%20a%20significant%20challenge%20in%20VLM%20training%20but%20also%20opens%20up%0Apromising%20avenues%20for%20the%20development%20of%20self-improving%20multi-modal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynth%2524%255E2%2524%253A%2520Boosting%2520Visual-Language%2520Models%2520with%2520Synthetic%2520Captions%2520and%250A%2520%2520Image%2520Embeddings%26entry.906535625%3DSahand%2520Sharifzadeh%2520and%2520Christos%2520Kaplanis%2520and%2520Shreya%2520Pathak%2520and%2520Dharshan%2520Kumaran%2520and%2520Anastasija%2520Ilic%2520and%2520Jovana%2520Mitrovic%2520and%2520Charles%2520Blundell%2520and%2520Andrea%2520Banino%26entry.1292438233%3D%2520%2520The%2520creation%2520of%2520high-quality%2520human-labeled%2520image-caption%2520datasets%2520presents%2520a%250Asignificant%2520bottleneck%2520in%2520the%2520development%2520of%2520Visual-Language%2520Models%2520%2528VLMs%2529.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520an%2520approach%2520that%2520leverages%2520the%2520strengths%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520and%2520image%2520generation%2520models%2520to%2520create%2520synthetic%250Aimage-text%2520pairs%2520for%2520efficient%2520and%2520effective%2520VLM%2520training.%2520Our%2520method%2520employs%2520a%250Apretrained%2520text-to-image%2520model%2520to%2520synthesize%2520image%2520embeddings%2520from%2520captions%250Agenerated%2520by%2520an%2520LLM.%2520Despite%2520the%2520text-to-image%2520model%2520and%2520VLM%2520initially%2520being%250Atrained%2520on%2520the%2520same%2520data%252C%2520our%2520approach%2520leverages%2520the%2520image%2520generator%2527s%2520ability%250Ato%2520create%2520novel%2520compositions%252C%2520resulting%2520in%2520synthetic%2520image%2520embeddings%2520that%250Aexpand%2520beyond%2520the%2520limitations%2520of%2520the%2520original%2520dataset.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520VLM%252C%2520finetuned%2520on%2520synthetic%2520data%2520achieves%2520comparable%250Aperformance%2520to%2520models%2520trained%2520solely%2520on%2520human-annotated%2520data%252C%2520while%2520requiring%250Asignificantly%2520less%2520data.%2520Furthermore%252C%2520we%2520perform%2520a%2520set%2520of%2520analyses%2520on%2520captions%250Awhich%2520reveals%2520that%2520semantic%2520diversity%2520and%2520balance%2520are%2520key%2520aspects%2520for%2520better%250Adownstream%2520performance.%2520Finally%252C%2520we%2520show%2520that%2520synthesizing%2520images%2520in%2520the%2520image%250Aembedding%2520space%2520is%252025%255C%2525%2520faster%2520than%2520in%2520the%2520pixel%2520space.%2520We%2520believe%2520our%2520work%2520not%250Aonly%2520addresses%2520a%2520significant%2520challenge%2520in%2520VLM%2520training%2520but%2520also%2520opens%2520up%250Apromising%2520avenues%2520for%2520the%2520development%2520of%2520self-improving%2520multi-modal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synth%24%5E2%24%3A%20Boosting%20Visual-Language%20Models%20with%20Synthetic%20Captions%20and%0A%20%20Image%20Embeddings&entry.906535625=Sahand%20Sharifzadeh%20and%20Christos%20Kaplanis%20and%20Shreya%20Pathak%20and%20Dharshan%20Kumaran%20and%20Anastasija%20Ilic%20and%20Jovana%20Mitrovic%20and%20Charles%20Blundell%20and%20Andrea%20Banino&entry.1292438233=%20%20The%20creation%20of%20high-quality%20human-labeled%20image-caption%20datasets%20presents%20a%0Asignificant%20bottleneck%20in%20the%20development%20of%20Visual-Language%20Models%20%28VLMs%29.%20In%0Athis%20work%2C%20we%20investigate%20an%20approach%20that%20leverages%20the%20strengths%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20image%20generation%20models%20to%20create%20synthetic%0Aimage-text%20pairs%20for%20efficient%20and%20effective%20VLM%20training.%20Our%20method%20employs%20a%0Apretrained%20text-to-image%20model%20to%20synthesize%20image%20embeddings%20from%20captions%0Agenerated%20by%20an%20LLM.%20Despite%20the%20text-to-image%20model%20and%20VLM%20initially%20being%0Atrained%20on%20the%20same%20data%2C%20our%20approach%20leverages%20the%20image%20generator%27s%20ability%0Ato%20create%20novel%20compositions%2C%20resulting%20in%20synthetic%20image%20embeddings%20that%0Aexpand%20beyond%20the%20limitations%20of%20the%20original%20dataset.%20Extensive%20experiments%0Ademonstrate%20that%20our%20VLM%2C%20finetuned%20on%20synthetic%20data%20achieves%20comparable%0Aperformance%20to%20models%20trained%20solely%20on%20human-annotated%20data%2C%20while%20requiring%0Asignificantly%20less%20data.%20Furthermore%2C%20we%20perform%20a%20set%20of%20analyses%20on%20captions%0Awhich%20reveals%20that%20semantic%20diversity%20and%20balance%20are%20key%20aspects%20for%20better%0Adownstream%20performance.%20Finally%2C%20we%20show%20that%20synthesizing%20images%20in%20the%20image%0Aembedding%20space%20is%2025%5C%25%20faster%20than%20in%20the%20pixel%20space.%20We%20believe%20our%20work%20not%0Aonly%20addresses%20a%20significant%20challenge%20in%20VLM%20training%20but%20also%20opens%20up%0Apromising%20avenues%20for%20the%20development%20of%20self-improving%20multi-modal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07750v2&entry.124074799=Read"},
{"title": "3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion\n  Expression guided Video Segmentation", "author": "Feiyu Pan and Hao Fang and Xiankai Lu", "abstract": "  Referring video object segmentation (RVOS) relies on natural language\nexpressions to segment target objects in video, emphasizing modeling dense\ntext-video relations. The current RVOS methods typically use independently\npre-trained vision and language models as backbones, resulting in a significant\ndomain gap between video and text. In cross-modal feature interaction, text\nfeatures are only used as query initialization and do not fully utilize\nimportant information in the text. In this work, we propose using frozen\npre-trained vision-language models (VLM) as backbones, with a specific emphasis\non enhancing cross-modal feature interaction. Firstly, we use frozen\nconvolutional CLIP backbone to generate feature-aligned vision and text\nfeatures, alleviating the issue of domain gap and reducing training costs.\nSecondly, we add more cross-modal feature fusion in the pipeline to enhance the\nutilization of multi-modal information. Furthermore, we propose a novel video\nquery initialization method to generate higher quality video queries. Without\nbells and whistles, our method achieved 51.5 J&F on the MeViS test set and\nranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression\nguided Video Segmentation.\n", "link": "http://arxiv.org/abs/2406.04842v1", "date": "2024-06-07", "relevancy": 2.2523, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5869}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203rd%20Place%20Solution%20for%20MeViS%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Motion%0A%20%20Expression%20guided%20Video%20Segmentation&body=Title%3A%203rd%20Place%20Solution%20for%20MeViS%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Motion%0A%20%20Expression%20guided%20Video%20Segmentation%0AAuthor%3A%20Feiyu%20Pan%20and%20Hao%20Fang%20and%20Xiankai%20Lu%0AAbstract%3A%20%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20target%20objects%20in%20video%2C%20emphasizing%20modeling%20dense%0Atext-video%20relations.%20The%20current%20RVOS%20methods%20typically%20use%20independently%0Apre-trained%20vision%20and%20language%20models%20as%20backbones%2C%20resulting%20in%20a%20significant%0Adomain%20gap%20between%20video%20and%20text.%20In%20cross-modal%20feature%20interaction%2C%20text%0Afeatures%20are%20only%20used%20as%20query%20initialization%20and%20do%20not%20fully%20utilize%0Aimportant%20information%20in%20the%20text.%20In%20this%20work%2C%20we%20propose%20using%20frozen%0Apre-trained%20vision-language%20models%20%28VLM%29%20as%20backbones%2C%20with%20a%20specific%20emphasis%0Aon%20enhancing%20cross-modal%20feature%20interaction.%20Firstly%2C%20we%20use%20frozen%0Aconvolutional%20CLIP%20backbone%20to%20generate%20feature-aligned%20vision%20and%20text%0Afeatures%2C%20alleviating%20the%20issue%20of%20domain%20gap%20and%20reducing%20training%20costs.%0ASecondly%2C%20we%20add%20more%20cross-modal%20feature%20fusion%20in%20the%20pipeline%20to%20enhance%20the%0Autilization%20of%20multi-modal%20information.%20Furthermore%2C%20we%20propose%20a%20novel%20video%0Aquery%20initialization%20method%20to%20generate%20higher%20quality%20video%20queries.%20Without%0Abells%20and%20whistles%2C%20our%20method%20achieved%2051.5%20J%26F%20on%20the%20MeViS%20test%20set%20and%0Aranked%203rd%20place%20for%20MeViS%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Motion%20Expression%0Aguided%20Video%20Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3rd%2520Place%2520Solution%2520for%2520MeViS%2520Track%2520in%2520CVPR%25202024%2520PVUW%2520workshop%253A%2520Motion%250A%2520%2520Expression%2520guided%2520Video%2520Segmentation%26entry.906535625%3DFeiyu%2520Pan%2520and%2520Hao%2520Fang%2520and%2520Xiankai%2520Lu%26entry.1292438233%3D%2520%2520Referring%2520video%2520object%2520segmentation%2520%2528RVOS%2529%2520relies%2520on%2520natural%2520language%250Aexpressions%2520to%2520segment%2520target%2520objects%2520in%2520video%252C%2520emphasizing%2520modeling%2520dense%250Atext-video%2520relations.%2520The%2520current%2520RVOS%2520methods%2520typically%2520use%2520independently%250Apre-trained%2520vision%2520and%2520language%2520models%2520as%2520backbones%252C%2520resulting%2520in%2520a%2520significant%250Adomain%2520gap%2520between%2520video%2520and%2520text.%2520In%2520cross-modal%2520feature%2520interaction%252C%2520text%250Afeatures%2520are%2520only%2520used%2520as%2520query%2520initialization%2520and%2520do%2520not%2520fully%2520utilize%250Aimportant%2520information%2520in%2520the%2520text.%2520In%2520this%2520work%252C%2520we%2520propose%2520using%2520frozen%250Apre-trained%2520vision-language%2520models%2520%2528VLM%2529%2520as%2520backbones%252C%2520with%2520a%2520specific%2520emphasis%250Aon%2520enhancing%2520cross-modal%2520feature%2520interaction.%2520Firstly%252C%2520we%2520use%2520frozen%250Aconvolutional%2520CLIP%2520backbone%2520to%2520generate%2520feature-aligned%2520vision%2520and%2520text%250Afeatures%252C%2520alleviating%2520the%2520issue%2520of%2520domain%2520gap%2520and%2520reducing%2520training%2520costs.%250ASecondly%252C%2520we%2520add%2520more%2520cross-modal%2520feature%2520fusion%2520in%2520the%2520pipeline%2520to%2520enhance%2520the%250Autilization%2520of%2520multi-modal%2520information.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520video%250Aquery%2520initialization%2520method%2520to%2520generate%2520higher%2520quality%2520video%2520queries.%2520Without%250Abells%2520and%2520whistles%252C%2520our%2520method%2520achieved%252051.5%2520J%2526F%2520on%2520the%2520MeViS%2520test%2520set%2520and%250Aranked%25203rd%2520place%2520for%2520MeViS%2520Track%2520in%2520CVPR%25202024%2520PVUW%2520workshop%253A%2520Motion%2520Expression%250Aguided%2520Video%2520Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3rd%20Place%20Solution%20for%20MeViS%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Motion%0A%20%20Expression%20guided%20Video%20Segmentation&entry.906535625=Feiyu%20Pan%20and%20Hao%20Fang%20and%20Xiankai%20Lu&entry.1292438233=%20%20Referring%20video%20object%20segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20target%20objects%20in%20video%2C%20emphasizing%20modeling%20dense%0Atext-video%20relations.%20The%20current%20RVOS%20methods%20typically%20use%20independently%0Apre-trained%20vision%20and%20language%20models%20as%20backbones%2C%20resulting%20in%20a%20significant%0Adomain%20gap%20between%20video%20and%20text.%20In%20cross-modal%20feature%20interaction%2C%20text%0Afeatures%20are%20only%20used%20as%20query%20initialization%20and%20do%20not%20fully%20utilize%0Aimportant%20information%20in%20the%20text.%20In%20this%20work%2C%20we%20propose%20using%20frozen%0Apre-trained%20vision-language%20models%20%28VLM%29%20as%20backbones%2C%20with%20a%20specific%20emphasis%0Aon%20enhancing%20cross-modal%20feature%20interaction.%20Firstly%2C%20we%20use%20frozen%0Aconvolutional%20CLIP%20backbone%20to%20generate%20feature-aligned%20vision%20and%20text%0Afeatures%2C%20alleviating%20the%20issue%20of%20domain%20gap%20and%20reducing%20training%20costs.%0ASecondly%2C%20we%20add%20more%20cross-modal%20feature%20fusion%20in%20the%20pipeline%20to%20enhance%20the%0Autilization%20of%20multi-modal%20information.%20Furthermore%2C%20we%20propose%20a%20novel%20video%0Aquery%20initialization%20method%20to%20generate%20higher%20quality%20video%20queries.%20Without%0Abells%20and%20whistles%2C%20our%20method%20achieved%2051.5%20J%26F%20on%20the%20MeViS%20test%20set%20and%0Aranked%203rd%20place%20for%20MeViS%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Motion%20Expression%0Aguided%20Video%20Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04842v1&entry.124074799=Read"},
{"title": "Diving Deep into the Motion Representation of Video-Text Models", "author": "Chinmaya Devaraj and Cornelia Fermuller and Yiannis Aloimonos", "abstract": "  Videos are more informative than images because they capture the dynamics of\nthe scene. By representing motion in videos, we can capture dynamic activities.\nIn this work, we introduce GPT-4 generated motion descriptions that capture\nfine-grained motion descriptions of activities and apply them to three action\ndatasets. We evaluated several video-text models on the task of retrieval of\nmotion descriptions. We found that they fall far behind human expert\nperformance on two action datasets, raising the question of whether video-text\nmodels understand motion in videos. To address it, we introduce a method of\nimproving motion understanding in video-text models by utilizing motion\ndescriptions. This method proves to be effective on two action datasets for the\nmotion description retrieval task. The results draw attention to the need for\nquality captions involving fine-grained motion information in existing datasets\nand demonstrate the effectiveness of the proposed pipeline in understanding\nfine-grained motion during video-text retrieval.\n", "link": "http://arxiv.org/abs/2406.05075v1", "date": "2024-06-07", "relevancy": 2.2445, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6382}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diving%20Deep%20into%20the%20Motion%20Representation%20of%20Video-Text%20Models&body=Title%3A%20Diving%20Deep%20into%20the%20Motion%20Representation%20of%20Video-Text%20Models%0AAuthor%3A%20Chinmaya%20Devaraj%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20Videos%20are%20more%20informative%20than%20images%20because%20they%20capture%20the%20dynamics%20of%0Athe%20scene.%20By%20representing%20motion%20in%20videos%2C%20we%20can%20capture%20dynamic%20activities.%0AIn%20this%20work%2C%20we%20introduce%20GPT-4%20generated%20motion%20descriptions%20that%20capture%0Afine-grained%20motion%20descriptions%20of%20activities%20and%20apply%20them%20to%20three%20action%0Adatasets.%20We%20evaluated%20several%20video-text%20models%20on%20the%20task%20of%20retrieval%20of%0Amotion%20descriptions.%20We%20found%20that%20they%20fall%20far%20behind%20human%20expert%0Aperformance%20on%20two%20action%20datasets%2C%20raising%20the%20question%20of%20whether%20video-text%0Amodels%20understand%20motion%20in%20videos.%20To%20address%20it%2C%20we%20introduce%20a%20method%20of%0Aimproving%20motion%20understanding%20in%20video-text%20models%20by%20utilizing%20motion%0Adescriptions.%20This%20method%20proves%20to%20be%20effective%20on%20two%20action%20datasets%20for%20the%0Amotion%20description%20retrieval%20task.%20The%20results%20draw%20attention%20to%20the%20need%20for%0Aquality%20captions%20involving%20fine-grained%20motion%20information%20in%20existing%20datasets%0Aand%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20pipeline%20in%20understanding%0Afine-grained%20motion%20during%20video-text%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiving%2520Deep%2520into%2520the%2520Motion%2520Representation%2520of%2520Video-Text%2520Models%26entry.906535625%3DChinmaya%2520Devaraj%2520and%2520Cornelia%2520Fermuller%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520Videos%2520are%2520more%2520informative%2520than%2520images%2520because%2520they%2520capture%2520the%2520dynamics%2520of%250Athe%2520scene.%2520By%2520representing%2520motion%2520in%2520videos%252C%2520we%2520can%2520capture%2520dynamic%2520activities.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520GPT-4%2520generated%2520motion%2520descriptions%2520that%2520capture%250Afine-grained%2520motion%2520descriptions%2520of%2520activities%2520and%2520apply%2520them%2520to%2520three%2520action%250Adatasets.%2520We%2520evaluated%2520several%2520video-text%2520models%2520on%2520the%2520task%2520of%2520retrieval%2520of%250Amotion%2520descriptions.%2520We%2520found%2520that%2520they%2520fall%2520far%2520behind%2520human%2520expert%250Aperformance%2520on%2520two%2520action%2520datasets%252C%2520raising%2520the%2520question%2520of%2520whether%2520video-text%250Amodels%2520understand%2520motion%2520in%2520videos.%2520To%2520address%2520it%252C%2520we%2520introduce%2520a%2520method%2520of%250Aimproving%2520motion%2520understanding%2520in%2520video-text%2520models%2520by%2520utilizing%2520motion%250Adescriptions.%2520This%2520method%2520proves%2520to%2520be%2520effective%2520on%2520two%2520action%2520datasets%2520for%2520the%250Amotion%2520description%2520retrieval%2520task.%2520The%2520results%2520draw%2520attention%2520to%2520the%2520need%2520for%250Aquality%2520captions%2520involving%2520fine-grained%2520motion%2520information%2520in%2520existing%2520datasets%250Aand%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520pipeline%2520in%2520understanding%250Afine-grained%2520motion%2520during%2520video-text%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diving%20Deep%20into%20the%20Motion%20Representation%20of%20Video-Text%20Models&entry.906535625=Chinmaya%20Devaraj%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20Videos%20are%20more%20informative%20than%20images%20because%20they%20capture%20the%20dynamics%20of%0Athe%20scene.%20By%20representing%20motion%20in%20videos%2C%20we%20can%20capture%20dynamic%20activities.%0AIn%20this%20work%2C%20we%20introduce%20GPT-4%20generated%20motion%20descriptions%20that%20capture%0Afine-grained%20motion%20descriptions%20of%20activities%20and%20apply%20them%20to%20three%20action%0Adatasets.%20We%20evaluated%20several%20video-text%20models%20on%20the%20task%20of%20retrieval%20of%0Amotion%20descriptions.%20We%20found%20that%20they%20fall%20far%20behind%20human%20expert%0Aperformance%20on%20two%20action%20datasets%2C%20raising%20the%20question%20of%20whether%20video-text%0Amodels%20understand%20motion%20in%20videos.%20To%20address%20it%2C%20we%20introduce%20a%20method%20of%0Aimproving%20motion%20understanding%20in%20video-text%20models%20by%20utilizing%20motion%0Adescriptions.%20This%20method%20proves%20to%20be%20effective%20on%20two%20action%20datasets%20for%20the%0Amotion%20description%20retrieval%20task.%20The%20results%20draw%20attention%20to%20the%20need%20for%0Aquality%20captions%20involving%20fine-grained%20motion%20information%20in%20existing%20datasets%0Aand%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20pipeline%20in%20understanding%0Afine-grained%20motion%20during%20video-text%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05075v1&entry.124074799=Read"},
{"title": "Auto-Multilift: Distributed Learning and Control for Cooperative Load\n  Transportation With Quadrotors", "author": "Bingheng Wang and Kuankuan Sima and Rui Huang and Lin Zhao", "abstract": "  Designing motion control and planning algorithms for multilift systems\nremains challenging due to the complexities of dynamics, collision avoidance,\nactuator limits, and scalability. Existing methods that use optimization and\ndistributed techniques effectively address these constraints and scalability\nissues. However, they often require substantial manual tuning, leading to\nsuboptimal performance. This paper proposes Auto-Multilift, a novel framework\nthat automates the tuning of model predictive controllers (MPCs) for multilift\nsystems. We model the MPC cost functions with deep neural networks (DNNs),\nenabling fast online adaptation to various scenarios. We develop a distributed\npolicy gradient algorithm to train these DNNs efficiently in a closed-loop\nmanner. Central to our algorithm is distributed sensitivity propagation, which\nparallelizes gradient computation across quadrotors, focusing on actual system\nstate sensitivities relative to key MPC parameters. We also provide theoretical\nguarantees for the convergence of this algorithm. Extensive simulations show\nrapid convergence and favorable scalability to a large number of quadrotors.\nOur method outperforms a state-of-the-art open-loop MPC tuning approach by\neffectively learning adaptive MPCs from trajectory tracking errors and handling\nthe unique dynamics couplings within the multilift system. Additionally, our\nframework can learn an adaptive reference for reconfigurating the system when\ntraversing through multiple narrow slots.\n", "link": "http://arxiv.org/abs/2406.04858v1", "date": "2024-06-07", "relevancy": 2.2384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5833}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5595}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&body=Title%3A%20Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors%0AAuthor%3A%20Bingheng%20Wang%20and%20Kuankuan%20Sima%20and%20Rui%20Huang%20and%20Lin%20Zhao%0AAbstract%3A%20%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Aparallelizes%20gradient%20computation%20across%20quadrotors%2C%20focusing%20on%20actual%20system%0Astate%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20We%20also%20provide%20theoretical%0Aguarantees%20for%20the%20convergence%20of%20this%20algorithm.%20Extensive%20simulations%20show%0Arapid%20convergence%20and%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors%20and%20handling%0Athe%20unique%20dynamics%20couplings%20within%20the%20multilift%20system.%20Additionally%2C%20our%0Aframework%20can%20learn%20an%20adaptive%20reference%20for%20reconfigurating%20the%20system%20when%0Atraversing%20through%20multiple%20narrow%20slots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Multilift%253A%2520Distributed%2520Learning%2520and%2520Control%2520for%2520Cooperative%2520Load%250A%2520%2520Transportation%2520With%2520Quadrotors%26entry.906535625%3DBingheng%2520Wang%2520and%2520Kuankuan%2520Sima%2520and%2520Rui%2520Huang%2520and%2520Lin%2520Zhao%26entry.1292438233%3D%2520%2520Designing%2520motion%2520control%2520and%2520planning%2520algorithms%2520for%2520multilift%2520systems%250Aremains%2520challenging%2520due%2520to%2520the%2520complexities%2520of%2520dynamics%252C%2520collision%2520avoidance%252C%250Aactuator%2520limits%252C%2520and%2520scalability.%2520Existing%2520methods%2520that%2520use%2520optimization%2520and%250Adistributed%2520techniques%2520effectively%2520address%2520these%2520constraints%2520and%2520scalability%250Aissues.%2520However%252C%2520they%2520often%2520require%2520substantial%2520manual%2520tuning%252C%2520leading%2520to%250Asuboptimal%2520performance.%2520This%2520paper%2520proposes%2520Auto-Multilift%252C%2520a%2520novel%2520framework%250Athat%2520automates%2520the%2520tuning%2520of%2520model%2520predictive%2520controllers%2520%2528MPCs%2529%2520for%2520multilift%250Asystems.%2520We%2520model%2520the%2520MPC%2520cost%2520functions%2520with%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%250Aenabling%2520fast%2520online%2520adaptation%2520to%2520various%2520scenarios.%2520We%2520develop%2520a%2520distributed%250Apolicy%2520gradient%2520algorithm%2520to%2520train%2520these%2520DNNs%2520efficiently%2520in%2520a%2520closed-loop%250Amanner.%2520Central%2520to%2520our%2520algorithm%2520is%2520distributed%2520sensitivity%2520propagation%252C%2520which%250Aparallelizes%2520gradient%2520computation%2520across%2520quadrotors%252C%2520focusing%2520on%2520actual%2520system%250Astate%2520sensitivities%2520relative%2520to%2520key%2520MPC%2520parameters.%2520We%2520also%2520provide%2520theoretical%250Aguarantees%2520for%2520the%2520convergence%2520of%2520this%2520algorithm.%2520Extensive%2520simulations%2520show%250Arapid%2520convergence%2520and%2520favorable%2520scalability%2520to%2520a%2520large%2520number%2520of%2520quadrotors.%250AOur%2520method%2520outperforms%2520a%2520state-of-the-art%2520open-loop%2520MPC%2520tuning%2520approach%2520by%250Aeffectively%2520learning%2520adaptive%2520MPCs%2520from%2520trajectory%2520tracking%2520errors%2520and%2520handling%250Athe%2520unique%2520dynamics%2520couplings%2520within%2520the%2520multilift%2520system.%2520Additionally%252C%2520our%250Aframework%2520can%2520learn%2520an%2520adaptive%2520reference%2520for%2520reconfigurating%2520the%2520system%2520when%250Atraversing%2520through%2520multiple%2520narrow%2520slots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Multilift%3A%20Distributed%20Learning%20and%20Control%20for%20Cooperative%20Load%0A%20%20Transportation%20With%20Quadrotors&entry.906535625=Bingheng%20Wang%20and%20Kuankuan%20Sima%20and%20Rui%20Huang%20and%20Lin%20Zhao&entry.1292438233=%20%20Designing%20motion%20control%20and%20planning%20algorithms%20for%20multilift%20systems%0Aremains%20challenging%20due%20to%20the%20complexities%20of%20dynamics%2C%20collision%20avoidance%2C%0Aactuator%20limits%2C%20and%20scalability.%20Existing%20methods%20that%20use%20optimization%20and%0Adistributed%20techniques%20effectively%20address%20these%20constraints%20and%20scalability%0Aissues.%20However%2C%20they%20often%20require%20substantial%20manual%20tuning%2C%20leading%20to%0Asuboptimal%20performance.%20This%20paper%20proposes%20Auto-Multilift%2C%20a%20novel%20framework%0Athat%20automates%20the%20tuning%20of%20model%20predictive%20controllers%20%28MPCs%29%20for%20multilift%0Asystems.%20We%20model%20the%20MPC%20cost%20functions%20with%20deep%20neural%20networks%20%28DNNs%29%2C%0Aenabling%20fast%20online%20adaptation%20to%20various%20scenarios.%20We%20develop%20a%20distributed%0Apolicy%20gradient%20algorithm%20to%20train%20these%20DNNs%20efficiently%20in%20a%20closed-loop%0Amanner.%20Central%20to%20our%20algorithm%20is%20distributed%20sensitivity%20propagation%2C%20which%0Aparallelizes%20gradient%20computation%20across%20quadrotors%2C%20focusing%20on%20actual%20system%0Astate%20sensitivities%20relative%20to%20key%20MPC%20parameters.%20We%20also%20provide%20theoretical%0Aguarantees%20for%20the%20convergence%20of%20this%20algorithm.%20Extensive%20simulations%20show%0Arapid%20convergence%20and%20favorable%20scalability%20to%20a%20large%20number%20of%20quadrotors.%0AOur%20method%20outperforms%20a%20state-of-the-art%20open-loop%20MPC%20tuning%20approach%20by%0Aeffectively%20learning%20adaptive%20MPCs%20from%20trajectory%20tracking%20errors%20and%20handling%0Athe%20unique%20dynamics%20couplings%20within%20the%20multilift%20system.%20Additionally%2C%20our%0Aframework%20can%20learn%20an%20adaptive%20reference%20for%20reconfigurating%20the%20system%20when%0Atraversing%20through%20multiple%20narrow%20slots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04858v1&entry.124074799=Read"},
{"title": "Exploiting Activation Sparsity with Dense to Dynamic-k\n  Mixture-of-Experts Conversion", "author": "Filip Szatkowski and Bartosz W\u00f3jcik and Miko\u0142aj Pi\u00f3rczy\u0144ski and Simone Scardapane", "abstract": "  Transformer models can face practical limitations due to their high\ncomputational requirements. At the same time, such models exhibit significant\nactivation sparsity, which can be leveraged to reduce the inference cost by\nconverting parts of the network into equivalent Mixture-of-Experts (MoE)\nlayers. Despite the crucial role played by activation sparsity, its impact on\nthis process remains unexplored. In particular, we show that the efficiency of\nthe conversion can be significantly enhanced by a proper regularization of the\nactivation sparsity of the base model. Moreover, motivated by the high variance\nof the number of activated neurons for different inputs, we introduce a more\neffective dynamic-k expert selection rule that adjusts the number of executed\nexperts on a per-token basis. Finally, we extend this approach to multi-head\nattention projections, which results in additional savings compared to only\nconverting the FFN blocks. The proposed method, Dense to Dynamic-$k$\nMixture-of-Experts (D2DMoE), outperforms existing approaches on common NLP and\nvision tasks, allowing us to save up to 60% of inference cost without\nsignificantly affecting model performance.\n", "link": "http://arxiv.org/abs/2310.04361v3", "date": "2024-06-07", "relevancy": 2.2379, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5673}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5666}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion&body=Title%3A%20Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion%0AAuthor%3A%20Filip%20Szatkowski%20and%20Bartosz%20W%C3%B3jcik%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20Transformer%20models%20can%20face%20practical%20limitations%20due%20to%20their%20high%0Acomputational%20requirements.%20At%20the%20same%20time%2C%20such%20models%20exhibit%20significant%0Aactivation%20sparsity%2C%20which%20can%20be%20leveraged%20to%20reduce%20the%20inference%20cost%20by%0Aconverting%20parts%20of%20the%20network%20into%20equivalent%20Mixture-of-Experts%20%28MoE%29%0Alayers.%20Despite%20the%20crucial%20role%20played%20by%20activation%20sparsity%2C%20its%20impact%20on%0Athis%20process%20remains%20unexplored.%20In%20particular%2C%20we%20show%20that%20the%20efficiency%20of%0Athe%20conversion%20can%20be%20significantly%20enhanced%20by%20a%20proper%20regularization%20of%20the%0Aactivation%20sparsity%20of%20the%20base%20model.%20Moreover%2C%20motivated%20by%20the%20high%20variance%0Aof%20the%20number%20of%20activated%20neurons%20for%20different%20inputs%2C%20we%20introduce%20a%20more%0Aeffective%20dynamic-k%20expert%20selection%20rule%20that%20adjusts%20the%20number%20of%20executed%0Aexperts%20on%20a%20per-token%20basis.%20Finally%2C%20we%20extend%20this%20approach%20to%20multi-head%0Aattention%20projections%2C%20which%20results%20in%20additional%20savings%20compared%20to%20only%0Aconverting%20the%20FFN%20blocks.%20The%20proposed%20method%2C%20Dense%20to%20Dynamic-%24k%24%0AMixture-of-Experts%20%28D2DMoE%29%2C%20outperforms%20existing%20approaches%20on%20common%20NLP%20and%0Avision%20tasks%2C%20allowing%20us%20to%20save%20up%20to%2060%25%20of%20inference%20cost%20without%0Asignificantly%20affecting%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04361v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Activation%2520Sparsity%2520with%2520Dense%2520to%2520Dynamic-k%250A%2520%2520Mixture-of-Experts%2520Conversion%26entry.906535625%3DFilip%2520Szatkowski%2520and%2520Bartosz%2520W%25C3%25B3jcik%2520and%2520Miko%25C5%2582aj%2520Pi%25C3%25B3rczy%25C5%2584ski%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520Transformer%2520models%2520can%2520face%2520practical%2520limitations%2520due%2520to%2520their%2520high%250Acomputational%2520requirements.%2520At%2520the%2520same%2520time%252C%2520such%2520models%2520exhibit%2520significant%250Aactivation%2520sparsity%252C%2520which%2520can%2520be%2520leveraged%2520to%2520reduce%2520the%2520inference%2520cost%2520by%250Aconverting%2520parts%2520of%2520the%2520network%2520into%2520equivalent%2520Mixture-of-Experts%2520%2528MoE%2529%250Alayers.%2520Despite%2520the%2520crucial%2520role%2520played%2520by%2520activation%2520sparsity%252C%2520its%2520impact%2520on%250Athis%2520process%2520remains%2520unexplored.%2520In%2520particular%252C%2520we%2520show%2520that%2520the%2520efficiency%2520of%250Athe%2520conversion%2520can%2520be%2520significantly%2520enhanced%2520by%2520a%2520proper%2520regularization%2520of%2520the%250Aactivation%2520sparsity%2520of%2520the%2520base%2520model.%2520Moreover%252C%2520motivated%2520by%2520the%2520high%2520variance%250Aof%2520the%2520number%2520of%2520activated%2520neurons%2520for%2520different%2520inputs%252C%2520we%2520introduce%2520a%2520more%250Aeffective%2520dynamic-k%2520expert%2520selection%2520rule%2520that%2520adjusts%2520the%2520number%2520of%2520executed%250Aexperts%2520on%2520a%2520per-token%2520basis.%2520Finally%252C%2520we%2520extend%2520this%2520approach%2520to%2520multi-head%250Aattention%2520projections%252C%2520which%2520results%2520in%2520additional%2520savings%2520compared%2520to%2520only%250Aconverting%2520the%2520FFN%2520blocks.%2520The%2520proposed%2520method%252C%2520Dense%2520to%2520Dynamic-%2524k%2524%250AMixture-of-Experts%2520%2528D2DMoE%2529%252C%2520outperforms%2520existing%2520approaches%2520on%2520common%2520NLP%2520and%250Avision%2520tasks%252C%2520allowing%2520us%2520to%2520save%2520up%2520to%252060%2525%2520of%2520inference%2520cost%2520without%250Asignificantly%2520affecting%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04361v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion&entry.906535625=Filip%20Szatkowski%20and%20Bartosz%20W%C3%B3jcik%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane&entry.1292438233=%20%20Transformer%20models%20can%20face%20practical%20limitations%20due%20to%20their%20high%0Acomputational%20requirements.%20At%20the%20same%20time%2C%20such%20models%20exhibit%20significant%0Aactivation%20sparsity%2C%20which%20can%20be%20leveraged%20to%20reduce%20the%20inference%20cost%20by%0Aconverting%20parts%20of%20the%20network%20into%20equivalent%20Mixture-of-Experts%20%28MoE%29%0Alayers.%20Despite%20the%20crucial%20role%20played%20by%20activation%20sparsity%2C%20its%20impact%20on%0Athis%20process%20remains%20unexplored.%20In%20particular%2C%20we%20show%20that%20the%20efficiency%20of%0Athe%20conversion%20can%20be%20significantly%20enhanced%20by%20a%20proper%20regularization%20of%20the%0Aactivation%20sparsity%20of%20the%20base%20model.%20Moreover%2C%20motivated%20by%20the%20high%20variance%0Aof%20the%20number%20of%20activated%20neurons%20for%20different%20inputs%2C%20we%20introduce%20a%20more%0Aeffective%20dynamic-k%20expert%20selection%20rule%20that%20adjusts%20the%20number%20of%20executed%0Aexperts%20on%20a%20per-token%20basis.%20Finally%2C%20we%20extend%20this%20approach%20to%20multi-head%0Aattention%20projections%2C%20which%20results%20in%20additional%20savings%20compared%20to%20only%0Aconverting%20the%20FFN%20blocks.%20The%20proposed%20method%2C%20Dense%20to%20Dynamic-%24k%24%0AMixture-of-Experts%20%28D2DMoE%29%2C%20outperforms%20existing%20approaches%20on%20common%20NLP%20and%0Avision%20tasks%2C%20allowing%20us%20to%20save%20up%20to%2060%25%20of%20inference%20cost%20without%0Asignificantly%20affecting%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04361v3&entry.124074799=Read"},
{"title": "3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs", "author": "Jianing Yang and Xuweiyi Chen and Nikhil Madaan and Madhavan Iyengar and Shengyi Qian and David F. Fouhey and Joyce Chai", "abstract": "  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n", "link": "http://arxiv.org/abs/2406.05132v1", "date": "2024-06-07", "relevancy": 2.229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-GRAND%3A%20Towards%20Better%20Grounding%20and%20Less%20Hallucination%20for%203D-LLMs&body=Title%3A%203D-GRAND%3A%20Towards%20Better%20Grounding%20and%20Less%20Hallucination%20for%203D-LLMs%0AAuthor%3A%20Jianing%20Yang%20and%20Xuweiyi%20Chen%20and%20Nikhil%20Madaan%20and%20Madhavan%20Iyengar%20and%20Shengyi%20Qian%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20crucial%20for%20developing%0Aembodied%20agents%20and%20robots%20that%20comprehend%20and%20interact%20with%20the%20physical%0Aworld.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20language%0Aunderstanding%20and%20generation%20capabilities%2C%20their%20adaptation%20to%203D%20environments%0A%283D-LLMs%29%20remains%20in%20its%20early%20stages.%20A%20primary%20challenge%20is%20the%20absence%20of%0Alarge-scale%20datasets%20that%20provide%20dense%20grounding%20between%20language%20and%203D%0Ascenes.%20In%20this%20paper%2C%20we%20introduce%203D-GRAND%2C%20a%20pioneering%20large-scale%20dataset%0Acomprising%2040%2C087%20household%20scenes%20paired%20with%206.2%20million%20densely-grounded%0Ascene-language%20instructions.%20Our%20results%20show%20that%20instruction%20tuning%20with%0A3D-GRAND%20significantly%20enhances%20grounding%20capabilities%20and%20reduces%0Ahallucinations%20in%203D-LLMs.%20As%20part%20of%20our%20contributions%2C%20we%20propose%20a%0Acomprehensive%20benchmark%203D-POPE%20to%20systematically%20evaluate%20hallucination%20in%0A3D-LLMs%2C%20enabling%20fair%20comparisons%20among%20future%20models.%20Our%20experiments%0Ahighlight%20a%20scaling%20effect%20between%20dataset%20size%20and%203D-LLM%20performance%2C%0Aemphasizing%20the%20critical%20role%20of%20large-scale%203D-text%20datasets%20in%20advancing%0Aembodied%20AI%20research.%20Notably%2C%20our%20results%20demonstrate%20early%20signals%20for%0Aeffective%20sim-to-real%20transfer%2C%20indicating%20that%20models%20trained%20on%20large%0Asynthetic%20data%20can%20perform%20well%20on%20real-world%203D%20scans.%20Through%203D-GRAND%20and%0A3D-POPE%2C%20we%20aim%20to%20equip%20the%20embodied%20AI%20community%20with%20essential%20resources%20and%0Ainsights%2C%20setting%20the%20stage%20for%20more%20reliable%20and%20better-grounded%203D-LLMs.%0AProject%20website%3A%20https%3A//3d-grand.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-GRAND%253A%2520Towards%2520Better%2520Grounding%2520and%2520Less%2520Hallucination%2520for%25203D-LLMs%26entry.906535625%3DJianing%2520Yang%2520and%2520Xuweiyi%2520Chen%2520and%2520Nikhil%2520Madaan%2520and%2520Madhavan%2520Iyengar%2520and%2520Shengyi%2520Qian%2520and%2520David%2520F.%2520Fouhey%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520language%2520and%25203D%2520perception%2520is%2520crucial%2520for%2520developing%250Aembodied%2520agents%2520and%2520robots%2520that%2520comprehend%2520and%2520interact%2520with%2520the%2520physical%250Aworld.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520language%250Aunderstanding%2520and%2520generation%2520capabilities%252C%2520their%2520adaptation%2520to%25203D%2520environments%250A%25283D-LLMs%2529%2520remains%2520in%2520its%2520early%2520stages.%2520A%2520primary%2520challenge%2520is%2520the%2520absence%2520of%250Alarge-scale%2520datasets%2520that%2520provide%2520dense%2520grounding%2520between%2520language%2520and%25203D%250Ascenes.%2520In%2520this%2520paper%252C%2520we%2520introduce%25203D-GRAND%252C%2520a%2520pioneering%2520large-scale%2520dataset%250Acomprising%252040%252C087%2520household%2520scenes%2520paired%2520with%25206.2%2520million%2520densely-grounded%250Ascene-language%2520instructions.%2520Our%2520results%2520show%2520that%2520instruction%2520tuning%2520with%250A3D-GRAND%2520significantly%2520enhances%2520grounding%2520capabilities%2520and%2520reduces%250Ahallucinations%2520in%25203D-LLMs.%2520As%2520part%2520of%2520our%2520contributions%252C%2520we%2520propose%2520a%250Acomprehensive%2520benchmark%25203D-POPE%2520to%2520systematically%2520evaluate%2520hallucination%2520in%250A3D-LLMs%252C%2520enabling%2520fair%2520comparisons%2520among%2520future%2520models.%2520Our%2520experiments%250Ahighlight%2520a%2520scaling%2520effect%2520between%2520dataset%2520size%2520and%25203D-LLM%2520performance%252C%250Aemphasizing%2520the%2520critical%2520role%2520of%2520large-scale%25203D-text%2520datasets%2520in%2520advancing%250Aembodied%2520AI%2520research.%2520Notably%252C%2520our%2520results%2520demonstrate%2520early%2520signals%2520for%250Aeffective%2520sim-to-real%2520transfer%252C%2520indicating%2520that%2520models%2520trained%2520on%2520large%250Asynthetic%2520data%2520can%2520perform%2520well%2520on%2520real-world%25203D%2520scans.%2520Through%25203D-GRAND%2520and%250A3D-POPE%252C%2520we%2520aim%2520to%2520equip%2520the%2520embodied%2520AI%2520community%2520with%2520essential%2520resources%2520and%250Ainsights%252C%2520setting%2520the%2520stage%2520for%2520more%2520reliable%2520and%2520better-grounded%25203D-LLMs.%250AProject%2520website%253A%2520https%253A//3d-grand.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-GRAND%3A%20Towards%20Better%20Grounding%20and%20Less%20Hallucination%20for%203D-LLMs&entry.906535625=Jianing%20Yang%20and%20Xuweiyi%20Chen%20and%20Nikhil%20Madaan%20and%20Madhavan%20Iyengar%20and%20Shengyi%20Qian%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai&entry.1292438233=%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20crucial%20for%20developing%0Aembodied%20agents%20and%20robots%20that%20comprehend%20and%20interact%20with%20the%20physical%0Aworld.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20language%0Aunderstanding%20and%20generation%20capabilities%2C%20their%20adaptation%20to%203D%20environments%0A%283D-LLMs%29%20remains%20in%20its%20early%20stages.%20A%20primary%20challenge%20is%20the%20absence%20of%0Alarge-scale%20datasets%20that%20provide%20dense%20grounding%20between%20language%20and%203D%0Ascenes.%20In%20this%20paper%2C%20we%20introduce%203D-GRAND%2C%20a%20pioneering%20large-scale%20dataset%0Acomprising%2040%2C087%20household%20scenes%20paired%20with%206.2%20million%20densely-grounded%0Ascene-language%20instructions.%20Our%20results%20show%20that%20instruction%20tuning%20with%0A3D-GRAND%20significantly%20enhances%20grounding%20capabilities%20and%20reduces%0Ahallucinations%20in%203D-LLMs.%20As%20part%20of%20our%20contributions%2C%20we%20propose%20a%0Acomprehensive%20benchmark%203D-POPE%20to%20systematically%20evaluate%20hallucination%20in%0A3D-LLMs%2C%20enabling%20fair%20comparisons%20among%20future%20models.%20Our%20experiments%0Ahighlight%20a%20scaling%20effect%20between%20dataset%20size%20and%203D-LLM%20performance%2C%0Aemphasizing%20the%20critical%20role%20of%20large-scale%203D-text%20datasets%20in%20advancing%0Aembodied%20AI%20research.%20Notably%2C%20our%20results%20demonstrate%20early%20signals%20for%0Aeffective%20sim-to-real%20transfer%2C%20indicating%20that%20models%20trained%20on%20large%0Asynthetic%20data%20can%20perform%20well%20on%20real-world%203D%20scans.%20Through%203D-GRAND%20and%0A3D-POPE%2C%20we%20aim%20to%20equip%20the%20embodied%20AI%20community%20with%20essential%20resources%20and%0Ainsights%2C%20setting%20the%20stage%20for%20more%20reliable%20and%20better-grounded%203D-LLMs.%0AProject%20website%3A%20https%3A//3d-grand.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05132v1&entry.124074799=Read"},
{"title": "Bootstrapping Referring Multi-Object Tracking", "author": "Yani Zhang and Dongming Wu and Wencheng Han and Xingping Dong", "abstract": "  Referring multi-object tracking (RMOT) aims at detecting and tracking\nmultiple objects following human instruction represented by a natural language\nexpression. Existing RMOT benchmarks are usually formulated through manual\nannotations, integrated with static regulations. This approach results in a\ndearth of notable diversity and a constrained scope of implementation. In this\nwork, our key idea is to bootstrap the task of referring multi-object tracking\nby introducing discriminative language words as much as possible. In specific,\nwe first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2.\nIt starts with 2,719 manual annotations, addressing the issue of class\nimbalance and introducing more keywords to make it closer to real-world\nscenarios compared to Refer-KITTI. They are further expanded to a total of\n9,758 annotations by prompting large language models, which create 617\ndifferent words, surpassing previous RMOT benchmarks. In addition, the\nend-to-end framework in RMOT is also bootstrapped by a simple yet elegant\ntemporal advancement strategy, which achieves better performance than previous\napproaches. The source code and dataset is available at\nhttps://github.com/zyn213/TempRMOT.\n", "link": "http://arxiv.org/abs/2406.05039v1", "date": "2024-06-07", "relevancy": 2.2192, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Referring%20Multi-Object%20Tracking&body=Title%3A%20Bootstrapping%20Referring%20Multi-Object%20Tracking%0AAuthor%3A%20Yani%20Zhang%20and%20Dongming%20Wu%20and%20Wencheng%20Han%20and%20Xingping%20Dong%0AAbstract%3A%20%20%20Referring%20multi-object%20tracking%20%28RMOT%29%20aims%20at%20detecting%20and%20tracking%0Amultiple%20objects%20following%20human%20instruction%20represented%20by%20a%20natural%20language%0Aexpression.%20Existing%20RMOT%20benchmarks%20are%20usually%20formulated%20through%20manual%0Aannotations%2C%20integrated%20with%20static%20regulations.%20This%20approach%20results%20in%20a%0Adearth%20of%20notable%20diversity%20and%20a%20constrained%20scope%20of%20implementation.%20In%20this%0Awork%2C%20our%20key%20idea%20is%20to%20bootstrap%20the%20task%20of%20referring%20multi-object%20tracking%0Aby%20introducing%20discriminative%20language%20words%20as%20much%20as%20possible.%20In%20specific%2C%0Awe%20first%20develop%20Refer-KITTI%20into%20a%20large-scale%20dataset%2C%20named%20Refer-KITTI-V2.%0AIt%20starts%20with%202%2C719%20manual%20annotations%2C%20addressing%20the%20issue%20of%20class%0Aimbalance%20and%20introducing%20more%20keywords%20to%20make%20it%20closer%20to%20real-world%0Ascenarios%20compared%20to%20Refer-KITTI.%20They%20are%20further%20expanded%20to%20a%20total%20of%0A9%2C758%20annotations%20by%20prompting%20large%20language%20models%2C%20which%20create%20617%0Adifferent%20words%2C%20surpassing%20previous%20RMOT%20benchmarks.%20In%20addition%2C%20the%0Aend-to-end%20framework%20in%20RMOT%20is%20also%20bootstrapped%20by%20a%20simple%20yet%20elegant%0Atemporal%20advancement%20strategy%2C%20which%20achieves%20better%20performance%20than%20previous%0Aapproaches.%20The%20source%20code%20and%20dataset%20is%20available%20at%0Ahttps%3A//github.com/zyn213/TempRMOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Referring%2520Multi-Object%2520Tracking%26entry.906535625%3DYani%2520Zhang%2520and%2520Dongming%2520Wu%2520and%2520Wencheng%2520Han%2520and%2520Xingping%2520Dong%26entry.1292438233%3D%2520%2520Referring%2520multi-object%2520tracking%2520%2528RMOT%2529%2520aims%2520at%2520detecting%2520and%2520tracking%250Amultiple%2520objects%2520following%2520human%2520instruction%2520represented%2520by%2520a%2520natural%2520language%250Aexpression.%2520Existing%2520RMOT%2520benchmarks%2520are%2520usually%2520formulated%2520through%2520manual%250Aannotations%252C%2520integrated%2520with%2520static%2520regulations.%2520This%2520approach%2520results%2520in%2520a%250Adearth%2520of%2520notable%2520diversity%2520and%2520a%2520constrained%2520scope%2520of%2520implementation.%2520In%2520this%250Awork%252C%2520our%2520key%2520idea%2520is%2520to%2520bootstrap%2520the%2520task%2520of%2520referring%2520multi-object%2520tracking%250Aby%2520introducing%2520discriminative%2520language%2520words%2520as%2520much%2520as%2520possible.%2520In%2520specific%252C%250Awe%2520first%2520develop%2520Refer-KITTI%2520into%2520a%2520large-scale%2520dataset%252C%2520named%2520Refer-KITTI-V2.%250AIt%2520starts%2520with%25202%252C719%2520manual%2520annotations%252C%2520addressing%2520the%2520issue%2520of%2520class%250Aimbalance%2520and%2520introducing%2520more%2520keywords%2520to%2520make%2520it%2520closer%2520to%2520real-world%250Ascenarios%2520compared%2520to%2520Refer-KITTI.%2520They%2520are%2520further%2520expanded%2520to%2520a%2520total%2520of%250A9%252C758%2520annotations%2520by%2520prompting%2520large%2520language%2520models%252C%2520which%2520create%2520617%250Adifferent%2520words%252C%2520surpassing%2520previous%2520RMOT%2520benchmarks.%2520In%2520addition%252C%2520the%250Aend-to-end%2520framework%2520in%2520RMOT%2520is%2520also%2520bootstrapped%2520by%2520a%2520simple%2520yet%2520elegant%250Atemporal%2520advancement%2520strategy%252C%2520which%2520achieves%2520better%2520performance%2520than%2520previous%250Aapproaches.%2520The%2520source%2520code%2520and%2520dataset%2520is%2520available%2520at%250Ahttps%253A//github.com/zyn213/TempRMOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Referring%20Multi-Object%20Tracking&entry.906535625=Yani%20Zhang%20and%20Dongming%20Wu%20and%20Wencheng%20Han%20and%20Xingping%20Dong&entry.1292438233=%20%20Referring%20multi-object%20tracking%20%28RMOT%29%20aims%20at%20detecting%20and%20tracking%0Amultiple%20objects%20following%20human%20instruction%20represented%20by%20a%20natural%20language%0Aexpression.%20Existing%20RMOT%20benchmarks%20are%20usually%20formulated%20through%20manual%0Aannotations%2C%20integrated%20with%20static%20regulations.%20This%20approach%20results%20in%20a%0Adearth%20of%20notable%20diversity%20and%20a%20constrained%20scope%20of%20implementation.%20In%20this%0Awork%2C%20our%20key%20idea%20is%20to%20bootstrap%20the%20task%20of%20referring%20multi-object%20tracking%0Aby%20introducing%20discriminative%20language%20words%20as%20much%20as%20possible.%20In%20specific%2C%0Awe%20first%20develop%20Refer-KITTI%20into%20a%20large-scale%20dataset%2C%20named%20Refer-KITTI-V2.%0AIt%20starts%20with%202%2C719%20manual%20annotations%2C%20addressing%20the%20issue%20of%20class%0Aimbalance%20and%20introducing%20more%20keywords%20to%20make%20it%20closer%20to%20real-world%0Ascenarios%20compared%20to%20Refer-KITTI.%20They%20are%20further%20expanded%20to%20a%20total%20of%0A9%2C758%20annotations%20by%20prompting%20large%20language%20models%2C%20which%20create%20617%0Adifferent%20words%2C%20surpassing%20previous%20RMOT%20benchmarks.%20In%20addition%2C%20the%0Aend-to-end%20framework%20in%20RMOT%20is%20also%20bootstrapped%20by%20a%20simple%20yet%20elegant%0Atemporal%20advancement%20strategy%2C%20which%20achieves%20better%20performance%20than%20previous%0Aapproaches.%20The%20source%20code%20and%20dataset%20is%20available%20at%0Ahttps%3A//github.com/zyn213/TempRMOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05039v1&entry.124074799=Read"},
{"title": "AGBD: A Global-scale Biomass Dataset", "author": "Ghjulia Sialelli and Torben Peters and Jan D. Wegner and Konrad Schindler", "abstract": "  Accurate estimates of Above Ground Biomass (AGB) are essential in addressing\ntwo of humanity's biggest challenges, climate change and biodiversity loss.\nExisting datasets for AGB estimation from satellite imagery are limited. Either\nthey focus on specific, local regions at high resolution, or they offer global\ncoverage at low resolution. There is a need for a machine learning-ready,\nglobally representative, high-resolution benchmark. Our findings indicate\nsignificant variability in biomass estimates across different vegetation types,\nemphasizing the necessity for a dataset that accurately captures global\ndiversity. To address these gaps, we introduce a comprehensive new dataset that\nis globally distributed, covers a range of vegetation types, and spans several\nyears. This dataset combines AGB reference data from the GEDI mission with data\nfrom Sentinel-2 and PALSAR-2 imagery. Additionally, it includes pre-processed\nhigh-level features such as a dense canopy height map, an elevation map, and a\nland-cover classification map. We also produce a dense, high-resolution (10m)\nmap of AGB predictions for the entire area covered by the dataset. Rigorously\ntested, our dataset is accompanied by several benchmark models and is publicly\navailable. It can be easily accessed using a single line of code, offering a\nsolid basis for efforts towards global AGB estimation. The GitHub repository\ngithub.com/ghjuliasialelli/AGBD serves as a one-stop shop for all code and\ndata.\n", "link": "http://arxiv.org/abs/2406.04928v1", "date": "2024-06-07", "relevancy": 2.2119, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4503}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4441}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGBD%3A%20A%20Global-scale%20Biomass%20Dataset&body=Title%3A%20AGBD%3A%20A%20Global-scale%20Biomass%20Dataset%0AAuthor%3A%20Ghjulia%20Sialelli%20and%20Torben%20Peters%20and%20Jan%20D.%20Wegner%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20Accurate%20estimates%20of%20Above%20Ground%20Biomass%20%28AGB%29%20are%20essential%20in%20addressing%0Atwo%20of%20humanity%27s%20biggest%20challenges%2C%20climate%20change%20and%20biodiversity%20loss.%0AExisting%20datasets%20for%20AGB%20estimation%20from%20satellite%20imagery%20are%20limited.%20Either%0Athey%20focus%20on%20specific%2C%20local%20regions%20at%20high%20resolution%2C%20or%20they%20offer%20global%0Acoverage%20at%20low%20resolution.%20There%20is%20a%20need%20for%20a%20machine%20learning-ready%2C%0Aglobally%20representative%2C%20high-resolution%20benchmark.%20Our%20findings%20indicate%0Asignificant%20variability%20in%20biomass%20estimates%20across%20different%20vegetation%20types%2C%0Aemphasizing%20the%20necessity%20for%20a%20dataset%20that%20accurately%20captures%20global%0Adiversity.%20To%20address%20these%20gaps%2C%20we%20introduce%20a%20comprehensive%20new%20dataset%20that%0Ais%20globally%20distributed%2C%20covers%20a%20range%20of%20vegetation%20types%2C%20and%20spans%20several%0Ayears.%20This%20dataset%20combines%20AGB%20reference%20data%20from%20the%20GEDI%20mission%20with%20data%0Afrom%20Sentinel-2%20and%20PALSAR-2%20imagery.%20Additionally%2C%20it%20includes%20pre-processed%0Ahigh-level%20features%20such%20as%20a%20dense%20canopy%20height%20map%2C%20an%20elevation%20map%2C%20and%20a%0Aland-cover%20classification%20map.%20We%20also%20produce%20a%20dense%2C%20high-resolution%20%2810m%29%0Amap%20of%20AGB%20predictions%20for%20the%20entire%20area%20covered%20by%20the%20dataset.%20Rigorously%0Atested%2C%20our%20dataset%20is%20accompanied%20by%20several%20benchmark%20models%20and%20is%20publicly%0Aavailable.%20It%20can%20be%20easily%20accessed%20using%20a%20single%20line%20of%20code%2C%20offering%20a%0Asolid%20basis%20for%20efforts%20towards%20global%20AGB%20estimation.%20The%20GitHub%20repository%0Agithub.com/ghjuliasialelli/AGBD%20serves%20as%20a%20one-stop%20shop%20for%20all%20code%20and%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGBD%253A%2520A%2520Global-scale%2520Biomass%2520Dataset%26entry.906535625%3DGhjulia%2520Sialelli%2520and%2520Torben%2520Peters%2520and%2520Jan%2520D.%2520Wegner%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520Accurate%2520estimates%2520of%2520Above%2520Ground%2520Biomass%2520%2528AGB%2529%2520are%2520essential%2520in%2520addressing%250Atwo%2520of%2520humanity%2527s%2520biggest%2520challenges%252C%2520climate%2520change%2520and%2520biodiversity%2520loss.%250AExisting%2520datasets%2520for%2520AGB%2520estimation%2520from%2520satellite%2520imagery%2520are%2520limited.%2520Either%250Athey%2520focus%2520on%2520specific%252C%2520local%2520regions%2520at%2520high%2520resolution%252C%2520or%2520they%2520offer%2520global%250Acoverage%2520at%2520low%2520resolution.%2520There%2520is%2520a%2520need%2520for%2520a%2520machine%2520learning-ready%252C%250Aglobally%2520representative%252C%2520high-resolution%2520benchmark.%2520Our%2520findings%2520indicate%250Asignificant%2520variability%2520in%2520biomass%2520estimates%2520across%2520different%2520vegetation%2520types%252C%250Aemphasizing%2520the%2520necessity%2520for%2520a%2520dataset%2520that%2520accurately%2520captures%2520global%250Adiversity.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520a%2520comprehensive%2520new%2520dataset%2520that%250Ais%2520globally%2520distributed%252C%2520covers%2520a%2520range%2520of%2520vegetation%2520types%252C%2520and%2520spans%2520several%250Ayears.%2520This%2520dataset%2520combines%2520AGB%2520reference%2520data%2520from%2520the%2520GEDI%2520mission%2520with%2520data%250Afrom%2520Sentinel-2%2520and%2520PALSAR-2%2520imagery.%2520Additionally%252C%2520it%2520includes%2520pre-processed%250Ahigh-level%2520features%2520such%2520as%2520a%2520dense%2520canopy%2520height%2520map%252C%2520an%2520elevation%2520map%252C%2520and%2520a%250Aland-cover%2520classification%2520map.%2520We%2520also%2520produce%2520a%2520dense%252C%2520high-resolution%2520%252810m%2529%250Amap%2520of%2520AGB%2520predictions%2520for%2520the%2520entire%2520area%2520covered%2520by%2520the%2520dataset.%2520Rigorously%250Atested%252C%2520our%2520dataset%2520is%2520accompanied%2520by%2520several%2520benchmark%2520models%2520and%2520is%2520publicly%250Aavailable.%2520It%2520can%2520be%2520easily%2520accessed%2520using%2520a%2520single%2520line%2520of%2520code%252C%2520offering%2520a%250Asolid%2520basis%2520for%2520efforts%2520towards%2520global%2520AGB%2520estimation.%2520The%2520GitHub%2520repository%250Agithub.com/ghjuliasialelli/AGBD%2520serves%2520as%2520a%2520one-stop%2520shop%2520for%2520all%2520code%2520and%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGBD%3A%20A%20Global-scale%20Biomass%20Dataset&entry.906535625=Ghjulia%20Sialelli%20and%20Torben%20Peters%20and%20Jan%20D.%20Wegner%20and%20Konrad%20Schindler&entry.1292438233=%20%20Accurate%20estimates%20of%20Above%20Ground%20Biomass%20%28AGB%29%20are%20essential%20in%20addressing%0Atwo%20of%20humanity%27s%20biggest%20challenges%2C%20climate%20change%20and%20biodiversity%20loss.%0AExisting%20datasets%20for%20AGB%20estimation%20from%20satellite%20imagery%20are%20limited.%20Either%0Athey%20focus%20on%20specific%2C%20local%20regions%20at%20high%20resolution%2C%20or%20they%20offer%20global%0Acoverage%20at%20low%20resolution.%20There%20is%20a%20need%20for%20a%20machine%20learning-ready%2C%0Aglobally%20representative%2C%20high-resolution%20benchmark.%20Our%20findings%20indicate%0Asignificant%20variability%20in%20biomass%20estimates%20across%20different%20vegetation%20types%2C%0Aemphasizing%20the%20necessity%20for%20a%20dataset%20that%20accurately%20captures%20global%0Adiversity.%20To%20address%20these%20gaps%2C%20we%20introduce%20a%20comprehensive%20new%20dataset%20that%0Ais%20globally%20distributed%2C%20covers%20a%20range%20of%20vegetation%20types%2C%20and%20spans%20several%0Ayears.%20This%20dataset%20combines%20AGB%20reference%20data%20from%20the%20GEDI%20mission%20with%20data%0Afrom%20Sentinel-2%20and%20PALSAR-2%20imagery.%20Additionally%2C%20it%20includes%20pre-processed%0Ahigh-level%20features%20such%20as%20a%20dense%20canopy%20height%20map%2C%20an%20elevation%20map%2C%20and%20a%0Aland-cover%20classification%20map.%20We%20also%20produce%20a%20dense%2C%20high-resolution%20%2810m%29%0Amap%20of%20AGB%20predictions%20for%20the%20entire%20area%20covered%20by%20the%20dataset.%20Rigorously%0Atested%2C%20our%20dataset%20is%20accompanied%20by%20several%20benchmark%20models%20and%20is%20publicly%0Aavailable.%20It%20can%20be%20easily%20accessed%20using%20a%20single%20line%20of%20code%2C%20offering%20a%0Asolid%20basis%20for%20efforts%20towards%20global%20AGB%20estimation.%20The%20GitHub%20repository%0Agithub.com/ghjuliasialelli/AGBD%20serves%20as%20a%20one-stop%20shop%20for%20all%20code%20and%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04928v1&entry.124074799=Read"},
{"title": "RU-AI: A Large Multimodal Dataset for Machine Generated Content\n  Detection", "author": "Liting Huang and Zhihao Zhang and Yiran Zhang and Xiyue Zhou and Shoujin Wang", "abstract": "  The recent advancements in generative AI models, which can create realistic\nand human-like content, are significantly transforming how people communicate,\ncreate, and work. While the appropriate use of generative AI models can benefit\nthe society, their misuse poses significant threats to data reliability and\nauthentication. However, due to a lack of aligned multimodal datasets,\neffective and robust methods for detecting machine-generated content are still\nin the early stages of development. In this paper, we introduce RU-AI, a new\nlarge-scale multimodal dataset designed for the robust and efficient detection\nof machine-generated content in text, image, and voice. Our dataset is\nconstructed from three large publicly available datasets: Flickr8K, COCO, and\nPlaces205, by combining the original datasets and their corresponding\nmachine-generated pairs. Additionally, experimental results show that our\nproposed unified model, which incorporates a multimodal embedding module with a\nmultilayer perceptron network, can effectively determine the origin of the data\n(i.e., original data samples or machine-generated ones) from RU-AI. However,\nfuture work is still required to address the remaining challenges posed by\nRU-AI. The source code and dataset are available at\nhttps://github.com/ZhihaoZhang97/RU-AI.\n", "link": "http://arxiv.org/abs/2406.04906v1", "date": "2024-06-07", "relevancy": 2.2069, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5573}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RU-AI%3A%20A%20Large%20Multimodal%20Dataset%20for%20Machine%20Generated%20Content%0A%20%20Detection&body=Title%3A%20RU-AI%3A%20A%20Large%20Multimodal%20Dataset%20for%20Machine%20Generated%20Content%0A%20%20Detection%0AAuthor%3A%20Liting%20Huang%20and%20Zhihao%20Zhang%20and%20Yiran%20Zhang%20and%20Xiyue%20Zhou%20and%20Shoujin%20Wang%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20generative%20AI%20models%2C%20which%20can%20create%20realistic%0Aand%20human-like%20content%2C%20are%20significantly%20transforming%20how%20people%20communicate%2C%0Acreate%2C%20and%20work.%20While%20the%20appropriate%20use%20of%20generative%20AI%20models%20can%20benefit%0Athe%20society%2C%20their%20misuse%20poses%20significant%20threats%20to%20data%20reliability%20and%0Aauthentication.%20However%2C%20due%20to%20a%20lack%20of%20aligned%20multimodal%20datasets%2C%0Aeffective%20and%20robust%20methods%20for%20detecting%20machine-generated%20content%20are%20still%0Ain%20the%20early%20stages%20of%20development.%20In%20this%20paper%2C%20we%20introduce%20RU-AI%2C%20a%20new%0Alarge-scale%20multimodal%20dataset%20designed%20for%20the%20robust%20and%20efficient%20detection%0Aof%20machine-generated%20content%20in%20text%2C%20image%2C%20and%20voice.%20Our%20dataset%20is%0Aconstructed%20from%20three%20large%20publicly%20available%20datasets%3A%20Flickr8K%2C%20COCO%2C%20and%0APlaces205%2C%20by%20combining%20the%20original%20datasets%20and%20their%20corresponding%0Amachine-generated%20pairs.%20Additionally%2C%20experimental%20results%20show%20that%20our%0Aproposed%20unified%20model%2C%20which%20incorporates%20a%20multimodal%20embedding%20module%20with%20a%0Amultilayer%20perceptron%20network%2C%20can%20effectively%20determine%20the%20origin%20of%20the%20data%0A%28i.e.%2C%20original%20data%20samples%20or%20machine-generated%20ones%29%20from%20RU-AI.%20However%2C%0Afuture%20work%20is%20still%20required%20to%20address%20the%20remaining%20challenges%20posed%20by%0ARU-AI.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ZhihaoZhang97/RU-AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRU-AI%253A%2520A%2520Large%2520Multimodal%2520Dataset%2520for%2520Machine%2520Generated%2520Content%250A%2520%2520Detection%26entry.906535625%3DLiting%2520Huang%2520and%2520Zhihao%2520Zhang%2520and%2520Yiran%2520Zhang%2520and%2520Xiyue%2520Zhou%2520and%2520Shoujin%2520Wang%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520generative%2520AI%2520models%252C%2520which%2520can%2520create%2520realistic%250Aand%2520human-like%2520content%252C%2520are%2520significantly%2520transforming%2520how%2520people%2520communicate%252C%250Acreate%252C%2520and%2520work.%2520While%2520the%2520appropriate%2520use%2520of%2520generative%2520AI%2520models%2520can%2520benefit%250Athe%2520society%252C%2520their%2520misuse%2520poses%2520significant%2520threats%2520to%2520data%2520reliability%2520and%250Aauthentication.%2520However%252C%2520due%2520to%2520a%2520lack%2520of%2520aligned%2520multimodal%2520datasets%252C%250Aeffective%2520and%2520robust%2520methods%2520for%2520detecting%2520machine-generated%2520content%2520are%2520still%250Ain%2520the%2520early%2520stages%2520of%2520development.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RU-AI%252C%2520a%2520new%250Alarge-scale%2520multimodal%2520dataset%2520designed%2520for%2520the%2520robust%2520and%2520efficient%2520detection%250Aof%2520machine-generated%2520content%2520in%2520text%252C%2520image%252C%2520and%2520voice.%2520Our%2520dataset%2520is%250Aconstructed%2520from%2520three%2520large%2520publicly%2520available%2520datasets%253A%2520Flickr8K%252C%2520COCO%252C%2520and%250APlaces205%252C%2520by%2520combining%2520the%2520original%2520datasets%2520and%2520their%2520corresponding%250Amachine-generated%2520pairs.%2520Additionally%252C%2520experimental%2520results%2520show%2520that%2520our%250Aproposed%2520unified%2520model%252C%2520which%2520incorporates%2520a%2520multimodal%2520embedding%2520module%2520with%2520a%250Amultilayer%2520perceptron%2520network%252C%2520can%2520effectively%2520determine%2520the%2520origin%2520of%2520the%2520data%250A%2528i.e.%252C%2520original%2520data%2520samples%2520or%2520machine-generated%2520ones%2529%2520from%2520RU-AI.%2520However%252C%250Afuture%2520work%2520is%2520still%2520required%2520to%2520address%2520the%2520remaining%2520challenges%2520posed%2520by%250ARU-AI.%2520The%2520source%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/ZhihaoZhang97/RU-AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RU-AI%3A%20A%20Large%20Multimodal%20Dataset%20for%20Machine%20Generated%20Content%0A%20%20Detection&entry.906535625=Liting%20Huang%20and%20Zhihao%20Zhang%20and%20Yiran%20Zhang%20and%20Xiyue%20Zhou%20and%20Shoujin%20Wang&entry.1292438233=%20%20The%20recent%20advancements%20in%20generative%20AI%20models%2C%20which%20can%20create%20realistic%0Aand%20human-like%20content%2C%20are%20significantly%20transforming%20how%20people%20communicate%2C%0Acreate%2C%20and%20work.%20While%20the%20appropriate%20use%20of%20generative%20AI%20models%20can%20benefit%0Athe%20society%2C%20their%20misuse%20poses%20significant%20threats%20to%20data%20reliability%20and%0Aauthentication.%20However%2C%20due%20to%20a%20lack%20of%20aligned%20multimodal%20datasets%2C%0Aeffective%20and%20robust%20methods%20for%20detecting%20machine-generated%20content%20are%20still%0Ain%20the%20early%20stages%20of%20development.%20In%20this%20paper%2C%20we%20introduce%20RU-AI%2C%20a%20new%0Alarge-scale%20multimodal%20dataset%20designed%20for%20the%20robust%20and%20efficient%20detection%0Aof%20machine-generated%20content%20in%20text%2C%20image%2C%20and%20voice.%20Our%20dataset%20is%0Aconstructed%20from%20three%20large%20publicly%20available%20datasets%3A%20Flickr8K%2C%20COCO%2C%20and%0APlaces205%2C%20by%20combining%20the%20original%20datasets%20and%20their%20corresponding%0Amachine-generated%20pairs.%20Additionally%2C%20experimental%20results%20show%20that%20our%0Aproposed%20unified%20model%2C%20which%20incorporates%20a%20multimodal%20embedding%20module%20with%20a%0Amultilayer%20perceptron%20network%2C%20can%20effectively%20determine%20the%20origin%20of%20the%20data%0A%28i.e.%2C%20original%20data%20samples%20or%20machine-generated%20ones%29%20from%20RU-AI.%20However%2C%0Afuture%20work%20is%20still%20required%20to%20address%20the%20remaining%20challenges%20posed%20by%0ARU-AI.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ZhihaoZhang97/RU-AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04906v1&entry.124074799=Read"},
{"title": "Submodular Framework for Structured-Sparse Optimal Transport", "author": "Piyushi Manupriya and Pratik Jawanpuria and Karthik S. Gurumoorthy and SakethaNath Jagarlapudi and Bamdev Mishra", "abstract": "  Unbalanced optimal transport (UOT) has recently gained much attention due to\nits flexible framework for handling un-normalized measures and its robustness\nproperties. In this work, we explore learning (structured) sparse transport\nplans in the UOT setting, i.e., transport plans have an upper bound on the\nnumber of non-sparse entries in each column (structured sparse pattern) or in\nthe whole plan (general sparse pattern). We propose novel sparsity-constrained\nUOT formulations building on the recently explored maximum mean discrepancy\nbased UOT. We show that the proposed optimization problem is equivalent to the\nmaximization of a weakly submodular function over a uniform matroid or a\npartition matroid. We develop efficient gradient-based discrete greedy\nalgorithms and provide the corresponding theoretical guarantees. Empirically,\nwe observe that our proposed greedy algorithms select a diverse support set and\nwe illustrate the efficacy of the proposed approach in various applications.\n", "link": "http://arxiv.org/abs/2406.04914v1", "date": "2024-06-07", "relevancy": 2.1951, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4507}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport&body=Title%3A%20Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport%0AAuthor%3A%20Piyushi%20Manupriya%20and%20Pratik%20Jawanpuria%20and%20Karthik%20S.%20Gurumoorthy%20and%20SakethaNath%20Jagarlapudi%20and%20Bamdev%20Mishra%0AAbstract%3A%20%20%20Unbalanced%20optimal%20transport%20%28UOT%29%20has%20recently%20gained%20much%20attention%20due%20to%0Aits%20flexible%20framework%20for%20handling%20un-normalized%20measures%20and%20its%20robustness%0Aproperties.%20In%20this%20work%2C%20we%20explore%20learning%20%28structured%29%20sparse%20transport%0Aplans%20in%20the%20UOT%20setting%2C%20i.e.%2C%20transport%20plans%20have%20an%20upper%20bound%20on%20the%0Anumber%20of%20non-sparse%20entries%20in%20each%20column%20%28structured%20sparse%20pattern%29%20or%20in%0Athe%20whole%20plan%20%28general%20sparse%20pattern%29.%20We%20propose%20novel%20sparsity-constrained%0AUOT%20formulations%20building%20on%20the%20recently%20explored%20maximum%20mean%20discrepancy%0Abased%20UOT.%20We%20show%20that%20the%20proposed%20optimization%20problem%20is%20equivalent%20to%20the%0Amaximization%20of%20a%20weakly%20submodular%20function%20over%20a%20uniform%20matroid%20or%20a%0Apartition%20matroid.%20We%20develop%20efficient%20gradient-based%20discrete%20greedy%0Aalgorithms%20and%20provide%20the%20corresponding%20theoretical%20guarantees.%20Empirically%2C%0Awe%20observe%20that%20our%20proposed%20greedy%20algorithms%20select%20a%20diverse%20support%20set%20and%0Awe%20illustrate%20the%20efficacy%20of%20the%20proposed%20approach%20in%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubmodular%2520Framework%2520for%2520Structured-Sparse%2520Optimal%2520Transport%26entry.906535625%3DPiyushi%2520Manupriya%2520and%2520Pratik%2520Jawanpuria%2520and%2520Karthik%2520S.%2520Gurumoorthy%2520and%2520SakethaNath%2520Jagarlapudi%2520and%2520Bamdev%2520Mishra%26entry.1292438233%3D%2520%2520Unbalanced%2520optimal%2520transport%2520%2528UOT%2529%2520has%2520recently%2520gained%2520much%2520attention%2520due%2520to%250Aits%2520flexible%2520framework%2520for%2520handling%2520un-normalized%2520measures%2520and%2520its%2520robustness%250Aproperties.%2520In%2520this%2520work%252C%2520we%2520explore%2520learning%2520%2528structured%2529%2520sparse%2520transport%250Aplans%2520in%2520the%2520UOT%2520setting%252C%2520i.e.%252C%2520transport%2520plans%2520have%2520an%2520upper%2520bound%2520on%2520the%250Anumber%2520of%2520non-sparse%2520entries%2520in%2520each%2520column%2520%2528structured%2520sparse%2520pattern%2529%2520or%2520in%250Athe%2520whole%2520plan%2520%2528general%2520sparse%2520pattern%2529.%2520We%2520propose%2520novel%2520sparsity-constrained%250AUOT%2520formulations%2520building%2520on%2520the%2520recently%2520explored%2520maximum%2520mean%2520discrepancy%250Abased%2520UOT.%2520We%2520show%2520that%2520the%2520proposed%2520optimization%2520problem%2520is%2520equivalent%2520to%2520the%250Amaximization%2520of%2520a%2520weakly%2520submodular%2520function%2520over%2520a%2520uniform%2520matroid%2520or%2520a%250Apartition%2520matroid.%2520We%2520develop%2520efficient%2520gradient-based%2520discrete%2520greedy%250Aalgorithms%2520and%2520provide%2520the%2520corresponding%2520theoretical%2520guarantees.%2520Empirically%252C%250Awe%2520observe%2520that%2520our%2520proposed%2520greedy%2520algorithms%2520select%2520a%2520diverse%2520support%2520set%2520and%250Awe%2520illustrate%2520the%2520efficacy%2520of%2520the%2520proposed%2520approach%2520in%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Submodular%20Framework%20for%20Structured-Sparse%20Optimal%20Transport&entry.906535625=Piyushi%20Manupriya%20and%20Pratik%20Jawanpuria%20and%20Karthik%20S.%20Gurumoorthy%20and%20SakethaNath%20Jagarlapudi%20and%20Bamdev%20Mishra&entry.1292438233=%20%20Unbalanced%20optimal%20transport%20%28UOT%29%20has%20recently%20gained%20much%20attention%20due%20to%0Aits%20flexible%20framework%20for%20handling%20un-normalized%20measures%20and%20its%20robustness%0Aproperties.%20In%20this%20work%2C%20we%20explore%20learning%20%28structured%29%20sparse%20transport%0Aplans%20in%20the%20UOT%20setting%2C%20i.e.%2C%20transport%20plans%20have%20an%20upper%20bound%20on%20the%0Anumber%20of%20non-sparse%20entries%20in%20each%20column%20%28structured%20sparse%20pattern%29%20or%20in%0Athe%20whole%20plan%20%28general%20sparse%20pattern%29.%20We%20propose%20novel%20sparsity-constrained%0AUOT%20formulations%20building%20on%20the%20recently%20explored%20maximum%20mean%20discrepancy%0Abased%20UOT.%20We%20show%20that%20the%20proposed%20optimization%20problem%20is%20equivalent%20to%20the%0Amaximization%20of%20a%20weakly%20submodular%20function%20over%20a%20uniform%20matroid%20or%20a%0Apartition%20matroid.%20We%20develop%20efficient%20gradient-based%20discrete%20greedy%0Aalgorithms%20and%20provide%20the%20corresponding%20theoretical%20guarantees.%20Empirically%2C%0Awe%20observe%20that%20our%20proposed%20greedy%20algorithms%20select%20a%20diverse%20support%20set%20and%0Awe%20illustrate%20the%20efficacy%20of%20the%20proposed%20approach%20in%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04914v1&entry.124074799=Read"},
{"title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "author": "Fengqing Jiang and Zhangchen Xu and Luyao Niu and Zhen Xiang and Bhaskar Ramasubramanian and Bo Li and Radha Poovendran", "abstract": "  Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs. Our code is available at\nhttps://github.com/uw-nsl/ArtPrompt.\n", "link": "http://arxiv.org/abs/2402.11753v4", "date": "2024-06-07", "relevancy": 2.1928, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4537}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4326}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtPrompt%3A%20ASCII%20Art-based%20Jailbreak%20Attacks%20against%20Aligned%20LLMs&body=Title%3A%20ArtPrompt%3A%20ASCII%20Art-based%20Jailbreak%20Attacks%20against%20Aligned%20LLMs%0AAuthor%3A%20Fengqing%20Jiang%20and%20Zhangchen%20Xu%20and%20Luyao%20Niu%20and%20Zhen%20Xiang%20and%20Bhaskar%20Ramasubramanian%20and%20Bo%20Li%20and%20Radha%20Poovendran%0AAbstract%3A%20%20%20Safety%20is%20critical%20to%20the%20usage%20of%20large%20language%20models%20%28LLMs%29.%20Multiple%0Atechniques%20such%20as%20data%20filtering%20and%20supervised%20fine-tuning%20have%20been%0Adeveloped%20to%20strengthen%20LLM%20safety.%20However%2C%20currently%20known%20techniques%20presume%0Athat%20corpora%20used%20for%20safety%20alignment%20of%20LLMs%20are%20solely%20interpreted%20by%0Asemantics.%20This%20assumption%2C%20however%2C%20does%20not%20hold%20in%20real-world%20applications%2C%0Awhich%20leads%20to%20severe%20vulnerabilities%20in%20LLMs.%20For%20example%2C%20users%20of%20forums%0Aoften%20use%20ASCII%20art%2C%20a%20form%20of%20text-based%20art%2C%20to%20convey%20image%20information.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20ASCII%20art-based%20jailbreak%20attack%20and%20introduce%20a%0Acomprehensive%20benchmark%20Vision-in-Text%20Challenge%20%28ViTC%29%20to%20evaluate%20the%0Acapabilities%20of%20LLMs%20in%20recognizing%20prompts%20that%20cannot%20be%20solely%20interpreted%0Aby%20semantics.%20We%20show%20that%20five%20SOTA%20LLMs%20%28GPT-3.5%2C%20GPT-4%2C%20Gemini%2C%20Claude%2C%20and%0ALlama2%29%20struggle%20to%20recognize%20prompts%20provided%20in%20the%20form%20of%20ASCII%20art.%20Based%0Aon%20this%20observation%2C%20we%20develop%20the%20jailbreak%20attack%20ArtPrompt%2C%20which%20leverages%0Athe%20poor%20performance%20of%20LLMs%20in%20recognizing%20ASCII%20art%20to%20bypass%20safety%20measures%0Aand%20elicit%20undesired%20behaviors%20from%20LLMs.%20ArtPrompt%20only%20requires%20black-box%0Aaccess%20to%20the%20victim%20LLMs%2C%20making%20it%20a%20practical%20attack.%20We%20evaluate%20ArtPrompt%0Aon%20five%20SOTA%20LLMs%2C%20and%20show%20that%20ArtPrompt%20can%20effectively%20and%20efficiently%0Ainduce%20undesired%20behaviors%20from%20all%20five%20LLMs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/uw-nsl/ArtPrompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11753v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtPrompt%253A%2520ASCII%2520Art-based%2520Jailbreak%2520Attacks%2520against%2520Aligned%2520LLMs%26entry.906535625%3DFengqing%2520Jiang%2520and%2520Zhangchen%2520Xu%2520and%2520Luyao%2520Niu%2520and%2520Zhen%2520Xiang%2520and%2520Bhaskar%2520Ramasubramanian%2520and%2520Bo%2520Li%2520and%2520Radha%2520Poovendran%26entry.1292438233%3D%2520%2520Safety%2520is%2520critical%2520to%2520the%2520usage%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Multiple%250Atechniques%2520such%2520as%2520data%2520filtering%2520and%2520supervised%2520fine-tuning%2520have%2520been%250Adeveloped%2520to%2520strengthen%2520LLM%2520safety.%2520However%252C%2520currently%2520known%2520techniques%2520presume%250Athat%2520corpora%2520used%2520for%2520safety%2520alignment%2520of%2520LLMs%2520are%2520solely%2520interpreted%2520by%250Asemantics.%2520This%2520assumption%252C%2520however%252C%2520does%2520not%2520hold%2520in%2520real-world%2520applications%252C%250Awhich%2520leads%2520to%2520severe%2520vulnerabilities%2520in%2520LLMs.%2520For%2520example%252C%2520users%2520of%2520forums%250Aoften%2520use%2520ASCII%2520art%252C%2520a%2520form%2520of%2520text-based%2520art%252C%2520to%2520convey%2520image%2520information.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520ASCII%2520art-based%2520jailbreak%2520attack%2520and%2520introduce%2520a%250Acomprehensive%2520benchmark%2520Vision-in-Text%2520Challenge%2520%2528ViTC%2529%2520to%2520evaluate%2520the%250Acapabilities%2520of%2520LLMs%2520in%2520recognizing%2520prompts%2520that%2520cannot%2520be%2520solely%2520interpreted%250Aby%2520semantics.%2520We%2520show%2520that%2520five%2520SOTA%2520LLMs%2520%2528GPT-3.5%252C%2520GPT-4%252C%2520Gemini%252C%2520Claude%252C%2520and%250ALlama2%2529%2520struggle%2520to%2520recognize%2520prompts%2520provided%2520in%2520the%2520form%2520of%2520ASCII%2520art.%2520Based%250Aon%2520this%2520observation%252C%2520we%2520develop%2520the%2520jailbreak%2520attack%2520ArtPrompt%252C%2520which%2520leverages%250Athe%2520poor%2520performance%2520of%2520LLMs%2520in%2520recognizing%2520ASCII%2520art%2520to%2520bypass%2520safety%2520measures%250Aand%2520elicit%2520undesired%2520behaviors%2520from%2520LLMs.%2520ArtPrompt%2520only%2520requires%2520black-box%250Aaccess%2520to%2520the%2520victim%2520LLMs%252C%2520making%2520it%2520a%2520practical%2520attack.%2520We%2520evaluate%2520ArtPrompt%250Aon%2520five%2520SOTA%2520LLMs%252C%2520and%2520show%2520that%2520ArtPrompt%2520can%2520effectively%2520and%2520efficiently%250Ainduce%2520undesired%2520behaviors%2520from%2520all%2520five%2520LLMs.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/uw-nsl/ArtPrompt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11753v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtPrompt%3A%20ASCII%20Art-based%20Jailbreak%20Attacks%20against%20Aligned%20LLMs&entry.906535625=Fengqing%20Jiang%20and%20Zhangchen%20Xu%20and%20Luyao%20Niu%20and%20Zhen%20Xiang%20and%20Bhaskar%20Ramasubramanian%20and%20Bo%20Li%20and%20Radha%20Poovendran&entry.1292438233=%20%20Safety%20is%20critical%20to%20the%20usage%20of%20large%20language%20models%20%28LLMs%29.%20Multiple%0Atechniques%20such%20as%20data%20filtering%20and%20supervised%20fine-tuning%20have%20been%0Adeveloped%20to%20strengthen%20LLM%20safety.%20However%2C%20currently%20known%20techniques%20presume%0Athat%20corpora%20used%20for%20safety%20alignment%20of%20LLMs%20are%20solely%20interpreted%20by%0Asemantics.%20This%20assumption%2C%20however%2C%20does%20not%20hold%20in%20real-world%20applications%2C%0Awhich%20leads%20to%20severe%20vulnerabilities%20in%20LLMs.%20For%20example%2C%20users%20of%20forums%0Aoften%20use%20ASCII%20art%2C%20a%20form%20of%20text-based%20art%2C%20to%20convey%20image%20information.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20ASCII%20art-based%20jailbreak%20attack%20and%20introduce%20a%0Acomprehensive%20benchmark%20Vision-in-Text%20Challenge%20%28ViTC%29%20to%20evaluate%20the%0Acapabilities%20of%20LLMs%20in%20recognizing%20prompts%20that%20cannot%20be%20solely%20interpreted%0Aby%20semantics.%20We%20show%20that%20five%20SOTA%20LLMs%20%28GPT-3.5%2C%20GPT-4%2C%20Gemini%2C%20Claude%2C%20and%0ALlama2%29%20struggle%20to%20recognize%20prompts%20provided%20in%20the%20form%20of%20ASCII%20art.%20Based%0Aon%20this%20observation%2C%20we%20develop%20the%20jailbreak%20attack%20ArtPrompt%2C%20which%20leverages%0Athe%20poor%20performance%20of%20LLMs%20in%20recognizing%20ASCII%20art%20to%20bypass%20safety%20measures%0Aand%20elicit%20undesired%20behaviors%20from%20LLMs.%20ArtPrompt%20only%20requires%20black-box%0Aaccess%20to%20the%20victim%20LLMs%2C%20making%20it%20a%20practical%20attack.%20We%20evaluate%20ArtPrompt%0Aon%20five%20SOTA%20LLMs%2C%20and%20show%20that%20ArtPrompt%20can%20effectively%20and%20efficiently%0Ainduce%20undesired%20behaviors%20from%20all%20five%20LLMs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/uw-nsl/ArtPrompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11753v4&entry.124074799=Read"},
{"title": "DVOS: Self-Supervised Dense-Pattern Video Object Segmentation", "author": "Keyhan Najafian and Farhad Maleki and Ian Stavness and Lingling Jin", "abstract": "  Video object segmentation approaches primarily rely on large-scale\npixel-accurate human-annotated datasets for model development. In Dense Video\nObject Segmentation (DVOS) scenarios, each video frame encompasses hundreds of\nsmall, dense, and partially occluded objects. Accordingly, the labor-intensive\nmanual annotation of even a single frame often takes hours, which hinders the\ndevelopment of DVOS for many applications. Furthermore, in videos with dense\npatterns, following a large number of objects that move in different directions\nposes additional challenges. To address these challenges, we proposed a\nsemi-self-supervised spatiotemporal approach for DVOS utilizing a\ndiffusion-based method through multi-task learning. Emulating real videos'\noptical flow and simulating their motion, we developed a methodology to\nsynthesize computationally annotated videos that can be used for training DVOS\nmodels; The model performance was further improved by utilizing weakly labeled\n(computationally generated but imprecise) data. To demonstrate the utility and\nefficacy of the proposed approach, we developed DVOS models for wheat head\nsegmentation of handheld and drone-captured videos, capturing wheat crops in\nfields of different locations across various growth stages, spanning from\nheading to maturity. Despite using only a few manually annotated video frames,\nthe proposed approach yielded high-performing models, achieving a Dice score of\n0.82 when tested on a drone-captured external test set. While we showed the\nefficacy of the proposed approach for wheat head segmentation, its application\ncan be extended to other crops or DVOS in other domains, such as crowd analysis\nor microscopic image analysis.\n", "link": "http://arxiv.org/abs/2406.05131v1", "date": "2024-06-07", "relevancy": 2.1792, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5552}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5501}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVOS%3A%20Self-Supervised%20Dense-Pattern%20Video%20Object%20Segmentation&body=Title%3A%20DVOS%3A%20Self-Supervised%20Dense-Pattern%20Video%20Object%20Segmentation%0AAuthor%3A%20Keyhan%20Najafian%20and%20Farhad%20Maleki%20and%20Ian%20Stavness%20and%20Lingling%20Jin%0AAbstract%3A%20%20%20Video%20object%20segmentation%20approaches%20primarily%20rely%20on%20large-scale%0Apixel-accurate%20human-annotated%20datasets%20for%20model%20development.%20In%20Dense%20Video%0AObject%20Segmentation%20%28DVOS%29%20scenarios%2C%20each%20video%20frame%20encompasses%20hundreds%20of%0Asmall%2C%20dense%2C%20and%20partially%20occluded%20objects.%20Accordingly%2C%20the%20labor-intensive%0Amanual%20annotation%20of%20even%20a%20single%20frame%20often%20takes%20hours%2C%20which%20hinders%20the%0Adevelopment%20of%20DVOS%20for%20many%20applications.%20Furthermore%2C%20in%20videos%20with%20dense%0Apatterns%2C%20following%20a%20large%20number%20of%20objects%20that%20move%20in%20different%20directions%0Aposes%20additional%20challenges.%20To%20address%20these%20challenges%2C%20we%20proposed%20a%0Asemi-self-supervised%20spatiotemporal%20approach%20for%20DVOS%20utilizing%20a%0Adiffusion-based%20method%20through%20multi-task%20learning.%20Emulating%20real%20videos%27%0Aoptical%20flow%20and%20simulating%20their%20motion%2C%20we%20developed%20a%20methodology%20to%0Asynthesize%20computationally%20annotated%20videos%20that%20can%20be%20used%20for%20training%20DVOS%0Amodels%3B%20The%20model%20performance%20was%20further%20improved%20by%20utilizing%20weakly%20labeled%0A%28computationally%20generated%20but%20imprecise%29%20data.%20To%20demonstrate%20the%20utility%20and%0Aefficacy%20of%20the%20proposed%20approach%2C%20we%20developed%20DVOS%20models%20for%20wheat%20head%0Asegmentation%20of%20handheld%20and%20drone-captured%20videos%2C%20capturing%20wheat%20crops%20in%0Afields%20of%20different%20locations%20across%20various%20growth%20stages%2C%20spanning%20from%0Aheading%20to%20maturity.%20Despite%20using%20only%20a%20few%20manually%20annotated%20video%20frames%2C%0Athe%20proposed%20approach%20yielded%20high-performing%20models%2C%20achieving%20a%20Dice%20score%20of%0A0.82%20when%20tested%20on%20a%20drone-captured%20external%20test%20set.%20While%20we%20showed%20the%0Aefficacy%20of%20the%20proposed%20approach%20for%20wheat%20head%20segmentation%2C%20its%20application%0Acan%20be%20extended%20to%20other%20crops%20or%20DVOS%20in%20other%20domains%2C%20such%20as%20crowd%20analysis%0Aor%20microscopic%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVOS%253A%2520Self-Supervised%2520Dense-Pattern%2520Video%2520Object%2520Segmentation%26entry.906535625%3DKeyhan%2520Najafian%2520and%2520Farhad%2520Maleki%2520and%2520Ian%2520Stavness%2520and%2520Lingling%2520Jin%26entry.1292438233%3D%2520%2520Video%2520object%2520segmentation%2520approaches%2520primarily%2520rely%2520on%2520large-scale%250Apixel-accurate%2520human-annotated%2520datasets%2520for%2520model%2520development.%2520In%2520Dense%2520Video%250AObject%2520Segmentation%2520%2528DVOS%2529%2520scenarios%252C%2520each%2520video%2520frame%2520encompasses%2520hundreds%2520of%250Asmall%252C%2520dense%252C%2520and%2520partially%2520occluded%2520objects.%2520Accordingly%252C%2520the%2520labor-intensive%250Amanual%2520annotation%2520of%2520even%2520a%2520single%2520frame%2520often%2520takes%2520hours%252C%2520which%2520hinders%2520the%250Adevelopment%2520of%2520DVOS%2520for%2520many%2520applications.%2520Furthermore%252C%2520in%2520videos%2520with%2520dense%250Apatterns%252C%2520following%2520a%2520large%2520number%2520of%2520objects%2520that%2520move%2520in%2520different%2520directions%250Aposes%2520additional%2520challenges.%2520To%2520address%2520these%2520challenges%252C%2520we%2520proposed%2520a%250Asemi-self-supervised%2520spatiotemporal%2520approach%2520for%2520DVOS%2520utilizing%2520a%250Adiffusion-based%2520method%2520through%2520multi-task%2520learning.%2520Emulating%2520real%2520videos%2527%250Aoptical%2520flow%2520and%2520simulating%2520their%2520motion%252C%2520we%2520developed%2520a%2520methodology%2520to%250Asynthesize%2520computationally%2520annotated%2520videos%2520that%2520can%2520be%2520used%2520for%2520training%2520DVOS%250Amodels%253B%2520The%2520model%2520performance%2520was%2520further%2520improved%2520by%2520utilizing%2520weakly%2520labeled%250A%2528computationally%2520generated%2520but%2520imprecise%2529%2520data.%2520To%2520demonstrate%2520the%2520utility%2520and%250Aefficacy%2520of%2520the%2520proposed%2520approach%252C%2520we%2520developed%2520DVOS%2520models%2520for%2520wheat%2520head%250Asegmentation%2520of%2520handheld%2520and%2520drone-captured%2520videos%252C%2520capturing%2520wheat%2520crops%2520in%250Afields%2520of%2520different%2520locations%2520across%2520various%2520growth%2520stages%252C%2520spanning%2520from%250Aheading%2520to%2520maturity.%2520Despite%2520using%2520only%2520a%2520few%2520manually%2520annotated%2520video%2520frames%252C%250Athe%2520proposed%2520approach%2520yielded%2520high-performing%2520models%252C%2520achieving%2520a%2520Dice%2520score%2520of%250A0.82%2520when%2520tested%2520on%2520a%2520drone-captured%2520external%2520test%2520set.%2520While%2520we%2520showed%2520the%250Aefficacy%2520of%2520the%2520proposed%2520approach%2520for%2520wheat%2520head%2520segmentation%252C%2520its%2520application%250Acan%2520be%2520extended%2520to%2520other%2520crops%2520or%2520DVOS%2520in%2520other%2520domains%252C%2520such%2520as%2520crowd%2520analysis%250Aor%2520microscopic%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVOS%3A%20Self-Supervised%20Dense-Pattern%20Video%20Object%20Segmentation&entry.906535625=Keyhan%20Najafian%20and%20Farhad%20Maleki%20and%20Ian%20Stavness%20and%20Lingling%20Jin&entry.1292438233=%20%20Video%20object%20segmentation%20approaches%20primarily%20rely%20on%20large-scale%0Apixel-accurate%20human-annotated%20datasets%20for%20model%20development.%20In%20Dense%20Video%0AObject%20Segmentation%20%28DVOS%29%20scenarios%2C%20each%20video%20frame%20encompasses%20hundreds%20of%0Asmall%2C%20dense%2C%20and%20partially%20occluded%20objects.%20Accordingly%2C%20the%20labor-intensive%0Amanual%20annotation%20of%20even%20a%20single%20frame%20often%20takes%20hours%2C%20which%20hinders%20the%0Adevelopment%20of%20DVOS%20for%20many%20applications.%20Furthermore%2C%20in%20videos%20with%20dense%0Apatterns%2C%20following%20a%20large%20number%20of%20objects%20that%20move%20in%20different%20directions%0Aposes%20additional%20challenges.%20To%20address%20these%20challenges%2C%20we%20proposed%20a%0Asemi-self-supervised%20spatiotemporal%20approach%20for%20DVOS%20utilizing%20a%0Adiffusion-based%20method%20through%20multi-task%20learning.%20Emulating%20real%20videos%27%0Aoptical%20flow%20and%20simulating%20their%20motion%2C%20we%20developed%20a%20methodology%20to%0Asynthesize%20computationally%20annotated%20videos%20that%20can%20be%20used%20for%20training%20DVOS%0Amodels%3B%20The%20model%20performance%20was%20further%20improved%20by%20utilizing%20weakly%20labeled%0A%28computationally%20generated%20but%20imprecise%29%20data.%20To%20demonstrate%20the%20utility%20and%0Aefficacy%20of%20the%20proposed%20approach%2C%20we%20developed%20DVOS%20models%20for%20wheat%20head%0Asegmentation%20of%20handheld%20and%20drone-captured%20videos%2C%20capturing%20wheat%20crops%20in%0Afields%20of%20different%20locations%20across%20various%20growth%20stages%2C%20spanning%20from%0Aheading%20to%20maturity.%20Despite%20using%20only%20a%20few%20manually%20annotated%20video%20frames%2C%0Athe%20proposed%20approach%20yielded%20high-performing%20models%2C%20achieving%20a%20Dice%20score%20of%0A0.82%20when%20tested%20on%20a%20drone-captured%20external%20test%20set.%20While%20we%20showed%20the%0Aefficacy%20of%20the%20proposed%20approach%20for%20wheat%20head%20segmentation%2C%20its%20application%0Acan%20be%20extended%20to%20other%20crops%20or%20DVOS%20in%20other%20domains%2C%20such%20as%20crowd%20analysis%0Aor%20microscopic%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05131v1&entry.124074799=Read"},
{"title": "iHERO: Interactive Human-oriented Exploration and Supervision Under\n  Scarce Communication", "author": "Zhuoli Tian and Yuyang Zhang and Jinsheng Wei and Meng Guo", "abstract": "  Exploration of unknown scenes before human entry is essential for safety and\nefficiency in numerous scenarios, e.g., subterranean exploration,\nreconnaissance, search and rescue missions. Fleets of autonomous robots are\nparticularly suitable for this task, via concurrent exploration, multi-sensory\nperception and autonomous navigation. Communication however among the robots\ncan be severely restricted to only close-range exchange via ad-hoc networks.\nAlthough some recent works have addressed the problem of collaborative\nexploration under restricted communication, the crucial role of the human\noperator has been mostly neglected. Indeed, the operator may: (i) require\ntimely update regarding the exploration progress and fleet status; (ii)\nprioritize certain regions; and (iii) dynamically move within the explored\narea; To facilitate these requests, this work proposes an interactive\nhuman-oriented online coordination framework for collaborative exploration and\nsupervision under scarce communication (iHERO). The robots switch smoothly and\noptimally among fast exploration, intermittent exchange of map and sensory\ndata, and return to the operator for status update. It is ensured that these\nrequests are fulfilled online interactively with a pre-specified latency.\nExtensive large-scale human-in-the-loop simulations and hardware experiments\nare performed over numerous challenging scenes, which signify its performance\nsuch as explored area and efficiency, and validate its potential applicability\nto real-world scenarios. The videos are available on\nhttps://zl-tian.github.io/iHERO/.\n", "link": "http://arxiv.org/abs/2405.12571v2", "date": "2024-06-07", "relevancy": 2.1779, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6177}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5363}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iHERO%3A%20Interactive%20Human-oriented%20Exploration%20and%20Supervision%20Under%0A%20%20Scarce%20Communication&body=Title%3A%20iHERO%3A%20Interactive%20Human-oriented%20Exploration%20and%20Supervision%20Under%0A%20%20Scarce%20Communication%0AAuthor%3A%20Zhuoli%20Tian%20and%20Yuyang%20Zhang%20and%20Jinsheng%20Wei%20and%20Meng%20Guo%0AAbstract%3A%20%20%20Exploration%20of%20unknown%20scenes%20before%20human%20entry%20is%20essential%20for%20safety%20and%0Aefficiency%20in%20numerous%20scenarios%2C%20e.g.%2C%20subterranean%20exploration%2C%0Areconnaissance%2C%20search%20and%20rescue%20missions.%20Fleets%20of%20autonomous%20robots%20are%0Aparticularly%20suitable%20for%20this%20task%2C%20via%20concurrent%20exploration%2C%20multi-sensory%0Aperception%20and%20autonomous%20navigation.%20Communication%20however%20among%20the%20robots%0Acan%20be%20severely%20restricted%20to%20only%20close-range%20exchange%20via%20ad-hoc%20networks.%0AAlthough%20some%20recent%20works%20have%20addressed%20the%20problem%20of%20collaborative%0Aexploration%20under%20restricted%20communication%2C%20the%20crucial%20role%20of%20the%20human%0Aoperator%20has%20been%20mostly%20neglected.%20Indeed%2C%20the%20operator%20may%3A%20%28i%29%20require%0Atimely%20update%20regarding%20the%20exploration%20progress%20and%20fleet%20status%3B%20%28ii%29%0Aprioritize%20certain%20regions%3B%20and%20%28iii%29%20dynamically%20move%20within%20the%20explored%0Aarea%3B%20To%20facilitate%20these%20requests%2C%20this%20work%20proposes%20an%20interactive%0Ahuman-oriented%20online%20coordination%20framework%20for%20collaborative%20exploration%20and%0Asupervision%20under%20scarce%20communication%20%28iHERO%29.%20The%20robots%20switch%20smoothly%20and%0Aoptimally%20among%20fast%20exploration%2C%20intermittent%20exchange%20of%20map%20and%20sensory%0Adata%2C%20and%20return%20to%20the%20operator%20for%20status%20update.%20It%20is%20ensured%20that%20these%0Arequests%20are%20fulfilled%20online%20interactively%20with%20a%20pre-specified%20latency.%0AExtensive%20large-scale%20human-in-the-loop%20simulations%20and%20hardware%20experiments%0Aare%20performed%20over%20numerous%20challenging%20scenes%2C%20which%20signify%20its%20performance%0Asuch%20as%20explored%20area%20and%20efficiency%2C%20and%20validate%20its%20potential%20applicability%0Ato%20real-world%20scenarios.%20The%20videos%20are%20available%20on%0Ahttps%3A//zl-tian.github.io/iHERO/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiHERO%253A%2520Interactive%2520Human-oriented%2520Exploration%2520and%2520Supervision%2520Under%250A%2520%2520Scarce%2520Communication%26entry.906535625%3DZhuoli%2520Tian%2520and%2520Yuyang%2520Zhang%2520and%2520Jinsheng%2520Wei%2520and%2520Meng%2520Guo%26entry.1292438233%3D%2520%2520Exploration%2520of%2520unknown%2520scenes%2520before%2520human%2520entry%2520is%2520essential%2520for%2520safety%2520and%250Aefficiency%2520in%2520numerous%2520scenarios%252C%2520e.g.%252C%2520subterranean%2520exploration%252C%250Areconnaissance%252C%2520search%2520and%2520rescue%2520missions.%2520Fleets%2520of%2520autonomous%2520robots%2520are%250Aparticularly%2520suitable%2520for%2520this%2520task%252C%2520via%2520concurrent%2520exploration%252C%2520multi-sensory%250Aperception%2520and%2520autonomous%2520navigation.%2520Communication%2520however%2520among%2520the%2520robots%250Acan%2520be%2520severely%2520restricted%2520to%2520only%2520close-range%2520exchange%2520via%2520ad-hoc%2520networks.%250AAlthough%2520some%2520recent%2520works%2520have%2520addressed%2520the%2520problem%2520of%2520collaborative%250Aexploration%2520under%2520restricted%2520communication%252C%2520the%2520crucial%2520role%2520of%2520the%2520human%250Aoperator%2520has%2520been%2520mostly%2520neglected.%2520Indeed%252C%2520the%2520operator%2520may%253A%2520%2528i%2529%2520require%250Atimely%2520update%2520regarding%2520the%2520exploration%2520progress%2520and%2520fleet%2520status%253B%2520%2528ii%2529%250Aprioritize%2520certain%2520regions%253B%2520and%2520%2528iii%2529%2520dynamically%2520move%2520within%2520the%2520explored%250Aarea%253B%2520To%2520facilitate%2520these%2520requests%252C%2520this%2520work%2520proposes%2520an%2520interactive%250Ahuman-oriented%2520online%2520coordination%2520framework%2520for%2520collaborative%2520exploration%2520and%250Asupervision%2520under%2520scarce%2520communication%2520%2528iHERO%2529.%2520The%2520robots%2520switch%2520smoothly%2520and%250Aoptimally%2520among%2520fast%2520exploration%252C%2520intermittent%2520exchange%2520of%2520map%2520and%2520sensory%250Adata%252C%2520and%2520return%2520to%2520the%2520operator%2520for%2520status%2520update.%2520It%2520is%2520ensured%2520that%2520these%250Arequests%2520are%2520fulfilled%2520online%2520interactively%2520with%2520a%2520pre-specified%2520latency.%250AExtensive%2520large-scale%2520human-in-the-loop%2520simulations%2520and%2520hardware%2520experiments%250Aare%2520performed%2520over%2520numerous%2520challenging%2520scenes%252C%2520which%2520signify%2520its%2520performance%250Asuch%2520as%2520explored%2520area%2520and%2520efficiency%252C%2520and%2520validate%2520its%2520potential%2520applicability%250Ato%2520real-world%2520scenarios.%2520The%2520videos%2520are%2520available%2520on%250Ahttps%253A//zl-tian.github.io/iHERO/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iHERO%3A%20Interactive%20Human-oriented%20Exploration%20and%20Supervision%20Under%0A%20%20Scarce%20Communication&entry.906535625=Zhuoli%20Tian%20and%20Yuyang%20Zhang%20and%20Jinsheng%20Wei%20and%20Meng%20Guo&entry.1292438233=%20%20Exploration%20of%20unknown%20scenes%20before%20human%20entry%20is%20essential%20for%20safety%20and%0Aefficiency%20in%20numerous%20scenarios%2C%20e.g.%2C%20subterranean%20exploration%2C%0Areconnaissance%2C%20search%20and%20rescue%20missions.%20Fleets%20of%20autonomous%20robots%20are%0Aparticularly%20suitable%20for%20this%20task%2C%20via%20concurrent%20exploration%2C%20multi-sensory%0Aperception%20and%20autonomous%20navigation.%20Communication%20however%20among%20the%20robots%0Acan%20be%20severely%20restricted%20to%20only%20close-range%20exchange%20via%20ad-hoc%20networks.%0AAlthough%20some%20recent%20works%20have%20addressed%20the%20problem%20of%20collaborative%0Aexploration%20under%20restricted%20communication%2C%20the%20crucial%20role%20of%20the%20human%0Aoperator%20has%20been%20mostly%20neglected.%20Indeed%2C%20the%20operator%20may%3A%20%28i%29%20require%0Atimely%20update%20regarding%20the%20exploration%20progress%20and%20fleet%20status%3B%20%28ii%29%0Aprioritize%20certain%20regions%3B%20and%20%28iii%29%20dynamically%20move%20within%20the%20explored%0Aarea%3B%20To%20facilitate%20these%20requests%2C%20this%20work%20proposes%20an%20interactive%0Ahuman-oriented%20online%20coordination%20framework%20for%20collaborative%20exploration%20and%0Asupervision%20under%20scarce%20communication%20%28iHERO%29.%20The%20robots%20switch%20smoothly%20and%0Aoptimally%20among%20fast%20exploration%2C%20intermittent%20exchange%20of%20map%20and%20sensory%0Adata%2C%20and%20return%20to%20the%20operator%20for%20status%20update.%20It%20is%20ensured%20that%20these%0Arequests%20are%20fulfilled%20online%20interactively%20with%20a%20pre-specified%20latency.%0AExtensive%20large-scale%20human-in-the-loop%20simulations%20and%20hardware%20experiments%0Aare%20performed%20over%20numerous%20challenging%20scenes%2C%20which%20signify%20its%20performance%0Asuch%20as%20explored%20area%20and%20efficiency%2C%20and%20validate%20its%20potential%20applicability%0Ato%20real-world%20scenarios.%20The%20videos%20are%20available%20on%0Ahttps%3A//zl-tian.github.io/iHERO/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12571v2&entry.124074799=Read"},
{"title": "Advances in Embodied Navigation Using Large Language Models: A Survey", "author": "Jinzhou Lin and Han Gao and Xuxiang Feng and Rongtao Xu and Changwei Wang and Man Zhang and Li Guo and Shibiao Xu", "abstract": "  In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN.\n", "link": "http://arxiv.org/abs/2311.00530v4", "date": "2024-06-07", "relevancy": 2.1675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5521}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Embodied%20Navigation%20Using%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Advances%20in%20Embodied%20Navigation%20Using%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Jinzhou%20Lin%20and%20Han%20Gao%20and%20Xuxiang%20Feng%20and%20Rongtao%20Xu%20and%20Changwei%20Wang%20and%20Man%20Zhang%20and%20Li%20Guo%20and%20Shibiao%20Xu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20such%0Aas%20the%20Generative%20Pre-trained%20Transformer%20%28GPT%29%20has%20attracted%20increasing%0Aattention%20due%20to%20their%20potential%20in%20a%20variety%20of%20practical%20applications.%20The%0Aapplication%20of%20LLMs%20with%20Embodied%20Intelligence%20has%20emerged%20as%20a%20significant%0Aarea%20of%20focus.%20Among%20the%20myriad%20applications%20of%20LLMs%2C%20navigation%20tasks%20are%0Aparticularly%20noteworthy%20because%20they%20demand%20a%20deep%20understanding%20of%20the%0Aenvironment%20and%20quick%2C%20accurate%20decision-making.%20LLMs%20can%20augment%20embodied%0Aintelligence%20systems%20with%20sophisticated%20environmental%20perception%20and%0Adecision-making%20support%2C%20leveraging%20their%20robust%20language%20and%20image-processing%0Acapabilities.%20This%20article%20offers%20an%20exhaustive%20summary%20of%20the%20symbiosis%0Abetween%20LLMs%20and%20embodied%20intelligence%20with%20a%20focus%20on%20navigation.%20It%20reviews%0Astate-of-the-art%20models%2C%20research%20methodologies%2C%20and%20assesses%20the%20advantages%0Aand%20disadvantages%20of%20existing%20embodied%20navigation%20models%20and%20datasets.%20Finally%2C%0Athe%20article%20elucidates%20the%20role%20of%20LLMs%20in%20embodied%20intelligence%2C%20based%20on%0Acurrent%20research%2C%20and%20forecasts%20future%20directions%20in%20the%20field.%20A%20comprehensive%0Alist%20of%20studies%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Rongtao-Xu/Awesome-LLM-EN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00530v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Embodied%2520Navigation%2520Using%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DJinzhou%2520Lin%2520and%2520Han%2520Gao%2520and%2520Xuxiang%2520Feng%2520and%2520Rongtao%2520Xu%2520and%2520Changwei%2520Wang%2520and%2520Man%2520Zhang%2520and%2520Li%2520Guo%2520and%2520Shibiao%2520Xu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%250Aas%2520the%2520Generative%2520Pre-trained%2520Transformer%2520%2528GPT%2529%2520has%2520attracted%2520increasing%250Aattention%2520due%2520to%2520their%2520potential%2520in%2520a%2520variety%2520of%2520practical%2520applications.%2520The%250Aapplication%2520of%2520LLMs%2520with%2520Embodied%2520Intelligence%2520has%2520emerged%2520as%2520a%2520significant%250Aarea%2520of%2520focus.%2520Among%2520the%2520myriad%2520applications%2520of%2520LLMs%252C%2520navigation%2520tasks%2520are%250Aparticularly%2520noteworthy%2520because%2520they%2520demand%2520a%2520deep%2520understanding%2520of%2520the%250Aenvironment%2520and%2520quick%252C%2520accurate%2520decision-making.%2520LLMs%2520can%2520augment%2520embodied%250Aintelligence%2520systems%2520with%2520sophisticated%2520environmental%2520perception%2520and%250Adecision-making%2520support%252C%2520leveraging%2520their%2520robust%2520language%2520and%2520image-processing%250Acapabilities.%2520This%2520article%2520offers%2520an%2520exhaustive%2520summary%2520of%2520the%2520symbiosis%250Abetween%2520LLMs%2520and%2520embodied%2520intelligence%2520with%2520a%2520focus%2520on%2520navigation.%2520It%2520reviews%250Astate-of-the-art%2520models%252C%2520research%2520methodologies%252C%2520and%2520assesses%2520the%2520advantages%250Aand%2520disadvantages%2520of%2520existing%2520embodied%2520navigation%2520models%2520and%2520datasets.%2520Finally%252C%250Athe%2520article%2520elucidates%2520the%2520role%2520of%2520LLMs%2520in%2520embodied%2520intelligence%252C%2520based%2520on%250Acurrent%2520research%252C%2520and%2520forecasts%2520future%2520directions%2520in%2520the%2520field.%2520A%2520comprehensive%250Alist%2520of%2520studies%2520in%2520this%2520survey%2520is%2520available%2520at%250Ahttps%253A//github.com/Rongtao-Xu/Awesome-LLM-EN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00530v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Embodied%20Navigation%20Using%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Jinzhou%20Lin%20and%20Han%20Gao%20and%20Xuxiang%20Feng%20and%20Rongtao%20Xu%20and%20Changwei%20Wang%20and%20Man%20Zhang%20and%20Li%20Guo%20and%20Shibiao%20Xu&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20such%0Aas%20the%20Generative%20Pre-trained%20Transformer%20%28GPT%29%20has%20attracted%20increasing%0Aattention%20due%20to%20their%20potential%20in%20a%20variety%20of%20practical%20applications.%20The%0Aapplication%20of%20LLMs%20with%20Embodied%20Intelligence%20has%20emerged%20as%20a%20significant%0Aarea%20of%20focus.%20Among%20the%20myriad%20applications%20of%20LLMs%2C%20navigation%20tasks%20are%0Aparticularly%20noteworthy%20because%20they%20demand%20a%20deep%20understanding%20of%20the%0Aenvironment%20and%20quick%2C%20accurate%20decision-making.%20LLMs%20can%20augment%20embodied%0Aintelligence%20systems%20with%20sophisticated%20environmental%20perception%20and%0Adecision-making%20support%2C%20leveraging%20their%20robust%20language%20and%20image-processing%0Acapabilities.%20This%20article%20offers%20an%20exhaustive%20summary%20of%20the%20symbiosis%0Abetween%20LLMs%20and%20embodied%20intelligence%20with%20a%20focus%20on%20navigation.%20It%20reviews%0Astate-of-the-art%20models%2C%20research%20methodologies%2C%20and%20assesses%20the%20advantages%0Aand%20disadvantages%20of%20existing%20embodied%20navigation%20models%20and%20datasets.%20Finally%2C%0Athe%20article%20elucidates%20the%20role%20of%20LLMs%20in%20embodied%20intelligence%2C%20based%20on%0Acurrent%20research%2C%20and%20forecasts%20future%20directions%20in%20the%20field.%20A%20comprehensive%0Alist%20of%20studies%20in%20this%20survey%20is%20available%20at%0Ahttps%3A//github.com/Rongtao-Xu/Awesome-LLM-EN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00530v4&entry.124074799=Read"},
{"title": "Navigating Efficiency in MobileViT through Gaussian Process on Global\n  Architecture Factors", "author": "Ke Meng and Kai Chen", "abstract": "  Numerous techniques have been meticulously designed to achieve optimal\narchitectures for convolutional neural networks (CNNs), yet a comparable focus\non vision transformers (ViTs) has been somewhat lacking. Despite the remarkable\nsuccess of ViTs in various vision tasks, their heavyweight nature presents\nchallenges of computational costs. In this paper, we leverage the Gaussian\nprocess to systematically explore the nonlinear and uncertain relationship\nbetween performance and global architecture factors of MobileViT, such as\nresolution, width, and depth including the depth of in-verted residual blocks\nand the depth of ViT blocks, and joint factors including resolution-depth and\nresolution-width. We present design principles twisting magic 4D cube of the\nglobal architecture factors that minimize model sizes and computational costs\nwith higher model accuracy. We introduce a formula for downsizing architectures\nby iteratively deriving smaller MobileViT V2, all while adhering to a specified\nconstraint of multiply-accumulate operations (MACs). Experiment results show\nthat our formula significantly outperforms CNNs and mobile ViTs across\ndiversified datasets\n", "link": "http://arxiv.org/abs/2406.04820v1", "date": "2024-06-07", "relevancy": 2.1483, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5516}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5392}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Efficiency%20in%20MobileViT%20through%20Gaussian%20Process%20on%20Global%0A%20%20Architecture%20Factors&body=Title%3A%20Navigating%20Efficiency%20in%20MobileViT%20through%20Gaussian%20Process%20on%20Global%0A%20%20Architecture%20Factors%0AAuthor%3A%20Ke%20Meng%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Numerous%20techniques%20have%20been%20meticulously%20designed%20to%20achieve%20optimal%0Aarchitectures%20for%20convolutional%20neural%20networks%20%28CNNs%29%2C%20yet%20a%20comparable%20focus%0Aon%20vision%20transformers%20%28ViTs%29%20has%20been%20somewhat%20lacking.%20Despite%20the%20remarkable%0Asuccess%20of%20ViTs%20in%20various%20vision%20tasks%2C%20their%20heavyweight%20nature%20presents%0Achallenges%20of%20computational%20costs.%20In%20this%20paper%2C%20we%20leverage%20the%20Gaussian%0Aprocess%20to%20systematically%20explore%20the%20nonlinear%20and%20uncertain%20relationship%0Abetween%20performance%20and%20global%20architecture%20factors%20of%20MobileViT%2C%20such%20as%0Aresolution%2C%20width%2C%20and%20depth%20including%20the%20depth%20of%20in-verted%20residual%20blocks%0Aand%20the%20depth%20of%20ViT%20blocks%2C%20and%20joint%20factors%20including%20resolution-depth%20and%0Aresolution-width.%20We%20present%20design%20principles%20twisting%20magic%204D%20cube%20of%20the%0Aglobal%20architecture%20factors%20that%20minimize%20model%20sizes%20and%20computational%20costs%0Awith%20higher%20model%20accuracy.%20We%20introduce%20a%20formula%20for%20downsizing%20architectures%0Aby%20iteratively%20deriving%20smaller%20MobileViT%20V2%2C%20all%20while%20adhering%20to%20a%20specified%0Aconstraint%20of%20multiply-accumulate%20operations%20%28MACs%29.%20Experiment%20results%20show%0Athat%20our%20formula%20significantly%20outperforms%20CNNs%20and%20mobile%20ViTs%20across%0Adiversified%20datasets%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Efficiency%2520in%2520MobileViT%2520through%2520Gaussian%2520Process%2520on%2520Global%250A%2520%2520Architecture%2520Factors%26entry.906535625%3DKe%2520Meng%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Numerous%2520techniques%2520have%2520been%2520meticulously%2520designed%2520to%2520achieve%2520optimal%250Aarchitectures%2520for%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520yet%2520a%2520comparable%2520focus%250Aon%2520vision%2520transformers%2520%2528ViTs%2529%2520has%2520been%2520somewhat%2520lacking.%2520Despite%2520the%2520remarkable%250Asuccess%2520of%2520ViTs%2520in%2520various%2520vision%2520tasks%252C%2520their%2520heavyweight%2520nature%2520presents%250Achallenges%2520of%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520the%2520Gaussian%250Aprocess%2520to%2520systematically%2520explore%2520the%2520nonlinear%2520and%2520uncertain%2520relationship%250Abetween%2520performance%2520and%2520global%2520architecture%2520factors%2520of%2520MobileViT%252C%2520such%2520as%250Aresolution%252C%2520width%252C%2520and%2520depth%2520including%2520the%2520depth%2520of%2520in-verted%2520residual%2520blocks%250Aand%2520the%2520depth%2520of%2520ViT%2520blocks%252C%2520and%2520joint%2520factors%2520including%2520resolution-depth%2520and%250Aresolution-width.%2520We%2520present%2520design%2520principles%2520twisting%2520magic%25204D%2520cube%2520of%2520the%250Aglobal%2520architecture%2520factors%2520that%2520minimize%2520model%2520sizes%2520and%2520computational%2520costs%250Awith%2520higher%2520model%2520accuracy.%2520We%2520introduce%2520a%2520formula%2520for%2520downsizing%2520architectures%250Aby%2520iteratively%2520deriving%2520smaller%2520MobileViT%2520V2%252C%2520all%2520while%2520adhering%2520to%2520a%2520specified%250Aconstraint%2520of%2520multiply-accumulate%2520operations%2520%2528MACs%2529.%2520Experiment%2520results%2520show%250Athat%2520our%2520formula%2520significantly%2520outperforms%2520CNNs%2520and%2520mobile%2520ViTs%2520across%250Adiversified%2520datasets%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Efficiency%20in%20MobileViT%20through%20Gaussian%20Process%20on%20Global%0A%20%20Architecture%20Factors&entry.906535625=Ke%20Meng%20and%20Kai%20Chen&entry.1292438233=%20%20Numerous%20techniques%20have%20been%20meticulously%20designed%20to%20achieve%20optimal%0Aarchitectures%20for%20convolutional%20neural%20networks%20%28CNNs%29%2C%20yet%20a%20comparable%20focus%0Aon%20vision%20transformers%20%28ViTs%29%20has%20been%20somewhat%20lacking.%20Despite%20the%20remarkable%0Asuccess%20of%20ViTs%20in%20various%20vision%20tasks%2C%20their%20heavyweight%20nature%20presents%0Achallenges%20of%20computational%20costs.%20In%20this%20paper%2C%20we%20leverage%20the%20Gaussian%0Aprocess%20to%20systematically%20explore%20the%20nonlinear%20and%20uncertain%20relationship%0Abetween%20performance%20and%20global%20architecture%20factors%20of%20MobileViT%2C%20such%20as%0Aresolution%2C%20width%2C%20and%20depth%20including%20the%20depth%20of%20in-verted%20residual%20blocks%0Aand%20the%20depth%20of%20ViT%20blocks%2C%20and%20joint%20factors%20including%20resolution-depth%20and%0Aresolution-width.%20We%20present%20design%20principles%20twisting%20magic%204D%20cube%20of%20the%0Aglobal%20architecture%20factors%20that%20minimize%20model%20sizes%20and%20computational%20costs%0Awith%20higher%20model%20accuracy.%20We%20introduce%20a%20formula%20for%20downsizing%20architectures%0Aby%20iteratively%20deriving%20smaller%20MobileViT%20V2%2C%20all%20while%20adhering%20to%20a%20specified%0Aconstraint%20of%20multiply-accumulate%20operations%20%28MACs%29.%20Experiment%20results%20show%0Athat%20our%20formula%20significantly%20outperforms%20CNNs%20and%20mobile%20ViTs%20across%0Adiversified%20datasets%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04820v1&entry.124074799=Read"},
{"title": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for\n  Vision Tasks", "author": "Xingkui Zhu and Yiran Guan and Dingkang Liang and Yuchao Chen and Yuliang Liu and Xiang Bai", "abstract": "  The sparsely activated mixture of experts (MoE) model presents a promising\nalternative to traditional densely activated (dense) models, enhancing both\nquality and computational efficiency. However, training MoE models from scratch\ndemands extensive data and computational resources. Moreover, public\nrepositories like timm mainly provide pre-trained dense checkpoints, lacking\nsimilar resources for MoE models, hindering their adoption. To bridge this gap,\nwe introduce MoE Jetpack, an effective method for fine-tuning dense checkpoints\ninto MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint\nrecycling, which repurposes dense checkpoints as initial weights for MoE\nmodels, thereby accelerating convergence, enhancing accuracy, and alleviating\nthe computational burden of pre-training; (2) hyperspherical adaptive MoE\n(SpheroMoE) layer, which optimizes the MoE architecture for better integration\nof dense checkpoints, enhancing fine-tuning performance. Our experiments on\nvision tasks demonstrate that MoE Jetpack significantly improves convergence\nspeed and accuracy when fine-tuning dense checkpoints into MoE models. Our code\nwill be publicly available at https://github.com/Adlith/MoE-Jetpack.\n", "link": "http://arxiv.org/abs/2406.04801v1", "date": "2024-06-07", "relevancy": 2.1211, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5281}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE%20Jetpack%3A%20From%20Dense%20Checkpoints%20to%20Adaptive%20Mixture%20of%20Experts%20for%0A%20%20Vision%20Tasks&body=Title%3A%20MoE%20Jetpack%3A%20From%20Dense%20Checkpoints%20to%20Adaptive%20Mixture%20of%20Experts%20for%0A%20%20Vision%20Tasks%0AAuthor%3A%20Xingkui%20Zhu%20and%20Yiran%20Guan%20and%20Dingkang%20Liang%20and%20Yuchao%20Chen%20and%20Yuliang%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20sparsely%20activated%20mixture%20of%20experts%20%28MoE%29%20model%20presents%20a%20promising%0Aalternative%20to%20traditional%20densely%20activated%20%28dense%29%20models%2C%20enhancing%20both%0Aquality%20and%20computational%20efficiency.%20However%2C%20training%20MoE%20models%20from%20scratch%0Ademands%20extensive%20data%20and%20computational%20resources.%20Moreover%2C%20public%0Arepositories%20like%20timm%20mainly%20provide%20pre-trained%20dense%20checkpoints%2C%20lacking%0Asimilar%20resources%20for%20MoE%20models%2C%20hindering%20their%20adoption.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20MoE%20Jetpack%2C%20an%20effective%20method%20for%20fine-tuning%20dense%20checkpoints%0Ainto%20MoE%20models.%20MoE%20Jetpack%20incorporates%20two%20key%20techniques%3A%20%281%29%20checkpoint%0Arecycling%2C%20which%20repurposes%20dense%20checkpoints%20as%20initial%20weights%20for%20MoE%0Amodels%2C%20thereby%20accelerating%20convergence%2C%20enhancing%20accuracy%2C%20and%20alleviating%0Athe%20computational%20burden%20of%20pre-training%3B%20%282%29%20hyperspherical%20adaptive%20MoE%0A%28SpheroMoE%29%20layer%2C%20which%20optimizes%20the%20MoE%20architecture%20for%20better%20integration%0Aof%20dense%20checkpoints%2C%20enhancing%20fine-tuning%20performance.%20Our%20experiments%20on%0Avision%20tasks%20demonstrate%20that%20MoE%20Jetpack%20significantly%20improves%20convergence%0Aspeed%20and%20accuracy%20when%20fine-tuning%20dense%20checkpoints%20into%20MoE%20models.%20Our%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/Adlith/MoE-Jetpack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE%2520Jetpack%253A%2520From%2520Dense%2520Checkpoints%2520to%2520Adaptive%2520Mixture%2520of%2520Experts%2520for%250A%2520%2520Vision%2520Tasks%26entry.906535625%3DXingkui%2520Zhu%2520and%2520Yiran%2520Guan%2520and%2520Dingkang%2520Liang%2520and%2520Yuchao%2520Chen%2520and%2520Yuliang%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520sparsely%2520activated%2520mixture%2520of%2520experts%2520%2528MoE%2529%2520model%2520presents%2520a%2520promising%250Aalternative%2520to%2520traditional%2520densely%2520activated%2520%2528dense%2529%2520models%252C%2520enhancing%2520both%250Aquality%2520and%2520computational%2520efficiency.%2520However%252C%2520training%2520MoE%2520models%2520from%2520scratch%250Ademands%2520extensive%2520data%2520and%2520computational%2520resources.%2520Moreover%252C%2520public%250Arepositories%2520like%2520timm%2520mainly%2520provide%2520pre-trained%2520dense%2520checkpoints%252C%2520lacking%250Asimilar%2520resources%2520for%2520MoE%2520models%252C%2520hindering%2520their%2520adoption.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520introduce%2520MoE%2520Jetpack%252C%2520an%2520effective%2520method%2520for%2520fine-tuning%2520dense%2520checkpoints%250Ainto%2520MoE%2520models.%2520MoE%2520Jetpack%2520incorporates%2520two%2520key%2520techniques%253A%2520%25281%2529%2520checkpoint%250Arecycling%252C%2520which%2520repurposes%2520dense%2520checkpoints%2520as%2520initial%2520weights%2520for%2520MoE%250Amodels%252C%2520thereby%2520accelerating%2520convergence%252C%2520enhancing%2520accuracy%252C%2520and%2520alleviating%250Athe%2520computational%2520burden%2520of%2520pre-training%253B%2520%25282%2529%2520hyperspherical%2520adaptive%2520MoE%250A%2528SpheroMoE%2529%2520layer%252C%2520which%2520optimizes%2520the%2520MoE%2520architecture%2520for%2520better%2520integration%250Aof%2520dense%2520checkpoints%252C%2520enhancing%2520fine-tuning%2520performance.%2520Our%2520experiments%2520on%250Avision%2520tasks%2520demonstrate%2520that%2520MoE%2520Jetpack%2520significantly%2520improves%2520convergence%250Aspeed%2520and%2520accuracy%2520when%2520fine-tuning%2520dense%2520checkpoints%2520into%2520MoE%2520models.%2520Our%2520code%250Awill%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/Adlith/MoE-Jetpack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE%20Jetpack%3A%20From%20Dense%20Checkpoints%20to%20Adaptive%20Mixture%20of%20Experts%20for%0A%20%20Vision%20Tasks&entry.906535625=Xingkui%20Zhu%20and%20Yiran%20Guan%20and%20Dingkang%20Liang%20and%20Yuchao%20Chen%20and%20Yuliang%20Liu%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20sparsely%20activated%20mixture%20of%20experts%20%28MoE%29%20model%20presents%20a%20promising%0Aalternative%20to%20traditional%20densely%20activated%20%28dense%29%20models%2C%20enhancing%20both%0Aquality%20and%20computational%20efficiency.%20However%2C%20training%20MoE%20models%20from%20scratch%0Ademands%20extensive%20data%20and%20computational%20resources.%20Moreover%2C%20public%0Arepositories%20like%20timm%20mainly%20provide%20pre-trained%20dense%20checkpoints%2C%20lacking%0Asimilar%20resources%20for%20MoE%20models%2C%20hindering%20their%20adoption.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20MoE%20Jetpack%2C%20an%20effective%20method%20for%20fine-tuning%20dense%20checkpoints%0Ainto%20MoE%20models.%20MoE%20Jetpack%20incorporates%20two%20key%20techniques%3A%20%281%29%20checkpoint%0Arecycling%2C%20which%20repurposes%20dense%20checkpoints%20as%20initial%20weights%20for%20MoE%0Amodels%2C%20thereby%20accelerating%20convergence%2C%20enhancing%20accuracy%2C%20and%20alleviating%0Athe%20computational%20burden%20of%20pre-training%3B%20%282%29%20hyperspherical%20adaptive%20MoE%0A%28SpheroMoE%29%20layer%2C%20which%20optimizes%20the%20MoE%20architecture%20for%20better%20integration%0Aof%20dense%20checkpoints%2C%20enhancing%20fine-tuning%20performance.%20Our%20experiments%20on%0Avision%20tasks%20demonstrate%20that%20MoE%20Jetpack%20significantly%20improves%20convergence%0Aspeed%20and%20accuracy%20when%20fine-tuning%20dense%20checkpoints%20into%20MoE%20models.%20Our%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/Adlith/MoE-Jetpack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04801v1&entry.124074799=Read"},
{"title": "Prototype Correlation Matching and Class-Relation Reasoning for Few-Shot\n  Medical Image Segmentation", "author": "Yumin Zhang and Hongliu Li and Yajun Gao and Haoran Duan and Yawen Huang and Yefeng Zheng", "abstract": "  Few-shot medical image segmentation has achieved great progress in improving\naccuracy and efficiency of medical analysis in the biomedical imaging field.\nHowever, most existing methods cannot explore inter-class relations among base\nand novel medical classes to reason unseen novel classes. Moreover, the same\nkind of medical class has large intra-class variations brought by diverse\nappearances, shapes and scales, thus causing ambiguous visual characterization\nto degrade generalization performance of these existing methods on unseen novel\nclasses. To address the above challenges, in this paper, we propose a\n\\underline{\\textbf{P}}rototype correlation \\underline{\\textbf{M}}atching and\n\\underline{\\textbf{C}}lass-relation \\underline{\\textbf{R}}easoning (i.e.,\n\\textbf{PMCR}) model. The proposed model can effectively mitigate false pixel\ncorrelation matches caused by large intra-class variations while reasoning\ninter-class relations among different medical classes. Specifically, in order\nto address false pixel correlation match brought by large intra-class\nvariations, we propose a prototype correlation matching module to mine\nrepresentative prototypes that can characterize diverse visual information of\ndifferent appearances well. We aim to explore prototype-level rather than\npixel-level correlation matching between support and query features via optimal\ntransport algorithm to tackle false matches caused by intra-class variations.\nMeanwhile, in order to explore inter-class relations, we design a\nclass-relation reasoning module to segment unseen novel medical objects via\nreasoning inter-class relations between base and novel classes. Such\ninter-class relations can be well propagated to semantic encoding of local\nquery features to improve few-shot segmentation performance. Quantitative\ncomparisons illustrates the large performance improvement of our model over\nother baseline methods.\n", "link": "http://arxiv.org/abs/2406.05054v1", "date": "2024-06-07", "relevancy": 2.1149, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototype%20Correlation%20Matching%20and%20Class-Relation%20Reasoning%20for%20Few-Shot%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Prototype%20Correlation%20Matching%20and%20Class-Relation%20Reasoning%20for%20Few-Shot%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yumin%20Zhang%20and%20Hongliu%20Li%20and%20Yajun%20Gao%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Few-shot%20medical%20image%20segmentation%20has%20achieved%20great%20progress%20in%20improving%0Aaccuracy%20and%20efficiency%20of%20medical%20analysis%20in%20the%20biomedical%20imaging%20field.%0AHowever%2C%20most%20existing%20methods%20cannot%20explore%20inter-class%20relations%20among%20base%0Aand%20novel%20medical%20classes%20to%20reason%20unseen%20novel%20classes.%20Moreover%2C%20the%20same%0Akind%20of%20medical%20class%20has%20large%20intra-class%20variations%20brought%20by%20diverse%0Aappearances%2C%20shapes%20and%20scales%2C%20thus%20causing%20ambiguous%20visual%20characterization%0Ato%20degrade%20generalization%20performance%20of%20these%20existing%20methods%20on%20unseen%20novel%0Aclasses.%20To%20address%20the%20above%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%0A%5Cunderline%7B%5Ctextbf%7BP%7D%7Drototype%20correlation%20%5Cunderline%7B%5Ctextbf%7BM%7D%7Datching%20and%0A%5Cunderline%7B%5Ctextbf%7BC%7D%7Dlass-relation%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Deasoning%20%28i.e.%2C%0A%5Ctextbf%7BPMCR%7D%29%20model.%20The%20proposed%20model%20can%20effectively%20mitigate%20false%20pixel%0Acorrelation%20matches%20caused%20by%20large%20intra-class%20variations%20while%20reasoning%0Ainter-class%20relations%20among%20different%20medical%20classes.%20Specifically%2C%20in%20order%0Ato%20address%20false%20pixel%20correlation%20match%20brought%20by%20large%20intra-class%0Avariations%2C%20we%20propose%20a%20prototype%20correlation%20matching%20module%20to%20mine%0Arepresentative%20prototypes%20that%20can%20characterize%20diverse%20visual%20information%20of%0Adifferent%20appearances%20well.%20We%20aim%20to%20explore%20prototype-level%20rather%20than%0Apixel-level%20correlation%20matching%20between%20support%20and%20query%20features%20via%20optimal%0Atransport%20algorithm%20to%20tackle%20false%20matches%20caused%20by%20intra-class%20variations.%0AMeanwhile%2C%20in%20order%20to%20explore%20inter-class%20relations%2C%20we%20design%20a%0Aclass-relation%20reasoning%20module%20to%20segment%20unseen%20novel%20medical%20objects%20via%0Areasoning%20inter-class%20relations%20between%20base%20and%20novel%20classes.%20Such%0Ainter-class%20relations%20can%20be%20well%20propagated%20to%20semantic%20encoding%20of%20local%0Aquery%20features%20to%20improve%20few-shot%20segmentation%20performance.%20Quantitative%0Acomparisons%20illustrates%20the%20large%20performance%20improvement%20of%20our%20model%20over%0Aother%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototype%2520Correlation%2520Matching%2520and%2520Class-Relation%2520Reasoning%2520for%2520Few-Shot%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYumin%2520Zhang%2520and%2520Hongliu%2520Li%2520and%2520Yajun%2520Gao%2520and%2520Haoran%2520Duan%2520and%2520Yawen%2520Huang%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520Few-shot%2520medical%2520image%2520segmentation%2520has%2520achieved%2520great%2520progress%2520in%2520improving%250Aaccuracy%2520and%2520efficiency%2520of%2520medical%2520analysis%2520in%2520the%2520biomedical%2520imaging%2520field.%250AHowever%252C%2520most%2520existing%2520methods%2520cannot%2520explore%2520inter-class%2520relations%2520among%2520base%250Aand%2520novel%2520medical%2520classes%2520to%2520reason%2520unseen%2520novel%2520classes.%2520Moreover%252C%2520the%2520same%250Akind%2520of%2520medical%2520class%2520has%2520large%2520intra-class%2520variations%2520brought%2520by%2520diverse%250Aappearances%252C%2520shapes%2520and%2520scales%252C%2520thus%2520causing%2520ambiguous%2520visual%2520characterization%250Ato%2520degrade%2520generalization%2520performance%2520of%2520these%2520existing%2520methods%2520on%2520unseen%2520novel%250Aclasses.%2520To%2520address%2520the%2520above%2520challenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%250A%255Cunderline%257B%255Ctextbf%257BP%257D%257Drototype%2520correlation%2520%255Cunderline%257B%255Ctextbf%257BM%257D%257Datching%2520and%250A%255Cunderline%257B%255Ctextbf%257BC%257D%257Dlass-relation%2520%255Cunderline%257B%255Ctextbf%257BR%257D%257Deasoning%2520%2528i.e.%252C%250A%255Ctextbf%257BPMCR%257D%2529%2520model.%2520The%2520proposed%2520model%2520can%2520effectively%2520mitigate%2520false%2520pixel%250Acorrelation%2520matches%2520caused%2520by%2520large%2520intra-class%2520variations%2520while%2520reasoning%250Ainter-class%2520relations%2520among%2520different%2520medical%2520classes.%2520Specifically%252C%2520in%2520order%250Ato%2520address%2520false%2520pixel%2520correlation%2520match%2520brought%2520by%2520large%2520intra-class%250Avariations%252C%2520we%2520propose%2520a%2520prototype%2520correlation%2520matching%2520module%2520to%2520mine%250Arepresentative%2520prototypes%2520that%2520can%2520characterize%2520diverse%2520visual%2520information%2520of%250Adifferent%2520appearances%2520well.%2520We%2520aim%2520to%2520explore%2520prototype-level%2520rather%2520than%250Apixel-level%2520correlation%2520matching%2520between%2520support%2520and%2520query%2520features%2520via%2520optimal%250Atransport%2520algorithm%2520to%2520tackle%2520false%2520matches%2520caused%2520by%2520intra-class%2520variations.%250AMeanwhile%252C%2520in%2520order%2520to%2520explore%2520inter-class%2520relations%252C%2520we%2520design%2520a%250Aclass-relation%2520reasoning%2520module%2520to%2520segment%2520unseen%2520novel%2520medical%2520objects%2520via%250Areasoning%2520inter-class%2520relations%2520between%2520base%2520and%2520novel%2520classes.%2520Such%250Ainter-class%2520relations%2520can%2520be%2520well%2520propagated%2520to%2520semantic%2520encoding%2520of%2520local%250Aquery%2520features%2520to%2520improve%2520few-shot%2520segmentation%2520performance.%2520Quantitative%250Acomparisons%2520illustrates%2520the%2520large%2520performance%2520improvement%2520of%2520our%2520model%2520over%250Aother%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototype%20Correlation%20Matching%20and%20Class-Relation%20Reasoning%20for%20Few-Shot%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yumin%20Zhang%20and%20Hongliu%20Li%20and%20Yajun%20Gao%20and%20Haoran%20Duan%20and%20Yawen%20Huang%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Few-shot%20medical%20image%20segmentation%20has%20achieved%20great%20progress%20in%20improving%0Aaccuracy%20and%20efficiency%20of%20medical%20analysis%20in%20the%20biomedical%20imaging%20field.%0AHowever%2C%20most%20existing%20methods%20cannot%20explore%20inter-class%20relations%20among%20base%0Aand%20novel%20medical%20classes%20to%20reason%20unseen%20novel%20classes.%20Moreover%2C%20the%20same%0Akind%20of%20medical%20class%20has%20large%20intra-class%20variations%20brought%20by%20diverse%0Aappearances%2C%20shapes%20and%20scales%2C%20thus%20causing%20ambiguous%20visual%20characterization%0Ato%20degrade%20generalization%20performance%20of%20these%20existing%20methods%20on%20unseen%20novel%0Aclasses.%20To%20address%20the%20above%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%0A%5Cunderline%7B%5Ctextbf%7BP%7D%7Drototype%20correlation%20%5Cunderline%7B%5Ctextbf%7BM%7D%7Datching%20and%0A%5Cunderline%7B%5Ctextbf%7BC%7D%7Dlass-relation%20%5Cunderline%7B%5Ctextbf%7BR%7D%7Deasoning%20%28i.e.%2C%0A%5Ctextbf%7BPMCR%7D%29%20model.%20The%20proposed%20model%20can%20effectively%20mitigate%20false%20pixel%0Acorrelation%20matches%20caused%20by%20large%20intra-class%20variations%20while%20reasoning%0Ainter-class%20relations%20among%20different%20medical%20classes.%20Specifically%2C%20in%20order%0Ato%20address%20false%20pixel%20correlation%20match%20brought%20by%20large%20intra-class%0Avariations%2C%20we%20propose%20a%20prototype%20correlation%20matching%20module%20to%20mine%0Arepresentative%20prototypes%20that%20can%20characterize%20diverse%20visual%20information%20of%0Adifferent%20appearances%20well.%20We%20aim%20to%20explore%20prototype-level%20rather%20than%0Apixel-level%20correlation%20matching%20between%20support%20and%20query%20features%20via%20optimal%0Atransport%20algorithm%20to%20tackle%20false%20matches%20caused%20by%20intra-class%20variations.%0AMeanwhile%2C%20in%20order%20to%20explore%20inter-class%20relations%2C%20we%20design%20a%0Aclass-relation%20reasoning%20module%20to%20segment%20unseen%20novel%20medical%20objects%20via%0Areasoning%20inter-class%20relations%20between%20base%20and%20novel%20classes.%20Such%0Ainter-class%20relations%20can%20be%20well%20propagated%20to%20semantic%20encoding%20of%20local%0Aquery%20features%20to%20improve%20few-shot%20segmentation%20performance.%20Quantitative%0Acomparisons%20illustrates%20the%20large%20performance%20improvement%20of%20our%20model%20over%0Aother%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05054v1&entry.124074799=Read"},
{"title": "MedYOLO: A Medical Image Object Detection Framework", "author": "Joseph Sobek and Jose R. Medina Inojosa and Betsy J. Medina Inojosa and S. M. Rassoulinejad-Mousavi and Gian Marco Conte and Francisco Lopez-Jimenez and Bradley J. Erickson", "abstract": "  Artificial intelligence-enhanced identification of organs, lesions, and other\nstructures in medical imaging is typically done using convolutional neural\nnetworks (CNNs) designed to make voxel-accurate segmentations of the region of\ninterest. However, the labels required to train these CNNs are time-consuming\nto generate and require attention from subject matter experts to ensure\nquality. For tasks where voxel-level precision is not required, object\ndetection models offer a viable alternative that can reduce annotation effort.\nDespite this potential application, there are few options for general purpose\nobject detection frameworks available for 3-D medical imaging. We report on\nMedYOLO, a 3-D object detection framework using the one-shot detection method\nof the YOLO family of models and designed for use with medical imaging. We\ntested this model on four different datasets: BRaTS, LIDC, an abdominal organ\nComputed Tomography (CT) dataset, and an ECG-gated heart CT dataset. We found\nour models achieve high performance on commonly present medium and large-sized\nstructures such as the heart, liver, and pancreas even without hyperparameter\ntuning. However, the models struggle with very small or rarely present\nstructures.\n", "link": "http://arxiv.org/abs/2312.07729v2", "date": "2024-06-07", "relevancy": 2.1077, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5264}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedYOLO%3A%20A%20Medical%20Image%20Object%20Detection%20Framework&body=Title%3A%20MedYOLO%3A%20A%20Medical%20Image%20Object%20Detection%20Framework%0AAuthor%3A%20Joseph%20Sobek%20and%20Jose%20R.%20Medina%20Inojosa%20and%20Betsy%20J.%20Medina%20Inojosa%20and%20S.%20M.%20Rassoulinejad-Mousavi%20and%20Gian%20Marco%20Conte%20and%20Francisco%20Lopez-Jimenez%20and%20Bradley%20J.%20Erickson%0AAbstract%3A%20%20%20Artificial%20intelligence-enhanced%20identification%20of%20organs%2C%20lesions%2C%20and%20other%0Astructures%20in%20medical%20imaging%20is%20typically%20done%20using%20convolutional%20neural%0Anetworks%20%28CNNs%29%20designed%20to%20make%20voxel-accurate%20segmentations%20of%20the%20region%20of%0Ainterest.%20However%2C%20the%20labels%20required%20to%20train%20these%20CNNs%20are%20time-consuming%0Ato%20generate%20and%20require%20attention%20from%20subject%20matter%20experts%20to%20ensure%0Aquality.%20For%20tasks%20where%20voxel-level%20precision%20is%20not%20required%2C%20object%0Adetection%20models%20offer%20a%20viable%20alternative%20that%20can%20reduce%20annotation%20effort.%0ADespite%20this%20potential%20application%2C%20there%20are%20few%20options%20for%20general%20purpose%0Aobject%20detection%20frameworks%20available%20for%203-D%20medical%20imaging.%20We%20report%20on%0AMedYOLO%2C%20a%203-D%20object%20detection%20framework%20using%20the%20one-shot%20detection%20method%0Aof%20the%20YOLO%20family%20of%20models%20and%20designed%20for%20use%20with%20medical%20imaging.%20We%0Atested%20this%20model%20on%20four%20different%20datasets%3A%20BRaTS%2C%20LIDC%2C%20an%20abdominal%20organ%0AComputed%20Tomography%20%28CT%29%20dataset%2C%20and%20an%20ECG-gated%20heart%20CT%20dataset.%20We%20found%0Aour%20models%20achieve%20high%20performance%20on%20commonly%20present%20medium%20and%20large-sized%0Astructures%20such%20as%20the%20heart%2C%20liver%2C%20and%20pancreas%20even%20without%20hyperparameter%0Atuning.%20However%2C%20the%20models%20struggle%20with%20very%20small%20or%20rarely%20present%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedYOLO%253A%2520A%2520Medical%2520Image%2520Object%2520Detection%2520Framework%26entry.906535625%3DJoseph%2520Sobek%2520and%2520Jose%2520R.%2520Medina%2520Inojosa%2520and%2520Betsy%2520J.%2520Medina%2520Inojosa%2520and%2520S.%2520M.%2520Rassoulinejad-Mousavi%2520and%2520Gian%2520Marco%2520Conte%2520and%2520Francisco%2520Lopez-Jimenez%2520and%2520Bradley%2520J.%2520Erickson%26entry.1292438233%3D%2520%2520Artificial%2520intelligence-enhanced%2520identification%2520of%2520organs%252C%2520lesions%252C%2520and%2520other%250Astructures%2520in%2520medical%2520imaging%2520is%2520typically%2520done%2520using%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520designed%2520to%2520make%2520voxel-accurate%2520segmentations%2520of%2520the%2520region%2520of%250Ainterest.%2520However%252C%2520the%2520labels%2520required%2520to%2520train%2520these%2520CNNs%2520are%2520time-consuming%250Ato%2520generate%2520and%2520require%2520attention%2520from%2520subject%2520matter%2520experts%2520to%2520ensure%250Aquality.%2520For%2520tasks%2520where%2520voxel-level%2520precision%2520is%2520not%2520required%252C%2520object%250Adetection%2520models%2520offer%2520a%2520viable%2520alternative%2520that%2520can%2520reduce%2520annotation%2520effort.%250ADespite%2520this%2520potential%2520application%252C%2520there%2520are%2520few%2520options%2520for%2520general%2520purpose%250Aobject%2520detection%2520frameworks%2520available%2520for%25203-D%2520medical%2520imaging.%2520We%2520report%2520on%250AMedYOLO%252C%2520a%25203-D%2520object%2520detection%2520framework%2520using%2520the%2520one-shot%2520detection%2520method%250Aof%2520the%2520YOLO%2520family%2520of%2520models%2520and%2520designed%2520for%2520use%2520with%2520medical%2520imaging.%2520We%250Atested%2520this%2520model%2520on%2520four%2520different%2520datasets%253A%2520BRaTS%252C%2520LIDC%252C%2520an%2520abdominal%2520organ%250AComputed%2520Tomography%2520%2528CT%2529%2520dataset%252C%2520and%2520an%2520ECG-gated%2520heart%2520CT%2520dataset.%2520We%2520found%250Aour%2520models%2520achieve%2520high%2520performance%2520on%2520commonly%2520present%2520medium%2520and%2520large-sized%250Astructures%2520such%2520as%2520the%2520heart%252C%2520liver%252C%2520and%2520pancreas%2520even%2520without%2520hyperparameter%250Atuning.%2520However%252C%2520the%2520models%2520struggle%2520with%2520very%2520small%2520or%2520rarely%2520present%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedYOLO%3A%20A%20Medical%20Image%20Object%20Detection%20Framework&entry.906535625=Joseph%20Sobek%20and%20Jose%20R.%20Medina%20Inojosa%20and%20Betsy%20J.%20Medina%20Inojosa%20and%20S.%20M.%20Rassoulinejad-Mousavi%20and%20Gian%20Marco%20Conte%20and%20Francisco%20Lopez-Jimenez%20and%20Bradley%20J.%20Erickson&entry.1292438233=%20%20Artificial%20intelligence-enhanced%20identification%20of%20organs%2C%20lesions%2C%20and%20other%0Astructures%20in%20medical%20imaging%20is%20typically%20done%20using%20convolutional%20neural%0Anetworks%20%28CNNs%29%20designed%20to%20make%20voxel-accurate%20segmentations%20of%20the%20region%20of%0Ainterest.%20However%2C%20the%20labels%20required%20to%20train%20these%20CNNs%20are%20time-consuming%0Ato%20generate%20and%20require%20attention%20from%20subject%20matter%20experts%20to%20ensure%0Aquality.%20For%20tasks%20where%20voxel-level%20precision%20is%20not%20required%2C%20object%0Adetection%20models%20offer%20a%20viable%20alternative%20that%20can%20reduce%20annotation%20effort.%0ADespite%20this%20potential%20application%2C%20there%20are%20few%20options%20for%20general%20purpose%0Aobject%20detection%20frameworks%20available%20for%203-D%20medical%20imaging.%20We%20report%20on%0AMedYOLO%2C%20a%203-D%20object%20detection%20framework%20using%20the%20one-shot%20detection%20method%0Aof%20the%20YOLO%20family%20of%20models%20and%20designed%20for%20use%20with%20medical%20imaging.%20We%0Atested%20this%20model%20on%20four%20different%20datasets%3A%20BRaTS%2C%20LIDC%2C%20an%20abdominal%20organ%0AComputed%20Tomography%20%28CT%29%20dataset%2C%20and%20an%20ECG-gated%20heart%20CT%20dataset.%20We%20found%0Aour%20models%20achieve%20high%20performance%20on%20commonly%20present%20medium%20and%20large-sized%0Astructures%20such%20as%20the%20heart%2C%20liver%2C%20and%20pancreas%20even%20without%20hyperparameter%0Atuning.%20However%2C%20the%20models%20struggle%20with%20very%20small%20or%20rarely%20present%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07729v2&entry.124074799=Read"},
{"title": "Scaling up Probabilistic PDE Simulators with Structured Volumetric\n  Information", "author": "Tim Weiland and Marvin Pf\u00f6rtner and Philipp Hennig", "abstract": "  Modeling real-world problems with partial differential equations (PDEs) is a\nprominent topic in scientific machine learning. Classic solvers for this task\ncontinue to play a central role, e.g. to generate training data for deep\nlearning analogues. Any such numerical solution is subject to multiple sources\nof uncertainty, both from limited computational resources and limited data\n(including unknown parameters). Gaussian process analogues to classic PDE\nsimulation methods have recently emerged as a framework to construct fully\nprobabilistic estimates of all these types of uncertainty. So far, much of this\nwork focused on theoretical foundations, and as such is not particularly data\nefficient or scalable. Here we propose a framework combining a discretization\nscheme based on the popular Finite Volume Method with complementary numerical\nlinear algebra techniques. Practical experiments, including a spatiotemporal\ntsunami simulation, demonstrate substantially improved scaling behavior of this\napproach over previous collocation-based techniques.\n", "link": "http://arxiv.org/abs/2406.05020v1", "date": "2024-06-07", "relevancy": 2.1045, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5281}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20up%20Probabilistic%20PDE%20Simulators%20with%20Structured%20Volumetric%0A%20%20Information&body=Title%3A%20Scaling%20up%20Probabilistic%20PDE%20Simulators%20with%20Structured%20Volumetric%0A%20%20Information%0AAuthor%3A%20Tim%20Weiland%20and%20Marvin%20Pf%C3%B6rtner%20and%20Philipp%20Hennig%0AAbstract%3A%20%20%20Modeling%20real-world%20problems%20with%20partial%20differential%20equations%20%28PDEs%29%20is%20a%0Aprominent%20topic%20in%20scientific%20machine%20learning.%20Classic%20solvers%20for%20this%20task%0Acontinue%20to%20play%20a%20central%20role%2C%20e.g.%20to%20generate%20training%20data%20for%20deep%0Alearning%20analogues.%20Any%20such%20numerical%20solution%20is%20subject%20to%20multiple%20sources%0Aof%20uncertainty%2C%20both%20from%20limited%20computational%20resources%20and%20limited%20data%0A%28including%20unknown%20parameters%29.%20Gaussian%20process%20analogues%20to%20classic%20PDE%0Asimulation%20methods%20have%20recently%20emerged%20as%20a%20framework%20to%20construct%20fully%0Aprobabilistic%20estimates%20of%20all%20these%20types%20of%20uncertainty.%20So%20far%2C%20much%20of%20this%0Awork%20focused%20on%20theoretical%20foundations%2C%20and%20as%20such%20is%20not%20particularly%20data%0Aefficient%20or%20scalable.%20Here%20we%20propose%20a%20framework%20combining%20a%20discretization%0Ascheme%20based%20on%20the%20popular%20Finite%20Volume%20Method%20with%20complementary%20numerical%0Alinear%20algebra%20techniques.%20Practical%20experiments%2C%20including%20a%20spatiotemporal%0Atsunami%20simulation%2C%20demonstrate%20substantially%20improved%20scaling%20behavior%20of%20this%0Aapproach%20over%20previous%20collocation-based%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520up%2520Probabilistic%2520PDE%2520Simulators%2520with%2520Structured%2520Volumetric%250A%2520%2520Information%26entry.906535625%3DTim%2520Weiland%2520and%2520Marvin%2520Pf%25C3%25B6rtner%2520and%2520Philipp%2520Hennig%26entry.1292438233%3D%2520%2520Modeling%2520real-world%2520problems%2520with%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%2520a%250Aprominent%2520topic%2520in%2520scientific%2520machine%2520learning.%2520Classic%2520solvers%2520for%2520this%2520task%250Acontinue%2520to%2520play%2520a%2520central%2520role%252C%2520e.g.%2520to%2520generate%2520training%2520data%2520for%2520deep%250Alearning%2520analogues.%2520Any%2520such%2520numerical%2520solution%2520is%2520subject%2520to%2520multiple%2520sources%250Aof%2520uncertainty%252C%2520both%2520from%2520limited%2520computational%2520resources%2520and%2520limited%2520data%250A%2528including%2520unknown%2520parameters%2529.%2520Gaussian%2520process%2520analogues%2520to%2520classic%2520PDE%250Asimulation%2520methods%2520have%2520recently%2520emerged%2520as%2520a%2520framework%2520to%2520construct%2520fully%250Aprobabilistic%2520estimates%2520of%2520all%2520these%2520types%2520of%2520uncertainty.%2520So%2520far%252C%2520much%2520of%2520this%250Awork%2520focused%2520on%2520theoretical%2520foundations%252C%2520and%2520as%2520such%2520is%2520not%2520particularly%2520data%250Aefficient%2520or%2520scalable.%2520Here%2520we%2520propose%2520a%2520framework%2520combining%2520a%2520discretization%250Ascheme%2520based%2520on%2520the%2520popular%2520Finite%2520Volume%2520Method%2520with%2520complementary%2520numerical%250Alinear%2520algebra%2520techniques.%2520Practical%2520experiments%252C%2520including%2520a%2520spatiotemporal%250Atsunami%2520simulation%252C%2520demonstrate%2520substantially%2520improved%2520scaling%2520behavior%2520of%2520this%250Aapproach%2520over%2520previous%2520collocation-based%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20up%20Probabilistic%20PDE%20Simulators%20with%20Structured%20Volumetric%0A%20%20Information&entry.906535625=Tim%20Weiland%20and%20Marvin%20Pf%C3%B6rtner%20and%20Philipp%20Hennig&entry.1292438233=%20%20Modeling%20real-world%20problems%20with%20partial%20differential%20equations%20%28PDEs%29%20is%20a%0Aprominent%20topic%20in%20scientific%20machine%20learning.%20Classic%20solvers%20for%20this%20task%0Acontinue%20to%20play%20a%20central%20role%2C%20e.g.%20to%20generate%20training%20data%20for%20deep%0Alearning%20analogues.%20Any%20such%20numerical%20solution%20is%20subject%20to%20multiple%20sources%0Aof%20uncertainty%2C%20both%20from%20limited%20computational%20resources%20and%20limited%20data%0A%28including%20unknown%20parameters%29.%20Gaussian%20process%20analogues%20to%20classic%20PDE%0Asimulation%20methods%20have%20recently%20emerged%20as%20a%20framework%20to%20construct%20fully%0Aprobabilistic%20estimates%20of%20all%20these%20types%20of%20uncertainty.%20So%20far%2C%20much%20of%20this%0Awork%20focused%20on%20theoretical%20foundations%2C%20and%20as%20such%20is%20not%20particularly%20data%0Aefficient%20or%20scalable.%20Here%20we%20propose%20a%20framework%20combining%20a%20discretization%0Ascheme%20based%20on%20the%20popular%20Finite%20Volume%20Method%20with%20complementary%20numerical%0Alinear%20algebra%20techniques.%20Practical%20experiments%2C%20including%20a%20spatiotemporal%0Atsunami%20simulation%2C%20demonstrate%20substantially%20improved%20scaling%20behavior%20of%20this%0Aapproach%20over%20previous%20collocation-based%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05020v1&entry.124074799=Read"},
{"title": "Massively Multiagent Minigames for Training Generalist Agents", "author": "Kyoung Whan Choe and Ryan Sullivan and Joseph Su\u00e1rez", "abstract": "  We present Meta MMO, a collection of many-agent minigames for use as a\nreinforcement learning benchmark. Meta MMO is built on top of Neural MMO, a\nmassively multiagent environment that has been the subject of two previous\nNeurIPS competitions. Our work expands Neural MMO with several computationally\nefficient minigames. We explore generalization across Meta MMO by learning to\nplay several minigames with a single set of weights. We release the\nenvironment, baselines, and training code under the MIT license. We hope that\nMeta MMO will spur additional progress on Neural MMO and, more generally, will\nserve as a useful benchmark for many-agent generalization.\n", "link": "http://arxiv.org/abs/2406.05071v1", "date": "2024-06-07", "relevancy": 2.0987, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5614}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5017}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Massively%20Multiagent%20Minigames%20for%20Training%20Generalist%20Agents&body=Title%3A%20Massively%20Multiagent%20Minigames%20for%20Training%20Generalist%20Agents%0AAuthor%3A%20Kyoung%20Whan%20Choe%20and%20Ryan%20Sullivan%20and%20Joseph%20Su%C3%A1rez%0AAbstract%3A%20%20%20We%20present%20Meta%20MMO%2C%20a%20collection%20of%20many-agent%20minigames%20for%20use%20as%20a%0Areinforcement%20learning%20benchmark.%20Meta%20MMO%20is%20built%20on%20top%20of%20Neural%20MMO%2C%20a%0Amassively%20multiagent%20environment%20that%20has%20been%20the%20subject%20of%20two%20previous%0ANeurIPS%20competitions.%20Our%20work%20expands%20Neural%20MMO%20with%20several%20computationally%0Aefficient%20minigames.%20We%20explore%20generalization%20across%20Meta%20MMO%20by%20learning%20to%0Aplay%20several%20minigames%20with%20a%20single%20set%20of%20weights.%20We%20release%20the%0Aenvironment%2C%20baselines%2C%20and%20training%20code%20under%20the%20MIT%20license.%20We%20hope%20that%0AMeta%20MMO%20will%20spur%20additional%20progress%20on%20Neural%20MMO%20and%2C%20more%20generally%2C%20will%0Aserve%20as%20a%20useful%20benchmark%20for%20many-agent%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMassively%2520Multiagent%2520Minigames%2520for%2520Training%2520Generalist%2520Agents%26entry.906535625%3DKyoung%2520Whan%2520Choe%2520and%2520Ryan%2520Sullivan%2520and%2520Joseph%2520Su%25C3%25A1rez%26entry.1292438233%3D%2520%2520We%2520present%2520Meta%2520MMO%252C%2520a%2520collection%2520of%2520many-agent%2520minigames%2520for%2520use%2520as%2520a%250Areinforcement%2520learning%2520benchmark.%2520Meta%2520MMO%2520is%2520built%2520on%2520top%2520of%2520Neural%2520MMO%252C%2520a%250Amassively%2520multiagent%2520environment%2520that%2520has%2520been%2520the%2520subject%2520of%2520two%2520previous%250ANeurIPS%2520competitions.%2520Our%2520work%2520expands%2520Neural%2520MMO%2520with%2520several%2520computationally%250Aefficient%2520minigames.%2520We%2520explore%2520generalization%2520across%2520Meta%2520MMO%2520by%2520learning%2520to%250Aplay%2520several%2520minigames%2520with%2520a%2520single%2520set%2520of%2520weights.%2520We%2520release%2520the%250Aenvironment%252C%2520baselines%252C%2520and%2520training%2520code%2520under%2520the%2520MIT%2520license.%2520We%2520hope%2520that%250AMeta%2520MMO%2520will%2520spur%2520additional%2520progress%2520on%2520Neural%2520MMO%2520and%252C%2520more%2520generally%252C%2520will%250Aserve%2520as%2520a%2520useful%2520benchmark%2520for%2520many-agent%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Massively%20Multiagent%20Minigames%20for%20Training%20Generalist%20Agents&entry.906535625=Kyoung%20Whan%20Choe%20and%20Ryan%20Sullivan%20and%20Joseph%20Su%C3%A1rez&entry.1292438233=%20%20We%20present%20Meta%20MMO%2C%20a%20collection%20of%20many-agent%20minigames%20for%20use%20as%20a%0Areinforcement%20learning%20benchmark.%20Meta%20MMO%20is%20built%20on%20top%20of%20Neural%20MMO%2C%20a%0Amassively%20multiagent%20environment%20that%20has%20been%20the%20subject%20of%20two%20previous%0ANeurIPS%20competitions.%20Our%20work%20expands%20Neural%20MMO%20with%20several%20computationally%0Aefficient%20minigames.%20We%20explore%20generalization%20across%20Meta%20MMO%20by%20learning%20to%0Aplay%20several%20minigames%20with%20a%20single%20set%20of%20weights.%20We%20release%20the%0Aenvironment%2C%20baselines%2C%20and%20training%20code%20under%20the%20MIT%20license.%20We%20hope%20that%0AMeta%20MMO%20will%20spur%20additional%20progress%20on%20Neural%20MMO%20and%2C%20more%20generally%2C%20will%0Aserve%20as%20a%20useful%20benchmark%20for%20many-agent%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05071v1&entry.124074799=Read"},
{"title": "Diversified Batch Selection for Training Acceleration", "author": "Feng Hong and Yueming Lyu and Jiangchao Yao and Ya Zhang and Ivor W. Tsang and Yanfeng Wang", "abstract": "  The remarkable success of modern machine learning models on large datasets\noften demands extensive training time and resource consumption. To save cost, a\nprevalent research line, known as online batch selection, explores selecting\ninformative subsets during the training process. Although recent efforts\nachieve advancements by measuring the impact of each sample on generalization,\ntheir reliance on additional reference models inherently limits their practical\napplications, when there are no such ideal models available. On the other hand,\nthe vanilla reference-model-free methods involve independently scoring and\nselecting data in a sample-wise manner, which sacrifices the diversity and\ninduces the redundancy. To tackle this dilemma, we propose Diversified Batch\nSelection (DivBS), which is reference-model-free and can efficiently select\ndiverse and representative samples. Specifically, we define a novel selection\nobjective that measures the group-wise orthogonalized representativeness to\ncombat the redundancy issue of previous sample-wise criteria, and provide a\nprincipled selection-efficient realization. Extensive experiments across\nvarious tasks demonstrate the significant superiority of DivBS in the\nperformance-speedup trade-off. The code is publicly available.\n", "link": "http://arxiv.org/abs/2406.04872v1", "date": "2024-06-07", "relevancy": 2.0941, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.53}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.519}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversified%20Batch%20Selection%20for%20Training%20Acceleration&body=Title%3A%20Diversified%20Batch%20Selection%20for%20Training%20Acceleration%0AAuthor%3A%20Feng%20Hong%20and%20Yueming%20Lyu%20and%20Jiangchao%20Yao%20and%20Ya%20Zhang%20and%20Ivor%20W.%20Tsang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20The%20remarkable%20success%20of%20modern%20machine%20learning%20models%20on%20large%20datasets%0Aoften%20demands%20extensive%20training%20time%20and%20resource%20consumption.%20To%20save%20cost%2C%20a%0Aprevalent%20research%20line%2C%20known%20as%20online%20batch%20selection%2C%20explores%20selecting%0Ainformative%20subsets%20during%20the%20training%20process.%20Although%20recent%20efforts%0Aachieve%20advancements%20by%20measuring%20the%20impact%20of%20each%20sample%20on%20generalization%2C%0Atheir%20reliance%20on%20additional%20reference%20models%20inherently%20limits%20their%20practical%0Aapplications%2C%20when%20there%20are%20no%20such%20ideal%20models%20available.%20On%20the%20other%20hand%2C%0Athe%20vanilla%20reference-model-free%20methods%20involve%20independently%20scoring%20and%0Aselecting%20data%20in%20a%20sample-wise%20manner%2C%20which%20sacrifices%20the%20diversity%20and%0Ainduces%20the%20redundancy.%20To%20tackle%20this%20dilemma%2C%20we%20propose%20Diversified%20Batch%0ASelection%20%28DivBS%29%2C%20which%20is%20reference-model-free%20and%20can%20efficiently%20select%0Adiverse%20and%20representative%20samples.%20Specifically%2C%20we%20define%20a%20novel%20selection%0Aobjective%20that%20measures%20the%20group-wise%20orthogonalized%20representativeness%20to%0Acombat%20the%20redundancy%20issue%20of%20previous%20sample-wise%20criteria%2C%20and%20provide%20a%0Aprincipled%20selection-efficient%20realization.%20Extensive%20experiments%20across%0Avarious%20tasks%20demonstrate%20the%20significant%20superiority%20of%20DivBS%20in%20the%0Aperformance-speedup%20trade-off.%20The%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversified%2520Batch%2520Selection%2520for%2520Training%2520Acceleration%26entry.906535625%3DFeng%2520Hong%2520and%2520Yueming%2520Lyu%2520and%2520Jiangchao%2520Yao%2520and%2520Ya%2520Zhang%2520and%2520Ivor%2520W.%2520Tsang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520The%2520remarkable%2520success%2520of%2520modern%2520machine%2520learning%2520models%2520on%2520large%2520datasets%250Aoften%2520demands%2520extensive%2520training%2520time%2520and%2520resource%2520consumption.%2520To%2520save%2520cost%252C%2520a%250Aprevalent%2520research%2520line%252C%2520known%2520as%2520online%2520batch%2520selection%252C%2520explores%2520selecting%250Ainformative%2520subsets%2520during%2520the%2520training%2520process.%2520Although%2520recent%2520efforts%250Aachieve%2520advancements%2520by%2520measuring%2520the%2520impact%2520of%2520each%2520sample%2520on%2520generalization%252C%250Atheir%2520reliance%2520on%2520additional%2520reference%2520models%2520inherently%2520limits%2520their%2520practical%250Aapplications%252C%2520when%2520there%2520are%2520no%2520such%2520ideal%2520models%2520available.%2520On%2520the%2520other%2520hand%252C%250Athe%2520vanilla%2520reference-model-free%2520methods%2520involve%2520independently%2520scoring%2520and%250Aselecting%2520data%2520in%2520a%2520sample-wise%2520manner%252C%2520which%2520sacrifices%2520the%2520diversity%2520and%250Ainduces%2520the%2520redundancy.%2520To%2520tackle%2520this%2520dilemma%252C%2520we%2520propose%2520Diversified%2520Batch%250ASelection%2520%2528DivBS%2529%252C%2520which%2520is%2520reference-model-free%2520and%2520can%2520efficiently%2520select%250Adiverse%2520and%2520representative%2520samples.%2520Specifically%252C%2520we%2520define%2520a%2520novel%2520selection%250Aobjective%2520that%2520measures%2520the%2520group-wise%2520orthogonalized%2520representativeness%2520to%250Acombat%2520the%2520redundancy%2520issue%2520of%2520previous%2520sample-wise%2520criteria%252C%2520and%2520provide%2520a%250Aprincipled%2520selection-efficient%2520realization.%2520Extensive%2520experiments%2520across%250Avarious%2520tasks%2520demonstrate%2520the%2520significant%2520superiority%2520of%2520DivBS%2520in%2520the%250Aperformance-speedup%2520trade-off.%2520The%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversified%20Batch%20Selection%20for%20Training%20Acceleration&entry.906535625=Feng%20Hong%20and%20Yueming%20Lyu%20and%20Jiangchao%20Yao%20and%20Ya%20Zhang%20and%20Ivor%20W.%20Tsang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20The%20remarkable%20success%20of%20modern%20machine%20learning%20models%20on%20large%20datasets%0Aoften%20demands%20extensive%20training%20time%20and%20resource%20consumption.%20To%20save%20cost%2C%20a%0Aprevalent%20research%20line%2C%20known%20as%20online%20batch%20selection%2C%20explores%20selecting%0Ainformative%20subsets%20during%20the%20training%20process.%20Although%20recent%20efforts%0Aachieve%20advancements%20by%20measuring%20the%20impact%20of%20each%20sample%20on%20generalization%2C%0Atheir%20reliance%20on%20additional%20reference%20models%20inherently%20limits%20their%20practical%0Aapplications%2C%20when%20there%20are%20no%20such%20ideal%20models%20available.%20On%20the%20other%20hand%2C%0Athe%20vanilla%20reference-model-free%20methods%20involve%20independently%20scoring%20and%0Aselecting%20data%20in%20a%20sample-wise%20manner%2C%20which%20sacrifices%20the%20diversity%20and%0Ainduces%20the%20redundancy.%20To%20tackle%20this%20dilemma%2C%20we%20propose%20Diversified%20Batch%0ASelection%20%28DivBS%29%2C%20which%20is%20reference-model-free%20and%20can%20efficiently%20select%0Adiverse%20and%20representative%20samples.%20Specifically%2C%20we%20define%20a%20novel%20selection%0Aobjective%20that%20measures%20the%20group-wise%20orthogonalized%20representativeness%20to%0Acombat%20the%20redundancy%20issue%20of%20previous%20sample-wise%20criteria%2C%20and%20provide%20a%0Aprincipled%20selection-efficient%20realization.%20Extensive%20experiments%20across%0Avarious%20tasks%20demonstrate%20the%20significant%20superiority%20of%20DivBS%20in%20the%0Aperformance-speedup%20trade-off.%20The%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04872v1&entry.124074799=Read"},
{"title": "Spiking Neural Networks for event-based action recognition: A new task\n  to understand their advantage", "author": "Alex Vicente-Sola and Davide L. Manna and Paul Kirkland and Gaetano Di Caterina and Trevor Bihl", "abstract": "  Spiking Neural Networks (SNN) are characterised by their unique temporal\ndynamics, but the properties and advantages of such computations are still not\nwell understood. In order to provide answers, in this work we demonstrate how\nSpiking neurons can enable temporal feature extraction in feed-forward neural\nnetworks without the need for recurrent synapses, and how recurrent SNNs can\nachieve comparable results to LSTM with a smaller number of parameters. This\nshows how their bio-inspired computing principles can be successfully exploited\nbeyond energy efficiency gains and evidences their differences with respect to\nconventional artificial neural networks. These results are obtained through a\nnew task, DVS-Gesture-Chain (DVS-GC), which allows, for the first time, to\nevaluate the perception of temporal dependencies in a real event-based action\nrecognition dataset. Our study proves how the widely used DVS Gesture benchmark\ncan be solved by networks without temporal feature extraction when its events\nare accumulated in frames, unlike the new DVS-GC which demands an understanding\nof the order in which events happen. Furthermore, this setup allowed us to\nreveal the role of the leakage rate in spiking neurons for temporal processing\ntasks and demonstrated the benefits of \"hard reset\" mechanisms. Additionally,\nwe also show how time-dependent weights and normalization can lead to\nunderstanding order by means of temporal attention.\n", "link": "http://arxiv.org/abs/2209.14915v3", "date": "2024-06-07", "relevancy": 2.0922, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5351}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5242}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Neural%20Networks%20for%20event-based%20action%20recognition%3A%20A%20new%20task%0A%20%20to%20understand%20their%20advantage&body=Title%3A%20Spiking%20Neural%20Networks%20for%20event-based%20action%20recognition%3A%20A%20new%20task%0A%20%20to%20understand%20their%20advantage%0AAuthor%3A%20Alex%20Vicente-Sola%20and%20Davide%20L.%20Manna%20and%20Paul%20Kirkland%20and%20Gaetano%20Di%20Caterina%20and%20Trevor%20Bihl%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNN%29%20are%20characterised%20by%20their%20unique%20temporal%0Adynamics%2C%20but%20the%20properties%20and%20advantages%20of%20such%20computations%20are%20still%20not%0Awell%20understood.%20In%20order%20to%20provide%20answers%2C%20in%20this%20work%20we%20demonstrate%20how%0ASpiking%20neurons%20can%20enable%20temporal%20feature%20extraction%20in%20feed-forward%20neural%0Anetworks%20without%20the%20need%20for%20recurrent%20synapses%2C%20and%20how%20recurrent%20SNNs%20can%0Aachieve%20comparable%20results%20to%20LSTM%20with%20a%20smaller%20number%20of%20parameters.%20This%0Ashows%20how%20their%20bio-inspired%20computing%20principles%20can%20be%20successfully%20exploited%0Abeyond%20energy%20efficiency%20gains%20and%20evidences%20their%20differences%20with%20respect%20to%0Aconventional%20artificial%20neural%20networks.%20These%20results%20are%20obtained%20through%20a%0Anew%20task%2C%20DVS-Gesture-Chain%20%28DVS-GC%29%2C%20which%20allows%2C%20for%20the%20first%20time%2C%20to%0Aevaluate%20the%20perception%20of%20temporal%20dependencies%20in%20a%20real%20event-based%20action%0Arecognition%20dataset.%20Our%20study%20proves%20how%20the%20widely%20used%20DVS%20Gesture%20benchmark%0Acan%20be%20solved%20by%20networks%20without%20temporal%20feature%20extraction%20when%20its%20events%0Aare%20accumulated%20in%20frames%2C%20unlike%20the%20new%20DVS-GC%20which%20demands%20an%20understanding%0Aof%20the%20order%20in%20which%20events%20happen.%20Furthermore%2C%20this%20setup%20allowed%20us%20to%0Areveal%20the%20role%20of%20the%20leakage%20rate%20in%20spiking%20neurons%20for%20temporal%20processing%0Atasks%20and%20demonstrated%20the%20benefits%20of%20%22hard%20reset%22%20mechanisms.%20Additionally%2C%0Awe%20also%20show%20how%20time-dependent%20weights%20and%20normalization%20can%20lead%20to%0Aunderstanding%20order%20by%20means%20of%20temporal%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.14915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Neural%2520Networks%2520for%2520event-based%2520action%2520recognition%253A%2520A%2520new%2520task%250A%2520%2520to%2520understand%2520their%2520advantage%26entry.906535625%3DAlex%2520Vicente-Sola%2520and%2520Davide%2520L.%2520Manna%2520and%2520Paul%2520Kirkland%2520and%2520Gaetano%2520Di%2520Caterina%2520and%2520Trevor%2520Bihl%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNN%2529%2520are%2520characterised%2520by%2520their%2520unique%2520temporal%250Adynamics%252C%2520but%2520the%2520properties%2520and%2520advantages%2520of%2520such%2520computations%2520are%2520still%2520not%250Awell%2520understood.%2520In%2520order%2520to%2520provide%2520answers%252C%2520in%2520this%2520work%2520we%2520demonstrate%2520how%250ASpiking%2520neurons%2520can%2520enable%2520temporal%2520feature%2520extraction%2520in%2520feed-forward%2520neural%250Anetworks%2520without%2520the%2520need%2520for%2520recurrent%2520synapses%252C%2520and%2520how%2520recurrent%2520SNNs%2520can%250Aachieve%2520comparable%2520results%2520to%2520LSTM%2520with%2520a%2520smaller%2520number%2520of%2520parameters.%2520This%250Ashows%2520how%2520their%2520bio-inspired%2520computing%2520principles%2520can%2520be%2520successfully%2520exploited%250Abeyond%2520energy%2520efficiency%2520gains%2520and%2520evidences%2520their%2520differences%2520with%2520respect%2520to%250Aconventional%2520artificial%2520neural%2520networks.%2520These%2520results%2520are%2520obtained%2520through%2520a%250Anew%2520task%252C%2520DVS-Gesture-Chain%2520%2528DVS-GC%2529%252C%2520which%2520allows%252C%2520for%2520the%2520first%2520time%252C%2520to%250Aevaluate%2520the%2520perception%2520of%2520temporal%2520dependencies%2520in%2520a%2520real%2520event-based%2520action%250Arecognition%2520dataset.%2520Our%2520study%2520proves%2520how%2520the%2520widely%2520used%2520DVS%2520Gesture%2520benchmark%250Acan%2520be%2520solved%2520by%2520networks%2520without%2520temporal%2520feature%2520extraction%2520when%2520its%2520events%250Aare%2520accumulated%2520in%2520frames%252C%2520unlike%2520the%2520new%2520DVS-GC%2520which%2520demands%2520an%2520understanding%250Aof%2520the%2520order%2520in%2520which%2520events%2520happen.%2520Furthermore%252C%2520this%2520setup%2520allowed%2520us%2520to%250Areveal%2520the%2520role%2520of%2520the%2520leakage%2520rate%2520in%2520spiking%2520neurons%2520for%2520temporal%2520processing%250Atasks%2520and%2520demonstrated%2520the%2520benefits%2520of%2520%2522hard%2520reset%2522%2520mechanisms.%2520Additionally%252C%250Awe%2520also%2520show%2520how%2520time-dependent%2520weights%2520and%2520normalization%2520can%2520lead%2520to%250Aunderstanding%2520order%2520by%2520means%2520of%2520temporal%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.14915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Neural%20Networks%20for%20event-based%20action%20recognition%3A%20A%20new%20task%0A%20%20to%20understand%20their%20advantage&entry.906535625=Alex%20Vicente-Sola%20and%20Davide%20L.%20Manna%20and%20Paul%20Kirkland%20and%20Gaetano%20Di%20Caterina%20and%20Trevor%20Bihl&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNN%29%20are%20characterised%20by%20their%20unique%20temporal%0Adynamics%2C%20but%20the%20properties%20and%20advantages%20of%20such%20computations%20are%20still%20not%0Awell%20understood.%20In%20order%20to%20provide%20answers%2C%20in%20this%20work%20we%20demonstrate%20how%0ASpiking%20neurons%20can%20enable%20temporal%20feature%20extraction%20in%20feed-forward%20neural%0Anetworks%20without%20the%20need%20for%20recurrent%20synapses%2C%20and%20how%20recurrent%20SNNs%20can%0Aachieve%20comparable%20results%20to%20LSTM%20with%20a%20smaller%20number%20of%20parameters.%20This%0Ashows%20how%20their%20bio-inspired%20computing%20principles%20can%20be%20successfully%20exploited%0Abeyond%20energy%20efficiency%20gains%20and%20evidences%20their%20differences%20with%20respect%20to%0Aconventional%20artificial%20neural%20networks.%20These%20results%20are%20obtained%20through%20a%0Anew%20task%2C%20DVS-Gesture-Chain%20%28DVS-GC%29%2C%20which%20allows%2C%20for%20the%20first%20time%2C%20to%0Aevaluate%20the%20perception%20of%20temporal%20dependencies%20in%20a%20real%20event-based%20action%0Arecognition%20dataset.%20Our%20study%20proves%20how%20the%20widely%20used%20DVS%20Gesture%20benchmark%0Acan%20be%20solved%20by%20networks%20without%20temporal%20feature%20extraction%20when%20its%20events%0Aare%20accumulated%20in%20frames%2C%20unlike%20the%20new%20DVS-GC%20which%20demands%20an%20understanding%0Aof%20the%20order%20in%20which%20events%20happen.%20Furthermore%2C%20this%20setup%20allowed%20us%20to%0Areveal%20the%20role%20of%20the%20leakage%20rate%20in%20spiking%20neurons%20for%20temporal%20processing%0Atasks%20and%20demonstrated%20the%20benefits%20of%20%22hard%20reset%22%20mechanisms.%20Additionally%2C%0Awe%20also%20show%20how%20time-dependent%20weights%20and%20normalization%20can%20lead%20to%0Aunderstanding%20order%20by%20means%20of%20temporal%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.14915v3&entry.124074799=Read"},
{"title": "Seeing the Unseen: Visual Metaphor Captioning for Videos", "author": "Abisek Rajakumar Kalarani and Pushpak Bhattacharyya and Sumit Shekhar", "abstract": "  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n", "link": "http://arxiv.org/abs/2406.04886v1", "date": "2024-06-07", "relevancy": 2.0878, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5312}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20the%20Unseen%3A%20Visual%20Metaphor%20Captioning%20for%20Videos&body=Title%3A%20Seeing%20the%20Unseen%3A%20Visual%20Metaphor%20Captioning%20for%20Videos%0AAuthor%3A%20Abisek%20Rajakumar%20Kalarani%20and%20Pushpak%20Bhattacharyya%20and%20Sumit%20Shekhar%0AAbstract%3A%20%20%20Metaphors%20are%20a%20common%20communication%20tool%20used%20in%20our%20day-to-day%20life.%20The%0Adetection%20and%20generation%20of%20metaphors%20in%20textual%20form%20have%20been%20studied%0Aextensively%20but%20metaphors%20in%20other%20forms%20have%20been%20under-explored.%20Recent%0Astudies%20have%20shown%20that%20Vision-Language%20%28VL%29%20models%20cannot%20understand%20visual%0Ametaphors%20in%20memes%20and%20adverts.%20As%20of%20now%2C%20no%20probing%20studies%20have%20been%20done%0Athat%20involve%20complex%20language%20phenomena%20like%20metaphors%20with%20videos.%20Hence%2C%20we%0Aintroduce%20a%20new%20VL%20task%20of%20describing%20the%20metaphors%20present%20in%20the%20videos%20in%0Aour%20work.%20To%20facilitate%20this%20novel%20task%2C%20we%20construct%20and%20release%20a%20manually%0Acreated%20dataset%20with%20705%20videos%20and%202115%20human-written%20captions%2C%20along%20with%20a%0Anew%20metric%20called%20Average%20Concept%20Distance%20%28ACD%29%2C%20to%20automatically%20evaluate%20the%0Acreativity%20of%20the%20metaphors%20generated.%20We%20also%20propose%20a%20novel%20low-resource%0Avideo%20metaphor%20captioning%20system%3A%20GIT-LLaVA%2C%20which%20obtains%20comparable%0Aperformance%20to%20SoTA%20video%20language%20models%20on%20the%20proposed%20task.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20existing%20video%20language%20models%20on%20this%20task%20and%0Apublish%20our%20dataset%2C%20models%2C%20and%20benchmark%20results%20to%20enable%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520the%2520Unseen%253A%2520Visual%2520Metaphor%2520Captioning%2520for%2520Videos%26entry.906535625%3DAbisek%2520Rajakumar%2520Kalarani%2520and%2520Pushpak%2520Bhattacharyya%2520and%2520Sumit%2520Shekhar%26entry.1292438233%3D%2520%2520Metaphors%2520are%2520a%2520common%2520communication%2520tool%2520used%2520in%2520our%2520day-to-day%2520life.%2520The%250Adetection%2520and%2520generation%2520of%2520metaphors%2520in%2520textual%2520form%2520have%2520been%2520studied%250Aextensively%2520but%2520metaphors%2520in%2520other%2520forms%2520have%2520been%2520under-explored.%2520Recent%250Astudies%2520have%2520shown%2520that%2520Vision-Language%2520%2528VL%2529%2520models%2520cannot%2520understand%2520visual%250Ametaphors%2520in%2520memes%2520and%2520adverts.%2520As%2520of%2520now%252C%2520no%2520probing%2520studies%2520have%2520been%2520done%250Athat%2520involve%2520complex%2520language%2520phenomena%2520like%2520metaphors%2520with%2520videos.%2520Hence%252C%2520we%250Aintroduce%2520a%2520new%2520VL%2520task%2520of%2520describing%2520the%2520metaphors%2520present%2520in%2520the%2520videos%2520in%250Aour%2520work.%2520To%2520facilitate%2520this%2520novel%2520task%252C%2520we%2520construct%2520and%2520release%2520a%2520manually%250Acreated%2520dataset%2520with%2520705%2520videos%2520and%25202115%2520human-written%2520captions%252C%2520along%2520with%2520a%250Anew%2520metric%2520called%2520Average%2520Concept%2520Distance%2520%2528ACD%2529%252C%2520to%2520automatically%2520evaluate%2520the%250Acreativity%2520of%2520the%2520metaphors%2520generated.%2520We%2520also%2520propose%2520a%2520novel%2520low-resource%250Avideo%2520metaphor%2520captioning%2520system%253A%2520GIT-LLaVA%252C%2520which%2520obtains%2520comparable%250Aperformance%2520to%2520SoTA%2520video%2520language%2520models%2520on%2520the%2520proposed%2520task.%2520We%2520perform%2520a%250Acomprehensive%2520analysis%2520of%2520existing%2520video%2520language%2520models%2520on%2520this%2520task%2520and%250Apublish%2520our%2520dataset%252C%2520models%252C%2520and%2520benchmark%2520results%2520to%2520enable%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20the%20Unseen%3A%20Visual%20Metaphor%20Captioning%20for%20Videos&entry.906535625=Abisek%20Rajakumar%20Kalarani%20and%20Pushpak%20Bhattacharyya%20and%20Sumit%20Shekhar&entry.1292438233=%20%20Metaphors%20are%20a%20common%20communication%20tool%20used%20in%20our%20day-to-day%20life.%20The%0Adetection%20and%20generation%20of%20metaphors%20in%20textual%20form%20have%20been%20studied%0Aextensively%20but%20metaphors%20in%20other%20forms%20have%20been%20under-explored.%20Recent%0Astudies%20have%20shown%20that%20Vision-Language%20%28VL%29%20models%20cannot%20understand%20visual%0Ametaphors%20in%20memes%20and%20adverts.%20As%20of%20now%2C%20no%20probing%20studies%20have%20been%20done%0Athat%20involve%20complex%20language%20phenomena%20like%20metaphors%20with%20videos.%20Hence%2C%20we%0Aintroduce%20a%20new%20VL%20task%20of%20describing%20the%20metaphors%20present%20in%20the%20videos%20in%0Aour%20work.%20To%20facilitate%20this%20novel%20task%2C%20we%20construct%20and%20release%20a%20manually%0Acreated%20dataset%20with%20705%20videos%20and%202115%20human-written%20captions%2C%20along%20with%20a%0Anew%20metric%20called%20Average%20Concept%20Distance%20%28ACD%29%2C%20to%20automatically%20evaluate%20the%0Acreativity%20of%20the%20metaphors%20generated.%20We%20also%20propose%20a%20novel%20low-resource%0Avideo%20metaphor%20captioning%20system%3A%20GIT-LLaVA%2C%20which%20obtains%20comparable%0Aperformance%20to%20SoTA%20video%20language%20models%20on%20the%20proposed%20task.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20existing%20video%20language%20models%20on%20this%20task%20and%0Apublish%20our%20dataset%2C%20models%2C%20and%20benchmark%20results%20to%20enable%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04886v1&entry.124074799=Read"},
{"title": "Semantic Segmentation on VSPW Dataset through Masked Video Consistency", "author": "Chen Liang and Qiang Guo and Chongkai Yu and Chengjing Wu and Ting Liu and Luoqi Liu", "abstract": "  Pixel-level Video Understanding requires effectively integrating\nthree-dimensional data in both spatial and temporal dimensions to learn\naccurate and stable semantic information from continuous frames. However,\nexisting advanced models on the VSPW dataset have not fully modeled\nspatiotemporal relationships. In this paper, we present our solution for the\nPVUW competition, where we introduce masked video consistency (MVC) based on\nexisting models. MVC enforces the consistency between predictions of masked\nframes where random patches are withheld. The model needs to learn the\nsegmentation results of the masked parts through the context of images and the\nrelationship between preceding and succeeding frames of the video.\nAdditionally, we employed test-time augmentation, model aggeregation and a\nmultimodal model-based post-processing method. Our approach achieves 67.27%\nmIoU performance on the VSPW dataset, ranking 2nd place in the PVUW2024\nchallenge VSS track.\n", "link": "http://arxiv.org/abs/2406.04979v1", "date": "2024-06-07", "relevancy": 2.0741, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Segmentation%20on%20VSPW%20Dataset%20through%20Masked%20Video%20Consistency&body=Title%3A%20Semantic%20Segmentation%20on%20VSPW%20Dataset%20through%20Masked%20Video%20Consistency%0AAuthor%3A%20Chen%20Liang%20and%20Qiang%20Guo%20and%20Chongkai%20Yu%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu%0AAbstract%3A%20%20%20Pixel-level%20Video%20Understanding%20requires%20effectively%20integrating%0Athree-dimensional%20data%20in%20both%20spatial%20and%20temporal%20dimensions%20to%20learn%0Aaccurate%20and%20stable%20semantic%20information%20from%20continuous%20frames.%20However%2C%0Aexisting%20advanced%20models%20on%20the%20VSPW%20dataset%20have%20not%20fully%20modeled%0Aspatiotemporal%20relationships.%20In%20this%20paper%2C%20we%20present%20our%20solution%20for%20the%0APVUW%20competition%2C%20where%20we%20introduce%20masked%20video%20consistency%20%28MVC%29%20based%20on%0Aexisting%20models.%20MVC%20enforces%20the%20consistency%20between%20predictions%20of%20masked%0Aframes%20where%20random%20patches%20are%20withheld.%20The%20model%20needs%20to%20learn%20the%0Asegmentation%20results%20of%20the%20masked%20parts%20through%20the%20context%20of%20images%20and%20the%0Arelationship%20between%20preceding%20and%20succeeding%20frames%20of%20the%20video.%0AAdditionally%2C%20we%20employed%20test-time%20augmentation%2C%20model%20aggeregation%20and%20a%0Amultimodal%20model-based%20post-processing%20method.%20Our%20approach%20achieves%2067.27%25%0AmIoU%20performance%20on%20the%20VSPW%20dataset%2C%20ranking%202nd%20place%20in%20the%20PVUW2024%0Achallenge%20VSS%20track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Segmentation%2520on%2520VSPW%2520Dataset%2520through%2520Masked%2520Video%2520Consistency%26entry.906535625%3DChen%2520Liang%2520and%2520Qiang%2520Guo%2520and%2520Chongkai%2520Yu%2520and%2520Chengjing%2520Wu%2520and%2520Ting%2520Liu%2520and%2520Luoqi%2520Liu%26entry.1292438233%3D%2520%2520Pixel-level%2520Video%2520Understanding%2520requires%2520effectively%2520integrating%250Athree-dimensional%2520data%2520in%2520both%2520spatial%2520and%2520temporal%2520dimensions%2520to%2520learn%250Aaccurate%2520and%2520stable%2520semantic%2520information%2520from%2520continuous%2520frames.%2520However%252C%250Aexisting%2520advanced%2520models%2520on%2520the%2520VSPW%2520dataset%2520have%2520not%2520fully%2520modeled%250Aspatiotemporal%2520relationships.%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520solution%2520for%2520the%250APVUW%2520competition%252C%2520where%2520we%2520introduce%2520masked%2520video%2520consistency%2520%2528MVC%2529%2520based%2520on%250Aexisting%2520models.%2520MVC%2520enforces%2520the%2520consistency%2520between%2520predictions%2520of%2520masked%250Aframes%2520where%2520random%2520patches%2520are%2520withheld.%2520The%2520model%2520needs%2520to%2520learn%2520the%250Asegmentation%2520results%2520of%2520the%2520masked%2520parts%2520through%2520the%2520context%2520of%2520images%2520and%2520the%250Arelationship%2520between%2520preceding%2520and%2520succeeding%2520frames%2520of%2520the%2520video.%250AAdditionally%252C%2520we%2520employed%2520test-time%2520augmentation%252C%2520model%2520aggeregation%2520and%2520a%250Amultimodal%2520model-based%2520post-processing%2520method.%2520Our%2520approach%2520achieves%252067.27%2525%250AmIoU%2520performance%2520on%2520the%2520VSPW%2520dataset%252C%2520ranking%25202nd%2520place%2520in%2520the%2520PVUW2024%250Achallenge%2520VSS%2520track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Segmentation%20on%20VSPW%20Dataset%20through%20Masked%20Video%20Consistency&entry.906535625=Chen%20Liang%20and%20Qiang%20Guo%20and%20Chongkai%20Yu%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu&entry.1292438233=%20%20Pixel-level%20Video%20Understanding%20requires%20effectively%20integrating%0Athree-dimensional%20data%20in%20both%20spatial%20and%20temporal%20dimensions%20to%20learn%0Aaccurate%20and%20stable%20semantic%20information%20from%20continuous%20frames.%20However%2C%0Aexisting%20advanced%20models%20on%20the%20VSPW%20dataset%20have%20not%20fully%20modeled%0Aspatiotemporal%20relationships.%20In%20this%20paper%2C%20we%20present%20our%20solution%20for%20the%0APVUW%20competition%2C%20where%20we%20introduce%20masked%20video%20consistency%20%28MVC%29%20based%20on%0Aexisting%20models.%20MVC%20enforces%20the%20consistency%20between%20predictions%20of%20masked%0Aframes%20where%20random%20patches%20are%20withheld.%20The%20model%20needs%20to%20learn%20the%0Asegmentation%20results%20of%20the%20masked%20parts%20through%20the%20context%20of%20images%20and%20the%0Arelationship%20between%20preceding%20and%20succeeding%20frames%20of%20the%20video.%0AAdditionally%2C%20we%20employed%20test-time%20augmentation%2C%20model%20aggeregation%20and%20a%0Amultimodal%20model-based%20post-processing%20method.%20Our%20approach%20achieves%2067.27%25%0AmIoU%20performance%20on%20the%20VSPW%20dataset%2C%20ranking%202nd%20place%20in%20the%20PVUW2024%0Achallenge%20VSS%20track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04979v1&entry.124074799=Read"},
{"title": "Espresso: Robust Concept Filtering in Text-to-Image Models", "author": "Anudeep Das and Vasisht Duddu and Rui Zhang and N. Asokan", "abstract": "  Diffusion-based text-to-image (T2I) models generate high-fidelity images for\ngiven textual prompts. They are trained on large datasets scraped from the\nInternet, potentially containing unacceptable concepts (e.g., copyright\ninfringing or unsafe). Retraining T2I models after filtering out unacceptable\nconcepts in the training data is inefficient and degrades utility. Hence, there\nis a need for concept removal techniques (CRTs) which are effective in removing\nunacceptable concepts, utility-preserving on acceptable concepts, and robust\nagainst evasion with adversarial prompts. None of the prior filtering and\nfine-tuning CRTs satisfy all these requirements simultaneously.\n  We introduce Espresso, the first robust concept filter based on Contrastive\nLanguage-Image Pre-Training (CLIP). It identifies unacceptable concepts by\nprojecting the generated image's embedding onto the vector connecting\nunacceptable and acceptable concepts in the joint text-image embedding space.\nThis ensures robustness by restricting the adversary to adding noise only along\nthis vector, in the direction of the acceptable concept. Further fine-tuning\nEspresso to separate embeddings of acceptable and unacceptable concepts, while\npreserving their pairing with image embeddings, ensures both effectiveness and\nutility. We evaluate Espresso on eleven concepts to show that it is effective\n(~5% CLIP accuracy on unacceptable concepts), utility-preserving (~93%\nnormalized CLIP score on acceptable concepts), and robust (~4% CLIP accuracy on\nadversarial prompts for unacceptable concepts). Finally, we present theoretical\nbounds for the certified robustness of Espresso against adversarial prompts,\nand an empirical analysis.\n", "link": "http://arxiv.org/abs/2404.19227v4", "date": "2024-06-07", "relevancy": 2.07, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5172}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models&body=Title%3A%20Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models%0AAuthor%3A%20Anudeep%20Das%20and%20Vasisht%20Duddu%20and%20Rui%20Zhang%20and%20N.%20Asokan%0AAbstract%3A%20%20%20Diffusion-based%20text-to-image%20%28T2I%29%20models%20generate%20high-fidelity%20images%20for%0Agiven%20textual%20prompts.%20They%20are%20trained%20on%20large%20datasets%20scraped%20from%20the%0AInternet%2C%20potentially%20containing%20unacceptable%20concepts%20%28e.g.%2C%20copyright%0Ainfringing%20or%20unsafe%29.%20Retraining%20T2I%20models%20after%20filtering%20out%20unacceptable%0Aconcepts%20in%20the%20training%20data%20is%20inefficient%20and%20degrades%20utility.%20Hence%2C%20there%0Ais%20a%20need%20for%20concept%20removal%20techniques%20%28CRTs%29%20which%20are%20effective%20in%20removing%0Aunacceptable%20concepts%2C%20utility-preserving%20on%20acceptable%20concepts%2C%20and%20robust%0Aagainst%20evasion%20with%20adversarial%20prompts.%20None%20of%20the%20prior%20filtering%20and%0Afine-tuning%20CRTs%20satisfy%20all%20these%20requirements%20simultaneously.%0A%20%20We%20introduce%20Espresso%2C%20the%20first%20robust%20concept%20filter%20based%20on%20Contrastive%0ALanguage-Image%20Pre-Training%20%28CLIP%29.%20It%20identifies%20unacceptable%20concepts%20by%0Aprojecting%20the%20generated%20image%27s%20embedding%20onto%20the%20vector%20connecting%0Aunacceptable%20and%20acceptable%20concepts%20in%20the%20joint%20text-image%20embedding%20space.%0AThis%20ensures%20robustness%20by%20restricting%20the%20adversary%20to%20adding%20noise%20only%20along%0Athis%20vector%2C%20in%20the%20direction%20of%20the%20acceptable%20concept.%20Further%20fine-tuning%0AEspresso%20to%20separate%20embeddings%20of%20acceptable%20and%20unacceptable%20concepts%2C%20while%0Apreserving%20their%20pairing%20with%20image%20embeddings%2C%20ensures%20both%20effectiveness%20and%0Autility.%20We%20evaluate%20Espresso%20on%20eleven%20concepts%20to%20show%20that%20it%20is%20effective%0A%28~5%25%20CLIP%20accuracy%20on%20unacceptable%20concepts%29%2C%20utility-preserving%20%28~93%25%0Anormalized%20CLIP%20score%20on%20acceptable%20concepts%29%2C%20and%20robust%20%28~4%25%20CLIP%20accuracy%20on%0Aadversarial%20prompts%20for%20unacceptable%20concepts%29.%20Finally%2C%20we%20present%20theoretical%0Abounds%20for%20the%20certified%20robustness%20of%20Espresso%20against%20adversarial%20prompts%2C%0Aand%20an%20empirical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19227v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEspresso%253A%2520Robust%2520Concept%2520Filtering%2520in%2520Text-to-Image%2520Models%26entry.906535625%3DAnudeep%2520Das%2520and%2520Vasisht%2520Duddu%2520and%2520Rui%2520Zhang%2520and%2520N.%2520Asokan%26entry.1292438233%3D%2520%2520Diffusion-based%2520text-to-image%2520%2528T2I%2529%2520models%2520generate%2520high-fidelity%2520images%2520for%250Agiven%2520textual%2520prompts.%2520They%2520are%2520trained%2520on%2520large%2520datasets%2520scraped%2520from%2520the%250AInternet%252C%2520potentially%2520containing%2520unacceptable%2520concepts%2520%2528e.g.%252C%2520copyright%250Ainfringing%2520or%2520unsafe%2529.%2520Retraining%2520T2I%2520models%2520after%2520filtering%2520out%2520unacceptable%250Aconcepts%2520in%2520the%2520training%2520data%2520is%2520inefficient%2520and%2520degrades%2520utility.%2520Hence%252C%2520there%250Ais%2520a%2520need%2520for%2520concept%2520removal%2520techniques%2520%2528CRTs%2529%2520which%2520are%2520effective%2520in%2520removing%250Aunacceptable%2520concepts%252C%2520utility-preserving%2520on%2520acceptable%2520concepts%252C%2520and%2520robust%250Aagainst%2520evasion%2520with%2520adversarial%2520prompts.%2520None%2520of%2520the%2520prior%2520filtering%2520and%250Afine-tuning%2520CRTs%2520satisfy%2520all%2520these%2520requirements%2520simultaneously.%250A%2520%2520We%2520introduce%2520Espresso%252C%2520the%2520first%2520robust%2520concept%2520filter%2520based%2520on%2520Contrastive%250ALanguage-Image%2520Pre-Training%2520%2528CLIP%2529.%2520It%2520identifies%2520unacceptable%2520concepts%2520by%250Aprojecting%2520the%2520generated%2520image%2527s%2520embedding%2520onto%2520the%2520vector%2520connecting%250Aunacceptable%2520and%2520acceptable%2520concepts%2520in%2520the%2520joint%2520text-image%2520embedding%2520space.%250AThis%2520ensures%2520robustness%2520by%2520restricting%2520the%2520adversary%2520to%2520adding%2520noise%2520only%2520along%250Athis%2520vector%252C%2520in%2520the%2520direction%2520of%2520the%2520acceptable%2520concept.%2520Further%2520fine-tuning%250AEspresso%2520to%2520separate%2520embeddings%2520of%2520acceptable%2520and%2520unacceptable%2520concepts%252C%2520while%250Apreserving%2520their%2520pairing%2520with%2520image%2520embeddings%252C%2520ensures%2520both%2520effectiveness%2520and%250Autility.%2520We%2520evaluate%2520Espresso%2520on%2520eleven%2520concepts%2520to%2520show%2520that%2520it%2520is%2520effective%250A%2528~5%2525%2520CLIP%2520accuracy%2520on%2520unacceptable%2520concepts%2529%252C%2520utility-preserving%2520%2528~93%2525%250Anormalized%2520CLIP%2520score%2520on%2520acceptable%2520concepts%2529%252C%2520and%2520robust%2520%2528~4%2525%2520CLIP%2520accuracy%2520on%250Aadversarial%2520prompts%2520for%2520unacceptable%2520concepts%2529.%2520Finally%252C%2520we%2520present%2520theoretical%250Abounds%2520for%2520the%2520certified%2520robustness%2520of%2520Espresso%2520against%2520adversarial%2520prompts%252C%250Aand%2520an%2520empirical%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19227v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Espresso%3A%20Robust%20Concept%20Filtering%20in%20Text-to-Image%20Models&entry.906535625=Anudeep%20Das%20and%20Vasisht%20Duddu%20and%20Rui%20Zhang%20and%20N.%20Asokan&entry.1292438233=%20%20Diffusion-based%20text-to-image%20%28T2I%29%20models%20generate%20high-fidelity%20images%20for%0Agiven%20textual%20prompts.%20They%20are%20trained%20on%20large%20datasets%20scraped%20from%20the%0AInternet%2C%20potentially%20containing%20unacceptable%20concepts%20%28e.g.%2C%20copyright%0Ainfringing%20or%20unsafe%29.%20Retraining%20T2I%20models%20after%20filtering%20out%20unacceptable%0Aconcepts%20in%20the%20training%20data%20is%20inefficient%20and%20degrades%20utility.%20Hence%2C%20there%0Ais%20a%20need%20for%20concept%20removal%20techniques%20%28CRTs%29%20which%20are%20effective%20in%20removing%0Aunacceptable%20concepts%2C%20utility-preserving%20on%20acceptable%20concepts%2C%20and%20robust%0Aagainst%20evasion%20with%20adversarial%20prompts.%20None%20of%20the%20prior%20filtering%20and%0Afine-tuning%20CRTs%20satisfy%20all%20these%20requirements%20simultaneously.%0A%20%20We%20introduce%20Espresso%2C%20the%20first%20robust%20concept%20filter%20based%20on%20Contrastive%0ALanguage-Image%20Pre-Training%20%28CLIP%29.%20It%20identifies%20unacceptable%20concepts%20by%0Aprojecting%20the%20generated%20image%27s%20embedding%20onto%20the%20vector%20connecting%0Aunacceptable%20and%20acceptable%20concepts%20in%20the%20joint%20text-image%20embedding%20space.%0AThis%20ensures%20robustness%20by%20restricting%20the%20adversary%20to%20adding%20noise%20only%20along%0Athis%20vector%2C%20in%20the%20direction%20of%20the%20acceptable%20concept.%20Further%20fine-tuning%0AEspresso%20to%20separate%20embeddings%20of%20acceptable%20and%20unacceptable%20concepts%2C%20while%0Apreserving%20their%20pairing%20with%20image%20embeddings%2C%20ensures%20both%20effectiveness%20and%0Autility.%20We%20evaluate%20Espresso%20on%20eleven%20concepts%20to%20show%20that%20it%20is%20effective%0A%28~5%25%20CLIP%20accuracy%20on%20unacceptable%20concepts%29%2C%20utility-preserving%20%28~93%25%0Anormalized%20CLIP%20score%20on%20acceptable%20concepts%29%2C%20and%20robust%20%28~4%25%20CLIP%20accuracy%20on%0Aadversarial%20prompts%20for%20unacceptable%20concepts%29.%20Finally%2C%20we%20present%20theoretical%0Abounds%20for%20the%20certified%20robustness%20of%20Espresso%20against%20adversarial%20prompts%2C%0Aand%20an%20empirical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19227v4&entry.124074799=Read"},
{"title": "BERTs are Generative In-Context Learners", "author": "David Samuel", "abstract": "  This paper explores the in-context learning capabilities of masked language\nmodels, challenging the common view that this ability does not 'emerge' in\nthem. We present an embarrassingly simple inference technique that enables\nDeBERTa to operate as a generative model without any additional training. Our\nfindings demonstrate that DeBERTa can match and even surpass GPT-3, its\ncontemporary that famously introduced the paradigm of in-context learning. The\ncomparative analysis reveals that the masked and causal language models behave\nvery differently, as they clearly outperform each other on different categories\nof tasks. This suggests that there is great potential for a hybrid training\napproach that takes advantage of the strengths of both training objectives.\n", "link": "http://arxiv.org/abs/2406.04823v1", "date": "2024-06-07", "relevancy": 2.0697, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5942}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BERTs%20are%20Generative%20In-Context%20Learners&body=Title%3A%20BERTs%20are%20Generative%20In-Context%20Learners%0AAuthor%3A%20David%20Samuel%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20in-context%20learning%20capabilities%20of%20masked%20language%0Amodels%2C%20challenging%20the%20common%20view%20that%20this%20ability%20does%20not%20%27emerge%27%20in%0Athem.%20We%20present%20an%20embarrassingly%20simple%20inference%20technique%20that%20enables%0ADeBERTa%20to%20operate%20as%20a%20generative%20model%20without%20any%20additional%20training.%20Our%0Afindings%20demonstrate%20that%20DeBERTa%20can%20match%20and%20even%20surpass%20GPT-3%2C%20its%0Acontemporary%20that%20famously%20introduced%20the%20paradigm%20of%20in-context%20learning.%20The%0Acomparative%20analysis%20reveals%20that%20the%20masked%20and%20causal%20language%20models%20behave%0Avery%20differently%2C%20as%20they%20clearly%20outperform%20each%20other%20on%20different%20categories%0Aof%20tasks.%20This%20suggests%20that%20there%20is%20great%20potential%20for%20a%20hybrid%20training%0Aapproach%20that%20takes%20advantage%20of%20the%20strengths%20of%20both%20training%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBERTs%2520are%2520Generative%2520In-Context%2520Learners%26entry.906535625%3DDavid%2520Samuel%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520in-context%2520learning%2520capabilities%2520of%2520masked%2520language%250Amodels%252C%2520challenging%2520the%2520common%2520view%2520that%2520this%2520ability%2520does%2520not%2520%2527emerge%2527%2520in%250Athem.%2520We%2520present%2520an%2520embarrassingly%2520simple%2520inference%2520technique%2520that%2520enables%250ADeBERTa%2520to%2520operate%2520as%2520a%2520generative%2520model%2520without%2520any%2520additional%2520training.%2520Our%250Afindings%2520demonstrate%2520that%2520DeBERTa%2520can%2520match%2520and%2520even%2520surpass%2520GPT-3%252C%2520its%250Acontemporary%2520that%2520famously%2520introduced%2520the%2520paradigm%2520of%2520in-context%2520learning.%2520The%250Acomparative%2520analysis%2520reveals%2520that%2520the%2520masked%2520and%2520causal%2520language%2520models%2520behave%250Avery%2520differently%252C%2520as%2520they%2520clearly%2520outperform%2520each%2520other%2520on%2520different%2520categories%250Aof%2520tasks.%2520This%2520suggests%2520that%2520there%2520is%2520great%2520potential%2520for%2520a%2520hybrid%2520training%250Aapproach%2520that%2520takes%2520advantage%2520of%2520the%2520strengths%2520of%2520both%2520training%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BERTs%20are%20Generative%20In-Context%20Learners&entry.906535625=David%20Samuel&entry.1292438233=%20%20This%20paper%20explores%20the%20in-context%20learning%20capabilities%20of%20masked%20language%0Amodels%2C%20challenging%20the%20common%20view%20that%20this%20ability%20does%20not%20%27emerge%27%20in%0Athem.%20We%20present%20an%20embarrassingly%20simple%20inference%20technique%20that%20enables%0ADeBERTa%20to%20operate%20as%20a%20generative%20model%20without%20any%20additional%20training.%20Our%0Afindings%20demonstrate%20that%20DeBERTa%20can%20match%20and%20even%20surpass%20GPT-3%2C%20its%0Acontemporary%20that%20famously%20introduced%20the%20paradigm%20of%20in-context%20learning.%20The%0Acomparative%20analysis%20reveals%20that%20the%20masked%20and%20causal%20language%20models%20behave%0Avery%20differently%2C%20as%20they%20clearly%20outperform%20each%20other%20on%20different%20categories%0Aof%20tasks.%20This%20suggests%20that%20there%20is%20great%20potential%20for%20a%20hybrid%20training%0Aapproach%20that%20takes%20advantage%20of%20the%20strengths%20of%20both%20training%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04823v1&entry.124074799=Read"},
{"title": "EGOR: Efficient Generated Objects Replay for incremental object\n  detection", "author": "Zijia An and Boyu Diao and Libo Huang and Ruiqi Liu and Zhulin An and Yongjun Xu", "abstract": "  Incremental object detection aims to simultaneously maintain old-class\naccuracy and detect emerging new-class objects in incremental data. Most\nexisting distillation-based methods underperform when unlabeled old-class\nobjects are absent in the incremental dataset. While the absence can be\nmitigated by generating old-class samples, it also incurs high computational\ncosts. In this paper, we argue that the extra computational cost stems from the\ninconsistency between the detector and the generative model, along with\nredundant generation. To overcome this problem, we propose Efficient Generated\nObject Replay (EGOR). Specifically, we generate old-class samples by inversing\nthe original detectors, thus eliminating the necessity of training and storing\nadditional generative models. We also propose augmented replay to reuse the\nobjects in generated samples, thereby reducing the redundant generation. In\naddition, we propose high-response knowledge distillation focusing on the\nknowledge related to the old class, which transfers the knowledge in generated\nobjects to the incremental detector. With the addition of the generated objects\nand losses, we observe a bias towards old classes in the detector. We balance\nthe losses for old and new classes to alleviate the bias, thereby increasing\nthe overall detection accuracy. Extensive experiments conducted on MS COCO 2017\ndemonstrate that our method can efficiently improve detection performance in\nthe absence of old-class objects.\n", "link": "http://arxiv.org/abs/2406.04829v1", "date": "2024-06-07", "relevancy": 2.0671, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5291}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5109}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EGOR%3A%20Efficient%20Generated%20Objects%20Replay%20for%20incremental%20object%0A%20%20detection&body=Title%3A%20EGOR%3A%20Efficient%20Generated%20Objects%20Replay%20for%20incremental%20object%0A%20%20detection%0AAuthor%3A%20Zijia%20An%20and%20Boyu%20Diao%20and%20Libo%20Huang%20and%20Ruiqi%20Liu%20and%20Zhulin%20An%20and%20Yongjun%20Xu%0AAbstract%3A%20%20%20Incremental%20object%20detection%20aims%20to%20simultaneously%20maintain%20old-class%0Aaccuracy%20and%20detect%20emerging%20new-class%20objects%20in%20incremental%20data.%20Most%0Aexisting%20distillation-based%20methods%20underperform%20when%20unlabeled%20old-class%0Aobjects%20are%20absent%20in%20the%20incremental%20dataset.%20While%20the%20absence%20can%20be%0Amitigated%20by%20generating%20old-class%20samples%2C%20it%20also%20incurs%20high%20computational%0Acosts.%20In%20this%20paper%2C%20we%20argue%20that%20the%20extra%20computational%20cost%20stems%20from%20the%0Ainconsistency%20between%20the%20detector%20and%20the%20generative%20model%2C%20along%20with%0Aredundant%20generation.%20To%20overcome%20this%20problem%2C%20we%20propose%20Efficient%20Generated%0AObject%20Replay%20%28EGOR%29.%20Specifically%2C%20we%20generate%20old-class%20samples%20by%20inversing%0Athe%20original%20detectors%2C%20thus%20eliminating%20the%20necessity%20of%20training%20and%20storing%0Aadditional%20generative%20models.%20We%20also%20propose%20augmented%20replay%20to%20reuse%20the%0Aobjects%20in%20generated%20samples%2C%20thereby%20reducing%20the%20redundant%20generation.%20In%0Aaddition%2C%20we%20propose%20high-response%20knowledge%20distillation%20focusing%20on%20the%0Aknowledge%20related%20to%20the%20old%20class%2C%20which%20transfers%20the%20knowledge%20in%20generated%0Aobjects%20to%20the%20incremental%20detector.%20With%20the%20addition%20of%20the%20generated%20objects%0Aand%20losses%2C%20we%20observe%20a%20bias%20towards%20old%20classes%20in%20the%20detector.%20We%20balance%0Athe%20losses%20for%20old%20and%20new%20classes%20to%20alleviate%20the%20bias%2C%20thereby%20increasing%0Athe%20overall%20detection%20accuracy.%20Extensive%20experiments%20conducted%20on%20MS%20COCO%202017%0Ademonstrate%20that%20our%20method%20can%20efficiently%20improve%20detection%20performance%20in%0Athe%20absence%20of%20old-class%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEGOR%253A%2520Efficient%2520Generated%2520Objects%2520Replay%2520for%2520incremental%2520object%250A%2520%2520detection%26entry.906535625%3DZijia%2520An%2520and%2520Boyu%2520Diao%2520and%2520Libo%2520Huang%2520and%2520Ruiqi%2520Liu%2520and%2520Zhulin%2520An%2520and%2520Yongjun%2520Xu%26entry.1292438233%3D%2520%2520Incremental%2520object%2520detection%2520aims%2520to%2520simultaneously%2520maintain%2520old-class%250Aaccuracy%2520and%2520detect%2520emerging%2520new-class%2520objects%2520in%2520incremental%2520data.%2520Most%250Aexisting%2520distillation-based%2520methods%2520underperform%2520when%2520unlabeled%2520old-class%250Aobjects%2520are%2520absent%2520in%2520the%2520incremental%2520dataset.%2520While%2520the%2520absence%2520can%2520be%250Amitigated%2520by%2520generating%2520old-class%2520samples%252C%2520it%2520also%2520incurs%2520high%2520computational%250Acosts.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520extra%2520computational%2520cost%2520stems%2520from%2520the%250Ainconsistency%2520between%2520the%2520detector%2520and%2520the%2520generative%2520model%252C%2520along%2520with%250Aredundant%2520generation.%2520To%2520overcome%2520this%2520problem%252C%2520we%2520propose%2520Efficient%2520Generated%250AObject%2520Replay%2520%2528EGOR%2529.%2520Specifically%252C%2520we%2520generate%2520old-class%2520samples%2520by%2520inversing%250Athe%2520original%2520detectors%252C%2520thus%2520eliminating%2520the%2520necessity%2520of%2520training%2520and%2520storing%250Aadditional%2520generative%2520models.%2520We%2520also%2520propose%2520augmented%2520replay%2520to%2520reuse%2520the%250Aobjects%2520in%2520generated%2520samples%252C%2520thereby%2520reducing%2520the%2520redundant%2520generation.%2520In%250Aaddition%252C%2520we%2520propose%2520high-response%2520knowledge%2520distillation%2520focusing%2520on%2520the%250Aknowledge%2520related%2520to%2520the%2520old%2520class%252C%2520which%2520transfers%2520the%2520knowledge%2520in%2520generated%250Aobjects%2520to%2520the%2520incremental%2520detector.%2520With%2520the%2520addition%2520of%2520the%2520generated%2520objects%250Aand%2520losses%252C%2520we%2520observe%2520a%2520bias%2520towards%2520old%2520classes%2520in%2520the%2520detector.%2520We%2520balance%250Athe%2520losses%2520for%2520old%2520and%2520new%2520classes%2520to%2520alleviate%2520the%2520bias%252C%2520thereby%2520increasing%250Athe%2520overall%2520detection%2520accuracy.%2520Extensive%2520experiments%2520conducted%2520on%2520MS%2520COCO%25202017%250Ademonstrate%2520that%2520our%2520method%2520can%2520efficiently%2520improve%2520detection%2520performance%2520in%250Athe%2520absence%2520of%2520old-class%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGOR%3A%20Efficient%20Generated%20Objects%20Replay%20for%20incremental%20object%0A%20%20detection&entry.906535625=Zijia%20An%20and%20Boyu%20Diao%20and%20Libo%20Huang%20and%20Ruiqi%20Liu%20and%20Zhulin%20An%20and%20Yongjun%20Xu&entry.1292438233=%20%20Incremental%20object%20detection%20aims%20to%20simultaneously%20maintain%20old-class%0Aaccuracy%20and%20detect%20emerging%20new-class%20objects%20in%20incremental%20data.%20Most%0Aexisting%20distillation-based%20methods%20underperform%20when%20unlabeled%20old-class%0Aobjects%20are%20absent%20in%20the%20incremental%20dataset.%20While%20the%20absence%20can%20be%0Amitigated%20by%20generating%20old-class%20samples%2C%20it%20also%20incurs%20high%20computational%0Acosts.%20In%20this%20paper%2C%20we%20argue%20that%20the%20extra%20computational%20cost%20stems%20from%20the%0Ainconsistency%20between%20the%20detector%20and%20the%20generative%20model%2C%20along%20with%0Aredundant%20generation.%20To%20overcome%20this%20problem%2C%20we%20propose%20Efficient%20Generated%0AObject%20Replay%20%28EGOR%29.%20Specifically%2C%20we%20generate%20old-class%20samples%20by%20inversing%0Athe%20original%20detectors%2C%20thus%20eliminating%20the%20necessity%20of%20training%20and%20storing%0Aadditional%20generative%20models.%20We%20also%20propose%20augmented%20replay%20to%20reuse%20the%0Aobjects%20in%20generated%20samples%2C%20thereby%20reducing%20the%20redundant%20generation.%20In%0Aaddition%2C%20we%20propose%20high-response%20knowledge%20distillation%20focusing%20on%20the%0Aknowledge%20related%20to%20the%20old%20class%2C%20which%20transfers%20the%20knowledge%20in%20generated%0Aobjects%20to%20the%20incremental%20detector.%20With%20the%20addition%20of%20the%20generated%20objects%0Aand%20losses%2C%20we%20observe%20a%20bias%20towards%20old%20classes%20in%20the%20detector.%20We%20balance%0Athe%20losses%20for%20old%20and%20new%20classes%20to%20alleviate%20the%20bias%2C%20thereby%20increasing%0Athe%20overall%20detection%20accuracy.%20Extensive%20experiments%20conducted%20on%20MS%20COCO%202017%0Ademonstrate%20that%20our%20method%20can%20efficiently%20improve%20detection%20performance%20in%0Athe%20absence%20of%20old-class%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04829v1&entry.124074799=Read"},
{"title": "NeuralThink: Learning Algorithms For Consistent and Efficient\n  Extrapolation Across General Tasks", "author": "Bernardo Esteves and Miguel Vasco and Francisco S. Melo", "abstract": "  We propose NeuralThink, a novel deep thinking architecture that can\nefficiently and consistently extrapolate, i.e., learn algorithms from smaller\nproblems (in terms of observation size) and execute those algorithms in large\nproblems. Contrary to previous deep thinking architectures, NeuralThink can be\nnaturally applied in both same-size problems, where the input and output sizes\nare the same, and in different-size problems, where the size of the input and\noutput differ. To allow for this versatility, we design NeuralThink with three\nmain components: a recurrent module, that iteratively processes input\ninformation at different scales, a processing module, responsible for\naggregating the previously processed information, and a curriculum-based\ntraining scheme, that improves the extrapolation performance of the method. To\nevaluate our method we introduce a set of novel different-size tasks and we\nshow that NeuralThink consistently outperforms the prior state-of-the-art deep\nthinking approaches in extrapolating to larger problems, considering smaller\ntraining problems and requiring less parameters than other approaches.\n", "link": "http://arxiv.org/abs/2402.15393v2", "date": "2024-06-07", "relevancy": 2.0625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5105}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralThink%3A%20Learning%20Algorithms%20For%20Consistent%20and%20Efficient%0A%20%20Extrapolation%20Across%20General%20Tasks&body=Title%3A%20NeuralThink%3A%20Learning%20Algorithms%20For%20Consistent%20and%20Efficient%0A%20%20Extrapolation%20Across%20General%20Tasks%0AAuthor%3A%20Bernardo%20Esteves%20and%20Miguel%20Vasco%20and%20Francisco%20S.%20Melo%0AAbstract%3A%20%20%20We%20propose%20NeuralThink%2C%20a%20novel%20deep%20thinking%20architecture%20that%20can%0Aefficiently%20and%20consistently%20extrapolate%2C%20i.e.%2C%20learn%20algorithms%20from%20smaller%0Aproblems%20%28in%20terms%20of%20observation%20size%29%20and%20execute%20those%20algorithms%20in%20large%0Aproblems.%20Contrary%20to%20previous%20deep%20thinking%20architectures%2C%20NeuralThink%20can%20be%0Anaturally%20applied%20in%20both%20same-size%20problems%2C%20where%20the%20input%20and%20output%20sizes%0Aare%20the%20same%2C%20and%20in%20different-size%20problems%2C%20where%20the%20size%20of%20the%20input%20and%0Aoutput%20differ.%20To%20allow%20for%20this%20versatility%2C%20we%20design%20NeuralThink%20with%20three%0Amain%20components%3A%20a%20recurrent%20module%2C%20that%20iteratively%20processes%20input%0Ainformation%20at%20different%20scales%2C%20a%20processing%20module%2C%20responsible%20for%0Aaggregating%20the%20previously%20processed%20information%2C%20and%20a%20curriculum-based%0Atraining%20scheme%2C%20that%20improves%20the%20extrapolation%20performance%20of%20the%20method.%20To%0Aevaluate%20our%20method%20we%20introduce%20a%20set%20of%20novel%20different-size%20tasks%20and%20we%0Ashow%20that%20NeuralThink%20consistently%20outperforms%20the%20prior%20state-of-the-art%20deep%0Athinking%20approaches%20in%20extrapolating%20to%20larger%20problems%2C%20considering%20smaller%0Atraining%20problems%20and%20requiring%20less%20parameters%20than%20other%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralThink%253A%2520Learning%2520Algorithms%2520For%2520Consistent%2520and%2520Efficient%250A%2520%2520Extrapolation%2520Across%2520General%2520Tasks%26entry.906535625%3DBernardo%2520Esteves%2520and%2520Miguel%2520Vasco%2520and%2520Francisco%2520S.%2520Melo%26entry.1292438233%3D%2520%2520We%2520propose%2520NeuralThink%252C%2520a%2520novel%2520deep%2520thinking%2520architecture%2520that%2520can%250Aefficiently%2520and%2520consistently%2520extrapolate%252C%2520i.e.%252C%2520learn%2520algorithms%2520from%2520smaller%250Aproblems%2520%2528in%2520terms%2520of%2520observation%2520size%2529%2520and%2520execute%2520those%2520algorithms%2520in%2520large%250Aproblems.%2520Contrary%2520to%2520previous%2520deep%2520thinking%2520architectures%252C%2520NeuralThink%2520can%2520be%250Anaturally%2520applied%2520in%2520both%2520same-size%2520problems%252C%2520where%2520the%2520input%2520and%2520output%2520sizes%250Aare%2520the%2520same%252C%2520and%2520in%2520different-size%2520problems%252C%2520where%2520the%2520size%2520of%2520the%2520input%2520and%250Aoutput%2520differ.%2520To%2520allow%2520for%2520this%2520versatility%252C%2520we%2520design%2520NeuralThink%2520with%2520three%250Amain%2520components%253A%2520a%2520recurrent%2520module%252C%2520that%2520iteratively%2520processes%2520input%250Ainformation%2520at%2520different%2520scales%252C%2520a%2520processing%2520module%252C%2520responsible%2520for%250Aaggregating%2520the%2520previously%2520processed%2520information%252C%2520and%2520a%2520curriculum-based%250Atraining%2520scheme%252C%2520that%2520improves%2520the%2520extrapolation%2520performance%2520of%2520the%2520method.%2520To%250Aevaluate%2520our%2520method%2520we%2520introduce%2520a%2520set%2520of%2520novel%2520different-size%2520tasks%2520and%2520we%250Ashow%2520that%2520NeuralThink%2520consistently%2520outperforms%2520the%2520prior%2520state-of-the-art%2520deep%250Athinking%2520approaches%2520in%2520extrapolating%2520to%2520larger%2520problems%252C%2520considering%2520smaller%250Atraining%2520problems%2520and%2520requiring%2520less%2520parameters%2520than%2520other%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralThink%3A%20Learning%20Algorithms%20For%20Consistent%20and%20Efficient%0A%20%20Extrapolation%20Across%20General%20Tasks&entry.906535625=Bernardo%20Esteves%20and%20Miguel%20Vasco%20and%20Francisco%20S.%20Melo&entry.1292438233=%20%20We%20propose%20NeuralThink%2C%20a%20novel%20deep%20thinking%20architecture%20that%20can%0Aefficiently%20and%20consistently%20extrapolate%2C%20i.e.%2C%20learn%20algorithms%20from%20smaller%0Aproblems%20%28in%20terms%20of%20observation%20size%29%20and%20execute%20those%20algorithms%20in%20large%0Aproblems.%20Contrary%20to%20previous%20deep%20thinking%20architectures%2C%20NeuralThink%20can%20be%0Anaturally%20applied%20in%20both%20same-size%20problems%2C%20where%20the%20input%20and%20output%20sizes%0Aare%20the%20same%2C%20and%20in%20different-size%20problems%2C%20where%20the%20size%20of%20the%20input%20and%0Aoutput%20differ.%20To%20allow%20for%20this%20versatility%2C%20we%20design%20NeuralThink%20with%20three%0Amain%20components%3A%20a%20recurrent%20module%2C%20that%20iteratively%20processes%20input%0Ainformation%20at%20different%20scales%2C%20a%20processing%20module%2C%20responsible%20for%0Aaggregating%20the%20previously%20processed%20information%2C%20and%20a%20curriculum-based%0Atraining%20scheme%2C%20that%20improves%20the%20extrapolation%20performance%20of%20the%20method.%20To%0Aevaluate%20our%20method%20we%20introduce%20a%20set%20of%20novel%20different-size%20tasks%20and%20we%0Ashow%20that%20NeuralThink%20consistently%20outperforms%20the%20prior%20state-of-the-art%20deep%0Athinking%20approaches%20in%20extrapolating%20to%20larger%20problems%2C%20considering%20smaller%0Atraining%20problems%20and%20requiring%20less%20parameters%20than%20other%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15393v2&entry.124074799=Read"},
{"title": "Pretraining Decision Transformers with Reward Prediction for In-Context\n  Multi-task Structured Bandit Learning", "author": "Subhojyoti Mukherjee and Josiah P. Hanna and Qiaomin Xie and Robert Nowak", "abstract": "  In this paper, we study multi-task structured bandit problem where the goal\nis to learn a near-optimal algorithm that minimizes cumulative regret. The\ntasks share a common structure and the algorithm exploits the shared structure\nto minimize the cumulative regret for an unseen but related test task. We use a\ntransformer as a decision-making algorithm to learn this shared structure so as\nto generalize to the test task. The prior work of pretrained decision\ntransformers like DPT requires access to the optimal action during training\nwhich may be hard in several scenarios. Diverging from these works, our\nlearning algorithm does not need the knowledge of optimal action per task\nduring training but predicts a reward vector for each of the actions using only\nthe observed offline data from the diverse training tasks. Finally, during\ninference time, it selects action using the reward predictions employing\nvarious exploration strategies in-context for an unseen test task. Our model\noutperforms other SOTA methods like DPT, and Algorithmic Distillation over a\nseries of experiments on several structured bandit problems (linear, bilinear,\nlatent, non-linear). Interestingly, we show that our algorithm, without the\nknowledge of the underlying problem structure, can learn a near-optimal policy\nin-context by leveraging the shared structure across diverse tasks. We further\nextend the field of pre-trained decision transformers by showing that they can\nleverage unseen tasks with new actions and still learn the underlying latent\nstructure to derive a near-optimal policy. We validate this over several\nexperiments to show that our proposed solution is very general and has wide\napplications to potentially emergent online and offline strategies at test\ntime. Finally, we theoretically analyze the performance of our algorithm and\nobtain generalization bounds in the in-context multi-task learning setting.\n", "link": "http://arxiv.org/abs/2406.05064v1", "date": "2024-06-07", "relevancy": 2.0546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining%20Decision%20Transformers%20with%20Reward%20Prediction%20for%20In-Context%0A%20%20Multi-task%20Structured%20Bandit%20Learning&body=Title%3A%20Pretraining%20Decision%20Transformers%20with%20Reward%20Prediction%20for%20In-Context%0A%20%20Multi-task%20Structured%20Bandit%20Learning%0AAuthor%3A%20Subhojyoti%20Mukherjee%20and%20Josiah%20P.%20Hanna%20and%20Qiaomin%20Xie%20and%20Robert%20Nowak%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20multi-task%20structured%20bandit%20problem%20where%20the%20goal%0Ais%20to%20learn%20a%20near-optimal%20algorithm%20that%20minimizes%20cumulative%20regret.%20The%0Atasks%20share%20a%20common%20structure%20and%20the%20algorithm%20exploits%20the%20shared%20structure%0Ato%20minimize%20the%20cumulative%20regret%20for%20an%20unseen%20but%20related%20test%20task.%20We%20use%20a%0Atransformer%20as%20a%20decision-making%20algorithm%20to%20learn%20this%20shared%20structure%20so%20as%0Ato%20generalize%20to%20the%20test%20task.%20The%20prior%20work%20of%20pretrained%20decision%0Atransformers%20like%20DPT%20requires%20access%20to%20the%20optimal%20action%20during%20training%0Awhich%20may%20be%20hard%20in%20several%20scenarios.%20Diverging%20from%20these%20works%2C%20our%0Alearning%20algorithm%20does%20not%20need%20the%20knowledge%20of%20optimal%20action%20per%20task%0Aduring%20training%20but%20predicts%20a%20reward%20vector%20for%20each%20of%20the%20actions%20using%20only%0Athe%20observed%20offline%20data%20from%20the%20diverse%20training%20tasks.%20Finally%2C%20during%0Ainference%20time%2C%20it%20selects%20action%20using%20the%20reward%20predictions%20employing%0Avarious%20exploration%20strategies%20in-context%20for%20an%20unseen%20test%20task.%20Our%20model%0Aoutperforms%20other%20SOTA%20methods%20like%20DPT%2C%20and%20Algorithmic%20Distillation%20over%20a%0Aseries%20of%20experiments%20on%20several%20structured%20bandit%20problems%20%28linear%2C%20bilinear%2C%0Alatent%2C%20non-linear%29.%20Interestingly%2C%20we%20show%20that%20our%20algorithm%2C%20without%20the%0Aknowledge%20of%20the%20underlying%20problem%20structure%2C%20can%20learn%20a%20near-optimal%20policy%0Ain-context%20by%20leveraging%20the%20shared%20structure%20across%20diverse%20tasks.%20We%20further%0Aextend%20the%20field%20of%20pre-trained%20decision%20transformers%20by%20showing%20that%20they%20can%0Aleverage%20unseen%20tasks%20with%20new%20actions%20and%20still%20learn%20the%20underlying%20latent%0Astructure%20to%20derive%20a%20near-optimal%20policy.%20We%20validate%20this%20over%20several%0Aexperiments%20to%20show%20that%20our%20proposed%20solution%20is%20very%20general%20and%20has%20wide%0Aapplications%20to%20potentially%20emergent%20online%20and%20offline%20strategies%20at%20test%0Atime.%20Finally%2C%20we%20theoretically%20analyze%20the%20performance%20of%20our%20algorithm%20and%0Aobtain%20generalization%20bounds%20in%20the%20in-context%20multi-task%20learning%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining%2520Decision%2520Transformers%2520with%2520Reward%2520Prediction%2520for%2520In-Context%250A%2520%2520Multi-task%2520Structured%2520Bandit%2520Learning%26entry.906535625%3DSubhojyoti%2520Mukherjee%2520and%2520Josiah%2520P.%2520Hanna%2520and%2520Qiaomin%2520Xie%2520and%2520Robert%2520Nowak%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520multi-task%2520structured%2520bandit%2520problem%2520where%2520the%2520goal%250Ais%2520to%2520learn%2520a%2520near-optimal%2520algorithm%2520that%2520minimizes%2520cumulative%2520regret.%2520The%250Atasks%2520share%2520a%2520common%2520structure%2520and%2520the%2520algorithm%2520exploits%2520the%2520shared%2520structure%250Ato%2520minimize%2520the%2520cumulative%2520regret%2520for%2520an%2520unseen%2520but%2520related%2520test%2520task.%2520We%2520use%2520a%250Atransformer%2520as%2520a%2520decision-making%2520algorithm%2520to%2520learn%2520this%2520shared%2520structure%2520so%2520as%250Ato%2520generalize%2520to%2520the%2520test%2520task.%2520The%2520prior%2520work%2520of%2520pretrained%2520decision%250Atransformers%2520like%2520DPT%2520requires%2520access%2520to%2520the%2520optimal%2520action%2520during%2520training%250Awhich%2520may%2520be%2520hard%2520in%2520several%2520scenarios.%2520Diverging%2520from%2520these%2520works%252C%2520our%250Alearning%2520algorithm%2520does%2520not%2520need%2520the%2520knowledge%2520of%2520optimal%2520action%2520per%2520task%250Aduring%2520training%2520but%2520predicts%2520a%2520reward%2520vector%2520for%2520each%2520of%2520the%2520actions%2520using%2520only%250Athe%2520observed%2520offline%2520data%2520from%2520the%2520diverse%2520training%2520tasks.%2520Finally%252C%2520during%250Ainference%2520time%252C%2520it%2520selects%2520action%2520using%2520the%2520reward%2520predictions%2520employing%250Avarious%2520exploration%2520strategies%2520in-context%2520for%2520an%2520unseen%2520test%2520task.%2520Our%2520model%250Aoutperforms%2520other%2520SOTA%2520methods%2520like%2520DPT%252C%2520and%2520Algorithmic%2520Distillation%2520over%2520a%250Aseries%2520of%2520experiments%2520on%2520several%2520structured%2520bandit%2520problems%2520%2528linear%252C%2520bilinear%252C%250Alatent%252C%2520non-linear%2529.%2520Interestingly%252C%2520we%2520show%2520that%2520our%2520algorithm%252C%2520without%2520the%250Aknowledge%2520of%2520the%2520underlying%2520problem%2520structure%252C%2520can%2520learn%2520a%2520near-optimal%2520policy%250Ain-context%2520by%2520leveraging%2520the%2520shared%2520structure%2520across%2520diverse%2520tasks.%2520We%2520further%250Aextend%2520the%2520field%2520of%2520pre-trained%2520decision%2520transformers%2520by%2520showing%2520that%2520they%2520can%250Aleverage%2520unseen%2520tasks%2520with%2520new%2520actions%2520and%2520still%2520learn%2520the%2520underlying%2520latent%250Astructure%2520to%2520derive%2520a%2520near-optimal%2520policy.%2520We%2520validate%2520this%2520over%2520several%250Aexperiments%2520to%2520show%2520that%2520our%2520proposed%2520solution%2520is%2520very%2520general%2520and%2520has%2520wide%250Aapplications%2520to%2520potentially%2520emergent%2520online%2520and%2520offline%2520strategies%2520at%2520test%250Atime.%2520Finally%252C%2520we%2520theoretically%2520analyze%2520the%2520performance%2520of%2520our%2520algorithm%2520and%250Aobtain%2520generalization%2520bounds%2520in%2520the%2520in-context%2520multi-task%2520learning%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20Decision%20Transformers%20with%20Reward%20Prediction%20for%20In-Context%0A%20%20Multi-task%20Structured%20Bandit%20Learning&entry.906535625=Subhojyoti%20Mukherjee%20and%20Josiah%20P.%20Hanna%20and%20Qiaomin%20Xie%20and%20Robert%20Nowak&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20multi-task%20structured%20bandit%20problem%20where%20the%20goal%0Ais%20to%20learn%20a%20near-optimal%20algorithm%20that%20minimizes%20cumulative%20regret.%20The%0Atasks%20share%20a%20common%20structure%20and%20the%20algorithm%20exploits%20the%20shared%20structure%0Ato%20minimize%20the%20cumulative%20regret%20for%20an%20unseen%20but%20related%20test%20task.%20We%20use%20a%0Atransformer%20as%20a%20decision-making%20algorithm%20to%20learn%20this%20shared%20structure%20so%20as%0Ato%20generalize%20to%20the%20test%20task.%20The%20prior%20work%20of%20pretrained%20decision%0Atransformers%20like%20DPT%20requires%20access%20to%20the%20optimal%20action%20during%20training%0Awhich%20may%20be%20hard%20in%20several%20scenarios.%20Diverging%20from%20these%20works%2C%20our%0Alearning%20algorithm%20does%20not%20need%20the%20knowledge%20of%20optimal%20action%20per%20task%0Aduring%20training%20but%20predicts%20a%20reward%20vector%20for%20each%20of%20the%20actions%20using%20only%0Athe%20observed%20offline%20data%20from%20the%20diverse%20training%20tasks.%20Finally%2C%20during%0Ainference%20time%2C%20it%20selects%20action%20using%20the%20reward%20predictions%20employing%0Avarious%20exploration%20strategies%20in-context%20for%20an%20unseen%20test%20task.%20Our%20model%0Aoutperforms%20other%20SOTA%20methods%20like%20DPT%2C%20and%20Algorithmic%20Distillation%20over%20a%0Aseries%20of%20experiments%20on%20several%20structured%20bandit%20problems%20%28linear%2C%20bilinear%2C%0Alatent%2C%20non-linear%29.%20Interestingly%2C%20we%20show%20that%20our%20algorithm%2C%20without%20the%0Aknowledge%20of%20the%20underlying%20problem%20structure%2C%20can%20learn%20a%20near-optimal%20policy%0Ain-context%20by%20leveraging%20the%20shared%20structure%20across%20diverse%20tasks.%20We%20further%0Aextend%20the%20field%20of%20pre-trained%20decision%20transformers%20by%20showing%20that%20they%20can%0Aleverage%20unseen%20tasks%20with%20new%20actions%20and%20still%20learn%20the%20underlying%20latent%0Astructure%20to%20derive%20a%20near-optimal%20policy.%20We%20validate%20this%20over%20several%0Aexperiments%20to%20show%20that%20our%20proposed%20solution%20is%20very%20general%20and%20has%20wide%0Aapplications%20to%20potentially%20emergent%20online%20and%20offline%20strategies%20at%20test%0Atime.%20Finally%2C%20we%20theoretically%20analyze%20the%20performance%20of%20our%20algorithm%20and%0Aobtain%20generalization%20bounds%20in%20the%20in-context%20multi-task%20learning%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05064v1&entry.124074799=Read"},
{"title": "Linearization Turns Neural Operators into Function-Valued Gaussian\n  Processes", "author": "Emilia Magnani and Marvin Pf\u00f6rtner and Tobias Weber and Philipp Hennig", "abstract": "  Modeling dynamical systems, e.g. in climate and engineering sciences, often\nnecessitates solving partial differential equations. Neural operators are deep\nneural networks designed to learn nontrivial solution operators of such\ndifferential equations from data. As for all statistical models, the\npredictions of these models are imperfect and exhibit errors. Such errors are\nparticularly difficult to spot in the complex nonlinear behaviour of dynamical\nsystems. We introduce a new framework for approximate Bayesian uncertainty\nquantification in neural operators using function-valued Gaussian processes.\nOur approach can be interpreted as a probabilistic analogue of the concept of\ncurrying from functional programming and provides a practical yet theoretically\nsound way to apply the linearized Laplace approximation to neural operators. In\na case study on Fourier neural operators, we show that, even for a discretized\ninput, our method yields a Gaussian closure--a structured Gaussian process\nposterior capturing the uncertainty in the output function of the neural\noperator, which can be evaluated at an arbitrary set of points. The method adds\nminimal prediction overhead, can be applied post-hoc without retraining the\nneural operator, and scales to large models and datasets. We showcase the\nefficacy of our approach through applications to different types of partial\ndifferential equations.\n", "link": "http://arxiv.org/abs/2406.05072v1", "date": "2024-06-07", "relevancy": 2.0523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5143}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linearization%20Turns%20Neural%20Operators%20into%20Function-Valued%20Gaussian%0A%20%20Processes&body=Title%3A%20Linearization%20Turns%20Neural%20Operators%20into%20Function-Valued%20Gaussian%0A%20%20Processes%0AAuthor%3A%20Emilia%20Magnani%20and%20Marvin%20Pf%C3%B6rtner%20and%20Tobias%20Weber%20and%20Philipp%20Hennig%0AAbstract%3A%20%20%20Modeling%20dynamical%20systems%2C%20e.g.%20in%20climate%20and%20engineering%20sciences%2C%20often%0Anecessitates%20solving%20partial%20differential%20equations.%20Neural%20operators%20are%20deep%0Aneural%20networks%20designed%20to%20learn%20nontrivial%20solution%20operators%20of%20such%0Adifferential%20equations%20from%20data.%20As%20for%20all%20statistical%20models%2C%20the%0Apredictions%20of%20these%20models%20are%20imperfect%20and%20exhibit%20errors.%20Such%20errors%20are%0Aparticularly%20difficult%20to%20spot%20in%20the%20complex%20nonlinear%20behaviour%20of%20dynamical%0Asystems.%20We%20introduce%20a%20new%20framework%20for%20approximate%20Bayesian%20uncertainty%0Aquantification%20in%20neural%20operators%20using%20function-valued%20Gaussian%20processes.%0AOur%20approach%20can%20be%20interpreted%20as%20a%20probabilistic%20analogue%20of%20the%20concept%20of%0Acurrying%20from%20functional%20programming%20and%20provides%20a%20practical%20yet%20theoretically%0Asound%20way%20to%20apply%20the%20linearized%20Laplace%20approximation%20to%20neural%20operators.%20In%0Aa%20case%20study%20on%20Fourier%20neural%20operators%2C%20we%20show%20that%2C%20even%20for%20a%20discretized%0Ainput%2C%20our%20method%20yields%20a%20Gaussian%20closure--a%20structured%20Gaussian%20process%0Aposterior%20capturing%20the%20uncertainty%20in%20the%20output%20function%20of%20the%20neural%0Aoperator%2C%20which%20can%20be%20evaluated%20at%20an%20arbitrary%20set%20of%20points.%20The%20method%20adds%0Aminimal%20prediction%20overhead%2C%20can%20be%20applied%20post-hoc%20without%20retraining%20the%0Aneural%20operator%2C%20and%20scales%20to%20large%20models%20and%20datasets.%20We%20showcase%20the%0Aefficacy%20of%20our%20approach%20through%20applications%20to%20different%20types%20of%20partial%0Adifferential%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinearization%2520Turns%2520Neural%2520Operators%2520into%2520Function-Valued%2520Gaussian%250A%2520%2520Processes%26entry.906535625%3DEmilia%2520Magnani%2520and%2520Marvin%2520Pf%25C3%25B6rtner%2520and%2520Tobias%2520Weber%2520and%2520Philipp%2520Hennig%26entry.1292438233%3D%2520%2520Modeling%2520dynamical%2520systems%252C%2520e.g.%2520in%2520climate%2520and%2520engineering%2520sciences%252C%2520often%250Anecessitates%2520solving%2520partial%2520differential%2520equations.%2520Neural%2520operators%2520are%2520deep%250Aneural%2520networks%2520designed%2520to%2520learn%2520nontrivial%2520solution%2520operators%2520of%2520such%250Adifferential%2520equations%2520from%2520data.%2520As%2520for%2520all%2520statistical%2520models%252C%2520the%250Apredictions%2520of%2520these%2520models%2520are%2520imperfect%2520and%2520exhibit%2520errors.%2520Such%2520errors%2520are%250Aparticularly%2520difficult%2520to%2520spot%2520in%2520the%2520complex%2520nonlinear%2520behaviour%2520of%2520dynamical%250Asystems.%2520We%2520introduce%2520a%2520new%2520framework%2520for%2520approximate%2520Bayesian%2520uncertainty%250Aquantification%2520in%2520neural%2520operators%2520using%2520function-valued%2520Gaussian%2520processes.%250AOur%2520approach%2520can%2520be%2520interpreted%2520as%2520a%2520probabilistic%2520analogue%2520of%2520the%2520concept%2520of%250Acurrying%2520from%2520functional%2520programming%2520and%2520provides%2520a%2520practical%2520yet%2520theoretically%250Asound%2520way%2520to%2520apply%2520the%2520linearized%2520Laplace%2520approximation%2520to%2520neural%2520operators.%2520In%250Aa%2520case%2520study%2520on%2520Fourier%2520neural%2520operators%252C%2520we%2520show%2520that%252C%2520even%2520for%2520a%2520discretized%250Ainput%252C%2520our%2520method%2520yields%2520a%2520Gaussian%2520closure--a%2520structured%2520Gaussian%2520process%250Aposterior%2520capturing%2520the%2520uncertainty%2520in%2520the%2520output%2520function%2520of%2520the%2520neural%250Aoperator%252C%2520which%2520can%2520be%2520evaluated%2520at%2520an%2520arbitrary%2520set%2520of%2520points.%2520The%2520method%2520adds%250Aminimal%2520prediction%2520overhead%252C%2520can%2520be%2520applied%2520post-hoc%2520without%2520retraining%2520the%250Aneural%2520operator%252C%2520and%2520scales%2520to%2520large%2520models%2520and%2520datasets.%2520We%2520showcase%2520the%250Aefficacy%2520of%2520our%2520approach%2520through%2520applications%2520to%2520different%2520types%2520of%2520partial%250Adifferential%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linearization%20Turns%20Neural%20Operators%20into%20Function-Valued%20Gaussian%0A%20%20Processes&entry.906535625=Emilia%20Magnani%20and%20Marvin%20Pf%C3%B6rtner%20and%20Tobias%20Weber%20and%20Philipp%20Hennig&entry.1292438233=%20%20Modeling%20dynamical%20systems%2C%20e.g.%20in%20climate%20and%20engineering%20sciences%2C%20often%0Anecessitates%20solving%20partial%20differential%20equations.%20Neural%20operators%20are%20deep%0Aneural%20networks%20designed%20to%20learn%20nontrivial%20solution%20operators%20of%20such%0Adifferential%20equations%20from%20data.%20As%20for%20all%20statistical%20models%2C%20the%0Apredictions%20of%20these%20models%20are%20imperfect%20and%20exhibit%20errors.%20Such%20errors%20are%0Aparticularly%20difficult%20to%20spot%20in%20the%20complex%20nonlinear%20behaviour%20of%20dynamical%0Asystems.%20We%20introduce%20a%20new%20framework%20for%20approximate%20Bayesian%20uncertainty%0Aquantification%20in%20neural%20operators%20using%20function-valued%20Gaussian%20processes.%0AOur%20approach%20can%20be%20interpreted%20as%20a%20probabilistic%20analogue%20of%20the%20concept%20of%0Acurrying%20from%20functional%20programming%20and%20provides%20a%20practical%20yet%20theoretically%0Asound%20way%20to%20apply%20the%20linearized%20Laplace%20approximation%20to%20neural%20operators.%20In%0Aa%20case%20study%20on%20Fourier%20neural%20operators%2C%20we%20show%20that%2C%20even%20for%20a%20discretized%0Ainput%2C%20our%20method%20yields%20a%20Gaussian%20closure--a%20structured%20Gaussian%20process%0Aposterior%20capturing%20the%20uncertainty%20in%20the%20output%20function%20of%20the%20neural%0Aoperator%2C%20which%20can%20be%20evaluated%20at%20an%20arbitrary%20set%20of%20points.%20The%20method%20adds%0Aminimal%20prediction%20overhead%2C%20can%20be%20applied%20post-hoc%20without%20retraining%20the%0Aneural%20operator%2C%20and%20scales%20to%20large%20models%20and%20datasets.%20We%20showcase%20the%0Aefficacy%20of%20our%20approach%20through%20applications%20to%20different%20types%20of%20partial%0Adifferential%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05072v1&entry.124074799=Read"},
{"title": "Faster Than Lies: Real-time Deepfake Detection using Binary Neural\n  Networks", "author": "Lanzino Romeo and Fontana Federico and Diko Anxhelo and Marini Marco Raoul and Cinque Luigi", "abstract": "  Deepfake detection aims to contrast the spread of deep-generated media that\nundermines trust in online content. While existing methods focus on large and\ncomplex models, the need for real-time detection demands greater efficiency.\nWith this in mind, unlike previous work, we introduce a novel deepfake\ndetection approach on images using Binary Neural Networks (BNNs) for fast\ninference with minimal accuracy loss. Moreover, our method incorporates Fast\nFourier Transform (FFT) and Local Binary Pattern (LBP) as additional channel\nfeatures to uncover manipulation traces in frequency and texture domains.\nEvaluations on COCOFake, DFFD, and CIFAKE datasets demonstrate our method's\nstate-of-the-art performance in most scenarios with a significant efficiency\ngain of up to a $20\\times$ reduction in FLOPs during inference. Finally, by\nexploring BNNs in deepfake detection to balance accuracy and efficiency, this\nwork paves the way for future research on efficient deepfake detection.\n", "link": "http://arxiv.org/abs/2406.04932v1", "date": "2024-06-07", "relevancy": 2.0481, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.52}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Than%20Lies%3A%20Real-time%20Deepfake%20Detection%20using%20Binary%20Neural%0A%20%20Networks&body=Title%3A%20Faster%20Than%20Lies%3A%20Real-time%20Deepfake%20Detection%20using%20Binary%20Neural%0A%20%20Networks%0AAuthor%3A%20Lanzino%20Romeo%20and%20Fontana%20Federico%20and%20Diko%20Anxhelo%20and%20Marini%20Marco%20Raoul%20and%20Cinque%20Luigi%0AAbstract%3A%20%20%20Deepfake%20detection%20aims%20to%20contrast%20the%20spread%20of%20deep-generated%20media%20that%0Aundermines%20trust%20in%20online%20content.%20While%20existing%20methods%20focus%20on%20large%20and%0Acomplex%20models%2C%20the%20need%20for%20real-time%20detection%20demands%20greater%20efficiency.%0AWith%20this%20in%20mind%2C%20unlike%20previous%20work%2C%20we%20introduce%20a%20novel%20deepfake%0Adetection%20approach%20on%20images%20using%20Binary%20Neural%20Networks%20%28BNNs%29%20for%20fast%0Ainference%20with%20minimal%20accuracy%20loss.%20Moreover%2C%20our%20method%20incorporates%20Fast%0AFourier%20Transform%20%28FFT%29%20and%20Local%20Binary%20Pattern%20%28LBP%29%20as%20additional%20channel%0Afeatures%20to%20uncover%20manipulation%20traces%20in%20frequency%20and%20texture%20domains.%0AEvaluations%20on%20COCOFake%2C%20DFFD%2C%20and%20CIFAKE%20datasets%20demonstrate%20our%20method%27s%0Astate-of-the-art%20performance%20in%20most%20scenarios%20with%20a%20significant%20efficiency%0Again%20of%20up%20to%20a%20%2420%5Ctimes%24%20reduction%20in%20FLOPs%20during%20inference.%20Finally%2C%20by%0Aexploring%20BNNs%20in%20deepfake%20detection%20to%20balance%20accuracy%20and%20efficiency%2C%20this%0Awork%20paves%20the%20way%20for%20future%20research%20on%20efficient%20deepfake%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Than%2520Lies%253A%2520Real-time%2520Deepfake%2520Detection%2520using%2520Binary%2520Neural%250A%2520%2520Networks%26entry.906535625%3DLanzino%2520Romeo%2520and%2520Fontana%2520Federico%2520and%2520Diko%2520Anxhelo%2520and%2520Marini%2520Marco%2520Raoul%2520and%2520Cinque%2520Luigi%26entry.1292438233%3D%2520%2520Deepfake%2520detection%2520aims%2520to%2520contrast%2520the%2520spread%2520of%2520deep-generated%2520media%2520that%250Aundermines%2520trust%2520in%2520online%2520content.%2520While%2520existing%2520methods%2520focus%2520on%2520large%2520and%250Acomplex%2520models%252C%2520the%2520need%2520for%2520real-time%2520detection%2520demands%2520greater%2520efficiency.%250AWith%2520this%2520in%2520mind%252C%2520unlike%2520previous%2520work%252C%2520we%2520introduce%2520a%2520novel%2520deepfake%250Adetection%2520approach%2520on%2520images%2520using%2520Binary%2520Neural%2520Networks%2520%2528BNNs%2529%2520for%2520fast%250Ainference%2520with%2520minimal%2520accuracy%2520loss.%2520Moreover%252C%2520our%2520method%2520incorporates%2520Fast%250AFourier%2520Transform%2520%2528FFT%2529%2520and%2520Local%2520Binary%2520Pattern%2520%2528LBP%2529%2520as%2520additional%2520channel%250Afeatures%2520to%2520uncover%2520manipulation%2520traces%2520in%2520frequency%2520and%2520texture%2520domains.%250AEvaluations%2520on%2520COCOFake%252C%2520DFFD%252C%2520and%2520CIFAKE%2520datasets%2520demonstrate%2520our%2520method%2527s%250Astate-of-the-art%2520performance%2520in%2520most%2520scenarios%2520with%2520a%2520significant%2520efficiency%250Again%2520of%2520up%2520to%2520a%2520%252420%255Ctimes%2524%2520reduction%2520in%2520FLOPs%2520during%2520inference.%2520Finally%252C%2520by%250Aexploring%2520BNNs%2520in%2520deepfake%2520detection%2520to%2520balance%2520accuracy%2520and%2520efficiency%252C%2520this%250Awork%2520paves%2520the%2520way%2520for%2520future%2520research%2520on%2520efficient%2520deepfake%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Than%20Lies%3A%20Real-time%20Deepfake%20Detection%20using%20Binary%20Neural%0A%20%20Networks&entry.906535625=Lanzino%20Romeo%20and%20Fontana%20Federico%20and%20Diko%20Anxhelo%20and%20Marini%20Marco%20Raoul%20and%20Cinque%20Luigi&entry.1292438233=%20%20Deepfake%20detection%20aims%20to%20contrast%20the%20spread%20of%20deep-generated%20media%20that%0Aundermines%20trust%20in%20online%20content.%20While%20existing%20methods%20focus%20on%20large%20and%0Acomplex%20models%2C%20the%20need%20for%20real-time%20detection%20demands%20greater%20efficiency.%0AWith%20this%20in%20mind%2C%20unlike%20previous%20work%2C%20we%20introduce%20a%20novel%20deepfake%0Adetection%20approach%20on%20images%20using%20Binary%20Neural%20Networks%20%28BNNs%29%20for%20fast%0Ainference%20with%20minimal%20accuracy%20loss.%20Moreover%2C%20our%20method%20incorporates%20Fast%0AFourier%20Transform%20%28FFT%29%20and%20Local%20Binary%20Pattern%20%28LBP%29%20as%20additional%20channel%0Afeatures%20to%20uncover%20manipulation%20traces%20in%20frequency%20and%20texture%20domains.%0AEvaluations%20on%20COCOFake%2C%20DFFD%2C%20and%20CIFAKE%20datasets%20demonstrate%20our%20method%27s%0Astate-of-the-art%20performance%20in%20most%20scenarios%20with%20a%20significant%20efficiency%0Again%20of%20up%20to%20a%20%2420%5Ctimes%24%20reduction%20in%20FLOPs%20during%20inference.%20Finally%2C%20by%0Aexploring%20BNNs%20in%20deepfake%20detection%20to%20balance%20accuracy%20and%20efficiency%2C%20this%0Awork%20paves%20the%20way%20for%20future%20research%20on%20efficient%20deepfake%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04932v1&entry.124074799=Read"},
{"title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM", "author": "Shengqiong Wu and Hao Fei and Xiangtai Li and Jiayi Ji and Hanwang Zhang and Tat-Seng Chua and Shuicheng Yan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n", "link": "http://arxiv.org/abs/2406.05127v1", "date": "2024-06-07", "relevancy": 2.0455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&body=Title%3A%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM%0AAuthor%3A%20Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Semantic%2520Equivalence%2520of%2520Tokenization%2520in%2520Multimodal%2520LLM%26entry.906535625%3DShengqiong%2520Wu%2520and%2520Hao%2520Fei%2520and%2520Xiangtai%2520Li%2520and%2520Jiayi%2520Ji%2520and%2520Hanwang%2520Zhang%2520and%2520Tat-Seng%2520Chua%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520exceptional%250Acapabilities%2520in%2520processing%2520vision-language%2520tasks.%2520One%2520of%2520the%2520crux%2520of%2520MLLMs%2520lies%250Ain%2520vision%2520tokenization%252C%2520which%2520involves%2520efficiently%2520transforming%2520input%2520visual%250Asignals%2520into%2520feature%2520representations%2520that%2520are%2520most%2520beneficial%2520for%2520LLMs.%250AHowever%252C%2520existing%2520vision%2520tokenizers%252C%2520essential%2520for%2520semantic%2520alignment%2520between%250Avision%2520and%2520language%252C%2520remain%2520problematic.%2520Existing%2520methods%2520aggressively%2520fragment%250Avisual%2520input%252C%2520corrupting%2520the%2520visual%2520semantic%2520integrity.%2520To%2520address%2520this%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520dynamic%2520Semantic-Equivalent%2520Vision%2520Tokenizer%2520%2528SeTok%2529%252C%250Awhich%2520groups%2520visual%2520features%2520into%2520semantic%2520units%2520via%2520a%2520dynamic%2520clustering%250Aalgorithm%252C%2520flexibly%2520determining%2520the%2520number%2520of%2520tokens%2520based%2520on%2520image%2520complexity.%250AThe%2520resulting%2520vision%2520tokens%2520effectively%2520preserve%2520semantic%2520integrity%2520and%2520capture%250Aboth%2520low-frequency%2520and%2520high-frequency%2520visual%2520features.%2520The%2520proposed%2520MLLM%250A%2528Setokim%2529%2520equipped%2520with%2520SeTok%2520significantly%2520demonstrates%2520superior%2520performance%250Aacross%2520various%2520tasks%252C%2520as%2520evidenced%2520by%2520our%2520experimental%2520results.%2520The%2520project%250Apage%2520is%2520at%2520https%253A//chocowu.github.io/SeTok-web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&entry.906535625=Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05127v1&entry.124074799=Read"},
{"title": "Optimal Recurrent Network Topologies for Dynamical Systems\n  Reconstruction", "author": "Christoph J\u00fcrgen Hemmer and Manuel Brenner and Florian Hess and Daniel Durstewitz", "abstract": "  In dynamical systems reconstruction (DSR) we seek to infer from time series\nmeasurements a generative model of the underlying dynamical process. This is a\nprime objective in any scientific discipline, where we are particularly\ninterested in parsimonious models with a low parameter load. A common strategy\nhere is parameter pruning, removing all parameters with small weights. However,\nhere we find this strategy does not work for DSR, where even low magnitude\nparameters can contribute considerably to the system dynamics. On the other\nhand, it is well known that many natural systems which generate complex\ndynamics, like the brain or ecological networks, have a sparse topology with\ncomparatively few links. Inspired by this, we show that geometric pruning,\nwhere in contrast to magnitude-based pruning weights with a low contribution to\nan attractor's geometrical structure are removed, indeed manages to reduce\nparameter load substantially without significantly hampering DSR quality. We\nfurther find that the networks resulting from geometric pruning have a specific\ntype of topology, and that this topology, and not the magnitude of weights, is\nwhat is most crucial to performance. We provide an algorithm that automatically\ngenerates such topologies which can be used as priors for generative modeling\nof dynamical systems by RNNs, and compare it to other well studied topologies\nlike small-world or scale-free networks.\n", "link": "http://arxiv.org/abs/2406.04934v1", "date": "2024-06-07", "relevancy": 2.0304, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5138}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Recurrent%20Network%20Topologies%20for%20Dynamical%20Systems%0A%20%20Reconstruction&body=Title%3A%20Optimal%20Recurrent%20Network%20Topologies%20for%20Dynamical%20Systems%0A%20%20Reconstruction%0AAuthor%3A%20Christoph%20J%C3%BCrgen%20Hemmer%20and%20Manuel%20Brenner%20and%20Florian%20Hess%20and%20Daniel%20Durstewitz%0AAbstract%3A%20%20%20In%20dynamical%20systems%20reconstruction%20%28DSR%29%20we%20seek%20to%20infer%20from%20time%20series%0Ameasurements%20a%20generative%20model%20of%20the%20underlying%20dynamical%20process.%20This%20is%20a%0Aprime%20objective%20in%20any%20scientific%20discipline%2C%20where%20we%20are%20particularly%0Ainterested%20in%20parsimonious%20models%20with%20a%20low%20parameter%20load.%20A%20common%20strategy%0Ahere%20is%20parameter%20pruning%2C%20removing%20all%20parameters%20with%20small%20weights.%20However%2C%0Ahere%20we%20find%20this%20strategy%20does%20not%20work%20for%20DSR%2C%20where%20even%20low%20magnitude%0Aparameters%20can%20contribute%20considerably%20to%20the%20system%20dynamics.%20On%20the%20other%0Ahand%2C%20it%20is%20well%20known%20that%20many%20natural%20systems%20which%20generate%20complex%0Adynamics%2C%20like%20the%20brain%20or%20ecological%20networks%2C%20have%20a%20sparse%20topology%20with%0Acomparatively%20few%20links.%20Inspired%20by%20this%2C%20we%20show%20that%20geometric%20pruning%2C%0Awhere%20in%20contrast%20to%20magnitude-based%20pruning%20weights%20with%20a%20low%20contribution%20to%0Aan%20attractor%27s%20geometrical%20structure%20are%20removed%2C%20indeed%20manages%20to%20reduce%0Aparameter%20load%20substantially%20without%20significantly%20hampering%20DSR%20quality.%20We%0Afurther%20find%20that%20the%20networks%20resulting%20from%20geometric%20pruning%20have%20a%20specific%0Atype%20of%20topology%2C%20and%20that%20this%20topology%2C%20and%20not%20the%20magnitude%20of%20weights%2C%20is%0Awhat%20is%20most%20crucial%20to%20performance.%20We%20provide%20an%20algorithm%20that%20automatically%0Agenerates%20such%20topologies%20which%20can%20be%20used%20as%20priors%20for%20generative%20modeling%0Aof%20dynamical%20systems%20by%20RNNs%2C%20and%20compare%20it%20to%20other%20well%20studied%20topologies%0Alike%20small-world%20or%20scale-free%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Recurrent%2520Network%2520Topologies%2520for%2520Dynamical%2520Systems%250A%2520%2520Reconstruction%26entry.906535625%3DChristoph%2520J%25C3%25BCrgen%2520Hemmer%2520and%2520Manuel%2520Brenner%2520and%2520Florian%2520Hess%2520and%2520Daniel%2520Durstewitz%26entry.1292438233%3D%2520%2520In%2520dynamical%2520systems%2520reconstruction%2520%2528DSR%2529%2520we%2520seek%2520to%2520infer%2520from%2520time%2520series%250Ameasurements%2520a%2520generative%2520model%2520of%2520the%2520underlying%2520dynamical%2520process.%2520This%2520is%2520a%250Aprime%2520objective%2520in%2520any%2520scientific%2520discipline%252C%2520where%2520we%2520are%2520particularly%250Ainterested%2520in%2520parsimonious%2520models%2520with%2520a%2520low%2520parameter%2520load.%2520A%2520common%2520strategy%250Ahere%2520is%2520parameter%2520pruning%252C%2520removing%2520all%2520parameters%2520with%2520small%2520weights.%2520However%252C%250Ahere%2520we%2520find%2520this%2520strategy%2520does%2520not%2520work%2520for%2520DSR%252C%2520where%2520even%2520low%2520magnitude%250Aparameters%2520can%2520contribute%2520considerably%2520to%2520the%2520system%2520dynamics.%2520On%2520the%2520other%250Ahand%252C%2520it%2520is%2520well%2520known%2520that%2520many%2520natural%2520systems%2520which%2520generate%2520complex%250Adynamics%252C%2520like%2520the%2520brain%2520or%2520ecological%2520networks%252C%2520have%2520a%2520sparse%2520topology%2520with%250Acomparatively%2520few%2520links.%2520Inspired%2520by%2520this%252C%2520we%2520show%2520that%2520geometric%2520pruning%252C%250Awhere%2520in%2520contrast%2520to%2520magnitude-based%2520pruning%2520weights%2520with%2520a%2520low%2520contribution%2520to%250Aan%2520attractor%2527s%2520geometrical%2520structure%2520are%2520removed%252C%2520indeed%2520manages%2520to%2520reduce%250Aparameter%2520load%2520substantially%2520without%2520significantly%2520hampering%2520DSR%2520quality.%2520We%250Afurther%2520find%2520that%2520the%2520networks%2520resulting%2520from%2520geometric%2520pruning%2520have%2520a%2520specific%250Atype%2520of%2520topology%252C%2520and%2520that%2520this%2520topology%252C%2520and%2520not%2520the%2520magnitude%2520of%2520weights%252C%2520is%250Awhat%2520is%2520most%2520crucial%2520to%2520performance.%2520We%2520provide%2520an%2520algorithm%2520that%2520automatically%250Agenerates%2520such%2520topologies%2520which%2520can%2520be%2520used%2520as%2520priors%2520for%2520generative%2520modeling%250Aof%2520dynamical%2520systems%2520by%2520RNNs%252C%2520and%2520compare%2520it%2520to%2520other%2520well%2520studied%2520topologies%250Alike%2520small-world%2520or%2520scale-free%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Recurrent%20Network%20Topologies%20for%20Dynamical%20Systems%0A%20%20Reconstruction&entry.906535625=Christoph%20J%C3%BCrgen%20Hemmer%20and%20Manuel%20Brenner%20and%20Florian%20Hess%20and%20Daniel%20Durstewitz&entry.1292438233=%20%20In%20dynamical%20systems%20reconstruction%20%28DSR%29%20we%20seek%20to%20infer%20from%20time%20series%0Ameasurements%20a%20generative%20model%20of%20the%20underlying%20dynamical%20process.%20This%20is%20a%0Aprime%20objective%20in%20any%20scientific%20discipline%2C%20where%20we%20are%20particularly%0Ainterested%20in%20parsimonious%20models%20with%20a%20low%20parameter%20load.%20A%20common%20strategy%0Ahere%20is%20parameter%20pruning%2C%20removing%20all%20parameters%20with%20small%20weights.%20However%2C%0Ahere%20we%20find%20this%20strategy%20does%20not%20work%20for%20DSR%2C%20where%20even%20low%20magnitude%0Aparameters%20can%20contribute%20considerably%20to%20the%20system%20dynamics.%20On%20the%20other%0Ahand%2C%20it%20is%20well%20known%20that%20many%20natural%20systems%20which%20generate%20complex%0Adynamics%2C%20like%20the%20brain%20or%20ecological%20networks%2C%20have%20a%20sparse%20topology%20with%0Acomparatively%20few%20links.%20Inspired%20by%20this%2C%20we%20show%20that%20geometric%20pruning%2C%0Awhere%20in%20contrast%20to%20magnitude-based%20pruning%20weights%20with%20a%20low%20contribution%20to%0Aan%20attractor%27s%20geometrical%20structure%20are%20removed%2C%20indeed%20manages%20to%20reduce%0Aparameter%20load%20substantially%20without%20significantly%20hampering%20DSR%20quality.%20We%0Afurther%20find%20that%20the%20networks%20resulting%20from%20geometric%20pruning%20have%20a%20specific%0Atype%20of%20topology%2C%20and%20that%20this%20topology%2C%20and%20not%20the%20magnitude%20of%20weights%2C%20is%0Awhat%20is%20most%20crucial%20to%20performance.%20We%20provide%20an%20algorithm%20that%20automatically%0Agenerates%20such%20topologies%20which%20can%20be%20used%20as%20priors%20for%20generative%20modeling%0Aof%20dynamical%20systems%20by%20RNNs%2C%20and%20compare%20it%20to%20other%20well%20studied%20topologies%0Alike%20small-world%20or%20scale-free%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04934v1&entry.124074799=Read"},
{"title": "From Link Prediction to Forecasting: Information Loss in Batch-based\n  Temporal Graph Learning", "author": "Moritz Lampert and Christopher Bl\u00f6cker and Ingo Scholtes", "abstract": "  Dynamic link prediction is an important problem considered by many recent\nworks proposing various approaches for learning temporal edge patterns. To\nassess their efficacy, models are evaluated on publicly available benchmark\ndatasets involving continuous-time and discrete-time temporal graphs. However,\nas we show in this work, the suitability of common batch-oriented evaluation\ndepends on the datasets' characteristics, which can cause two issues: First,\nfor continuous-time temporal graphs, fixed-size batches create time windows\nwith different durations, resulting in an inconsistent dynamic link prediction\ntask. Second, for discrete-time temporal graphs, the sequence of batches can\nadditionally introduce temporal dependencies that are not present in the data.\nIn this work, we empirically show that this common evaluation approach leads to\nskewed model performance and hinders the fair comparison of methods. We\nmitigate this problem by reformulating dynamic link prediction as a link\nforecasting task that better accounts for temporal information present in the\ndata. We provide implementations of our new evaluation method for commonly used\ngraph learning frameworks.\n", "link": "http://arxiv.org/abs/2406.04897v1", "date": "2024-06-07", "relevancy": 2.0261, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5321}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Link%20Prediction%20to%20Forecasting%3A%20Information%20Loss%20in%20Batch-based%0A%20%20Temporal%20Graph%20Learning&body=Title%3A%20From%20Link%20Prediction%20to%20Forecasting%3A%20Information%20Loss%20in%20Batch-based%0A%20%20Temporal%20Graph%20Learning%0AAuthor%3A%20Moritz%20Lampert%20and%20Christopher%20Bl%C3%B6cker%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20Dynamic%20link%20prediction%20is%20an%20important%20problem%20considered%20by%20many%20recent%0Aworks%20proposing%20various%20approaches%20for%20learning%20temporal%20edge%20patterns.%20To%0Aassess%20their%20efficacy%2C%20models%20are%20evaluated%20on%20publicly%20available%20benchmark%0Adatasets%20involving%20continuous-time%20and%20discrete-time%20temporal%20graphs.%20However%2C%0Aas%20we%20show%20in%20this%20work%2C%20the%20suitability%20of%20common%20batch-oriented%20evaluation%0Adepends%20on%20the%20datasets%27%20characteristics%2C%20which%20can%20cause%20two%20issues%3A%20First%2C%0Afor%20continuous-time%20temporal%20graphs%2C%20fixed-size%20batches%20create%20time%20windows%0Awith%20different%20durations%2C%20resulting%20in%20an%20inconsistent%20dynamic%20link%20prediction%0Atask.%20Second%2C%20for%20discrete-time%20temporal%20graphs%2C%20the%20sequence%20of%20batches%20can%0Aadditionally%20introduce%20temporal%20dependencies%20that%20are%20not%20present%20in%20the%20data.%0AIn%20this%20work%2C%20we%20empirically%20show%20that%20this%20common%20evaluation%20approach%20leads%20to%0Askewed%20model%20performance%20and%20hinders%20the%20fair%20comparison%20of%20methods.%20We%0Amitigate%20this%20problem%20by%20reformulating%20dynamic%20link%20prediction%20as%20a%20link%0Aforecasting%20task%20that%20better%20accounts%20for%20temporal%20information%20present%20in%20the%0Adata.%20We%20provide%20implementations%20of%20our%20new%20evaluation%20method%20for%20commonly%20used%0Agraph%20learning%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Link%2520Prediction%2520to%2520Forecasting%253A%2520Information%2520Loss%2520in%2520Batch-based%250A%2520%2520Temporal%2520Graph%2520Learning%26entry.906535625%3DMoritz%2520Lampert%2520and%2520Christopher%2520Bl%25C3%25B6cker%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520Dynamic%2520link%2520prediction%2520is%2520an%2520important%2520problem%2520considered%2520by%2520many%2520recent%250Aworks%2520proposing%2520various%2520approaches%2520for%2520learning%2520temporal%2520edge%2520patterns.%2520To%250Aassess%2520their%2520efficacy%252C%2520models%2520are%2520evaluated%2520on%2520publicly%2520available%2520benchmark%250Adatasets%2520involving%2520continuous-time%2520and%2520discrete-time%2520temporal%2520graphs.%2520However%252C%250Aas%2520we%2520show%2520in%2520this%2520work%252C%2520the%2520suitability%2520of%2520common%2520batch-oriented%2520evaluation%250Adepends%2520on%2520the%2520datasets%2527%2520characteristics%252C%2520which%2520can%2520cause%2520two%2520issues%253A%2520First%252C%250Afor%2520continuous-time%2520temporal%2520graphs%252C%2520fixed-size%2520batches%2520create%2520time%2520windows%250Awith%2520different%2520durations%252C%2520resulting%2520in%2520an%2520inconsistent%2520dynamic%2520link%2520prediction%250Atask.%2520Second%252C%2520for%2520discrete-time%2520temporal%2520graphs%252C%2520the%2520sequence%2520of%2520batches%2520can%250Aadditionally%2520introduce%2520temporal%2520dependencies%2520that%2520are%2520not%2520present%2520in%2520the%2520data.%250AIn%2520this%2520work%252C%2520we%2520empirically%2520show%2520that%2520this%2520common%2520evaluation%2520approach%2520leads%2520to%250Askewed%2520model%2520performance%2520and%2520hinders%2520the%2520fair%2520comparison%2520of%2520methods.%2520We%250Amitigate%2520this%2520problem%2520by%2520reformulating%2520dynamic%2520link%2520prediction%2520as%2520a%2520link%250Aforecasting%2520task%2520that%2520better%2520accounts%2520for%2520temporal%2520information%2520present%2520in%2520the%250Adata.%2520We%2520provide%2520implementations%2520of%2520our%2520new%2520evaluation%2520method%2520for%2520commonly%2520used%250Agraph%2520learning%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Link%20Prediction%20to%20Forecasting%3A%20Information%20Loss%20in%20Batch-based%0A%20%20Temporal%20Graph%20Learning&entry.906535625=Moritz%20Lampert%20and%20Christopher%20Bl%C3%B6cker%20and%20Ingo%20Scholtes&entry.1292438233=%20%20Dynamic%20link%20prediction%20is%20an%20important%20problem%20considered%20by%20many%20recent%0Aworks%20proposing%20various%20approaches%20for%20learning%20temporal%20edge%20patterns.%20To%0Aassess%20their%20efficacy%2C%20models%20are%20evaluated%20on%20publicly%20available%20benchmark%0Adatasets%20involving%20continuous-time%20and%20discrete-time%20temporal%20graphs.%20However%2C%0Aas%20we%20show%20in%20this%20work%2C%20the%20suitability%20of%20common%20batch-oriented%20evaluation%0Adepends%20on%20the%20datasets%27%20characteristics%2C%20which%20can%20cause%20two%20issues%3A%20First%2C%0Afor%20continuous-time%20temporal%20graphs%2C%20fixed-size%20batches%20create%20time%20windows%0Awith%20different%20durations%2C%20resulting%20in%20an%20inconsistent%20dynamic%20link%20prediction%0Atask.%20Second%2C%20for%20discrete-time%20temporal%20graphs%2C%20the%20sequence%20of%20batches%20can%0Aadditionally%20introduce%20temporal%20dependencies%20that%20are%20not%20present%20in%20the%20data.%0AIn%20this%20work%2C%20we%20empirically%20show%20that%20this%20common%20evaluation%20approach%20leads%20to%0Askewed%20model%20performance%20and%20hinders%20the%20fair%20comparison%20of%20methods.%20We%0Amitigate%20this%20problem%20by%20reformulating%20dynamic%20link%20prediction%20as%20a%20link%0Aforecasting%20task%20that%20better%20accounts%20for%20temporal%20information%20present%20in%20the%0Adata.%20We%20provide%20implementations%20of%20our%20new%20evaluation%20method%20for%20commonly%20used%0Agraph%20learning%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04897v1&entry.124074799=Read"},
{"title": "I2EDL: Interactive Instruction Error Detection and Localization", "author": "Francesco Taioli and Stefano Rosa and Alberto Castellini and Lorenzo Natale and Alessio Del Bue and Alessandro Farinelli and Marco Cristani and Yiming Wang", "abstract": "  In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)\ntask, the human user guides an autonomous agent to reach a target goal via a\nseries of low-level actions following a textual instruction in natural\nlanguage. However, most existing methods do not address the likely case where\nusers may make mistakes when providing such instruction (e.g. \"turn left\"\ninstead of \"turn right\"). In this work, we address a novel task of Interactive\nVLN in Continuous Environments (IVLN-CE), which allows the agent to interact\nwith the user during the VLN-CE navigation to verify any doubts regarding the\ninstruction errors. We propose an Interactive Instruction Error Detector and\nLocalizer (I2EDL) that triggers the user-agent interaction upon the detection\nof instruction errors during the navigation. We leverage a pre-trained module\nto detect instruction errors and pinpoint them in the instruction by\ncross-referencing the textual input and past observations. In such way, the\nagent is able to query the user for a timely correction, without demanding the\nuser's cognitive load, as we locate the probable errors to a precise part of\nthe instruction. We evaluate the proposed I2EDL on a dataset of instructions\ncontaining errors, and further devise a novel metric, the Success weighted by\nInteraction Number (SIN), to reflect both the navigation performance and the\ninteraction effectiveness. We show how the proposed method can ask focused\nrequests for corrections to the user, which in turn increases the navigation\nsuccess, while minimizing the interactions.\n", "link": "http://arxiv.org/abs/2406.05080v1", "date": "2024-06-07", "relevancy": 2.0247, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5309}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2EDL%3A%20Interactive%20Instruction%20Error%20Detection%20and%20Localization&body=Title%3A%20I2EDL%3A%20Interactive%20Instruction%20Error%20Detection%20and%20Localization%0AAuthor%3A%20Francesco%20Taioli%20and%20Stefano%20Rosa%20and%20Alberto%20Castellini%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue%20and%20Alessandro%20Farinelli%20and%20Marco%20Cristani%20and%20Yiming%20Wang%0AAbstract%3A%20%20%20In%20the%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%0Atask%2C%20the%20human%20user%20guides%20an%20autonomous%20agent%20to%20reach%20a%20target%20goal%20via%20a%0Aseries%20of%20low-level%20actions%20following%20a%20textual%20instruction%20in%20natural%0Alanguage.%20However%2C%20most%20existing%20methods%20do%20not%20address%20the%20likely%20case%20where%0Ausers%20may%20make%20mistakes%20when%20providing%20such%20instruction%20%28e.g.%20%22turn%20left%22%0Ainstead%20of%20%22turn%20right%22%29.%20In%20this%20work%2C%20we%20address%20a%20novel%20task%20of%20Interactive%0AVLN%20in%20Continuous%20Environments%20%28IVLN-CE%29%2C%20which%20allows%20the%20agent%20to%20interact%0Awith%20the%20user%20during%20the%20VLN-CE%20navigation%20to%20verify%20any%20doubts%20regarding%20the%0Ainstruction%20errors.%20We%20propose%20an%20Interactive%20Instruction%20Error%20Detector%20and%0ALocalizer%20%28I2EDL%29%20that%20triggers%20the%20user-agent%20interaction%20upon%20the%20detection%0Aof%20instruction%20errors%20during%20the%20navigation.%20We%20leverage%20a%20pre-trained%20module%0Ato%20detect%20instruction%20errors%20and%20pinpoint%20them%20in%20the%20instruction%20by%0Across-referencing%20the%20textual%20input%20and%20past%20observations.%20In%20such%20way%2C%20the%0Aagent%20is%20able%20to%20query%20the%20user%20for%20a%20timely%20correction%2C%20without%20demanding%20the%0Auser%27s%20cognitive%20load%2C%20as%20we%20locate%20the%20probable%20errors%20to%20a%20precise%20part%20of%0Athe%20instruction.%20We%20evaluate%20the%20proposed%20I2EDL%20on%20a%20dataset%20of%20instructions%0Acontaining%20errors%2C%20and%20further%20devise%20a%20novel%20metric%2C%20the%20Success%20weighted%20by%0AInteraction%20Number%20%28SIN%29%2C%20to%20reflect%20both%20the%20navigation%20performance%20and%20the%0Ainteraction%20effectiveness.%20We%20show%20how%20the%20proposed%20method%20can%20ask%20focused%0Arequests%20for%20corrections%20to%20the%20user%2C%20which%20in%20turn%20increases%20the%20navigation%0Asuccess%2C%20while%20minimizing%20the%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2EDL%253A%2520Interactive%2520Instruction%2520Error%2520Detection%2520and%2520Localization%26entry.906535625%3DFrancesco%2520Taioli%2520and%2520Stefano%2520Rosa%2520and%2520Alberto%2520Castellini%2520and%2520Lorenzo%2520Natale%2520and%2520Alessio%2520Del%2520Bue%2520and%2520Alessandro%2520Farinelli%2520and%2520Marco%2520Cristani%2520and%2520Yiming%2520Wang%26entry.1292438233%3D%2520%2520In%2520the%2520Vision-and-Language%2520Navigation%2520in%2520Continuous%2520Environments%2520%2528VLN-CE%2529%250Atask%252C%2520the%2520human%2520user%2520guides%2520an%2520autonomous%2520agent%2520to%2520reach%2520a%2520target%2520goal%2520via%2520a%250Aseries%2520of%2520low-level%2520actions%2520following%2520a%2520textual%2520instruction%2520in%2520natural%250Alanguage.%2520However%252C%2520most%2520existing%2520methods%2520do%2520not%2520address%2520the%2520likely%2520case%2520where%250Ausers%2520may%2520make%2520mistakes%2520when%2520providing%2520such%2520instruction%2520%2528e.g.%2520%2522turn%2520left%2522%250Ainstead%2520of%2520%2522turn%2520right%2522%2529.%2520In%2520this%2520work%252C%2520we%2520address%2520a%2520novel%2520task%2520of%2520Interactive%250AVLN%2520in%2520Continuous%2520Environments%2520%2528IVLN-CE%2529%252C%2520which%2520allows%2520the%2520agent%2520to%2520interact%250Awith%2520the%2520user%2520during%2520the%2520VLN-CE%2520navigation%2520to%2520verify%2520any%2520doubts%2520regarding%2520the%250Ainstruction%2520errors.%2520We%2520propose%2520an%2520Interactive%2520Instruction%2520Error%2520Detector%2520and%250ALocalizer%2520%2528I2EDL%2529%2520that%2520triggers%2520the%2520user-agent%2520interaction%2520upon%2520the%2520detection%250Aof%2520instruction%2520errors%2520during%2520the%2520navigation.%2520We%2520leverage%2520a%2520pre-trained%2520module%250Ato%2520detect%2520instruction%2520errors%2520and%2520pinpoint%2520them%2520in%2520the%2520instruction%2520by%250Across-referencing%2520the%2520textual%2520input%2520and%2520past%2520observations.%2520In%2520such%2520way%252C%2520the%250Aagent%2520is%2520able%2520to%2520query%2520the%2520user%2520for%2520a%2520timely%2520correction%252C%2520without%2520demanding%2520the%250Auser%2527s%2520cognitive%2520load%252C%2520as%2520we%2520locate%2520the%2520probable%2520errors%2520to%2520a%2520precise%2520part%2520of%250Athe%2520instruction.%2520We%2520evaluate%2520the%2520proposed%2520I2EDL%2520on%2520a%2520dataset%2520of%2520instructions%250Acontaining%2520errors%252C%2520and%2520further%2520devise%2520a%2520novel%2520metric%252C%2520the%2520Success%2520weighted%2520by%250AInteraction%2520Number%2520%2528SIN%2529%252C%2520to%2520reflect%2520both%2520the%2520navigation%2520performance%2520and%2520the%250Ainteraction%2520effectiveness.%2520We%2520show%2520how%2520the%2520proposed%2520method%2520can%2520ask%2520focused%250Arequests%2520for%2520corrections%2520to%2520the%2520user%252C%2520which%2520in%2520turn%2520increases%2520the%2520navigation%250Asuccess%252C%2520while%2520minimizing%2520the%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2EDL%3A%20Interactive%20Instruction%20Error%20Detection%20and%20Localization&entry.906535625=Francesco%20Taioli%20and%20Stefano%20Rosa%20and%20Alberto%20Castellini%20and%20Lorenzo%20Natale%20and%20Alessio%20Del%20Bue%20and%20Alessandro%20Farinelli%20and%20Marco%20Cristani%20and%20Yiming%20Wang&entry.1292438233=%20%20In%20the%20Vision-and-Language%20Navigation%20in%20Continuous%20Environments%20%28VLN-CE%29%0Atask%2C%20the%20human%20user%20guides%20an%20autonomous%20agent%20to%20reach%20a%20target%20goal%20via%20a%0Aseries%20of%20low-level%20actions%20following%20a%20textual%20instruction%20in%20natural%0Alanguage.%20However%2C%20most%20existing%20methods%20do%20not%20address%20the%20likely%20case%20where%0Ausers%20may%20make%20mistakes%20when%20providing%20such%20instruction%20%28e.g.%20%22turn%20left%22%0Ainstead%20of%20%22turn%20right%22%29.%20In%20this%20work%2C%20we%20address%20a%20novel%20task%20of%20Interactive%0AVLN%20in%20Continuous%20Environments%20%28IVLN-CE%29%2C%20which%20allows%20the%20agent%20to%20interact%0Awith%20the%20user%20during%20the%20VLN-CE%20navigation%20to%20verify%20any%20doubts%20regarding%20the%0Ainstruction%20errors.%20We%20propose%20an%20Interactive%20Instruction%20Error%20Detector%20and%0ALocalizer%20%28I2EDL%29%20that%20triggers%20the%20user-agent%20interaction%20upon%20the%20detection%0Aof%20instruction%20errors%20during%20the%20navigation.%20We%20leverage%20a%20pre-trained%20module%0Ato%20detect%20instruction%20errors%20and%20pinpoint%20them%20in%20the%20instruction%20by%0Across-referencing%20the%20textual%20input%20and%20past%20observations.%20In%20such%20way%2C%20the%0Aagent%20is%20able%20to%20query%20the%20user%20for%20a%20timely%20correction%2C%20without%20demanding%20the%0Auser%27s%20cognitive%20load%2C%20as%20we%20locate%20the%20probable%20errors%20to%20a%20precise%20part%20of%0Athe%20instruction.%20We%20evaluate%20the%20proposed%20I2EDL%20on%20a%20dataset%20of%20instructions%0Acontaining%20errors%2C%20and%20further%20devise%20a%20novel%20metric%2C%20the%20Success%20weighted%20by%0AInteraction%20Number%20%28SIN%29%2C%20to%20reflect%20both%20the%20navigation%20performance%20and%20the%0Ainteraction%20effectiveness.%20We%20show%20how%20the%20proposed%20method%20can%20ask%20focused%0Arequests%20for%20corrections%20to%20the%20user%2C%20which%20in%20turn%20increases%20the%20navigation%0Asuccess%2C%20while%20minimizing%20the%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05080v1&entry.124074799=Read"},
{"title": "Vulnerable Road User Detection and Safety Enhancement: A Comprehensive\n  Survey", "author": "Renato M. Silva and Greg\u00f3rio F. Azevedo and Matheus V. V. Berto and Jean R. Rocha and Eduardo C. Fidelis and Matheus V. Nogueira and Pedro H. Lisboa and Tiago A. Almeida", "abstract": "  Traffic incidents involving vulnerable road users (VRUs) constitute a\nsignificant proportion of global road accidents. Advances in traffic\ncommunication ecosystems, coupled with sophisticated signal processing and\nmachine learning techniques, have facilitated the utilization of data from\ndiverse sensors. Despite these advancements and the availability of extensive\ndatasets, substantial progress is required to mitigate traffic casualties. This\npaper provides a comprehensive survey of state-of-the-art technologies and\nmethodologies to enhance the safety of VRUs. The study delves into the\ncommunication networks between vehicles and VRUs, emphasizing the integration\nof advanced sensors and the availability of relevant datasets. It explores\npreprocessing techniques and data fusion methods to enhance sensor data\nquality. Furthermore, our study assesses critical simulation environments\nessential for developing and testing VRU safety systems. Our research also\nhighlights recent advances in VRU detection and classification algorithms,\naddressing challenges such as variable environmental conditions. Additionally,\nwe cover cutting-edge research in predicting VRU intentions and behaviors,\nwhich is crucial for proactive collision avoidance strategies. Through this\nsurvey, we aim to provide a comprehensive understanding of the current\nlandscape of VRU safety technologies, identifying areas of progress and areas\nneeding further research and development.\n", "link": "http://arxiv.org/abs/2405.19202v2", "date": "2024-06-07", "relevancy": 2.0161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5173}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey&body=Title%3A%20Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey%0AAuthor%3A%20Renato%20M.%20Silva%20and%20Greg%C3%B3rio%20F.%20Azevedo%20and%20Matheus%20V.%20V.%20Berto%20and%20Jean%20R.%20Rocha%20and%20Eduardo%20C.%20Fidelis%20and%20Matheus%20V.%20Nogueira%20and%20Pedro%20H.%20Lisboa%20and%20Tiago%20A.%20Almeida%0AAbstract%3A%20%20%20Traffic%20incidents%20involving%20vulnerable%20road%20users%20%28VRUs%29%20constitute%20a%0Asignificant%20proportion%20of%20global%20road%20accidents.%20Advances%20in%20traffic%0Acommunication%20ecosystems%2C%20coupled%20with%20sophisticated%20signal%20processing%20and%0Amachine%20learning%20techniques%2C%20have%20facilitated%20the%20utilization%20of%20data%20from%0Adiverse%20sensors.%20Despite%20these%20advancements%20and%20the%20availability%20of%20extensive%0Adatasets%2C%20substantial%20progress%20is%20required%20to%20mitigate%20traffic%20casualties.%20This%0Apaper%20provides%20a%20comprehensive%20survey%20of%20state-of-the-art%20technologies%20and%0Amethodologies%20to%20enhance%20the%20safety%20of%20VRUs.%20The%20study%20delves%20into%20the%0Acommunication%20networks%20between%20vehicles%20and%20VRUs%2C%20emphasizing%20the%20integration%0Aof%20advanced%20sensors%20and%20the%20availability%20of%20relevant%20datasets.%20It%20explores%0Apreprocessing%20techniques%20and%20data%20fusion%20methods%20to%20enhance%20sensor%20data%0Aquality.%20Furthermore%2C%20our%20study%20assesses%20critical%20simulation%20environments%0Aessential%20for%20developing%20and%20testing%20VRU%20safety%20systems.%20Our%20research%20also%0Ahighlights%20recent%20advances%20in%20VRU%20detection%20and%20classification%20algorithms%2C%0Aaddressing%20challenges%20such%20as%20variable%20environmental%20conditions.%20Additionally%2C%0Awe%20cover%20cutting-edge%20research%20in%20predicting%20VRU%20intentions%20and%20behaviors%2C%0Awhich%20is%20crucial%20for%20proactive%20collision%20avoidance%20strategies.%20Through%20this%0Asurvey%2C%20we%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%20the%20current%0Alandscape%20of%20VRU%20safety%20technologies%2C%20identifying%20areas%20of%20progress%20and%20areas%0Aneeding%20further%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVulnerable%2520Road%2520User%2520Detection%2520and%2520Safety%2520Enhancement%253A%2520A%2520Comprehensive%250A%2520%2520Survey%26entry.906535625%3DRenato%2520M.%2520Silva%2520and%2520Greg%25C3%25B3rio%2520F.%2520Azevedo%2520and%2520Matheus%2520V.%2520V.%2520Berto%2520and%2520Jean%2520R.%2520Rocha%2520and%2520Eduardo%2520C.%2520Fidelis%2520and%2520Matheus%2520V.%2520Nogueira%2520and%2520Pedro%2520H.%2520Lisboa%2520and%2520Tiago%2520A.%2520Almeida%26entry.1292438233%3D%2520%2520Traffic%2520incidents%2520involving%2520vulnerable%2520road%2520users%2520%2528VRUs%2529%2520constitute%2520a%250Asignificant%2520proportion%2520of%2520global%2520road%2520accidents.%2520Advances%2520in%2520traffic%250Acommunication%2520ecosystems%252C%2520coupled%2520with%2520sophisticated%2520signal%2520processing%2520and%250Amachine%2520learning%2520techniques%252C%2520have%2520facilitated%2520the%2520utilization%2520of%2520data%2520from%250Adiverse%2520sensors.%2520Despite%2520these%2520advancements%2520and%2520the%2520availability%2520of%2520extensive%250Adatasets%252C%2520substantial%2520progress%2520is%2520required%2520to%2520mitigate%2520traffic%2520casualties.%2520This%250Apaper%2520provides%2520a%2520comprehensive%2520survey%2520of%2520state-of-the-art%2520technologies%2520and%250Amethodologies%2520to%2520enhance%2520the%2520safety%2520of%2520VRUs.%2520The%2520study%2520delves%2520into%2520the%250Acommunication%2520networks%2520between%2520vehicles%2520and%2520VRUs%252C%2520emphasizing%2520the%2520integration%250Aof%2520advanced%2520sensors%2520and%2520the%2520availability%2520of%2520relevant%2520datasets.%2520It%2520explores%250Apreprocessing%2520techniques%2520and%2520data%2520fusion%2520methods%2520to%2520enhance%2520sensor%2520data%250Aquality.%2520Furthermore%252C%2520our%2520study%2520assesses%2520critical%2520simulation%2520environments%250Aessential%2520for%2520developing%2520and%2520testing%2520VRU%2520safety%2520systems.%2520Our%2520research%2520also%250Ahighlights%2520recent%2520advances%2520in%2520VRU%2520detection%2520and%2520classification%2520algorithms%252C%250Aaddressing%2520challenges%2520such%2520as%2520variable%2520environmental%2520conditions.%2520Additionally%252C%250Awe%2520cover%2520cutting-edge%2520research%2520in%2520predicting%2520VRU%2520intentions%2520and%2520behaviors%252C%250Awhich%2520is%2520crucial%2520for%2520proactive%2520collision%2520avoidance%2520strategies.%2520Through%2520this%250Asurvey%252C%2520we%2520aim%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%2520the%2520current%250Alandscape%2520of%2520VRU%2520safety%2520technologies%252C%2520identifying%2520areas%2520of%2520progress%2520and%2520areas%250Aneeding%2520further%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey&entry.906535625=Renato%20M.%20Silva%20and%20Greg%C3%B3rio%20F.%20Azevedo%20and%20Matheus%20V.%20V.%20Berto%20and%20Jean%20R.%20Rocha%20and%20Eduardo%20C.%20Fidelis%20and%20Matheus%20V.%20Nogueira%20and%20Pedro%20H.%20Lisboa%20and%20Tiago%20A.%20Almeida&entry.1292438233=%20%20Traffic%20incidents%20involving%20vulnerable%20road%20users%20%28VRUs%29%20constitute%20a%0Asignificant%20proportion%20of%20global%20road%20accidents.%20Advances%20in%20traffic%0Acommunication%20ecosystems%2C%20coupled%20with%20sophisticated%20signal%20processing%20and%0Amachine%20learning%20techniques%2C%20have%20facilitated%20the%20utilization%20of%20data%20from%0Adiverse%20sensors.%20Despite%20these%20advancements%20and%20the%20availability%20of%20extensive%0Adatasets%2C%20substantial%20progress%20is%20required%20to%20mitigate%20traffic%20casualties.%20This%0Apaper%20provides%20a%20comprehensive%20survey%20of%20state-of-the-art%20technologies%20and%0Amethodologies%20to%20enhance%20the%20safety%20of%20VRUs.%20The%20study%20delves%20into%20the%0Acommunication%20networks%20between%20vehicles%20and%20VRUs%2C%20emphasizing%20the%20integration%0Aof%20advanced%20sensors%20and%20the%20availability%20of%20relevant%20datasets.%20It%20explores%0Apreprocessing%20techniques%20and%20data%20fusion%20methods%20to%20enhance%20sensor%20data%0Aquality.%20Furthermore%2C%20our%20study%20assesses%20critical%20simulation%20environments%0Aessential%20for%20developing%20and%20testing%20VRU%20safety%20systems.%20Our%20research%20also%0Ahighlights%20recent%20advances%20in%20VRU%20detection%20and%20classification%20algorithms%2C%0Aaddressing%20challenges%20such%20as%20variable%20environmental%20conditions.%20Additionally%2C%0Awe%20cover%20cutting-edge%20research%20in%20predicting%20VRU%20intentions%20and%20behaviors%2C%0Awhich%20is%20crucial%20for%20proactive%20collision%20avoidance%20strategies.%20Through%20this%0Asurvey%2C%20we%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%20the%20current%0Alandscape%20of%20VRU%20safety%20technologies%2C%20identifying%20areas%20of%20progress%20and%20areas%0Aneeding%20further%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19202v2&entry.124074799=Read"},
{"title": "How Abilities in Large Language Models are Affected by Supervised\n  Fine-tuning Data Composition", "author": "Guanting Dong and Hongyi Yuan and Keming Lu and Chengpeng Li and Mingfeng Xue and Dayiheng Liu and Wei Wang and Zheng Yuan and Chang Zhou and Jingren Zhou", "abstract": "  Large language models (LLMs) with enormous pre-training tokens and parameters\nemerge diverse abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). While the open-source community has explored ad-hoc SFT for\nenhancing individual capabilities, proprietary LLMs exhibit versatility across\nvarious skills. Therefore, understanding the facilitation of multiple abilities\nvia SFT is paramount. In this study, we specifically focuses on the interplay\nof data composition between mathematical reasoning, code generation, and\ngeneral human-aligning abilities during SFT. We propose four intriguing\nresearch questions to explore the association between model performance and\nvarious factors including data amount, composition ratio, model size and SFT\nstrategies. Our experiments reveal that distinct capabilities scale differently\nand larger models generally show superior performance with same amount of data.\nMathematical reasoning and code generation consistently improve with increasing\ndata amount, whereas general abilities plateau after roughly a thousand\nsamples. Moreover, we observe data composition appears to enhance various\nabilities under limited data conditions, yet can lead to performance conflicts\nwhen data is plentiful. Our findings also suggest the amount of composition\ndata influences performance more than the composition ratio. In analysis of SFT\nstrategies, we find that sequentially learning multiple skills risks\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy offers a promising solution to learn multiple abilities with different\nscaling patterns.\n", "link": "http://arxiv.org/abs/2310.05492v4", "date": "2024-06-07", "relevancy": 2.016, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Abilities%20in%20Large%20Language%20Models%20are%20Affected%20by%20Supervised%0A%20%20Fine-tuning%20Data%20Composition&body=Title%3A%20How%20Abilities%20in%20Large%20Language%20Models%20are%20Affected%20by%20Supervised%0A%20%20Fine-tuning%20Data%20Composition%0AAuthor%3A%20Guanting%20Dong%20and%20Hongyi%20Yuan%20and%20Keming%20Lu%20and%20Chengpeng%20Li%20and%20Mingfeng%20Xue%20and%20Dayiheng%20Liu%20and%20Wei%20Wang%20and%20Zheng%20Yuan%20and%20Chang%20Zhou%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20with%20enormous%20pre-training%20tokens%20and%20parameters%0Aemerge%20diverse%20abilities%2C%20including%20math%20reasoning%2C%20code%20generation%2C%20and%0Ainstruction%20following.%20These%20abilities%20are%20further%20enhanced%20by%20supervised%0Afine-tuning%20%28SFT%29.%20While%20the%20open-source%20community%20has%20explored%20ad-hoc%20SFT%20for%0Aenhancing%20individual%20capabilities%2C%20proprietary%20LLMs%20exhibit%20versatility%20across%0Avarious%20skills.%20Therefore%2C%20understanding%20the%20facilitation%20of%20multiple%20abilities%0Avia%20SFT%20is%20paramount.%20In%20this%20study%2C%20we%20specifically%20focuses%20on%20the%20interplay%0Aof%20data%20composition%20between%20mathematical%20reasoning%2C%20code%20generation%2C%20and%0Ageneral%20human-aligning%20abilities%20during%20SFT.%20We%20propose%20four%20intriguing%0Aresearch%20questions%20to%20explore%20the%20association%20between%20model%20performance%20and%0Avarious%20factors%20including%20data%20amount%2C%20composition%20ratio%2C%20model%20size%20and%20SFT%0Astrategies.%20Our%20experiments%20reveal%20that%20distinct%20capabilities%20scale%20differently%0Aand%20larger%20models%20generally%20show%20superior%20performance%20with%20same%20amount%20of%20data.%0AMathematical%20reasoning%20and%20code%20generation%20consistently%20improve%20with%20increasing%0Adata%20amount%2C%20whereas%20general%20abilities%20plateau%20after%20roughly%20a%20thousand%0Asamples.%20Moreover%2C%20we%20observe%20data%20composition%20appears%20to%20enhance%20various%0Aabilities%20under%20limited%20data%20conditions%2C%20yet%20can%20lead%20to%20performance%20conflicts%0Awhen%20data%20is%20plentiful.%20Our%20findings%20also%20suggest%20the%20amount%20of%20composition%0Adata%20influences%20performance%20more%20than%20the%20composition%20ratio.%20In%20analysis%20of%20SFT%0Astrategies%2C%20we%20find%20that%20sequentially%20learning%20multiple%20skills%20risks%0Acatastrophic%20forgetting.%20Our%20proposed%20Dual-stage%20Mixed%20Fine-tuning%20%28DMT%29%0Astrategy%20offers%20a%20promising%20solution%20to%20learn%20multiple%20abilities%20with%20different%0Ascaling%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05492v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Abilities%2520in%2520Large%2520Language%2520Models%2520are%2520Affected%2520by%2520Supervised%250A%2520%2520Fine-tuning%2520Data%2520Composition%26entry.906535625%3DGuanting%2520Dong%2520and%2520Hongyi%2520Yuan%2520and%2520Keming%2520Lu%2520and%2520Chengpeng%2520Li%2520and%2520Mingfeng%2520Xue%2520and%2520Dayiheng%2520Liu%2520and%2520Wei%2520Wang%2520and%2520Zheng%2520Yuan%2520and%2520Chang%2520Zhou%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520with%2520enormous%2520pre-training%2520tokens%2520and%2520parameters%250Aemerge%2520diverse%2520abilities%252C%2520including%2520math%2520reasoning%252C%2520code%2520generation%252C%2520and%250Ainstruction%2520following.%2520These%2520abilities%2520are%2520further%2520enhanced%2520by%2520supervised%250Afine-tuning%2520%2528SFT%2529.%2520While%2520the%2520open-source%2520community%2520has%2520explored%2520ad-hoc%2520SFT%2520for%250Aenhancing%2520individual%2520capabilities%252C%2520proprietary%2520LLMs%2520exhibit%2520versatility%2520across%250Avarious%2520skills.%2520Therefore%252C%2520understanding%2520the%2520facilitation%2520of%2520multiple%2520abilities%250Avia%2520SFT%2520is%2520paramount.%2520In%2520this%2520study%252C%2520we%2520specifically%2520focuses%2520on%2520the%2520interplay%250Aof%2520data%2520composition%2520between%2520mathematical%2520reasoning%252C%2520code%2520generation%252C%2520and%250Ageneral%2520human-aligning%2520abilities%2520during%2520SFT.%2520We%2520propose%2520four%2520intriguing%250Aresearch%2520questions%2520to%2520explore%2520the%2520association%2520between%2520model%2520performance%2520and%250Avarious%2520factors%2520including%2520data%2520amount%252C%2520composition%2520ratio%252C%2520model%2520size%2520and%2520SFT%250Astrategies.%2520Our%2520experiments%2520reveal%2520that%2520distinct%2520capabilities%2520scale%2520differently%250Aand%2520larger%2520models%2520generally%2520show%2520superior%2520performance%2520with%2520same%2520amount%2520of%2520data.%250AMathematical%2520reasoning%2520and%2520code%2520generation%2520consistently%2520improve%2520with%2520increasing%250Adata%2520amount%252C%2520whereas%2520general%2520abilities%2520plateau%2520after%2520roughly%2520a%2520thousand%250Asamples.%2520Moreover%252C%2520we%2520observe%2520data%2520composition%2520appears%2520to%2520enhance%2520various%250Aabilities%2520under%2520limited%2520data%2520conditions%252C%2520yet%2520can%2520lead%2520to%2520performance%2520conflicts%250Awhen%2520data%2520is%2520plentiful.%2520Our%2520findings%2520also%2520suggest%2520the%2520amount%2520of%2520composition%250Adata%2520influences%2520performance%2520more%2520than%2520the%2520composition%2520ratio.%2520In%2520analysis%2520of%2520SFT%250Astrategies%252C%2520we%2520find%2520that%2520sequentially%2520learning%2520multiple%2520skills%2520risks%250Acatastrophic%2520forgetting.%2520Our%2520proposed%2520Dual-stage%2520Mixed%2520Fine-tuning%2520%2528DMT%2529%250Astrategy%2520offers%2520a%2520promising%2520solution%2520to%2520learn%2520multiple%2520abilities%2520with%2520different%250Ascaling%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05492v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Abilities%20in%20Large%20Language%20Models%20are%20Affected%20by%20Supervised%0A%20%20Fine-tuning%20Data%20Composition&entry.906535625=Guanting%20Dong%20and%20Hongyi%20Yuan%20and%20Keming%20Lu%20and%20Chengpeng%20Li%20and%20Mingfeng%20Xue%20and%20Dayiheng%20Liu%20and%20Wei%20Wang%20and%20Zheng%20Yuan%20and%20Chang%20Zhou%20and%20Jingren%20Zhou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20with%20enormous%20pre-training%20tokens%20and%20parameters%0Aemerge%20diverse%20abilities%2C%20including%20math%20reasoning%2C%20code%20generation%2C%20and%0Ainstruction%20following.%20These%20abilities%20are%20further%20enhanced%20by%20supervised%0Afine-tuning%20%28SFT%29.%20While%20the%20open-source%20community%20has%20explored%20ad-hoc%20SFT%20for%0Aenhancing%20individual%20capabilities%2C%20proprietary%20LLMs%20exhibit%20versatility%20across%0Avarious%20skills.%20Therefore%2C%20understanding%20the%20facilitation%20of%20multiple%20abilities%0Avia%20SFT%20is%20paramount.%20In%20this%20study%2C%20we%20specifically%20focuses%20on%20the%20interplay%0Aof%20data%20composition%20between%20mathematical%20reasoning%2C%20code%20generation%2C%20and%0Ageneral%20human-aligning%20abilities%20during%20SFT.%20We%20propose%20four%20intriguing%0Aresearch%20questions%20to%20explore%20the%20association%20between%20model%20performance%20and%0Avarious%20factors%20including%20data%20amount%2C%20composition%20ratio%2C%20model%20size%20and%20SFT%0Astrategies.%20Our%20experiments%20reveal%20that%20distinct%20capabilities%20scale%20differently%0Aand%20larger%20models%20generally%20show%20superior%20performance%20with%20same%20amount%20of%20data.%0AMathematical%20reasoning%20and%20code%20generation%20consistently%20improve%20with%20increasing%0Adata%20amount%2C%20whereas%20general%20abilities%20plateau%20after%20roughly%20a%20thousand%0Asamples.%20Moreover%2C%20we%20observe%20data%20composition%20appears%20to%20enhance%20various%0Aabilities%20under%20limited%20data%20conditions%2C%20yet%20can%20lead%20to%20performance%20conflicts%0Awhen%20data%20is%20plentiful.%20Our%20findings%20also%20suggest%20the%20amount%20of%20composition%0Adata%20influences%20performance%20more%20than%20the%20composition%20ratio.%20In%20analysis%20of%20SFT%0Astrategies%2C%20we%20find%20that%20sequentially%20learning%20multiple%20skills%20risks%0Acatastrophic%20forgetting.%20Our%20proposed%20Dual-stage%20Mixed%20Fine-tuning%20%28DMT%29%0Astrategy%20offers%20a%20promising%20solution%20to%20learn%20multiple%20abilities%20with%20different%0Ascaling%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05492v4&entry.124074799=Read"},
{"title": "Leveraging Activations for Superpixel Explanations", "author": "Ahc\u00e8ne Boubekki and Samuel G. Fadel and Sebastian Mair", "abstract": "  Saliency methods have become standard in the explanation toolkit of deep\nneural networks. Recent developments specific to image classifiers have\ninvestigated region-based explanations with either new methods or by adapting\nwell-established ones using ad-hoc superpixel algorithms. In this paper, we aim\nto avoid relying on these segmenters by extracting a segmentation from the\nactivations of a deep neural network image classifier without fine-tuning the\nnetwork. Our so-called Neuro-Activated Superpixels (NAS) can isolate the\nregions of interest in the input relevant to the model's prediction, which\nboosts high-threshold weakly supervised object localization performance. This\nproperty enables the semi-supervised semantic evaluation of saliency methods.\nThe aggregation of NAS with existing saliency methods eases their\ninterpretation and reveals the inconsistencies of the widely used area under\nthe relevance curve metric.\n", "link": "http://arxiv.org/abs/2406.04933v1", "date": "2024-06-07", "relevancy": 2.0154, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Activations%20for%20Superpixel%20Explanations&body=Title%3A%20Leveraging%20Activations%20for%20Superpixel%20Explanations%0AAuthor%3A%20Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair%0AAbstract%3A%20%20%20Saliency%20methods%20have%20become%20standard%20in%20the%20explanation%20toolkit%20of%20deep%0Aneural%20networks.%20Recent%20developments%20specific%20to%20image%20classifiers%20have%0Ainvestigated%20region-based%20explanations%20with%20either%20new%20methods%20or%20by%20adapting%0Awell-established%20ones%20using%20ad-hoc%20superpixel%20algorithms.%20In%20this%20paper%2C%20we%20aim%0Ato%20avoid%20relying%20on%20these%20segmenters%20by%20extracting%20a%20segmentation%20from%20the%0Aactivations%20of%20a%20deep%20neural%20network%20image%20classifier%20without%20fine-tuning%20the%0Anetwork.%20Our%20so-called%20Neuro-Activated%20Superpixels%20%28NAS%29%20can%20isolate%20the%0Aregions%20of%20interest%20in%20the%20input%20relevant%20to%20the%20model%27s%20prediction%2C%20which%0Aboosts%20high-threshold%20weakly%20supervised%20object%20localization%20performance.%20This%0Aproperty%20enables%20the%20semi-supervised%20semantic%20evaluation%20of%20saliency%20methods.%0AThe%20aggregation%20of%20NAS%20with%20existing%20saliency%20methods%20eases%20their%0Ainterpretation%20and%20reveals%20the%20inconsistencies%20of%20the%20widely%20used%20area%20under%0Athe%20relevance%20curve%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Activations%2520for%2520Superpixel%2520Explanations%26entry.906535625%3DAhc%25C3%25A8ne%2520Boubekki%2520and%2520Samuel%2520G.%2520Fadel%2520and%2520Sebastian%2520Mair%26entry.1292438233%3D%2520%2520Saliency%2520methods%2520have%2520become%2520standard%2520in%2520the%2520explanation%2520toolkit%2520of%2520deep%250Aneural%2520networks.%2520Recent%2520developments%2520specific%2520to%2520image%2520classifiers%2520have%250Ainvestigated%2520region-based%2520explanations%2520with%2520either%2520new%2520methods%2520or%2520by%2520adapting%250Awell-established%2520ones%2520using%2520ad-hoc%2520superpixel%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520aim%250Ato%2520avoid%2520relying%2520on%2520these%2520segmenters%2520by%2520extracting%2520a%2520segmentation%2520from%2520the%250Aactivations%2520of%2520a%2520deep%2520neural%2520network%2520image%2520classifier%2520without%2520fine-tuning%2520the%250Anetwork.%2520Our%2520so-called%2520Neuro-Activated%2520Superpixels%2520%2528NAS%2529%2520can%2520isolate%2520the%250Aregions%2520of%2520interest%2520in%2520the%2520input%2520relevant%2520to%2520the%2520model%2527s%2520prediction%252C%2520which%250Aboosts%2520high-threshold%2520weakly%2520supervised%2520object%2520localization%2520performance.%2520This%250Aproperty%2520enables%2520the%2520semi-supervised%2520semantic%2520evaluation%2520of%2520saliency%2520methods.%250AThe%2520aggregation%2520of%2520NAS%2520with%2520existing%2520saliency%2520methods%2520eases%2520their%250Ainterpretation%2520and%2520reveals%2520the%2520inconsistencies%2520of%2520the%2520widely%2520used%2520area%2520under%250Athe%2520relevance%2520curve%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Activations%20for%20Superpixel%20Explanations&entry.906535625=Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair&entry.1292438233=%20%20Saliency%20methods%20have%20become%20standard%20in%20the%20explanation%20toolkit%20of%20deep%0Aneural%20networks.%20Recent%20developments%20specific%20to%20image%20classifiers%20have%0Ainvestigated%20region-based%20explanations%20with%20either%20new%20methods%20or%20by%20adapting%0Awell-established%20ones%20using%20ad-hoc%20superpixel%20algorithms.%20In%20this%20paper%2C%20we%20aim%0Ato%20avoid%20relying%20on%20these%20segmenters%20by%20extracting%20a%20segmentation%20from%20the%0Aactivations%20of%20a%20deep%20neural%20network%20image%20classifier%20without%20fine-tuning%20the%0Anetwork.%20Our%20so-called%20Neuro-Activated%20Superpixels%20%28NAS%29%20can%20isolate%20the%0Aregions%20of%20interest%20in%20the%20input%20relevant%20to%20the%20model%27s%20prediction%2C%20which%0Aboosts%20high-threshold%20weakly%20supervised%20object%20localization%20performance.%20This%0Aproperty%20enables%20the%20semi-supervised%20semantic%20evaluation%20of%20saliency%20methods.%0AThe%20aggregation%20of%20NAS%20with%20existing%20saliency%20methods%20eases%20their%0Ainterpretation%20and%20reveals%20the%20inconsistencies%20of%20the%20widely%20used%20area%20under%0Athe%20relevance%20curve%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04933v1&entry.124074799=Read"},
{"title": "Clarifying Myths About the Relationship Between Shape Bias, Accuracy,\n  and Robustness", "author": "Zahra Golpayegani and Patrick St-Amant and Nizar Bouguila", "abstract": "  Deep learning models can perform well when evaluated on images from the same\ndistribution as the training set. However, applying small perturbations in the\nforms of noise, artifacts, occlusions, blurring, etc. to a model's input image\nand feeding the model with out-of-distribution (OOD) data can significantly\ndrop the model's accuracy, making it not applicable to real-world scenarios.\nData augmentation is one of the well-practiced methods to improve model\nrobustness against OOD data; however, examining which augmentation type to\nchoose and how it affects the OOD robustness remains understudied. There is a\ngrowing belief that augmenting datasets using data augmentations that improve a\nmodel's bias to shape-based features rather than texture-based features results\nin increased OOD robustness for Convolutional Neural Networks trained on the\nImageNet-1K dataset. This is usually stated as ``an increase in the model's\nshape bias results in an increase in its OOD robustness\". Based on this\nhypothesis, some works in the literature aim to find augmentations with higher\neffects on model shape bias and use those for data augmentation. By evaluating\n39 types of data augmentations on a widely used OOD dataset, we demonstrate the\nimpact of each data augmentation on the model's robustness to OOD data and\nfurther show that the mentioned hypothesis is not true; an increase in shape\nbias does not necessarily result in higher OOD robustness. By analyzing the\nresults, we also find some biases in the ImageNet-1K dataset that can easily be\nreduced using proper data augmentation. Our evaluation results further show\nthat there is not necessarily a trade-off between in-domain accuracy and OOD\nrobustness, and choosing the proper augmentations can help increase both\nin-domain accuracy and OOD robustness simultaneously.\n", "link": "http://arxiv.org/abs/2406.05006v1", "date": "2024-06-07", "relevancy": 1.9947, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clarifying%20Myths%20About%20the%20Relationship%20Between%20Shape%20Bias%2C%20Accuracy%2C%0A%20%20and%20Robustness&body=Title%3A%20Clarifying%20Myths%20About%20the%20Relationship%20Between%20Shape%20Bias%2C%20Accuracy%2C%0A%20%20and%20Robustness%0AAuthor%3A%20Zahra%20Golpayegani%20and%20Patrick%20St-Amant%20and%20Nizar%20Bouguila%0AAbstract%3A%20%20%20Deep%20learning%20models%20can%20perform%20well%20when%20evaluated%20on%20images%20from%20the%20same%0Adistribution%20as%20the%20training%20set.%20However%2C%20applying%20small%20perturbations%20in%20the%0Aforms%20of%20noise%2C%20artifacts%2C%20occlusions%2C%20blurring%2C%20etc.%20to%20a%20model%27s%20input%20image%0Aand%20feeding%20the%20model%20with%20out-of-distribution%20%28OOD%29%20data%20can%20significantly%0Adrop%20the%20model%27s%20accuracy%2C%20making%20it%20not%20applicable%20to%20real-world%20scenarios.%0AData%20augmentation%20is%20one%20of%20the%20well-practiced%20methods%20to%20improve%20model%0Arobustness%20against%20OOD%20data%3B%20however%2C%20examining%20which%20augmentation%20type%20to%0Achoose%20and%20how%20it%20affects%20the%20OOD%20robustness%20remains%20understudied.%20There%20is%20a%0Agrowing%20belief%20that%20augmenting%20datasets%20using%20data%20augmentations%20that%20improve%20a%0Amodel%27s%20bias%20to%20shape-based%20features%20rather%20than%20texture-based%20features%20results%0Ain%20increased%20OOD%20robustness%20for%20Convolutional%20Neural%20Networks%20trained%20on%20the%0AImageNet-1K%20dataset.%20This%20is%20usually%20stated%20as%20%60%60an%20increase%20in%20the%20model%27s%0Ashape%20bias%20results%20in%20an%20increase%20in%20its%20OOD%20robustness%22.%20Based%20on%20this%0Ahypothesis%2C%20some%20works%20in%20the%20literature%20aim%20to%20find%20augmentations%20with%20higher%0Aeffects%20on%20model%20shape%20bias%20and%20use%20those%20for%20data%20augmentation.%20By%20evaluating%0A39%20types%20of%20data%20augmentations%20on%20a%20widely%20used%20OOD%20dataset%2C%20we%20demonstrate%20the%0Aimpact%20of%20each%20data%20augmentation%20on%20the%20model%27s%20robustness%20to%20OOD%20data%20and%0Afurther%20show%20that%20the%20mentioned%20hypothesis%20is%20not%20true%3B%20an%20increase%20in%20shape%0Abias%20does%20not%20necessarily%20result%20in%20higher%20OOD%20robustness.%20By%20analyzing%20the%0Aresults%2C%20we%20also%20find%20some%20biases%20in%20the%20ImageNet-1K%20dataset%20that%20can%20easily%20be%0Areduced%20using%20proper%20data%20augmentation.%20Our%20evaluation%20results%20further%20show%0Athat%20there%20is%20not%20necessarily%20a%20trade-off%20between%20in-domain%20accuracy%20and%20OOD%0Arobustness%2C%20and%20choosing%20the%20proper%20augmentations%20can%20help%20increase%20both%0Ain-domain%20accuracy%20and%20OOD%20robustness%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClarifying%2520Myths%2520About%2520the%2520Relationship%2520Between%2520Shape%2520Bias%252C%2520Accuracy%252C%250A%2520%2520and%2520Robustness%26entry.906535625%3DZahra%2520Golpayegani%2520and%2520Patrick%2520St-Amant%2520and%2520Nizar%2520Bouguila%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520can%2520perform%2520well%2520when%2520evaluated%2520on%2520images%2520from%2520the%2520same%250Adistribution%2520as%2520the%2520training%2520set.%2520However%252C%2520applying%2520small%2520perturbations%2520in%2520the%250Aforms%2520of%2520noise%252C%2520artifacts%252C%2520occlusions%252C%2520blurring%252C%2520etc.%2520to%2520a%2520model%2527s%2520input%2520image%250Aand%2520feeding%2520the%2520model%2520with%2520out-of-distribution%2520%2528OOD%2529%2520data%2520can%2520significantly%250Adrop%2520the%2520model%2527s%2520accuracy%252C%2520making%2520it%2520not%2520applicable%2520to%2520real-world%2520scenarios.%250AData%2520augmentation%2520is%2520one%2520of%2520the%2520well-practiced%2520methods%2520to%2520improve%2520model%250Arobustness%2520against%2520OOD%2520data%253B%2520however%252C%2520examining%2520which%2520augmentation%2520type%2520to%250Achoose%2520and%2520how%2520it%2520affects%2520the%2520OOD%2520robustness%2520remains%2520understudied.%2520There%2520is%2520a%250Agrowing%2520belief%2520that%2520augmenting%2520datasets%2520using%2520data%2520augmentations%2520that%2520improve%2520a%250Amodel%2527s%2520bias%2520to%2520shape-based%2520features%2520rather%2520than%2520texture-based%2520features%2520results%250Ain%2520increased%2520OOD%2520robustness%2520for%2520Convolutional%2520Neural%2520Networks%2520trained%2520on%2520the%250AImageNet-1K%2520dataset.%2520This%2520is%2520usually%2520stated%2520as%2520%2560%2560an%2520increase%2520in%2520the%2520model%2527s%250Ashape%2520bias%2520results%2520in%2520an%2520increase%2520in%2520its%2520OOD%2520robustness%2522.%2520Based%2520on%2520this%250Ahypothesis%252C%2520some%2520works%2520in%2520the%2520literature%2520aim%2520to%2520find%2520augmentations%2520with%2520higher%250Aeffects%2520on%2520model%2520shape%2520bias%2520and%2520use%2520those%2520for%2520data%2520augmentation.%2520By%2520evaluating%250A39%2520types%2520of%2520data%2520augmentations%2520on%2520a%2520widely%2520used%2520OOD%2520dataset%252C%2520we%2520demonstrate%2520the%250Aimpact%2520of%2520each%2520data%2520augmentation%2520on%2520the%2520model%2527s%2520robustness%2520to%2520OOD%2520data%2520and%250Afurther%2520show%2520that%2520the%2520mentioned%2520hypothesis%2520is%2520not%2520true%253B%2520an%2520increase%2520in%2520shape%250Abias%2520does%2520not%2520necessarily%2520result%2520in%2520higher%2520OOD%2520robustness.%2520By%2520analyzing%2520the%250Aresults%252C%2520we%2520also%2520find%2520some%2520biases%2520in%2520the%2520ImageNet-1K%2520dataset%2520that%2520can%2520easily%2520be%250Areduced%2520using%2520proper%2520data%2520augmentation.%2520Our%2520evaluation%2520results%2520further%2520show%250Athat%2520there%2520is%2520not%2520necessarily%2520a%2520trade-off%2520between%2520in-domain%2520accuracy%2520and%2520OOD%250Arobustness%252C%2520and%2520choosing%2520the%2520proper%2520augmentations%2520can%2520help%2520increase%2520both%250Ain-domain%2520accuracy%2520and%2520OOD%2520robustness%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clarifying%20Myths%20About%20the%20Relationship%20Between%20Shape%20Bias%2C%20Accuracy%2C%0A%20%20and%20Robustness&entry.906535625=Zahra%20Golpayegani%20and%20Patrick%20St-Amant%20and%20Nizar%20Bouguila&entry.1292438233=%20%20Deep%20learning%20models%20can%20perform%20well%20when%20evaluated%20on%20images%20from%20the%20same%0Adistribution%20as%20the%20training%20set.%20However%2C%20applying%20small%20perturbations%20in%20the%0Aforms%20of%20noise%2C%20artifacts%2C%20occlusions%2C%20blurring%2C%20etc.%20to%20a%20model%27s%20input%20image%0Aand%20feeding%20the%20model%20with%20out-of-distribution%20%28OOD%29%20data%20can%20significantly%0Adrop%20the%20model%27s%20accuracy%2C%20making%20it%20not%20applicable%20to%20real-world%20scenarios.%0AData%20augmentation%20is%20one%20of%20the%20well-practiced%20methods%20to%20improve%20model%0Arobustness%20against%20OOD%20data%3B%20however%2C%20examining%20which%20augmentation%20type%20to%0Achoose%20and%20how%20it%20affects%20the%20OOD%20robustness%20remains%20understudied.%20There%20is%20a%0Agrowing%20belief%20that%20augmenting%20datasets%20using%20data%20augmentations%20that%20improve%20a%0Amodel%27s%20bias%20to%20shape-based%20features%20rather%20than%20texture-based%20features%20results%0Ain%20increased%20OOD%20robustness%20for%20Convolutional%20Neural%20Networks%20trained%20on%20the%0AImageNet-1K%20dataset.%20This%20is%20usually%20stated%20as%20%60%60an%20increase%20in%20the%20model%27s%0Ashape%20bias%20results%20in%20an%20increase%20in%20its%20OOD%20robustness%22.%20Based%20on%20this%0Ahypothesis%2C%20some%20works%20in%20the%20literature%20aim%20to%20find%20augmentations%20with%20higher%0Aeffects%20on%20model%20shape%20bias%20and%20use%20those%20for%20data%20augmentation.%20By%20evaluating%0A39%20types%20of%20data%20augmentations%20on%20a%20widely%20used%20OOD%20dataset%2C%20we%20demonstrate%20the%0Aimpact%20of%20each%20data%20augmentation%20on%20the%20model%27s%20robustness%20to%20OOD%20data%20and%0Afurther%20show%20that%20the%20mentioned%20hypothesis%20is%20not%20true%3B%20an%20increase%20in%20shape%0Abias%20does%20not%20necessarily%20result%20in%20higher%20OOD%20robustness.%20By%20analyzing%20the%0Aresults%2C%20we%20also%20find%20some%20biases%20in%20the%20ImageNet-1K%20dataset%20that%20can%20easily%20be%0Areduced%20using%20proper%20data%20augmentation.%20Our%20evaluation%20results%20further%20show%0Athat%20there%20is%20not%20necessarily%20a%20trade-off%20between%20in-domain%20accuracy%20and%20OOD%0Arobustness%2C%20and%20choosing%20the%20proper%20augmentations%20can%20help%20increase%20both%0Ain-domain%20accuracy%20and%20OOD%20robustness%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05006v1&entry.124074799=Read"},
{"title": "On the Independence Assumption in Neurosymbolic Learning", "author": "Emile van Krieken and Pasquale Minervini and Edoardo M. Ponti and Antonio Vergari", "abstract": "  State-of-the-art neurosymbolic learning systems use probabilistic reasoning\nto guide neural networks towards predictions that conform to logical\nconstraints over symbols. Many such systems assume that the probabilities of\nthe considered symbols are conditionally independent given the input to\nsimplify learning and reasoning. We study and criticise this assumption,\nhighlighting how it can hinder optimisation and prevent uncertainty\nquantification. We prove that loss functions bias conditionally independent\nneural networks to become overconfident in their predictions. As a result, they\nare unable to represent uncertainty over multiple valid options. Furthermore,\nwe prove that these loss functions are difficult to optimise: they are\nnon-convex, and their minima are usually highly disconnected. Our theoretical\nanalysis gives the foundation for replacing the conditional independence\nassumption and designing more expressive neurosymbolic probabilistic models.\n", "link": "http://arxiv.org/abs/2404.08458v2", "date": "2024-06-07", "relevancy": 1.9905, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5243}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning&body=Title%3A%20On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning%0AAuthor%3A%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini%20and%20Edoardo%20M.%20Ponti%20and%20Antonio%20Vergari%0AAbstract%3A%20%20%20State-of-the-art%20neurosymbolic%20learning%20systems%20use%20probabilistic%20reasoning%0Ato%20guide%20neural%20networks%20towards%20predictions%20that%20conform%20to%20logical%0Aconstraints%20over%20symbols.%20Many%20such%20systems%20assume%20that%20the%20probabilities%20of%0Athe%20considered%20symbols%20are%20conditionally%20independent%20given%20the%20input%20to%0Asimplify%20learning%20and%20reasoning.%20We%20study%20and%20criticise%20this%20assumption%2C%0Ahighlighting%20how%20it%20can%20hinder%20optimisation%20and%20prevent%20uncertainty%0Aquantification.%20We%20prove%20that%20loss%20functions%20bias%20conditionally%20independent%0Aneural%20networks%20to%20become%20overconfident%20in%20their%20predictions.%20As%20a%20result%2C%20they%0Aare%20unable%20to%20represent%20uncertainty%20over%20multiple%20valid%20options.%20Furthermore%2C%0Awe%20prove%20that%20these%20loss%20functions%20are%20difficult%20to%20optimise%3A%20they%20are%0Anon-convex%2C%20and%20their%20minima%20are%20usually%20highly%20disconnected.%20Our%20theoretical%0Aanalysis%20gives%20the%20foundation%20for%20replacing%20the%20conditional%20independence%0Aassumption%20and%20designing%20more%20expressive%20neurosymbolic%20probabilistic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Independence%2520Assumption%2520in%2520Neurosymbolic%2520Learning%26entry.906535625%3DEmile%2520van%2520Krieken%2520and%2520Pasquale%2520Minervini%2520and%2520Edoardo%2520M.%2520Ponti%2520and%2520Antonio%2520Vergari%26entry.1292438233%3D%2520%2520State-of-the-art%2520neurosymbolic%2520learning%2520systems%2520use%2520probabilistic%2520reasoning%250Ato%2520guide%2520neural%2520networks%2520towards%2520predictions%2520that%2520conform%2520to%2520logical%250Aconstraints%2520over%2520symbols.%2520Many%2520such%2520systems%2520assume%2520that%2520the%2520probabilities%2520of%250Athe%2520considered%2520symbols%2520are%2520conditionally%2520independent%2520given%2520the%2520input%2520to%250Asimplify%2520learning%2520and%2520reasoning.%2520We%2520study%2520and%2520criticise%2520this%2520assumption%252C%250Ahighlighting%2520how%2520it%2520can%2520hinder%2520optimisation%2520and%2520prevent%2520uncertainty%250Aquantification.%2520We%2520prove%2520that%2520loss%2520functions%2520bias%2520conditionally%2520independent%250Aneural%2520networks%2520to%2520become%2520overconfident%2520in%2520their%2520predictions.%2520As%2520a%2520result%252C%2520they%250Aare%2520unable%2520to%2520represent%2520uncertainty%2520over%2520multiple%2520valid%2520options.%2520Furthermore%252C%250Awe%2520prove%2520that%2520these%2520loss%2520functions%2520are%2520difficult%2520to%2520optimise%253A%2520they%2520are%250Anon-convex%252C%2520and%2520their%2520minima%2520are%2520usually%2520highly%2520disconnected.%2520Our%2520theoretical%250Aanalysis%2520gives%2520the%2520foundation%2520for%2520replacing%2520the%2520conditional%2520independence%250Aassumption%2520and%2520designing%2520more%2520expressive%2520neurosymbolic%2520probabilistic%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Independence%20Assumption%20in%20Neurosymbolic%20Learning&entry.906535625=Emile%20van%20Krieken%20and%20Pasquale%20Minervini%20and%20Edoardo%20M.%20Ponti%20and%20Antonio%20Vergari&entry.1292438233=%20%20State-of-the-art%20neurosymbolic%20learning%20systems%20use%20probabilistic%20reasoning%0Ato%20guide%20neural%20networks%20towards%20predictions%20that%20conform%20to%20logical%0Aconstraints%20over%20symbols.%20Many%20such%20systems%20assume%20that%20the%20probabilities%20of%0Athe%20considered%20symbols%20are%20conditionally%20independent%20given%20the%20input%20to%0Asimplify%20learning%20and%20reasoning.%20We%20study%20and%20criticise%20this%20assumption%2C%0Ahighlighting%20how%20it%20can%20hinder%20optimisation%20and%20prevent%20uncertainty%0Aquantification.%20We%20prove%20that%20loss%20functions%20bias%20conditionally%20independent%0Aneural%20networks%20to%20become%20overconfident%20in%20their%20predictions.%20As%20a%20result%2C%20they%0Aare%20unable%20to%20represent%20uncertainty%20over%20multiple%20valid%20options.%20Furthermore%2C%0Awe%20prove%20that%20these%20loss%20functions%20are%20difficult%20to%20optimise%3A%20they%20are%0Anon-convex%2C%20and%20their%20minima%20are%20usually%20highly%20disconnected.%20Our%20theoretical%0Aanalysis%20gives%20the%20foundation%20for%20replacing%20the%20conditional%20independence%0Aassumption%20and%20designing%20more%20expressive%20neurosymbolic%20probabilistic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08458v2&entry.124074799=Read"},
{"title": "Online Adaptation for Enhancing Imitation Learning Policies", "author": "Federico Malato and Ville Hautamaki", "abstract": "  Imitation learning enables autonomous agents to learn from human examples,\nwithout the need for a reward signal. Still, if the provided dataset does not\nencapsulate the task correctly, or when the task is too complex to be modeled,\nsuch agents fail to reproduce the expert policy. We propose to recover from\nthese failures through online adaptation. Our approach combines the action\nproposal coming from a pre-trained policy with relevant experience recorded by\nan expert. The combination results in an adapted action that closely follows\nthe expert. Our experiments show that an adapted agent performs better than its\npure imitation learning counterpart. Notably, adapted agents can achieve\nreasonable performance even when the base, non-adapted policy catastrophically\nfails.\n", "link": "http://arxiv.org/abs/2406.04913v1", "date": "2024-06-07", "relevancy": 1.989, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5433}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Adaptation%20for%20Enhancing%20Imitation%20Learning%20Policies&body=Title%3A%20Online%20Adaptation%20for%20Enhancing%20Imitation%20Learning%20Policies%0AAuthor%3A%20Federico%20Malato%20and%20Ville%20Hautamaki%0AAbstract%3A%20%20%20Imitation%20learning%20enables%20autonomous%20agents%20to%20learn%20from%20human%20examples%2C%0Awithout%20the%20need%20for%20a%20reward%20signal.%20Still%2C%20if%20the%20provided%20dataset%20does%20not%0Aencapsulate%20the%20task%20correctly%2C%20or%20when%20the%20task%20is%20too%20complex%20to%20be%20modeled%2C%0Asuch%20agents%20fail%20to%20reproduce%20the%20expert%20policy.%20We%20propose%20to%20recover%20from%0Athese%20failures%20through%20online%20adaptation.%20Our%20approach%20combines%20the%20action%0Aproposal%20coming%20from%20a%20pre-trained%20policy%20with%20relevant%20experience%20recorded%20by%0Aan%20expert.%20The%20combination%20results%20in%20an%20adapted%20action%20that%20closely%20follows%0Athe%20expert.%20Our%20experiments%20show%20that%20an%20adapted%20agent%20performs%20better%20than%20its%0Apure%20imitation%20learning%20counterpart.%20Notably%2C%20adapted%20agents%20can%20achieve%0Areasonable%20performance%20even%20when%20the%20base%2C%20non-adapted%20policy%20catastrophically%0Afails.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Adaptation%2520for%2520Enhancing%2520Imitation%2520Learning%2520Policies%26entry.906535625%3DFederico%2520Malato%2520and%2520Ville%2520Hautamaki%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520enables%2520autonomous%2520agents%2520to%2520learn%2520from%2520human%2520examples%252C%250Awithout%2520the%2520need%2520for%2520a%2520reward%2520signal.%2520Still%252C%2520if%2520the%2520provided%2520dataset%2520does%2520not%250Aencapsulate%2520the%2520task%2520correctly%252C%2520or%2520when%2520the%2520task%2520is%2520too%2520complex%2520to%2520be%2520modeled%252C%250Asuch%2520agents%2520fail%2520to%2520reproduce%2520the%2520expert%2520policy.%2520We%2520propose%2520to%2520recover%2520from%250Athese%2520failures%2520through%2520online%2520adaptation.%2520Our%2520approach%2520combines%2520the%2520action%250Aproposal%2520coming%2520from%2520a%2520pre-trained%2520policy%2520with%2520relevant%2520experience%2520recorded%2520by%250Aan%2520expert.%2520The%2520combination%2520results%2520in%2520an%2520adapted%2520action%2520that%2520closely%2520follows%250Athe%2520expert.%2520Our%2520experiments%2520show%2520that%2520an%2520adapted%2520agent%2520performs%2520better%2520than%2520its%250Apure%2520imitation%2520learning%2520counterpart.%2520Notably%252C%2520adapted%2520agents%2520can%2520achieve%250Areasonable%2520performance%2520even%2520when%2520the%2520base%252C%2520non-adapted%2520policy%2520catastrophically%250Afails.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Adaptation%20for%20Enhancing%20Imitation%20Learning%20Policies&entry.906535625=Federico%20Malato%20and%20Ville%20Hautamaki&entry.1292438233=%20%20Imitation%20learning%20enables%20autonomous%20agents%20to%20learn%20from%20human%20examples%2C%0Awithout%20the%20need%20for%20a%20reward%20signal.%20Still%2C%20if%20the%20provided%20dataset%20does%20not%0Aencapsulate%20the%20task%20correctly%2C%20or%20when%20the%20task%20is%20too%20complex%20to%20be%20modeled%2C%0Asuch%20agents%20fail%20to%20reproduce%20the%20expert%20policy.%20We%20propose%20to%20recover%20from%0Athese%20failures%20through%20online%20adaptation.%20Our%20approach%20combines%20the%20action%0Aproposal%20coming%20from%20a%20pre-trained%20policy%20with%20relevant%20experience%20recorded%20by%0Aan%20expert.%20The%20combination%20results%20in%20an%20adapted%20action%20that%20closely%20follows%0Athe%20expert.%20Our%20experiments%20show%20that%20an%20adapted%20agent%20performs%20better%20than%20its%0Apure%20imitation%20learning%20counterpart.%20Notably%2C%20adapted%20agents%20can%20achieve%0Areasonable%20performance%20even%20when%20the%20base%2C%20non-adapted%20policy%20catastrophically%0Afails.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04913v1&entry.124074799=Read"},
{"title": "Hibou: A Family of Foundational Vision Transformers for Pathology", "author": "Dmitry Nechaev and Alexey Pchelnikov and Ekaterina Ivanova", "abstract": "  Pathology, the microscopic examination of diseased tissue, is critical for\ndiagnosing various medical conditions, particularly cancers. Traditional\nmethods are labor-intensive and prone to human error. Digital pathology, which\nconverts glass slides into high-resolution digital images for analysis by\ncomputer algorithms, revolutionizes the field by enhancing diagnostic accuracy,\nconsistency, and efficiency through automated image analysis and large-scale\ndata processing. Foundational transformer pretraining is crucial for developing\nrobust, generalizable models as it enables learning from vast amounts of\nunannotated data.\n  This paper introduces the Hibou family of foundational vision transformers\nfor pathology, leveraging the DINOv2 framework to pretrain two model variants,\nHibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide\nimages (WSIs) representing diverse tissue types and staining techniques. Our\npretrained models demonstrate superior performance on both patch-level and\nslide-level benchmarks, surpassing existing state-of-the-art methods. Notably,\nHibou-L achieves the highest average accuracy across multiple benchmark\ndatasets. To support further research and application in the field, we have\nopen-sourced the Hibou-B model, which can be accessed at\nhttps://github.com/HistAI/hibou\n", "link": "http://arxiv.org/abs/2406.05074v1", "date": "2024-06-07", "relevancy": 1.9879, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology&body=Title%3A%20Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology%0AAuthor%3A%20Dmitry%20Nechaev%20and%20Alexey%20Pchelnikov%20and%20Ekaterina%20Ivanova%0AAbstract%3A%20%20%20Pathology%2C%20the%20microscopic%20examination%20of%20diseased%20tissue%2C%20is%20critical%20for%0Adiagnosing%20various%20medical%20conditions%2C%20particularly%20cancers.%20Traditional%0Amethods%20are%20labor-intensive%20and%20prone%20to%20human%20error.%20Digital%20pathology%2C%20which%0Aconverts%20glass%20slides%20into%20high-resolution%20digital%20images%20for%20analysis%20by%0Acomputer%20algorithms%2C%20revolutionizes%20the%20field%20by%20enhancing%20diagnostic%20accuracy%2C%0Aconsistency%2C%20and%20efficiency%20through%20automated%20image%20analysis%20and%20large-scale%0Adata%20processing.%20Foundational%20transformer%20pretraining%20is%20crucial%20for%20developing%0Arobust%2C%20generalizable%20models%20as%20it%20enables%20learning%20from%20vast%20amounts%20of%0Aunannotated%20data.%0A%20%20This%20paper%20introduces%20the%20Hibou%20family%20of%20foundational%20vision%20transformers%0Afor%20pathology%2C%20leveraging%20the%20DINOv2%20framework%20to%20pretrain%20two%20model%20variants%2C%0AHibou-B%20and%20Hibou-L%2C%20on%20a%20proprietary%20dataset%20of%20over%201%20million%20whole%20slide%0Aimages%20%28WSIs%29%20representing%20diverse%20tissue%20types%20and%20staining%20techniques.%20Our%0Apretrained%20models%20demonstrate%20superior%20performance%20on%20both%20patch-level%20and%0Aslide-level%20benchmarks%2C%20surpassing%20existing%20state-of-the-art%20methods.%20Notably%2C%0AHibou-L%20achieves%20the%20highest%20average%20accuracy%20across%20multiple%20benchmark%0Adatasets.%20To%20support%20further%20research%20and%20application%20in%20the%20field%2C%20we%20have%0Aopen-sourced%20the%20Hibou-B%20model%2C%20which%20can%20be%20accessed%20at%0Ahttps%3A//github.com/HistAI/hibou%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHibou%253A%2520A%2520Family%2520of%2520Foundational%2520Vision%2520Transformers%2520for%2520Pathology%26entry.906535625%3DDmitry%2520Nechaev%2520and%2520Alexey%2520Pchelnikov%2520and%2520Ekaterina%2520Ivanova%26entry.1292438233%3D%2520%2520Pathology%252C%2520the%2520microscopic%2520examination%2520of%2520diseased%2520tissue%252C%2520is%2520critical%2520for%250Adiagnosing%2520various%2520medical%2520conditions%252C%2520particularly%2520cancers.%2520Traditional%250Amethods%2520are%2520labor-intensive%2520and%2520prone%2520to%2520human%2520error.%2520Digital%2520pathology%252C%2520which%250Aconverts%2520glass%2520slides%2520into%2520high-resolution%2520digital%2520images%2520for%2520analysis%2520by%250Acomputer%2520algorithms%252C%2520revolutionizes%2520the%2520field%2520by%2520enhancing%2520diagnostic%2520accuracy%252C%250Aconsistency%252C%2520and%2520efficiency%2520through%2520automated%2520image%2520analysis%2520and%2520large-scale%250Adata%2520processing.%2520Foundational%2520transformer%2520pretraining%2520is%2520crucial%2520for%2520developing%250Arobust%252C%2520generalizable%2520models%2520as%2520it%2520enables%2520learning%2520from%2520vast%2520amounts%2520of%250Aunannotated%2520data.%250A%2520%2520This%2520paper%2520introduces%2520the%2520Hibou%2520family%2520of%2520foundational%2520vision%2520transformers%250Afor%2520pathology%252C%2520leveraging%2520the%2520DINOv2%2520framework%2520to%2520pretrain%2520two%2520model%2520variants%252C%250AHibou-B%2520and%2520Hibou-L%252C%2520on%2520a%2520proprietary%2520dataset%2520of%2520over%25201%2520million%2520whole%2520slide%250Aimages%2520%2528WSIs%2529%2520representing%2520diverse%2520tissue%2520types%2520and%2520staining%2520techniques.%2520Our%250Apretrained%2520models%2520demonstrate%2520superior%2520performance%2520on%2520both%2520patch-level%2520and%250Aslide-level%2520benchmarks%252C%2520surpassing%2520existing%2520state-of-the-art%2520methods.%2520Notably%252C%250AHibou-L%2520achieves%2520the%2520highest%2520average%2520accuracy%2520across%2520multiple%2520benchmark%250Adatasets.%2520To%2520support%2520further%2520research%2520and%2520application%2520in%2520the%2520field%252C%2520we%2520have%250Aopen-sourced%2520the%2520Hibou-B%2520model%252C%2520which%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/HistAI/hibou%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology&entry.906535625=Dmitry%20Nechaev%20and%20Alexey%20Pchelnikov%20and%20Ekaterina%20Ivanova&entry.1292438233=%20%20Pathology%2C%20the%20microscopic%20examination%20of%20diseased%20tissue%2C%20is%20critical%20for%0Adiagnosing%20various%20medical%20conditions%2C%20particularly%20cancers.%20Traditional%0Amethods%20are%20labor-intensive%20and%20prone%20to%20human%20error.%20Digital%20pathology%2C%20which%0Aconverts%20glass%20slides%20into%20high-resolution%20digital%20images%20for%20analysis%20by%0Acomputer%20algorithms%2C%20revolutionizes%20the%20field%20by%20enhancing%20diagnostic%20accuracy%2C%0Aconsistency%2C%20and%20efficiency%20through%20automated%20image%20analysis%20and%20large-scale%0Adata%20processing.%20Foundational%20transformer%20pretraining%20is%20crucial%20for%20developing%0Arobust%2C%20generalizable%20models%20as%20it%20enables%20learning%20from%20vast%20amounts%20of%0Aunannotated%20data.%0A%20%20This%20paper%20introduces%20the%20Hibou%20family%20of%20foundational%20vision%20transformers%0Afor%20pathology%2C%20leveraging%20the%20DINOv2%20framework%20to%20pretrain%20two%20model%20variants%2C%0AHibou-B%20and%20Hibou-L%2C%20on%20a%20proprietary%20dataset%20of%20over%201%20million%20whole%20slide%0Aimages%20%28WSIs%29%20representing%20diverse%20tissue%20types%20and%20staining%20techniques.%20Our%0Apretrained%20models%20demonstrate%20superior%20performance%20on%20both%20patch-level%20and%0Aslide-level%20benchmarks%2C%20surpassing%20existing%20state-of-the-art%20methods.%20Notably%2C%0AHibou-L%20achieves%20the%20highest%20average%20accuracy%20across%20multiple%20benchmark%0Adatasets.%20To%20support%20further%20research%20and%20application%20in%20the%20field%2C%20we%20have%0Aopen-sourced%20the%20Hibou-B%20model%2C%20which%20can%20be%20accessed%20at%0Ahttps%3A//github.com/HistAI/hibou%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05074v1&entry.124074799=Read"},
{"title": "Labeled Data Selection for Category Discovery", "author": "Bingchen Zhao and Nico Lang and Serge Belongie and Oisin Mac Aodha", "abstract": "  Category discovery methods aim to find novel categories in unlabeled visual\ndata. At training time, a set of labeled and unlabeled images are provided,\nwhere the labels correspond to the categories present in the images. The\nlabeled data provides guidance during training by indicating what types of\nvisual properties and features are relevant for performing discovery in the\nunlabeled data. As a result, changing the categories present in the labeled set\ncan have a large impact on what is ultimately discovered in the unlabeled set.\nDespite its importance, the impact of labeled data selection has not been\nexplored in the category discovery literature to date. We show that changing\nthe labeled data can significantly impact discovery performance. Motivated by\nthis, we propose two new approaches for automatically selecting the most\nsuitable labeled data based on the similarity between the labeled and unlabeled\ndata. Our observation is that, unlike in conventional supervised transfer\nlearning, the best labeled is neither too similar, nor too dissimilar, to the\nunlabeled categories. Our resulting approaches obtains state-of-the-art\ndiscovery performance across a range of challenging fine-grained benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2406.04898v1", "date": "2024-06-07", "relevancy": 1.9844, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Labeled%20Data%20Selection%20for%20Category%20Discovery&body=Title%3A%20Labeled%20Data%20Selection%20for%20Category%20Discovery%0AAuthor%3A%20Bingchen%20Zhao%20and%20Nico%20Lang%20and%20Serge%20Belongie%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20Category%20discovery%20methods%20aim%20to%20find%20novel%20categories%20in%20unlabeled%20visual%0Adata.%20At%20training%20time%2C%20a%20set%20of%20labeled%20and%20unlabeled%20images%20are%20provided%2C%0Awhere%20the%20labels%20correspond%20to%20the%20categories%20present%20in%20the%20images.%20The%0Alabeled%20data%20provides%20guidance%20during%20training%20by%20indicating%20what%20types%20of%0Avisual%20properties%20and%20features%20are%20relevant%20for%20performing%20discovery%20in%20the%0Aunlabeled%20data.%20As%20a%20result%2C%20changing%20the%20categories%20present%20in%20the%20labeled%20set%0Acan%20have%20a%20large%20impact%20on%20what%20is%20ultimately%20discovered%20in%20the%20unlabeled%20set.%0ADespite%20its%20importance%2C%20the%20impact%20of%20labeled%20data%20selection%20has%20not%20been%0Aexplored%20in%20the%20category%20discovery%20literature%20to%20date.%20We%20show%20that%20changing%0Athe%20labeled%20data%20can%20significantly%20impact%20discovery%20performance.%20Motivated%20by%0Athis%2C%20we%20propose%20two%20new%20approaches%20for%20automatically%20selecting%20the%20most%0Asuitable%20labeled%20data%20based%20on%20the%20similarity%20between%20the%20labeled%20and%20unlabeled%0Adata.%20Our%20observation%20is%20that%2C%20unlike%20in%20conventional%20supervised%20transfer%0Alearning%2C%20the%20best%20labeled%20is%20neither%20too%20similar%2C%20nor%20too%20dissimilar%2C%20to%20the%0Aunlabeled%20categories.%20Our%20resulting%20approaches%20obtains%20state-of-the-art%0Adiscovery%20performance%20across%20a%20range%20of%20challenging%20fine-grained%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabeled%2520Data%2520Selection%2520for%2520Category%2520Discovery%26entry.906535625%3DBingchen%2520Zhao%2520and%2520Nico%2520Lang%2520and%2520Serge%2520Belongie%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520Category%2520discovery%2520methods%2520aim%2520to%2520find%2520novel%2520categories%2520in%2520unlabeled%2520visual%250Adata.%2520At%2520training%2520time%252C%2520a%2520set%2520of%2520labeled%2520and%2520unlabeled%2520images%2520are%2520provided%252C%250Awhere%2520the%2520labels%2520correspond%2520to%2520the%2520categories%2520present%2520in%2520the%2520images.%2520The%250Alabeled%2520data%2520provides%2520guidance%2520during%2520training%2520by%2520indicating%2520what%2520types%2520of%250Avisual%2520properties%2520and%2520features%2520are%2520relevant%2520for%2520performing%2520discovery%2520in%2520the%250Aunlabeled%2520data.%2520As%2520a%2520result%252C%2520changing%2520the%2520categories%2520present%2520in%2520the%2520labeled%2520set%250Acan%2520have%2520a%2520large%2520impact%2520on%2520what%2520is%2520ultimately%2520discovered%2520in%2520the%2520unlabeled%2520set.%250ADespite%2520its%2520importance%252C%2520the%2520impact%2520of%2520labeled%2520data%2520selection%2520has%2520not%2520been%250Aexplored%2520in%2520the%2520category%2520discovery%2520literature%2520to%2520date.%2520We%2520show%2520that%2520changing%250Athe%2520labeled%2520data%2520can%2520significantly%2520impact%2520discovery%2520performance.%2520Motivated%2520by%250Athis%252C%2520we%2520propose%2520two%2520new%2520approaches%2520for%2520automatically%2520selecting%2520the%2520most%250Asuitable%2520labeled%2520data%2520based%2520on%2520the%2520similarity%2520between%2520the%2520labeled%2520and%2520unlabeled%250Adata.%2520Our%2520observation%2520is%2520that%252C%2520unlike%2520in%2520conventional%2520supervised%2520transfer%250Alearning%252C%2520the%2520best%2520labeled%2520is%2520neither%2520too%2520similar%252C%2520nor%2520too%2520dissimilar%252C%2520to%2520the%250Aunlabeled%2520categories.%2520Our%2520resulting%2520approaches%2520obtains%2520state-of-the-art%250Adiscovery%2520performance%2520across%2520a%2520range%2520of%2520challenging%2520fine-grained%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Labeled%20Data%20Selection%20for%20Category%20Discovery&entry.906535625=Bingchen%20Zhao%20and%20Nico%20Lang%20and%20Serge%20Belongie%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20Category%20discovery%20methods%20aim%20to%20find%20novel%20categories%20in%20unlabeled%20visual%0Adata.%20At%20training%20time%2C%20a%20set%20of%20labeled%20and%20unlabeled%20images%20are%20provided%2C%0Awhere%20the%20labels%20correspond%20to%20the%20categories%20present%20in%20the%20images.%20The%0Alabeled%20data%20provides%20guidance%20during%20training%20by%20indicating%20what%20types%20of%0Avisual%20properties%20and%20features%20are%20relevant%20for%20performing%20discovery%20in%20the%0Aunlabeled%20data.%20As%20a%20result%2C%20changing%20the%20categories%20present%20in%20the%20labeled%20set%0Acan%20have%20a%20large%20impact%20on%20what%20is%20ultimately%20discovered%20in%20the%20unlabeled%20set.%0ADespite%20its%20importance%2C%20the%20impact%20of%20labeled%20data%20selection%20has%20not%20been%0Aexplored%20in%20the%20category%20discovery%20literature%20to%20date.%20We%20show%20that%20changing%0Athe%20labeled%20data%20can%20significantly%20impact%20discovery%20performance.%20Motivated%20by%0Athis%2C%20we%20propose%20two%20new%20approaches%20for%20automatically%20selecting%20the%20most%0Asuitable%20labeled%20data%20based%20on%20the%20similarity%20between%20the%20labeled%20and%20unlabeled%0Adata.%20Our%20observation%20is%20that%2C%20unlike%20in%20conventional%20supervised%20transfer%0Alearning%2C%20the%20best%20labeled%20is%20neither%20too%20similar%2C%20nor%20too%20dissimilar%2C%20to%20the%0Aunlabeled%20categories.%20Our%20resulting%20approaches%20obtains%20state-of-the-art%0Adiscovery%20performance%20across%20a%20range%20of%20challenging%20fine-grained%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04898v1&entry.124074799=Read"},
{"title": "A Manifold Representation of the Key in Vision Transformers", "author": "Li Meng and Morten Goodwin and Anis Yazidi and Paal Engelstad", "abstract": "  Vision Transformers implement multi-head self-attention via stacking multiple\nattention blocks. The query, key, and value are often intertwined and generated\nwithin those blocks via a single, shared linear transformation. This paper\nexplores the concept of disentangling the key from the query and value, and\nadopting a manifold representation for the key. Our experiments reveal that\ndecoupling and endowing the key with a manifold structure can enhance the\nmodel's performance. Specifically, ViT-B exhibits a 0.87% increase in top-1\naccuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the\nImageNet-1K dataset, with eight charts in the manifold key. Our approach also\nyields positive results in object detection and instance segmentation tasks on\nthe COCO dataset. We establish that these performance gains are not merely due\nto the simplicity of adding more parameters and computations. Future research\nmay investigate strategies for cutting the budget of such representations and\naim for further performance improvements based on our findings.\n", "link": "http://arxiv.org/abs/2402.00534v2", "date": "2024-06-07", "relevancy": 1.9811, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Manifold%20Representation%20of%20the%20Key%20in%20Vision%20Transformers&body=Title%3A%20A%20Manifold%20Representation%20of%20the%20Key%20in%20Vision%20Transformers%0AAuthor%3A%20Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad%0AAbstract%3A%20%20%20Vision%20Transformers%20implement%20multi-head%20self-attention%20via%20stacking%20multiple%0Aattention%20blocks.%20The%20query%2C%20key%2C%20and%20value%20are%20often%20intertwined%20and%20generated%0Awithin%20those%20blocks%20via%20a%20single%2C%20shared%20linear%20transformation.%20This%20paper%0Aexplores%20the%20concept%20of%20disentangling%20the%20key%20from%20the%20query%20and%20value%2C%20and%0Aadopting%20a%20manifold%20representation%20for%20the%20key.%20Our%20experiments%20reveal%20that%0Adecoupling%20and%20endowing%20the%20key%20with%20a%20manifold%20structure%20can%20enhance%20the%0Amodel%27s%20performance.%20Specifically%2C%20ViT-B%20exhibits%20a%200.87%25%20increase%20in%20top-1%0Aaccuracy%2C%20while%20Swin-T%20sees%20a%20boost%20of%200.52%25%20in%20top-1%20accuracy%20on%20the%0AImageNet-1K%20dataset%2C%20with%20eight%20charts%20in%20the%20manifold%20key.%20Our%20approach%20also%0Ayields%20positive%20results%20in%20object%20detection%20and%20instance%20segmentation%20tasks%20on%0Athe%20COCO%20dataset.%20We%20establish%20that%20these%20performance%20gains%20are%20not%20merely%20due%0Ato%20the%20simplicity%20of%20adding%20more%20parameters%20and%20computations.%20Future%20research%0Amay%20investigate%20strategies%20for%20cutting%20the%20budget%20of%20such%20representations%20and%0Aaim%20for%20further%20performance%20improvements%20based%20on%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Manifold%2520Representation%2520of%2520the%2520Key%2520in%2520Vision%2520Transformers%26entry.906535625%3DLi%2520Meng%2520and%2520Morten%2520Goodwin%2520and%2520Anis%2520Yazidi%2520and%2520Paal%2520Engelstad%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520implement%2520multi-head%2520self-attention%2520via%2520stacking%2520multiple%250Aattention%2520blocks.%2520The%2520query%252C%2520key%252C%2520and%2520value%2520are%2520often%2520intertwined%2520and%2520generated%250Awithin%2520those%2520blocks%2520via%2520a%2520single%252C%2520shared%2520linear%2520transformation.%2520This%2520paper%250Aexplores%2520the%2520concept%2520of%2520disentangling%2520the%2520key%2520from%2520the%2520query%2520and%2520value%252C%2520and%250Aadopting%2520a%2520manifold%2520representation%2520for%2520the%2520key.%2520Our%2520experiments%2520reveal%2520that%250Adecoupling%2520and%2520endowing%2520the%2520key%2520with%2520a%2520manifold%2520structure%2520can%2520enhance%2520the%250Amodel%2527s%2520performance.%2520Specifically%252C%2520ViT-B%2520exhibits%2520a%25200.87%2525%2520increase%2520in%2520top-1%250Aaccuracy%252C%2520while%2520Swin-T%2520sees%2520a%2520boost%2520of%25200.52%2525%2520in%2520top-1%2520accuracy%2520on%2520the%250AImageNet-1K%2520dataset%252C%2520with%2520eight%2520charts%2520in%2520the%2520manifold%2520key.%2520Our%2520approach%2520also%250Ayields%2520positive%2520results%2520in%2520object%2520detection%2520and%2520instance%2520segmentation%2520tasks%2520on%250Athe%2520COCO%2520dataset.%2520We%2520establish%2520that%2520these%2520performance%2520gains%2520are%2520not%2520merely%2520due%250Ato%2520the%2520simplicity%2520of%2520adding%2520more%2520parameters%2520and%2520computations.%2520Future%2520research%250Amay%2520investigate%2520strategies%2520for%2520cutting%2520the%2520budget%2520of%2520such%2520representations%2520and%250Aaim%2520for%2520further%2520performance%2520improvements%2520based%2520on%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Manifold%20Representation%20of%20the%20Key%20in%20Vision%20Transformers&entry.906535625=Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad&entry.1292438233=%20%20Vision%20Transformers%20implement%20multi-head%20self-attention%20via%20stacking%20multiple%0Aattention%20blocks.%20The%20query%2C%20key%2C%20and%20value%20are%20often%20intertwined%20and%20generated%0Awithin%20those%20blocks%20via%20a%20single%2C%20shared%20linear%20transformation.%20This%20paper%0Aexplores%20the%20concept%20of%20disentangling%20the%20key%20from%20the%20query%20and%20value%2C%20and%0Aadopting%20a%20manifold%20representation%20for%20the%20key.%20Our%20experiments%20reveal%20that%0Adecoupling%20and%20endowing%20the%20key%20with%20a%20manifold%20structure%20can%20enhance%20the%0Amodel%27s%20performance.%20Specifically%2C%20ViT-B%20exhibits%20a%200.87%25%20increase%20in%20top-1%0Aaccuracy%2C%20while%20Swin-T%20sees%20a%20boost%20of%200.52%25%20in%20top-1%20accuracy%20on%20the%0AImageNet-1K%20dataset%2C%20with%20eight%20charts%20in%20the%20manifold%20key.%20Our%20approach%20also%0Ayields%20positive%20results%20in%20object%20detection%20and%20instance%20segmentation%20tasks%20on%0Athe%20COCO%20dataset.%20We%20establish%20that%20these%20performance%20gains%20are%20not%20merely%20due%0Ato%20the%20simplicity%20of%20adding%20more%20parameters%20and%20computations.%20Future%20research%0Amay%20investigate%20strategies%20for%20cutting%20the%20budget%20of%20such%20representations%20and%0Aaim%20for%20further%20performance%20improvements%20based%20on%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00534v2&entry.124074799=Read"},
{"title": "A short review on graphonometric evaluation tools in children", "author": "Belen Esther Aleman and Moises Diaz and Miguel Angel Ferrer", "abstract": "  Handwriting is a complex task that involves the coordination of motor,\nperceptual and cognitive skills. It is a fundamental skill for the cognitive\nand academic development of children. However, the technological, and\neducational changes in recent decades have affected both the teaching and\nassessment of handwriting. This paper presents a literature review of\nhandwriting analysis in children, including a bibliometric analysis of\npublished articles, the study participants, and the methods of evaluating the\ngraphonometric state of children. The aim is to synthesize the state of the art\nand provide an overview of the main study trends over the last decade. The\nreview concludes that handwriting remains a fundamental tool for early\nestimation of cognitive problems and early intervention. The article analyzes\ngraphonometric evaluation tools. Likewise, it reflects on the importance of\ngraphonometric evaluation as a means to detect possible difficulties or\ndisorders in learning to write. The article concludes by highlighting the need\nto agree on an evaluation methodology and to combine databases.\n", "link": "http://arxiv.org/abs/2406.04818v1", "date": "2024-06-07", "relevancy": 1.9798, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4089}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20short%20review%20on%20graphonometric%20evaluation%20tools%20in%20children&body=Title%3A%20A%20short%20review%20on%20graphonometric%20evaluation%20tools%20in%20children%0AAuthor%3A%20Belen%20Esther%20Aleman%20and%20Moises%20Diaz%20and%20Miguel%20Angel%20Ferrer%0AAbstract%3A%20%20%20Handwriting%20is%20a%20complex%20task%20that%20involves%20the%20coordination%20of%20motor%2C%0Aperceptual%20and%20cognitive%20skills.%20It%20is%20a%20fundamental%20skill%20for%20the%20cognitive%0Aand%20academic%20development%20of%20children.%20However%2C%20the%20technological%2C%20and%0Aeducational%20changes%20in%20recent%20decades%20have%20affected%20both%20the%20teaching%20and%0Aassessment%20of%20handwriting.%20This%20paper%20presents%20a%20literature%20review%20of%0Ahandwriting%20analysis%20in%20children%2C%20including%20a%20bibliometric%20analysis%20of%0Apublished%20articles%2C%20the%20study%20participants%2C%20and%20the%20methods%20of%20evaluating%20the%0Agraphonometric%20state%20of%20children.%20The%20aim%20is%20to%20synthesize%20the%20state%20of%20the%20art%0Aand%20provide%20an%20overview%20of%20the%20main%20study%20trends%20over%20the%20last%20decade.%20The%0Areview%20concludes%20that%20handwriting%20remains%20a%20fundamental%20tool%20for%20early%0Aestimation%20of%20cognitive%20problems%20and%20early%20intervention.%20The%20article%20analyzes%0Agraphonometric%20evaluation%20tools.%20Likewise%2C%20it%20reflects%20on%20the%20importance%20of%0Agraphonometric%20evaluation%20as%20a%20means%20to%20detect%20possible%20difficulties%20or%0Adisorders%20in%20learning%20to%20write.%20The%20article%20concludes%20by%20highlighting%20the%20need%0Ato%20agree%20on%20an%20evaluation%20methodology%20and%20to%20combine%20databases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520short%2520review%2520on%2520graphonometric%2520evaluation%2520tools%2520in%2520children%26entry.906535625%3DBelen%2520Esther%2520Aleman%2520and%2520Moises%2520Diaz%2520and%2520Miguel%2520Angel%2520Ferrer%26entry.1292438233%3D%2520%2520Handwriting%2520is%2520a%2520complex%2520task%2520that%2520involves%2520the%2520coordination%2520of%2520motor%252C%250Aperceptual%2520and%2520cognitive%2520skills.%2520It%2520is%2520a%2520fundamental%2520skill%2520for%2520the%2520cognitive%250Aand%2520academic%2520development%2520of%2520children.%2520However%252C%2520the%2520technological%252C%2520and%250Aeducational%2520changes%2520in%2520recent%2520decades%2520have%2520affected%2520both%2520the%2520teaching%2520and%250Aassessment%2520of%2520handwriting.%2520This%2520paper%2520presents%2520a%2520literature%2520review%2520of%250Ahandwriting%2520analysis%2520in%2520children%252C%2520including%2520a%2520bibliometric%2520analysis%2520of%250Apublished%2520articles%252C%2520the%2520study%2520participants%252C%2520and%2520the%2520methods%2520of%2520evaluating%2520the%250Agraphonometric%2520state%2520of%2520children.%2520The%2520aim%2520is%2520to%2520synthesize%2520the%2520state%2520of%2520the%2520art%250Aand%2520provide%2520an%2520overview%2520of%2520the%2520main%2520study%2520trends%2520over%2520the%2520last%2520decade.%2520The%250Areview%2520concludes%2520that%2520handwriting%2520remains%2520a%2520fundamental%2520tool%2520for%2520early%250Aestimation%2520of%2520cognitive%2520problems%2520and%2520early%2520intervention.%2520The%2520article%2520analyzes%250Agraphonometric%2520evaluation%2520tools.%2520Likewise%252C%2520it%2520reflects%2520on%2520the%2520importance%2520of%250Agraphonometric%2520evaluation%2520as%2520a%2520means%2520to%2520detect%2520possible%2520difficulties%2520or%250Adisorders%2520in%2520learning%2520to%2520write.%2520The%2520article%2520concludes%2520by%2520highlighting%2520the%2520need%250Ato%2520agree%2520on%2520an%2520evaluation%2520methodology%2520and%2520to%2520combine%2520databases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20short%20review%20on%20graphonometric%20evaluation%20tools%20in%20children&entry.906535625=Belen%20Esther%20Aleman%20and%20Moises%20Diaz%20and%20Miguel%20Angel%20Ferrer&entry.1292438233=%20%20Handwriting%20is%20a%20complex%20task%20that%20involves%20the%20coordination%20of%20motor%2C%0Aperceptual%20and%20cognitive%20skills.%20It%20is%20a%20fundamental%20skill%20for%20the%20cognitive%0Aand%20academic%20development%20of%20children.%20However%2C%20the%20technological%2C%20and%0Aeducational%20changes%20in%20recent%20decades%20have%20affected%20both%20the%20teaching%20and%0Aassessment%20of%20handwriting.%20This%20paper%20presents%20a%20literature%20review%20of%0Ahandwriting%20analysis%20in%20children%2C%20including%20a%20bibliometric%20analysis%20of%0Apublished%20articles%2C%20the%20study%20participants%2C%20and%20the%20methods%20of%20evaluating%20the%0Agraphonometric%20state%20of%20children.%20The%20aim%20is%20to%20synthesize%20the%20state%20of%20the%20art%0Aand%20provide%20an%20overview%20of%20the%20main%20study%20trends%20over%20the%20last%20decade.%20The%0Areview%20concludes%20that%20handwriting%20remains%20a%20fundamental%20tool%20for%20early%0Aestimation%20of%20cognitive%20problems%20and%20early%20intervention.%20The%20article%20analyzes%0Agraphonometric%20evaluation%20tools.%20Likewise%2C%20it%20reflects%20on%20the%20importance%20of%0Agraphonometric%20evaluation%20as%20a%20means%20to%20detect%20possible%20difficulties%20or%0Adisorders%20in%20learning%20to%20write.%20The%20article%20concludes%20by%20highlighting%20the%20need%0Ato%20agree%20on%20an%20evaluation%20methodology%20and%20to%20combine%20databases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04818v1&entry.124074799=Read"},
{"title": "AGALE: A Graph-Aware Continual Learning Evaluation Framework", "author": "Tianqi Zhao and Alan Hanjalic and Megha Khosla", "abstract": "  In recent years, continual learning (CL) techniques have made significant\nprogress in learning from streaming data while preserving knowledge across\nsequential tasks, particularly in the realm of euclidean data. To foster fair\nevaluation and recognize challenges in CL settings, several evaluation\nframeworks have been proposed, focusing mainly on the single- and multi-label\nclassification task on euclidean data. However, these evaluation frameworks are\nnot trivially applicable when the input data is graph-structured, as they do\nnot consider the topological structure inherent in graphs. Existing continual\ngraph learning (CGL) evaluation frameworks have predominantly focussed on\nsingle-label scenarios in the node classification (NC) task. This focus has\noverlooked the complexities of multi-label scenarios, where nodes may exhibit\naffiliations with multiple labels, simultaneously participating in multiple\ntasks. We develop a graph-aware evaluation (\\agale) framework that accommodates\nboth single-labeled and multi-labeled nodes, addressing the limitations of\nprevious evaluation frameworks. In particular, we define new incremental\nsettings and devise data partitioning algorithms tailored to CGL datasets. We\nperform extensive experiments comparing methods from the domains of continual\nlearning, continual graph learning, and dynamic graph learning (DGL). We\ntheoretically analyze \\agale and provide new insights about the role of\nhomophily in the performance of compared methods. We release our framework at\nhttps://github.com/Tianqi-py/AGALE.\n", "link": "http://arxiv.org/abs/2406.01229v2", "date": "2024-06-07", "relevancy": 1.9762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5052}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4865}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGALE%3A%20A%20Graph-Aware%20Continual%20Learning%20Evaluation%20Framework&body=Title%3A%20AGALE%3A%20A%20Graph-Aware%20Continual%20Learning%20Evaluation%20Framework%0AAuthor%3A%20Tianqi%20Zhao%20and%20Alan%20Hanjalic%20and%20Megha%20Khosla%0AAbstract%3A%20%20%20In%20recent%20years%2C%20continual%20learning%20%28CL%29%20techniques%20have%20made%20significant%0Aprogress%20in%20learning%20from%20streaming%20data%20while%20preserving%20knowledge%20across%0Asequential%20tasks%2C%20particularly%20in%20the%20realm%20of%20euclidean%20data.%20To%20foster%20fair%0Aevaluation%20and%20recognize%20challenges%20in%20CL%20settings%2C%20several%20evaluation%0Aframeworks%20have%20been%20proposed%2C%20focusing%20mainly%20on%20the%20single-%20and%20multi-label%0Aclassification%20task%20on%20euclidean%20data.%20However%2C%20these%20evaluation%20frameworks%20are%0Anot%20trivially%20applicable%20when%20the%20input%20data%20is%20graph-structured%2C%20as%20they%20do%0Anot%20consider%20the%20topological%20structure%20inherent%20in%20graphs.%20Existing%20continual%0Agraph%20learning%20%28CGL%29%20evaluation%20frameworks%20have%20predominantly%20focussed%20on%0Asingle-label%20scenarios%20in%20the%20node%20classification%20%28NC%29%20task.%20This%20focus%20has%0Aoverlooked%20the%20complexities%20of%20multi-label%20scenarios%2C%20where%20nodes%20may%20exhibit%0Aaffiliations%20with%20multiple%20labels%2C%20simultaneously%20participating%20in%20multiple%0Atasks.%20We%20develop%20a%20graph-aware%20evaluation%20%28%5Cagale%29%20framework%20that%20accommodates%0Aboth%20single-labeled%20and%20multi-labeled%20nodes%2C%20addressing%20the%20limitations%20of%0Aprevious%20evaluation%20frameworks.%20In%20particular%2C%20we%20define%20new%20incremental%0Asettings%20and%20devise%20data%20partitioning%20algorithms%20tailored%20to%20CGL%20datasets.%20We%0Aperform%20extensive%20experiments%20comparing%20methods%20from%20the%20domains%20of%20continual%0Alearning%2C%20continual%20graph%20learning%2C%20and%20dynamic%20graph%20learning%20%28DGL%29.%20We%0Atheoretically%20analyze%20%5Cagale%20and%20provide%20new%20insights%20about%20the%20role%20of%0Ahomophily%20in%20the%20performance%20of%20compared%20methods.%20We%20release%20our%20framework%20at%0Ahttps%3A//github.com/Tianqi-py/AGALE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGALE%253A%2520A%2520Graph-Aware%2520Continual%2520Learning%2520Evaluation%2520Framework%26entry.906535625%3DTianqi%2520Zhao%2520and%2520Alan%2520Hanjalic%2520and%2520Megha%2520Khosla%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520continual%2520learning%2520%2528CL%2529%2520techniques%2520have%2520made%2520significant%250Aprogress%2520in%2520learning%2520from%2520streaming%2520data%2520while%2520preserving%2520knowledge%2520across%250Asequential%2520tasks%252C%2520particularly%2520in%2520the%2520realm%2520of%2520euclidean%2520data.%2520To%2520foster%2520fair%250Aevaluation%2520and%2520recognize%2520challenges%2520in%2520CL%2520settings%252C%2520several%2520evaluation%250Aframeworks%2520have%2520been%2520proposed%252C%2520focusing%2520mainly%2520on%2520the%2520single-%2520and%2520multi-label%250Aclassification%2520task%2520on%2520euclidean%2520data.%2520However%252C%2520these%2520evaluation%2520frameworks%2520are%250Anot%2520trivially%2520applicable%2520when%2520the%2520input%2520data%2520is%2520graph-structured%252C%2520as%2520they%2520do%250Anot%2520consider%2520the%2520topological%2520structure%2520inherent%2520in%2520graphs.%2520Existing%2520continual%250Agraph%2520learning%2520%2528CGL%2529%2520evaluation%2520frameworks%2520have%2520predominantly%2520focussed%2520on%250Asingle-label%2520scenarios%2520in%2520the%2520node%2520classification%2520%2528NC%2529%2520task.%2520This%2520focus%2520has%250Aoverlooked%2520the%2520complexities%2520of%2520multi-label%2520scenarios%252C%2520where%2520nodes%2520may%2520exhibit%250Aaffiliations%2520with%2520multiple%2520labels%252C%2520simultaneously%2520participating%2520in%2520multiple%250Atasks.%2520We%2520develop%2520a%2520graph-aware%2520evaluation%2520%2528%255Cagale%2529%2520framework%2520that%2520accommodates%250Aboth%2520single-labeled%2520and%2520multi-labeled%2520nodes%252C%2520addressing%2520the%2520limitations%2520of%250Aprevious%2520evaluation%2520frameworks.%2520In%2520particular%252C%2520we%2520define%2520new%2520incremental%250Asettings%2520and%2520devise%2520data%2520partitioning%2520algorithms%2520tailored%2520to%2520CGL%2520datasets.%2520We%250Aperform%2520extensive%2520experiments%2520comparing%2520methods%2520from%2520the%2520domains%2520of%2520continual%250Alearning%252C%2520continual%2520graph%2520learning%252C%2520and%2520dynamic%2520graph%2520learning%2520%2528DGL%2529.%2520We%250Atheoretically%2520analyze%2520%255Cagale%2520and%2520provide%2520new%2520insights%2520about%2520the%2520role%2520of%250Ahomophily%2520in%2520the%2520performance%2520of%2520compared%2520methods.%2520We%2520release%2520our%2520framework%2520at%250Ahttps%253A//github.com/Tianqi-py/AGALE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGALE%3A%20A%20Graph-Aware%20Continual%20Learning%20Evaluation%20Framework&entry.906535625=Tianqi%20Zhao%20and%20Alan%20Hanjalic%20and%20Megha%20Khosla&entry.1292438233=%20%20In%20recent%20years%2C%20continual%20learning%20%28CL%29%20techniques%20have%20made%20significant%0Aprogress%20in%20learning%20from%20streaming%20data%20while%20preserving%20knowledge%20across%0Asequential%20tasks%2C%20particularly%20in%20the%20realm%20of%20euclidean%20data.%20To%20foster%20fair%0Aevaluation%20and%20recognize%20challenges%20in%20CL%20settings%2C%20several%20evaluation%0Aframeworks%20have%20been%20proposed%2C%20focusing%20mainly%20on%20the%20single-%20and%20multi-label%0Aclassification%20task%20on%20euclidean%20data.%20However%2C%20these%20evaluation%20frameworks%20are%0Anot%20trivially%20applicable%20when%20the%20input%20data%20is%20graph-structured%2C%20as%20they%20do%0Anot%20consider%20the%20topological%20structure%20inherent%20in%20graphs.%20Existing%20continual%0Agraph%20learning%20%28CGL%29%20evaluation%20frameworks%20have%20predominantly%20focussed%20on%0Asingle-label%20scenarios%20in%20the%20node%20classification%20%28NC%29%20task.%20This%20focus%20has%0Aoverlooked%20the%20complexities%20of%20multi-label%20scenarios%2C%20where%20nodes%20may%20exhibit%0Aaffiliations%20with%20multiple%20labels%2C%20simultaneously%20participating%20in%20multiple%0Atasks.%20We%20develop%20a%20graph-aware%20evaluation%20%28%5Cagale%29%20framework%20that%20accommodates%0Aboth%20single-labeled%20and%20multi-labeled%20nodes%2C%20addressing%20the%20limitations%20of%0Aprevious%20evaluation%20frameworks.%20In%20particular%2C%20we%20define%20new%20incremental%0Asettings%20and%20devise%20data%20partitioning%20algorithms%20tailored%20to%20CGL%20datasets.%20We%0Aperform%20extensive%20experiments%20comparing%20methods%20from%20the%20domains%20of%20continual%0Alearning%2C%20continual%20graph%20learning%2C%20and%20dynamic%20graph%20learning%20%28DGL%29.%20We%0Atheoretically%20analyze%20%5Cagale%20and%20provide%20new%20insights%20about%20the%20role%20of%0Ahomophily%20in%20the%20performance%20of%20compared%20methods.%20We%20release%20our%20framework%20at%0Ahttps%3A//github.com/Tianqi-py/AGALE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01229v2&entry.124074799=Read"},
{"title": "Multi-target stain normalization for histology slides", "author": "Desislav Ivanov and Carlo Alberto Barbano and Marco Grangetto", "abstract": "  Traditional staining normalization approaches, e.g. Macenko, typically rely\non the choice of a single representative reference image, which may not\nadequately account for the diverse staining patterns of datasets collected in\npractical scenarios. In this study, we introduce a novel approach that\nleverages multiple reference images to enhance robustness against stain\nvariation. Our method is parameter-free and can be adopted in existing\ncomputational pathology pipelines with no significant changes. We evaluate the\neffectiveness of our method through experiments using a deep-learning pipeline\nfor automatic nuclei segmentation on colorectal images. Our results show that\nby leveraging multiple reference images, better results can be achieved when\ngeneralizing to external data, where the staining can widely differ from the\ntraining set.\n", "link": "http://arxiv.org/abs/2406.02077v2", "date": "2024-06-07", "relevancy": 1.9697, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4876}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-target%20stain%20normalization%20for%20histology%20slides&body=Title%3A%20Multi-target%20stain%20normalization%20for%20histology%20slides%0AAuthor%3A%20Desislav%20Ivanov%20and%20Carlo%20Alberto%20Barbano%20and%20Marco%20Grangetto%0AAbstract%3A%20%20%20Traditional%20staining%20normalization%20approaches%2C%20e.g.%20Macenko%2C%20typically%20rely%0Aon%20the%20choice%20of%20a%20single%20representative%20reference%20image%2C%20which%20may%20not%0Aadequately%20account%20for%20the%20diverse%20staining%20patterns%20of%20datasets%20collected%20in%0Apractical%20scenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20approach%20that%0Aleverages%20multiple%20reference%20images%20to%20enhance%20robustness%20against%20stain%0Avariation.%20Our%20method%20is%20parameter-free%20and%20can%20be%20adopted%20in%20existing%0Acomputational%20pathology%20pipelines%20with%20no%20significant%20changes.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20through%20experiments%20using%20a%20deep-learning%20pipeline%0Afor%20automatic%20nuclei%20segmentation%20on%20colorectal%20images.%20Our%20results%20show%20that%0Aby%20leveraging%20multiple%20reference%20images%2C%20better%20results%20can%20be%20achieved%20when%0Ageneralizing%20to%20external%20data%2C%20where%20the%20staining%20can%20widely%20differ%20from%20the%0Atraining%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-target%2520stain%2520normalization%2520for%2520histology%2520slides%26entry.906535625%3DDesislav%2520Ivanov%2520and%2520Carlo%2520Alberto%2520Barbano%2520and%2520Marco%2520Grangetto%26entry.1292438233%3D%2520%2520Traditional%2520staining%2520normalization%2520approaches%252C%2520e.g.%2520Macenko%252C%2520typically%2520rely%250Aon%2520the%2520choice%2520of%2520a%2520single%2520representative%2520reference%2520image%252C%2520which%2520may%2520not%250Aadequately%2520account%2520for%2520the%2520diverse%2520staining%2520patterns%2520of%2520datasets%2520collected%2520in%250Apractical%2520scenarios.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%250Aleverages%2520multiple%2520reference%2520images%2520to%2520enhance%2520robustness%2520against%2520stain%250Avariation.%2520Our%2520method%2520is%2520parameter-free%2520and%2520can%2520be%2520adopted%2520in%2520existing%250Acomputational%2520pathology%2520pipelines%2520with%2520no%2520significant%2520changes.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520through%2520experiments%2520using%2520a%2520deep-learning%2520pipeline%250Afor%2520automatic%2520nuclei%2520segmentation%2520on%2520colorectal%2520images.%2520Our%2520results%2520show%2520that%250Aby%2520leveraging%2520multiple%2520reference%2520images%252C%2520better%2520results%2520can%2520be%2520achieved%2520when%250Ageneralizing%2520to%2520external%2520data%252C%2520where%2520the%2520staining%2520can%2520widely%2520differ%2520from%2520the%250Atraining%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-target%20stain%20normalization%20for%20histology%20slides&entry.906535625=Desislav%20Ivanov%20and%20Carlo%20Alberto%20Barbano%20and%20Marco%20Grangetto&entry.1292438233=%20%20Traditional%20staining%20normalization%20approaches%2C%20e.g.%20Macenko%2C%20typically%20rely%0Aon%20the%20choice%20of%20a%20single%20representative%20reference%20image%2C%20which%20may%20not%0Aadequately%20account%20for%20the%20diverse%20staining%20patterns%20of%20datasets%20collected%20in%0Apractical%20scenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20approach%20that%0Aleverages%20multiple%20reference%20images%20to%20enhance%20robustness%20against%20stain%0Avariation.%20Our%20method%20is%20parameter-free%20and%20can%20be%20adopted%20in%20existing%0Acomputational%20pathology%20pipelines%20with%20no%20significant%20changes.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20method%20through%20experiments%20using%20a%20deep-learning%20pipeline%0Afor%20automatic%20nuclei%20segmentation%20on%20colorectal%20images.%20Our%20results%20show%20that%0Aby%20leveraging%20multiple%20reference%20images%2C%20better%20results%20can%20be%20achieved%20when%0Ageneralizing%20to%20external%20data%2C%20where%20the%20staining%20can%20widely%20differ%20from%20the%0Atraining%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02077v2&entry.124074799=Read"},
{"title": "Concept Drift Detection using Ensemble of Integrally Private Models", "author": "Ayush K. Varshney and Vicenc Torra", "abstract": "  Deep neural networks (DNNs) are one of the most widely used machine learning\nalgorithm. DNNs requires the training data to be available beforehand with true\nlabels. This is not feasible for many real-world problems where data arrives in\nthe streaming form and acquisition of true labels are scarce and expensive. In\nthe literature, not much focus has been given to the privacy prospect of the\nstreaming data, where data may change its distribution frequently. These\nconcept drifts must be detected privately in order to avoid any disclosure risk\nfrom DNNs. Existing privacy models use concept drift detection schemes such\nADWIN, KSWIN to detect the drifts. In this paper, we focus on the notion of\nintegrally private DNNs to detect concept drifts. Integrally private DNNs are\nthe models which recur frequently from different datasets. Based on this, we\nintroduce an ensemble methodology which we call 'Integrally Private Drift\nDetection' (IPDD) method to detect concept drift from private models. Our IPDD\nmethod does not require labels to detect drift but assumes true labels are\navailable once the drift has been detected. We have experimented with binary\nand multi-class synthetic and real-world data. Our experimental results show\nthat our methodology can privately detect concept drift, has comparable utility\n(even better in some cases) with ADWIN and outperforms utility from different\nlevels of differentially private models. The source code for the paper is\navailable\n\\hyperlink{https://github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models}{here}.\n", "link": "http://arxiv.org/abs/2406.04903v1", "date": "2024-06-07", "relevancy": 1.9668, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Drift%20Detection%20using%20Ensemble%20of%20Integrally%20Private%20Models&body=Title%3A%20Concept%20Drift%20Detection%20using%20Ensemble%20of%20Integrally%20Private%20Models%0AAuthor%3A%20Ayush%20K.%20Varshney%20and%20Vicenc%20Torra%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20one%20of%20the%20most%20widely%20used%20machine%20learning%0Aalgorithm.%20DNNs%20requires%20the%20training%20data%20to%20be%20available%20beforehand%20with%20true%0Alabels.%20This%20is%20not%20feasible%20for%20many%20real-world%20problems%20where%20data%20arrives%20in%0Athe%20streaming%20form%20and%20acquisition%20of%20true%20labels%20are%20scarce%20and%20expensive.%20In%0Athe%20literature%2C%20not%20much%20focus%20has%20been%20given%20to%20the%20privacy%20prospect%20of%20the%0Astreaming%20data%2C%20where%20data%20may%20change%20its%20distribution%20frequently.%20These%0Aconcept%20drifts%20must%20be%20detected%20privately%20in%20order%20to%20avoid%20any%20disclosure%20risk%0Afrom%20DNNs.%20Existing%20privacy%20models%20use%20concept%20drift%20detection%20schemes%20such%0AADWIN%2C%20KSWIN%20to%20detect%20the%20drifts.%20In%20this%20paper%2C%20we%20focus%20on%20the%20notion%20of%0Aintegrally%20private%20DNNs%20to%20detect%20concept%20drifts.%20Integrally%20private%20DNNs%20are%0Athe%20models%20which%20recur%20frequently%20from%20different%20datasets.%20Based%20on%20this%2C%20we%0Aintroduce%20an%20ensemble%20methodology%20which%20we%20call%20%27Integrally%20Private%20Drift%0ADetection%27%20%28IPDD%29%20method%20to%20detect%20concept%20drift%20from%20private%20models.%20Our%20IPDD%0Amethod%20does%20not%20require%20labels%20to%20detect%20drift%20but%20assumes%20true%20labels%20are%0Aavailable%20once%20the%20drift%20has%20been%20detected.%20We%20have%20experimented%20with%20binary%0Aand%20multi-class%20synthetic%20and%20real-world%20data.%20Our%20experimental%20results%20show%0Athat%20our%20methodology%20can%20privately%20detect%20concept%20drift%2C%20has%20comparable%20utility%0A%28even%20better%20in%20some%20cases%29%20with%20ADWIN%20and%20outperforms%20utility%20from%20different%0Alevels%20of%20differentially%20private%20models.%20The%20source%20code%20for%20the%20paper%20is%0Aavailable%0A%5Chyperlink%7Bhttps%3A//github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models%7D%7Bhere%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Drift%2520Detection%2520using%2520Ensemble%2520of%2520Integrally%2520Private%2520Models%26entry.906535625%3DAyush%2520K.%2520Varshney%2520and%2520Vicenc%2520Torra%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520one%2520of%2520the%2520most%2520widely%2520used%2520machine%2520learning%250Aalgorithm.%2520DNNs%2520requires%2520the%2520training%2520data%2520to%2520be%2520available%2520beforehand%2520with%2520true%250Alabels.%2520This%2520is%2520not%2520feasible%2520for%2520many%2520real-world%2520problems%2520where%2520data%2520arrives%2520in%250Athe%2520streaming%2520form%2520and%2520acquisition%2520of%2520true%2520labels%2520are%2520scarce%2520and%2520expensive.%2520In%250Athe%2520literature%252C%2520not%2520much%2520focus%2520has%2520been%2520given%2520to%2520the%2520privacy%2520prospect%2520of%2520the%250Astreaming%2520data%252C%2520where%2520data%2520may%2520change%2520its%2520distribution%2520frequently.%2520These%250Aconcept%2520drifts%2520must%2520be%2520detected%2520privately%2520in%2520order%2520to%2520avoid%2520any%2520disclosure%2520risk%250Afrom%2520DNNs.%2520Existing%2520privacy%2520models%2520use%2520concept%2520drift%2520detection%2520schemes%2520such%250AADWIN%252C%2520KSWIN%2520to%2520detect%2520the%2520drifts.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520notion%2520of%250Aintegrally%2520private%2520DNNs%2520to%2520detect%2520concept%2520drifts.%2520Integrally%2520private%2520DNNs%2520are%250Athe%2520models%2520which%2520recur%2520frequently%2520from%2520different%2520datasets.%2520Based%2520on%2520this%252C%2520we%250Aintroduce%2520an%2520ensemble%2520methodology%2520which%2520we%2520call%2520%2527Integrally%2520Private%2520Drift%250ADetection%2527%2520%2528IPDD%2529%2520method%2520to%2520detect%2520concept%2520drift%2520from%2520private%2520models.%2520Our%2520IPDD%250Amethod%2520does%2520not%2520require%2520labels%2520to%2520detect%2520drift%2520but%2520assumes%2520true%2520labels%2520are%250Aavailable%2520once%2520the%2520drift%2520has%2520been%2520detected.%2520We%2520have%2520experimented%2520with%2520binary%250Aand%2520multi-class%2520synthetic%2520and%2520real-world%2520data.%2520Our%2520experimental%2520results%2520show%250Athat%2520our%2520methodology%2520can%2520privately%2520detect%2520concept%2520drift%252C%2520has%2520comparable%2520utility%250A%2528even%2520better%2520in%2520some%2520cases%2529%2520with%2520ADWIN%2520and%2520outperforms%2520utility%2520from%2520different%250Alevels%2520of%2520differentially%2520private%2520models.%2520The%2520source%2520code%2520for%2520the%2520paper%2520is%250Aavailable%250A%255Chyperlink%257Bhttps%253A//github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models%257D%257Bhere%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Drift%20Detection%20using%20Ensemble%20of%20Integrally%20Private%20Models&entry.906535625=Ayush%20K.%20Varshney%20and%20Vicenc%20Torra&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20one%20of%20the%20most%20widely%20used%20machine%20learning%0Aalgorithm.%20DNNs%20requires%20the%20training%20data%20to%20be%20available%20beforehand%20with%20true%0Alabels.%20This%20is%20not%20feasible%20for%20many%20real-world%20problems%20where%20data%20arrives%20in%0Athe%20streaming%20form%20and%20acquisition%20of%20true%20labels%20are%20scarce%20and%20expensive.%20In%0Athe%20literature%2C%20not%20much%20focus%20has%20been%20given%20to%20the%20privacy%20prospect%20of%20the%0Astreaming%20data%2C%20where%20data%20may%20change%20its%20distribution%20frequently.%20These%0Aconcept%20drifts%20must%20be%20detected%20privately%20in%20order%20to%20avoid%20any%20disclosure%20risk%0Afrom%20DNNs.%20Existing%20privacy%20models%20use%20concept%20drift%20detection%20schemes%20such%0AADWIN%2C%20KSWIN%20to%20detect%20the%20drifts.%20In%20this%20paper%2C%20we%20focus%20on%20the%20notion%20of%0Aintegrally%20private%20DNNs%20to%20detect%20concept%20drifts.%20Integrally%20private%20DNNs%20are%0Athe%20models%20which%20recur%20frequently%20from%20different%20datasets.%20Based%20on%20this%2C%20we%0Aintroduce%20an%20ensemble%20methodology%20which%20we%20call%20%27Integrally%20Private%20Drift%0ADetection%27%20%28IPDD%29%20method%20to%20detect%20concept%20drift%20from%20private%20models.%20Our%20IPDD%0Amethod%20does%20not%20require%20labels%20to%20detect%20drift%20but%20assumes%20true%20labels%20are%0Aavailable%20once%20the%20drift%20has%20been%20detected.%20We%20have%20experimented%20with%20binary%0Aand%20multi-class%20synthetic%20and%20real-world%20data.%20Our%20experimental%20results%20show%0Athat%20our%20methodology%20can%20privately%20detect%20concept%20drift%2C%20has%20comparable%20utility%0A%28even%20better%20in%20some%20cases%29%20with%20ADWIN%20and%20outperforms%20utility%20from%20different%0Alevels%20of%20differentially%20private%20models.%20The%20source%20code%20for%20the%20paper%20is%0Aavailable%0A%5Chyperlink%7Bhttps%3A//github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models%7D%7Bhere%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04903v1&entry.124074799=Read"},
{"title": "Graph Mining under Data scarcity", "author": "Appan Rakaraddi and Lam Siew-Kei and Mahardhika Pratama and Marcus de Carvalho", "abstract": "  Multitude of deep learning models have been proposed for node classification\nin graphs. However, they tend to perform poorly under labeled-data scarcity.\nAlthough Few-shot learning for graphs has been introduced to overcome this\nproblem, the existing models are not easily adaptable for generic graph\nlearning frameworks like Graph Neural Networks (GNNs). Our work proposes an\nUncertainty Estimator framework that can be applied on top of any generic GNN\nbackbone network (which are typically designed for supervised/semi-supervised\nnode classification) to improve the node classification performance. A neural\nnetwork is used to model the Uncertainty Estimator as a probability\ndistribution rather than probabilistic discrete scalar values. We train these\nmodels under the classic episodic learning paradigm in the $n$-way, $k$-shot\nfashion, in an end-to-end setting.\n  Our work demonstrates that implementation of the uncertainty estimator on a\nGNN backbone network improves the classification accuracy under Few-shot\nsetting without any meta-learning specific architecture. We conduct experiments\non multiple datasets under different Few-shot settings and different GNN-based\nbackbone networks. Our method outperforms the baselines, which demonstrates the\nefficacy of the Uncertainty Estimator for Few-shot node classification on\ngraphs with a GNN.\n", "link": "http://arxiv.org/abs/2406.04825v1", "date": "2024-06-07", "relevancy": 1.9664, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5036}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5004}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Mining%20under%20Data%20scarcity&body=Title%3A%20Graph%20Mining%20under%20Data%20scarcity%0AAuthor%3A%20Appan%20Rakaraddi%20and%20Lam%20Siew-Kei%20and%20Mahardhika%20Pratama%20and%20Marcus%20de%20Carvalho%0AAbstract%3A%20%20%20Multitude%20of%20deep%20learning%20models%20have%20been%20proposed%20for%20node%20classification%0Ain%20graphs.%20However%2C%20they%20tend%20to%20perform%20poorly%20under%20labeled-data%20scarcity.%0AAlthough%20Few-shot%20learning%20for%20graphs%20has%20been%20introduced%20to%20overcome%20this%0Aproblem%2C%20the%20existing%20models%20are%20not%20easily%20adaptable%20for%20generic%20graph%0Alearning%20frameworks%20like%20Graph%20Neural%20Networks%20%28GNNs%29.%20Our%20work%20proposes%20an%0AUncertainty%20Estimator%20framework%20that%20can%20be%20applied%20on%20top%20of%20any%20generic%20GNN%0Abackbone%20network%20%28which%20are%20typically%20designed%20for%20supervised/semi-supervised%0Anode%20classification%29%20to%20improve%20the%20node%20classification%20performance.%20A%20neural%0Anetwork%20is%20used%20to%20model%20the%20Uncertainty%20Estimator%20as%20a%20probability%0Adistribution%20rather%20than%20probabilistic%20discrete%20scalar%20values.%20We%20train%20these%0Amodels%20under%20the%20classic%20episodic%20learning%20paradigm%20in%20the%20%24n%24-way%2C%20%24k%24-shot%0Afashion%2C%20in%20an%20end-to-end%20setting.%0A%20%20Our%20work%20demonstrates%20that%20implementation%20of%20the%20uncertainty%20estimator%20on%20a%0AGNN%20backbone%20network%20improves%20the%20classification%20accuracy%20under%20Few-shot%0Asetting%20without%20any%20meta-learning%20specific%20architecture.%20We%20conduct%20experiments%0Aon%20multiple%20datasets%20under%20different%20Few-shot%20settings%20and%20different%20GNN-based%0Abackbone%20networks.%20Our%20method%20outperforms%20the%20baselines%2C%20which%20demonstrates%20the%0Aefficacy%20of%20the%20Uncertainty%20Estimator%20for%20Few-shot%20node%20classification%20on%0Agraphs%20with%20a%20GNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Mining%2520under%2520Data%2520scarcity%26entry.906535625%3DAppan%2520Rakaraddi%2520and%2520Lam%2520Siew-Kei%2520and%2520Mahardhika%2520Pratama%2520and%2520Marcus%2520de%2520Carvalho%26entry.1292438233%3D%2520%2520Multitude%2520of%2520deep%2520learning%2520models%2520have%2520been%2520proposed%2520for%2520node%2520classification%250Ain%2520graphs.%2520However%252C%2520they%2520tend%2520to%2520perform%2520poorly%2520under%2520labeled-data%2520scarcity.%250AAlthough%2520Few-shot%2520learning%2520for%2520graphs%2520has%2520been%2520introduced%2520to%2520overcome%2520this%250Aproblem%252C%2520the%2520existing%2520models%2520are%2520not%2520easily%2520adaptable%2520for%2520generic%2520graph%250Alearning%2520frameworks%2520like%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Our%2520work%2520proposes%2520an%250AUncertainty%2520Estimator%2520framework%2520that%2520can%2520be%2520applied%2520on%2520top%2520of%2520any%2520generic%2520GNN%250Abackbone%2520network%2520%2528which%2520are%2520typically%2520designed%2520for%2520supervised/semi-supervised%250Anode%2520classification%2529%2520to%2520improve%2520the%2520node%2520classification%2520performance.%2520A%2520neural%250Anetwork%2520is%2520used%2520to%2520model%2520the%2520Uncertainty%2520Estimator%2520as%2520a%2520probability%250Adistribution%2520rather%2520than%2520probabilistic%2520discrete%2520scalar%2520values.%2520We%2520train%2520these%250Amodels%2520under%2520the%2520classic%2520episodic%2520learning%2520paradigm%2520in%2520the%2520%2524n%2524-way%252C%2520%2524k%2524-shot%250Afashion%252C%2520in%2520an%2520end-to-end%2520setting.%250A%2520%2520Our%2520work%2520demonstrates%2520that%2520implementation%2520of%2520the%2520uncertainty%2520estimator%2520on%2520a%250AGNN%2520backbone%2520network%2520improves%2520the%2520classification%2520accuracy%2520under%2520Few-shot%250Asetting%2520without%2520any%2520meta-learning%2520specific%2520architecture.%2520We%2520conduct%2520experiments%250Aon%2520multiple%2520datasets%2520under%2520different%2520Few-shot%2520settings%2520and%2520different%2520GNN-based%250Abackbone%2520networks.%2520Our%2520method%2520outperforms%2520the%2520baselines%252C%2520which%2520demonstrates%2520the%250Aefficacy%2520of%2520the%2520Uncertainty%2520Estimator%2520for%2520Few-shot%2520node%2520classification%2520on%250Agraphs%2520with%2520a%2520GNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Mining%20under%20Data%20scarcity&entry.906535625=Appan%20Rakaraddi%20and%20Lam%20Siew-Kei%20and%20Mahardhika%20Pratama%20and%20Marcus%20de%20Carvalho&entry.1292438233=%20%20Multitude%20of%20deep%20learning%20models%20have%20been%20proposed%20for%20node%20classification%0Ain%20graphs.%20However%2C%20they%20tend%20to%20perform%20poorly%20under%20labeled-data%20scarcity.%0AAlthough%20Few-shot%20learning%20for%20graphs%20has%20been%20introduced%20to%20overcome%20this%0Aproblem%2C%20the%20existing%20models%20are%20not%20easily%20adaptable%20for%20generic%20graph%0Alearning%20frameworks%20like%20Graph%20Neural%20Networks%20%28GNNs%29.%20Our%20work%20proposes%20an%0AUncertainty%20Estimator%20framework%20that%20can%20be%20applied%20on%20top%20of%20any%20generic%20GNN%0Abackbone%20network%20%28which%20are%20typically%20designed%20for%20supervised/semi-supervised%0Anode%20classification%29%20to%20improve%20the%20node%20classification%20performance.%20A%20neural%0Anetwork%20is%20used%20to%20model%20the%20Uncertainty%20Estimator%20as%20a%20probability%0Adistribution%20rather%20than%20probabilistic%20discrete%20scalar%20values.%20We%20train%20these%0Amodels%20under%20the%20classic%20episodic%20learning%20paradigm%20in%20the%20%24n%24-way%2C%20%24k%24-shot%0Afashion%2C%20in%20an%20end-to-end%20setting.%0A%20%20Our%20work%20demonstrates%20that%20implementation%20of%20the%20uncertainty%20estimator%20on%20a%0AGNN%20backbone%20network%20improves%20the%20classification%20accuracy%20under%20Few-shot%0Asetting%20without%20any%20meta-learning%20specific%20architecture.%20We%20conduct%20experiments%0Aon%20multiple%20datasets%20under%20different%20Few-shot%20settings%20and%20different%20GNN-based%0Abackbone%20networks.%20Our%20method%20outperforms%20the%20baselines%2C%20which%20demonstrates%20the%0Aefficacy%20of%20the%20Uncertainty%20Estimator%20for%20Few-shot%20node%20classification%20on%0Agraphs%20with%20a%20GNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04825v1&entry.124074799=Read"},
{"title": "VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via\n  Diffusion Transformers", "author": "Jun Zheng and Fuwei Zhao and Youjiang Xu and Xin Dong and Xiaodan Liang", "abstract": "  Video try-on stands as a promising area for its tremendous real-world\npotential. Prior works are limited to transferring product clothing images onto\nperson videos with simple poses and backgrounds, while underperforming on\ncasually captured videos. Recently, Sora revealed the scalability of Diffusion\nTransformer (DiT) in generating lifelike videos featuring real-world scenarios.\nInspired by this, we explore and propose the first DiT-based video try-on\nframework for practical in-the-wild applications, named VITON-DiT.\nSpecifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal\ndenoising DiT, and an identity preservation ControlNet. To faithfully recover\nthe clothing details, the extracted garment features are fused with the\nself-attention outputs of the denoising DiT and the ControlNet. We also\nintroduce novel random selection strategies during training and an Interpolated\nAuto-Regressive (IAR) technique at inference to facilitate long video\ngeneration. Unlike existing attempts that require the laborious and restrictive\nconstruction of a paired training dataset, severely limiting their scalability,\nVITON-DiT alleviates this by relying solely on unpaired human dance videos and\na carefully designed multi-stage training strategy. Furthermore, we curate a\nchallenging benchmark dataset to evaluate the performance of casual video\ntry-on. Extensive experiments demonstrate the superiority of VITON-DiT in\ngenerating spatio-temporal consistent try-on results for in-the-wild videos\nwith complicated human poses.\n", "link": "http://arxiv.org/abs/2405.18326v2", "date": "2024-06-07", "relevancy": 1.9536, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.661}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6524}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITON-DiT%3A%20Learning%20In-the-Wild%20Video%20Try-On%20from%20Human%20Dance%20Videos%20via%0A%20%20Diffusion%20Transformers&body=Title%3A%20VITON-DiT%3A%20Learning%20In-the-Wild%20Video%20Try-On%20from%20Human%20Dance%20Videos%20via%0A%20%20Diffusion%20Transformers%0AAuthor%3A%20Jun%20Zheng%20and%20Fuwei%20Zhao%20and%20Youjiang%20Xu%20and%20Xin%20Dong%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Video%20try-on%20stands%20as%20a%20promising%20area%20for%20its%20tremendous%20real-world%0Apotential.%20Prior%20works%20are%20limited%20to%20transferring%20product%20clothing%20images%20onto%0Aperson%20videos%20with%20simple%20poses%20and%20backgrounds%2C%20while%20underperforming%20on%0Acasually%20captured%20videos.%20Recently%2C%20Sora%20revealed%20the%20scalability%20of%20Diffusion%0ATransformer%20%28DiT%29%20in%20generating%20lifelike%20videos%20featuring%20real-world%20scenarios.%0AInspired%20by%20this%2C%20we%20explore%20and%20propose%20the%20first%20DiT-based%20video%20try-on%0Aframework%20for%20practical%20in-the-wild%20applications%2C%20named%20VITON-DiT.%0ASpecifically%2C%20VITON-DiT%20consists%20of%20a%20garment%20extractor%2C%20a%20Spatial-Temporal%0Adenoising%20DiT%2C%20and%20an%20identity%20preservation%20ControlNet.%20To%20faithfully%20recover%0Athe%20clothing%20details%2C%20the%20extracted%20garment%20features%20are%20fused%20with%20the%0Aself-attention%20outputs%20of%20the%20denoising%20DiT%20and%20the%20ControlNet.%20We%20also%0Aintroduce%20novel%20random%20selection%20strategies%20during%20training%20and%20an%20Interpolated%0AAuto-Regressive%20%28IAR%29%20technique%20at%20inference%20to%20facilitate%20long%20video%0Ageneration.%20Unlike%20existing%20attempts%20that%20require%20the%20laborious%20and%20restrictive%0Aconstruction%20of%20a%20paired%20training%20dataset%2C%20severely%20limiting%20their%20scalability%2C%0AVITON-DiT%20alleviates%20this%20by%20relying%20solely%20on%20unpaired%20human%20dance%20videos%20and%0Aa%20carefully%20designed%20multi-stage%20training%20strategy.%20Furthermore%2C%20we%20curate%20a%0Achallenging%20benchmark%20dataset%20to%20evaluate%20the%20performance%20of%20casual%20video%0Atry-on.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20VITON-DiT%20in%0Agenerating%20spatio-temporal%20consistent%20try-on%20results%20for%20in-the-wild%20videos%0Awith%20complicated%20human%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITON-DiT%253A%2520Learning%2520In-the-Wild%2520Video%2520Try-On%2520from%2520Human%2520Dance%2520Videos%2520via%250A%2520%2520Diffusion%2520Transformers%26entry.906535625%3DJun%2520Zheng%2520and%2520Fuwei%2520Zhao%2520and%2520Youjiang%2520Xu%2520and%2520Xin%2520Dong%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Video%2520try-on%2520stands%2520as%2520a%2520promising%2520area%2520for%2520its%2520tremendous%2520real-world%250Apotential.%2520Prior%2520works%2520are%2520limited%2520to%2520transferring%2520product%2520clothing%2520images%2520onto%250Aperson%2520videos%2520with%2520simple%2520poses%2520and%2520backgrounds%252C%2520while%2520underperforming%2520on%250Acasually%2520captured%2520videos.%2520Recently%252C%2520Sora%2520revealed%2520the%2520scalability%2520of%2520Diffusion%250ATransformer%2520%2528DiT%2529%2520in%2520generating%2520lifelike%2520videos%2520featuring%2520real-world%2520scenarios.%250AInspired%2520by%2520this%252C%2520we%2520explore%2520and%2520propose%2520the%2520first%2520DiT-based%2520video%2520try-on%250Aframework%2520for%2520practical%2520in-the-wild%2520applications%252C%2520named%2520VITON-DiT.%250ASpecifically%252C%2520VITON-DiT%2520consists%2520of%2520a%2520garment%2520extractor%252C%2520a%2520Spatial-Temporal%250Adenoising%2520DiT%252C%2520and%2520an%2520identity%2520preservation%2520ControlNet.%2520To%2520faithfully%2520recover%250Athe%2520clothing%2520details%252C%2520the%2520extracted%2520garment%2520features%2520are%2520fused%2520with%2520the%250Aself-attention%2520outputs%2520of%2520the%2520denoising%2520DiT%2520and%2520the%2520ControlNet.%2520We%2520also%250Aintroduce%2520novel%2520random%2520selection%2520strategies%2520during%2520training%2520and%2520an%2520Interpolated%250AAuto-Regressive%2520%2528IAR%2529%2520technique%2520at%2520inference%2520to%2520facilitate%2520long%2520video%250Ageneration.%2520Unlike%2520existing%2520attempts%2520that%2520require%2520the%2520laborious%2520and%2520restrictive%250Aconstruction%2520of%2520a%2520paired%2520training%2520dataset%252C%2520severely%2520limiting%2520their%2520scalability%252C%250AVITON-DiT%2520alleviates%2520this%2520by%2520relying%2520solely%2520on%2520unpaired%2520human%2520dance%2520videos%2520and%250Aa%2520carefully%2520designed%2520multi-stage%2520training%2520strategy.%2520Furthermore%252C%2520we%2520curate%2520a%250Achallenging%2520benchmark%2520dataset%2520to%2520evaluate%2520the%2520performance%2520of%2520casual%2520video%250Atry-on.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520VITON-DiT%2520in%250Agenerating%2520spatio-temporal%2520consistent%2520try-on%2520results%2520for%2520in-the-wild%2520videos%250Awith%2520complicated%2520human%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITON-DiT%3A%20Learning%20In-the-Wild%20Video%20Try-On%20from%20Human%20Dance%20Videos%20via%0A%20%20Diffusion%20Transformers&entry.906535625=Jun%20Zheng%20and%20Fuwei%20Zhao%20and%20Youjiang%20Xu%20and%20Xin%20Dong%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Video%20try-on%20stands%20as%20a%20promising%20area%20for%20its%20tremendous%20real-world%0Apotential.%20Prior%20works%20are%20limited%20to%20transferring%20product%20clothing%20images%20onto%0Aperson%20videos%20with%20simple%20poses%20and%20backgrounds%2C%20while%20underperforming%20on%0Acasually%20captured%20videos.%20Recently%2C%20Sora%20revealed%20the%20scalability%20of%20Diffusion%0ATransformer%20%28DiT%29%20in%20generating%20lifelike%20videos%20featuring%20real-world%20scenarios.%0AInspired%20by%20this%2C%20we%20explore%20and%20propose%20the%20first%20DiT-based%20video%20try-on%0Aframework%20for%20practical%20in-the-wild%20applications%2C%20named%20VITON-DiT.%0ASpecifically%2C%20VITON-DiT%20consists%20of%20a%20garment%20extractor%2C%20a%20Spatial-Temporal%0Adenoising%20DiT%2C%20and%20an%20identity%20preservation%20ControlNet.%20To%20faithfully%20recover%0Athe%20clothing%20details%2C%20the%20extracted%20garment%20features%20are%20fused%20with%20the%0Aself-attention%20outputs%20of%20the%20denoising%20DiT%20and%20the%20ControlNet.%20We%20also%0Aintroduce%20novel%20random%20selection%20strategies%20during%20training%20and%20an%20Interpolated%0AAuto-Regressive%20%28IAR%29%20technique%20at%20inference%20to%20facilitate%20long%20video%0Ageneration.%20Unlike%20existing%20attempts%20that%20require%20the%20laborious%20and%20restrictive%0Aconstruction%20of%20a%20paired%20training%20dataset%2C%20severely%20limiting%20their%20scalability%2C%0AVITON-DiT%20alleviates%20this%20by%20relying%20solely%20on%20unpaired%20human%20dance%20videos%20and%0Aa%20carefully%20designed%20multi-stage%20training%20strategy.%20Furthermore%2C%20we%20curate%20a%0Achallenging%20benchmark%20dataset%20to%20evaluate%20the%20performance%20of%20casual%20video%0Atry-on.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20VITON-DiT%20in%0Agenerating%20spatio-temporal%20consistent%20try-on%20results%20for%20in-the-wild%20videos%0Awith%20complicated%20human%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18326v2&entry.124074799=Read"},
{"title": "Towards a theory of out-of-distribution learning", "author": "Jayanta Dey and Ali Geisa and Ronak Mehta and Tyler M. Tomita and Hayden S. Helm and Haoyin Xu and Eric Eaton and Jeffery Dick and Carey E. Priebe and Joshua T. Vogelstein", "abstract": "  Learning is a process wherein a learning agent enhances its performance\nthrough exposure of experience or data. Throughout this journey, the agent may\nencounter diverse learning environments. For example, data may be presented to\nthe leaner all at once, in multiple batches, or sequentially. Furthermore, the\ndistribution of each data sample could be either identical and independent\n(iid) or non-iid. Additionally, there may exist computational and space\nconstraints for the deployment of the learning algorithms. The complexity of a\nlearning task can vary significantly, depending on the learning setup and the\nconstraints imposed upon it. However, it is worth noting that the current\nliterature lacks formal definitions for many of the in-distribution and\nout-of-distribution learning paradigms. Establishing proper and universally\nagreed-upon definitions for these learning setups is essential for thoroughly\nexploring the evolution of ideas across different learning scenarios and\nderiving generalized mathematical bounds for these learners. In this paper, we\naim to address this issue by proposing a chronological approach to defining\ndifferent learning tasks using the provably approximately correct (PAC)\nlearning framework. We will start with in-distribution learning and progress to\nrecently proposed lifelong or continual learning. We employ consistent\nterminology and notation to demonstrate how each of these learning frameworks\nrepresents a specific instance of a broader, more generalized concept of\nlearnability. Our hope is that this work will inspire a universally agreed-upon\napproach to quantifying different types of learning, fostering greater\nunderstanding and progress in the field.\n", "link": "http://arxiv.org/abs/2109.14501v5", "date": "2024-06-07", "relevancy": 1.9507, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5079}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4788}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20theory%20of%20out-of-distribution%20learning&body=Title%3A%20Towards%20a%20theory%20of%20out-of-distribution%20learning%0AAuthor%3A%20Jayanta%20Dey%20and%20Ali%20Geisa%20and%20Ronak%20Mehta%20and%20Tyler%20M.%20Tomita%20and%20Hayden%20S.%20Helm%20and%20Haoyin%20Xu%20and%20Eric%20Eaton%20and%20Jeffery%20Dick%20and%20Carey%20E.%20Priebe%20and%20Joshua%20T.%20Vogelstein%0AAbstract%3A%20%20%20Learning%20is%20a%20process%20wherein%20a%20learning%20agent%20enhances%20its%20performance%0Athrough%20exposure%20of%20experience%20or%20data.%20Throughout%20this%20journey%2C%20the%20agent%20may%0Aencounter%20diverse%20learning%20environments.%20For%20example%2C%20data%20may%20be%20presented%20to%0Athe%20leaner%20all%20at%20once%2C%20in%20multiple%20batches%2C%20or%20sequentially.%20Furthermore%2C%20the%0Adistribution%20of%20each%20data%20sample%20could%20be%20either%20identical%20and%20independent%0A%28iid%29%20or%20non-iid.%20Additionally%2C%20there%20may%20exist%20computational%20and%20space%0Aconstraints%20for%20the%20deployment%20of%20the%20learning%20algorithms.%20The%20complexity%20of%20a%0Alearning%20task%20can%20vary%20significantly%2C%20depending%20on%20the%20learning%20setup%20and%20the%0Aconstraints%20imposed%20upon%20it.%20However%2C%20it%20is%20worth%20noting%20that%20the%20current%0Aliterature%20lacks%20formal%20definitions%20for%20many%20of%20the%20in-distribution%20and%0Aout-of-distribution%20learning%20paradigms.%20Establishing%20proper%20and%20universally%0Aagreed-upon%20definitions%20for%20these%20learning%20setups%20is%20essential%20for%20thoroughly%0Aexploring%20the%20evolution%20of%20ideas%20across%20different%20learning%20scenarios%20and%0Aderiving%20generalized%20mathematical%20bounds%20for%20these%20learners.%20In%20this%20paper%2C%20we%0Aaim%20to%20address%20this%20issue%20by%20proposing%20a%20chronological%20approach%20to%20defining%0Adifferent%20learning%20tasks%20using%20the%20provably%20approximately%20correct%20%28PAC%29%0Alearning%20framework.%20We%20will%20start%20with%20in-distribution%20learning%20and%20progress%20to%0Arecently%20proposed%20lifelong%20or%20continual%20learning.%20We%20employ%20consistent%0Aterminology%20and%20notation%20to%20demonstrate%20how%20each%20of%20these%20learning%20frameworks%0Arepresents%20a%20specific%20instance%20of%20a%20broader%2C%20more%20generalized%20concept%20of%0Alearnability.%20Our%20hope%20is%20that%20this%20work%20will%20inspire%20a%20universally%20agreed-upon%0Aapproach%20to%20quantifying%20different%20types%20of%20learning%2C%20fostering%20greater%0Aunderstanding%20and%20progress%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.14501v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520theory%2520of%2520out-of-distribution%2520learning%26entry.906535625%3DJayanta%2520Dey%2520and%2520Ali%2520Geisa%2520and%2520Ronak%2520Mehta%2520and%2520Tyler%2520M.%2520Tomita%2520and%2520Hayden%2520S.%2520Helm%2520and%2520Haoyin%2520Xu%2520and%2520Eric%2520Eaton%2520and%2520Jeffery%2520Dick%2520and%2520Carey%2520E.%2520Priebe%2520and%2520Joshua%2520T.%2520Vogelstein%26entry.1292438233%3D%2520%2520Learning%2520is%2520a%2520process%2520wherein%2520a%2520learning%2520agent%2520enhances%2520its%2520performance%250Athrough%2520exposure%2520of%2520experience%2520or%2520data.%2520Throughout%2520this%2520journey%252C%2520the%2520agent%2520may%250Aencounter%2520diverse%2520learning%2520environments.%2520For%2520example%252C%2520data%2520may%2520be%2520presented%2520to%250Athe%2520leaner%2520all%2520at%2520once%252C%2520in%2520multiple%2520batches%252C%2520or%2520sequentially.%2520Furthermore%252C%2520the%250Adistribution%2520of%2520each%2520data%2520sample%2520could%2520be%2520either%2520identical%2520and%2520independent%250A%2528iid%2529%2520or%2520non-iid.%2520Additionally%252C%2520there%2520may%2520exist%2520computational%2520and%2520space%250Aconstraints%2520for%2520the%2520deployment%2520of%2520the%2520learning%2520algorithms.%2520The%2520complexity%2520of%2520a%250Alearning%2520task%2520can%2520vary%2520significantly%252C%2520depending%2520on%2520the%2520learning%2520setup%2520and%2520the%250Aconstraints%2520imposed%2520upon%2520it.%2520However%252C%2520it%2520is%2520worth%2520noting%2520that%2520the%2520current%250Aliterature%2520lacks%2520formal%2520definitions%2520for%2520many%2520of%2520the%2520in-distribution%2520and%250Aout-of-distribution%2520learning%2520paradigms.%2520Establishing%2520proper%2520and%2520universally%250Aagreed-upon%2520definitions%2520for%2520these%2520learning%2520setups%2520is%2520essential%2520for%2520thoroughly%250Aexploring%2520the%2520evolution%2520of%2520ideas%2520across%2520different%2520learning%2520scenarios%2520and%250Aderiving%2520generalized%2520mathematical%2520bounds%2520for%2520these%2520learners.%2520In%2520this%2520paper%252C%2520we%250Aaim%2520to%2520address%2520this%2520issue%2520by%2520proposing%2520a%2520chronological%2520approach%2520to%2520defining%250Adifferent%2520learning%2520tasks%2520using%2520the%2520provably%2520approximately%2520correct%2520%2528PAC%2529%250Alearning%2520framework.%2520We%2520will%2520start%2520with%2520in-distribution%2520learning%2520and%2520progress%2520to%250Arecently%2520proposed%2520lifelong%2520or%2520continual%2520learning.%2520We%2520employ%2520consistent%250Aterminology%2520and%2520notation%2520to%2520demonstrate%2520how%2520each%2520of%2520these%2520learning%2520frameworks%250Arepresents%2520a%2520specific%2520instance%2520of%2520a%2520broader%252C%2520more%2520generalized%2520concept%2520of%250Alearnability.%2520Our%2520hope%2520is%2520that%2520this%2520work%2520will%2520inspire%2520a%2520universally%2520agreed-upon%250Aapproach%2520to%2520quantifying%2520different%2520types%2520of%2520learning%252C%2520fostering%2520greater%250Aunderstanding%2520and%2520progress%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2109.14501v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20theory%20of%20out-of-distribution%20learning&entry.906535625=Jayanta%20Dey%20and%20Ali%20Geisa%20and%20Ronak%20Mehta%20and%20Tyler%20M.%20Tomita%20and%20Hayden%20S.%20Helm%20and%20Haoyin%20Xu%20and%20Eric%20Eaton%20and%20Jeffery%20Dick%20and%20Carey%20E.%20Priebe%20and%20Joshua%20T.%20Vogelstein&entry.1292438233=%20%20Learning%20is%20a%20process%20wherein%20a%20learning%20agent%20enhances%20its%20performance%0Athrough%20exposure%20of%20experience%20or%20data.%20Throughout%20this%20journey%2C%20the%20agent%20may%0Aencounter%20diverse%20learning%20environments.%20For%20example%2C%20data%20may%20be%20presented%20to%0Athe%20leaner%20all%20at%20once%2C%20in%20multiple%20batches%2C%20or%20sequentially.%20Furthermore%2C%20the%0Adistribution%20of%20each%20data%20sample%20could%20be%20either%20identical%20and%20independent%0A%28iid%29%20or%20non-iid.%20Additionally%2C%20there%20may%20exist%20computational%20and%20space%0Aconstraints%20for%20the%20deployment%20of%20the%20learning%20algorithms.%20The%20complexity%20of%20a%0Alearning%20task%20can%20vary%20significantly%2C%20depending%20on%20the%20learning%20setup%20and%20the%0Aconstraints%20imposed%20upon%20it.%20However%2C%20it%20is%20worth%20noting%20that%20the%20current%0Aliterature%20lacks%20formal%20definitions%20for%20many%20of%20the%20in-distribution%20and%0Aout-of-distribution%20learning%20paradigms.%20Establishing%20proper%20and%20universally%0Aagreed-upon%20definitions%20for%20these%20learning%20setups%20is%20essential%20for%20thoroughly%0Aexploring%20the%20evolution%20of%20ideas%20across%20different%20learning%20scenarios%20and%0Aderiving%20generalized%20mathematical%20bounds%20for%20these%20learners.%20In%20this%20paper%2C%20we%0Aaim%20to%20address%20this%20issue%20by%20proposing%20a%20chronological%20approach%20to%20defining%0Adifferent%20learning%20tasks%20using%20the%20provably%20approximately%20correct%20%28PAC%29%0Alearning%20framework.%20We%20will%20start%20with%20in-distribution%20learning%20and%20progress%20to%0Arecently%20proposed%20lifelong%20or%20continual%20learning.%20We%20employ%20consistent%0Aterminology%20and%20notation%20to%20demonstrate%20how%20each%20of%20these%20learning%20frameworks%0Arepresents%20a%20specific%20instance%20of%20a%20broader%2C%20more%20generalized%20concept%20of%0Alearnability.%20Our%20hope%20is%20that%20this%20work%20will%20inspire%20a%20universally%20agreed-upon%0Aapproach%20to%20quantifying%20different%20types%20of%20learning%2C%20fostering%20greater%0Aunderstanding%20and%20progress%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.14501v5&entry.124074799=Read"},
{"title": "The Price of Implicit Bias in Adversarially Robust Generalization", "author": "Nikolaos Tsilivis and Natalie Frank and Nathan Srebro and Julia Kempe", "abstract": "  We study the implicit bias of optimization in robust empirical risk\nminimization (robust ERM) and its connection with robust generalization. In\nclassification settings under adversarial perturbations with linear models, we\nstudy what type of regularization should ideally be applied for a given\nperturbation set to improve (robust) generalization. We then show that the\nimplicit bias of optimization in robust ERM can significantly affect the\nrobustness of the model and identify two ways this can happen; either through\nthe optimization algorithm or the architecture. We verify our predictions in\nsimulations with synthetic data and experimentally study the importance of\nimplicit bias in robust ERM with deep neural networks.\n", "link": "http://arxiv.org/abs/2406.04981v1", "date": "2024-06-07", "relevancy": 1.9464, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4921}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Price%20of%20Implicit%20Bias%20in%20Adversarially%20Robust%20Generalization&body=Title%3A%20The%20Price%20of%20Implicit%20Bias%20in%20Adversarially%20Robust%20Generalization%0AAuthor%3A%20Nikolaos%20Tsilivis%20and%20Natalie%20Frank%20and%20Nathan%20Srebro%20and%20Julia%20Kempe%0AAbstract%3A%20%20%20We%20study%20the%20implicit%20bias%20of%20optimization%20in%20robust%20empirical%20risk%0Aminimization%20%28robust%20ERM%29%20and%20its%20connection%20with%20robust%20generalization.%20In%0Aclassification%20settings%20under%20adversarial%20perturbations%20with%20linear%20models%2C%20we%0Astudy%20what%20type%20of%20regularization%20should%20ideally%20be%20applied%20for%20a%20given%0Aperturbation%20set%20to%20improve%20%28robust%29%20generalization.%20We%20then%20show%20that%20the%0Aimplicit%20bias%20of%20optimization%20in%20robust%20ERM%20can%20significantly%20affect%20the%0Arobustness%20of%20the%20model%20and%20identify%20two%20ways%20this%20can%20happen%3B%20either%20through%0Athe%20optimization%20algorithm%20or%20the%20architecture.%20We%20verify%20our%20predictions%20in%0Asimulations%20with%20synthetic%20data%20and%20experimentally%20study%20the%20importance%20of%0Aimplicit%20bias%20in%20robust%20ERM%20with%20deep%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Price%2520of%2520Implicit%2520Bias%2520in%2520Adversarially%2520Robust%2520Generalization%26entry.906535625%3DNikolaos%2520Tsilivis%2520and%2520Natalie%2520Frank%2520and%2520Nathan%2520Srebro%2520and%2520Julia%2520Kempe%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520implicit%2520bias%2520of%2520optimization%2520in%2520robust%2520empirical%2520risk%250Aminimization%2520%2528robust%2520ERM%2529%2520and%2520its%2520connection%2520with%2520robust%2520generalization.%2520In%250Aclassification%2520settings%2520under%2520adversarial%2520perturbations%2520with%2520linear%2520models%252C%2520we%250Astudy%2520what%2520type%2520of%2520regularization%2520should%2520ideally%2520be%2520applied%2520for%2520a%2520given%250Aperturbation%2520set%2520to%2520improve%2520%2528robust%2529%2520generalization.%2520We%2520then%2520show%2520that%2520the%250Aimplicit%2520bias%2520of%2520optimization%2520in%2520robust%2520ERM%2520can%2520significantly%2520affect%2520the%250Arobustness%2520of%2520the%2520model%2520and%2520identify%2520two%2520ways%2520this%2520can%2520happen%253B%2520either%2520through%250Athe%2520optimization%2520algorithm%2520or%2520the%2520architecture.%2520We%2520verify%2520our%2520predictions%2520in%250Asimulations%2520with%2520synthetic%2520data%2520and%2520experimentally%2520study%2520the%2520importance%2520of%250Aimplicit%2520bias%2520in%2520robust%2520ERM%2520with%2520deep%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Price%20of%20Implicit%20Bias%20in%20Adversarially%20Robust%20Generalization&entry.906535625=Nikolaos%20Tsilivis%20and%20Natalie%20Frank%20and%20Nathan%20Srebro%20and%20Julia%20Kempe&entry.1292438233=%20%20We%20study%20the%20implicit%20bias%20of%20optimization%20in%20robust%20empirical%20risk%0Aminimization%20%28robust%20ERM%29%20and%20its%20connection%20with%20robust%20generalization.%20In%0Aclassification%20settings%20under%20adversarial%20perturbations%20with%20linear%20models%2C%20we%0Astudy%20what%20type%20of%20regularization%20should%20ideally%20be%20applied%20for%20a%20given%0Aperturbation%20set%20to%20improve%20%28robust%29%20generalization.%20We%20then%20show%20that%20the%0Aimplicit%20bias%20of%20optimization%20in%20robust%20ERM%20can%20significantly%20affect%20the%0Arobustness%20of%20the%20model%20and%20identify%20two%20ways%20this%20can%20happen%3B%20either%20through%0Athe%20optimization%20algorithm%20or%20the%20architecture.%20We%20verify%20our%20predictions%20in%0Asimulations%20with%20synthetic%20data%20and%20experimentally%20study%20the%20importance%20of%0Aimplicit%20bias%20in%20robust%20ERM%20with%20deep%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04981v1&entry.124074799=Read"},
{"title": "CTBENCH: A Library and Benchmark for Certified Training", "author": "Yuhao Mao and Stefan Balauca and Martin Vechev", "abstract": "  Training certifiably robust neural networks is an important but challenging\ntask. While many algorithms for (deterministic) certified training have been\nproposed, they are often evaluated on different training schedules,\ncertification methods, and systematically under-tuned hyperparameters, making\nit difficult to compare their performance. To address this challenge, we\nintroduce CTBENCH, a unified library and a high-quality benchmark for certified\ntraining that evaluates all algorithms under fair settings and systematically\ntuned hyperparameters. We show that (1) almost all algorithms in CTBENCH\nsurpass the corresponding reported performance in literature in the magnitude\nof algorithmic improvements, thus establishing new state-of-the-art, and (2)\nthe claimed advantage of recent algorithms drops significantly when we enhance\nthe outdated baselines with a fair training schedule, a fair certification\nmethod and well-tuned hyperparameters. Based on CTBENCH, we provide new\ninsights into the current state of certified training and suggest future\nresearch directions. We are confident that CTBENCH will serve as a benchmark\nand testbed for future research in certified training.\n", "link": "http://arxiv.org/abs/2406.04848v1", "date": "2024-06-07", "relevancy": 1.9396, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5262}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training&body=Title%3A%20CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training%0AAuthor%3A%20Yuhao%20Mao%20and%20Stefan%20Balauca%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Training%20certifiably%20robust%20neural%20networks%20is%20an%20important%20but%20challenging%0Atask.%20While%20many%20algorithms%20for%20%28deterministic%29%20certified%20training%20have%20been%0Aproposed%2C%20they%20are%20often%20evaluated%20on%20different%20training%20schedules%2C%0Acertification%20methods%2C%20and%20systematically%20under-tuned%20hyperparameters%2C%20making%0Ait%20difficult%20to%20compare%20their%20performance.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20CTBENCH%2C%20a%20unified%20library%20and%20a%20high-quality%20benchmark%20for%20certified%0Atraining%20that%20evaluates%20all%20algorithms%20under%20fair%20settings%20and%20systematically%0Atuned%20hyperparameters.%20We%20show%20that%20%281%29%20almost%20all%20algorithms%20in%20CTBENCH%0Asurpass%20the%20corresponding%20reported%20performance%20in%20literature%20in%20the%20magnitude%0Aof%20algorithmic%20improvements%2C%20thus%20establishing%20new%20state-of-the-art%2C%20and%20%282%29%0Athe%20claimed%20advantage%20of%20recent%20algorithms%20drops%20significantly%20when%20we%20enhance%0Athe%20outdated%20baselines%20with%20a%20fair%20training%20schedule%2C%20a%20fair%20certification%0Amethod%20and%20well-tuned%20hyperparameters.%20Based%20on%20CTBENCH%2C%20we%20provide%20new%0Ainsights%20into%20the%20current%20state%20of%20certified%20training%20and%20suggest%20future%0Aresearch%20directions.%20We%20are%20confident%20that%20CTBENCH%20will%20serve%20as%20a%20benchmark%0Aand%20testbed%20for%20future%20research%20in%20certified%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTBENCH%253A%2520A%2520Library%2520and%2520Benchmark%2520for%2520Certified%2520Training%26entry.906535625%3DYuhao%2520Mao%2520and%2520Stefan%2520Balauca%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Training%2520certifiably%2520robust%2520neural%2520networks%2520is%2520an%2520important%2520but%2520challenging%250Atask.%2520While%2520many%2520algorithms%2520for%2520%2528deterministic%2529%2520certified%2520training%2520have%2520been%250Aproposed%252C%2520they%2520are%2520often%2520evaluated%2520on%2520different%2520training%2520schedules%252C%250Acertification%2520methods%252C%2520and%2520systematically%2520under-tuned%2520hyperparameters%252C%2520making%250Ait%2520difficult%2520to%2520compare%2520their%2520performance.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520CTBENCH%252C%2520a%2520unified%2520library%2520and%2520a%2520high-quality%2520benchmark%2520for%2520certified%250Atraining%2520that%2520evaluates%2520all%2520algorithms%2520under%2520fair%2520settings%2520and%2520systematically%250Atuned%2520hyperparameters.%2520We%2520show%2520that%2520%25281%2529%2520almost%2520all%2520algorithms%2520in%2520CTBENCH%250Asurpass%2520the%2520corresponding%2520reported%2520performance%2520in%2520literature%2520in%2520the%2520magnitude%250Aof%2520algorithmic%2520improvements%252C%2520thus%2520establishing%2520new%2520state-of-the-art%252C%2520and%2520%25282%2529%250Athe%2520claimed%2520advantage%2520of%2520recent%2520algorithms%2520drops%2520significantly%2520when%2520we%2520enhance%250Athe%2520outdated%2520baselines%2520with%2520a%2520fair%2520training%2520schedule%252C%2520a%2520fair%2520certification%250Amethod%2520and%2520well-tuned%2520hyperparameters.%2520Based%2520on%2520CTBENCH%252C%2520we%2520provide%2520new%250Ainsights%2520into%2520the%2520current%2520state%2520of%2520certified%2520training%2520and%2520suggest%2520future%250Aresearch%2520directions.%2520We%2520are%2520confident%2520that%2520CTBENCH%2520will%2520serve%2520as%2520a%2520benchmark%250Aand%2520testbed%2520for%2520future%2520research%2520in%2520certified%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTBENCH%3A%20A%20Library%20and%20Benchmark%20for%20Certified%20Training&entry.906535625=Yuhao%20Mao%20and%20Stefan%20Balauca%20and%20Martin%20Vechev&entry.1292438233=%20%20Training%20certifiably%20robust%20neural%20networks%20is%20an%20important%20but%20challenging%0Atask.%20While%20many%20algorithms%20for%20%28deterministic%29%20certified%20training%20have%20been%0Aproposed%2C%20they%20are%20often%20evaluated%20on%20different%20training%20schedules%2C%0Acertification%20methods%2C%20and%20systematically%20under-tuned%20hyperparameters%2C%20making%0Ait%20difficult%20to%20compare%20their%20performance.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20CTBENCH%2C%20a%20unified%20library%20and%20a%20high-quality%20benchmark%20for%20certified%0Atraining%20that%20evaluates%20all%20algorithms%20under%20fair%20settings%20and%20systematically%0Atuned%20hyperparameters.%20We%20show%20that%20%281%29%20almost%20all%20algorithms%20in%20CTBENCH%0Asurpass%20the%20corresponding%20reported%20performance%20in%20literature%20in%20the%20magnitude%0Aof%20algorithmic%20improvements%2C%20thus%20establishing%20new%20state-of-the-art%2C%20and%20%282%29%0Athe%20claimed%20advantage%20of%20recent%20algorithms%20drops%20significantly%20when%20we%20enhance%0Athe%20outdated%20baselines%20with%20a%20fair%20training%20schedule%2C%20a%20fair%20certification%0Amethod%20and%20well-tuned%20hyperparameters.%20Based%20on%20CTBENCH%2C%20we%20provide%20new%0Ainsights%20into%20the%20current%20state%20of%20certified%20training%20and%20suggest%20future%0Aresearch%20directions.%20We%20are%20confident%20that%20CTBENCH%20will%20serve%20as%20a%20benchmark%0Aand%20testbed%20for%20future%20research%20in%20certified%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04848v1&entry.124074799=Read"},
{"title": "Learning mirror maps in policy mirror descent", "author": "Carlo Alfano and Sebastian Towers and Silvia Sapora and Chris Lu and Patrick Rebeschini", "abstract": "  Policy Mirror Descent (PMD) is a popular framework in reinforcement learning,\nserving as a unifying perspective that encompasses numerous algorithms. These\nalgorithms are derived through the selection of a mirror map and enjoy\nfinite-time convergence guarantees. Despite its popularity, the exploration of\nPMD's full potential is limited, with the majority of research focusing on a\nparticular mirror map -- namely, the negative entropy -- which gives rise to\nthe renowned Natural Policy Gradient (NPG) method. It remains uncertain from\nexisting theoretical studies whether the choice of mirror map significantly\ninfluences PMD's efficacy. In our work, we conduct empirical investigations to\nshow that the conventional mirror map choice (NPG) often yields\nless-than-optimal outcomes across several standard benchmark environments.\nUsing evolutionary strategies, we identify more efficient mirror maps that\nenhance the performance of PMD. We first focus on a tabular environment, i.e.\nGrid-World, where we relate existing theoretical bounds with the performance of\nPMD for a few standard mirror maps and the learned one. We then show that it is\npossible to learn a mirror map that outperforms the negative entropy in more\ncomplex environments, such as the MinAtar suite. Our results suggest that\nmirror maps generalize well across various environments, raising questions\nabout how to best match a mirror map to an environment's structure and\ncharacteristics.\n", "link": "http://arxiv.org/abs/2402.05187v2", "date": "2024-06-07", "relevancy": 1.9387, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5106}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20mirror%20maps%20in%20policy%20mirror%20descent&body=Title%3A%20Learning%20mirror%20maps%20in%20policy%20mirror%20descent%0AAuthor%3A%20Carlo%20Alfano%20and%20Sebastian%20Towers%20and%20Silvia%20Sapora%20and%20Chris%20Lu%20and%20Patrick%20Rebeschini%0AAbstract%3A%20%20%20Policy%20Mirror%20Descent%20%28PMD%29%20is%20a%20popular%20framework%20in%20reinforcement%20learning%2C%0Aserving%20as%20a%20unifying%20perspective%20that%20encompasses%20numerous%20algorithms.%20These%0Aalgorithms%20are%20derived%20through%20the%20selection%20of%20a%20mirror%20map%20and%20enjoy%0Afinite-time%20convergence%20guarantees.%20Despite%20its%20popularity%2C%20the%20exploration%20of%0APMD%27s%20full%20potential%20is%20limited%2C%20with%20the%20majority%20of%20research%20focusing%20on%20a%0Aparticular%20mirror%20map%20--%20namely%2C%20the%20negative%20entropy%20--%20which%20gives%20rise%20to%0Athe%20renowned%20Natural%20Policy%20Gradient%20%28NPG%29%20method.%20It%20remains%20uncertain%20from%0Aexisting%20theoretical%20studies%20whether%20the%20choice%20of%20mirror%20map%20significantly%0Ainfluences%20PMD%27s%20efficacy.%20In%20our%20work%2C%20we%20conduct%20empirical%20investigations%20to%0Ashow%20that%20the%20conventional%20mirror%20map%20choice%20%28NPG%29%20often%20yields%0Aless-than-optimal%20outcomes%20across%20several%20standard%20benchmark%20environments.%0AUsing%20evolutionary%20strategies%2C%20we%20identify%20more%20efficient%20mirror%20maps%20that%0Aenhance%20the%20performance%20of%20PMD.%20We%20first%20focus%20on%20a%20tabular%20environment%2C%20i.e.%0AGrid-World%2C%20where%20we%20relate%20existing%20theoretical%20bounds%20with%20the%20performance%20of%0APMD%20for%20a%20few%20standard%20mirror%20maps%20and%20the%20learned%20one.%20We%20then%20show%20that%20it%20is%0Apossible%20to%20learn%20a%20mirror%20map%20that%20outperforms%20the%20negative%20entropy%20in%20more%0Acomplex%20environments%2C%20such%20as%20the%20MinAtar%20suite.%20Our%20results%20suggest%20that%0Amirror%20maps%20generalize%20well%20across%20various%20environments%2C%20raising%20questions%0Aabout%20how%20to%20best%20match%20a%20mirror%20map%20to%20an%20environment%27s%20structure%20and%0Acharacteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520mirror%2520maps%2520in%2520policy%2520mirror%2520descent%26entry.906535625%3DCarlo%2520Alfano%2520and%2520Sebastian%2520Towers%2520and%2520Silvia%2520Sapora%2520and%2520Chris%2520Lu%2520and%2520Patrick%2520Rebeschini%26entry.1292438233%3D%2520%2520Policy%2520Mirror%2520Descent%2520%2528PMD%2529%2520is%2520a%2520popular%2520framework%2520in%2520reinforcement%2520learning%252C%250Aserving%2520as%2520a%2520unifying%2520perspective%2520that%2520encompasses%2520numerous%2520algorithms.%2520These%250Aalgorithms%2520are%2520derived%2520through%2520the%2520selection%2520of%2520a%2520mirror%2520map%2520and%2520enjoy%250Afinite-time%2520convergence%2520guarantees.%2520Despite%2520its%2520popularity%252C%2520the%2520exploration%2520of%250APMD%2527s%2520full%2520potential%2520is%2520limited%252C%2520with%2520the%2520majority%2520of%2520research%2520focusing%2520on%2520a%250Aparticular%2520mirror%2520map%2520--%2520namely%252C%2520the%2520negative%2520entropy%2520--%2520which%2520gives%2520rise%2520to%250Athe%2520renowned%2520Natural%2520Policy%2520Gradient%2520%2528NPG%2529%2520method.%2520It%2520remains%2520uncertain%2520from%250Aexisting%2520theoretical%2520studies%2520whether%2520the%2520choice%2520of%2520mirror%2520map%2520significantly%250Ainfluences%2520PMD%2527s%2520efficacy.%2520In%2520our%2520work%252C%2520we%2520conduct%2520empirical%2520investigations%2520to%250Ashow%2520that%2520the%2520conventional%2520mirror%2520map%2520choice%2520%2528NPG%2529%2520often%2520yields%250Aless-than-optimal%2520outcomes%2520across%2520several%2520standard%2520benchmark%2520environments.%250AUsing%2520evolutionary%2520strategies%252C%2520we%2520identify%2520more%2520efficient%2520mirror%2520maps%2520that%250Aenhance%2520the%2520performance%2520of%2520PMD.%2520We%2520first%2520focus%2520on%2520a%2520tabular%2520environment%252C%2520i.e.%250AGrid-World%252C%2520where%2520we%2520relate%2520existing%2520theoretical%2520bounds%2520with%2520the%2520performance%2520of%250APMD%2520for%2520a%2520few%2520standard%2520mirror%2520maps%2520and%2520the%2520learned%2520one.%2520We%2520then%2520show%2520that%2520it%2520is%250Apossible%2520to%2520learn%2520a%2520mirror%2520map%2520that%2520outperforms%2520the%2520negative%2520entropy%2520in%2520more%250Acomplex%2520environments%252C%2520such%2520as%2520the%2520MinAtar%2520suite.%2520Our%2520results%2520suggest%2520that%250Amirror%2520maps%2520generalize%2520well%2520across%2520various%2520environments%252C%2520raising%2520questions%250Aabout%2520how%2520to%2520best%2520match%2520a%2520mirror%2520map%2520to%2520an%2520environment%2527s%2520structure%2520and%250Acharacteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20mirror%20maps%20in%20policy%20mirror%20descent&entry.906535625=Carlo%20Alfano%20and%20Sebastian%20Towers%20and%20Silvia%20Sapora%20and%20Chris%20Lu%20and%20Patrick%20Rebeschini&entry.1292438233=%20%20Policy%20Mirror%20Descent%20%28PMD%29%20is%20a%20popular%20framework%20in%20reinforcement%20learning%2C%0Aserving%20as%20a%20unifying%20perspective%20that%20encompasses%20numerous%20algorithms.%20These%0Aalgorithms%20are%20derived%20through%20the%20selection%20of%20a%20mirror%20map%20and%20enjoy%0Afinite-time%20convergence%20guarantees.%20Despite%20its%20popularity%2C%20the%20exploration%20of%0APMD%27s%20full%20potential%20is%20limited%2C%20with%20the%20majority%20of%20research%20focusing%20on%20a%0Aparticular%20mirror%20map%20--%20namely%2C%20the%20negative%20entropy%20--%20which%20gives%20rise%20to%0Athe%20renowned%20Natural%20Policy%20Gradient%20%28NPG%29%20method.%20It%20remains%20uncertain%20from%0Aexisting%20theoretical%20studies%20whether%20the%20choice%20of%20mirror%20map%20significantly%0Ainfluences%20PMD%27s%20efficacy.%20In%20our%20work%2C%20we%20conduct%20empirical%20investigations%20to%0Ashow%20that%20the%20conventional%20mirror%20map%20choice%20%28NPG%29%20often%20yields%0Aless-than-optimal%20outcomes%20across%20several%20standard%20benchmark%20environments.%0AUsing%20evolutionary%20strategies%2C%20we%20identify%20more%20efficient%20mirror%20maps%20that%0Aenhance%20the%20performance%20of%20PMD.%20We%20first%20focus%20on%20a%20tabular%20environment%2C%20i.e.%0AGrid-World%2C%20where%20we%20relate%20existing%20theoretical%20bounds%20with%20the%20performance%20of%0APMD%20for%20a%20few%20standard%20mirror%20maps%20and%20the%20learned%20one.%20We%20then%20show%20that%20it%20is%0Apossible%20to%20learn%20a%20mirror%20map%20that%20outperforms%20the%20negative%20entropy%20in%20more%0Acomplex%20environments%2C%20such%20as%20the%20MinAtar%20suite.%20Our%20results%20suggest%20that%0Amirror%20maps%20generalize%20well%20across%20various%20environments%2C%20raising%20questions%0Aabout%20how%20to%20best%20match%20a%20mirror%20map%20to%20an%20environment%27s%20structure%20and%0Acharacteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05187v2&entry.124074799=Read"},
{"title": "BEADs: Bias Evaluation Across Domains", "author": "Shaina Raza and Mizanur Rahman and Michael R. Zhang", "abstract": "  Recent improvements in large language models (LLMs) have significantly\nenhanced natural language processing (NLP) applications. However, these models\ncan also inherit and perpetuate biases from their training data. Addressing\nthis issue is crucial, yet many existing datasets do not offer evaluation\nacross diverse NLP tasks. To tackle this, we introduce the Bias Evaluations\nAcross Domains (BEADs) dataset, designed to support a wide range of NLP tasks,\nincluding text classification, bias entity recognition, bias quantification,\nand benign language generation. BEADs uses AI-driven annotation combined with\nexperts' verification to provide reliable labels. This method overcomes the\nlimitations of existing datasets that typically depend on crowd-sourcing,\nexpert-only annotations with limited bias evaluations, or unverified AI\nlabeling. Our empirical analysis shows that BEADs is effective in detecting and\nreducing biases across different language models, with smaller models\nfine-tuned on BEADs often outperforming LLMs in bias classification tasks.\nHowever, these models may still exhibit biases towards certain demographics.\nFine-tuning LLMs with our benign language data also reduces biases while\npreserving the models' knowledge. Our findings highlight the importance of\ncomprehensive bias evaluation and the potential of targeted fine-tuning for\nreducing the bias of LLMs. We are making BEADs publicly available at\nhttps://huggingface.co/datasets/shainar/BEAD\n  Warning: This paper contains examples that may be considered offensive.\n", "link": "http://arxiv.org/abs/2406.04220v2", "date": "2024-06-07", "relevancy": 1.9348, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5047}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEADs%3A%20Bias%20Evaluation%20Across%20Domains&body=Title%3A%20BEADs%3A%20Bias%20Evaluation%20Across%20Domains%0AAuthor%3A%20Shaina%20Raza%20and%20Mizanur%20Rahman%20and%20Michael%20R.%20Zhang%0AAbstract%3A%20%20%20Recent%20improvements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aenhanced%20natural%20language%20processing%20%28NLP%29%20applications.%20However%2C%20these%20models%0Acan%20also%20inherit%20and%20perpetuate%20biases%20from%20their%20training%20data.%20Addressing%0Athis%20issue%20is%20crucial%2C%20yet%20many%20existing%20datasets%20do%20not%20offer%20evaluation%0Aacross%20diverse%20NLP%20tasks.%20To%20tackle%20this%2C%20we%20introduce%20the%20Bias%20Evaluations%0AAcross%20Domains%20%28BEADs%29%20dataset%2C%20designed%20to%20support%20a%20wide%20range%20of%20NLP%20tasks%2C%0Aincluding%20text%20classification%2C%20bias%20entity%20recognition%2C%20bias%20quantification%2C%0Aand%20benign%20language%20generation.%20BEADs%20uses%20AI-driven%20annotation%20combined%20with%0Aexperts%27%20verification%20to%20provide%20reliable%20labels.%20This%20method%20overcomes%20the%0Alimitations%20of%20existing%20datasets%20that%20typically%20depend%20on%20crowd-sourcing%2C%0Aexpert-only%20annotations%20with%20limited%20bias%20evaluations%2C%20or%20unverified%20AI%0Alabeling.%20Our%20empirical%20analysis%20shows%20that%20BEADs%20is%20effective%20in%20detecting%20and%0Areducing%20biases%20across%20different%20language%20models%2C%20with%20smaller%20models%0Afine-tuned%20on%20BEADs%20often%20outperforming%20LLMs%20in%20bias%20classification%20tasks.%0AHowever%2C%20these%20models%20may%20still%20exhibit%20biases%20towards%20certain%20demographics.%0AFine-tuning%20LLMs%20with%20our%20benign%20language%20data%20also%20reduces%20biases%20while%0Apreserving%20the%20models%27%20knowledge.%20Our%20findings%20highlight%20the%20importance%20of%0Acomprehensive%20bias%20evaluation%20and%20the%20potential%20of%20targeted%20fine-tuning%20for%0Areducing%20the%20bias%20of%20LLMs.%20We%20are%20making%20BEADs%20publicly%20available%20at%0Ahttps%3A//huggingface.co/datasets/shainar/BEAD%0A%20%20Warning%3A%20This%20paper%20contains%20examples%20that%20may%20be%20considered%20offensive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEADs%253A%2520Bias%2520Evaluation%2520Across%2520Domains%26entry.906535625%3DShaina%2520Raza%2520and%2520Mizanur%2520Rahman%2520and%2520Michael%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520improvements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%250Aenhanced%2520natural%2520language%2520processing%2520%2528NLP%2529%2520applications.%2520However%252C%2520these%2520models%250Acan%2520also%2520inherit%2520and%2520perpetuate%2520biases%2520from%2520their%2520training%2520data.%2520Addressing%250Athis%2520issue%2520is%2520crucial%252C%2520yet%2520many%2520existing%2520datasets%2520do%2520not%2520offer%2520evaluation%250Aacross%2520diverse%2520NLP%2520tasks.%2520To%2520tackle%2520this%252C%2520we%2520introduce%2520the%2520Bias%2520Evaluations%250AAcross%2520Domains%2520%2528BEADs%2529%2520dataset%252C%2520designed%2520to%2520support%2520a%2520wide%2520range%2520of%2520NLP%2520tasks%252C%250Aincluding%2520text%2520classification%252C%2520bias%2520entity%2520recognition%252C%2520bias%2520quantification%252C%250Aand%2520benign%2520language%2520generation.%2520BEADs%2520uses%2520AI-driven%2520annotation%2520combined%2520with%250Aexperts%2527%2520verification%2520to%2520provide%2520reliable%2520labels.%2520This%2520method%2520overcomes%2520the%250Alimitations%2520of%2520existing%2520datasets%2520that%2520typically%2520depend%2520on%2520crowd-sourcing%252C%250Aexpert-only%2520annotations%2520with%2520limited%2520bias%2520evaluations%252C%2520or%2520unverified%2520AI%250Alabeling.%2520Our%2520empirical%2520analysis%2520shows%2520that%2520BEADs%2520is%2520effective%2520in%2520detecting%2520and%250Areducing%2520biases%2520across%2520different%2520language%2520models%252C%2520with%2520smaller%2520models%250Afine-tuned%2520on%2520BEADs%2520often%2520outperforming%2520LLMs%2520in%2520bias%2520classification%2520tasks.%250AHowever%252C%2520these%2520models%2520may%2520still%2520exhibit%2520biases%2520towards%2520certain%2520demographics.%250AFine-tuning%2520LLMs%2520with%2520our%2520benign%2520language%2520data%2520also%2520reduces%2520biases%2520while%250Apreserving%2520the%2520models%2527%2520knowledge.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%250Acomprehensive%2520bias%2520evaluation%2520and%2520the%2520potential%2520of%2520targeted%2520fine-tuning%2520for%250Areducing%2520the%2520bias%2520of%2520LLMs.%2520We%2520are%2520making%2520BEADs%2520publicly%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/shainar/BEAD%250A%2520%2520Warning%253A%2520This%2520paper%2520contains%2520examples%2520that%2520may%2520be%2520considered%2520offensive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEADs%3A%20Bias%20Evaluation%20Across%20Domains&entry.906535625=Shaina%20Raza%20and%20Mizanur%20Rahman%20and%20Michael%20R.%20Zhang&entry.1292438233=%20%20Recent%20improvements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aenhanced%20natural%20language%20processing%20%28NLP%29%20applications.%20However%2C%20these%20models%0Acan%20also%20inherit%20and%20perpetuate%20biases%20from%20their%20training%20data.%20Addressing%0Athis%20issue%20is%20crucial%2C%20yet%20many%20existing%20datasets%20do%20not%20offer%20evaluation%0Aacross%20diverse%20NLP%20tasks.%20To%20tackle%20this%2C%20we%20introduce%20the%20Bias%20Evaluations%0AAcross%20Domains%20%28BEADs%29%20dataset%2C%20designed%20to%20support%20a%20wide%20range%20of%20NLP%20tasks%2C%0Aincluding%20text%20classification%2C%20bias%20entity%20recognition%2C%20bias%20quantification%2C%0Aand%20benign%20language%20generation.%20BEADs%20uses%20AI-driven%20annotation%20combined%20with%0Aexperts%27%20verification%20to%20provide%20reliable%20labels.%20This%20method%20overcomes%20the%0Alimitations%20of%20existing%20datasets%20that%20typically%20depend%20on%20crowd-sourcing%2C%0Aexpert-only%20annotations%20with%20limited%20bias%20evaluations%2C%20or%20unverified%20AI%0Alabeling.%20Our%20empirical%20analysis%20shows%20that%20BEADs%20is%20effective%20in%20detecting%20and%0Areducing%20biases%20across%20different%20language%20models%2C%20with%20smaller%20models%0Afine-tuned%20on%20BEADs%20often%20outperforming%20LLMs%20in%20bias%20classification%20tasks.%0AHowever%2C%20these%20models%20may%20still%20exhibit%20biases%20towards%20certain%20demographics.%0AFine-tuning%20LLMs%20with%20our%20benign%20language%20data%20also%20reduces%20biases%20while%0Apreserving%20the%20models%27%20knowledge.%20Our%20findings%20highlight%20the%20importance%20of%0Acomprehensive%20bias%20evaluation%20and%20the%20potential%20of%20targeted%20fine-tuning%20for%0Areducing%20the%20bias%20of%20LLMs.%20We%20are%20making%20BEADs%20publicly%20available%20at%0Ahttps%3A//huggingface.co/datasets/shainar/BEAD%0A%20%20Warning%3A%20This%20paper%20contains%20examples%20that%20may%20be%20considered%20offensive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04220v2&entry.124074799=Read"},
{"title": "A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph\n  Clustering", "author": "Vincent Cohen-Addad and Tommaso d'Orsi and Aida Mousavifar", "abstract": "  We consider the semi-random graph model of [Makarychev, Makarychev and\nVijayaraghavan, STOC'12], where, given a random bipartite graph with $\\alpha$\nedges and an unknown bipartition $(A, B)$ of the vertex set, an adversary can\nadd arbitrary edges inside each community and remove arbitrary edges from the\ncut $(A, B)$ (i.e. all adversarial changes are \\textit{monotone} with respect\nto the bipartition). For this model, a polynomial time algorithm is known to\napproximate the Balanced Cut problem up to value $O(\\alpha)$ [MMV'12] as long\nas the cut $(A, B)$ has size $\\Omega(\\alpha)$. However, it consists of slow\nsubroutines requiring optimal solutions for logarithmically many semidefinite\nprograms. We study the fine-grained complexity of the problem and present the\nfirst near-linear time algorithm that achieves similar performances to that of\n[MMV'12]. Our algorithm runs in time $O(|V(G)|^{1+o(1)} + |E(G)|^{1+o(1)})$ and\nfinds a balanced cut of value $O(\\alpha)$. Our approach appears easily\nextendible to related problem, such as Sparsest Cut, and also yields an\nnear-linear time $O(1)$-approximation to Dagupta's objective function for\nhierarchical clustering [Dasgupta, STOC'16] for the semi-random hierarchical\nstochastic block model inputs of [Cohen-Addad, Kanade, Mallmann-Trenn, Mathieu,\nJACM'19].\n", "link": "http://arxiv.org/abs/2406.04857v1", "date": "2024-06-07", "relevancy": 1.9338, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3852}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Near-Linear%20Time%20Approximation%20Algorithm%20for%20Beyond-Worst-Case%20Graph%0A%20%20Clustering&body=Title%3A%20A%20Near-Linear%20Time%20Approximation%20Algorithm%20for%20Beyond-Worst-Case%20Graph%0A%20%20Clustering%0AAuthor%3A%20Vincent%20Cohen-Addad%20and%20Tommaso%20d%27Orsi%20and%20Aida%20Mousavifar%0AAbstract%3A%20%20%20We%20consider%20the%20semi-random%20graph%20model%20of%20%5BMakarychev%2C%20Makarychev%20and%0AVijayaraghavan%2C%20STOC%2712%5D%2C%20where%2C%20given%20a%20random%20bipartite%20graph%20with%20%24%5Calpha%24%0Aedges%20and%20an%20unknown%20bipartition%20%24%28A%2C%20B%29%24%20of%20the%20vertex%20set%2C%20an%20adversary%20can%0Aadd%20arbitrary%20edges%20inside%20each%20community%20and%20remove%20arbitrary%20edges%20from%20the%0Acut%20%24%28A%2C%20B%29%24%20%28i.e.%20all%20adversarial%20changes%20are%20%5Ctextit%7Bmonotone%7D%20with%20respect%0Ato%20the%20bipartition%29.%20For%20this%20model%2C%20a%20polynomial%20time%20algorithm%20is%20known%20to%0Aapproximate%20the%20Balanced%20Cut%20problem%20up%20to%20value%20%24O%28%5Calpha%29%24%20%5BMMV%2712%5D%20as%20long%0Aas%20the%20cut%20%24%28A%2C%20B%29%24%20has%20size%20%24%5COmega%28%5Calpha%29%24.%20However%2C%20it%20consists%20of%20slow%0Asubroutines%20requiring%20optimal%20solutions%20for%20logarithmically%20many%20semidefinite%0Aprograms.%20We%20study%20the%20fine-grained%20complexity%20of%20the%20problem%20and%20present%20the%0Afirst%20near-linear%20time%20algorithm%20that%20achieves%20similar%20performances%20to%20that%20of%0A%5BMMV%2712%5D.%20Our%20algorithm%20runs%20in%20time%20%24O%28%7CV%28G%29%7C%5E%7B1%2Bo%281%29%7D%20%2B%20%7CE%28G%29%7C%5E%7B1%2Bo%281%29%7D%29%24%20and%0Afinds%20a%20balanced%20cut%20of%20value%20%24O%28%5Calpha%29%24.%20Our%20approach%20appears%20easily%0Aextendible%20to%20related%20problem%2C%20such%20as%20Sparsest%20Cut%2C%20and%20also%20yields%20an%0Anear-linear%20time%20%24O%281%29%24-approximation%20to%20Dagupta%27s%20objective%20function%20for%0Ahierarchical%20clustering%20%5BDasgupta%2C%20STOC%2716%5D%20for%20the%20semi-random%20hierarchical%0Astochastic%20block%20model%20inputs%20of%20%5BCohen-Addad%2C%20Kanade%2C%20Mallmann-Trenn%2C%20Mathieu%2C%0AJACM%2719%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Near-Linear%2520Time%2520Approximation%2520Algorithm%2520for%2520Beyond-Worst-Case%2520Graph%250A%2520%2520Clustering%26entry.906535625%3DVincent%2520Cohen-Addad%2520and%2520Tommaso%2520d%2527Orsi%2520and%2520Aida%2520Mousavifar%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520semi-random%2520graph%2520model%2520of%2520%255BMakarychev%252C%2520Makarychev%2520and%250AVijayaraghavan%252C%2520STOC%252712%255D%252C%2520where%252C%2520given%2520a%2520random%2520bipartite%2520graph%2520with%2520%2524%255Calpha%2524%250Aedges%2520and%2520an%2520unknown%2520bipartition%2520%2524%2528A%252C%2520B%2529%2524%2520of%2520the%2520vertex%2520set%252C%2520an%2520adversary%2520can%250Aadd%2520arbitrary%2520edges%2520inside%2520each%2520community%2520and%2520remove%2520arbitrary%2520edges%2520from%2520the%250Acut%2520%2524%2528A%252C%2520B%2529%2524%2520%2528i.e.%2520all%2520adversarial%2520changes%2520are%2520%255Ctextit%257Bmonotone%257D%2520with%2520respect%250Ato%2520the%2520bipartition%2529.%2520For%2520this%2520model%252C%2520a%2520polynomial%2520time%2520algorithm%2520is%2520known%2520to%250Aapproximate%2520the%2520Balanced%2520Cut%2520problem%2520up%2520to%2520value%2520%2524O%2528%255Calpha%2529%2524%2520%255BMMV%252712%255D%2520as%2520long%250Aas%2520the%2520cut%2520%2524%2528A%252C%2520B%2529%2524%2520has%2520size%2520%2524%255COmega%2528%255Calpha%2529%2524.%2520However%252C%2520it%2520consists%2520of%2520slow%250Asubroutines%2520requiring%2520optimal%2520solutions%2520for%2520logarithmically%2520many%2520semidefinite%250Aprograms.%2520We%2520study%2520the%2520fine-grained%2520complexity%2520of%2520the%2520problem%2520and%2520present%2520the%250Afirst%2520near-linear%2520time%2520algorithm%2520that%2520achieves%2520similar%2520performances%2520to%2520that%2520of%250A%255BMMV%252712%255D.%2520Our%2520algorithm%2520runs%2520in%2520time%2520%2524O%2528%257CV%2528G%2529%257C%255E%257B1%252Bo%25281%2529%257D%2520%252B%2520%257CE%2528G%2529%257C%255E%257B1%252Bo%25281%2529%257D%2529%2524%2520and%250Afinds%2520a%2520balanced%2520cut%2520of%2520value%2520%2524O%2528%255Calpha%2529%2524.%2520Our%2520approach%2520appears%2520easily%250Aextendible%2520to%2520related%2520problem%252C%2520such%2520as%2520Sparsest%2520Cut%252C%2520and%2520also%2520yields%2520an%250Anear-linear%2520time%2520%2524O%25281%2529%2524-approximation%2520to%2520Dagupta%2527s%2520objective%2520function%2520for%250Ahierarchical%2520clustering%2520%255BDasgupta%252C%2520STOC%252716%255D%2520for%2520the%2520semi-random%2520hierarchical%250Astochastic%2520block%2520model%2520inputs%2520of%2520%255BCohen-Addad%252C%2520Kanade%252C%2520Mallmann-Trenn%252C%2520Mathieu%252C%250AJACM%252719%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Near-Linear%20Time%20Approximation%20Algorithm%20for%20Beyond-Worst-Case%20Graph%0A%20%20Clustering&entry.906535625=Vincent%20Cohen-Addad%20and%20Tommaso%20d%27Orsi%20and%20Aida%20Mousavifar&entry.1292438233=%20%20We%20consider%20the%20semi-random%20graph%20model%20of%20%5BMakarychev%2C%20Makarychev%20and%0AVijayaraghavan%2C%20STOC%2712%5D%2C%20where%2C%20given%20a%20random%20bipartite%20graph%20with%20%24%5Calpha%24%0Aedges%20and%20an%20unknown%20bipartition%20%24%28A%2C%20B%29%24%20of%20the%20vertex%20set%2C%20an%20adversary%20can%0Aadd%20arbitrary%20edges%20inside%20each%20community%20and%20remove%20arbitrary%20edges%20from%20the%0Acut%20%24%28A%2C%20B%29%24%20%28i.e.%20all%20adversarial%20changes%20are%20%5Ctextit%7Bmonotone%7D%20with%20respect%0Ato%20the%20bipartition%29.%20For%20this%20model%2C%20a%20polynomial%20time%20algorithm%20is%20known%20to%0Aapproximate%20the%20Balanced%20Cut%20problem%20up%20to%20value%20%24O%28%5Calpha%29%24%20%5BMMV%2712%5D%20as%20long%0Aas%20the%20cut%20%24%28A%2C%20B%29%24%20has%20size%20%24%5COmega%28%5Calpha%29%24.%20However%2C%20it%20consists%20of%20slow%0Asubroutines%20requiring%20optimal%20solutions%20for%20logarithmically%20many%20semidefinite%0Aprograms.%20We%20study%20the%20fine-grained%20complexity%20of%20the%20problem%20and%20present%20the%0Afirst%20near-linear%20time%20algorithm%20that%20achieves%20similar%20performances%20to%20that%20of%0A%5BMMV%2712%5D.%20Our%20algorithm%20runs%20in%20time%20%24O%28%7CV%28G%29%7C%5E%7B1%2Bo%281%29%7D%20%2B%20%7CE%28G%29%7C%5E%7B1%2Bo%281%29%7D%29%24%20and%0Afinds%20a%20balanced%20cut%20of%20value%20%24O%28%5Calpha%29%24.%20Our%20approach%20appears%20easily%0Aextendible%20to%20related%20problem%2C%20such%20as%20Sparsest%20Cut%2C%20and%20also%20yields%20an%0Anear-linear%20time%20%24O%281%29%24-approximation%20to%20Dagupta%27s%20objective%20function%20for%0Ahierarchical%20clustering%20%5BDasgupta%2C%20STOC%2716%5D%20for%20the%20semi-random%20hierarchical%0Astochastic%20block%20model%20inputs%20of%20%5BCohen-Addad%2C%20Kanade%2C%20Mallmann-Trenn%2C%20Mathieu%2C%0AJACM%2719%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04857v1&entry.124074799=Read"},
{"title": "LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety\n  Assessment", "author": "Lukas Helff and Felix Friedrich and Manuel Brack and Kristian Kersting and Patrick Schramowski", "abstract": "  We introduce LlavaGuard, a family of VLM-based safeguard models, offering a\nversatile framework for evaluating the safety compliance of visual content.\nSpecifically, we designed LlavaGuard for dataset annotation and generative\nmodel safeguarding. To this end, we collected and annotated a high-quality\nvisual dataset incorporating a broad safety taxonomy, which we use to tune VLMs\non context-aware safety risks. As a key innovation, LlavaGuard's new responses\ncontain comprehensive information, including a safety rating, the violated\nsafety categories, and an in-depth rationale. Further, our introduced\ncustomizable taxonomy categories enable the context-specific alignment of\nLlavaGuard to various scenarios. Our experiments highlight the capabilities of\nLlavaGuard in complex and real-world applications. We provide checkpoints\nranging from 7B to 34B parameters demonstrating state-of-the-art performance,\nwith even the smallest models outperforming baselines like GPT-4. We make our\ndataset and model weights publicly available and invite further research to\naddress the diverse needs of communities and contexts.\n", "link": "http://arxiv.org/abs/2406.05113v1", "date": "2024-06-07", "relevancy": 1.9327, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5296}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLavaGuard%3A%20VLM-based%20Safeguards%20for%20Vision%20Dataset%20Curation%20and%20Safety%0A%20%20Assessment&body=Title%3A%20LLavaGuard%3A%20VLM-based%20Safeguards%20for%20Vision%20Dataset%20Curation%20and%20Safety%0A%20%20Assessment%0AAuthor%3A%20Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski%0AAbstract%3A%20%20%20We%20introduce%20LlavaGuard%2C%20a%20family%20of%20VLM-based%20safeguard%20models%2C%20offering%20a%0Aversatile%20framework%20for%20evaluating%20the%20safety%20compliance%20of%20visual%20content.%0ASpecifically%2C%20we%20designed%20LlavaGuard%20for%20dataset%20annotation%20and%20generative%0Amodel%20safeguarding.%20To%20this%20end%2C%20we%20collected%20and%20annotated%20a%20high-quality%0Avisual%20dataset%20incorporating%20a%20broad%20safety%20taxonomy%2C%20which%20we%20use%20to%20tune%20VLMs%0Aon%20context-aware%20safety%20risks.%20As%20a%20key%20innovation%2C%20LlavaGuard%27s%20new%20responses%0Acontain%20comprehensive%20information%2C%20including%20a%20safety%20rating%2C%20the%20violated%0Asafety%20categories%2C%20and%20an%20in-depth%20rationale.%20Further%2C%20our%20introduced%0Acustomizable%20taxonomy%20categories%20enable%20the%20context-specific%20alignment%20of%0ALlavaGuard%20to%20various%20scenarios.%20Our%20experiments%20highlight%20the%20capabilities%20of%0ALlavaGuard%20in%20complex%20and%20real-world%20applications.%20We%20provide%20checkpoints%0Aranging%20from%207B%20to%2034B%20parameters%20demonstrating%20state-of-the-art%20performance%2C%0Awith%20even%20the%20smallest%20models%20outperforming%20baselines%20like%20GPT-4.%20We%20make%20our%0Adataset%20and%20model%20weights%20publicly%20available%20and%20invite%20further%20research%20to%0Aaddress%20the%20diverse%20needs%20of%20communities%20and%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLavaGuard%253A%2520VLM-based%2520Safeguards%2520for%2520Vision%2520Dataset%2520Curation%2520and%2520Safety%250A%2520%2520Assessment%26entry.906535625%3DLukas%2520Helff%2520and%2520Felix%2520Friedrich%2520and%2520Manuel%2520Brack%2520and%2520Kristian%2520Kersting%2520and%2520Patrick%2520Schramowski%26entry.1292438233%3D%2520%2520We%2520introduce%2520LlavaGuard%252C%2520a%2520family%2520of%2520VLM-based%2520safeguard%2520models%252C%2520offering%2520a%250Aversatile%2520framework%2520for%2520evaluating%2520the%2520safety%2520compliance%2520of%2520visual%2520content.%250ASpecifically%252C%2520we%2520designed%2520LlavaGuard%2520for%2520dataset%2520annotation%2520and%2520generative%250Amodel%2520safeguarding.%2520To%2520this%2520end%252C%2520we%2520collected%2520and%2520annotated%2520a%2520high-quality%250Avisual%2520dataset%2520incorporating%2520a%2520broad%2520safety%2520taxonomy%252C%2520which%2520we%2520use%2520to%2520tune%2520VLMs%250Aon%2520context-aware%2520safety%2520risks.%2520As%2520a%2520key%2520innovation%252C%2520LlavaGuard%2527s%2520new%2520responses%250Acontain%2520comprehensive%2520information%252C%2520including%2520a%2520safety%2520rating%252C%2520the%2520violated%250Asafety%2520categories%252C%2520and%2520an%2520in-depth%2520rationale.%2520Further%252C%2520our%2520introduced%250Acustomizable%2520taxonomy%2520categories%2520enable%2520the%2520context-specific%2520alignment%2520of%250ALlavaGuard%2520to%2520various%2520scenarios.%2520Our%2520experiments%2520highlight%2520the%2520capabilities%2520of%250ALlavaGuard%2520in%2520complex%2520and%2520real-world%2520applications.%2520We%2520provide%2520checkpoints%250Aranging%2520from%25207B%2520to%252034B%2520parameters%2520demonstrating%2520state-of-the-art%2520performance%252C%250Awith%2520even%2520the%2520smallest%2520models%2520outperforming%2520baselines%2520like%2520GPT-4.%2520We%2520make%2520our%250Adataset%2520and%2520model%2520weights%2520publicly%2520available%2520and%2520invite%2520further%2520research%2520to%250Aaddress%2520the%2520diverse%2520needs%2520of%2520communities%2520and%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLavaGuard%3A%20VLM-based%20Safeguards%20for%20Vision%20Dataset%20Curation%20and%20Safety%0A%20%20Assessment&entry.906535625=Lukas%20Helff%20and%20Felix%20Friedrich%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski&entry.1292438233=%20%20We%20introduce%20LlavaGuard%2C%20a%20family%20of%20VLM-based%20safeguard%20models%2C%20offering%20a%0Aversatile%20framework%20for%20evaluating%20the%20safety%20compliance%20of%20visual%20content.%0ASpecifically%2C%20we%20designed%20LlavaGuard%20for%20dataset%20annotation%20and%20generative%0Amodel%20safeguarding.%20To%20this%20end%2C%20we%20collected%20and%20annotated%20a%20high-quality%0Avisual%20dataset%20incorporating%20a%20broad%20safety%20taxonomy%2C%20which%20we%20use%20to%20tune%20VLMs%0Aon%20context-aware%20safety%20risks.%20As%20a%20key%20innovation%2C%20LlavaGuard%27s%20new%20responses%0Acontain%20comprehensive%20information%2C%20including%20a%20safety%20rating%2C%20the%20violated%0Asafety%20categories%2C%20and%20an%20in-depth%20rationale.%20Further%2C%20our%20introduced%0Acustomizable%20taxonomy%20categories%20enable%20the%20context-specific%20alignment%20of%0ALlavaGuard%20to%20various%20scenarios.%20Our%20experiments%20highlight%20the%20capabilities%20of%0ALlavaGuard%20in%20complex%20and%20real-world%20applications.%20We%20provide%20checkpoints%0Aranging%20from%207B%20to%2034B%20parameters%20demonstrating%20state-of-the-art%20performance%2C%0Awith%20even%20the%20smallest%20models%20outperforming%20baselines%20like%20GPT-4.%20We%20make%20our%0Adataset%20and%20model%20weights%20publicly%20available%20and%20invite%20further%20research%20to%0Aaddress%20the%20diverse%20needs%20of%20communities%20and%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05113v1&entry.124074799=Read"},
{"title": "PolyLUT-Add: FPGA-based LUT Inference with Wide Inputs", "author": "Binglei Lou and Richard Rademacher and David Boland and Philip H. W. Leong", "abstract": "  FPGAs have distinct advantages as a technology for deploying deep neural\nnetworks (DNNs) at the edge. Lookup Table (LUT) based networks, where neurons\nare directly modelled using LUTs, help maximize this promise of offering\nultra-low latency and high area efficiency on FPGAs. Unfortunately, LUT\nresource usage scales exponentially with the number of inputs to the LUT,\nrestricting PolyLUT to small LUT sizes. This work introduces PolyLUT-Add, a\ntechnique that enhances neuron connectivity by combining $A$ PolyLUT\nsub-neurons via addition to improve accuracy. Moreover, we describe a novel\narchitecture to improve its scalability. We evaluated our implementation over\nthe MNIST, Jet Substructure classification and Network Intrusion Detection\nbenchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT\nreduction of $1.3-7.7\\times$ with a $1.2-2.2\\times$ decrease in latency.\n", "link": "http://arxiv.org/abs/2406.04910v1", "date": "2024-06-07", "relevancy": 1.928, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5201}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyLUT-Add%3A%20FPGA-based%20LUT%20Inference%20with%20Wide%20Inputs&body=Title%3A%20PolyLUT-Add%3A%20FPGA-based%20LUT%20Inference%20with%20Wide%20Inputs%0AAuthor%3A%20Binglei%20Lou%20and%20Richard%20Rademacher%20and%20David%20Boland%20and%20Philip%20H.%20W.%20Leong%0AAbstract%3A%20%20%20FPGAs%20have%20distinct%20advantages%20as%20a%20technology%20for%20deploying%20deep%20neural%0Anetworks%20%28DNNs%29%20at%20the%20edge.%20Lookup%20Table%20%28LUT%29%20based%20networks%2C%20where%20neurons%0Aare%20directly%20modelled%20using%20LUTs%2C%20help%20maximize%20this%20promise%20of%20offering%0Aultra-low%20latency%20and%20high%20area%20efficiency%20on%20FPGAs.%20Unfortunately%2C%20LUT%0Aresource%20usage%20scales%20exponentially%20with%20the%20number%20of%20inputs%20to%20the%20LUT%2C%0Arestricting%20PolyLUT%20to%20small%20LUT%20sizes.%20This%20work%20introduces%20PolyLUT-Add%2C%20a%0Atechnique%20that%20enhances%20neuron%20connectivity%20by%20combining%20%24A%24%20PolyLUT%0Asub-neurons%20via%20addition%20to%20improve%20accuracy.%20Moreover%2C%20we%20describe%20a%20novel%0Aarchitecture%20to%20improve%20its%20scalability.%20We%20evaluated%20our%20implementation%20over%0Athe%20MNIST%2C%20Jet%20Substructure%20classification%20and%20Network%20Intrusion%20Detection%0Abenchmark%20and%20found%20that%20for%20similar%20accuracy%2C%20PolyLUT-Add%20achieves%20a%20LUT%0Areduction%20of%20%241.3-7.7%5Ctimes%24%20with%20a%20%241.2-2.2%5Ctimes%24%20decrease%20in%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyLUT-Add%253A%2520FPGA-based%2520LUT%2520Inference%2520with%2520Wide%2520Inputs%26entry.906535625%3DBinglei%2520Lou%2520and%2520Richard%2520Rademacher%2520and%2520David%2520Boland%2520and%2520Philip%2520H.%2520W.%2520Leong%26entry.1292438233%3D%2520%2520FPGAs%2520have%2520distinct%2520advantages%2520as%2520a%2520technology%2520for%2520deploying%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%2520at%2520the%2520edge.%2520Lookup%2520Table%2520%2528LUT%2529%2520based%2520networks%252C%2520where%2520neurons%250Aare%2520directly%2520modelled%2520using%2520LUTs%252C%2520help%2520maximize%2520this%2520promise%2520of%2520offering%250Aultra-low%2520latency%2520and%2520high%2520area%2520efficiency%2520on%2520FPGAs.%2520Unfortunately%252C%2520LUT%250Aresource%2520usage%2520scales%2520exponentially%2520with%2520the%2520number%2520of%2520inputs%2520to%2520the%2520LUT%252C%250Arestricting%2520PolyLUT%2520to%2520small%2520LUT%2520sizes.%2520This%2520work%2520introduces%2520PolyLUT-Add%252C%2520a%250Atechnique%2520that%2520enhances%2520neuron%2520connectivity%2520by%2520combining%2520%2524A%2524%2520PolyLUT%250Asub-neurons%2520via%2520addition%2520to%2520improve%2520accuracy.%2520Moreover%252C%2520we%2520describe%2520a%2520novel%250Aarchitecture%2520to%2520improve%2520its%2520scalability.%2520We%2520evaluated%2520our%2520implementation%2520over%250Athe%2520MNIST%252C%2520Jet%2520Substructure%2520classification%2520and%2520Network%2520Intrusion%2520Detection%250Abenchmark%2520and%2520found%2520that%2520for%2520similar%2520accuracy%252C%2520PolyLUT-Add%2520achieves%2520a%2520LUT%250Areduction%2520of%2520%25241.3-7.7%255Ctimes%2524%2520with%2520a%2520%25241.2-2.2%255Ctimes%2524%2520decrease%2520in%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyLUT-Add%3A%20FPGA-based%20LUT%20Inference%20with%20Wide%20Inputs&entry.906535625=Binglei%20Lou%20and%20Richard%20Rademacher%20and%20David%20Boland%20and%20Philip%20H.%20W.%20Leong&entry.1292438233=%20%20FPGAs%20have%20distinct%20advantages%20as%20a%20technology%20for%20deploying%20deep%20neural%0Anetworks%20%28DNNs%29%20at%20the%20edge.%20Lookup%20Table%20%28LUT%29%20based%20networks%2C%20where%20neurons%0Aare%20directly%20modelled%20using%20LUTs%2C%20help%20maximize%20this%20promise%20of%20offering%0Aultra-low%20latency%20and%20high%20area%20efficiency%20on%20FPGAs.%20Unfortunately%2C%20LUT%0Aresource%20usage%20scales%20exponentially%20with%20the%20number%20of%20inputs%20to%20the%20LUT%2C%0Arestricting%20PolyLUT%20to%20small%20LUT%20sizes.%20This%20work%20introduces%20PolyLUT-Add%2C%20a%0Atechnique%20that%20enhances%20neuron%20connectivity%20by%20combining%20%24A%24%20PolyLUT%0Asub-neurons%20via%20addition%20to%20improve%20accuracy.%20Moreover%2C%20we%20describe%20a%20novel%0Aarchitecture%20to%20improve%20its%20scalability.%20We%20evaluated%20our%20implementation%20over%0Athe%20MNIST%2C%20Jet%20Substructure%20classification%20and%20Network%20Intrusion%20Detection%0Abenchmark%20and%20found%20that%20for%20similar%20accuracy%2C%20PolyLUT-Add%20achieves%20a%20LUT%0Areduction%20of%20%241.3-7.7%5Ctimes%24%20with%20a%20%241.2-2.2%5Ctimes%24%20decrease%20in%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04910v1&entry.124074799=Read"},
{"title": "Are We Done with MMLU?", "author": "Aryo Pradipta Gema and Joshua Ong Jun Leang and Giwon Hong and Alessio Devoto and Alberto Carlo Maria Mancino and Rohit Saxena and Xuanli He and Yu Zhao and Xiaotang Du and Mohammad Reza Ghasemi Madani and Claire Barale and Robert McHardy and Joshua Harris and Jean Kaddour and Emile van Krieken and Pasquale Minervini", "abstract": "  Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\ntaxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually\nre-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we\ndemonstrate significant discrepancies with the model performance metrics that\nwere originally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. Therefore, we open up MMLU-Redux for additional annotation\nhttps://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.\n", "link": "http://arxiv.org/abs/2406.04127v2", "date": "2024-06-07", "relevancy": 1.9268, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20We%20Done%20with%20MMLU%3F&body=Title%3A%20Are%20We%20Done%20with%20MMLU%3F%0AAuthor%3A%20Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini%0AAbstract%3A%20%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Ataxonomy.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%203%2C000%20manually%0Are-annotated%20questions%20across%2030%20MMLU%20subjects.%20Using%20MMLU-Redux%2C%20we%0Ademonstrate%20significant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%0Awere%20originally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20Therefore%2C%20we%20open%20up%20MMLU-Redux%20for%20additional%20annotation%0Ahttps%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520We%2520Done%2520with%2520MMLU%253F%26entry.906535625%3DAryo%2520Pradipta%2520Gema%2520and%2520Joshua%2520Ong%2520Jun%2520Leang%2520and%2520Giwon%2520Hong%2520and%2520Alessio%2520Devoto%2520and%2520Alberto%2520Carlo%2520Maria%2520Mancino%2520and%2520Rohit%2520Saxena%2520and%2520Xuanli%2520He%2520and%2520Yu%2520Zhao%2520and%2520Xiaotang%2520Du%2520and%2520Mohammad%2520Reza%2520Ghasemi%2520Madani%2520and%2520Claire%2520Barale%2520and%2520Robert%2520McHardy%2520and%2520Joshua%2520Harris%2520and%2520Jean%2520Kaddour%2520and%2520Emile%2520van%2520Krieken%2520and%2520Pasquale%2520Minervini%26entry.1292438233%3D%2520%2520Maybe%2520not.%2520We%2520identify%2520and%2520analyse%2520errors%2520in%2520the%2520popular%2520Massive%2520Multitask%250ALanguage%2520Understanding%2520%2528MMLU%2529%2520benchmark.%2520Even%2520though%2520MMLU%2520is%2520widely%2520adopted%252C%250Aour%2520analysis%2520demonstrates%2520numerous%2520ground%2520truth%2520errors%2520that%2520obscure%2520the%2520true%250Acapabilities%2520of%2520LLMs.%2520For%2520example%252C%2520we%2520find%2520that%252057%2525%2520of%2520the%2520analysed%2520questions%250Ain%2520the%2520Virology%2520subset%2520contain%2520errors.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%250Acomprehensive%2520framework%2520for%2520identifying%2520dataset%2520errors%2520using%2520a%2520novel%2520error%250Ataxonomy.%2520Then%252C%2520we%2520create%2520MMLU-Redux%252C%2520which%2520is%2520a%2520subset%2520of%25203%252C000%2520manually%250Are-annotated%2520questions%2520across%252030%2520MMLU%2520subjects.%2520Using%2520MMLU-Redux%252C%2520we%250Ademonstrate%2520significant%2520discrepancies%2520with%2520the%2520model%2520performance%2520metrics%2520that%250Awere%2520originally%2520reported.%2520Our%2520results%2520strongly%2520advocate%2520for%2520revising%2520MMLU%2527s%250Aerror-ridden%2520questions%2520to%2520enhance%2520its%2520future%2520utility%2520and%2520reliability%2520as%2520a%250Abenchmark.%2520Therefore%252C%2520we%2520open%2520up%2520MMLU-Redux%2520for%2520additional%2520annotation%250Ahttps%253A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20Done%20with%20MMLU%3F&entry.906535625=Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini&entry.1292438233=%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Ataxonomy.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%203%2C000%20manually%0Are-annotated%20questions%20across%2030%20MMLU%20subjects.%20Using%20MMLU-Redux%2C%20we%0Ademonstrate%20significant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%0Awere%20originally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20Therefore%2C%20we%20open%20up%20MMLU-Redux%20for%20additional%20annotation%0Ahttps%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04127v2&entry.124074799=Read"},
{"title": "Byzantine Robustness and Partial Participation Can Be Achieved at Once:\n  Just Clip Gradient Differences", "author": "Grigory Malinovsky and Peter Richt\u00e1rik and Samuel Horv\u00e1th and Eduard Gorbunov", "abstract": "  Distributed learning has emerged as a leading paradigm for training large\nmachine learning models. However, in real-world scenarios, participants may be\nunreliable or malicious, posing a significant challenge to the integrity and\naccuracy of the trained models. Byzantine fault tolerance mechanisms have been\nproposed to address these issues, but they often assume full participation from\nall clients, which is not always practical due to the unavailability of some\nclients or communication constraints. In our work, we propose the first\ndistributed method with client sampling and provable tolerance to Byzantine\nworkers. The key idea behind the developed method is the use of gradient\nclipping to control stochastic gradient differences in recursive variance\nreduction. This allows us to bound the potential harm caused by Byzantine\nworkers, even during iterations when all sampled clients are Byzantine.\nFurthermore, we incorporate communication compression into the method to\nenhance communication efficiency. Under general assumptions, we prove\nconvergence rates for the proposed method that match the existing\nstate-of-the-art (SOTA) theoretical results. We also propose a heuristic on\nadjusting any Byzantine-robust method to a partial participation scenario via\nclipping.\n", "link": "http://arxiv.org/abs/2311.14127v2", "date": "2024-06-07", "relevancy": 1.9252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4933}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4783}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Byzantine%20Robustness%20and%20Partial%20Participation%20Can%20Be%20Achieved%20at%20Once%3A%0A%20%20Just%20Clip%20Gradient%20Differences&body=Title%3A%20Byzantine%20Robustness%20and%20Partial%20Participation%20Can%20Be%20Achieved%20at%20Once%3A%0A%20%20Just%20Clip%20Gradient%20Differences%0AAuthor%3A%20Grigory%20Malinovsky%20and%20Peter%20Richt%C3%A1rik%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov%0AAbstract%3A%20%20%20Distributed%20learning%20has%20emerged%20as%20a%20leading%20paradigm%20for%20training%20large%0Amachine%20learning%20models.%20However%2C%20in%20real-world%20scenarios%2C%20participants%20may%20be%0Aunreliable%20or%20malicious%2C%20posing%20a%20significant%20challenge%20to%20the%20integrity%20and%0Aaccuracy%20of%20the%20trained%20models.%20Byzantine%20fault%20tolerance%20mechanisms%20have%20been%0Aproposed%20to%20address%20these%20issues%2C%20but%20they%20often%20assume%20full%20participation%20from%0Aall%20clients%2C%20which%20is%20not%20always%20practical%20due%20to%20the%20unavailability%20of%20some%0Aclients%20or%20communication%20constraints.%20In%20our%20work%2C%20we%20propose%20the%20first%0Adistributed%20method%20with%20client%20sampling%20and%20provable%20tolerance%20to%20Byzantine%0Aworkers.%20The%20key%20idea%20behind%20the%20developed%20method%20is%20the%20use%20of%20gradient%0Aclipping%20to%20control%20stochastic%20gradient%20differences%20in%20recursive%20variance%0Areduction.%20This%20allows%20us%20to%20bound%20the%20potential%20harm%20caused%20by%20Byzantine%0Aworkers%2C%20even%20during%20iterations%20when%20all%20sampled%20clients%20are%20Byzantine.%0AFurthermore%2C%20we%20incorporate%20communication%20compression%20into%20the%20method%20to%0Aenhance%20communication%20efficiency.%20Under%20general%20assumptions%2C%20we%20prove%0Aconvergence%20rates%20for%20the%20proposed%20method%20that%20match%20the%20existing%0Astate-of-the-art%20%28SOTA%29%20theoretical%20results.%20We%20also%20propose%20a%20heuristic%20on%0Aadjusting%20any%20Byzantine-robust%20method%20to%20a%20partial%20participation%20scenario%20via%0Aclipping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByzantine%2520Robustness%2520and%2520Partial%2520Participation%2520Can%2520Be%2520Achieved%2520at%2520Once%253A%250A%2520%2520Just%2520Clip%2520Gradient%2520Differences%26entry.906535625%3DGrigory%2520Malinovsky%2520and%2520Peter%2520Richt%25C3%25A1rik%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Eduard%2520Gorbunov%26entry.1292438233%3D%2520%2520Distributed%2520learning%2520has%2520emerged%2520as%2520a%2520leading%2520paradigm%2520for%2520training%2520large%250Amachine%2520learning%2520models.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520participants%2520may%2520be%250Aunreliable%2520or%2520malicious%252C%2520posing%2520a%2520significant%2520challenge%2520to%2520the%2520integrity%2520and%250Aaccuracy%2520of%2520the%2520trained%2520models.%2520Byzantine%2520fault%2520tolerance%2520mechanisms%2520have%2520been%250Aproposed%2520to%2520address%2520these%2520issues%252C%2520but%2520they%2520often%2520assume%2520full%2520participation%2520from%250Aall%2520clients%252C%2520which%2520is%2520not%2520always%2520practical%2520due%2520to%2520the%2520unavailability%2520of%2520some%250Aclients%2520or%2520communication%2520constraints.%2520In%2520our%2520work%252C%2520we%2520propose%2520the%2520first%250Adistributed%2520method%2520with%2520client%2520sampling%2520and%2520provable%2520tolerance%2520to%2520Byzantine%250Aworkers.%2520The%2520key%2520idea%2520behind%2520the%2520developed%2520method%2520is%2520the%2520use%2520of%2520gradient%250Aclipping%2520to%2520control%2520stochastic%2520gradient%2520differences%2520in%2520recursive%2520variance%250Areduction.%2520This%2520allows%2520us%2520to%2520bound%2520the%2520potential%2520harm%2520caused%2520by%2520Byzantine%250Aworkers%252C%2520even%2520during%2520iterations%2520when%2520all%2520sampled%2520clients%2520are%2520Byzantine.%250AFurthermore%252C%2520we%2520incorporate%2520communication%2520compression%2520into%2520the%2520method%2520to%250Aenhance%2520communication%2520efficiency.%2520Under%2520general%2520assumptions%252C%2520we%2520prove%250Aconvergence%2520rates%2520for%2520the%2520proposed%2520method%2520that%2520match%2520the%2520existing%250Astate-of-the-art%2520%2528SOTA%2529%2520theoretical%2520results.%2520We%2520also%2520propose%2520a%2520heuristic%2520on%250Aadjusting%2520any%2520Byzantine-robust%2520method%2520to%2520a%2520partial%2520participation%2520scenario%2520via%250Aclipping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Byzantine%20Robustness%20and%20Partial%20Participation%20Can%20Be%20Achieved%20at%20Once%3A%0A%20%20Just%20Clip%20Gradient%20Differences&entry.906535625=Grigory%20Malinovsky%20and%20Peter%20Richt%C3%A1rik%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov&entry.1292438233=%20%20Distributed%20learning%20has%20emerged%20as%20a%20leading%20paradigm%20for%20training%20large%0Amachine%20learning%20models.%20However%2C%20in%20real-world%20scenarios%2C%20participants%20may%20be%0Aunreliable%20or%20malicious%2C%20posing%20a%20significant%20challenge%20to%20the%20integrity%20and%0Aaccuracy%20of%20the%20trained%20models.%20Byzantine%20fault%20tolerance%20mechanisms%20have%20been%0Aproposed%20to%20address%20these%20issues%2C%20but%20they%20often%20assume%20full%20participation%20from%0Aall%20clients%2C%20which%20is%20not%20always%20practical%20due%20to%20the%20unavailability%20of%20some%0Aclients%20or%20communication%20constraints.%20In%20our%20work%2C%20we%20propose%20the%20first%0Adistributed%20method%20with%20client%20sampling%20and%20provable%20tolerance%20to%20Byzantine%0Aworkers.%20The%20key%20idea%20behind%20the%20developed%20method%20is%20the%20use%20of%20gradient%0Aclipping%20to%20control%20stochastic%20gradient%20differences%20in%20recursive%20variance%0Areduction.%20This%20allows%20us%20to%20bound%20the%20potential%20harm%20caused%20by%20Byzantine%0Aworkers%2C%20even%20during%20iterations%20when%20all%20sampled%20clients%20are%20Byzantine.%0AFurthermore%2C%20we%20incorporate%20communication%20compression%20into%20the%20method%20to%0Aenhance%20communication%20efficiency.%20Under%20general%20assumptions%2C%20we%20prove%0Aconvergence%20rates%20for%20the%20proposed%20method%20that%20match%20the%20existing%0Astate-of-the-art%20%28SOTA%29%20theoretical%20results.%20We%20also%20propose%20a%20heuristic%20on%0Aadjusting%20any%20Byzantine-robust%20method%20to%20a%20partial%20participation%20scenario%20via%0Aclipping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14127v2&entry.124074799=Read"},
{"title": "Self-Improving Robust Preference Optimization", "author": "Eugene Choi and Arash Ahmadian and Matthieu Geist and Oilvier Pietquin and Mohammad Gheshlaghi Azar", "abstract": "  Both online and offline RLHF methods such as PPO and DPO have been extremely\nsuccessful in aligning AI with human preferences. Despite their success, the\nexisting methods suffer from a fundamental problem that their optimal solution\nis highly task-dependent (i.e., not robust to out-of-distribution (OOD) tasks).\nHere we address this challenge by proposing Self-Improving Robust Preference\nOptimization SRPO, a practical and mathematically principled offline RLHF\nframework that is completely robust to the changes in the task. The key idea of\nSRPO is to cast the problem of learning from human preferences as a\nself-improvement process, which can be mathematically expressed in terms of a\nmin-max objective that aims at joint optimization of self-improvement policy\nand the generative policy in an adversarial fashion. The solution for this\noptimization problem is independent of the training task and thus it is robust\nto its changes. We then show that this objective can be re-expressed in the\nform of a non-adversarial offline loss which can be optimized using standard\nsupervised optimization techniques at scale without any need for reward model\nand online inference. We show the effectiveness of SRPO in terms of AI Win-Rate\n(WR) against human (GOLD) completions. In particular, when SRPO is evaluated on\nthe OOD XSUM dataset, it outperforms the celebrated DPO by a clear margin of\n15% after 5 self-revisions, achieving WR of 90%.\n", "link": "http://arxiv.org/abs/2406.01660v3", "date": "2024-06-07", "relevancy": 1.4556, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5115}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Improving%20Robust%20Preference%20Optimization&body=Title%3A%20Self-Improving%20Robust%20Preference%20Optimization%0AAuthor%3A%20Eugene%20Choi%20and%20Arash%20Ahmadian%20and%20Matthieu%20Geist%20and%20Oilvier%20Pietquin%20and%20Mohammad%20Gheshlaghi%20Azar%0AAbstract%3A%20%20%20Both%20online%20and%20offline%20RLHF%20methods%20such%20as%20PPO%20and%20DPO%20have%20been%20extremely%0Asuccessful%20in%20aligning%20AI%20with%20human%20preferences.%20Despite%20their%20success%2C%20the%0Aexisting%20methods%20suffer%20from%20a%20fundamental%20problem%20that%20their%20optimal%20solution%0Ais%20highly%20task-dependent%20%28i.e.%2C%20not%20robust%20to%20out-of-distribution%20%28OOD%29%20tasks%29.%0AHere%20we%20address%20this%20challenge%20by%20proposing%20Self-Improving%20Robust%20Preference%0AOptimization%20SRPO%2C%20a%20practical%20and%20mathematically%20principled%20offline%20RLHF%0Aframework%20that%20is%20completely%20robust%20to%20the%20changes%20in%20the%20task.%20The%20key%20idea%20of%0ASRPO%20is%20to%20cast%20the%20problem%20of%20learning%20from%20human%20preferences%20as%20a%0Aself-improvement%20process%2C%20which%20can%20be%20mathematically%20expressed%20in%20terms%20of%20a%0Amin-max%20objective%20that%20aims%20at%20joint%20optimization%20of%20self-improvement%20policy%0Aand%20the%20generative%20policy%20in%20an%20adversarial%20fashion.%20The%20solution%20for%20this%0Aoptimization%20problem%20is%20independent%20of%20the%20training%20task%20and%20thus%20it%20is%20robust%0Ato%20its%20changes.%20We%20then%20show%20that%20this%20objective%20can%20be%20re-expressed%20in%20the%0Aform%20of%20a%20non-adversarial%20offline%20loss%20which%20can%20be%20optimized%20using%20standard%0Asupervised%20optimization%20techniques%20at%20scale%20without%20any%20need%20for%20reward%20model%0Aand%20online%20inference.%20We%20show%20the%20effectiveness%20of%20SRPO%20in%20terms%20of%20AI%20Win-Rate%0A%28WR%29%20against%20human%20%28GOLD%29%20completions.%20In%20particular%2C%20when%20SRPO%20is%20evaluated%20on%0Athe%20OOD%20XSUM%20dataset%2C%20it%20outperforms%20the%20celebrated%20DPO%20by%20a%20clear%20margin%20of%0A15%25%20after%205%20self-revisions%2C%20achieving%20WR%20of%2090%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01660v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Improving%2520Robust%2520Preference%2520Optimization%26entry.906535625%3DEugene%2520Choi%2520and%2520Arash%2520Ahmadian%2520and%2520Matthieu%2520Geist%2520and%2520Oilvier%2520Pietquin%2520and%2520Mohammad%2520Gheshlaghi%2520Azar%26entry.1292438233%3D%2520%2520Both%2520online%2520and%2520offline%2520RLHF%2520methods%2520such%2520as%2520PPO%2520and%2520DPO%2520have%2520been%2520extremely%250Asuccessful%2520in%2520aligning%2520AI%2520with%2520human%2520preferences.%2520Despite%2520their%2520success%252C%2520the%250Aexisting%2520methods%2520suffer%2520from%2520a%2520fundamental%2520problem%2520that%2520their%2520optimal%2520solution%250Ais%2520highly%2520task-dependent%2520%2528i.e.%252C%2520not%2520robust%2520to%2520out-of-distribution%2520%2528OOD%2529%2520tasks%2529.%250AHere%2520we%2520address%2520this%2520challenge%2520by%2520proposing%2520Self-Improving%2520Robust%2520Preference%250AOptimization%2520SRPO%252C%2520a%2520practical%2520and%2520mathematically%2520principled%2520offline%2520RLHF%250Aframework%2520that%2520is%2520completely%2520robust%2520to%2520the%2520changes%2520in%2520the%2520task.%2520The%2520key%2520idea%2520of%250ASRPO%2520is%2520to%2520cast%2520the%2520problem%2520of%2520learning%2520from%2520human%2520preferences%2520as%2520a%250Aself-improvement%2520process%252C%2520which%2520can%2520be%2520mathematically%2520expressed%2520in%2520terms%2520of%2520a%250Amin-max%2520objective%2520that%2520aims%2520at%2520joint%2520optimization%2520of%2520self-improvement%2520policy%250Aand%2520the%2520generative%2520policy%2520in%2520an%2520adversarial%2520fashion.%2520The%2520solution%2520for%2520this%250Aoptimization%2520problem%2520is%2520independent%2520of%2520the%2520training%2520task%2520and%2520thus%2520it%2520is%2520robust%250Ato%2520its%2520changes.%2520We%2520then%2520show%2520that%2520this%2520objective%2520can%2520be%2520re-expressed%2520in%2520the%250Aform%2520of%2520a%2520non-adversarial%2520offline%2520loss%2520which%2520can%2520be%2520optimized%2520using%2520standard%250Asupervised%2520optimization%2520techniques%2520at%2520scale%2520without%2520any%2520need%2520for%2520reward%2520model%250Aand%2520online%2520inference.%2520We%2520show%2520the%2520effectiveness%2520of%2520SRPO%2520in%2520terms%2520of%2520AI%2520Win-Rate%250A%2528WR%2529%2520against%2520human%2520%2528GOLD%2529%2520completions.%2520In%2520particular%252C%2520when%2520SRPO%2520is%2520evaluated%2520on%250Athe%2520OOD%2520XSUM%2520dataset%252C%2520it%2520outperforms%2520the%2520celebrated%2520DPO%2520by%2520a%2520clear%2520margin%2520of%250A15%2525%2520after%25205%2520self-revisions%252C%2520achieving%2520WR%2520of%252090%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01660v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Improving%20Robust%20Preference%20Optimization&entry.906535625=Eugene%20Choi%20and%20Arash%20Ahmadian%20and%20Matthieu%20Geist%20and%20Oilvier%20Pietquin%20and%20Mohammad%20Gheshlaghi%20Azar&entry.1292438233=%20%20Both%20online%20and%20offline%20RLHF%20methods%20such%20as%20PPO%20and%20DPO%20have%20been%20extremely%0Asuccessful%20in%20aligning%20AI%20with%20human%20preferences.%20Despite%20their%20success%2C%20the%0Aexisting%20methods%20suffer%20from%20a%20fundamental%20problem%20that%20their%20optimal%20solution%0Ais%20highly%20task-dependent%20%28i.e.%2C%20not%20robust%20to%20out-of-distribution%20%28OOD%29%20tasks%29.%0AHere%20we%20address%20this%20challenge%20by%20proposing%20Self-Improving%20Robust%20Preference%0AOptimization%20SRPO%2C%20a%20practical%20and%20mathematically%20principled%20offline%20RLHF%0Aframework%20that%20is%20completely%20robust%20to%20the%20changes%20in%20the%20task.%20The%20key%20idea%20of%0ASRPO%20is%20to%20cast%20the%20problem%20of%20learning%20from%20human%20preferences%20as%20a%0Aself-improvement%20process%2C%20which%20can%20be%20mathematically%20expressed%20in%20terms%20of%20a%0Amin-max%20objective%20that%20aims%20at%20joint%20optimization%20of%20self-improvement%20policy%0Aand%20the%20generative%20policy%20in%20an%20adversarial%20fashion.%20The%20solution%20for%20this%0Aoptimization%20problem%20is%20independent%20of%20the%20training%20task%20and%20thus%20it%20is%20robust%0Ato%20its%20changes.%20We%20then%20show%20that%20this%20objective%20can%20be%20re-expressed%20in%20the%0Aform%20of%20a%20non-adversarial%20offline%20loss%20which%20can%20be%20optimized%20using%20standard%0Asupervised%20optimization%20techniques%20at%20scale%20without%20any%20need%20for%20reward%20model%0Aand%20online%20inference.%20We%20show%20the%20effectiveness%20of%20SRPO%20in%20terms%20of%20AI%20Win-Rate%0A%28WR%29%20against%20human%20%28GOLD%29%20completions.%20In%20particular%2C%20when%20SRPO%20is%20evaluated%20on%0Athe%20OOD%20XSUM%20dataset%2C%20it%20outperforms%20the%20celebrated%20DPO%20by%20a%20clear%20margin%20of%0A15%25%20after%205%20self-revisions%2C%20achieving%20WR%20of%2090%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01660v3&entry.124074799=Read"},
{"title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning", "author": "Nikhil Vyas and Depen Morwani and Rosie Zhao and Gal Kaplun and Sham Kakade and Boaz Barak", "abstract": "  The success of SGD in deep learning has been ascribed by prior works to the\nimplicit bias induced by finite batch sizes (\"SGD noise\"). While prior works\nfocused on offline learning (i.e., multiple-epoch training), we study the\nimpact of SGD noise on online (i.e., single epoch) learning. Through an\nextensive empirical analysis of image and language data, we demonstrate that\nsmall batch sizes do not confer any implicit bias advantages in online\nlearning. In contrast to offline learning, the benefits of SGD noise in online\nlearning are strictly computational, facilitating more cost-effective gradient\nsteps. This suggests that SGD in the online regime can be construed as taking\nnoisy steps along the \"golden path\" of the noiseless gradient descent\nalgorithm. We study this hypothesis and provide supporting evidence in loss and\nfunction space. Our findings challenge the prevailing understanding of SGD and\noffer novel insights into its role in online learning.\n", "link": "http://arxiv.org/abs/2306.08590v2", "date": "2024-06-07", "relevancy": 1.4732, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4923}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4916}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Implicit%20Bias%3A%20The%20Insignificance%20of%20SGD%20Noise%20in%20Online%20Learning&body=Title%3A%20Beyond%20Implicit%20Bias%3A%20The%20Insignificance%20of%20SGD%20Noise%20in%20Online%20Learning%0AAuthor%3A%20Nikhil%20Vyas%20and%20Depen%20Morwani%20and%20Rosie%20Zhao%20and%20Gal%20Kaplun%20and%20Sham%20Kakade%20and%20Boaz%20Barak%0AAbstract%3A%20%20%20The%20success%20of%20SGD%20in%20deep%20learning%20has%20been%20ascribed%20by%20prior%20works%20to%20the%0Aimplicit%20bias%20induced%20by%20finite%20batch%20sizes%20%28%22SGD%20noise%22%29.%20While%20prior%20works%0Afocused%20on%20offline%20learning%20%28i.e.%2C%20multiple-epoch%20training%29%2C%20we%20study%20the%0Aimpact%20of%20SGD%20noise%20on%20online%20%28i.e.%2C%20single%20epoch%29%20learning.%20Through%20an%0Aextensive%20empirical%20analysis%20of%20image%20and%20language%20data%2C%20we%20demonstrate%20that%0Asmall%20batch%20sizes%20do%20not%20confer%20any%20implicit%20bias%20advantages%20in%20online%0Alearning.%20In%20contrast%20to%20offline%20learning%2C%20the%20benefits%20of%20SGD%20noise%20in%20online%0Alearning%20are%20strictly%20computational%2C%20facilitating%20more%20cost-effective%20gradient%0Asteps.%20This%20suggests%20that%20SGD%20in%20the%20online%20regime%20can%20be%20construed%20as%20taking%0Anoisy%20steps%20along%20the%20%22golden%20path%22%20of%20the%20noiseless%20gradient%20descent%0Aalgorithm.%20We%20study%20this%20hypothesis%20and%20provide%20supporting%20evidence%20in%20loss%20and%0Afunction%20space.%20Our%20findings%20challenge%20the%20prevailing%20understanding%20of%20SGD%20and%0Aoffer%20novel%20insights%20into%20its%20role%20in%20online%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Implicit%2520Bias%253A%2520The%2520Insignificance%2520of%2520SGD%2520Noise%2520in%2520Online%2520Learning%26entry.906535625%3DNikhil%2520Vyas%2520and%2520Depen%2520Morwani%2520and%2520Rosie%2520Zhao%2520and%2520Gal%2520Kaplun%2520and%2520Sham%2520Kakade%2520and%2520Boaz%2520Barak%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520SGD%2520in%2520deep%2520learning%2520has%2520been%2520ascribed%2520by%2520prior%2520works%2520to%2520the%250Aimplicit%2520bias%2520induced%2520by%2520finite%2520batch%2520sizes%2520%2528%2522SGD%2520noise%2522%2529.%2520While%2520prior%2520works%250Afocused%2520on%2520offline%2520learning%2520%2528i.e.%252C%2520multiple-epoch%2520training%2529%252C%2520we%2520study%2520the%250Aimpact%2520of%2520SGD%2520noise%2520on%2520online%2520%2528i.e.%252C%2520single%2520epoch%2529%2520learning.%2520Through%2520an%250Aextensive%2520empirical%2520analysis%2520of%2520image%2520and%2520language%2520data%252C%2520we%2520demonstrate%2520that%250Asmall%2520batch%2520sizes%2520do%2520not%2520confer%2520any%2520implicit%2520bias%2520advantages%2520in%2520online%250Alearning.%2520In%2520contrast%2520to%2520offline%2520learning%252C%2520the%2520benefits%2520of%2520SGD%2520noise%2520in%2520online%250Alearning%2520are%2520strictly%2520computational%252C%2520facilitating%2520more%2520cost-effective%2520gradient%250Asteps.%2520This%2520suggests%2520that%2520SGD%2520in%2520the%2520online%2520regime%2520can%2520be%2520construed%2520as%2520taking%250Anoisy%2520steps%2520along%2520the%2520%2522golden%2520path%2522%2520of%2520the%2520noiseless%2520gradient%2520descent%250Aalgorithm.%2520We%2520study%2520this%2520hypothesis%2520and%2520provide%2520supporting%2520evidence%2520in%2520loss%2520and%250Afunction%2520space.%2520Our%2520findings%2520challenge%2520the%2520prevailing%2520understanding%2520of%2520SGD%2520and%250Aoffer%2520novel%2520insights%2520into%2520its%2520role%2520in%2520online%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.08590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Implicit%20Bias%3A%20The%20Insignificance%20of%20SGD%20Noise%20in%20Online%20Learning&entry.906535625=Nikhil%20Vyas%20and%20Depen%20Morwani%20and%20Rosie%20Zhao%20and%20Gal%20Kaplun%20and%20Sham%20Kakade%20and%20Boaz%20Barak&entry.1292438233=%20%20The%20success%20of%20SGD%20in%20deep%20learning%20has%20been%20ascribed%20by%20prior%20works%20to%20the%0Aimplicit%20bias%20induced%20by%20finite%20batch%20sizes%20%28%22SGD%20noise%22%29.%20While%20prior%20works%0Afocused%20on%20offline%20learning%20%28i.e.%2C%20multiple-epoch%20training%29%2C%20we%20study%20the%0Aimpact%20of%20SGD%20noise%20on%20online%20%28i.e.%2C%20single%20epoch%29%20learning.%20Through%20an%0Aextensive%20empirical%20analysis%20of%20image%20and%20language%20data%2C%20we%20demonstrate%20that%0Asmall%20batch%20sizes%20do%20not%20confer%20any%20implicit%20bias%20advantages%20in%20online%0Alearning.%20In%20contrast%20to%20offline%20learning%2C%20the%20benefits%20of%20SGD%20noise%20in%20online%0Alearning%20are%20strictly%20computational%2C%20facilitating%20more%20cost-effective%20gradient%0Asteps.%20This%20suggests%20that%20SGD%20in%20the%20online%20regime%20can%20be%20construed%20as%20taking%0Anoisy%20steps%20along%20the%20%22golden%20path%22%20of%20the%20noiseless%20gradient%20descent%0Aalgorithm.%20We%20study%20this%20hypothesis%20and%20provide%20supporting%20evidence%20in%20loss%20and%0Afunction%20space.%20Our%20findings%20challenge%20the%20prevailing%20understanding%20of%20SGD%20and%0Aoffer%20novel%20insights%20into%20its%20role%20in%20online%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08590v2&entry.124074799=Read"},
{"title": "Contextual fusion enhances robustness to image blurring", "author": "Shruti Joshi and Aiswarya Akumalla and Seth Haney and Maxim Bazhenov", "abstract": "  Mammalian brains handle complex reasoning by integrating information across\nbrain regions specialized for particular sensory modalities. This enables\nimproved robustness and generalization versus deep neural networks, which\ntypically process one modality and are vulnerable to perturbations. While\ndefense methods exist, they do not generalize well across perturbations. We\ndeveloped a fusion model combining background and foreground features from CNNs\ntrained on Imagenet and Places365. We tested its robustness to\nhuman-perceivable perturbations on MS COCO. The fusion model improved\nrobustness, especially for classes with greater context variability. Our\nproposed solution for integrating multiple modalities provides a new approach\nto enhance robustness and may be complementary to existing methods.\n", "link": "http://arxiv.org/abs/2406.05120v1", "date": "2024-06-07", "relevancy": 1.5704, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5381}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20fusion%20enhances%20robustness%20to%20image%20blurring&body=Title%3A%20Contextual%20fusion%20enhances%20robustness%20to%20image%20blurring%0AAuthor%3A%20Shruti%20Joshi%20and%20Aiswarya%20Akumalla%20and%20Seth%20Haney%20and%20Maxim%20Bazhenov%0AAbstract%3A%20%20%20Mammalian%20brains%20handle%20complex%20reasoning%20by%20integrating%20information%20across%0Abrain%20regions%20specialized%20for%20particular%20sensory%20modalities.%20This%20enables%0Aimproved%20robustness%20and%20generalization%20versus%20deep%20neural%20networks%2C%20which%0Atypically%20process%20one%20modality%20and%20are%20vulnerable%20to%20perturbations.%20While%0Adefense%20methods%20exist%2C%20they%20do%20not%20generalize%20well%20across%20perturbations.%20We%0Adeveloped%20a%20fusion%20model%20combining%20background%20and%20foreground%20features%20from%20CNNs%0Atrained%20on%20Imagenet%20and%20Places365.%20We%20tested%20its%20robustness%20to%0Ahuman-perceivable%20perturbations%20on%20MS%20COCO.%20The%20fusion%20model%20improved%0Arobustness%2C%20especially%20for%20classes%20with%20greater%20context%20variability.%20Our%0Aproposed%20solution%20for%20integrating%20multiple%20modalities%20provides%20a%20new%20approach%0Ato%20enhance%20robustness%20and%20may%20be%20complementary%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520fusion%2520enhances%2520robustness%2520to%2520image%2520blurring%26entry.906535625%3DShruti%2520Joshi%2520and%2520Aiswarya%2520Akumalla%2520and%2520Seth%2520Haney%2520and%2520Maxim%2520Bazhenov%26entry.1292438233%3D%2520%2520Mammalian%2520brains%2520handle%2520complex%2520reasoning%2520by%2520integrating%2520information%2520across%250Abrain%2520regions%2520specialized%2520for%2520particular%2520sensory%2520modalities.%2520This%2520enables%250Aimproved%2520robustness%2520and%2520generalization%2520versus%2520deep%2520neural%2520networks%252C%2520which%250Atypically%2520process%2520one%2520modality%2520and%2520are%2520vulnerable%2520to%2520perturbations.%2520While%250Adefense%2520methods%2520exist%252C%2520they%2520do%2520not%2520generalize%2520well%2520across%2520perturbations.%2520We%250Adeveloped%2520a%2520fusion%2520model%2520combining%2520background%2520and%2520foreground%2520features%2520from%2520CNNs%250Atrained%2520on%2520Imagenet%2520and%2520Places365.%2520We%2520tested%2520its%2520robustness%2520to%250Ahuman-perceivable%2520perturbations%2520on%2520MS%2520COCO.%2520The%2520fusion%2520model%2520improved%250Arobustness%252C%2520especially%2520for%2520classes%2520with%2520greater%2520context%2520variability.%2520Our%250Aproposed%2520solution%2520for%2520integrating%2520multiple%2520modalities%2520provides%2520a%2520new%2520approach%250Ato%2520enhance%2520robustness%2520and%2520may%2520be%2520complementary%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20fusion%20enhances%20robustness%20to%20image%20blurring&entry.906535625=Shruti%20Joshi%20and%20Aiswarya%20Akumalla%20and%20Seth%20Haney%20and%20Maxim%20Bazhenov&entry.1292438233=%20%20Mammalian%20brains%20handle%20complex%20reasoning%20by%20integrating%20information%20across%0Abrain%20regions%20specialized%20for%20particular%20sensory%20modalities.%20This%20enables%0Aimproved%20robustness%20and%20generalization%20versus%20deep%20neural%20networks%2C%20which%0Atypically%20process%20one%20modality%20and%20are%20vulnerable%20to%20perturbations.%20While%0Adefense%20methods%20exist%2C%20they%20do%20not%20generalize%20well%20across%20perturbations.%20We%0Adeveloped%20a%20fusion%20model%20combining%20background%20and%20foreground%20features%20from%20CNNs%0Atrained%20on%20Imagenet%20and%20Places365.%20We%20tested%20its%20robustness%20to%0Ahuman-perceivable%20perturbations%20on%20MS%20COCO.%20The%20fusion%20model%20improved%0Arobustness%2C%20especially%20for%20classes%20with%20greater%20context%20variability.%20Our%0Aproposed%20solution%20for%20integrating%20multiple%20modalities%20provides%20a%20new%20approach%0Ato%20enhance%20robustness%20and%20may%20be%20complementary%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05120v1&entry.124074799=Read"},
{"title": "Neural Networks with (Low-Precision) Polynomial Approximations: New\n  Insights and Techniques for Accuracy Improvement", "author": "Chi Zhang and Jingjing Fan and Man Ho Au and Siu Ming Yiu", "abstract": "  Replacing non-polynomial functions (e.g., non-linear activation functions\nsuch as ReLU) in a neural network with their polynomial approximations is a\nstandard practice in privacy-preserving machine learning. The resulting neural\nnetwork, called polynomial approximation of neural network (PANN) in this\npaper, is compatible with advanced cryptosystems to enable privacy-preserving\nmodel inference. Using ``highly precise'' approximation, state-of-the-art PANN\noffers similar inference accuracy as the underlying backbone model. However,\nlittle is known about the effect of approximation, and existing literature\noften determined the required approximation precision empirically. In this\npaper, we initiate the investigation of PANN as a standalone object.\nSpecifically, our contribution is two-fold. Firstly, we provide an explanation\non the effect of approximate error in PANN. In particular, we discovered that\n(1) PANN is susceptible to some type of perturbations; and (2) weight\nregularisation significantly reduces PANN's accuracy. We support our\nexplanation with experiments. Secondly, based on the insights from our\ninvestigations, we propose solutions to increase inference accuracy for PANN.\nExperiments showed that combination of our solutions is very effective: at the\nsame precision, our PANN is 10% to 50% more accurate than state-of-the-arts;\nand at the same accuracy, our PANN only requires a precision of 2^{-9} while\nstate-of-the-art solution requires a precision of 2^{-12} using the ResNet-20\nmodel on CIFAR-10 dataset.\n", "link": "http://arxiv.org/abs/2402.11224v2", "date": "2024-06-07", "relevancy": 1.8519, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4311}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20with%20%28Low-Precision%29%20Polynomial%20Approximations%3A%20New%0A%20%20Insights%20and%20Techniques%20for%20Accuracy%20Improvement&body=Title%3A%20Neural%20Networks%20with%20%28Low-Precision%29%20Polynomial%20Approximations%3A%20New%0A%20%20Insights%20and%20Techniques%20for%20Accuracy%20Improvement%0AAuthor%3A%20Chi%20Zhang%20and%20Jingjing%20Fan%20and%20Man%20Ho%20Au%20and%20Siu%20Ming%20Yiu%0AAbstract%3A%20%20%20Replacing%20non-polynomial%20functions%20%28e.g.%2C%20non-linear%20activation%20functions%0Asuch%20as%20ReLU%29%20in%20a%20neural%20network%20with%20their%20polynomial%20approximations%20is%20a%0Astandard%20practice%20in%20privacy-preserving%20machine%20learning.%20The%20resulting%20neural%0Anetwork%2C%20called%20polynomial%20approximation%20of%20neural%20network%20%28PANN%29%20in%20this%0Apaper%2C%20is%20compatible%20with%20advanced%20cryptosystems%20to%20enable%20privacy-preserving%0Amodel%20inference.%20Using%20%60%60highly%20precise%27%27%20approximation%2C%20state-of-the-art%20PANN%0Aoffers%20similar%20inference%20accuracy%20as%20the%20underlying%20backbone%20model.%20However%2C%0Alittle%20is%20known%20about%20the%20effect%20of%20approximation%2C%20and%20existing%20literature%0Aoften%20determined%20the%20required%20approximation%20precision%20empirically.%20In%20this%0Apaper%2C%20we%20initiate%20the%20investigation%20of%20PANN%20as%20a%20standalone%20object.%0ASpecifically%2C%20our%20contribution%20is%20two-fold.%20Firstly%2C%20we%20provide%20an%20explanation%0Aon%20the%20effect%20of%20approximate%20error%20in%20PANN.%20In%20particular%2C%20we%20discovered%20that%0A%281%29%20PANN%20is%20susceptible%20to%20some%20type%20of%20perturbations%3B%20and%20%282%29%20weight%0Aregularisation%20significantly%20reduces%20PANN%27s%20accuracy.%20We%20support%20our%0Aexplanation%20with%20experiments.%20Secondly%2C%20based%20on%20the%20insights%20from%20our%0Ainvestigations%2C%20we%20propose%20solutions%20to%20increase%20inference%20accuracy%20for%20PANN.%0AExperiments%20showed%20that%20combination%20of%20our%20solutions%20is%20very%20effective%3A%20at%20the%0Asame%20precision%2C%20our%20PANN%20is%2010%25%20to%2050%25%20more%20accurate%20than%20state-of-the-arts%3B%0Aand%20at%20the%20same%20accuracy%2C%20our%20PANN%20only%20requires%20a%20precision%20of%202%5E%7B-9%7D%20while%0Astate-of-the-art%20solution%20requires%20a%20precision%20of%202%5E%7B-12%7D%20using%20the%20ResNet-20%0Amodel%20on%20CIFAR-10%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Networks%2520with%2520%2528Low-Precision%2529%2520Polynomial%2520Approximations%253A%2520New%250A%2520%2520Insights%2520and%2520Techniques%2520for%2520Accuracy%2520Improvement%26entry.906535625%3DChi%2520Zhang%2520and%2520Jingjing%2520Fan%2520and%2520Man%2520Ho%2520Au%2520and%2520Siu%2520Ming%2520Yiu%26entry.1292438233%3D%2520%2520Replacing%2520non-polynomial%2520functions%2520%2528e.g.%252C%2520non-linear%2520activation%2520functions%250Asuch%2520as%2520ReLU%2529%2520in%2520a%2520neural%2520network%2520with%2520their%2520polynomial%2520approximations%2520is%2520a%250Astandard%2520practice%2520in%2520privacy-preserving%2520machine%2520learning.%2520The%2520resulting%2520neural%250Anetwork%252C%2520called%2520polynomial%2520approximation%2520of%2520neural%2520network%2520%2528PANN%2529%2520in%2520this%250Apaper%252C%2520is%2520compatible%2520with%2520advanced%2520cryptosystems%2520to%2520enable%2520privacy-preserving%250Amodel%2520inference.%2520Using%2520%2560%2560highly%2520precise%2527%2527%2520approximation%252C%2520state-of-the-art%2520PANN%250Aoffers%2520similar%2520inference%2520accuracy%2520as%2520the%2520underlying%2520backbone%2520model.%2520However%252C%250Alittle%2520is%2520known%2520about%2520the%2520effect%2520of%2520approximation%252C%2520and%2520existing%2520literature%250Aoften%2520determined%2520the%2520required%2520approximation%2520precision%2520empirically.%2520In%2520this%250Apaper%252C%2520we%2520initiate%2520the%2520investigation%2520of%2520PANN%2520as%2520a%2520standalone%2520object.%250ASpecifically%252C%2520our%2520contribution%2520is%2520two-fold.%2520Firstly%252C%2520we%2520provide%2520an%2520explanation%250Aon%2520the%2520effect%2520of%2520approximate%2520error%2520in%2520PANN.%2520In%2520particular%252C%2520we%2520discovered%2520that%250A%25281%2529%2520PANN%2520is%2520susceptible%2520to%2520some%2520type%2520of%2520perturbations%253B%2520and%2520%25282%2529%2520weight%250Aregularisation%2520significantly%2520reduces%2520PANN%2527s%2520accuracy.%2520We%2520support%2520our%250Aexplanation%2520with%2520experiments.%2520Secondly%252C%2520based%2520on%2520the%2520insights%2520from%2520our%250Ainvestigations%252C%2520we%2520propose%2520solutions%2520to%2520increase%2520inference%2520accuracy%2520for%2520PANN.%250AExperiments%2520showed%2520that%2520combination%2520of%2520our%2520solutions%2520is%2520very%2520effective%253A%2520at%2520the%250Asame%2520precision%252C%2520our%2520PANN%2520is%252010%2525%2520to%252050%2525%2520more%2520accurate%2520than%2520state-of-the-arts%253B%250Aand%2520at%2520the%2520same%2520accuracy%252C%2520our%2520PANN%2520only%2520requires%2520a%2520precision%2520of%25202%255E%257B-9%257D%2520while%250Astate-of-the-art%2520solution%2520requires%2520a%2520precision%2520of%25202%255E%257B-12%257D%2520using%2520the%2520ResNet-20%250Amodel%2520on%2520CIFAR-10%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20with%20%28Low-Precision%29%20Polynomial%20Approximations%3A%20New%0A%20%20Insights%20and%20Techniques%20for%20Accuracy%20Improvement&entry.906535625=Chi%20Zhang%20and%20Jingjing%20Fan%20and%20Man%20Ho%20Au%20and%20Siu%20Ming%20Yiu&entry.1292438233=%20%20Replacing%20non-polynomial%20functions%20%28e.g.%2C%20non-linear%20activation%20functions%0Asuch%20as%20ReLU%29%20in%20a%20neural%20network%20with%20their%20polynomial%20approximations%20is%20a%0Astandard%20practice%20in%20privacy-preserving%20machine%20learning.%20The%20resulting%20neural%0Anetwork%2C%20called%20polynomial%20approximation%20of%20neural%20network%20%28PANN%29%20in%20this%0Apaper%2C%20is%20compatible%20with%20advanced%20cryptosystems%20to%20enable%20privacy-preserving%0Amodel%20inference.%20Using%20%60%60highly%20precise%27%27%20approximation%2C%20state-of-the-art%20PANN%0Aoffers%20similar%20inference%20accuracy%20as%20the%20underlying%20backbone%20model.%20However%2C%0Alittle%20is%20known%20about%20the%20effect%20of%20approximation%2C%20and%20existing%20literature%0Aoften%20determined%20the%20required%20approximation%20precision%20empirically.%20In%20this%0Apaper%2C%20we%20initiate%20the%20investigation%20of%20PANN%20as%20a%20standalone%20object.%0ASpecifically%2C%20our%20contribution%20is%20two-fold.%20Firstly%2C%20we%20provide%20an%20explanation%0Aon%20the%20effect%20of%20approximate%20error%20in%20PANN.%20In%20particular%2C%20we%20discovered%20that%0A%281%29%20PANN%20is%20susceptible%20to%20some%20type%20of%20perturbations%3B%20and%20%282%29%20weight%0Aregularisation%20significantly%20reduces%20PANN%27s%20accuracy.%20We%20support%20our%0Aexplanation%20with%20experiments.%20Secondly%2C%20based%20on%20the%20insights%20from%20our%0Ainvestigations%2C%20we%20propose%20solutions%20to%20increase%20inference%20accuracy%20for%20PANN.%0AExperiments%20showed%20that%20combination%20of%20our%20solutions%20is%20very%20effective%3A%20at%20the%0Asame%20precision%2C%20our%20PANN%20is%2010%25%20to%2050%25%20more%20accurate%20than%20state-of-the-arts%3B%0Aand%20at%20the%20same%20accuracy%2C%20our%20PANN%20only%20requires%20a%20precision%20of%202%5E%7B-9%7D%20while%0Astate-of-the-art%20solution%20requires%20a%20precision%20of%202%5E%7B-12%7D%20using%20the%20ResNet-20%0Amodel%20on%20CIFAR-10%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11224v2&entry.124074799=Read"},
{"title": "Cross-Domain Synthetic-to-Real In-the-Wild Depth and Normal Estimation\n  for 3D Scene Understanding", "author": "Jay Bhanushali and Manivannan Muniyandi and Praneeth Chakravarthula", "abstract": "  We present a cross-domain inference technique that learns from synthetic data\nto estimate depth and normals for in-the-wild omnidirectional 3D scenes\nencountered in real-world uncontrolled settings. To this end, we introduce\nUBotNet, an architecture that combines UNet and Bottleneck Transformer elements\nto predict consistent scene normals and depth. We also introduce the\nOmniHorizon synthetic dataset containing 24,335 omnidirectional images that\nrepresent a wide variety of outdoor environments, including buildings, streets,\nand diverse vegetation. This dataset is generated from expansive, lifelike\nvirtual spaces and encompasses dynamic scene elements, such as changing\nlighting conditions, different times of day, pedestrians, and vehicles. Our\nexperiments show that UBotNet achieves significantly improved accuracy in depth\nestimation and normal estimation compared to existing models. Lastly, we\nvalidate cross-domain synthetic-to-real depth and normal estimation on real\noutdoor images using UBotNet trained solely on our synthetic OmniHorizon\ndataset, demonstrating the potential of both the synthetic dataset and the\nproposed network for real-world scene understanding applications.\n", "link": "http://arxiv.org/abs/2212.05040v3", "date": "2024-06-07", "relevancy": 1.6346, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Synthetic-to-Real%20In-the-Wild%20Depth%20and%20Normal%20Estimation%0A%20%20for%203D%20Scene%20Understanding&body=Title%3A%20Cross-Domain%20Synthetic-to-Real%20In-the-Wild%20Depth%20and%20Normal%20Estimation%0A%20%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Jay%20Bhanushali%20and%20Manivannan%20Muniyandi%20and%20Praneeth%20Chakravarthula%0AAbstract%3A%20%20%20We%20present%20a%20cross-domain%20inference%20technique%20that%20learns%20from%20synthetic%20data%0Ato%20estimate%20depth%20and%20normals%20for%20in-the-wild%20omnidirectional%203D%20scenes%0Aencountered%20in%20real-world%20uncontrolled%20settings.%20To%20this%20end%2C%20we%20introduce%0AUBotNet%2C%20an%20architecture%20that%20combines%20UNet%20and%20Bottleneck%20Transformer%20elements%0Ato%20predict%20consistent%20scene%20normals%20and%20depth.%20We%20also%20introduce%20the%0AOmniHorizon%20synthetic%20dataset%20containing%2024%2C335%20omnidirectional%20images%20that%0Arepresent%20a%20wide%20variety%20of%20outdoor%20environments%2C%20including%20buildings%2C%20streets%2C%0Aand%20diverse%20vegetation.%20This%20dataset%20is%20generated%20from%20expansive%2C%20lifelike%0Avirtual%20spaces%20and%20encompasses%20dynamic%20scene%20elements%2C%20such%20as%20changing%0Alighting%20conditions%2C%20different%20times%20of%20day%2C%20pedestrians%2C%20and%20vehicles.%20Our%0Aexperiments%20show%20that%20UBotNet%20achieves%20significantly%20improved%20accuracy%20in%20depth%0Aestimation%20and%20normal%20estimation%20compared%20to%20existing%20models.%20Lastly%2C%20we%0Avalidate%20cross-domain%20synthetic-to-real%20depth%20and%20normal%20estimation%20on%20real%0Aoutdoor%20images%20using%20UBotNet%20trained%20solely%20on%20our%20synthetic%20OmniHorizon%0Adataset%2C%20demonstrating%20the%20potential%20of%20both%20the%20synthetic%20dataset%20and%20the%0Aproposed%20network%20for%20real-world%20scene%20understanding%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.05040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Synthetic-to-Real%2520In-the-Wild%2520Depth%2520and%2520Normal%2520Estimation%250A%2520%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DJay%2520Bhanushali%2520and%2520Manivannan%2520Muniyandi%2520and%2520Praneeth%2520Chakravarthula%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520cross-domain%2520inference%2520technique%2520that%2520learns%2520from%2520synthetic%2520data%250Ato%2520estimate%2520depth%2520and%2520normals%2520for%2520in-the-wild%2520omnidirectional%25203D%2520scenes%250Aencountered%2520in%2520real-world%2520uncontrolled%2520settings.%2520To%2520this%2520end%252C%2520we%2520introduce%250AUBotNet%252C%2520an%2520architecture%2520that%2520combines%2520UNet%2520and%2520Bottleneck%2520Transformer%2520elements%250Ato%2520predict%2520consistent%2520scene%2520normals%2520and%2520depth.%2520We%2520also%2520introduce%2520the%250AOmniHorizon%2520synthetic%2520dataset%2520containing%252024%252C335%2520omnidirectional%2520images%2520that%250Arepresent%2520a%2520wide%2520variety%2520of%2520outdoor%2520environments%252C%2520including%2520buildings%252C%2520streets%252C%250Aand%2520diverse%2520vegetation.%2520This%2520dataset%2520is%2520generated%2520from%2520expansive%252C%2520lifelike%250Avirtual%2520spaces%2520and%2520encompasses%2520dynamic%2520scene%2520elements%252C%2520such%2520as%2520changing%250Alighting%2520conditions%252C%2520different%2520times%2520of%2520day%252C%2520pedestrians%252C%2520and%2520vehicles.%2520Our%250Aexperiments%2520show%2520that%2520UBotNet%2520achieves%2520significantly%2520improved%2520accuracy%2520in%2520depth%250Aestimation%2520and%2520normal%2520estimation%2520compared%2520to%2520existing%2520models.%2520Lastly%252C%2520we%250Avalidate%2520cross-domain%2520synthetic-to-real%2520depth%2520and%2520normal%2520estimation%2520on%2520real%250Aoutdoor%2520images%2520using%2520UBotNet%2520trained%2520solely%2520on%2520our%2520synthetic%2520OmniHorizon%250Adataset%252C%2520demonstrating%2520the%2520potential%2520of%2520both%2520the%2520synthetic%2520dataset%2520and%2520the%250Aproposed%2520network%2520for%2520real-world%2520scene%2520understanding%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.05040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Synthetic-to-Real%20In-the-Wild%20Depth%20and%20Normal%20Estimation%0A%20%20for%203D%20Scene%20Understanding&entry.906535625=Jay%20Bhanushali%20and%20Manivannan%20Muniyandi%20and%20Praneeth%20Chakravarthula&entry.1292438233=%20%20We%20present%20a%20cross-domain%20inference%20technique%20that%20learns%20from%20synthetic%20data%0Ato%20estimate%20depth%20and%20normals%20for%20in-the-wild%20omnidirectional%203D%20scenes%0Aencountered%20in%20real-world%20uncontrolled%20settings.%20To%20this%20end%2C%20we%20introduce%0AUBotNet%2C%20an%20architecture%20that%20combines%20UNet%20and%20Bottleneck%20Transformer%20elements%0Ato%20predict%20consistent%20scene%20normals%20and%20depth.%20We%20also%20introduce%20the%0AOmniHorizon%20synthetic%20dataset%20containing%2024%2C335%20omnidirectional%20images%20that%0Arepresent%20a%20wide%20variety%20of%20outdoor%20environments%2C%20including%20buildings%2C%20streets%2C%0Aand%20diverse%20vegetation.%20This%20dataset%20is%20generated%20from%20expansive%2C%20lifelike%0Avirtual%20spaces%20and%20encompasses%20dynamic%20scene%20elements%2C%20such%20as%20changing%0Alighting%20conditions%2C%20different%20times%20of%20day%2C%20pedestrians%2C%20and%20vehicles.%20Our%0Aexperiments%20show%20that%20UBotNet%20achieves%20significantly%20improved%20accuracy%20in%20depth%0Aestimation%20and%20normal%20estimation%20compared%20to%20existing%20models.%20Lastly%2C%20we%0Avalidate%20cross-domain%20synthetic-to-real%20depth%20and%20normal%20estimation%20on%20real%0Aoutdoor%20images%20using%20UBotNet%20trained%20solely%20on%20our%20synthetic%20OmniHorizon%0Adataset%2C%20demonstrating%20the%20potential%20of%20both%20the%20synthetic%20dataset%20and%20the%0Aproposed%20network%20for%20real-world%20scene%20understanding%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.05040v3&entry.124074799=Read"},
{"title": "MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual\n  Transformers", "author": "Tanvir Mahmud and Shentong Mo and Yapeng Tian and Diana Marculescu", "abstract": "  Recent advances in pre-trained vision transformers have shown promise in\nparameter-efficient audio-visual learning without audio pre-training. However,\nfew studies have investigated effective methods for aligning multimodal\nfeatures in parameter-efficient audio-visual transformers. In this paper, we\npropose MA-AVT, a new parameter-efficient audio-visual transformer employing\ndeep modality alignment for corresponding multimodal semantic features.\nSpecifically, we introduce joint unimodal and multimodal token learning for\naligning the two modalities with a frozen modality-shared transformer. This\nallows the model to learn separate representations for each modality, while\nalso attending to the cross-modal relationships between them. In addition,\nunlike prior work that only aligns coarse features from the output of unimodal\nencoders, we introduce blockwise contrastive learning to align\ncoarse-to-fine-grain hierarchical features throughout the encoding phase.\nFurthermore, to suppress the background features in each modality from\nforeground matched audio-visual features, we introduce a robust discriminative\nforeground mining scheme. Through extensive experiments on benchmark AVE,\nVGGSound, and CREMA-D datasets, we achieve considerable performance\nimprovements over SOTA methods.\n", "link": "http://arxiv.org/abs/2406.04930v1", "date": "2024-06-07", "relevancy": 1.704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5281}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MA-AVT%3A%20Modality%20Alignment%20for%20Parameter-Efficient%20Audio-Visual%0A%20%20Transformers&body=Title%3A%20MA-AVT%3A%20Modality%20Alignment%20for%20Parameter-Efficient%20Audio-Visual%0A%20%20Transformers%0AAuthor%3A%20Tanvir%20Mahmud%20and%20Shentong%20Mo%20and%20Yapeng%20Tian%20and%20Diana%20Marculescu%0AAbstract%3A%20%20%20Recent%20advances%20in%20pre-trained%20vision%20transformers%20have%20shown%20promise%20in%0Aparameter-efficient%20audio-visual%20learning%20without%20audio%20pre-training.%20However%2C%0Afew%20studies%20have%20investigated%20effective%20methods%20for%20aligning%20multimodal%0Afeatures%20in%20parameter-efficient%20audio-visual%20transformers.%20In%20this%20paper%2C%20we%0Apropose%20MA-AVT%2C%20a%20new%20parameter-efficient%20audio-visual%20transformer%20employing%0Adeep%20modality%20alignment%20for%20corresponding%20multimodal%20semantic%20features.%0ASpecifically%2C%20we%20introduce%20joint%20unimodal%20and%20multimodal%20token%20learning%20for%0Aaligning%20the%20two%20modalities%20with%20a%20frozen%20modality-shared%20transformer.%20This%0Aallows%20the%20model%20to%20learn%20separate%20representations%20for%20each%20modality%2C%20while%0Aalso%20attending%20to%20the%20cross-modal%20relationships%20between%20them.%20In%20addition%2C%0Aunlike%20prior%20work%20that%20only%20aligns%20coarse%20features%20from%20the%20output%20of%20unimodal%0Aencoders%2C%20we%20introduce%20blockwise%20contrastive%20learning%20to%20align%0Acoarse-to-fine-grain%20hierarchical%20features%20throughout%20the%20encoding%20phase.%0AFurthermore%2C%20to%20suppress%20the%20background%20features%20in%20each%20modality%20from%0Aforeground%20matched%20audio-visual%20features%2C%20we%20introduce%20a%20robust%20discriminative%0Aforeground%20mining%20scheme.%20Through%20extensive%20experiments%20on%20benchmark%20AVE%2C%0AVGGSound%2C%20and%20CREMA-D%20datasets%2C%20we%20achieve%20considerable%20performance%0Aimprovements%20over%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMA-AVT%253A%2520Modality%2520Alignment%2520for%2520Parameter-Efficient%2520Audio-Visual%250A%2520%2520Transformers%26entry.906535625%3DTanvir%2520Mahmud%2520and%2520Shentong%2520Mo%2520and%2520Yapeng%2520Tian%2520and%2520Diana%2520Marculescu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520pre-trained%2520vision%2520transformers%2520have%2520shown%2520promise%2520in%250Aparameter-efficient%2520audio-visual%2520learning%2520without%2520audio%2520pre-training.%2520However%252C%250Afew%2520studies%2520have%2520investigated%2520effective%2520methods%2520for%2520aligning%2520multimodal%250Afeatures%2520in%2520parameter-efficient%2520audio-visual%2520transformers.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520MA-AVT%252C%2520a%2520new%2520parameter-efficient%2520audio-visual%2520transformer%2520employing%250Adeep%2520modality%2520alignment%2520for%2520corresponding%2520multimodal%2520semantic%2520features.%250ASpecifically%252C%2520we%2520introduce%2520joint%2520unimodal%2520and%2520multimodal%2520token%2520learning%2520for%250Aaligning%2520the%2520two%2520modalities%2520with%2520a%2520frozen%2520modality-shared%2520transformer.%2520This%250Aallows%2520the%2520model%2520to%2520learn%2520separate%2520representations%2520for%2520each%2520modality%252C%2520while%250Aalso%2520attending%2520to%2520the%2520cross-modal%2520relationships%2520between%2520them.%2520In%2520addition%252C%250Aunlike%2520prior%2520work%2520that%2520only%2520aligns%2520coarse%2520features%2520from%2520the%2520output%2520of%2520unimodal%250Aencoders%252C%2520we%2520introduce%2520blockwise%2520contrastive%2520learning%2520to%2520align%250Acoarse-to-fine-grain%2520hierarchical%2520features%2520throughout%2520the%2520encoding%2520phase.%250AFurthermore%252C%2520to%2520suppress%2520the%2520background%2520features%2520in%2520each%2520modality%2520from%250Aforeground%2520matched%2520audio-visual%2520features%252C%2520we%2520introduce%2520a%2520robust%2520discriminative%250Aforeground%2520mining%2520scheme.%2520Through%2520extensive%2520experiments%2520on%2520benchmark%2520AVE%252C%250AVGGSound%252C%2520and%2520CREMA-D%2520datasets%252C%2520we%2520achieve%2520considerable%2520performance%250Aimprovements%2520over%2520SOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MA-AVT%3A%20Modality%20Alignment%20for%20Parameter-Efficient%20Audio-Visual%0A%20%20Transformers&entry.906535625=Tanvir%20Mahmud%20and%20Shentong%20Mo%20and%20Yapeng%20Tian%20and%20Diana%20Marculescu&entry.1292438233=%20%20Recent%20advances%20in%20pre-trained%20vision%20transformers%20have%20shown%20promise%20in%0Aparameter-efficient%20audio-visual%20learning%20without%20audio%20pre-training.%20However%2C%0Afew%20studies%20have%20investigated%20effective%20methods%20for%20aligning%20multimodal%0Afeatures%20in%20parameter-efficient%20audio-visual%20transformers.%20In%20this%20paper%2C%20we%0Apropose%20MA-AVT%2C%20a%20new%20parameter-efficient%20audio-visual%20transformer%20employing%0Adeep%20modality%20alignment%20for%20corresponding%20multimodal%20semantic%20features.%0ASpecifically%2C%20we%20introduce%20joint%20unimodal%20and%20multimodal%20token%20learning%20for%0Aaligning%20the%20two%20modalities%20with%20a%20frozen%20modality-shared%20transformer.%20This%0Aallows%20the%20model%20to%20learn%20separate%20representations%20for%20each%20modality%2C%20while%0Aalso%20attending%20to%20the%20cross-modal%20relationships%20between%20them.%20In%20addition%2C%0Aunlike%20prior%20work%20that%20only%20aligns%20coarse%20features%20from%20the%20output%20of%20unimodal%0Aencoders%2C%20we%20introduce%20blockwise%20contrastive%20learning%20to%20align%0Acoarse-to-fine-grain%20hierarchical%20features%20throughout%20the%20encoding%20phase.%0AFurthermore%2C%20to%20suppress%20the%20background%20features%20in%20each%20modality%20from%0Aforeground%20matched%20audio-visual%20features%2C%20we%20introduce%20a%20robust%20discriminative%0Aforeground%20mining%20scheme.%20Through%20extensive%20experiments%20on%20benchmark%20AVE%2C%0AVGGSound%2C%20and%20CREMA-D%20datasets%2C%20we%20achieve%20considerable%20performance%0Aimprovements%20over%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04930v1&entry.124074799=Read"},
{"title": "Stabilizing Extreme Q-learning by Maclaurin Expansion", "author": "Motoki Omura and Takayuki Osa and Yusuke Mukuta and Tatsuya Harada", "abstract": "  In Extreme Q-learning (XQL), Gumbel Regression is performed with an assumed\nGumbel distribution for the error distribution. This allows learning of the\nvalue function without sampling out-of-distribution actions and has shown\nexcellent performance mainly in Offline RL. However, issues remained, including\nthe exponential term in the loss function causing instability and the potential\nfor an error distribution diverging from the Gumbel distribution. Therefore, we\npropose Maclaurin Expanded Extreme Q-learning to enhance stability. In this\nmethod, applying Maclaurin expansion to the loss function in XQL enhances\nstability against large errors. It also allows adjusting the error distribution\nassumption from normal to Gumbel based on the expansion order. Our method\nsignificantly stabilizes learning in Online RL tasks from DM Control, where XQL\nwas previously unstable. Additionally, it improves performance in several\nOffline RL tasks from D4RL, where XQL already showed excellent results.\n", "link": "http://arxiv.org/abs/2406.04896v1", "date": "2024-06-07", "relevancy": 1.7278, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4402}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4318}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stabilizing%20Extreme%20Q-learning%20by%20Maclaurin%20Expansion&body=Title%3A%20Stabilizing%20Extreme%20Q-learning%20by%20Maclaurin%20Expansion%0AAuthor%3A%20Motoki%20Omura%20and%20Takayuki%20Osa%20and%20Yusuke%20Mukuta%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20In%20Extreme%20Q-learning%20%28XQL%29%2C%20Gumbel%20Regression%20is%20performed%20with%20an%20assumed%0AGumbel%20distribution%20for%20the%20error%20distribution.%20This%20allows%20learning%20of%20the%0Avalue%20function%20without%20sampling%20out-of-distribution%20actions%20and%20has%20shown%0Aexcellent%20performance%20mainly%20in%20Offline%20RL.%20However%2C%20issues%20remained%2C%20including%0Athe%20exponential%20term%20in%20the%20loss%20function%20causing%20instability%20and%20the%20potential%0Afor%20an%20error%20distribution%20diverging%20from%20the%20Gumbel%20distribution.%20Therefore%2C%20we%0Apropose%20Maclaurin%20Expanded%20Extreme%20Q-learning%20to%20enhance%20stability.%20In%20this%0Amethod%2C%20applying%20Maclaurin%20expansion%20to%20the%20loss%20function%20in%20XQL%20enhances%0Astability%20against%20large%20errors.%20It%20also%20allows%20adjusting%20the%20error%20distribution%0Aassumption%20from%20normal%20to%20Gumbel%20based%20on%20the%20expansion%20order.%20Our%20method%0Asignificantly%20stabilizes%20learning%20in%20Online%20RL%20tasks%20from%20DM%20Control%2C%20where%20XQL%0Awas%20previously%20unstable.%20Additionally%2C%20it%20improves%20performance%20in%20several%0AOffline%20RL%20tasks%20from%20D4RL%2C%20where%20XQL%20already%20showed%20excellent%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStabilizing%2520Extreme%2520Q-learning%2520by%2520Maclaurin%2520Expansion%26entry.906535625%3DMotoki%2520Omura%2520and%2520Takayuki%2520Osa%2520and%2520Yusuke%2520Mukuta%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520In%2520Extreme%2520Q-learning%2520%2528XQL%2529%252C%2520Gumbel%2520Regression%2520is%2520performed%2520with%2520an%2520assumed%250AGumbel%2520distribution%2520for%2520the%2520error%2520distribution.%2520This%2520allows%2520learning%2520of%2520the%250Avalue%2520function%2520without%2520sampling%2520out-of-distribution%2520actions%2520and%2520has%2520shown%250Aexcellent%2520performance%2520mainly%2520in%2520Offline%2520RL.%2520However%252C%2520issues%2520remained%252C%2520including%250Athe%2520exponential%2520term%2520in%2520the%2520loss%2520function%2520causing%2520instability%2520and%2520the%2520potential%250Afor%2520an%2520error%2520distribution%2520diverging%2520from%2520the%2520Gumbel%2520distribution.%2520Therefore%252C%2520we%250Apropose%2520Maclaurin%2520Expanded%2520Extreme%2520Q-learning%2520to%2520enhance%2520stability.%2520In%2520this%250Amethod%252C%2520applying%2520Maclaurin%2520expansion%2520to%2520the%2520loss%2520function%2520in%2520XQL%2520enhances%250Astability%2520against%2520large%2520errors.%2520It%2520also%2520allows%2520adjusting%2520the%2520error%2520distribution%250Aassumption%2520from%2520normal%2520to%2520Gumbel%2520based%2520on%2520the%2520expansion%2520order.%2520Our%2520method%250Asignificantly%2520stabilizes%2520learning%2520in%2520Online%2520RL%2520tasks%2520from%2520DM%2520Control%252C%2520where%2520XQL%250Awas%2520previously%2520unstable.%2520Additionally%252C%2520it%2520improves%2520performance%2520in%2520several%250AOffline%2520RL%2520tasks%2520from%2520D4RL%252C%2520where%2520XQL%2520already%2520showed%2520excellent%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stabilizing%20Extreme%20Q-learning%20by%20Maclaurin%20Expansion&entry.906535625=Motoki%20Omura%20and%20Takayuki%20Osa%20and%20Yusuke%20Mukuta%20and%20Tatsuya%20Harada&entry.1292438233=%20%20In%20Extreme%20Q-learning%20%28XQL%29%2C%20Gumbel%20Regression%20is%20performed%20with%20an%20assumed%0AGumbel%20distribution%20for%20the%20error%20distribution.%20This%20allows%20learning%20of%20the%0Avalue%20function%20without%20sampling%20out-of-distribution%20actions%20and%20has%20shown%0Aexcellent%20performance%20mainly%20in%20Offline%20RL.%20However%2C%20issues%20remained%2C%20including%0Athe%20exponential%20term%20in%20the%20loss%20function%20causing%20instability%20and%20the%20potential%0Afor%20an%20error%20distribution%20diverging%20from%20the%20Gumbel%20distribution.%20Therefore%2C%20we%0Apropose%20Maclaurin%20Expanded%20Extreme%20Q-learning%20to%20enhance%20stability.%20In%20this%0Amethod%2C%20applying%20Maclaurin%20expansion%20to%20the%20loss%20function%20in%20XQL%20enhances%0Astability%20against%20large%20errors.%20It%20also%20allows%20adjusting%20the%20error%20distribution%0Aassumption%20from%20normal%20to%20Gumbel%20based%20on%20the%20expansion%20order.%20Our%20method%0Asignificantly%20stabilizes%20learning%20in%20Online%20RL%20tasks%20from%20DM%20Control%2C%20where%20XQL%0Awas%20previously%20unstable.%20Additionally%2C%20it%20improves%20performance%20in%20several%0AOffline%20RL%20tasks%20from%20D4RL%2C%20where%20XQL%20already%20showed%20excellent%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04896v1&entry.124074799=Read"},
{"title": "Diffusion posterior sampling for simulation-based inference in tall data\n  settings", "author": "Julia Linhart and Gabriel Victorino Cardoso and Alexandre Gramfort and Sylvain Le Corff and Pedro L. C. Rodrigues", "abstract": "  Determining which parameters of a non-linear model best describe a set of\nexperimental data is a fundamental problem in science and it has gained much\ntraction lately with the rise of complex large-scale simulators. The likelihood\nof such models is typically intractable, which is why classical MCMC methods\ncan not be used. Simulation-based inference (SBI) stands out in this context by\nonly requiring a dataset of simulations to train deep generative models capable\nof approximating the posterior distribution that relates input parameters to a\ngiven observation. In this work, we consider a tall data extension in which\nmultiple observations are available to better infer the parameters of the\nmodel. The proposed method is built upon recent developments from the\nflourishing score-based diffusion literature and allows to estimate the tall\ndata posterior distribution, while simply using information from a score\nnetwork trained for a single context observation. We compare our method to\nrecently proposed competing approaches on various numerical experiments and\ndemonstrate its superiority in terms of numerical stability and computational\ncost.\n", "link": "http://arxiv.org/abs/2404.07593v2", "date": "2024-06-07", "relevancy": 1.582, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5598}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20posterior%20sampling%20for%20simulation-based%20inference%20in%20tall%20data%0A%20%20settings&body=Title%3A%20Diffusion%20posterior%20sampling%20for%20simulation-based%20inference%20in%20tall%20data%0A%20%20settings%0AAuthor%3A%20Julia%20Linhart%20and%20Gabriel%20Victorino%20Cardoso%20and%20Alexandre%20Gramfort%20and%20Sylvain%20Le%20Corff%20and%20Pedro%20L.%20C.%20Rodrigues%0AAbstract%3A%20%20%20Determining%20which%20parameters%20of%20a%20non-linear%20model%20best%20describe%20a%20set%20of%0Aexperimental%20data%20is%20a%20fundamental%20problem%20in%20science%20and%20it%20has%20gained%20much%0Atraction%20lately%20with%20the%20rise%20of%20complex%20large-scale%20simulators.%20The%20likelihood%0Aof%20such%20models%20is%20typically%20intractable%2C%20which%20is%20why%20classical%20MCMC%20methods%0Acan%20not%20be%20used.%20Simulation-based%20inference%20%28SBI%29%20stands%20out%20in%20this%20context%20by%0Aonly%20requiring%20a%20dataset%20of%20simulations%20to%20train%20deep%20generative%20models%20capable%0Aof%20approximating%20the%20posterior%20distribution%20that%20relates%20input%20parameters%20to%20a%0Agiven%20observation.%20In%20this%20work%2C%20we%20consider%20a%20tall%20data%20extension%20in%20which%0Amultiple%20observations%20are%20available%20to%20better%20infer%20the%20parameters%20of%20the%0Amodel.%20The%20proposed%20method%20is%20built%20upon%20recent%20developments%20from%20the%0Aflourishing%20score-based%20diffusion%20literature%20and%20allows%20to%20estimate%20the%20tall%0Adata%20posterior%20distribution%2C%20while%20simply%20using%20information%20from%20a%20score%0Anetwork%20trained%20for%20a%20single%20context%20observation.%20We%20compare%20our%20method%20to%0Arecently%20proposed%20competing%20approaches%20on%20various%20numerical%20experiments%20and%0Ademonstrate%20its%20superiority%20in%20terms%20of%20numerical%20stability%20and%20computational%0Acost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520posterior%2520sampling%2520for%2520simulation-based%2520inference%2520in%2520tall%2520data%250A%2520%2520settings%26entry.906535625%3DJulia%2520Linhart%2520and%2520Gabriel%2520Victorino%2520Cardoso%2520and%2520Alexandre%2520Gramfort%2520and%2520Sylvain%2520Le%2520Corff%2520and%2520Pedro%2520L.%2520C.%2520Rodrigues%26entry.1292438233%3D%2520%2520Determining%2520which%2520parameters%2520of%2520a%2520non-linear%2520model%2520best%2520describe%2520a%2520set%2520of%250Aexperimental%2520data%2520is%2520a%2520fundamental%2520problem%2520in%2520science%2520and%2520it%2520has%2520gained%2520much%250Atraction%2520lately%2520with%2520the%2520rise%2520of%2520complex%2520large-scale%2520simulators.%2520The%2520likelihood%250Aof%2520such%2520models%2520is%2520typically%2520intractable%252C%2520which%2520is%2520why%2520classical%2520MCMC%2520methods%250Acan%2520not%2520be%2520used.%2520Simulation-based%2520inference%2520%2528SBI%2529%2520stands%2520out%2520in%2520this%2520context%2520by%250Aonly%2520requiring%2520a%2520dataset%2520of%2520simulations%2520to%2520train%2520deep%2520generative%2520models%2520capable%250Aof%2520approximating%2520the%2520posterior%2520distribution%2520that%2520relates%2520input%2520parameters%2520to%2520a%250Agiven%2520observation.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520tall%2520data%2520extension%2520in%2520which%250Amultiple%2520observations%2520are%2520available%2520to%2520better%2520infer%2520the%2520parameters%2520of%2520the%250Amodel.%2520The%2520proposed%2520method%2520is%2520built%2520upon%2520recent%2520developments%2520from%2520the%250Aflourishing%2520score-based%2520diffusion%2520literature%2520and%2520allows%2520to%2520estimate%2520the%2520tall%250Adata%2520posterior%2520distribution%252C%2520while%2520simply%2520using%2520information%2520from%2520a%2520score%250Anetwork%2520trained%2520for%2520a%2520single%2520context%2520observation.%2520We%2520compare%2520our%2520method%2520to%250Arecently%2520proposed%2520competing%2520approaches%2520on%2520various%2520numerical%2520experiments%2520and%250Ademonstrate%2520its%2520superiority%2520in%2520terms%2520of%2520numerical%2520stability%2520and%2520computational%250Acost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20posterior%20sampling%20for%20simulation-based%20inference%20in%20tall%20data%0A%20%20settings&entry.906535625=Julia%20Linhart%20and%20Gabriel%20Victorino%20Cardoso%20and%20Alexandre%20Gramfort%20and%20Sylvain%20Le%20Corff%20and%20Pedro%20L.%20C.%20Rodrigues&entry.1292438233=%20%20Determining%20which%20parameters%20of%20a%20non-linear%20model%20best%20describe%20a%20set%20of%0Aexperimental%20data%20is%20a%20fundamental%20problem%20in%20science%20and%20it%20has%20gained%20much%0Atraction%20lately%20with%20the%20rise%20of%20complex%20large-scale%20simulators.%20The%20likelihood%0Aof%20such%20models%20is%20typically%20intractable%2C%20which%20is%20why%20classical%20MCMC%20methods%0Acan%20not%20be%20used.%20Simulation-based%20inference%20%28SBI%29%20stands%20out%20in%20this%20context%20by%0Aonly%20requiring%20a%20dataset%20of%20simulations%20to%20train%20deep%20generative%20models%20capable%0Aof%20approximating%20the%20posterior%20distribution%20that%20relates%20input%20parameters%20to%20a%0Agiven%20observation.%20In%20this%20work%2C%20we%20consider%20a%20tall%20data%20extension%20in%20which%0Amultiple%20observations%20are%20available%20to%20better%20infer%20the%20parameters%20of%20the%0Amodel.%20The%20proposed%20method%20is%20built%20upon%20recent%20developments%20from%20the%0Aflourishing%20score-based%20diffusion%20literature%20and%20allows%20to%20estimate%20the%20tall%0Adata%20posterior%20distribution%2C%20while%20simply%20using%20information%20from%20a%20score%0Anetwork%20trained%20for%20a%20single%20context%20observation.%20We%20compare%20our%20method%20to%0Arecently%20proposed%20competing%20approaches%20on%20various%20numerical%20experiments%20and%0Ademonstrate%20its%20superiority%20in%20terms%20of%20numerical%20stability%20and%20computational%0Acost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07593v2&entry.124074799=Read"},
{"title": "BOtied: Multi-objective Bayesian optimization with tied multivariate\n  ranks", "author": "Ji Won Park and Nata\u0161a Tagasovska and Michael Maser and Stephen Ra and Kyunghyun Cho", "abstract": "  Many scientific and industrial applications require the joint optimization of\nmultiple, potentially competing objectives. Multi-objective Bayesian\noptimization (MOBO) is a sample-efficient framework for identifying\nPareto-optimal solutions. At the heart of MOBO is the acquisition function,\nwhich determines the next candidate to evaluate by navigating the best\ncompromises among the objectives. In this paper, we show a natural connection\nbetween non-dominated solutions and the extreme quantile of the joint\ncumulative distribution function (CDF). Motivated by this link, we propose the\nPareto-compliant CDF indicator and the associated acquisition function, BOtied.\nBOtied inherits desirable invariance properties of the CDF, and an efficient\nimplementation with copulas allows it to scale to many objectives. Our\nexperiments on a variety of synthetic and real-world problems demonstrate that\nBOtied outperforms state-of-the-art MOBO acquisition functions while being\ncomputationally efficient for many objectives.\n", "link": "http://arxiv.org/abs/2306.00344v2", "date": "2024-06-07", "relevancy": 1.7681, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4235}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOtied%3A%20Multi-objective%20Bayesian%20optimization%20with%20tied%20multivariate%0A%20%20ranks&body=Title%3A%20BOtied%3A%20Multi-objective%20Bayesian%20optimization%20with%20tied%20multivariate%0A%20%20ranks%0AAuthor%3A%20Ji%20Won%20Park%20and%20Nata%C5%A1a%20Tagasovska%20and%20Michael%20Maser%20and%20Stephen%20Ra%20and%20Kyunghyun%20Cho%0AAbstract%3A%20%20%20Many%20scientific%20and%20industrial%20applications%20require%20the%20joint%20optimization%20of%0Amultiple%2C%20potentially%20competing%20objectives.%20Multi-objective%20Bayesian%0Aoptimization%20%28MOBO%29%20is%20a%20sample-efficient%20framework%20for%20identifying%0APareto-optimal%20solutions.%20At%20the%20heart%20of%20MOBO%20is%20the%20acquisition%20function%2C%0Awhich%20determines%20the%20next%20candidate%20to%20evaluate%20by%20navigating%20the%20best%0Acompromises%20among%20the%20objectives.%20In%20this%20paper%2C%20we%20show%20a%20natural%20connection%0Abetween%20non-dominated%20solutions%20and%20the%20extreme%20quantile%20of%20the%20joint%0Acumulative%20distribution%20function%20%28CDF%29.%20Motivated%20by%20this%20link%2C%20we%20propose%20the%0APareto-compliant%20CDF%20indicator%20and%20the%20associated%20acquisition%20function%2C%20BOtied.%0ABOtied%20inherits%20desirable%20invariance%20properties%20of%20the%20CDF%2C%20and%20an%20efficient%0Aimplementation%20with%20copulas%20allows%20it%20to%20scale%20to%20many%20objectives.%20Our%0Aexperiments%20on%20a%20variety%20of%20synthetic%20and%20real-world%20problems%20demonstrate%20that%0ABOtied%20outperforms%20state-of-the-art%20MOBO%20acquisition%20functions%20while%20being%0Acomputationally%20efficient%20for%20many%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOtied%253A%2520Multi-objective%2520Bayesian%2520optimization%2520with%2520tied%2520multivariate%250A%2520%2520ranks%26entry.906535625%3DJi%2520Won%2520Park%2520and%2520Nata%25C5%25A1a%2520Tagasovska%2520and%2520Michael%2520Maser%2520and%2520Stephen%2520Ra%2520and%2520Kyunghyun%2520Cho%26entry.1292438233%3D%2520%2520Many%2520scientific%2520and%2520industrial%2520applications%2520require%2520the%2520joint%2520optimization%2520of%250Amultiple%252C%2520potentially%2520competing%2520objectives.%2520Multi-objective%2520Bayesian%250Aoptimization%2520%2528MOBO%2529%2520is%2520a%2520sample-efficient%2520framework%2520for%2520identifying%250APareto-optimal%2520solutions.%2520At%2520the%2520heart%2520of%2520MOBO%2520is%2520the%2520acquisition%2520function%252C%250Awhich%2520determines%2520the%2520next%2520candidate%2520to%2520evaluate%2520by%2520navigating%2520the%2520best%250Acompromises%2520among%2520the%2520objectives.%2520In%2520this%2520paper%252C%2520we%2520show%2520a%2520natural%2520connection%250Abetween%2520non-dominated%2520solutions%2520and%2520the%2520extreme%2520quantile%2520of%2520the%2520joint%250Acumulative%2520distribution%2520function%2520%2528CDF%2529.%2520Motivated%2520by%2520this%2520link%252C%2520we%2520propose%2520the%250APareto-compliant%2520CDF%2520indicator%2520and%2520the%2520associated%2520acquisition%2520function%252C%2520BOtied.%250ABOtied%2520inherits%2520desirable%2520invariance%2520properties%2520of%2520the%2520CDF%252C%2520and%2520an%2520efficient%250Aimplementation%2520with%2520copulas%2520allows%2520it%2520to%2520scale%2520to%2520many%2520objectives.%2520Our%250Aexperiments%2520on%2520a%2520variety%2520of%2520synthetic%2520and%2520real-world%2520problems%2520demonstrate%2520that%250ABOtied%2520outperforms%2520state-of-the-art%2520MOBO%2520acquisition%2520functions%2520while%2520being%250Acomputationally%2520efficient%2520for%2520many%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOtied%3A%20Multi-objective%20Bayesian%20optimization%20with%20tied%20multivariate%0A%20%20ranks&entry.906535625=Ji%20Won%20Park%20and%20Nata%C5%A1a%20Tagasovska%20and%20Michael%20Maser%20and%20Stephen%20Ra%20and%20Kyunghyun%20Cho&entry.1292438233=%20%20Many%20scientific%20and%20industrial%20applications%20require%20the%20joint%20optimization%20of%0Amultiple%2C%20potentially%20competing%20objectives.%20Multi-objective%20Bayesian%0Aoptimization%20%28MOBO%29%20is%20a%20sample-efficient%20framework%20for%20identifying%0APareto-optimal%20solutions.%20At%20the%20heart%20of%20MOBO%20is%20the%20acquisition%20function%2C%0Awhich%20determines%20the%20next%20candidate%20to%20evaluate%20by%20navigating%20the%20best%0Acompromises%20among%20the%20objectives.%20In%20this%20paper%2C%20we%20show%20a%20natural%20connection%0Abetween%20non-dominated%20solutions%20and%20the%20extreme%20quantile%20of%20the%20joint%0Acumulative%20distribution%20function%20%28CDF%29.%20Motivated%20by%20this%20link%2C%20we%20propose%20the%0APareto-compliant%20CDF%20indicator%20and%20the%20associated%20acquisition%20function%2C%20BOtied.%0ABOtied%20inherits%20desirable%20invariance%20properties%20of%20the%20CDF%2C%20and%20an%20efficient%0Aimplementation%20with%20copulas%20allows%20it%20to%20scale%20to%20many%20objectives.%20Our%0Aexperiments%20on%20a%20variety%20of%20synthetic%20and%20real-world%20problems%20demonstrate%20that%0ABOtied%20outperforms%20state-of-the-art%20MOBO%20acquisition%20functions%20while%20being%0Acomputationally%20efficient%20for%20many%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00344v2&entry.124074799=Read"},
{"title": "SLR: Learning Quadruped Locomotion without Privileged Information", "author": "Shiyi Chen and Zeyu Wan and Shiyang Yan and Chun Zhang and Weiyi Zhang and Qiang Li and Debing Zhang and Fasih Ud Din Farrukh", "abstract": "  Traditional reinforcement learning control for quadruped robots often relies\non privileged information, demanding meticulous selection and precise\nestimation, thereby imposing constraints on the development process. This work\nproposes a Self-learning Latent Representation (SLR) method, which achieves\nhigh-performance control policy learning without the need for privileged\ninformation. To enhance the credibility of our proposed method's evaluation,\nSLR is compared with open-source code repositories of state-of-the-art\nalgorithms, retaining the original authors' configuration parameters. Across\nfour repositories, SLR consistently outperforms the reference results.\nUltimately, the trained policy and encoder empower the quadruped robot to\nnavigate steps, climb stairs, ascend rocks, and traverse various challenging\nterrains. Robot experiment videos are at https://11chens.github.io/SLR/\n", "link": "http://arxiv.org/abs/2406.04835v1", "date": "2024-06-07", "relevancy": 1.7022, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6114}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5552}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLR%3A%20Learning%20Quadruped%20Locomotion%20without%20Privileged%20Information&body=Title%3A%20SLR%3A%20Learning%20Quadruped%20Locomotion%20without%20Privileged%20Information%0AAuthor%3A%20Shiyi%20Chen%20and%20Zeyu%20Wan%20and%20Shiyang%20Yan%20and%20Chun%20Zhang%20and%20Weiyi%20Zhang%20and%20Qiang%20Li%20and%20Debing%20Zhang%20and%20Fasih%20Ud%20Din%20Farrukh%0AAbstract%3A%20%20%20Traditional%20reinforcement%20learning%20control%20for%20quadruped%20robots%20often%20relies%0Aon%20privileged%20information%2C%20demanding%20meticulous%20selection%20and%20precise%0Aestimation%2C%20thereby%20imposing%20constraints%20on%20the%20development%20process.%20This%20work%0Aproposes%20a%20Self-learning%20Latent%20Representation%20%28SLR%29%20method%2C%20which%20achieves%0Ahigh-performance%20control%20policy%20learning%20without%20the%20need%20for%20privileged%0Ainformation.%20To%20enhance%20the%20credibility%20of%20our%20proposed%20method%27s%20evaluation%2C%0ASLR%20is%20compared%20with%20open-source%20code%20repositories%20of%20state-of-the-art%0Aalgorithms%2C%20retaining%20the%20original%20authors%27%20configuration%20parameters.%20Across%0Afour%20repositories%2C%20SLR%20consistently%20outperforms%20the%20reference%20results.%0AUltimately%2C%20the%20trained%20policy%20and%20encoder%20empower%20the%20quadruped%20robot%20to%0Anavigate%20steps%2C%20climb%20stairs%2C%20ascend%20rocks%2C%20and%20traverse%20various%20challenging%0Aterrains.%20Robot%20experiment%20videos%20are%20at%20https%3A//11chens.github.io/SLR/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLR%253A%2520Learning%2520Quadruped%2520Locomotion%2520without%2520Privileged%2520Information%26entry.906535625%3DShiyi%2520Chen%2520and%2520Zeyu%2520Wan%2520and%2520Shiyang%2520Yan%2520and%2520Chun%2520Zhang%2520and%2520Weiyi%2520Zhang%2520and%2520Qiang%2520Li%2520and%2520Debing%2520Zhang%2520and%2520Fasih%2520Ud%2520Din%2520Farrukh%26entry.1292438233%3D%2520%2520Traditional%2520reinforcement%2520learning%2520control%2520for%2520quadruped%2520robots%2520often%2520relies%250Aon%2520privileged%2520information%252C%2520demanding%2520meticulous%2520selection%2520and%2520precise%250Aestimation%252C%2520thereby%2520imposing%2520constraints%2520on%2520the%2520development%2520process.%2520This%2520work%250Aproposes%2520a%2520Self-learning%2520Latent%2520Representation%2520%2528SLR%2529%2520method%252C%2520which%2520achieves%250Ahigh-performance%2520control%2520policy%2520learning%2520without%2520the%2520need%2520for%2520privileged%250Ainformation.%2520To%2520enhance%2520the%2520credibility%2520of%2520our%2520proposed%2520method%2527s%2520evaluation%252C%250ASLR%2520is%2520compared%2520with%2520open-source%2520code%2520repositories%2520of%2520state-of-the-art%250Aalgorithms%252C%2520retaining%2520the%2520original%2520authors%2527%2520configuration%2520parameters.%2520Across%250Afour%2520repositories%252C%2520SLR%2520consistently%2520outperforms%2520the%2520reference%2520results.%250AUltimately%252C%2520the%2520trained%2520policy%2520and%2520encoder%2520empower%2520the%2520quadruped%2520robot%2520to%250Anavigate%2520steps%252C%2520climb%2520stairs%252C%2520ascend%2520rocks%252C%2520and%2520traverse%2520various%2520challenging%250Aterrains.%2520Robot%2520experiment%2520videos%2520are%2520at%2520https%253A//11chens.github.io/SLR/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLR%3A%20Learning%20Quadruped%20Locomotion%20without%20Privileged%20Information&entry.906535625=Shiyi%20Chen%20and%20Zeyu%20Wan%20and%20Shiyang%20Yan%20and%20Chun%20Zhang%20and%20Weiyi%20Zhang%20and%20Qiang%20Li%20and%20Debing%20Zhang%20and%20Fasih%20Ud%20Din%20Farrukh&entry.1292438233=%20%20Traditional%20reinforcement%20learning%20control%20for%20quadruped%20robots%20often%20relies%0Aon%20privileged%20information%2C%20demanding%20meticulous%20selection%20and%20precise%0Aestimation%2C%20thereby%20imposing%20constraints%20on%20the%20development%20process.%20This%20work%0Aproposes%20a%20Self-learning%20Latent%20Representation%20%28SLR%29%20method%2C%20which%20achieves%0Ahigh-performance%20control%20policy%20learning%20without%20the%20need%20for%20privileged%0Ainformation.%20To%20enhance%20the%20credibility%20of%20our%20proposed%20method%27s%20evaluation%2C%0ASLR%20is%20compared%20with%20open-source%20code%20repositories%20of%20state-of-the-art%0Aalgorithms%2C%20retaining%20the%20original%20authors%27%20configuration%20parameters.%20Across%0Afour%20repositories%2C%20SLR%20consistently%20outperforms%20the%20reference%20results.%0AUltimately%2C%20the%20trained%20policy%20and%20encoder%20empower%20the%20quadruped%20robot%20to%0Anavigate%20steps%2C%20climb%20stairs%2C%20ascend%20rocks%2C%20and%20traverse%20various%20challenging%0Aterrains.%20Robot%20experiment%20videos%20are%20at%20https%3A//11chens.github.io/SLR/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04835v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


