<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250825.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene\n  Representations", "author": "Fadi Khatib and Dror Moran and Guy Trostianetsky and Yoni Kasten and Meirav Galun and Ronen Basri", "abstract": "  We introduce GSVisLoc, a visual localization method designed for 3D Gaussian\nSplatting (3DGS) scene representations. Given a 3DGS model of a scene and a\nquery image, our goal is to estimate the camera's position and orientation. We\naccomplish this by robustly matching scene features to image features. Scene\nfeatures are produced by downsampling and encoding the 3D Gaussians while image\nfeatures are obtained by encoding image patches. Our algorithm proceeds in\nthree steps, starting with coarse matching, then fine matching, and finally by\napplying pose refinement for an accurate final estimate. Importantly, our\nmethod leverages the explicit 3DGS scene representation for visual localization\nwithout requiring modifications, retraining, or additional reference images. We\nevaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive\nlocalization performance on standard benchmarks while outperforming existing\n3DGS-based baselines. Moreover, our approach generalizes effectively to novel\nscenes without additional training.\n", "link": "http://arxiv.org/abs/2508.18242v1", "date": "2025-08-25", "relevancy": 3.3003, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6923}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6913}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSVisLoc%3A%20Generalizable%20Visual%20Localization%20for%20Gaussian%20Splatting%20Scene%0A%20%20Representations&body=Title%3A%20GSVisLoc%3A%20Generalizable%20Visual%20Localization%20for%20Gaussian%20Splatting%20Scene%0A%20%20Representations%0AAuthor%3A%20Fadi%20Khatib%20and%20Dror%20Moran%20and%20Guy%20Trostianetsky%20and%20Yoni%20Kasten%20and%20Meirav%20Galun%20and%20Ronen%20Basri%0AAbstract%3A%20%20%20We%20introduce%20GSVisLoc%2C%20a%20visual%20localization%20method%20designed%20for%203D%20Gaussian%0ASplatting%20%283DGS%29%20scene%20representations.%20Given%20a%203DGS%20model%20of%20a%20scene%20and%20a%0Aquery%20image%2C%20our%20goal%20is%20to%20estimate%20the%20camera%27s%20position%20and%20orientation.%20We%0Aaccomplish%20this%20by%20robustly%20matching%20scene%20features%20to%20image%20features.%20Scene%0Afeatures%20are%20produced%20by%20downsampling%20and%20encoding%20the%203D%20Gaussians%20while%20image%0Afeatures%20are%20obtained%20by%20encoding%20image%20patches.%20Our%20algorithm%20proceeds%20in%0Athree%20steps%2C%20starting%20with%20coarse%20matching%2C%20then%20fine%20matching%2C%20and%20finally%20by%0Aapplying%20pose%20refinement%20for%20an%20accurate%20final%20estimate.%20Importantly%2C%20our%0Amethod%20leverages%20the%20explicit%203DGS%20scene%20representation%20for%20visual%20localization%0Awithout%20requiring%20modifications%2C%20retraining%2C%20or%20additional%20reference%20images.%20We%0Aevaluate%20GSVisLoc%20on%20both%20indoor%20and%20outdoor%20scenes%2C%20demonstrating%20competitive%0Alocalization%20performance%20on%20standard%20benchmarks%20while%20outperforming%20existing%0A3DGS-based%20baselines.%20Moreover%2C%20our%20approach%20generalizes%20effectively%20to%20novel%0Ascenes%20without%20additional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSVisLoc%253A%2520Generalizable%2520Visual%2520Localization%2520for%2520Gaussian%2520Splatting%2520Scene%250A%2520%2520Representations%26entry.906535625%3DFadi%2520Khatib%2520and%2520Dror%2520Moran%2520and%2520Guy%2520Trostianetsky%2520and%2520Yoni%2520Kasten%2520and%2520Meirav%2520Galun%2520and%2520Ronen%2520Basri%26entry.1292438233%3D%2520%2520We%2520introduce%2520GSVisLoc%252C%2520a%2520visual%2520localization%2520method%2520designed%2520for%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520scene%2520representations.%2520Given%2520a%25203DGS%2520model%2520of%2520a%2520scene%2520and%2520a%250Aquery%2520image%252C%2520our%2520goal%2520is%2520to%2520estimate%2520the%2520camera%2527s%2520position%2520and%2520orientation.%2520We%250Aaccomplish%2520this%2520by%2520robustly%2520matching%2520scene%2520features%2520to%2520image%2520features.%2520Scene%250Afeatures%2520are%2520produced%2520by%2520downsampling%2520and%2520encoding%2520the%25203D%2520Gaussians%2520while%2520image%250Afeatures%2520are%2520obtained%2520by%2520encoding%2520image%2520patches.%2520Our%2520algorithm%2520proceeds%2520in%250Athree%2520steps%252C%2520starting%2520with%2520coarse%2520matching%252C%2520then%2520fine%2520matching%252C%2520and%2520finally%2520by%250Aapplying%2520pose%2520refinement%2520for%2520an%2520accurate%2520final%2520estimate.%2520Importantly%252C%2520our%250Amethod%2520leverages%2520the%2520explicit%25203DGS%2520scene%2520representation%2520for%2520visual%2520localization%250Awithout%2520requiring%2520modifications%252C%2520retraining%252C%2520or%2520additional%2520reference%2520images.%2520We%250Aevaluate%2520GSVisLoc%2520on%2520both%2520indoor%2520and%2520outdoor%2520scenes%252C%2520demonstrating%2520competitive%250Alocalization%2520performance%2520on%2520standard%2520benchmarks%2520while%2520outperforming%2520existing%250A3DGS-based%2520baselines.%2520Moreover%252C%2520our%2520approach%2520generalizes%2520effectively%2520to%2520novel%250Ascenes%2520without%2520additional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSVisLoc%3A%20Generalizable%20Visual%20Localization%20for%20Gaussian%20Splatting%20Scene%0A%20%20Representations&entry.906535625=Fadi%20Khatib%20and%20Dror%20Moran%20and%20Guy%20Trostianetsky%20and%20Yoni%20Kasten%20and%20Meirav%20Galun%20and%20Ronen%20Basri&entry.1292438233=%20%20We%20introduce%20GSVisLoc%2C%20a%20visual%20localization%20method%20designed%20for%203D%20Gaussian%0ASplatting%20%283DGS%29%20scene%20representations.%20Given%20a%203DGS%20model%20of%20a%20scene%20and%20a%0Aquery%20image%2C%20our%20goal%20is%20to%20estimate%20the%20camera%27s%20position%20and%20orientation.%20We%0Aaccomplish%20this%20by%20robustly%20matching%20scene%20features%20to%20image%20features.%20Scene%0Afeatures%20are%20produced%20by%20downsampling%20and%20encoding%20the%203D%20Gaussians%20while%20image%0Afeatures%20are%20obtained%20by%20encoding%20image%20patches.%20Our%20algorithm%20proceeds%20in%0Athree%20steps%2C%20starting%20with%20coarse%20matching%2C%20then%20fine%20matching%2C%20and%20finally%20by%0Aapplying%20pose%20refinement%20for%20an%20accurate%20final%20estimate.%20Importantly%2C%20our%0Amethod%20leverages%20the%20explicit%203DGS%20scene%20representation%20for%20visual%20localization%0Awithout%20requiring%20modifications%2C%20retraining%2C%20or%20additional%20reference%20images.%20We%0Aevaluate%20GSVisLoc%20on%20both%20indoor%20and%20outdoor%20scenes%2C%20demonstrating%20competitive%0Alocalization%20performance%20on%20standard%20benchmarks%20while%20outperforming%20existing%0A3DGS-based%20baselines.%20Moreover%2C%20our%20approach%20generalizes%20effectively%20to%20novel%0Ascenes%20without%20additional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18242v1&entry.124074799=Read"},
{"title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis", "author": "Radek Dan\u011b\u010dek and Carolin Schmitt and Senya Polikovsky and Michael J. Black", "abstract": "  In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/\n", "link": "http://arxiv.org/abs/2504.13386v3", "date": "2025-08-25", "relevancy": 3.1895, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6398}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6369}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis&body=Title%3A%20Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis%0AAuthor%3A%20Radek%20Dan%C4%9B%C4%8Dek%20and%20Carolin%20Schmitt%20and%20Senya%20Polikovsky%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20In%20order%20to%20be%20widely%20applicable%2C%20speech-driven%203D%20head%20avatars%20must%0Aarticulate%20their%20lips%20in%20accordance%20with%20speech%2C%20while%20also%20conveying%20the%0Aappropriate%20emotions%20with%20dynamically%20changing%20facial%20expressions.%20The%20key%0Aproblem%20is%20that%20deterministic%20models%20produce%20high-quality%20lip-sync%20but%20without%0Arich%20expressions%2C%20whereas%20stochastic%20models%20generate%20diverse%20expressions%20but%0Awith%20lower%20lip-sync%20quality.%20To%20get%20the%20best%20of%20both%2C%20we%20seek%20a%20stochastic%0Amodel%20with%20accurate%20lip-sync.%20To%20that%20end%2C%20we%20develop%20a%20new%20approach%20based%20on%0Athe%20following%20observation%3A%20if%20a%20method%20generates%20realistic%203D%20lip%20motions%2C%20it%0Ashould%20be%20possible%20to%20infer%20the%20spoken%20audio%20from%20the%20lip%20motion.%20The%20inferred%0Aspeech%20should%20match%20the%20original%20input%20audio%2C%20and%20erroneous%20predictions%20create%0Aa%20novel%20supervision%20signal%20for%20training%203D%20talking%20head%20avatars%20with%20accurate%0Alip-sync.%20To%20demonstrate%20this%20effect%2C%20we%20propose%20THUNDER%20%28Talking%20Heads%20Under%0ANeural%20Differentiable%20Elocution%20Reconstruction%29%2C%20a%203D%20talking%20head%20avatar%0Aframework%20that%20introduces%20a%20novel%20supervision%20mechanism%20via%20differentiable%0Asound%20production.%20First%2C%20we%20train%20a%20novel%20mesh-to-speech%20model%20that%20regresses%0Aaudio%20from%20facial%20animation.%20Then%2C%20we%20incorporate%20this%20model%20into%20a%0Adiffusion-based%20talking%20avatar%20framework.%20During%20training%2C%20the%20mesh-to-speech%0Amodel%20takes%20the%20generated%20animation%20and%20produces%20a%20sound%20that%20is%20compared%20to%0Athe%20input%20speech%2C%20creating%20a%20differentiable%20analysis-by-audio-synthesis%0Asupervision%20loop.%20Our%20extensive%20qualitative%20and%20quantitative%20experiments%0Ademonstrate%20that%20THUNDER%20significantly%20improves%20the%20quality%20of%20the%20lip-sync%20of%0Atalking%20head%20avatars%20while%20still%20allowing%20for%20generation%20of%20diverse%2C%0Ahigh-quality%2C%20expressive%20facial%20animations.%20The%20code%20and%20models%20will%20be%0Aavailable%20at%20https%3A//thunder.is.tue.mpg.de/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13386v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervising%25203D%2520Talking%2520Head%2520Avatars%2520with%2520Analysis-by-Audio-Synthesis%26entry.906535625%3DRadek%2520Dan%25C4%259B%25C4%258Dek%2520and%2520Carolin%2520Schmitt%2520and%2520Senya%2520Polikovsky%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520be%2520widely%2520applicable%252C%2520speech-driven%25203D%2520head%2520avatars%2520must%250Aarticulate%2520their%2520lips%2520in%2520accordance%2520with%2520speech%252C%2520while%2520also%2520conveying%2520the%250Aappropriate%2520emotions%2520with%2520dynamically%2520changing%2520facial%2520expressions.%2520The%2520key%250Aproblem%2520is%2520that%2520deterministic%2520models%2520produce%2520high-quality%2520lip-sync%2520but%2520without%250Arich%2520expressions%252C%2520whereas%2520stochastic%2520models%2520generate%2520diverse%2520expressions%2520but%250Awith%2520lower%2520lip-sync%2520quality.%2520To%2520get%2520the%2520best%2520of%2520both%252C%2520we%2520seek%2520a%2520stochastic%250Amodel%2520with%2520accurate%2520lip-sync.%2520To%2520that%2520end%252C%2520we%2520develop%2520a%2520new%2520approach%2520based%2520on%250Athe%2520following%2520observation%253A%2520if%2520a%2520method%2520generates%2520realistic%25203D%2520lip%2520motions%252C%2520it%250Ashould%2520be%2520possible%2520to%2520infer%2520the%2520spoken%2520audio%2520from%2520the%2520lip%2520motion.%2520The%2520inferred%250Aspeech%2520should%2520match%2520the%2520original%2520input%2520audio%252C%2520and%2520erroneous%2520predictions%2520create%250Aa%2520novel%2520supervision%2520signal%2520for%2520training%25203D%2520talking%2520head%2520avatars%2520with%2520accurate%250Alip-sync.%2520To%2520demonstrate%2520this%2520effect%252C%2520we%2520propose%2520THUNDER%2520%2528Talking%2520Heads%2520Under%250ANeural%2520Differentiable%2520Elocution%2520Reconstruction%2529%252C%2520a%25203D%2520talking%2520head%2520avatar%250Aframework%2520that%2520introduces%2520a%2520novel%2520supervision%2520mechanism%2520via%2520differentiable%250Asound%2520production.%2520First%252C%2520we%2520train%2520a%2520novel%2520mesh-to-speech%2520model%2520that%2520regresses%250Aaudio%2520from%2520facial%2520animation.%2520Then%252C%2520we%2520incorporate%2520this%2520model%2520into%2520a%250Adiffusion-based%2520talking%2520avatar%2520framework.%2520During%2520training%252C%2520the%2520mesh-to-speech%250Amodel%2520takes%2520the%2520generated%2520animation%2520and%2520produces%2520a%2520sound%2520that%2520is%2520compared%2520to%250Athe%2520input%2520speech%252C%2520creating%2520a%2520differentiable%2520analysis-by-audio-synthesis%250Asupervision%2520loop.%2520Our%2520extensive%2520qualitative%2520and%2520quantitative%2520experiments%250Ademonstrate%2520that%2520THUNDER%2520significantly%2520improves%2520the%2520quality%2520of%2520the%2520lip-sync%2520of%250Atalking%2520head%2520avatars%2520while%2520still%2520allowing%2520for%2520generation%2520of%2520diverse%252C%250Ahigh-quality%252C%2520expressive%2520facial%2520animations.%2520The%2520code%2520and%2520models%2520will%2520be%250Aavailable%2520at%2520https%253A//thunder.is.tue.mpg.de/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13386v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis&entry.906535625=Radek%20Dan%C4%9B%C4%8Dek%20and%20Carolin%20Schmitt%20and%20Senya%20Polikovsky%20and%20Michael%20J.%20Black&entry.1292438233=%20%20In%20order%20to%20be%20widely%20applicable%2C%20speech-driven%203D%20head%20avatars%20must%0Aarticulate%20their%20lips%20in%20accordance%20with%20speech%2C%20while%20also%20conveying%20the%0Aappropriate%20emotions%20with%20dynamically%20changing%20facial%20expressions.%20The%20key%0Aproblem%20is%20that%20deterministic%20models%20produce%20high-quality%20lip-sync%20but%20without%0Arich%20expressions%2C%20whereas%20stochastic%20models%20generate%20diverse%20expressions%20but%0Awith%20lower%20lip-sync%20quality.%20To%20get%20the%20best%20of%20both%2C%20we%20seek%20a%20stochastic%0Amodel%20with%20accurate%20lip-sync.%20To%20that%20end%2C%20we%20develop%20a%20new%20approach%20based%20on%0Athe%20following%20observation%3A%20if%20a%20method%20generates%20realistic%203D%20lip%20motions%2C%20it%0Ashould%20be%20possible%20to%20infer%20the%20spoken%20audio%20from%20the%20lip%20motion.%20The%20inferred%0Aspeech%20should%20match%20the%20original%20input%20audio%2C%20and%20erroneous%20predictions%20create%0Aa%20novel%20supervision%20signal%20for%20training%203D%20talking%20head%20avatars%20with%20accurate%0Alip-sync.%20To%20demonstrate%20this%20effect%2C%20we%20propose%20THUNDER%20%28Talking%20Heads%20Under%0ANeural%20Differentiable%20Elocution%20Reconstruction%29%2C%20a%203D%20talking%20head%20avatar%0Aframework%20that%20introduces%20a%20novel%20supervision%20mechanism%20via%20differentiable%0Asound%20production.%20First%2C%20we%20train%20a%20novel%20mesh-to-speech%20model%20that%20regresses%0Aaudio%20from%20facial%20animation.%20Then%2C%20we%20incorporate%20this%20model%20into%20a%0Adiffusion-based%20talking%20avatar%20framework.%20During%20training%2C%20the%20mesh-to-speech%0Amodel%20takes%20the%20generated%20animation%20and%20produces%20a%20sound%20that%20is%20compared%20to%0Athe%20input%20speech%2C%20creating%20a%20differentiable%20analysis-by-audio-synthesis%0Asupervision%20loop.%20Our%20extensive%20qualitative%20and%20quantitative%20experiments%0Ademonstrate%20that%20THUNDER%20significantly%20improves%20the%20quality%20of%20the%20lip-sync%20of%0Atalking%20head%20avatars%20while%20still%20allowing%20for%20generation%20of%20diverse%2C%0Ahigh-quality%2C%20expressive%20facial%20animations.%20The%20code%20and%20models%20will%20be%0Aavailable%20at%20https%3A//thunder.is.tue.mpg.de/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13386v3&entry.124074799=Read"},
{"title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models", "author": "Haitang Feng and Jie Liu and Jie Tang and Gangshan Wu and Beiqi Chen and Jianhuang Lai and Guangcong Wang", "abstract": "  3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .\n", "link": "http://arxiv.org/abs/2508.18271v1", "date": "2025-08-25", "relevancy": 3.1387, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6379}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6379}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjFiller-3D%3A%20Consistent%20Multi-view%203D%20Inpainting%20via%20Video%20Diffusion%0A%20%20Models&body=Title%3A%20ObjFiller-3D%3A%20Consistent%20Multi-view%203D%20Inpainting%20via%20Video%20Diffusion%0A%20%20Models%0AAuthor%3A%20Haitang%20Feng%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Beiqi%20Chen%20and%20Jianhuang%20Lai%20and%20Guangcong%20Wang%0AAbstract%3A%20%20%203D%20inpainting%20often%20relies%20on%20multi-view%202D%20image%20inpainting%2C%20where%20the%0Ainherent%20inconsistencies%20across%20different%20inpainted%20views%20can%20result%20in%20blurred%0Atextures%2C%20spatial%20discontinuities%2C%20and%20distracting%20visual%20artifacts.%20These%0Ainconsistencies%20pose%20significant%20challenges%20when%20striving%20for%20accurate%20and%0Arealistic%203D%20object%20completion%2C%20particularly%20in%20applications%20that%20demand%20high%0Afidelity%20and%20structural%20coherence.%20To%20overcome%20these%20limitations%2C%20we%20propose%0AObjFiller-3D%2C%20a%20novel%20method%20designed%20for%20the%20completion%20and%20editing%20of%0Ahigh-quality%20and%20consistent%203D%20objects.%20Instead%20of%20employing%20a%20conventional%202D%0Aimage%20inpainting%20model%2C%20our%20approach%20leverages%20a%20curated%20selection%20of%0Astate-of-the-art%20video%20editing%20model%20to%20fill%20in%20the%20masked%20regions%20of%203D%0Aobjects.%20We%20analyze%20the%20representation%20gap%20between%203D%20and%20videos%2C%20and%20propose%0Aan%20adaptation%20of%20a%20video%20inpainting%20model%20for%203D%20scene%20inpainting.%20In%20addition%2C%0Awe%20introduce%20a%20reference-based%203D%20inpainting%20method%20to%20further%20enhance%20the%0Aquality%20of%20reconstruction.%20Experiments%20across%20diverse%20datasets%20show%20that%0Acompared%20to%20previous%20methods%2C%20ObjFiller-3D%20produces%20more%20faithful%20and%0Afine-grained%20reconstructions%20%28PSNR%20of%2026.6%20vs.%20NeRFiller%20%2815.9%29%20and%20LPIPS%20of%0A0.19%20vs.%20Instant3dit%20%280.25%29%29.%20Moreover%2C%20it%20demonstrates%20strong%20potential%20for%0Apractical%20deployment%20in%20real-world%203D%20editing%20applications.%20Project%20page%3A%0Ahttps%3A//objfiller3d.github.io/%20Code%3A%0Ahttps%3A//github.com/objfiller3d/ObjFiller-3D%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjFiller-3D%253A%2520Consistent%2520Multi-view%25203D%2520Inpainting%2520via%2520Video%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DHaitang%2520Feng%2520and%2520Jie%2520Liu%2520and%2520Jie%2520Tang%2520and%2520Gangshan%2520Wu%2520and%2520Beiqi%2520Chen%2520and%2520Jianhuang%2520Lai%2520and%2520Guangcong%2520Wang%26entry.1292438233%3D%2520%25203D%2520inpainting%2520often%2520relies%2520on%2520multi-view%25202D%2520image%2520inpainting%252C%2520where%2520the%250Ainherent%2520inconsistencies%2520across%2520different%2520inpainted%2520views%2520can%2520result%2520in%2520blurred%250Atextures%252C%2520spatial%2520discontinuities%252C%2520and%2520distracting%2520visual%2520artifacts.%2520These%250Ainconsistencies%2520pose%2520significant%2520challenges%2520when%2520striving%2520for%2520accurate%2520and%250Arealistic%25203D%2520object%2520completion%252C%2520particularly%2520in%2520applications%2520that%2520demand%2520high%250Afidelity%2520and%2520structural%2520coherence.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%250AObjFiller-3D%252C%2520a%2520novel%2520method%2520designed%2520for%2520the%2520completion%2520and%2520editing%2520of%250Ahigh-quality%2520and%2520consistent%25203D%2520objects.%2520Instead%2520of%2520employing%2520a%2520conventional%25202D%250Aimage%2520inpainting%2520model%252C%2520our%2520approach%2520leverages%2520a%2520curated%2520selection%2520of%250Astate-of-the-art%2520video%2520editing%2520model%2520to%2520fill%2520in%2520the%2520masked%2520regions%2520of%25203D%250Aobjects.%2520We%2520analyze%2520the%2520representation%2520gap%2520between%25203D%2520and%2520videos%252C%2520and%2520propose%250Aan%2520adaptation%2520of%2520a%2520video%2520inpainting%2520model%2520for%25203D%2520scene%2520inpainting.%2520In%2520addition%252C%250Awe%2520introduce%2520a%2520reference-based%25203D%2520inpainting%2520method%2520to%2520further%2520enhance%2520the%250Aquality%2520of%2520reconstruction.%2520Experiments%2520across%2520diverse%2520datasets%2520show%2520that%250Acompared%2520to%2520previous%2520methods%252C%2520ObjFiller-3D%2520produces%2520more%2520faithful%2520and%250Afine-grained%2520reconstructions%2520%2528PSNR%2520of%252026.6%2520vs.%2520NeRFiller%2520%252815.9%2529%2520and%2520LPIPS%2520of%250A0.19%2520vs.%2520Instant3dit%2520%25280.25%2529%2529.%2520Moreover%252C%2520it%2520demonstrates%2520strong%2520potential%2520for%250Apractical%2520deployment%2520in%2520real-world%25203D%2520editing%2520applications.%2520Project%2520page%253A%250Ahttps%253A//objfiller3d.github.io/%2520Code%253A%250Ahttps%253A//github.com/objfiller3d/ObjFiller-3D%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjFiller-3D%3A%20Consistent%20Multi-view%203D%20Inpainting%20via%20Video%20Diffusion%0A%20%20Models&entry.906535625=Haitang%20Feng%20and%20Jie%20Liu%20and%20Jie%20Tang%20and%20Gangshan%20Wu%20and%20Beiqi%20Chen%20and%20Jianhuang%20Lai%20and%20Guangcong%20Wang&entry.1292438233=%20%203D%20inpainting%20often%20relies%20on%20multi-view%202D%20image%20inpainting%2C%20where%20the%0Ainherent%20inconsistencies%20across%20different%20inpainted%20views%20can%20result%20in%20blurred%0Atextures%2C%20spatial%20discontinuities%2C%20and%20distracting%20visual%20artifacts.%20These%0Ainconsistencies%20pose%20significant%20challenges%20when%20striving%20for%20accurate%20and%0Arealistic%203D%20object%20completion%2C%20particularly%20in%20applications%20that%20demand%20high%0Afidelity%20and%20structural%20coherence.%20To%20overcome%20these%20limitations%2C%20we%20propose%0AObjFiller-3D%2C%20a%20novel%20method%20designed%20for%20the%20completion%20and%20editing%20of%0Ahigh-quality%20and%20consistent%203D%20objects.%20Instead%20of%20employing%20a%20conventional%202D%0Aimage%20inpainting%20model%2C%20our%20approach%20leverages%20a%20curated%20selection%20of%0Astate-of-the-art%20video%20editing%20model%20to%20fill%20in%20the%20masked%20regions%20of%203D%0Aobjects.%20We%20analyze%20the%20representation%20gap%20between%203D%20and%20videos%2C%20and%20propose%0Aan%20adaptation%20of%20a%20video%20inpainting%20model%20for%203D%20scene%20inpainting.%20In%20addition%2C%0Awe%20introduce%20a%20reference-based%203D%20inpainting%20method%20to%20further%20enhance%20the%0Aquality%20of%20reconstruction.%20Experiments%20across%20diverse%20datasets%20show%20that%0Acompared%20to%20previous%20methods%2C%20ObjFiller-3D%20produces%20more%20faithful%20and%0Afine-grained%20reconstructions%20%28PSNR%20of%2026.6%20vs.%20NeRFiller%20%2815.9%29%20and%20LPIPS%20of%0A0.19%20vs.%20Instant3dit%20%280.25%29%29.%20Moreover%2C%20it%20demonstrates%20strong%20potential%20for%0Apractical%20deployment%20in%20real-world%203D%20editing%20applications.%20Project%20page%3A%0Ahttps%3A//objfiller3d.github.io/%20Code%3A%0Ahttps%3A//github.com/objfiller3d/ObjFiller-3D%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18271v1&entry.124074799=Read"},
{"title": "Beyond Imaging: Vision Transformer Digital Twin Surrogates for 3D+T\n  Biological Tissue Dynamics", "author": "Kaan Berke Ugurlar and Joaqu\u00edn de Navascu\u00e9s and Michael Taynnan Barros", "abstract": "  Understanding the dynamic organization and homeostasis of living tissues\nrequires high-resolution, time-resolved imaging coupled with methods capable of\nextracting interpretable, predictive insights from complex datasets. Here, we\npresent the Vision Transformer Digital Twin Surrogate Network (VT-DTSN), a deep\nlearning framework for predictive modeling of 3D+T imaging data from biological\ntissue. By leveraging Vision Transformers pretrained with DINO\n(Self-Distillation with NO Labels) and employing a multi-view fusion strategy,\nVT-DTSN learns to reconstruct high-fidelity, time-resolved dynamics of a\nDrosophila midgut while preserving morphological and feature-level integrity\nacross imaging depths. The model is trained with a composite loss prioritizing\npixel-level accuracy, perceptual structure, and feature-space alignment,\nensuring biologically meaningful outputs suitable for in silico experimentation\nand hypothesis testing. Evaluation across layers and biological replicates\ndemonstrates VT-DTSN's robustness and consistency, achieving low error rates\nand high structural similarity while maintaining efficient inference through\nmodel optimization. This work establishes VT-DTSN as a feasible, high-fidelity\nsurrogate for cross-timepoint reconstruction and for studying tissue dynamics,\nenabling computational exploration of cellular behaviors and homeostasis to\ncomplement time-resolved imaging studies in biological research.\n", "link": "http://arxiv.org/abs/2508.15883v2", "date": "2025-08-25", "relevancy": 2.988, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6076}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Imaging%3A%20Vision%20Transformer%20Digital%20Twin%20Surrogates%20for%203D%2BT%0A%20%20Biological%20Tissue%20Dynamics&body=Title%3A%20Beyond%20Imaging%3A%20Vision%20Transformer%20Digital%20Twin%20Surrogates%20for%203D%2BT%0A%20%20Biological%20Tissue%20Dynamics%0AAuthor%3A%20Kaan%20Berke%20Ugurlar%20and%20Joaqu%C3%ADn%20de%20Navascu%C3%A9s%20and%20Michael%20Taynnan%20Barros%0AAbstract%3A%20%20%20Understanding%20the%20dynamic%20organization%20and%20homeostasis%20of%20living%20tissues%0Arequires%20high-resolution%2C%20time-resolved%20imaging%20coupled%20with%20methods%20capable%20of%0Aextracting%20interpretable%2C%20predictive%20insights%20from%20complex%20datasets.%20Here%2C%20we%0Apresent%20the%20Vision%20Transformer%20Digital%20Twin%20Surrogate%20Network%20%28VT-DTSN%29%2C%20a%20deep%0Alearning%20framework%20for%20predictive%20modeling%20of%203D%2BT%20imaging%20data%20from%20biological%0Atissue.%20By%20leveraging%20Vision%20Transformers%20pretrained%20with%20DINO%0A%28Self-Distillation%20with%20NO%20Labels%29%20and%20employing%20a%20multi-view%20fusion%20strategy%2C%0AVT-DTSN%20learns%20to%20reconstruct%20high-fidelity%2C%20time-resolved%20dynamics%20of%20a%0ADrosophila%20midgut%20while%20preserving%20morphological%20and%20feature-level%20integrity%0Aacross%20imaging%20depths.%20The%20model%20is%20trained%20with%20a%20composite%20loss%20prioritizing%0Apixel-level%20accuracy%2C%20perceptual%20structure%2C%20and%20feature-space%20alignment%2C%0Aensuring%20biologically%20meaningful%20outputs%20suitable%20for%20in%20silico%20experimentation%0Aand%20hypothesis%20testing.%20Evaluation%20across%20layers%20and%20biological%20replicates%0Ademonstrates%20VT-DTSN%27s%20robustness%20and%20consistency%2C%20achieving%20low%20error%20rates%0Aand%20high%20structural%20similarity%20while%20maintaining%20efficient%20inference%20through%0Amodel%20optimization.%20This%20work%20establishes%20VT-DTSN%20as%20a%20feasible%2C%20high-fidelity%0Asurrogate%20for%20cross-timepoint%20reconstruction%20and%20for%20studying%20tissue%20dynamics%2C%0Aenabling%20computational%20exploration%20of%20cellular%20behaviors%20and%20homeostasis%20to%0Acomplement%20time-resolved%20imaging%20studies%20in%20biological%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Imaging%253A%2520Vision%2520Transformer%2520Digital%2520Twin%2520Surrogates%2520for%25203D%252BT%250A%2520%2520Biological%2520Tissue%2520Dynamics%26entry.906535625%3DKaan%2520Berke%2520Ugurlar%2520and%2520Joaqu%25C3%25ADn%2520de%2520Navascu%25C3%25A9s%2520and%2520Michael%2520Taynnan%2520Barros%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamic%2520organization%2520and%2520homeostasis%2520of%2520living%2520tissues%250Arequires%2520high-resolution%252C%2520time-resolved%2520imaging%2520coupled%2520with%2520methods%2520capable%2520of%250Aextracting%2520interpretable%252C%2520predictive%2520insights%2520from%2520complex%2520datasets.%2520Here%252C%2520we%250Apresent%2520the%2520Vision%2520Transformer%2520Digital%2520Twin%2520Surrogate%2520Network%2520%2528VT-DTSN%2529%252C%2520a%2520deep%250Alearning%2520framework%2520for%2520predictive%2520modeling%2520of%25203D%252BT%2520imaging%2520data%2520from%2520biological%250Atissue.%2520By%2520leveraging%2520Vision%2520Transformers%2520pretrained%2520with%2520DINO%250A%2528Self-Distillation%2520with%2520NO%2520Labels%2529%2520and%2520employing%2520a%2520multi-view%2520fusion%2520strategy%252C%250AVT-DTSN%2520learns%2520to%2520reconstruct%2520high-fidelity%252C%2520time-resolved%2520dynamics%2520of%2520a%250ADrosophila%2520midgut%2520while%2520preserving%2520morphological%2520and%2520feature-level%2520integrity%250Aacross%2520imaging%2520depths.%2520The%2520model%2520is%2520trained%2520with%2520a%2520composite%2520loss%2520prioritizing%250Apixel-level%2520accuracy%252C%2520perceptual%2520structure%252C%2520and%2520feature-space%2520alignment%252C%250Aensuring%2520biologically%2520meaningful%2520outputs%2520suitable%2520for%2520in%2520silico%2520experimentation%250Aand%2520hypothesis%2520testing.%2520Evaluation%2520across%2520layers%2520and%2520biological%2520replicates%250Ademonstrates%2520VT-DTSN%2527s%2520robustness%2520and%2520consistency%252C%2520achieving%2520low%2520error%2520rates%250Aand%2520high%2520structural%2520similarity%2520while%2520maintaining%2520efficient%2520inference%2520through%250Amodel%2520optimization.%2520This%2520work%2520establishes%2520VT-DTSN%2520as%2520a%2520feasible%252C%2520high-fidelity%250Asurrogate%2520for%2520cross-timepoint%2520reconstruction%2520and%2520for%2520studying%2520tissue%2520dynamics%252C%250Aenabling%2520computational%2520exploration%2520of%2520cellular%2520behaviors%2520and%2520homeostasis%2520to%250Acomplement%2520time-resolved%2520imaging%2520studies%2520in%2520biological%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Imaging%3A%20Vision%20Transformer%20Digital%20Twin%20Surrogates%20for%203D%2BT%0A%20%20Biological%20Tissue%20Dynamics&entry.906535625=Kaan%20Berke%20Ugurlar%20and%20Joaqu%C3%ADn%20de%20Navascu%C3%A9s%20and%20Michael%20Taynnan%20Barros&entry.1292438233=%20%20Understanding%20the%20dynamic%20organization%20and%20homeostasis%20of%20living%20tissues%0Arequires%20high-resolution%2C%20time-resolved%20imaging%20coupled%20with%20methods%20capable%20of%0Aextracting%20interpretable%2C%20predictive%20insights%20from%20complex%20datasets.%20Here%2C%20we%0Apresent%20the%20Vision%20Transformer%20Digital%20Twin%20Surrogate%20Network%20%28VT-DTSN%29%2C%20a%20deep%0Alearning%20framework%20for%20predictive%20modeling%20of%203D%2BT%20imaging%20data%20from%20biological%0Atissue.%20By%20leveraging%20Vision%20Transformers%20pretrained%20with%20DINO%0A%28Self-Distillation%20with%20NO%20Labels%29%20and%20employing%20a%20multi-view%20fusion%20strategy%2C%0AVT-DTSN%20learns%20to%20reconstruct%20high-fidelity%2C%20time-resolved%20dynamics%20of%20a%0ADrosophila%20midgut%20while%20preserving%20morphological%20and%20feature-level%20integrity%0Aacross%20imaging%20depths.%20The%20model%20is%20trained%20with%20a%20composite%20loss%20prioritizing%0Apixel-level%20accuracy%2C%20perceptual%20structure%2C%20and%20feature-space%20alignment%2C%0Aensuring%20biologically%20meaningful%20outputs%20suitable%20for%20in%20silico%20experimentation%0Aand%20hypothesis%20testing.%20Evaluation%20across%20layers%20and%20biological%20replicates%0Ademonstrates%20VT-DTSN%27s%20robustness%20and%20consistency%2C%20achieving%20low%20error%20rates%0Aand%20high%20structural%20similarity%20while%20maintaining%20efficient%20inference%20through%0Amodel%20optimization.%20This%20work%20establishes%20VT-DTSN%20as%20a%20feasible%2C%20high-fidelity%0Asurrogate%20for%20cross-timepoint%20reconstruction%20and%20for%20studying%20tissue%20dynamics%2C%0Aenabling%20computational%20exploration%20of%20cellular%20behaviors%20and%20homeostasis%20to%0Acomplement%20time-resolved%20imaging%20studies%20in%20biological%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15883v2&entry.124074799=Read"},
{"title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency", "author": "Weiyun Wang and Zhangwei Gao and Lixin Gu and Hengjun Pu and Long Cui and Xingguang Wei and Zhaoyang Liu and Linglin Jing and Shenglong Ye and Jie Shao and Zhaokai Wang and Zhe Chen and Hongjie Zhang and Ganlin Yang and Haomin Wang and Qi Wei and Jinhui Yin and Wenhao Li and Erfei Cui and Guanzhou Chen and Zichen Ding and Changyao Tian and Zhenyu Wu and Jingjing Xie and Zehao Li and Bowen Yang and Yuchen Duan and Xuehui Wang and Songze Li and Xiangyu Zhao and Haodong Duan and Nianchen Deng and Bin Fu and Yinan He and Yi Wang and Conghui He and Botian Shi and Junjun He and Yingtong Xiong and Han Lv and Lijun Wu and Wenqi Shao and Kaipeng Zhang and Huipeng Deng and Biqing Qi and Jiaye Ge and Qipeng Guo and Wenwei Zhang and Wanli Ouyang and Limin Wang and Min Dou and Xizhou Zhu and Tong Lu and Dahua Lin and Jifeng Dai and Bowen Zhou and Weijie Su and Kai Chen and Yu Qiao and Wenhai Wang and Gen Luo", "abstract": "  We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.\n", "link": "http://arxiv.org/abs/2508.18265v1", "date": "2025-08-25", "relevancy": 2.9862, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency&body=Title%3A%20InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency%0AAuthor%3A%20Weiyun%20Wang%20and%20Zhangwei%20Gao%20and%20Lixin%20Gu%20and%20Hengjun%20Pu%20and%20Long%20Cui%20and%20Xingguang%20Wei%20and%20Zhaoyang%20Liu%20and%20Linglin%20Jing%20and%20Shenglong%20Ye%20and%20Jie%20Shao%20and%20Zhaokai%20Wang%20and%20Zhe%20Chen%20and%20Hongjie%20Zhang%20and%20Ganlin%20Yang%20and%20Haomin%20Wang%20and%20Qi%20Wei%20and%20Jinhui%20Yin%20and%20Wenhao%20Li%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Zichen%20Ding%20and%20Changyao%20Tian%20and%20Zhenyu%20Wu%20and%20Jingjing%20Xie%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Yuchen%20Duan%20and%20Xuehui%20Wang%20and%20Songze%20Li%20and%20Xiangyu%20Zhao%20and%20Haodong%20Duan%20and%20Nianchen%20Deng%20and%20Bin%20Fu%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Junjun%20He%20and%20Yingtong%20Xiong%20and%20Han%20Lv%20and%20Lijun%20Wu%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang%20and%20Huipeng%20Deng%20and%20Biqing%20Qi%20and%20Jiaye%20Ge%20and%20Qipeng%20Guo%20and%20Wenwei%20Zhang%20and%20Wanli%20Ouyang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Jifeng%20Dai%20and%20Bowen%20Zhou%20and%20Weijie%20Su%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo%0AAbstract%3A%20%20%20We%20introduce%20InternVL%203.5%2C%20a%20new%20family%20of%20open-source%20multimodal%20models%20that%0Asignificantly%20advances%20versatility%2C%20reasoning%20capability%2C%20and%20inference%0Aefficiency%20along%20the%20InternVL%20series.%20A%20key%20innovation%20is%20the%20Cascade%0AReinforcement%20Learning%20%28Cascade%20RL%29%20framework%2C%20which%20enhances%20reasoning%20through%0Aa%20two-stage%20process%3A%20offline%20RL%20for%20stable%20convergence%20and%20online%20RL%20for%0Arefined%20alignment.%20This%20coarse-to-fine%20training%20strategy%20leads%20to%20substantial%0Aimprovements%20on%20downstream%20reasoning%20tasks%2C%20e.g.%2C%20MMMU%20and%20MathVista.%20To%0Aoptimize%20efficiency%2C%20we%20propose%20a%20Visual%20Resolution%20Router%20%28ViR%29%20that%0Adynamically%20adjusts%20the%20resolution%20of%20visual%20tokens%20without%20compromising%0Aperformance.%20Coupled%20with%20ViR%2C%20our%20Decoupled%20Vision-Language%20Deployment%20%28DvD%29%0Astrategy%20separates%20the%20vision%20encoder%20and%20language%20model%20across%20different%20GPUs%2C%0Aeffectively%20balancing%20computational%20load.%20These%20contributions%20collectively%0Aenable%20InternVL3.5%20to%20achieve%20up%20to%20a%20%2B16.0%5C%25%20gain%20in%20overall%20reasoning%0Aperformance%20and%20a%204.05%24%5Ctimes%24%20inference%20speedup%20compared%20to%20its%20predecessor%2C%0Ai.e.%2C%20InternVL3.%20In%20addition%2C%20InternVL3.5%20supports%20novel%20capabilities%20such%20as%0AGUI%20interaction%20and%20embodied%20agency.%20Notably%2C%20our%20largest%20model%2C%20i.e.%2C%0AInternVL3.5-241B-A28B%2C%20attains%20state-of-the-art%20results%20among%20open-source%20MLLMs%0Aacross%20general%20multimodal%2C%20reasoning%2C%20text%2C%20and%20agentic%20tasks%20--%20narrowing%20the%0Aperformance%20gap%20with%20leading%20commercial%20models%20like%20GPT-5.%20All%20models%20and%20code%0Aare%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVL3.5%253A%2520Advancing%2520Open-Source%2520Multimodal%2520Models%2520in%2520Versatility%252C%250A%2520%2520Reasoning%252C%2520and%2520Efficiency%26entry.906535625%3DWeiyun%2520Wang%2520and%2520Zhangwei%2520Gao%2520and%2520Lixin%2520Gu%2520and%2520Hengjun%2520Pu%2520and%2520Long%2520Cui%2520and%2520Xingguang%2520Wei%2520and%2520Zhaoyang%2520Liu%2520and%2520Linglin%2520Jing%2520and%2520Shenglong%2520Ye%2520and%2520Jie%2520Shao%2520and%2520Zhaokai%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Hongjie%2520Zhang%2520and%2520Ganlin%2520Yang%2520and%2520Haomin%2520Wang%2520and%2520Qi%2520Wei%2520and%2520Jinhui%2520Yin%2520and%2520Wenhao%2520Li%2520and%2520Erfei%2520Cui%2520and%2520Guanzhou%2520Chen%2520and%2520Zichen%2520Ding%2520and%2520Changyao%2520Tian%2520and%2520Zhenyu%2520Wu%2520and%2520Jingjing%2520Xie%2520and%2520Zehao%2520Li%2520and%2520Bowen%2520Yang%2520and%2520Yuchen%2520Duan%2520and%2520Xuehui%2520Wang%2520and%2520Songze%2520Li%2520and%2520Xiangyu%2520Zhao%2520and%2520Haodong%2520Duan%2520and%2520Nianchen%2520Deng%2520and%2520Bin%2520Fu%2520and%2520Yinan%2520He%2520and%2520Yi%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Junjun%2520He%2520and%2520Yingtong%2520Xiong%2520and%2520Han%2520Lv%2520and%2520Lijun%2520Wu%2520and%2520Wenqi%2520Shao%2520and%2520Kaipeng%2520Zhang%2520and%2520Huipeng%2520Deng%2520and%2520Biqing%2520Qi%2520and%2520Jiaye%2520Ge%2520and%2520Qipeng%2520Guo%2520and%2520Wenwei%2520Zhang%2520and%2520Wanli%2520Ouyang%2520and%2520Limin%2520Wang%2520and%2520Min%2520Dou%2520and%2520Xizhou%2520Zhu%2520and%2520Tong%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Jifeng%2520Dai%2520and%2520Bowen%2520Zhou%2520and%2520Weijie%2520Su%2520and%2520Kai%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVL%25203.5%252C%2520a%2520new%2520family%2520of%2520open-source%2520multimodal%2520models%2520that%250Asignificantly%2520advances%2520versatility%252C%2520reasoning%2520capability%252C%2520and%2520inference%250Aefficiency%2520along%2520the%2520InternVL%2520series.%2520A%2520key%2520innovation%2520is%2520the%2520Cascade%250AReinforcement%2520Learning%2520%2528Cascade%2520RL%2529%2520framework%252C%2520which%2520enhances%2520reasoning%2520through%250Aa%2520two-stage%2520process%253A%2520offline%2520RL%2520for%2520stable%2520convergence%2520and%2520online%2520RL%2520for%250Arefined%2520alignment.%2520This%2520coarse-to-fine%2520training%2520strategy%2520leads%2520to%2520substantial%250Aimprovements%2520on%2520downstream%2520reasoning%2520tasks%252C%2520e.g.%252C%2520MMMU%2520and%2520MathVista.%2520To%250Aoptimize%2520efficiency%252C%2520we%2520propose%2520a%2520Visual%2520Resolution%2520Router%2520%2528ViR%2529%2520that%250Adynamically%2520adjusts%2520the%2520resolution%2520of%2520visual%2520tokens%2520without%2520compromising%250Aperformance.%2520Coupled%2520with%2520ViR%252C%2520our%2520Decoupled%2520Vision-Language%2520Deployment%2520%2528DvD%2529%250Astrategy%2520separates%2520the%2520vision%2520encoder%2520and%2520language%2520model%2520across%2520different%2520GPUs%252C%250Aeffectively%2520balancing%2520computational%2520load.%2520These%2520contributions%2520collectively%250Aenable%2520InternVL3.5%2520to%2520achieve%2520up%2520to%2520a%2520%252B16.0%255C%2525%2520gain%2520in%2520overall%2520reasoning%250Aperformance%2520and%2520a%25204.05%2524%255Ctimes%2524%2520inference%2520speedup%2520compared%2520to%2520its%2520predecessor%252C%250Ai.e.%252C%2520InternVL3.%2520In%2520addition%252C%2520InternVL3.5%2520supports%2520novel%2520capabilities%2520such%2520as%250AGUI%2520interaction%2520and%2520embodied%2520agency.%2520Notably%252C%2520our%2520largest%2520model%252C%2520i.e.%252C%250AInternVL3.5-241B-A28B%252C%2520attains%2520state-of-the-art%2520results%2520among%2520open-source%2520MLLMs%250Aacross%2520general%2520multimodal%252C%2520reasoning%252C%2520text%252C%2520and%2520agentic%2520tasks%2520--%2520narrowing%2520the%250Aperformance%2520gap%2520with%2520leading%2520commercial%2520models%2520like%2520GPT-5.%2520All%2520models%2520and%2520code%250Aare%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency&entry.906535625=Weiyun%20Wang%20and%20Zhangwei%20Gao%20and%20Lixin%20Gu%20and%20Hengjun%20Pu%20and%20Long%20Cui%20and%20Xingguang%20Wei%20and%20Zhaoyang%20Liu%20and%20Linglin%20Jing%20and%20Shenglong%20Ye%20and%20Jie%20Shao%20and%20Zhaokai%20Wang%20and%20Zhe%20Chen%20and%20Hongjie%20Zhang%20and%20Ganlin%20Yang%20and%20Haomin%20Wang%20and%20Qi%20Wei%20and%20Jinhui%20Yin%20and%20Wenhao%20Li%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Zichen%20Ding%20and%20Changyao%20Tian%20and%20Zhenyu%20Wu%20and%20Jingjing%20Xie%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Yuchen%20Duan%20and%20Xuehui%20Wang%20and%20Songze%20Li%20and%20Xiangyu%20Zhao%20and%20Haodong%20Duan%20and%20Nianchen%20Deng%20and%20Bin%20Fu%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Junjun%20He%20and%20Yingtong%20Xiong%20and%20Han%20Lv%20and%20Lijun%20Wu%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang%20and%20Huipeng%20Deng%20and%20Biqing%20Qi%20and%20Jiaye%20Ge%20and%20Qipeng%20Guo%20and%20Wenwei%20Zhang%20and%20Wanli%20Ouyang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Jifeng%20Dai%20and%20Bowen%20Zhou%20and%20Weijie%20Su%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo&entry.1292438233=%20%20We%20introduce%20InternVL%203.5%2C%20a%20new%20family%20of%20open-source%20multimodal%20models%20that%0Asignificantly%20advances%20versatility%2C%20reasoning%20capability%2C%20and%20inference%0Aefficiency%20along%20the%20InternVL%20series.%20A%20key%20innovation%20is%20the%20Cascade%0AReinforcement%20Learning%20%28Cascade%20RL%29%20framework%2C%20which%20enhances%20reasoning%20through%0Aa%20two-stage%20process%3A%20offline%20RL%20for%20stable%20convergence%20and%20online%20RL%20for%0Arefined%20alignment.%20This%20coarse-to-fine%20training%20strategy%20leads%20to%20substantial%0Aimprovements%20on%20downstream%20reasoning%20tasks%2C%20e.g.%2C%20MMMU%20and%20MathVista.%20To%0Aoptimize%20efficiency%2C%20we%20propose%20a%20Visual%20Resolution%20Router%20%28ViR%29%20that%0Adynamically%20adjusts%20the%20resolution%20of%20visual%20tokens%20without%20compromising%0Aperformance.%20Coupled%20with%20ViR%2C%20our%20Decoupled%20Vision-Language%20Deployment%20%28DvD%29%0Astrategy%20separates%20the%20vision%20encoder%20and%20language%20model%20across%20different%20GPUs%2C%0Aeffectively%20balancing%20computational%20load.%20These%20contributions%20collectively%0Aenable%20InternVL3.5%20to%20achieve%20up%20to%20a%20%2B16.0%5C%25%20gain%20in%20overall%20reasoning%0Aperformance%20and%20a%204.05%24%5Ctimes%24%20inference%20speedup%20compared%20to%20its%20predecessor%2C%0Ai.e.%2C%20InternVL3.%20In%20addition%2C%20InternVL3.5%20supports%20novel%20capabilities%20such%20as%0AGUI%20interaction%20and%20embodied%20agency.%20Notably%2C%20our%20largest%20model%2C%20i.e.%2C%0AInternVL3.5-241B-A28B%2C%20attains%20state-of-the-art%20results%20among%20open-source%20MLLMs%0Aacross%20general%20multimodal%2C%20reasoning%2C%20text%2C%20and%20agentic%20tasks%20--%20narrowing%20the%0Aperformance%20gap%20with%20leading%20commercial%20models%20like%20GPT-5.%20All%20models%20and%20code%0Aare%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18265v1&entry.124074799=Read"},
{"title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in\n  Vision-Language Models", "author": "Zesen Lyu and Dandan Zhang and Wei Ye and Fangdi Li and Zhihang Jiang and Yao Yang", "abstract": "  Spatial reasoning is a core component of human cognition, enabling\nindividuals to perceive, comprehend, and interact with the physical world. It\nrelies on a nuanced understanding of spatial structures and inter-object\nrelationships, serving as the foundation for complex reasoning and\ndecision-making. To investigate whether current vision-language models (VLMs)\nexhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark\nconsisting of 1,100 carefully curated real-world images with high spatial\ncomplexity. Based on this dataset, we design five tasks to rigorously evaluate\nVLMs' spatial perception, structural understanding, and reasoning capabilities,\nwhile deliberately minimizing reliance on domain-specific knowledge to better\nisolate and assess the general spatial reasoning capability. We conduct a\ncomprehensive evaluation across 24 state-of-the-art VLMs. The results show that\neven the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy\nand performs particularly poorly on the Order Generation task, with only 30.00%\naccuracy, far below the performance exceeding 90% achieved by human\nparticipants. This persistent gap underscores the need for continued progress,\npositioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for\nadvancing spatial reasoning research in VLMs. Our project page is at\nhttps://zesen01.github.io/jigsaw-puzzles.\n", "link": "http://arxiv.org/abs/2505.20728v3", "date": "2025-08-25", "relevancy": 2.9792, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jigsaw-Puzzles%3A%20From%20Seeing%20to%20Understanding%20to%20Reasoning%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20Jigsaw-Puzzles%3A%20From%20Seeing%20to%20Understanding%20to%20Reasoning%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Zesen%20Lyu%20and%20Dandan%20Zhang%20and%20Wei%20Ye%20and%20Fangdi%20Li%20and%20Zhihang%20Jiang%20and%20Yao%20Yang%0AAbstract%3A%20%20%20Spatial%20reasoning%20is%20a%20core%20component%20of%20human%20cognition%2C%20enabling%0Aindividuals%20to%20perceive%2C%20comprehend%2C%20and%20interact%20with%20the%20physical%20world.%20It%0Arelies%20on%20a%20nuanced%20understanding%20of%20spatial%20structures%20and%20inter-object%0Arelationships%2C%20serving%20as%20the%20foundation%20for%20complex%20reasoning%20and%0Adecision-making.%20To%20investigate%20whether%20current%20vision-language%20models%20%28VLMs%29%0Aexhibit%20similar%20capability%2C%20we%20introduce%20Jigsaw-Puzzles%2C%20a%20novel%20benchmark%0Aconsisting%20of%201%2C100%20carefully%20curated%20real-world%20images%20with%20high%20spatial%0Acomplexity.%20Based%20on%20this%20dataset%2C%20we%20design%20five%20tasks%20to%20rigorously%20evaluate%0AVLMs%27%20spatial%20perception%2C%20structural%20understanding%2C%20and%20reasoning%20capabilities%2C%0Awhile%20deliberately%20minimizing%20reliance%20on%20domain-specific%20knowledge%20to%20better%0Aisolate%20and%20assess%20the%20general%20spatial%20reasoning%20capability.%20We%20conduct%20a%0Acomprehensive%20evaluation%20across%2024%20state-of-the-art%20VLMs.%20The%20results%20show%20that%0Aeven%20the%20strongest%20model%2C%20Gemini-2.5-Pro%2C%20achieves%20only%2077.14%25%20overall%20accuracy%0Aand%20performs%20particularly%20poorly%20on%20the%20Order%20Generation%20task%2C%20with%20only%2030.00%25%0Aaccuracy%2C%20far%20below%20the%20performance%20exceeding%2090%25%20achieved%20by%20human%0Aparticipants.%20This%20persistent%20gap%20underscores%20the%20need%20for%20continued%20progress%2C%0Apositioning%20Jigsaw-Puzzles%20as%20a%20challenging%20and%20diagnostic%20benchmark%20for%0Aadvancing%20spatial%20reasoning%20research%20in%20VLMs.%20Our%20project%20page%20is%20at%0Ahttps%3A//zesen01.github.io/jigsaw-puzzles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20728v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJigsaw-Puzzles%253A%2520From%2520Seeing%2520to%2520Understanding%2520to%2520Reasoning%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DZesen%2520Lyu%2520and%2520Dandan%2520Zhang%2520and%2520Wei%2520Ye%2520and%2520Fangdi%2520Li%2520and%2520Zhihang%2520Jiang%2520and%2520Yao%2520Yang%26entry.1292438233%3D%2520%2520Spatial%2520reasoning%2520is%2520a%2520core%2520component%2520of%2520human%2520cognition%252C%2520enabling%250Aindividuals%2520to%2520perceive%252C%2520comprehend%252C%2520and%2520interact%2520with%2520the%2520physical%2520world.%2520It%250Arelies%2520on%2520a%2520nuanced%2520understanding%2520of%2520spatial%2520structures%2520and%2520inter-object%250Arelationships%252C%2520serving%2520as%2520the%2520foundation%2520for%2520complex%2520reasoning%2520and%250Adecision-making.%2520To%2520investigate%2520whether%2520current%2520vision-language%2520models%2520%2528VLMs%2529%250Aexhibit%2520similar%2520capability%252C%2520we%2520introduce%2520Jigsaw-Puzzles%252C%2520a%2520novel%2520benchmark%250Aconsisting%2520of%25201%252C100%2520carefully%2520curated%2520real-world%2520images%2520with%2520high%2520spatial%250Acomplexity.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520design%2520five%2520tasks%2520to%2520rigorously%2520evaluate%250AVLMs%2527%2520spatial%2520perception%252C%2520structural%2520understanding%252C%2520and%2520reasoning%2520capabilities%252C%250Awhile%2520deliberately%2520minimizing%2520reliance%2520on%2520domain-specific%2520knowledge%2520to%2520better%250Aisolate%2520and%2520assess%2520the%2520general%2520spatial%2520reasoning%2520capability.%2520We%2520conduct%2520a%250Acomprehensive%2520evaluation%2520across%252024%2520state-of-the-art%2520VLMs.%2520The%2520results%2520show%2520that%250Aeven%2520the%2520strongest%2520model%252C%2520Gemini-2.5-Pro%252C%2520achieves%2520only%252077.14%2525%2520overall%2520accuracy%250Aand%2520performs%2520particularly%2520poorly%2520on%2520the%2520Order%2520Generation%2520task%252C%2520with%2520only%252030.00%2525%250Aaccuracy%252C%2520far%2520below%2520the%2520performance%2520exceeding%252090%2525%2520achieved%2520by%2520human%250Aparticipants.%2520This%2520persistent%2520gap%2520underscores%2520the%2520need%2520for%2520continued%2520progress%252C%250Apositioning%2520Jigsaw-Puzzles%2520as%2520a%2520challenging%2520and%2520diagnostic%2520benchmark%2520for%250Aadvancing%2520spatial%2520reasoning%2520research%2520in%2520VLMs.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//zesen01.github.io/jigsaw-puzzles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20728v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jigsaw-Puzzles%3A%20From%20Seeing%20to%20Understanding%20to%20Reasoning%20in%0A%20%20Vision-Language%20Models&entry.906535625=Zesen%20Lyu%20and%20Dandan%20Zhang%20and%20Wei%20Ye%20and%20Fangdi%20Li%20and%20Zhihang%20Jiang%20and%20Yao%20Yang&entry.1292438233=%20%20Spatial%20reasoning%20is%20a%20core%20component%20of%20human%20cognition%2C%20enabling%0Aindividuals%20to%20perceive%2C%20comprehend%2C%20and%20interact%20with%20the%20physical%20world.%20It%0Arelies%20on%20a%20nuanced%20understanding%20of%20spatial%20structures%20and%20inter-object%0Arelationships%2C%20serving%20as%20the%20foundation%20for%20complex%20reasoning%20and%0Adecision-making.%20To%20investigate%20whether%20current%20vision-language%20models%20%28VLMs%29%0Aexhibit%20similar%20capability%2C%20we%20introduce%20Jigsaw-Puzzles%2C%20a%20novel%20benchmark%0Aconsisting%20of%201%2C100%20carefully%20curated%20real-world%20images%20with%20high%20spatial%0Acomplexity.%20Based%20on%20this%20dataset%2C%20we%20design%20five%20tasks%20to%20rigorously%20evaluate%0AVLMs%27%20spatial%20perception%2C%20structural%20understanding%2C%20and%20reasoning%20capabilities%2C%0Awhile%20deliberately%20minimizing%20reliance%20on%20domain-specific%20knowledge%20to%20better%0Aisolate%20and%20assess%20the%20general%20spatial%20reasoning%20capability.%20We%20conduct%20a%0Acomprehensive%20evaluation%20across%2024%20state-of-the-art%20VLMs.%20The%20results%20show%20that%0Aeven%20the%20strongest%20model%2C%20Gemini-2.5-Pro%2C%20achieves%20only%2077.14%25%20overall%20accuracy%0Aand%20performs%20particularly%20poorly%20on%20the%20Order%20Generation%20task%2C%20with%20only%2030.00%25%0Aaccuracy%2C%20far%20below%20the%20performance%20exceeding%2090%25%20achieved%20by%20human%0Aparticipants.%20This%20persistent%20gap%20underscores%20the%20need%20for%20continued%20progress%2C%0Apositioning%20Jigsaw-Puzzles%20as%20a%20challenging%20and%20diagnostic%20benchmark%20for%0Aadvancing%20spatial%20reasoning%20research%20in%20VLMs.%20Our%20project%20page%20is%20at%0Ahttps%3A//zesen01.github.io/jigsaw-puzzles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20728v3&entry.124074799=Read"},
{"title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal\n  Foundation Models", "author": "Ziyao Shangguan and Chuhan Li and Yuxuan Ding and Yanan Zheng and Yilun Zhao and Tesca Fitzgerald and Arman Cohan", "abstract": "  Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.\n", "link": "http://arxiv.org/abs/2410.23266v2", "date": "2025-08-25", "relevancy": 2.9658, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Ziyao%20Shangguan%20and%20Chuhan%20Li%20and%20Yuxuan%20Ding%20and%20Yanan%20Zheng%20and%20Yilun%20Zhao%20and%20Tesca%20Fitzgerald%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20Existing%20benchmarks%20often%20highlight%20the%20remarkable%20performance%20achieved%20by%0Astate-of-the-art%20Multimodal%20Foundation%20Models%20%28MFMs%29%20in%20leveraging%20temporal%0Acontext%20for%20video%20understanding.%20However%2C%20how%20well%20do%20the%20models%20truly%20perform%0Avisual%20temporal%20reasoning%3F%20Our%20study%20of%20existing%20benchmarks%20shows%20that%20this%0Acapability%20of%20MFMs%20is%20likely%20overestimated%20as%20many%20questions%20can%20be%20solved%20by%0Ausing%20a%20single%2C%20few%2C%20or%20out-of-order%20frames.%20To%20systematically%20examine%20current%0Avisual%20temporal%20reasoning%20tasks%2C%20we%20propose%20three%20principles%20with%20corresponding%0Ametrics%3A%20%281%29%20Multi-Frame%20Gain%2C%20%282%29%20Frame%20Order%20Sensitivity%2C%20and%20%283%29%20Frame%0AInformation%20Disparity.%20Following%20these%20principles%2C%20we%20introduce%20TOMATO%2C%0ATemporal%20Reasoning%20Multimodal%20Evaluation%2C%20a%20novel%20benchmark%20crafted%20to%0Arigorously%20assess%20MFMs%27%20temporal%20reasoning%20capabilities%20in%20video%20understanding.%0ATOMATO%20comprises%201%2C484%20carefully%20curated%2C%20human-annotated%20questions%20spanning%0Asix%20tasks%20%28i.e.%2C%20action%20count%2C%20direction%2C%20rotation%2C%20shape%20%26%20trend%2C%20velocity%20%26%0Afrequency%2C%20and%20visual%20cues%29%2C%20applied%20to%201%2C417%20videos%2C%20including%20805%0Aself-recorded%20and%20-generated%20videos%2C%20that%20encompass%20human-centric%2C%20real-world%2C%0Aand%20simulated%20scenarios.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%0Aperformance%20gap%20of%2057.3%25%20with%20the%20best-performing%20model.%20Moreover%2C%20our%20in-depth%0Aanalysis%20uncovers%20more%20fundamental%20limitations%20beyond%20this%20gap%20in%20current%20MFMs.%0AWhile%20they%20can%20accurately%20recognize%20events%20in%20isolated%20frames%2C%20they%20fail%20to%0Ainterpret%20these%20frames%20as%20a%20continuous%20sequence.%20We%20believe%20TOMATO%20will%20serve%0Aas%20a%20crucial%20testbed%20for%20evaluating%20the%20next-generation%20MFMs%20and%20as%20a%20call%20to%0Athe%20community%20to%20develop%20AI%20systems%20capable%20of%20comprehending%20human%20world%0Adynamics%20through%20the%20video%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTOMATO%253A%2520Assessing%2520Visual%2520Temporal%2520Reasoning%2520Capabilities%2520in%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DZiyao%2520Shangguan%2520and%2520Chuhan%2520Li%2520and%2520Yuxuan%2520Ding%2520and%2520Yanan%2520Zheng%2520and%2520Yilun%2520Zhao%2520and%2520Tesca%2520Fitzgerald%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520Existing%2520benchmarks%2520often%2520highlight%2520the%2520remarkable%2520performance%2520achieved%2520by%250Astate-of-the-art%2520Multimodal%2520Foundation%2520Models%2520%2528MFMs%2529%2520in%2520leveraging%2520temporal%250Acontext%2520for%2520video%2520understanding.%2520However%252C%2520how%2520well%2520do%2520the%2520models%2520truly%2520perform%250Avisual%2520temporal%2520reasoning%253F%2520Our%2520study%2520of%2520existing%2520benchmarks%2520shows%2520that%2520this%250Acapability%2520of%2520MFMs%2520is%2520likely%2520overestimated%2520as%2520many%2520questions%2520can%2520be%2520solved%2520by%250Ausing%2520a%2520single%252C%2520few%252C%2520or%2520out-of-order%2520frames.%2520To%2520systematically%2520examine%2520current%250Avisual%2520temporal%2520reasoning%2520tasks%252C%2520we%2520propose%2520three%2520principles%2520with%2520corresponding%250Ametrics%253A%2520%25281%2529%2520Multi-Frame%2520Gain%252C%2520%25282%2529%2520Frame%2520Order%2520Sensitivity%252C%2520and%2520%25283%2529%2520Frame%250AInformation%2520Disparity.%2520Following%2520these%2520principles%252C%2520we%2520introduce%2520TOMATO%252C%250ATemporal%2520Reasoning%2520Multimodal%2520Evaluation%252C%2520a%2520novel%2520benchmark%2520crafted%2520to%250Arigorously%2520assess%2520MFMs%2527%2520temporal%2520reasoning%2520capabilities%2520in%2520video%2520understanding.%250ATOMATO%2520comprises%25201%252C484%2520carefully%2520curated%252C%2520human-annotated%2520questions%2520spanning%250Asix%2520tasks%2520%2528i.e.%252C%2520action%2520count%252C%2520direction%252C%2520rotation%252C%2520shape%2520%2526%2520trend%252C%2520velocity%2520%2526%250Afrequency%252C%2520and%2520visual%2520cues%2529%252C%2520applied%2520to%25201%252C417%2520videos%252C%2520including%2520805%250Aself-recorded%2520and%2520-generated%2520videos%252C%2520that%2520encompass%2520human-centric%252C%2520real-world%252C%250Aand%2520simulated%2520scenarios.%2520Our%2520comprehensive%2520evaluation%2520reveals%2520a%2520human-model%250Aperformance%2520gap%2520of%252057.3%2525%2520with%2520the%2520best-performing%2520model.%2520Moreover%252C%2520our%2520in-depth%250Aanalysis%2520uncovers%2520more%2520fundamental%2520limitations%2520beyond%2520this%2520gap%2520in%2520current%2520MFMs.%250AWhile%2520they%2520can%2520accurately%2520recognize%2520events%2520in%2520isolated%2520frames%252C%2520they%2520fail%2520to%250Ainterpret%2520these%2520frames%2520as%2520a%2520continuous%2520sequence.%2520We%2520believe%2520TOMATO%2520will%2520serve%250Aas%2520a%2520crucial%2520testbed%2520for%2520evaluating%2520the%2520next-generation%2520MFMs%2520and%2520as%2520a%2520call%2520to%250Athe%2520community%2520to%2520develop%2520AI%2520systems%2520capable%2520of%2520comprehending%2520human%2520world%250Adynamics%2520through%2520the%2520video%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOMATO%3A%20Assessing%20Visual%20Temporal%20Reasoning%20Capabilities%20in%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Ziyao%20Shangguan%20and%20Chuhan%20Li%20and%20Yuxuan%20Ding%20and%20Yanan%20Zheng%20and%20Yilun%20Zhao%20and%20Tesca%20Fitzgerald%20and%20Arman%20Cohan&entry.1292438233=%20%20Existing%20benchmarks%20often%20highlight%20the%20remarkable%20performance%20achieved%20by%0Astate-of-the-art%20Multimodal%20Foundation%20Models%20%28MFMs%29%20in%20leveraging%20temporal%0Acontext%20for%20video%20understanding.%20However%2C%20how%20well%20do%20the%20models%20truly%20perform%0Avisual%20temporal%20reasoning%3F%20Our%20study%20of%20existing%20benchmarks%20shows%20that%20this%0Acapability%20of%20MFMs%20is%20likely%20overestimated%20as%20many%20questions%20can%20be%20solved%20by%0Ausing%20a%20single%2C%20few%2C%20or%20out-of-order%20frames.%20To%20systematically%20examine%20current%0Avisual%20temporal%20reasoning%20tasks%2C%20we%20propose%20three%20principles%20with%20corresponding%0Ametrics%3A%20%281%29%20Multi-Frame%20Gain%2C%20%282%29%20Frame%20Order%20Sensitivity%2C%20and%20%283%29%20Frame%0AInformation%20Disparity.%20Following%20these%20principles%2C%20we%20introduce%20TOMATO%2C%0ATemporal%20Reasoning%20Multimodal%20Evaluation%2C%20a%20novel%20benchmark%20crafted%20to%0Arigorously%20assess%20MFMs%27%20temporal%20reasoning%20capabilities%20in%20video%20understanding.%0ATOMATO%20comprises%201%2C484%20carefully%20curated%2C%20human-annotated%20questions%20spanning%0Asix%20tasks%20%28i.e.%2C%20action%20count%2C%20direction%2C%20rotation%2C%20shape%20%26%20trend%2C%20velocity%20%26%0Afrequency%2C%20and%20visual%20cues%29%2C%20applied%20to%201%2C417%20videos%2C%20including%20805%0Aself-recorded%20and%20-generated%20videos%2C%20that%20encompass%20human-centric%2C%20real-world%2C%0Aand%20simulated%20scenarios.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%0Aperformance%20gap%20of%2057.3%25%20with%20the%20best-performing%20model.%20Moreover%2C%20our%20in-depth%0Aanalysis%20uncovers%20more%20fundamental%20limitations%20beyond%20this%20gap%20in%20current%20MFMs.%0AWhile%20they%20can%20accurately%20recognize%20events%20in%20isolated%20frames%2C%20they%20fail%20to%0Ainterpret%20these%20frames%20as%20a%20continuous%20sequence.%20We%20believe%20TOMATO%20will%20serve%0Aas%20a%20crucial%20testbed%20for%20evaluating%20the%20next-generation%20MFMs%20and%20as%20a%20call%20to%0Athe%20community%20to%20develop%20AI%20systems%20capable%20of%20comprehending%20human%20world%0Adynamics%20through%20the%20video%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23266v2&entry.124074799=Read"},
{"title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling\n  Representation and Dynamically Fusing Features in CLIP", "author": "Ke Ma and Jun Long and Hongxiao Fei and Liujie Hua and Yiran Qian and Zhen Dai and Yueyi Luo", "abstract": "  Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method proposes a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.\n", "link": "http://arxiv.org/abs/2508.07819v3", "date": "2025-08-25", "relevancy": 2.9637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP&body=Title%3A%20Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP%0AAuthor%3A%20Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Yiran%20Qian%20and%20Zhen%20Dai%20and%20Yueyi%20Luo%0AAbstract%3A%20%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20face%20a%20significant%20adaptation%20gap%0Awhen%20applied%20to%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%2C%20stemming%20from%20their%20lack%20of%0Alocal%20inductive%20biases%20for%20dense%20prediction%20and%20their%20reliance%20on%20inflexible%0Afeature%20fusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20proposes%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07819v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectural%2520Co-Design%2520for%2520Zero-Shot%2520Anomaly%2520Detection%253A%2520Decoupling%250A%2520%2520Representation%2520and%2520Dynamically%2520Fusing%2520Features%2520in%2520CLIP%26entry.906535625%3DKe%2520Ma%2520and%2520Jun%2520Long%2520and%2520Hongxiao%2520Fei%2520and%2520Liujie%2520Hua%2520and%2520Yiran%2520Qian%2520and%2520Zhen%2520Dai%2520and%2520Yueyi%2520Luo%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520face%2520a%2520significant%2520adaptation%2520gap%250Awhen%2520applied%2520to%2520Zero-Shot%2520Anomaly%2520Detection%2520%2528ZSAD%2529%252C%2520stemming%2520from%2520their%2520lack%2520of%250Alocal%2520inductive%2520biases%2520for%2520dense%2520prediction%2520and%2520their%2520reliance%2520on%2520inflexible%250Afeature%2520fusion%2520paradigms.%2520We%2520address%2520these%2520limitations%2520through%2520an%2520Architectural%250ACo-Design%2520framework%2520that%2520jointly%2520refines%2520feature%2520representation%2520and%2520cross-modal%250Afusion.%2520Our%2520method%2520proposes%2520a%2520parameter-efficient%2520Convolutional%2520Low-Rank%250AAdaptation%2520%2528Conv-LoRA%2529%2520adapter%2520to%2520inject%2520local%2520inductive%2520biases%2520for%250Afine-grained%2520representation%252C%2520and%2520introduces%2520a%2520Dynamic%2520Fusion%2520Gateway%2520%2528DFG%2529%2520that%250Aleverages%2520visual%2520context%2520to%2520adaptively%2520modulate%2520text%2520prompts%252C%2520enabling%2520a%250Apowerful%2520bidirectional%2520fusion.%2520Extensive%2520experiments%2520on%2520diverse%2520industrial%2520and%250Amedical%2520benchmarks%2520demonstrate%2520superior%2520accuracy%2520and%2520robustness%252C%2520validating%250Athat%2520this%2520synergistic%2520co-design%2520is%2520critical%2520for%2520robustly%2520adapting%2520foundation%250Amodels%2520to%2520dense%2520perception%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07819v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectural%20Co-Design%20for%20Zero-Shot%20Anomaly%20Detection%3A%20Decoupling%0A%20%20Representation%20and%20Dynamically%20Fusing%20Features%20in%20CLIP&entry.906535625=Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Yiran%20Qian%20and%20Zhen%20Dai%20and%20Yueyi%20Luo&entry.1292438233=%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20face%20a%20significant%20adaptation%20gap%0Awhen%20applied%20to%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%2C%20stemming%20from%20their%20lack%20of%0Alocal%20inductive%20biases%20for%20dense%20prediction%20and%20their%20reliance%20on%20inflexible%0Afeature%20fusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20proposes%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07819v3&entry.124074799=Read"},
{"title": "SEAM: Semantically Equivalent Across Modalities Benchmark for\n  Vision-Language Models", "author": "Zhenwei Tang and Difan Jiao and Blair Yang and Ashton Anderson", "abstract": "  Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.\n", "link": "http://arxiv.org/abs/2508.18179v1", "date": "2025-08-25", "relevancy": 2.9098, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAM%3A%20Semantically%20Equivalent%20Across%20Modalities%20Benchmark%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20SEAM%3A%20Semantically%20Equivalent%20Across%20Modalities%20Benchmark%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Zhenwei%20Tang%20and%20Difan%20Jiao%20and%20Blair%20Yang%20and%20Ashton%20Anderson%0AAbstract%3A%20%20%20Evaluating%20whether%20vision-language%20models%20%28VLMs%29%20reason%20consistently%20across%0Arepresentations%20is%20challenging%20because%20modality%20comparisons%20are%20typically%0Aconfounded%20by%20task%20differences%20and%20asymmetric%20information.%20We%20introduce%20SEAM%2C%20a%0Abenchmark%20that%20pairs%20semantically%20equivalent%20inputs%20across%20four%20domains%20that%0Ahave%20existing%20standardized%20textual%20and%20visual%20notations.%20By%20employing%20distinct%0Anotation%20systems%20across%20modalities%2C%20in%20contrast%20to%20OCR-based%20image-text%0Apairing%2C%20SEAM%20provides%20a%20rigorous%20comparative%20assessment%20of%20the%0Atextual-symbolic%20and%20visual-spatial%20reasoning%20capabilities%20of%20VLMs.%20Across%2021%0Acontemporary%20models%2C%20we%20observe%20systematic%20modality%20imbalance%3A%20vision%0Afrequently%20lags%20language%20in%20overall%20performance%2C%20despite%20the%20problems%0Acontaining%20semantically%20equivalent%20information%2C%20and%20cross-modal%20agreement%20is%0Arelatively%20low.%20Our%20error%20analysis%20reveals%20two%20main%20drivers%3A%20textual%20perception%0Afailures%20from%20tokenization%20in%20domain%20notation%20and%20visual%20perception%20failures%0Athat%20induce%20hallucinations.%20We%20also%20show%20that%20our%20results%20are%20largely%20robust%20to%0Avisual%20transformations.%20SEAM%20establishes%20a%20controlled%2C%20semantically%20equivalent%0Asetting%20for%20measuring%20and%20improving%20modality-agnostic%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAM%253A%2520Semantically%2520Equivalent%2520Across%2520Modalities%2520Benchmark%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DZhenwei%2520Tang%2520and%2520Difan%2520Jiao%2520and%2520Blair%2520Yang%2520and%2520Ashton%2520Anderson%26entry.1292438233%3D%2520%2520Evaluating%2520whether%2520vision-language%2520models%2520%2528VLMs%2529%2520reason%2520consistently%2520across%250Arepresentations%2520is%2520challenging%2520because%2520modality%2520comparisons%2520are%2520typically%250Aconfounded%2520by%2520task%2520differences%2520and%2520asymmetric%2520information.%2520We%2520introduce%2520SEAM%252C%2520a%250Abenchmark%2520that%2520pairs%2520semantically%2520equivalent%2520inputs%2520across%2520four%2520domains%2520that%250Ahave%2520existing%2520standardized%2520textual%2520and%2520visual%2520notations.%2520By%2520employing%2520distinct%250Anotation%2520systems%2520across%2520modalities%252C%2520in%2520contrast%2520to%2520OCR-based%2520image-text%250Apairing%252C%2520SEAM%2520provides%2520a%2520rigorous%2520comparative%2520assessment%2520of%2520the%250Atextual-symbolic%2520and%2520visual-spatial%2520reasoning%2520capabilities%2520of%2520VLMs.%2520Across%252021%250Acontemporary%2520models%252C%2520we%2520observe%2520systematic%2520modality%2520imbalance%253A%2520vision%250Afrequently%2520lags%2520language%2520in%2520overall%2520performance%252C%2520despite%2520the%2520problems%250Acontaining%2520semantically%2520equivalent%2520information%252C%2520and%2520cross-modal%2520agreement%2520is%250Arelatively%2520low.%2520Our%2520error%2520analysis%2520reveals%2520two%2520main%2520drivers%253A%2520textual%2520perception%250Afailures%2520from%2520tokenization%2520in%2520domain%2520notation%2520and%2520visual%2520perception%2520failures%250Athat%2520induce%2520hallucinations.%2520We%2520also%2520show%2520that%2520our%2520results%2520are%2520largely%2520robust%2520to%250Avisual%2520transformations.%2520SEAM%2520establishes%2520a%2520controlled%252C%2520semantically%2520equivalent%250Asetting%2520for%2520measuring%2520and%2520improving%2520modality-agnostic%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAM%3A%20Semantically%20Equivalent%20Across%20Modalities%20Benchmark%20for%0A%20%20Vision-Language%20Models&entry.906535625=Zhenwei%20Tang%20and%20Difan%20Jiao%20and%20Blair%20Yang%20and%20Ashton%20Anderson&entry.1292438233=%20%20Evaluating%20whether%20vision-language%20models%20%28VLMs%29%20reason%20consistently%20across%0Arepresentations%20is%20challenging%20because%20modality%20comparisons%20are%20typically%0Aconfounded%20by%20task%20differences%20and%20asymmetric%20information.%20We%20introduce%20SEAM%2C%20a%0Abenchmark%20that%20pairs%20semantically%20equivalent%20inputs%20across%20four%20domains%20that%0Ahave%20existing%20standardized%20textual%20and%20visual%20notations.%20By%20employing%20distinct%0Anotation%20systems%20across%20modalities%2C%20in%20contrast%20to%20OCR-based%20image-text%0Apairing%2C%20SEAM%20provides%20a%20rigorous%20comparative%20assessment%20of%20the%0Atextual-symbolic%20and%20visual-spatial%20reasoning%20capabilities%20of%20VLMs.%20Across%2021%0Acontemporary%20models%2C%20we%20observe%20systematic%20modality%20imbalance%3A%20vision%0Afrequently%20lags%20language%20in%20overall%20performance%2C%20despite%20the%20problems%0Acontaining%20semantically%20equivalent%20information%2C%20and%20cross-modal%20agreement%20is%0Arelatively%20low.%20Our%20error%20analysis%20reveals%20two%20main%20drivers%3A%20textual%20perception%0Afailures%20from%20tokenization%20in%20domain%20notation%20and%20visual%20perception%20failures%0Athat%20induce%20hallucinations.%20We%20also%20show%20that%20our%20results%20are%20largely%20robust%20to%0Avisual%20transformations.%20SEAM%20establishes%20a%20controlled%2C%20semantically%20equivalent%0Asetting%20for%20measuring%20and%20improving%20modality-agnostic%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18179v1&entry.124074799=Read"},
{"title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images", "author": "Anilkumar Swamy and Vincent Leroy and Philippe Weinzaepfel and Jean-S\u00e9bastien Franco and Gr\u00e9gory Rogez", "abstract": "  Hand-object 3D reconstruction has become increasingly important for\napplications in human-robot interaction and immersive AR/VR experiences. A\ncommon approach for object-agnostic hand-object reconstruction from RGB\nsequences involves a two-stage pipeline: hand-object 3D tracking followed by\nmulti-view 3D reconstruction. However, existing methods rely on keypoint\ndetection techniques, such as Structure from Motion (SfM) and hand-keypoint\noptimization, which struggle with diverse object geometries, weak textures, and\nmutual hand-object occlusions, limiting scalability and generalization. As a\nkey enabler to generic and seamless, non-intrusive applicability, we propose in\nthis work a robust, keypoint detector-free approach to estimating hand-object\n3D transformations from monocular motion video/images. We further integrate\nthis with a multi-view reconstruction pipeline to accurately recover\nhand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely\non pre-scanned object templates or camera intrinsics, and reaches\nstate-of-the-art performance for the tasks of object-agnostic hand-object 3D\ntransformation and shape estimation on the SHOWMe benchmark. We also experiment\non sequences from the HO3D dataset, demonstrating generalization to unseen\nobject categories.\n", "link": "http://arxiv.org/abs/2508.16465v2", "date": "2025-08-25", "relevancy": 2.9042, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5878}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images&body=Title%3A%20HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images%0AAuthor%3A%20Anilkumar%20Swamy%20and%20Vincent%20Leroy%20and%20Philippe%20Weinzaepfel%20and%20Jean-S%C3%A9bastien%20Franco%20and%20Gr%C3%A9gory%20Rogez%0AAbstract%3A%20%20%20Hand-object%203D%20reconstruction%20has%20become%20increasingly%20important%20for%0Aapplications%20in%20human-robot%20interaction%20and%20immersive%20AR/VR%20experiences.%20A%0Acommon%20approach%20for%20object-agnostic%20hand-object%20reconstruction%20from%20RGB%0Asequences%20involves%20a%20two-stage%20pipeline%3A%20hand-object%203D%20tracking%20followed%20by%0Amulti-view%203D%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20keypoint%0Adetection%20techniques%2C%20such%20as%20Structure%20from%20Motion%20%28SfM%29%20and%20hand-keypoint%0Aoptimization%2C%20which%20struggle%20with%20diverse%20object%20geometries%2C%20weak%20textures%2C%20and%0Amutual%20hand-object%20occlusions%2C%20limiting%20scalability%20and%20generalization.%20As%20a%0Akey%20enabler%20to%20generic%20and%20seamless%2C%20non-intrusive%20applicability%2C%20we%20propose%20in%0Athis%20work%20a%20robust%2C%20keypoint%20detector-free%20approach%20to%20estimating%20hand-object%0A3D%20transformations%20from%20monocular%20motion%20video/images.%20We%20further%20integrate%0Athis%20with%20a%20multi-view%20reconstruction%20pipeline%20to%20accurately%20recover%0Ahand-object%203D%20shape.%20Our%20method%2C%20named%20HOSt3R%2C%20is%20unconstrained%2C%20does%20not%20rely%0Aon%20pre-scanned%20object%20templates%20or%20camera%20intrinsics%2C%20and%20reaches%0Astate-of-the-art%20performance%20for%20the%20tasks%20of%20object-agnostic%20hand-object%203D%0Atransformation%20and%20shape%20estimation%20on%20the%20SHOWMe%20benchmark.%20We%20also%20experiment%0Aon%20sequences%20from%20the%20HO3D%20dataset%2C%20demonstrating%20generalization%20to%20unseen%0Aobject%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOSt3R%253A%2520Keypoint-free%2520Hand-Object%25203D%2520Reconstruction%2520from%2520RGB%2520images%26entry.906535625%3DAnilkumar%2520Swamy%2520and%2520Vincent%2520Leroy%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Jean-S%25C3%25A9bastien%2520Franco%2520and%2520Gr%25C3%25A9gory%2520Rogez%26entry.1292438233%3D%2520%2520Hand-object%25203D%2520reconstruction%2520has%2520become%2520increasingly%2520important%2520for%250Aapplications%2520in%2520human-robot%2520interaction%2520and%2520immersive%2520AR/VR%2520experiences.%2520A%250Acommon%2520approach%2520for%2520object-agnostic%2520hand-object%2520reconstruction%2520from%2520RGB%250Asequences%2520involves%2520a%2520two-stage%2520pipeline%253A%2520hand-object%25203D%2520tracking%2520followed%2520by%250Amulti-view%25203D%2520reconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520keypoint%250Adetection%2520techniques%252C%2520such%2520as%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520and%2520hand-keypoint%250Aoptimization%252C%2520which%2520struggle%2520with%2520diverse%2520object%2520geometries%252C%2520weak%2520textures%252C%2520and%250Amutual%2520hand-object%2520occlusions%252C%2520limiting%2520scalability%2520and%2520generalization.%2520As%2520a%250Akey%2520enabler%2520to%2520generic%2520and%2520seamless%252C%2520non-intrusive%2520applicability%252C%2520we%2520propose%2520in%250Athis%2520work%2520a%2520robust%252C%2520keypoint%2520detector-free%2520approach%2520to%2520estimating%2520hand-object%250A3D%2520transformations%2520from%2520monocular%2520motion%2520video/images.%2520We%2520further%2520integrate%250Athis%2520with%2520a%2520multi-view%2520reconstruction%2520pipeline%2520to%2520accurately%2520recover%250Ahand-object%25203D%2520shape.%2520Our%2520method%252C%2520named%2520HOSt3R%252C%2520is%2520unconstrained%252C%2520does%2520not%2520rely%250Aon%2520pre-scanned%2520object%2520templates%2520or%2520camera%2520intrinsics%252C%2520and%2520reaches%250Astate-of-the-art%2520performance%2520for%2520the%2520tasks%2520of%2520object-agnostic%2520hand-object%25203D%250Atransformation%2520and%2520shape%2520estimation%2520on%2520the%2520SHOWMe%2520benchmark.%2520We%2520also%2520experiment%250Aon%2520sequences%2520from%2520the%2520HO3D%2520dataset%252C%2520demonstrating%2520generalization%2520to%2520unseen%250Aobject%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOSt3R%3A%20Keypoint-free%20Hand-Object%203D%20Reconstruction%20from%20RGB%20images&entry.906535625=Anilkumar%20Swamy%20and%20Vincent%20Leroy%20and%20Philippe%20Weinzaepfel%20and%20Jean-S%C3%A9bastien%20Franco%20and%20Gr%C3%A9gory%20Rogez&entry.1292438233=%20%20Hand-object%203D%20reconstruction%20has%20become%20increasingly%20important%20for%0Aapplications%20in%20human-robot%20interaction%20and%20immersive%20AR/VR%20experiences.%20A%0Acommon%20approach%20for%20object-agnostic%20hand-object%20reconstruction%20from%20RGB%0Asequences%20involves%20a%20two-stage%20pipeline%3A%20hand-object%203D%20tracking%20followed%20by%0Amulti-view%203D%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20keypoint%0Adetection%20techniques%2C%20such%20as%20Structure%20from%20Motion%20%28SfM%29%20and%20hand-keypoint%0Aoptimization%2C%20which%20struggle%20with%20diverse%20object%20geometries%2C%20weak%20textures%2C%20and%0Amutual%20hand-object%20occlusions%2C%20limiting%20scalability%20and%20generalization.%20As%20a%0Akey%20enabler%20to%20generic%20and%20seamless%2C%20non-intrusive%20applicability%2C%20we%20propose%20in%0Athis%20work%20a%20robust%2C%20keypoint%20detector-free%20approach%20to%20estimating%20hand-object%0A3D%20transformations%20from%20monocular%20motion%20video/images.%20We%20further%20integrate%0Athis%20with%20a%20multi-view%20reconstruction%20pipeline%20to%20accurately%20recover%0Ahand-object%203D%20shape.%20Our%20method%2C%20named%20HOSt3R%2C%20is%20unconstrained%2C%20does%20not%20rely%0Aon%20pre-scanned%20object%20templates%20or%20camera%20intrinsics%2C%20and%20reaches%0Astate-of-the-art%20performance%20for%20the%20tasks%20of%20object-agnostic%20hand-object%203D%0Atransformation%20and%20shape%20estimation%20on%20the%20SHOWMe%20benchmark.%20We%20also%20experiment%0Aon%20sequences%20from%20the%20HO3D%20dataset%2C%20demonstrating%20generalization%20to%20unseen%0Aobject%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16465v2&entry.124074799=Read"},
{"title": "See What You Need: Query-Aware Visual Intelligence through\n  Reasoning-Perception Loops", "author": "Zixuan Dong and Baoyun Peng and Yufei Wang and Lin Liu and Xinxin Dong and Yunlong Cao and Xiaodong Wang", "abstract": "  Human video comprehension demonstrates dynamic coordination between reasoning\nand visual attention, adaptively focusing on query-relevant details. However,\ncurrent long-form video question answering systems employ rigid pipelines that\ndecouple reasoning from perception, leading to either information loss through\npremature visual abstraction or computational inefficiency through exhaustive\nprocessing. The core limitation lies in the inability to adapt visual\nextraction to specific reasoning requirements, different queries demand\nfundamentally different visual evidence from the same video content. In this\nwork, we present CAVIA, a training-free framework that revolutionizes video\nunderstanding through reasoning, perception coordination. Unlike conventional\napproaches where visual processing operates independently of reasoning, CAVIA\ncreates a closed-loop system where reasoning continuously guides visual\nextraction based on identified information gaps. CAVIA introduces three\ninnovations: (1) hierarchical reasoning, guided localization to precise frames;\n(2) cross-modal semantic bridging for targeted extraction; (3)\nconfidence-driven iterative synthesis. CAVIA achieves state-of-the-art\nperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA\n(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic\nreasoning-perception coordination provides a scalable paradigm for video\nunderstanding.\n", "link": "http://arxiv.org/abs/2508.17932v1", "date": "2025-08-25", "relevancy": 2.8977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20What%20You%20Need%3A%20Query-Aware%20Visual%20Intelligence%20through%0A%20%20Reasoning-Perception%20Loops&body=Title%3A%20See%20What%20You%20Need%3A%20Query-Aware%20Visual%20Intelligence%20through%0A%20%20Reasoning-Perception%20Loops%0AAuthor%3A%20Zixuan%20Dong%20and%20Baoyun%20Peng%20and%20Yufei%20Wang%20and%20Lin%20Liu%20and%20Xinxin%20Dong%20and%20Yunlong%20Cao%20and%20Xiaodong%20Wang%0AAbstract%3A%20%20%20Human%20video%20comprehension%20demonstrates%20dynamic%20coordination%20between%20reasoning%0Aand%20visual%20attention%2C%20adaptively%20focusing%20on%20query-relevant%20details.%20However%2C%0Acurrent%20long-form%20video%20question%20answering%20systems%20employ%20rigid%20pipelines%20that%0Adecouple%20reasoning%20from%20perception%2C%20leading%20to%20either%20information%20loss%20through%0Apremature%20visual%20abstraction%20or%20computational%20inefficiency%20through%20exhaustive%0Aprocessing.%20The%20core%20limitation%20lies%20in%20the%20inability%20to%20adapt%20visual%0Aextraction%20to%20specific%20reasoning%20requirements%2C%20different%20queries%20demand%0Afundamentally%20different%20visual%20evidence%20from%20the%20same%20video%20content.%20In%20this%0Awork%2C%20we%20present%20CAVIA%2C%20a%20training-free%20framework%20that%20revolutionizes%20video%0Aunderstanding%20through%20reasoning%2C%20perception%20coordination.%20Unlike%20conventional%0Aapproaches%20where%20visual%20processing%20operates%20independently%20of%20reasoning%2C%20CAVIA%0Acreates%20a%20closed-loop%20system%20where%20reasoning%20continuously%20guides%20visual%0Aextraction%20based%20on%20identified%20information%20gaps.%20CAVIA%20introduces%20three%0Ainnovations%3A%20%281%29%20hierarchical%20reasoning%2C%20guided%20localization%20to%20precise%20frames%3B%0A%282%29%20cross-modal%20semantic%20bridging%20for%20targeted%20extraction%3B%20%283%29%0Aconfidence-driven%20iterative%20synthesis.%20CAVIA%20achieves%20state-of-the-art%0Aperformance%20on%20challenging%20benchmarks%3A%20EgoSchema%20%2865.7%25%2C%20%2B5.3%25%29%2C%20NExT-QA%0A%2876.1%25%2C%20%2B2.6%25%29%2C%20and%20IntentQA%20%2873.8%25%2C%20%2B6.9%25%29%2C%20demonstrating%20that%20dynamic%0Areasoning-perception%20coordination%20provides%20a%20scalable%20paradigm%20for%20video%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520What%2520You%2520Need%253A%2520Query-Aware%2520Visual%2520Intelligence%2520through%250A%2520%2520Reasoning-Perception%2520Loops%26entry.906535625%3DZixuan%2520Dong%2520and%2520Baoyun%2520Peng%2520and%2520Yufei%2520Wang%2520and%2520Lin%2520Liu%2520and%2520Xinxin%2520Dong%2520and%2520Yunlong%2520Cao%2520and%2520Xiaodong%2520Wang%26entry.1292438233%3D%2520%2520Human%2520video%2520comprehension%2520demonstrates%2520dynamic%2520coordination%2520between%2520reasoning%250Aand%2520visual%2520attention%252C%2520adaptively%2520focusing%2520on%2520query-relevant%2520details.%2520However%252C%250Acurrent%2520long-form%2520video%2520question%2520answering%2520systems%2520employ%2520rigid%2520pipelines%2520that%250Adecouple%2520reasoning%2520from%2520perception%252C%2520leading%2520to%2520either%2520information%2520loss%2520through%250Apremature%2520visual%2520abstraction%2520or%2520computational%2520inefficiency%2520through%2520exhaustive%250Aprocessing.%2520The%2520core%2520limitation%2520lies%2520in%2520the%2520inability%2520to%2520adapt%2520visual%250Aextraction%2520to%2520specific%2520reasoning%2520requirements%252C%2520different%2520queries%2520demand%250Afundamentally%2520different%2520visual%2520evidence%2520from%2520the%2520same%2520video%2520content.%2520In%2520this%250Awork%252C%2520we%2520present%2520CAVIA%252C%2520a%2520training-free%2520framework%2520that%2520revolutionizes%2520video%250Aunderstanding%2520through%2520reasoning%252C%2520perception%2520coordination.%2520Unlike%2520conventional%250Aapproaches%2520where%2520visual%2520processing%2520operates%2520independently%2520of%2520reasoning%252C%2520CAVIA%250Acreates%2520a%2520closed-loop%2520system%2520where%2520reasoning%2520continuously%2520guides%2520visual%250Aextraction%2520based%2520on%2520identified%2520information%2520gaps.%2520CAVIA%2520introduces%2520three%250Ainnovations%253A%2520%25281%2529%2520hierarchical%2520reasoning%252C%2520guided%2520localization%2520to%2520precise%2520frames%253B%250A%25282%2529%2520cross-modal%2520semantic%2520bridging%2520for%2520targeted%2520extraction%253B%2520%25283%2529%250Aconfidence-driven%2520iterative%2520synthesis.%2520CAVIA%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520challenging%2520benchmarks%253A%2520EgoSchema%2520%252865.7%2525%252C%2520%252B5.3%2525%2529%252C%2520NExT-QA%250A%252876.1%2525%252C%2520%252B2.6%2525%2529%252C%2520and%2520IntentQA%2520%252873.8%2525%252C%2520%252B6.9%2525%2529%252C%2520demonstrating%2520that%2520dynamic%250Areasoning-perception%2520coordination%2520provides%2520a%2520scalable%2520paradigm%2520for%2520video%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20What%20You%20Need%3A%20Query-Aware%20Visual%20Intelligence%20through%0A%20%20Reasoning-Perception%20Loops&entry.906535625=Zixuan%20Dong%20and%20Baoyun%20Peng%20and%20Yufei%20Wang%20and%20Lin%20Liu%20and%20Xinxin%20Dong%20and%20Yunlong%20Cao%20and%20Xiaodong%20Wang&entry.1292438233=%20%20Human%20video%20comprehension%20demonstrates%20dynamic%20coordination%20between%20reasoning%0Aand%20visual%20attention%2C%20adaptively%20focusing%20on%20query-relevant%20details.%20However%2C%0Acurrent%20long-form%20video%20question%20answering%20systems%20employ%20rigid%20pipelines%20that%0Adecouple%20reasoning%20from%20perception%2C%20leading%20to%20either%20information%20loss%20through%0Apremature%20visual%20abstraction%20or%20computational%20inefficiency%20through%20exhaustive%0Aprocessing.%20The%20core%20limitation%20lies%20in%20the%20inability%20to%20adapt%20visual%0Aextraction%20to%20specific%20reasoning%20requirements%2C%20different%20queries%20demand%0Afundamentally%20different%20visual%20evidence%20from%20the%20same%20video%20content.%20In%20this%0Awork%2C%20we%20present%20CAVIA%2C%20a%20training-free%20framework%20that%20revolutionizes%20video%0Aunderstanding%20through%20reasoning%2C%20perception%20coordination.%20Unlike%20conventional%0Aapproaches%20where%20visual%20processing%20operates%20independently%20of%20reasoning%2C%20CAVIA%0Acreates%20a%20closed-loop%20system%20where%20reasoning%20continuously%20guides%20visual%0Aextraction%20based%20on%20identified%20information%20gaps.%20CAVIA%20introduces%20three%0Ainnovations%3A%20%281%29%20hierarchical%20reasoning%2C%20guided%20localization%20to%20precise%20frames%3B%0A%282%29%20cross-modal%20semantic%20bridging%20for%20targeted%20extraction%3B%20%283%29%0Aconfidence-driven%20iterative%20synthesis.%20CAVIA%20achieves%20state-of-the-art%0Aperformance%20on%20challenging%20benchmarks%3A%20EgoSchema%20%2865.7%25%2C%20%2B5.3%25%29%2C%20NExT-QA%0A%2876.1%25%2C%20%2B2.6%25%29%2C%20and%20IntentQA%20%2873.8%25%2C%20%2B6.9%25%29%2C%20demonstrating%20that%20dynamic%0Areasoning-perception%20coordination%20provides%20a%20scalable%20paradigm%20for%20video%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17932v1&entry.124074799=Read"},
{"title": "One Framework to Rule Them All: Unifying Multimodal Tasks with LLM\n  Neural-Tuning", "author": "Hao Sun and Yu Song and Jiaqing Liu and Jihong Hu and Yen-Wei Chen and Lanfen Lin", "abstract": "  Large-scale models have exhibited remarkable capabilities across diverse\ndomains, including automated medical services and intelligent customer support.\nHowever, as most large models are trained on single-modality corpora, enabling\nthem to effectively process and understand multimodal signals remains a\nsignificant challenge. Current research often focuses on designing\ntask-specific or scenario-specific tuning strategies, which limits the\nscalability and versatility. To address this limitation, we propose a unified\nframework that concurrently handles multiple tasks and modalities. In this\nframework, all modalities and tasks are represented as unified tokens and\ntrained using a single, consistent approach. To enable efficient multitask\nprocessing, we introduce a novel tuning strategy termed neural tuning, inspired\nby the concept of sparse distributed representation in the human brain, where\nonly specific subsets of neurons are activated for each task. Furthermore, to\nadvance research in multimodal and multitask learning, we present a new\nbenchmark, MMUD, which includes samples annotated with multiple task labels\nspanning reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. By applying neural tuning to pretrained large models\non the MMUD benchmark, we demonstrate the ability to handle multiple tasks\nsimultaneously in a streamlined and efficient manner. All models, code, and\ndatasets will be released publicly upon publication, fostering further research\nand innovation in this field.\n", "link": "http://arxiv.org/abs/2408.03001v3", "date": "2025-08-25", "relevancy": 2.8908, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Framework%20to%20Rule%20Them%20All%3A%20Unifying%20Multimodal%20Tasks%20with%20LLM%0A%20%20Neural-Tuning&body=Title%3A%20One%20Framework%20to%20Rule%20Them%20All%3A%20Unifying%20Multimodal%20Tasks%20with%20LLM%0A%20%20Neural-Tuning%0AAuthor%3A%20Hao%20Sun%20and%20Yu%20Song%20and%20Jiaqing%20Liu%20and%20Jihong%20Hu%20and%20Yen-Wei%20Chen%20and%20Lanfen%20Lin%0AAbstract%3A%20%20%20Large-scale%20models%20have%20exhibited%20remarkable%20capabilities%20across%20diverse%0Adomains%2C%20including%20automated%20medical%20services%20and%20intelligent%20customer%20support.%0AHowever%2C%20as%20most%20large%20models%20are%20trained%20on%20single-modality%20corpora%2C%20enabling%0Athem%20to%20effectively%20process%20and%20understand%20multimodal%20signals%20remains%20a%0Asignificant%20challenge.%20Current%20research%20often%20focuses%20on%20designing%0Atask-specific%20or%20scenario-specific%20tuning%20strategies%2C%20which%20limits%20the%0Ascalability%20and%20versatility.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20unified%0Aframework%20that%20concurrently%20handles%20multiple%20tasks%20and%20modalities.%20In%20this%0Aframework%2C%20all%20modalities%20and%20tasks%20are%20represented%20as%20unified%20tokens%20and%0Atrained%20using%20a%20single%2C%20consistent%20approach.%20To%20enable%20efficient%20multitask%0Aprocessing%2C%20we%20introduce%20a%20novel%20tuning%20strategy%20termed%20neural%20tuning%2C%20inspired%0Aby%20the%20concept%20of%20sparse%20distributed%20representation%20in%20the%20human%20brain%2C%20where%0Aonly%20specific%20subsets%20of%20neurons%20are%20activated%20for%20each%20task.%20Furthermore%2C%20to%0Aadvance%20research%20in%20multimodal%20and%20multitask%20learning%2C%20we%20present%20a%20new%0Abenchmark%2C%20MMUD%2C%20which%20includes%20samples%20annotated%20with%20multiple%20task%20labels%0Aspanning%20reasoning%20segmentation%2C%20referring%20segmentation%2C%20image%20captioning%2C%20and%0Atext-to-image%20generation.%20By%20applying%20neural%20tuning%20to%20pretrained%20large%20models%0Aon%20the%20MMUD%20benchmark%2C%20we%20demonstrate%20the%20ability%20to%20handle%20multiple%20tasks%0Asimultaneously%20in%20a%20streamlined%20and%20efficient%20manner.%20All%20models%2C%20code%2C%20and%0Adatasets%20will%20be%20released%20publicly%20upon%20publication%2C%20fostering%20further%20research%0Aand%20innovation%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Framework%2520to%2520Rule%2520Them%2520All%253A%2520Unifying%2520Multimodal%2520Tasks%2520with%2520LLM%250A%2520%2520Neural-Tuning%26entry.906535625%3DHao%2520Sun%2520and%2520Yu%2520Song%2520and%2520Jiaqing%2520Liu%2520and%2520Jihong%2520Hu%2520and%2520Yen-Wei%2520Chen%2520and%2520Lanfen%2520Lin%26entry.1292438233%3D%2520%2520Large-scale%2520models%2520have%2520exhibited%2520remarkable%2520capabilities%2520across%2520diverse%250Adomains%252C%2520including%2520automated%2520medical%2520services%2520and%2520intelligent%2520customer%2520support.%250AHowever%252C%2520as%2520most%2520large%2520models%2520are%2520trained%2520on%2520single-modality%2520corpora%252C%2520enabling%250Athem%2520to%2520effectively%2520process%2520and%2520understand%2520multimodal%2520signals%2520remains%2520a%250Asignificant%2520challenge.%2520Current%2520research%2520often%2520focuses%2520on%2520designing%250Atask-specific%2520or%2520scenario-specific%2520tuning%2520strategies%252C%2520which%2520limits%2520the%250Ascalability%2520and%2520versatility.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520unified%250Aframework%2520that%2520concurrently%2520handles%2520multiple%2520tasks%2520and%2520modalities.%2520In%2520this%250Aframework%252C%2520all%2520modalities%2520and%2520tasks%2520are%2520represented%2520as%2520unified%2520tokens%2520and%250Atrained%2520using%2520a%2520single%252C%2520consistent%2520approach.%2520To%2520enable%2520efficient%2520multitask%250Aprocessing%252C%2520we%2520introduce%2520a%2520novel%2520tuning%2520strategy%2520termed%2520neural%2520tuning%252C%2520inspired%250Aby%2520the%2520concept%2520of%2520sparse%2520distributed%2520representation%2520in%2520the%2520human%2520brain%252C%2520where%250Aonly%2520specific%2520subsets%2520of%2520neurons%2520are%2520activated%2520for%2520each%2520task.%2520Furthermore%252C%2520to%250Aadvance%2520research%2520in%2520multimodal%2520and%2520multitask%2520learning%252C%2520we%2520present%2520a%2520new%250Abenchmark%252C%2520MMUD%252C%2520which%2520includes%2520samples%2520annotated%2520with%2520multiple%2520task%2520labels%250Aspanning%2520reasoning%2520segmentation%252C%2520referring%2520segmentation%252C%2520image%2520captioning%252C%2520and%250Atext-to-image%2520generation.%2520By%2520applying%2520neural%2520tuning%2520to%2520pretrained%2520large%2520models%250Aon%2520the%2520MMUD%2520benchmark%252C%2520we%2520demonstrate%2520the%2520ability%2520to%2520handle%2520multiple%2520tasks%250Asimultaneously%2520in%2520a%2520streamlined%2520and%2520efficient%2520manner.%2520All%2520models%252C%2520code%252C%2520and%250Adatasets%2520will%2520be%2520released%2520publicly%2520upon%2520publication%252C%2520fostering%2520further%2520research%250Aand%2520innovation%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Framework%20to%20Rule%20Them%20All%3A%20Unifying%20Multimodal%20Tasks%20with%20LLM%0A%20%20Neural-Tuning&entry.906535625=Hao%20Sun%20and%20Yu%20Song%20and%20Jiaqing%20Liu%20and%20Jihong%20Hu%20and%20Yen-Wei%20Chen%20and%20Lanfen%20Lin&entry.1292438233=%20%20Large-scale%20models%20have%20exhibited%20remarkable%20capabilities%20across%20diverse%0Adomains%2C%20including%20automated%20medical%20services%20and%20intelligent%20customer%20support.%0AHowever%2C%20as%20most%20large%20models%20are%20trained%20on%20single-modality%20corpora%2C%20enabling%0Athem%20to%20effectively%20process%20and%20understand%20multimodal%20signals%20remains%20a%0Asignificant%20challenge.%20Current%20research%20often%20focuses%20on%20designing%0Atask-specific%20or%20scenario-specific%20tuning%20strategies%2C%20which%20limits%20the%0Ascalability%20and%20versatility.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20unified%0Aframework%20that%20concurrently%20handles%20multiple%20tasks%20and%20modalities.%20In%20this%0Aframework%2C%20all%20modalities%20and%20tasks%20are%20represented%20as%20unified%20tokens%20and%0Atrained%20using%20a%20single%2C%20consistent%20approach.%20To%20enable%20efficient%20multitask%0Aprocessing%2C%20we%20introduce%20a%20novel%20tuning%20strategy%20termed%20neural%20tuning%2C%20inspired%0Aby%20the%20concept%20of%20sparse%20distributed%20representation%20in%20the%20human%20brain%2C%20where%0Aonly%20specific%20subsets%20of%20neurons%20are%20activated%20for%20each%20task.%20Furthermore%2C%20to%0Aadvance%20research%20in%20multimodal%20and%20multitask%20learning%2C%20we%20present%20a%20new%0Abenchmark%2C%20MMUD%2C%20which%20includes%20samples%20annotated%20with%20multiple%20task%20labels%0Aspanning%20reasoning%20segmentation%2C%20referring%20segmentation%2C%20image%20captioning%2C%20and%0Atext-to-image%20generation.%20By%20applying%20neural%20tuning%20to%20pretrained%20large%20models%0Aon%20the%20MMUD%20benchmark%2C%20we%20demonstrate%20the%20ability%20to%20handle%20multiple%20tasks%0Asimultaneously%20in%20a%20streamlined%20and%20efficient%20manner.%20All%20models%2C%20code%2C%20and%0Adatasets%20will%20be%20released%20publicly%20upon%20publication%2C%20fostering%20further%20research%0Aand%20innovation%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03001v3&entry.124074799=Read"},
{"title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind", "author": "William Rudman and Michal Golovanevsky and Amir Bar and Vedant Palit and Yann LeCun and Carsten Eickhoff and Ritambhara Singh", "abstract": "  Despite strong performance on vision-language tasks, Multimodal Large\nLanguage Models (MLLMs) struggle with mathematical problem-solving, with both\nopen-source and state-of-the-art models falling short of human performance on\nvisual-math benchmarks. To systematically examine visual-mathematical reasoning\nin MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test\nmulti-step reasoning, and (3) explore a potential solution to improve visual\nreasoning capabilities. Our findings reveal fundamental shortcomings in shape\nrecognition, with top models achieving under 50% accuracy in identifying\nregular polygons. We analyze these failures through the lens of dual-process\ntheory and show that MLLMs rely on System 1 (intuitive, memorized associations)\nrather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count\nthe sides of both familiar and novel shapes, suggesting they have neither\nlearned the concept of sides nor effectively process visual inputs. Finally, we\npropose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances\nmulti-step mathematical reasoning by explicitly referencing visual annotations\nin diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting\ntask from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs\nremains an open problem, and visually-guided prompting is essential for\nsuccessfully engaging visual reasoning. Code available at:\nhttps://github.com/rsinghlab/Shape-Blind.\n", "link": "http://arxiv.org/abs/2502.15969v4", "date": "2025-08-25", "relevancy": 2.8675, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forgotten%20Polygons%3A%20Multimodal%20Large%20Language%20Models%20are%20Shape-Blind&body=Title%3A%20Forgotten%20Polygons%3A%20Multimodal%20Large%20Language%20Models%20are%20Shape-Blind%0AAuthor%3A%20William%20Rudman%20and%20Michal%20Golovanevsky%20and%20Amir%20Bar%20and%20Vedant%20Palit%20and%20Yann%20LeCun%20and%20Carsten%20Eickhoff%20and%20Ritambhara%20Singh%0AAbstract%3A%20%20%20Despite%20strong%20performance%20on%20vision-language%20tasks%2C%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20struggle%20with%20mathematical%20problem-solving%2C%20with%20both%0Aopen-source%20and%20state-of-the-art%20models%20falling%20short%20of%20human%20performance%20on%0Avisual-math%20benchmarks.%20To%20systematically%20examine%20visual-mathematical%20reasoning%0Ain%20MLLMs%2C%20we%20%281%29%20evaluate%20their%20understanding%20of%20geometric%20primitives%2C%20%282%29%20test%0Amulti-step%20reasoning%2C%20and%20%283%29%20explore%20a%20potential%20solution%20to%20improve%20visual%0Areasoning%20capabilities.%20Our%20findings%20reveal%20fundamental%20shortcomings%20in%20shape%0Arecognition%2C%20with%20top%20models%20achieving%20under%2050%25%20accuracy%20in%20identifying%0Aregular%20polygons.%20We%20analyze%20these%20failures%20through%20the%20lens%20of%20dual-process%0Atheory%20and%20show%20that%20MLLMs%20rely%20on%20System%201%20%28intuitive%2C%20memorized%20associations%29%0Arather%20than%20System%202%20%28deliberate%20reasoning%29.%20Consequently%2C%20MLLMs%20fail%20to%20count%0Athe%20sides%20of%20both%20familiar%20and%20novel%20shapes%2C%20suggesting%20they%20have%20neither%0Alearned%20the%20concept%20of%20sides%20nor%20effectively%20process%20visual%20inputs.%20Finally%2C%20we%0Apropose%20Visually%20Cued%20Chain-of-Thought%20%28VC-CoT%29%20prompting%2C%20which%20enhances%0Amulti-step%20mathematical%20reasoning%20by%20explicitly%20referencing%20visual%20annotations%0Ain%20diagrams%2C%20boosting%20GPT-4o%27s%20accuracy%20on%20an%20irregular%20polygon%20side-counting%0Atask%20from%207%25%20to%2093%25.%20Our%20findings%20suggest%20that%20System%202%20reasoning%20in%20MLLMs%0Aremains%20an%20open%20problem%2C%20and%20visually-guided%20prompting%20is%20essential%20for%0Asuccessfully%20engaging%20visual%20reasoning.%20Code%20available%20at%3A%0Ahttps%3A//github.com/rsinghlab/Shape-Blind.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15969v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForgotten%2520Polygons%253A%2520Multimodal%2520Large%2520Language%2520Models%2520are%2520Shape-Blind%26entry.906535625%3DWilliam%2520Rudman%2520and%2520Michal%2520Golovanevsky%2520and%2520Amir%2520Bar%2520and%2520Vedant%2520Palit%2520and%2520Yann%2520LeCun%2520and%2520Carsten%2520Eickhoff%2520and%2520Ritambhara%2520Singh%26entry.1292438233%3D%2520%2520Despite%2520strong%2520performance%2520on%2520vision-language%2520tasks%252C%2520Multimodal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520struggle%2520with%2520mathematical%2520problem-solving%252C%2520with%2520both%250Aopen-source%2520and%2520state-of-the-art%2520models%2520falling%2520short%2520of%2520human%2520performance%2520on%250Avisual-math%2520benchmarks.%2520To%2520systematically%2520examine%2520visual-mathematical%2520reasoning%250Ain%2520MLLMs%252C%2520we%2520%25281%2529%2520evaluate%2520their%2520understanding%2520of%2520geometric%2520primitives%252C%2520%25282%2529%2520test%250Amulti-step%2520reasoning%252C%2520and%2520%25283%2529%2520explore%2520a%2520potential%2520solution%2520to%2520improve%2520visual%250Areasoning%2520capabilities.%2520Our%2520findings%2520reveal%2520fundamental%2520shortcomings%2520in%2520shape%250Arecognition%252C%2520with%2520top%2520models%2520achieving%2520under%252050%2525%2520accuracy%2520in%2520identifying%250Aregular%2520polygons.%2520We%2520analyze%2520these%2520failures%2520through%2520the%2520lens%2520of%2520dual-process%250Atheory%2520and%2520show%2520that%2520MLLMs%2520rely%2520on%2520System%25201%2520%2528intuitive%252C%2520memorized%2520associations%2529%250Arather%2520than%2520System%25202%2520%2528deliberate%2520reasoning%2529.%2520Consequently%252C%2520MLLMs%2520fail%2520to%2520count%250Athe%2520sides%2520of%2520both%2520familiar%2520and%2520novel%2520shapes%252C%2520suggesting%2520they%2520have%2520neither%250Alearned%2520the%2520concept%2520of%2520sides%2520nor%2520effectively%2520process%2520visual%2520inputs.%2520Finally%252C%2520we%250Apropose%2520Visually%2520Cued%2520Chain-of-Thought%2520%2528VC-CoT%2529%2520prompting%252C%2520which%2520enhances%250Amulti-step%2520mathematical%2520reasoning%2520by%2520explicitly%2520referencing%2520visual%2520annotations%250Ain%2520diagrams%252C%2520boosting%2520GPT-4o%2527s%2520accuracy%2520on%2520an%2520irregular%2520polygon%2520side-counting%250Atask%2520from%25207%2525%2520to%252093%2525.%2520Our%2520findings%2520suggest%2520that%2520System%25202%2520reasoning%2520in%2520MLLMs%250Aremains%2520an%2520open%2520problem%252C%2520and%2520visually-guided%2520prompting%2520is%2520essential%2520for%250Asuccessfully%2520engaging%2520visual%2520reasoning.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/rsinghlab/Shape-Blind.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15969v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forgotten%20Polygons%3A%20Multimodal%20Large%20Language%20Models%20are%20Shape-Blind&entry.906535625=William%20Rudman%20and%20Michal%20Golovanevsky%20and%20Amir%20Bar%20and%20Vedant%20Palit%20and%20Yann%20LeCun%20and%20Carsten%20Eickhoff%20and%20Ritambhara%20Singh&entry.1292438233=%20%20Despite%20strong%20performance%20on%20vision-language%20tasks%2C%20Multimodal%20Large%0ALanguage%20Models%20%28MLLMs%29%20struggle%20with%20mathematical%20problem-solving%2C%20with%20both%0Aopen-source%20and%20state-of-the-art%20models%20falling%20short%20of%20human%20performance%20on%0Avisual-math%20benchmarks.%20To%20systematically%20examine%20visual-mathematical%20reasoning%0Ain%20MLLMs%2C%20we%20%281%29%20evaluate%20their%20understanding%20of%20geometric%20primitives%2C%20%282%29%20test%0Amulti-step%20reasoning%2C%20and%20%283%29%20explore%20a%20potential%20solution%20to%20improve%20visual%0Areasoning%20capabilities.%20Our%20findings%20reveal%20fundamental%20shortcomings%20in%20shape%0Arecognition%2C%20with%20top%20models%20achieving%20under%2050%25%20accuracy%20in%20identifying%0Aregular%20polygons.%20We%20analyze%20these%20failures%20through%20the%20lens%20of%20dual-process%0Atheory%20and%20show%20that%20MLLMs%20rely%20on%20System%201%20%28intuitive%2C%20memorized%20associations%29%0Arather%20than%20System%202%20%28deliberate%20reasoning%29.%20Consequently%2C%20MLLMs%20fail%20to%20count%0Athe%20sides%20of%20both%20familiar%20and%20novel%20shapes%2C%20suggesting%20they%20have%20neither%0Alearned%20the%20concept%20of%20sides%20nor%20effectively%20process%20visual%20inputs.%20Finally%2C%20we%0Apropose%20Visually%20Cued%20Chain-of-Thought%20%28VC-CoT%29%20prompting%2C%20which%20enhances%0Amulti-step%20mathematical%20reasoning%20by%20explicitly%20referencing%20visual%20annotations%0Ain%20diagrams%2C%20boosting%20GPT-4o%27s%20accuracy%20on%20an%20irregular%20polygon%20side-counting%0Atask%20from%207%25%20to%2093%25.%20Our%20findings%20suggest%20that%20System%202%20reasoning%20in%20MLLMs%0Aremains%20an%20open%20problem%2C%20and%20visually-guided%20prompting%20is%20essential%20for%0Asuccessfully%20engaging%20visual%20reasoning.%20Code%20available%20at%3A%0Ahttps%3A//github.com/rsinghlab/Shape-Blind.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15969v4&entry.124074799=Read"},
{"title": "Propose and Rectify: A Forensics-Driven MLLM Framework for Image\n  Manipulation Localization", "author": "Keyang Zhang and Chenqi Kong and Hui Liu and Bo Ding and Xinghao Jiang and Haoliang Li", "abstract": "  The increasing sophistication of image manipulation techniques demands robust\nforensic solutions that can both reliably detect alterations and precisely\nlocalize tampered regions. Recent Multimodal Large Language Models (MLLMs) show\npromise by leveraging world knowledge and semantic understanding for\ncontext-aware detection, yet they struggle with perceiving subtle, low-level\nforensic artifacts crucial for accurate manipulation localization. This paper\npresents a novel Propose-Rectify framework that effectively bridges semantic\nreasoning with forensic-specific analysis. In the proposal stage, our approach\nutilizes a forensic-adapted LLaVA model to generate initial manipulation\nanalysis and preliminary localization of suspicious regions based on semantic\nunderstanding and contextual reasoning. In the rectification stage, we\nintroduce a Forensics Rectification Module that systematically validates and\nrefines these initial proposals through multi-scale forensic feature analysis,\nintegrating technical evidence from several specialized filters. Additionally,\nwe present an Enhanced Segmentation Module that incorporates critical forensic\ncues into SAM's encoded image embeddings, thereby overcoming inherent semantic\nbiases to achieve precise delineation of manipulated regions. By\nsynergistically combining advanced multimodal reasoning with established\nforensic methodologies, our framework ensures that initial semantic proposals\nare systematically validated and enhanced through concrete technical evidence,\nresulting in comprehensive detection accuracy and localization precision.\nExtensive experimental validation demonstrates state-of-the-art performance\nacross diverse datasets with exceptional robustness and generalization\ncapabilities.\n", "link": "http://arxiv.org/abs/2508.17976v1", "date": "2025-08-25", "relevancy": 2.8352, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Propose%20and%20Rectify%3A%20A%20Forensics-Driven%20MLLM%20Framework%20for%20Image%0A%20%20Manipulation%20Localization&body=Title%3A%20Propose%20and%20Rectify%3A%20A%20Forensics-Driven%20MLLM%20Framework%20for%20Image%0A%20%20Manipulation%20Localization%0AAuthor%3A%20Keyang%20Zhang%20and%20Chenqi%20Kong%20and%20Hui%20Liu%20and%20Bo%20Ding%20and%20Xinghao%20Jiang%20and%20Haoliang%20Li%0AAbstract%3A%20%20%20The%20increasing%20sophistication%20of%20image%20manipulation%20techniques%20demands%20robust%0Aforensic%20solutions%20that%20can%20both%20reliably%20detect%20alterations%20and%20precisely%0Alocalize%20tampered%20regions.%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%0Apromise%20by%20leveraging%20world%20knowledge%20and%20semantic%20understanding%20for%0Acontext-aware%20detection%2C%20yet%20they%20struggle%20with%20perceiving%20subtle%2C%20low-level%0Aforensic%20artifacts%20crucial%20for%20accurate%20manipulation%20localization.%20This%20paper%0Apresents%20a%20novel%20Propose-Rectify%20framework%20that%20effectively%20bridges%20semantic%0Areasoning%20with%20forensic-specific%20analysis.%20In%20the%20proposal%20stage%2C%20our%20approach%0Autilizes%20a%20forensic-adapted%20LLaVA%20model%20to%20generate%20initial%20manipulation%0Aanalysis%20and%20preliminary%20localization%20of%20suspicious%20regions%20based%20on%20semantic%0Aunderstanding%20and%20contextual%20reasoning.%20In%20the%20rectification%20stage%2C%20we%0Aintroduce%20a%20Forensics%20Rectification%20Module%20that%20systematically%20validates%20and%0Arefines%20these%20initial%20proposals%20through%20multi-scale%20forensic%20feature%20analysis%2C%0Aintegrating%20technical%20evidence%20from%20several%20specialized%20filters.%20Additionally%2C%0Awe%20present%20an%20Enhanced%20Segmentation%20Module%20that%20incorporates%20critical%20forensic%0Acues%20into%20SAM%27s%20encoded%20image%20embeddings%2C%20thereby%20overcoming%20inherent%20semantic%0Abiases%20to%20achieve%20precise%20delineation%20of%20manipulated%20regions.%20By%0Asynergistically%20combining%20advanced%20multimodal%20reasoning%20with%20established%0Aforensic%20methodologies%2C%20our%20framework%20ensures%20that%20initial%20semantic%20proposals%0Aare%20systematically%20validated%20and%20enhanced%20through%20concrete%20technical%20evidence%2C%0Aresulting%20in%20comprehensive%20detection%20accuracy%20and%20localization%20precision.%0AExtensive%20experimental%20validation%20demonstrates%20state-of-the-art%20performance%0Aacross%20diverse%20datasets%20with%20exceptional%20robustness%20and%20generalization%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPropose%2520and%2520Rectify%253A%2520A%2520Forensics-Driven%2520MLLM%2520Framework%2520for%2520Image%250A%2520%2520Manipulation%2520Localization%26entry.906535625%3DKeyang%2520Zhang%2520and%2520Chenqi%2520Kong%2520and%2520Hui%2520Liu%2520and%2520Bo%2520Ding%2520and%2520Xinghao%2520Jiang%2520and%2520Haoliang%2520Li%26entry.1292438233%3D%2520%2520The%2520increasing%2520sophistication%2520of%2520image%2520manipulation%2520techniques%2520demands%2520robust%250Aforensic%2520solutions%2520that%2520can%2520both%2520reliably%2520detect%2520alterations%2520and%2520precisely%250Alocalize%2520tampered%2520regions.%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520show%250Apromise%2520by%2520leveraging%2520world%2520knowledge%2520and%2520semantic%2520understanding%2520for%250Acontext-aware%2520detection%252C%2520yet%2520they%2520struggle%2520with%2520perceiving%2520subtle%252C%2520low-level%250Aforensic%2520artifacts%2520crucial%2520for%2520accurate%2520manipulation%2520localization.%2520This%2520paper%250Apresents%2520a%2520novel%2520Propose-Rectify%2520framework%2520that%2520effectively%2520bridges%2520semantic%250Areasoning%2520with%2520forensic-specific%2520analysis.%2520In%2520the%2520proposal%2520stage%252C%2520our%2520approach%250Autilizes%2520a%2520forensic-adapted%2520LLaVA%2520model%2520to%2520generate%2520initial%2520manipulation%250Aanalysis%2520and%2520preliminary%2520localization%2520of%2520suspicious%2520regions%2520based%2520on%2520semantic%250Aunderstanding%2520and%2520contextual%2520reasoning.%2520In%2520the%2520rectification%2520stage%252C%2520we%250Aintroduce%2520a%2520Forensics%2520Rectification%2520Module%2520that%2520systematically%2520validates%2520and%250Arefines%2520these%2520initial%2520proposals%2520through%2520multi-scale%2520forensic%2520feature%2520analysis%252C%250Aintegrating%2520technical%2520evidence%2520from%2520several%2520specialized%2520filters.%2520Additionally%252C%250Awe%2520present%2520an%2520Enhanced%2520Segmentation%2520Module%2520that%2520incorporates%2520critical%2520forensic%250Acues%2520into%2520SAM%2527s%2520encoded%2520image%2520embeddings%252C%2520thereby%2520overcoming%2520inherent%2520semantic%250Abiases%2520to%2520achieve%2520precise%2520delineation%2520of%2520manipulated%2520regions.%2520By%250Asynergistically%2520combining%2520advanced%2520multimodal%2520reasoning%2520with%2520established%250Aforensic%2520methodologies%252C%2520our%2520framework%2520ensures%2520that%2520initial%2520semantic%2520proposals%250Aare%2520systematically%2520validated%2520and%2520enhanced%2520through%2520concrete%2520technical%2520evidence%252C%250Aresulting%2520in%2520comprehensive%2520detection%2520accuracy%2520and%2520localization%2520precision.%250AExtensive%2520experimental%2520validation%2520demonstrates%2520state-of-the-art%2520performance%250Aacross%2520diverse%2520datasets%2520with%2520exceptional%2520robustness%2520and%2520generalization%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Propose%20and%20Rectify%3A%20A%20Forensics-Driven%20MLLM%20Framework%20for%20Image%0A%20%20Manipulation%20Localization&entry.906535625=Keyang%20Zhang%20and%20Chenqi%20Kong%20and%20Hui%20Liu%20and%20Bo%20Ding%20and%20Xinghao%20Jiang%20and%20Haoliang%20Li&entry.1292438233=%20%20The%20increasing%20sophistication%20of%20image%20manipulation%20techniques%20demands%20robust%0Aforensic%20solutions%20that%20can%20both%20reliably%20detect%20alterations%20and%20precisely%0Alocalize%20tampered%20regions.%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%0Apromise%20by%20leveraging%20world%20knowledge%20and%20semantic%20understanding%20for%0Acontext-aware%20detection%2C%20yet%20they%20struggle%20with%20perceiving%20subtle%2C%20low-level%0Aforensic%20artifacts%20crucial%20for%20accurate%20manipulation%20localization.%20This%20paper%0Apresents%20a%20novel%20Propose-Rectify%20framework%20that%20effectively%20bridges%20semantic%0Areasoning%20with%20forensic-specific%20analysis.%20In%20the%20proposal%20stage%2C%20our%20approach%0Autilizes%20a%20forensic-adapted%20LLaVA%20model%20to%20generate%20initial%20manipulation%0Aanalysis%20and%20preliminary%20localization%20of%20suspicious%20regions%20based%20on%20semantic%0Aunderstanding%20and%20contextual%20reasoning.%20In%20the%20rectification%20stage%2C%20we%0Aintroduce%20a%20Forensics%20Rectification%20Module%20that%20systematically%20validates%20and%0Arefines%20these%20initial%20proposals%20through%20multi-scale%20forensic%20feature%20analysis%2C%0Aintegrating%20technical%20evidence%20from%20several%20specialized%20filters.%20Additionally%2C%0Awe%20present%20an%20Enhanced%20Segmentation%20Module%20that%20incorporates%20critical%20forensic%0Acues%20into%20SAM%27s%20encoded%20image%20embeddings%2C%20thereby%20overcoming%20inherent%20semantic%0Abiases%20to%20achieve%20precise%20delineation%20of%20manipulated%20regions.%20By%0Asynergistically%20combining%20advanced%20multimodal%20reasoning%20with%20established%0Aforensic%20methodologies%2C%20our%20framework%20ensures%20that%20initial%20semantic%20proposals%0Aare%20systematically%20validated%20and%20enhanced%20through%20concrete%20technical%20evidence%2C%0Aresulting%20in%20comprehensive%20detection%20accuracy%20and%20localization%20precision.%0AExtensive%20experimental%20validation%20demonstrates%20state-of-the-art%20performance%0Aacross%20diverse%20datasets%20with%20exceptional%20robustness%20and%20generalization%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17976v1&entry.124074799=Read"},
{"title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs", "author": "Sixun Dong and Juhua Hu and Mian Zhang and Ming Yin and Yanjie Fu and Qi Qian", "abstract": "  Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.\n", "link": "http://arxiv.org/abs/2508.18264v1", "date": "2025-08-25", "relevancy": 2.8187, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMTok%3A%20Multimodal%20Coverage%20Maximization%20for%20Efficient%20Inference%20of%20VLMs&body=Title%3A%20MMTok%3A%20Multimodal%20Coverage%20Maximization%20for%20Efficient%20Inference%20of%20VLMs%0AAuthor%3A%20Sixun%20Dong%20and%20Juhua%20Hu%20and%20Mian%20Zhang%20and%20Ming%20Yin%20and%20Yanjie%20Fu%20and%20Qi%20Qian%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20performance%20in%0Aunderstanding%20visual%20content%20with%20language%20instruction%20by%20converting%20visual%0Ainput%20to%20vision%20tokens.%20However%2C%20redundancy%20in%20vision%20tokens%20results%20in%20the%0Adegenerated%20inference%20efficiency%20of%20VLMs.%20While%20many%20algorithms%20have%20been%0Aproposed%20to%20reduce%20the%20number%20of%20vision%20tokens%2C%20most%20of%20them%20apply%20only%0Aunimodal%20information%20%28i.e.%2C%20vision/text%29%20for%20pruning%20and%20ignore%20the%20inherent%0Amultimodal%20property%20of%20vision-language%20tasks.%20Moreover%2C%20it%20lacks%20a%20generic%0Acriterion%20that%20can%20be%20applied%20to%20different%20modalities.%20To%20mitigate%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20propose%20to%20leverage%20both%20vision%20and%20text%20tokens%20to%0Aselect%20informative%20vision%20tokens%20by%20the%20criterion%20of%20coverage.%20We%20first%0Aformulate%20the%20subset%20selection%20problem%20as%20a%20maximum%20coverage%20problem.%0AAfterward%2C%20a%20subset%20of%20vision%20tokens%20is%20optimized%20to%20cover%20the%20text%20tokens%20and%0Athe%20original%20set%20of%20vision%20tokens%2C%20simultaneously.%20Finally%2C%20a%20VLM%20agent%20can%20be%0Aadopted%20to%20further%20improve%20the%20quality%20of%20text%20tokens%20for%20guiding%20vision%0Apruning.%20The%20proposed%20method%20MMTok%20is%20extensively%20evaluated%20on%20benchmark%0Adatasets%20with%20different%20VLMs.%20The%20comparison%20illustrates%20that%20vision%20and%20text%0Ainformation%20are%20complementary%2C%20and%20combining%20multimodal%20information%20can%20surpass%0Athe%20unimodal%20baseline%20with%20a%20clear%20margin.%20Moreover%2C%20under%20the%20maximum%20coverage%0Acriterion%20on%20the%20POPE%20dataset%2C%20our%20method%20achieves%20a%201.87x%20speedup%20while%0Amaintaining%2098.7%25%20of%20the%20original%20performance%20on%20LLaVA-NeXT-13B.%20Furthermore%2C%0Awith%20only%20four%20vision%20tokens%2C%20it%20still%20preserves%2087.7%25%20of%20the%20original%0Aperformance%20on%20LLaVA-1.5-7B.%20These%20results%20highlight%20the%20effectiveness%20of%0Acoverage%20in%20token%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMTok%253A%2520Multimodal%2520Coverage%2520Maximization%2520for%2520Efficient%2520Inference%2520of%2520VLMs%26entry.906535625%3DSixun%2520Dong%2520and%2520Juhua%2520Hu%2520and%2520Mian%2520Zhang%2520and%2520Ming%2520Yin%2520and%2520Yanjie%2520Fu%2520and%2520Qi%2520Qian%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520performance%2520in%250Aunderstanding%2520visual%2520content%2520with%2520language%2520instruction%2520by%2520converting%2520visual%250Ainput%2520to%2520vision%2520tokens.%2520However%252C%2520redundancy%2520in%2520vision%2520tokens%2520results%2520in%2520the%250Adegenerated%2520inference%2520efficiency%2520of%2520VLMs.%2520While%2520many%2520algorithms%2520have%2520been%250Aproposed%2520to%2520reduce%2520the%2520number%2520of%2520vision%2520tokens%252C%2520most%2520of%2520them%2520apply%2520only%250Aunimodal%2520information%2520%2528i.e.%252C%2520vision/text%2529%2520for%2520pruning%2520and%2520ignore%2520the%2520inherent%250Amultimodal%2520property%2520of%2520vision-language%2520tasks.%2520Moreover%252C%2520it%2520lacks%2520a%2520generic%250Acriterion%2520that%2520can%2520be%2520applied%2520to%2520different%2520modalities.%2520To%2520mitigate%2520this%250Alimitation%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520to%2520leverage%2520both%2520vision%2520and%2520text%2520tokens%2520to%250Aselect%2520informative%2520vision%2520tokens%2520by%2520the%2520criterion%2520of%2520coverage.%2520We%2520first%250Aformulate%2520the%2520subset%2520selection%2520problem%2520as%2520a%2520maximum%2520coverage%2520problem.%250AAfterward%252C%2520a%2520subset%2520of%2520vision%2520tokens%2520is%2520optimized%2520to%2520cover%2520the%2520text%2520tokens%2520and%250Athe%2520original%2520set%2520of%2520vision%2520tokens%252C%2520simultaneously.%2520Finally%252C%2520a%2520VLM%2520agent%2520can%2520be%250Aadopted%2520to%2520further%2520improve%2520the%2520quality%2520of%2520text%2520tokens%2520for%2520guiding%2520vision%250Apruning.%2520The%2520proposed%2520method%2520MMTok%2520is%2520extensively%2520evaluated%2520on%2520benchmark%250Adatasets%2520with%2520different%2520VLMs.%2520The%2520comparison%2520illustrates%2520that%2520vision%2520and%2520text%250Ainformation%2520are%2520complementary%252C%2520and%2520combining%2520multimodal%2520information%2520can%2520surpass%250Athe%2520unimodal%2520baseline%2520with%2520a%2520clear%2520margin.%2520Moreover%252C%2520under%2520the%2520maximum%2520coverage%250Acriterion%2520on%2520the%2520POPE%2520dataset%252C%2520our%2520method%2520achieves%2520a%25201.87x%2520speedup%2520while%250Amaintaining%252098.7%2525%2520of%2520the%2520original%2520performance%2520on%2520LLaVA-NeXT-13B.%2520Furthermore%252C%250Awith%2520only%2520four%2520vision%2520tokens%252C%2520it%2520still%2520preserves%252087.7%2525%2520of%2520the%2520original%250Aperformance%2520on%2520LLaVA-1.5-7B.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%250Acoverage%2520in%2520token%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMTok%3A%20Multimodal%20Coverage%20Maximization%20for%20Efficient%20Inference%20of%20VLMs&entry.906535625=Sixun%20Dong%20and%20Juhua%20Hu%20and%20Mian%20Zhang%20and%20Ming%20Yin%20and%20Yanjie%20Fu%20and%20Qi%20Qian&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20impressive%20performance%20in%0Aunderstanding%20visual%20content%20with%20language%20instruction%20by%20converting%20visual%0Ainput%20to%20vision%20tokens.%20However%2C%20redundancy%20in%20vision%20tokens%20results%20in%20the%0Adegenerated%20inference%20efficiency%20of%20VLMs.%20While%20many%20algorithms%20have%20been%0Aproposed%20to%20reduce%20the%20number%20of%20vision%20tokens%2C%20most%20of%20them%20apply%20only%0Aunimodal%20information%20%28i.e.%2C%20vision/text%29%20for%20pruning%20and%20ignore%20the%20inherent%0Amultimodal%20property%20of%20vision-language%20tasks.%20Moreover%2C%20it%20lacks%20a%20generic%0Acriterion%20that%20can%20be%20applied%20to%20different%20modalities.%20To%20mitigate%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20propose%20to%20leverage%20both%20vision%20and%20text%20tokens%20to%0Aselect%20informative%20vision%20tokens%20by%20the%20criterion%20of%20coverage.%20We%20first%0Aformulate%20the%20subset%20selection%20problem%20as%20a%20maximum%20coverage%20problem.%0AAfterward%2C%20a%20subset%20of%20vision%20tokens%20is%20optimized%20to%20cover%20the%20text%20tokens%20and%0Athe%20original%20set%20of%20vision%20tokens%2C%20simultaneously.%20Finally%2C%20a%20VLM%20agent%20can%20be%0Aadopted%20to%20further%20improve%20the%20quality%20of%20text%20tokens%20for%20guiding%20vision%0Apruning.%20The%20proposed%20method%20MMTok%20is%20extensively%20evaluated%20on%20benchmark%0Adatasets%20with%20different%20VLMs.%20The%20comparison%20illustrates%20that%20vision%20and%20text%0Ainformation%20are%20complementary%2C%20and%20combining%20multimodal%20information%20can%20surpass%0Athe%20unimodal%20baseline%20with%20a%20clear%20margin.%20Moreover%2C%20under%20the%20maximum%20coverage%0Acriterion%20on%20the%20POPE%20dataset%2C%20our%20method%20achieves%20a%201.87x%20speedup%20while%0Amaintaining%2098.7%25%20of%20the%20original%20performance%20on%20LLaVA-NeXT-13B.%20Furthermore%2C%0Awith%20only%20four%20vision%20tokens%2C%20it%20still%20preserves%2087.7%25%20of%20the%20original%0Aperformance%20on%20LLaVA-1.5-7B.%20These%20results%20highlight%20the%20effectiveness%20of%0Acoverage%20in%20token%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18264v1&entry.124074799=Read"},
{"title": "A Survey of the Self Supervised Learning Mechanisms for Vision\n  Transformers", "author": "Asifullah Khan and Anabia Sohail and Mustansar Fiaz and Mehdi Hassan and Tariq Habib Afridi and Sibghat Ullah Marwat and Farzeen Munir and Safdar Ali and Hannan Naseem and Muhammad Zaigham Zaheer and Kamran Ali and Tangina Sultana and Ziaurrehman Tanoli and Naeem Akhter", "abstract": "  Advances in deep learning are re-defining how visual data is processed and\nunderstand by the machines. Vision Transformers (ViTs) have recently\ndemonstrated prominent performance in computer vision related tasks. However,\ntheir performance improves with increasing numbers of labeled data, indicating\nreliance on labeled data. Humanly annotated data are difficult to acquire and\nthus shifted the focus from traditional annotations to unsupervised learning\nstrategies that learn structures inside the data. In response to this\nchallenge, self-supervised learning (SSL) has emerged as a promising technique.\nSSL utilize inherent relationships within the data as a form of supervision.\nThis technique can reduce the dependence on manual annotations and offers a\nmore scalable and resource-effective approach to training models. Taking these\nstrengths into account, it is necessary to assess the combination of SSL\nmethods with ViTs, especially in the cases of limited labeled data. Inspired by\nthis evolving trend, this survey aims to systematically review SSL mechanisms\ntailored for ViTs. We propose a comprehensive taxonomy to classify SSL\ntechniques based on their representations and pre-training tasks. Furthermore,\nwe highlighted the motivations behind the study of SSL, reviewed prominent\npre-training tasks, and highlight advancements and challenges in this field.\nFurthermore, we conduct a comparative analysis of various SSL methods designed\nfor ViTs, evaluating their strengths, limitations, and applicability to\ndifferent scenarios.\n", "link": "http://arxiv.org/abs/2408.17059v6", "date": "2025-08-25", "relevancy": 2.7569, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20the%20Self%20Supervised%20Learning%20Mechanisms%20for%20Vision%0A%20%20Transformers&body=Title%3A%20A%20Survey%20of%20the%20Self%20Supervised%20Learning%20Mechanisms%20for%20Vision%0A%20%20Transformers%0AAuthor%3A%20Asifullah%20Khan%20and%20Anabia%20Sohail%20and%20Mustansar%20Fiaz%20and%20Mehdi%20Hassan%20and%20Tariq%20Habib%20Afridi%20and%20Sibghat%20Ullah%20Marwat%20and%20Farzeen%20Munir%20and%20Safdar%20Ali%20and%20Hannan%20Naseem%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Kamran%20Ali%20and%20Tangina%20Sultana%20and%20Ziaurrehman%20Tanoli%20and%20Naeem%20Akhter%0AAbstract%3A%20%20%20Advances%20in%20deep%20learning%20are%20re-defining%20how%20visual%20data%20is%20processed%20and%0Aunderstand%20by%20the%20machines.%20Vision%20Transformers%20%28ViTs%29%20have%20recently%0Ademonstrated%20prominent%20performance%20in%20computer%20vision%20related%20tasks.%20However%2C%0Atheir%20performance%20improves%20with%20increasing%20numbers%20of%20labeled%20data%2C%20indicating%0Areliance%20on%20labeled%20data.%20Humanly%20annotated%20data%20are%20difficult%20to%20acquire%20and%0Athus%20shifted%20the%20focus%20from%20traditional%20annotations%20to%20unsupervised%20learning%0Astrategies%20that%20learn%20structures%20inside%20the%20data.%20In%20response%20to%20this%0Achallenge%2C%20self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20promising%20technique.%0ASSL%20utilize%20inherent%20relationships%20within%20the%20data%20as%20a%20form%20of%20supervision.%0AThis%20technique%20can%20reduce%20the%20dependence%20on%20manual%20annotations%20and%20offers%20a%0Amore%20scalable%20and%20resource-effective%20approach%20to%20training%20models.%20Taking%20these%0Astrengths%20into%20account%2C%20it%20is%20necessary%20to%20assess%20the%20combination%20of%20SSL%0Amethods%20with%20ViTs%2C%20especially%20in%20the%20cases%20of%20limited%20labeled%20data.%20Inspired%20by%0Athis%20evolving%20trend%2C%20this%20survey%20aims%20to%20systematically%20review%20SSL%20mechanisms%0Atailored%20for%20ViTs.%20We%20propose%20a%20comprehensive%20taxonomy%20to%20classify%20SSL%0Atechniques%20based%20on%20their%20representations%20and%20pre-training%20tasks.%20Furthermore%2C%0Awe%20highlighted%20the%20motivations%20behind%20the%20study%20of%20SSL%2C%20reviewed%20prominent%0Apre-training%20tasks%2C%20and%20highlight%20advancements%20and%20challenges%20in%20this%20field.%0AFurthermore%2C%20we%20conduct%20a%20comparative%20analysis%20of%20various%20SSL%20methods%20designed%0Afor%20ViTs%2C%20evaluating%20their%20strengths%2C%20limitations%2C%20and%20applicability%20to%0Adifferent%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17059v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520the%2520Self%2520Supervised%2520Learning%2520Mechanisms%2520for%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DAsifullah%2520Khan%2520and%2520Anabia%2520Sohail%2520and%2520Mustansar%2520Fiaz%2520and%2520Mehdi%2520Hassan%2520and%2520Tariq%2520Habib%2520Afridi%2520and%2520Sibghat%2520Ullah%2520Marwat%2520and%2520Farzeen%2520Munir%2520and%2520Safdar%2520Ali%2520and%2520Hannan%2520Naseem%2520and%2520Muhammad%2520Zaigham%2520Zaheer%2520and%2520Kamran%2520Ali%2520and%2520Tangina%2520Sultana%2520and%2520Ziaurrehman%2520Tanoli%2520and%2520Naeem%2520Akhter%26entry.1292438233%3D%2520%2520Advances%2520in%2520deep%2520learning%2520are%2520re-defining%2520how%2520visual%2520data%2520is%2520processed%2520and%250Aunderstand%2520by%2520the%2520machines.%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520recently%250Ademonstrated%2520prominent%2520performance%2520in%2520computer%2520vision%2520related%2520tasks.%2520However%252C%250Atheir%2520performance%2520improves%2520with%2520increasing%2520numbers%2520of%2520labeled%2520data%252C%2520indicating%250Areliance%2520on%2520labeled%2520data.%2520Humanly%2520annotated%2520data%2520are%2520difficult%2520to%2520acquire%2520and%250Athus%2520shifted%2520the%2520focus%2520from%2520traditional%2520annotations%2520to%2520unsupervised%2520learning%250Astrategies%2520that%2520learn%2520structures%2520inside%2520the%2520data.%2520In%2520response%2520to%2520this%250Achallenge%252C%2520self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520technique.%250ASSL%2520utilize%2520inherent%2520relationships%2520within%2520the%2520data%2520as%2520a%2520form%2520of%2520supervision.%250AThis%2520technique%2520can%2520reduce%2520the%2520dependence%2520on%2520manual%2520annotations%2520and%2520offers%2520a%250Amore%2520scalable%2520and%2520resource-effective%2520approach%2520to%2520training%2520models.%2520Taking%2520these%250Astrengths%2520into%2520account%252C%2520it%2520is%2520necessary%2520to%2520assess%2520the%2520combination%2520of%2520SSL%250Amethods%2520with%2520ViTs%252C%2520especially%2520in%2520the%2520cases%2520of%2520limited%2520labeled%2520data.%2520Inspired%2520by%250Athis%2520evolving%2520trend%252C%2520this%2520survey%2520aims%2520to%2520systematically%2520review%2520SSL%2520mechanisms%250Atailored%2520for%2520ViTs.%2520We%2520propose%2520a%2520comprehensive%2520taxonomy%2520to%2520classify%2520SSL%250Atechniques%2520based%2520on%2520their%2520representations%2520and%2520pre-training%2520tasks.%2520Furthermore%252C%250Awe%2520highlighted%2520the%2520motivations%2520behind%2520the%2520study%2520of%2520SSL%252C%2520reviewed%2520prominent%250Apre-training%2520tasks%252C%2520and%2520highlight%2520advancements%2520and%2520challenges%2520in%2520this%2520field.%250AFurthermore%252C%2520we%2520conduct%2520a%2520comparative%2520analysis%2520of%2520various%2520SSL%2520methods%2520designed%250Afor%2520ViTs%252C%2520evaluating%2520their%2520strengths%252C%2520limitations%252C%2520and%2520applicability%2520to%250Adifferent%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17059v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20the%20Self%20Supervised%20Learning%20Mechanisms%20for%20Vision%0A%20%20Transformers&entry.906535625=Asifullah%20Khan%20and%20Anabia%20Sohail%20and%20Mustansar%20Fiaz%20and%20Mehdi%20Hassan%20and%20Tariq%20Habib%20Afridi%20and%20Sibghat%20Ullah%20Marwat%20and%20Farzeen%20Munir%20and%20Safdar%20Ali%20and%20Hannan%20Naseem%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Kamran%20Ali%20and%20Tangina%20Sultana%20and%20Ziaurrehman%20Tanoli%20and%20Naeem%20Akhter&entry.1292438233=%20%20Advances%20in%20deep%20learning%20are%20re-defining%20how%20visual%20data%20is%20processed%20and%0Aunderstand%20by%20the%20machines.%20Vision%20Transformers%20%28ViTs%29%20have%20recently%0Ademonstrated%20prominent%20performance%20in%20computer%20vision%20related%20tasks.%20However%2C%0Atheir%20performance%20improves%20with%20increasing%20numbers%20of%20labeled%20data%2C%20indicating%0Areliance%20on%20labeled%20data.%20Humanly%20annotated%20data%20are%20difficult%20to%20acquire%20and%0Athus%20shifted%20the%20focus%20from%20traditional%20annotations%20to%20unsupervised%20learning%0Astrategies%20that%20learn%20structures%20inside%20the%20data.%20In%20response%20to%20this%0Achallenge%2C%20self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20promising%20technique.%0ASSL%20utilize%20inherent%20relationships%20within%20the%20data%20as%20a%20form%20of%20supervision.%0AThis%20technique%20can%20reduce%20the%20dependence%20on%20manual%20annotations%20and%20offers%20a%0Amore%20scalable%20and%20resource-effective%20approach%20to%20training%20models.%20Taking%20these%0Astrengths%20into%20account%2C%20it%20is%20necessary%20to%20assess%20the%20combination%20of%20SSL%0Amethods%20with%20ViTs%2C%20especially%20in%20the%20cases%20of%20limited%20labeled%20data.%20Inspired%20by%0Athis%20evolving%20trend%2C%20this%20survey%20aims%20to%20systematically%20review%20SSL%20mechanisms%0Atailored%20for%20ViTs.%20We%20propose%20a%20comprehensive%20taxonomy%20to%20classify%20SSL%0Atechniques%20based%20on%20their%20representations%20and%20pre-training%20tasks.%20Furthermore%2C%0Awe%20highlighted%20the%20motivations%20behind%20the%20study%20of%20SSL%2C%20reviewed%20prominent%0Apre-training%20tasks%2C%20and%20highlight%20advancements%20and%20challenges%20in%20this%20field.%0AFurthermore%2C%20we%20conduct%20a%20comparative%20analysis%20of%20various%20SSL%20methods%20designed%0Afor%20ViTs%2C%20evaluating%20their%20strengths%2C%20limitations%2C%20and%20applicability%20to%0Adifferent%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17059v6&entry.124074799=Read"},
{"title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields", "author": "Zhenxing Mi and Ping Yin and Xue Xiao and Dan Xu", "abstract": "  Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decompose scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts by our proposed Sparsely Gated Mixture of Experts\n(MoE) NeRF framework. We incorporate a hash-based gating network and distinct\nheterogeneous hash experts. The hash-based gating efficiently learns the\ndecomposition of the large-scale scene. The distinct heterogeneous hash experts\nconsist of hash grids of different resolution ranges, enabling effective\nlearning of the heterogeneous representation of different scene parts. These\ndesign choices make our framework an end-to-end and highly scalable NeRF\nsolution for real-world large-scale scene modeling to achieve both quality and\nefficiency. We evaluate our accuracy and scalability on existing large-scale\nNeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from\nUrbanBIS. Extensive experiments demonstrate that our approach can be easily\nscaled to various large-scale scenes and achieve state-of-the-art scene\nrendering accuracy. Furthermore, our method exhibits significant efficiency,\nwith an 8x acceleration in training and a 16x acceleration in rendering\ncompared to Switch-NeRF. Codes will be released at\nhttps://github.com/MiZhenxing/Switch-NeRF.\n", "link": "http://arxiv.org/abs/2505.02005v2", "date": "2025-08-25", "relevancy": 2.7517, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Heterogeneous%20Mixture%20of%20Scene%20Experts%20for%20Large-scale%20Neural%0A%20%20Radiance%20Fields&body=Title%3A%20Learning%20Heterogeneous%20Mixture%20of%20Scene%20Experts%20for%20Large-scale%20Neural%0A%20%20Radiance%20Fields%0AAuthor%3A%20Zhenxing%20Mi%20and%20Ping%20Yin%20and%20Xue%20Xiao%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Recent%20NeRF%20methods%20on%20large-scale%20scenes%20have%20underlined%20the%20importance%20of%0Ascene%20decomposition%20for%20scalable%20NeRFs.%20Although%20achieving%20reasonable%0Ascalability%2C%20there%20are%20several%20critical%20problems%20remaining%20unexplored%2C%20i.e.%2C%0Alearnable%20decomposition%2C%20modeling%20scene%20heterogeneity%2C%20and%20modeling%20efficiency.%0AIn%20this%20paper%2C%20we%20introduce%20Switch-NeRF%2B%2B%2C%20a%20Heterogeneous%20Mixture%20of%20Hash%0AExperts%20%28HMoHE%29%20network%20that%20addresses%20these%20challenges%20within%20a%20unified%0Aframework.%20It%20is%20a%20highly%20scalable%20NeRF%20that%20learns%20heterogeneous%20decomposition%0Aand%20heterogeneous%20NeRFs%20efficiently%20for%20large-scale%20scenes%20in%20an%20end-to-end%0Amanner.%20In%20our%20framework%2C%20a%20gating%20network%20learns%20to%20decompose%20scenes%20and%0Aallocates%203D%20points%20to%20specialized%20NeRF%20experts.%20This%20gating%20network%20is%0Aco-optimized%20with%20the%20experts%20by%20our%20proposed%20Sparsely%20Gated%20Mixture%20of%20Experts%0A%28MoE%29%20NeRF%20framework.%20We%20incorporate%20a%20hash-based%20gating%20network%20and%20distinct%0Aheterogeneous%20hash%20experts.%20The%20hash-based%20gating%20efficiently%20learns%20the%0Adecomposition%20of%20the%20large-scale%20scene.%20The%20distinct%20heterogeneous%20hash%20experts%0Aconsist%20of%20hash%20grids%20of%20different%20resolution%20ranges%2C%20enabling%20effective%0Alearning%20of%20the%20heterogeneous%20representation%20of%20different%20scene%20parts.%20These%0Adesign%20choices%20make%20our%20framework%20an%20end-to-end%20and%20highly%20scalable%20NeRF%0Asolution%20for%20real-world%20large-scale%20scene%20modeling%20to%20achieve%20both%20quality%20and%0Aefficiency.%20We%20evaluate%20our%20accuracy%20and%20scalability%20on%20existing%20large-scale%0ANeRF%20datasets%20and%20a%20new%20dataset%20with%20very%20large-scale%20scenes%20%28%24%3E6.5km%5E2%24%29%20from%0AUrbanBIS.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20can%20be%20easily%0Ascaled%20to%20various%20large-scale%20scenes%20and%20achieve%20state-of-the-art%20scene%0Arendering%20accuracy.%20Furthermore%2C%20our%20method%20exhibits%20significant%20efficiency%2C%0Awith%20an%208x%20acceleration%20in%20training%20and%20a%2016x%20acceleration%20in%20rendering%0Acompared%20to%20Switch-NeRF.%20Codes%20will%20be%20released%20at%0Ahttps%3A//github.com/MiZhenxing/Switch-NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Heterogeneous%2520Mixture%2520of%2520Scene%2520Experts%2520for%2520Large-scale%2520Neural%250A%2520%2520Radiance%2520Fields%26entry.906535625%3DZhenxing%2520Mi%2520and%2520Ping%2520Yin%2520and%2520Xue%2520Xiao%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520NeRF%2520methods%2520on%2520large-scale%2520scenes%2520have%2520underlined%2520the%2520importance%2520of%250Ascene%2520decomposition%2520for%2520scalable%2520NeRFs.%2520Although%2520achieving%2520reasonable%250Ascalability%252C%2520there%2520are%2520several%2520critical%2520problems%2520remaining%2520unexplored%252C%2520i.e.%252C%250Alearnable%2520decomposition%252C%2520modeling%2520scene%2520heterogeneity%252C%2520and%2520modeling%2520efficiency.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Switch-NeRF%252B%252B%252C%2520a%2520Heterogeneous%2520Mixture%2520of%2520Hash%250AExperts%2520%2528HMoHE%2529%2520network%2520that%2520addresses%2520these%2520challenges%2520within%2520a%2520unified%250Aframework.%2520It%2520is%2520a%2520highly%2520scalable%2520NeRF%2520that%2520learns%2520heterogeneous%2520decomposition%250Aand%2520heterogeneous%2520NeRFs%2520efficiently%2520for%2520large-scale%2520scenes%2520in%2520an%2520end-to-end%250Amanner.%2520In%2520our%2520framework%252C%2520a%2520gating%2520network%2520learns%2520to%2520decompose%2520scenes%2520and%250Aallocates%25203D%2520points%2520to%2520specialized%2520NeRF%2520experts.%2520This%2520gating%2520network%2520is%250Aco-optimized%2520with%2520the%2520experts%2520by%2520our%2520proposed%2520Sparsely%2520Gated%2520Mixture%2520of%2520Experts%250A%2528MoE%2529%2520NeRF%2520framework.%2520We%2520incorporate%2520a%2520hash-based%2520gating%2520network%2520and%2520distinct%250Aheterogeneous%2520hash%2520experts.%2520The%2520hash-based%2520gating%2520efficiently%2520learns%2520the%250Adecomposition%2520of%2520the%2520large-scale%2520scene.%2520The%2520distinct%2520heterogeneous%2520hash%2520experts%250Aconsist%2520of%2520hash%2520grids%2520of%2520different%2520resolution%2520ranges%252C%2520enabling%2520effective%250Alearning%2520of%2520the%2520heterogeneous%2520representation%2520of%2520different%2520scene%2520parts.%2520These%250Adesign%2520choices%2520make%2520our%2520framework%2520an%2520end-to-end%2520and%2520highly%2520scalable%2520NeRF%250Asolution%2520for%2520real-world%2520large-scale%2520scene%2520modeling%2520to%2520achieve%2520both%2520quality%2520and%250Aefficiency.%2520We%2520evaluate%2520our%2520accuracy%2520and%2520scalability%2520on%2520existing%2520large-scale%250ANeRF%2520datasets%2520and%2520a%2520new%2520dataset%2520with%2520very%2520large-scale%2520scenes%2520%2528%2524%253E6.5km%255E2%2524%2529%2520from%250AUrbanBIS.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520be%2520easily%250Ascaled%2520to%2520various%2520large-scale%2520scenes%2520and%2520achieve%2520state-of-the-art%2520scene%250Arendering%2520accuracy.%2520Furthermore%252C%2520our%2520method%2520exhibits%2520significant%2520efficiency%252C%250Awith%2520an%25208x%2520acceleration%2520in%2520training%2520and%2520a%252016x%2520acceleration%2520in%2520rendering%250Acompared%2520to%2520Switch-NeRF.%2520Codes%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/MiZhenxing/Switch-NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Heterogeneous%20Mixture%20of%20Scene%20Experts%20for%20Large-scale%20Neural%0A%20%20Radiance%20Fields&entry.906535625=Zhenxing%20Mi%20and%20Ping%20Yin%20and%20Xue%20Xiao%20and%20Dan%20Xu&entry.1292438233=%20%20Recent%20NeRF%20methods%20on%20large-scale%20scenes%20have%20underlined%20the%20importance%20of%0Ascene%20decomposition%20for%20scalable%20NeRFs.%20Although%20achieving%20reasonable%0Ascalability%2C%20there%20are%20several%20critical%20problems%20remaining%20unexplored%2C%20i.e.%2C%0Alearnable%20decomposition%2C%20modeling%20scene%20heterogeneity%2C%20and%20modeling%20efficiency.%0AIn%20this%20paper%2C%20we%20introduce%20Switch-NeRF%2B%2B%2C%20a%20Heterogeneous%20Mixture%20of%20Hash%0AExperts%20%28HMoHE%29%20network%20that%20addresses%20these%20challenges%20within%20a%20unified%0Aframework.%20It%20is%20a%20highly%20scalable%20NeRF%20that%20learns%20heterogeneous%20decomposition%0Aand%20heterogeneous%20NeRFs%20efficiently%20for%20large-scale%20scenes%20in%20an%20end-to-end%0Amanner.%20In%20our%20framework%2C%20a%20gating%20network%20learns%20to%20decompose%20scenes%20and%0Aallocates%203D%20points%20to%20specialized%20NeRF%20experts.%20This%20gating%20network%20is%0Aco-optimized%20with%20the%20experts%20by%20our%20proposed%20Sparsely%20Gated%20Mixture%20of%20Experts%0A%28MoE%29%20NeRF%20framework.%20We%20incorporate%20a%20hash-based%20gating%20network%20and%20distinct%0Aheterogeneous%20hash%20experts.%20The%20hash-based%20gating%20efficiently%20learns%20the%0Adecomposition%20of%20the%20large-scale%20scene.%20The%20distinct%20heterogeneous%20hash%20experts%0Aconsist%20of%20hash%20grids%20of%20different%20resolution%20ranges%2C%20enabling%20effective%0Alearning%20of%20the%20heterogeneous%20representation%20of%20different%20scene%20parts.%20These%0Adesign%20choices%20make%20our%20framework%20an%20end-to-end%20and%20highly%20scalable%20NeRF%0Asolution%20for%20real-world%20large-scale%20scene%20modeling%20to%20achieve%20both%20quality%20and%0Aefficiency.%20We%20evaluate%20our%20accuracy%20and%20scalability%20on%20existing%20large-scale%0ANeRF%20datasets%20and%20a%20new%20dataset%20with%20very%20large-scale%20scenes%20%28%24%3E6.5km%5E2%24%29%20from%0AUrbanBIS.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20can%20be%20easily%0Ascaled%20to%20various%20large-scale%20scenes%20and%20achieve%20state-of-the-art%20scene%0Arendering%20accuracy.%20Furthermore%2C%20our%20method%20exhibits%20significant%20efficiency%2C%0Awith%20an%208x%20acceleration%20in%20training%20and%20a%2016x%20acceleration%20in%20rendering%0Acompared%20to%20Switch-NeRF.%20Codes%20will%20be%20released%20at%0Ahttps%3A//github.com/MiZhenxing/Switch-NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02005v2&entry.124074799=Read"},
{"title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities", "author": "Kushal Raj Bhandari and Pin-Yu Chen and Jianxi Gao", "abstract": "  Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.\n", "link": "http://arxiv.org/abs/2508.18192v1", "date": "2025-08-25", "relevancy": 2.7512, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20cognitive%20patterns%20of%20Large%20Language%20Models%20through%0A%20%20module%20communities&body=Title%3A%20Unraveling%20the%20cognitive%20patterns%20of%20Large%20Language%20Models%20through%0A%20%20module%20communities%0AAuthor%3A%20Kushal%20Raj%20Bhandari%20and%20Pin-Yu%20Chen%20and%20Jianxi%20Gao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20reshaped%20our%20world%20with%20significant%0Aadvancements%20in%20science%2C%20engineering%2C%20and%20society%20through%20applications%20ranging%0Afrom%20scientific%20discoveries%20and%20medical%20diagnostics%20to%20Chatbots.%20Despite%20their%0Aubiquity%20and%20utility%2C%20the%20underlying%20mechanisms%20of%20LLM%20remain%20concealed%20within%0Abillions%20of%20parameters%20and%20complex%20structures%2C%20making%20their%20inner%20architecture%0Aand%20cognitive%20processes%20challenging%20to%20comprehend.%20We%20address%20this%20gap%20by%0Aadopting%20approaches%20to%20understanding%20emerging%20cognition%20in%20biology%20and%0Adeveloping%20a%20network-based%20framework%20that%20links%20cognitive%20skills%2C%20LLM%0Aarchitectures%2C%20and%20datasets%2C%20ushering%20in%20a%20paradigm%20shift%20in%20foundation%20model%0Aanalysis.%20The%20skill%20distribution%20in%20the%20module%20communities%20demonstrates%20that%0Awhile%20LLMs%20do%20not%20strictly%20parallel%20the%20focalized%20specialization%20observed%20in%0Aspecific%20biological%20systems%2C%20they%20exhibit%20unique%20communities%20of%20modules%20whose%0Aemergent%20skill%20patterns%20partially%20mirror%20the%20distributed%20yet%20interconnected%0Acognitive%20organization%20seen%20in%20avian%20and%20small%20mammalian%20brains.%20Our%20numerical%0Aresults%20highlight%20a%20key%20divergence%20from%20biological%20systems%20to%20LLMs%2C%20where%20skill%0Aacquisition%20benefits%20substantially%20from%20dynamic%2C%20cross-regional%20interactions%0Aand%20neural%20plasticity.%20By%20integrating%20cognitive%20science%20principles%20with%20machine%0Alearning%2C%20our%20framework%20provides%20new%20insights%20into%20LLM%20interpretability%20and%0Asuggests%20that%20effective%20fine-tuning%20strategies%20should%20leverage%20distributed%0Alearning%20dynamics%20rather%20than%20rigid%20modular%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520cognitive%2520patterns%2520of%2520Large%2520Language%2520Models%2520through%250A%2520%2520module%2520communities%26entry.906535625%3DKushal%2520Raj%2520Bhandari%2520and%2520Pin-Yu%2520Chen%2520and%2520Jianxi%2520Gao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520reshaped%2520our%2520world%2520with%2520significant%250Aadvancements%2520in%2520science%252C%2520engineering%252C%2520and%2520society%2520through%2520applications%2520ranging%250Afrom%2520scientific%2520discoveries%2520and%2520medical%2520diagnostics%2520to%2520Chatbots.%2520Despite%2520their%250Aubiquity%2520and%2520utility%252C%2520the%2520underlying%2520mechanisms%2520of%2520LLM%2520remain%2520concealed%2520within%250Abillions%2520of%2520parameters%2520and%2520complex%2520structures%252C%2520making%2520their%2520inner%2520architecture%250Aand%2520cognitive%2520processes%2520challenging%2520to%2520comprehend.%2520We%2520address%2520this%2520gap%2520by%250Aadopting%2520approaches%2520to%2520understanding%2520emerging%2520cognition%2520in%2520biology%2520and%250Adeveloping%2520a%2520network-based%2520framework%2520that%2520links%2520cognitive%2520skills%252C%2520LLM%250Aarchitectures%252C%2520and%2520datasets%252C%2520ushering%2520in%2520a%2520paradigm%2520shift%2520in%2520foundation%2520model%250Aanalysis.%2520The%2520skill%2520distribution%2520in%2520the%2520module%2520communities%2520demonstrates%2520that%250Awhile%2520LLMs%2520do%2520not%2520strictly%2520parallel%2520the%2520focalized%2520specialization%2520observed%2520in%250Aspecific%2520biological%2520systems%252C%2520they%2520exhibit%2520unique%2520communities%2520of%2520modules%2520whose%250Aemergent%2520skill%2520patterns%2520partially%2520mirror%2520the%2520distributed%2520yet%2520interconnected%250Acognitive%2520organization%2520seen%2520in%2520avian%2520and%2520small%2520mammalian%2520brains.%2520Our%2520numerical%250Aresults%2520highlight%2520a%2520key%2520divergence%2520from%2520biological%2520systems%2520to%2520LLMs%252C%2520where%2520skill%250Aacquisition%2520benefits%2520substantially%2520from%2520dynamic%252C%2520cross-regional%2520interactions%250Aand%2520neural%2520plasticity.%2520By%2520integrating%2520cognitive%2520science%2520principles%2520with%2520machine%250Alearning%252C%2520our%2520framework%2520provides%2520new%2520insights%2520into%2520LLM%2520interpretability%2520and%250Asuggests%2520that%2520effective%2520fine-tuning%2520strategies%2520should%2520leverage%2520distributed%250Alearning%2520dynamics%2520rather%2520than%2520rigid%2520modular%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20cognitive%20patterns%20of%20Large%20Language%20Models%20through%0A%20%20module%20communities&entry.906535625=Kushal%20Raj%20Bhandari%20and%20Pin-Yu%20Chen%20and%20Jianxi%20Gao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20reshaped%20our%20world%20with%20significant%0Aadvancements%20in%20science%2C%20engineering%2C%20and%20society%20through%20applications%20ranging%0Afrom%20scientific%20discoveries%20and%20medical%20diagnostics%20to%20Chatbots.%20Despite%20their%0Aubiquity%20and%20utility%2C%20the%20underlying%20mechanisms%20of%20LLM%20remain%20concealed%20within%0Abillions%20of%20parameters%20and%20complex%20structures%2C%20making%20their%20inner%20architecture%0Aand%20cognitive%20processes%20challenging%20to%20comprehend.%20We%20address%20this%20gap%20by%0Aadopting%20approaches%20to%20understanding%20emerging%20cognition%20in%20biology%20and%0Adeveloping%20a%20network-based%20framework%20that%20links%20cognitive%20skills%2C%20LLM%0Aarchitectures%2C%20and%20datasets%2C%20ushering%20in%20a%20paradigm%20shift%20in%20foundation%20model%0Aanalysis.%20The%20skill%20distribution%20in%20the%20module%20communities%20demonstrates%20that%0Awhile%20LLMs%20do%20not%20strictly%20parallel%20the%20focalized%20specialization%20observed%20in%0Aspecific%20biological%20systems%2C%20they%20exhibit%20unique%20communities%20of%20modules%20whose%0Aemergent%20skill%20patterns%20partially%20mirror%20the%20distributed%20yet%20interconnected%0Acognitive%20organization%20seen%20in%20avian%20and%20small%20mammalian%20brains.%20Our%20numerical%0Aresults%20highlight%20a%20key%20divergence%20from%20biological%20systems%20to%20LLMs%2C%20where%20skill%0Aacquisition%20benefits%20substantially%20from%20dynamic%2C%20cross-regional%20interactions%0Aand%20neural%20plasticity.%20By%20integrating%20cognitive%20science%20principles%20with%20machine%0Alearning%2C%20our%20framework%20provides%20new%20insights%20into%20LLM%20interpretability%20and%0Asuggests%20that%20effective%20fine-tuning%20strategies%20should%20leverage%20distributed%0Alearning%20dynamics%20rather%20than%20rigid%20modular%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18192v1&entry.124074799=Read"},
{"title": "Camera Pose Refinement via 3D Gaussian Splatting", "author": "Lulu Hao and Lipu Zhou and Zhenzhong Wei and Xu Wang", "abstract": "  Camera pose refinement aims at improving the accuracy of initial pose\nestimation for applications in 3D computer vision. Most refinement approaches\nrely on 2D-3D correspondences with specific descriptors or dedicated networks,\nrequiring reconstructing the scene again for a different descriptor or fully\nretraining the network for each scene. Some recent methods instead infer pose\nfrom feature similarity, but their lack of geometry constraints results in less\naccuracy. To overcome these limitations, we propose a novel camera pose\nrefinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as\nGS-SMC. Given the widespread usage of 3DGS, our method can employ an existing\n3DGS model to render novel views, providing a lightweight solution that can be\ndirectly applied to diverse scenes without additional training or fine-tuning.\nSpecifically, we introduce an iterative optimization approach, which refines\nthe camera pose using epipolar geometric constraints among the query and\nmultiple rendered images. Our method allows flexibly choosing feature\nextractors and matchers to establish these constraints. Extensive empirical\nevaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate\nthat our method outperforms state-of-the-art camera pose refinement approaches,\nachieving 53.3% and 56.9% reductions in median translation and rotation errors\non 7-Scenes, and 40.7% and 53.2% on Cambridge.\n", "link": "http://arxiv.org/abs/2508.17876v1", "date": "2025-08-25", "relevancy": 2.7453, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7027}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.694}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&body=Title%3A%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Lulu%20Hao%20and%20Lipu%20Zhou%20and%20Zhenzhong%20Wei%20and%20Xu%20Wang%0AAbstract%3A%20%20%20Camera%20pose%20refinement%20aims%20at%20improving%20the%20accuracy%20of%20initial%20pose%0Aestimation%20for%20applications%20in%203D%20computer%20vision.%20Most%20refinement%20approaches%0Arely%20on%202D-3D%20correspondences%20with%20specific%20descriptors%20or%20dedicated%20networks%2C%0Arequiring%20reconstructing%20the%20scene%20again%20for%20a%20different%20descriptor%20or%20fully%0Aretraining%20the%20network%20for%20each%20scene.%20Some%20recent%20methods%20instead%20infer%20pose%0Afrom%20feature%20similarity%2C%20but%20their%20lack%20of%20geometry%20constraints%20results%20in%20less%0Aaccuracy.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20camera%20pose%0Arefinement%20framework%20leveraging%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20referred%20to%20as%0AGS-SMC.%20Given%20the%20widespread%20usage%20of%203DGS%2C%20our%20method%20can%20employ%20an%20existing%0A3DGS%20model%20to%20render%20novel%20views%2C%20providing%20a%20lightweight%20solution%20that%20can%20be%0Adirectly%20applied%20to%20diverse%20scenes%20without%20additional%20training%20or%20fine-tuning.%0ASpecifically%2C%20we%20introduce%20an%20iterative%20optimization%20approach%2C%20which%20refines%0Athe%20camera%20pose%20using%20epipolar%20geometric%20constraints%20among%20the%20query%20and%0Amultiple%20rendered%20images.%20Our%20method%20allows%20flexibly%20choosing%20feature%0Aextractors%20and%20matchers%20to%20establish%20these%20constraints.%20Extensive%20empirical%0Aevaluations%20on%20the%207-Scenes%20and%20the%20Cambridge%20Landmarks%20datasets%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20camera%20pose%20refinement%20approaches%2C%0Aachieving%2053.3%25%20and%2056.9%25%20reductions%20in%20median%20translation%20and%20rotation%20errors%0Aon%207-Scenes%2C%20and%2040.7%25%20and%2053.2%25%20on%20Cambridge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera%2520Pose%2520Refinement%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DLulu%2520Hao%2520and%2520Lipu%2520Zhou%2520and%2520Zhenzhong%2520Wei%2520and%2520Xu%2520Wang%26entry.1292438233%3D%2520%2520Camera%2520pose%2520refinement%2520aims%2520at%2520improving%2520the%2520accuracy%2520of%2520initial%2520pose%250Aestimation%2520for%2520applications%2520in%25203D%2520computer%2520vision.%2520Most%2520refinement%2520approaches%250Arely%2520on%25202D-3D%2520correspondences%2520with%2520specific%2520descriptors%2520or%2520dedicated%2520networks%252C%250Arequiring%2520reconstructing%2520the%2520scene%2520again%2520for%2520a%2520different%2520descriptor%2520or%2520fully%250Aretraining%2520the%2520network%2520for%2520each%2520scene.%2520Some%2520recent%2520methods%2520instead%2520infer%2520pose%250Afrom%2520feature%2520similarity%252C%2520but%2520their%2520lack%2520of%2520geometry%2520constraints%2520results%2520in%2520less%250Aaccuracy.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520camera%2520pose%250Arefinement%2520framework%2520leveraging%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520referred%2520to%2520as%250AGS-SMC.%2520Given%2520the%2520widespread%2520usage%2520of%25203DGS%252C%2520our%2520method%2520can%2520employ%2520an%2520existing%250A3DGS%2520model%2520to%2520render%2520novel%2520views%252C%2520providing%2520a%2520lightweight%2520solution%2520that%2520can%2520be%250Adirectly%2520applied%2520to%2520diverse%2520scenes%2520without%2520additional%2520training%2520or%2520fine-tuning.%250ASpecifically%252C%2520we%2520introduce%2520an%2520iterative%2520optimization%2520approach%252C%2520which%2520refines%250Athe%2520camera%2520pose%2520using%2520epipolar%2520geometric%2520constraints%2520among%2520the%2520query%2520and%250Amultiple%2520rendered%2520images.%2520Our%2520method%2520allows%2520flexibly%2520choosing%2520feature%250Aextractors%2520and%2520matchers%2520to%2520establish%2520these%2520constraints.%2520Extensive%2520empirical%250Aevaluations%2520on%2520the%25207-Scenes%2520and%2520the%2520Cambridge%2520Landmarks%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520outperforms%2520state-of-the-art%2520camera%2520pose%2520refinement%2520approaches%252C%250Aachieving%252053.3%2525%2520and%252056.9%2525%2520reductions%2520in%2520median%2520translation%2520and%2520rotation%2520errors%250Aon%25207-Scenes%252C%2520and%252040.7%2525%2520and%252053.2%2525%2520on%2520Cambridge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&entry.906535625=Lulu%20Hao%20and%20Lipu%20Zhou%20and%20Zhenzhong%20Wei%20and%20Xu%20Wang&entry.1292438233=%20%20Camera%20pose%20refinement%20aims%20at%20improving%20the%20accuracy%20of%20initial%20pose%0Aestimation%20for%20applications%20in%203D%20computer%20vision.%20Most%20refinement%20approaches%0Arely%20on%202D-3D%20correspondences%20with%20specific%20descriptors%20or%20dedicated%20networks%2C%0Arequiring%20reconstructing%20the%20scene%20again%20for%20a%20different%20descriptor%20or%20fully%0Aretraining%20the%20network%20for%20each%20scene.%20Some%20recent%20methods%20instead%20infer%20pose%0Afrom%20feature%20similarity%2C%20but%20their%20lack%20of%20geometry%20constraints%20results%20in%20less%0Aaccuracy.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20camera%20pose%0Arefinement%20framework%20leveraging%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20referred%20to%20as%0AGS-SMC.%20Given%20the%20widespread%20usage%20of%203DGS%2C%20our%20method%20can%20employ%20an%20existing%0A3DGS%20model%20to%20render%20novel%20views%2C%20providing%20a%20lightweight%20solution%20that%20can%20be%0Adirectly%20applied%20to%20diverse%20scenes%20without%20additional%20training%20or%20fine-tuning.%0ASpecifically%2C%20we%20introduce%20an%20iterative%20optimization%20approach%2C%20which%20refines%0Athe%20camera%20pose%20using%20epipolar%20geometric%20constraints%20among%20the%20query%20and%0Amultiple%20rendered%20images.%20Our%20method%20allows%20flexibly%20choosing%20feature%0Aextractors%20and%20matchers%20to%20establish%20these%20constraints.%20Extensive%20empirical%0Aevaluations%20on%20the%207-Scenes%20and%20the%20Cambridge%20Landmarks%20datasets%20demonstrate%0Athat%20our%20method%20outperforms%20state-of-the-art%20camera%20pose%20refinement%20approaches%2C%0Aachieving%2053.3%25%20and%2056.9%25%20reductions%20in%20median%20translation%20and%20rotation%20errors%0Aon%207-Scenes%2C%20and%2040.7%25%20and%2053.2%25%20on%20Cambridge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17876v1&entry.124074799=Read"},
{"title": "Denoising, segmentation and volumetric rendering of optical coherence\n  tomography angiography (OCTA) image using deep learning techniques: a review", "author": "Kejie Chen and Guanbing Gao and Xiaochun Yang and Wenbo Wang and Jing Na", "abstract": "  Optical coherence tomography angiography (OCTA) is a non-invasive imaging\ntechnique widely used to study vascular structures and micro-circulation\ndynamics in the retina and choroid. OCTA has been widely used in clinics for\ndiagnosing ocular disease and monitoring its progression, because OCTA is safer\nand faster than dye-based angiography while retaining the ability to\ncharacterize micro-scale structures. However, OCTA data contains many inherent\nnoises from the devices and acquisition protocols and suffers from various\ntypes of artifacts, which impairs diagnostic accuracy and repeatability. Deep\nlearning (DL) based imaging analysis models are able to automatically detect\nand remove artifacts and noises, and enhance the quality of image data. It is\nalso a powerful tool for segmentation and identification of normal and\npathological structures in the images. Thus, the value of OCTA imaging can be\nsignificantly enhanced by the DL-based approaches for interpreting and\nperforming measurements and predictions on the OCTA data. In this study, we\nreviewed literature on the DL models for OCTA images in the latest five years.\nIn particular, we focused on discussing the current problems in the OCTA data\nand the corresponding design principles of the DL models. We also reviewed the\nstate-of-art DL models for 3D volumetric reconstruction of the vascular\nnetworks and pathological structures such as the edema and distorted optic\ndisc. In addition, the publicly available dataset of OCTA images are summarized\nat the end of this review. Overall, this review can provide valuable insights\nfor engineers to develop novel DL models by utilizing the characteristics of\nOCTA signals and images. The pros and cons of each DL methods and their\napplications discussed in this review can be helpful to assist technicians and\nclinicians to use proper DL models for fundamental research and disease\nscreening.\n", "link": "http://arxiv.org/abs/2502.14935v2", "date": "2025-08-25", "relevancy": 2.7416, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%2C%20segmentation%20and%20volumetric%20rendering%20of%20optical%20coherence%0A%20%20tomography%20angiography%20%28OCTA%29%20image%20using%20deep%20learning%20techniques%3A%20a%20review&body=Title%3A%20Denoising%2C%20segmentation%20and%20volumetric%20rendering%20of%20optical%20coherence%0A%20%20tomography%20angiography%20%28OCTA%29%20image%20using%20deep%20learning%20techniques%3A%20a%20review%0AAuthor%3A%20Kejie%20Chen%20and%20Guanbing%20Gao%20and%20Xiaochun%20Yang%20and%20Wenbo%20Wang%20and%20Jing%20Na%0AAbstract%3A%20%20%20Optical%20coherence%20tomography%20angiography%20%28OCTA%29%20is%20a%20non-invasive%20imaging%0Atechnique%20widely%20used%20to%20study%20vascular%20structures%20and%20micro-circulation%0Adynamics%20in%20the%20retina%20and%20choroid.%20OCTA%20has%20been%20widely%20used%20in%20clinics%20for%0Adiagnosing%20ocular%20disease%20and%20monitoring%20its%20progression%2C%20because%20OCTA%20is%20safer%0Aand%20faster%20than%20dye-based%20angiography%20while%20retaining%20the%20ability%20to%0Acharacterize%20micro-scale%20structures.%20However%2C%20OCTA%20data%20contains%20many%20inherent%0Anoises%20from%20the%20devices%20and%20acquisition%20protocols%20and%20suffers%20from%20various%0Atypes%20of%20artifacts%2C%20which%20impairs%20diagnostic%20accuracy%20and%20repeatability.%20Deep%0Alearning%20%28DL%29%20based%20imaging%20analysis%20models%20are%20able%20to%20automatically%20detect%0Aand%20remove%20artifacts%20and%20noises%2C%20and%20enhance%20the%20quality%20of%20image%20data.%20It%20is%0Aalso%20a%20powerful%20tool%20for%20segmentation%20and%20identification%20of%20normal%20and%0Apathological%20structures%20in%20the%20images.%20Thus%2C%20the%20value%20of%20OCTA%20imaging%20can%20be%0Asignificantly%20enhanced%20by%20the%20DL-based%20approaches%20for%20interpreting%20and%0Aperforming%20measurements%20and%20predictions%20on%20the%20OCTA%20data.%20In%20this%20study%2C%20we%0Areviewed%20literature%20on%20the%20DL%20models%20for%20OCTA%20images%20in%20the%20latest%20five%20years.%0AIn%20particular%2C%20we%20focused%20on%20discussing%20the%20current%20problems%20in%20the%20OCTA%20data%0Aand%20the%20corresponding%20design%20principles%20of%20the%20DL%20models.%20We%20also%20reviewed%20the%0Astate-of-art%20DL%20models%20for%203D%20volumetric%20reconstruction%20of%20the%20vascular%0Anetworks%20and%20pathological%20structures%20such%20as%20the%20edema%20and%20distorted%20optic%0Adisc.%20In%20addition%2C%20the%20publicly%20available%20dataset%20of%20OCTA%20images%20are%20summarized%0Aat%20the%20end%20of%20this%20review.%20Overall%2C%20this%20review%20can%20provide%20valuable%20insights%0Afor%20engineers%20to%20develop%20novel%20DL%20models%20by%20utilizing%20the%20characteristics%20of%0AOCTA%20signals%20and%20images.%20The%20pros%20and%20cons%20of%20each%20DL%20methods%20and%20their%0Aapplications%20discussed%20in%20this%20review%20can%20be%20helpful%20to%20assist%20technicians%20and%0Aclinicians%20to%20use%20proper%20DL%20models%20for%20fundamental%20research%20and%20disease%0Ascreening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%252C%2520segmentation%2520and%2520volumetric%2520rendering%2520of%2520optical%2520coherence%250A%2520%2520tomography%2520angiography%2520%2528OCTA%2529%2520image%2520using%2520deep%2520learning%2520techniques%253A%2520a%2520review%26entry.906535625%3DKejie%2520Chen%2520and%2520Guanbing%2520Gao%2520and%2520Xiaochun%2520Yang%2520and%2520Wenbo%2520Wang%2520and%2520Jing%2520Na%26entry.1292438233%3D%2520%2520Optical%2520coherence%2520tomography%2520angiography%2520%2528OCTA%2529%2520is%2520a%2520non-invasive%2520imaging%250Atechnique%2520widely%2520used%2520to%2520study%2520vascular%2520structures%2520and%2520micro-circulation%250Adynamics%2520in%2520the%2520retina%2520and%2520choroid.%2520OCTA%2520has%2520been%2520widely%2520used%2520in%2520clinics%2520for%250Adiagnosing%2520ocular%2520disease%2520and%2520monitoring%2520its%2520progression%252C%2520because%2520OCTA%2520is%2520safer%250Aand%2520faster%2520than%2520dye-based%2520angiography%2520while%2520retaining%2520the%2520ability%2520to%250Acharacterize%2520micro-scale%2520structures.%2520However%252C%2520OCTA%2520data%2520contains%2520many%2520inherent%250Anoises%2520from%2520the%2520devices%2520and%2520acquisition%2520protocols%2520and%2520suffers%2520from%2520various%250Atypes%2520of%2520artifacts%252C%2520which%2520impairs%2520diagnostic%2520accuracy%2520and%2520repeatability.%2520Deep%250Alearning%2520%2528DL%2529%2520based%2520imaging%2520analysis%2520models%2520are%2520able%2520to%2520automatically%2520detect%250Aand%2520remove%2520artifacts%2520and%2520noises%252C%2520and%2520enhance%2520the%2520quality%2520of%2520image%2520data.%2520It%2520is%250Aalso%2520a%2520powerful%2520tool%2520for%2520segmentation%2520and%2520identification%2520of%2520normal%2520and%250Apathological%2520structures%2520in%2520the%2520images.%2520Thus%252C%2520the%2520value%2520of%2520OCTA%2520imaging%2520can%2520be%250Asignificantly%2520enhanced%2520by%2520the%2520DL-based%2520approaches%2520for%2520interpreting%2520and%250Aperforming%2520measurements%2520and%2520predictions%2520on%2520the%2520OCTA%2520data.%2520In%2520this%2520study%252C%2520we%250Areviewed%2520literature%2520on%2520the%2520DL%2520models%2520for%2520OCTA%2520images%2520in%2520the%2520latest%2520five%2520years.%250AIn%2520particular%252C%2520we%2520focused%2520on%2520discussing%2520the%2520current%2520problems%2520in%2520the%2520OCTA%2520data%250Aand%2520the%2520corresponding%2520design%2520principles%2520of%2520the%2520DL%2520models.%2520We%2520also%2520reviewed%2520the%250Astate-of-art%2520DL%2520models%2520for%25203D%2520volumetric%2520reconstruction%2520of%2520the%2520vascular%250Anetworks%2520and%2520pathological%2520structures%2520such%2520as%2520the%2520edema%2520and%2520distorted%2520optic%250Adisc.%2520In%2520addition%252C%2520the%2520publicly%2520available%2520dataset%2520of%2520OCTA%2520images%2520are%2520summarized%250Aat%2520the%2520end%2520of%2520this%2520review.%2520Overall%252C%2520this%2520review%2520can%2520provide%2520valuable%2520insights%250Afor%2520engineers%2520to%2520develop%2520novel%2520DL%2520models%2520by%2520utilizing%2520the%2520characteristics%2520of%250AOCTA%2520signals%2520and%2520images.%2520The%2520pros%2520and%2520cons%2520of%2520each%2520DL%2520methods%2520and%2520their%250Aapplications%2520discussed%2520in%2520this%2520review%2520can%2520be%2520helpful%2520to%2520assist%2520technicians%2520and%250Aclinicians%2520to%2520use%2520proper%2520DL%2520models%2520for%2520fundamental%2520research%2520and%2520disease%250Ascreening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%2C%20segmentation%20and%20volumetric%20rendering%20of%20optical%20coherence%0A%20%20tomography%20angiography%20%28OCTA%29%20image%20using%20deep%20learning%20techniques%3A%20a%20review&entry.906535625=Kejie%20Chen%20and%20Guanbing%20Gao%20and%20Xiaochun%20Yang%20and%20Wenbo%20Wang%20and%20Jing%20Na&entry.1292438233=%20%20Optical%20coherence%20tomography%20angiography%20%28OCTA%29%20is%20a%20non-invasive%20imaging%0Atechnique%20widely%20used%20to%20study%20vascular%20structures%20and%20micro-circulation%0Adynamics%20in%20the%20retina%20and%20choroid.%20OCTA%20has%20been%20widely%20used%20in%20clinics%20for%0Adiagnosing%20ocular%20disease%20and%20monitoring%20its%20progression%2C%20because%20OCTA%20is%20safer%0Aand%20faster%20than%20dye-based%20angiography%20while%20retaining%20the%20ability%20to%0Acharacterize%20micro-scale%20structures.%20However%2C%20OCTA%20data%20contains%20many%20inherent%0Anoises%20from%20the%20devices%20and%20acquisition%20protocols%20and%20suffers%20from%20various%0Atypes%20of%20artifacts%2C%20which%20impairs%20diagnostic%20accuracy%20and%20repeatability.%20Deep%0Alearning%20%28DL%29%20based%20imaging%20analysis%20models%20are%20able%20to%20automatically%20detect%0Aand%20remove%20artifacts%20and%20noises%2C%20and%20enhance%20the%20quality%20of%20image%20data.%20It%20is%0Aalso%20a%20powerful%20tool%20for%20segmentation%20and%20identification%20of%20normal%20and%0Apathological%20structures%20in%20the%20images.%20Thus%2C%20the%20value%20of%20OCTA%20imaging%20can%20be%0Asignificantly%20enhanced%20by%20the%20DL-based%20approaches%20for%20interpreting%20and%0Aperforming%20measurements%20and%20predictions%20on%20the%20OCTA%20data.%20In%20this%20study%2C%20we%0Areviewed%20literature%20on%20the%20DL%20models%20for%20OCTA%20images%20in%20the%20latest%20five%20years.%0AIn%20particular%2C%20we%20focused%20on%20discussing%20the%20current%20problems%20in%20the%20OCTA%20data%0Aand%20the%20corresponding%20design%20principles%20of%20the%20DL%20models.%20We%20also%20reviewed%20the%0Astate-of-art%20DL%20models%20for%203D%20volumetric%20reconstruction%20of%20the%20vascular%0Anetworks%20and%20pathological%20structures%20such%20as%20the%20edema%20and%20distorted%20optic%0Adisc.%20In%20addition%2C%20the%20publicly%20available%20dataset%20of%20OCTA%20images%20are%20summarized%0Aat%20the%20end%20of%20this%20review.%20Overall%2C%20this%20review%20can%20provide%20valuable%20insights%0Afor%20engineers%20to%20develop%20novel%20DL%20models%20by%20utilizing%20the%20characteristics%20of%0AOCTA%20signals%20and%20images.%20The%20pros%20and%20cons%20of%20each%20DL%20methods%20and%20their%0Aapplications%20discussed%20in%20this%20review%20can%20be%20helpful%20to%20assist%20technicians%20and%0Aclinicians%20to%20use%20proper%20DL%20models%20for%20fundamental%20research%20and%20disease%0Ascreening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14935v2&entry.124074799=Read"},
{"title": "Theory of Mind in Large Language Models: Assessment and Enhancement", "author": "Ruirui Chen and Weifeng Jiang and Chengwei Qin and Cheston Tan", "abstract": "  Theory of Mind (ToM)-the ability to reason about the mental states of oneself\nand others-is a cornerstone of human social intelligence. As Large Language\nModels (LLMs) become increasingly integrated into daily life, understanding\ntheir ability to interpret and respond to human mental states is crucial for\nenabling effective interactions. In this paper, we review LLMs' ToM\ncapabilities by analyzing both evaluation benchmarks and enhancement\nstrategies. For evaluation, we focus on recently proposed and widely used\nstory-based benchmarks. For enhancement, we provide an in-depth analysis of\nrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline\npromising directions for future research to further advance these capabilities\nand better adapt LLMs to more realistic and diverse scenarios. Our survey\nserves as a valuable resource for researchers interested in evaluating and\nadvancing LLMs' ToM capabilities.\n", "link": "http://arxiv.org/abs/2505.00026v2", "date": "2025-08-25", "relevancy": 2.7022, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theory%20of%20Mind%20in%20Large%20Language%20Models%3A%20Assessment%20and%20Enhancement&body=Title%3A%20Theory%20of%20Mind%20in%20Large%20Language%20Models%3A%20Assessment%20and%20Enhancement%0AAuthor%3A%20Ruirui%20Chen%20and%20Weifeng%20Jiang%20and%20Chengwei%20Qin%20and%20Cheston%20Tan%0AAbstract%3A%20%20%20Theory%20of%20Mind%20%28ToM%29-the%20ability%20to%20reason%20about%20the%20mental%20states%20of%20oneself%0Aand%20others-is%20a%20cornerstone%20of%20human%20social%20intelligence.%20As%20Large%20Language%0AModels%20%28LLMs%29%20become%20increasingly%20integrated%20into%20daily%20life%2C%20understanding%0Atheir%20ability%20to%20interpret%20and%20respond%20to%20human%20mental%20states%20is%20crucial%20for%0Aenabling%20effective%20interactions.%20In%20this%20paper%2C%20we%20review%20LLMs%27%20ToM%0Acapabilities%20by%20analyzing%20both%20evaluation%20benchmarks%20and%20enhancement%0Astrategies.%20For%20evaluation%2C%20we%20focus%20on%20recently%20proposed%20and%20widely%20used%0Astory-based%20benchmarks.%20For%20enhancement%2C%20we%20provide%20an%20in-depth%20analysis%20of%0Arecent%20methods%20aimed%20at%20improving%20LLMs%27%20ToM%20abilities.%20Furthermore%2C%20we%20outline%0Apromising%20directions%20for%20future%20research%20to%20further%20advance%20these%20capabilities%0Aand%20better%20adapt%20LLMs%20to%20more%20realistic%20and%20diverse%20scenarios.%20Our%20survey%0Aserves%20as%20a%20valuable%20resource%20for%20researchers%20interested%20in%20evaluating%20and%0Aadvancing%20LLMs%27%20ToM%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheory%2520of%2520Mind%2520in%2520Large%2520Language%2520Models%253A%2520Assessment%2520and%2520Enhancement%26entry.906535625%3DRuirui%2520Chen%2520and%2520Weifeng%2520Jiang%2520and%2520Chengwei%2520Qin%2520and%2520Cheston%2520Tan%26entry.1292438233%3D%2520%2520Theory%2520of%2520Mind%2520%2528ToM%2529-the%2520ability%2520to%2520reason%2520about%2520the%2520mental%2520states%2520of%2520oneself%250Aand%2520others-is%2520a%2520cornerstone%2520of%2520human%2520social%2520intelligence.%2520As%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520become%2520increasingly%2520integrated%2520into%2520daily%2520life%252C%2520understanding%250Atheir%2520ability%2520to%2520interpret%2520and%2520respond%2520to%2520human%2520mental%2520states%2520is%2520crucial%2520for%250Aenabling%2520effective%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520review%2520LLMs%2527%2520ToM%250Acapabilities%2520by%2520analyzing%2520both%2520evaluation%2520benchmarks%2520and%2520enhancement%250Astrategies.%2520For%2520evaluation%252C%2520we%2520focus%2520on%2520recently%2520proposed%2520and%2520widely%2520used%250Astory-based%2520benchmarks.%2520For%2520enhancement%252C%2520we%2520provide%2520an%2520in-depth%2520analysis%2520of%250Arecent%2520methods%2520aimed%2520at%2520improving%2520LLMs%2527%2520ToM%2520abilities.%2520Furthermore%252C%2520we%2520outline%250Apromising%2520directions%2520for%2520future%2520research%2520to%2520further%2520advance%2520these%2520capabilities%250Aand%2520better%2520adapt%2520LLMs%2520to%2520more%2520realistic%2520and%2520diverse%2520scenarios.%2520Our%2520survey%250Aserves%2520as%2520a%2520valuable%2520resource%2520for%2520researchers%2520interested%2520in%2520evaluating%2520and%250Aadvancing%2520LLMs%2527%2520ToM%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theory%20of%20Mind%20in%20Large%20Language%20Models%3A%20Assessment%20and%20Enhancement&entry.906535625=Ruirui%20Chen%20and%20Weifeng%20Jiang%20and%20Chengwei%20Qin%20and%20Cheston%20Tan&entry.1292438233=%20%20Theory%20of%20Mind%20%28ToM%29-the%20ability%20to%20reason%20about%20the%20mental%20states%20of%20oneself%0Aand%20others-is%20a%20cornerstone%20of%20human%20social%20intelligence.%20As%20Large%20Language%0AModels%20%28LLMs%29%20become%20increasingly%20integrated%20into%20daily%20life%2C%20understanding%0Atheir%20ability%20to%20interpret%20and%20respond%20to%20human%20mental%20states%20is%20crucial%20for%0Aenabling%20effective%20interactions.%20In%20this%20paper%2C%20we%20review%20LLMs%27%20ToM%0Acapabilities%20by%20analyzing%20both%20evaluation%20benchmarks%20and%20enhancement%0Astrategies.%20For%20evaluation%2C%20we%20focus%20on%20recently%20proposed%20and%20widely%20used%0Astory-based%20benchmarks.%20For%20enhancement%2C%20we%20provide%20an%20in-depth%20analysis%20of%0Arecent%20methods%20aimed%20at%20improving%20LLMs%27%20ToM%20abilities.%20Furthermore%2C%20we%20outline%0Apromising%20directions%20for%20future%20research%20to%20further%20advance%20these%20capabilities%0Aand%20better%20adapt%20LLMs%20to%20more%20realistic%20and%20diverse%20scenarios.%20Our%20survey%0Aserves%20as%20a%20valuable%20resource%20for%20researchers%20interested%20in%20evaluating%20and%0Aadvancing%20LLMs%27%20ToM%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00026v2&entry.124074799=Read"},
{"title": "Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images", "author": "Kaiyu Li and Xiangyong Cao and Ruixun Liu and Shihong Wang and Zixuan Jiang and Zhi Wang and Deyu Meng", "abstract": "  Semantic segmentation of remote sensing (RS) images is pivotal for\ncomprehensive Earth observation, but the demand for interpreting new object\ncategories, coupled with the high expense of manual annotation, poses\nsignificant challenges. Although open-vocabulary semantic segmentation (OVSS)\noffers a promising solution, existing frameworks designed for natural images\nare insufficient for the unique complexities of RS data. They struggle with\nvast scale variations and fine-grained details, and their adaptation often\nrelies on extensive, costly annotations. To address this critical gap, this\npaper introduces SegEarth-OV, the first framework for annotation-free\nopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,\na universal upsampler that robustly restores high-resolution spatial details\nfrom coarse features, correcting distorted target shapes without any\ntask-specific post-training. We also present a simple yet effective Global Bias\nAlleviation operation to subtract the inherent global context from patch\nfeatures, significantly enhancing local semantic fidelity. These components\nempower SegEarth-OV to effectively harness the rich semantics of pre-trained\nVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the\nframework's universality to other challenging RS modalities like SAR images,\nwhere large-scale VLMs are unavailable and expensive to create, we introduce\nAlignEarth, which is a distillation-based strategy and can efficiently transfer\nsemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the\nneed to build SAR foundation models from scratch and enabling universal OVSS\nacross diverse sensor types. Extensive experiments on both optical and SAR\ndatasets validate that SegEarth-OV can achieve dramatic improvements over the\nSOTA methods, establishing a robust foundation for annotation-free and\nopen-world Earth observation.\n", "link": "http://arxiv.org/abs/2508.18067v1", "date": "2025-08-25", "relevancy": 2.6938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation-Free%20Open-Vocabulary%20Segmentation%20for%20Remote-Sensing%20Images&body=Title%3A%20Annotation-Free%20Open-Vocabulary%20Segmentation%20for%20Remote-Sensing%20Images%0AAuthor%3A%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Ruixun%20Liu%20and%20Shihong%20Wang%20and%20Zixuan%20Jiang%20and%20Zhi%20Wang%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20remote%20sensing%20%28RS%29%20images%20is%20pivotal%20for%0Acomprehensive%20Earth%20observation%2C%20but%20the%20demand%20for%20interpreting%20new%20object%0Acategories%2C%20coupled%20with%20the%20high%20expense%20of%20manual%20annotation%2C%20poses%0Asignificant%20challenges.%20Although%20open-vocabulary%20semantic%20segmentation%20%28OVSS%29%0Aoffers%20a%20promising%20solution%2C%20existing%20frameworks%20designed%20for%20natural%20images%0Aare%20insufficient%20for%20the%20unique%20complexities%20of%20RS%20data.%20They%20struggle%20with%0Avast%20scale%20variations%20and%20fine-grained%20details%2C%20and%20their%20adaptation%20often%0Arelies%20on%20extensive%2C%20costly%20annotations.%20To%20address%20this%20critical%20gap%2C%20this%0Apaper%20introduces%20SegEarth-OV%2C%20the%20first%20framework%20for%20annotation-free%0Aopen-vocabulary%20segmentation%20of%20RS%20images.%20Specifically%2C%20we%20propose%20SimFeatUp%2C%0Aa%20universal%20upsampler%20that%20robustly%20restores%20high-resolution%20spatial%20details%0Afrom%20coarse%20features%2C%20correcting%20distorted%20target%20shapes%20without%20any%0Atask-specific%20post-training.%20We%20also%20present%20a%20simple%20yet%20effective%20Global%20Bias%0AAlleviation%20operation%20to%20subtract%20the%20inherent%20global%20context%20from%20patch%0Afeatures%2C%20significantly%20enhancing%20local%20semantic%20fidelity.%20These%20components%0Aempower%20SegEarth-OV%20to%20effectively%20harness%20the%20rich%20semantics%20of%20pre-trained%0AVLMs%2C%20making%20OVSS%20possible%20in%20optical%20RS%20contexts.%20Furthermore%2C%20to%20extend%20the%0Aframework%27s%20universality%20to%20other%20challenging%20RS%20modalities%20like%20SAR%20images%2C%0Awhere%20large-scale%20VLMs%20are%20unavailable%20and%20expensive%20to%20create%2C%20we%20introduce%0AAlignEarth%2C%20which%20is%20a%20distillation-based%20strategy%20and%20can%20efficiently%20transfer%0Asemantic%20knowledge%20from%20an%20optical%20VLM%20encoder%20to%20an%20SAR%20encoder%2C%20bypassing%20the%0Aneed%20to%20build%20SAR%20foundation%20models%20from%20scratch%20and%20enabling%20universal%20OVSS%0Aacross%20diverse%20sensor%20types.%20Extensive%20experiments%20on%20both%20optical%20and%20SAR%0Adatasets%20validate%20that%20SegEarth-OV%20can%20achieve%20dramatic%20improvements%20over%20the%0ASOTA%20methods%2C%20establishing%20a%20robust%20foundation%20for%20annotation-free%20and%0Aopen-world%20Earth%20observation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation-Free%2520Open-Vocabulary%2520Segmentation%2520for%2520Remote-Sensing%2520Images%26entry.906535625%3DKaiyu%2520Li%2520and%2520Xiangyong%2520Cao%2520and%2520Ruixun%2520Liu%2520and%2520Shihong%2520Wang%2520and%2520Zixuan%2520Jiang%2520and%2520Zhi%2520Wang%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520remote%2520sensing%2520%2528RS%2529%2520images%2520is%2520pivotal%2520for%250Acomprehensive%2520Earth%2520observation%252C%2520but%2520the%2520demand%2520for%2520interpreting%2520new%2520object%250Acategories%252C%2520coupled%2520with%2520the%2520high%2520expense%2520of%2520manual%2520annotation%252C%2520poses%250Asignificant%2520challenges.%2520Although%2520open-vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%250Aoffers%2520a%2520promising%2520solution%252C%2520existing%2520frameworks%2520designed%2520for%2520natural%2520images%250Aare%2520insufficient%2520for%2520the%2520unique%2520complexities%2520of%2520RS%2520data.%2520They%2520struggle%2520with%250Avast%2520scale%2520variations%2520and%2520fine-grained%2520details%252C%2520and%2520their%2520adaptation%2520often%250Arelies%2520on%2520extensive%252C%2520costly%2520annotations.%2520To%2520address%2520this%2520critical%2520gap%252C%2520this%250Apaper%2520introduces%2520SegEarth-OV%252C%2520the%2520first%2520framework%2520for%2520annotation-free%250Aopen-vocabulary%2520segmentation%2520of%2520RS%2520images.%2520Specifically%252C%2520we%2520propose%2520SimFeatUp%252C%250Aa%2520universal%2520upsampler%2520that%2520robustly%2520restores%2520high-resolution%2520spatial%2520details%250Afrom%2520coarse%2520features%252C%2520correcting%2520distorted%2520target%2520shapes%2520without%2520any%250Atask-specific%2520post-training.%2520We%2520also%2520present%2520a%2520simple%2520yet%2520effective%2520Global%2520Bias%250AAlleviation%2520operation%2520to%2520subtract%2520the%2520inherent%2520global%2520context%2520from%2520patch%250Afeatures%252C%2520significantly%2520enhancing%2520local%2520semantic%2520fidelity.%2520These%2520components%250Aempower%2520SegEarth-OV%2520to%2520effectively%2520harness%2520the%2520rich%2520semantics%2520of%2520pre-trained%250AVLMs%252C%2520making%2520OVSS%2520possible%2520in%2520optical%2520RS%2520contexts.%2520Furthermore%252C%2520to%2520extend%2520the%250Aframework%2527s%2520universality%2520to%2520other%2520challenging%2520RS%2520modalities%2520like%2520SAR%2520images%252C%250Awhere%2520large-scale%2520VLMs%2520are%2520unavailable%2520and%2520expensive%2520to%2520create%252C%2520we%2520introduce%250AAlignEarth%252C%2520which%2520is%2520a%2520distillation-based%2520strategy%2520and%2520can%2520efficiently%2520transfer%250Asemantic%2520knowledge%2520from%2520an%2520optical%2520VLM%2520encoder%2520to%2520an%2520SAR%2520encoder%252C%2520bypassing%2520the%250Aneed%2520to%2520build%2520SAR%2520foundation%2520models%2520from%2520scratch%2520and%2520enabling%2520universal%2520OVSS%250Aacross%2520diverse%2520sensor%2520types.%2520Extensive%2520experiments%2520on%2520both%2520optical%2520and%2520SAR%250Adatasets%2520validate%2520that%2520SegEarth-OV%2520can%2520achieve%2520dramatic%2520improvements%2520over%2520the%250ASOTA%2520methods%252C%2520establishing%2520a%2520robust%2520foundation%2520for%2520annotation-free%2520and%250Aopen-world%2520Earth%2520observation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation-Free%20Open-Vocabulary%20Segmentation%20for%20Remote-Sensing%20Images&entry.906535625=Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Ruixun%20Liu%20and%20Shihong%20Wang%20and%20Zixuan%20Jiang%20and%20Zhi%20Wang%20and%20Deyu%20Meng&entry.1292438233=%20%20Semantic%20segmentation%20of%20remote%20sensing%20%28RS%29%20images%20is%20pivotal%20for%0Acomprehensive%20Earth%20observation%2C%20but%20the%20demand%20for%20interpreting%20new%20object%0Acategories%2C%20coupled%20with%20the%20high%20expense%20of%20manual%20annotation%2C%20poses%0Asignificant%20challenges.%20Although%20open-vocabulary%20semantic%20segmentation%20%28OVSS%29%0Aoffers%20a%20promising%20solution%2C%20existing%20frameworks%20designed%20for%20natural%20images%0Aare%20insufficient%20for%20the%20unique%20complexities%20of%20RS%20data.%20They%20struggle%20with%0Avast%20scale%20variations%20and%20fine-grained%20details%2C%20and%20their%20adaptation%20often%0Arelies%20on%20extensive%2C%20costly%20annotations.%20To%20address%20this%20critical%20gap%2C%20this%0Apaper%20introduces%20SegEarth-OV%2C%20the%20first%20framework%20for%20annotation-free%0Aopen-vocabulary%20segmentation%20of%20RS%20images.%20Specifically%2C%20we%20propose%20SimFeatUp%2C%0Aa%20universal%20upsampler%20that%20robustly%20restores%20high-resolution%20spatial%20details%0Afrom%20coarse%20features%2C%20correcting%20distorted%20target%20shapes%20without%20any%0Atask-specific%20post-training.%20We%20also%20present%20a%20simple%20yet%20effective%20Global%20Bias%0AAlleviation%20operation%20to%20subtract%20the%20inherent%20global%20context%20from%20patch%0Afeatures%2C%20significantly%20enhancing%20local%20semantic%20fidelity.%20These%20components%0Aempower%20SegEarth-OV%20to%20effectively%20harness%20the%20rich%20semantics%20of%20pre-trained%0AVLMs%2C%20making%20OVSS%20possible%20in%20optical%20RS%20contexts.%20Furthermore%2C%20to%20extend%20the%0Aframework%27s%20universality%20to%20other%20challenging%20RS%20modalities%20like%20SAR%20images%2C%0Awhere%20large-scale%20VLMs%20are%20unavailable%20and%20expensive%20to%20create%2C%20we%20introduce%0AAlignEarth%2C%20which%20is%20a%20distillation-based%20strategy%20and%20can%20efficiently%20transfer%0Asemantic%20knowledge%20from%20an%20optical%20VLM%20encoder%20to%20an%20SAR%20encoder%2C%20bypassing%20the%0Aneed%20to%20build%20SAR%20foundation%20models%20from%20scratch%20and%20enabling%20universal%20OVSS%0Aacross%20diverse%20sensor%20types.%20Extensive%20experiments%20on%20both%20optical%20and%20SAR%0Adatasets%20validate%20that%20SegEarth-OV%20can%20achieve%20dramatic%20improvements%20over%20the%0ASOTA%20methods%2C%20establishing%20a%20robust%20foundation%20for%20annotation-free%20and%0Aopen-world%20Earth%20observation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18067v1&entry.124074799=Read"},
{"title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health\n  Biomarkers Estimation", "author": "Konstantin Egorov and Stepan Botman and Pavel Blinov and Galina Zubkova and Anton Ivaschenko and Alexander Kolsanov and Andrey Savchenko", "abstract": "  Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical\nissues of existing publicly available datasets: small size, privacy concerns\nwith facial videos, and lack of diversity in conditions. The paper introduces a\nnovel comprehensive large-scale multi-view video dataset for rPPG and health\nbiomarkers estimation. Our dataset comprises 3600 synchronized video recordings\nfrom 600 subjects, captured under varied conditions (resting and post-exercise)\nusing multiple consumer-grade cameras at different angles. To enable multimodal\nanalysis of physiological states, each recording is paired with a 100 Hz PPG\nsignal and extended health metrics, such as electrocardiogram, arterial blood\npressure, biomarkers, temperature, oxygen saturation, respiratory rate, and\nstress level. Using this data, we train an efficient rPPG model and compare its\nquality with existing approaches in cross-dataset scenarios. The public release\nof our dataset and model should significantly speed up the progress in the\ndevelopment of AI medical assistants.\n", "link": "http://arxiv.org/abs/2508.17924v1", "date": "2025-08-25", "relevancy": 2.6924, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5518}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze%20into%20the%20Heart%3A%20A%20Multi-View%20Video%20Dataset%20for%20rPPG%20and%20Health%0A%20%20Biomarkers%20Estimation&body=Title%3A%20Gaze%20into%20the%20Heart%3A%20A%20Multi-View%20Video%20Dataset%20for%20rPPG%20and%20Health%0A%20%20Biomarkers%20Estimation%0AAuthor%3A%20Konstantin%20Egorov%20and%20Stepan%20Botman%20and%20Pavel%20Blinov%20and%20Galina%20Zubkova%20and%20Anton%20Ivaschenko%20and%20Alexander%20Kolsanov%20and%20Andrey%20Savchenko%0AAbstract%3A%20%20%20Progress%20in%20remote%20PhotoPlethysmoGraphy%20%28rPPG%29%20is%20limited%20by%20the%20critical%0Aissues%20of%20existing%20publicly%20available%20datasets%3A%20small%20size%2C%20privacy%20concerns%0Awith%20facial%20videos%2C%20and%20lack%20of%20diversity%20in%20conditions.%20The%20paper%20introduces%20a%0Anovel%20comprehensive%20large-scale%20multi-view%20video%20dataset%20for%20rPPG%20and%20health%0Abiomarkers%20estimation.%20Our%20dataset%20comprises%203600%20synchronized%20video%20recordings%0Afrom%20600%20subjects%2C%20captured%20under%20varied%20conditions%20%28resting%20and%20post-exercise%29%0Ausing%20multiple%20consumer-grade%20cameras%20at%20different%20angles.%20To%20enable%20multimodal%0Aanalysis%20of%20physiological%20states%2C%20each%20recording%20is%20paired%20with%20a%20100%20Hz%20PPG%0Asignal%20and%20extended%20health%20metrics%2C%20such%20as%20electrocardiogram%2C%20arterial%20blood%0Apressure%2C%20biomarkers%2C%20temperature%2C%20oxygen%20saturation%2C%20respiratory%20rate%2C%20and%0Astress%20level.%20Using%20this%20data%2C%20we%20train%20an%20efficient%20rPPG%20model%20and%20compare%20its%0Aquality%20with%20existing%20approaches%20in%20cross-dataset%20scenarios.%20The%20public%20release%0Aof%20our%20dataset%20and%20model%20should%20significantly%20speed%20up%20the%20progress%20in%20the%0Adevelopment%20of%20AI%20medical%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze%2520into%2520the%2520Heart%253A%2520A%2520Multi-View%2520Video%2520Dataset%2520for%2520rPPG%2520and%2520Health%250A%2520%2520Biomarkers%2520Estimation%26entry.906535625%3DKonstantin%2520Egorov%2520and%2520Stepan%2520Botman%2520and%2520Pavel%2520Blinov%2520and%2520Galina%2520Zubkova%2520and%2520Anton%2520Ivaschenko%2520and%2520Alexander%2520Kolsanov%2520and%2520Andrey%2520Savchenko%26entry.1292438233%3D%2520%2520Progress%2520in%2520remote%2520PhotoPlethysmoGraphy%2520%2528rPPG%2529%2520is%2520limited%2520by%2520the%2520critical%250Aissues%2520of%2520existing%2520publicly%2520available%2520datasets%253A%2520small%2520size%252C%2520privacy%2520concerns%250Awith%2520facial%2520videos%252C%2520and%2520lack%2520of%2520diversity%2520in%2520conditions.%2520The%2520paper%2520introduces%2520a%250Anovel%2520comprehensive%2520large-scale%2520multi-view%2520video%2520dataset%2520for%2520rPPG%2520and%2520health%250Abiomarkers%2520estimation.%2520Our%2520dataset%2520comprises%25203600%2520synchronized%2520video%2520recordings%250Afrom%2520600%2520subjects%252C%2520captured%2520under%2520varied%2520conditions%2520%2528resting%2520and%2520post-exercise%2529%250Ausing%2520multiple%2520consumer-grade%2520cameras%2520at%2520different%2520angles.%2520To%2520enable%2520multimodal%250Aanalysis%2520of%2520physiological%2520states%252C%2520each%2520recording%2520is%2520paired%2520with%2520a%2520100%2520Hz%2520PPG%250Asignal%2520and%2520extended%2520health%2520metrics%252C%2520such%2520as%2520electrocardiogram%252C%2520arterial%2520blood%250Apressure%252C%2520biomarkers%252C%2520temperature%252C%2520oxygen%2520saturation%252C%2520respiratory%2520rate%252C%2520and%250Astress%2520level.%2520Using%2520this%2520data%252C%2520we%2520train%2520an%2520efficient%2520rPPG%2520model%2520and%2520compare%2520its%250Aquality%2520with%2520existing%2520approaches%2520in%2520cross-dataset%2520scenarios.%2520The%2520public%2520release%250Aof%2520our%2520dataset%2520and%2520model%2520should%2520significantly%2520speed%2520up%2520the%2520progress%2520in%2520the%250Adevelopment%2520of%2520AI%2520medical%2520assistants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze%20into%20the%20Heart%3A%20A%20Multi-View%20Video%20Dataset%20for%20rPPG%20and%20Health%0A%20%20Biomarkers%20Estimation&entry.906535625=Konstantin%20Egorov%20and%20Stepan%20Botman%20and%20Pavel%20Blinov%20and%20Galina%20Zubkova%20and%20Anton%20Ivaschenko%20and%20Alexander%20Kolsanov%20and%20Andrey%20Savchenko&entry.1292438233=%20%20Progress%20in%20remote%20PhotoPlethysmoGraphy%20%28rPPG%29%20is%20limited%20by%20the%20critical%0Aissues%20of%20existing%20publicly%20available%20datasets%3A%20small%20size%2C%20privacy%20concerns%0Awith%20facial%20videos%2C%20and%20lack%20of%20diversity%20in%20conditions.%20The%20paper%20introduces%20a%0Anovel%20comprehensive%20large-scale%20multi-view%20video%20dataset%20for%20rPPG%20and%20health%0Abiomarkers%20estimation.%20Our%20dataset%20comprises%203600%20synchronized%20video%20recordings%0Afrom%20600%20subjects%2C%20captured%20under%20varied%20conditions%20%28resting%20and%20post-exercise%29%0Ausing%20multiple%20consumer-grade%20cameras%20at%20different%20angles.%20To%20enable%20multimodal%0Aanalysis%20of%20physiological%20states%2C%20each%20recording%20is%20paired%20with%20a%20100%20Hz%20PPG%0Asignal%20and%20extended%20health%20metrics%2C%20such%20as%20electrocardiogram%2C%20arterial%20blood%0Apressure%2C%20biomarkers%2C%20temperature%2C%20oxygen%20saturation%2C%20respiratory%20rate%2C%20and%0Astress%20level.%20Using%20this%20data%2C%20we%20train%20an%20efficient%20rPPG%20model%20and%20compare%20its%0Aquality%20with%20existing%20approaches%20in%20cross-dataset%20scenarios.%20The%20public%20release%0Aof%20our%20dataset%20and%20model%20should%20significantly%20speed%20up%20the%20progress%20in%20the%0Adevelopment%20of%20AI%20medical%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17924v1&entry.124074799=Read"},
{"title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for\n  Tumor Flagging and Staging", "author": "Farnaz Khun Jush and Steffen Vogler and Matthias Lenga", "abstract": "  The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.\n", "link": "http://arxiv.org/abs/2507.17412v2", "date": "2025-08-25", "relevancy": 2.6653, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5355}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging&body=Title%3A%20Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging%0AAuthor%3A%20Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20The%20increasing%20volume%20of%20medical%20images%20poses%20challenges%20for%20radiologists%20in%0Aretrieving%20relevant%20cases.%20Content-based%20image%20retrieval%20%28CBIR%29%20systems%20offer%0Apotential%20for%20efficient%20access%20to%20similar%20cases%2C%20yet%20lack%20standardized%0Aevaluation%20and%20comprehensive%20studies.%20Building%20on%20prior%20studies%20for%20tumor%0Acharacterization%20via%20CBIR%2C%20this%20study%20advances%20CBIR%20research%20for%20volumetric%0Amedical%20images%20through%20three%20key%20contributions%3A%20%281%29%20a%20framework%20eliminating%0Areliance%20on%20pre-segmented%20data%20and%20organ-specific%20datasets%2C%20aligning%20with%20large%0Aand%20unstructured%20image%20archiving%20systems%2C%20i.e.%20PACS%20in%20clinical%20practice%3B%20%282%29%0Aintroduction%20of%20C-MIR%2C%20a%20novel%20volumetric%20re-ranking%20method%20adapting%20ColBERT%27s%0Acontextualized%20late%20interaction%20mechanism%20for%203D%20medical%20imaging%3B%20%283%29%0Acomprehensive%20evaluation%20across%20four%20tumor%20sites%20using%20three%20feature%20extractors%0Aand%20three%20database%20configurations.%20Our%20evaluations%20highlight%20the%20significant%0Aadvantages%20of%20C-MIR.%20We%20demonstrate%20the%20successful%20adaptation%20of%20the%20late%0Ainteraction%20principle%20to%20volumetric%20medical%20images%2C%20enabling%20effective%0Acontext-aware%20re-ranking.%20A%20key%20finding%20is%20C-MIR%27s%20ability%20to%20effectively%0Alocalize%20the%20region%20of%20interest%2C%20eliminating%20the%20need%20for%20pre-segmentation%20of%0Adatasets%20and%20offering%20a%20computationally%20efficient%20alternative%20to%20systems%0Arelying%20on%20expensive%20data%20enrichment%20steps.%20C-MIR%20demonstrates%20promising%0Aimprovements%20in%20tumor%20flagging%2C%20achieving%20improved%20performance%2C%20particularly%0Afor%20colon%20and%20lung%20tumors%20%28p%3C0.05%29.%20C-MIR%20also%20shows%20potential%20for%20improving%0Atumor%20staging%2C%20warranting%20further%20exploration%20of%20its%20capabilities.%20Ultimately%2C%0Aour%20work%20seeks%20to%20bridge%20the%20gap%20between%20advanced%20retrieval%20techniques%20and%0Atheir%20practical%20applications%20in%20healthcare%2C%20paving%20the%20way%20for%20improved%0Adiagnostic%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-based%25203D%2520Image%2520Retrieval%2520and%2520a%2520ColBERT-inspired%2520Re-ranking%2520for%250A%2520%2520Tumor%2520Flagging%2520and%2520Staging%26entry.906535625%3DFarnaz%2520Khun%2520Jush%2520and%2520Steffen%2520Vogler%2520and%2520Matthias%2520Lenga%26entry.1292438233%3D%2520%2520The%2520increasing%2520volume%2520of%2520medical%2520images%2520poses%2520challenges%2520for%2520radiologists%2520in%250Aretrieving%2520relevant%2520cases.%2520Content-based%2520image%2520retrieval%2520%2528CBIR%2529%2520systems%2520offer%250Apotential%2520for%2520efficient%2520access%2520to%2520similar%2520cases%252C%2520yet%2520lack%2520standardized%250Aevaluation%2520and%2520comprehensive%2520studies.%2520Building%2520on%2520prior%2520studies%2520for%2520tumor%250Acharacterization%2520via%2520CBIR%252C%2520this%2520study%2520advances%2520CBIR%2520research%2520for%2520volumetric%250Amedical%2520images%2520through%2520three%2520key%2520contributions%253A%2520%25281%2529%2520a%2520framework%2520eliminating%250Areliance%2520on%2520pre-segmented%2520data%2520and%2520organ-specific%2520datasets%252C%2520aligning%2520with%2520large%250Aand%2520unstructured%2520image%2520archiving%2520systems%252C%2520i.e.%2520PACS%2520in%2520clinical%2520practice%253B%2520%25282%2529%250Aintroduction%2520of%2520C-MIR%252C%2520a%2520novel%2520volumetric%2520re-ranking%2520method%2520adapting%2520ColBERT%2527s%250Acontextualized%2520late%2520interaction%2520mechanism%2520for%25203D%2520medical%2520imaging%253B%2520%25283%2529%250Acomprehensive%2520evaluation%2520across%2520four%2520tumor%2520sites%2520using%2520three%2520feature%2520extractors%250Aand%2520three%2520database%2520configurations.%2520Our%2520evaluations%2520highlight%2520the%2520significant%250Aadvantages%2520of%2520C-MIR.%2520We%2520demonstrate%2520the%2520successful%2520adaptation%2520of%2520the%2520late%250Ainteraction%2520principle%2520to%2520volumetric%2520medical%2520images%252C%2520enabling%2520effective%250Acontext-aware%2520re-ranking.%2520A%2520key%2520finding%2520is%2520C-MIR%2527s%2520ability%2520to%2520effectively%250Alocalize%2520the%2520region%2520of%2520interest%252C%2520eliminating%2520the%2520need%2520for%2520pre-segmentation%2520of%250Adatasets%2520and%2520offering%2520a%2520computationally%2520efficient%2520alternative%2520to%2520systems%250Arelying%2520on%2520expensive%2520data%2520enrichment%2520steps.%2520C-MIR%2520demonstrates%2520promising%250Aimprovements%2520in%2520tumor%2520flagging%252C%2520achieving%2520improved%2520performance%252C%2520particularly%250Afor%2520colon%2520and%2520lung%2520tumors%2520%2528p%253C0.05%2529.%2520C-MIR%2520also%2520shows%2520potential%2520for%2520improving%250Atumor%2520staging%252C%2520warranting%2520further%2520exploration%2520of%2520its%2520capabilities.%2520Ultimately%252C%250Aour%2520work%2520seeks%2520to%2520bridge%2520the%2520gap%2520between%2520advanced%2520retrieval%2520techniques%2520and%250Atheir%2520practical%2520applications%2520in%2520healthcare%252C%2520paving%2520the%2520way%2520for%2520improved%250Adiagnostic%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-based%203D%20Image%20Retrieval%20and%20a%20ColBERT-inspired%20Re-ranking%20for%0A%20%20Tumor%20Flagging%20and%20Staging&entry.906535625=Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Matthias%20Lenga&entry.1292438233=%20%20The%20increasing%20volume%20of%20medical%20images%20poses%20challenges%20for%20radiologists%20in%0Aretrieving%20relevant%20cases.%20Content-based%20image%20retrieval%20%28CBIR%29%20systems%20offer%0Apotential%20for%20efficient%20access%20to%20similar%20cases%2C%20yet%20lack%20standardized%0Aevaluation%20and%20comprehensive%20studies.%20Building%20on%20prior%20studies%20for%20tumor%0Acharacterization%20via%20CBIR%2C%20this%20study%20advances%20CBIR%20research%20for%20volumetric%0Amedical%20images%20through%20three%20key%20contributions%3A%20%281%29%20a%20framework%20eliminating%0Areliance%20on%20pre-segmented%20data%20and%20organ-specific%20datasets%2C%20aligning%20with%20large%0Aand%20unstructured%20image%20archiving%20systems%2C%20i.e.%20PACS%20in%20clinical%20practice%3B%20%282%29%0Aintroduction%20of%20C-MIR%2C%20a%20novel%20volumetric%20re-ranking%20method%20adapting%20ColBERT%27s%0Acontextualized%20late%20interaction%20mechanism%20for%203D%20medical%20imaging%3B%20%283%29%0Acomprehensive%20evaluation%20across%20four%20tumor%20sites%20using%20three%20feature%20extractors%0Aand%20three%20database%20configurations.%20Our%20evaluations%20highlight%20the%20significant%0Aadvantages%20of%20C-MIR.%20We%20demonstrate%20the%20successful%20adaptation%20of%20the%20late%0Ainteraction%20principle%20to%20volumetric%20medical%20images%2C%20enabling%20effective%0Acontext-aware%20re-ranking.%20A%20key%20finding%20is%20C-MIR%27s%20ability%20to%20effectively%0Alocalize%20the%20region%20of%20interest%2C%20eliminating%20the%20need%20for%20pre-segmentation%20of%0Adatasets%20and%20offering%20a%20computationally%20efficient%20alternative%20to%20systems%0Arelying%20on%20expensive%20data%20enrichment%20steps.%20C-MIR%20demonstrates%20promising%0Aimprovements%20in%20tumor%20flagging%2C%20achieving%20improved%20performance%2C%20particularly%0Afor%20colon%20and%20lung%20tumors%20%28p%3C0.05%29.%20C-MIR%20also%20shows%20potential%20for%20improving%0Atumor%20staging%2C%20warranting%20further%20exploration%20of%20its%20capabilities.%20Ultimately%2C%0Aour%20work%20seeks%20to%20bridge%20the%20gap%20between%20advanced%20retrieval%20techniques%20and%0Atheir%20practical%20applications%20in%20healthcare%2C%20paving%20the%20way%20for%20improved%0Adiagnostic%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17412v2&entry.124074799=Read"},
{"title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital\n  Twins", "author": "Eduardo de Conto and Blaise Genest and Arvind Easwaran and Nicholas Ng and Shweta Menon", "abstract": "  Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure.\n", "link": "http://arxiv.org/abs/2508.17988v1", "date": "2025-08-25", "relevancy": 2.6279, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5289}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DesCartes%20Builder%3A%20A%20Tool%20to%20Develop%20Machine-Learning%20Based%20Digital%0A%20%20Twins&body=Title%3A%20DesCartes%20Builder%3A%20A%20Tool%20to%20Develop%20Machine-Learning%20Based%20Digital%0A%20%20Twins%0AAuthor%3A%20Eduardo%20de%20Conto%20and%20Blaise%20Genest%20and%20Arvind%20Easwaran%20and%20Nicholas%20Ng%20and%20Shweta%20Menon%0AAbstract%3A%20%20%20Digital%20twins%20%28DTs%29%20are%20increasingly%20utilized%20to%20monitor%2C%20manage%2C%20and%0Aoptimize%20complex%20systems%20across%20various%20domains%2C%20including%20civil%20engineering.%20A%0Acore%20requirement%20for%20an%20effective%20DT%20is%20to%20act%20as%20a%20fast%2C%20accurate%2C%20and%0Amaintainable%20surrogate%20of%20its%20physical%20counterpart%2C%20the%20physical%20twin%20%28PT%29.%20To%0Athis%20end%2C%20machine%20learning%20%28ML%29%20is%20frequently%20employed%20to%20%28i%29%20construct%0Areal-time%20DT%20prototypes%20using%20efficient%20reduced-order%20models%20%28ROMs%29%20derived%0Afrom%20high-fidelity%20simulations%20of%20the%20PT%27s%20nominal%20behavior%2C%20and%20%28ii%29%0Aspecialize%20these%20prototypes%20into%20DT%20instances%20by%20leveraging%20historical%20sensor%0Adata%20from%20the%20target%20PT.%20Despite%20the%20broad%20applicability%20of%20ML%2C%20its%20use%20in%20DT%0Aengineering%20remains%20largely%20ad%20hoc.%20Indeed%2C%20while%20conventional%20ML%20pipelines%0Aoften%20train%20a%20single%20model%20for%20a%20specific%20task%2C%20DTs%20typically%20require%20multiple%2C%0Atask-%20and%20domain-dependent%20models.%20Thus%2C%20a%20more%20structured%20approach%20is%20required%0Ato%20design%20DTs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20DesCartes%20Builder%2C%20an%20open-source%20tool%20to%20enable%0Athe%20systematic%20engineering%20of%20ML-based%20pipelines%20for%20real-time%20DT%20prototypes%0Aand%20DT%20instances.%20The%20tool%20leverages%20an%20open%20and%20flexible%20visual%20data%20flow%0Aparadigm%20to%20facilitate%20the%20specification%2C%20composition%2C%20and%20reuse%20of%20ML%20models.%0AIt%20also%20integrates%20a%20library%20of%20parameterizable%20core%20operations%20and%20ML%0Aalgorithms%20tailored%20for%20DT%20design.%20We%20demonstrate%20the%20effectiveness%20and%0Ausability%20of%20DesCartes%20Builder%20through%20a%20civil%20engineering%20use%20case%20involving%0Athe%20design%20of%20a%20real-time%20DT%20prototype%20to%20predict%20the%20plastic%20strain%20of%20a%0Astructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesCartes%2520Builder%253A%2520A%2520Tool%2520to%2520Develop%2520Machine-Learning%2520Based%2520Digital%250A%2520%2520Twins%26entry.906535625%3DEduardo%2520de%2520Conto%2520and%2520Blaise%2520Genest%2520and%2520Arvind%2520Easwaran%2520and%2520Nicholas%2520Ng%2520and%2520Shweta%2520Menon%26entry.1292438233%3D%2520%2520Digital%2520twins%2520%2528DTs%2529%2520are%2520increasingly%2520utilized%2520to%2520monitor%252C%2520manage%252C%2520and%250Aoptimize%2520complex%2520systems%2520across%2520various%2520domains%252C%2520including%2520civil%2520engineering.%2520A%250Acore%2520requirement%2520for%2520an%2520effective%2520DT%2520is%2520to%2520act%2520as%2520a%2520fast%252C%2520accurate%252C%2520and%250Amaintainable%2520surrogate%2520of%2520its%2520physical%2520counterpart%252C%2520the%2520physical%2520twin%2520%2528PT%2529.%2520To%250Athis%2520end%252C%2520machine%2520learning%2520%2528ML%2529%2520is%2520frequently%2520employed%2520to%2520%2528i%2529%2520construct%250Areal-time%2520DT%2520prototypes%2520using%2520efficient%2520reduced-order%2520models%2520%2528ROMs%2529%2520derived%250Afrom%2520high-fidelity%2520simulations%2520of%2520the%2520PT%2527s%2520nominal%2520behavior%252C%2520and%2520%2528ii%2529%250Aspecialize%2520these%2520prototypes%2520into%2520DT%2520instances%2520by%2520leveraging%2520historical%2520sensor%250Adata%2520from%2520the%2520target%2520PT.%2520Despite%2520the%2520broad%2520applicability%2520of%2520ML%252C%2520its%2520use%2520in%2520DT%250Aengineering%2520remains%2520largely%2520ad%2520hoc.%2520Indeed%252C%2520while%2520conventional%2520ML%2520pipelines%250Aoften%2520train%2520a%2520single%2520model%2520for%2520a%2520specific%2520task%252C%2520DTs%2520typically%2520require%2520multiple%252C%250Atask-%2520and%2520domain-dependent%2520models.%2520Thus%252C%2520a%2520more%2520structured%2520approach%2520is%2520required%250Ato%2520design%2520DTs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DesCartes%2520Builder%252C%2520an%2520open-source%2520tool%2520to%2520enable%250Athe%2520systematic%2520engineering%2520of%2520ML-based%2520pipelines%2520for%2520real-time%2520DT%2520prototypes%250Aand%2520DT%2520instances.%2520The%2520tool%2520leverages%2520an%2520open%2520and%2520flexible%2520visual%2520data%2520flow%250Aparadigm%2520to%2520facilitate%2520the%2520specification%252C%2520composition%252C%2520and%2520reuse%2520of%2520ML%2520models.%250AIt%2520also%2520integrates%2520a%2520library%2520of%2520parameterizable%2520core%2520operations%2520and%2520ML%250Aalgorithms%2520tailored%2520for%2520DT%2520design.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%250Ausability%2520of%2520DesCartes%2520Builder%2520through%2520a%2520civil%2520engineering%2520use%2520case%2520involving%250Athe%2520design%2520of%2520a%2520real-time%2520DT%2520prototype%2520to%2520predict%2520the%2520plastic%2520strain%2520of%2520a%250Astructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DesCartes%20Builder%3A%20A%20Tool%20to%20Develop%20Machine-Learning%20Based%20Digital%0A%20%20Twins&entry.906535625=Eduardo%20de%20Conto%20and%20Blaise%20Genest%20and%20Arvind%20Easwaran%20and%20Nicholas%20Ng%20and%20Shweta%20Menon&entry.1292438233=%20%20Digital%20twins%20%28DTs%29%20are%20increasingly%20utilized%20to%20monitor%2C%20manage%2C%20and%0Aoptimize%20complex%20systems%20across%20various%20domains%2C%20including%20civil%20engineering.%20A%0Acore%20requirement%20for%20an%20effective%20DT%20is%20to%20act%20as%20a%20fast%2C%20accurate%2C%20and%0Amaintainable%20surrogate%20of%20its%20physical%20counterpart%2C%20the%20physical%20twin%20%28PT%29.%20To%0Athis%20end%2C%20machine%20learning%20%28ML%29%20is%20frequently%20employed%20to%20%28i%29%20construct%0Areal-time%20DT%20prototypes%20using%20efficient%20reduced-order%20models%20%28ROMs%29%20derived%0Afrom%20high-fidelity%20simulations%20of%20the%20PT%27s%20nominal%20behavior%2C%20and%20%28ii%29%0Aspecialize%20these%20prototypes%20into%20DT%20instances%20by%20leveraging%20historical%20sensor%0Adata%20from%20the%20target%20PT.%20Despite%20the%20broad%20applicability%20of%20ML%2C%20its%20use%20in%20DT%0Aengineering%20remains%20largely%20ad%20hoc.%20Indeed%2C%20while%20conventional%20ML%20pipelines%0Aoften%20train%20a%20single%20model%20for%20a%20specific%20task%2C%20DTs%20typically%20require%20multiple%2C%0Atask-%20and%20domain-dependent%20models.%20Thus%2C%20a%20more%20structured%20approach%20is%20required%0Ato%20design%20DTs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20DesCartes%20Builder%2C%20an%20open-source%20tool%20to%20enable%0Athe%20systematic%20engineering%20of%20ML-based%20pipelines%20for%20real-time%20DT%20prototypes%0Aand%20DT%20instances.%20The%20tool%20leverages%20an%20open%20and%20flexible%20visual%20data%20flow%0Aparadigm%20to%20facilitate%20the%20specification%2C%20composition%2C%20and%20reuse%20of%20ML%20models.%0AIt%20also%20integrates%20a%20library%20of%20parameterizable%20core%20operations%20and%20ML%0Aalgorithms%20tailored%20for%20DT%20design.%20We%20demonstrate%20the%20effectiveness%20and%0Ausability%20of%20DesCartes%20Builder%20through%20a%20civil%20engineering%20use%20case%20involving%0Athe%20design%20of%20a%20real-time%20DT%20prototype%20to%20predict%20the%20plastic%20strain%20of%20a%0Astructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17988v1&entry.124074799=Read"},
{"title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer\n  for Photorealistic Video Restoration", "author": "Haoran Bai and Xiaoxu Chen and Canqian Yang and Zongyao He and Sibin Deng and Ying Chen", "abstract": "  We present Vivid-VR, a DiT-based generative video restoration method built\nupon an advanced T2V foundation model, where ControlNet is leveraged to control\nthe generation process, ensuring content consistency. However, conventional\nfine-tuning of such controllable pipelines frequently suffers from distribution\ndrift due to limitations in imperfect multimodal alignment, resulting in\ncompromised texture realism and temporal coherence. To tackle this challenge,\nwe propose a concept distillation training strategy that utilizes the\npretrained T2V model to synthesize training samples with embedded textual\nconcepts, thereby distilling its conceptual understanding to preserve texture\nand temporal quality. To enhance generation controllability, we redesign the\ncontrol architecture with two key components: 1) a control feature projector\nthat filters degradation artifacts from input video latents to minimize their\npropagation through the generation pipeline, and 2) a new ControlNet connector\nemploying a dual-branch design. This connector synergistically combines\nMLP-based feature mapping with cross-attention mechanism for dynamic control\nfeature retrieval, enabling both content preservation and adaptive control\nsignal modulation. Extensive experiments show that Vivid-VR performs favorably\nagainst existing approaches on both synthetic and real-world benchmarks, as\nwell as AIGC videos, achieving impressive texture realism, visual vividness,\nand temporal consistency. The codes and checkpoints are publicly available at\nhttps://github.com/csbhr/Vivid-VR.\n", "link": "http://arxiv.org/abs/2508.14483v2", "date": "2025-08-25", "relevancy": 2.6228, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6682}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6508}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vivid-VR%3A%20Distilling%20Concepts%20from%20Text-to-Video%20Diffusion%20Transformer%0A%20%20for%20Photorealistic%20Video%20Restoration&body=Title%3A%20Vivid-VR%3A%20Distilling%20Concepts%20from%20Text-to-Video%20Diffusion%20Transformer%0A%20%20for%20Photorealistic%20Video%20Restoration%0AAuthor%3A%20Haoran%20Bai%20and%20Xiaoxu%20Chen%20and%20Canqian%20Yang%20and%20Zongyao%20He%20and%20Sibin%20Deng%20and%20Ying%20Chen%0AAbstract%3A%20%20%20We%20present%20Vivid-VR%2C%20a%20DiT-based%20generative%20video%20restoration%20method%20built%0Aupon%20an%20advanced%20T2V%20foundation%20model%2C%20where%20ControlNet%20is%20leveraged%20to%20control%0Athe%20generation%20process%2C%20ensuring%20content%20consistency.%20However%2C%20conventional%0Afine-tuning%20of%20such%20controllable%20pipelines%20frequently%20suffers%20from%20distribution%0Adrift%20due%20to%20limitations%20in%20imperfect%20multimodal%20alignment%2C%20resulting%20in%0Acompromised%20texture%20realism%20and%20temporal%20coherence.%20To%20tackle%20this%20challenge%2C%0Awe%20propose%20a%20concept%20distillation%20training%20strategy%20that%20utilizes%20the%0Apretrained%20T2V%20model%20to%20synthesize%20training%20samples%20with%20embedded%20textual%0Aconcepts%2C%20thereby%20distilling%20its%20conceptual%20understanding%20to%20preserve%20texture%0Aand%20temporal%20quality.%20To%20enhance%20generation%20controllability%2C%20we%20redesign%20the%0Acontrol%20architecture%20with%20two%20key%20components%3A%201%29%20a%20control%20feature%20projector%0Athat%20filters%20degradation%20artifacts%20from%20input%20video%20latents%20to%20minimize%20their%0Apropagation%20through%20the%20generation%20pipeline%2C%20and%202%29%20a%20new%20ControlNet%20connector%0Aemploying%20a%20dual-branch%20design.%20This%20connector%20synergistically%20combines%0AMLP-based%20feature%20mapping%20with%20cross-attention%20mechanism%20for%20dynamic%20control%0Afeature%20retrieval%2C%20enabling%20both%20content%20preservation%20and%20adaptive%20control%0Asignal%20modulation.%20Extensive%20experiments%20show%20that%20Vivid-VR%20performs%20favorably%0Aagainst%20existing%20approaches%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20as%0Awell%20as%20AIGC%20videos%2C%20achieving%20impressive%20texture%20realism%2C%20visual%20vividness%2C%0Aand%20temporal%20consistency.%20The%20codes%20and%20checkpoints%20are%20publicly%20available%20at%0Ahttps%3A//github.com/csbhr/Vivid-VR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVivid-VR%253A%2520Distilling%2520Concepts%2520from%2520Text-to-Video%2520Diffusion%2520Transformer%250A%2520%2520for%2520Photorealistic%2520Video%2520Restoration%26entry.906535625%3DHaoran%2520Bai%2520and%2520Xiaoxu%2520Chen%2520and%2520Canqian%2520Yang%2520and%2520Zongyao%2520He%2520and%2520Sibin%2520Deng%2520and%2520Ying%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520Vivid-VR%252C%2520a%2520DiT-based%2520generative%2520video%2520restoration%2520method%2520built%250Aupon%2520an%2520advanced%2520T2V%2520foundation%2520model%252C%2520where%2520ControlNet%2520is%2520leveraged%2520to%2520control%250Athe%2520generation%2520process%252C%2520ensuring%2520content%2520consistency.%2520However%252C%2520conventional%250Afine-tuning%2520of%2520such%2520controllable%2520pipelines%2520frequently%2520suffers%2520from%2520distribution%250Adrift%2520due%2520to%2520limitations%2520in%2520imperfect%2520multimodal%2520alignment%252C%2520resulting%2520in%250Acompromised%2520texture%2520realism%2520and%2520temporal%2520coherence.%2520To%2520tackle%2520this%2520challenge%252C%250Awe%2520propose%2520a%2520concept%2520distillation%2520training%2520strategy%2520that%2520utilizes%2520the%250Apretrained%2520T2V%2520model%2520to%2520synthesize%2520training%2520samples%2520with%2520embedded%2520textual%250Aconcepts%252C%2520thereby%2520distilling%2520its%2520conceptual%2520understanding%2520to%2520preserve%2520texture%250Aand%2520temporal%2520quality.%2520To%2520enhance%2520generation%2520controllability%252C%2520we%2520redesign%2520the%250Acontrol%2520architecture%2520with%2520two%2520key%2520components%253A%25201%2529%2520a%2520control%2520feature%2520projector%250Athat%2520filters%2520degradation%2520artifacts%2520from%2520input%2520video%2520latents%2520to%2520minimize%2520their%250Apropagation%2520through%2520the%2520generation%2520pipeline%252C%2520and%25202%2529%2520a%2520new%2520ControlNet%2520connector%250Aemploying%2520a%2520dual-branch%2520design.%2520This%2520connector%2520synergistically%2520combines%250AMLP-based%2520feature%2520mapping%2520with%2520cross-attention%2520mechanism%2520for%2520dynamic%2520control%250Afeature%2520retrieval%252C%2520enabling%2520both%2520content%2520preservation%2520and%2520adaptive%2520control%250Asignal%2520modulation.%2520Extensive%2520experiments%2520show%2520that%2520Vivid-VR%2520performs%2520favorably%250Aagainst%2520existing%2520approaches%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%252C%2520as%250Awell%2520as%2520AIGC%2520videos%252C%2520achieving%2520impressive%2520texture%2520realism%252C%2520visual%2520vividness%252C%250Aand%2520temporal%2520consistency.%2520The%2520codes%2520and%2520checkpoints%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/csbhr/Vivid-VR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vivid-VR%3A%20Distilling%20Concepts%20from%20Text-to-Video%20Diffusion%20Transformer%0A%20%20for%20Photorealistic%20Video%20Restoration&entry.906535625=Haoran%20Bai%20and%20Xiaoxu%20Chen%20and%20Canqian%20Yang%20and%20Zongyao%20He%20and%20Sibin%20Deng%20and%20Ying%20Chen&entry.1292438233=%20%20We%20present%20Vivid-VR%2C%20a%20DiT-based%20generative%20video%20restoration%20method%20built%0Aupon%20an%20advanced%20T2V%20foundation%20model%2C%20where%20ControlNet%20is%20leveraged%20to%20control%0Athe%20generation%20process%2C%20ensuring%20content%20consistency.%20However%2C%20conventional%0Afine-tuning%20of%20such%20controllable%20pipelines%20frequently%20suffers%20from%20distribution%0Adrift%20due%20to%20limitations%20in%20imperfect%20multimodal%20alignment%2C%20resulting%20in%0Acompromised%20texture%20realism%20and%20temporal%20coherence.%20To%20tackle%20this%20challenge%2C%0Awe%20propose%20a%20concept%20distillation%20training%20strategy%20that%20utilizes%20the%0Apretrained%20T2V%20model%20to%20synthesize%20training%20samples%20with%20embedded%20textual%0Aconcepts%2C%20thereby%20distilling%20its%20conceptual%20understanding%20to%20preserve%20texture%0Aand%20temporal%20quality.%20To%20enhance%20generation%20controllability%2C%20we%20redesign%20the%0Acontrol%20architecture%20with%20two%20key%20components%3A%201%29%20a%20control%20feature%20projector%0Athat%20filters%20degradation%20artifacts%20from%20input%20video%20latents%20to%20minimize%20their%0Apropagation%20through%20the%20generation%20pipeline%2C%20and%202%29%20a%20new%20ControlNet%20connector%0Aemploying%20a%20dual-branch%20design.%20This%20connector%20synergistically%20combines%0AMLP-based%20feature%20mapping%20with%20cross-attention%20mechanism%20for%20dynamic%20control%0Afeature%20retrieval%2C%20enabling%20both%20content%20preservation%20and%20adaptive%20control%0Asignal%20modulation.%20Extensive%20experiments%20show%20that%20Vivid-VR%20performs%20favorably%0Aagainst%20existing%20approaches%20on%20both%20synthetic%20and%20real-world%20benchmarks%2C%20as%0Awell%20as%20AIGC%20videos%2C%20achieving%20impressive%20texture%20realism%2C%20visual%20vividness%2C%0Aand%20temporal%20consistency.%20The%20codes%20and%20checkpoints%20are%20publicly%20available%20at%0Ahttps%3A//github.com/csbhr/Vivid-VR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14483v2&entry.124074799=Read"},
{"title": "Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation", "author": "Jungeun Kim and Hyeongwoo Jeon and Jongseong Bae and Ha Young Kim", "abstract": "  Sign language translation (SLT) is a challenging task that involves\ntranslating sign language images into spoken language. For SLT models to\nperform this task successfully, they must bridge the modality gap and identify\nsubtle variations in sign language components to understand their meanings\naccurately. To address these challenges, we propose a novel gloss-free SLT\nframework called Multimodal Sign Language Translation (MMSLT), which leverages\nthe representational capabilities of off-the-shelf multimodal large language\nmodels (MLLMs). Specifically, we use MLLMs to generate detailed textual\ndescriptions of sign language components. Then, through our proposed\nmultimodal-language pre-training module, we integrate these description\nfeatures with sign video features to align them within the spoken sentence\nspace. Our approach achieves state-of-the-art performance on benchmark datasets\nPHOENIX14T and CSL-Daily, highlighting the potential of MLLMs to be utilized\neffectively in SLT. Code is available at https://github.com/hwjeon98/MMSLT.\n", "link": "http://arxiv.org/abs/2411.16789v2", "date": "2025-08-25", "relevancy": 2.6205, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20the%20Power%20of%20MLLMs%20for%20Gloss-Free%20Sign%20Language%20Translation&body=Title%3A%20Leveraging%20the%20Power%20of%20MLLMs%20for%20Gloss-Free%20Sign%20Language%20Translation%0AAuthor%3A%20Jungeun%20Kim%20and%20Hyeongwoo%20Jeon%20and%20Jongseong%20Bae%20and%20Ha%20Young%20Kim%0AAbstract%3A%20%20%20Sign%20language%20translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20involves%0Atranslating%20sign%20language%20images%20into%20spoken%20language.%20For%20SLT%20models%20to%0Aperform%20this%20task%20successfully%2C%20they%20must%20bridge%20the%20modality%20gap%20and%20identify%0Asubtle%20variations%20in%20sign%20language%20components%20to%20understand%20their%20meanings%0Aaccurately.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20gloss-free%20SLT%0Aframework%20called%20Multimodal%20Sign%20Language%20Translation%20%28MMSLT%29%2C%20which%20leverages%0Athe%20representational%20capabilities%20of%20off-the-shelf%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20Specifically%2C%20we%20use%20MLLMs%20to%20generate%20detailed%20textual%0Adescriptions%20of%20sign%20language%20components.%20Then%2C%20through%20our%20proposed%0Amultimodal-language%20pre-training%20module%2C%20we%20integrate%20these%20description%0Afeatures%20with%20sign%20video%20features%20to%20align%20them%20within%20the%20spoken%20sentence%0Aspace.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20benchmark%20datasets%0APHOENIX14T%20and%20CSL-Daily%2C%20highlighting%20the%20potential%20of%20MLLMs%20to%20be%20utilized%0Aeffectively%20in%20SLT.%20Code%20is%20available%20at%20https%3A//github.com/hwjeon98/MMSLT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520the%2520Power%2520of%2520MLLMs%2520for%2520Gloss-Free%2520Sign%2520Language%2520Translation%26entry.906535625%3DJungeun%2520Kim%2520and%2520Hyeongwoo%2520Jeon%2520and%2520Jongseong%2520Bae%2520and%2520Ha%2520Young%2520Kim%26entry.1292438233%3D%2520%2520Sign%2520language%2520translation%2520%2528SLT%2529%2520is%2520a%2520challenging%2520task%2520that%2520involves%250Atranslating%2520sign%2520language%2520images%2520into%2520spoken%2520language.%2520For%2520SLT%2520models%2520to%250Aperform%2520this%2520task%2520successfully%252C%2520they%2520must%2520bridge%2520the%2520modality%2520gap%2520and%2520identify%250Asubtle%2520variations%2520in%2520sign%2520language%2520components%2520to%2520understand%2520their%2520meanings%250Aaccurately.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520gloss-free%2520SLT%250Aframework%2520called%2520Multimodal%2520Sign%2520Language%2520Translation%2520%2528MMSLT%2529%252C%2520which%2520leverages%250Athe%2520representational%2520capabilities%2520of%2520off-the-shelf%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529.%2520Specifically%252C%2520we%2520use%2520MLLMs%2520to%2520generate%2520detailed%2520textual%250Adescriptions%2520of%2520sign%2520language%2520components.%2520Then%252C%2520through%2520our%2520proposed%250Amultimodal-language%2520pre-training%2520module%252C%2520we%2520integrate%2520these%2520description%250Afeatures%2520with%2520sign%2520video%2520features%2520to%2520align%2520them%2520within%2520the%2520spoken%2520sentence%250Aspace.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520benchmark%2520datasets%250APHOENIX14T%2520and%2520CSL-Daily%252C%2520highlighting%2520the%2520potential%2520of%2520MLLMs%2520to%2520be%2520utilized%250Aeffectively%2520in%2520SLT.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/hwjeon98/MMSLT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20the%20Power%20of%20MLLMs%20for%20Gloss-Free%20Sign%20Language%20Translation&entry.906535625=Jungeun%20Kim%20and%20Hyeongwoo%20Jeon%20and%20Jongseong%20Bae%20and%20Ha%20Young%20Kim&entry.1292438233=%20%20Sign%20language%20translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20involves%0Atranslating%20sign%20language%20images%20into%20spoken%20language.%20For%20SLT%20models%20to%0Aperform%20this%20task%20successfully%2C%20they%20must%20bridge%20the%20modality%20gap%20and%20identify%0Asubtle%20variations%20in%20sign%20language%20components%20to%20understand%20their%20meanings%0Aaccurately.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20gloss-free%20SLT%0Aframework%20called%20Multimodal%20Sign%20Language%20Translation%20%28MMSLT%29%2C%20which%20leverages%0Athe%20representational%20capabilities%20of%20off-the-shelf%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20Specifically%2C%20we%20use%20MLLMs%20to%20generate%20detailed%20textual%0Adescriptions%20of%20sign%20language%20components.%20Then%2C%20through%20our%20proposed%0Amultimodal-language%20pre-training%20module%2C%20we%20integrate%20these%20description%0Afeatures%20with%20sign%20video%20features%20to%20align%20them%20within%20the%20spoken%20sentence%0Aspace.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20benchmark%20datasets%0APHOENIX14T%20and%20CSL-Daily%2C%20highlighting%20the%20potential%20of%20MLLMs%20to%20be%20utilized%0Aeffectively%20in%20SLT.%20Code%20is%20available%20at%20https%3A//github.com/hwjeon98/MMSLT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16789v2&entry.124074799=Read"},
{"title": "Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling\n  Artifact Correction for Clinical CT with U-Nets", "author": "Tina Dorosti and Johannes Thalhammer and Sebastian Peterhansl and Daniela Pfeiffer and Franz Pfeiffer and Florian Schaff", "abstract": "  This study aims to investigate the effect of various beam geometries and\ndimensions of input data on the sparse-sampling streak artifact correction task\nwith U-Nets for clinical CT scans as a means of incorporating the volumetric\ncontext into artifact reduction tasks to improve model performance. A total of\n22 subjects were retrospectively selected (01.2016-12.2018) from the Technical\nUniversity of Munich's research hospital, TUM Klinikum rechts der Isar.\nSparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,\nfan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and\n3D U-Nets were trained and validated on 14, and tested on 8 subjects,\nrespectively. For the dimensionality study, in addition to the 512x512 2D CT\nimages, the CT scans were further pre-processed to generate a so-called '2.5D',\nand 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data\nrefers to individual 64-voxel blocks. An axial, coronal, and sagittal cut\nthrough the center of each block resulted in three 64x64 2D patches that were\nrearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance\nwas assessed with the mean squared error (MSE) and structural similarity index\nmeasure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices\nresults in the best MSE and SSIM values, outperforming the 2.5D and 3D input\ndata dimensions.\n", "link": "http://arxiv.org/abs/2508.17961v1", "date": "2025-08-25", "relevancy": 2.6161, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.526}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5218}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beam%20Geometry%20and%20Input%20Dimensionality%3A%20Impact%20on%20Sparse-Sampling%0A%20%20Artifact%20Correction%20for%20Clinical%20CT%20with%20U-Nets&body=Title%3A%20Beam%20Geometry%20and%20Input%20Dimensionality%3A%20Impact%20on%20Sparse-Sampling%0A%20%20Artifact%20Correction%20for%20Clinical%20CT%20with%20U-Nets%0AAuthor%3A%20Tina%20Dorosti%20and%20Johannes%20Thalhammer%20and%20Sebastian%20Peterhansl%20and%20Daniela%20Pfeiffer%20and%20Franz%20Pfeiffer%20and%20Florian%20Schaff%0AAbstract%3A%20%20%20This%20study%20aims%20to%20investigate%20the%20effect%20of%20various%20beam%20geometries%20and%0Adimensions%20of%20input%20data%20on%20the%20sparse-sampling%20streak%20artifact%20correction%20task%0Awith%20U-Nets%20for%20clinical%20CT%20scans%20as%20a%20means%20of%20incorporating%20the%20volumetric%0Acontext%20into%20artifact%20reduction%20tasks%20to%20improve%20model%20performance.%20A%20total%20of%0A22%20subjects%20were%20retrospectively%20selected%20%2801.2016-12.2018%29%20from%20the%20Technical%0AUniversity%20of%20Munich%27s%20research%20hospital%2C%20TUM%20Klinikum%20rechts%20der%20Isar.%0ASparsely-sampled%20CT%20volumes%20were%20simulated%20with%20the%20Astra%20toolbox%20for%20parallel%2C%0Afan%2C%20and%20cone%20beam%20geometries.%202048%20views%20were%20taken%20as%20full-view%20scans.%202D%20and%0A3D%20U-Nets%20were%20trained%20and%20validated%20on%2014%2C%20and%20tested%20on%208%20subjects%2C%0Arespectively.%20For%20the%20dimensionality%20study%2C%20in%20addition%20to%20the%20512x512%202D%20CT%0Aimages%2C%20the%20CT%20scans%20were%20further%20pre-processed%20to%20generate%20a%20so-called%20%272.5D%27%2C%0Aand%203D%20data%3A%20Each%20CT%20volume%20was%20divided%20into%2064x64x64%20voxel%20blocks.%20The%203D%20data%0Arefers%20to%20individual%2064-voxel%20blocks.%20An%20axial%2C%20coronal%2C%20and%20sagittal%20cut%0Athrough%20the%20center%20of%20each%20block%20resulted%20in%20three%2064x64%202D%20patches%20that%20were%0Arearranged%20as%20a%20single%2064x64x3%20image%2C%20proposed%20as%202.5D%20data.%20Model%20performance%0Awas%20assessed%20with%20the%20mean%20squared%20error%20%28MSE%29%20and%20structural%20similarity%20index%0Ameasure%20%28SSIM%29.%20For%20all%20geometries%2C%20the%202D%20U-Net%20trained%20on%20axial%202D%20slices%0Aresults%20in%20the%20best%20MSE%20and%20SSIM%20values%2C%20outperforming%20the%202.5D%20and%203D%20input%0Adata%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeam%2520Geometry%2520and%2520Input%2520Dimensionality%253A%2520Impact%2520on%2520Sparse-Sampling%250A%2520%2520Artifact%2520Correction%2520for%2520Clinical%2520CT%2520with%2520U-Nets%26entry.906535625%3DTina%2520Dorosti%2520and%2520Johannes%2520Thalhammer%2520and%2520Sebastian%2520Peterhansl%2520and%2520Daniela%2520Pfeiffer%2520and%2520Franz%2520Pfeiffer%2520and%2520Florian%2520Schaff%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520investigate%2520the%2520effect%2520of%2520various%2520beam%2520geometries%2520and%250Adimensions%2520of%2520input%2520data%2520on%2520the%2520sparse-sampling%2520streak%2520artifact%2520correction%2520task%250Awith%2520U-Nets%2520for%2520clinical%2520CT%2520scans%2520as%2520a%2520means%2520of%2520incorporating%2520the%2520volumetric%250Acontext%2520into%2520artifact%2520reduction%2520tasks%2520to%2520improve%2520model%2520performance.%2520A%2520total%2520of%250A22%2520subjects%2520were%2520retrospectively%2520selected%2520%252801.2016-12.2018%2529%2520from%2520the%2520Technical%250AUniversity%2520of%2520Munich%2527s%2520research%2520hospital%252C%2520TUM%2520Klinikum%2520rechts%2520der%2520Isar.%250ASparsely-sampled%2520CT%2520volumes%2520were%2520simulated%2520with%2520the%2520Astra%2520toolbox%2520for%2520parallel%252C%250Afan%252C%2520and%2520cone%2520beam%2520geometries.%25202048%2520views%2520were%2520taken%2520as%2520full-view%2520scans.%25202D%2520and%250A3D%2520U-Nets%2520were%2520trained%2520and%2520validated%2520on%252014%252C%2520and%2520tested%2520on%25208%2520subjects%252C%250Arespectively.%2520For%2520the%2520dimensionality%2520study%252C%2520in%2520addition%2520to%2520the%2520512x512%25202D%2520CT%250Aimages%252C%2520the%2520CT%2520scans%2520were%2520further%2520pre-processed%2520to%2520generate%2520a%2520so-called%2520%25272.5D%2527%252C%250Aand%25203D%2520data%253A%2520Each%2520CT%2520volume%2520was%2520divided%2520into%252064x64x64%2520voxel%2520blocks.%2520The%25203D%2520data%250Arefers%2520to%2520individual%252064-voxel%2520blocks.%2520An%2520axial%252C%2520coronal%252C%2520and%2520sagittal%2520cut%250Athrough%2520the%2520center%2520of%2520each%2520block%2520resulted%2520in%2520three%252064x64%25202D%2520patches%2520that%2520were%250Arearranged%2520as%2520a%2520single%252064x64x3%2520image%252C%2520proposed%2520as%25202.5D%2520data.%2520Model%2520performance%250Awas%2520assessed%2520with%2520the%2520mean%2520squared%2520error%2520%2528MSE%2529%2520and%2520structural%2520similarity%2520index%250Ameasure%2520%2528SSIM%2529.%2520For%2520all%2520geometries%252C%2520the%25202D%2520U-Net%2520trained%2520on%2520axial%25202D%2520slices%250Aresults%2520in%2520the%2520best%2520MSE%2520and%2520SSIM%2520values%252C%2520outperforming%2520the%25202.5D%2520and%25203D%2520input%250Adata%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beam%20Geometry%20and%20Input%20Dimensionality%3A%20Impact%20on%20Sparse-Sampling%0A%20%20Artifact%20Correction%20for%20Clinical%20CT%20with%20U-Nets&entry.906535625=Tina%20Dorosti%20and%20Johannes%20Thalhammer%20and%20Sebastian%20Peterhansl%20and%20Daniela%20Pfeiffer%20and%20Franz%20Pfeiffer%20and%20Florian%20Schaff&entry.1292438233=%20%20This%20study%20aims%20to%20investigate%20the%20effect%20of%20various%20beam%20geometries%20and%0Adimensions%20of%20input%20data%20on%20the%20sparse-sampling%20streak%20artifact%20correction%20task%0Awith%20U-Nets%20for%20clinical%20CT%20scans%20as%20a%20means%20of%20incorporating%20the%20volumetric%0Acontext%20into%20artifact%20reduction%20tasks%20to%20improve%20model%20performance.%20A%20total%20of%0A22%20subjects%20were%20retrospectively%20selected%20%2801.2016-12.2018%29%20from%20the%20Technical%0AUniversity%20of%20Munich%27s%20research%20hospital%2C%20TUM%20Klinikum%20rechts%20der%20Isar.%0ASparsely-sampled%20CT%20volumes%20were%20simulated%20with%20the%20Astra%20toolbox%20for%20parallel%2C%0Afan%2C%20and%20cone%20beam%20geometries.%202048%20views%20were%20taken%20as%20full-view%20scans.%202D%20and%0A3D%20U-Nets%20were%20trained%20and%20validated%20on%2014%2C%20and%20tested%20on%208%20subjects%2C%0Arespectively.%20For%20the%20dimensionality%20study%2C%20in%20addition%20to%20the%20512x512%202D%20CT%0Aimages%2C%20the%20CT%20scans%20were%20further%20pre-processed%20to%20generate%20a%20so-called%20%272.5D%27%2C%0Aand%203D%20data%3A%20Each%20CT%20volume%20was%20divided%20into%2064x64x64%20voxel%20blocks.%20The%203D%20data%0Arefers%20to%20individual%2064-voxel%20blocks.%20An%20axial%2C%20coronal%2C%20and%20sagittal%20cut%0Athrough%20the%20center%20of%20each%20block%20resulted%20in%20three%2064x64%202D%20patches%20that%20were%0Arearranged%20as%20a%20single%2064x64x3%20image%2C%20proposed%20as%202.5D%20data.%20Model%20performance%0Awas%20assessed%20with%20the%20mean%20squared%20error%20%28MSE%29%20and%20structural%20similarity%20index%0Ameasure%20%28SSIM%29.%20For%20all%20geometries%2C%20the%202D%20U-Net%20trained%20on%20axial%202D%20slices%0Aresults%20in%20the%20best%20MSE%20and%20SSIM%20values%2C%20outperforming%20the%202.5D%20and%203D%20input%0Adata%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17961v1&entry.124074799=Read"},
{"title": "ANO : Faster is Better in Noisy Landscape", "author": "Adrien Kegreisz", "abstract": "  Stochastic optimizers are central to deep learning, yet widely used methods\nsuch as Adam and Adan can degrade in non-stationary or noisy environments,\npartly due to their reliance on momentum-based magnitude estimates. We\nintroduce Ano, a novel optimizer that decouples direction and magnitude:\nmomentum is used for directional smoothing, while instantaneous gradient\nmagnitudes determine step size. This design improves robustness to gradient\nnoise while retaining the simplicity and efficiency of first-order methods. We\nfurther propose Anolog, which removes sensitivity to the momentum coefficient\nby expanding its window over time via a logarithmic schedule. We establish\nnon-convex convergence guarantees with a convergence rate similar to other\nsign-based methods, and empirically show that Ano provides substantial gains in\nnoisy and non-stationary regimes such as reinforcement learning, while\nremaining competitive on low-noise tasks such as standard computer vision\nbenchmarks.\n", "link": "http://arxiv.org/abs/2508.18258v1", "date": "2025-08-25", "relevancy": 2.6029, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.508}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANO%20%3A%20Faster%20is%20Better%20in%20Noisy%20Landscape&body=Title%3A%20ANO%20%3A%20Faster%20is%20Better%20in%20Noisy%20Landscape%0AAuthor%3A%20Adrien%20Kegreisz%0AAbstract%3A%20%20%20Stochastic%20optimizers%20are%20central%20to%20deep%20learning%2C%20yet%20widely%20used%20methods%0Asuch%20as%20Adam%20and%20Adan%20can%20degrade%20in%20non-stationary%20or%20noisy%20environments%2C%0Apartly%20due%20to%20their%20reliance%20on%20momentum-based%20magnitude%20estimates.%20We%0Aintroduce%20Ano%2C%20a%20novel%20optimizer%20that%20decouples%20direction%20and%20magnitude%3A%0Amomentum%20is%20used%20for%20directional%20smoothing%2C%20while%20instantaneous%20gradient%0Amagnitudes%20determine%20step%20size.%20This%20design%20improves%20robustness%20to%20gradient%0Anoise%20while%20retaining%20the%20simplicity%20and%20efficiency%20of%20first-order%20methods.%20We%0Afurther%20propose%20Anolog%2C%20which%20removes%20sensitivity%20to%20the%20momentum%20coefficient%0Aby%20expanding%20its%20window%20over%20time%20via%20a%20logarithmic%20schedule.%20We%20establish%0Anon-convex%20convergence%20guarantees%20with%20a%20convergence%20rate%20similar%20to%20other%0Asign-based%20methods%2C%20and%20empirically%20show%20that%20Ano%20provides%20substantial%20gains%20in%0Anoisy%20and%20non-stationary%20regimes%20such%20as%20reinforcement%20learning%2C%20while%0Aremaining%20competitive%20on%20low-noise%20tasks%20such%20as%20standard%20computer%20vision%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANO%2520%253A%2520Faster%2520is%2520Better%2520in%2520Noisy%2520Landscape%26entry.906535625%3DAdrien%2520Kegreisz%26entry.1292438233%3D%2520%2520Stochastic%2520optimizers%2520are%2520central%2520to%2520deep%2520learning%252C%2520yet%2520widely%2520used%2520methods%250Asuch%2520as%2520Adam%2520and%2520Adan%2520can%2520degrade%2520in%2520non-stationary%2520or%2520noisy%2520environments%252C%250Apartly%2520due%2520to%2520their%2520reliance%2520on%2520momentum-based%2520magnitude%2520estimates.%2520We%250Aintroduce%2520Ano%252C%2520a%2520novel%2520optimizer%2520that%2520decouples%2520direction%2520and%2520magnitude%253A%250Amomentum%2520is%2520used%2520for%2520directional%2520smoothing%252C%2520while%2520instantaneous%2520gradient%250Amagnitudes%2520determine%2520step%2520size.%2520This%2520design%2520improves%2520robustness%2520to%2520gradient%250Anoise%2520while%2520retaining%2520the%2520simplicity%2520and%2520efficiency%2520of%2520first-order%2520methods.%2520We%250Afurther%2520propose%2520Anolog%252C%2520which%2520removes%2520sensitivity%2520to%2520the%2520momentum%2520coefficient%250Aby%2520expanding%2520its%2520window%2520over%2520time%2520via%2520a%2520logarithmic%2520schedule.%2520We%2520establish%250Anon-convex%2520convergence%2520guarantees%2520with%2520a%2520convergence%2520rate%2520similar%2520to%2520other%250Asign-based%2520methods%252C%2520and%2520empirically%2520show%2520that%2520Ano%2520provides%2520substantial%2520gains%2520in%250Anoisy%2520and%2520non-stationary%2520regimes%2520such%2520as%2520reinforcement%2520learning%252C%2520while%250Aremaining%2520competitive%2520on%2520low-noise%2520tasks%2520such%2520as%2520standard%2520computer%2520vision%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANO%20%3A%20Faster%20is%20Better%20in%20Noisy%20Landscape&entry.906535625=Adrien%20Kegreisz&entry.1292438233=%20%20Stochastic%20optimizers%20are%20central%20to%20deep%20learning%2C%20yet%20widely%20used%20methods%0Asuch%20as%20Adam%20and%20Adan%20can%20degrade%20in%20non-stationary%20or%20noisy%20environments%2C%0Apartly%20due%20to%20their%20reliance%20on%20momentum-based%20magnitude%20estimates.%20We%0Aintroduce%20Ano%2C%20a%20novel%20optimizer%20that%20decouples%20direction%20and%20magnitude%3A%0Amomentum%20is%20used%20for%20directional%20smoothing%2C%20while%20instantaneous%20gradient%0Amagnitudes%20determine%20step%20size.%20This%20design%20improves%20robustness%20to%20gradient%0Anoise%20while%20retaining%20the%20simplicity%20and%20efficiency%20of%20first-order%20methods.%20We%0Afurther%20propose%20Anolog%2C%20which%20removes%20sensitivity%20to%20the%20momentum%20coefficient%0Aby%20expanding%20its%20window%20over%20time%20via%20a%20logarithmic%20schedule.%20We%20establish%0Anon-convex%20convergence%20guarantees%20with%20a%20convergence%20rate%20similar%20to%20other%0Asign-based%20methods%2C%20and%20empirically%20show%20that%20Ano%20provides%20substantial%20gains%20in%0Anoisy%20and%20non-stationary%20regimes%20such%20as%20reinforcement%20learning%2C%20while%0Aremaining%20competitive%20on%20low-noise%20tasks%20such%20as%20standard%20computer%20vision%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18258v1&entry.124074799=Read"},
{"title": "Understanding Subword Compositionality of Large Language Models", "author": "Qiwei Peng and Yekun Chai and Anders S\u00f8gaard", "abstract": "  Large language models (LLMs) take sequences of subwords as input, requiring\nthem to effective compose subword representations into meaningful word-level\nrepresentations. In this paper, we present a comprehensive set of experiments\nto probe how LLMs compose subword information, focusing on three key aspects:\nstructural similarity, semantic decomposability, and form retention. Our\nanalysis of the experiments suggests that these five LLM families can be\nclassified into three distinct groups, likely reflecting difference in their\nunderlying composition strategies. Specifically, we observe (i) three distinct\npatterns in the evolution of structural similarity between subword compositions\nand whole-word representations across layers; (ii) great performance when\nprobing layer by layer their sensitivity to semantic decompositionality; and\n(iii) three distinct patterns when probing sensitivity to formal features,\ne.g., character sequence length. These findings provide valuable insights into\nthe compositional dynamics of LLMs and highlight different compositional\npattens in how LLMs encode and integrate subword information.\n", "link": "http://arxiv.org/abs/2508.17953v1", "date": "2025-08-25", "relevancy": 2.5822, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Subword%20Compositionality%20of%20Large%20Language%20Models&body=Title%3A%20Understanding%20Subword%20Compositionality%20of%20Large%20Language%20Models%0AAuthor%3A%20Qiwei%20Peng%20and%20Yekun%20Chai%20and%20Anders%20S%C3%B8gaard%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20take%20sequences%20of%20subwords%20as%20input%2C%20requiring%0Athem%20to%20effective%20compose%20subword%20representations%20into%20meaningful%20word-level%0Arepresentations.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20set%20of%20experiments%0Ato%20probe%20how%20LLMs%20compose%20subword%20information%2C%20focusing%20on%20three%20key%20aspects%3A%0Astructural%20similarity%2C%20semantic%20decomposability%2C%20and%20form%20retention.%20Our%0Aanalysis%20of%20the%20experiments%20suggests%20that%20these%20five%20LLM%20families%20can%20be%0Aclassified%20into%20three%20distinct%20groups%2C%20likely%20reflecting%20difference%20in%20their%0Aunderlying%20composition%20strategies.%20Specifically%2C%20we%20observe%20%28i%29%20three%20distinct%0Apatterns%20in%20the%20evolution%20of%20structural%20similarity%20between%20subword%20compositions%0Aand%20whole-word%20representations%20across%20layers%3B%20%28ii%29%20great%20performance%20when%0Aprobing%20layer%20by%20layer%20their%20sensitivity%20to%20semantic%20decompositionality%3B%20and%0A%28iii%29%20three%20distinct%20patterns%20when%20probing%20sensitivity%20to%20formal%20features%2C%0Ae.g.%2C%20character%20sequence%20length.%20These%20findings%20provide%20valuable%20insights%20into%0Athe%20compositional%20dynamics%20of%20LLMs%20and%20highlight%20different%20compositional%0Apattens%20in%20how%20LLMs%20encode%20and%20integrate%20subword%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Subword%2520Compositionality%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DQiwei%2520Peng%2520and%2520Yekun%2520Chai%2520and%2520Anders%2520S%25C3%25B8gaard%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520take%2520sequences%2520of%2520subwords%2520as%2520input%252C%2520requiring%250Athem%2520to%2520effective%2520compose%2520subword%2520representations%2520into%2520meaningful%2520word-level%250Arepresentations.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520set%2520of%2520experiments%250Ato%2520probe%2520how%2520LLMs%2520compose%2520subword%2520information%252C%2520focusing%2520on%2520three%2520key%2520aspects%253A%250Astructural%2520similarity%252C%2520semantic%2520decomposability%252C%2520and%2520form%2520retention.%2520Our%250Aanalysis%2520of%2520the%2520experiments%2520suggests%2520that%2520these%2520five%2520LLM%2520families%2520can%2520be%250Aclassified%2520into%2520three%2520distinct%2520groups%252C%2520likely%2520reflecting%2520difference%2520in%2520their%250Aunderlying%2520composition%2520strategies.%2520Specifically%252C%2520we%2520observe%2520%2528i%2529%2520three%2520distinct%250Apatterns%2520in%2520the%2520evolution%2520of%2520structural%2520similarity%2520between%2520subword%2520compositions%250Aand%2520whole-word%2520representations%2520across%2520layers%253B%2520%2528ii%2529%2520great%2520performance%2520when%250Aprobing%2520layer%2520by%2520layer%2520their%2520sensitivity%2520to%2520semantic%2520decompositionality%253B%2520and%250A%2528iii%2529%2520three%2520distinct%2520patterns%2520when%2520probing%2520sensitivity%2520to%2520formal%2520features%252C%250Ae.g.%252C%2520character%2520sequence%2520length.%2520These%2520findings%2520provide%2520valuable%2520insights%2520into%250Athe%2520compositional%2520dynamics%2520of%2520LLMs%2520and%2520highlight%2520different%2520compositional%250Apattens%2520in%2520how%2520LLMs%2520encode%2520and%2520integrate%2520subword%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Subword%20Compositionality%20of%20Large%20Language%20Models&entry.906535625=Qiwei%20Peng%20and%20Yekun%20Chai%20and%20Anders%20S%C3%B8gaard&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20take%20sequences%20of%20subwords%20as%20input%2C%20requiring%0Athem%20to%20effective%20compose%20subword%20representations%20into%20meaningful%20word-level%0Arepresentations.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20set%20of%20experiments%0Ato%20probe%20how%20LLMs%20compose%20subword%20information%2C%20focusing%20on%20three%20key%20aspects%3A%0Astructural%20similarity%2C%20semantic%20decomposability%2C%20and%20form%20retention.%20Our%0Aanalysis%20of%20the%20experiments%20suggests%20that%20these%20five%20LLM%20families%20can%20be%0Aclassified%20into%20three%20distinct%20groups%2C%20likely%20reflecting%20difference%20in%20their%0Aunderlying%20composition%20strategies.%20Specifically%2C%20we%20observe%20%28i%29%20three%20distinct%0Apatterns%20in%20the%20evolution%20of%20structural%20similarity%20between%20subword%20compositions%0Aand%20whole-word%20representations%20across%20layers%3B%20%28ii%29%20great%20performance%20when%0Aprobing%20layer%20by%20layer%20their%20sensitivity%20to%20semantic%20decompositionality%3B%20and%0A%28iii%29%20three%20distinct%20patterns%20when%20probing%20sensitivity%20to%20formal%20features%2C%0Ae.g.%2C%20character%20sequence%20length.%20These%20findings%20provide%20valuable%20insights%20into%0Athe%20compositional%20dynamics%20of%20LLMs%20and%20highlight%20different%20compositional%0Apattens%20in%20how%20LLMs%20encode%20and%20integrate%20subword%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17953v1&entry.124074799=Read"},
{"title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in\n  Versatile Clinical Applications", "author": "Zelin Qiu and Xi Wang and Zhuoyao Xie and Juan Zhou and Yu Wang and Lingjie Yang and Xinrui Jiang and Juyoung Bae and Moo Hyun Son and Qiang Ye and Dexuan Chen and Rui Zhang and Tao Li and Neeraj Ramesh Mahboobani and Varut Vardhanabhuti and Xiaohui Duan and Yinghua Zhao and Hao Chen", "abstract": "  Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable\nversatility, enabling the distinct visualization of different tissue types.\nNevertheless, the inherent heterogeneity among MRI sequences poses significant\nchallenges to the generalization capability of deep learning models. These\nchallenges undermine model performance when faced with varying acquisition\nparameters, thereby severely restricting their clinical utility. In this study,\nwe present PRISM, a foundation model PRe-trained with large-scale\nmultI-Sequence MRI. We collected a total of 64 datasets from both public and\nprivate sources, encompassing a wide range of whole-body anatomical structures,\nwith scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI\nscans from 34 datasets (8 public and 26 private) were curated to construct the\nlargest multi-organ multi-sequence MRI pretraining corpus to date. We propose a\nnovel pretraining paradigm that disentangles anatomically invariant features\nfrom sequence-specific variations in MRI, while preserving high-level semantic\nrepresentations. We established a benchmark comprising 44 downstream tasks,\nincluding disease diagnosis, image segmentation, registration, progression\nprediction, and report generation. These tasks were evaluated on 32 public\ndatasets and 5 private cohorts. PRISM consistently outperformed both\nnon-pretrained models and existing foundation models, achieving first-rank\nresults in 39 out of 44 downstream benchmarks with statistical significance\nimprovements. These results underscore its ability to learn robust and\ngeneralizable representations across unseen data acquired under diverse MRI\nprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,\nthereby enhancing the translational potential of AI in radiology. It delivers\nconsistent performance across diverse imaging protocols, reinforcing its\nclinical applicability.\n", "link": "http://arxiv.org/abs/2508.07165v2", "date": "2025-08-25", "relevancy": 2.5669, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Multi-sequence%20Pretraining%20for%20Generalizable%20MRI%20Analysis%20in%0A%20%20Versatile%20Clinical%20Applications&body=Title%3A%20Large-scale%20Multi-sequence%20Pretraining%20for%20Generalizable%20MRI%20Analysis%20in%0A%20%20Versatile%20Clinical%20Applications%0AAuthor%3A%20Zelin%20Qiu%20and%20Xi%20Wang%20and%20Zhuoyao%20Xie%20and%20Juan%20Zhou%20and%20Yu%20Wang%20and%20Lingjie%20Yang%20and%20Xinrui%20Jiang%20and%20Juyoung%20Bae%20and%20Moo%20Hyun%20Son%20and%20Qiang%20Ye%20and%20Dexuan%20Chen%20and%20Rui%20Zhang%20and%20Tao%20Li%20and%20Neeraj%20Ramesh%20Mahboobani%20and%20Varut%20Vardhanabhuti%20and%20Xiaohui%20Duan%20and%20Yinghua%20Zhao%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Multi-sequence%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20offers%20remarkable%0Aversatility%2C%20enabling%20the%20distinct%20visualization%20of%20different%20tissue%20types.%0ANevertheless%2C%20the%20inherent%20heterogeneity%20among%20MRI%20sequences%20poses%20significant%0Achallenges%20to%20the%20generalization%20capability%20of%20deep%20learning%20models.%20These%0Achallenges%20undermine%20model%20performance%20when%20faced%20with%20varying%20acquisition%0Aparameters%2C%20thereby%20severely%20restricting%20their%20clinical%20utility.%20In%20this%20study%2C%0Awe%20present%20PRISM%2C%20a%20foundation%20model%20PRe-trained%20with%20large-scale%0AmultI-Sequence%20MRI.%20We%20collected%20a%20total%20of%2064%20datasets%20from%20both%20public%20and%0Aprivate%20sources%2C%20encompassing%20a%20wide%20range%20of%20whole-body%20anatomical%20structures%2C%0Awith%20scans%20spanning%20diverse%20MRI%20sequences.%20Among%20them%2C%20336%2C476%20volumetric%20MRI%0Ascans%20from%2034%20datasets%20%288%20public%20and%2026%20private%29%20were%20curated%20to%20construct%20the%0Alargest%20multi-organ%20multi-sequence%20MRI%20pretraining%20corpus%20to%20date.%20We%20propose%20a%0Anovel%20pretraining%20paradigm%20that%20disentangles%20anatomically%20invariant%20features%0Afrom%20sequence-specific%20variations%20in%20MRI%2C%20while%20preserving%20high-level%20semantic%0Arepresentations.%20We%20established%20a%20benchmark%20comprising%2044%20downstream%20tasks%2C%0Aincluding%20disease%20diagnosis%2C%20image%20segmentation%2C%20registration%2C%20progression%0Aprediction%2C%20and%20report%20generation.%20These%20tasks%20were%20evaluated%20on%2032%20public%0Adatasets%20and%205%20private%20cohorts.%20PRISM%20consistently%20outperformed%20both%0Anon-pretrained%20models%20and%20existing%20foundation%20models%2C%20achieving%20first-rank%0Aresults%20in%2039%20out%20of%2044%20downstream%20benchmarks%20with%20statistical%20significance%0Aimprovements.%20These%20results%20underscore%20its%20ability%20to%20learn%20robust%20and%0Ageneralizable%20representations%20across%20unseen%20data%20acquired%20under%20diverse%20MRI%0Aprotocols.%20PRISM%20provides%20a%20scalable%20framework%20for%20multi-sequence%20MRI%20analysis%2C%0Athereby%20enhancing%20the%20translational%20potential%20of%20AI%20in%20radiology.%20It%20delivers%0Aconsistent%20performance%20across%20diverse%20imaging%20protocols%2C%20reinforcing%20its%0Aclinical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07165v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Multi-sequence%2520Pretraining%2520for%2520Generalizable%2520MRI%2520Analysis%2520in%250A%2520%2520Versatile%2520Clinical%2520Applications%26entry.906535625%3DZelin%2520Qiu%2520and%2520Xi%2520Wang%2520and%2520Zhuoyao%2520Xie%2520and%2520Juan%2520Zhou%2520and%2520Yu%2520Wang%2520and%2520Lingjie%2520Yang%2520and%2520Xinrui%2520Jiang%2520and%2520Juyoung%2520Bae%2520and%2520Moo%2520Hyun%2520Son%2520and%2520Qiang%2520Ye%2520and%2520Dexuan%2520Chen%2520and%2520Rui%2520Zhang%2520and%2520Tao%2520Li%2520and%2520Neeraj%2520Ramesh%2520Mahboobani%2520and%2520Varut%2520Vardhanabhuti%2520and%2520Xiaohui%2520Duan%2520and%2520Yinghua%2520Zhao%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Multi-sequence%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520offers%2520remarkable%250Aversatility%252C%2520enabling%2520the%2520distinct%2520visualization%2520of%2520different%2520tissue%2520types.%250ANevertheless%252C%2520the%2520inherent%2520heterogeneity%2520among%2520MRI%2520sequences%2520poses%2520significant%250Achallenges%2520to%2520the%2520generalization%2520capability%2520of%2520deep%2520learning%2520models.%2520These%250Achallenges%2520undermine%2520model%2520performance%2520when%2520faced%2520with%2520varying%2520acquisition%250Aparameters%252C%2520thereby%2520severely%2520restricting%2520their%2520clinical%2520utility.%2520In%2520this%2520study%252C%250Awe%2520present%2520PRISM%252C%2520a%2520foundation%2520model%2520PRe-trained%2520with%2520large-scale%250AmultI-Sequence%2520MRI.%2520We%2520collected%2520a%2520total%2520of%252064%2520datasets%2520from%2520both%2520public%2520and%250Aprivate%2520sources%252C%2520encompassing%2520a%2520wide%2520range%2520of%2520whole-body%2520anatomical%2520structures%252C%250Awith%2520scans%2520spanning%2520diverse%2520MRI%2520sequences.%2520Among%2520them%252C%2520336%252C476%2520volumetric%2520MRI%250Ascans%2520from%252034%2520datasets%2520%25288%2520public%2520and%252026%2520private%2529%2520were%2520curated%2520to%2520construct%2520the%250Alargest%2520multi-organ%2520multi-sequence%2520MRI%2520pretraining%2520corpus%2520to%2520date.%2520We%2520propose%2520a%250Anovel%2520pretraining%2520paradigm%2520that%2520disentangles%2520anatomically%2520invariant%2520features%250Afrom%2520sequence-specific%2520variations%2520in%2520MRI%252C%2520while%2520preserving%2520high-level%2520semantic%250Arepresentations.%2520We%2520established%2520a%2520benchmark%2520comprising%252044%2520downstream%2520tasks%252C%250Aincluding%2520disease%2520diagnosis%252C%2520image%2520segmentation%252C%2520registration%252C%2520progression%250Aprediction%252C%2520and%2520report%2520generation.%2520These%2520tasks%2520were%2520evaluated%2520on%252032%2520public%250Adatasets%2520and%25205%2520private%2520cohorts.%2520PRISM%2520consistently%2520outperformed%2520both%250Anon-pretrained%2520models%2520and%2520existing%2520foundation%2520models%252C%2520achieving%2520first-rank%250Aresults%2520in%252039%2520out%2520of%252044%2520downstream%2520benchmarks%2520with%2520statistical%2520significance%250Aimprovements.%2520These%2520results%2520underscore%2520its%2520ability%2520to%2520learn%2520robust%2520and%250Ageneralizable%2520representations%2520across%2520unseen%2520data%2520acquired%2520under%2520diverse%2520MRI%250Aprotocols.%2520PRISM%2520provides%2520a%2520scalable%2520framework%2520for%2520multi-sequence%2520MRI%2520analysis%252C%250Athereby%2520enhancing%2520the%2520translational%2520potential%2520of%2520AI%2520in%2520radiology.%2520It%2520delivers%250Aconsistent%2520performance%2520across%2520diverse%2520imaging%2520protocols%252C%2520reinforcing%2520its%250Aclinical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07165v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Multi-sequence%20Pretraining%20for%20Generalizable%20MRI%20Analysis%20in%0A%20%20Versatile%20Clinical%20Applications&entry.906535625=Zelin%20Qiu%20and%20Xi%20Wang%20and%20Zhuoyao%20Xie%20and%20Juan%20Zhou%20and%20Yu%20Wang%20and%20Lingjie%20Yang%20and%20Xinrui%20Jiang%20and%20Juyoung%20Bae%20and%20Moo%20Hyun%20Son%20and%20Qiang%20Ye%20and%20Dexuan%20Chen%20and%20Rui%20Zhang%20and%20Tao%20Li%20and%20Neeraj%20Ramesh%20Mahboobani%20and%20Varut%20Vardhanabhuti%20and%20Xiaohui%20Duan%20and%20Yinghua%20Zhao%20and%20Hao%20Chen&entry.1292438233=%20%20Multi-sequence%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20offers%20remarkable%0Aversatility%2C%20enabling%20the%20distinct%20visualization%20of%20different%20tissue%20types.%0ANevertheless%2C%20the%20inherent%20heterogeneity%20among%20MRI%20sequences%20poses%20significant%0Achallenges%20to%20the%20generalization%20capability%20of%20deep%20learning%20models.%20These%0Achallenges%20undermine%20model%20performance%20when%20faced%20with%20varying%20acquisition%0Aparameters%2C%20thereby%20severely%20restricting%20their%20clinical%20utility.%20In%20this%20study%2C%0Awe%20present%20PRISM%2C%20a%20foundation%20model%20PRe-trained%20with%20large-scale%0AmultI-Sequence%20MRI.%20We%20collected%20a%20total%20of%2064%20datasets%20from%20both%20public%20and%0Aprivate%20sources%2C%20encompassing%20a%20wide%20range%20of%20whole-body%20anatomical%20structures%2C%0Awith%20scans%20spanning%20diverse%20MRI%20sequences.%20Among%20them%2C%20336%2C476%20volumetric%20MRI%0Ascans%20from%2034%20datasets%20%288%20public%20and%2026%20private%29%20were%20curated%20to%20construct%20the%0Alargest%20multi-organ%20multi-sequence%20MRI%20pretraining%20corpus%20to%20date.%20We%20propose%20a%0Anovel%20pretraining%20paradigm%20that%20disentangles%20anatomically%20invariant%20features%0Afrom%20sequence-specific%20variations%20in%20MRI%2C%20while%20preserving%20high-level%20semantic%0Arepresentations.%20We%20established%20a%20benchmark%20comprising%2044%20downstream%20tasks%2C%0Aincluding%20disease%20diagnosis%2C%20image%20segmentation%2C%20registration%2C%20progression%0Aprediction%2C%20and%20report%20generation.%20These%20tasks%20were%20evaluated%20on%2032%20public%0Adatasets%20and%205%20private%20cohorts.%20PRISM%20consistently%20outperformed%20both%0Anon-pretrained%20models%20and%20existing%20foundation%20models%2C%20achieving%20first-rank%0Aresults%20in%2039%20out%20of%2044%20downstream%20benchmarks%20with%20statistical%20significance%0Aimprovements.%20These%20results%20underscore%20its%20ability%20to%20learn%20robust%20and%0Ageneralizable%20representations%20across%20unseen%20data%20acquired%20under%20diverse%20MRI%0Aprotocols.%20PRISM%20provides%20a%20scalable%20framework%20for%20multi-sequence%20MRI%20analysis%2C%0Athereby%20enhancing%20the%20translational%20potential%20of%20AI%20in%20radiology.%20It%20delivers%0Aconsistent%20performance%20across%20diverse%20imaging%20protocols%2C%20reinforcing%20its%0Aclinical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07165v2&entry.124074799=Read"},
{"title": "Pr$^2$R: Information-Fused and Style-Aware Privacy-Preserving Replay for\n  Lifelong Person Re-Identification", "author": "Mingyu Wang and Haojie Liu and Zhiyong Li and Wei Jiang", "abstract": "  Lifelong person re-identification (LReID) aims to incrementally accumulate\nknowledge across a sequence of tasks under domain shifts. Recently,\nreplay-based methods have demonstrated strong effectiveness in LReID by\nrehearsing past samples stored in an auxiliary memory. However, storing\nhistorical exemplars raises concerns over data privacy. To avoid this,\nexemplar-free approaches attempt to match the distribution of past data without\nstoring raw samples. Despite being privacy-friendly, these methods often suffer\nfrom performance degradation due to the forgetting of specific past knowledge\nrepresentations. To this end, we propose to fuse information from sequential\ndata into the pixel space in the replay memory, enabling Privacy-Preserving\nReplay (Pr$^2$R). More specifically, by distilling the training characteristics\nof multiple real images into a single image, the fused samples undergo\npixel-level changes. This not only protects the privacy of the original data\nbut also makes the replay samples more representative for sequential tasks.\nDuring the style replay phase, we align the current domain to the previous one\nwhile simultaneously adapting the replay samples to match the style of the\ncurrent domain. This dual-alignment strategy effectively mitigates both\nclass-incremental challenges and forgetting caused by domain shifts. Extensive\nexperiments on multiple benchmarks show that the proposed method significantly\nimproves replay effectiveness while preserving data privacy. Specifically,\nPr$^2$R achieves 4% and 6% higher accuracy on sequential tasks compared to the\ncurrent state-of-the-art and other replay-based methods, respectively.\n", "link": "http://arxiv.org/abs/2508.01587v2", "date": "2025-08-25", "relevancy": 2.5603, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5325}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pr%24%5E2%24R%3A%20Information-Fused%20and%20Style-Aware%20Privacy-Preserving%20Replay%20for%0A%20%20Lifelong%20Person%20Re-Identification&body=Title%3A%20Pr%24%5E2%24R%3A%20Information-Fused%20and%20Style-Aware%20Privacy-Preserving%20Replay%20for%0A%20%20Lifelong%20Person%20Re-Identification%0AAuthor%3A%20Mingyu%20Wang%20and%20Haojie%20Liu%20and%20Zhiyong%20Li%20and%20Wei%20Jiang%0AAbstract%3A%20%20%20Lifelong%20person%20re-identification%20%28LReID%29%20aims%20to%20incrementally%20accumulate%0Aknowledge%20across%20a%20sequence%20of%20tasks%20under%20domain%20shifts.%20Recently%2C%0Areplay-based%20methods%20have%20demonstrated%20strong%20effectiveness%20in%20LReID%20by%0Arehearsing%20past%20samples%20stored%20in%20an%20auxiliary%20memory.%20However%2C%20storing%0Ahistorical%20exemplars%20raises%20concerns%20over%20data%20privacy.%20To%20avoid%20this%2C%0Aexemplar-free%20approaches%20attempt%20to%20match%20the%20distribution%20of%20past%20data%20without%0Astoring%20raw%20samples.%20Despite%20being%20privacy-friendly%2C%20these%20methods%20often%20suffer%0Afrom%20performance%20degradation%20due%20to%20the%20forgetting%20of%20specific%20past%20knowledge%0Arepresentations.%20To%20this%20end%2C%20we%20propose%20to%20fuse%20information%20from%20sequential%0Adata%20into%20the%20pixel%20space%20in%20the%20replay%20memory%2C%20enabling%20Privacy-Preserving%0AReplay%20%28Pr%24%5E2%24R%29.%20More%20specifically%2C%20by%20distilling%20the%20training%20characteristics%0Aof%20multiple%20real%20images%20into%20a%20single%20image%2C%20the%20fused%20samples%20undergo%0Apixel-level%20changes.%20This%20not%20only%20protects%20the%20privacy%20of%20the%20original%20data%0Abut%20also%20makes%20the%20replay%20samples%20more%20representative%20for%20sequential%20tasks.%0ADuring%20the%20style%20replay%20phase%2C%20we%20align%20the%20current%20domain%20to%20the%20previous%20one%0Awhile%20simultaneously%20adapting%20the%20replay%20samples%20to%20match%20the%20style%20of%20the%0Acurrent%20domain.%20This%20dual-alignment%20strategy%20effectively%20mitigates%20both%0Aclass-incremental%20challenges%20and%20forgetting%20caused%20by%20domain%20shifts.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20show%20that%20the%20proposed%20method%20significantly%0Aimproves%20replay%20effectiveness%20while%20preserving%20data%20privacy.%20Specifically%2C%0APr%24%5E2%24R%20achieves%204%25%20and%206%25%20higher%20accuracy%20on%20sequential%20tasks%20compared%20to%20the%0Acurrent%20state-of-the-art%20and%20other%20replay-based%20methods%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPr%2524%255E2%2524R%253A%2520Information-Fused%2520and%2520Style-Aware%2520Privacy-Preserving%2520Replay%2520for%250A%2520%2520Lifelong%2520Person%2520Re-Identification%26entry.906535625%3DMingyu%2520Wang%2520and%2520Haojie%2520Liu%2520and%2520Zhiyong%2520Li%2520and%2520Wei%2520Jiang%26entry.1292438233%3D%2520%2520Lifelong%2520person%2520re-identification%2520%2528LReID%2529%2520aims%2520to%2520incrementally%2520accumulate%250Aknowledge%2520across%2520a%2520sequence%2520of%2520tasks%2520under%2520domain%2520shifts.%2520Recently%252C%250Areplay-based%2520methods%2520have%2520demonstrated%2520strong%2520effectiveness%2520in%2520LReID%2520by%250Arehearsing%2520past%2520samples%2520stored%2520in%2520an%2520auxiliary%2520memory.%2520However%252C%2520storing%250Ahistorical%2520exemplars%2520raises%2520concerns%2520over%2520data%2520privacy.%2520To%2520avoid%2520this%252C%250Aexemplar-free%2520approaches%2520attempt%2520to%2520match%2520the%2520distribution%2520of%2520past%2520data%2520without%250Astoring%2520raw%2520samples.%2520Despite%2520being%2520privacy-friendly%252C%2520these%2520methods%2520often%2520suffer%250Afrom%2520performance%2520degradation%2520due%2520to%2520the%2520forgetting%2520of%2520specific%2520past%2520knowledge%250Arepresentations.%2520To%2520this%2520end%252C%2520we%2520propose%2520to%2520fuse%2520information%2520from%2520sequential%250Adata%2520into%2520the%2520pixel%2520space%2520in%2520the%2520replay%2520memory%252C%2520enabling%2520Privacy-Preserving%250AReplay%2520%2528Pr%2524%255E2%2524R%2529.%2520More%2520specifically%252C%2520by%2520distilling%2520the%2520training%2520characteristics%250Aof%2520multiple%2520real%2520images%2520into%2520a%2520single%2520image%252C%2520the%2520fused%2520samples%2520undergo%250Apixel-level%2520changes.%2520This%2520not%2520only%2520protects%2520the%2520privacy%2520of%2520the%2520original%2520data%250Abut%2520also%2520makes%2520the%2520replay%2520samples%2520more%2520representative%2520for%2520sequential%2520tasks.%250ADuring%2520the%2520style%2520replay%2520phase%252C%2520we%2520align%2520the%2520current%2520domain%2520to%2520the%2520previous%2520one%250Awhile%2520simultaneously%2520adapting%2520the%2520replay%2520samples%2520to%2520match%2520the%2520style%2520of%2520the%250Acurrent%2520domain.%2520This%2520dual-alignment%2520strategy%2520effectively%2520mitigates%2520both%250Aclass-incremental%2520challenges%2520and%2520forgetting%2520caused%2520by%2520domain%2520shifts.%2520Extensive%250Aexperiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520the%2520proposed%2520method%2520significantly%250Aimproves%2520replay%2520effectiveness%2520while%2520preserving%2520data%2520privacy.%2520Specifically%252C%250APr%2524%255E2%2524R%2520achieves%25204%2525%2520and%25206%2525%2520higher%2520accuracy%2520on%2520sequential%2520tasks%2520compared%2520to%2520the%250Acurrent%2520state-of-the-art%2520and%2520other%2520replay-based%2520methods%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pr%24%5E2%24R%3A%20Information-Fused%20and%20Style-Aware%20Privacy-Preserving%20Replay%20for%0A%20%20Lifelong%20Person%20Re-Identification&entry.906535625=Mingyu%20Wang%20and%20Haojie%20Liu%20and%20Zhiyong%20Li%20and%20Wei%20Jiang&entry.1292438233=%20%20Lifelong%20person%20re-identification%20%28LReID%29%20aims%20to%20incrementally%20accumulate%0Aknowledge%20across%20a%20sequence%20of%20tasks%20under%20domain%20shifts.%20Recently%2C%0Areplay-based%20methods%20have%20demonstrated%20strong%20effectiveness%20in%20LReID%20by%0Arehearsing%20past%20samples%20stored%20in%20an%20auxiliary%20memory.%20However%2C%20storing%0Ahistorical%20exemplars%20raises%20concerns%20over%20data%20privacy.%20To%20avoid%20this%2C%0Aexemplar-free%20approaches%20attempt%20to%20match%20the%20distribution%20of%20past%20data%20without%0Astoring%20raw%20samples.%20Despite%20being%20privacy-friendly%2C%20these%20methods%20often%20suffer%0Afrom%20performance%20degradation%20due%20to%20the%20forgetting%20of%20specific%20past%20knowledge%0Arepresentations.%20To%20this%20end%2C%20we%20propose%20to%20fuse%20information%20from%20sequential%0Adata%20into%20the%20pixel%20space%20in%20the%20replay%20memory%2C%20enabling%20Privacy-Preserving%0AReplay%20%28Pr%24%5E2%24R%29.%20More%20specifically%2C%20by%20distilling%20the%20training%20characteristics%0Aof%20multiple%20real%20images%20into%20a%20single%20image%2C%20the%20fused%20samples%20undergo%0Apixel-level%20changes.%20This%20not%20only%20protects%20the%20privacy%20of%20the%20original%20data%0Abut%20also%20makes%20the%20replay%20samples%20more%20representative%20for%20sequential%20tasks.%0ADuring%20the%20style%20replay%20phase%2C%20we%20align%20the%20current%20domain%20to%20the%20previous%20one%0Awhile%20simultaneously%20adapting%20the%20replay%20samples%20to%20match%20the%20style%20of%20the%0Acurrent%20domain.%20This%20dual-alignment%20strategy%20effectively%20mitigates%20both%0Aclass-incremental%20challenges%20and%20forgetting%20caused%20by%20domain%20shifts.%20Extensive%0Aexperiments%20on%20multiple%20benchmarks%20show%20that%20the%20proposed%20method%20significantly%0Aimproves%20replay%20effectiveness%20while%20preserving%20data%20privacy.%20Specifically%2C%0APr%24%5E2%24R%20achieves%204%25%20and%206%25%20higher%20accuracy%20on%20sequential%20tasks%20compared%20to%20the%0Acurrent%20state-of-the-art%20and%20other%20replay-based%20methods%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01587v2&entry.124074799=Read"},
{"title": "CLAP: Coreference-Linked Augmentation for Passage Retrieval", "author": "Huanwei Xu and Lin Xu and Liang Yuan", "abstract": "  Large Language Model (LLM)-based passage expansion has shown promise for\nenhancing first-stage retrieval, but often underperforms with dense retrievers\ndue to semantic drift and misalignment with their pretrained semantic space.\nBeyond this, only a portion of a passage is typically relevant to a query,\nwhile the rest introduces noise--an issue compounded by chunking techniques\nthat break coreference continuity. We propose Coreference-Linked Augmentation\nfor Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that\nsegments passages into coherent chunks, resolves coreference chains, and\ngenerates localized pseudo-queries aligned with dense retriever\nrepresentations. A simple fusion of global topical signals and fine-grained\nsubtopic signals achieves robust performance across domains. CLAP yields\nconsistent gains even as retriever strength increases, enabling dense\nretrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,\nwith up to 20.68% absolute nDCG@10 improvement. These improvements are\nespecially notable in out-of-domain settings, where conventional LLM-based\nexpansion methods relying on domain knowledge often falter. CLAP instead adopts\na logic-centric pipeline that enables robust, domain-agnostic generalization.\n", "link": "http://arxiv.org/abs/2508.06941v2", "date": "2025-08-25", "relevancy": 2.5057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAP%3A%20Coreference-Linked%20Augmentation%20for%20Passage%20Retrieval&body=Title%3A%20CLAP%3A%20Coreference-Linked%20Augmentation%20for%20Passage%20Retrieval%0AAuthor%3A%20Huanwei%20Xu%20and%20Lin%20Xu%20and%20Liang%20Yuan%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20passage%20expansion%20has%20shown%20promise%20for%0Aenhancing%20first-stage%20retrieval%2C%20but%20often%20underperforms%20with%20dense%20retrievers%0Adue%20to%20semantic%20drift%20and%20misalignment%20with%20their%20pretrained%20semantic%20space.%0ABeyond%20this%2C%20only%20a%20portion%20of%20a%20passage%20is%20typically%20relevant%20to%20a%20query%2C%0Awhile%20the%20rest%20introduces%20noise--an%20issue%20compounded%20by%20chunking%20techniques%0Athat%20break%20coreference%20continuity.%20We%20propose%20Coreference-Linked%20Augmentation%0Afor%20Passage%20Retrieval%20%28CLAP%29%2C%20a%20lightweight%20LLM-based%20expansion%20framework%20that%0Asegments%20passages%20into%20coherent%20chunks%2C%20resolves%20coreference%20chains%2C%20and%0Agenerates%20localized%20pseudo-queries%20aligned%20with%20dense%20retriever%0Arepresentations.%20A%20simple%20fusion%20of%20global%20topical%20signals%20and%20fine-grained%0Asubtopic%20signals%20achieves%20robust%20performance%20across%20domains.%20CLAP%20yields%0Aconsistent%20gains%20even%20as%20retriever%20strength%20increases%2C%20enabling%20dense%0Aretrievers%20to%20match%20or%20surpass%20second-stage%20rankers%20such%20as%20BM25%20%2B%20MonoT5-3B%2C%0Awith%20up%20to%2020.68%25%20absolute%20nDCG%4010%20improvement.%20These%20improvements%20are%0Aespecially%20notable%20in%20out-of-domain%20settings%2C%20where%20conventional%20LLM-based%0Aexpansion%20methods%20relying%20on%20domain%20knowledge%20often%20falter.%20CLAP%20instead%20adopts%0Aa%20logic-centric%20pipeline%20that%20enables%20robust%2C%20domain-agnostic%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06941v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAP%253A%2520Coreference-Linked%2520Augmentation%2520for%2520Passage%2520Retrieval%26entry.906535625%3DHuanwei%2520Xu%2520and%2520Lin%2520Xu%2520and%2520Liang%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520passage%2520expansion%2520has%2520shown%2520promise%2520for%250Aenhancing%2520first-stage%2520retrieval%252C%2520but%2520often%2520underperforms%2520with%2520dense%2520retrievers%250Adue%2520to%2520semantic%2520drift%2520and%2520misalignment%2520with%2520their%2520pretrained%2520semantic%2520space.%250ABeyond%2520this%252C%2520only%2520a%2520portion%2520of%2520a%2520passage%2520is%2520typically%2520relevant%2520to%2520a%2520query%252C%250Awhile%2520the%2520rest%2520introduces%2520noise--an%2520issue%2520compounded%2520by%2520chunking%2520techniques%250Athat%2520break%2520coreference%2520continuity.%2520We%2520propose%2520Coreference-Linked%2520Augmentation%250Afor%2520Passage%2520Retrieval%2520%2528CLAP%2529%252C%2520a%2520lightweight%2520LLM-based%2520expansion%2520framework%2520that%250Asegments%2520passages%2520into%2520coherent%2520chunks%252C%2520resolves%2520coreference%2520chains%252C%2520and%250Agenerates%2520localized%2520pseudo-queries%2520aligned%2520with%2520dense%2520retriever%250Arepresentations.%2520A%2520simple%2520fusion%2520of%2520global%2520topical%2520signals%2520and%2520fine-grained%250Asubtopic%2520signals%2520achieves%2520robust%2520performance%2520across%2520domains.%2520CLAP%2520yields%250Aconsistent%2520gains%2520even%2520as%2520retriever%2520strength%2520increases%252C%2520enabling%2520dense%250Aretrievers%2520to%2520match%2520or%2520surpass%2520second-stage%2520rankers%2520such%2520as%2520BM25%2520%252B%2520MonoT5-3B%252C%250Awith%2520up%2520to%252020.68%2525%2520absolute%2520nDCG%254010%2520improvement.%2520These%2520improvements%2520are%250Aespecially%2520notable%2520in%2520out-of-domain%2520settings%252C%2520where%2520conventional%2520LLM-based%250Aexpansion%2520methods%2520relying%2520on%2520domain%2520knowledge%2520often%2520falter.%2520CLAP%2520instead%2520adopts%250Aa%2520logic-centric%2520pipeline%2520that%2520enables%2520robust%252C%2520domain-agnostic%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06941v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAP%3A%20Coreference-Linked%20Augmentation%20for%20Passage%20Retrieval&entry.906535625=Huanwei%20Xu%20and%20Lin%20Xu%20and%20Liang%20Yuan&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20passage%20expansion%20has%20shown%20promise%20for%0Aenhancing%20first-stage%20retrieval%2C%20but%20often%20underperforms%20with%20dense%20retrievers%0Adue%20to%20semantic%20drift%20and%20misalignment%20with%20their%20pretrained%20semantic%20space.%0ABeyond%20this%2C%20only%20a%20portion%20of%20a%20passage%20is%20typically%20relevant%20to%20a%20query%2C%0Awhile%20the%20rest%20introduces%20noise--an%20issue%20compounded%20by%20chunking%20techniques%0Athat%20break%20coreference%20continuity.%20We%20propose%20Coreference-Linked%20Augmentation%0Afor%20Passage%20Retrieval%20%28CLAP%29%2C%20a%20lightweight%20LLM-based%20expansion%20framework%20that%0Asegments%20passages%20into%20coherent%20chunks%2C%20resolves%20coreference%20chains%2C%20and%0Agenerates%20localized%20pseudo-queries%20aligned%20with%20dense%20retriever%0Arepresentations.%20A%20simple%20fusion%20of%20global%20topical%20signals%20and%20fine-grained%0Asubtopic%20signals%20achieves%20robust%20performance%20across%20domains.%20CLAP%20yields%0Aconsistent%20gains%20even%20as%20retriever%20strength%20increases%2C%20enabling%20dense%0Aretrievers%20to%20match%20or%20surpass%20second-stage%20rankers%20such%20as%20BM25%20%2B%20MonoT5-3B%2C%0Awith%20up%20to%2020.68%25%20absolute%20nDCG%4010%20improvement.%20These%20improvements%20are%0Aespecially%20notable%20in%20out-of-domain%20settings%2C%20where%20conventional%20LLM-based%0Aexpansion%20methods%20relying%20on%20domain%20knowledge%20often%20falter.%20CLAP%20instead%20adopts%0Aa%20logic-centric%20pipeline%20that%20enables%20robust%2C%20domain-agnostic%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06941v2&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Accurate Sign Language Translation\n  in Low-Resource Scenarios", "author": "Luana Bulla and Gabriele Tuccio and Misael Mongiov\u00ec and Aldo Gangemi", "abstract": "  Translating natural languages into sign languages is a highly complex and\nunderexplored task. Despite growing interest in accessibility and inclusivity,\nthe development of robust translation systems remains hindered by the limited\navailability of parallel corpora which align natural language with sign\nlanguage data. Existing methods often struggle to generalize in these\ndata-scarce environments, as the few datasets available are typically\ndomain-specific, lack standardization, or fail to capture the full linguistic\nrichness of sign languages. To address this limitation, we propose Advanced Use\nof LLMs for Sign Language Translation (AulSign), a novel method that leverages\nLarge Language Models via dynamic prompting and in-context learning with sample\nselection and subsequent sign association. Despite their impressive abilities\nin processing text, LLMs lack intrinsic knowledge of sign languages; therefore,\nthey are unable to natively perform this kind of translation. To overcome this\nlimitation, we associate the signs with compact descriptions in natural\nlanguage and instruct the model to use them. We evaluate our method on both\nEnglish and Italian languages using SignBank+, a recognized benchmark in the\nfield, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior\nperformance compared to state-of-the-art models in low-data scenario. Our\nfindings demonstrate the effectiveness of AulSign, with the potential to\nenhance accessibility and inclusivity in communication technologies for\nunderrepresented linguistic communities.\n", "link": "http://arxiv.org/abs/2508.18183v1", "date": "2025-08-25", "relevancy": 2.4998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios%0AAuthor%3A%20Luana%20Bulla%20and%20Gabriele%20Tuccio%20and%20Misael%20Mongiov%C3%AC%20and%20Aldo%20Gangemi%0AAbstract%3A%20%20%20Translating%20natural%20languages%20into%20sign%20languages%20is%20a%20highly%20complex%20and%0Aunderexplored%20task.%20Despite%20growing%20interest%20in%20accessibility%20and%20inclusivity%2C%0Athe%20development%20of%20robust%20translation%20systems%20remains%20hindered%20by%20the%20limited%0Aavailability%20of%20parallel%20corpora%20which%20align%20natural%20language%20with%20sign%0Alanguage%20data.%20Existing%20methods%20often%20struggle%20to%20generalize%20in%20these%0Adata-scarce%20environments%2C%20as%20the%20few%20datasets%20available%20are%20typically%0Adomain-specific%2C%20lack%20standardization%2C%20or%20fail%20to%20capture%20the%20full%20linguistic%0Arichness%20of%20sign%20languages.%20To%20address%20this%20limitation%2C%20we%20propose%20Advanced%20Use%0Aof%20LLMs%20for%20Sign%20Language%20Translation%20%28AulSign%29%2C%20a%20novel%20method%20that%20leverages%0ALarge%20Language%20Models%20via%20dynamic%20prompting%20and%20in-context%20learning%20with%20sample%0Aselection%20and%20subsequent%20sign%20association.%20Despite%20their%20impressive%20abilities%0Ain%20processing%20text%2C%20LLMs%20lack%20intrinsic%20knowledge%20of%20sign%20languages%3B%20therefore%2C%0Athey%20are%20unable%20to%20natively%20perform%20this%20kind%20of%20translation.%20To%20overcome%20this%0Alimitation%2C%20we%20associate%20the%20signs%20with%20compact%20descriptions%20in%20natural%0Alanguage%20and%20instruct%20the%20model%20to%20use%20them.%20We%20evaluate%20our%20method%20on%20both%0AEnglish%20and%20Italian%20languages%20using%20SignBank%2B%2C%20a%20recognized%20benchmark%20in%20the%0Afield%2C%20as%20well%20as%20the%20Italian%20LaCAM%20CNR-ISTC%20dataset.%20We%20demonstrate%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20in%20low-data%20scenario.%20Our%0Afindings%20demonstrate%20the%20effectiveness%20of%20AulSign%2C%20with%20the%20potential%20to%0Aenhance%20accessibility%20and%20inclusivity%20in%20communication%20technologies%20for%0Aunderrepresented%20linguistic%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Accurate%2520Sign%2520Language%2520Translation%250A%2520%2520in%2520Low-Resource%2520Scenarios%26entry.906535625%3DLuana%2520Bulla%2520and%2520Gabriele%2520Tuccio%2520and%2520Misael%2520Mongiov%25C3%25AC%2520and%2520Aldo%2520Gangemi%26entry.1292438233%3D%2520%2520Translating%2520natural%2520languages%2520into%2520sign%2520languages%2520is%2520a%2520highly%2520complex%2520and%250Aunderexplored%2520task.%2520Despite%2520growing%2520interest%2520in%2520accessibility%2520and%2520inclusivity%252C%250Athe%2520development%2520of%2520robust%2520translation%2520systems%2520remains%2520hindered%2520by%2520the%2520limited%250Aavailability%2520of%2520parallel%2520corpora%2520which%2520align%2520natural%2520language%2520with%2520sign%250Alanguage%2520data.%2520Existing%2520methods%2520often%2520struggle%2520to%2520generalize%2520in%2520these%250Adata-scarce%2520environments%252C%2520as%2520the%2520few%2520datasets%2520available%2520are%2520typically%250Adomain-specific%252C%2520lack%2520standardization%252C%2520or%2520fail%2520to%2520capture%2520the%2520full%2520linguistic%250Arichness%2520of%2520sign%2520languages.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Advanced%2520Use%250Aof%2520LLMs%2520for%2520Sign%2520Language%2520Translation%2520%2528AulSign%2529%252C%2520a%2520novel%2520method%2520that%2520leverages%250ALarge%2520Language%2520Models%2520via%2520dynamic%2520prompting%2520and%2520in-context%2520learning%2520with%2520sample%250Aselection%2520and%2520subsequent%2520sign%2520association.%2520Despite%2520their%2520impressive%2520abilities%250Ain%2520processing%2520text%252C%2520LLMs%2520lack%2520intrinsic%2520knowledge%2520of%2520sign%2520languages%253B%2520therefore%252C%250Athey%2520are%2520unable%2520to%2520natively%2520perform%2520this%2520kind%2520of%2520translation.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520associate%2520the%2520signs%2520with%2520compact%2520descriptions%2520in%2520natural%250Alanguage%2520and%2520instruct%2520the%2520model%2520to%2520use%2520them.%2520We%2520evaluate%2520our%2520method%2520on%2520both%250AEnglish%2520and%2520Italian%2520languages%2520using%2520SignBank%252B%252C%2520a%2520recognized%2520benchmark%2520in%2520the%250Afield%252C%2520as%2520well%2520as%2520the%2520Italian%2520LaCAM%2520CNR-ISTC%2520dataset.%2520We%2520demonstrate%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%2520models%2520in%2520low-data%2520scenario.%2520Our%250Afindings%2520demonstrate%2520the%2520effectiveness%2520of%2520AulSign%252C%2520with%2520the%2520potential%2520to%250Aenhance%2520accessibility%2520and%2520inclusivity%2520in%2520communication%2520technologies%2520for%250Aunderrepresented%2520linguistic%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Accurate%20Sign%20Language%20Translation%0A%20%20in%20Low-Resource%20Scenarios&entry.906535625=Luana%20Bulla%20and%20Gabriele%20Tuccio%20and%20Misael%20Mongiov%C3%AC%20and%20Aldo%20Gangemi&entry.1292438233=%20%20Translating%20natural%20languages%20into%20sign%20languages%20is%20a%20highly%20complex%20and%0Aunderexplored%20task.%20Despite%20growing%20interest%20in%20accessibility%20and%20inclusivity%2C%0Athe%20development%20of%20robust%20translation%20systems%20remains%20hindered%20by%20the%20limited%0Aavailability%20of%20parallel%20corpora%20which%20align%20natural%20language%20with%20sign%0Alanguage%20data.%20Existing%20methods%20often%20struggle%20to%20generalize%20in%20these%0Adata-scarce%20environments%2C%20as%20the%20few%20datasets%20available%20are%20typically%0Adomain-specific%2C%20lack%20standardization%2C%20or%20fail%20to%20capture%20the%20full%20linguistic%0Arichness%20of%20sign%20languages.%20To%20address%20this%20limitation%2C%20we%20propose%20Advanced%20Use%0Aof%20LLMs%20for%20Sign%20Language%20Translation%20%28AulSign%29%2C%20a%20novel%20method%20that%20leverages%0ALarge%20Language%20Models%20via%20dynamic%20prompting%20and%20in-context%20learning%20with%20sample%0Aselection%20and%20subsequent%20sign%20association.%20Despite%20their%20impressive%20abilities%0Ain%20processing%20text%2C%20LLMs%20lack%20intrinsic%20knowledge%20of%20sign%20languages%3B%20therefore%2C%0Athey%20are%20unable%20to%20natively%20perform%20this%20kind%20of%20translation.%20To%20overcome%20this%0Alimitation%2C%20we%20associate%20the%20signs%20with%20compact%20descriptions%20in%20natural%0Alanguage%20and%20instruct%20the%20model%20to%20use%20them.%20We%20evaluate%20our%20method%20on%20both%0AEnglish%20and%20Italian%20languages%20using%20SignBank%2B%2C%20a%20recognized%20benchmark%20in%20the%0Afield%2C%20as%20well%20as%20the%20Italian%20LaCAM%20CNR-ISTC%20dataset.%20We%20demonstrate%20superior%0Aperformance%20compared%20to%20state-of-the-art%20models%20in%20low-data%20scenario.%20Our%0Afindings%20demonstrate%20the%20effectiveness%20of%20AulSign%2C%20with%20the%20potential%20to%0Aenhance%20accessibility%20and%20inclusivity%20in%20communication%20technologies%20for%0Aunderrepresented%20linguistic%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18183v1&entry.124074799=Read"},
{"title": "Topology Aware Neural Interpolation of Scalar Fields", "author": "Mohamed Kissi and Keanu Sisouk and Joshua A. Levine and Julien Tierny", "abstract": "  This paper presents a neural scheme for the topology-aware interpolation of\ntime-varying scalar fields. Given a time-varying sequence of persistence\ndiagrams, along with a sparse temporal sampling of the corresponding scalar\nfields, denoted as keyframes, our interpolation approach aims at \"inverting\"\nthe non-keyframe diagrams to produce plausible estimations of the\ncorresponding, missing data. For this, we rely on a neural architecture which\nlearns the relation from a time value to the corresponding scalar field, based\non the keyframe examples, and reliably extends this relation to the\nnon-keyframe time steps. We show how augmenting this architecture with specific\ntopological losses exploiting the input diagrams both improves the geometrical\nand topological reconstruction of the non-keyframe time steps. At query time,\ngiven an input time value for which an interpolation is desired, our approach\ninstantaneously produces an output, via a single propagation of the time input\nthrough the network. Experiments interpolating 2D and 3D time-varying datasets\nshow our approach superiority, both in terms of data and topological fitting,\nwith regard to reference interpolation schemes.\n", "link": "http://arxiv.org/abs/2508.17995v1", "date": "2025-08-25", "relevancy": 2.4933, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5062}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5048}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields&body=Title%3A%20Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields%0AAuthor%3A%20Mohamed%20Kissi%20and%20Keanu%20Sisouk%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20neural%20scheme%20for%20the%20topology-aware%20interpolation%20of%0Atime-varying%20scalar%20fields.%20Given%20a%20time-varying%20sequence%20of%20persistence%0Adiagrams%2C%20along%20with%20a%20sparse%20temporal%20sampling%20of%20the%20corresponding%20scalar%0Afields%2C%20denoted%20as%20keyframes%2C%20our%20interpolation%20approach%20aims%20at%20%22inverting%22%0Athe%20non-keyframe%20diagrams%20to%20produce%20plausible%20estimations%20of%20the%0Acorresponding%2C%20missing%20data.%20For%20this%2C%20we%20rely%20on%20a%20neural%20architecture%20which%0Alearns%20the%20relation%20from%20a%20time%20value%20to%20the%20corresponding%20scalar%20field%2C%20based%0Aon%20the%20keyframe%20examples%2C%20and%20reliably%20extends%20this%20relation%20to%20the%0Anon-keyframe%20time%20steps.%20We%20show%20how%20augmenting%20this%20architecture%20with%20specific%0Atopological%20losses%20exploiting%20the%20input%20diagrams%20both%20improves%20the%20geometrical%0Aand%20topological%20reconstruction%20of%20the%20non-keyframe%20time%20steps.%20At%20query%20time%2C%0Agiven%20an%20input%20time%20value%20for%20which%20an%20interpolation%20is%20desired%2C%20our%20approach%0Ainstantaneously%20produces%20an%20output%2C%20via%20a%20single%20propagation%20of%20the%20time%20input%0Athrough%20the%20network.%20Experiments%20interpolating%202D%20and%203D%20time-varying%20datasets%0Ashow%20our%20approach%20superiority%2C%20both%20in%20terms%20of%20data%20and%20topological%20fitting%2C%0Awith%20regard%20to%20reference%20interpolation%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology%2520Aware%2520Neural%2520Interpolation%2520of%2520Scalar%2520Fields%26entry.906535625%3DMohamed%2520Kissi%2520and%2520Keanu%2520Sisouk%2520and%2520Joshua%2520A.%2520Levine%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520neural%2520scheme%2520for%2520the%2520topology-aware%2520interpolation%2520of%250Atime-varying%2520scalar%2520fields.%2520Given%2520a%2520time-varying%2520sequence%2520of%2520persistence%250Adiagrams%252C%2520along%2520with%2520a%2520sparse%2520temporal%2520sampling%2520of%2520the%2520corresponding%2520scalar%250Afields%252C%2520denoted%2520as%2520keyframes%252C%2520our%2520interpolation%2520approach%2520aims%2520at%2520%2522inverting%2522%250Athe%2520non-keyframe%2520diagrams%2520to%2520produce%2520plausible%2520estimations%2520of%2520the%250Acorresponding%252C%2520missing%2520data.%2520For%2520this%252C%2520we%2520rely%2520on%2520a%2520neural%2520architecture%2520which%250Alearns%2520the%2520relation%2520from%2520a%2520time%2520value%2520to%2520the%2520corresponding%2520scalar%2520field%252C%2520based%250Aon%2520the%2520keyframe%2520examples%252C%2520and%2520reliably%2520extends%2520this%2520relation%2520to%2520the%250Anon-keyframe%2520time%2520steps.%2520We%2520show%2520how%2520augmenting%2520this%2520architecture%2520with%2520specific%250Atopological%2520losses%2520exploiting%2520the%2520input%2520diagrams%2520both%2520improves%2520the%2520geometrical%250Aand%2520topological%2520reconstruction%2520of%2520the%2520non-keyframe%2520time%2520steps.%2520At%2520query%2520time%252C%250Agiven%2520an%2520input%2520time%2520value%2520for%2520which%2520an%2520interpolation%2520is%2520desired%252C%2520our%2520approach%250Ainstantaneously%2520produces%2520an%2520output%252C%2520via%2520a%2520single%2520propagation%2520of%2520the%2520time%2520input%250Athrough%2520the%2520network.%2520Experiments%2520interpolating%25202D%2520and%25203D%2520time-varying%2520datasets%250Ashow%2520our%2520approach%2520superiority%252C%2520both%2520in%2520terms%2520of%2520data%2520and%2520topological%2520fitting%252C%250Awith%2520regard%2520to%2520reference%2520interpolation%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields&entry.906535625=Mohamed%20Kissi%20and%20Keanu%20Sisouk%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20presents%20a%20neural%20scheme%20for%20the%20topology-aware%20interpolation%20of%0Atime-varying%20scalar%20fields.%20Given%20a%20time-varying%20sequence%20of%20persistence%0Adiagrams%2C%20along%20with%20a%20sparse%20temporal%20sampling%20of%20the%20corresponding%20scalar%0Afields%2C%20denoted%20as%20keyframes%2C%20our%20interpolation%20approach%20aims%20at%20%22inverting%22%0Athe%20non-keyframe%20diagrams%20to%20produce%20plausible%20estimations%20of%20the%0Acorresponding%2C%20missing%20data.%20For%20this%2C%20we%20rely%20on%20a%20neural%20architecture%20which%0Alearns%20the%20relation%20from%20a%20time%20value%20to%20the%20corresponding%20scalar%20field%2C%20based%0Aon%20the%20keyframe%20examples%2C%20and%20reliably%20extends%20this%20relation%20to%20the%0Anon-keyframe%20time%20steps.%20We%20show%20how%20augmenting%20this%20architecture%20with%20specific%0Atopological%20losses%20exploiting%20the%20input%20diagrams%20both%20improves%20the%20geometrical%0Aand%20topological%20reconstruction%20of%20the%20non-keyframe%20time%20steps.%20At%20query%20time%2C%0Agiven%20an%20input%20time%20value%20for%20which%20an%20interpolation%20is%20desired%2C%20our%20approach%0Ainstantaneously%20produces%20an%20output%2C%20via%20a%20single%20propagation%20of%20the%20time%20input%0Athrough%20the%20network.%20Experiments%20interpolating%202D%20and%203D%20time-varying%20datasets%0Ashow%20our%20approach%20superiority%2C%20both%20in%20terms%20of%20data%20and%20topological%20fitting%2C%0Awith%20regard%20to%20reference%20interpolation%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17995v1&entry.124074799=Read"},
{"title": "Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated\n  Image Detection", "author": "Dabbrata Das and Mahshar Yahan and Md Tareq Zaman and Md Rishadul Bayesh", "abstract": "  The rapid advancement of generative models has led to a growing prevalence of\nhighly realistic AI-generated images, posing significant challenges for digital\nforensics and content authentication. Conventional detection methods mainly\nrely on deep learning models that extract global features, which often overlook\nsubtle structural inconsistencies and demand substantial computational\nresources. To address these limitations, we propose a hybrid detection\nframework that combines a fine-tuned Vision Transformer (ViT) with a novel\nedge-based image processing module. The edge-based module computes variance\nfrom edge-difference maps generated before and after smoothing, exploiting the\nobservation that AI-generated images typically exhibit smoother textures,\nweaker edges, and reduced noise compared to real images. When applied as a\npost-processing step on ViT predictions, this module enhances sensitivity to\nfine-grained structural cues while maintaining computational efficiency.\nExtensive experiments on the CIFAKE, Artistic, and Custom Curated datasets\ndemonstrate that the proposed framework achieves superior detection performance\nacross all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on\nCIFAKE, surpassing widely adopted state-of-the-art models. These results\nestablish the proposed method as a lightweight, interpretable, and effective\nsolution for both still images and video frames, making it highly suitable for\nreal-world applications in automated content verification and digital\nforensics.\n", "link": "http://arxiv.org/abs/2508.17877v1", "date": "2025-08-25", "relevancy": 2.4773, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6293}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Enhanced%20Vision%20Transformer%20Framework%20for%20Accurate%20AI-Generated%0A%20%20Image%20Detection&body=Title%3A%20Edge-Enhanced%20Vision%20Transformer%20Framework%20for%20Accurate%20AI-Generated%0A%20%20Image%20Detection%0AAuthor%3A%20Dabbrata%20Das%20and%20Mahshar%20Yahan%20and%20Md%20Tareq%20Zaman%20and%20Md%20Rishadul%20Bayesh%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20models%20has%20led%20to%20a%20growing%20prevalence%20of%0Ahighly%20realistic%20AI-generated%20images%2C%20posing%20significant%20challenges%20for%20digital%0Aforensics%20and%20content%20authentication.%20Conventional%20detection%20methods%20mainly%0Arely%20on%20deep%20learning%20models%20that%20extract%20global%20features%2C%20which%20often%20overlook%0Asubtle%20structural%20inconsistencies%20and%20demand%20substantial%20computational%0Aresources.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hybrid%20detection%0Aframework%20that%20combines%20a%20fine-tuned%20Vision%20Transformer%20%28ViT%29%20with%20a%20novel%0Aedge-based%20image%20processing%20module.%20The%20edge-based%20module%20computes%20variance%0Afrom%20edge-difference%20maps%20generated%20before%20and%20after%20smoothing%2C%20exploiting%20the%0Aobservation%20that%20AI-generated%20images%20typically%20exhibit%20smoother%20textures%2C%0Aweaker%20edges%2C%20and%20reduced%20noise%20compared%20to%20real%20images.%20When%20applied%20as%20a%0Apost-processing%20step%20on%20ViT%20predictions%2C%20this%20module%20enhances%20sensitivity%20to%0Afine-grained%20structural%20cues%20while%20maintaining%20computational%20efficiency.%0AExtensive%20experiments%20on%20the%20CIFAKE%2C%20Artistic%2C%20and%20Custom%20Curated%20datasets%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%20detection%20performance%0Aacross%20all%20benchmarks%2C%20attaining%2097.75%25%20accuracy%20and%20a%2097.77%25%20F1-score%20on%0ACIFAKE%2C%20surpassing%20widely%20adopted%20state-of-the-art%20models.%20These%20results%0Aestablish%20the%20proposed%20method%20as%20a%20lightweight%2C%20interpretable%2C%20and%20effective%0Asolution%20for%20both%20still%20images%20and%20video%20frames%2C%20making%20it%20highly%20suitable%20for%0Areal-world%20applications%20in%20automated%20content%20verification%20and%20digital%0Aforensics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Enhanced%2520Vision%2520Transformer%2520Framework%2520for%2520Accurate%2520AI-Generated%250A%2520%2520Image%2520Detection%26entry.906535625%3DDabbrata%2520Das%2520and%2520Mahshar%2520Yahan%2520and%2520Md%2520Tareq%2520Zaman%2520and%2520Md%2520Rishadul%2520Bayesh%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520models%2520has%2520led%2520to%2520a%2520growing%2520prevalence%2520of%250Ahighly%2520realistic%2520AI-generated%2520images%252C%2520posing%2520significant%2520challenges%2520for%2520digital%250Aforensics%2520and%2520content%2520authentication.%2520Conventional%2520detection%2520methods%2520mainly%250Arely%2520on%2520deep%2520learning%2520models%2520that%2520extract%2520global%2520features%252C%2520which%2520often%2520overlook%250Asubtle%2520structural%2520inconsistencies%2520and%2520demand%2520substantial%2520computational%250Aresources.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520hybrid%2520detection%250Aframework%2520that%2520combines%2520a%2520fine-tuned%2520Vision%2520Transformer%2520%2528ViT%2529%2520with%2520a%2520novel%250Aedge-based%2520image%2520processing%2520module.%2520The%2520edge-based%2520module%2520computes%2520variance%250Afrom%2520edge-difference%2520maps%2520generated%2520before%2520and%2520after%2520smoothing%252C%2520exploiting%2520the%250Aobservation%2520that%2520AI-generated%2520images%2520typically%2520exhibit%2520smoother%2520textures%252C%250Aweaker%2520edges%252C%2520and%2520reduced%2520noise%2520compared%2520to%2520real%2520images.%2520When%2520applied%2520as%2520a%250Apost-processing%2520step%2520on%2520ViT%2520predictions%252C%2520this%2520module%2520enhances%2520sensitivity%2520to%250Afine-grained%2520structural%2520cues%2520while%2520maintaining%2520computational%2520efficiency.%250AExtensive%2520experiments%2520on%2520the%2520CIFAKE%252C%2520Artistic%252C%2520and%2520Custom%2520Curated%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520achieves%2520superior%2520detection%2520performance%250Aacross%2520all%2520benchmarks%252C%2520attaining%252097.75%2525%2520accuracy%2520and%2520a%252097.77%2525%2520F1-score%2520on%250ACIFAKE%252C%2520surpassing%2520widely%2520adopted%2520state-of-the-art%2520models.%2520These%2520results%250Aestablish%2520the%2520proposed%2520method%2520as%2520a%2520lightweight%252C%2520interpretable%252C%2520and%2520effective%250Asolution%2520for%2520both%2520still%2520images%2520and%2520video%2520frames%252C%2520making%2520it%2520highly%2520suitable%2520for%250Areal-world%2520applications%2520in%2520automated%2520content%2520verification%2520and%2520digital%250Aforensics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Enhanced%20Vision%20Transformer%20Framework%20for%20Accurate%20AI-Generated%0A%20%20Image%20Detection&entry.906535625=Dabbrata%20Das%20and%20Mahshar%20Yahan%20and%20Md%20Tareq%20Zaman%20and%20Md%20Rishadul%20Bayesh&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20models%20has%20led%20to%20a%20growing%20prevalence%20of%0Ahighly%20realistic%20AI-generated%20images%2C%20posing%20significant%20challenges%20for%20digital%0Aforensics%20and%20content%20authentication.%20Conventional%20detection%20methods%20mainly%0Arely%20on%20deep%20learning%20models%20that%20extract%20global%20features%2C%20which%20often%20overlook%0Asubtle%20structural%20inconsistencies%20and%20demand%20substantial%20computational%0Aresources.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hybrid%20detection%0Aframework%20that%20combines%20a%20fine-tuned%20Vision%20Transformer%20%28ViT%29%20with%20a%20novel%0Aedge-based%20image%20processing%20module.%20The%20edge-based%20module%20computes%20variance%0Afrom%20edge-difference%20maps%20generated%20before%20and%20after%20smoothing%2C%20exploiting%20the%0Aobservation%20that%20AI-generated%20images%20typically%20exhibit%20smoother%20textures%2C%0Aweaker%20edges%2C%20and%20reduced%20noise%20compared%20to%20real%20images.%20When%20applied%20as%20a%0Apost-processing%20step%20on%20ViT%20predictions%2C%20this%20module%20enhances%20sensitivity%20to%0Afine-grained%20structural%20cues%20while%20maintaining%20computational%20efficiency.%0AExtensive%20experiments%20on%20the%20CIFAKE%2C%20Artistic%2C%20and%20Custom%20Curated%20datasets%0Ademonstrate%20that%20the%20proposed%20framework%20achieves%20superior%20detection%20performance%0Aacross%20all%20benchmarks%2C%20attaining%2097.75%25%20accuracy%20and%20a%2097.77%25%20F1-score%20on%0ACIFAKE%2C%20surpassing%20widely%20adopted%20state-of-the-art%20models.%20These%20results%0Aestablish%20the%20proposed%20method%20as%20a%20lightweight%2C%20interpretable%2C%20and%20effective%0Asolution%20for%20both%20still%20images%20and%20video%20frames%2C%20making%20it%20highly%20suitable%20for%0Areal-world%20applications%20in%20automated%20content%20verification%20and%20digital%0Aforensics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17877v1&entry.124074799=Read"},
{"title": "3D Feature Distillation with Object-Centric Priors", "author": "Georgios Tziafas and Yucheng Xu and Zhibin Li and Hamidreza Kasaei", "abstract": "  Grounding natural language to the physical world is a ubiquitous topic with a\nwide range of applications in computer vision and robotics. Recently, 2D\nvision-language models such as CLIP have been widely popularized, due to their\nimpressive capabilities for open-vocabulary grounding in 2D images. Recent\nworks aim to elevate 2D CLIP features to 3D via feature distillation, but\neither learn neural fields that are scene-specific and hence lack\ngeneralization, or focus on indoor room scan data that require access to\nmultiple camera views, which is not practical in robot manipulation scenarios.\nAdditionally, related methods typically fuse features at pixel-level and assume\nthat all camera views are equally informative. In this work, we show that this\napproach leads to sub-optimal 3D features, both in terms of grounding accuracy,\nas well as segmentation crispness. To alleviate this, we propose a multi-view\nfeature fusion strategy that employs object-centric priors to eliminate\nuninformative views based on semantic information, and fuse features at\nobject-level via instance segmentation masks. To distill our object-centric 3D\nfeatures, we generate a large-scale synthetic multi-view dataset of cluttered\ntabletop scenes, spawning 15k scenes from over 3300 unique object instances,\nwhich we make publicly available. We show that our method reconstructs 3D CLIP\nfeatures with improved grounding capacity and spatial consistency, while doing\nso from single-view RGB-D, thus departing from the assumption of multiple\ncamera views at test time. Finally, we show that our approach can generalize to\nnovel tabletop domains and be re-purposed for 3D instance segmentation without\nfine-tuning, and demonstrate its utility for language-guided robotic grasping\nin clutter.\n", "link": "http://arxiv.org/abs/2406.18742v5", "date": "2025-08-25", "relevancy": 2.4732, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6184}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6184}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Feature%20Distillation%20with%20Object-Centric%20Priors&body=Title%3A%203D%20Feature%20Distillation%20with%20Object-Centric%20Priors%0AAuthor%3A%20Georgios%20Tziafas%20and%20Yucheng%20Xu%20and%20Zhibin%20Li%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20Grounding%20natural%20language%20to%20the%20physical%20world%20is%20a%20ubiquitous%20topic%20with%20a%0Awide%20range%20of%20applications%20in%20computer%20vision%20and%20robotics.%20Recently%2C%202D%0Avision-language%20models%20such%20as%20CLIP%20have%20been%20widely%20popularized%2C%20due%20to%20their%0Aimpressive%20capabilities%20for%20open-vocabulary%20grounding%20in%202D%20images.%20Recent%0Aworks%20aim%20to%20elevate%202D%20CLIP%20features%20to%203D%20via%20feature%20distillation%2C%20but%0Aeither%20learn%20neural%20fields%20that%20are%20scene-specific%20and%20hence%20lack%0Ageneralization%2C%20or%20focus%20on%20indoor%20room%20scan%20data%20that%20require%20access%20to%0Amultiple%20camera%20views%2C%20which%20is%20not%20practical%20in%20robot%20manipulation%20scenarios.%0AAdditionally%2C%20related%20methods%20typically%20fuse%20features%20at%20pixel-level%20and%20assume%0Athat%20all%20camera%20views%20are%20equally%20informative.%20In%20this%20work%2C%20we%20show%20that%20this%0Aapproach%20leads%20to%20sub-optimal%203D%20features%2C%20both%20in%20terms%20of%20grounding%20accuracy%2C%0Aas%20well%20as%20segmentation%20crispness.%20To%20alleviate%20this%2C%20we%20propose%20a%20multi-view%0Afeature%20fusion%20strategy%20that%20employs%20object-centric%20priors%20to%20eliminate%0Auninformative%20views%20based%20on%20semantic%20information%2C%20and%20fuse%20features%20at%0Aobject-level%20via%20instance%20segmentation%20masks.%20To%20distill%20our%20object-centric%203D%0Afeatures%2C%20we%20generate%20a%20large-scale%20synthetic%20multi-view%20dataset%20of%20cluttered%0Atabletop%20scenes%2C%20spawning%2015k%20scenes%20from%20over%203300%20unique%20object%20instances%2C%0Awhich%20we%20make%20publicly%20available.%20We%20show%20that%20our%20method%20reconstructs%203D%20CLIP%0Afeatures%20with%20improved%20grounding%20capacity%20and%20spatial%20consistency%2C%20while%20doing%0Aso%20from%20single-view%20RGB-D%2C%20thus%20departing%20from%20the%20assumption%20of%20multiple%0Acamera%20views%20at%20test%20time.%20Finally%2C%20we%20show%20that%20our%20approach%20can%20generalize%20to%0Anovel%20tabletop%20domains%20and%20be%20re-purposed%20for%203D%20instance%20segmentation%20without%0Afine-tuning%2C%20and%20demonstrate%20its%20utility%20for%20language-guided%20robotic%20grasping%0Ain%20clutter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18742v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Feature%2520Distillation%2520with%2520Object-Centric%2520Priors%26entry.906535625%3DGeorgios%2520Tziafas%2520and%2520Yucheng%2520Xu%2520and%2520Zhibin%2520Li%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520Grounding%2520natural%2520language%2520to%2520the%2520physical%2520world%2520is%2520a%2520ubiquitous%2520topic%2520with%2520a%250Awide%2520range%2520of%2520applications%2520in%2520computer%2520vision%2520and%2520robotics.%2520Recently%252C%25202D%250Avision-language%2520models%2520such%2520as%2520CLIP%2520have%2520been%2520widely%2520popularized%252C%2520due%2520to%2520their%250Aimpressive%2520capabilities%2520for%2520open-vocabulary%2520grounding%2520in%25202D%2520images.%2520Recent%250Aworks%2520aim%2520to%2520elevate%25202D%2520CLIP%2520features%2520to%25203D%2520via%2520feature%2520distillation%252C%2520but%250Aeither%2520learn%2520neural%2520fields%2520that%2520are%2520scene-specific%2520and%2520hence%2520lack%250Ageneralization%252C%2520or%2520focus%2520on%2520indoor%2520room%2520scan%2520data%2520that%2520require%2520access%2520to%250Amultiple%2520camera%2520views%252C%2520which%2520is%2520not%2520practical%2520in%2520robot%2520manipulation%2520scenarios.%250AAdditionally%252C%2520related%2520methods%2520typically%2520fuse%2520features%2520at%2520pixel-level%2520and%2520assume%250Athat%2520all%2520camera%2520views%2520are%2520equally%2520informative.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520this%250Aapproach%2520leads%2520to%2520sub-optimal%25203D%2520features%252C%2520both%2520in%2520terms%2520of%2520grounding%2520accuracy%252C%250Aas%2520well%2520as%2520segmentation%2520crispness.%2520To%2520alleviate%2520this%252C%2520we%2520propose%2520a%2520multi-view%250Afeature%2520fusion%2520strategy%2520that%2520employs%2520object-centric%2520priors%2520to%2520eliminate%250Auninformative%2520views%2520based%2520on%2520semantic%2520information%252C%2520and%2520fuse%2520features%2520at%250Aobject-level%2520via%2520instance%2520segmentation%2520masks.%2520To%2520distill%2520our%2520object-centric%25203D%250Afeatures%252C%2520we%2520generate%2520a%2520large-scale%2520synthetic%2520multi-view%2520dataset%2520of%2520cluttered%250Atabletop%2520scenes%252C%2520spawning%252015k%2520scenes%2520from%2520over%25203300%2520unique%2520object%2520instances%252C%250Awhich%2520we%2520make%2520publicly%2520available.%2520We%2520show%2520that%2520our%2520method%2520reconstructs%25203D%2520CLIP%250Afeatures%2520with%2520improved%2520grounding%2520capacity%2520and%2520spatial%2520consistency%252C%2520while%2520doing%250Aso%2520from%2520single-view%2520RGB-D%252C%2520thus%2520departing%2520from%2520the%2520assumption%2520of%2520multiple%250Acamera%2520views%2520at%2520test%2520time.%2520Finally%252C%2520we%2520show%2520that%2520our%2520approach%2520can%2520generalize%2520to%250Anovel%2520tabletop%2520domains%2520and%2520be%2520re-purposed%2520for%25203D%2520instance%2520segmentation%2520without%250Afine-tuning%252C%2520and%2520demonstrate%2520its%2520utility%2520for%2520language-guided%2520robotic%2520grasping%250Ain%2520clutter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18742v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Feature%20Distillation%20with%20Object-Centric%20Priors&entry.906535625=Georgios%20Tziafas%20and%20Yucheng%20Xu%20and%20Zhibin%20Li%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20Grounding%20natural%20language%20to%20the%20physical%20world%20is%20a%20ubiquitous%20topic%20with%20a%0Awide%20range%20of%20applications%20in%20computer%20vision%20and%20robotics.%20Recently%2C%202D%0Avision-language%20models%20such%20as%20CLIP%20have%20been%20widely%20popularized%2C%20due%20to%20their%0Aimpressive%20capabilities%20for%20open-vocabulary%20grounding%20in%202D%20images.%20Recent%0Aworks%20aim%20to%20elevate%202D%20CLIP%20features%20to%203D%20via%20feature%20distillation%2C%20but%0Aeither%20learn%20neural%20fields%20that%20are%20scene-specific%20and%20hence%20lack%0Ageneralization%2C%20or%20focus%20on%20indoor%20room%20scan%20data%20that%20require%20access%20to%0Amultiple%20camera%20views%2C%20which%20is%20not%20practical%20in%20robot%20manipulation%20scenarios.%0AAdditionally%2C%20related%20methods%20typically%20fuse%20features%20at%20pixel-level%20and%20assume%0Athat%20all%20camera%20views%20are%20equally%20informative.%20In%20this%20work%2C%20we%20show%20that%20this%0Aapproach%20leads%20to%20sub-optimal%203D%20features%2C%20both%20in%20terms%20of%20grounding%20accuracy%2C%0Aas%20well%20as%20segmentation%20crispness.%20To%20alleviate%20this%2C%20we%20propose%20a%20multi-view%0Afeature%20fusion%20strategy%20that%20employs%20object-centric%20priors%20to%20eliminate%0Auninformative%20views%20based%20on%20semantic%20information%2C%20and%20fuse%20features%20at%0Aobject-level%20via%20instance%20segmentation%20masks.%20To%20distill%20our%20object-centric%203D%0Afeatures%2C%20we%20generate%20a%20large-scale%20synthetic%20multi-view%20dataset%20of%20cluttered%0Atabletop%20scenes%2C%20spawning%2015k%20scenes%20from%20over%203300%20unique%20object%20instances%2C%0Awhich%20we%20make%20publicly%20available.%20We%20show%20that%20our%20method%20reconstructs%203D%20CLIP%0Afeatures%20with%20improved%20grounding%20capacity%20and%20spatial%20consistency%2C%20while%20doing%0Aso%20from%20single-view%20RGB-D%2C%20thus%20departing%20from%20the%20assumption%20of%20multiple%0Acamera%20views%20at%20test%20time.%20Finally%2C%20we%20show%20that%20our%20approach%20can%20generalize%20to%0Anovel%20tabletop%20domains%20and%20be%20re-purposed%20for%203D%20instance%20segmentation%20without%0Afine-tuning%2C%20and%20demonstrate%20its%20utility%20for%20language-guided%20robotic%20grasping%0Ain%20clutter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18742v5&entry.124074799=Read"},
{"title": "Learning from Few Samples: A Novel Approach for High-Quality Malcode\n  Generation", "author": "Haijian Ma and Daizong Liu and Xiaowen Cai and Pan Zhou and Yulai Xie", "abstract": "  Intrusion Detection Systems (IDS) play a crucial role in network security\ndefense. However, a significant challenge for IDS in training detection models\nis the shortage of adequately labeled malicious samples. To address these\nissues, this paper introduces a novel semi-supervised framework\n\\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)\nwith Large Language Models (LLMs) to enhance malicious code generation and SQL\nInjection (SQLi) detection capabilities in few-sample learning scenarios.\nSpecifically, our framework adopts a collaborative training paradigm where: (1)\nthe GAN-based discriminator improves malicious pattern recognition through\nadversarial learning with generated samples and limited real samples; and (2)\nthe LLM-based generator refines the quality of malicious code synthesis using\nreward signals from the discriminator. The experimental results demonstrate\nthat even with a limited number of labeled samples, our training framework is\nhighly effective in enhancing both malicious code generation and detection\ncapabilities. This dual enhancement capability offers a promising solution for\ndeveloping adaptive defense systems capable of countering evolving cyber\nthreats.\n", "link": "http://arxiv.org/abs/2508.18148v1", "date": "2025-08-25", "relevancy": 2.4678, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5014}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5012}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Few%20Samples%3A%20A%20Novel%20Approach%20for%20High-Quality%20Malcode%0A%20%20Generation&body=Title%3A%20Learning%20from%20Few%20Samples%3A%20A%20Novel%20Approach%20for%20High-Quality%20Malcode%0A%20%20Generation%0AAuthor%3A%20Haijian%20Ma%20and%20Daizong%20Liu%20and%20Xiaowen%20Cai%20and%20Pan%20Zhou%20and%20Yulai%20Xie%0AAbstract%3A%20%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20play%20a%20crucial%20role%20in%20network%20security%0Adefense.%20However%2C%20a%20significant%20challenge%20for%20IDS%20in%20training%20detection%20models%0Ais%20the%20shortage%20of%20adequately%20labeled%20malicious%20samples.%20To%20address%20these%0Aissues%2C%20this%20paper%20introduces%20a%20novel%20semi-supervised%20framework%0A%5Ctextbf%7BGANGRL-LLM%7D%2C%20which%20integrates%20Generative%20Adversarial%20Networks%20%28GANs%29%0Awith%20Large%20Language%20Models%20%28LLMs%29%20to%20enhance%20malicious%20code%20generation%20and%20SQL%0AInjection%20%28SQLi%29%20detection%20capabilities%20in%20few-sample%20learning%20scenarios.%0ASpecifically%2C%20our%20framework%20adopts%20a%20collaborative%20training%20paradigm%20where%3A%20%281%29%0Athe%20GAN-based%20discriminator%20improves%20malicious%20pattern%20recognition%20through%0Aadversarial%20learning%20with%20generated%20samples%20and%20limited%20real%20samples%3B%20and%20%282%29%0Athe%20LLM-based%20generator%20refines%20the%20quality%20of%20malicious%20code%20synthesis%20using%0Areward%20signals%20from%20the%20discriminator.%20The%20experimental%20results%20demonstrate%0Athat%20even%20with%20a%20limited%20number%20of%20labeled%20samples%2C%20our%20training%20framework%20is%0Ahighly%20effective%20in%20enhancing%20both%20malicious%20code%20generation%20and%20detection%0Acapabilities.%20This%20dual%20enhancement%20capability%20offers%20a%20promising%20solution%20for%0Adeveloping%20adaptive%20defense%20systems%20capable%20of%20countering%20evolving%20cyber%0Athreats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Few%2520Samples%253A%2520A%2520Novel%2520Approach%2520for%2520High-Quality%2520Malcode%250A%2520%2520Generation%26entry.906535625%3DHaijian%2520Ma%2520and%2520Daizong%2520Liu%2520and%2520Xiaowen%2520Cai%2520and%2520Pan%2520Zhou%2520and%2520Yulai%2520Xie%26entry.1292438233%3D%2520%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520play%2520a%2520crucial%2520role%2520in%2520network%2520security%250Adefense.%2520However%252C%2520a%2520significant%2520challenge%2520for%2520IDS%2520in%2520training%2520detection%2520models%250Ais%2520the%2520shortage%2520of%2520adequately%2520labeled%2520malicious%2520samples.%2520To%2520address%2520these%250Aissues%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520semi-supervised%2520framework%250A%255Ctextbf%257BGANGRL-LLM%257D%252C%2520which%2520integrates%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%250Awith%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520enhance%2520malicious%2520code%2520generation%2520and%2520SQL%250AInjection%2520%2528SQLi%2529%2520detection%2520capabilities%2520in%2520few-sample%2520learning%2520scenarios.%250ASpecifically%252C%2520our%2520framework%2520adopts%2520a%2520collaborative%2520training%2520paradigm%2520where%253A%2520%25281%2529%250Athe%2520GAN-based%2520discriminator%2520improves%2520malicious%2520pattern%2520recognition%2520through%250Aadversarial%2520learning%2520with%2520generated%2520samples%2520and%2520limited%2520real%2520samples%253B%2520and%2520%25282%2529%250Athe%2520LLM-based%2520generator%2520refines%2520the%2520quality%2520of%2520malicious%2520code%2520synthesis%2520using%250Areward%2520signals%2520from%2520the%2520discriminator.%2520The%2520experimental%2520results%2520demonstrate%250Athat%2520even%2520with%2520a%2520limited%2520number%2520of%2520labeled%2520samples%252C%2520our%2520training%2520framework%2520is%250Ahighly%2520effective%2520in%2520enhancing%2520both%2520malicious%2520code%2520generation%2520and%2520detection%250Acapabilities.%2520This%2520dual%2520enhancement%2520capability%2520offers%2520a%2520promising%2520solution%2520for%250Adeveloping%2520adaptive%2520defense%2520systems%2520capable%2520of%2520countering%2520evolving%2520cyber%250Athreats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Few%20Samples%3A%20A%20Novel%20Approach%20for%20High-Quality%20Malcode%0A%20%20Generation&entry.906535625=Haijian%20Ma%20and%20Daizong%20Liu%20and%20Xiaowen%20Cai%20and%20Pan%20Zhou%20and%20Yulai%20Xie&entry.1292438233=%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20play%20a%20crucial%20role%20in%20network%20security%0Adefense.%20However%2C%20a%20significant%20challenge%20for%20IDS%20in%20training%20detection%20models%0Ais%20the%20shortage%20of%20adequately%20labeled%20malicious%20samples.%20To%20address%20these%0Aissues%2C%20this%20paper%20introduces%20a%20novel%20semi-supervised%20framework%0A%5Ctextbf%7BGANGRL-LLM%7D%2C%20which%20integrates%20Generative%20Adversarial%20Networks%20%28GANs%29%0Awith%20Large%20Language%20Models%20%28LLMs%29%20to%20enhance%20malicious%20code%20generation%20and%20SQL%0AInjection%20%28SQLi%29%20detection%20capabilities%20in%20few-sample%20learning%20scenarios.%0ASpecifically%2C%20our%20framework%20adopts%20a%20collaborative%20training%20paradigm%20where%3A%20%281%29%0Athe%20GAN-based%20discriminator%20improves%20malicious%20pattern%20recognition%20through%0Aadversarial%20learning%20with%20generated%20samples%20and%20limited%20real%20samples%3B%20and%20%282%29%0Athe%20LLM-based%20generator%20refines%20the%20quality%20of%20malicious%20code%20synthesis%20using%0Areward%20signals%20from%20the%20discriminator.%20The%20experimental%20results%20demonstrate%0Athat%20even%20with%20a%20limited%20number%20of%20labeled%20samples%2C%20our%20training%20framework%20is%0Ahighly%20effective%20in%20enhancing%20both%20malicious%20code%20generation%20and%20detection%0Acapabilities.%20This%20dual%20enhancement%20capability%20offers%20a%20promising%20solution%20for%0Adeveloping%20adaptive%20defense%20systems%20capable%20of%20countering%20evolving%20cyber%0Athreats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18148v1&entry.124074799=Read"},
{"title": "Disentangling the Factors of Convergence between Brains and Computer\n  Vision Models", "author": "Jos\u00e9phine Raugel and Marc Szafraniec and Huy V. Vo and Camille Couprie and Patrick Labatut and Piotr Bojanowski and Valentin Wyart and Jean-R\u00e9mi King", "abstract": "  Many AI models trained on natural images develop representations that\nresemble those of the human brain. However, the factors that drive this\nbrain-model similarity remain poorly understood. To disentangle how the model,\ntraining and data independently lead a neural network to develop brain-like\nrepresentations, we trained a family of self-supervised vision transformers\n(DINOv3) that systematically varied these different factors. We compare their\nrepresentations of images to those of the human brain recorded with both fMRI\nand MEG, providing high resolution in spatial and temporal analyses. We assess\nthe brain-model similarity with three complementary metrics focusing on overall\nrepresentational similarity, topographical organization, and temporal dynamics.\nWe show that all three factors - model size, training amount, and image type -\nindependently and interactively impact each of these brain similarity metrics.\nIn particular, the largest DINOv3 models trained with the most human-centric\nimages reach the highest brain-similarity. This emergence of brain-like\nrepresentations in AI models follows a specific chronology during training:\nmodels first align with the early representations of the sensory cortices, and\nonly align with the late and prefrontal representations of the brain with\nconsiderably more training. Finally, this developmental trajectory is indexed\nby both structural and functional properties of the human cortex: the\nrepresentations that are acquired last by the models specifically align with\nthe cortical areas with the largest developmental expansion, thickness, least\nmyelination, and slowest timescales. Overall, these findings disentangle the\ninterplay between architecture and experience in shaping how artificial neural\nnetworks come to see the world as humans do, thus offering a promising\nframework to understand how the human brain comes to represent its visual\nworld.\n", "link": "http://arxiv.org/abs/2508.18226v1", "date": "2025-08-25", "relevancy": 2.4563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6284}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6284}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20the%20Factors%20of%20Convergence%20between%20Brains%20and%20Computer%0A%20%20Vision%20Models&body=Title%3A%20Disentangling%20the%20Factors%20of%20Convergence%20between%20Brains%20and%20Computer%0A%20%20Vision%20Models%0AAuthor%3A%20Jos%C3%A9phine%20Raugel%20and%20Marc%20Szafraniec%20and%20Huy%20V.%20Vo%20and%20Camille%20Couprie%20and%20Patrick%20Labatut%20and%20Piotr%20Bojanowski%20and%20Valentin%20Wyart%20and%20Jean-R%C3%A9mi%20King%0AAbstract%3A%20%20%20Many%20AI%20models%20trained%20on%20natural%20images%20develop%20representations%20that%0Aresemble%20those%20of%20the%20human%20brain.%20However%2C%20the%20factors%20that%20drive%20this%0Abrain-model%20similarity%20remain%20poorly%20understood.%20To%20disentangle%20how%20the%20model%2C%0Atraining%20and%20data%20independently%20lead%20a%20neural%20network%20to%20develop%20brain-like%0Arepresentations%2C%20we%20trained%20a%20family%20of%20self-supervised%20vision%20transformers%0A%28DINOv3%29%20that%20systematically%20varied%20these%20different%20factors.%20We%20compare%20their%0Arepresentations%20of%20images%20to%20those%20of%20the%20human%20brain%20recorded%20with%20both%20fMRI%0Aand%20MEG%2C%20providing%20high%20resolution%20in%20spatial%20and%20temporal%20analyses.%20We%20assess%0Athe%20brain-model%20similarity%20with%20three%20complementary%20metrics%20focusing%20on%20overall%0Arepresentational%20similarity%2C%20topographical%20organization%2C%20and%20temporal%20dynamics.%0AWe%20show%20that%20all%20three%20factors%20-%20model%20size%2C%20training%20amount%2C%20and%20image%20type%20-%0Aindependently%20and%20interactively%20impact%20each%20of%20these%20brain%20similarity%20metrics.%0AIn%20particular%2C%20the%20largest%20DINOv3%20models%20trained%20with%20the%20most%20human-centric%0Aimages%20reach%20the%20highest%20brain-similarity.%20This%20emergence%20of%20brain-like%0Arepresentations%20in%20AI%20models%20follows%20a%20specific%20chronology%20during%20training%3A%0Amodels%20first%20align%20with%20the%20early%20representations%20of%20the%20sensory%20cortices%2C%20and%0Aonly%20align%20with%20the%20late%20and%20prefrontal%20representations%20of%20the%20brain%20with%0Aconsiderably%20more%20training.%20Finally%2C%20this%20developmental%20trajectory%20is%20indexed%0Aby%20both%20structural%20and%20functional%20properties%20of%20the%20human%20cortex%3A%20the%0Arepresentations%20that%20are%20acquired%20last%20by%20the%20models%20specifically%20align%20with%0Athe%20cortical%20areas%20with%20the%20largest%20developmental%20expansion%2C%20thickness%2C%20least%0Amyelination%2C%20and%20slowest%20timescales.%20Overall%2C%20these%20findings%20disentangle%20the%0Ainterplay%20between%20architecture%20and%20experience%20in%20shaping%20how%20artificial%20neural%0Anetworks%20come%20to%20see%20the%20world%20as%20humans%20do%2C%20thus%20offering%20a%20promising%0Aframework%20to%20understand%20how%20the%20human%20brain%20comes%20to%20represent%20its%20visual%0Aworld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520the%2520Factors%2520of%2520Convergence%2520between%2520Brains%2520and%2520Computer%250A%2520%2520Vision%2520Models%26entry.906535625%3DJos%25C3%25A9phine%2520Raugel%2520and%2520Marc%2520Szafraniec%2520and%2520Huy%2520V.%2520Vo%2520and%2520Camille%2520Couprie%2520and%2520Patrick%2520Labatut%2520and%2520Piotr%2520Bojanowski%2520and%2520Valentin%2520Wyart%2520and%2520Jean-R%25C3%25A9mi%2520King%26entry.1292438233%3D%2520%2520Many%2520AI%2520models%2520trained%2520on%2520natural%2520images%2520develop%2520representations%2520that%250Aresemble%2520those%2520of%2520the%2520human%2520brain.%2520However%252C%2520the%2520factors%2520that%2520drive%2520this%250Abrain-model%2520similarity%2520remain%2520poorly%2520understood.%2520To%2520disentangle%2520how%2520the%2520model%252C%250Atraining%2520and%2520data%2520independently%2520lead%2520a%2520neural%2520network%2520to%2520develop%2520brain-like%250Arepresentations%252C%2520we%2520trained%2520a%2520family%2520of%2520self-supervised%2520vision%2520transformers%250A%2528DINOv3%2529%2520that%2520systematically%2520varied%2520these%2520different%2520factors.%2520We%2520compare%2520their%250Arepresentations%2520of%2520images%2520to%2520those%2520of%2520the%2520human%2520brain%2520recorded%2520with%2520both%2520fMRI%250Aand%2520MEG%252C%2520providing%2520high%2520resolution%2520in%2520spatial%2520and%2520temporal%2520analyses.%2520We%2520assess%250Athe%2520brain-model%2520similarity%2520with%2520three%2520complementary%2520metrics%2520focusing%2520on%2520overall%250Arepresentational%2520similarity%252C%2520topographical%2520organization%252C%2520and%2520temporal%2520dynamics.%250AWe%2520show%2520that%2520all%2520three%2520factors%2520-%2520model%2520size%252C%2520training%2520amount%252C%2520and%2520image%2520type%2520-%250Aindependently%2520and%2520interactively%2520impact%2520each%2520of%2520these%2520brain%2520similarity%2520metrics.%250AIn%2520particular%252C%2520the%2520largest%2520DINOv3%2520models%2520trained%2520with%2520the%2520most%2520human-centric%250Aimages%2520reach%2520the%2520highest%2520brain-similarity.%2520This%2520emergence%2520of%2520brain-like%250Arepresentations%2520in%2520AI%2520models%2520follows%2520a%2520specific%2520chronology%2520during%2520training%253A%250Amodels%2520first%2520align%2520with%2520the%2520early%2520representations%2520of%2520the%2520sensory%2520cortices%252C%2520and%250Aonly%2520align%2520with%2520the%2520late%2520and%2520prefrontal%2520representations%2520of%2520the%2520brain%2520with%250Aconsiderably%2520more%2520training.%2520Finally%252C%2520this%2520developmental%2520trajectory%2520is%2520indexed%250Aby%2520both%2520structural%2520and%2520functional%2520properties%2520of%2520the%2520human%2520cortex%253A%2520the%250Arepresentations%2520that%2520are%2520acquired%2520last%2520by%2520the%2520models%2520specifically%2520align%2520with%250Athe%2520cortical%2520areas%2520with%2520the%2520largest%2520developmental%2520expansion%252C%2520thickness%252C%2520least%250Amyelination%252C%2520and%2520slowest%2520timescales.%2520Overall%252C%2520these%2520findings%2520disentangle%2520the%250Ainterplay%2520between%2520architecture%2520and%2520experience%2520in%2520shaping%2520how%2520artificial%2520neural%250Anetworks%2520come%2520to%2520see%2520the%2520world%2520as%2520humans%2520do%252C%2520thus%2520offering%2520a%2520promising%250Aframework%2520to%2520understand%2520how%2520the%2520human%2520brain%2520comes%2520to%2520represent%2520its%2520visual%250Aworld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20the%20Factors%20of%20Convergence%20between%20Brains%20and%20Computer%0A%20%20Vision%20Models&entry.906535625=Jos%C3%A9phine%20Raugel%20and%20Marc%20Szafraniec%20and%20Huy%20V.%20Vo%20and%20Camille%20Couprie%20and%20Patrick%20Labatut%20and%20Piotr%20Bojanowski%20and%20Valentin%20Wyart%20and%20Jean-R%C3%A9mi%20King&entry.1292438233=%20%20Many%20AI%20models%20trained%20on%20natural%20images%20develop%20representations%20that%0Aresemble%20those%20of%20the%20human%20brain.%20However%2C%20the%20factors%20that%20drive%20this%0Abrain-model%20similarity%20remain%20poorly%20understood.%20To%20disentangle%20how%20the%20model%2C%0Atraining%20and%20data%20independently%20lead%20a%20neural%20network%20to%20develop%20brain-like%0Arepresentations%2C%20we%20trained%20a%20family%20of%20self-supervised%20vision%20transformers%0A%28DINOv3%29%20that%20systematically%20varied%20these%20different%20factors.%20We%20compare%20their%0Arepresentations%20of%20images%20to%20those%20of%20the%20human%20brain%20recorded%20with%20both%20fMRI%0Aand%20MEG%2C%20providing%20high%20resolution%20in%20spatial%20and%20temporal%20analyses.%20We%20assess%0Athe%20brain-model%20similarity%20with%20three%20complementary%20metrics%20focusing%20on%20overall%0Arepresentational%20similarity%2C%20topographical%20organization%2C%20and%20temporal%20dynamics.%0AWe%20show%20that%20all%20three%20factors%20-%20model%20size%2C%20training%20amount%2C%20and%20image%20type%20-%0Aindependently%20and%20interactively%20impact%20each%20of%20these%20brain%20similarity%20metrics.%0AIn%20particular%2C%20the%20largest%20DINOv3%20models%20trained%20with%20the%20most%20human-centric%0Aimages%20reach%20the%20highest%20brain-similarity.%20This%20emergence%20of%20brain-like%0Arepresentations%20in%20AI%20models%20follows%20a%20specific%20chronology%20during%20training%3A%0Amodels%20first%20align%20with%20the%20early%20representations%20of%20the%20sensory%20cortices%2C%20and%0Aonly%20align%20with%20the%20late%20and%20prefrontal%20representations%20of%20the%20brain%20with%0Aconsiderably%20more%20training.%20Finally%2C%20this%20developmental%20trajectory%20is%20indexed%0Aby%20both%20structural%20and%20functional%20properties%20of%20the%20human%20cortex%3A%20the%0Arepresentations%20that%20are%20acquired%20last%20by%20the%20models%20specifically%20align%20with%0Athe%20cortical%20areas%20with%20the%20largest%20developmental%20expansion%2C%20thickness%2C%20least%0Amyelination%2C%20and%20slowest%20timescales.%20Overall%2C%20these%20findings%20disentangle%20the%0Ainterplay%20between%20architecture%20and%20experience%20in%20shaping%20how%20artificial%20neural%0Anetworks%20come%20to%20see%20the%20world%20as%20humans%20do%2C%20thus%20offering%20a%20promising%0Aframework%20to%20understand%20how%20the%20human%20brain%20comes%20to%20represent%20its%20visual%0Aworld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18226v1&entry.124074799=Read"},
{"title": "Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype\n  Learning and Clustering", "author": "Chun Liu and Chen Zhang and Zhuo Li and Zheng Li and Wei Yang", "abstract": "  Open-set few-shot hyperspectral image (HSI) classification aims to classify\nimage pixels by using few labeled pixels per class, where the pixels to be\nclassified may be not all from the classes that have been seen. To address the\nopen-set HSI classification challenge, current methods focus mainly on\ndistinguishing the unknown class samples from the known class samples and\nrejecting them to increase the accuracy of identifying known class samples.\nThey fails to further identify or discovery the unknow classes among the\nsamples. This paper proposes a prototype learning and clustering method for\ndiscoverying unknown classes in HSIs under the few-shot environment. Using few\nlabeled samples, it strives to develop the ability of infering the prototypes\nof unknown classes while distinguishing unknown classes from known classes.\nOnce the unknown class samples are rejected by the learned known class\nclassifier, the proposed method can further cluster the unknown class samples\ninto different classes according to their distance to the inferred unknown\nclass prototypes. Compared to existing state-of-the-art methods, extensive\nexperiments on four benchmark HSI datasets demonstrate that our proposed method\nexhibits competitive performance in open-set few-shot HSI classification tasks.\nAll the codes are available at \\href{https://github.com/KOBEN-ff/OpenFUCD-main}\n{https://github.com/KOBEN-ff/OpenFUCD-main}\n", "link": "http://arxiv.org/abs/2508.18075v1", "date": "2025-08-25", "relevancy": 2.4189, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4897}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4828}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Unknown%20Class%20Discovery%20of%20Hyperspectral%20Images%20with%20Prototype%0A%20%20Learning%20and%20Clustering&body=Title%3A%20Few-shot%20Unknown%20Class%20Discovery%20of%20Hyperspectral%20Images%20with%20Prototype%0A%20%20Learning%20and%20Clustering%0AAuthor%3A%20Chun%20Liu%20and%20Chen%20Zhang%20and%20Zhuo%20Li%20and%20Zheng%20Li%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Open-set%20few-shot%20hyperspectral%20image%20%28HSI%29%20classification%20aims%20to%20classify%0Aimage%20pixels%20by%20using%20few%20labeled%20pixels%20per%20class%2C%20where%20the%20pixels%20to%20be%0Aclassified%20may%20be%20not%20all%20from%20the%20classes%20that%20have%20been%20seen.%20To%20address%20the%0Aopen-set%20HSI%20classification%20challenge%2C%20current%20methods%20focus%20mainly%20on%0Adistinguishing%20the%20unknown%20class%20samples%20from%20the%20known%20class%20samples%20and%0Arejecting%20them%20to%20increase%20the%20accuracy%20of%20identifying%20known%20class%20samples.%0AThey%20fails%20to%20further%20identify%20or%20discovery%20the%20unknow%20classes%20among%20the%0Asamples.%20This%20paper%20proposes%20a%20prototype%20learning%20and%20clustering%20method%20for%0Adiscoverying%20unknown%20classes%20in%20HSIs%20under%20the%20few-shot%20environment.%20Using%20few%0Alabeled%20samples%2C%20it%20strives%20to%20develop%20the%20ability%20of%20infering%20the%20prototypes%0Aof%20unknown%20classes%20while%20distinguishing%20unknown%20classes%20from%20known%20classes.%0AOnce%20the%20unknown%20class%20samples%20are%20rejected%20by%20the%20learned%20known%20class%0Aclassifier%2C%20the%20proposed%20method%20can%20further%20cluster%20the%20unknown%20class%20samples%0Ainto%20different%20classes%20according%20to%20their%20distance%20to%20the%20inferred%20unknown%0Aclass%20prototypes.%20Compared%20to%20existing%20state-of-the-art%20methods%2C%20extensive%0Aexperiments%20on%20four%20benchmark%20HSI%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aexhibits%20competitive%20performance%20in%20open-set%20few-shot%20HSI%20classification%20tasks.%0AAll%20the%20codes%20are%20available%20at%20%5Chref%7Bhttps%3A//github.com/KOBEN-ff/OpenFUCD-main%7D%0A%7Bhttps%3A//github.com/KOBEN-ff/OpenFUCD-main%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Unknown%2520Class%2520Discovery%2520of%2520Hyperspectral%2520Images%2520with%2520Prototype%250A%2520%2520Learning%2520and%2520Clustering%26entry.906535625%3DChun%2520Liu%2520and%2520Chen%2520Zhang%2520and%2520Zhuo%2520Li%2520and%2520Zheng%2520Li%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Open-set%2520few-shot%2520hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520aims%2520to%2520classify%250Aimage%2520pixels%2520by%2520using%2520few%2520labeled%2520pixels%2520per%2520class%252C%2520where%2520the%2520pixels%2520to%2520be%250Aclassified%2520may%2520be%2520not%2520all%2520from%2520the%2520classes%2520that%2520have%2520been%2520seen.%2520To%2520address%2520the%250Aopen-set%2520HSI%2520classification%2520challenge%252C%2520current%2520methods%2520focus%2520mainly%2520on%250Adistinguishing%2520the%2520unknown%2520class%2520samples%2520from%2520the%2520known%2520class%2520samples%2520and%250Arejecting%2520them%2520to%2520increase%2520the%2520accuracy%2520of%2520identifying%2520known%2520class%2520samples.%250AThey%2520fails%2520to%2520further%2520identify%2520or%2520discovery%2520the%2520unknow%2520classes%2520among%2520the%250Asamples.%2520This%2520paper%2520proposes%2520a%2520prototype%2520learning%2520and%2520clustering%2520method%2520for%250Adiscoverying%2520unknown%2520classes%2520in%2520HSIs%2520under%2520the%2520few-shot%2520environment.%2520Using%2520few%250Alabeled%2520samples%252C%2520it%2520strives%2520to%2520develop%2520the%2520ability%2520of%2520infering%2520the%2520prototypes%250Aof%2520unknown%2520classes%2520while%2520distinguishing%2520unknown%2520classes%2520from%2520known%2520classes.%250AOnce%2520the%2520unknown%2520class%2520samples%2520are%2520rejected%2520by%2520the%2520learned%2520known%2520class%250Aclassifier%252C%2520the%2520proposed%2520method%2520can%2520further%2520cluster%2520the%2520unknown%2520class%2520samples%250Ainto%2520different%2520classes%2520according%2520to%2520their%2520distance%2520to%2520the%2520inferred%2520unknown%250Aclass%2520prototypes.%2520Compared%2520to%2520existing%2520state-of-the-art%2520methods%252C%2520extensive%250Aexperiments%2520on%2520four%2520benchmark%2520HSI%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520method%250Aexhibits%2520competitive%2520performance%2520in%2520open-set%2520few-shot%2520HSI%2520classification%2520tasks.%250AAll%2520the%2520codes%2520are%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/KOBEN-ff/OpenFUCD-main%257D%250A%257Bhttps%253A//github.com/KOBEN-ff/OpenFUCD-main%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Unknown%20Class%20Discovery%20of%20Hyperspectral%20Images%20with%20Prototype%0A%20%20Learning%20and%20Clustering&entry.906535625=Chun%20Liu%20and%20Chen%20Zhang%20and%20Zhuo%20Li%20and%20Zheng%20Li%20and%20Wei%20Yang&entry.1292438233=%20%20Open-set%20few-shot%20hyperspectral%20image%20%28HSI%29%20classification%20aims%20to%20classify%0Aimage%20pixels%20by%20using%20few%20labeled%20pixels%20per%20class%2C%20where%20the%20pixels%20to%20be%0Aclassified%20may%20be%20not%20all%20from%20the%20classes%20that%20have%20been%20seen.%20To%20address%20the%0Aopen-set%20HSI%20classification%20challenge%2C%20current%20methods%20focus%20mainly%20on%0Adistinguishing%20the%20unknown%20class%20samples%20from%20the%20known%20class%20samples%20and%0Arejecting%20them%20to%20increase%20the%20accuracy%20of%20identifying%20known%20class%20samples.%0AThey%20fails%20to%20further%20identify%20or%20discovery%20the%20unknow%20classes%20among%20the%0Asamples.%20This%20paper%20proposes%20a%20prototype%20learning%20and%20clustering%20method%20for%0Adiscoverying%20unknown%20classes%20in%20HSIs%20under%20the%20few-shot%20environment.%20Using%20few%0Alabeled%20samples%2C%20it%20strives%20to%20develop%20the%20ability%20of%20infering%20the%20prototypes%0Aof%20unknown%20classes%20while%20distinguishing%20unknown%20classes%20from%20known%20classes.%0AOnce%20the%20unknown%20class%20samples%20are%20rejected%20by%20the%20learned%20known%20class%0Aclassifier%2C%20the%20proposed%20method%20can%20further%20cluster%20the%20unknown%20class%20samples%0Ainto%20different%20classes%20according%20to%20their%20distance%20to%20the%20inferred%20unknown%0Aclass%20prototypes.%20Compared%20to%20existing%20state-of-the-art%20methods%2C%20extensive%0Aexperiments%20on%20four%20benchmark%20HSI%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aexhibits%20competitive%20performance%20in%20open-set%20few-shot%20HSI%20classification%20tasks.%0AAll%20the%20codes%20are%20available%20at%20%5Chref%7Bhttps%3A//github.com/KOBEN-ff/OpenFUCD-main%7D%0A%7Bhttps%3A//github.com/KOBEN-ff/OpenFUCD-main%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18075v1&entry.124074799=Read"},
{"title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph\n  Representation Learning", "author": "Yudan Song and Yuecen Wei and Yuhang Lu and Qingyun Sun and Minglai Shao and Li-e Wang and Chunming Hu and Xianxian Li and Xingcheng Fu", "abstract": "  Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection (MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.\n", "link": "http://arxiv.org/abs/2507.06469v2", "date": "2025-08-25", "relevancy": 2.4175, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5011}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.479}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning&body=Title%3A%20Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning%0AAuthor%3A%20Yudan%20Song%20and%20Yuecen%20Wei%20and%20Yuhang%20Lu%20and%20Qingyun%20Sun%20and%20Minglai%20Shao%20and%20Li-e%20Wang%20and%20Chunming%20Hu%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu%0AAbstract%3A%20%20%20Graph%20representation%20learning%20has%20become%20a%20mainstream%20method%20for%20fraud%0Adetection%20due%20to%20its%20strong%20expressive%20power%2C%20which%20focuses%20on%20enhancing%20node%0Arepresentations%20through%20improved%20neighborhood%20knowledge%20capture.%20However%2C%20the%0Afocus%20on%20local%20interactions%20leads%20to%20imbalanced%20transmission%20of%20global%0Atopological%20information%20and%20increased%20risk%20of%20node-specific%20information%20being%0Aoverwhelmed%20during%20aggregation%20due%20to%20the%20imbalance%20between%20fraud%20and%20benign%0Anodes.%20In%20this%20paper%2C%20we%20first%20summarize%20the%20impact%20of%20topology%20and%20class%0Aimbalance%20on%20downstream%20tasks%20in%20GNN-based%20fraud%20detection%2C%20as%20the%20problem%20of%0Aimbalanced%20supervisory%20messages%20is%20caused%20by%20fraudsters%27%20topological%20behavior%0Aobfuscation%20and%20identity%20feature%20concealment.%20Based%20on%20statistical%20validation%2C%0Awe%20propose%20a%20novel%20dual-view%20graph%20representation%20learning%20method%20to%20mitigate%0AMessage%20imbalance%20in%20Fraud%20Detection%20%28MimbFD%29.%20Specifically%2C%20we%20design%20a%0Atopological%20message%20reachability%20module%20for%20high-quality%20node%20representation%0Alearning%20to%20penetrate%20fraudsters%27%20camouflage%20and%20alleviate%20insufficient%0Apropagation.%20Then%2C%20we%20introduce%20a%20local%20confounding%20debiasing%20module%20to%20adjust%0Anode%20representations%2C%20enhancing%20the%20stable%20association%20between%20node%0Arepresentations%20and%20labels%20to%20balance%20the%20influence%20of%20different%20classes.%0AFinally%2C%20we%20conducted%20experiments%20on%20three%20public%20fraud%20datasets%2C%20and%20the%0Aresults%20demonstrate%20that%20MimbFD%20exhibits%20outstanding%20performance%20in%20fraud%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Message%2520Imbalance%2520in%2520Fraud%2520Detection%2520with%2520Dual-View%2520Graph%250A%2520%2520Representation%2520Learning%26entry.906535625%3DYudan%2520Song%2520and%2520Yuecen%2520Wei%2520and%2520Yuhang%2520Lu%2520and%2520Qingyun%2520Sun%2520and%2520Minglai%2520Shao%2520and%2520Li-e%2520Wang%2520and%2520Chunming%2520Hu%2520and%2520Xianxian%2520Li%2520and%2520Xingcheng%2520Fu%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520has%2520become%2520a%2520mainstream%2520method%2520for%2520fraud%250Adetection%2520due%2520to%2520its%2520strong%2520expressive%2520power%252C%2520which%2520focuses%2520on%2520enhancing%2520node%250Arepresentations%2520through%2520improved%2520neighborhood%2520knowledge%2520capture.%2520However%252C%2520the%250Afocus%2520on%2520local%2520interactions%2520leads%2520to%2520imbalanced%2520transmission%2520of%2520global%250Atopological%2520information%2520and%2520increased%2520risk%2520of%2520node-specific%2520information%2520being%250Aoverwhelmed%2520during%2520aggregation%2520due%2520to%2520the%2520imbalance%2520between%2520fraud%2520and%2520benign%250Anodes.%2520In%2520this%2520paper%252C%2520we%2520first%2520summarize%2520the%2520impact%2520of%2520topology%2520and%2520class%250Aimbalance%2520on%2520downstream%2520tasks%2520in%2520GNN-based%2520fraud%2520detection%252C%2520as%2520the%2520problem%2520of%250Aimbalanced%2520supervisory%2520messages%2520is%2520caused%2520by%2520fraudsters%2527%2520topological%2520behavior%250Aobfuscation%2520and%2520identity%2520feature%2520concealment.%2520Based%2520on%2520statistical%2520validation%252C%250Awe%2520propose%2520a%2520novel%2520dual-view%2520graph%2520representation%2520learning%2520method%2520to%2520mitigate%250AMessage%2520imbalance%2520in%2520Fraud%2520Detection%2520%2528MimbFD%2529.%2520Specifically%252C%2520we%2520design%2520a%250Atopological%2520message%2520reachability%2520module%2520for%2520high-quality%2520node%2520representation%250Alearning%2520to%2520penetrate%2520fraudsters%2527%2520camouflage%2520and%2520alleviate%2520insufficient%250Apropagation.%2520Then%252C%2520we%2520introduce%2520a%2520local%2520confounding%2520debiasing%2520module%2520to%2520adjust%250Anode%2520representations%252C%2520enhancing%2520the%2520stable%2520association%2520between%2520node%250Arepresentations%2520and%2520labels%2520to%2520balance%2520the%2520influence%2520of%2520different%2520classes.%250AFinally%252C%2520we%2520conducted%2520experiments%2520on%2520three%2520public%2520fraud%2520datasets%252C%2520and%2520the%250Aresults%2520demonstrate%2520that%2520MimbFD%2520exhibits%2520outstanding%2520performance%2520in%2520fraud%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Message%20Imbalance%20in%20Fraud%20Detection%20with%20Dual-View%20Graph%0A%20%20Representation%20Learning&entry.906535625=Yudan%20Song%20and%20Yuecen%20Wei%20and%20Yuhang%20Lu%20and%20Qingyun%20Sun%20and%20Minglai%20Shao%20and%20Li-e%20Wang%20and%20Chunming%20Hu%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu&entry.1292438233=%20%20Graph%20representation%20learning%20has%20become%20a%20mainstream%20method%20for%20fraud%0Adetection%20due%20to%20its%20strong%20expressive%20power%2C%20which%20focuses%20on%20enhancing%20node%0Arepresentations%20through%20improved%20neighborhood%20knowledge%20capture.%20However%2C%20the%0Afocus%20on%20local%20interactions%20leads%20to%20imbalanced%20transmission%20of%20global%0Atopological%20information%20and%20increased%20risk%20of%20node-specific%20information%20being%0Aoverwhelmed%20during%20aggregation%20due%20to%20the%20imbalance%20between%20fraud%20and%20benign%0Anodes.%20In%20this%20paper%2C%20we%20first%20summarize%20the%20impact%20of%20topology%20and%20class%0Aimbalance%20on%20downstream%20tasks%20in%20GNN-based%20fraud%20detection%2C%20as%20the%20problem%20of%0Aimbalanced%20supervisory%20messages%20is%20caused%20by%20fraudsters%27%20topological%20behavior%0Aobfuscation%20and%20identity%20feature%20concealment.%20Based%20on%20statistical%20validation%2C%0Awe%20propose%20a%20novel%20dual-view%20graph%20representation%20learning%20method%20to%20mitigate%0AMessage%20imbalance%20in%20Fraud%20Detection%20%28MimbFD%29.%20Specifically%2C%20we%20design%20a%0Atopological%20message%20reachability%20module%20for%20high-quality%20node%20representation%0Alearning%20to%20penetrate%20fraudsters%27%20camouflage%20and%20alleviate%20insufficient%0Apropagation.%20Then%2C%20we%20introduce%20a%20local%20confounding%20debiasing%20module%20to%20adjust%0Anode%20representations%2C%20enhancing%20the%20stable%20association%20between%20node%0Arepresentations%20and%20labels%20to%20balance%20the%20influence%20of%20different%20classes.%0AFinally%2C%20we%20conducted%20experiments%20on%20three%20public%20fraud%20datasets%2C%20and%20the%0Aresults%20demonstrate%20that%20MimbFD%20exhibits%20outstanding%20performance%20in%20fraud%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06469v2&entry.124074799=Read"},
{"title": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric\n  Guidance", "author": "Ayce Idil Aytekin and Helge Rhodin and Rishabh Dabral and Christian Theobalt", "abstract": "  We propose a novel diffusion-based framework for reconstructing 3D geometry\nof hand-held objects from monocular RGB images by leveraging hand-object\ninteraction as geometric guidance. Our method conditions a latent diffusion\nmodel on an inpainted object appearance and uses inference-time guidance to\noptimize the object reconstruction, while simultaneously ensuring plausible\nhand-object interactions. Unlike prior methods that rely on extensive\npost-processing or produce low-quality reconstructions, our approach directly\ngenerates high-quality object geometry during the diffusion process by\nintroducing guidance with an optimization-in-the-loop design. Specifically, we\nguide the diffusion model by applying supervision to the velocity field while\nsimultaneously optimizing the transformations of both the hand and the object\nbeing reconstructed. This optimization is driven by multi-modal geometric cues,\nincluding normal and depth alignment, silhouette consistency, and 2D keypoint\nreprojection. We further incorporate signed distance field supervision and\nenforce contact and non-intersection constraints to ensure physical\nplausibility of hand-object interaction. Our method yields accurate, robust and\ncoherent reconstructions under occlusion while generalizing well to in-the-wild\nscenarios.\n", "link": "http://arxiv.org/abs/2508.18213v1", "date": "2025-08-25", "relevancy": 2.4108, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6161}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5983}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Follow%20My%20Hold%3A%20Hand-Object%20Interaction%20Reconstruction%20through%20Geometric%0A%20%20Guidance&body=Title%3A%20Follow%20My%20Hold%3A%20Hand-Object%20Interaction%20Reconstruction%20through%20Geometric%0A%20%20Guidance%0AAuthor%3A%20Ayce%20Idil%20Aytekin%20and%20Helge%20Rhodin%20and%20Rishabh%20Dabral%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20diffusion-based%20framework%20for%20reconstructing%203D%20geometry%0Aof%20hand-held%20objects%20from%20monocular%20RGB%20images%20by%20leveraging%20hand-object%0Ainteraction%20as%20geometric%20guidance.%20Our%20method%20conditions%20a%20latent%20diffusion%0Amodel%20on%20an%20inpainted%20object%20appearance%20and%20uses%20inference-time%20guidance%20to%0Aoptimize%20the%20object%20reconstruction%2C%20while%20simultaneously%20ensuring%20plausible%0Ahand-object%20interactions.%20Unlike%20prior%20methods%20that%20rely%20on%20extensive%0Apost-processing%20or%20produce%20low-quality%20reconstructions%2C%20our%20approach%20directly%0Agenerates%20high-quality%20object%20geometry%20during%20the%20diffusion%20process%20by%0Aintroducing%20guidance%20with%20an%20optimization-in-the-loop%20design.%20Specifically%2C%20we%0Aguide%20the%20diffusion%20model%20by%20applying%20supervision%20to%20the%20velocity%20field%20while%0Asimultaneously%20optimizing%20the%20transformations%20of%20both%20the%20hand%20and%20the%20object%0Abeing%20reconstructed.%20This%20optimization%20is%20driven%20by%20multi-modal%20geometric%20cues%2C%0Aincluding%20normal%20and%20depth%20alignment%2C%20silhouette%20consistency%2C%20and%202D%20keypoint%0Areprojection.%20We%20further%20incorporate%20signed%20distance%20field%20supervision%20and%0Aenforce%20contact%20and%20non-intersection%20constraints%20to%20ensure%20physical%0Aplausibility%20of%20hand-object%20interaction.%20Our%20method%20yields%20accurate%2C%20robust%20and%0Acoherent%20reconstructions%20under%20occlusion%20while%20generalizing%20well%20to%20in-the-wild%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFollow%2520My%2520Hold%253A%2520Hand-Object%2520Interaction%2520Reconstruction%2520through%2520Geometric%250A%2520%2520Guidance%26entry.906535625%3DAyce%2520Idil%2520Aytekin%2520and%2520Helge%2520Rhodin%2520and%2520Rishabh%2520Dabral%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520diffusion-based%2520framework%2520for%2520reconstructing%25203D%2520geometry%250Aof%2520hand-held%2520objects%2520from%2520monocular%2520RGB%2520images%2520by%2520leveraging%2520hand-object%250Ainteraction%2520as%2520geometric%2520guidance.%2520Our%2520method%2520conditions%2520a%2520latent%2520diffusion%250Amodel%2520on%2520an%2520inpainted%2520object%2520appearance%2520and%2520uses%2520inference-time%2520guidance%2520to%250Aoptimize%2520the%2520object%2520reconstruction%252C%2520while%2520simultaneously%2520ensuring%2520plausible%250Ahand-object%2520interactions.%2520Unlike%2520prior%2520methods%2520that%2520rely%2520on%2520extensive%250Apost-processing%2520or%2520produce%2520low-quality%2520reconstructions%252C%2520our%2520approach%2520directly%250Agenerates%2520high-quality%2520object%2520geometry%2520during%2520the%2520diffusion%2520process%2520by%250Aintroducing%2520guidance%2520with%2520an%2520optimization-in-the-loop%2520design.%2520Specifically%252C%2520we%250Aguide%2520the%2520diffusion%2520model%2520by%2520applying%2520supervision%2520to%2520the%2520velocity%2520field%2520while%250Asimultaneously%2520optimizing%2520the%2520transformations%2520of%2520both%2520the%2520hand%2520and%2520the%2520object%250Abeing%2520reconstructed.%2520This%2520optimization%2520is%2520driven%2520by%2520multi-modal%2520geometric%2520cues%252C%250Aincluding%2520normal%2520and%2520depth%2520alignment%252C%2520silhouette%2520consistency%252C%2520and%25202D%2520keypoint%250Areprojection.%2520We%2520further%2520incorporate%2520signed%2520distance%2520field%2520supervision%2520and%250Aenforce%2520contact%2520and%2520non-intersection%2520constraints%2520to%2520ensure%2520physical%250Aplausibility%2520of%2520hand-object%2520interaction.%2520Our%2520method%2520yields%2520accurate%252C%2520robust%2520and%250Acoherent%2520reconstructions%2520under%2520occlusion%2520while%2520generalizing%2520well%2520to%2520in-the-wild%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow%20My%20Hold%3A%20Hand-Object%20Interaction%20Reconstruction%20through%20Geometric%0A%20%20Guidance&entry.906535625=Ayce%20Idil%20Aytekin%20and%20Helge%20Rhodin%20and%20Rishabh%20Dabral%20and%20Christian%20Theobalt&entry.1292438233=%20%20We%20propose%20a%20novel%20diffusion-based%20framework%20for%20reconstructing%203D%20geometry%0Aof%20hand-held%20objects%20from%20monocular%20RGB%20images%20by%20leveraging%20hand-object%0Ainteraction%20as%20geometric%20guidance.%20Our%20method%20conditions%20a%20latent%20diffusion%0Amodel%20on%20an%20inpainted%20object%20appearance%20and%20uses%20inference-time%20guidance%20to%0Aoptimize%20the%20object%20reconstruction%2C%20while%20simultaneously%20ensuring%20plausible%0Ahand-object%20interactions.%20Unlike%20prior%20methods%20that%20rely%20on%20extensive%0Apost-processing%20or%20produce%20low-quality%20reconstructions%2C%20our%20approach%20directly%0Agenerates%20high-quality%20object%20geometry%20during%20the%20diffusion%20process%20by%0Aintroducing%20guidance%20with%20an%20optimization-in-the-loop%20design.%20Specifically%2C%20we%0Aguide%20the%20diffusion%20model%20by%20applying%20supervision%20to%20the%20velocity%20field%20while%0Asimultaneously%20optimizing%20the%20transformations%20of%20both%20the%20hand%20and%20the%20object%0Abeing%20reconstructed.%20This%20optimization%20is%20driven%20by%20multi-modal%20geometric%20cues%2C%0Aincluding%20normal%20and%20depth%20alignment%2C%20silhouette%20consistency%2C%20and%202D%20keypoint%0Areprojection.%20We%20further%20incorporate%20signed%20distance%20field%20supervision%20and%0Aenforce%20contact%20and%20non-intersection%20constraints%20to%20ensure%20physical%0Aplausibility%20of%20hand-object%20interaction.%20Our%20method%20yields%20accurate%2C%20robust%20and%0Acoherent%20reconstructions%20under%20occlusion%20while%20generalizing%20well%20to%20in-the-wild%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18213v1&entry.124074799=Read"},
{"title": "Robust Federated Learning under Adversarial Attacks via Loss-Based\n  Client Clustering", "author": "Emmanouil Kritharakis and Dusan Jakovetic and Antonios Makris and Konstantinos Tserpes", "abstract": "  Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing private data. We consider FL scenarios wherein FL\nclients are subject to adversarial (Byzantine) attacks, while the FL server is\ntrusted (honest) and has a trustworthy side dataset. This may correspond to,\ne.g., cases where the server possesses trusted data prior to federation, or to\nthe presence of a trusted client that temporarily assumes the server role. Our\napproach requires only two honest participants, i.e., the server and one\nclient, to function effectively, without prior knowledge of the number of\nmalicious clients. Theoretical analysis demonstrates bounded optimality gaps\neven under strong Byzantine attacks. Experimental results show that our\nalgorithm significantly outperforms standard and robust FL baselines such as\nMean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack\nstrategies including label flipping, sign flipping, and Gaussian noise addition\nacross MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.\n", "link": "http://arxiv.org/abs/2508.12672v3", "date": "2025-08-25", "relevancy": 2.4042, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4891}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Federated%20Learning%20under%20Adversarial%20Attacks%20via%20Loss-Based%0A%20%20Client%20Clustering&body=Title%3A%20Robust%20Federated%20Learning%20under%20Adversarial%20Attacks%20via%20Loss-Based%0A%20%20Client%20Clustering%0AAuthor%3A%20Emmanouil%20Kritharakis%20and%20Dusan%20Jakovetic%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20multiple%0Aclients%20without%20sharing%20private%20data.%20We%20consider%20FL%20scenarios%20wherein%20FL%0Aclients%20are%20subject%20to%20adversarial%20%28Byzantine%29%20attacks%2C%20while%20the%20FL%20server%20is%0Atrusted%20%28honest%29%20and%20has%20a%20trustworthy%20side%20dataset.%20This%20may%20correspond%20to%2C%0Ae.g.%2C%20cases%20where%20the%20server%20possesses%20trusted%20data%20prior%20to%20federation%2C%20or%20to%0Athe%20presence%20of%20a%20trusted%20client%20that%20temporarily%20assumes%20the%20server%20role.%20Our%0Aapproach%20requires%20only%20two%20honest%20participants%2C%20i.e.%2C%20the%20server%20and%20one%0Aclient%2C%20to%20function%20effectively%2C%20without%20prior%20knowledge%20of%20the%20number%20of%0Amalicious%20clients.%20Theoretical%20analysis%20demonstrates%20bounded%20optimality%20gaps%0Aeven%20under%20strong%20Byzantine%20attacks.%20Experimental%20results%20show%20that%20our%0Aalgorithm%20significantly%20outperforms%20standard%20and%20robust%20FL%20baselines%20such%20as%0AMean%2C%20Trimmed%20Mean%2C%20Median%2C%20Krum%2C%20and%20Multi-Krum%20under%20various%20attack%0Astrategies%20including%20label%20flipping%2C%20sign%20flipping%2C%20and%20Gaussian%20noise%20addition%0Aacross%20MNIST%2C%20FMNIST%2C%20and%20CIFAR-10%20benchmarks%20using%20the%20Flower%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Federated%2520Learning%2520under%2520Adversarial%2520Attacks%2520via%2520Loss-Based%250A%2520%2520Client%2520Clustering%26entry.906535625%3DEmmanouil%2520Kritharakis%2520and%2520Dusan%2520Jakovetic%2520and%2520Antonios%2520Makris%2520and%2520Konstantinos%2520Tserpes%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%2520multiple%250Aclients%2520without%2520sharing%2520private%2520data.%2520We%2520consider%2520FL%2520scenarios%2520wherein%2520FL%250Aclients%2520are%2520subject%2520to%2520adversarial%2520%2528Byzantine%2529%2520attacks%252C%2520while%2520the%2520FL%2520server%2520is%250Atrusted%2520%2528honest%2529%2520and%2520has%2520a%2520trustworthy%2520side%2520dataset.%2520This%2520may%2520correspond%2520to%252C%250Ae.g.%252C%2520cases%2520where%2520the%2520server%2520possesses%2520trusted%2520data%2520prior%2520to%2520federation%252C%2520or%2520to%250Athe%2520presence%2520of%2520a%2520trusted%2520client%2520that%2520temporarily%2520assumes%2520the%2520server%2520role.%2520Our%250Aapproach%2520requires%2520only%2520two%2520honest%2520participants%252C%2520i.e.%252C%2520the%2520server%2520and%2520one%250Aclient%252C%2520to%2520function%2520effectively%252C%2520without%2520prior%2520knowledge%2520of%2520the%2520number%2520of%250Amalicious%2520clients.%2520Theoretical%2520analysis%2520demonstrates%2520bounded%2520optimality%2520gaps%250Aeven%2520under%2520strong%2520Byzantine%2520attacks.%2520Experimental%2520results%2520show%2520that%2520our%250Aalgorithm%2520significantly%2520outperforms%2520standard%2520and%2520robust%2520FL%2520baselines%2520such%2520as%250AMean%252C%2520Trimmed%2520Mean%252C%2520Median%252C%2520Krum%252C%2520and%2520Multi-Krum%2520under%2520various%2520attack%250Astrategies%2520including%2520label%2520flipping%252C%2520sign%2520flipping%252C%2520and%2520Gaussian%2520noise%2520addition%250Aacross%2520MNIST%252C%2520FMNIST%252C%2520and%2520CIFAR-10%2520benchmarks%2520using%2520the%2520Flower%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Federated%20Learning%20under%20Adversarial%20Attacks%20via%20Loss-Based%0A%20%20Client%20Clustering&entry.906535625=Emmanouil%20Kritharakis%20and%20Dusan%20Jakovetic%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20multiple%0Aclients%20without%20sharing%20private%20data.%20We%20consider%20FL%20scenarios%20wherein%20FL%0Aclients%20are%20subject%20to%20adversarial%20%28Byzantine%29%20attacks%2C%20while%20the%20FL%20server%20is%0Atrusted%20%28honest%29%20and%20has%20a%20trustworthy%20side%20dataset.%20This%20may%20correspond%20to%2C%0Ae.g.%2C%20cases%20where%20the%20server%20possesses%20trusted%20data%20prior%20to%20federation%2C%20or%20to%0Athe%20presence%20of%20a%20trusted%20client%20that%20temporarily%20assumes%20the%20server%20role.%20Our%0Aapproach%20requires%20only%20two%20honest%20participants%2C%20i.e.%2C%20the%20server%20and%20one%0Aclient%2C%20to%20function%20effectively%2C%20without%20prior%20knowledge%20of%20the%20number%20of%0Amalicious%20clients.%20Theoretical%20analysis%20demonstrates%20bounded%20optimality%20gaps%0Aeven%20under%20strong%20Byzantine%20attacks.%20Experimental%20results%20show%20that%20our%0Aalgorithm%20significantly%20outperforms%20standard%20and%20robust%20FL%20baselines%20such%20as%0AMean%2C%20Trimmed%20Mean%2C%20Median%2C%20Krum%2C%20and%20Multi-Krum%20under%20various%20attack%0Astrategies%20including%20label%20flipping%2C%20sign%20flipping%2C%20and%20Gaussian%20noise%20addition%0Aacross%20MNIST%2C%20FMNIST%2C%20and%20CIFAR-10%20benchmarks%20using%20the%20Flower%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12672v3&entry.124074799=Read"},
{"title": "ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and\n  Omnidirectional Reasoning in Camouflaged Object Segmentation", "author": "Jianwen Tan and Huiyao Zhang and Rui Xiong and Han Zhou and Hongfei Wang and Ye Li", "abstract": "  Camouflaged Object Segmentation (COS) poses a significant challenge due to\nthe intrinsic high similarity between targets and backgrounds, demanding models\ncapable of profound holistic understanding beyond superficial cues. Prevailing\nmethods, often limited by shallow feature representation, inadequate reasoning\nmechanisms, and weak cross-modal integration, struggle to achieve this depth of\ncognition, resulting in prevalent issues like incomplete target separation and\nimprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed\nGiant-emphasizing holistic observation, omnidirectional focus, and intensive\nscrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought\nframework underpinned by cross-modal synergy and omnidirectional reasoning\nwithin Vision-Language Models (VLMs). ArgusCogito orchestrates three\ncognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive\nprior through global reasoning with cross-modal fusion (RGB, depth, semantic\nmaps), enabling holistic scene understanding and enhanced target-background\ndisambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning\nand focused reasoning, guided by semantic priors from Conjecture, enabling\nprecise target localization and region-of-interest refinement. (3) Sculpting:\nProgressively sculpts high-fidelity segmentation masks by integrating\ncross-modal information and iteratively generating dense positive/negative\npoint prompts within focused regions, emulating Argus' intensive scrutiny.\nExtensive evaluations on four challenging COS benchmarks and three Medical\nImage Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves\nstate-of-the-art (SOTA) performance, validating the framework's exceptional\nefficacy, superior generalization capability, and robustness.\n", "link": "http://arxiv.org/abs/2508.18050v1", "date": "2025-08-25", "relevancy": 2.3937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArgusCogito%3A%20Chain-of-Thought%20for%20Cross-Modal%20Synergy%20and%0A%20%20Omnidirectional%20Reasoning%20in%20Camouflaged%20Object%20Segmentation&body=Title%3A%20ArgusCogito%3A%20Chain-of-Thought%20for%20Cross-Modal%20Synergy%20and%0A%20%20Omnidirectional%20Reasoning%20in%20Camouflaged%20Object%20Segmentation%0AAuthor%3A%20Jianwen%20Tan%20and%20Huiyao%20Zhang%20and%20Rui%20Xiong%20and%20Han%20Zhou%20and%20Hongfei%20Wang%20and%20Ye%20Li%0AAbstract%3A%20%20%20Camouflaged%20Object%20Segmentation%20%28COS%29%20poses%20a%20significant%20challenge%20due%20to%0Athe%20intrinsic%20high%20similarity%20between%20targets%20and%20backgrounds%2C%20demanding%20models%0Acapable%20of%20profound%20holistic%20understanding%20beyond%20superficial%20cues.%20Prevailing%0Amethods%2C%20often%20limited%20by%20shallow%20feature%20representation%2C%20inadequate%20reasoning%0Amechanisms%2C%20and%20weak%20cross-modal%20integration%2C%20struggle%20to%20achieve%20this%20depth%20of%0Acognition%2C%20resulting%20in%20prevalent%20issues%20like%20incomplete%20target%20separation%20and%0Aimprecise%20segmentation.%20Inspired%20by%20the%20perceptual%20strategy%20of%20the%20Hundred-eyed%0AGiant-emphasizing%20holistic%20observation%2C%20omnidirectional%20focus%2C%20and%20intensive%0Ascrutiny-we%20introduce%20ArgusCogito%2C%20a%20novel%20zero-shot%2C%20chain-of-thought%0Aframework%20underpinned%20by%20cross-modal%20synergy%20and%20omnidirectional%20reasoning%0Awithin%20Vision-Language%20Models%20%28VLMs%29.%20ArgusCogito%20orchestrates%20three%0Acognitively-inspired%20stages%3A%20%281%29%20Conjecture%3A%20Constructs%20a%20strong%20cognitive%0Aprior%20through%20global%20reasoning%20with%20cross-modal%20fusion%20%28RGB%2C%20depth%2C%20semantic%0Amaps%29%2C%20enabling%20holistic%20scene%20understanding%20and%20enhanced%20target-background%0Adisambiguation.%20%282%29%20Focus%3A%20Performs%20omnidirectional%2C%20attention-driven%20scanning%0Aand%20focused%20reasoning%2C%20guided%20by%20semantic%20priors%20from%20Conjecture%2C%20enabling%0Aprecise%20target%20localization%20and%20region-of-interest%20refinement.%20%283%29%20Sculpting%3A%0AProgressively%20sculpts%20high-fidelity%20segmentation%20masks%20by%20integrating%0Across-modal%20information%20and%20iteratively%20generating%20dense%20positive/negative%0Apoint%20prompts%20within%20focused%20regions%2C%20emulating%20Argus%27%20intensive%20scrutiny.%0AExtensive%20evaluations%20on%20four%20challenging%20COS%20benchmarks%20and%20three%20Medical%0AImage%20Segmentation%20%28MIS%29%20benchmarks%20demonstrate%20that%20ArgusCogito%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%2C%20validating%20the%20framework%27s%20exceptional%0Aefficacy%2C%20superior%20generalization%20capability%2C%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArgusCogito%253A%2520Chain-of-Thought%2520for%2520Cross-Modal%2520Synergy%2520and%250A%2520%2520Omnidirectional%2520Reasoning%2520in%2520Camouflaged%2520Object%2520Segmentation%26entry.906535625%3DJianwen%2520Tan%2520and%2520Huiyao%2520Zhang%2520and%2520Rui%2520Xiong%2520and%2520Han%2520Zhou%2520and%2520Hongfei%2520Wang%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Camouflaged%2520Object%2520Segmentation%2520%2528COS%2529%2520poses%2520a%2520significant%2520challenge%2520due%2520to%250Athe%2520intrinsic%2520high%2520similarity%2520between%2520targets%2520and%2520backgrounds%252C%2520demanding%2520models%250Acapable%2520of%2520profound%2520holistic%2520understanding%2520beyond%2520superficial%2520cues.%2520Prevailing%250Amethods%252C%2520often%2520limited%2520by%2520shallow%2520feature%2520representation%252C%2520inadequate%2520reasoning%250Amechanisms%252C%2520and%2520weak%2520cross-modal%2520integration%252C%2520struggle%2520to%2520achieve%2520this%2520depth%2520of%250Acognition%252C%2520resulting%2520in%2520prevalent%2520issues%2520like%2520incomplete%2520target%2520separation%2520and%250Aimprecise%2520segmentation.%2520Inspired%2520by%2520the%2520perceptual%2520strategy%2520of%2520the%2520Hundred-eyed%250AGiant-emphasizing%2520holistic%2520observation%252C%2520omnidirectional%2520focus%252C%2520and%2520intensive%250Ascrutiny-we%2520introduce%2520ArgusCogito%252C%2520a%2520novel%2520zero-shot%252C%2520chain-of-thought%250Aframework%2520underpinned%2520by%2520cross-modal%2520synergy%2520and%2520omnidirectional%2520reasoning%250Awithin%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520ArgusCogito%2520orchestrates%2520three%250Acognitively-inspired%2520stages%253A%2520%25281%2529%2520Conjecture%253A%2520Constructs%2520a%2520strong%2520cognitive%250Aprior%2520through%2520global%2520reasoning%2520with%2520cross-modal%2520fusion%2520%2528RGB%252C%2520depth%252C%2520semantic%250Amaps%2529%252C%2520enabling%2520holistic%2520scene%2520understanding%2520and%2520enhanced%2520target-background%250Adisambiguation.%2520%25282%2529%2520Focus%253A%2520Performs%2520omnidirectional%252C%2520attention-driven%2520scanning%250Aand%2520focused%2520reasoning%252C%2520guided%2520by%2520semantic%2520priors%2520from%2520Conjecture%252C%2520enabling%250Aprecise%2520target%2520localization%2520and%2520region-of-interest%2520refinement.%2520%25283%2529%2520Sculpting%253A%250AProgressively%2520sculpts%2520high-fidelity%2520segmentation%2520masks%2520by%2520integrating%250Across-modal%2520information%2520and%2520iteratively%2520generating%2520dense%2520positive/negative%250Apoint%2520prompts%2520within%2520focused%2520regions%252C%2520emulating%2520Argus%2527%2520intensive%2520scrutiny.%250AExtensive%2520evaluations%2520on%2520four%2520challenging%2520COS%2520benchmarks%2520and%2520three%2520Medical%250AImage%2520Segmentation%2520%2528MIS%2529%2520benchmarks%2520demonstrate%2520that%2520ArgusCogito%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%252C%2520validating%2520the%2520framework%2527s%2520exceptional%250Aefficacy%252C%2520superior%2520generalization%2520capability%252C%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArgusCogito%3A%20Chain-of-Thought%20for%20Cross-Modal%20Synergy%20and%0A%20%20Omnidirectional%20Reasoning%20in%20Camouflaged%20Object%20Segmentation&entry.906535625=Jianwen%20Tan%20and%20Huiyao%20Zhang%20and%20Rui%20Xiong%20and%20Han%20Zhou%20and%20Hongfei%20Wang%20and%20Ye%20Li&entry.1292438233=%20%20Camouflaged%20Object%20Segmentation%20%28COS%29%20poses%20a%20significant%20challenge%20due%20to%0Athe%20intrinsic%20high%20similarity%20between%20targets%20and%20backgrounds%2C%20demanding%20models%0Acapable%20of%20profound%20holistic%20understanding%20beyond%20superficial%20cues.%20Prevailing%0Amethods%2C%20often%20limited%20by%20shallow%20feature%20representation%2C%20inadequate%20reasoning%0Amechanisms%2C%20and%20weak%20cross-modal%20integration%2C%20struggle%20to%20achieve%20this%20depth%20of%0Acognition%2C%20resulting%20in%20prevalent%20issues%20like%20incomplete%20target%20separation%20and%0Aimprecise%20segmentation.%20Inspired%20by%20the%20perceptual%20strategy%20of%20the%20Hundred-eyed%0AGiant-emphasizing%20holistic%20observation%2C%20omnidirectional%20focus%2C%20and%20intensive%0Ascrutiny-we%20introduce%20ArgusCogito%2C%20a%20novel%20zero-shot%2C%20chain-of-thought%0Aframework%20underpinned%20by%20cross-modal%20synergy%20and%20omnidirectional%20reasoning%0Awithin%20Vision-Language%20Models%20%28VLMs%29.%20ArgusCogito%20orchestrates%20three%0Acognitively-inspired%20stages%3A%20%281%29%20Conjecture%3A%20Constructs%20a%20strong%20cognitive%0Aprior%20through%20global%20reasoning%20with%20cross-modal%20fusion%20%28RGB%2C%20depth%2C%20semantic%0Amaps%29%2C%20enabling%20holistic%20scene%20understanding%20and%20enhanced%20target-background%0Adisambiguation.%20%282%29%20Focus%3A%20Performs%20omnidirectional%2C%20attention-driven%20scanning%0Aand%20focused%20reasoning%2C%20guided%20by%20semantic%20priors%20from%20Conjecture%2C%20enabling%0Aprecise%20target%20localization%20and%20region-of-interest%20refinement.%20%283%29%20Sculpting%3A%0AProgressively%20sculpts%20high-fidelity%20segmentation%20masks%20by%20integrating%0Across-modal%20information%20and%20iteratively%20generating%20dense%20positive/negative%0Apoint%20prompts%20within%20focused%20regions%2C%20emulating%20Argus%27%20intensive%20scrutiny.%0AExtensive%20evaluations%20on%20four%20challenging%20COS%20benchmarks%20and%20three%20Medical%0AImage%20Segmentation%20%28MIS%29%20benchmarks%20demonstrate%20that%20ArgusCogito%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%2C%20validating%20the%20framework%27s%20exceptional%0Aefficacy%2C%20superior%20generalization%20capability%2C%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18050v1&entry.124074799=Read"},
{"title": "Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center\n  Dialogue Generation", "author": "Rishikesh Devanathan and Varun Nathan and Ayush Kumar", "abstract": "  Synthetic transcript generation is critical in contact center domains, where\nprivacy and data scarcity limit model training and evaluation. Unlike prior\nsynthetic dialogue generation work on open-domain or medical dialogues, contact\ncenter conversations are goal-oriented, role-asymmetric, and behaviorally\ncomplex, featuring disfluencies, ASR noise, and compliance-driven agent\nactions. In deployments where transcripts are unavailable, standard pipelines\nstill yield derived call attributes such as Intent Summaries, Topic Flow, and\nQA Evaluation Forms. We leverage these as supervision signals to guide\ngeneration. To assess the quality of such outputs, we introduce a diagnostic\nframework of 18 linguistically and behaviorally grounded metrics for comparing\nreal and synthetic transcripts. We benchmark four language-agnostic generation\nstrategies, from simple prompting to characteristic-aware multi-stage\napproaches, alongside reference-free baselines. Results reveal persistent\nchallenges: no method excels across all traits, with notable deficits in\ndisfluency, sentiment, and behavioral realism. Our diagnostic tool exposes\nthese gaps, enabling fine-grained evaluation and stress testing of synthetic\ndialogue across languages.\n", "link": "http://arxiv.org/abs/2508.18210v1", "date": "2025-08-25", "relevancy": 2.3927, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4884}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%0A%20%20Dialogue%20Generation&body=Title%3A%20Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%0A%20%20Dialogue%20Generation%0AAuthor%3A%20Rishikesh%20Devanathan%20and%20Varun%20Nathan%20and%20Ayush%20Kumar%0AAbstract%3A%20%20%20Synthetic%20transcript%20generation%20is%20critical%20in%20contact%20center%20domains%2C%20where%0Aprivacy%20and%20data%20scarcity%20limit%20model%20training%20and%20evaluation.%20Unlike%20prior%0Asynthetic%20dialogue%20generation%20work%20on%20open-domain%20or%20medical%20dialogues%2C%20contact%0Acenter%20conversations%20are%20goal-oriented%2C%20role-asymmetric%2C%20and%20behaviorally%0Acomplex%2C%20featuring%20disfluencies%2C%20ASR%20noise%2C%20and%20compliance-driven%20agent%0Aactions.%20In%20deployments%20where%20transcripts%20are%20unavailable%2C%20standard%20pipelines%0Astill%20yield%20derived%20call%20attributes%20such%20as%20Intent%20Summaries%2C%20Topic%20Flow%2C%20and%0AQA%20Evaluation%20Forms.%20We%20leverage%20these%20as%20supervision%20signals%20to%20guide%0Ageneration.%20To%20assess%20the%20quality%20of%20such%20outputs%2C%20we%20introduce%20a%20diagnostic%0Aframework%20of%2018%20linguistically%20and%20behaviorally%20grounded%20metrics%20for%20comparing%0Areal%20and%20synthetic%20transcripts.%20We%20benchmark%20four%20language-agnostic%20generation%0Astrategies%2C%20from%20simple%20prompting%20to%20characteristic-aware%20multi-stage%0Aapproaches%2C%20alongside%20reference-free%20baselines.%20Results%20reveal%20persistent%0Achallenges%3A%20no%20method%20excels%20across%20all%20traits%2C%20with%20notable%20deficits%20in%0Adisfluency%2C%20sentiment%2C%20and%20behavioral%20realism.%20Our%20diagnostic%20tool%20exposes%0Athese%20gaps%2C%20enabling%20fine-grained%20evaluation%20and%20stress%20testing%20of%20synthetic%0Adialogue%20across%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Synthetic%2520Isn%2527t%2520Real%2520Yet%253A%2520A%2520Diagnostic%2520Framework%2520for%2520Contact%2520Center%250A%2520%2520Dialogue%2520Generation%26entry.906535625%3DRishikesh%2520Devanathan%2520and%2520Varun%2520Nathan%2520and%2520Ayush%2520Kumar%26entry.1292438233%3D%2520%2520Synthetic%2520transcript%2520generation%2520is%2520critical%2520in%2520contact%2520center%2520domains%252C%2520where%250Aprivacy%2520and%2520data%2520scarcity%2520limit%2520model%2520training%2520and%2520evaluation.%2520Unlike%2520prior%250Asynthetic%2520dialogue%2520generation%2520work%2520on%2520open-domain%2520or%2520medical%2520dialogues%252C%2520contact%250Acenter%2520conversations%2520are%2520goal-oriented%252C%2520role-asymmetric%252C%2520and%2520behaviorally%250Acomplex%252C%2520featuring%2520disfluencies%252C%2520ASR%2520noise%252C%2520and%2520compliance-driven%2520agent%250Aactions.%2520In%2520deployments%2520where%2520transcripts%2520are%2520unavailable%252C%2520standard%2520pipelines%250Astill%2520yield%2520derived%2520call%2520attributes%2520such%2520as%2520Intent%2520Summaries%252C%2520Topic%2520Flow%252C%2520and%250AQA%2520Evaluation%2520Forms.%2520We%2520leverage%2520these%2520as%2520supervision%2520signals%2520to%2520guide%250Ageneration.%2520To%2520assess%2520the%2520quality%2520of%2520such%2520outputs%252C%2520we%2520introduce%2520a%2520diagnostic%250Aframework%2520of%252018%2520linguistically%2520and%2520behaviorally%2520grounded%2520metrics%2520for%2520comparing%250Areal%2520and%2520synthetic%2520transcripts.%2520We%2520benchmark%2520four%2520language-agnostic%2520generation%250Astrategies%252C%2520from%2520simple%2520prompting%2520to%2520characteristic-aware%2520multi-stage%250Aapproaches%252C%2520alongside%2520reference-free%2520baselines.%2520Results%2520reveal%2520persistent%250Achallenges%253A%2520no%2520method%2520excels%2520across%2520all%2520traits%252C%2520with%2520notable%2520deficits%2520in%250Adisfluency%252C%2520sentiment%252C%2520and%2520behavioral%2520realism.%2520Our%2520diagnostic%2520tool%2520exposes%250Athese%2520gaps%252C%2520enabling%2520fine-grained%2520evaluation%2520and%2520stress%2520testing%2520of%2520synthetic%250Adialogue%2520across%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%0A%20%20Dialogue%20Generation&entry.906535625=Rishikesh%20Devanathan%20and%20Varun%20Nathan%20and%20Ayush%20Kumar&entry.1292438233=%20%20Synthetic%20transcript%20generation%20is%20critical%20in%20contact%20center%20domains%2C%20where%0Aprivacy%20and%20data%20scarcity%20limit%20model%20training%20and%20evaluation.%20Unlike%20prior%0Asynthetic%20dialogue%20generation%20work%20on%20open-domain%20or%20medical%20dialogues%2C%20contact%0Acenter%20conversations%20are%20goal-oriented%2C%20role-asymmetric%2C%20and%20behaviorally%0Acomplex%2C%20featuring%20disfluencies%2C%20ASR%20noise%2C%20and%20compliance-driven%20agent%0Aactions.%20In%20deployments%20where%20transcripts%20are%20unavailable%2C%20standard%20pipelines%0Astill%20yield%20derived%20call%20attributes%20such%20as%20Intent%20Summaries%2C%20Topic%20Flow%2C%20and%0AQA%20Evaluation%20Forms.%20We%20leverage%20these%20as%20supervision%20signals%20to%20guide%0Ageneration.%20To%20assess%20the%20quality%20of%20such%20outputs%2C%20we%20introduce%20a%20diagnostic%0Aframework%20of%2018%20linguistically%20and%20behaviorally%20grounded%20metrics%20for%20comparing%0Areal%20and%20synthetic%20transcripts.%20We%20benchmark%20four%20language-agnostic%20generation%0Astrategies%2C%20from%20simple%20prompting%20to%20characteristic-aware%20multi-stage%0Aapproaches%2C%20alongside%20reference-free%20baselines.%20Results%20reveal%20persistent%0Achallenges%3A%20no%20method%20excels%20across%20all%20traits%2C%20with%20notable%20deficits%20in%0Adisfluency%2C%20sentiment%2C%20and%20behavioral%20realism.%20Our%20diagnostic%20tool%20exposes%0Athese%20gaps%2C%20enabling%20fine-grained%20evaluation%20and%20stress%20testing%20of%20synthetic%0Adialogue%20across%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18210v1&entry.124074799=Read"},
{"title": "Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal\n  Differentiated Quantization VLMs for Visually Impaired Assistance", "author": "Xiangxiang Wang and Xuanyu Wang and YiJia Luo and Yongbin Yu and Manping Fan and Jingtao Zhang and Liyong Ren", "abstract": "  This study proposes the dual technological innovation framework, including a\ncross-modal differ entiated quantization framework for vision-language models\n(VLMs) and a scene-aware vectorized\n  memory multi-agent system for visually impaired assistance. The modular\nframework was developed\n  implementing differentiated processing strategies, effectively reducing\nmemory requirements from\n  38GB to 16GB while maintaining model performance. The multi-agent\narchitecture combines\n  scene classification, vectorized memory, and multimodal interaction, enabling\npersistent storage\n  and efficient retrieval of scene memories. Through\nperception-memory-reasoning workflows, the\n  system provides environmental information beyond the current view using\nhistorical memories.\n  Experiments show the quantized 19B-parameter model only experiences a 2.05%\nperformance drop\n  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),\noutperforming smaller\n  models with equivalent memory requirements like the Molmo-7B series. The\nsystem maintains\n  response latency between 2.83-3.52 seconds from scene analysis to initial\nspeech output, substantially\n  faster than non-streaming methods. This research advances computational\nefficiency and assistive\n  technology, offering visually impaired users comprehensive real-time\nassistance in scene perception,\n  text recognition, and navigation.\n", "link": "http://arxiv.org/abs/2508.18177v1", "date": "2025-08-25", "relevancy": 2.3772, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene-Aware%20Vectorized%20Memory%20Multi-Agent%20Framework%20with%20Cross-Modal%0A%20%20Differentiated%20Quantization%20VLMs%20for%20Visually%20Impaired%20Assistance&body=Title%3A%20Scene-Aware%20Vectorized%20Memory%20Multi-Agent%20Framework%20with%20Cross-Modal%0A%20%20Differentiated%20Quantization%20VLMs%20for%20Visually%20Impaired%20Assistance%0AAuthor%3A%20Xiangxiang%20Wang%20and%20Xuanyu%20Wang%20and%20YiJia%20Luo%20and%20Yongbin%20Yu%20and%20Manping%20Fan%20and%20Jingtao%20Zhang%20and%20Liyong%20Ren%0AAbstract%3A%20%20%20This%20study%20proposes%20the%20dual%20technological%20innovation%20framework%2C%20including%20a%0Across-modal%20differ%20entiated%20quantization%20framework%20for%20vision-language%20models%0A%28VLMs%29%20and%20a%20scene-aware%20vectorized%0A%20%20memory%20multi-agent%20system%20for%20visually%20impaired%20assistance.%20The%20modular%0Aframework%20was%20developed%0A%20%20implementing%20differentiated%20processing%20strategies%2C%20effectively%20reducing%0Amemory%20requirements%20from%0A%20%2038GB%20to%2016GB%20while%20maintaining%20model%20performance.%20The%20multi-agent%0Aarchitecture%20combines%0A%20%20scene%20classification%2C%20vectorized%20memory%2C%20and%20multimodal%20interaction%2C%20enabling%0Apersistent%20storage%0A%20%20and%20efficient%20retrieval%20of%20scene%20memories.%20Through%0Aperception-memory-reasoning%20workflows%2C%20the%0A%20%20system%20provides%20environmental%20information%20beyond%20the%20current%20view%20using%0Ahistorical%20memories.%0A%20%20Experiments%20show%20the%20quantized%2019B-parameter%20model%20only%20experiences%20a%202.05%25%0Aperformance%20drop%0A%20%20on%20MMBench%20and%20maintains%2063.7%20accuracy%20on%20OCR-VQA%20%28original%3A%2064.9%29%2C%0Aoutperforming%20smaller%0A%20%20models%20with%20equivalent%20memory%20requirements%20like%20the%20Molmo-7B%20series.%20The%0Asystem%20maintains%0A%20%20response%20latency%20between%202.83-3.52%20seconds%20from%20scene%20analysis%20to%20initial%0Aspeech%20output%2C%20substantially%0A%20%20faster%20than%20non-streaming%20methods.%20This%20research%20advances%20computational%0Aefficiency%20and%20assistive%0A%20%20technology%2C%20offering%20visually%20impaired%20users%20comprehensive%20real-time%0Aassistance%20in%20scene%20perception%2C%0A%20%20text%20recognition%2C%20and%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene-Aware%2520Vectorized%2520Memory%2520Multi-Agent%2520Framework%2520with%2520Cross-Modal%250A%2520%2520Differentiated%2520Quantization%2520VLMs%2520for%2520Visually%2520Impaired%2520Assistance%26entry.906535625%3DXiangxiang%2520Wang%2520and%2520Xuanyu%2520Wang%2520and%2520YiJia%2520Luo%2520and%2520Yongbin%2520Yu%2520and%2520Manping%2520Fan%2520and%2520Jingtao%2520Zhang%2520and%2520Liyong%2520Ren%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520the%2520dual%2520technological%2520innovation%2520framework%252C%2520including%2520a%250Across-modal%2520differ%2520entiated%2520quantization%2520framework%2520for%2520vision-language%2520models%250A%2528VLMs%2529%2520and%2520a%2520scene-aware%2520vectorized%250A%2520%2520memory%2520multi-agent%2520system%2520for%2520visually%2520impaired%2520assistance.%2520The%2520modular%250Aframework%2520was%2520developed%250A%2520%2520implementing%2520differentiated%2520processing%2520strategies%252C%2520effectively%2520reducing%250Amemory%2520requirements%2520from%250A%2520%252038GB%2520to%252016GB%2520while%2520maintaining%2520model%2520performance.%2520The%2520multi-agent%250Aarchitecture%2520combines%250A%2520%2520scene%2520classification%252C%2520vectorized%2520memory%252C%2520and%2520multimodal%2520interaction%252C%2520enabling%250Apersistent%2520storage%250A%2520%2520and%2520efficient%2520retrieval%2520of%2520scene%2520memories.%2520Through%250Aperception-memory-reasoning%2520workflows%252C%2520the%250A%2520%2520system%2520provides%2520environmental%2520information%2520beyond%2520the%2520current%2520view%2520using%250Ahistorical%2520memories.%250A%2520%2520Experiments%2520show%2520the%2520quantized%252019B-parameter%2520model%2520only%2520experiences%2520a%25202.05%2525%250Aperformance%2520drop%250A%2520%2520on%2520MMBench%2520and%2520maintains%252063.7%2520accuracy%2520on%2520OCR-VQA%2520%2528original%253A%252064.9%2529%252C%250Aoutperforming%2520smaller%250A%2520%2520models%2520with%2520equivalent%2520memory%2520requirements%2520like%2520the%2520Molmo-7B%2520series.%2520The%250Asystem%2520maintains%250A%2520%2520response%2520latency%2520between%25202.83-3.52%2520seconds%2520from%2520scene%2520analysis%2520to%2520initial%250Aspeech%2520output%252C%2520substantially%250A%2520%2520faster%2520than%2520non-streaming%2520methods.%2520This%2520research%2520advances%2520computational%250Aefficiency%2520and%2520assistive%250A%2520%2520technology%252C%2520offering%2520visually%2520impaired%2520users%2520comprehensive%2520real-time%250Aassistance%2520in%2520scene%2520perception%252C%250A%2520%2520text%2520recognition%252C%2520and%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene-Aware%20Vectorized%20Memory%20Multi-Agent%20Framework%20with%20Cross-Modal%0A%20%20Differentiated%20Quantization%20VLMs%20for%20Visually%20Impaired%20Assistance&entry.906535625=Xiangxiang%20Wang%20and%20Xuanyu%20Wang%20and%20YiJia%20Luo%20and%20Yongbin%20Yu%20and%20Manping%20Fan%20and%20Jingtao%20Zhang%20and%20Liyong%20Ren&entry.1292438233=%20%20This%20study%20proposes%20the%20dual%20technological%20innovation%20framework%2C%20including%20a%0Across-modal%20differ%20entiated%20quantization%20framework%20for%20vision-language%20models%0A%28VLMs%29%20and%20a%20scene-aware%20vectorized%0A%20%20memory%20multi-agent%20system%20for%20visually%20impaired%20assistance.%20The%20modular%0Aframework%20was%20developed%0A%20%20implementing%20differentiated%20processing%20strategies%2C%20effectively%20reducing%0Amemory%20requirements%20from%0A%20%2038GB%20to%2016GB%20while%20maintaining%20model%20performance.%20The%20multi-agent%0Aarchitecture%20combines%0A%20%20scene%20classification%2C%20vectorized%20memory%2C%20and%20multimodal%20interaction%2C%20enabling%0Apersistent%20storage%0A%20%20and%20efficient%20retrieval%20of%20scene%20memories.%20Through%0Aperception-memory-reasoning%20workflows%2C%20the%0A%20%20system%20provides%20environmental%20information%20beyond%20the%20current%20view%20using%0Ahistorical%20memories.%0A%20%20Experiments%20show%20the%20quantized%2019B-parameter%20model%20only%20experiences%20a%202.05%25%0Aperformance%20drop%0A%20%20on%20MMBench%20and%20maintains%2063.7%20accuracy%20on%20OCR-VQA%20%28original%3A%2064.9%29%2C%0Aoutperforming%20smaller%0A%20%20models%20with%20equivalent%20memory%20requirements%20like%20the%20Molmo-7B%20series.%20The%0Asystem%20maintains%0A%20%20response%20latency%20between%202.83-3.52%20seconds%20from%20scene%20analysis%20to%20initial%0Aspeech%20output%2C%20substantially%0A%20%20faster%20than%20non-streaming%20methods.%20This%20research%20advances%20computational%0Aefficiency%20and%20assistive%0A%20%20technology%2C%20offering%20visually%20impaired%20users%20comprehensive%20real-time%0Aassistance%20in%20scene%20perception%2C%0A%20%20text%20recognition%2C%20and%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18177v1&entry.124074799=Read"},
{"title": "Riemannian Optimization for LoRA on the Stiefel Manifold", "author": "Juneyoung Park and Minjae Kang and Seongbae Lee and Haegang Lee and Seongwan Kim and Jaeho Lee", "abstract": "  While powerful, large language models (LLMs) present significant fine-tuning\nchallenges due to their size. Parameter-efficient fine-tuning (PEFT) methods\nlike LoRA provide solutions, yet suffer from critical optimizer inefficiencies;\nnotably basis redundancy in LoRA's $B$ matrix when using AdamW, which\nfundamentally limits performance. We address this by optimizing the $B$ matrix\non the Stiefel manifold, imposing explicit orthogonality constraints that\nachieve near-perfect orthogonality and full effective rank. This geometric\napproach dramatically enhances parameter efficiency and representational\ncapacity. Our Stiefel optimizer consistently outperforms AdamW across\nbenchmarks with both LoRA and DoRA, demonstrating that geometric constraints\nare the key to unlocking LoRA's full potential for effective LLM fine-tuning.\n", "link": "http://arxiv.org/abs/2508.17901v1", "date": "2025-08-25", "relevancy": 2.3742, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Optimization%20for%20LoRA%20on%20the%20Stiefel%20Manifold&body=Title%3A%20Riemannian%20Optimization%20for%20LoRA%20on%20the%20Stiefel%20Manifold%0AAuthor%3A%20Juneyoung%20Park%20and%20Minjae%20Kang%20and%20Seongbae%20Lee%20and%20Haegang%20Lee%20and%20Seongwan%20Kim%20and%20Jaeho%20Lee%0AAbstract%3A%20%20%20While%20powerful%2C%20large%20language%20models%20%28LLMs%29%20present%20significant%20fine-tuning%0Achallenges%20due%20to%20their%20size.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%0Alike%20LoRA%20provide%20solutions%2C%20yet%20suffer%20from%20critical%20optimizer%20inefficiencies%3B%0Anotably%20basis%20redundancy%20in%20LoRA%27s%20%24B%24%20matrix%20when%20using%20AdamW%2C%20which%0Afundamentally%20limits%20performance.%20We%20address%20this%20by%20optimizing%20the%20%24B%24%20matrix%0Aon%20the%20Stiefel%20manifold%2C%20imposing%20explicit%20orthogonality%20constraints%20that%0Aachieve%20near-perfect%20orthogonality%20and%20full%20effective%20rank.%20This%20geometric%0Aapproach%20dramatically%20enhances%20parameter%20efficiency%20and%20representational%0Acapacity.%20Our%20Stiefel%20optimizer%20consistently%20outperforms%20AdamW%20across%0Abenchmarks%20with%20both%20LoRA%20and%20DoRA%2C%20demonstrating%20that%20geometric%20constraints%0Aare%20the%20key%20to%20unlocking%20LoRA%27s%20full%20potential%20for%20effective%20LLM%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Optimization%2520for%2520LoRA%2520on%2520the%2520Stiefel%2520Manifold%26entry.906535625%3DJuneyoung%2520Park%2520and%2520Minjae%2520Kang%2520and%2520Seongbae%2520Lee%2520and%2520Haegang%2520Lee%2520and%2520Seongwan%2520Kim%2520and%2520Jaeho%2520Lee%26entry.1292438233%3D%2520%2520While%2520powerful%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520present%2520significant%2520fine-tuning%250Achallenges%2520due%2520to%2520their%2520size.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%250Alike%2520LoRA%2520provide%2520solutions%252C%2520yet%2520suffer%2520from%2520critical%2520optimizer%2520inefficiencies%253B%250Anotably%2520basis%2520redundancy%2520in%2520LoRA%2527s%2520%2524B%2524%2520matrix%2520when%2520using%2520AdamW%252C%2520which%250Afundamentally%2520limits%2520performance.%2520We%2520address%2520this%2520by%2520optimizing%2520the%2520%2524B%2524%2520matrix%250Aon%2520the%2520Stiefel%2520manifold%252C%2520imposing%2520explicit%2520orthogonality%2520constraints%2520that%250Aachieve%2520near-perfect%2520orthogonality%2520and%2520full%2520effective%2520rank.%2520This%2520geometric%250Aapproach%2520dramatically%2520enhances%2520parameter%2520efficiency%2520and%2520representational%250Acapacity.%2520Our%2520Stiefel%2520optimizer%2520consistently%2520outperforms%2520AdamW%2520across%250Abenchmarks%2520with%2520both%2520LoRA%2520and%2520DoRA%252C%2520demonstrating%2520that%2520geometric%2520constraints%250Aare%2520the%2520key%2520to%2520unlocking%2520LoRA%2527s%2520full%2520potential%2520for%2520effective%2520LLM%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Optimization%20for%20LoRA%20on%20the%20Stiefel%20Manifold&entry.906535625=Juneyoung%20Park%20and%20Minjae%20Kang%20and%20Seongbae%20Lee%20and%20Haegang%20Lee%20and%20Seongwan%20Kim%20and%20Jaeho%20Lee&entry.1292438233=%20%20While%20powerful%2C%20large%20language%20models%20%28LLMs%29%20present%20significant%20fine-tuning%0Achallenges%20due%20to%20their%20size.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%0Alike%20LoRA%20provide%20solutions%2C%20yet%20suffer%20from%20critical%20optimizer%20inefficiencies%3B%0Anotably%20basis%20redundancy%20in%20LoRA%27s%20%24B%24%20matrix%20when%20using%20AdamW%2C%20which%0Afundamentally%20limits%20performance.%20We%20address%20this%20by%20optimizing%20the%20%24B%24%20matrix%0Aon%20the%20Stiefel%20manifold%2C%20imposing%20explicit%20orthogonality%20constraints%20that%0Aachieve%20near-perfect%20orthogonality%20and%20full%20effective%20rank.%20This%20geometric%0Aapproach%20dramatically%20enhances%20parameter%20efficiency%20and%20representational%0Acapacity.%20Our%20Stiefel%20optimizer%20consistently%20outperforms%20AdamW%20across%0Abenchmarks%20with%20both%20LoRA%20and%20DoRA%2C%20demonstrating%20that%20geometric%20constraints%0Aare%20the%20key%20to%20unlocking%20LoRA%27s%20full%20potential%20for%20effective%20LLM%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17901v1&entry.124074799=Read"},
{"title": "Manifold learning in metric spaces", "author": "Liane Xu and Amit Singer", "abstract": "  Laplacian-based methods are popular for dimensionality reduction of data\nlying in $\\mathbb{R}^N$. Several theoretical results for these algorithms\ndepend on the fact that the Euclidean distance locally approximates the\ngeodesic distance on the underlying submanifold which the data are assumed to\nlie on. However, for some applications, other metrics, such as the Wasserstein\ndistance, may provide a more appropriate notion of distance than the Euclidean\ndistance. We provide a framework that generalizes the problem of manifold\nlearning to metric spaces and study when a metric satisfies sufficient\nconditions for the pointwise convergence of the graph Laplacian.\n", "link": "http://arxiv.org/abs/2503.16187v2", "date": "2025-08-25", "relevancy": 2.3613, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4745}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20learning%20in%20metric%20spaces&body=Title%3A%20Manifold%20learning%20in%20metric%20spaces%0AAuthor%3A%20Liane%20Xu%20and%20Amit%20Singer%0AAbstract%3A%20%20%20Laplacian-based%20methods%20are%20popular%20for%20dimensionality%20reduction%20of%20data%0Alying%20in%20%24%5Cmathbb%7BR%7D%5EN%24.%20Several%20theoretical%20results%20for%20these%20algorithms%0Adepend%20on%20the%20fact%20that%20the%20Euclidean%20distance%20locally%20approximates%20the%0Ageodesic%20distance%20on%20the%20underlying%20submanifold%20which%20the%20data%20are%20assumed%20to%0Alie%20on.%20However%2C%20for%20some%20applications%2C%20other%20metrics%2C%20such%20as%20the%20Wasserstein%0Adistance%2C%20may%20provide%20a%20more%20appropriate%20notion%20of%20distance%20than%20the%20Euclidean%0Adistance.%20We%20provide%20a%20framework%20that%20generalizes%20the%20problem%20of%20manifold%0Alearning%20to%20metric%20spaces%20and%20study%20when%20a%20metric%20satisfies%20sufficient%0Aconditions%20for%20the%20pointwise%20convergence%20of%20the%20graph%20Laplacian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520learning%2520in%2520metric%2520spaces%26entry.906535625%3DLiane%2520Xu%2520and%2520Amit%2520Singer%26entry.1292438233%3D%2520%2520Laplacian-based%2520methods%2520are%2520popular%2520for%2520dimensionality%2520reduction%2520of%2520data%250Alying%2520in%2520%2524%255Cmathbb%257BR%257D%255EN%2524.%2520Several%2520theoretical%2520results%2520for%2520these%2520algorithms%250Adepend%2520on%2520the%2520fact%2520that%2520the%2520Euclidean%2520distance%2520locally%2520approximates%2520the%250Ageodesic%2520distance%2520on%2520the%2520underlying%2520submanifold%2520which%2520the%2520data%2520are%2520assumed%2520to%250Alie%2520on.%2520However%252C%2520for%2520some%2520applications%252C%2520other%2520metrics%252C%2520such%2520as%2520the%2520Wasserstein%250Adistance%252C%2520may%2520provide%2520a%2520more%2520appropriate%2520notion%2520of%2520distance%2520than%2520the%2520Euclidean%250Adistance.%2520We%2520provide%2520a%2520framework%2520that%2520generalizes%2520the%2520problem%2520of%2520manifold%250Alearning%2520to%2520metric%2520spaces%2520and%2520study%2520when%2520a%2520metric%2520satisfies%2520sufficient%250Aconditions%2520for%2520the%2520pointwise%2520convergence%2520of%2520the%2520graph%2520Laplacian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20learning%20in%20metric%20spaces&entry.906535625=Liane%20Xu%20and%20Amit%20Singer&entry.1292438233=%20%20Laplacian-based%20methods%20are%20popular%20for%20dimensionality%20reduction%20of%20data%0Alying%20in%20%24%5Cmathbb%7BR%7D%5EN%24.%20Several%20theoretical%20results%20for%20these%20algorithms%0Adepend%20on%20the%20fact%20that%20the%20Euclidean%20distance%20locally%20approximates%20the%0Ageodesic%20distance%20on%20the%20underlying%20submanifold%20which%20the%20data%20are%20assumed%20to%0Alie%20on.%20However%2C%20for%20some%20applications%2C%20other%20metrics%2C%20such%20as%20the%20Wasserstein%0Adistance%2C%20may%20provide%20a%20more%20appropriate%20notion%20of%20distance%20than%20the%20Euclidean%0Adistance.%20We%20provide%20a%20framework%20that%20generalizes%20the%20problem%20of%20manifold%0Alearning%20to%20metric%20spaces%20and%20study%20when%20a%20metric%20satisfies%20sufficient%0Aconditions%20for%20the%20pointwise%20convergence%20of%20the%20graph%20Laplacian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16187v2&entry.124074799=Read"},
{"title": "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of\n  endoscopic images", "author": "Xinning Yao and Bo Liu and Bojian Li and Jingjing Wang and Jinghua Yue and Fugen Zhou", "abstract": "  Depth estimation is a foundational component for 3D reconstruction in\nminimally invasive endoscopic surgeries. However, existing monocular depth\nestimation techniques often exhibit limited performance to the varying\nillumination and complex textures of the surgical environment. While powerful\nvisual foundation models offer a promising solution, their training on natural\nimages leads to significant domain adaptability limitations and semantic\nperception deficiencies when applied to endoscopy. In this study, we introduce\nEndoUFM, an unsupervised monocular depth estimation framework that innovatively\nintegrating dual foundation models for surgical scenes, which enhance the depth\nestimation performance by leveraging the powerful pre-learned priors. The\nframework features a novel adaptive fine-tuning strategy that incorporates\nRandom Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a\nResidual block based on Depthwise Separable Convolution (Res-DSC) to improve\nthe capture of fine-grained local features. Furthermore, we design a\nmask-guided smoothness loss to enforce depth consistency within anatomical\ntissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and\nEndoNeRF datasets confirm that our method achieves state-of-the-art performance\nwhile maintaining an efficient model size. This work contributes to augmenting\nsurgeons' spatial perception during minimally invasive procedures, thereby\nenhancing surgical precision and safety, with crucial implications for\naugmented reality and navigation systems.\n", "link": "http://arxiv.org/abs/2508.17916v1", "date": "2025-08-25", "relevancy": 2.3464, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoUFM%3A%20Utilizing%20Foundation%20Models%20for%20Monocular%20depth%20estimation%20of%0A%20%20endoscopic%20images&body=Title%3A%20EndoUFM%3A%20Utilizing%20Foundation%20Models%20for%20Monocular%20depth%20estimation%20of%0A%20%20endoscopic%20images%0AAuthor%3A%20Xinning%20Yao%20and%20Bo%20Liu%20and%20Bojian%20Li%20and%20Jingjing%20Wang%20and%20Jinghua%20Yue%20and%20Fugen%20Zhou%0AAbstract%3A%20%20%20Depth%20estimation%20is%20a%20foundational%20component%20for%203D%20reconstruction%20in%0Aminimally%20invasive%20endoscopic%20surgeries.%20However%2C%20existing%20monocular%20depth%0Aestimation%20techniques%20often%20exhibit%20limited%20performance%20to%20the%20varying%0Aillumination%20and%20complex%20textures%20of%20the%20surgical%20environment.%20While%20powerful%0Avisual%20foundation%20models%20offer%20a%20promising%20solution%2C%20their%20training%20on%20natural%0Aimages%20leads%20to%20significant%20domain%20adaptability%20limitations%20and%20semantic%0Aperception%20deficiencies%20when%20applied%20to%20endoscopy.%20In%20this%20study%2C%20we%20introduce%0AEndoUFM%2C%20an%20unsupervised%20monocular%20depth%20estimation%20framework%20that%20innovatively%0Aintegrating%20dual%20foundation%20models%20for%20surgical%20scenes%2C%20which%20enhance%20the%20depth%0Aestimation%20performance%20by%20leveraging%20the%20powerful%20pre-learned%20priors.%20The%0Aframework%20features%20a%20novel%20adaptive%20fine-tuning%20strategy%20that%20incorporates%0ARandom%20Vector%20Low-Rank%20Adaptation%20%28RVLoRA%29%20to%20enhance%20model%20adaptability%2C%20and%20a%0AResidual%20block%20based%20on%20Depthwise%20Separable%20Convolution%20%28Res-DSC%29%20to%20improve%0Athe%20capture%20of%20fine-grained%20local%20features.%20Furthermore%2C%20we%20design%20a%0Amask-guided%20smoothness%20loss%20to%20enforce%20depth%20consistency%20within%20anatomical%0Atissue%20structures.%20Extensive%20experiments%20on%20the%20SCARED%2C%20Hamlyn%2C%20SERV-CT%2C%20and%0AEndoNeRF%20datasets%20confirm%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Awhile%20maintaining%20an%20efficient%20model%20size.%20This%20work%20contributes%20to%20augmenting%0Asurgeons%27%20spatial%20perception%20during%20minimally%20invasive%20procedures%2C%20thereby%0Aenhancing%20surgical%20precision%20and%20safety%2C%20with%20crucial%20implications%20for%0Aaugmented%20reality%20and%20navigation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoUFM%253A%2520Utilizing%2520Foundation%2520Models%2520for%2520Monocular%2520depth%2520estimation%2520of%250A%2520%2520endoscopic%2520images%26entry.906535625%3DXinning%2520Yao%2520and%2520Bo%2520Liu%2520and%2520Bojian%2520Li%2520and%2520Jingjing%2520Wang%2520and%2520Jinghua%2520Yue%2520and%2520Fugen%2520Zhou%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520is%2520a%2520foundational%2520component%2520for%25203D%2520reconstruction%2520in%250Aminimally%2520invasive%2520endoscopic%2520surgeries.%2520However%252C%2520existing%2520monocular%2520depth%250Aestimation%2520techniques%2520often%2520exhibit%2520limited%2520performance%2520to%2520the%2520varying%250Aillumination%2520and%2520complex%2520textures%2520of%2520the%2520surgical%2520environment.%2520While%2520powerful%250Avisual%2520foundation%2520models%2520offer%2520a%2520promising%2520solution%252C%2520their%2520training%2520on%2520natural%250Aimages%2520leads%2520to%2520significant%2520domain%2520adaptability%2520limitations%2520and%2520semantic%250Aperception%2520deficiencies%2520when%2520applied%2520to%2520endoscopy.%2520In%2520this%2520study%252C%2520we%2520introduce%250AEndoUFM%252C%2520an%2520unsupervised%2520monocular%2520depth%2520estimation%2520framework%2520that%2520innovatively%250Aintegrating%2520dual%2520foundation%2520models%2520for%2520surgical%2520scenes%252C%2520which%2520enhance%2520the%2520depth%250Aestimation%2520performance%2520by%2520leveraging%2520the%2520powerful%2520pre-learned%2520priors.%2520The%250Aframework%2520features%2520a%2520novel%2520adaptive%2520fine-tuning%2520strategy%2520that%2520incorporates%250ARandom%2520Vector%2520Low-Rank%2520Adaptation%2520%2528RVLoRA%2529%2520to%2520enhance%2520model%2520adaptability%252C%2520and%2520a%250AResidual%2520block%2520based%2520on%2520Depthwise%2520Separable%2520Convolution%2520%2528Res-DSC%2529%2520to%2520improve%250Athe%2520capture%2520of%2520fine-grained%2520local%2520features.%2520Furthermore%252C%2520we%2520design%2520a%250Amask-guided%2520smoothness%2520loss%2520to%2520enforce%2520depth%2520consistency%2520within%2520anatomical%250Atissue%2520structures.%2520Extensive%2520experiments%2520on%2520the%2520SCARED%252C%2520Hamlyn%252C%2520SERV-CT%252C%2520and%250AEndoNeRF%2520datasets%2520confirm%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Awhile%2520maintaining%2520an%2520efficient%2520model%2520size.%2520This%2520work%2520contributes%2520to%2520augmenting%250Asurgeons%2527%2520spatial%2520perception%2520during%2520minimally%2520invasive%2520procedures%252C%2520thereby%250Aenhancing%2520surgical%2520precision%2520and%2520safety%252C%2520with%2520crucial%2520implications%2520for%250Aaugmented%2520reality%2520and%2520navigation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoUFM%3A%20Utilizing%20Foundation%20Models%20for%20Monocular%20depth%20estimation%20of%0A%20%20endoscopic%20images&entry.906535625=Xinning%20Yao%20and%20Bo%20Liu%20and%20Bojian%20Li%20and%20Jingjing%20Wang%20and%20Jinghua%20Yue%20and%20Fugen%20Zhou&entry.1292438233=%20%20Depth%20estimation%20is%20a%20foundational%20component%20for%203D%20reconstruction%20in%0Aminimally%20invasive%20endoscopic%20surgeries.%20However%2C%20existing%20monocular%20depth%0Aestimation%20techniques%20often%20exhibit%20limited%20performance%20to%20the%20varying%0Aillumination%20and%20complex%20textures%20of%20the%20surgical%20environment.%20While%20powerful%0Avisual%20foundation%20models%20offer%20a%20promising%20solution%2C%20their%20training%20on%20natural%0Aimages%20leads%20to%20significant%20domain%20adaptability%20limitations%20and%20semantic%0Aperception%20deficiencies%20when%20applied%20to%20endoscopy.%20In%20this%20study%2C%20we%20introduce%0AEndoUFM%2C%20an%20unsupervised%20monocular%20depth%20estimation%20framework%20that%20innovatively%0Aintegrating%20dual%20foundation%20models%20for%20surgical%20scenes%2C%20which%20enhance%20the%20depth%0Aestimation%20performance%20by%20leveraging%20the%20powerful%20pre-learned%20priors.%20The%0Aframework%20features%20a%20novel%20adaptive%20fine-tuning%20strategy%20that%20incorporates%0ARandom%20Vector%20Low-Rank%20Adaptation%20%28RVLoRA%29%20to%20enhance%20model%20adaptability%2C%20and%20a%0AResidual%20block%20based%20on%20Depthwise%20Separable%20Convolution%20%28Res-DSC%29%20to%20improve%0Athe%20capture%20of%20fine-grained%20local%20features.%20Furthermore%2C%20we%20design%20a%0Amask-guided%20smoothness%20loss%20to%20enforce%20depth%20consistency%20within%20anatomical%0Atissue%20structures.%20Extensive%20experiments%20on%20the%20SCARED%2C%20Hamlyn%2C%20SERV-CT%2C%20and%0AEndoNeRF%20datasets%20confirm%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Awhile%20maintaining%20an%20efficient%20model%20size.%20This%20work%20contributes%20to%20augmenting%0Asurgeons%27%20spatial%20perception%20during%20minimally%20invasive%20procedures%2C%20thereby%0Aenhancing%20surgical%20precision%20and%20safety%2C%20with%20crucial%20implications%20for%0Aaugmented%20reality%20and%20navigation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17916v1&entry.124074799=Read"},
{"title": "Hessian-Based Lightweight Neural Network HessNet for State-of-the-Art\n  Brain Vessel Segmentation on a Minimal Training Dataset", "author": "Alexandra Bernadotte and Elfimov Nikita and Mikhail Shutov and Ivan Menshikov", "abstract": "  Accurate segmentation of blood vessels in brain magnetic resonance\nangiography (MRA) is essential for successful surgical procedures, such as\naneurysm repair or bypass surgery. Currently, annotation is primarily performed\nthrough manual segmentation or classical methods, such as the Frangi filter,\nwhich often lack sufficient accuracy. Neural networks have emerged as powerful\ntools for medical image segmentation, but their development depends on\nwell-annotated training datasets. However, there is a notable lack of publicly\navailable MRA datasets with detailed brain vessel annotations. To address this\ngap, we propose a novel semi-supervised learning lightweight neural network\nwith Hessian matrices on board for 3D segmentation of complex structures such\nas tubular structures, which we named HessNet. The solution is a Hessian-based\nneural network with only 6000 parameters. HessNet can run on the CPU and\nsignificantly reduces the resource requirements for training neural networks.\nThe accuracy of vessel segmentation on a minimal training dataset reaches\nstate-of-the-art results. It helps us create a large, semi-manually annotated\nbrain vessel dataset of brain MRA images based on the IXI dataset (annotated\n200 images). Annotation was performed by three experts under the supervision of\nthree neurovascular surgeons after applying HessNet. It provides high accuracy\nof vessel segmentation and allows experts to focus only on the most complex\nimportant cases. The dataset is available at\nhttps://git.scinalytics.com/terilat/VesselDatasetPartly.\n", "link": "http://arxiv.org/abs/2508.15660v2", "date": "2025-08-25", "relevancy": 2.3426, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.473}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4664}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hessian-Based%20Lightweight%20Neural%20Network%20HessNet%20for%20State-of-the-Art%0A%20%20Brain%20Vessel%20Segmentation%20on%20a%20Minimal%20Training%20Dataset&body=Title%3A%20Hessian-Based%20Lightweight%20Neural%20Network%20HessNet%20for%20State-of-the-Art%0A%20%20Brain%20Vessel%20Segmentation%20on%20a%20Minimal%20Training%20Dataset%0AAuthor%3A%20Alexandra%20Bernadotte%20and%20Elfimov%20Nikita%20and%20Mikhail%20Shutov%20and%20Ivan%20Menshikov%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20blood%20vessels%20in%20brain%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20essential%20for%20successful%20surgical%20procedures%2C%20such%20as%0Aaneurysm%20repair%20or%20bypass%20surgery.%20Currently%2C%20annotation%20is%20primarily%20performed%0Athrough%20manual%20segmentation%20or%20classical%20methods%2C%20such%20as%20the%20Frangi%20filter%2C%0Awhich%20often%20lack%20sufficient%20accuracy.%20Neural%20networks%20have%20emerged%20as%20powerful%0Atools%20for%20medical%20image%20segmentation%2C%20but%20their%20development%20depends%20on%0Awell-annotated%20training%20datasets.%20However%2C%20there%20is%20a%20notable%20lack%20of%20publicly%0Aavailable%20MRA%20datasets%20with%20detailed%20brain%20vessel%20annotations.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20novel%20semi-supervised%20learning%20lightweight%20neural%20network%0Awith%20Hessian%20matrices%20on%20board%20for%203D%20segmentation%20of%20complex%20structures%20such%0Aas%20tubular%20structures%2C%20which%20we%20named%20HessNet.%20The%20solution%20is%20a%20Hessian-based%0Aneural%20network%20with%20only%206000%20parameters.%20HessNet%20can%20run%20on%20the%20CPU%20and%0Asignificantly%20reduces%20the%20resource%20requirements%20for%20training%20neural%20networks.%0AThe%20accuracy%20of%20vessel%20segmentation%20on%20a%20minimal%20training%20dataset%20reaches%0Astate-of-the-art%20results.%20It%20helps%20us%20create%20a%20large%2C%20semi-manually%20annotated%0Abrain%20vessel%20dataset%20of%20brain%20MRA%20images%20based%20on%20the%20IXI%20dataset%20%28annotated%0A200%20images%29.%20Annotation%20was%20performed%20by%20three%20experts%20under%20the%20supervision%20of%0Athree%20neurovascular%20surgeons%20after%20applying%20HessNet.%20It%20provides%20high%20accuracy%0Aof%20vessel%20segmentation%20and%20allows%20experts%20to%20focus%20only%20on%20the%20most%20complex%0Aimportant%20cases.%20The%20dataset%20is%20available%20at%0Ahttps%3A//git.scinalytics.com/terilat/VesselDatasetPartly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHessian-Based%2520Lightweight%2520Neural%2520Network%2520HessNet%2520for%2520State-of-the-Art%250A%2520%2520Brain%2520Vessel%2520Segmentation%2520on%2520a%2520Minimal%2520Training%2520Dataset%26entry.906535625%3DAlexandra%2520Bernadotte%2520and%2520Elfimov%2520Nikita%2520and%2520Mikhail%2520Shutov%2520and%2520Ivan%2520Menshikov%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520blood%2520vessels%2520in%2520brain%2520magnetic%2520resonance%250Aangiography%2520%2528MRA%2529%2520is%2520essential%2520for%2520successful%2520surgical%2520procedures%252C%2520such%2520as%250Aaneurysm%2520repair%2520or%2520bypass%2520surgery.%2520Currently%252C%2520annotation%2520is%2520primarily%2520performed%250Athrough%2520manual%2520segmentation%2520or%2520classical%2520methods%252C%2520such%2520as%2520the%2520Frangi%2520filter%252C%250Awhich%2520often%2520lack%2520sufficient%2520accuracy.%2520Neural%2520networks%2520have%2520emerged%2520as%2520powerful%250Atools%2520for%2520medical%2520image%2520segmentation%252C%2520but%2520their%2520development%2520depends%2520on%250Awell-annotated%2520training%2520datasets.%2520However%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%2520publicly%250Aavailable%2520MRA%2520datasets%2520with%2520detailed%2520brain%2520vessel%2520annotations.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520a%2520novel%2520semi-supervised%2520learning%2520lightweight%2520neural%2520network%250Awith%2520Hessian%2520matrices%2520on%2520board%2520for%25203D%2520segmentation%2520of%2520complex%2520structures%2520such%250Aas%2520tubular%2520structures%252C%2520which%2520we%2520named%2520HessNet.%2520The%2520solution%2520is%2520a%2520Hessian-based%250Aneural%2520network%2520with%2520only%25206000%2520parameters.%2520HessNet%2520can%2520run%2520on%2520the%2520CPU%2520and%250Asignificantly%2520reduces%2520the%2520resource%2520requirements%2520for%2520training%2520neural%2520networks.%250AThe%2520accuracy%2520of%2520vessel%2520segmentation%2520on%2520a%2520minimal%2520training%2520dataset%2520reaches%250Astate-of-the-art%2520results.%2520It%2520helps%2520us%2520create%2520a%2520large%252C%2520semi-manually%2520annotated%250Abrain%2520vessel%2520dataset%2520of%2520brain%2520MRA%2520images%2520based%2520on%2520the%2520IXI%2520dataset%2520%2528annotated%250A200%2520images%2529.%2520Annotation%2520was%2520performed%2520by%2520three%2520experts%2520under%2520the%2520supervision%2520of%250Athree%2520neurovascular%2520surgeons%2520after%2520applying%2520HessNet.%2520It%2520provides%2520high%2520accuracy%250Aof%2520vessel%2520segmentation%2520and%2520allows%2520experts%2520to%2520focus%2520only%2520on%2520the%2520most%2520complex%250Aimportant%2520cases.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//git.scinalytics.com/terilat/VesselDatasetPartly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hessian-Based%20Lightweight%20Neural%20Network%20HessNet%20for%20State-of-the-Art%0A%20%20Brain%20Vessel%20Segmentation%20on%20a%20Minimal%20Training%20Dataset&entry.906535625=Alexandra%20Bernadotte%20and%20Elfimov%20Nikita%20and%20Mikhail%20Shutov%20and%20Ivan%20Menshikov&entry.1292438233=%20%20Accurate%20segmentation%20of%20blood%20vessels%20in%20brain%20magnetic%20resonance%0Aangiography%20%28MRA%29%20is%20essential%20for%20successful%20surgical%20procedures%2C%20such%20as%0Aaneurysm%20repair%20or%20bypass%20surgery.%20Currently%2C%20annotation%20is%20primarily%20performed%0Athrough%20manual%20segmentation%20or%20classical%20methods%2C%20such%20as%20the%20Frangi%20filter%2C%0Awhich%20often%20lack%20sufficient%20accuracy.%20Neural%20networks%20have%20emerged%20as%20powerful%0Atools%20for%20medical%20image%20segmentation%2C%20but%20their%20development%20depends%20on%0Awell-annotated%20training%20datasets.%20However%2C%20there%20is%20a%20notable%20lack%20of%20publicly%0Aavailable%20MRA%20datasets%20with%20detailed%20brain%20vessel%20annotations.%20To%20address%20this%0Agap%2C%20we%20propose%20a%20novel%20semi-supervised%20learning%20lightweight%20neural%20network%0Awith%20Hessian%20matrices%20on%20board%20for%203D%20segmentation%20of%20complex%20structures%20such%0Aas%20tubular%20structures%2C%20which%20we%20named%20HessNet.%20The%20solution%20is%20a%20Hessian-based%0Aneural%20network%20with%20only%206000%20parameters.%20HessNet%20can%20run%20on%20the%20CPU%20and%0Asignificantly%20reduces%20the%20resource%20requirements%20for%20training%20neural%20networks.%0AThe%20accuracy%20of%20vessel%20segmentation%20on%20a%20minimal%20training%20dataset%20reaches%0Astate-of-the-art%20results.%20It%20helps%20us%20create%20a%20large%2C%20semi-manually%20annotated%0Abrain%20vessel%20dataset%20of%20brain%20MRA%20images%20based%20on%20the%20IXI%20dataset%20%28annotated%0A200%20images%29.%20Annotation%20was%20performed%20by%20three%20experts%20under%20the%20supervision%20of%0Athree%20neurovascular%20surgeons%20after%20applying%20HessNet.%20It%20provides%20high%20accuracy%0Aof%20vessel%20segmentation%20and%20allows%20experts%20to%20focus%20only%20on%20the%20most%20complex%0Aimportant%20cases.%20The%20dataset%20is%20available%20at%0Ahttps%3A//git.scinalytics.com/terilat/VesselDatasetPartly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15660v2&entry.124074799=Read"},
{"title": "SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization", "author": "Junyuan Deng and Heng Li and Tao Xie and Weiqiang Ren and Qian Zhang and Ping Tan and Xiaoyang Guo", "abstract": "  Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)\nproblem by directly regressing camera poses and 3D scene structures from input\nimages. They demonstrate impressive performance in handling images under\nextreme viewpoint changes. However, these methods struggle to handle a large\nnumber of input images. To address this problem, we introduce SAIL-Recon, a\nfeed-forward Transformer for large scale SfM, by augmenting the scene\nregression network with visual localization capabilities. Specifically, our\nmethod first computes a neural scene representation from a subset of anchor\nimages. The regression network is then fine-tuned to reconstruct all input\nimages conditioned on this neural scene representation. Comprehensive\nexperiments show that our method not only scales efficiently to large-scale\nscenes, but also achieves state-of-the-art results on both camera pose\nestimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and\nTanks & Temples. We will publish our model and code. Code and models are\npublicly available at: https://hkust-sail.github.io/ sail-recon/.\n", "link": "http://arxiv.org/abs/2508.17972v1", "date": "2025-08-25", "relevancy": 2.3424, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6019}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5779}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAIL-Recon%3A%20Large%20SfM%20by%20Augmenting%20Scene%20Regression%20with%20Localization&body=Title%3A%20SAIL-Recon%3A%20Large%20SfM%20by%20Augmenting%20Scene%20Regression%20with%20Localization%0AAuthor%3A%20Junyuan%20Deng%20and%20Heng%20Li%20and%20Tao%20Xie%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang%20and%20Ping%20Tan%20and%20Xiaoyang%20Guo%0AAbstract%3A%20%20%20Scene%20regression%20methods%2C%20such%20as%20VGGT%2C%20solve%20the%20Structure-from-Motion%20%28SfM%29%0Aproblem%20by%20directly%20regressing%20camera%20poses%20and%203D%20scene%20structures%20from%20input%0Aimages.%20They%20demonstrate%20impressive%20performance%20in%20handling%20images%20under%0Aextreme%20viewpoint%20changes.%20However%2C%20these%20methods%20struggle%20to%20handle%20a%20large%0Anumber%20of%20input%20images.%20To%20address%20this%20problem%2C%20we%20introduce%20SAIL-Recon%2C%20a%0Afeed-forward%20Transformer%20for%20large%20scale%20SfM%2C%20by%20augmenting%20the%20scene%0Aregression%20network%20with%20visual%20localization%20capabilities.%20Specifically%2C%20our%0Amethod%20first%20computes%20a%20neural%20scene%20representation%20from%20a%20subset%20of%20anchor%0Aimages.%20The%20regression%20network%20is%20then%20fine-tuned%20to%20reconstruct%20all%20input%0Aimages%20conditioned%20on%20this%20neural%20scene%20representation.%20Comprehensive%0Aexperiments%20show%20that%20our%20method%20not%20only%20scales%20efficiently%20to%20large-scale%0Ascenes%2C%20but%20also%20achieves%20state-of-the-art%20results%20on%20both%20camera%20pose%0Aestimation%20and%20novel%20view%20synthesis%20benchmarks%2C%20including%20TUM-RGBD%2C%20CO3Dv2%2C%20and%0ATanks%20%26%20Temples.%20We%20will%20publish%20our%20model%20and%20code.%20Code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//hkust-sail.github.io/%20sail-recon/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAIL-Recon%253A%2520Large%2520SfM%2520by%2520Augmenting%2520Scene%2520Regression%2520with%2520Localization%26entry.906535625%3DJunyuan%2520Deng%2520and%2520Heng%2520Li%2520and%2520Tao%2520Xie%2520and%2520Weiqiang%2520Ren%2520and%2520Qian%2520Zhang%2520and%2520Ping%2520Tan%2520and%2520Xiaoyang%2520Guo%26entry.1292438233%3D%2520%2520Scene%2520regression%2520methods%252C%2520such%2520as%2520VGGT%252C%2520solve%2520the%2520Structure-from-Motion%2520%2528SfM%2529%250Aproblem%2520by%2520directly%2520regressing%2520camera%2520poses%2520and%25203D%2520scene%2520structures%2520from%2520input%250Aimages.%2520They%2520demonstrate%2520impressive%2520performance%2520in%2520handling%2520images%2520under%250Aextreme%2520viewpoint%2520changes.%2520However%252C%2520these%2520methods%2520struggle%2520to%2520handle%2520a%2520large%250Anumber%2520of%2520input%2520images.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520SAIL-Recon%252C%2520a%250Afeed-forward%2520Transformer%2520for%2520large%2520scale%2520SfM%252C%2520by%2520augmenting%2520the%2520scene%250Aregression%2520network%2520with%2520visual%2520localization%2520capabilities.%2520Specifically%252C%2520our%250Amethod%2520first%2520computes%2520a%2520neural%2520scene%2520representation%2520from%2520a%2520subset%2520of%2520anchor%250Aimages.%2520The%2520regression%2520network%2520is%2520then%2520fine-tuned%2520to%2520reconstruct%2520all%2520input%250Aimages%2520conditioned%2520on%2520this%2520neural%2520scene%2520representation.%2520Comprehensive%250Aexperiments%2520show%2520that%2520our%2520method%2520not%2520only%2520scales%2520efficiently%2520to%2520large-scale%250Ascenes%252C%2520but%2520also%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520camera%2520pose%250Aestimation%2520and%2520novel%2520view%2520synthesis%2520benchmarks%252C%2520including%2520TUM-RGBD%252C%2520CO3Dv2%252C%2520and%250ATanks%2520%2526%2520Temples.%2520We%2520will%2520publish%2520our%2520model%2520and%2520code.%2520Code%2520and%2520models%2520are%250Apublicly%2520available%2520at%253A%2520https%253A//hkust-sail.github.io/%2520sail-recon/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAIL-Recon%3A%20Large%20SfM%20by%20Augmenting%20Scene%20Regression%20with%20Localization&entry.906535625=Junyuan%20Deng%20and%20Heng%20Li%20and%20Tao%20Xie%20and%20Weiqiang%20Ren%20and%20Qian%20Zhang%20and%20Ping%20Tan%20and%20Xiaoyang%20Guo&entry.1292438233=%20%20Scene%20regression%20methods%2C%20such%20as%20VGGT%2C%20solve%20the%20Structure-from-Motion%20%28SfM%29%0Aproblem%20by%20directly%20regressing%20camera%20poses%20and%203D%20scene%20structures%20from%20input%0Aimages.%20They%20demonstrate%20impressive%20performance%20in%20handling%20images%20under%0Aextreme%20viewpoint%20changes.%20However%2C%20these%20methods%20struggle%20to%20handle%20a%20large%0Anumber%20of%20input%20images.%20To%20address%20this%20problem%2C%20we%20introduce%20SAIL-Recon%2C%20a%0Afeed-forward%20Transformer%20for%20large%20scale%20SfM%2C%20by%20augmenting%20the%20scene%0Aregression%20network%20with%20visual%20localization%20capabilities.%20Specifically%2C%20our%0Amethod%20first%20computes%20a%20neural%20scene%20representation%20from%20a%20subset%20of%20anchor%0Aimages.%20The%20regression%20network%20is%20then%20fine-tuned%20to%20reconstruct%20all%20input%0Aimages%20conditioned%20on%20this%20neural%20scene%20representation.%20Comprehensive%0Aexperiments%20show%20that%20our%20method%20not%20only%20scales%20efficiently%20to%20large-scale%0Ascenes%2C%20but%20also%20achieves%20state-of-the-art%20results%20on%20both%20camera%20pose%0Aestimation%20and%20novel%20view%20synthesis%20benchmarks%2C%20including%20TUM-RGBD%2C%20CO3Dv2%2C%20and%0ATanks%20%26%20Temples.%20We%20will%20publish%20our%20model%20and%20code.%20Code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//hkust-sail.github.io/%20sail-recon/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17972v1&entry.124074799=Read"},
{"title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding", "author": "Kuleen Sasse and Efsun Sarioglu Kayi and Arun Reddy", "abstract": "  Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.\n", "link": "http://arxiv.org/abs/2507.17047v2", "date": "2025-08-25", "relevancy": 2.3327, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Hybrid%20Captioner%20for%20Improved%20Long-form%20Video%20Understanding&body=Title%3A%20Controllable%20Hybrid%20Captioner%20for%20Improved%20Long-form%20Video%20Understanding%0AAuthor%3A%20Kuleen%20Sasse%20and%20Efsun%20Sarioglu%20Kayi%20and%20Arun%20Reddy%0AAbstract%3A%20%20%20Video%20data%2C%20especially%20long-form%20video%2C%20is%20extremely%20dense%20and%0Ahigh-dimensional.%20Text-based%20summaries%20of%20video%20content%20offer%20a%20way%20to%0Arepresent%20query-relevant%20content%20in%20a%20much%20more%20compact%20manner%20than%20raw%20video.%0AIn%20addition%2C%20textual%20representations%20are%20easily%20ingested%20by%20state-of-the-art%0Alarge%20language%20models%20%28LLMs%29%2C%20which%20enable%20reasoning%20over%20video%20content%20to%0Aanswer%20complex%20natural%20language%20queries.%20To%20solve%20this%20issue%2C%20we%20rely%20on%20the%0Aprogressive%20construction%20of%20a%20text-based%20memory%20by%20a%20video%20captioner%20operating%0Aon%20shorter%20chunks%20of%20the%20video%2C%20where%20spatio-temporal%20modeling%20is%0Acomputationally%20feasible.%20We%20explore%20ways%20to%20improve%20the%20quality%20of%20the%0Aactivity%20log%20comprised%20solely%20of%20short%20video%20captions.%20Because%20the%20video%0Acaptions%20tend%20to%20be%20focused%20on%20human%20actions%2C%20and%20questions%20may%20pertain%20to%0Aother%20information%20in%20the%20scene%2C%20we%20seek%20to%20enrich%20the%20memory%20with%20static%20scene%0Adescriptions%20using%20Vision%20Language%20Models%20%28VLMs%29.%20Our%20video%20understanding%0Asystem%20relies%20on%20the%20LaViLa%20video%20captioner%20in%20combination%20with%20a%20LLM%20to%20answer%0Aquestions%20about%20videos.%20We%20first%20explored%20different%20ways%20of%20partitioning%20the%0Avideo%20into%20meaningful%20segments%20such%20that%20the%20textual%20descriptions%20more%0Aaccurately%20reflect%20the%20structure%20of%20the%20video%20content.%20Furthermore%2C%20we%0Aincorporated%20static%20scene%20descriptions%20into%20the%20captioning%20pipeline%20using%20LLaVA%0AVLM%2C%20resulting%20in%20a%20more%20detailed%20and%20complete%20caption%20log%20and%20expanding%20the%0Aspace%20of%20questions%20that%20are%20answerable%20from%20the%20textual%20memory.%20Finally%2C%20we%0Ahave%20successfully%20fine-tuned%20the%20LaViLa%20video%20captioner%20to%20produce%20both%20action%0Aand%20scene%20captions%2C%20significantly%20improving%20the%20efficiency%20of%20the%20captioning%0Apipeline%20compared%20to%20using%20separate%20captioning%20models%20for%20the%20two%20tasks.%20Our%0Amodel%2C%20controllable%20hybrid%20captioner%2C%20can%20alternate%20between%20different%20types%20of%0Acaptions%20according%20to%20special%20input%20tokens%20that%20signals%20scene%20changes%20detected%0Ain%20the%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Hybrid%2520Captioner%2520for%2520Improved%2520Long-form%2520Video%2520Understanding%26entry.906535625%3DKuleen%2520Sasse%2520and%2520Efsun%2520Sarioglu%2520Kayi%2520and%2520Arun%2520Reddy%26entry.1292438233%3D%2520%2520Video%2520data%252C%2520especially%2520long-form%2520video%252C%2520is%2520extremely%2520dense%2520and%250Ahigh-dimensional.%2520Text-based%2520summaries%2520of%2520video%2520content%2520offer%2520a%2520way%2520to%250Arepresent%2520query-relevant%2520content%2520in%2520a%2520much%2520more%2520compact%2520manner%2520than%2520raw%2520video.%250AIn%2520addition%252C%2520textual%2520representations%2520are%2520easily%2520ingested%2520by%2520state-of-the-art%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520enable%2520reasoning%2520over%2520video%2520content%2520to%250Aanswer%2520complex%2520natural%2520language%2520queries.%2520To%2520solve%2520this%2520issue%252C%2520we%2520rely%2520on%2520the%250Aprogressive%2520construction%2520of%2520a%2520text-based%2520memory%2520by%2520a%2520video%2520captioner%2520operating%250Aon%2520shorter%2520chunks%2520of%2520the%2520video%252C%2520where%2520spatio-temporal%2520modeling%2520is%250Acomputationally%2520feasible.%2520We%2520explore%2520ways%2520to%2520improve%2520the%2520quality%2520of%2520the%250Aactivity%2520log%2520comprised%2520solely%2520of%2520short%2520video%2520captions.%2520Because%2520the%2520video%250Acaptions%2520tend%2520to%2520be%2520focused%2520on%2520human%2520actions%252C%2520and%2520questions%2520may%2520pertain%2520to%250Aother%2520information%2520in%2520the%2520scene%252C%2520we%2520seek%2520to%2520enrich%2520the%2520memory%2520with%2520static%2520scene%250Adescriptions%2520using%2520Vision%2520Language%2520Models%2520%2528VLMs%2529.%2520Our%2520video%2520understanding%250Asystem%2520relies%2520on%2520the%2520LaViLa%2520video%2520captioner%2520in%2520combination%2520with%2520a%2520LLM%2520to%2520answer%250Aquestions%2520about%2520videos.%2520We%2520first%2520explored%2520different%2520ways%2520of%2520partitioning%2520the%250Avideo%2520into%2520meaningful%2520segments%2520such%2520that%2520the%2520textual%2520descriptions%2520more%250Aaccurately%2520reflect%2520the%2520structure%2520of%2520the%2520video%2520content.%2520Furthermore%252C%2520we%250Aincorporated%2520static%2520scene%2520descriptions%2520into%2520the%2520captioning%2520pipeline%2520using%2520LLaVA%250AVLM%252C%2520resulting%2520in%2520a%2520more%2520detailed%2520and%2520complete%2520caption%2520log%2520and%2520expanding%2520the%250Aspace%2520of%2520questions%2520that%2520are%2520answerable%2520from%2520the%2520textual%2520memory.%2520Finally%252C%2520we%250Ahave%2520successfully%2520fine-tuned%2520the%2520LaViLa%2520video%2520captioner%2520to%2520produce%2520both%2520action%250Aand%2520scene%2520captions%252C%2520significantly%2520improving%2520the%2520efficiency%2520of%2520the%2520captioning%250Apipeline%2520compared%2520to%2520using%2520separate%2520captioning%2520models%2520for%2520the%2520two%2520tasks.%2520Our%250Amodel%252C%2520controllable%2520hybrid%2520captioner%252C%2520can%2520alternate%2520between%2520different%2520types%2520of%250Acaptions%2520according%2520to%2520special%2520input%2520tokens%2520that%2520signals%2520scene%2520changes%2520detected%250Ain%2520the%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Hybrid%20Captioner%20for%20Improved%20Long-form%20Video%20Understanding&entry.906535625=Kuleen%20Sasse%20and%20Efsun%20Sarioglu%20Kayi%20and%20Arun%20Reddy&entry.1292438233=%20%20Video%20data%2C%20especially%20long-form%20video%2C%20is%20extremely%20dense%20and%0Ahigh-dimensional.%20Text-based%20summaries%20of%20video%20content%20offer%20a%20way%20to%0Arepresent%20query-relevant%20content%20in%20a%20much%20more%20compact%20manner%20than%20raw%20video.%0AIn%20addition%2C%20textual%20representations%20are%20easily%20ingested%20by%20state-of-the-art%0Alarge%20language%20models%20%28LLMs%29%2C%20which%20enable%20reasoning%20over%20video%20content%20to%0Aanswer%20complex%20natural%20language%20queries.%20To%20solve%20this%20issue%2C%20we%20rely%20on%20the%0Aprogressive%20construction%20of%20a%20text-based%20memory%20by%20a%20video%20captioner%20operating%0Aon%20shorter%20chunks%20of%20the%20video%2C%20where%20spatio-temporal%20modeling%20is%0Acomputationally%20feasible.%20We%20explore%20ways%20to%20improve%20the%20quality%20of%20the%0Aactivity%20log%20comprised%20solely%20of%20short%20video%20captions.%20Because%20the%20video%0Acaptions%20tend%20to%20be%20focused%20on%20human%20actions%2C%20and%20questions%20may%20pertain%20to%0Aother%20information%20in%20the%20scene%2C%20we%20seek%20to%20enrich%20the%20memory%20with%20static%20scene%0Adescriptions%20using%20Vision%20Language%20Models%20%28VLMs%29.%20Our%20video%20understanding%0Asystem%20relies%20on%20the%20LaViLa%20video%20captioner%20in%20combination%20with%20a%20LLM%20to%20answer%0Aquestions%20about%20videos.%20We%20first%20explored%20different%20ways%20of%20partitioning%20the%0Avideo%20into%20meaningful%20segments%20such%20that%20the%20textual%20descriptions%20more%0Aaccurately%20reflect%20the%20structure%20of%20the%20video%20content.%20Furthermore%2C%20we%0Aincorporated%20static%20scene%20descriptions%20into%20the%20captioning%20pipeline%20using%20LLaVA%0AVLM%2C%20resulting%20in%20a%20more%20detailed%20and%20complete%20caption%20log%20and%20expanding%20the%0Aspace%20of%20questions%20that%20are%20answerable%20from%20the%20textual%20memory.%20Finally%2C%20we%0Ahave%20successfully%20fine-tuned%20the%20LaViLa%20video%20captioner%20to%20produce%20both%20action%0Aand%20scene%20captions%2C%20significantly%20improving%20the%20efficiency%20of%20the%20captioning%0Apipeline%20compared%20to%20using%20separate%20captioning%20models%20for%20the%20two%20tasks.%20Our%0Amodel%2C%20controllable%20hybrid%20captioner%2C%20can%20alternate%20between%20different%20types%20of%0Acaptions%20according%20to%20special%20input%20tokens%20that%20signals%20scene%20changes%20detected%0Ain%20the%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17047v2&entry.124074799=Read"},
{"title": "Deep Learning-based Cross-modal Reconstruction of Vehicle Target from\n  Sparse 3D SAR Image", "author": "Da Li and Guoqiang Zhao and Chen Yao and Kaiqiang Zhu and Houjun Sun and Jiacheng Bao and Maokun Li", "abstract": "  Three-dimensional synthetic aperture radar (3D SAR) is an advanced active\nmicrowave imaging technology widely utilized in remote sensing area. To achieve\nhigh-resolution 3D imaging,3D SAR requires observations from multiple aspects\nand altitude baselines surrounding the target. However, constrained flight\ntrajectories often lead to sparse observations, which degrade imaging quality,\nparticularly for anisotropic man-made small targets, such as vehicles and\naircraft. In the past, compressive sensing (CS) was the mainstream approach for\nsparse 3D SAR image reconstruction. More recently, deep learning (DL) has\nemerged as a powerful alternative, markedly boosting reconstruction quality and\nefficiency. However, existing DL-based methods typically rely solely on\nhigh-quality 3D SAR images as supervisory signals to train deep neural networks\n(DNNs). This unimodal learning paradigm prevents the integration of\ncomplementary information from other data modalities, which limits\nreconstruction performance and reduces target discriminability due to the\ninherent constraints of electromagnetic scattering. In this paper, we introduce\ncross-modal learning and propose a Cross-Modal 3D-SAR Reconstruction Network\n(CMAR-Net) for enhancing sparse 3D SAR images of vehicle targets by fusing\noptical information. Leveraging cross-modal supervision from 2D optical images\nand error propagation guaranteed by differentiable rendering, CMAR-Net achieves\nefficient training and reconstructs sparse 3D SAR images, which are derived\nfrom highly sparse-aspect observations, into visually structured 3D vehicle\nimages. Trained exclusively on simulated data, CMAR-Net exhibits robust\ngeneralization to real-world data, outperforming state-of-the-art CS and DL\nmethods in structural accuracy within a large-scale parking lot experiment\ninvolving numerous civilian vehicles, thereby demonstrating its strong\npractical applicability.\n", "link": "http://arxiv.org/abs/2406.04158v6", "date": "2025-08-25", "relevancy": 2.3131, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5923}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5692}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Cross-modal%20Reconstruction%20of%20Vehicle%20Target%20from%0A%20%20Sparse%203D%20SAR%20Image&body=Title%3A%20Deep%20Learning-based%20Cross-modal%20Reconstruction%20of%20Vehicle%20Target%20from%0A%20%20Sparse%203D%20SAR%20Image%0AAuthor%3A%20Da%20Li%20and%20Guoqiang%20Zhao%20and%20Chen%20Yao%20and%20Kaiqiang%20Zhu%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao%20and%20Maokun%20Li%0AAbstract%3A%20%20%20Three-dimensional%20synthetic%20aperture%20radar%20%283D%20SAR%29%20is%20an%20advanced%20active%0Amicrowave%20imaging%20technology%20widely%20utilized%20in%20remote%20sensing%20area.%20To%20achieve%0Ahigh-resolution%203D%20imaging%2C3D%20SAR%20requires%20observations%20from%20multiple%20aspects%0Aand%20altitude%20baselines%20surrounding%20the%20target.%20However%2C%20constrained%20flight%0Atrajectories%20often%20lead%20to%20sparse%20observations%2C%20which%20degrade%20imaging%20quality%2C%0Aparticularly%20for%20anisotropic%20man-made%20small%20targets%2C%20such%20as%20vehicles%20and%0Aaircraft.%20In%20the%20past%2C%20compressive%20sensing%20%28CS%29%20was%20the%20mainstream%20approach%20for%0Asparse%203D%20SAR%20image%20reconstruction.%20More%20recently%2C%20deep%20learning%20%28DL%29%20has%0Aemerged%20as%20a%20powerful%20alternative%2C%20markedly%20boosting%20reconstruction%20quality%20and%0Aefficiency.%20However%2C%20existing%20DL-based%20methods%20typically%20rely%20solely%20on%0Ahigh-quality%203D%20SAR%20images%20as%20supervisory%20signals%20to%20train%20deep%20neural%20networks%0A%28DNNs%29.%20This%20unimodal%20learning%20paradigm%20prevents%20the%20integration%20of%0Acomplementary%20information%20from%20other%20data%20modalities%2C%20which%20limits%0Areconstruction%20performance%20and%20reduces%20target%20discriminability%20due%20to%20the%0Ainherent%20constraints%20of%20electromagnetic%20scattering.%20In%20this%20paper%2C%20we%20introduce%0Across-modal%20learning%20and%20propose%20a%20Cross-Modal%203D-SAR%20Reconstruction%20Network%0A%28CMAR-Net%29%20for%20enhancing%20sparse%203D%20SAR%20images%20of%20vehicle%20targets%20by%20fusing%0Aoptical%20information.%20Leveraging%20cross-modal%20supervision%20from%202D%20optical%20images%0Aand%20error%20propagation%20guaranteed%20by%20differentiable%20rendering%2C%20CMAR-Net%20achieves%0Aefficient%20training%20and%20reconstructs%20sparse%203D%20SAR%20images%2C%20which%20are%20derived%0Afrom%20highly%20sparse-aspect%20observations%2C%20into%20visually%20structured%203D%20vehicle%0Aimages.%20Trained%20exclusively%20on%20simulated%20data%2C%20CMAR-Net%20exhibits%20robust%0Ageneralization%20to%20real-world%20data%2C%20outperforming%20state-of-the-art%20CS%20and%20DL%0Amethods%20in%20structural%20accuracy%20within%20a%20large-scale%20parking%20lot%20experiment%0Ainvolving%20numerous%20civilian%20vehicles%2C%20thereby%20demonstrating%20its%20strong%0Apractical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04158v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Cross-modal%2520Reconstruction%2520of%2520Vehicle%2520Target%2520from%250A%2520%2520Sparse%25203D%2520SAR%2520Image%26entry.906535625%3DDa%2520Li%2520and%2520Guoqiang%2520Zhao%2520and%2520Chen%2520Yao%2520and%2520Kaiqiang%2520Zhu%2520and%2520Houjun%2520Sun%2520and%2520Jiacheng%2520Bao%2520and%2520Maokun%2520Li%26entry.1292438233%3D%2520%2520Three-dimensional%2520synthetic%2520aperture%2520radar%2520%25283D%2520SAR%2529%2520is%2520an%2520advanced%2520active%250Amicrowave%2520imaging%2520technology%2520widely%2520utilized%2520in%2520remote%2520sensing%2520area.%2520To%2520achieve%250Ahigh-resolution%25203D%2520imaging%252C3D%2520SAR%2520requires%2520observations%2520from%2520multiple%2520aspects%250Aand%2520altitude%2520baselines%2520surrounding%2520the%2520target.%2520However%252C%2520constrained%2520flight%250Atrajectories%2520often%2520lead%2520to%2520sparse%2520observations%252C%2520which%2520degrade%2520imaging%2520quality%252C%250Aparticularly%2520for%2520anisotropic%2520man-made%2520small%2520targets%252C%2520such%2520as%2520vehicles%2520and%250Aaircraft.%2520In%2520the%2520past%252C%2520compressive%2520sensing%2520%2528CS%2529%2520was%2520the%2520mainstream%2520approach%2520for%250Asparse%25203D%2520SAR%2520image%2520reconstruction.%2520More%2520recently%252C%2520deep%2520learning%2520%2528DL%2529%2520has%250Aemerged%2520as%2520a%2520powerful%2520alternative%252C%2520markedly%2520boosting%2520reconstruction%2520quality%2520and%250Aefficiency.%2520However%252C%2520existing%2520DL-based%2520methods%2520typically%2520rely%2520solely%2520on%250Ahigh-quality%25203D%2520SAR%2520images%2520as%2520supervisory%2520signals%2520to%2520train%2520deep%2520neural%2520networks%250A%2528DNNs%2529.%2520This%2520unimodal%2520learning%2520paradigm%2520prevents%2520the%2520integration%2520of%250Acomplementary%2520information%2520from%2520other%2520data%2520modalities%252C%2520which%2520limits%250Areconstruction%2520performance%2520and%2520reduces%2520target%2520discriminability%2520due%2520to%2520the%250Ainherent%2520constraints%2520of%2520electromagnetic%2520scattering.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Across-modal%2520learning%2520and%2520propose%2520a%2520Cross-Modal%25203D-SAR%2520Reconstruction%2520Network%250A%2528CMAR-Net%2529%2520for%2520enhancing%2520sparse%25203D%2520SAR%2520images%2520of%2520vehicle%2520targets%2520by%2520fusing%250Aoptical%2520information.%2520Leveraging%2520cross-modal%2520supervision%2520from%25202D%2520optical%2520images%250Aand%2520error%2520propagation%2520guaranteed%2520by%2520differentiable%2520rendering%252C%2520CMAR-Net%2520achieves%250Aefficient%2520training%2520and%2520reconstructs%2520sparse%25203D%2520SAR%2520images%252C%2520which%2520are%2520derived%250Afrom%2520highly%2520sparse-aspect%2520observations%252C%2520into%2520visually%2520structured%25203D%2520vehicle%250Aimages.%2520Trained%2520exclusively%2520on%2520simulated%2520data%252C%2520CMAR-Net%2520exhibits%2520robust%250Ageneralization%2520to%2520real-world%2520data%252C%2520outperforming%2520state-of-the-art%2520CS%2520and%2520DL%250Amethods%2520in%2520structural%2520accuracy%2520within%2520a%2520large-scale%2520parking%2520lot%2520experiment%250Ainvolving%2520numerous%2520civilian%2520vehicles%252C%2520thereby%2520demonstrating%2520its%2520strong%250Apractical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04158v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Cross-modal%20Reconstruction%20of%20Vehicle%20Target%20from%0A%20%20Sparse%203D%20SAR%20Image&entry.906535625=Da%20Li%20and%20Guoqiang%20Zhao%20and%20Chen%20Yao%20and%20Kaiqiang%20Zhu%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao%20and%20Maokun%20Li&entry.1292438233=%20%20Three-dimensional%20synthetic%20aperture%20radar%20%283D%20SAR%29%20is%20an%20advanced%20active%0Amicrowave%20imaging%20technology%20widely%20utilized%20in%20remote%20sensing%20area.%20To%20achieve%0Ahigh-resolution%203D%20imaging%2C3D%20SAR%20requires%20observations%20from%20multiple%20aspects%0Aand%20altitude%20baselines%20surrounding%20the%20target.%20However%2C%20constrained%20flight%0Atrajectories%20often%20lead%20to%20sparse%20observations%2C%20which%20degrade%20imaging%20quality%2C%0Aparticularly%20for%20anisotropic%20man-made%20small%20targets%2C%20such%20as%20vehicles%20and%0Aaircraft.%20In%20the%20past%2C%20compressive%20sensing%20%28CS%29%20was%20the%20mainstream%20approach%20for%0Asparse%203D%20SAR%20image%20reconstruction.%20More%20recently%2C%20deep%20learning%20%28DL%29%20has%0Aemerged%20as%20a%20powerful%20alternative%2C%20markedly%20boosting%20reconstruction%20quality%20and%0Aefficiency.%20However%2C%20existing%20DL-based%20methods%20typically%20rely%20solely%20on%0Ahigh-quality%203D%20SAR%20images%20as%20supervisory%20signals%20to%20train%20deep%20neural%20networks%0A%28DNNs%29.%20This%20unimodal%20learning%20paradigm%20prevents%20the%20integration%20of%0Acomplementary%20information%20from%20other%20data%20modalities%2C%20which%20limits%0Areconstruction%20performance%20and%20reduces%20target%20discriminability%20due%20to%20the%0Ainherent%20constraints%20of%20electromagnetic%20scattering.%20In%20this%20paper%2C%20we%20introduce%0Across-modal%20learning%20and%20propose%20a%20Cross-Modal%203D-SAR%20Reconstruction%20Network%0A%28CMAR-Net%29%20for%20enhancing%20sparse%203D%20SAR%20images%20of%20vehicle%20targets%20by%20fusing%0Aoptical%20information.%20Leveraging%20cross-modal%20supervision%20from%202D%20optical%20images%0Aand%20error%20propagation%20guaranteed%20by%20differentiable%20rendering%2C%20CMAR-Net%20achieves%0Aefficient%20training%20and%20reconstructs%20sparse%203D%20SAR%20images%2C%20which%20are%20derived%0Afrom%20highly%20sparse-aspect%20observations%2C%20into%20visually%20structured%203D%20vehicle%0Aimages.%20Trained%20exclusively%20on%20simulated%20data%2C%20CMAR-Net%20exhibits%20robust%0Ageneralization%20to%20real-world%20data%2C%20outperforming%20state-of-the-art%20CS%20and%20DL%0Amethods%20in%20structural%20accuracy%20within%20a%20large-scale%20parking%20lot%20experiment%0Ainvolving%20numerous%20civilian%20vehicles%2C%20thereby%20demonstrating%20its%20strong%0Apractical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04158v6&entry.124074799=Read"},
{"title": "Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving", "author": "Md Shahi Amran Hossain and Abu Shad Ahammed and Sayeri Mukherjee and Roman Obermaisser", "abstract": "  The use of computer vision in automotive is a trending research in which\nsafety and security are a primary concern. In particular, for autonomous\ndriving, preventing road accidents requires highly accurate object detection\nunder diverse conditions. To address this issue, recently the International\nOrganization for Standardization (ISO) released the 8800 norm, providing\nstructured frameworks for managing associated AI relevant risks. However,\nchallenging scenarios such as adverse weather or low lighting often introduce\ndata drift, leading to degraded model performance and potential safety\nviolations. In this work, we present a novel hybrid computer vision\narchitecture trained with thousands of synthetic image data from the road\nenvironment to improve robustness in unseen drifted environments. Our dual mode\nframework utilized YOLO version 8 for swift detection and incorporated a\nfive-layer CNN for verification. The system functioned in sequence and improved\nthe detection accuracy by more than 90\\% when tested with drift-augmented road\nimages. The focus was to demonstrate how such a hybrid model can provide better\nroad safety when working together in a hybrid structure.\n", "link": "http://arxiv.org/abs/2508.17975v1", "date": "2025-08-25", "relevancy": 2.3036, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5854}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5742}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Drift-Aware%20Computer%20Vision%20Architecture%20for%20Autonomous%20Driving&body=Title%3A%20Enhanced%20Drift-Aware%20Computer%20Vision%20Architecture%20for%20Autonomous%20Driving%0AAuthor%3A%20Md%20Shahi%20Amran%20Hossain%20and%20Abu%20Shad%20Ahammed%20and%20Sayeri%20Mukherjee%20and%20Roman%20Obermaisser%0AAbstract%3A%20%20%20The%20use%20of%20computer%20vision%20in%20automotive%20is%20a%20trending%20research%20in%20which%0Asafety%20and%20security%20are%20a%20primary%20concern.%20In%20particular%2C%20for%20autonomous%0Adriving%2C%20preventing%20road%20accidents%20requires%20highly%20accurate%20object%20detection%0Aunder%20diverse%20conditions.%20To%20address%20this%20issue%2C%20recently%20the%20International%0AOrganization%20for%20Standardization%20%28ISO%29%20released%20the%208800%20norm%2C%20providing%0Astructured%20frameworks%20for%20managing%20associated%20AI%20relevant%20risks.%20However%2C%0Achallenging%20scenarios%20such%20as%20adverse%20weather%20or%20low%20lighting%20often%20introduce%0Adata%20drift%2C%20leading%20to%20degraded%20model%20performance%20and%20potential%20safety%0Aviolations.%20In%20this%20work%2C%20we%20present%20a%20novel%20hybrid%20computer%20vision%0Aarchitecture%20trained%20with%20thousands%20of%20synthetic%20image%20data%20from%20the%20road%0Aenvironment%20to%20improve%20robustness%20in%20unseen%20drifted%20environments.%20Our%20dual%20mode%0Aframework%20utilized%20YOLO%20version%208%20for%20swift%20detection%20and%20incorporated%20a%0Afive-layer%20CNN%20for%20verification.%20The%20system%20functioned%20in%20sequence%20and%20improved%0Athe%20detection%20accuracy%20by%20more%20than%2090%5C%25%20when%20tested%20with%20drift-augmented%20road%0Aimages.%20The%20focus%20was%20to%20demonstrate%20how%20such%20a%20hybrid%20model%20can%20provide%20better%0Aroad%20safety%20when%20working%20together%20in%20a%20hybrid%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Drift-Aware%2520Computer%2520Vision%2520Architecture%2520for%2520Autonomous%2520Driving%26entry.906535625%3DMd%2520Shahi%2520Amran%2520Hossain%2520and%2520Abu%2520Shad%2520Ahammed%2520and%2520Sayeri%2520Mukherjee%2520and%2520Roman%2520Obermaisser%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520computer%2520vision%2520in%2520automotive%2520is%2520a%2520trending%2520research%2520in%2520which%250Asafety%2520and%2520security%2520are%2520a%2520primary%2520concern.%2520In%2520particular%252C%2520for%2520autonomous%250Adriving%252C%2520preventing%2520road%2520accidents%2520requires%2520highly%2520accurate%2520object%2520detection%250Aunder%2520diverse%2520conditions.%2520To%2520address%2520this%2520issue%252C%2520recently%2520the%2520International%250AOrganization%2520for%2520Standardization%2520%2528ISO%2529%2520released%2520the%25208800%2520norm%252C%2520providing%250Astructured%2520frameworks%2520for%2520managing%2520associated%2520AI%2520relevant%2520risks.%2520However%252C%250Achallenging%2520scenarios%2520such%2520as%2520adverse%2520weather%2520or%2520low%2520lighting%2520often%2520introduce%250Adata%2520drift%252C%2520leading%2520to%2520degraded%2520model%2520performance%2520and%2520potential%2520safety%250Aviolations.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520hybrid%2520computer%2520vision%250Aarchitecture%2520trained%2520with%2520thousands%2520of%2520synthetic%2520image%2520data%2520from%2520the%2520road%250Aenvironment%2520to%2520improve%2520robustness%2520in%2520unseen%2520drifted%2520environments.%2520Our%2520dual%2520mode%250Aframework%2520utilized%2520YOLO%2520version%25208%2520for%2520swift%2520detection%2520and%2520incorporated%2520a%250Afive-layer%2520CNN%2520for%2520verification.%2520The%2520system%2520functioned%2520in%2520sequence%2520and%2520improved%250Athe%2520detection%2520accuracy%2520by%2520more%2520than%252090%255C%2525%2520when%2520tested%2520with%2520drift-augmented%2520road%250Aimages.%2520The%2520focus%2520was%2520to%2520demonstrate%2520how%2520such%2520a%2520hybrid%2520model%2520can%2520provide%2520better%250Aroad%2520safety%2520when%2520working%2520together%2520in%2520a%2520hybrid%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Drift-Aware%20Computer%20Vision%20Architecture%20for%20Autonomous%20Driving&entry.906535625=Md%20Shahi%20Amran%20Hossain%20and%20Abu%20Shad%20Ahammed%20and%20Sayeri%20Mukherjee%20and%20Roman%20Obermaisser&entry.1292438233=%20%20The%20use%20of%20computer%20vision%20in%20automotive%20is%20a%20trending%20research%20in%20which%0Asafety%20and%20security%20are%20a%20primary%20concern.%20In%20particular%2C%20for%20autonomous%0Adriving%2C%20preventing%20road%20accidents%20requires%20highly%20accurate%20object%20detection%0Aunder%20diverse%20conditions.%20To%20address%20this%20issue%2C%20recently%20the%20International%0AOrganization%20for%20Standardization%20%28ISO%29%20released%20the%208800%20norm%2C%20providing%0Astructured%20frameworks%20for%20managing%20associated%20AI%20relevant%20risks.%20However%2C%0Achallenging%20scenarios%20such%20as%20adverse%20weather%20or%20low%20lighting%20often%20introduce%0Adata%20drift%2C%20leading%20to%20degraded%20model%20performance%20and%20potential%20safety%0Aviolations.%20In%20this%20work%2C%20we%20present%20a%20novel%20hybrid%20computer%20vision%0Aarchitecture%20trained%20with%20thousands%20of%20synthetic%20image%20data%20from%20the%20road%0Aenvironment%20to%20improve%20robustness%20in%20unseen%20drifted%20environments.%20Our%20dual%20mode%0Aframework%20utilized%20YOLO%20version%208%20for%20swift%20detection%20and%20incorporated%20a%0Afive-layer%20CNN%20for%20verification.%20The%20system%20functioned%20in%20sequence%20and%20improved%0Athe%20detection%20accuracy%20by%20more%20than%2090%5C%25%20when%20tested%20with%20drift-augmented%20road%0Aimages.%20The%20focus%20was%20to%20demonstrate%20how%20such%20a%20hybrid%20model%20can%20provide%20better%0Aroad%20safety%20when%20working%20together%20in%20a%20hybrid%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17975v1&entry.124074799=Read"},
{"title": "EventTracer: Fast Path Tracing-based Event Stream Rendering", "author": "Zhenyang Li and Xiaoyang Bai and Jinfan Lu and Pengfei Shen and Edmund Y. Lam and Yifan Peng", "abstract": "  Simulating event streams from 3D scenes has become a common practice in\nevent-based vision research, as it meets the demand for large-scale, high\ntemporal frequency data without setting up expensive hardware devices or\nundertaking extensive data collections. Yet existing methods in this direction\ntypically work with noiseless RGB frames that are costly to render, and\ntherefore they can only achieve a temporal resolution equivalent to 100-300\nFPS, far lower than that of real-world event data. In this work, we propose\nEventTracer, a path tracing-based rendering pipeline that simulates\nhigh-fidelity event sequences from complex 3D scenes in an efficient and\nphysics-aware manner. Specifically, we speed up the rendering process via low\nsample-per-pixel (SPP) path tracing, and train a lightweight event spiking\nnetwork to denoise the resulting RGB videos into realistic event sequences. To\ncapture the physical properties of event streams, the network is equipped with\na bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a\nbidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at\na speed of about 4 minutes per second of 720p video, and it inherits the merit\nof accurate spatiotemporal modeling from its path tracing backbone. We show in\ntwo downstream tasks that EventTracer captures better scene details and\ndemonstrates a greater similarity to real-world event data than other event\nsimulators, which establishes it as a promising tool for creating large-scale\nevent-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based\nvision, and boosting various application scenarios such as robotics, autonomous\ndriving, and VRAR.\n", "link": "http://arxiv.org/abs/2508.18071v1", "date": "2025-08-25", "relevancy": 2.3022, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6083}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5761}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventTracer%3A%20Fast%20Path%20Tracing-based%20Event%20Stream%20Rendering&body=Title%3A%20EventTracer%3A%20Fast%20Path%20Tracing-based%20Event%20Stream%20Rendering%0AAuthor%3A%20Zhenyang%20Li%20and%20Xiaoyang%20Bai%20and%20Jinfan%20Lu%20and%20Pengfei%20Shen%20and%20Edmund%20Y.%20Lam%20and%20Yifan%20Peng%0AAbstract%3A%20%20%20Simulating%20event%20streams%20from%203D%20scenes%20has%20become%20a%20common%20practice%20in%0Aevent-based%20vision%20research%2C%20as%20it%20meets%20the%20demand%20for%20large-scale%2C%20high%0Atemporal%20frequency%20data%20without%20setting%20up%20expensive%20hardware%20devices%20or%0Aundertaking%20extensive%20data%20collections.%20Yet%20existing%20methods%20in%20this%20direction%0Atypically%20work%20with%20noiseless%20RGB%20frames%20that%20are%20costly%20to%20render%2C%20and%0Atherefore%20they%20can%20only%20achieve%20a%20temporal%20resolution%20equivalent%20to%20100-300%0AFPS%2C%20far%20lower%20than%20that%20of%20real-world%20event%20data.%20In%20this%20work%2C%20we%20propose%0AEventTracer%2C%20a%20path%20tracing-based%20rendering%20pipeline%20that%20simulates%0Ahigh-fidelity%20event%20sequences%20from%20complex%203D%20scenes%20in%20an%20efficient%20and%0Aphysics-aware%20manner.%20Specifically%2C%20we%20speed%20up%20the%20rendering%20process%20via%20low%0Asample-per-pixel%20%28SPP%29%20path%20tracing%2C%20and%20train%20a%20lightweight%20event%20spiking%0Anetwork%20to%20denoise%20the%20resulting%20RGB%20videos%20into%20realistic%20event%20sequences.%20To%0Acapture%20the%20physical%20properties%20of%20event%20streams%2C%20the%20network%20is%20equipped%20with%0Aa%20bipolar%20leaky%20integrate-and-fired%20%28BiLIF%29%20spiking%20unit%20and%20trained%20with%20a%0Abidirectional%20earth%20mover%20distance%20%28EMD%29%20loss.%20Our%20EventTracer%20pipeline%20runs%20at%0Aa%20speed%20of%20about%204%20minutes%20per%20second%20of%20720p%20video%2C%20and%20it%20inherits%20the%20merit%0Aof%20accurate%20spatiotemporal%20modeling%20from%20its%20path%20tracing%20backbone.%20We%20show%20in%0Atwo%20downstream%20tasks%20that%20EventTracer%20captures%20better%20scene%20details%20and%0Ademonstrates%20a%20greater%20similarity%20to%20real-world%20event%20data%20than%20other%20event%0Asimulators%2C%20which%20establishes%20it%20as%20a%20promising%20tool%20for%20creating%20large-scale%0Aevent-RGB%20datasets%20at%20a%20low%20cost%2C%20narrowing%20the%20sim-to-real%20gap%20in%20event-based%0Avision%2C%20and%20boosting%20various%20application%20scenarios%20such%20as%20robotics%2C%20autonomous%0Adriving%2C%20and%20VRAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventTracer%253A%2520Fast%2520Path%2520Tracing-based%2520Event%2520Stream%2520Rendering%26entry.906535625%3DZhenyang%2520Li%2520and%2520Xiaoyang%2520Bai%2520and%2520Jinfan%2520Lu%2520and%2520Pengfei%2520Shen%2520and%2520Edmund%2520Y.%2520Lam%2520and%2520Yifan%2520Peng%26entry.1292438233%3D%2520%2520Simulating%2520event%2520streams%2520from%25203D%2520scenes%2520has%2520become%2520a%2520common%2520practice%2520in%250Aevent-based%2520vision%2520research%252C%2520as%2520it%2520meets%2520the%2520demand%2520for%2520large-scale%252C%2520high%250Atemporal%2520frequency%2520data%2520without%2520setting%2520up%2520expensive%2520hardware%2520devices%2520or%250Aundertaking%2520extensive%2520data%2520collections.%2520Yet%2520existing%2520methods%2520in%2520this%2520direction%250Atypically%2520work%2520with%2520noiseless%2520RGB%2520frames%2520that%2520are%2520costly%2520to%2520render%252C%2520and%250Atherefore%2520they%2520can%2520only%2520achieve%2520a%2520temporal%2520resolution%2520equivalent%2520to%2520100-300%250AFPS%252C%2520far%2520lower%2520than%2520that%2520of%2520real-world%2520event%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%250AEventTracer%252C%2520a%2520path%2520tracing-based%2520rendering%2520pipeline%2520that%2520simulates%250Ahigh-fidelity%2520event%2520sequences%2520from%2520complex%25203D%2520scenes%2520in%2520an%2520efficient%2520and%250Aphysics-aware%2520manner.%2520Specifically%252C%2520we%2520speed%2520up%2520the%2520rendering%2520process%2520via%2520low%250Asample-per-pixel%2520%2528SPP%2529%2520path%2520tracing%252C%2520and%2520train%2520a%2520lightweight%2520event%2520spiking%250Anetwork%2520to%2520denoise%2520the%2520resulting%2520RGB%2520videos%2520into%2520realistic%2520event%2520sequences.%2520To%250Acapture%2520the%2520physical%2520properties%2520of%2520event%2520streams%252C%2520the%2520network%2520is%2520equipped%2520with%250Aa%2520bipolar%2520leaky%2520integrate-and-fired%2520%2528BiLIF%2529%2520spiking%2520unit%2520and%2520trained%2520with%2520a%250Abidirectional%2520earth%2520mover%2520distance%2520%2528EMD%2529%2520loss.%2520Our%2520EventTracer%2520pipeline%2520runs%2520at%250Aa%2520speed%2520of%2520about%25204%2520minutes%2520per%2520second%2520of%2520720p%2520video%252C%2520and%2520it%2520inherits%2520the%2520merit%250Aof%2520accurate%2520spatiotemporal%2520modeling%2520from%2520its%2520path%2520tracing%2520backbone.%2520We%2520show%2520in%250Atwo%2520downstream%2520tasks%2520that%2520EventTracer%2520captures%2520better%2520scene%2520details%2520and%250Ademonstrates%2520a%2520greater%2520similarity%2520to%2520real-world%2520event%2520data%2520than%2520other%2520event%250Asimulators%252C%2520which%2520establishes%2520it%2520as%2520a%2520promising%2520tool%2520for%2520creating%2520large-scale%250Aevent-RGB%2520datasets%2520at%2520a%2520low%2520cost%252C%2520narrowing%2520the%2520sim-to-real%2520gap%2520in%2520event-based%250Avision%252C%2520and%2520boosting%2520various%2520application%2520scenarios%2520such%2520as%2520robotics%252C%2520autonomous%250Adriving%252C%2520and%2520VRAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventTracer%3A%20Fast%20Path%20Tracing-based%20Event%20Stream%20Rendering&entry.906535625=Zhenyang%20Li%20and%20Xiaoyang%20Bai%20and%20Jinfan%20Lu%20and%20Pengfei%20Shen%20and%20Edmund%20Y.%20Lam%20and%20Yifan%20Peng&entry.1292438233=%20%20Simulating%20event%20streams%20from%203D%20scenes%20has%20become%20a%20common%20practice%20in%0Aevent-based%20vision%20research%2C%20as%20it%20meets%20the%20demand%20for%20large-scale%2C%20high%0Atemporal%20frequency%20data%20without%20setting%20up%20expensive%20hardware%20devices%20or%0Aundertaking%20extensive%20data%20collections.%20Yet%20existing%20methods%20in%20this%20direction%0Atypically%20work%20with%20noiseless%20RGB%20frames%20that%20are%20costly%20to%20render%2C%20and%0Atherefore%20they%20can%20only%20achieve%20a%20temporal%20resolution%20equivalent%20to%20100-300%0AFPS%2C%20far%20lower%20than%20that%20of%20real-world%20event%20data.%20In%20this%20work%2C%20we%20propose%0AEventTracer%2C%20a%20path%20tracing-based%20rendering%20pipeline%20that%20simulates%0Ahigh-fidelity%20event%20sequences%20from%20complex%203D%20scenes%20in%20an%20efficient%20and%0Aphysics-aware%20manner.%20Specifically%2C%20we%20speed%20up%20the%20rendering%20process%20via%20low%0Asample-per-pixel%20%28SPP%29%20path%20tracing%2C%20and%20train%20a%20lightweight%20event%20spiking%0Anetwork%20to%20denoise%20the%20resulting%20RGB%20videos%20into%20realistic%20event%20sequences.%20To%0Acapture%20the%20physical%20properties%20of%20event%20streams%2C%20the%20network%20is%20equipped%20with%0Aa%20bipolar%20leaky%20integrate-and-fired%20%28BiLIF%29%20spiking%20unit%20and%20trained%20with%20a%0Abidirectional%20earth%20mover%20distance%20%28EMD%29%20loss.%20Our%20EventTracer%20pipeline%20runs%20at%0Aa%20speed%20of%20about%204%20minutes%20per%20second%20of%20720p%20video%2C%20and%20it%20inherits%20the%20merit%0Aof%20accurate%20spatiotemporal%20modeling%20from%20its%20path%20tracing%20backbone.%20We%20show%20in%0Atwo%20downstream%20tasks%20that%20EventTracer%20captures%20better%20scene%20details%20and%0Ademonstrates%20a%20greater%20similarity%20to%20real-world%20event%20data%20than%20other%20event%0Asimulators%2C%20which%20establishes%20it%20as%20a%20promising%20tool%20for%20creating%20large-scale%0Aevent-RGB%20datasets%20at%20a%20low%20cost%2C%20narrowing%20the%20sim-to-real%20gap%20in%20event-based%0Avision%2C%20and%20boosting%20various%20application%20scenarios%20such%20as%20robotics%2C%20autonomous%0Adriving%2C%20and%20VRAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18071v1&entry.124074799=Read"},
{"title": "A holistic perception system of internal and external monitoring for\n  ground autonomous vehicles: AutoTRUST paradigm", "author": "Alexandros Gkillas and Christos Anagnostopoulos and Nikos Piperigkos and Dimitris Tsiktsiris and Theofilos Christodoulou and Theofanis Siamatras and Dimitrios Triantafyllou and Christos Basdekis and Theoktisti Marinopoulou and Panagiotis Lepentsiotis and Elefterios Blitsis and Aggeliki Zacharaki and Nearchos Stylianidis and Leonidas Katelaris and Lamberto Salvan and Aris S. Lalos and Christos Laoudias and Antonios Lalas and Konstantinos Votis", "abstract": "  This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.\n", "link": "http://arxiv.org/abs/2508.17969v1", "date": "2025-08-25", "relevancy": 2.299, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5968}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20holistic%20perception%20system%20of%20internal%20and%20external%20monitoring%20for%0A%20%20ground%20autonomous%20vehicles%3A%20AutoTRUST%20paradigm&body=Title%3A%20A%20holistic%20perception%20system%20of%20internal%20and%20external%20monitoring%20for%0A%20%20ground%20autonomous%20vehicles%3A%20AutoTRUST%20paradigm%0AAuthor%3A%20Alexandros%20Gkillas%20and%20Christos%20Anagnostopoulos%20and%20Nikos%20Piperigkos%20and%20Dimitris%20Tsiktsiris%20and%20Theofilos%20Christodoulou%20and%20Theofanis%20Siamatras%20and%20Dimitrios%20Triantafyllou%20and%20Christos%20Basdekis%20and%20Theoktisti%20Marinopoulou%20and%20Panagiotis%20Lepentsiotis%20and%20Elefterios%20Blitsis%20and%20Aggeliki%20Zacharaki%20and%20Nearchos%20Stylianidis%20and%20Leonidas%20Katelaris%20and%20Lamberto%20Salvan%20and%20Aris%20S.%20Lalos%20and%20Christos%20Laoudias%20and%20Antonios%20Lalas%20and%20Konstantinos%20Votis%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20holistic%20perception%20system%20for%20internal%20and%20external%0Amonitoring%20of%20autonomous%20vehicles%2C%20with%20the%20aim%20of%20demonstrating%20a%20novel%0AAI-leveraged%20self-adaptive%20framework%20of%20advanced%20vehicle%20technologies%20and%0Asolutions%20that%20optimize%20perception%20and%20experience%20on-board.%20Internal%20monitoring%0Asystem%20relies%20on%20a%20multi-camera%20setup%20designed%20for%20predicting%20and%20identifying%0Adriver%20and%20occupant%20behavior%20through%20facial%20recognition%2C%20exploiting%20in%20addition%0Aa%20large%20language%20model%20as%20virtual%20assistant.%20Moreover%2C%20the%20in-cabin%20monitoring%0Asystem%20includes%20AI-empowered%20smart%20sensors%20that%20measure%20air-quality%20and%20perform%0Athermal%20comfort%20analysis%20for%20efficient%20on%20and%20off-boarding.%20On%20the%20other%20hand%2C%0Aexternal%20monitoring%20system%20perceives%20the%20surrounding%20environment%20of%20vehicle%2C%0Athrough%20a%20LiDAR-based%20cost-efficient%20semantic%20segmentation%20approach%2C%20that%0Aperforms%20highly%20accurate%20and%20efficient%20super-resolution%20on%20low-quality%20raw%203D%0Apoint%20clouds.%20The%20holistic%20perception%20framework%20is%20developed%20in%20the%20context%20of%0AEU%27s%20Horizon%20Europe%20programm%20AutoTRUST%2C%20and%20has%20been%20integrated%20and%20deployed%20on%0Aa%20real%20electric%20vehicle%20provided%20by%20ALKE.%20Experimental%20validation%20and%0Aevaluation%20at%20the%20integration%20site%20of%20Joint%20Research%20Centre%20at%20Ispra%2C%20Italy%2C%0Ahighlights%20increased%20performance%20and%20efficiency%20of%20the%20modular%20blocks%20of%20the%0Aproposed%20perception%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520holistic%2520perception%2520system%2520of%2520internal%2520and%2520external%2520monitoring%2520for%250A%2520%2520ground%2520autonomous%2520vehicles%253A%2520AutoTRUST%2520paradigm%26entry.906535625%3DAlexandros%2520Gkillas%2520and%2520Christos%2520Anagnostopoulos%2520and%2520Nikos%2520Piperigkos%2520and%2520Dimitris%2520Tsiktsiris%2520and%2520Theofilos%2520Christodoulou%2520and%2520Theofanis%2520Siamatras%2520and%2520Dimitrios%2520Triantafyllou%2520and%2520Christos%2520Basdekis%2520and%2520Theoktisti%2520Marinopoulou%2520and%2520Panagiotis%2520Lepentsiotis%2520and%2520Elefterios%2520Blitsis%2520and%2520Aggeliki%2520Zacharaki%2520and%2520Nearchos%2520Stylianidis%2520and%2520Leonidas%2520Katelaris%2520and%2520Lamberto%2520Salvan%2520and%2520Aris%2520S.%2520Lalos%2520and%2520Christos%2520Laoudias%2520and%2520Antonios%2520Lalas%2520and%2520Konstantinos%2520Votis%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520holistic%2520perception%2520system%2520for%2520internal%2520and%2520external%250Amonitoring%2520of%2520autonomous%2520vehicles%252C%2520with%2520the%2520aim%2520of%2520demonstrating%2520a%2520novel%250AAI-leveraged%2520self-adaptive%2520framework%2520of%2520advanced%2520vehicle%2520technologies%2520and%250Asolutions%2520that%2520optimize%2520perception%2520and%2520experience%2520on-board.%2520Internal%2520monitoring%250Asystem%2520relies%2520on%2520a%2520multi-camera%2520setup%2520designed%2520for%2520predicting%2520and%2520identifying%250Adriver%2520and%2520occupant%2520behavior%2520through%2520facial%2520recognition%252C%2520exploiting%2520in%2520addition%250Aa%2520large%2520language%2520model%2520as%2520virtual%2520assistant.%2520Moreover%252C%2520the%2520in-cabin%2520monitoring%250Asystem%2520includes%2520AI-empowered%2520smart%2520sensors%2520that%2520measure%2520air-quality%2520and%2520perform%250Athermal%2520comfort%2520analysis%2520for%2520efficient%2520on%2520and%2520off-boarding.%2520On%2520the%2520other%2520hand%252C%250Aexternal%2520monitoring%2520system%2520perceives%2520the%2520surrounding%2520environment%2520of%2520vehicle%252C%250Athrough%2520a%2520LiDAR-based%2520cost-efficient%2520semantic%2520segmentation%2520approach%252C%2520that%250Aperforms%2520highly%2520accurate%2520and%2520efficient%2520super-resolution%2520on%2520low-quality%2520raw%25203D%250Apoint%2520clouds.%2520The%2520holistic%2520perception%2520framework%2520is%2520developed%2520in%2520the%2520context%2520of%250AEU%2527s%2520Horizon%2520Europe%2520programm%2520AutoTRUST%252C%2520and%2520has%2520been%2520integrated%2520and%2520deployed%2520on%250Aa%2520real%2520electric%2520vehicle%2520provided%2520by%2520ALKE.%2520Experimental%2520validation%2520and%250Aevaluation%2520at%2520the%2520integration%2520site%2520of%2520Joint%2520Research%2520Centre%2520at%2520Ispra%252C%2520Italy%252C%250Ahighlights%2520increased%2520performance%2520and%2520efficiency%2520of%2520the%2520modular%2520blocks%2520of%2520the%250Aproposed%2520perception%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20holistic%20perception%20system%20of%20internal%20and%20external%20monitoring%20for%0A%20%20ground%20autonomous%20vehicles%3A%20AutoTRUST%20paradigm&entry.906535625=Alexandros%20Gkillas%20and%20Christos%20Anagnostopoulos%20and%20Nikos%20Piperigkos%20and%20Dimitris%20Tsiktsiris%20and%20Theofilos%20Christodoulou%20and%20Theofanis%20Siamatras%20and%20Dimitrios%20Triantafyllou%20and%20Christos%20Basdekis%20and%20Theoktisti%20Marinopoulou%20and%20Panagiotis%20Lepentsiotis%20and%20Elefterios%20Blitsis%20and%20Aggeliki%20Zacharaki%20and%20Nearchos%20Stylianidis%20and%20Leonidas%20Katelaris%20and%20Lamberto%20Salvan%20and%20Aris%20S.%20Lalos%20and%20Christos%20Laoudias%20and%20Antonios%20Lalas%20and%20Konstantinos%20Votis&entry.1292438233=%20%20This%20paper%20introduces%20a%20holistic%20perception%20system%20for%20internal%20and%20external%0Amonitoring%20of%20autonomous%20vehicles%2C%20with%20the%20aim%20of%20demonstrating%20a%20novel%0AAI-leveraged%20self-adaptive%20framework%20of%20advanced%20vehicle%20technologies%20and%0Asolutions%20that%20optimize%20perception%20and%20experience%20on-board.%20Internal%20monitoring%0Asystem%20relies%20on%20a%20multi-camera%20setup%20designed%20for%20predicting%20and%20identifying%0Adriver%20and%20occupant%20behavior%20through%20facial%20recognition%2C%20exploiting%20in%20addition%0Aa%20large%20language%20model%20as%20virtual%20assistant.%20Moreover%2C%20the%20in-cabin%20monitoring%0Asystem%20includes%20AI-empowered%20smart%20sensors%20that%20measure%20air-quality%20and%20perform%0Athermal%20comfort%20analysis%20for%20efficient%20on%20and%20off-boarding.%20On%20the%20other%20hand%2C%0Aexternal%20monitoring%20system%20perceives%20the%20surrounding%20environment%20of%20vehicle%2C%0Athrough%20a%20LiDAR-based%20cost-efficient%20semantic%20segmentation%20approach%2C%20that%0Aperforms%20highly%20accurate%20and%20efficient%20super-resolution%20on%20low-quality%20raw%203D%0Apoint%20clouds.%20The%20holistic%20perception%20framework%20is%20developed%20in%20the%20context%20of%0AEU%27s%20Horizon%20Europe%20programm%20AutoTRUST%2C%20and%20has%20been%20integrated%20and%20deployed%20on%0Aa%20real%20electric%20vehicle%20provided%20by%20ALKE.%20Experimental%20validation%20and%0Aevaluation%20at%20the%20integration%20site%20of%20Joint%20Research%20Centre%20at%20Ispra%2C%20Italy%2C%0Ahighlights%20increased%20performance%20and%20efficiency%20of%20the%20modular%20blocks%20of%20the%0Aproposed%20perception%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17969v1&entry.124074799=Read"},
{"title": "BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification\n  with Swin-HAFNet", "author": "Amirreza Fateh and Yasin Rezvani and Sara Moayedi and Sadjad Rezvani and Fatemeh Fateh and Mansoor Fateh and Vahid Abolghasemi", "abstract": "  Accurate segmentation and classification of brain tumors from Magnetic\nResonance Imaging (MRI) remain key challenges in medical image analysis,\nprimarily due to the lack of high-quality, balanced, and diverse datasets. In\nthis work, we present a newly developed MRI dataset named BRISC designed\nspecifically for brain tumor segmentation and classification tasks. The dataset\ncomprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified\nradiologists and physicians. It includes three major tumor types, namely\nglioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample\nincludes high-resolution labels and is categorized across axial, sagittal, and\ncoronal imaging planes to facilitate robust model development and cross-view\ngeneralization. To demonstrate the utility of the dataset, we propose a\ntransformer-based model, leveraging a Swin Transformer backbone for multi-scale\nfeature representation, to benchmark both segmentation and classification\ntasks. This model serves as a benchmark to demonstrate the utility of the BRISC\ndataset for advancing methodological research in neuro-oncological image\nanalysis. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/\n", "link": "http://arxiv.org/abs/2506.14318v3", "date": "2025-08-25", "relevancy": 2.2803, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4545}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet&body=Title%3A%20BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet%0AAuthor%3A%20Amirreza%20Fateh%20and%20Yasin%20Rezvani%20and%20Sara%20Moayedi%20and%20Sadjad%20Rezvani%20and%20Fatemeh%20Fateh%20and%20Mansoor%20Fateh%20and%20Vahid%20Abolghasemi%0AAbstract%3A%20%20%20Accurate%20segmentation%20and%20classification%20of%20brain%20tumors%20from%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20remain%20key%20challenges%20in%20medical%20image%20analysis%2C%0Aprimarily%20due%20to%20the%20lack%20of%20high-quality%2C%20balanced%2C%20and%20diverse%20datasets.%20In%0Athis%20work%2C%20we%20present%20a%20newly%20developed%20MRI%20dataset%20named%20BRISC%20designed%0Aspecifically%20for%20brain%20tumor%20segmentation%20and%20classification%20tasks.%20The%20dataset%0Acomprises%206%2C000%20contrast-enhanced%20T1-weighted%20MRI%20scans%20annotated%20by%20certified%0Aradiologists%20and%20physicians.%20It%20includes%20three%20major%20tumor%20types%2C%20namely%0Aglioma%2C%20meningioma%2C%20and%20pituitary%2C%20as%20well%20as%20non-tumorous%20cases.%20Each%20sample%0Aincludes%20high-resolution%20labels%20and%20is%20categorized%20across%20axial%2C%20sagittal%2C%20and%0Acoronal%20imaging%20planes%20to%20facilitate%20robust%20model%20development%20and%20cross-view%0Ageneralization.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20propose%20a%0Atransformer-based%20model%2C%20leveraging%20a%20Swin%20Transformer%20backbone%20for%20multi-scale%0Afeature%20representation%2C%20to%20benchmark%20both%20segmentation%20and%20classification%0Atasks.%20This%20model%20serves%20as%20a%20benchmark%20to%20demonstrate%20the%20utility%20of%20the%20BRISC%0Adataset%20for%20advancing%20methodological%20research%20in%20neuro-oncological%20image%0Aanalysis.%20datasetlink%3A%20https%3A//www.kaggle.com/datasets/briscdataset/brisc2025/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14318v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRISC%253A%2520Annotated%2520Dataset%2520for%2520Brain%2520Tumor%2520Segmentation%2520and%2520Classification%250A%2520%2520with%2520Swin-HAFNet%26entry.906535625%3DAmirreza%2520Fateh%2520and%2520Yasin%2520Rezvani%2520and%2520Sara%2520Moayedi%2520and%2520Sadjad%2520Rezvani%2520and%2520Fatemeh%2520Fateh%2520and%2520Mansoor%2520Fateh%2520and%2520Vahid%2520Abolghasemi%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520and%2520classification%2520of%2520brain%2520tumors%2520from%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529%2520remain%2520key%2520challenges%2520in%2520medical%2520image%2520analysis%252C%250Aprimarily%2520due%2520to%2520the%2520lack%2520of%2520high-quality%252C%2520balanced%252C%2520and%2520diverse%2520datasets.%2520In%250Athis%2520work%252C%2520we%2520present%2520a%2520newly%2520developed%2520MRI%2520dataset%2520named%2520BRISC%2520designed%250Aspecifically%2520for%2520brain%2520tumor%2520segmentation%2520and%2520classification%2520tasks.%2520The%2520dataset%250Acomprises%25206%252C000%2520contrast-enhanced%2520T1-weighted%2520MRI%2520scans%2520annotated%2520by%2520certified%250Aradiologists%2520and%2520physicians.%2520It%2520includes%2520three%2520major%2520tumor%2520types%252C%2520namely%250Aglioma%252C%2520meningioma%252C%2520and%2520pituitary%252C%2520as%2520well%2520as%2520non-tumorous%2520cases.%2520Each%2520sample%250Aincludes%2520high-resolution%2520labels%2520and%2520is%2520categorized%2520across%2520axial%252C%2520sagittal%252C%2520and%250Acoronal%2520imaging%2520planes%2520to%2520facilitate%2520robust%2520model%2520development%2520and%2520cross-view%250Ageneralization.%2520To%2520demonstrate%2520the%2520utility%2520of%2520the%2520dataset%252C%2520we%2520propose%2520a%250Atransformer-based%2520model%252C%2520leveraging%2520a%2520Swin%2520Transformer%2520backbone%2520for%2520multi-scale%250Afeature%2520representation%252C%2520to%2520benchmark%2520both%2520segmentation%2520and%2520classification%250Atasks.%2520This%2520model%2520serves%2520as%2520a%2520benchmark%2520to%2520demonstrate%2520the%2520utility%2520of%2520the%2520BRISC%250Adataset%2520for%2520advancing%2520methodological%2520research%2520in%2520neuro-oncological%2520image%250Aanalysis.%2520datasetlink%253A%2520https%253A//www.kaggle.com/datasets/briscdataset/brisc2025/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14318v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRISC%3A%20Annotated%20Dataset%20for%20Brain%20Tumor%20Segmentation%20and%20Classification%0A%20%20with%20Swin-HAFNet&entry.906535625=Amirreza%20Fateh%20and%20Yasin%20Rezvani%20and%20Sara%20Moayedi%20and%20Sadjad%20Rezvani%20and%20Fatemeh%20Fateh%20and%20Mansoor%20Fateh%20and%20Vahid%20Abolghasemi&entry.1292438233=%20%20Accurate%20segmentation%20and%20classification%20of%20brain%20tumors%20from%20Magnetic%0AResonance%20Imaging%20%28MRI%29%20remain%20key%20challenges%20in%20medical%20image%20analysis%2C%0Aprimarily%20due%20to%20the%20lack%20of%20high-quality%2C%20balanced%2C%20and%20diverse%20datasets.%20In%0Athis%20work%2C%20we%20present%20a%20newly%20developed%20MRI%20dataset%20named%20BRISC%20designed%0Aspecifically%20for%20brain%20tumor%20segmentation%20and%20classification%20tasks.%20The%20dataset%0Acomprises%206%2C000%20contrast-enhanced%20T1-weighted%20MRI%20scans%20annotated%20by%20certified%0Aradiologists%20and%20physicians.%20It%20includes%20three%20major%20tumor%20types%2C%20namely%0Aglioma%2C%20meningioma%2C%20and%20pituitary%2C%20as%20well%20as%20non-tumorous%20cases.%20Each%20sample%0Aincludes%20high-resolution%20labels%20and%20is%20categorized%20across%20axial%2C%20sagittal%2C%20and%0Acoronal%20imaging%20planes%20to%20facilitate%20robust%20model%20development%20and%20cross-view%0Ageneralization.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20propose%20a%0Atransformer-based%20model%2C%20leveraging%20a%20Swin%20Transformer%20backbone%20for%20multi-scale%0Afeature%20representation%2C%20to%20benchmark%20both%20segmentation%20and%20classification%0Atasks.%20This%20model%20serves%20as%20a%20benchmark%20to%20demonstrate%20the%20utility%20of%20the%20BRISC%0Adataset%20for%20advancing%20methodological%20research%20in%20neuro-oncological%20image%0Aanalysis.%20datasetlink%3A%20https%3A//www.kaggle.com/datasets/briscdataset/brisc2025/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14318v3&entry.124074799=Read"},
{"title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation", "author": "Yaqi Li and Peng Chen and Mingyang Han and Bu Pi and Haoxiang Shi and Runzhou Zhao and Yang Yao and Xuan Zhang and Jun Song", "abstract": "  Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.\n", "link": "http://arxiv.org/abs/2508.18032v1", "date": "2025-08-25", "relevancy": 2.2721, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5698}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-CoG%3A%20Stage-Aware%20Reinforcement%20Learning%20with%20Chain%20of%20Guidance%0A%20%20for%20Text-to-Image%20Generation&body=Title%3A%20Visual-CoG%3A%20Stage-Aware%20Reinforcement%20Learning%20with%20Chain%20of%20Guidance%0A%20%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Yaqi%20Li%20and%20Peng%20Chen%20and%20Mingyang%20Han%20and%20Bu%20Pi%20and%20Haoxiang%20Shi%20and%20Runzhou%20Zhao%20and%20Yang%20Yao%20and%20Xuan%20Zhang%20and%20Jun%20Song%0AAbstract%3A%20%20%20Despite%20the%20promising%20progress%20of%20recent%20autoregressive%20models%20in%0Atext-to-image%20%28T2I%29%20generation%2C%20their%20ability%20to%20handle%20multi-attribute%20and%0Aambiguous%20prompts%20remains%20limited.%20To%20address%20these%20limitations%2C%20existing%20works%0Ahave%20applied%20chain-of-thought%20%28CoT%29%20to%20enable%20stage-aware%20visual%20synthesis%20and%0Aemployed%20reinforcement%20learning%20%28RL%29%20to%20improve%20reasoning%20capabilities.%0AHowever%2C%20most%20models%20provide%20reward%20signals%20only%20at%20the%20end%20of%20the%20generation%0Astage.%20This%20monolithic%20final-only%20guidance%20makes%20it%20difficult%20to%20identify%20which%0Astages%20contribute%20positively%20to%20the%20final%20outcome%20and%20may%20lead%20to%20suboptimal%0Apolicies.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20Visual-Chain%20of%20Guidance%0A%28Visual-CoG%29%20paradigm%20consisting%20of%20three%20stages%3A%20semantic%20reasoning%2C%20process%0Arefining%2C%20and%20outcome%20evaluation%2C%20with%20stage-aware%20rewards%20providing%20immediate%0Aguidance%20throughout%20the%20image%20generation%20pipeline.%20We%20further%20construct%20a%0Avisual%20cognition%20benchmark%2C%20VisCog-Bench%2C%20which%20comprises%20four%20subtasks%20to%0Aevaluate%20the%20effectiveness%20of%20semantic%20reasoning.%20Comprehensive%20evaluations%20on%0AGenEval%2C%20T2I-CompBench%2C%20and%20the%20proposed%20VisCog-Bench%20show%20improvements%20of%2015%25%2C%0A5%25%2C%20and%2019%25%2C%20respectively%2C%20demonstrating%20the%20superior%20performance%20of%20the%0Aproposed%20Visual-CoG.%20We%20will%20release%20all%20the%20resources%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-CoG%253A%2520Stage-Aware%2520Reinforcement%2520Learning%2520with%2520Chain%2520of%2520Guidance%250A%2520%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DYaqi%2520Li%2520and%2520Peng%2520Chen%2520and%2520Mingyang%2520Han%2520and%2520Bu%2520Pi%2520and%2520Haoxiang%2520Shi%2520and%2520Runzhou%2520Zhao%2520and%2520Yang%2520Yao%2520and%2520Xuan%2520Zhang%2520and%2520Jun%2520Song%26entry.1292438233%3D%2520%2520Despite%2520the%2520promising%2520progress%2520of%2520recent%2520autoregressive%2520models%2520in%250Atext-to-image%2520%2528T2I%2529%2520generation%252C%2520their%2520ability%2520to%2520handle%2520multi-attribute%2520and%250Aambiguous%2520prompts%2520remains%2520limited.%2520To%2520address%2520these%2520limitations%252C%2520existing%2520works%250Ahave%2520applied%2520chain-of-thought%2520%2528CoT%2529%2520to%2520enable%2520stage-aware%2520visual%2520synthesis%2520and%250Aemployed%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520improve%2520reasoning%2520capabilities.%250AHowever%252C%2520most%2520models%2520provide%2520reward%2520signals%2520only%2520at%2520the%2520end%2520of%2520the%2520generation%250Astage.%2520This%2520monolithic%2520final-only%2520guidance%2520makes%2520it%2520difficult%2520to%2520identify%2520which%250Astages%2520contribute%2520positively%2520to%2520the%2520final%2520outcome%2520and%2520may%2520lead%2520to%2520suboptimal%250Apolicies.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520Visual-Chain%2520of%2520Guidance%250A%2528Visual-CoG%2529%2520paradigm%2520consisting%2520of%2520three%2520stages%253A%2520semantic%2520reasoning%252C%2520process%250Arefining%252C%2520and%2520outcome%2520evaluation%252C%2520with%2520stage-aware%2520rewards%2520providing%2520immediate%250Aguidance%2520throughout%2520the%2520image%2520generation%2520pipeline.%2520We%2520further%2520construct%2520a%250Avisual%2520cognition%2520benchmark%252C%2520VisCog-Bench%252C%2520which%2520comprises%2520four%2520subtasks%2520to%250Aevaluate%2520the%2520effectiveness%2520of%2520semantic%2520reasoning.%2520Comprehensive%2520evaluations%2520on%250AGenEval%252C%2520T2I-CompBench%252C%2520and%2520the%2520proposed%2520VisCog-Bench%2520show%2520improvements%2520of%252015%2525%252C%250A5%2525%252C%2520and%252019%2525%252C%2520respectively%252C%2520demonstrating%2520the%2520superior%2520performance%2520of%2520the%250Aproposed%2520Visual-CoG.%2520We%2520will%2520release%2520all%2520the%2520resources%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-CoG%3A%20Stage-Aware%20Reinforcement%20Learning%20with%20Chain%20of%20Guidance%0A%20%20for%20Text-to-Image%20Generation&entry.906535625=Yaqi%20Li%20and%20Peng%20Chen%20and%20Mingyang%20Han%20and%20Bu%20Pi%20and%20Haoxiang%20Shi%20and%20Runzhou%20Zhao%20and%20Yang%20Yao%20and%20Xuan%20Zhang%20and%20Jun%20Song&entry.1292438233=%20%20Despite%20the%20promising%20progress%20of%20recent%20autoregressive%20models%20in%0Atext-to-image%20%28T2I%29%20generation%2C%20their%20ability%20to%20handle%20multi-attribute%20and%0Aambiguous%20prompts%20remains%20limited.%20To%20address%20these%20limitations%2C%20existing%20works%0Ahave%20applied%20chain-of-thought%20%28CoT%29%20to%20enable%20stage-aware%20visual%20synthesis%20and%0Aemployed%20reinforcement%20learning%20%28RL%29%20to%20improve%20reasoning%20capabilities.%0AHowever%2C%20most%20models%20provide%20reward%20signals%20only%20at%20the%20end%20of%20the%20generation%0Astage.%20This%20monolithic%20final-only%20guidance%20makes%20it%20difficult%20to%20identify%20which%0Astages%20contribute%20positively%20to%20the%20final%20outcome%20and%20may%20lead%20to%20suboptimal%0Apolicies.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20Visual-Chain%20of%20Guidance%0A%28Visual-CoG%29%20paradigm%20consisting%20of%20three%20stages%3A%20semantic%20reasoning%2C%20process%0Arefining%2C%20and%20outcome%20evaluation%2C%20with%20stage-aware%20rewards%20providing%20immediate%0Aguidance%20throughout%20the%20image%20generation%20pipeline.%20We%20further%20construct%20a%0Avisual%20cognition%20benchmark%2C%20VisCog-Bench%2C%20which%20comprises%20four%20subtasks%20to%0Aevaluate%20the%20effectiveness%20of%20semantic%20reasoning.%20Comprehensive%20evaluations%20on%0AGenEval%2C%20T2I-CompBench%2C%20and%20the%20proposed%20VisCog-Bench%20show%20improvements%20of%2015%25%2C%0A5%25%2C%20and%2019%25%2C%20respectively%2C%20demonstrating%20the%20superior%20performance%20of%20the%0Aproposed%20Visual-CoG.%20We%20will%20release%20all%20the%20resources%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18032v1&entry.124074799=Read"},
{"title": "EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D\n  kinematics to challenge video and language models", "author": "Andy Bonnetto and Haozhe Qi and Franklin Leong and Matea Tashkovska and Mahdi Rad and Solaiman Shokur and Friedhelm Hummel and Silvestro Micera and Marc Pollefeys and Alexander Mathis", "abstract": "  Understanding behavior requires datasets that capture humans while carrying\nout complex tasks. The kitchen is an excellent environment for assessing human\nmotor and cognitive function, as many complex actions are naturally exhibited\nin kitchens from chopping to cleaning. Here, we introduce the\nEPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture\nplatform inside a kitchen environment. Nine static RGB-D cameras, inertial\nmeasurement units (IMUs) and one head-mounted HoloLens~2 headset were used to\ncapture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is\na multi-view action dataset with synchronized exocentric, egocentric, depth,\nIMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects\ncooking four different recipes. Action sequences were densely annotated with\n33.78 action segments per minute. Leveraging this multi-modal dataset, we\npropose four benchmarks to advance behavior understanding and modeling through\n1) a vision-language benchmark, 2) a semantic text-to-motion generation\nbenchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based\naction segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to\npave the way for better methods as well as insights to understand the nature of\necologically-valid human behavior. Code and data are available at\nhttps://github.com/amathislab/EPFL-Smart-Kitchen\n", "link": "http://arxiv.org/abs/2506.01608v2", "date": "2025-08-25", "relevancy": 2.2564, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.574}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPFL-Smart-Kitchen-30%3A%20Densely%20annotated%20cooking%20dataset%20with%203D%0A%20%20kinematics%20to%20challenge%20video%20and%20language%20models&body=Title%3A%20EPFL-Smart-Kitchen-30%3A%20Densely%20annotated%20cooking%20dataset%20with%203D%0A%20%20kinematics%20to%20challenge%20video%20and%20language%20models%0AAuthor%3A%20Andy%20Bonnetto%20and%20Haozhe%20Qi%20and%20Franklin%20Leong%20and%20Matea%20Tashkovska%20and%20Mahdi%20Rad%20and%20Solaiman%20Shokur%20and%20Friedhelm%20Hummel%20and%20Silvestro%20Micera%20and%20Marc%20Pollefeys%20and%20Alexander%20Mathis%0AAbstract%3A%20%20%20Understanding%20behavior%20requires%20datasets%20that%20capture%20humans%20while%20carrying%0Aout%20complex%20tasks.%20The%20kitchen%20is%20an%20excellent%20environment%20for%20assessing%20human%0Amotor%20and%20cognitive%20function%2C%20as%20many%20complex%20actions%20are%20naturally%20exhibited%0Ain%20kitchens%20from%20chopping%20to%20cleaning.%20Here%2C%20we%20introduce%20the%0AEPFL-Smart-Kitchen-30%20dataset%2C%20collected%20in%20a%20noninvasive%20motion%20capture%0Aplatform%20inside%20a%20kitchen%20environment.%20Nine%20static%20RGB-D%20cameras%2C%20inertial%0Ameasurement%20units%20%28IMUs%29%20and%20one%20head-mounted%20HoloLens~2%20headset%20were%20used%20to%0Acapture%203D%20hand%2C%20body%2C%20and%20eye%20movements.%20The%20EPFL-Smart-Kitchen-30%20dataset%20is%0Aa%20multi-view%20action%20dataset%20with%20synchronized%20exocentric%2C%20egocentric%2C%20depth%2C%0AIMUs%2C%20eye%20gaze%2C%20body%20and%20hand%20kinematics%20spanning%2029.7%20hours%20of%2016%20subjects%0Acooking%20four%20different%20recipes.%20Action%20sequences%20were%20densely%20annotated%20with%0A33.78%20action%20segments%20per%20minute.%20Leveraging%20this%20multi-modal%20dataset%2C%20we%0Apropose%20four%20benchmarks%20to%20advance%20behavior%20understanding%20and%20modeling%20through%0A1%29%20a%20vision-language%20benchmark%2C%202%29%20a%20semantic%20text-to-motion%20generation%0Abenchmark%2C%203%29%20a%20multi-modal%20action%20recognition%20benchmark%2C%204%29%20a%20pose-based%0Aaction%20segmentation%20benchmark.%20We%20expect%20the%20EPFL-Smart-Kitchen-30%20dataset%20to%0Apave%20the%20way%20for%20better%20methods%20as%20well%20as%20insights%20to%20understand%20the%20nature%20of%0Aecologically-valid%20human%20behavior.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/amathislab/EPFL-Smart-Kitchen%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPFL-Smart-Kitchen-30%253A%2520Densely%2520annotated%2520cooking%2520dataset%2520with%25203D%250A%2520%2520kinematics%2520to%2520challenge%2520video%2520and%2520language%2520models%26entry.906535625%3DAndy%2520Bonnetto%2520and%2520Haozhe%2520Qi%2520and%2520Franklin%2520Leong%2520and%2520Matea%2520Tashkovska%2520and%2520Mahdi%2520Rad%2520and%2520Solaiman%2520Shokur%2520and%2520Friedhelm%2520Hummel%2520and%2520Silvestro%2520Micera%2520and%2520Marc%2520Pollefeys%2520and%2520Alexander%2520Mathis%26entry.1292438233%3D%2520%2520Understanding%2520behavior%2520requires%2520datasets%2520that%2520capture%2520humans%2520while%2520carrying%250Aout%2520complex%2520tasks.%2520The%2520kitchen%2520is%2520an%2520excellent%2520environment%2520for%2520assessing%2520human%250Amotor%2520and%2520cognitive%2520function%252C%2520as%2520many%2520complex%2520actions%2520are%2520naturally%2520exhibited%250Ain%2520kitchens%2520from%2520chopping%2520to%2520cleaning.%2520Here%252C%2520we%2520introduce%2520the%250AEPFL-Smart-Kitchen-30%2520dataset%252C%2520collected%2520in%2520a%2520noninvasive%2520motion%2520capture%250Aplatform%2520inside%2520a%2520kitchen%2520environment.%2520Nine%2520static%2520RGB-D%2520cameras%252C%2520inertial%250Ameasurement%2520units%2520%2528IMUs%2529%2520and%2520one%2520head-mounted%2520HoloLens~2%2520headset%2520were%2520used%2520to%250Acapture%25203D%2520hand%252C%2520body%252C%2520and%2520eye%2520movements.%2520The%2520EPFL-Smart-Kitchen-30%2520dataset%2520is%250Aa%2520multi-view%2520action%2520dataset%2520with%2520synchronized%2520exocentric%252C%2520egocentric%252C%2520depth%252C%250AIMUs%252C%2520eye%2520gaze%252C%2520body%2520and%2520hand%2520kinematics%2520spanning%252029.7%2520hours%2520of%252016%2520subjects%250Acooking%2520four%2520different%2520recipes.%2520Action%2520sequences%2520were%2520densely%2520annotated%2520with%250A33.78%2520action%2520segments%2520per%2520minute.%2520Leveraging%2520this%2520multi-modal%2520dataset%252C%2520we%250Apropose%2520four%2520benchmarks%2520to%2520advance%2520behavior%2520understanding%2520and%2520modeling%2520through%250A1%2529%2520a%2520vision-language%2520benchmark%252C%25202%2529%2520a%2520semantic%2520text-to-motion%2520generation%250Abenchmark%252C%25203%2529%2520a%2520multi-modal%2520action%2520recognition%2520benchmark%252C%25204%2529%2520a%2520pose-based%250Aaction%2520segmentation%2520benchmark.%2520We%2520expect%2520the%2520EPFL-Smart-Kitchen-30%2520dataset%2520to%250Apave%2520the%2520way%2520for%2520better%2520methods%2520as%2520well%2520as%2520insights%2520to%2520understand%2520the%2520nature%2520of%250Aecologically-valid%2520human%2520behavior.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/amathislab/EPFL-Smart-Kitchen%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPFL-Smart-Kitchen-30%3A%20Densely%20annotated%20cooking%20dataset%20with%203D%0A%20%20kinematics%20to%20challenge%20video%20and%20language%20models&entry.906535625=Andy%20Bonnetto%20and%20Haozhe%20Qi%20and%20Franklin%20Leong%20and%20Matea%20Tashkovska%20and%20Mahdi%20Rad%20and%20Solaiman%20Shokur%20and%20Friedhelm%20Hummel%20and%20Silvestro%20Micera%20and%20Marc%20Pollefeys%20and%20Alexander%20Mathis&entry.1292438233=%20%20Understanding%20behavior%20requires%20datasets%20that%20capture%20humans%20while%20carrying%0Aout%20complex%20tasks.%20The%20kitchen%20is%20an%20excellent%20environment%20for%20assessing%20human%0Amotor%20and%20cognitive%20function%2C%20as%20many%20complex%20actions%20are%20naturally%20exhibited%0Ain%20kitchens%20from%20chopping%20to%20cleaning.%20Here%2C%20we%20introduce%20the%0AEPFL-Smart-Kitchen-30%20dataset%2C%20collected%20in%20a%20noninvasive%20motion%20capture%0Aplatform%20inside%20a%20kitchen%20environment.%20Nine%20static%20RGB-D%20cameras%2C%20inertial%0Ameasurement%20units%20%28IMUs%29%20and%20one%20head-mounted%20HoloLens~2%20headset%20were%20used%20to%0Acapture%203D%20hand%2C%20body%2C%20and%20eye%20movements.%20The%20EPFL-Smart-Kitchen-30%20dataset%20is%0Aa%20multi-view%20action%20dataset%20with%20synchronized%20exocentric%2C%20egocentric%2C%20depth%2C%0AIMUs%2C%20eye%20gaze%2C%20body%20and%20hand%20kinematics%20spanning%2029.7%20hours%20of%2016%20subjects%0Acooking%20four%20different%20recipes.%20Action%20sequences%20were%20densely%20annotated%20with%0A33.78%20action%20segments%20per%20minute.%20Leveraging%20this%20multi-modal%20dataset%2C%20we%0Apropose%20four%20benchmarks%20to%20advance%20behavior%20understanding%20and%20modeling%20through%0A1%29%20a%20vision-language%20benchmark%2C%202%29%20a%20semantic%20text-to-motion%20generation%0Abenchmark%2C%203%29%20a%20multi-modal%20action%20recognition%20benchmark%2C%204%29%20a%20pose-based%0Aaction%20segmentation%20benchmark.%20We%20expect%20the%20EPFL-Smart-Kitchen-30%20dataset%20to%0Apave%20the%20way%20for%20better%20methods%20as%20well%20as%20insights%20to%20understand%20the%20nature%20of%0Aecologically-valid%20human%20behavior.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/amathislab/EPFL-Smart-Kitchen%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01608v2&entry.124074799=Read"},
{"title": "Training Transformers for Mesh-Based Simulations", "author": "Paul Garnier and Vincent Lannelongue and Jonathan Viquerat and Elie Hachem", "abstract": "  Simulating physics using Graph Neural Networks (GNNs) is predominantly driven\nby message-passing architectures, which face challenges in scaling and\nefficiency, particularly in handling large, complex meshes. These architectures\nhave inspired numerous enhancements, including multigrid approaches and $K$-hop\naggregation (using neighbours of distance $K$), yet they often introduce\nsignificant complexity and suffer from limited in-depth investigations. In\nresponse to these challenges, we propose a novel Graph Transformer architecture\nthat leverages the adjacency matrix as an attention mask. The proposed approach\nincorporates innovative augmentations, including Dilated Sliding Windows and\nGlobal Attention, to extend receptive fields without sacrificing computational\nefficiency. Through extensive experimentation, we evaluate model size,\nadjacency matrix augmentations, positional encoding and $K$-hop configurations\nusing challenging 3D computational fluid dynamics (CFD) datasets. We also train\nover 60 models to find a scaling law between training FLOPs and parameters. The\nintroduced models demonstrate remarkable scalability, performing on meshes with\nup to 300k nodes and 3 million edges. Notably, the smallest model achieves\nparity with MeshGraphNet while being $7\\times$ faster and $6\\times$ smaller.\nThe largest model surpasses the previous state-of-the-art by $38.8$\\% on\naverage and outperforms MeshGraphNet by $52$\\% on the all-rollout RMSE, while\nhaving a similar training speed. Code and datasets are available at\nhttps://github.com/DonsetPG/graph-physics.\n", "link": "http://arxiv.org/abs/2508.18051v1", "date": "2025-08-25", "relevancy": 2.2533, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6053}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5727}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Transformers%20for%20Mesh-Based%20Simulations&body=Title%3A%20Training%20Transformers%20for%20Mesh-Based%20Simulations%0AAuthor%3A%20Paul%20Garnier%20and%20Vincent%20Lannelongue%20and%20Jonathan%20Viquerat%20and%20Elie%20Hachem%0AAbstract%3A%20%20%20Simulating%20physics%20using%20Graph%20Neural%20Networks%20%28GNNs%29%20is%20predominantly%20driven%0Aby%20message-passing%20architectures%2C%20which%20face%20challenges%20in%20scaling%20and%0Aefficiency%2C%20particularly%20in%20handling%20large%2C%20complex%20meshes.%20These%20architectures%0Ahave%20inspired%20numerous%20enhancements%2C%20including%20multigrid%20approaches%20and%20%24K%24-hop%0Aaggregation%20%28using%20neighbours%20of%20distance%20%24K%24%29%2C%20yet%20they%20often%20introduce%0Asignificant%20complexity%20and%20suffer%20from%20limited%20in-depth%20investigations.%20In%0Aresponse%20to%20these%20challenges%2C%20we%20propose%20a%20novel%20Graph%20Transformer%20architecture%0Athat%20leverages%20the%20adjacency%20matrix%20as%20an%20attention%20mask.%20The%20proposed%20approach%0Aincorporates%20innovative%20augmentations%2C%20including%20Dilated%20Sliding%20Windows%20and%0AGlobal%20Attention%2C%20to%20extend%20receptive%20fields%20without%20sacrificing%20computational%0Aefficiency.%20Through%20extensive%20experimentation%2C%20we%20evaluate%20model%20size%2C%0Aadjacency%20matrix%20augmentations%2C%20positional%20encoding%20and%20%24K%24-hop%20configurations%0Ausing%20challenging%203D%20computational%20fluid%20dynamics%20%28CFD%29%20datasets.%20We%20also%20train%0Aover%2060%20models%20to%20find%20a%20scaling%20law%20between%20training%20FLOPs%20and%20parameters.%20The%0Aintroduced%20models%20demonstrate%20remarkable%20scalability%2C%20performing%20on%20meshes%20with%0Aup%20to%20300k%20nodes%20and%203%20million%20edges.%20Notably%2C%20the%20smallest%20model%20achieves%0Aparity%20with%20MeshGraphNet%20while%20being%20%247%5Ctimes%24%20faster%20and%20%246%5Ctimes%24%20smaller.%0AThe%20largest%20model%20surpasses%20the%20previous%20state-of-the-art%20by%20%2438.8%24%5C%25%20on%0Aaverage%20and%20outperforms%20MeshGraphNet%20by%20%2452%24%5C%25%20on%20the%20all-rollout%20RMSE%2C%20while%0Ahaving%20a%20similar%20training%20speed.%20Code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/DonsetPG/graph-physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Transformers%2520for%2520Mesh-Based%2520Simulations%26entry.906535625%3DPaul%2520Garnier%2520and%2520Vincent%2520Lannelongue%2520and%2520Jonathan%2520Viquerat%2520and%2520Elie%2520Hachem%26entry.1292438233%3D%2520%2520Simulating%2520physics%2520using%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520is%2520predominantly%2520driven%250Aby%2520message-passing%2520architectures%252C%2520which%2520face%2520challenges%2520in%2520scaling%2520and%250Aefficiency%252C%2520particularly%2520in%2520handling%2520large%252C%2520complex%2520meshes.%2520These%2520architectures%250Ahave%2520inspired%2520numerous%2520enhancements%252C%2520including%2520multigrid%2520approaches%2520and%2520%2524K%2524-hop%250Aaggregation%2520%2528using%2520neighbours%2520of%2520distance%2520%2524K%2524%2529%252C%2520yet%2520they%2520often%2520introduce%250Asignificant%2520complexity%2520and%2520suffer%2520from%2520limited%2520in-depth%2520investigations.%2520In%250Aresponse%2520to%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Graph%2520Transformer%2520architecture%250Athat%2520leverages%2520the%2520adjacency%2520matrix%2520as%2520an%2520attention%2520mask.%2520The%2520proposed%2520approach%250Aincorporates%2520innovative%2520augmentations%252C%2520including%2520Dilated%2520Sliding%2520Windows%2520and%250AGlobal%2520Attention%252C%2520to%2520extend%2520receptive%2520fields%2520without%2520sacrificing%2520computational%250Aefficiency.%2520Through%2520extensive%2520experimentation%252C%2520we%2520evaluate%2520model%2520size%252C%250Aadjacency%2520matrix%2520augmentations%252C%2520positional%2520encoding%2520and%2520%2524K%2524-hop%2520configurations%250Ausing%2520challenging%25203D%2520computational%2520fluid%2520dynamics%2520%2528CFD%2529%2520datasets.%2520We%2520also%2520train%250Aover%252060%2520models%2520to%2520find%2520a%2520scaling%2520law%2520between%2520training%2520FLOPs%2520and%2520parameters.%2520The%250Aintroduced%2520models%2520demonstrate%2520remarkable%2520scalability%252C%2520performing%2520on%2520meshes%2520with%250Aup%2520to%2520300k%2520nodes%2520and%25203%2520million%2520edges.%2520Notably%252C%2520the%2520smallest%2520model%2520achieves%250Aparity%2520with%2520MeshGraphNet%2520while%2520being%2520%25247%255Ctimes%2524%2520faster%2520and%2520%25246%255Ctimes%2524%2520smaller.%250AThe%2520largest%2520model%2520surpasses%2520the%2520previous%2520state-of-the-art%2520by%2520%252438.8%2524%255C%2525%2520on%250Aaverage%2520and%2520outperforms%2520MeshGraphNet%2520by%2520%252452%2524%255C%2525%2520on%2520the%2520all-rollout%2520RMSE%252C%2520while%250Ahaving%2520a%2520similar%2520training%2520speed.%2520Code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/DonsetPG/graph-physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Transformers%20for%20Mesh-Based%20Simulations&entry.906535625=Paul%20Garnier%20and%20Vincent%20Lannelongue%20and%20Jonathan%20Viquerat%20and%20Elie%20Hachem&entry.1292438233=%20%20Simulating%20physics%20using%20Graph%20Neural%20Networks%20%28GNNs%29%20is%20predominantly%20driven%0Aby%20message-passing%20architectures%2C%20which%20face%20challenges%20in%20scaling%20and%0Aefficiency%2C%20particularly%20in%20handling%20large%2C%20complex%20meshes.%20These%20architectures%0Ahave%20inspired%20numerous%20enhancements%2C%20including%20multigrid%20approaches%20and%20%24K%24-hop%0Aaggregation%20%28using%20neighbours%20of%20distance%20%24K%24%29%2C%20yet%20they%20often%20introduce%0Asignificant%20complexity%20and%20suffer%20from%20limited%20in-depth%20investigations.%20In%0Aresponse%20to%20these%20challenges%2C%20we%20propose%20a%20novel%20Graph%20Transformer%20architecture%0Athat%20leverages%20the%20adjacency%20matrix%20as%20an%20attention%20mask.%20The%20proposed%20approach%0Aincorporates%20innovative%20augmentations%2C%20including%20Dilated%20Sliding%20Windows%20and%0AGlobal%20Attention%2C%20to%20extend%20receptive%20fields%20without%20sacrificing%20computational%0Aefficiency.%20Through%20extensive%20experimentation%2C%20we%20evaluate%20model%20size%2C%0Aadjacency%20matrix%20augmentations%2C%20positional%20encoding%20and%20%24K%24-hop%20configurations%0Ausing%20challenging%203D%20computational%20fluid%20dynamics%20%28CFD%29%20datasets.%20We%20also%20train%0Aover%2060%20models%20to%20find%20a%20scaling%20law%20between%20training%20FLOPs%20and%20parameters.%20The%0Aintroduced%20models%20demonstrate%20remarkable%20scalability%2C%20performing%20on%20meshes%20with%0Aup%20to%20300k%20nodes%20and%203%20million%20edges.%20Notably%2C%20the%20smallest%20model%20achieves%0Aparity%20with%20MeshGraphNet%20while%20being%20%247%5Ctimes%24%20faster%20and%20%246%5Ctimes%24%20smaller.%0AThe%20largest%20model%20surpasses%20the%20previous%20state-of-the-art%20by%20%2438.8%24%5C%25%20on%0Aaverage%20and%20outperforms%20MeshGraphNet%20by%20%2452%24%5C%25%20on%20the%20all-rollout%20RMSE%2C%20while%0Ahaving%20a%20similar%20training%20speed.%20Code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/DonsetPG/graph-physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18051v1&entry.124074799=Read"},
{"title": "BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain\n  Understanding", "author": "Xuan-Bac Nguyen and Thanh-Dat Truong and Pawan Sinha and Khoa Luu", "abstract": "  Memory decay makes it harder for the human brain to recognize visual objects\nand retain details. Consequently, recorded brain signals become weaker,\nuncertain, and contain poor visual context over time. This paper presents one\nof the first vision-learning approaches to address this problem. First, we\nstatistically and experimentally demonstrate the existence of inconsistency in\nbrain signals and its impact on the Vision-Brain Understanding (VBU) model. Our\nfindings show that brain signal representations shift over recording sessions,\nleading to compounding bias, which poses challenges for model learning and\ndegrades performance. Then, we propose a new Bias-Mitigation Continual Learning\n(BRAIN) approach to address these limitations. In this approach, the model is\ntrained in a continual learning setup and mitigates the growing bias from each\nlearning step. A new loss function named De-bias Contrastive Learning is also\nintroduced to address the bias problem. In addition, to prevent catastrophic\nforgetting, where the model loses knowledge from previous sessions, the new\nAngular-based Forgetting Mitigation approach is introduced to preserve learned\nknowledge in the model. Finally, the empirical experiments demonstrate that our\napproach achieves State-of-the-Art (SOTA) performance across various\nbenchmarks, surpassing prior and non-continual learning methods.\n", "link": "http://arxiv.org/abs/2508.18187v1", "date": "2025-08-25", "relevancy": 2.2527, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRAIN%3A%20Bias-Mitigation%20Continual%20Learning%20Approach%20to%20Vision-Brain%0A%20%20Understanding&body=Title%3A%20BRAIN%3A%20Bias-Mitigation%20Continual%20Learning%20Approach%20to%20Vision-Brain%0A%20%20Understanding%0AAuthor%3A%20Xuan-Bac%20Nguyen%20and%20Thanh-Dat%20Truong%20and%20Pawan%20Sinha%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Memory%20decay%20makes%20it%20harder%20for%20the%20human%20brain%20to%20recognize%20visual%20objects%0Aand%20retain%20details.%20Consequently%2C%20recorded%20brain%20signals%20become%20weaker%2C%0Auncertain%2C%20and%20contain%20poor%20visual%20context%20over%20time.%20This%20paper%20presents%20one%0Aof%20the%20first%20vision-learning%20approaches%20to%20address%20this%20problem.%20First%2C%20we%0Astatistically%20and%20experimentally%20demonstrate%20the%20existence%20of%20inconsistency%20in%0Abrain%20signals%20and%20its%20impact%20on%20the%20Vision-Brain%20Understanding%20%28VBU%29%20model.%20Our%0Afindings%20show%20that%20brain%20signal%20representations%20shift%20over%20recording%20sessions%2C%0Aleading%20to%20compounding%20bias%2C%20which%20poses%20challenges%20for%20model%20learning%20and%0Adegrades%20performance.%20Then%2C%20we%20propose%20a%20new%20Bias-Mitigation%20Continual%20Learning%0A%28BRAIN%29%20approach%20to%20address%20these%20limitations.%20In%20this%20approach%2C%20the%20model%20is%0Atrained%20in%20a%20continual%20learning%20setup%20and%20mitigates%20the%20growing%20bias%20from%20each%0Alearning%20step.%20A%20new%20loss%20function%20named%20De-bias%20Contrastive%20Learning%20is%20also%0Aintroduced%20to%20address%20the%20bias%20problem.%20In%20addition%2C%20to%20prevent%20catastrophic%0Aforgetting%2C%20where%20the%20model%20loses%20knowledge%20from%20previous%20sessions%2C%20the%20new%0AAngular-based%20Forgetting%20Mitigation%20approach%20is%20introduced%20to%20preserve%20learned%0Aknowledge%20in%20the%20model.%20Finally%2C%20the%20empirical%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20State-of-the-Art%20%28SOTA%29%20performance%20across%20various%0Abenchmarks%2C%20surpassing%20prior%20and%20non-continual%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRAIN%253A%2520Bias-Mitigation%2520Continual%2520Learning%2520Approach%2520to%2520Vision-Brain%250A%2520%2520Understanding%26entry.906535625%3DXuan-Bac%2520Nguyen%2520and%2520Thanh-Dat%2520Truong%2520and%2520Pawan%2520Sinha%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Memory%2520decay%2520makes%2520it%2520harder%2520for%2520the%2520human%2520brain%2520to%2520recognize%2520visual%2520objects%250Aand%2520retain%2520details.%2520Consequently%252C%2520recorded%2520brain%2520signals%2520become%2520weaker%252C%250Auncertain%252C%2520and%2520contain%2520poor%2520visual%2520context%2520over%2520time.%2520This%2520paper%2520presents%2520one%250Aof%2520the%2520first%2520vision-learning%2520approaches%2520to%2520address%2520this%2520problem.%2520First%252C%2520we%250Astatistically%2520and%2520experimentally%2520demonstrate%2520the%2520existence%2520of%2520inconsistency%2520in%250Abrain%2520signals%2520and%2520its%2520impact%2520on%2520the%2520Vision-Brain%2520Understanding%2520%2528VBU%2529%2520model.%2520Our%250Afindings%2520show%2520that%2520brain%2520signal%2520representations%2520shift%2520over%2520recording%2520sessions%252C%250Aleading%2520to%2520compounding%2520bias%252C%2520which%2520poses%2520challenges%2520for%2520model%2520learning%2520and%250Adegrades%2520performance.%2520Then%252C%2520we%2520propose%2520a%2520new%2520Bias-Mitigation%2520Continual%2520Learning%250A%2528BRAIN%2529%2520approach%2520to%2520address%2520these%2520limitations.%2520In%2520this%2520approach%252C%2520the%2520model%2520is%250Atrained%2520in%2520a%2520continual%2520learning%2520setup%2520and%2520mitigates%2520the%2520growing%2520bias%2520from%2520each%250Alearning%2520step.%2520A%2520new%2520loss%2520function%2520named%2520De-bias%2520Contrastive%2520Learning%2520is%2520also%250Aintroduced%2520to%2520address%2520the%2520bias%2520problem.%2520In%2520addition%252C%2520to%2520prevent%2520catastrophic%250Aforgetting%252C%2520where%2520the%2520model%2520loses%2520knowledge%2520from%2520previous%2520sessions%252C%2520the%2520new%250AAngular-based%2520Forgetting%2520Mitigation%2520approach%2520is%2520introduced%2520to%2520preserve%2520learned%250Aknowledge%2520in%2520the%2520model.%2520Finally%252C%2520the%2520empirical%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520State-of-the-Art%2520%2528SOTA%2529%2520performance%2520across%2520various%250Abenchmarks%252C%2520surpassing%2520prior%2520and%2520non-continual%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRAIN%3A%20Bias-Mitigation%20Continual%20Learning%20Approach%20to%20Vision-Brain%0A%20%20Understanding&entry.906535625=Xuan-Bac%20Nguyen%20and%20Thanh-Dat%20Truong%20and%20Pawan%20Sinha%20and%20Khoa%20Luu&entry.1292438233=%20%20Memory%20decay%20makes%20it%20harder%20for%20the%20human%20brain%20to%20recognize%20visual%20objects%0Aand%20retain%20details.%20Consequently%2C%20recorded%20brain%20signals%20become%20weaker%2C%0Auncertain%2C%20and%20contain%20poor%20visual%20context%20over%20time.%20This%20paper%20presents%20one%0Aof%20the%20first%20vision-learning%20approaches%20to%20address%20this%20problem.%20First%2C%20we%0Astatistically%20and%20experimentally%20demonstrate%20the%20existence%20of%20inconsistency%20in%0Abrain%20signals%20and%20its%20impact%20on%20the%20Vision-Brain%20Understanding%20%28VBU%29%20model.%20Our%0Afindings%20show%20that%20brain%20signal%20representations%20shift%20over%20recording%20sessions%2C%0Aleading%20to%20compounding%20bias%2C%20which%20poses%20challenges%20for%20model%20learning%20and%0Adegrades%20performance.%20Then%2C%20we%20propose%20a%20new%20Bias-Mitigation%20Continual%20Learning%0A%28BRAIN%29%20approach%20to%20address%20these%20limitations.%20In%20this%20approach%2C%20the%20model%20is%0Atrained%20in%20a%20continual%20learning%20setup%20and%20mitigates%20the%20growing%20bias%20from%20each%0Alearning%20step.%20A%20new%20loss%20function%20named%20De-bias%20Contrastive%20Learning%20is%20also%0Aintroduced%20to%20address%20the%20bias%20problem.%20In%20addition%2C%20to%20prevent%20catastrophic%0Aforgetting%2C%20where%20the%20model%20loses%20knowledge%20from%20previous%20sessions%2C%20the%20new%0AAngular-based%20Forgetting%20Mitigation%20approach%20is%20introduced%20to%20preserve%20learned%0Aknowledge%20in%20the%20model.%20Finally%2C%20the%20empirical%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20State-of-the-Art%20%28SOTA%29%20performance%20across%20various%0Abenchmarks%2C%20surpassing%20prior%20and%20non-continual%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18187v1&entry.124074799=Read"},
{"title": "An Unsupervised Deep XAI Framework for Localization of Concurrent Replay\n  Attacks in Nuclear Reactor Signals", "author": "Konstantinos Vasili and Zachery T. Dahm and Stylianos Chatzidakis", "abstract": "  Next generation advanced nuclear reactors are expected to be smaller both in\nsize and power output, relying extensively on fully digital instrumentation and\ncontrol systems. These reactors will generate a large flow of information in\nthe form of multivariate time series data, conveying simultaneously various non\nlinear cyber physical, process, control, sensor, and operational states.\nEnsuring data integrity against deception attacks is becoming increasingly\nimportant for networked communication and a requirement for safe and reliable\noperation. Current efforts to address replay attacks, almost universally focus\non watermarking or supervised anomaly detection approaches without further\nidentifying and characterizing the root cause of the anomaly. In addition,\nthese approaches rely mostly on synthetic data with uncorrelated Gaussian\nprocess and measurement noise and full state feedback or are limited to\nunivariate signals, signal stationarity, linear quadratic regulators, or other\nlinear-time invariant state-space which may fail to capture any unmodeled\nsystem dynamics. In the realm of regulated nuclear cyber-physical systems,\nadditional work is needed on characterization of replay attacks and\nexplainability of predictions using real data. Here, we propose an unsupervised\nexplainable AI framework based on a combination of autoencoder and customized\nwindowSHAP algorithm to fully characterize real-time replay attacks, i.e.,\ndetection, source identification, timing and type, of increasing complexity\nduring a dynamic time evolving reactor process. The proposed XAI framework was\nbenchmarked on several real world datasets from Purdue's nuclear reactor PUR-1\nwith up to six signals concurrently being replayed. In all cases, the XAI\nframework was able to detect and identify the source and number of signals\nbeing replayed and the duration of the falsification with 95 percent or better\naccuracy.\n", "link": "http://arxiv.org/abs/2508.09162v2", "date": "2025-08-25", "relevancy": 2.2508, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4678}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4438}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Unsupervised%20Deep%20XAI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%0A%20%20Attacks%20in%20Nuclear%20Reactor%20Signals&body=Title%3A%20An%20Unsupervised%20Deep%20XAI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%0A%20%20Attacks%20in%20Nuclear%20Reactor%20Signals%0AAuthor%3A%20Konstantinos%20Vasili%20and%20Zachery%20T.%20Dahm%20and%20Stylianos%20Chatzidakis%0AAbstract%3A%20%20%20Next%20generation%20advanced%20nuclear%20reactors%20are%20expected%20to%20be%20smaller%20both%20in%0Asize%20and%20power%20output%2C%20relying%20extensively%20on%20fully%20digital%20instrumentation%20and%0Acontrol%20systems.%20These%20reactors%20will%20generate%20a%20large%20flow%20of%20information%20in%0Athe%20form%20of%20multivariate%20time%20series%20data%2C%20conveying%20simultaneously%20various%20non%0Alinear%20cyber%20physical%2C%20process%2C%20control%2C%20sensor%2C%20and%20operational%20states.%0AEnsuring%20data%20integrity%20against%20deception%20attacks%20is%20becoming%20increasingly%0Aimportant%20for%20networked%20communication%20and%20a%20requirement%20for%20safe%20and%20reliable%0Aoperation.%20Current%20efforts%20to%20address%20replay%20attacks%2C%20almost%20universally%20focus%0Aon%20watermarking%20or%20supervised%20anomaly%20detection%20approaches%20without%20further%0Aidentifying%20and%20characterizing%20the%20root%20cause%20of%20the%20anomaly.%20In%20addition%2C%0Athese%20approaches%20rely%20mostly%20on%20synthetic%20data%20with%20uncorrelated%20Gaussian%0Aprocess%20and%20measurement%20noise%20and%20full%20state%20feedback%20or%20are%20limited%20to%0Aunivariate%20signals%2C%20signal%20stationarity%2C%20linear%20quadratic%20regulators%2C%20or%20other%0Alinear-time%20invariant%20state-space%20which%20may%20fail%20to%20capture%20any%20unmodeled%0Asystem%20dynamics.%20In%20the%20realm%20of%20regulated%20nuclear%20cyber-physical%20systems%2C%0Aadditional%20work%20is%20needed%20on%20characterization%20of%20replay%20attacks%20and%0Aexplainability%20of%20predictions%20using%20real%20data.%20Here%2C%20we%20propose%20an%20unsupervised%0Aexplainable%20AI%20framework%20based%20on%20a%20combination%20of%20autoencoder%20and%20customized%0AwindowSHAP%20algorithm%20to%20fully%20characterize%20real-time%20replay%20attacks%2C%20i.e.%2C%0Adetection%2C%20source%20identification%2C%20timing%20and%20type%2C%20of%20increasing%20complexity%0Aduring%20a%20dynamic%20time%20evolving%20reactor%20process.%20The%20proposed%20XAI%20framework%20was%0Abenchmarked%20on%20several%20real%20world%20datasets%20from%20Purdue%27s%20nuclear%20reactor%20PUR-1%0Awith%20up%20to%20six%20signals%20concurrently%20being%20replayed.%20In%20all%20cases%2C%20the%20XAI%0Aframework%20was%20able%20to%20detect%20and%20identify%20the%20source%20and%20number%20of%20signals%0Abeing%20replayed%20and%20the%20duration%20of%20the%20falsification%20with%2095%20percent%20or%20better%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Unsupervised%2520Deep%2520XAI%2520Framework%2520for%2520Localization%2520of%2520Concurrent%2520Replay%250A%2520%2520Attacks%2520in%2520Nuclear%2520Reactor%2520Signals%26entry.906535625%3DKonstantinos%2520Vasili%2520and%2520Zachery%2520T.%2520Dahm%2520and%2520Stylianos%2520Chatzidakis%26entry.1292438233%3D%2520%2520Next%2520generation%2520advanced%2520nuclear%2520reactors%2520are%2520expected%2520to%2520be%2520smaller%2520both%2520in%250Asize%2520and%2520power%2520output%252C%2520relying%2520extensively%2520on%2520fully%2520digital%2520instrumentation%2520and%250Acontrol%2520systems.%2520These%2520reactors%2520will%2520generate%2520a%2520large%2520flow%2520of%2520information%2520in%250Athe%2520form%2520of%2520multivariate%2520time%2520series%2520data%252C%2520conveying%2520simultaneously%2520various%2520non%250Alinear%2520cyber%2520physical%252C%2520process%252C%2520control%252C%2520sensor%252C%2520and%2520operational%2520states.%250AEnsuring%2520data%2520integrity%2520against%2520deception%2520attacks%2520is%2520becoming%2520increasingly%250Aimportant%2520for%2520networked%2520communication%2520and%2520a%2520requirement%2520for%2520safe%2520and%2520reliable%250Aoperation.%2520Current%2520efforts%2520to%2520address%2520replay%2520attacks%252C%2520almost%2520universally%2520focus%250Aon%2520watermarking%2520or%2520supervised%2520anomaly%2520detection%2520approaches%2520without%2520further%250Aidentifying%2520and%2520characterizing%2520the%2520root%2520cause%2520of%2520the%2520anomaly.%2520In%2520addition%252C%250Athese%2520approaches%2520rely%2520mostly%2520on%2520synthetic%2520data%2520with%2520uncorrelated%2520Gaussian%250Aprocess%2520and%2520measurement%2520noise%2520and%2520full%2520state%2520feedback%2520or%2520are%2520limited%2520to%250Aunivariate%2520signals%252C%2520signal%2520stationarity%252C%2520linear%2520quadratic%2520regulators%252C%2520or%2520other%250Alinear-time%2520invariant%2520state-space%2520which%2520may%2520fail%2520to%2520capture%2520any%2520unmodeled%250Asystem%2520dynamics.%2520In%2520the%2520realm%2520of%2520regulated%2520nuclear%2520cyber-physical%2520systems%252C%250Aadditional%2520work%2520is%2520needed%2520on%2520characterization%2520of%2520replay%2520attacks%2520and%250Aexplainability%2520of%2520predictions%2520using%2520real%2520data.%2520Here%252C%2520we%2520propose%2520an%2520unsupervised%250Aexplainable%2520AI%2520framework%2520based%2520on%2520a%2520combination%2520of%2520autoencoder%2520and%2520customized%250AwindowSHAP%2520algorithm%2520to%2520fully%2520characterize%2520real-time%2520replay%2520attacks%252C%2520i.e.%252C%250Adetection%252C%2520source%2520identification%252C%2520timing%2520and%2520type%252C%2520of%2520increasing%2520complexity%250Aduring%2520a%2520dynamic%2520time%2520evolving%2520reactor%2520process.%2520The%2520proposed%2520XAI%2520framework%2520was%250Abenchmarked%2520on%2520several%2520real%2520world%2520datasets%2520from%2520Purdue%2527s%2520nuclear%2520reactor%2520PUR-1%250Awith%2520up%2520to%2520six%2520signals%2520concurrently%2520being%2520replayed.%2520In%2520all%2520cases%252C%2520the%2520XAI%250Aframework%2520was%2520able%2520to%2520detect%2520and%2520identify%2520the%2520source%2520and%2520number%2520of%2520signals%250Abeing%2520replayed%2520and%2520the%2520duration%2520of%2520the%2520falsification%2520with%252095%2520percent%2520or%2520better%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Unsupervised%20Deep%20XAI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%0A%20%20Attacks%20in%20Nuclear%20Reactor%20Signals&entry.906535625=Konstantinos%20Vasili%20and%20Zachery%20T.%20Dahm%20and%20Stylianos%20Chatzidakis&entry.1292438233=%20%20Next%20generation%20advanced%20nuclear%20reactors%20are%20expected%20to%20be%20smaller%20both%20in%0Asize%20and%20power%20output%2C%20relying%20extensively%20on%20fully%20digital%20instrumentation%20and%0Acontrol%20systems.%20These%20reactors%20will%20generate%20a%20large%20flow%20of%20information%20in%0Athe%20form%20of%20multivariate%20time%20series%20data%2C%20conveying%20simultaneously%20various%20non%0Alinear%20cyber%20physical%2C%20process%2C%20control%2C%20sensor%2C%20and%20operational%20states.%0AEnsuring%20data%20integrity%20against%20deception%20attacks%20is%20becoming%20increasingly%0Aimportant%20for%20networked%20communication%20and%20a%20requirement%20for%20safe%20and%20reliable%0Aoperation.%20Current%20efforts%20to%20address%20replay%20attacks%2C%20almost%20universally%20focus%0Aon%20watermarking%20or%20supervised%20anomaly%20detection%20approaches%20without%20further%0Aidentifying%20and%20characterizing%20the%20root%20cause%20of%20the%20anomaly.%20In%20addition%2C%0Athese%20approaches%20rely%20mostly%20on%20synthetic%20data%20with%20uncorrelated%20Gaussian%0Aprocess%20and%20measurement%20noise%20and%20full%20state%20feedback%20or%20are%20limited%20to%0Aunivariate%20signals%2C%20signal%20stationarity%2C%20linear%20quadratic%20regulators%2C%20or%20other%0Alinear-time%20invariant%20state-space%20which%20may%20fail%20to%20capture%20any%20unmodeled%0Asystem%20dynamics.%20In%20the%20realm%20of%20regulated%20nuclear%20cyber-physical%20systems%2C%0Aadditional%20work%20is%20needed%20on%20characterization%20of%20replay%20attacks%20and%0Aexplainability%20of%20predictions%20using%20real%20data.%20Here%2C%20we%20propose%20an%20unsupervised%0Aexplainable%20AI%20framework%20based%20on%20a%20combination%20of%20autoencoder%20and%20customized%0AwindowSHAP%20algorithm%20to%20fully%20characterize%20real-time%20replay%20attacks%2C%20i.e.%2C%0Adetection%2C%20source%20identification%2C%20timing%20and%20type%2C%20of%20increasing%20complexity%0Aduring%20a%20dynamic%20time%20evolving%20reactor%20process.%20The%20proposed%20XAI%20framework%20was%0Abenchmarked%20on%20several%20real%20world%20datasets%20from%20Purdue%27s%20nuclear%20reactor%20PUR-1%0Awith%20up%20to%20six%20signals%20concurrently%20being%20replayed.%20In%20all%20cases%2C%20the%20XAI%0Aframework%20was%20able%20to%20detect%20and%20identify%20the%20source%20and%20number%20of%20signals%0Abeing%20replayed%20and%20the%20duration%20of%20the%20falsification%20with%2095%20percent%20or%20better%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09162v2&entry.124074799=Read"},
{"title": "Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing\n  Detection", "author": "Abyad Enan and Mashrur Chowdhury and Sagar Dasgupta and Mizanur Rahman", "abstract": "  Global Navigation Satellite Systems (GNSS) are critical for Positioning,\nNavigation, and Timing (PNT) applications. However, GNSS are highly vulnerable\nto spoofing attacks, where adversaries transmit counterfeit signals to mislead\nreceivers. Such attacks can lead to severe consequences, including misdirected\nnavigation, compromised data integrity, and operational disruptions. Most\nexisting spoofing detection methods depend on supervised learning techniques\nand struggle to detect novel, evolved, and unseen attacks. To overcome this\nlimitation, we develop a zero-day spoofing detection method using a Hybrid\nQuantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSS\nsignals without exposure to spoofed data. By leveraging features extracted\nduring the tracking stage, our method enables proactive detection before PNT\nsolutions are computed. We focus on spoofing detection in static GNSS\nreceivers, which are particularly susceptible to time-push spoofing attacks,\nwhere attackers manipulate timing information to induce incorrect time\ncomputations at the receiver. We evaluate our model against different unseen\ntime-push spoofing attack scenarios: simplistic, intermediate, and\nsophisticated. Our analysis demonstrates that the HQC-AE consistently\noutperforms its classical counterpart, traditional supervised learning-based\nmodels, and existing unsupervised learning-based methods in detecting zero-day,\nunseen GNSS time-push spoofing attacks, achieving an average detection accuracy\nof 97.71% with an average false negative rate of 0.62% (when an attack occurs\nbut is not detected). For sophisticated spoofing attacks, the HQC-AE attains an\naccuracy of 98.23% with a false negative rate of 1.85%. These findings\nhighlight the effectiveness of our method in proactively detecting zero-day\nGNSS time-push spoofing attacks across various stationary GNSS receiver\nplatforms.\n", "link": "http://arxiv.org/abs/2508.18085v1", "date": "2025-08-25", "relevancy": 2.2392, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4343}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum-Classical%20Hybrid%20Framework%20for%20Zero-Day%20Time-Push%20GNSS%20Spoofing%0A%20%20Detection&body=Title%3A%20Quantum-Classical%20Hybrid%20Framework%20for%20Zero-Day%20Time-Push%20GNSS%20Spoofing%0A%20%20Detection%0AAuthor%3A%20Abyad%20Enan%20and%20Mashrur%20Chowdhury%20and%20Sagar%20Dasgupta%20and%20Mizanur%20Rahman%0AAbstract%3A%20%20%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29%20are%20critical%20for%20Positioning%2C%0ANavigation%2C%20and%20Timing%20%28PNT%29%20applications.%20However%2C%20GNSS%20are%20highly%20vulnerable%0Ato%20spoofing%20attacks%2C%20where%20adversaries%20transmit%20counterfeit%20signals%20to%20mislead%0Areceivers.%20Such%20attacks%20can%20lead%20to%20severe%20consequences%2C%20including%20misdirected%0Anavigation%2C%20compromised%20data%20integrity%2C%20and%20operational%20disruptions.%20Most%0Aexisting%20spoofing%20detection%20methods%20depend%20on%20supervised%20learning%20techniques%0Aand%20struggle%20to%20detect%20novel%2C%20evolved%2C%20and%20unseen%20attacks.%20To%20overcome%20this%0Alimitation%2C%20we%20develop%20a%20zero-day%20spoofing%20detection%20method%20using%20a%20Hybrid%0AQuantum-Classical%20Autoencoder%20%28HQC-AE%29%2C%20trained%20solely%20on%20authentic%20GNSS%0Asignals%20without%20exposure%20to%20spoofed%20data.%20By%20leveraging%20features%20extracted%0Aduring%20the%20tracking%20stage%2C%20our%20method%20enables%20proactive%20detection%20before%20PNT%0Asolutions%20are%20computed.%20We%20focus%20on%20spoofing%20detection%20in%20static%20GNSS%0Areceivers%2C%20which%20are%20particularly%20susceptible%20to%20time-push%20spoofing%20attacks%2C%0Awhere%20attackers%20manipulate%20timing%20information%20to%20induce%20incorrect%20time%0Acomputations%20at%20the%20receiver.%20We%20evaluate%20our%20model%20against%20different%20unseen%0Atime-push%20spoofing%20attack%20scenarios%3A%20simplistic%2C%20intermediate%2C%20and%0Asophisticated.%20Our%20analysis%20demonstrates%20that%20the%20HQC-AE%20consistently%0Aoutperforms%20its%20classical%20counterpart%2C%20traditional%20supervised%20learning-based%0Amodels%2C%20and%20existing%20unsupervised%20learning-based%20methods%20in%20detecting%20zero-day%2C%0Aunseen%20GNSS%20time-push%20spoofing%20attacks%2C%20achieving%20an%20average%20detection%20accuracy%0Aof%2097.71%25%20with%20an%20average%20false%20negative%20rate%20of%200.62%25%20%28when%20an%20attack%20occurs%0Abut%20is%20not%20detected%29.%20For%20sophisticated%20spoofing%20attacks%2C%20the%20HQC-AE%20attains%20an%0Aaccuracy%20of%2098.23%25%20with%20a%20false%20negative%20rate%20of%201.85%25.%20These%20findings%0Ahighlight%20the%20effectiveness%20of%20our%20method%20in%20proactively%20detecting%20zero-day%0AGNSS%20time-push%20spoofing%20attacks%20across%20various%20stationary%20GNSS%20receiver%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum-Classical%2520Hybrid%2520Framework%2520for%2520Zero-Day%2520Time-Push%2520GNSS%2520Spoofing%250A%2520%2520Detection%26entry.906535625%3DAbyad%2520Enan%2520and%2520Mashrur%2520Chowdhury%2520and%2520Sagar%2520Dasgupta%2520and%2520Mizanur%2520Rahman%26entry.1292438233%3D%2520%2520Global%2520Navigation%2520Satellite%2520Systems%2520%2528GNSS%2529%2520are%2520critical%2520for%2520Positioning%252C%250ANavigation%252C%2520and%2520Timing%2520%2528PNT%2529%2520applications.%2520However%252C%2520GNSS%2520are%2520highly%2520vulnerable%250Ato%2520spoofing%2520attacks%252C%2520where%2520adversaries%2520transmit%2520counterfeit%2520signals%2520to%2520mislead%250Areceivers.%2520Such%2520attacks%2520can%2520lead%2520to%2520severe%2520consequences%252C%2520including%2520misdirected%250Anavigation%252C%2520compromised%2520data%2520integrity%252C%2520and%2520operational%2520disruptions.%2520Most%250Aexisting%2520spoofing%2520detection%2520methods%2520depend%2520on%2520supervised%2520learning%2520techniques%250Aand%2520struggle%2520to%2520detect%2520novel%252C%2520evolved%252C%2520and%2520unseen%2520attacks.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520develop%2520a%2520zero-day%2520spoofing%2520detection%2520method%2520using%2520a%2520Hybrid%250AQuantum-Classical%2520Autoencoder%2520%2528HQC-AE%2529%252C%2520trained%2520solely%2520on%2520authentic%2520GNSS%250Asignals%2520without%2520exposure%2520to%2520spoofed%2520data.%2520By%2520leveraging%2520features%2520extracted%250Aduring%2520the%2520tracking%2520stage%252C%2520our%2520method%2520enables%2520proactive%2520detection%2520before%2520PNT%250Asolutions%2520are%2520computed.%2520We%2520focus%2520on%2520spoofing%2520detection%2520in%2520static%2520GNSS%250Areceivers%252C%2520which%2520are%2520particularly%2520susceptible%2520to%2520time-push%2520spoofing%2520attacks%252C%250Awhere%2520attackers%2520manipulate%2520timing%2520information%2520to%2520induce%2520incorrect%2520time%250Acomputations%2520at%2520the%2520receiver.%2520We%2520evaluate%2520our%2520model%2520against%2520different%2520unseen%250Atime-push%2520spoofing%2520attack%2520scenarios%253A%2520simplistic%252C%2520intermediate%252C%2520and%250Asophisticated.%2520Our%2520analysis%2520demonstrates%2520that%2520the%2520HQC-AE%2520consistently%250Aoutperforms%2520its%2520classical%2520counterpart%252C%2520traditional%2520supervised%2520learning-based%250Amodels%252C%2520and%2520existing%2520unsupervised%2520learning-based%2520methods%2520in%2520detecting%2520zero-day%252C%250Aunseen%2520GNSS%2520time-push%2520spoofing%2520attacks%252C%2520achieving%2520an%2520average%2520detection%2520accuracy%250Aof%252097.71%2525%2520with%2520an%2520average%2520false%2520negative%2520rate%2520of%25200.62%2525%2520%2528when%2520an%2520attack%2520occurs%250Abut%2520is%2520not%2520detected%2529.%2520For%2520sophisticated%2520spoofing%2520attacks%252C%2520the%2520HQC-AE%2520attains%2520an%250Aaccuracy%2520of%252098.23%2525%2520with%2520a%2520false%2520negative%2520rate%2520of%25201.85%2525.%2520These%2520findings%250Ahighlight%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520proactively%2520detecting%2520zero-day%250AGNSS%2520time-push%2520spoofing%2520attacks%2520across%2520various%2520stationary%2520GNSS%2520receiver%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum-Classical%20Hybrid%20Framework%20for%20Zero-Day%20Time-Push%20GNSS%20Spoofing%0A%20%20Detection&entry.906535625=Abyad%20Enan%20and%20Mashrur%20Chowdhury%20and%20Sagar%20Dasgupta%20and%20Mizanur%20Rahman&entry.1292438233=%20%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29%20are%20critical%20for%20Positioning%2C%0ANavigation%2C%20and%20Timing%20%28PNT%29%20applications.%20However%2C%20GNSS%20are%20highly%20vulnerable%0Ato%20spoofing%20attacks%2C%20where%20adversaries%20transmit%20counterfeit%20signals%20to%20mislead%0Areceivers.%20Such%20attacks%20can%20lead%20to%20severe%20consequences%2C%20including%20misdirected%0Anavigation%2C%20compromised%20data%20integrity%2C%20and%20operational%20disruptions.%20Most%0Aexisting%20spoofing%20detection%20methods%20depend%20on%20supervised%20learning%20techniques%0Aand%20struggle%20to%20detect%20novel%2C%20evolved%2C%20and%20unseen%20attacks.%20To%20overcome%20this%0Alimitation%2C%20we%20develop%20a%20zero-day%20spoofing%20detection%20method%20using%20a%20Hybrid%0AQuantum-Classical%20Autoencoder%20%28HQC-AE%29%2C%20trained%20solely%20on%20authentic%20GNSS%0Asignals%20without%20exposure%20to%20spoofed%20data.%20By%20leveraging%20features%20extracted%0Aduring%20the%20tracking%20stage%2C%20our%20method%20enables%20proactive%20detection%20before%20PNT%0Asolutions%20are%20computed.%20We%20focus%20on%20spoofing%20detection%20in%20static%20GNSS%0Areceivers%2C%20which%20are%20particularly%20susceptible%20to%20time-push%20spoofing%20attacks%2C%0Awhere%20attackers%20manipulate%20timing%20information%20to%20induce%20incorrect%20time%0Acomputations%20at%20the%20receiver.%20We%20evaluate%20our%20model%20against%20different%20unseen%0Atime-push%20spoofing%20attack%20scenarios%3A%20simplistic%2C%20intermediate%2C%20and%0Asophisticated.%20Our%20analysis%20demonstrates%20that%20the%20HQC-AE%20consistently%0Aoutperforms%20its%20classical%20counterpart%2C%20traditional%20supervised%20learning-based%0Amodels%2C%20and%20existing%20unsupervised%20learning-based%20methods%20in%20detecting%20zero-day%2C%0Aunseen%20GNSS%20time-push%20spoofing%20attacks%2C%20achieving%20an%20average%20detection%20accuracy%0Aof%2097.71%25%20with%20an%20average%20false%20negative%20rate%20of%200.62%25%20%28when%20an%20attack%20occurs%0Abut%20is%20not%20detected%29.%20For%20sophisticated%20spoofing%20attacks%2C%20the%20HQC-AE%20attains%20an%0Aaccuracy%20of%2098.23%25%20with%20a%20false%20negative%20rate%20of%201.85%25.%20These%20findings%0Ahighlight%20the%20effectiveness%20of%20our%20method%20in%20proactively%20detecting%20zero-day%0AGNSS%20time-push%20spoofing%20attacks%20across%20various%20stationary%20GNSS%20receiver%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18085v1&entry.124074799=Read"},
{"title": "Designing Practical Models for Isolated Word Visual Speech Recognition", "author": "Iason Ioannis Panagos and Giorgos Sfikas and Christophoros Nikou", "abstract": "  Visual speech recognition (VSR) systems decode spoken words from an input\nsequence using only the video data. Practical applications of such systems\ninclude medical assistance as well as human-machine interactions. A VSR system\nis typically employed in a complementary role in cases where the audio is\ncorrupt or not available. In order to accurately predict the spoken words,\nthese architectures often rely on deep neural networks in order to extract\nmeaningful representations from the input sequence. While deep architectures\nachieve impressive recognition performance, relying on such models incurs\nsignificant computation costs which translates into increased resource demands\nin terms of hardware requirements and results in limited applicability in\nreal-world scenarios where resources might be constrained. This factor prevents\nwider adoption and deployment of speech recognition systems in more practical\napplications. In this work, we aim to alleviate this issue by developing\narchitectures for VSR that have low hardware costs. Following the standard\ntwo-network design paradigm, where one network handles visual feature\nextraction and another one utilizes the extracted features to classify the\nentire sequence, we develop lightweight end-to-end architectures by first\nbenchmarking efficient models from the image classification literature, and\nthen adopting lightweight block designs in a temporal convolution network\nbackbone. We create several unified models with low resource requirements but\nstrong recognition performance. Experiments on the largest public database for\nEnglish words demonstrate the effectiveness and practicality of our developed\nmodels. Code and trained models will be made publicly available.\n", "link": "http://arxiv.org/abs/2508.17894v1", "date": "2025-08-25", "relevancy": 2.2034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Practical%20Models%20for%20Isolated%20Word%20Visual%20Speech%20Recognition&body=Title%3A%20Designing%20Practical%20Models%20for%20Isolated%20Word%20Visual%20Speech%20Recognition%0AAuthor%3A%20Iason%20Ioannis%20Panagos%20and%20Giorgos%20Sfikas%20and%20Christophoros%20Nikou%0AAbstract%3A%20%20%20Visual%20speech%20recognition%20%28VSR%29%20systems%20decode%20spoken%20words%20from%20an%20input%0Asequence%20using%20only%20the%20video%20data.%20Practical%20applications%20of%20such%20systems%0Ainclude%20medical%20assistance%20as%20well%20as%20human-machine%20interactions.%20A%20VSR%20system%0Ais%20typically%20employed%20in%20a%20complementary%20role%20in%20cases%20where%20the%20audio%20is%0Acorrupt%20or%20not%20available.%20In%20order%20to%20accurately%20predict%20the%20spoken%20words%2C%0Athese%20architectures%20often%20rely%20on%20deep%20neural%20networks%20in%20order%20to%20extract%0Ameaningful%20representations%20from%20the%20input%20sequence.%20While%20deep%20architectures%0Aachieve%20impressive%20recognition%20performance%2C%20relying%20on%20such%20models%20incurs%0Asignificant%20computation%20costs%20which%20translates%20into%20increased%20resource%20demands%0Ain%20terms%20of%20hardware%20requirements%20and%20results%20in%20limited%20applicability%20in%0Areal-world%20scenarios%20where%20resources%20might%20be%20constrained.%20This%20factor%20prevents%0Awider%20adoption%20and%20deployment%20of%20speech%20recognition%20systems%20in%20more%20practical%0Aapplications.%20In%20this%20work%2C%20we%20aim%20to%20alleviate%20this%20issue%20by%20developing%0Aarchitectures%20for%20VSR%20that%20have%20low%20hardware%20costs.%20Following%20the%20standard%0Atwo-network%20design%20paradigm%2C%20where%20one%20network%20handles%20visual%20feature%0Aextraction%20and%20another%20one%20utilizes%20the%20extracted%20features%20to%20classify%20the%0Aentire%20sequence%2C%20we%20develop%20lightweight%20end-to-end%20architectures%20by%20first%0Abenchmarking%20efficient%20models%20from%20the%20image%20classification%20literature%2C%20and%0Athen%20adopting%20lightweight%20block%20designs%20in%20a%20temporal%20convolution%20network%0Abackbone.%20We%20create%20several%20unified%20models%20with%20low%20resource%20requirements%20but%0Astrong%20recognition%20performance.%20Experiments%20on%20the%20largest%20public%20database%20for%0AEnglish%20words%20demonstrate%20the%20effectiveness%20and%20practicality%20of%20our%20developed%0Amodels.%20Code%20and%20trained%20models%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Practical%2520Models%2520for%2520Isolated%2520Word%2520Visual%2520Speech%2520Recognition%26entry.906535625%3DIason%2520Ioannis%2520Panagos%2520and%2520Giorgos%2520Sfikas%2520and%2520Christophoros%2520Nikou%26entry.1292438233%3D%2520%2520Visual%2520speech%2520recognition%2520%2528VSR%2529%2520systems%2520decode%2520spoken%2520words%2520from%2520an%2520input%250Asequence%2520using%2520only%2520the%2520video%2520data.%2520Practical%2520applications%2520of%2520such%2520systems%250Ainclude%2520medical%2520assistance%2520as%2520well%2520as%2520human-machine%2520interactions.%2520A%2520VSR%2520system%250Ais%2520typically%2520employed%2520in%2520a%2520complementary%2520role%2520in%2520cases%2520where%2520the%2520audio%2520is%250Acorrupt%2520or%2520not%2520available.%2520In%2520order%2520to%2520accurately%2520predict%2520the%2520spoken%2520words%252C%250Athese%2520architectures%2520often%2520rely%2520on%2520deep%2520neural%2520networks%2520in%2520order%2520to%2520extract%250Ameaningful%2520representations%2520from%2520the%2520input%2520sequence.%2520While%2520deep%2520architectures%250Aachieve%2520impressive%2520recognition%2520performance%252C%2520relying%2520on%2520such%2520models%2520incurs%250Asignificant%2520computation%2520costs%2520which%2520translates%2520into%2520increased%2520resource%2520demands%250Ain%2520terms%2520of%2520hardware%2520requirements%2520and%2520results%2520in%2520limited%2520applicability%2520in%250Areal-world%2520scenarios%2520where%2520resources%2520might%2520be%2520constrained.%2520This%2520factor%2520prevents%250Awider%2520adoption%2520and%2520deployment%2520of%2520speech%2520recognition%2520systems%2520in%2520more%2520practical%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520alleviate%2520this%2520issue%2520by%2520developing%250Aarchitectures%2520for%2520VSR%2520that%2520have%2520low%2520hardware%2520costs.%2520Following%2520the%2520standard%250Atwo-network%2520design%2520paradigm%252C%2520where%2520one%2520network%2520handles%2520visual%2520feature%250Aextraction%2520and%2520another%2520one%2520utilizes%2520the%2520extracted%2520features%2520to%2520classify%2520the%250Aentire%2520sequence%252C%2520we%2520develop%2520lightweight%2520end-to-end%2520architectures%2520by%2520first%250Abenchmarking%2520efficient%2520models%2520from%2520the%2520image%2520classification%2520literature%252C%2520and%250Athen%2520adopting%2520lightweight%2520block%2520designs%2520in%2520a%2520temporal%2520convolution%2520network%250Abackbone.%2520We%2520create%2520several%2520unified%2520models%2520with%2520low%2520resource%2520requirements%2520but%250Astrong%2520recognition%2520performance.%2520Experiments%2520on%2520the%2520largest%2520public%2520database%2520for%250AEnglish%2520words%2520demonstrate%2520the%2520effectiveness%2520and%2520practicality%2520of%2520our%2520developed%250Amodels.%2520Code%2520and%2520trained%2520models%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Practical%20Models%20for%20Isolated%20Word%20Visual%20Speech%20Recognition&entry.906535625=Iason%20Ioannis%20Panagos%20and%20Giorgos%20Sfikas%20and%20Christophoros%20Nikou&entry.1292438233=%20%20Visual%20speech%20recognition%20%28VSR%29%20systems%20decode%20spoken%20words%20from%20an%20input%0Asequence%20using%20only%20the%20video%20data.%20Practical%20applications%20of%20such%20systems%0Ainclude%20medical%20assistance%20as%20well%20as%20human-machine%20interactions.%20A%20VSR%20system%0Ais%20typically%20employed%20in%20a%20complementary%20role%20in%20cases%20where%20the%20audio%20is%0Acorrupt%20or%20not%20available.%20In%20order%20to%20accurately%20predict%20the%20spoken%20words%2C%0Athese%20architectures%20often%20rely%20on%20deep%20neural%20networks%20in%20order%20to%20extract%0Ameaningful%20representations%20from%20the%20input%20sequence.%20While%20deep%20architectures%0Aachieve%20impressive%20recognition%20performance%2C%20relying%20on%20such%20models%20incurs%0Asignificant%20computation%20costs%20which%20translates%20into%20increased%20resource%20demands%0Ain%20terms%20of%20hardware%20requirements%20and%20results%20in%20limited%20applicability%20in%0Areal-world%20scenarios%20where%20resources%20might%20be%20constrained.%20This%20factor%20prevents%0Awider%20adoption%20and%20deployment%20of%20speech%20recognition%20systems%20in%20more%20practical%0Aapplications.%20In%20this%20work%2C%20we%20aim%20to%20alleviate%20this%20issue%20by%20developing%0Aarchitectures%20for%20VSR%20that%20have%20low%20hardware%20costs.%20Following%20the%20standard%0Atwo-network%20design%20paradigm%2C%20where%20one%20network%20handles%20visual%20feature%0Aextraction%20and%20another%20one%20utilizes%20the%20extracted%20features%20to%20classify%20the%0Aentire%20sequence%2C%20we%20develop%20lightweight%20end-to-end%20architectures%20by%20first%0Abenchmarking%20efficient%20models%20from%20the%20image%20classification%20literature%2C%20and%0Athen%20adopting%20lightweight%20block%20designs%20in%20a%20temporal%20convolution%20network%0Abackbone.%20We%20create%20several%20unified%20models%20with%20low%20resource%20requirements%20but%0Astrong%20recognition%20performance.%20Experiments%20on%20the%20largest%20public%20database%20for%0AEnglish%20words%20demonstrate%20the%20effectiveness%20and%20practicality%20of%20our%20developed%0Amodels.%20Code%20and%20trained%20models%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17894v1&entry.124074799=Read"},
{"title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection", "author": "Noah Frahm and Dongxu Zhao and Andrea Dunn Beltran and Ron Alterovitz and Jan-Michael Frahm and Junier Oliva and Roni Sengupta", "abstract": "  Next Best View (NBV) algorithms aim to maximize 3D scene acquisition quality\nusing minimal resources, e.g. number of acquisitions, time taken, or distance\ntraversed. Prior methods often rely on coverage maximization as a proxy for\nreconstruction quality, but for complex scenes with occlusions and finer\ndetails, this is not always sufficient and leads to poor reconstructions. Our\nkey insight is to train an acquisition policy that directly optimizes for\nreconstruction quality rather than just coverage. To achieve this, we introduce\nthe View Introspection Network (VIN): a lightweight neural network that\npredicts the Relative Reconstruction Improvement (RRI) of a potential next\nviewpoint without making any new acquisitions. We use this network to power a\nsimple, yet effective, sequential samplingbased greedy NBV policy. Our\napproach, VIN-NBV, generalizes to unseen object categories, operates without\nprior scene knowledge, is adaptable to resource constraints, and can handle\nocclusions. We show that our RRI fitness criterion leads to a ~30% gain in\nreconstruction quality over a coverage-based criterion using the same greedy\nstrategy. Furthermore, VIN-NBV also outperforms deep reinforcement learning\nmethods, Scan-RL and GenNBV, by ~40%.\n", "link": "http://arxiv.org/abs/2505.06219v3", "date": "2025-08-25", "relevancy": 2.1878, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection&body=Title%3A%20VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection%0AAuthor%3A%20Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20maximize%203D%20scene%20acquisition%20quality%0Ausing%20minimal%20resources%2C%20e.g.%20number%20of%20acquisitions%2C%20time%20taken%2C%20or%20distance%0Atraversed.%20Prior%20methods%20often%20rely%20on%20coverage%20maximization%20as%20a%20proxy%20for%0Areconstruction%20quality%2C%20but%20for%20complex%20scenes%20with%20occlusions%20and%20finer%0Adetails%2C%20this%20is%20not%20always%20sufficient%20and%20leads%20to%20poor%20reconstructions.%20Our%0Akey%20insight%20is%20to%20train%20an%20acquisition%20policy%20that%20directly%20optimizes%20for%0Areconstruction%20quality%20rather%20than%20just%20coverage.%20To%20achieve%20this%2C%20we%20introduce%0Athe%20View%20Introspection%20Network%20%28VIN%29%3A%20a%20lightweight%20neural%20network%20that%0Apredicts%20the%20Relative%20Reconstruction%20Improvement%20%28RRI%29%20of%20a%20potential%20next%0Aviewpoint%20without%20making%20any%20new%20acquisitions.%20We%20use%20this%20network%20to%20power%20a%0Asimple%2C%20yet%20effective%2C%20sequential%20samplingbased%20greedy%20NBV%20policy.%20Our%0Aapproach%2C%20VIN-NBV%2C%20generalizes%20to%20unseen%20object%20categories%2C%20operates%20without%0Aprior%20scene%20knowledge%2C%20is%20adaptable%20to%20resource%20constraints%2C%20and%20can%20handle%0Aocclusions.%20We%20show%20that%20our%20RRI%20fitness%20criterion%20leads%20to%20a%20~30%25%20gain%20in%0Areconstruction%20quality%20over%20a%20coverage-based%20criterion%20using%20the%20same%20greedy%0Astrategy.%20Furthermore%2C%20VIN-NBV%20also%20outperforms%20deep%20reinforcement%20learning%0Amethods%2C%20Scan-RL%20and%20GenNBV%2C%20by%20~40%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06219v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIN-NBV%253A%2520A%2520View%2520Introspection%2520Network%2520for%2520Next-Best-View%2520Selection%26entry.906535625%3DNoah%2520Frahm%2520and%2520Dongxu%2520Zhao%2520and%2520Andrea%2520Dunn%2520Beltran%2520and%2520Ron%2520Alterovitz%2520and%2520Jan-Michael%2520Frahm%2520and%2520Junier%2520Oliva%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520Next%2520Best%2520View%2520%2528NBV%2529%2520algorithms%2520aim%2520to%2520maximize%25203D%2520scene%2520acquisition%2520quality%250Ausing%2520minimal%2520resources%252C%2520e.g.%2520number%2520of%2520acquisitions%252C%2520time%2520taken%252C%2520or%2520distance%250Atraversed.%2520Prior%2520methods%2520often%2520rely%2520on%2520coverage%2520maximization%2520as%2520a%2520proxy%2520for%250Areconstruction%2520quality%252C%2520but%2520for%2520complex%2520scenes%2520with%2520occlusions%2520and%2520finer%250Adetails%252C%2520this%2520is%2520not%2520always%2520sufficient%2520and%2520leads%2520to%2520poor%2520reconstructions.%2520Our%250Akey%2520insight%2520is%2520to%2520train%2520an%2520acquisition%2520policy%2520that%2520directly%2520optimizes%2520for%250Areconstruction%2520quality%2520rather%2520than%2520just%2520coverage.%2520To%2520achieve%2520this%252C%2520we%2520introduce%250Athe%2520View%2520Introspection%2520Network%2520%2528VIN%2529%253A%2520a%2520lightweight%2520neural%2520network%2520that%250Apredicts%2520the%2520Relative%2520Reconstruction%2520Improvement%2520%2528RRI%2529%2520of%2520a%2520potential%2520next%250Aviewpoint%2520without%2520making%2520any%2520new%2520acquisitions.%2520We%2520use%2520this%2520network%2520to%2520power%2520a%250Asimple%252C%2520yet%2520effective%252C%2520sequential%2520samplingbased%2520greedy%2520NBV%2520policy.%2520Our%250Aapproach%252C%2520VIN-NBV%252C%2520generalizes%2520to%2520unseen%2520object%2520categories%252C%2520operates%2520without%250Aprior%2520scene%2520knowledge%252C%2520is%2520adaptable%2520to%2520resource%2520constraints%252C%2520and%2520can%2520handle%250Aocclusions.%2520We%2520show%2520that%2520our%2520RRI%2520fitness%2520criterion%2520leads%2520to%2520a%2520~30%2525%2520gain%2520in%250Areconstruction%2520quality%2520over%2520a%2520coverage-based%2520criterion%2520using%2520the%2520same%2520greedy%250Astrategy.%2520Furthermore%252C%2520VIN-NBV%2520also%2520outperforms%2520deep%2520reinforcement%2520learning%250Amethods%252C%2520Scan-RL%2520and%2520GenNBV%252C%2520by%2520~40%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06219v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIN-NBV%3A%20A%20View%20Introspection%20Network%20for%20Next-Best-View%20Selection&entry.906535625=Noah%20Frahm%20and%20Dongxu%20Zhao%20and%20Andrea%20Dunn%20Beltran%20and%20Ron%20Alterovitz%20and%20Jan-Michael%20Frahm%20and%20Junier%20Oliva%20and%20Roni%20Sengupta&entry.1292438233=%20%20Next%20Best%20View%20%28NBV%29%20algorithms%20aim%20to%20maximize%203D%20scene%20acquisition%20quality%0Ausing%20minimal%20resources%2C%20e.g.%20number%20of%20acquisitions%2C%20time%20taken%2C%20or%20distance%0Atraversed.%20Prior%20methods%20often%20rely%20on%20coverage%20maximization%20as%20a%20proxy%20for%0Areconstruction%20quality%2C%20but%20for%20complex%20scenes%20with%20occlusions%20and%20finer%0Adetails%2C%20this%20is%20not%20always%20sufficient%20and%20leads%20to%20poor%20reconstructions.%20Our%0Akey%20insight%20is%20to%20train%20an%20acquisition%20policy%20that%20directly%20optimizes%20for%0Areconstruction%20quality%20rather%20than%20just%20coverage.%20To%20achieve%20this%2C%20we%20introduce%0Athe%20View%20Introspection%20Network%20%28VIN%29%3A%20a%20lightweight%20neural%20network%20that%0Apredicts%20the%20Relative%20Reconstruction%20Improvement%20%28RRI%29%20of%20a%20potential%20next%0Aviewpoint%20without%20making%20any%20new%20acquisitions.%20We%20use%20this%20network%20to%20power%20a%0Asimple%2C%20yet%20effective%2C%20sequential%20samplingbased%20greedy%20NBV%20policy.%20Our%0Aapproach%2C%20VIN-NBV%2C%20generalizes%20to%20unseen%20object%20categories%2C%20operates%20without%0Aprior%20scene%20knowledge%2C%20is%20adaptable%20to%20resource%20constraints%2C%20and%20can%20handle%0Aocclusions.%20We%20show%20that%20our%20RRI%20fitness%20criterion%20leads%20to%20a%20~30%25%20gain%20in%0Areconstruction%20quality%20over%20a%20coverage-based%20criterion%20using%20the%20same%20greedy%0Astrategy.%20Furthermore%2C%20VIN-NBV%20also%20outperforms%20deep%20reinforcement%20learning%0Amethods%2C%20Scan-RL%20and%20GenNBV%2C%20by%20~40%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06219v3&entry.124074799=Read"},
{"title": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning", "author": "Chao Feng and Yuanzhe Gao and Alberto Huertas Celdran and Gerome Bovet and Burkhard Stiller", "abstract": "  Federated Learning (FL) is widely recognized as a privacy-preserving Machine\nLearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. Nevertheless, model training leaves exploitable traces that can be\nused to infer sensitive information. In Decentralized FL (DFL), the topology,\ndefining how participants are connected, plays a crucial role in shaping the\nmodel's privacy, robustness, and convergence. However, the topology introduces\nan unexplored vulnerability: attackers can exploit it to infer participant\nrelationships and launch targeted attacks. This work uncovers the hidden risks\nof DFL topologies by proposing a novel Topology Inference Attack that infers\nthe topology solely from model behavior. A taxonomy of topology inference\nattacks is introduced, categorizing them by the attacker's capabilities and\nknowledge. Practical attack strategies are designed for various scenarios, and\nexperiments are conducted to identify key factors influencing attack success.\nThe results demonstrate that analyzing only the model of each node can\naccurately infer the DFL topology, highlighting a critical privacy risk in DFL\nsystems. These findings offer insights for improving privacy preservation in\nDFL environments.\n", "link": "http://arxiv.org/abs/2501.03119v3", "date": "2025-08-25", "relevancy": 2.1857, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning&body=Title%3A%20From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning%0AAuthor%3A%20Chao%20Feng%20and%20Yuanzhe%20Gao%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20widely%20recognized%20as%20a%20privacy-preserving%20Machine%0ALearning%20paradigm%20due%20to%20its%20model-sharing%20mechanism%20that%20avoids%20direct%20data%0Aexchange.%20Nevertheless%2C%20model%20training%20leaves%20exploitable%20traces%20that%20can%20be%0Aused%20to%20infer%20sensitive%20information.%20In%20Decentralized%20FL%20%28DFL%29%2C%20the%20topology%2C%0Adefining%20how%20participants%20are%20connected%2C%20plays%20a%20crucial%20role%20in%20shaping%20the%0Amodel%27s%20privacy%2C%20robustness%2C%20and%20convergence.%20However%2C%20the%20topology%20introduces%0Aan%20unexplored%20vulnerability%3A%20attackers%20can%20exploit%20it%20to%20infer%20participant%0Arelationships%20and%20launch%20targeted%20attacks.%20This%20work%20uncovers%20the%20hidden%20risks%0Aof%20DFL%20topologies%20by%20proposing%20a%20novel%20Topology%20Inference%20Attack%20that%20infers%0Athe%20topology%20solely%20from%20model%20behavior.%20A%20taxonomy%20of%20topology%20inference%0Aattacks%20is%20introduced%2C%20categorizing%20them%20by%20the%20attacker%27s%20capabilities%20and%0Aknowledge.%20Practical%20attack%20strategies%20are%20designed%20for%20various%20scenarios%2C%20and%0Aexperiments%20are%20conducted%20to%20identify%20key%20factors%20influencing%20attack%20success.%0AThe%20results%20demonstrate%20that%20analyzing%20only%20the%20model%20of%20each%20node%20can%0Aaccurately%20infer%20the%20DFL%20topology%2C%20highlighting%20a%20critical%20privacy%20risk%20in%20DFL%0Asystems.%20These%20findings%20offer%20insights%20for%20improving%20privacy%20preservation%20in%0ADFL%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Models%2520to%2520Network%2520Topologies%253A%2520A%2520Topology%2520Inference%2520Attack%2520in%250A%2520%2520Decentralized%2520Federated%2520Learning%26entry.906535625%3DChao%2520Feng%2520and%2520Yuanzhe%2520Gao%2520and%2520Alberto%2520Huertas%2520Celdran%2520and%2520Gerome%2520Bovet%2520and%2520Burkhard%2520Stiller%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520widely%2520recognized%2520as%2520a%2520privacy-preserving%2520Machine%250ALearning%2520paradigm%2520due%2520to%2520its%2520model-sharing%2520mechanism%2520that%2520avoids%2520direct%2520data%250Aexchange.%2520Nevertheless%252C%2520model%2520training%2520leaves%2520exploitable%2520traces%2520that%2520can%2520be%250Aused%2520to%2520infer%2520sensitive%2520information.%2520In%2520Decentralized%2520FL%2520%2528DFL%2529%252C%2520the%2520topology%252C%250Adefining%2520how%2520participants%2520are%2520connected%252C%2520plays%2520a%2520crucial%2520role%2520in%2520shaping%2520the%250Amodel%2527s%2520privacy%252C%2520robustness%252C%2520and%2520convergence.%2520However%252C%2520the%2520topology%2520introduces%250Aan%2520unexplored%2520vulnerability%253A%2520attackers%2520can%2520exploit%2520it%2520to%2520infer%2520participant%250Arelationships%2520and%2520launch%2520targeted%2520attacks.%2520This%2520work%2520uncovers%2520the%2520hidden%2520risks%250Aof%2520DFL%2520topologies%2520by%2520proposing%2520a%2520novel%2520Topology%2520Inference%2520Attack%2520that%2520infers%250Athe%2520topology%2520solely%2520from%2520model%2520behavior.%2520A%2520taxonomy%2520of%2520topology%2520inference%250Aattacks%2520is%2520introduced%252C%2520categorizing%2520them%2520by%2520the%2520attacker%2527s%2520capabilities%2520and%250Aknowledge.%2520Practical%2520attack%2520strategies%2520are%2520designed%2520for%2520various%2520scenarios%252C%2520and%250Aexperiments%2520are%2520conducted%2520to%2520identify%2520key%2520factors%2520influencing%2520attack%2520success.%250AThe%2520results%2520demonstrate%2520that%2520analyzing%2520only%2520the%2520model%2520of%2520each%2520node%2520can%250Aaccurately%2520infer%2520the%2520DFL%2520topology%252C%2520highlighting%2520a%2520critical%2520privacy%2520risk%2520in%2520DFL%250Asystems.%2520These%2520findings%2520offer%2520insights%2520for%2520improving%2520privacy%2520preservation%2520in%250ADFL%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning&entry.906535625=Chao%20Feng%20and%20Yuanzhe%20Gao%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20widely%20recognized%20as%20a%20privacy-preserving%20Machine%0ALearning%20paradigm%20due%20to%20its%20model-sharing%20mechanism%20that%20avoids%20direct%20data%0Aexchange.%20Nevertheless%2C%20model%20training%20leaves%20exploitable%20traces%20that%20can%20be%0Aused%20to%20infer%20sensitive%20information.%20In%20Decentralized%20FL%20%28DFL%29%2C%20the%20topology%2C%0Adefining%20how%20participants%20are%20connected%2C%20plays%20a%20crucial%20role%20in%20shaping%20the%0Amodel%27s%20privacy%2C%20robustness%2C%20and%20convergence.%20However%2C%20the%20topology%20introduces%0Aan%20unexplored%20vulnerability%3A%20attackers%20can%20exploit%20it%20to%20infer%20participant%0Arelationships%20and%20launch%20targeted%20attacks.%20This%20work%20uncovers%20the%20hidden%20risks%0Aof%20DFL%20topologies%20by%20proposing%20a%20novel%20Topology%20Inference%20Attack%20that%20infers%0Athe%20topology%20solely%20from%20model%20behavior.%20A%20taxonomy%20of%20topology%20inference%0Aattacks%20is%20introduced%2C%20categorizing%20them%20by%20the%20attacker%27s%20capabilities%20and%0Aknowledge.%20Practical%20attack%20strategies%20are%20designed%20for%20various%20scenarios%2C%20and%0Aexperiments%20are%20conducted%20to%20identify%20key%20factors%20influencing%20attack%20success.%0AThe%20results%20demonstrate%20that%20analyzing%20only%20the%20model%20of%20each%20node%20can%0Aaccurately%20infer%20the%20DFL%20topology%2C%20highlighting%20a%20critical%20privacy%20risk%20in%20DFL%0Asystems.%20These%20findings%20offer%20insights%20for%20improving%20privacy%20preservation%20in%0ADFL%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03119v3&entry.124074799=Read"},
{"title": "Integration of Computer Vision with Adaptive Control for Autonomous\n  Driving Using ADORE", "author": "Abu Shad Ahammed and Md Shahi Amran Hossain and Sayeri Mukherjee and Roman Obermaisser and Md. Ziaur Rahman", "abstract": "  Ensuring safety in autonomous driving requires a seamless integration of\nperception and decision making under uncertain conditions. Although computer\nvision (CV) models such as YOLO achieve high accuracy in detecting traffic\nsigns and obstacles, their performance degrades in drift scenarios caused by\nweather variations or unseen objects. This work presents a simulated autonomous\ndriving system that combines a context aware CV model with adaptive control\nusing the ADORE framework. The CARLA simulator was integrated with ADORE via\nthe ROS bridge, allowing real-time communication between perception, decision,\nand control modules. A simulated test case was designed in both clear and drift\nweather conditions to demonstrate the robust detection performance of the\nperception model while ADORE successfully adapted vehicle behavior to speed\nlimits and obstacles with low response latency. The findings highlight the\npotential of coupling deep learning-based perception with rule-based adaptive\ndecision making to improve automotive safety critical system.\n", "link": "http://arxiv.org/abs/2508.17985v1", "date": "2025-08-25", "relevancy": 2.1761, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20Computer%20Vision%20with%20Adaptive%20Control%20for%20Autonomous%0A%20%20Driving%20Using%20ADORE&body=Title%3A%20Integration%20of%20Computer%20Vision%20with%20Adaptive%20Control%20for%20Autonomous%0A%20%20Driving%20Using%20ADORE%0AAuthor%3A%20Abu%20Shad%20Ahammed%20and%20Md%20Shahi%20Amran%20Hossain%20and%20Sayeri%20Mukherjee%20and%20Roman%20Obermaisser%20and%20Md.%20Ziaur%20Rahman%0AAbstract%3A%20%20%20Ensuring%20safety%20in%20autonomous%20driving%20requires%20a%20seamless%20integration%20of%0Aperception%20and%20decision%20making%20under%20uncertain%20conditions.%20Although%20computer%0Avision%20%28CV%29%20models%20such%20as%20YOLO%20achieve%20high%20accuracy%20in%20detecting%20traffic%0Asigns%20and%20obstacles%2C%20their%20performance%20degrades%20in%20drift%20scenarios%20caused%20by%0Aweather%20variations%20or%20unseen%20objects.%20This%20work%20presents%20a%20simulated%20autonomous%0Adriving%20system%20that%20combines%20a%20context%20aware%20CV%20model%20with%20adaptive%20control%0Ausing%20the%20ADORE%20framework.%20The%20CARLA%20simulator%20was%20integrated%20with%20ADORE%20via%0Athe%20ROS%20bridge%2C%20allowing%20real-time%20communication%20between%20perception%2C%20decision%2C%0Aand%20control%20modules.%20A%20simulated%20test%20case%20was%20designed%20in%20both%20clear%20and%20drift%0Aweather%20conditions%20to%20demonstrate%20the%20robust%20detection%20performance%20of%20the%0Aperception%20model%20while%20ADORE%20successfully%20adapted%20vehicle%20behavior%20to%20speed%0Alimits%20and%20obstacles%20with%20low%20response%20latency.%20The%20findings%20highlight%20the%0Apotential%20of%20coupling%20deep%20learning-based%20perception%20with%20rule-based%20adaptive%0Adecision%20making%20to%20improve%20automotive%20safety%20critical%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520Computer%2520Vision%2520with%2520Adaptive%2520Control%2520for%2520Autonomous%250A%2520%2520Driving%2520Using%2520ADORE%26entry.906535625%3DAbu%2520Shad%2520Ahammed%2520and%2520Md%2520Shahi%2520Amran%2520Hossain%2520and%2520Sayeri%2520Mukherjee%2520and%2520Roman%2520Obermaisser%2520and%2520Md.%2520Ziaur%2520Rahman%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520in%2520autonomous%2520driving%2520requires%2520a%2520seamless%2520integration%2520of%250Aperception%2520and%2520decision%2520making%2520under%2520uncertain%2520conditions.%2520Although%2520computer%250Avision%2520%2528CV%2529%2520models%2520such%2520as%2520YOLO%2520achieve%2520high%2520accuracy%2520in%2520detecting%2520traffic%250Asigns%2520and%2520obstacles%252C%2520their%2520performance%2520degrades%2520in%2520drift%2520scenarios%2520caused%2520by%250Aweather%2520variations%2520or%2520unseen%2520objects.%2520This%2520work%2520presents%2520a%2520simulated%2520autonomous%250Adriving%2520system%2520that%2520combines%2520a%2520context%2520aware%2520CV%2520model%2520with%2520adaptive%2520control%250Ausing%2520the%2520ADORE%2520framework.%2520The%2520CARLA%2520simulator%2520was%2520integrated%2520with%2520ADORE%2520via%250Athe%2520ROS%2520bridge%252C%2520allowing%2520real-time%2520communication%2520between%2520perception%252C%2520decision%252C%250Aand%2520control%2520modules.%2520A%2520simulated%2520test%2520case%2520was%2520designed%2520in%2520both%2520clear%2520and%2520drift%250Aweather%2520conditions%2520to%2520demonstrate%2520the%2520robust%2520detection%2520performance%2520of%2520the%250Aperception%2520model%2520while%2520ADORE%2520successfully%2520adapted%2520vehicle%2520behavior%2520to%2520speed%250Alimits%2520and%2520obstacles%2520with%2520low%2520response%2520latency.%2520The%2520findings%2520highlight%2520the%250Apotential%2520of%2520coupling%2520deep%2520learning-based%2520perception%2520with%2520rule-based%2520adaptive%250Adecision%2520making%2520to%2520improve%2520automotive%2520safety%2520critical%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20Computer%20Vision%20with%20Adaptive%20Control%20for%20Autonomous%0A%20%20Driving%20Using%20ADORE&entry.906535625=Abu%20Shad%20Ahammed%20and%20Md%20Shahi%20Amran%20Hossain%20and%20Sayeri%20Mukherjee%20and%20Roman%20Obermaisser%20and%20Md.%20Ziaur%20Rahman&entry.1292438233=%20%20Ensuring%20safety%20in%20autonomous%20driving%20requires%20a%20seamless%20integration%20of%0Aperception%20and%20decision%20making%20under%20uncertain%20conditions.%20Although%20computer%0Avision%20%28CV%29%20models%20such%20as%20YOLO%20achieve%20high%20accuracy%20in%20detecting%20traffic%0Asigns%20and%20obstacles%2C%20their%20performance%20degrades%20in%20drift%20scenarios%20caused%20by%0Aweather%20variations%20or%20unseen%20objects.%20This%20work%20presents%20a%20simulated%20autonomous%0Adriving%20system%20that%20combines%20a%20context%20aware%20CV%20model%20with%20adaptive%20control%0Ausing%20the%20ADORE%20framework.%20The%20CARLA%20simulator%20was%20integrated%20with%20ADORE%20via%0Athe%20ROS%20bridge%2C%20allowing%20real-time%20communication%20between%20perception%2C%20decision%2C%0Aand%20control%20modules.%20A%20simulated%20test%20case%20was%20designed%20in%20both%20clear%20and%20drift%0Aweather%20conditions%20to%20demonstrate%20the%20robust%20detection%20performance%20of%20the%0Aperception%20model%20while%20ADORE%20successfully%20adapted%20vehicle%20behavior%20to%20speed%0Alimits%20and%20obstacles%20with%20low%20response%20latency.%20The%20findings%20highlight%20the%0Apotential%20of%20coupling%20deep%20learning-based%20perception%20with%20rule-based%20adaptive%0Adecision%20making%20to%20improve%20automotive%20safety%20critical%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17985v1&entry.124074799=Read"},
{"title": "Assessing the Noise Robustness of Class Activation Maps: A Framework for\n  Reliable Model Interpretability", "author": "Syamantak Sarkar and Revoti P. Bora and Bhupender Kaushal and Sudhish N George and Kiran Raja", "abstract": "  Class Activation Maps (CAMs) are one of the important methods for visualizing\nregions used by deep learning models. Yet their robustness to different noise\nremains underexplored. In this work, we evaluate and report the resilience of\nvarious CAM methods for different noise perturbations across multiple\narchitectures and datasets. By analyzing the influence of different noise types\non CAM explanations, we assess the susceptibility to noise and the extent to\nwhich dataset characteristics may impact explanation stability. The findings\nhighlight considerable variability in noise sensitivity for various CAMs. We\npropose a robustness metric for CAMs that captures two key properties:\nconsistency and responsiveness. Consistency reflects the ability of CAMs to\nremain stable under input perturbations that do not alter the predicted class,\nwhile responsiveness measures the sensitivity of CAMs to changes in the\nprediction caused by such perturbations. The metric is evaluated empirically\nacross models, different perturbations, and datasets along with complementary\nstatistical tests to exemplify the applicability of our proposed approach.\n", "link": "http://arxiv.org/abs/2508.18154v1", "date": "2025-08-25", "relevancy": 2.17, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Noise%20Robustness%20of%20Class%20Activation%20Maps%3A%20A%20Framework%20for%0A%20%20Reliable%20Model%20Interpretability&body=Title%3A%20Assessing%20the%20Noise%20Robustness%20of%20Class%20Activation%20Maps%3A%20A%20Framework%20for%0A%20%20Reliable%20Model%20Interpretability%0AAuthor%3A%20Syamantak%20Sarkar%20and%20Revoti%20P.%20Bora%20and%20Bhupender%20Kaushal%20and%20Sudhish%20N%20George%20and%20Kiran%20Raja%0AAbstract%3A%20%20%20Class%20Activation%20Maps%20%28CAMs%29%20are%20one%20of%20the%20important%20methods%20for%20visualizing%0Aregions%20used%20by%20deep%20learning%20models.%20Yet%20their%20robustness%20to%20different%20noise%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20evaluate%20and%20report%20the%20resilience%20of%0Avarious%20CAM%20methods%20for%20different%20noise%20perturbations%20across%20multiple%0Aarchitectures%20and%20datasets.%20By%20analyzing%20the%20influence%20of%20different%20noise%20types%0Aon%20CAM%20explanations%2C%20we%20assess%20the%20susceptibility%20to%20noise%20and%20the%20extent%20to%0Awhich%20dataset%20characteristics%20may%20impact%20explanation%20stability.%20The%20findings%0Ahighlight%20considerable%20variability%20in%20noise%20sensitivity%20for%20various%20CAMs.%20We%0Apropose%20a%20robustness%20metric%20for%20CAMs%20that%20captures%20two%20key%20properties%3A%0Aconsistency%20and%20responsiveness.%20Consistency%20reflects%20the%20ability%20of%20CAMs%20to%0Aremain%20stable%20under%20input%20perturbations%20that%20do%20not%20alter%20the%20predicted%20class%2C%0Awhile%20responsiveness%20measures%20the%20sensitivity%20of%20CAMs%20to%20changes%20in%20the%0Aprediction%20caused%20by%20such%20perturbations.%20The%20metric%20is%20evaluated%20empirically%0Aacross%20models%2C%20different%20perturbations%2C%20and%20datasets%20along%20with%20complementary%0Astatistical%20tests%20to%20exemplify%20the%20applicability%20of%20our%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Noise%2520Robustness%2520of%2520Class%2520Activation%2520Maps%253A%2520A%2520Framework%2520for%250A%2520%2520Reliable%2520Model%2520Interpretability%26entry.906535625%3DSyamantak%2520Sarkar%2520and%2520Revoti%2520P.%2520Bora%2520and%2520Bhupender%2520Kaushal%2520and%2520Sudhish%2520N%2520George%2520and%2520Kiran%2520Raja%26entry.1292438233%3D%2520%2520Class%2520Activation%2520Maps%2520%2528CAMs%2529%2520are%2520one%2520of%2520the%2520important%2520methods%2520for%2520visualizing%250Aregions%2520used%2520by%2520deep%2520learning%2520models.%2520Yet%2520their%2520robustness%2520to%2520different%2520noise%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520and%2520report%2520the%2520resilience%2520of%250Avarious%2520CAM%2520methods%2520for%2520different%2520noise%2520perturbations%2520across%2520multiple%250Aarchitectures%2520and%2520datasets.%2520By%2520analyzing%2520the%2520influence%2520of%2520different%2520noise%2520types%250Aon%2520CAM%2520explanations%252C%2520we%2520assess%2520the%2520susceptibility%2520to%2520noise%2520and%2520the%2520extent%2520to%250Awhich%2520dataset%2520characteristics%2520may%2520impact%2520explanation%2520stability.%2520The%2520findings%250Ahighlight%2520considerable%2520variability%2520in%2520noise%2520sensitivity%2520for%2520various%2520CAMs.%2520We%250Apropose%2520a%2520robustness%2520metric%2520for%2520CAMs%2520that%2520captures%2520two%2520key%2520properties%253A%250Aconsistency%2520and%2520responsiveness.%2520Consistency%2520reflects%2520the%2520ability%2520of%2520CAMs%2520to%250Aremain%2520stable%2520under%2520input%2520perturbations%2520that%2520do%2520not%2520alter%2520the%2520predicted%2520class%252C%250Awhile%2520responsiveness%2520measures%2520the%2520sensitivity%2520of%2520CAMs%2520to%2520changes%2520in%2520the%250Aprediction%2520caused%2520by%2520such%2520perturbations.%2520The%2520metric%2520is%2520evaluated%2520empirically%250Aacross%2520models%252C%2520different%2520perturbations%252C%2520and%2520datasets%2520along%2520with%2520complementary%250Astatistical%2520tests%2520to%2520exemplify%2520the%2520applicability%2520of%2520our%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Noise%20Robustness%20of%20Class%20Activation%20Maps%3A%20A%20Framework%20for%0A%20%20Reliable%20Model%20Interpretability&entry.906535625=Syamantak%20Sarkar%20and%20Revoti%20P.%20Bora%20and%20Bhupender%20Kaushal%20and%20Sudhish%20N%20George%20and%20Kiran%20Raja&entry.1292438233=%20%20Class%20Activation%20Maps%20%28CAMs%29%20are%20one%20of%20the%20important%20methods%20for%20visualizing%0Aregions%20used%20by%20deep%20learning%20models.%20Yet%20their%20robustness%20to%20different%20noise%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20evaluate%20and%20report%20the%20resilience%20of%0Avarious%20CAM%20methods%20for%20different%20noise%20perturbations%20across%20multiple%0Aarchitectures%20and%20datasets.%20By%20analyzing%20the%20influence%20of%20different%20noise%20types%0Aon%20CAM%20explanations%2C%20we%20assess%20the%20susceptibility%20to%20noise%20and%20the%20extent%20to%0Awhich%20dataset%20characteristics%20may%20impact%20explanation%20stability.%20The%20findings%0Ahighlight%20considerable%20variability%20in%20noise%20sensitivity%20for%20various%20CAMs.%20We%0Apropose%20a%20robustness%20metric%20for%20CAMs%20that%20captures%20two%20key%20properties%3A%0Aconsistency%20and%20responsiveness.%20Consistency%20reflects%20the%20ability%20of%20CAMs%20to%0Aremain%20stable%20under%20input%20perturbations%20that%20do%20not%20alter%20the%20predicted%20class%2C%0Awhile%20responsiveness%20measures%20the%20sensitivity%20of%20CAMs%20to%20changes%20in%20the%0Aprediction%20caused%20by%20such%20perturbations.%20The%20metric%20is%20evaluated%20empirically%0Aacross%20models%2C%20different%20perturbations%2C%20and%20datasets%20along%20with%20complementary%0Astatistical%20tests%20to%20exemplify%20the%20applicability%20of%20our%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18154v1&entry.124074799=Read"},
{"title": "FCR: Investigating Generative AI models for Forensic Craniofacial\n  Reconstruction", "author": "Ravi Shankar Prasad and Dinesh Singh", "abstract": "  Craniofacial reconstruction in forensics is one of the processes to identify\nvictims of crime and natural disasters. Identifying an individual from their\nremains plays a crucial role when all other identification methods fail.\nTraditional methods for this task, such as clay-based craniofacial\nreconstruction, require expert domain knowledge and are a time-consuming\nprocess. At the same time, other probabilistic generative models like the\nstatistical shape model or the Basel face model fail to capture the skull and\nface cross-domain attributes. Looking at these limitations, we propose a\ngeneric framework for craniofacial reconstruction from 2D X-ray images. Here,\nwe used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune\nthe generator and discriminator parts to generate more realistic images in two\ndistinct domains, which are the skull and face of an individual. This is the\nfirst time where 2D X-rays are being used as a representation of the skull by\ngenerative models for craniofacial reconstruction. We have evaluated the\nquality of generated faces using FID, IS, and SSIM scores. Finally, we have\nproposed a retrieval framework where the query is the generated face image and\nthe gallery is the database of real faces. By experimental results, we have\nfound that this can be an effective tool for forensic science.\n", "link": "http://arxiv.org/abs/2508.18031v1", "date": "2025-08-25", "relevancy": 2.1628, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5473}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5394}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCR%3A%20Investigating%20Generative%20AI%20models%20for%20Forensic%20Craniofacial%0A%20%20Reconstruction&body=Title%3A%20FCR%3A%20Investigating%20Generative%20AI%20models%20for%20Forensic%20Craniofacial%0A%20%20Reconstruction%0AAuthor%3A%20Ravi%20Shankar%20Prasad%20and%20Dinesh%20Singh%0AAbstract%3A%20%20%20Craniofacial%20reconstruction%20in%20forensics%20is%20one%20of%20the%20processes%20to%20identify%0Avictims%20of%20crime%20and%20natural%20disasters.%20Identifying%20an%20individual%20from%20their%0Aremains%20plays%20a%20crucial%20role%20when%20all%20other%20identification%20methods%20fail.%0ATraditional%20methods%20for%20this%20task%2C%20such%20as%20clay-based%20craniofacial%0Areconstruction%2C%20require%20expert%20domain%20knowledge%20and%20are%20a%20time-consuming%0Aprocess.%20At%20the%20same%20time%2C%20other%20probabilistic%20generative%20models%20like%20the%0Astatistical%20shape%20model%20or%20the%20Basel%20face%20model%20fail%20to%20capture%20the%20skull%20and%0Aface%20cross-domain%20attributes.%20Looking%20at%20these%20limitations%2C%20we%20propose%20a%0Ageneric%20framework%20for%20craniofacial%20reconstruction%20from%202D%20X-ray%20images.%20Here%2C%0Awe%20used%20various%20generative%20models%20%28i.e.%2C%20CycleGANs%2C%20cGANs%2C%20etc%29%20and%20fine-tune%0Athe%20generator%20and%20discriminator%20parts%20to%20generate%20more%20realistic%20images%20in%20two%0Adistinct%20domains%2C%20which%20are%20the%20skull%20and%20face%20of%20an%20individual.%20This%20is%20the%0Afirst%20time%20where%202D%20X-rays%20are%20being%20used%20as%20a%20representation%20of%20the%20skull%20by%0Agenerative%20models%20for%20craniofacial%20reconstruction.%20We%20have%20evaluated%20the%0Aquality%20of%20generated%20faces%20using%20FID%2C%20IS%2C%20and%20SSIM%20scores.%20Finally%2C%20we%20have%0Aproposed%20a%20retrieval%20framework%20where%20the%20query%20is%20the%20generated%20face%20image%20and%0Athe%20gallery%20is%20the%20database%20of%20real%20faces.%20By%20experimental%20results%2C%20we%20have%0Afound%20that%20this%20can%20be%20an%20effective%20tool%20for%20forensic%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCR%253A%2520Investigating%2520Generative%2520AI%2520models%2520for%2520Forensic%2520Craniofacial%250A%2520%2520Reconstruction%26entry.906535625%3DRavi%2520Shankar%2520Prasad%2520and%2520Dinesh%2520Singh%26entry.1292438233%3D%2520%2520Craniofacial%2520reconstruction%2520in%2520forensics%2520is%2520one%2520of%2520the%2520processes%2520to%2520identify%250Avictims%2520of%2520crime%2520and%2520natural%2520disasters.%2520Identifying%2520an%2520individual%2520from%2520their%250Aremains%2520plays%2520a%2520crucial%2520role%2520when%2520all%2520other%2520identification%2520methods%2520fail.%250ATraditional%2520methods%2520for%2520this%2520task%252C%2520such%2520as%2520clay-based%2520craniofacial%250Areconstruction%252C%2520require%2520expert%2520domain%2520knowledge%2520and%2520are%2520a%2520time-consuming%250Aprocess.%2520At%2520the%2520same%2520time%252C%2520other%2520probabilistic%2520generative%2520models%2520like%2520the%250Astatistical%2520shape%2520model%2520or%2520the%2520Basel%2520face%2520model%2520fail%2520to%2520capture%2520the%2520skull%2520and%250Aface%2520cross-domain%2520attributes.%2520Looking%2520at%2520these%2520limitations%252C%2520we%2520propose%2520a%250Ageneric%2520framework%2520for%2520craniofacial%2520reconstruction%2520from%25202D%2520X-ray%2520images.%2520Here%252C%250Awe%2520used%2520various%2520generative%2520models%2520%2528i.e.%252C%2520CycleGANs%252C%2520cGANs%252C%2520etc%2529%2520and%2520fine-tune%250Athe%2520generator%2520and%2520discriminator%2520parts%2520to%2520generate%2520more%2520realistic%2520images%2520in%2520two%250Adistinct%2520domains%252C%2520which%2520are%2520the%2520skull%2520and%2520face%2520of%2520an%2520individual.%2520This%2520is%2520the%250Afirst%2520time%2520where%25202D%2520X-rays%2520are%2520being%2520used%2520as%2520a%2520representation%2520of%2520the%2520skull%2520by%250Agenerative%2520models%2520for%2520craniofacial%2520reconstruction.%2520We%2520have%2520evaluated%2520the%250Aquality%2520of%2520generated%2520faces%2520using%2520FID%252C%2520IS%252C%2520and%2520SSIM%2520scores.%2520Finally%252C%2520we%2520have%250Aproposed%2520a%2520retrieval%2520framework%2520where%2520the%2520query%2520is%2520the%2520generated%2520face%2520image%2520and%250Athe%2520gallery%2520is%2520the%2520database%2520of%2520real%2520faces.%2520By%2520experimental%2520results%252C%2520we%2520have%250Afound%2520that%2520this%2520can%2520be%2520an%2520effective%2520tool%2520for%2520forensic%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCR%3A%20Investigating%20Generative%20AI%20models%20for%20Forensic%20Craniofacial%0A%20%20Reconstruction&entry.906535625=Ravi%20Shankar%20Prasad%20and%20Dinesh%20Singh&entry.1292438233=%20%20Craniofacial%20reconstruction%20in%20forensics%20is%20one%20of%20the%20processes%20to%20identify%0Avictims%20of%20crime%20and%20natural%20disasters.%20Identifying%20an%20individual%20from%20their%0Aremains%20plays%20a%20crucial%20role%20when%20all%20other%20identification%20methods%20fail.%0ATraditional%20methods%20for%20this%20task%2C%20such%20as%20clay-based%20craniofacial%0Areconstruction%2C%20require%20expert%20domain%20knowledge%20and%20are%20a%20time-consuming%0Aprocess.%20At%20the%20same%20time%2C%20other%20probabilistic%20generative%20models%20like%20the%0Astatistical%20shape%20model%20or%20the%20Basel%20face%20model%20fail%20to%20capture%20the%20skull%20and%0Aface%20cross-domain%20attributes.%20Looking%20at%20these%20limitations%2C%20we%20propose%20a%0Ageneric%20framework%20for%20craniofacial%20reconstruction%20from%202D%20X-ray%20images.%20Here%2C%0Awe%20used%20various%20generative%20models%20%28i.e.%2C%20CycleGANs%2C%20cGANs%2C%20etc%29%20and%20fine-tune%0Athe%20generator%20and%20discriminator%20parts%20to%20generate%20more%20realistic%20images%20in%20two%0Adistinct%20domains%2C%20which%20are%20the%20skull%20and%20face%20of%20an%20individual.%20This%20is%20the%0Afirst%20time%20where%202D%20X-rays%20are%20being%20used%20as%20a%20representation%20of%20the%20skull%20by%0Agenerative%20models%20for%20craniofacial%20reconstruction.%20We%20have%20evaluated%20the%0Aquality%20of%20generated%20faces%20using%20FID%2C%20IS%2C%20and%20SSIM%20scores.%20Finally%2C%20we%20have%0Aproposed%20a%20retrieval%20framework%20where%20the%20query%20is%20the%20generated%20face%20image%20and%0Athe%20gallery%20is%20the%20database%20of%20real%20faces.%20By%20experimental%20results%2C%20we%20have%0Afound%20that%20this%20can%20be%20an%20effective%20tool%20for%20forensic%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18031v1&entry.124074799=Read"},
{"title": "Explain and Monitor Deep Learning Models for Computer Vision using Obz\n  AI", "author": "Neo Christopher Chung and Jakub Binda", "abstract": "  Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems.\n", "link": "http://arxiv.org/abs/2508.18188v1", "date": "2025-08-25", "relevancy": 2.1572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explain%20and%20Monitor%20Deep%20Learning%20Models%20for%20Computer%20Vision%20using%20Obz%0A%20%20AI&body=Title%3A%20Explain%20and%20Monitor%20Deep%20Learning%20Models%20for%20Computer%20Vision%20using%20Obz%0A%20%20AI%0AAuthor%3A%20Neo%20Christopher%20Chung%20and%20Jakub%20Binda%0AAbstract%3A%20%20%20Deep%20learning%20has%20transformed%20computer%20vision%20%28CV%29%2C%20achieving%20outstanding%0Aperformance%20in%20classification%2C%20segmentation%2C%20and%20related%20tasks.%20Such%20AI-based%0ACV%20systems%20are%20becoming%20prevalent%2C%20with%20applications%20spanning%20from%20medical%0Aimaging%20to%20surveillance.%20State%20of%20the%20art%20models%20such%20as%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20vision%20transformers%20%28ViTs%29%20are%20often%20regarded%20as%20%60%60black%0Aboxes%2C%27%27%20offering%20limited%20transparency%20into%20their%20decision-making%20processes.%0ADespite%20a%20recent%20advancement%20in%20explainable%20AI%20%28XAI%29%2C%20explainability%20remains%0Aunderutilized%20in%20practical%20CV%20deployments.%20A%20primary%20obstacle%20is%20the%20absence%20of%0Aintegrated%20software%20solutions%20that%20connect%20XAI%20techniques%20with%20robust%20knowledge%0Amanagement%20and%20monitoring%20frameworks.%20To%20close%20this%20gap%2C%20we%20have%20developed%20Obz%0AAI%2C%20a%20comprehensive%20software%20ecosystem%20designed%20to%20facilitate%20state-of-the-art%0Aexplainability%20and%20observability%20for%20vision%20AI%20systems.%20Obz%20AI%20provides%20a%0Aseamless%20integration%20pipeline%2C%20from%20a%20Python%20client%20library%20to%20a%20full-stack%0Aanalytics%20dashboard.%20With%20Obz%20AI%2C%20a%20machine%20learning%20engineer%20can%20easily%0Aincorporate%20advanced%20XAI%20methodologies%2C%20extract%20and%20analyze%20features%20for%0Aoutlier%20detection%2C%20and%20continuously%20monitor%20AI%20models%20in%20real%20time.%20By%20making%0Athe%20decision-making%20mechanisms%20of%20deep%20models%20interpretable%2C%20Obz%20AI%20promotes%0Aobservability%20and%20responsible%20deployment%20of%20computer%20vision%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplain%2520and%2520Monitor%2520Deep%2520Learning%2520Models%2520for%2520Computer%2520Vision%2520using%2520Obz%250A%2520%2520AI%26entry.906535625%3DNeo%2520Christopher%2520Chung%2520and%2520Jakub%2520Binda%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520transformed%2520computer%2520vision%2520%2528CV%2529%252C%2520achieving%2520outstanding%250Aperformance%2520in%2520classification%252C%2520segmentation%252C%2520and%2520related%2520tasks.%2520Such%2520AI-based%250ACV%2520systems%2520are%2520becoming%2520prevalent%252C%2520with%2520applications%2520spanning%2520from%2520medical%250Aimaging%2520to%2520surveillance.%2520State%2520of%2520the%2520art%2520models%2520such%2520as%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520and%2520vision%2520transformers%2520%2528ViTs%2529%2520are%2520often%2520regarded%2520as%2520%2560%2560black%250Aboxes%252C%2527%2527%2520offering%2520limited%2520transparency%2520into%2520their%2520decision-making%2520processes.%250ADespite%2520a%2520recent%2520advancement%2520in%2520explainable%2520AI%2520%2528XAI%2529%252C%2520explainability%2520remains%250Aunderutilized%2520in%2520practical%2520CV%2520deployments.%2520A%2520primary%2520obstacle%2520is%2520the%2520absence%2520of%250Aintegrated%2520software%2520solutions%2520that%2520connect%2520XAI%2520techniques%2520with%2520robust%2520knowledge%250Amanagement%2520and%2520monitoring%2520frameworks.%2520To%2520close%2520this%2520gap%252C%2520we%2520have%2520developed%2520Obz%250AAI%252C%2520a%2520comprehensive%2520software%2520ecosystem%2520designed%2520to%2520facilitate%2520state-of-the-art%250Aexplainability%2520and%2520observability%2520for%2520vision%2520AI%2520systems.%2520Obz%2520AI%2520provides%2520a%250Aseamless%2520integration%2520pipeline%252C%2520from%2520a%2520Python%2520client%2520library%2520to%2520a%2520full-stack%250Aanalytics%2520dashboard.%2520With%2520Obz%2520AI%252C%2520a%2520machine%2520learning%2520engineer%2520can%2520easily%250Aincorporate%2520advanced%2520XAI%2520methodologies%252C%2520extract%2520and%2520analyze%2520features%2520for%250Aoutlier%2520detection%252C%2520and%2520continuously%2520monitor%2520AI%2520models%2520in%2520real%2520time.%2520By%2520making%250Athe%2520decision-making%2520mechanisms%2520of%2520deep%2520models%2520interpretable%252C%2520Obz%2520AI%2520promotes%250Aobservability%2520and%2520responsible%2520deployment%2520of%2520computer%2520vision%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explain%20and%20Monitor%20Deep%20Learning%20Models%20for%20Computer%20Vision%20using%20Obz%0A%20%20AI&entry.906535625=Neo%20Christopher%20Chung%20and%20Jakub%20Binda&entry.1292438233=%20%20Deep%20learning%20has%20transformed%20computer%20vision%20%28CV%29%2C%20achieving%20outstanding%0Aperformance%20in%20classification%2C%20segmentation%2C%20and%20related%20tasks.%20Such%20AI-based%0ACV%20systems%20are%20becoming%20prevalent%2C%20with%20applications%20spanning%20from%20medical%0Aimaging%20to%20surveillance.%20State%20of%20the%20art%20models%20such%20as%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20vision%20transformers%20%28ViTs%29%20are%20often%20regarded%20as%20%60%60black%0Aboxes%2C%27%27%20offering%20limited%20transparency%20into%20their%20decision-making%20processes.%0ADespite%20a%20recent%20advancement%20in%20explainable%20AI%20%28XAI%29%2C%20explainability%20remains%0Aunderutilized%20in%20practical%20CV%20deployments.%20A%20primary%20obstacle%20is%20the%20absence%20of%0Aintegrated%20software%20solutions%20that%20connect%20XAI%20techniques%20with%20robust%20knowledge%0Amanagement%20and%20monitoring%20frameworks.%20To%20close%20this%20gap%2C%20we%20have%20developed%20Obz%0AAI%2C%20a%20comprehensive%20software%20ecosystem%20designed%20to%20facilitate%20state-of-the-art%0Aexplainability%20and%20observability%20for%20vision%20AI%20systems.%20Obz%20AI%20provides%20a%0Aseamless%20integration%20pipeline%2C%20from%20a%20Python%20client%20library%20to%20a%20full-stack%0Aanalytics%20dashboard.%20With%20Obz%20AI%2C%20a%20machine%20learning%20engineer%20can%20easily%0Aincorporate%20advanced%20XAI%20methodologies%2C%20extract%20and%20analyze%20features%20for%0Aoutlier%20detection%2C%20and%20continuously%20monitor%20AI%20models%20in%20real%20time.%20By%20making%0Athe%20decision-making%20mechanisms%20of%20deep%20models%20interpretable%2C%20Obz%20AI%20promotes%0Aobservability%20and%20responsible%20deployment%20of%20computer%20vision%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18188v1&entry.124074799=Read"},
{"title": "V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D\n  Object Detection", "author": "Xun Huang and Jinlong Wang and Qiming Xia and Siheng Chen and Bisheng Yang and Xin Li and Cheng Wang and Chenglu Wen", "abstract": "  Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D\nobject detection using LiDAR and camera data. However, these methods suffer\nfrom performance degradation in adverse weather conditions. The weather-robust\n4D radar provides Doppler and additional geometric information, raising the\npossibility of addressing this challenge. To this end, we present V2X-R, the\nfirst simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R\ncontains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point\nclouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes.\nSubsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for\n3D object detection and implement it with various fusion strategies. To achieve\nweather-robust detection, we additionally propose a Multi-modal Denoising\nDiffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D\nradar feature as a condition to prompt the diffusion model to denoise noisy\nLiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline\ndemonstrates superior performance in the V2X-R dataset. Over and above this,\nour MDD module further improved the performance of basic fusion model by up to\n5.73%/6.70% in foggy/snowy conditions with barely disrupting normal\nperformance. The dataset and code will be publicly available at:\nhttps://github.com/ylwhxht/V2X-R.\n", "link": "http://arxiv.org/abs/2411.08402v5", "date": "2025-08-25", "relevancy": 2.1536, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5452}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20with%20Denoising%20Diffusion%20for%203D%0A%20%20Object%20Detection&body=Title%3A%20V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20with%20Denoising%20Diffusion%20for%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Xun%20Huang%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Siheng%20Chen%20and%20Bisheng%20Yang%20and%20Xin%20Li%20and%20Cheng%20Wang%20and%20Chenglu%20Wen%0AAbstract%3A%20%20%20Current%20Vehicle-to-Everything%20%28V2X%29%20systems%20have%20significantly%20enhanced%203D%0Aobject%20detection%20using%20LiDAR%20and%20camera%20data.%20However%2C%20these%20methods%20suffer%0Afrom%20performance%20degradation%20in%20adverse%20weather%20conditions.%20The%20weather-robust%0A4D%20radar%20provides%20Doppler%20and%20additional%20geometric%20information%2C%20raising%20the%0Apossibility%20of%20addressing%20this%20challenge.%20To%20this%20end%2C%20we%20present%20V2X-R%2C%20the%0Afirst%20simulated%20V2X%20dataset%20incorporating%20LiDAR%2C%20camera%2C%20and%204D%20radar.%20V2X-R%0Acontains%2012%2C079%20scenarios%20with%2037%2C727%20frames%20of%20LiDAR%20and%204D%20radar%20point%0Aclouds%2C%20150%2C908%20images%2C%20and%20170%2C859%20annotated%203D%20vehicle%20bounding%20boxes.%0ASubsequently%2C%20we%20propose%20a%20novel%20cooperative%20LiDAR-4D%20radar%20fusion%20pipeline%20for%0A3D%20object%20detection%20and%20implement%20it%20with%20various%20fusion%20strategies.%20To%20achieve%0Aweather-robust%20detection%2C%20we%20additionally%20propose%20a%20Multi-modal%20Denoising%0ADiffusion%20%28MDD%29%20module%20in%20our%20fusion%20pipeline.%20MDD%20utilizes%20weather-robust%204D%0Aradar%20feature%20as%20a%20condition%20to%20prompt%20the%20diffusion%20model%20to%20denoise%20noisy%0ALiDAR%20features.%20Experiments%20show%20that%20our%20LiDAR-4D%20radar%20fusion%20pipeline%0Ademonstrates%20superior%20performance%20in%20the%20V2X-R%20dataset.%20Over%20and%20above%20this%2C%0Aour%20MDD%20module%20further%20improved%20the%20performance%20of%20basic%20fusion%20model%20by%20up%20to%0A5.73%25/6.70%25%20in%20foggy/snowy%20conditions%20with%20barely%20disrupting%20normal%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ylwhxht/V2X-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08402v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2X-R%253A%2520Cooperative%2520LiDAR-4D%2520Radar%2520Fusion%2520with%2520Denoising%2520Diffusion%2520for%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DXun%2520Huang%2520and%2520Jinlong%2520Wang%2520and%2520Qiming%2520Xia%2520and%2520Siheng%2520Chen%2520and%2520Bisheng%2520Yang%2520and%2520Xin%2520Li%2520and%2520Cheng%2520Wang%2520and%2520Chenglu%2520Wen%26entry.1292438233%3D%2520%2520Current%2520Vehicle-to-Everything%2520%2528V2X%2529%2520systems%2520have%2520significantly%2520enhanced%25203D%250Aobject%2520detection%2520using%2520LiDAR%2520and%2520camera%2520data.%2520However%252C%2520these%2520methods%2520suffer%250Afrom%2520performance%2520degradation%2520in%2520adverse%2520weather%2520conditions.%2520The%2520weather-robust%250A4D%2520radar%2520provides%2520Doppler%2520and%2520additional%2520geometric%2520information%252C%2520raising%2520the%250Apossibility%2520of%2520addressing%2520this%2520challenge.%2520To%2520this%2520end%252C%2520we%2520present%2520V2X-R%252C%2520the%250Afirst%2520simulated%2520V2X%2520dataset%2520incorporating%2520LiDAR%252C%2520camera%252C%2520and%25204D%2520radar.%2520V2X-R%250Acontains%252012%252C079%2520scenarios%2520with%252037%252C727%2520frames%2520of%2520LiDAR%2520and%25204D%2520radar%2520point%250Aclouds%252C%2520150%252C908%2520images%252C%2520and%2520170%252C859%2520annotated%25203D%2520vehicle%2520bounding%2520boxes.%250ASubsequently%252C%2520we%2520propose%2520a%2520novel%2520cooperative%2520LiDAR-4D%2520radar%2520fusion%2520pipeline%2520for%250A3D%2520object%2520detection%2520and%2520implement%2520it%2520with%2520various%2520fusion%2520strategies.%2520To%2520achieve%250Aweather-robust%2520detection%252C%2520we%2520additionally%2520propose%2520a%2520Multi-modal%2520Denoising%250ADiffusion%2520%2528MDD%2529%2520module%2520in%2520our%2520fusion%2520pipeline.%2520MDD%2520utilizes%2520weather-robust%25204D%250Aradar%2520feature%2520as%2520a%2520condition%2520to%2520prompt%2520the%2520diffusion%2520model%2520to%2520denoise%2520noisy%250ALiDAR%2520features.%2520Experiments%2520show%2520that%2520our%2520LiDAR-4D%2520radar%2520fusion%2520pipeline%250Ademonstrates%2520superior%2520performance%2520in%2520the%2520V2X-R%2520dataset.%2520Over%2520and%2520above%2520this%252C%250Aour%2520MDD%2520module%2520further%2520improved%2520the%2520performance%2520of%2520basic%2520fusion%2520model%2520by%2520up%2520to%250A5.73%2525/6.70%2525%2520in%2520foggy/snowy%2520conditions%2520with%2520barely%2520disrupting%2520normal%250Aperformance.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/ylwhxht/V2X-R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08402v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20with%20Denoising%20Diffusion%20for%203D%0A%20%20Object%20Detection&entry.906535625=Xun%20Huang%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Siheng%20Chen%20and%20Bisheng%20Yang%20and%20Xin%20Li%20and%20Cheng%20Wang%20and%20Chenglu%20Wen&entry.1292438233=%20%20Current%20Vehicle-to-Everything%20%28V2X%29%20systems%20have%20significantly%20enhanced%203D%0Aobject%20detection%20using%20LiDAR%20and%20camera%20data.%20However%2C%20these%20methods%20suffer%0Afrom%20performance%20degradation%20in%20adverse%20weather%20conditions.%20The%20weather-robust%0A4D%20radar%20provides%20Doppler%20and%20additional%20geometric%20information%2C%20raising%20the%0Apossibility%20of%20addressing%20this%20challenge.%20To%20this%20end%2C%20we%20present%20V2X-R%2C%20the%0Afirst%20simulated%20V2X%20dataset%20incorporating%20LiDAR%2C%20camera%2C%20and%204D%20radar.%20V2X-R%0Acontains%2012%2C079%20scenarios%20with%2037%2C727%20frames%20of%20LiDAR%20and%204D%20radar%20point%0Aclouds%2C%20150%2C908%20images%2C%20and%20170%2C859%20annotated%203D%20vehicle%20bounding%20boxes.%0ASubsequently%2C%20we%20propose%20a%20novel%20cooperative%20LiDAR-4D%20radar%20fusion%20pipeline%20for%0A3D%20object%20detection%20and%20implement%20it%20with%20various%20fusion%20strategies.%20To%20achieve%0Aweather-robust%20detection%2C%20we%20additionally%20propose%20a%20Multi-modal%20Denoising%0ADiffusion%20%28MDD%29%20module%20in%20our%20fusion%20pipeline.%20MDD%20utilizes%20weather-robust%204D%0Aradar%20feature%20as%20a%20condition%20to%20prompt%20the%20diffusion%20model%20to%20denoise%20noisy%0ALiDAR%20features.%20Experiments%20show%20that%20our%20LiDAR-4D%20radar%20fusion%20pipeline%0Ademonstrates%20superior%20performance%20in%20the%20V2X-R%20dataset.%20Over%20and%20above%20this%2C%0Aour%20MDD%20module%20further%20improved%20the%20performance%20of%20basic%20fusion%20model%20by%20up%20to%0A5.73%25/6.70%25%20in%20foggy/snowy%20conditions%20with%20barely%20disrupting%20normal%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ylwhxht/V2X-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08402v5&entry.124074799=Read"},
{"title": "Weisfeiler-Lehman meets Events: An Expressivity Analysis for\n  Continuous-Time Dynamic Graph Neural Networks", "author": "Silvia Beddar-Wiesing and Alice Moallemy-Oureh", "abstract": "  Graph Neural Networks (GNNs) are known to match the distinguishing power of\nthe 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with\nthe unfolding tree equivalence classes of graphs. Preserving this equivalence,\nGNNs can universally approximate any target function on graphs in probability\nup to any precision. However, these results are limited to attributed\ndiscrete-dynamic graphs represented as sequences of connected graph snapshots.\nReal-world systems, such as communication networks, financial transaction\nnetworks, and molecular interactions, evolve asynchronously and may split into\ndisconnected components. In this paper, we extend the theory of attributed\ndiscrete-dynamic graphs to attributed continuous-time dynamic graphs with\narbitrary connectivity. To this end, we introduce a continuous-time dynamic\n1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,\nand identify a class of continuous-time dynamic GNNs (CGNNs) based on\ndiscrete-dynamic GNN architectures that retain both distinguishing power and\nuniversal approximation guarantees. Our constructive proofs further yield\npractical design guidelines, emphasizing a compact and expressive CGNN\narchitecture with piece-wise continuously differentiable temporal functions to\nprocess asynchronous, disconnected graphs.\n", "link": "http://arxiv.org/abs/2508.18052v1", "date": "2025-08-25", "relevancy": 2.1503, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5621}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5398}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weisfeiler-Lehman%20meets%20Events%3A%20An%20Expressivity%20Analysis%20for%0A%20%20Continuous-Time%20Dynamic%20Graph%20Neural%20Networks&body=Title%3A%20Weisfeiler-Lehman%20meets%20Events%3A%20An%20Expressivity%20Analysis%20for%0A%20%20Continuous-Time%20Dynamic%20Graph%20Neural%20Networks%0AAuthor%3A%20Silvia%20Beddar-Wiesing%20and%20Alice%20Moallemy-Oureh%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20known%20to%20match%20the%20distinguishing%20power%20of%0Athe%201-Weisfeiler-Lehman%20%281-WL%29%20test%2C%20and%20the%20resulting%20partitions%20coincide%20with%0Athe%20unfolding%20tree%20equivalence%20classes%20of%20graphs.%20Preserving%20this%20equivalence%2C%0AGNNs%20can%20universally%20approximate%20any%20target%20function%20on%20graphs%20in%20probability%0Aup%20to%20any%20precision.%20However%2C%20these%20results%20are%20limited%20to%20attributed%0Adiscrete-dynamic%20graphs%20represented%20as%20sequences%20of%20connected%20graph%20snapshots.%0AReal-world%20systems%2C%20such%20as%20communication%20networks%2C%20financial%20transaction%0Anetworks%2C%20and%20molecular%20interactions%2C%20evolve%20asynchronously%20and%20may%20split%20into%0Adisconnected%20components.%20In%20this%20paper%2C%20we%20extend%20the%20theory%20of%20attributed%0Adiscrete-dynamic%20graphs%20to%20attributed%20continuous-time%20dynamic%20graphs%20with%0Aarbitrary%20connectivity.%20To%20this%20end%2C%20we%20introduce%20a%20continuous-time%20dynamic%0A1-WL%20test%2C%20prove%20its%20equivalence%20to%20continuous-time%20dynamic%20unfolding%20trees%2C%0Aand%20identify%20a%20class%20of%20continuous-time%20dynamic%20GNNs%20%28CGNNs%29%20based%20on%0Adiscrete-dynamic%20GNN%20architectures%20that%20retain%20both%20distinguishing%20power%20and%0Auniversal%20approximation%20guarantees.%20Our%20constructive%20proofs%20further%20yield%0Apractical%20design%20guidelines%2C%20emphasizing%20a%20compact%20and%20expressive%20CGNN%0Aarchitecture%20with%20piece-wise%20continuously%20differentiable%20temporal%20functions%20to%0Aprocess%20asynchronous%2C%20disconnected%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeisfeiler-Lehman%2520meets%2520Events%253A%2520An%2520Expressivity%2520Analysis%2520for%250A%2520%2520Continuous-Time%2520Dynamic%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSilvia%2520Beddar-Wiesing%2520and%2520Alice%2520Moallemy-Oureh%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520known%2520to%2520match%2520the%2520distinguishing%2520power%2520of%250Athe%25201-Weisfeiler-Lehman%2520%25281-WL%2529%2520test%252C%2520and%2520the%2520resulting%2520partitions%2520coincide%2520with%250Athe%2520unfolding%2520tree%2520equivalence%2520classes%2520of%2520graphs.%2520Preserving%2520this%2520equivalence%252C%250AGNNs%2520can%2520universally%2520approximate%2520any%2520target%2520function%2520on%2520graphs%2520in%2520probability%250Aup%2520to%2520any%2520precision.%2520However%252C%2520these%2520results%2520are%2520limited%2520to%2520attributed%250Adiscrete-dynamic%2520graphs%2520represented%2520as%2520sequences%2520of%2520connected%2520graph%2520snapshots.%250AReal-world%2520systems%252C%2520such%2520as%2520communication%2520networks%252C%2520financial%2520transaction%250Anetworks%252C%2520and%2520molecular%2520interactions%252C%2520evolve%2520asynchronously%2520and%2520may%2520split%2520into%250Adisconnected%2520components.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520theory%2520of%2520attributed%250Adiscrete-dynamic%2520graphs%2520to%2520attributed%2520continuous-time%2520dynamic%2520graphs%2520with%250Aarbitrary%2520connectivity.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520continuous-time%2520dynamic%250A1-WL%2520test%252C%2520prove%2520its%2520equivalence%2520to%2520continuous-time%2520dynamic%2520unfolding%2520trees%252C%250Aand%2520identify%2520a%2520class%2520of%2520continuous-time%2520dynamic%2520GNNs%2520%2528CGNNs%2529%2520based%2520on%250Adiscrete-dynamic%2520GNN%2520architectures%2520that%2520retain%2520both%2520distinguishing%2520power%2520and%250Auniversal%2520approximation%2520guarantees.%2520Our%2520constructive%2520proofs%2520further%2520yield%250Apractical%2520design%2520guidelines%252C%2520emphasizing%2520a%2520compact%2520and%2520expressive%2520CGNN%250Aarchitecture%2520with%2520piece-wise%2520continuously%2520differentiable%2520temporal%2520functions%2520to%250Aprocess%2520asynchronous%252C%2520disconnected%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weisfeiler-Lehman%20meets%20Events%3A%20An%20Expressivity%20Analysis%20for%0A%20%20Continuous-Time%20Dynamic%20Graph%20Neural%20Networks&entry.906535625=Silvia%20Beddar-Wiesing%20and%20Alice%20Moallemy-Oureh&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20known%20to%20match%20the%20distinguishing%20power%20of%0Athe%201-Weisfeiler-Lehman%20%281-WL%29%20test%2C%20and%20the%20resulting%20partitions%20coincide%20with%0Athe%20unfolding%20tree%20equivalence%20classes%20of%20graphs.%20Preserving%20this%20equivalence%2C%0AGNNs%20can%20universally%20approximate%20any%20target%20function%20on%20graphs%20in%20probability%0Aup%20to%20any%20precision.%20However%2C%20these%20results%20are%20limited%20to%20attributed%0Adiscrete-dynamic%20graphs%20represented%20as%20sequences%20of%20connected%20graph%20snapshots.%0AReal-world%20systems%2C%20such%20as%20communication%20networks%2C%20financial%20transaction%0Anetworks%2C%20and%20molecular%20interactions%2C%20evolve%20asynchronously%20and%20may%20split%20into%0Adisconnected%20components.%20In%20this%20paper%2C%20we%20extend%20the%20theory%20of%20attributed%0Adiscrete-dynamic%20graphs%20to%20attributed%20continuous-time%20dynamic%20graphs%20with%0Aarbitrary%20connectivity.%20To%20this%20end%2C%20we%20introduce%20a%20continuous-time%20dynamic%0A1-WL%20test%2C%20prove%20its%20equivalence%20to%20continuous-time%20dynamic%20unfolding%20trees%2C%0Aand%20identify%20a%20class%20of%20continuous-time%20dynamic%20GNNs%20%28CGNNs%29%20based%20on%0Adiscrete-dynamic%20GNN%20architectures%20that%20retain%20both%20distinguishing%20power%20and%0Auniversal%20approximation%20guarantees.%20Our%20constructive%20proofs%20further%20yield%0Apractical%20design%20guidelines%2C%20emphasizing%20a%20compact%20and%20expressive%20CGNN%0Aarchitecture%20with%20piece-wise%20continuously%20differentiable%20temporal%20functions%20to%0Aprocess%20asynchronous%2C%20disconnected%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18052v1&entry.124074799=Read"},
{"title": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA", "author": "Karishma Thakrar and Shreyas Basavatia and Akshay Daftardar", "abstract": "  Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30\\% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70\\% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis.\n", "link": "http://arxiv.org/abs/2507.05520v2", "date": "2025-08-25", "relevancy": 2.1497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA&body=Title%3A%20Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA%0AAuthor%3A%20Karishma%20Thakrar%20and%20Shreyas%20Basavatia%20and%20Akshay%20Daftardar%0AAbstract%3A%20%20%20Dermatological%20care%20via%20telemedicine%20often%20lacks%20the%20rich%20context%20of%0Ain-person%20visits.%20Clinicians%20must%20make%20diagnoses%20based%20on%20a%20handful%20of%20images%0Aand%20brief%20descriptions%2C%20without%20the%20benefit%20of%20physical%20exams%2C%20second%20opinions%2C%0Aor%20reference%20materials.%20While%20many%20medical%20AI%20systems%20attempt%20to%20bridge%20these%0Agaps%20with%20domain-specific%20fine-tuning%2C%20this%20work%20hypothesized%20that%20mimicking%0Aclinical%20reasoning%20processes%20could%20offer%20a%20more%20effective%20path%20forward.%20This%0Astudy%20tested%20seven%20vision-language%20models%20on%20medical%20visual%20question%20answering%0Aacross%20six%20configurations%3A%20baseline%20models%2C%20fine-tuned%20variants%2C%20and%20both%0Aaugmented%20with%20either%20reasoning%20layers%20that%20combine%20multiple%20model%0Aperspectives%2C%20analogous%20to%20peer%20consultation%2C%20or%20retrieval-augmented%20generation%0Athat%20incorporates%20medical%20literature%20at%20inference%20time%2C%20serving%20a%20role%20similar%0Ato%20reference-checking.%20While%20fine-tuning%20degraded%20performance%20in%20four%20of%20seven%0Amodels%20with%20an%20average%2030%5C%25%20decrease%2C%20baseline%20models%20collapsed%20on%20test%20data.%0AClinical-inspired%20architectures%2C%20meanwhile%2C%20achieved%20up%20to%2070%5C%25%20accuracy%2C%0Amaintaining%20performance%20on%20unseen%20data%20while%20generating%20explainable%2C%0Aliterature-grounded%20outputs%20critical%20for%20clinical%20adoption.%20These%20findings%0Ademonstrate%20that%20medical%20AI%20succeeds%20by%20reconstructing%20the%20collaborative%20and%0Aevidence-based%20practices%20fundamental%20to%20clinical%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitecting%2520Clinical%2520Collaboration%253A%2520Multi-Agent%2520Reasoning%2520Systems%2520for%250A%2520%2520Multimodal%2520Medical%2520VQA%26entry.906535625%3DKarishma%2520Thakrar%2520and%2520Shreyas%2520Basavatia%2520and%2520Akshay%2520Daftardar%26entry.1292438233%3D%2520%2520Dermatological%2520care%2520via%2520telemedicine%2520often%2520lacks%2520the%2520rich%2520context%2520of%250Ain-person%2520visits.%2520Clinicians%2520must%2520make%2520diagnoses%2520based%2520on%2520a%2520handful%2520of%2520images%250Aand%2520brief%2520descriptions%252C%2520without%2520the%2520benefit%2520of%2520physical%2520exams%252C%2520second%2520opinions%252C%250Aor%2520reference%2520materials.%2520While%2520many%2520medical%2520AI%2520systems%2520attempt%2520to%2520bridge%2520these%250Agaps%2520with%2520domain-specific%2520fine-tuning%252C%2520this%2520work%2520hypothesized%2520that%2520mimicking%250Aclinical%2520reasoning%2520processes%2520could%2520offer%2520a%2520more%2520effective%2520path%2520forward.%2520This%250Astudy%2520tested%2520seven%2520vision-language%2520models%2520on%2520medical%2520visual%2520question%2520answering%250Aacross%2520six%2520configurations%253A%2520baseline%2520models%252C%2520fine-tuned%2520variants%252C%2520and%2520both%250Aaugmented%2520with%2520either%2520reasoning%2520layers%2520that%2520combine%2520multiple%2520model%250Aperspectives%252C%2520analogous%2520to%2520peer%2520consultation%252C%2520or%2520retrieval-augmented%2520generation%250Athat%2520incorporates%2520medical%2520literature%2520at%2520inference%2520time%252C%2520serving%2520a%2520role%2520similar%250Ato%2520reference-checking.%2520While%2520fine-tuning%2520degraded%2520performance%2520in%2520four%2520of%2520seven%250Amodels%2520with%2520an%2520average%252030%255C%2525%2520decrease%252C%2520baseline%2520models%2520collapsed%2520on%2520test%2520data.%250AClinical-inspired%2520architectures%252C%2520meanwhile%252C%2520achieved%2520up%2520to%252070%255C%2525%2520accuracy%252C%250Amaintaining%2520performance%2520on%2520unseen%2520data%2520while%2520generating%2520explainable%252C%250Aliterature-grounded%2520outputs%2520critical%2520for%2520clinical%2520adoption.%2520These%2520findings%250Ademonstrate%2520that%2520medical%2520AI%2520succeeds%2520by%2520reconstructing%2520the%2520collaborative%2520and%250Aevidence-based%2520practices%2520fundamental%2520to%2520clinical%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architecting%20Clinical%20Collaboration%3A%20Multi-Agent%20Reasoning%20Systems%20for%0A%20%20Multimodal%20Medical%20VQA&entry.906535625=Karishma%20Thakrar%20and%20Shreyas%20Basavatia%20and%20Akshay%20Daftardar&entry.1292438233=%20%20Dermatological%20care%20via%20telemedicine%20often%20lacks%20the%20rich%20context%20of%0Ain-person%20visits.%20Clinicians%20must%20make%20diagnoses%20based%20on%20a%20handful%20of%20images%0Aand%20brief%20descriptions%2C%20without%20the%20benefit%20of%20physical%20exams%2C%20second%20opinions%2C%0Aor%20reference%20materials.%20While%20many%20medical%20AI%20systems%20attempt%20to%20bridge%20these%0Agaps%20with%20domain-specific%20fine-tuning%2C%20this%20work%20hypothesized%20that%20mimicking%0Aclinical%20reasoning%20processes%20could%20offer%20a%20more%20effective%20path%20forward.%20This%0Astudy%20tested%20seven%20vision-language%20models%20on%20medical%20visual%20question%20answering%0Aacross%20six%20configurations%3A%20baseline%20models%2C%20fine-tuned%20variants%2C%20and%20both%0Aaugmented%20with%20either%20reasoning%20layers%20that%20combine%20multiple%20model%0Aperspectives%2C%20analogous%20to%20peer%20consultation%2C%20or%20retrieval-augmented%20generation%0Athat%20incorporates%20medical%20literature%20at%20inference%20time%2C%20serving%20a%20role%20similar%0Ato%20reference-checking.%20While%20fine-tuning%20degraded%20performance%20in%20four%20of%20seven%0Amodels%20with%20an%20average%2030%5C%25%20decrease%2C%20baseline%20models%20collapsed%20on%20test%20data.%0AClinical-inspired%20architectures%2C%20meanwhile%2C%20achieved%20up%20to%2070%5C%25%20accuracy%2C%0Amaintaining%20performance%20on%20unseen%20data%20while%20generating%20explainable%2C%0Aliterature-grounded%20outputs%20critical%20for%20clinical%20adoption.%20These%20findings%0Ademonstrate%20that%20medical%20AI%20succeeds%20by%20reconstructing%20the%20collaborative%20and%0Aevidence-based%20practices%20fundamental%20to%20clinical%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05520v2&entry.124074799=Read"},
{"title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", "author": "Huichi Zhou and Yihang Chen and Siyuan Guo and Xue Yan and Kin Hei Lee and Zihan Wang and Ka Yiu Lee and Guchun Zhang and Kun Shao and Linyi Yang and Jun Wang", "abstract": "  In this paper, we introduce a novel learning paradigm for Adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely \\emph{Memento}, which attains top-1\non GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It\nreaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset,\noutperforming the state-of-the-art training-based method, while case-based\nmemory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks.\nOur approach offers a scalable and efficient pathway for developing generalist\nLLM agents capable of continuous, real-time learning without gradient updates,\nadvancing machine learning towards open-ended skill acquisition and deep\nresearch scenarios. The code is available at\nhttps://github.com/Agent-on-the-Fly/Memento.\n", "link": "http://arxiv.org/abs/2508.16153v2", "date": "2025-08-25", "relevancy": 2.1412, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.54}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memento%3A%20Fine-tuning%20LLM%20Agents%20without%20Fine-tuning%20LLMs&body=Title%3A%20Memento%3A%20Fine-tuning%20LLM%20Agents%20without%20Fine-tuning%20LLMs%0AAuthor%3A%20Huichi%20Zhou%20and%20Yihang%20Chen%20and%20Siyuan%20Guo%20and%20Xue%20Yan%20and%20Kin%20Hei%20Lee%20and%20Zihan%20Wang%20and%20Ka%20Yiu%20Lee%20and%20Guchun%20Zhang%20and%20Kun%20Shao%20and%20Linyi%20Yang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%20paradigm%20for%20Adaptive%20Large%0ALanguage%20Model%20%28LLM%29%20agents%20that%20eliminates%20the%20need%20for%20fine-tuning%20the%0Aunderlying%20LLMs.%20Existing%20approaches%20are%20often%20either%20rigid%2C%20relying%20on%20static%2C%0Ahandcrafted%20reflection%20workflows%2C%20or%20computationally%20intensive%2C%20requiring%0Agradient%20updates%20of%20LLM%20model%20parameters.%20In%20contrast%2C%20our%20method%20enables%0Alow-cost%20continual%20adaptation%20via%20memory-based%20online%20reinforcement%20learning.%0AWe%20formalise%20this%20as%20a%20Memory-augmented%20Markov%20Decision%20Process%20%28M-MDP%29%2C%0Aequipped%20with%20a%20neural%20case-selection%20policy%20to%20guide%20action%20decisions.%20Past%0Aexperiences%20are%20stored%20in%20an%20episodic%20memory%2C%20either%20differentiable%20or%0Anon-parametric.%20The%20policy%20is%20continually%20updated%20based%20on%20environmental%0Afeedback%20through%20a%20memory%20rewriting%20mechanism%2C%20whereas%20policy%20improvement%20is%0Aachieved%20through%20efficient%20memory%20reading%20%28retrieval%29.%20We%20instantiate%20our%20agent%0Amodel%20in%20the%20deep%20research%20setting%2C%20namely%20%5Cemph%7BMemento%7D%2C%20which%20attains%20top-1%0Aon%20GAIA%20validation%20%28%2487.88%5C%25%24%20Pass%40%243%24%29%20and%20%2479.40%5C%25%24%20on%20the%20test%20set.%20It%0Areaches%20%2466.6%5C%25%24%20F1%20and%20%2480.4%5C%25%24%20PM%20on%20the%20DeepResearcher%20dataset%2C%0Aoutperforming%20the%20state-of-the-art%20training-based%20method%2C%20while%20case-based%0Amemory%20adds%20%244.7%5C%25%24%20to%20%249.6%5C%25%24%20absolute%20points%20on%20out-of-distribution%20tasks.%0AOur%20approach%20offers%20a%20scalable%20and%20efficient%20pathway%20for%20developing%20generalist%0ALLM%20agents%20capable%20of%20continuous%2C%20real-time%20learning%20without%20gradient%20updates%2C%0Aadvancing%20machine%20learning%20towards%20open-ended%20skill%20acquisition%20and%20deep%0Aresearch%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Agent-on-the-Fly/Memento.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemento%253A%2520Fine-tuning%2520LLM%2520Agents%2520without%2520Fine-tuning%2520LLMs%26entry.906535625%3DHuichi%2520Zhou%2520and%2520Yihang%2520Chen%2520and%2520Siyuan%2520Guo%2520and%2520Xue%2520Yan%2520and%2520Kin%2520Hei%2520Lee%2520and%2520Zihan%2520Wang%2520and%2520Ka%2520Yiu%2520Lee%2520and%2520Guchun%2520Zhang%2520and%2520Kun%2520Shao%2520and%2520Linyi%2520Yang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520learning%2520paradigm%2520for%2520Adaptive%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520agents%2520that%2520eliminates%2520the%2520need%2520for%2520fine-tuning%2520the%250Aunderlying%2520LLMs.%2520Existing%2520approaches%2520are%2520often%2520either%2520rigid%252C%2520relying%2520on%2520static%252C%250Ahandcrafted%2520reflection%2520workflows%252C%2520or%2520computationally%2520intensive%252C%2520requiring%250Agradient%2520updates%2520of%2520LLM%2520model%2520parameters.%2520In%2520contrast%252C%2520our%2520method%2520enables%250Alow-cost%2520continual%2520adaptation%2520via%2520memory-based%2520online%2520reinforcement%2520learning.%250AWe%2520formalise%2520this%2520as%2520a%2520Memory-augmented%2520Markov%2520Decision%2520Process%2520%2528M-MDP%2529%252C%250Aequipped%2520with%2520a%2520neural%2520case-selection%2520policy%2520to%2520guide%2520action%2520decisions.%2520Past%250Aexperiences%2520are%2520stored%2520in%2520an%2520episodic%2520memory%252C%2520either%2520differentiable%2520or%250Anon-parametric.%2520The%2520policy%2520is%2520continually%2520updated%2520based%2520on%2520environmental%250Afeedback%2520through%2520a%2520memory%2520rewriting%2520mechanism%252C%2520whereas%2520policy%2520improvement%2520is%250Aachieved%2520through%2520efficient%2520memory%2520reading%2520%2528retrieval%2529.%2520We%2520instantiate%2520our%2520agent%250Amodel%2520in%2520the%2520deep%2520research%2520setting%252C%2520namely%2520%255Cemph%257BMemento%257D%252C%2520which%2520attains%2520top-1%250Aon%2520GAIA%2520validation%2520%2528%252487.88%255C%2525%2524%2520Pass%2540%25243%2524%2529%2520and%2520%252479.40%255C%2525%2524%2520on%2520the%2520test%2520set.%2520It%250Areaches%2520%252466.6%255C%2525%2524%2520F1%2520and%2520%252480.4%255C%2525%2524%2520PM%2520on%2520the%2520DeepResearcher%2520dataset%252C%250Aoutperforming%2520the%2520state-of-the-art%2520training-based%2520method%252C%2520while%2520case-based%250Amemory%2520adds%2520%25244.7%255C%2525%2524%2520to%2520%25249.6%255C%2525%2524%2520absolute%2520points%2520on%2520out-of-distribution%2520tasks.%250AOur%2520approach%2520offers%2520a%2520scalable%2520and%2520efficient%2520pathway%2520for%2520developing%2520generalist%250ALLM%2520agents%2520capable%2520of%2520continuous%252C%2520real-time%2520learning%2520without%2520gradient%2520updates%252C%250Aadvancing%2520machine%2520learning%2520towards%2520open-ended%2520skill%2520acquisition%2520and%2520deep%250Aresearch%2520scenarios.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Agent-on-the-Fly/Memento.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memento%3A%20Fine-tuning%20LLM%20Agents%20without%20Fine-tuning%20LLMs&entry.906535625=Huichi%20Zhou%20and%20Yihang%20Chen%20and%20Siyuan%20Guo%20and%20Xue%20Yan%20and%20Kin%20Hei%20Lee%20and%20Zihan%20Wang%20and%20Ka%20Yiu%20Lee%20and%20Guchun%20Zhang%20and%20Kun%20Shao%20and%20Linyi%20Yang%20and%20Jun%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20learning%20paradigm%20for%20Adaptive%20Large%0ALanguage%20Model%20%28LLM%29%20agents%20that%20eliminates%20the%20need%20for%20fine-tuning%20the%0Aunderlying%20LLMs.%20Existing%20approaches%20are%20often%20either%20rigid%2C%20relying%20on%20static%2C%0Ahandcrafted%20reflection%20workflows%2C%20or%20computationally%20intensive%2C%20requiring%0Agradient%20updates%20of%20LLM%20model%20parameters.%20In%20contrast%2C%20our%20method%20enables%0Alow-cost%20continual%20adaptation%20via%20memory-based%20online%20reinforcement%20learning.%0AWe%20formalise%20this%20as%20a%20Memory-augmented%20Markov%20Decision%20Process%20%28M-MDP%29%2C%0Aequipped%20with%20a%20neural%20case-selection%20policy%20to%20guide%20action%20decisions.%20Past%0Aexperiences%20are%20stored%20in%20an%20episodic%20memory%2C%20either%20differentiable%20or%0Anon-parametric.%20The%20policy%20is%20continually%20updated%20based%20on%20environmental%0Afeedback%20through%20a%20memory%20rewriting%20mechanism%2C%20whereas%20policy%20improvement%20is%0Aachieved%20through%20efficient%20memory%20reading%20%28retrieval%29.%20We%20instantiate%20our%20agent%0Amodel%20in%20the%20deep%20research%20setting%2C%20namely%20%5Cemph%7BMemento%7D%2C%20which%20attains%20top-1%0Aon%20GAIA%20validation%20%28%2487.88%5C%25%24%20Pass%40%243%24%29%20and%20%2479.40%5C%25%24%20on%20the%20test%20set.%20It%0Areaches%20%2466.6%5C%25%24%20F1%20and%20%2480.4%5C%25%24%20PM%20on%20the%20DeepResearcher%20dataset%2C%0Aoutperforming%20the%20state-of-the-art%20training-based%20method%2C%20while%20case-based%0Amemory%20adds%20%244.7%5C%25%24%20to%20%249.6%5C%25%24%20absolute%20points%20on%20out-of-distribution%20tasks.%0AOur%20approach%20offers%20a%20scalable%20and%20efficient%20pathway%20for%20developing%20generalist%0ALLM%20agents%20capable%20of%20continuous%2C%20real-time%20learning%20without%20gradient%20updates%2C%0Aadvancing%20machine%20learning%20towards%20open-ended%20skill%20acquisition%20and%20deep%0Aresearch%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Agent-on-the-Fly/Memento.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16153v2&entry.124074799=Read"},
{"title": "Generative Feature Imputing - A Technique for Error-resilient Semantic\n  Communication", "author": "Jianhao Huang and Qunsong Zeng and Hongyang Du and Kaibin Huang", "abstract": "  Semantic communication (SemCom) has emerged as a promising paradigm for\nachieving unprecedented communication efficiency in sixth-generation (6G)\nnetworks by leveraging artificial intelligence (AI) to extract and transmit the\nunderlying meanings of source data. However, deploying SemCom over digital\nsystems presents new challenges, particularly in ensuring robustness against\ntransmission errors that may distort semantically critical content. To address\nthis issue, this paper proposes a novel framework, termed generative feature\nimputing, which comprises three key techniques. First, we introduce a spatial\nerror concentration packetization strategy that spatially concentrates feature\ndistortions by encoding feature elements based on their channel mappings, a\nproperty crucial for both the effectiveness and reduced complexity of the\nsubsequent techniques. Second, building on this strategy, we propose a\ngenerative feature imputing method that utilizes a diffusion model to\nefficiently reconstruct missing features caused by packet losses. Finally, we\ndevelop a semantic-aware power allocation scheme that enables unequal error\nprotection by allocating transmission power according to the semantic\nimportance of each packet. Experimental results demonstrate that the proposed\nframework outperforms conventional approaches, such as Deep Joint\nSource-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,\nachieving higher semantic accuracy and lower Learned Perceptual Image Patch\nSimilarity (LPIPS) scores.\n", "link": "http://arxiv.org/abs/2508.17957v1", "date": "2025-08-25", "relevancy": 2.1318, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.554}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5318}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Feature%20Imputing%20-%20A%20Technique%20for%20Error-resilient%20Semantic%0A%20%20Communication&body=Title%3A%20Generative%20Feature%20Imputing%20-%20A%20Technique%20for%20Error-resilient%20Semantic%0A%20%20Communication%0AAuthor%3A%20Jianhao%20Huang%20and%20Qunsong%20Zeng%20and%20Hongyang%20Du%20and%20Kaibin%20Huang%0AAbstract%3A%20%20%20Semantic%20communication%20%28SemCom%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Aachieving%20unprecedented%20communication%20efficiency%20in%20sixth-generation%20%286G%29%0Anetworks%20by%20leveraging%20artificial%20intelligence%20%28AI%29%20to%20extract%20and%20transmit%20the%0Aunderlying%20meanings%20of%20source%20data.%20However%2C%20deploying%20SemCom%20over%20digital%0Asystems%20presents%20new%20challenges%2C%20particularly%20in%20ensuring%20robustness%20against%0Atransmission%20errors%20that%20may%20distort%20semantically%20critical%20content.%20To%20address%0Athis%20issue%2C%20this%20paper%20proposes%20a%20novel%20framework%2C%20termed%20generative%20feature%0Aimputing%2C%20which%20comprises%20three%20key%20techniques.%20First%2C%20we%20introduce%20a%20spatial%0Aerror%20concentration%20packetization%20strategy%20that%20spatially%20concentrates%20feature%0Adistortions%20by%20encoding%20feature%20elements%20based%20on%20their%20channel%20mappings%2C%20a%0Aproperty%20crucial%20for%20both%20the%20effectiveness%20and%20reduced%20complexity%20of%20the%0Asubsequent%20techniques.%20Second%2C%20building%20on%20this%20strategy%2C%20we%20propose%20a%0Agenerative%20feature%20imputing%20method%20that%20utilizes%20a%20diffusion%20model%20to%0Aefficiently%20reconstruct%20missing%20features%20caused%20by%20packet%20losses.%20Finally%2C%20we%0Adevelop%20a%20semantic-aware%20power%20allocation%20scheme%20that%20enables%20unequal%20error%0Aprotection%20by%20allocating%20transmission%20power%20according%20to%20the%20semantic%0Aimportance%20of%20each%20packet.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0Aframework%20outperforms%20conventional%20approaches%2C%20such%20as%20Deep%20Joint%0ASource-Channel%20Coding%20%28DJSCC%29%20and%20JPEG2000%2C%20under%20block%20fading%20conditions%2C%0Aachieving%20higher%20semantic%20accuracy%20and%20lower%20Learned%20Perceptual%20Image%20Patch%0ASimilarity%20%28LPIPS%29%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Feature%2520Imputing%2520-%2520A%2520Technique%2520for%2520Error-resilient%2520Semantic%250A%2520%2520Communication%26entry.906535625%3DJianhao%2520Huang%2520and%2520Qunsong%2520Zeng%2520and%2520Hongyang%2520Du%2520and%2520Kaibin%2520Huang%26entry.1292438233%3D%2520%2520Semantic%2520communication%2520%2528SemCom%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Aachieving%2520unprecedented%2520communication%2520efficiency%2520in%2520sixth-generation%2520%25286G%2529%250Anetworks%2520by%2520leveraging%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520extract%2520and%2520transmit%2520the%250Aunderlying%2520meanings%2520of%2520source%2520data.%2520However%252C%2520deploying%2520SemCom%2520over%2520digital%250Asystems%2520presents%2520new%2520challenges%252C%2520particularly%2520in%2520ensuring%2520robustness%2520against%250Atransmission%2520errors%2520that%2520may%2520distort%2520semantically%2520critical%2520content.%2520To%2520address%250Athis%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%252C%2520termed%2520generative%2520feature%250Aimputing%252C%2520which%2520comprises%2520three%2520key%2520techniques.%2520First%252C%2520we%2520introduce%2520a%2520spatial%250Aerror%2520concentration%2520packetization%2520strategy%2520that%2520spatially%2520concentrates%2520feature%250Adistortions%2520by%2520encoding%2520feature%2520elements%2520based%2520on%2520their%2520channel%2520mappings%252C%2520a%250Aproperty%2520crucial%2520for%2520both%2520the%2520effectiveness%2520and%2520reduced%2520complexity%2520of%2520the%250Asubsequent%2520techniques.%2520Second%252C%2520building%2520on%2520this%2520strategy%252C%2520we%2520propose%2520a%250Agenerative%2520feature%2520imputing%2520method%2520that%2520utilizes%2520a%2520diffusion%2520model%2520to%250Aefficiently%2520reconstruct%2520missing%2520features%2520caused%2520by%2520packet%2520losses.%2520Finally%252C%2520we%250Adevelop%2520a%2520semantic-aware%2520power%2520allocation%2520scheme%2520that%2520enables%2520unequal%2520error%250Aprotection%2520by%2520allocating%2520transmission%2520power%2520according%2520to%2520the%2520semantic%250Aimportance%2520of%2520each%2520packet.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520outperforms%2520conventional%2520approaches%252C%2520such%2520as%2520Deep%2520Joint%250ASource-Channel%2520Coding%2520%2528DJSCC%2529%2520and%2520JPEG2000%252C%2520under%2520block%2520fading%2520conditions%252C%250Aachieving%2520higher%2520semantic%2520accuracy%2520and%2520lower%2520Learned%2520Perceptual%2520Image%2520Patch%250ASimilarity%2520%2528LPIPS%2529%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Feature%20Imputing%20-%20A%20Technique%20for%20Error-resilient%20Semantic%0A%20%20Communication&entry.906535625=Jianhao%20Huang%20and%20Qunsong%20Zeng%20and%20Hongyang%20Du%20and%20Kaibin%20Huang&entry.1292438233=%20%20Semantic%20communication%20%28SemCom%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Aachieving%20unprecedented%20communication%20efficiency%20in%20sixth-generation%20%286G%29%0Anetworks%20by%20leveraging%20artificial%20intelligence%20%28AI%29%20to%20extract%20and%20transmit%20the%0Aunderlying%20meanings%20of%20source%20data.%20However%2C%20deploying%20SemCom%20over%20digital%0Asystems%20presents%20new%20challenges%2C%20particularly%20in%20ensuring%20robustness%20against%0Atransmission%20errors%20that%20may%20distort%20semantically%20critical%20content.%20To%20address%0Athis%20issue%2C%20this%20paper%20proposes%20a%20novel%20framework%2C%20termed%20generative%20feature%0Aimputing%2C%20which%20comprises%20three%20key%20techniques.%20First%2C%20we%20introduce%20a%20spatial%0Aerror%20concentration%20packetization%20strategy%20that%20spatially%20concentrates%20feature%0Adistortions%20by%20encoding%20feature%20elements%20based%20on%20their%20channel%20mappings%2C%20a%0Aproperty%20crucial%20for%20both%20the%20effectiveness%20and%20reduced%20complexity%20of%20the%0Asubsequent%20techniques.%20Second%2C%20building%20on%20this%20strategy%2C%20we%20propose%20a%0Agenerative%20feature%20imputing%20method%20that%20utilizes%20a%20diffusion%20model%20to%0Aefficiently%20reconstruct%20missing%20features%20caused%20by%20packet%20losses.%20Finally%2C%20we%0Adevelop%20a%20semantic-aware%20power%20allocation%20scheme%20that%20enables%20unequal%20error%0Aprotection%20by%20allocating%20transmission%20power%20according%20to%20the%20semantic%0Aimportance%20of%20each%20packet.%20Experimental%20results%20demonstrate%20that%20the%20proposed%0Aframework%20outperforms%20conventional%20approaches%2C%20such%20as%20Deep%20Joint%0ASource-Channel%20Coding%20%28DJSCC%29%20and%20JPEG2000%2C%20under%20block%20fading%20conditions%2C%0Aachieving%20higher%20semantic%20accuracy%20and%20lower%20Learned%20Perceptual%20Image%20Patch%0ASimilarity%20%28LPIPS%29%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17957v1&entry.124074799=Read"},
{"title": "Learning to Detect Label Errors by Making Them: A Method for\n  Segmentation and Object Detection Datasets", "author": "Sarina Penquitt and Tobias Riedlinger and Timo Heller and Markus Reischl and Matthias Rottmann", "abstract": "  Recently, detection of label errors and improvement of label quality in\ndatasets for supervised learning tasks has become an increasingly important\ngoal in both research and industry. The consequences of incorrectly annotated\ndata include reduced model performance, biased benchmark results, and lower\noverall accuracy. Current state-of-the-art label error detection methods often\nfocus on a single computer vision task and, consequently, a specific type of\ndataset, containing, for example, either bounding boxes or pixel-wise\nannotations. Furthermore, previous methods are not learning-based. In this\nwork, we overcome this research gap. We present a unified method for detecting\nlabel errors in object detection, semantic segmentation, and instance\nsegmentation datasets. In a nutshell, our approach - learning to detect label\nerrors by making them - works as follows: we inject different kinds of label\nerrors into the ground truth. Then, the detection of label errors, across all\nmentioned primary tasks, is framed as an instance segmentation problem based on\na composite input. In our experiments, we compare the label error detection\nperformance of our method with various baselines and state-of-the-art\napproaches of each task's domain on simulated label errors across multiple\ntasks, datasets, and base models. This is complemented by a generalization\nstudy on real-world label errors. Additionally, we release 459 real label\nerrors identified in the Cityscapes dataset and provide a benchmark for real\nlabel error detection in Cityscapes.\n", "link": "http://arxiv.org/abs/2508.17930v1", "date": "2025-08-25", "relevancy": 2.1274, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5796}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.54}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20Label%20Errors%20by%20Making%20Them%3A%20A%20Method%20for%0A%20%20Segmentation%20and%20Object%20Detection%20Datasets&body=Title%3A%20Learning%20to%20Detect%20Label%20Errors%20by%20Making%20Them%3A%20A%20Method%20for%0A%20%20Segmentation%20and%20Object%20Detection%20Datasets%0AAuthor%3A%20Sarina%20Penquitt%20and%20Tobias%20Riedlinger%20and%20Timo%20Heller%20and%20Markus%20Reischl%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20Recently%2C%20detection%20of%20label%20errors%20and%20improvement%20of%20label%20quality%20in%0Adatasets%20for%20supervised%20learning%20tasks%20has%20become%20an%20increasingly%20important%0Agoal%20in%20both%20research%20and%20industry.%20The%20consequences%20of%20incorrectly%20annotated%0Adata%20include%20reduced%20model%20performance%2C%20biased%20benchmark%20results%2C%20and%20lower%0Aoverall%20accuracy.%20Current%20state-of-the-art%20label%20error%20detection%20methods%20often%0Afocus%20on%20a%20single%20computer%20vision%20task%20and%2C%20consequently%2C%20a%20specific%20type%20of%0Adataset%2C%20containing%2C%20for%20example%2C%20either%20bounding%20boxes%20or%20pixel-wise%0Aannotations.%20Furthermore%2C%20previous%20methods%20are%20not%20learning-based.%20In%20this%0Awork%2C%20we%20overcome%20this%20research%20gap.%20We%20present%20a%20unified%20method%20for%20detecting%0Alabel%20errors%20in%20object%20detection%2C%20semantic%20segmentation%2C%20and%20instance%0Asegmentation%20datasets.%20In%20a%20nutshell%2C%20our%20approach%20-%20learning%20to%20detect%20label%0Aerrors%20by%20making%20them%20-%20works%20as%20follows%3A%20we%20inject%20different%20kinds%20of%20label%0Aerrors%20into%20the%20ground%20truth.%20Then%2C%20the%20detection%20of%20label%20errors%2C%20across%20all%0Amentioned%20primary%20tasks%2C%20is%20framed%20as%20an%20instance%20segmentation%20problem%20based%20on%0Aa%20composite%20input.%20In%20our%20experiments%2C%20we%20compare%20the%20label%20error%20detection%0Aperformance%20of%20our%20method%20with%20various%20baselines%20and%20state-of-the-art%0Aapproaches%20of%20each%20task%27s%20domain%20on%20simulated%20label%20errors%20across%20multiple%0Atasks%2C%20datasets%2C%20and%20base%20models.%20This%20is%20complemented%20by%20a%20generalization%0Astudy%20on%20real-world%20label%20errors.%20Additionally%2C%20we%20release%20459%20real%20label%0Aerrors%20identified%20in%20the%20Cityscapes%20dataset%20and%20provide%20a%20benchmark%20for%20real%0Alabel%20error%20detection%20in%20Cityscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520Label%2520Errors%2520by%2520Making%2520Them%253A%2520A%2520Method%2520for%250A%2520%2520Segmentation%2520and%2520Object%2520Detection%2520Datasets%26entry.906535625%3DSarina%2520Penquitt%2520and%2520Tobias%2520Riedlinger%2520and%2520Timo%2520Heller%2520and%2520Markus%2520Reischl%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520Recently%252C%2520detection%2520of%2520label%2520errors%2520and%2520improvement%2520of%2520label%2520quality%2520in%250Adatasets%2520for%2520supervised%2520learning%2520tasks%2520has%2520become%2520an%2520increasingly%2520important%250Agoal%2520in%2520both%2520research%2520and%2520industry.%2520The%2520consequences%2520of%2520incorrectly%2520annotated%250Adata%2520include%2520reduced%2520model%2520performance%252C%2520biased%2520benchmark%2520results%252C%2520and%2520lower%250Aoverall%2520accuracy.%2520Current%2520state-of-the-art%2520label%2520error%2520detection%2520methods%2520often%250Afocus%2520on%2520a%2520single%2520computer%2520vision%2520task%2520and%252C%2520consequently%252C%2520a%2520specific%2520type%2520of%250Adataset%252C%2520containing%252C%2520for%2520example%252C%2520either%2520bounding%2520boxes%2520or%2520pixel-wise%250Aannotations.%2520Furthermore%252C%2520previous%2520methods%2520are%2520not%2520learning-based.%2520In%2520this%250Awork%252C%2520we%2520overcome%2520this%2520research%2520gap.%2520We%2520present%2520a%2520unified%2520method%2520for%2520detecting%250Alabel%2520errors%2520in%2520object%2520detection%252C%2520semantic%2520segmentation%252C%2520and%2520instance%250Asegmentation%2520datasets.%2520In%2520a%2520nutshell%252C%2520our%2520approach%2520-%2520learning%2520to%2520detect%2520label%250Aerrors%2520by%2520making%2520them%2520-%2520works%2520as%2520follows%253A%2520we%2520inject%2520different%2520kinds%2520of%2520label%250Aerrors%2520into%2520the%2520ground%2520truth.%2520Then%252C%2520the%2520detection%2520of%2520label%2520errors%252C%2520across%2520all%250Amentioned%2520primary%2520tasks%252C%2520is%2520framed%2520as%2520an%2520instance%2520segmentation%2520problem%2520based%2520on%250Aa%2520composite%2520input.%2520In%2520our%2520experiments%252C%2520we%2520compare%2520the%2520label%2520error%2520detection%250Aperformance%2520of%2520our%2520method%2520with%2520various%2520baselines%2520and%2520state-of-the-art%250Aapproaches%2520of%2520each%2520task%2527s%2520domain%2520on%2520simulated%2520label%2520errors%2520across%2520multiple%250Atasks%252C%2520datasets%252C%2520and%2520base%2520models.%2520This%2520is%2520complemented%2520by%2520a%2520generalization%250Astudy%2520on%2520real-world%2520label%2520errors.%2520Additionally%252C%2520we%2520release%2520459%2520real%2520label%250Aerrors%2520identified%2520in%2520the%2520Cityscapes%2520dataset%2520and%2520provide%2520a%2520benchmark%2520for%2520real%250Alabel%2520error%2520detection%2520in%2520Cityscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20Label%20Errors%20by%20Making%20Them%3A%20A%20Method%20for%0A%20%20Segmentation%20and%20Object%20Detection%20Datasets&entry.906535625=Sarina%20Penquitt%20and%20Tobias%20Riedlinger%20and%20Timo%20Heller%20and%20Markus%20Reischl%20and%20Matthias%20Rottmann&entry.1292438233=%20%20Recently%2C%20detection%20of%20label%20errors%20and%20improvement%20of%20label%20quality%20in%0Adatasets%20for%20supervised%20learning%20tasks%20has%20become%20an%20increasingly%20important%0Agoal%20in%20both%20research%20and%20industry.%20The%20consequences%20of%20incorrectly%20annotated%0Adata%20include%20reduced%20model%20performance%2C%20biased%20benchmark%20results%2C%20and%20lower%0Aoverall%20accuracy.%20Current%20state-of-the-art%20label%20error%20detection%20methods%20often%0Afocus%20on%20a%20single%20computer%20vision%20task%20and%2C%20consequently%2C%20a%20specific%20type%20of%0Adataset%2C%20containing%2C%20for%20example%2C%20either%20bounding%20boxes%20or%20pixel-wise%0Aannotations.%20Furthermore%2C%20previous%20methods%20are%20not%20learning-based.%20In%20this%0Awork%2C%20we%20overcome%20this%20research%20gap.%20We%20present%20a%20unified%20method%20for%20detecting%0Alabel%20errors%20in%20object%20detection%2C%20semantic%20segmentation%2C%20and%20instance%0Asegmentation%20datasets.%20In%20a%20nutshell%2C%20our%20approach%20-%20learning%20to%20detect%20label%0Aerrors%20by%20making%20them%20-%20works%20as%20follows%3A%20we%20inject%20different%20kinds%20of%20label%0Aerrors%20into%20the%20ground%20truth.%20Then%2C%20the%20detection%20of%20label%20errors%2C%20across%20all%0Amentioned%20primary%20tasks%2C%20is%20framed%20as%20an%20instance%20segmentation%20problem%20based%20on%0Aa%20composite%20input.%20In%20our%20experiments%2C%20we%20compare%20the%20label%20error%20detection%0Aperformance%20of%20our%20method%20with%20various%20baselines%20and%20state-of-the-art%0Aapproaches%20of%20each%20task%27s%20domain%20on%20simulated%20label%20errors%20across%20multiple%0Atasks%2C%20datasets%2C%20and%20base%20models.%20This%20is%20complemented%20by%20a%20generalization%0Astudy%20on%20real-world%20label%20errors.%20Additionally%2C%20we%20release%20459%20real%20label%0Aerrors%20identified%20in%20the%20Cityscapes%20dataset%20and%20provide%20a%20benchmark%20for%20real%0Alabel%20error%20detection%20in%20Cityscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17930v1&entry.124074799=Read"},
{"title": "Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in\n  Federated Learning via Re-calibration and Merit-discrimination", "author": "Ming Yang and Dongrun Li and Xin Wang and Xiaoyang Yu and Xiaoming Wu and Shibo He", "abstract": "  Cross-client data heterogeneity in federated learning induces biases that\nimpede unbiased consensus condensation and the complementary fusion of\ngeneralization- and personalization-oriented knowledge. While existing\napproaches mitigate heterogeneity through model decoupling and representation\ncenter loss, they often rely on static and restricted metrics to evaluate local\nknowledge and adopt global alignment too rigidly, leading to consensus\ndistortion and diminished model adaptability. To address these limitations, we\npropose FedMate, a method that implements bilateral optimization: On the server\nside, we construct a dynamic global prototype, with aggregation weights\ncalibrated by holistic integration of sample size, current parameters, and\nfuture prediction; a category-wise classifier is then fine-tuned using this\nprototype to preserve global consistency. On the client side, we introduce\ncomplementary classification fusion to enable merit-based discrimination\ntraining and incorporate cost-aware feature transmission to balance model\nperformance and communication efficiency. Experiments on five datasets of\nvarying complexity demonstrate that FedMate outperforms state-of-the-art\nmethods in harmonizing generalization and adaptation. Additionally, semantic\nsegmentation experiments on autonomous driving datasets validate the method's\nreal-world scalability.\n", "link": "http://arxiv.org/abs/2508.17954v1", "date": "2025-08-25", "relevancy": 2.1177, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5271}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Choice%20Outweighs%20Effort%3A%20Facilitating%20Complementary%20Knowledge%20Fusion%20in%0A%20%20Federated%20Learning%20via%20Re-calibration%20and%20Merit-discrimination&body=Title%3A%20Choice%20Outweighs%20Effort%3A%20Facilitating%20Complementary%20Knowledge%20Fusion%20in%0A%20%20Federated%20Learning%20via%20Re-calibration%20and%20Merit-discrimination%0AAuthor%3A%20Ming%20Yang%20and%20Dongrun%20Li%20and%20Xin%20Wang%20and%20Xiaoyang%20Yu%20and%20Xiaoming%20Wu%20and%20Shibo%20He%0AAbstract%3A%20%20%20Cross-client%20data%20heterogeneity%20in%20federated%20learning%20induces%20biases%20that%0Aimpede%20unbiased%20consensus%20condensation%20and%20the%20complementary%20fusion%20of%0Ageneralization-%20and%20personalization-oriented%20knowledge.%20While%20existing%0Aapproaches%20mitigate%20heterogeneity%20through%20model%20decoupling%20and%20representation%0Acenter%20loss%2C%20they%20often%20rely%20on%20static%20and%20restricted%20metrics%20to%20evaluate%20local%0Aknowledge%20and%20adopt%20global%20alignment%20too%20rigidly%2C%20leading%20to%20consensus%0Adistortion%20and%20diminished%20model%20adaptability.%20To%20address%20these%20limitations%2C%20we%0Apropose%20FedMate%2C%20a%20method%20that%20implements%20bilateral%20optimization%3A%20On%20the%20server%0Aside%2C%20we%20construct%20a%20dynamic%20global%20prototype%2C%20with%20aggregation%20weights%0Acalibrated%20by%20holistic%20integration%20of%20sample%20size%2C%20current%20parameters%2C%20and%0Afuture%20prediction%3B%20a%20category-wise%20classifier%20is%20then%20fine-tuned%20using%20this%0Aprototype%20to%20preserve%20global%20consistency.%20On%20the%20client%20side%2C%20we%20introduce%0Acomplementary%20classification%20fusion%20to%20enable%20merit-based%20discrimination%0Atraining%20and%20incorporate%20cost-aware%20feature%20transmission%20to%20balance%20model%0Aperformance%20and%20communication%20efficiency.%20Experiments%20on%20five%20datasets%20of%0Avarying%20complexity%20demonstrate%20that%20FedMate%20outperforms%20state-of-the-art%0Amethods%20in%20harmonizing%20generalization%20and%20adaptation.%20Additionally%2C%20semantic%0Asegmentation%20experiments%20on%20autonomous%20driving%20datasets%20validate%20the%20method%27s%0Areal-world%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChoice%2520Outweighs%2520Effort%253A%2520Facilitating%2520Complementary%2520Knowledge%2520Fusion%2520in%250A%2520%2520Federated%2520Learning%2520via%2520Re-calibration%2520and%2520Merit-discrimination%26entry.906535625%3DMing%2520Yang%2520and%2520Dongrun%2520Li%2520and%2520Xin%2520Wang%2520and%2520Xiaoyang%2520Yu%2520and%2520Xiaoming%2520Wu%2520and%2520Shibo%2520He%26entry.1292438233%3D%2520%2520Cross-client%2520data%2520heterogeneity%2520in%2520federated%2520learning%2520induces%2520biases%2520that%250Aimpede%2520unbiased%2520consensus%2520condensation%2520and%2520the%2520complementary%2520fusion%2520of%250Ageneralization-%2520and%2520personalization-oriented%2520knowledge.%2520While%2520existing%250Aapproaches%2520mitigate%2520heterogeneity%2520through%2520model%2520decoupling%2520and%2520representation%250Acenter%2520loss%252C%2520they%2520often%2520rely%2520on%2520static%2520and%2520restricted%2520metrics%2520to%2520evaluate%2520local%250Aknowledge%2520and%2520adopt%2520global%2520alignment%2520too%2520rigidly%252C%2520leading%2520to%2520consensus%250Adistortion%2520and%2520diminished%2520model%2520adaptability.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520FedMate%252C%2520a%2520method%2520that%2520implements%2520bilateral%2520optimization%253A%2520On%2520the%2520server%250Aside%252C%2520we%2520construct%2520a%2520dynamic%2520global%2520prototype%252C%2520with%2520aggregation%2520weights%250Acalibrated%2520by%2520holistic%2520integration%2520of%2520sample%2520size%252C%2520current%2520parameters%252C%2520and%250Afuture%2520prediction%253B%2520a%2520category-wise%2520classifier%2520is%2520then%2520fine-tuned%2520using%2520this%250Aprototype%2520to%2520preserve%2520global%2520consistency.%2520On%2520the%2520client%2520side%252C%2520we%2520introduce%250Acomplementary%2520classification%2520fusion%2520to%2520enable%2520merit-based%2520discrimination%250Atraining%2520and%2520incorporate%2520cost-aware%2520feature%2520transmission%2520to%2520balance%2520model%250Aperformance%2520and%2520communication%2520efficiency.%2520Experiments%2520on%2520five%2520datasets%2520of%250Avarying%2520complexity%2520demonstrate%2520that%2520FedMate%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520harmonizing%2520generalization%2520and%2520adaptation.%2520Additionally%252C%2520semantic%250Asegmentation%2520experiments%2520on%2520autonomous%2520driving%2520datasets%2520validate%2520the%2520method%2527s%250Areal-world%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Choice%20Outweighs%20Effort%3A%20Facilitating%20Complementary%20Knowledge%20Fusion%20in%0A%20%20Federated%20Learning%20via%20Re-calibration%20and%20Merit-discrimination&entry.906535625=Ming%20Yang%20and%20Dongrun%20Li%20and%20Xin%20Wang%20and%20Xiaoyang%20Yu%20and%20Xiaoming%20Wu%20and%20Shibo%20He&entry.1292438233=%20%20Cross-client%20data%20heterogeneity%20in%20federated%20learning%20induces%20biases%20that%0Aimpede%20unbiased%20consensus%20condensation%20and%20the%20complementary%20fusion%20of%0Ageneralization-%20and%20personalization-oriented%20knowledge.%20While%20existing%0Aapproaches%20mitigate%20heterogeneity%20through%20model%20decoupling%20and%20representation%0Acenter%20loss%2C%20they%20often%20rely%20on%20static%20and%20restricted%20metrics%20to%20evaluate%20local%0Aknowledge%20and%20adopt%20global%20alignment%20too%20rigidly%2C%20leading%20to%20consensus%0Adistortion%20and%20diminished%20model%20adaptability.%20To%20address%20these%20limitations%2C%20we%0Apropose%20FedMate%2C%20a%20method%20that%20implements%20bilateral%20optimization%3A%20On%20the%20server%0Aside%2C%20we%20construct%20a%20dynamic%20global%20prototype%2C%20with%20aggregation%20weights%0Acalibrated%20by%20holistic%20integration%20of%20sample%20size%2C%20current%20parameters%2C%20and%0Afuture%20prediction%3B%20a%20category-wise%20classifier%20is%20then%20fine-tuned%20using%20this%0Aprototype%20to%20preserve%20global%20consistency.%20On%20the%20client%20side%2C%20we%20introduce%0Acomplementary%20classification%20fusion%20to%20enable%20merit-based%20discrimination%0Atraining%20and%20incorporate%20cost-aware%20feature%20transmission%20to%20balance%20model%0Aperformance%20and%20communication%20efficiency.%20Experiments%20on%20five%20datasets%20of%0Avarying%20complexity%20demonstrate%20that%20FedMate%20outperforms%20state-of-the-art%0Amethods%20in%20harmonizing%20generalization%20and%20adaptation.%20Additionally%2C%20semantic%0Asegmentation%20experiments%20on%20autonomous%20driving%20datasets%20validate%20the%20method%27s%0Areal-world%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17954v1&entry.124074799=Read"},
{"title": "Physical Embodiment Enables Information Processing Beyond Explicit\n  Sensing in Active Matter", "author": "Diptabrata Paul and Nikola Milosevic and Nico Scherf and Frank Cichos", "abstract": "  Living microorganisms have evolved dedicated sensory machinery to detect\nenvironmental perturbations, processing these signals through biochemical\nnetworks to guide behavior. Replicating such capabilities in synthetic active\nmatter remains a fundamental challenge. Here, we demonstrate that synthetic\nactive particles can adapt to hidden hydrodynamic perturbations through\nphysical embodiment alone, without explicit sensing mechanisms. Using\nreinforcement learning to control self-thermophoretic particles, we show that\nthey learn navigation strategies to counteract unobserved flow fields by\nexploiting information encoded in their physical dynamics. Remarkably,\nparticles successfully navigate perturbations that are not included in their\nstate inputs, revealing that embodied dynamics can serve as an implicit sensing\nmechanism. This discovery establishes physical embodiment as a computational\nresource for information processing in active matter, with implications for\nautonomous microrobotic systems and bio-inspired computation.\n", "link": "http://arxiv.org/abs/2508.17921v1", "date": "2025-08-25", "relevancy": 2.1076, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5688}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5427}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physical%20Embodiment%20Enables%20Information%20Processing%20Beyond%20Explicit%0A%20%20Sensing%20in%20Active%20Matter&body=Title%3A%20Physical%20Embodiment%20Enables%20Information%20Processing%20Beyond%20Explicit%0A%20%20Sensing%20in%20Active%20Matter%0AAuthor%3A%20Diptabrata%20Paul%20and%20Nikola%20Milosevic%20and%20Nico%20Scherf%20and%20Frank%20Cichos%0AAbstract%3A%20%20%20Living%20microorganisms%20have%20evolved%20dedicated%20sensory%20machinery%20to%20detect%0Aenvironmental%20perturbations%2C%20processing%20these%20signals%20through%20biochemical%0Anetworks%20to%20guide%20behavior.%20Replicating%20such%20capabilities%20in%20synthetic%20active%0Amatter%20remains%20a%20fundamental%20challenge.%20Here%2C%20we%20demonstrate%20that%20synthetic%0Aactive%20particles%20can%20adapt%20to%20hidden%20hydrodynamic%20perturbations%20through%0Aphysical%20embodiment%20alone%2C%20without%20explicit%20sensing%20mechanisms.%20Using%0Areinforcement%20learning%20to%20control%20self-thermophoretic%20particles%2C%20we%20show%20that%0Athey%20learn%20navigation%20strategies%20to%20counteract%20unobserved%20flow%20fields%20by%0Aexploiting%20information%20encoded%20in%20their%20physical%20dynamics.%20Remarkably%2C%0Aparticles%20successfully%20navigate%20perturbations%20that%20are%20not%20included%20in%20their%0Astate%20inputs%2C%20revealing%20that%20embodied%20dynamics%20can%20serve%20as%20an%20implicit%20sensing%0Amechanism.%20This%20discovery%20establishes%20physical%20embodiment%20as%20a%20computational%0Aresource%20for%20information%20processing%20in%20active%20matter%2C%20with%20implications%20for%0Aautonomous%20microrobotic%20systems%20and%20bio-inspired%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysical%2520Embodiment%2520Enables%2520Information%2520Processing%2520Beyond%2520Explicit%250A%2520%2520Sensing%2520in%2520Active%2520Matter%26entry.906535625%3DDiptabrata%2520Paul%2520and%2520Nikola%2520Milosevic%2520and%2520Nico%2520Scherf%2520and%2520Frank%2520Cichos%26entry.1292438233%3D%2520%2520Living%2520microorganisms%2520have%2520evolved%2520dedicated%2520sensory%2520machinery%2520to%2520detect%250Aenvironmental%2520perturbations%252C%2520processing%2520these%2520signals%2520through%2520biochemical%250Anetworks%2520to%2520guide%2520behavior.%2520Replicating%2520such%2520capabilities%2520in%2520synthetic%2520active%250Amatter%2520remains%2520a%2520fundamental%2520challenge.%2520Here%252C%2520we%2520demonstrate%2520that%2520synthetic%250Aactive%2520particles%2520can%2520adapt%2520to%2520hidden%2520hydrodynamic%2520perturbations%2520through%250Aphysical%2520embodiment%2520alone%252C%2520without%2520explicit%2520sensing%2520mechanisms.%2520Using%250Areinforcement%2520learning%2520to%2520control%2520self-thermophoretic%2520particles%252C%2520we%2520show%2520that%250Athey%2520learn%2520navigation%2520strategies%2520to%2520counteract%2520unobserved%2520flow%2520fields%2520by%250Aexploiting%2520information%2520encoded%2520in%2520their%2520physical%2520dynamics.%2520Remarkably%252C%250Aparticles%2520successfully%2520navigate%2520perturbations%2520that%2520are%2520not%2520included%2520in%2520their%250Astate%2520inputs%252C%2520revealing%2520that%2520embodied%2520dynamics%2520can%2520serve%2520as%2520an%2520implicit%2520sensing%250Amechanism.%2520This%2520discovery%2520establishes%2520physical%2520embodiment%2520as%2520a%2520computational%250Aresource%2520for%2520information%2520processing%2520in%2520active%2520matter%252C%2520with%2520implications%2520for%250Aautonomous%2520microrobotic%2520systems%2520and%2520bio-inspired%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physical%20Embodiment%20Enables%20Information%20Processing%20Beyond%20Explicit%0A%20%20Sensing%20in%20Active%20Matter&entry.906535625=Diptabrata%20Paul%20and%20Nikola%20Milosevic%20and%20Nico%20Scherf%20and%20Frank%20Cichos&entry.1292438233=%20%20Living%20microorganisms%20have%20evolved%20dedicated%20sensory%20machinery%20to%20detect%0Aenvironmental%20perturbations%2C%20processing%20these%20signals%20through%20biochemical%0Anetworks%20to%20guide%20behavior.%20Replicating%20such%20capabilities%20in%20synthetic%20active%0Amatter%20remains%20a%20fundamental%20challenge.%20Here%2C%20we%20demonstrate%20that%20synthetic%0Aactive%20particles%20can%20adapt%20to%20hidden%20hydrodynamic%20perturbations%20through%0Aphysical%20embodiment%20alone%2C%20without%20explicit%20sensing%20mechanisms.%20Using%0Areinforcement%20learning%20to%20control%20self-thermophoretic%20particles%2C%20we%20show%20that%0Athey%20learn%20navigation%20strategies%20to%20counteract%20unobserved%20flow%20fields%20by%0Aexploiting%20information%20encoded%20in%20their%20physical%20dynamics.%20Remarkably%2C%0Aparticles%20successfully%20navigate%20perturbations%20that%20are%20not%20included%20in%20their%0Astate%20inputs%2C%20revealing%20that%20embodied%20dynamics%20can%20serve%20as%20an%20implicit%20sensing%0Amechanism.%20This%20discovery%20establishes%20physical%20embodiment%20as%20a%20computational%0Aresource%20for%20information%20processing%20in%20active%20matter%2C%20with%20implications%20for%0Aautonomous%20microrobotic%20systems%20and%20bio-inspired%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17921v1&entry.124074799=Read"},
{"title": "Neural Algorithmic Reasoners informed Large Language Model for\n  Multi-Agent Path Finding", "author": "Pu Feng and Size Wang and Yuhong Cao and Junkang Liang and Rongye Shi and Wenjun Wu", "abstract": "  The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems.\n", "link": "http://arxiv.org/abs/2508.17971v1", "date": "2025-08-25", "relevancy": 2.1059, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Algorithmic%20Reasoners%20informed%20Large%20Language%20Model%20for%0A%20%20Multi-Agent%20Path%20Finding&body=Title%3A%20Neural%20Algorithmic%20Reasoners%20informed%20Large%20Language%20Model%20for%0A%20%20Multi-Agent%20Path%20Finding%0AAuthor%3A%20Pu%20Feng%20and%20Size%20Wang%20and%20Yuhong%20Cao%20and%20Junkang%20Liang%20and%20Rongye%20Shi%20and%20Wenjun%20Wu%0AAbstract%3A%20%20%20The%20development%20and%20application%20of%20large%20language%20models%20%28LLM%29%20have%0Ademonstrated%20that%20foundational%20models%20can%20be%20utilized%20to%20solve%20a%20wide%20array%20of%0Atasks.%20However%2C%20their%20performance%20in%20multi-agent%20path%20finding%20%28MAPF%29%20tasks%20has%0Abeen%20less%20than%20satisfactory%2C%20with%20only%20a%20few%20studies%20exploring%20this%20area.%20MAPF%0Ais%20a%20complex%20problem%20requiring%20both%20planning%20and%20multi-agent%20coordination.%20To%0Aimprove%20the%20performance%20of%20LLM%20in%20MAPF%20tasks%2C%20we%20propose%20a%20novel%20framework%2C%0ALLM-NAR%2C%20which%20leverages%20neural%20algorithmic%20reasoners%20%28NAR%29%20to%20inform%20LLM%20for%0AMAPF.%20LLM-NAR%20consists%20of%20three%20key%20components%3A%20an%20LLM%20for%20MAPF%2C%20a%20pre-trained%0Agraph%20neural%20network-based%20NAR%2C%20and%20a%20cross-attention%20mechanism.%20This%20is%20the%0Afirst%20work%20to%20propose%20using%20a%20neural%20algorithmic%20reasoner%20to%20integrate%20GNNs%0Awith%20the%20map%20information%20for%20MAPF%2C%20thereby%20guiding%20LLM%20to%20achieve%20superior%0Aperformance.%20LLM-NAR%20can%20be%20easily%20adapted%20to%20various%20LLM%20models.%20Both%0Asimulation%20and%20real-world%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20LLM-based%20approaches%20in%20solving%20MAPF%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Algorithmic%2520Reasoners%2520informed%2520Large%2520Language%2520Model%2520for%250A%2520%2520Multi-Agent%2520Path%2520Finding%26entry.906535625%3DPu%2520Feng%2520and%2520Size%2520Wang%2520and%2520Yuhong%2520Cao%2520and%2520Junkang%2520Liang%2520and%2520Rongye%2520Shi%2520and%2520Wenjun%2520Wu%26entry.1292438233%3D%2520%2520The%2520development%2520and%2520application%2520of%2520large%2520language%2520models%2520%2528LLM%2529%2520have%250Ademonstrated%2520that%2520foundational%2520models%2520can%2520be%2520utilized%2520to%2520solve%2520a%2520wide%2520array%2520of%250Atasks.%2520However%252C%2520their%2520performance%2520in%2520multi-agent%2520path%2520finding%2520%2528MAPF%2529%2520tasks%2520has%250Abeen%2520less%2520than%2520satisfactory%252C%2520with%2520only%2520a%2520few%2520studies%2520exploring%2520this%2520area.%2520MAPF%250Ais%2520a%2520complex%2520problem%2520requiring%2520both%2520planning%2520and%2520multi-agent%2520coordination.%2520To%250Aimprove%2520the%2520performance%2520of%2520LLM%2520in%2520MAPF%2520tasks%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250ALLM-NAR%252C%2520which%2520leverages%2520neural%2520algorithmic%2520reasoners%2520%2528NAR%2529%2520to%2520inform%2520LLM%2520for%250AMAPF.%2520LLM-NAR%2520consists%2520of%2520three%2520key%2520components%253A%2520an%2520LLM%2520for%2520MAPF%252C%2520a%2520pre-trained%250Agraph%2520neural%2520network-based%2520NAR%252C%2520and%2520a%2520cross-attention%2520mechanism.%2520This%2520is%2520the%250Afirst%2520work%2520to%2520propose%2520using%2520a%2520neural%2520algorithmic%2520reasoner%2520to%2520integrate%2520GNNs%250Awith%2520the%2520map%2520information%2520for%2520MAPF%252C%2520thereby%2520guiding%2520LLM%2520to%2520achieve%2520superior%250Aperformance.%2520LLM-NAR%2520can%2520be%2520easily%2520adapted%2520to%2520various%2520LLM%2520models.%2520Both%250Asimulation%2520and%2520real-world%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520existing%2520LLM-based%2520approaches%2520in%2520solving%2520MAPF%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Algorithmic%20Reasoners%20informed%20Large%20Language%20Model%20for%0A%20%20Multi-Agent%20Path%20Finding&entry.906535625=Pu%20Feng%20and%20Size%20Wang%20and%20Yuhong%20Cao%20and%20Junkang%20Liang%20and%20Rongye%20Shi%20and%20Wenjun%20Wu&entry.1292438233=%20%20The%20development%20and%20application%20of%20large%20language%20models%20%28LLM%29%20have%0Ademonstrated%20that%20foundational%20models%20can%20be%20utilized%20to%20solve%20a%20wide%20array%20of%0Atasks.%20However%2C%20their%20performance%20in%20multi-agent%20path%20finding%20%28MAPF%29%20tasks%20has%0Abeen%20less%20than%20satisfactory%2C%20with%20only%20a%20few%20studies%20exploring%20this%20area.%20MAPF%0Ais%20a%20complex%20problem%20requiring%20both%20planning%20and%20multi-agent%20coordination.%20To%0Aimprove%20the%20performance%20of%20LLM%20in%20MAPF%20tasks%2C%20we%20propose%20a%20novel%20framework%2C%0ALLM-NAR%2C%20which%20leverages%20neural%20algorithmic%20reasoners%20%28NAR%29%20to%20inform%20LLM%20for%0AMAPF.%20LLM-NAR%20consists%20of%20three%20key%20components%3A%20an%20LLM%20for%20MAPF%2C%20a%20pre-trained%0Agraph%20neural%20network-based%20NAR%2C%20and%20a%20cross-attention%20mechanism.%20This%20is%20the%0Afirst%20work%20to%20propose%20using%20a%20neural%20algorithmic%20reasoner%20to%20integrate%20GNNs%0Awith%20the%20map%20information%20for%20MAPF%2C%20thereby%20guiding%20LLM%20to%20achieve%20superior%0Aperformance.%20LLM-NAR%20can%20be%20easily%20adapted%20to%20various%20LLM%20models.%20Both%0Asimulation%20and%20real-world%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20existing%20LLM-based%20approaches%20in%20solving%20MAPF%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17971v1&entry.124074799=Read"},
{"title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models", "author": "Manlai Liang and Mandi Liu and Jiangzhou Ji and Huaijun Li and Haobo Yang and Yaohan He and Jinlong Li", "abstract": "  Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.\n", "link": "http://arxiv.org/abs/2508.17892v1", "date": "2025-08-25", "relevancy": 2.0899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ILRe%3A%20Intermediate%20Layer%20Retrieval%20for%20Context%20Compression%20in%20Causal%0A%20%20Language%20Models&body=Title%3A%20ILRe%3A%20Intermediate%20Layer%20Retrieval%20for%20Context%20Compression%20in%20Causal%0A%20%20Language%20Models%0AAuthor%3A%20Manlai%20Liang%20and%20Mandi%20Liu%20and%20Jiangzhou%20Ji%20and%20Huaijun%20Li%20and%20Haobo%20Yang%20and%20Yaohan%20He%20and%20Jinlong%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20success%20across%20many%0Abenchmarks.%20However%2C%20they%20still%20exhibit%20limitations%20in%20long-context%20scenarios%2C%0Aprimarily%20due%20to%20their%20short%20effective%20context%20length%2C%20quadratic%20computational%0Acomplexity%2C%20and%20high%20memory%20overhead%20when%20processing%20lengthy%20inputs.%20To%0Amitigate%20these%20issues%2C%20we%20introduce%20a%20novel%20context%20compression%20pipeline%2C%0Acalled%20Intermediate%20Layer%20Retrieval%20%28ILRe%29%2C%20which%20determines%20one%20intermediate%0Adecoder%20layer%20offline%2C%20encodes%20context%20by%20streaming%20chunked%20prefill%20only%20up%20to%0Athat%20layer%2C%20and%20recalls%20tokens%20by%20the%20attention%20scores%20between%20the%20input%20query%0Aand%20full%20key%20cache%20in%20that%20specified%20layer.%20In%20particular%2C%20we%20propose%20a%0Amulti-pooling%20kernels%20allocating%20strategy%20in%20the%20token%20recalling%20process%20to%0Amaintain%20the%20completeness%20of%20semantics.%20Our%20approach%20not%20only%20reduces%20the%0Aprefilling%20complexity%20from%20%24O%28L%5E2%29%24%20to%20%24O%28L%29%24%2C%20but%20also%20achieves%20performance%0Acomparable%20to%20or%20better%20than%20the%20full%20context%20in%20the%20long%20context%20scenarios.%0AWithout%20additional%20post%20training%20or%20operator%20development%2C%20ILRe%20can%20process%20a%0Asingle%20%241M%24%20tokens%20request%20in%20less%20than%20half%20a%20minute%20%28speedup%20%24%5Capprox%0A180%5Ctimes%24%29%20and%20scores%20RULER-%241M%24%20benchmark%20of%20%24%5Capprox%2079.8%24%20with%20model%0ALlama-3.1-UltraLong-8B-1M-Instruct%20on%20a%20Huawei%20Ascend%20910B%20NPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DILRe%253A%2520Intermediate%2520Layer%2520Retrieval%2520for%2520Context%2520Compression%2520in%2520Causal%250A%2520%2520Language%2520Models%26entry.906535625%3DManlai%2520Liang%2520and%2520Mandi%2520Liu%2520and%2520Jiangzhou%2520Ji%2520and%2520Huaijun%2520Li%2520and%2520Haobo%2520Yang%2520and%2520Yaohan%2520He%2520and%2520Jinlong%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520success%2520across%2520many%250Abenchmarks.%2520However%252C%2520they%2520still%2520exhibit%2520limitations%2520in%2520long-context%2520scenarios%252C%250Aprimarily%2520due%2520to%2520their%2520short%2520effective%2520context%2520length%252C%2520quadratic%2520computational%250Acomplexity%252C%2520and%2520high%2520memory%2520overhead%2520when%2520processing%2520lengthy%2520inputs.%2520To%250Amitigate%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520context%2520compression%2520pipeline%252C%250Acalled%2520Intermediate%2520Layer%2520Retrieval%2520%2528ILRe%2529%252C%2520which%2520determines%2520one%2520intermediate%250Adecoder%2520layer%2520offline%252C%2520encodes%2520context%2520by%2520streaming%2520chunked%2520prefill%2520only%2520up%2520to%250Athat%2520layer%252C%2520and%2520recalls%2520tokens%2520by%2520the%2520attention%2520scores%2520between%2520the%2520input%2520query%250Aand%2520full%2520key%2520cache%2520in%2520that%2520specified%2520layer.%2520In%2520particular%252C%2520we%2520propose%2520a%250Amulti-pooling%2520kernels%2520allocating%2520strategy%2520in%2520the%2520token%2520recalling%2520process%2520to%250Amaintain%2520the%2520completeness%2520of%2520semantics.%2520Our%2520approach%2520not%2520only%2520reduces%2520the%250Aprefilling%2520complexity%2520from%2520%2524O%2528L%255E2%2529%2524%2520to%2520%2524O%2528L%2529%2524%252C%2520but%2520also%2520achieves%2520performance%250Acomparable%2520to%2520or%2520better%2520than%2520the%2520full%2520context%2520in%2520the%2520long%2520context%2520scenarios.%250AWithout%2520additional%2520post%2520training%2520or%2520operator%2520development%252C%2520ILRe%2520can%2520process%2520a%250Asingle%2520%25241M%2524%2520tokens%2520request%2520in%2520less%2520than%2520half%2520a%2520minute%2520%2528speedup%2520%2524%255Capprox%250A180%255Ctimes%2524%2529%2520and%2520scores%2520RULER-%25241M%2524%2520benchmark%2520of%2520%2524%255Capprox%252079.8%2524%2520with%2520model%250ALlama-3.1-UltraLong-8B-1M-Instruct%2520on%2520a%2520Huawei%2520Ascend%2520910B%2520NPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILRe%3A%20Intermediate%20Layer%20Retrieval%20for%20Context%20Compression%20in%20Causal%0A%20%20Language%20Models&entry.906535625=Manlai%20Liang%20and%20Mandi%20Liu%20and%20Jiangzhou%20Ji%20and%20Huaijun%20Li%20and%20Haobo%20Yang%20and%20Yaohan%20He%20and%20Jinlong%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20success%20across%20many%0Abenchmarks.%20However%2C%20they%20still%20exhibit%20limitations%20in%20long-context%20scenarios%2C%0Aprimarily%20due%20to%20their%20short%20effective%20context%20length%2C%20quadratic%20computational%0Acomplexity%2C%20and%20high%20memory%20overhead%20when%20processing%20lengthy%20inputs.%20To%0Amitigate%20these%20issues%2C%20we%20introduce%20a%20novel%20context%20compression%20pipeline%2C%0Acalled%20Intermediate%20Layer%20Retrieval%20%28ILRe%29%2C%20which%20determines%20one%20intermediate%0Adecoder%20layer%20offline%2C%20encodes%20context%20by%20streaming%20chunked%20prefill%20only%20up%20to%0Athat%20layer%2C%20and%20recalls%20tokens%20by%20the%20attention%20scores%20between%20the%20input%20query%0Aand%20full%20key%20cache%20in%20that%20specified%20layer.%20In%20particular%2C%20we%20propose%20a%0Amulti-pooling%20kernels%20allocating%20strategy%20in%20the%20token%20recalling%20process%20to%0Amaintain%20the%20completeness%20of%20semantics.%20Our%20approach%20not%20only%20reduces%20the%0Aprefilling%20complexity%20from%20%24O%28L%5E2%29%24%20to%20%24O%28L%29%24%2C%20but%20also%20achieves%20performance%0Acomparable%20to%20or%20better%20than%20the%20full%20context%20in%20the%20long%20context%20scenarios.%0AWithout%20additional%20post%20training%20or%20operator%20development%2C%20ILRe%20can%20process%20a%0Asingle%20%241M%24%20tokens%20request%20in%20less%20than%20half%20a%20minute%20%28speedup%20%24%5Capprox%0A180%5Ctimes%24%29%20and%20scores%20RULER-%241M%24%20benchmark%20of%20%24%5Capprox%2079.8%24%20with%20model%0ALlama-3.1-UltraLong-8B-1M-Instruct%20on%20a%20Huawei%20Ascend%20910B%20NPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17892v1&entry.124074799=Read"},
{"title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval", "author": "Meixiu Long and Duolin Sun and Dan Yang and Junjie Wang and Yue Shen and Jian Wang and Peng Wei and Jinjie Gu and Jiahai Wang", "abstract": "  Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent DIVER, a retrieval pipeline designed for reasoning-intensive\ninformation retrieval. It consists of four components. The document\npreprocessing stage enhances readability and preserves content by cleaning\nnoisy texts and segmenting long documents. The query expansion stage leverages\nlarge language models to iteratively refine user queries with explicit\nreasoning and evidence from retrieved documents. The retrieval stage employs a\nmodel fine-tuned on synthetic data spanning medical and mathematical domains,\nalong with hard negatives, enabling effective handling of reasoning-intensive\nqueries. Finally, the reranking stage combines pointwise and listwise\nstrategies to produce both fine-grained and globally consistent rankings. On\nthe BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 45.8\noverall and 28.9 on original queries, consistently outperforming competitive\nreasoning-aware models. These results demonstrate the effectiveness of\nreasoning-aware retrieval strategies in complex real-world tasks.\n", "link": "http://arxiv.org/abs/2508.07995v3", "date": "2025-08-25", "relevancy": 2.0872, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5292}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval&body=Title%3A%20DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval%0AAuthor%3A%20Meixiu%20Long%20and%20Duolin%20Sun%20and%20Dan%20Yang%20and%20Junjie%20Wang%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Jinjie%20Gu%20and%20Jiahai%20Wang%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20has%20achieved%20strong%20performance%20on%0Aknowledge-intensive%20tasks%20where%20query-document%20relevance%20can%20be%20identified%0Athrough%20direct%20lexical%20or%20semantic%20matches.%20However%2C%20many%20real-world%20queries%0Ainvolve%20abstract%20reasoning%2C%20analogical%20thinking%2C%20or%20multi-step%20inference%2C%20which%0Aexisting%20retrievers%20often%20struggle%20to%20capture.%20To%20address%20this%20challenge%2C%20we%0Apresent%20DIVER%2C%20a%20retrieval%20pipeline%20designed%20for%20reasoning-intensive%0Ainformation%20retrieval.%20It%20consists%20of%20four%20components.%20The%20document%0Apreprocessing%20stage%20enhances%20readability%20and%20preserves%20content%20by%20cleaning%0Anoisy%20texts%20and%20segmenting%20long%20documents.%20The%20query%20expansion%20stage%20leverages%0Alarge%20language%20models%20to%20iteratively%20refine%20user%20queries%20with%20explicit%0Areasoning%20and%20evidence%20from%20retrieved%20documents.%20The%20retrieval%20stage%20employs%20a%0Amodel%20fine-tuned%20on%20synthetic%20data%20spanning%20medical%20and%20mathematical%20domains%2C%0Aalong%20with%20hard%20negatives%2C%20enabling%20effective%20handling%20of%20reasoning-intensive%0Aqueries.%20Finally%2C%20the%20reranking%20stage%20combines%20pointwise%20and%20listwise%0Astrategies%20to%20produce%20both%20fine-grained%20and%20globally%20consistent%20rankings.%20On%0Athe%20BRIGHT%20benchmark%2C%20DIVER%20achieves%20state-of-the-art%20nDCG%4010%20scores%20of%2045.8%0Aoverall%20and%2028.9%20on%20original%20queries%2C%20consistently%20outperforming%20competitive%0Areasoning-aware%20models.%20These%20results%20demonstrate%20the%20effectiveness%20of%0Areasoning-aware%20retrieval%20strategies%20in%20complex%20real-world%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07995v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVER%253A%2520A%2520Multi-Stage%2520Approach%2520for%2520Reasoning-intensive%2520Information%250A%2520%2520Retrieval%26entry.906535625%3DMeixiu%2520Long%2520and%2520Duolin%2520Sun%2520and%2520Dan%2520Yang%2520and%2520Junjie%2520Wang%2520and%2520Yue%2520Shen%2520and%2520Jian%2520Wang%2520and%2520Peng%2520Wei%2520and%2520Jinjie%2520Gu%2520and%2520Jiahai%2520Wang%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520has%2520achieved%2520strong%2520performance%2520on%250Aknowledge-intensive%2520tasks%2520where%2520query-document%2520relevance%2520can%2520be%2520identified%250Athrough%2520direct%2520lexical%2520or%2520semantic%2520matches.%2520However%252C%2520many%2520real-world%2520queries%250Ainvolve%2520abstract%2520reasoning%252C%2520analogical%2520thinking%252C%2520or%2520multi-step%2520inference%252C%2520which%250Aexisting%2520retrievers%2520often%2520struggle%2520to%2520capture.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520DIVER%252C%2520a%2520retrieval%2520pipeline%2520designed%2520for%2520reasoning-intensive%250Ainformation%2520retrieval.%2520It%2520consists%2520of%2520four%2520components.%2520The%2520document%250Apreprocessing%2520stage%2520enhances%2520readability%2520and%2520preserves%2520content%2520by%2520cleaning%250Anoisy%2520texts%2520and%2520segmenting%2520long%2520documents.%2520The%2520query%2520expansion%2520stage%2520leverages%250Alarge%2520language%2520models%2520to%2520iteratively%2520refine%2520user%2520queries%2520with%2520explicit%250Areasoning%2520and%2520evidence%2520from%2520retrieved%2520documents.%2520The%2520retrieval%2520stage%2520employs%2520a%250Amodel%2520fine-tuned%2520on%2520synthetic%2520data%2520spanning%2520medical%2520and%2520mathematical%2520domains%252C%250Aalong%2520with%2520hard%2520negatives%252C%2520enabling%2520effective%2520handling%2520of%2520reasoning-intensive%250Aqueries.%2520Finally%252C%2520the%2520reranking%2520stage%2520combines%2520pointwise%2520and%2520listwise%250Astrategies%2520to%2520produce%2520both%2520fine-grained%2520and%2520globally%2520consistent%2520rankings.%2520On%250Athe%2520BRIGHT%2520benchmark%252C%2520DIVER%2520achieves%2520state-of-the-art%2520nDCG%254010%2520scores%2520of%252045.8%250Aoverall%2520and%252028.9%2520on%2520original%2520queries%252C%2520consistently%2520outperforming%2520competitive%250Areasoning-aware%2520models.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Areasoning-aware%2520retrieval%2520strategies%2520in%2520complex%2520real-world%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07995v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval&entry.906535625=Meixiu%20Long%20and%20Duolin%20Sun%20and%20Dan%20Yang%20and%20Junjie%20Wang%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Jinjie%20Gu%20and%20Jiahai%20Wang&entry.1292438233=%20%20Retrieval-augmented%20generation%20has%20achieved%20strong%20performance%20on%0Aknowledge-intensive%20tasks%20where%20query-document%20relevance%20can%20be%20identified%0Athrough%20direct%20lexical%20or%20semantic%20matches.%20However%2C%20many%20real-world%20queries%0Ainvolve%20abstract%20reasoning%2C%20analogical%20thinking%2C%20or%20multi-step%20inference%2C%20which%0Aexisting%20retrievers%20often%20struggle%20to%20capture.%20To%20address%20this%20challenge%2C%20we%0Apresent%20DIVER%2C%20a%20retrieval%20pipeline%20designed%20for%20reasoning-intensive%0Ainformation%20retrieval.%20It%20consists%20of%20four%20components.%20The%20document%0Apreprocessing%20stage%20enhances%20readability%20and%20preserves%20content%20by%20cleaning%0Anoisy%20texts%20and%20segmenting%20long%20documents.%20The%20query%20expansion%20stage%20leverages%0Alarge%20language%20models%20to%20iteratively%20refine%20user%20queries%20with%20explicit%0Areasoning%20and%20evidence%20from%20retrieved%20documents.%20The%20retrieval%20stage%20employs%20a%0Amodel%20fine-tuned%20on%20synthetic%20data%20spanning%20medical%20and%20mathematical%20domains%2C%0Aalong%20with%20hard%20negatives%2C%20enabling%20effective%20handling%20of%20reasoning-intensive%0Aqueries.%20Finally%2C%20the%20reranking%20stage%20combines%20pointwise%20and%20listwise%0Astrategies%20to%20produce%20both%20fine-grained%20and%20globally%20consistent%20rankings.%20On%0Athe%20BRIGHT%20benchmark%2C%20DIVER%20achieves%20state-of-the-art%20nDCG%4010%20scores%20of%2045.8%0Aoverall%20and%2028.9%20on%20original%20queries%2C%20consistently%20outperforming%20competitive%0Areasoning-aware%20models.%20These%20results%20demonstrate%20the%20effectiveness%20of%0Areasoning-aware%20retrieval%20strategies%20in%20complex%20real-world%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07995v3&entry.124074799=Read"},
{"title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model\n  Performance", "author": "Andrei-Valentin T\u0103nase and Elena Pelican", "abstract": "  Tokenization remains a fundamental yet underexplored bottleneck in natural\nlanguage processing, with strategies largely static despite remarkable progress\nin model architectures. We present SupraTok, a novel tokenization architecture\nthat reimagines subword segmentation through three innovations: cross-boundary\npattern learning that discovers multi-word semantic units, entropy-driven data\ncuration that optimizes training corpus quality, and multi-phase curriculum\nlearning for stable convergence. Our approach extends Byte-Pair Encoding by\nlearning \"superword\" tokens, coherent multi-word expressions that preserve\nsemantic unity while maximizing compression efficiency. SupraTok achieves 31%\nimprovement in English tokenization efficiency (5.91 versus 4.51 characters per\ntoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google's\nGemma 3 tokenizer (256k vocabulary), while maintaining competitive performance\nacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)\ntrained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%\nimprovement on HellaSWAG and 9.5% on MMLU benchmarks without architectural\nmodifications. While these results are promising at this scale, further\nvalidation at larger model scales is needed. These findings suggest that\nefficient tokenization can complement architectural innovations as a path to\nimproved language model performance.\n", "link": "http://arxiv.org/abs/2508.11857v2", "date": "2025-08-25", "relevancy": 2.0785, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5554}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5088}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SupraTok%3A%20Cross-Boundary%20Tokenization%20for%20Enhanced%20Language%20Model%0A%20%20Performance&body=Title%3A%20SupraTok%3A%20Cross-Boundary%20Tokenization%20for%20Enhanced%20Language%20Model%0A%20%20Performance%0AAuthor%3A%20Andrei-Valentin%20T%C4%83nase%20and%20Elena%20Pelican%0AAbstract%3A%20%20%20Tokenization%20remains%20a%20fundamental%20yet%20underexplored%20bottleneck%20in%20natural%0Alanguage%20processing%2C%20with%20strategies%20largely%20static%20despite%20remarkable%20progress%0Ain%20model%20architectures.%20We%20present%20SupraTok%2C%20a%20novel%20tokenization%20architecture%0Athat%20reimagines%20subword%20segmentation%20through%20three%20innovations%3A%20cross-boundary%0Apattern%20learning%20that%20discovers%20multi-word%20semantic%20units%2C%20entropy-driven%20data%0Acuration%20that%20optimizes%20training%20corpus%20quality%2C%20and%20multi-phase%20curriculum%0Alearning%20for%20stable%20convergence.%20Our%20approach%20extends%20Byte-Pair%20Encoding%20by%0Alearning%20%22superword%22%20tokens%2C%20coherent%20multi-word%20expressions%20that%20preserve%0Asemantic%20unity%20while%20maximizing%20compression%20efficiency.%20SupraTok%20achieves%2031%25%0Aimprovement%20in%20English%20tokenization%20efficiency%20%285.91%20versus%204.51%20characters%20per%0Atoken%29%20compared%20to%20OpenAI%27s%20o200k%20tokenizer%20and%2030%25%20improvement%20over%20Google%27s%0AGemma%203%20tokenizer%20%28256k%20vocabulary%29%2C%20while%20maintaining%20competitive%20performance%0Aacross%2038%20languages.%20When%20integrated%20with%20a%20GPT-2%20scale%20model%20%28124M%20parameters%29%0Atrained%20on%2010%20billion%20tokens%20from%20the%20FineWeb-Edu%20dataset%2C%20SupraTok%20yields%208.4%25%0Aimprovement%20on%20HellaSWAG%20and%209.5%25%20on%20MMLU%20benchmarks%20without%20architectural%0Amodifications.%20While%20these%20results%20are%20promising%20at%20this%20scale%2C%20further%0Avalidation%20at%20larger%20model%20scales%20is%20needed.%20These%20findings%20suggest%20that%0Aefficient%20tokenization%20can%20complement%20architectural%20innovations%20as%20a%20path%20to%0Aimproved%20language%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupraTok%253A%2520Cross-Boundary%2520Tokenization%2520for%2520Enhanced%2520Language%2520Model%250A%2520%2520Performance%26entry.906535625%3DAndrei-Valentin%2520T%25C4%2583nase%2520and%2520Elena%2520Pelican%26entry.1292438233%3D%2520%2520Tokenization%2520remains%2520a%2520fundamental%2520yet%2520underexplored%2520bottleneck%2520in%2520natural%250Alanguage%2520processing%252C%2520with%2520strategies%2520largely%2520static%2520despite%2520remarkable%2520progress%250Ain%2520model%2520architectures.%2520We%2520present%2520SupraTok%252C%2520a%2520novel%2520tokenization%2520architecture%250Athat%2520reimagines%2520subword%2520segmentation%2520through%2520three%2520innovations%253A%2520cross-boundary%250Apattern%2520learning%2520that%2520discovers%2520multi-word%2520semantic%2520units%252C%2520entropy-driven%2520data%250Acuration%2520that%2520optimizes%2520training%2520corpus%2520quality%252C%2520and%2520multi-phase%2520curriculum%250Alearning%2520for%2520stable%2520convergence.%2520Our%2520approach%2520extends%2520Byte-Pair%2520Encoding%2520by%250Alearning%2520%2522superword%2522%2520tokens%252C%2520coherent%2520multi-word%2520expressions%2520that%2520preserve%250Asemantic%2520unity%2520while%2520maximizing%2520compression%2520efficiency.%2520SupraTok%2520achieves%252031%2525%250Aimprovement%2520in%2520English%2520tokenization%2520efficiency%2520%25285.91%2520versus%25204.51%2520characters%2520per%250Atoken%2529%2520compared%2520to%2520OpenAI%2527s%2520o200k%2520tokenizer%2520and%252030%2525%2520improvement%2520over%2520Google%2527s%250AGemma%25203%2520tokenizer%2520%2528256k%2520vocabulary%2529%252C%2520while%2520maintaining%2520competitive%2520performance%250Aacross%252038%2520languages.%2520When%2520integrated%2520with%2520a%2520GPT-2%2520scale%2520model%2520%2528124M%2520parameters%2529%250Atrained%2520on%252010%2520billion%2520tokens%2520from%2520the%2520FineWeb-Edu%2520dataset%252C%2520SupraTok%2520yields%25208.4%2525%250Aimprovement%2520on%2520HellaSWAG%2520and%25209.5%2525%2520on%2520MMLU%2520benchmarks%2520without%2520architectural%250Amodifications.%2520While%2520these%2520results%2520are%2520promising%2520at%2520this%2520scale%252C%2520further%250Avalidation%2520at%2520larger%2520model%2520scales%2520is%2520needed.%2520These%2520findings%2520suggest%2520that%250Aefficient%2520tokenization%2520can%2520complement%2520architectural%2520innovations%2520as%2520a%2520path%2520to%250Aimproved%2520language%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SupraTok%3A%20Cross-Boundary%20Tokenization%20for%20Enhanced%20Language%20Model%0A%20%20Performance&entry.906535625=Andrei-Valentin%20T%C4%83nase%20and%20Elena%20Pelican&entry.1292438233=%20%20Tokenization%20remains%20a%20fundamental%20yet%20underexplored%20bottleneck%20in%20natural%0Alanguage%20processing%2C%20with%20strategies%20largely%20static%20despite%20remarkable%20progress%0Ain%20model%20architectures.%20We%20present%20SupraTok%2C%20a%20novel%20tokenization%20architecture%0Athat%20reimagines%20subword%20segmentation%20through%20three%20innovations%3A%20cross-boundary%0Apattern%20learning%20that%20discovers%20multi-word%20semantic%20units%2C%20entropy-driven%20data%0Acuration%20that%20optimizes%20training%20corpus%20quality%2C%20and%20multi-phase%20curriculum%0Alearning%20for%20stable%20convergence.%20Our%20approach%20extends%20Byte-Pair%20Encoding%20by%0Alearning%20%22superword%22%20tokens%2C%20coherent%20multi-word%20expressions%20that%20preserve%0Asemantic%20unity%20while%20maximizing%20compression%20efficiency.%20SupraTok%20achieves%2031%25%0Aimprovement%20in%20English%20tokenization%20efficiency%20%285.91%20versus%204.51%20characters%20per%0Atoken%29%20compared%20to%20OpenAI%27s%20o200k%20tokenizer%20and%2030%25%20improvement%20over%20Google%27s%0AGemma%203%20tokenizer%20%28256k%20vocabulary%29%2C%20while%20maintaining%20competitive%20performance%0Aacross%2038%20languages.%20When%20integrated%20with%20a%20GPT-2%20scale%20model%20%28124M%20parameters%29%0Atrained%20on%2010%20billion%20tokens%20from%20the%20FineWeb-Edu%20dataset%2C%20SupraTok%20yields%208.4%25%0Aimprovement%20on%20HellaSWAG%20and%209.5%25%20on%20MMLU%20benchmarks%20without%20architectural%0Amodifications.%20While%20these%20results%20are%20promising%20at%20this%20scale%2C%20further%0Avalidation%20at%20larger%20model%20scales%20is%20needed.%20These%20findings%20suggest%20that%0Aefficient%20tokenization%20can%20complement%20architectural%20innovations%20as%20a%20path%20to%0Aimproved%20language%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11857v2&entry.124074799=Read"},
{"title": "PerPilot: Personalizing VLM-based Mobile Agents via Memory and\n  Exploration", "author": "Xin Wang and Zhiyao Cui and Hao Li and Ya Zeng and Chenxu Wang and Ruiqi Song and Yihang Chen and Kun Shao and Qiaosheng Zhang and Jinzhuo Liu and Siyue Ren and Shuyue Hu and Zhen Wang", "abstract": "  Vision language model (VLM)-based mobile agents show great potential for\nassisting users in performing instruction-driven tasks. However, these agents\ntypically struggle with personalized instructions -- those containing\nambiguous, user-specific context -- a challenge that has been largely\noverlooked in previous research. In this paper, we define personalized\ninstructions and introduce PerInstruct, a novel human-annotated dataset\ncovering diverse personalized instructions across various mobile scenarios.\nFurthermore, given the limited personalization capabilities of existing mobile\nagents, we propose PerPilot, a plug-and-play framework powered by large\nlanguage models (LLMs) that enables mobile agents to autonomously perceive,\nunderstand, and execute personalized user instructions. PerPilot identifies\npersonalized elements and autonomously completes instructions via two\ncomplementary approaches: memory-based retrieval and reasoning-based\nexploration. Experimental results demonstrate that PerPilot effectively handles\npersonalized tasks with minimal user intervention and progressively improves\nits performance with continued use, underscoring the importance of\npersonalization-aware reasoning for next-generation mobile agents. The dataset\nand code are available at: https://github.com/xinwang-nwpu/PerPilot\n", "link": "http://arxiv.org/abs/2508.18040v1", "date": "2025-08-25", "relevancy": 2.0713, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerPilot%3A%20Personalizing%20VLM-based%20Mobile%20Agents%20via%20Memory%20and%0A%20%20Exploration&body=Title%3A%20PerPilot%3A%20Personalizing%20VLM-based%20Mobile%20Agents%20via%20Memory%20and%0A%20%20Exploration%0AAuthor%3A%20Xin%20Wang%20and%20Zhiyao%20Cui%20and%20Hao%20Li%20and%20Ya%20Zeng%20and%20Chenxu%20Wang%20and%20Ruiqi%20Song%20and%20Yihang%20Chen%20and%20Kun%20Shao%20and%20Qiaosheng%20Zhang%20and%20Jinzhuo%20Liu%20and%20Siyue%20Ren%20and%20Shuyue%20Hu%20and%20Zhen%20Wang%0AAbstract%3A%20%20%20Vision%20language%20model%20%28VLM%29-based%20mobile%20agents%20show%20great%20potential%20for%0Aassisting%20users%20in%20performing%20instruction-driven%20tasks.%20However%2C%20these%20agents%0Atypically%20struggle%20with%20personalized%20instructions%20--%20those%20containing%0Aambiguous%2C%20user-specific%20context%20--%20a%20challenge%20that%20has%20been%20largely%0Aoverlooked%20in%20previous%20research.%20In%20this%20paper%2C%20we%20define%20personalized%0Ainstructions%20and%20introduce%20PerInstruct%2C%20a%20novel%20human-annotated%20dataset%0Acovering%20diverse%20personalized%20instructions%20across%20various%20mobile%20scenarios.%0AFurthermore%2C%20given%20the%20limited%20personalization%20capabilities%20of%20existing%20mobile%0Aagents%2C%20we%20propose%20PerPilot%2C%20a%20plug-and-play%20framework%20powered%20by%20large%0Alanguage%20models%20%28LLMs%29%20that%20enables%20mobile%20agents%20to%20autonomously%20perceive%2C%0Aunderstand%2C%20and%20execute%20personalized%20user%20instructions.%20PerPilot%20identifies%0Apersonalized%20elements%20and%20autonomously%20completes%20instructions%20via%20two%0Acomplementary%20approaches%3A%20memory-based%20retrieval%20and%20reasoning-based%0Aexploration.%20Experimental%20results%20demonstrate%20that%20PerPilot%20effectively%20handles%0Apersonalized%20tasks%20with%20minimal%20user%20intervention%20and%20progressively%20improves%0Aits%20performance%20with%20continued%20use%2C%20underscoring%20the%20importance%20of%0Apersonalization-aware%20reasoning%20for%20next-generation%20mobile%20agents.%20The%20dataset%0Aand%20code%20are%20available%20at%3A%20https%3A//github.com/xinwang-nwpu/PerPilot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerPilot%253A%2520Personalizing%2520VLM-based%2520Mobile%2520Agents%2520via%2520Memory%2520and%250A%2520%2520Exploration%26entry.906535625%3DXin%2520Wang%2520and%2520Zhiyao%2520Cui%2520and%2520Hao%2520Li%2520and%2520Ya%2520Zeng%2520and%2520Chenxu%2520Wang%2520and%2520Ruiqi%2520Song%2520and%2520Yihang%2520Chen%2520and%2520Kun%2520Shao%2520and%2520Qiaosheng%2520Zhang%2520and%2520Jinzhuo%2520Liu%2520and%2520Siyue%2520Ren%2520and%2520Shuyue%2520Hu%2520and%2520Zhen%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520language%2520model%2520%2528VLM%2529-based%2520mobile%2520agents%2520show%2520great%2520potential%2520for%250Aassisting%2520users%2520in%2520performing%2520instruction-driven%2520tasks.%2520However%252C%2520these%2520agents%250Atypically%2520struggle%2520with%2520personalized%2520instructions%2520--%2520those%2520containing%250Aambiguous%252C%2520user-specific%2520context%2520--%2520a%2520challenge%2520that%2520has%2520been%2520largely%250Aoverlooked%2520in%2520previous%2520research.%2520In%2520this%2520paper%252C%2520we%2520define%2520personalized%250Ainstructions%2520and%2520introduce%2520PerInstruct%252C%2520a%2520novel%2520human-annotated%2520dataset%250Acovering%2520diverse%2520personalized%2520instructions%2520across%2520various%2520mobile%2520scenarios.%250AFurthermore%252C%2520given%2520the%2520limited%2520personalization%2520capabilities%2520of%2520existing%2520mobile%250Aagents%252C%2520we%2520propose%2520PerPilot%252C%2520a%2520plug-and-play%2520framework%2520powered%2520by%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520that%2520enables%2520mobile%2520agents%2520to%2520autonomously%2520perceive%252C%250Aunderstand%252C%2520and%2520execute%2520personalized%2520user%2520instructions.%2520PerPilot%2520identifies%250Apersonalized%2520elements%2520and%2520autonomously%2520completes%2520instructions%2520via%2520two%250Acomplementary%2520approaches%253A%2520memory-based%2520retrieval%2520and%2520reasoning-based%250Aexploration.%2520Experimental%2520results%2520demonstrate%2520that%2520PerPilot%2520effectively%2520handles%250Apersonalized%2520tasks%2520with%2520minimal%2520user%2520intervention%2520and%2520progressively%2520improves%250Aits%2520performance%2520with%2520continued%2520use%252C%2520underscoring%2520the%2520importance%2520of%250Apersonalization-aware%2520reasoning%2520for%2520next-generation%2520mobile%2520agents.%2520The%2520dataset%250Aand%2520code%2520are%2520available%2520at%253A%2520https%253A//github.com/xinwang-nwpu/PerPilot%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerPilot%3A%20Personalizing%20VLM-based%20Mobile%20Agents%20via%20Memory%20and%0A%20%20Exploration&entry.906535625=Xin%20Wang%20and%20Zhiyao%20Cui%20and%20Hao%20Li%20and%20Ya%20Zeng%20and%20Chenxu%20Wang%20and%20Ruiqi%20Song%20and%20Yihang%20Chen%20and%20Kun%20Shao%20and%20Qiaosheng%20Zhang%20and%20Jinzhuo%20Liu%20and%20Siyue%20Ren%20and%20Shuyue%20Hu%20and%20Zhen%20Wang&entry.1292438233=%20%20Vision%20language%20model%20%28VLM%29-based%20mobile%20agents%20show%20great%20potential%20for%0Aassisting%20users%20in%20performing%20instruction-driven%20tasks.%20However%2C%20these%20agents%0Atypically%20struggle%20with%20personalized%20instructions%20--%20those%20containing%0Aambiguous%2C%20user-specific%20context%20--%20a%20challenge%20that%20has%20been%20largely%0Aoverlooked%20in%20previous%20research.%20In%20this%20paper%2C%20we%20define%20personalized%0Ainstructions%20and%20introduce%20PerInstruct%2C%20a%20novel%20human-annotated%20dataset%0Acovering%20diverse%20personalized%20instructions%20across%20various%20mobile%20scenarios.%0AFurthermore%2C%20given%20the%20limited%20personalization%20capabilities%20of%20existing%20mobile%0Aagents%2C%20we%20propose%20PerPilot%2C%20a%20plug-and-play%20framework%20powered%20by%20large%0Alanguage%20models%20%28LLMs%29%20that%20enables%20mobile%20agents%20to%20autonomously%20perceive%2C%0Aunderstand%2C%20and%20execute%20personalized%20user%20instructions.%20PerPilot%20identifies%0Apersonalized%20elements%20and%20autonomously%20completes%20instructions%20via%20two%0Acomplementary%20approaches%3A%20memory-based%20retrieval%20and%20reasoning-based%0Aexploration.%20Experimental%20results%20demonstrate%20that%20PerPilot%20effectively%20handles%0Apersonalized%20tasks%20with%20minimal%20user%20intervention%20and%20progressively%20improves%0Aits%20performance%20with%20continued%20use%2C%20underscoring%20the%20importance%20of%0Apersonalization-aware%20reasoning%20for%20next-generation%20mobile%20agents.%20The%20dataset%0Aand%20code%20are%20available%20at%3A%20https%3A//github.com/xinwang-nwpu/PerPilot%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18040v1&entry.124074799=Read"},
{"title": "FastTracker: Real-Time and Accurate Visual Tracking", "author": "Hamidreza Hashempoor and Yu Dong Hwang", "abstract": "  Conventional multi-object tracking (MOT) systems are predominantly designed\nfor pedestrian tracking and often exhibit limited generalization to other\nobject categories. This paper presents a generalized tracking framework capable\nof handling multiple object types, with a particular emphasis on vehicle\ntracking in complex traffic scenes. The proposed method incorporates two key\ncomponents: (1) an occlusion-aware re-identification mechanism that enhances\nidentity preservation for heavily occluded objects, and (2) a\nroad-structure-aware tracklet refinement strategy that utilizes semantic scene\npriors such as lane directions, crosswalks, and road boundaries to improve\ntrajectory continuity and accuracy. In addition, we introduce a new benchmark\ndataset comprising diverse vehicle classes with frame-level tracking\nannotations, specifically curated to support evaluation of vehicle-focused\ntracking methods. Extensive experimental results demonstrate that the proposed\napproach achieves robust performance on both the newly introduced dataset and\nseveral public benchmarks, highlighting its effectiveness in general-purpose\nobject tracking. While our framework is designed for generalized multi-class\ntracking, it also achieves strong performance on conventional benchmarks, with\nHOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark\nare available: github.com/Hamidreza-Hashempoor/FastTracker,\nhuggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.\n", "link": "http://arxiv.org/abs/2508.14370v2", "date": "2025-08-25", "relevancy": 2.0576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5376}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastTracker%3A%20Real-Time%20and%20Accurate%20Visual%20Tracking&body=Title%3A%20FastTracker%3A%20Real-Time%20and%20Accurate%20Visual%20Tracking%0AAuthor%3A%20Hamidreza%20Hashempoor%20and%20Yu%20Dong%20Hwang%0AAbstract%3A%20%20%20Conventional%20multi-object%20tracking%20%28MOT%29%20systems%20are%20predominantly%20designed%0Afor%20pedestrian%20tracking%20and%20often%20exhibit%20limited%20generalization%20to%20other%0Aobject%20categories.%20This%20paper%20presents%20a%20generalized%20tracking%20framework%20capable%0Aof%20handling%20multiple%20object%20types%2C%20with%20a%20particular%20emphasis%20on%20vehicle%0Atracking%20in%20complex%20traffic%20scenes.%20The%20proposed%20method%20incorporates%20two%20key%0Acomponents%3A%20%281%29%20an%20occlusion-aware%20re-identification%20mechanism%20that%20enhances%0Aidentity%20preservation%20for%20heavily%20occluded%20objects%2C%20and%20%282%29%20a%0Aroad-structure-aware%20tracklet%20refinement%20strategy%20that%20utilizes%20semantic%20scene%0Apriors%20such%20as%20lane%20directions%2C%20crosswalks%2C%20and%20road%20boundaries%20to%20improve%0Atrajectory%20continuity%20and%20accuracy.%20In%20addition%2C%20we%20introduce%20a%20new%20benchmark%0Adataset%20comprising%20diverse%20vehicle%20classes%20with%20frame-level%20tracking%0Aannotations%2C%20specifically%20curated%20to%20support%20evaluation%20of%20vehicle-focused%0Atracking%20methods.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%0Aapproach%20achieves%20robust%20performance%20on%20both%20the%20newly%20introduced%20dataset%20and%0Aseveral%20public%20benchmarks%2C%20highlighting%20its%20effectiveness%20in%20general-purpose%0Aobject%20tracking.%20While%20our%20framework%20is%20designed%20for%20generalized%20multi-class%0Atracking%2C%20it%20also%20achieves%20strong%20performance%20on%20conventional%20benchmarks%2C%20with%0AHOTA%20scores%20of%2066.4%20on%20MOT17%20and%2065.7%20on%20MOT20%20test%20sets.%20Code%20and%20Benchmark%0Aare%20available%3A%20github.com/Hamidreza-Hashempoor/FastTracker%2C%0Ahuggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastTracker%253A%2520Real-Time%2520and%2520Accurate%2520Visual%2520Tracking%26entry.906535625%3DHamidreza%2520Hashempoor%2520and%2520Yu%2520Dong%2520Hwang%26entry.1292438233%3D%2520%2520Conventional%2520multi-object%2520tracking%2520%2528MOT%2529%2520systems%2520are%2520predominantly%2520designed%250Afor%2520pedestrian%2520tracking%2520and%2520often%2520exhibit%2520limited%2520generalization%2520to%2520other%250Aobject%2520categories.%2520This%2520paper%2520presents%2520a%2520generalized%2520tracking%2520framework%2520capable%250Aof%2520handling%2520multiple%2520object%2520types%252C%2520with%2520a%2520particular%2520emphasis%2520on%2520vehicle%250Atracking%2520in%2520complex%2520traffic%2520scenes.%2520The%2520proposed%2520method%2520incorporates%2520two%2520key%250Acomponents%253A%2520%25281%2529%2520an%2520occlusion-aware%2520re-identification%2520mechanism%2520that%2520enhances%250Aidentity%2520preservation%2520for%2520heavily%2520occluded%2520objects%252C%2520and%2520%25282%2529%2520a%250Aroad-structure-aware%2520tracklet%2520refinement%2520strategy%2520that%2520utilizes%2520semantic%2520scene%250Apriors%2520such%2520as%2520lane%2520directions%252C%2520crosswalks%252C%2520and%2520road%2520boundaries%2520to%2520improve%250Atrajectory%2520continuity%2520and%2520accuracy.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520new%2520benchmark%250Adataset%2520comprising%2520diverse%2520vehicle%2520classes%2520with%2520frame-level%2520tracking%250Aannotations%252C%2520specifically%2520curated%2520to%2520support%2520evaluation%2520of%2520vehicle-focused%250Atracking%2520methods.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%2520achieves%2520robust%2520performance%2520on%2520both%2520the%2520newly%2520introduced%2520dataset%2520and%250Aseveral%2520public%2520benchmarks%252C%2520highlighting%2520its%2520effectiveness%2520in%2520general-purpose%250Aobject%2520tracking.%2520While%2520our%2520framework%2520is%2520designed%2520for%2520generalized%2520multi-class%250Atracking%252C%2520it%2520also%2520achieves%2520strong%2520performance%2520on%2520conventional%2520benchmarks%252C%2520with%250AHOTA%2520scores%2520of%252066.4%2520on%2520MOT17%2520and%252065.7%2520on%2520MOT20%2520test%2520sets.%2520Code%2520and%2520Benchmark%250Aare%2520available%253A%2520github.com/Hamidreza-Hashempoor/FastTracker%252C%250Ahuggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastTracker%3A%20Real-Time%20and%20Accurate%20Visual%20Tracking&entry.906535625=Hamidreza%20Hashempoor%20and%20Yu%20Dong%20Hwang&entry.1292438233=%20%20Conventional%20multi-object%20tracking%20%28MOT%29%20systems%20are%20predominantly%20designed%0Afor%20pedestrian%20tracking%20and%20often%20exhibit%20limited%20generalization%20to%20other%0Aobject%20categories.%20This%20paper%20presents%20a%20generalized%20tracking%20framework%20capable%0Aof%20handling%20multiple%20object%20types%2C%20with%20a%20particular%20emphasis%20on%20vehicle%0Atracking%20in%20complex%20traffic%20scenes.%20The%20proposed%20method%20incorporates%20two%20key%0Acomponents%3A%20%281%29%20an%20occlusion-aware%20re-identification%20mechanism%20that%20enhances%0Aidentity%20preservation%20for%20heavily%20occluded%20objects%2C%20and%20%282%29%20a%0Aroad-structure-aware%20tracklet%20refinement%20strategy%20that%20utilizes%20semantic%20scene%0Apriors%20such%20as%20lane%20directions%2C%20crosswalks%2C%20and%20road%20boundaries%20to%20improve%0Atrajectory%20continuity%20and%20accuracy.%20In%20addition%2C%20we%20introduce%20a%20new%20benchmark%0Adataset%20comprising%20diverse%20vehicle%20classes%20with%20frame-level%20tracking%0Aannotations%2C%20specifically%20curated%20to%20support%20evaluation%20of%20vehicle-focused%0Atracking%20methods.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%0Aapproach%20achieves%20robust%20performance%20on%20both%20the%20newly%20introduced%20dataset%20and%0Aseveral%20public%20benchmarks%2C%20highlighting%20its%20effectiveness%20in%20general-purpose%0Aobject%20tracking.%20While%20our%20framework%20is%20designed%20for%20generalized%20multi-class%0Atracking%2C%20it%20also%20achieves%20strong%20performance%20on%20conventional%20benchmarks%2C%20with%0AHOTA%20scores%20of%2066.4%20on%20MOT17%20and%2065.7%20on%20MOT20%20test%20sets.%20Code%20and%20Benchmark%0Aare%20available%3A%20github.com/Hamidreza-Hashempoor/FastTracker%2C%0Ahuggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14370v2&entry.124074799=Read"},
{"title": "Language Models Coupled with Metacognition Can Outperform Reasoning\n  Models", "author": "Vedant Khandelwal and Francesca Rossi and Keerthiram Murugesan and Erik Miehling and Murray Campbell and Karthikeyan Natesan Ramamurthy and Lior Horesh", "abstract": "  Large language models (LLMs) excel in speed and adaptability across various\nreasoning tasks, but they often struggle when strict logic or constraint\nenforcement is required. In contrast, Large Reasoning Models (LRMs) are\nspecifically designed for complex, step-by-step reasoning, although they come\nwith significant computational costs and slower inference times. To address\nthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)\ncognitive architecture into SOFAI-LM, which coordinates a fast LLM with a\nslower but more powerful LRM through metacognition. The metacognitive module\nactively monitors the LLM's performance and provides targeted, iterative\nfeedback with relevant examples. This enables the LLM to progressively refine\nits solutions without requiring the need for additional model fine-tuning.\nExtensive experiments on graph coloring and code debugging problems demonstrate\nthat our feedback-driven approach significantly enhances the problem-solving\ncapabilities of the LLM. In many instances, it achieves performance levels that\nmatch or even exceed those of standalone LRMs while requiring considerably less\ntime. Additionally, when the LLM and feedback mechanism alone are insufficient,\nwe engage the LRM by providing appropriate information collected during the\nLLM's feedback loop, tailored to the specific characteristics of the problem\ndomain and leads to improved overall performance. Evaluations on two\ncontrasting domains: graph coloring, requiring globally consistent solutions,\nand code debugging, demanding localized fixes, demonstrate that SOFAI-LM\nenables LLMs to match or outperform standalone LRMs in accuracy while\nmaintaining significantly lower inference time.\n", "link": "http://arxiv.org/abs/2508.17959v1", "date": "2025-08-25", "relevancy": 2.0545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20Coupled%20with%20Metacognition%20Can%20Outperform%20Reasoning%0A%20%20Models&body=Title%3A%20Language%20Models%20Coupled%20with%20Metacognition%20Can%20Outperform%20Reasoning%0A%20%20Models%0AAuthor%3A%20Vedant%20Khandelwal%20and%20Francesca%20Rossi%20and%20Keerthiram%20Murugesan%20and%20Erik%20Miehling%20and%20Murray%20Campbell%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Lior%20Horesh%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20speed%20and%20adaptability%20across%20various%0Areasoning%20tasks%2C%20but%20they%20often%20struggle%20when%20strict%20logic%20or%20constraint%0Aenforcement%20is%20required.%20In%20contrast%2C%20Large%20Reasoning%20Models%20%28LRMs%29%20are%0Aspecifically%20designed%20for%20complex%2C%20step-by-step%20reasoning%2C%20although%20they%20come%0Awith%20significant%20computational%20costs%20and%20slower%20inference%20times.%20To%20address%0Athese%20trade-offs%2C%20we%20employ%20and%20generalize%20the%20SOFAI%20%28Slow%20and%20Fast%20AI%29%0Acognitive%20architecture%20into%20SOFAI-LM%2C%20which%20coordinates%20a%20fast%20LLM%20with%20a%0Aslower%20but%20more%20powerful%20LRM%20through%20metacognition.%20The%20metacognitive%20module%0Aactively%20monitors%20the%20LLM%27s%20performance%20and%20provides%20targeted%2C%20iterative%0Afeedback%20with%20relevant%20examples.%20This%20enables%20the%20LLM%20to%20progressively%20refine%0Aits%20solutions%20without%20requiring%20the%20need%20for%20additional%20model%20fine-tuning.%0AExtensive%20experiments%20on%20graph%20coloring%20and%20code%20debugging%20problems%20demonstrate%0Athat%20our%20feedback-driven%20approach%20significantly%20enhances%20the%20problem-solving%0Acapabilities%20of%20the%20LLM.%20In%20many%20instances%2C%20it%20achieves%20performance%20levels%20that%0Amatch%20or%20even%20exceed%20those%20of%20standalone%20LRMs%20while%20requiring%20considerably%20less%0Atime.%20Additionally%2C%20when%20the%20LLM%20and%20feedback%20mechanism%20alone%20are%20insufficient%2C%0Awe%20engage%20the%20LRM%20by%20providing%20appropriate%20information%20collected%20during%20the%0ALLM%27s%20feedback%20loop%2C%20tailored%20to%20the%20specific%20characteristics%20of%20the%20problem%0Adomain%20and%20leads%20to%20improved%20overall%20performance.%20Evaluations%20on%20two%0Acontrasting%20domains%3A%20graph%20coloring%2C%20requiring%20globally%20consistent%20solutions%2C%0Aand%20code%20debugging%2C%20demanding%20localized%20fixes%2C%20demonstrate%20that%20SOFAI-LM%0Aenables%20LLMs%20to%20match%20or%20outperform%20standalone%20LRMs%20in%20accuracy%20while%0Amaintaining%20significantly%20lower%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520Coupled%2520with%2520Metacognition%2520Can%2520Outperform%2520Reasoning%250A%2520%2520Models%26entry.906535625%3DVedant%2520Khandelwal%2520and%2520Francesca%2520Rossi%2520and%2520Keerthiram%2520Murugesan%2520and%2520Erik%2520Miehling%2520and%2520Murray%2520Campbell%2520and%2520Karthikeyan%2520Natesan%2520Ramamurthy%2520and%2520Lior%2520Horesh%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520speed%2520and%2520adaptability%2520across%2520various%250Areasoning%2520tasks%252C%2520but%2520they%2520often%2520struggle%2520when%2520strict%2520logic%2520or%2520constraint%250Aenforcement%2520is%2520required.%2520In%2520contrast%252C%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520are%250Aspecifically%2520designed%2520for%2520complex%252C%2520step-by-step%2520reasoning%252C%2520although%2520they%2520come%250Awith%2520significant%2520computational%2520costs%2520and%2520slower%2520inference%2520times.%2520To%2520address%250Athese%2520trade-offs%252C%2520we%2520employ%2520and%2520generalize%2520the%2520SOFAI%2520%2528Slow%2520and%2520Fast%2520AI%2529%250Acognitive%2520architecture%2520into%2520SOFAI-LM%252C%2520which%2520coordinates%2520a%2520fast%2520LLM%2520with%2520a%250Aslower%2520but%2520more%2520powerful%2520LRM%2520through%2520metacognition.%2520The%2520metacognitive%2520module%250Aactively%2520monitors%2520the%2520LLM%2527s%2520performance%2520and%2520provides%2520targeted%252C%2520iterative%250Afeedback%2520with%2520relevant%2520examples.%2520This%2520enables%2520the%2520LLM%2520to%2520progressively%2520refine%250Aits%2520solutions%2520without%2520requiring%2520the%2520need%2520for%2520additional%2520model%2520fine-tuning.%250AExtensive%2520experiments%2520on%2520graph%2520coloring%2520and%2520code%2520debugging%2520problems%2520demonstrate%250Athat%2520our%2520feedback-driven%2520approach%2520significantly%2520enhances%2520the%2520problem-solving%250Acapabilities%2520of%2520the%2520LLM.%2520In%2520many%2520instances%252C%2520it%2520achieves%2520performance%2520levels%2520that%250Amatch%2520or%2520even%2520exceed%2520those%2520of%2520standalone%2520LRMs%2520while%2520requiring%2520considerably%2520less%250Atime.%2520Additionally%252C%2520when%2520the%2520LLM%2520and%2520feedback%2520mechanism%2520alone%2520are%2520insufficient%252C%250Awe%2520engage%2520the%2520LRM%2520by%2520providing%2520appropriate%2520information%2520collected%2520during%2520the%250ALLM%2527s%2520feedback%2520loop%252C%2520tailored%2520to%2520the%2520specific%2520characteristics%2520of%2520the%2520problem%250Adomain%2520and%2520leads%2520to%2520improved%2520overall%2520performance.%2520Evaluations%2520on%2520two%250Acontrasting%2520domains%253A%2520graph%2520coloring%252C%2520requiring%2520globally%2520consistent%2520solutions%252C%250Aand%2520code%2520debugging%252C%2520demanding%2520localized%2520fixes%252C%2520demonstrate%2520that%2520SOFAI-LM%250Aenables%2520LLMs%2520to%2520match%2520or%2520outperform%2520standalone%2520LRMs%2520in%2520accuracy%2520while%250Amaintaining%2520significantly%2520lower%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20Coupled%20with%20Metacognition%20Can%20Outperform%20Reasoning%0A%20%20Models&entry.906535625=Vedant%20Khandelwal%20and%20Francesca%20Rossi%20and%20Keerthiram%20Murugesan%20and%20Erik%20Miehling%20and%20Murray%20Campbell%20and%20Karthikeyan%20Natesan%20Ramamurthy%20and%20Lior%20Horesh&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20speed%20and%20adaptability%20across%20various%0Areasoning%20tasks%2C%20but%20they%20often%20struggle%20when%20strict%20logic%20or%20constraint%0Aenforcement%20is%20required.%20In%20contrast%2C%20Large%20Reasoning%20Models%20%28LRMs%29%20are%0Aspecifically%20designed%20for%20complex%2C%20step-by-step%20reasoning%2C%20although%20they%20come%0Awith%20significant%20computational%20costs%20and%20slower%20inference%20times.%20To%20address%0Athese%20trade-offs%2C%20we%20employ%20and%20generalize%20the%20SOFAI%20%28Slow%20and%20Fast%20AI%29%0Acognitive%20architecture%20into%20SOFAI-LM%2C%20which%20coordinates%20a%20fast%20LLM%20with%20a%0Aslower%20but%20more%20powerful%20LRM%20through%20metacognition.%20The%20metacognitive%20module%0Aactively%20monitors%20the%20LLM%27s%20performance%20and%20provides%20targeted%2C%20iterative%0Afeedback%20with%20relevant%20examples.%20This%20enables%20the%20LLM%20to%20progressively%20refine%0Aits%20solutions%20without%20requiring%20the%20need%20for%20additional%20model%20fine-tuning.%0AExtensive%20experiments%20on%20graph%20coloring%20and%20code%20debugging%20problems%20demonstrate%0Athat%20our%20feedback-driven%20approach%20significantly%20enhances%20the%20problem-solving%0Acapabilities%20of%20the%20LLM.%20In%20many%20instances%2C%20it%20achieves%20performance%20levels%20that%0Amatch%20or%20even%20exceed%20those%20of%20standalone%20LRMs%20while%20requiring%20considerably%20less%0Atime.%20Additionally%2C%20when%20the%20LLM%20and%20feedback%20mechanism%20alone%20are%20insufficient%2C%0Awe%20engage%20the%20LRM%20by%20providing%20appropriate%20information%20collected%20during%20the%0ALLM%27s%20feedback%20loop%2C%20tailored%20to%20the%20specific%20characteristics%20of%20the%20problem%0Adomain%20and%20leads%20to%20improved%20overall%20performance.%20Evaluations%20on%20two%0Acontrasting%20domains%3A%20graph%20coloring%2C%20requiring%20globally%20consistent%20solutions%2C%0Aand%20code%20debugging%2C%20demanding%20localized%20fixes%2C%20demonstrate%20that%20SOFAI-LM%0Aenables%20LLMs%20to%20match%20or%20outperform%20standalone%20LRMs%20in%20accuracy%20while%0Amaintaining%20significantly%20lower%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17959v1&entry.124074799=Read"},
{"title": "Detecting and Characterizing Planning in Language Models", "author": "Jatin Nainani and Sankaran Vaidyanathan and Connor Watts and Andre N. Assis and Alice Rigg", "abstract": "  Modern large language models (LLMs) have demonstrated impressive performance\nacross a wide range of multi-step reasoning tasks. Recent work suggests that\nLLMs may perform planning - selecting a future target token in advance and\ngenerating intermediate tokens that lead towards it - rather than merely\nimprovising one token at a time. However, existing studies assume fixed\nplanning horizons and often focus on single prompts or narrow domains. To\ndistinguish planning from improvisation across models and tasks, we present\nformal and causally grounded criteria for detecting planning and operationalize\nthem as a semi-automated annotation pipeline. We apply this pipeline to both\nbase and instruction-tuned Gemma-2-2B models on the MBPP code generation\nbenchmark and a poem generation task where Claude 3.5 Haiku was previously\nshown to plan. Our findings show that planning is not universal: unlike Haiku,\nGemma-2-2B solves the same poem generation task through improvisation, and on\nMBPP it switches between planning and improvisation across similar tasks and\neven successive token predictions. We further show that instruction tuning\nrefines existing planning behaviors in the base model rather than creating them\nfrom scratch. Together, these studies provide a reproducible and scalable\nfoundation for mechanistic studies of planning in LLMs.\n", "link": "http://arxiv.org/abs/2508.18098v1", "date": "2025-08-25", "relevancy": 2.0515, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5191}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5191}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Characterizing%20Planning%20in%20Language%20Models&body=Title%3A%20Detecting%20and%20Characterizing%20Planning%20in%20Language%20Models%0AAuthor%3A%20Jatin%20Nainani%20and%20Sankaran%20Vaidyanathan%20and%20Connor%20Watts%20and%20Andre%20N.%20Assis%20and%20Alice%20Rigg%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%0Aacross%20a%20wide%20range%20of%20multi-step%20reasoning%20tasks.%20Recent%20work%20suggests%20that%0ALLMs%20may%20perform%20planning%20-%20selecting%20a%20future%20target%20token%20in%20advance%20and%0Agenerating%20intermediate%20tokens%20that%20lead%20towards%20it%20-%20rather%20than%20merely%0Aimprovising%20one%20token%20at%20a%20time.%20However%2C%20existing%20studies%20assume%20fixed%0Aplanning%20horizons%20and%20often%20focus%20on%20single%20prompts%20or%20narrow%20domains.%20To%0Adistinguish%20planning%20from%20improvisation%20across%20models%20and%20tasks%2C%20we%20present%0Aformal%20and%20causally%20grounded%20criteria%20for%20detecting%20planning%20and%20operationalize%0Athem%20as%20a%20semi-automated%20annotation%20pipeline.%20We%20apply%20this%20pipeline%20to%20both%0Abase%20and%20instruction-tuned%20Gemma-2-2B%20models%20on%20the%20MBPP%20code%20generation%0Abenchmark%20and%20a%20poem%20generation%20task%20where%20Claude%203.5%20Haiku%20was%20previously%0Ashown%20to%20plan.%20Our%20findings%20show%20that%20planning%20is%20not%20universal%3A%20unlike%20Haiku%2C%0AGemma-2-2B%20solves%20the%20same%20poem%20generation%20task%20through%20improvisation%2C%20and%20on%0AMBPP%20it%20switches%20between%20planning%20and%20improvisation%20across%20similar%20tasks%20and%0Aeven%20successive%20token%20predictions.%20We%20further%20show%20that%20instruction%20tuning%0Arefines%20existing%20planning%20behaviors%20in%20the%20base%20model%20rather%20than%20creating%20them%0Afrom%20scratch.%20Together%2C%20these%20studies%20provide%20a%20reproducible%20and%20scalable%0Afoundation%20for%20mechanistic%20studies%20of%20planning%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Characterizing%2520Planning%2520in%2520Language%2520Models%26entry.906535625%3DJatin%2520Nainani%2520and%2520Sankaran%2520Vaidyanathan%2520and%2520Connor%2520Watts%2520and%2520Andre%2520N.%2520Assis%2520and%2520Alice%2520Rigg%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%250Aacross%2520a%2520wide%2520range%2520of%2520multi-step%2520reasoning%2520tasks.%2520Recent%2520work%2520suggests%2520that%250ALLMs%2520may%2520perform%2520planning%2520-%2520selecting%2520a%2520future%2520target%2520token%2520in%2520advance%2520and%250Agenerating%2520intermediate%2520tokens%2520that%2520lead%2520towards%2520it%2520-%2520rather%2520than%2520merely%250Aimprovising%2520one%2520token%2520at%2520a%2520time.%2520However%252C%2520existing%2520studies%2520assume%2520fixed%250Aplanning%2520horizons%2520and%2520often%2520focus%2520on%2520single%2520prompts%2520or%2520narrow%2520domains.%2520To%250Adistinguish%2520planning%2520from%2520improvisation%2520across%2520models%2520and%2520tasks%252C%2520we%2520present%250Aformal%2520and%2520causally%2520grounded%2520criteria%2520for%2520detecting%2520planning%2520and%2520operationalize%250Athem%2520as%2520a%2520semi-automated%2520annotation%2520pipeline.%2520We%2520apply%2520this%2520pipeline%2520to%2520both%250Abase%2520and%2520instruction-tuned%2520Gemma-2-2B%2520models%2520on%2520the%2520MBPP%2520code%2520generation%250Abenchmark%2520and%2520a%2520poem%2520generation%2520task%2520where%2520Claude%25203.5%2520Haiku%2520was%2520previously%250Ashown%2520to%2520plan.%2520Our%2520findings%2520show%2520that%2520planning%2520is%2520not%2520universal%253A%2520unlike%2520Haiku%252C%250AGemma-2-2B%2520solves%2520the%2520same%2520poem%2520generation%2520task%2520through%2520improvisation%252C%2520and%2520on%250AMBPP%2520it%2520switches%2520between%2520planning%2520and%2520improvisation%2520across%2520similar%2520tasks%2520and%250Aeven%2520successive%2520token%2520predictions.%2520We%2520further%2520show%2520that%2520instruction%2520tuning%250Arefines%2520existing%2520planning%2520behaviors%2520in%2520the%2520base%2520model%2520rather%2520than%2520creating%2520them%250Afrom%2520scratch.%2520Together%252C%2520these%2520studies%2520provide%2520a%2520reproducible%2520and%2520scalable%250Afoundation%2520for%2520mechanistic%2520studies%2520of%2520planning%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Characterizing%20Planning%20in%20Language%20Models&entry.906535625=Jatin%20Nainani%20and%20Sankaran%20Vaidyanathan%20and%20Connor%20Watts%20and%20Andre%20N.%20Assis%20and%20Alice%20Rigg&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%0Aacross%20a%20wide%20range%20of%20multi-step%20reasoning%20tasks.%20Recent%20work%20suggests%20that%0ALLMs%20may%20perform%20planning%20-%20selecting%20a%20future%20target%20token%20in%20advance%20and%0Agenerating%20intermediate%20tokens%20that%20lead%20towards%20it%20-%20rather%20than%20merely%0Aimprovising%20one%20token%20at%20a%20time.%20However%2C%20existing%20studies%20assume%20fixed%0Aplanning%20horizons%20and%20often%20focus%20on%20single%20prompts%20or%20narrow%20domains.%20To%0Adistinguish%20planning%20from%20improvisation%20across%20models%20and%20tasks%2C%20we%20present%0Aformal%20and%20causally%20grounded%20criteria%20for%20detecting%20planning%20and%20operationalize%0Athem%20as%20a%20semi-automated%20annotation%20pipeline.%20We%20apply%20this%20pipeline%20to%20both%0Abase%20and%20instruction-tuned%20Gemma-2-2B%20models%20on%20the%20MBPP%20code%20generation%0Abenchmark%20and%20a%20poem%20generation%20task%20where%20Claude%203.5%20Haiku%20was%20previously%0Ashown%20to%20plan.%20Our%20findings%20show%20that%20planning%20is%20not%20universal%3A%20unlike%20Haiku%2C%0AGemma-2-2B%20solves%20the%20same%20poem%20generation%20task%20through%20improvisation%2C%20and%20on%0AMBPP%20it%20switches%20between%20planning%20and%20improvisation%20across%20similar%20tasks%20and%0Aeven%20successive%20token%20predictions.%20We%20further%20show%20that%20instruction%20tuning%0Arefines%20existing%20planning%20behaviors%20in%20the%20base%20model%20rather%20than%20creating%20them%0Afrom%20scratch.%20Together%2C%20these%20studies%20provide%20a%20reproducible%20and%20scalable%0Afoundation%20for%20mechanistic%20studies%20of%20planning%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18098v1&entry.124074799=Read"},
{"title": "Evasive Active Hypothesis Testing with Deep Neuroevolution: The Single-\n  and Multi-Agent Cases", "author": "George Stamatelis and Angelos-Nikolaos Kanatas and Ioannis Asprogerakas and George C. Alexandropoulos", "abstract": "  Active hypothesis testing is a thoroughly studied problem that finds numerous\napplications in wireless communications and sensor networks. In this paper, we\nfocus on one centralized and one decentralized problem of active hypothesis\ntesting in the presence of an eavesdropper. For the centralized problem\nincluding a single legitimate agent, we present a new framework based on deep\nNeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel\nNE-based method for solving collaborative multi-agent tasks, which,\ninterestingly, maintains all computational benefits of our single-agent\nNE-based scheme. To further reduce the computational complexity of the latter\nscheme, a novel multi-agent joint NE and pruning framework is also designed.\nThe superiority of the proposed NE-based evasive active hypothesis testing\nschemes over conventional active hypothesis testing policies, as well as\nlearning-based methods, is validated through extensive numerical investigations\nin an example use case of anomaly detection over wireless sensor networks. It\nis demonstrated that the proposed joint optimization and pruning framework\nachieves nearly identical performance with its unpruned counterpart, while\nremoving a very large percentage of redundant deep neural network weights.\n", "link": "http://arxiv.org/abs/2403.10112v2", "date": "2025-08-25", "relevancy": 2.051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evasive%20Active%20Hypothesis%20Testing%20with%20Deep%20Neuroevolution%3A%20The%20Single-%0A%20%20and%20Multi-Agent%20Cases&body=Title%3A%20Evasive%20Active%20Hypothesis%20Testing%20with%20Deep%20Neuroevolution%3A%20The%20Single-%0A%20%20and%20Multi-Agent%20Cases%0AAuthor%3A%20George%20Stamatelis%20and%20Angelos-Nikolaos%20Kanatas%20and%20Ioannis%20Asprogerakas%20and%20George%20C.%20Alexandropoulos%0AAbstract%3A%20%20%20Active%20hypothesis%20testing%20is%20a%20thoroughly%20studied%20problem%20that%20finds%20numerous%0Aapplications%20in%20wireless%20communications%20and%20sensor%20networks.%20In%20this%20paper%2C%20we%0Afocus%20on%20one%20centralized%20and%20one%20decentralized%20problem%20of%20active%20hypothesis%0Atesting%20in%20the%20presence%20of%20an%20eavesdropper.%20For%20the%20centralized%20problem%0Aincluding%20a%20single%20legitimate%20agent%2C%20we%20present%20a%20new%20framework%20based%20on%20deep%0ANeuroEvolution%20%28NE%29%2C%20whereas%2C%20for%20the%20decentralized%20problem%2C%20we%20develop%20a%20novel%0ANE-based%20method%20for%20solving%20collaborative%20multi-agent%20tasks%2C%20which%2C%0Ainterestingly%2C%20maintains%20all%20computational%20benefits%20of%20our%20single-agent%0ANE-based%20scheme.%20To%20further%20reduce%20the%20computational%20complexity%20of%20the%20latter%0Ascheme%2C%20a%20novel%20multi-agent%20joint%20NE%20and%20pruning%20framework%20is%20also%20designed.%0AThe%20superiority%20of%20the%20proposed%20NE-based%20evasive%20active%20hypothesis%20testing%0Aschemes%20over%20conventional%20active%20hypothesis%20testing%20policies%2C%20as%20well%20as%0Alearning-based%20methods%2C%20is%20validated%20through%20extensive%20numerical%20investigations%0Ain%20an%20example%20use%20case%20of%20anomaly%20detection%20over%20wireless%20sensor%20networks.%20It%0Ais%20demonstrated%20that%20the%20proposed%20joint%20optimization%20and%20pruning%20framework%0Aachieves%20nearly%20identical%20performance%20with%20its%20unpruned%20counterpart%2C%20while%0Aremoving%20a%20very%20large%20percentage%20of%20redundant%20deep%20neural%20network%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvasive%2520Active%2520Hypothesis%2520Testing%2520with%2520Deep%2520Neuroevolution%253A%2520The%2520Single-%250A%2520%2520and%2520Multi-Agent%2520Cases%26entry.906535625%3DGeorge%2520Stamatelis%2520and%2520Angelos-Nikolaos%2520Kanatas%2520and%2520Ioannis%2520Asprogerakas%2520and%2520George%2520C.%2520Alexandropoulos%26entry.1292438233%3D%2520%2520Active%2520hypothesis%2520testing%2520is%2520a%2520thoroughly%2520studied%2520problem%2520that%2520finds%2520numerous%250Aapplications%2520in%2520wireless%2520communications%2520and%2520sensor%2520networks.%2520In%2520this%2520paper%252C%2520we%250Afocus%2520on%2520one%2520centralized%2520and%2520one%2520decentralized%2520problem%2520of%2520active%2520hypothesis%250Atesting%2520in%2520the%2520presence%2520of%2520an%2520eavesdropper.%2520For%2520the%2520centralized%2520problem%250Aincluding%2520a%2520single%2520legitimate%2520agent%252C%2520we%2520present%2520a%2520new%2520framework%2520based%2520on%2520deep%250ANeuroEvolution%2520%2528NE%2529%252C%2520whereas%252C%2520for%2520the%2520decentralized%2520problem%252C%2520we%2520develop%2520a%2520novel%250ANE-based%2520method%2520for%2520solving%2520collaborative%2520multi-agent%2520tasks%252C%2520which%252C%250Ainterestingly%252C%2520maintains%2520all%2520computational%2520benefits%2520of%2520our%2520single-agent%250ANE-based%2520scheme.%2520To%2520further%2520reduce%2520the%2520computational%2520complexity%2520of%2520the%2520latter%250Ascheme%252C%2520a%2520novel%2520multi-agent%2520joint%2520NE%2520and%2520pruning%2520framework%2520is%2520also%2520designed.%250AThe%2520superiority%2520of%2520the%2520proposed%2520NE-based%2520evasive%2520active%2520hypothesis%2520testing%250Aschemes%2520over%2520conventional%2520active%2520hypothesis%2520testing%2520policies%252C%2520as%2520well%2520as%250Alearning-based%2520methods%252C%2520is%2520validated%2520through%2520extensive%2520numerical%2520investigations%250Ain%2520an%2520example%2520use%2520case%2520of%2520anomaly%2520detection%2520over%2520wireless%2520sensor%2520networks.%2520It%250Ais%2520demonstrated%2520that%2520the%2520proposed%2520joint%2520optimization%2520and%2520pruning%2520framework%250Aachieves%2520nearly%2520identical%2520performance%2520with%2520its%2520unpruned%2520counterpart%252C%2520while%250Aremoving%2520a%2520very%2520large%2520percentage%2520of%2520redundant%2520deep%2520neural%2520network%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evasive%20Active%20Hypothesis%20Testing%20with%20Deep%20Neuroevolution%3A%20The%20Single-%0A%20%20and%20Multi-Agent%20Cases&entry.906535625=George%20Stamatelis%20and%20Angelos-Nikolaos%20Kanatas%20and%20Ioannis%20Asprogerakas%20and%20George%20C.%20Alexandropoulos&entry.1292438233=%20%20Active%20hypothesis%20testing%20is%20a%20thoroughly%20studied%20problem%20that%20finds%20numerous%0Aapplications%20in%20wireless%20communications%20and%20sensor%20networks.%20In%20this%20paper%2C%20we%0Afocus%20on%20one%20centralized%20and%20one%20decentralized%20problem%20of%20active%20hypothesis%0Atesting%20in%20the%20presence%20of%20an%20eavesdropper.%20For%20the%20centralized%20problem%0Aincluding%20a%20single%20legitimate%20agent%2C%20we%20present%20a%20new%20framework%20based%20on%20deep%0ANeuroEvolution%20%28NE%29%2C%20whereas%2C%20for%20the%20decentralized%20problem%2C%20we%20develop%20a%20novel%0ANE-based%20method%20for%20solving%20collaborative%20multi-agent%20tasks%2C%20which%2C%0Ainterestingly%2C%20maintains%20all%20computational%20benefits%20of%20our%20single-agent%0ANE-based%20scheme.%20To%20further%20reduce%20the%20computational%20complexity%20of%20the%20latter%0Ascheme%2C%20a%20novel%20multi-agent%20joint%20NE%20and%20pruning%20framework%20is%20also%20designed.%0AThe%20superiority%20of%20the%20proposed%20NE-based%20evasive%20active%20hypothesis%20testing%0Aschemes%20over%20conventional%20active%20hypothesis%20testing%20policies%2C%20as%20well%20as%0Alearning-based%20methods%2C%20is%20validated%20through%20extensive%20numerical%20investigations%0Ain%20an%20example%20use%20case%20of%20anomaly%20detection%20over%20wireless%20sensor%20networks.%20It%0Ais%20demonstrated%20that%20the%20proposed%20joint%20optimization%20and%20pruning%20framework%0Aachieves%20nearly%20identical%20performance%20with%20its%20unpruned%20counterpart%2C%20while%0Aremoving%20a%20very%20large%20percentage%20of%20redundant%20deep%20neural%20network%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10112v2&entry.124074799=Read"},
{"title": "WetCat: Enabling Automated Skill Assessment in Wet-Lab Cataract Surgery\n  Videos", "author": "Negin Ghamsarian and Raphael Sznitman and Klaus Schoeffmann and Jens Kowal", "abstract": "  To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files.\n", "link": "http://arxiv.org/abs/2506.08896v2", "date": "2025-08-25", "relevancy": 2.0464, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5144}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos&body=Title%3A%20WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos%0AAuthor%3A%20Negin%20Ghamsarian%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann%20and%20Jens%20Kowal%0AAbstract%3A%20%20%20To%20meet%20the%20growing%20demand%20for%20systematic%20surgical%20training%2C%20wetlab%0Aenvironments%20have%20become%20indispensable%20platforms%20for%20hands-on%20practice%20in%0Aophthalmology.%20Yet%2C%20traditional%20wetlab%20training%20depends%20heavily%20on%20manual%0Aperformance%20evaluations%2C%20which%20are%20labor-intensive%2C%20time-consuming%2C%20and%20often%0Asubject%20to%20variability.%20Recent%20advances%20in%20computer%20vision%20offer%20promising%0Aavenues%20for%20automated%20skill%20assessment%2C%20enhancing%20both%20the%20efficiency%20and%0Aobjectivity%20of%20surgical%20education.%20Despite%20notable%20progress%20in%20ophthalmic%0Asurgical%20datasets%2C%20existing%20resources%20predominantly%20focus%20on%20real%20surgeries%20or%0Aisolated%20tasks%2C%20falling%20short%20of%20supporting%20comprehensive%20skill%20evaluation%20in%0Acontrolled%20wetlab%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20WetCat%2C%0Athe%20first%20dataset%20of%20wetlab%20cataract%20surgery%20videos%20specifically%20curated%20for%0Aautomated%20skill%20assessment.%20WetCat%20comprises%20high-resolution%20recordings%20of%0Asurgeries%20performed%20by%20trainees%20on%20artificial%20eyes%2C%20featuring%20comprehensive%0Aphase%20annotations%20and%20semantic%20segmentations%20of%20key%20anatomical%20structures.%0AThese%20annotations%20are%20meticulously%20designed%20to%20facilitate%20skill%20assessment%0Aduring%20the%20critical%20capsulorhexis%20and%20phacoemulsification%20phases%2C%20adhering%20to%0Astandardized%20surgical%20skill%20assessment%20frameworks.%20By%20focusing%20on%20these%0Aessential%20phases%2C%20WetCat%20enables%20the%20development%20of%20interpretable%2C%20AI-driven%0Aevaluation%20tools%20aligned%20with%20established%20clinical%20metrics.%20This%20dataset%20lays%20a%0Astrong%20foundation%20for%20advancing%20objective%2C%20scalable%20surgical%20education%20and%20sets%0Aa%20new%20benchmark%20for%20automated%20workflow%20analysis%20and%20skill%20assessment%20in%0Aophthalmology%20training.%20The%20dataset%20and%20annotations%20are%20publicly%20available%20in%0ASynapse%20https%3A//www.synapse.org/Synapse%3Asyn66401174/files.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWetCat%253A%2520Enabling%2520Automated%2520Skill%2520Assessment%2520in%2520Wet-Lab%2520Cataract%2520Surgery%250A%2520%2520Videos%26entry.906535625%3DNegin%2520Ghamsarian%2520and%2520Raphael%2520Sznitman%2520and%2520Klaus%2520Schoeffmann%2520and%2520Jens%2520Kowal%26entry.1292438233%3D%2520%2520To%2520meet%2520the%2520growing%2520demand%2520for%2520systematic%2520surgical%2520training%252C%2520wetlab%250Aenvironments%2520have%2520become%2520indispensable%2520platforms%2520for%2520hands-on%2520practice%2520in%250Aophthalmology.%2520Yet%252C%2520traditional%2520wetlab%2520training%2520depends%2520heavily%2520on%2520manual%250Aperformance%2520evaluations%252C%2520which%2520are%2520labor-intensive%252C%2520time-consuming%252C%2520and%2520often%250Asubject%2520to%2520variability.%2520Recent%2520advances%2520in%2520computer%2520vision%2520offer%2520promising%250Aavenues%2520for%2520automated%2520skill%2520assessment%252C%2520enhancing%2520both%2520the%2520efficiency%2520and%250Aobjectivity%2520of%2520surgical%2520education.%2520Despite%2520notable%2520progress%2520in%2520ophthalmic%250Asurgical%2520datasets%252C%2520existing%2520resources%2520predominantly%2520focus%2520on%2520real%2520surgeries%2520or%250Aisolated%2520tasks%252C%2520falling%2520short%2520of%2520supporting%2520comprehensive%2520skill%2520evaluation%2520in%250Acontrolled%2520wetlab%2520settings.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520WetCat%252C%250Athe%2520first%2520dataset%2520of%2520wetlab%2520cataract%2520surgery%2520videos%2520specifically%2520curated%2520for%250Aautomated%2520skill%2520assessment.%2520WetCat%2520comprises%2520high-resolution%2520recordings%2520of%250Asurgeries%2520performed%2520by%2520trainees%2520on%2520artificial%2520eyes%252C%2520featuring%2520comprehensive%250Aphase%2520annotations%2520and%2520semantic%2520segmentations%2520of%2520key%2520anatomical%2520structures.%250AThese%2520annotations%2520are%2520meticulously%2520designed%2520to%2520facilitate%2520skill%2520assessment%250Aduring%2520the%2520critical%2520capsulorhexis%2520and%2520phacoemulsification%2520phases%252C%2520adhering%2520to%250Astandardized%2520surgical%2520skill%2520assessment%2520frameworks.%2520By%2520focusing%2520on%2520these%250Aessential%2520phases%252C%2520WetCat%2520enables%2520the%2520development%2520of%2520interpretable%252C%2520AI-driven%250Aevaluation%2520tools%2520aligned%2520with%2520established%2520clinical%2520metrics.%2520This%2520dataset%2520lays%2520a%250Astrong%2520foundation%2520for%2520advancing%2520objective%252C%2520scalable%2520surgical%2520education%2520and%2520sets%250Aa%2520new%2520benchmark%2520for%2520automated%2520workflow%2520analysis%2520and%2520skill%2520assessment%2520in%250Aophthalmology%2520training.%2520The%2520dataset%2520and%2520annotations%2520are%2520publicly%2520available%2520in%250ASynapse%2520https%253A//www.synapse.org/Synapse%253Asyn66401174/files.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WetCat%3A%20Enabling%20Automated%20Skill%20Assessment%20in%20Wet-Lab%20Cataract%20Surgery%0A%20%20Videos&entry.906535625=Negin%20Ghamsarian%20and%20Raphael%20Sznitman%20and%20Klaus%20Schoeffmann%20and%20Jens%20Kowal&entry.1292438233=%20%20To%20meet%20the%20growing%20demand%20for%20systematic%20surgical%20training%2C%20wetlab%0Aenvironments%20have%20become%20indispensable%20platforms%20for%20hands-on%20practice%20in%0Aophthalmology.%20Yet%2C%20traditional%20wetlab%20training%20depends%20heavily%20on%20manual%0Aperformance%20evaluations%2C%20which%20are%20labor-intensive%2C%20time-consuming%2C%20and%20often%0Asubject%20to%20variability.%20Recent%20advances%20in%20computer%20vision%20offer%20promising%0Aavenues%20for%20automated%20skill%20assessment%2C%20enhancing%20both%20the%20efficiency%20and%0Aobjectivity%20of%20surgical%20education.%20Despite%20notable%20progress%20in%20ophthalmic%0Asurgical%20datasets%2C%20existing%20resources%20predominantly%20focus%20on%20real%20surgeries%20or%0Aisolated%20tasks%2C%20falling%20short%20of%20supporting%20comprehensive%20skill%20evaluation%20in%0Acontrolled%20wetlab%20settings.%20To%20address%20these%20limitations%2C%20we%20introduce%20WetCat%2C%0Athe%20first%20dataset%20of%20wetlab%20cataract%20surgery%20videos%20specifically%20curated%20for%0Aautomated%20skill%20assessment.%20WetCat%20comprises%20high-resolution%20recordings%20of%0Asurgeries%20performed%20by%20trainees%20on%20artificial%20eyes%2C%20featuring%20comprehensive%0Aphase%20annotations%20and%20semantic%20segmentations%20of%20key%20anatomical%20structures.%0AThese%20annotations%20are%20meticulously%20designed%20to%20facilitate%20skill%20assessment%0Aduring%20the%20critical%20capsulorhexis%20and%20phacoemulsification%20phases%2C%20adhering%20to%0Astandardized%20surgical%20skill%20assessment%20frameworks.%20By%20focusing%20on%20these%0Aessential%20phases%2C%20WetCat%20enables%20the%20development%20of%20interpretable%2C%20AI-driven%0Aevaluation%20tools%20aligned%20with%20established%20clinical%20metrics.%20This%20dataset%20lays%20a%0Astrong%20foundation%20for%20advancing%20objective%2C%20scalable%20surgical%20education%20and%20sets%0Aa%20new%20benchmark%20for%20automated%20workflow%20analysis%20and%20skill%20assessment%20in%0Aophthalmology%20training.%20The%20dataset%20and%20annotations%20are%20publicly%20available%20in%0ASynapse%20https%3A//www.synapse.org/Synapse%3Asyn66401174/files.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08896v2&entry.124074799=Read"},
{"title": "OwkinZero: Accelerating Biological Discovery with AI", "author": "Nathan Bigaud and Vincent Cabeli and Meltem G\u00fcrel and Arthur Pignet and John Klein and Gilles Wainrib and Eric Durand", "abstract": "  While large language models (LLMs) are rapidly advancing scientific research,\nthey continue to struggle with core biological reasoning tasks essential for\ntranslational and biomedical discovery. To address this limitation, we created\nand curated eight comprehensive benchmark datasets comprising over 300,000\nverifiable question-and-answer pairs, each targeting critical challenges in\ndrug discovery including target druggability, modality suitability, and drug\nperturbation effects. Using this resource, we developed the OwkinZero models by\npost-training open-source LLMs through a Reinforcement Learning from Verifiable\nRewards strategy. Our results demonstrate that specialized 8-32B OwkinZero\nmodels substantially outperform larger, state-of-the-art commercial LLMs on\nthese biological benchmarks. Remarkably, we uncover evidence of a key aspect of\ngeneralization: specialist models trained on a single task consistently\noutperform their base models on previously unseen tasks. This generalization\neffect is further amplified in our comprehensive OwkinZero models, which were\ntrained on a mixture of datasets and achieve even broader cross-task\nimprovements. This study represents a significant step toward addressing the\nbiological reasoning blind spot in current LLMs, demonstrating that targeted\nreinforcement learning on carefully curated data can unlock generalizable\nperformance in specialized models, thereby accelerating AI-driven biological\ndiscovery.\n", "link": "http://arxiv.org/abs/2508.16315v2", "date": "2025-08-25", "relevancy": 2.0452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI&body=Title%3A%20OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI%0AAuthor%3A%20Nathan%20Bigaud%20and%20Vincent%20Cabeli%20and%20Meltem%20G%C3%BCrel%20and%20Arthur%20Pignet%20and%20John%20Klein%20and%20Gilles%20Wainrib%20and%20Eric%20Durand%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20are%20rapidly%20advancing%20scientific%20research%2C%0Athey%20continue%20to%20struggle%20with%20core%20biological%20reasoning%20tasks%20essential%20for%0Atranslational%20and%20biomedical%20discovery.%20To%20address%20this%20limitation%2C%20we%20created%0Aand%20curated%20eight%20comprehensive%20benchmark%20datasets%20comprising%20over%20300%2C000%0Averifiable%20question-and-answer%20pairs%2C%20each%20targeting%20critical%20challenges%20in%0Adrug%20discovery%20including%20target%20druggability%2C%20modality%20suitability%2C%20and%20drug%0Aperturbation%20effects.%20Using%20this%20resource%2C%20we%20developed%20the%20OwkinZero%20models%20by%0Apost-training%20open-source%20LLMs%20through%20a%20Reinforcement%20Learning%20from%20Verifiable%0ARewards%20strategy.%20Our%20results%20demonstrate%20that%20specialized%208-32B%20OwkinZero%0Amodels%20substantially%20outperform%20larger%2C%20state-of-the-art%20commercial%20LLMs%20on%0Athese%20biological%20benchmarks.%20Remarkably%2C%20we%20uncover%20evidence%20of%20a%20key%20aspect%20of%0Ageneralization%3A%20specialist%20models%20trained%20on%20a%20single%20task%20consistently%0Aoutperform%20their%20base%20models%20on%20previously%20unseen%20tasks.%20This%20generalization%0Aeffect%20is%20further%20amplified%20in%20our%20comprehensive%20OwkinZero%20models%2C%20which%20were%0Atrained%20on%20a%20mixture%20of%20datasets%20and%20achieve%20even%20broader%20cross-task%0Aimprovements.%20This%20study%20represents%20a%20significant%20step%20toward%20addressing%20the%0Abiological%20reasoning%20blind%20spot%20in%20current%20LLMs%2C%20demonstrating%20that%20targeted%0Areinforcement%20learning%20on%20carefully%20curated%20data%20can%20unlock%20generalizable%0Aperformance%20in%20specialized%20models%2C%20thereby%20accelerating%20AI-driven%20biological%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOwkinZero%253A%2520Accelerating%2520Biological%2520Discovery%2520with%2520AI%26entry.906535625%3DNathan%2520Bigaud%2520and%2520Vincent%2520Cabeli%2520and%2520Meltem%2520G%25C3%25BCrel%2520and%2520Arthur%2520Pignet%2520and%2520John%2520Klein%2520and%2520Gilles%2520Wainrib%2520and%2520Eric%2520Durand%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520rapidly%2520advancing%2520scientific%2520research%252C%250Athey%2520continue%2520to%2520struggle%2520with%2520core%2520biological%2520reasoning%2520tasks%2520essential%2520for%250Atranslational%2520and%2520biomedical%2520discovery.%2520To%2520address%2520this%2520limitation%252C%2520we%2520created%250Aand%2520curated%2520eight%2520comprehensive%2520benchmark%2520datasets%2520comprising%2520over%2520300%252C000%250Averifiable%2520question-and-answer%2520pairs%252C%2520each%2520targeting%2520critical%2520challenges%2520in%250Adrug%2520discovery%2520including%2520target%2520druggability%252C%2520modality%2520suitability%252C%2520and%2520drug%250Aperturbation%2520effects.%2520Using%2520this%2520resource%252C%2520we%2520developed%2520the%2520OwkinZero%2520models%2520by%250Apost-training%2520open-source%2520LLMs%2520through%2520a%2520Reinforcement%2520Learning%2520from%2520Verifiable%250ARewards%2520strategy.%2520Our%2520results%2520demonstrate%2520that%2520specialized%25208-32B%2520OwkinZero%250Amodels%2520substantially%2520outperform%2520larger%252C%2520state-of-the-art%2520commercial%2520LLMs%2520on%250Athese%2520biological%2520benchmarks.%2520Remarkably%252C%2520we%2520uncover%2520evidence%2520of%2520a%2520key%2520aspect%2520of%250Ageneralization%253A%2520specialist%2520models%2520trained%2520on%2520a%2520single%2520task%2520consistently%250Aoutperform%2520their%2520base%2520models%2520on%2520previously%2520unseen%2520tasks.%2520This%2520generalization%250Aeffect%2520is%2520further%2520amplified%2520in%2520our%2520comprehensive%2520OwkinZero%2520models%252C%2520which%2520were%250Atrained%2520on%2520a%2520mixture%2520of%2520datasets%2520and%2520achieve%2520even%2520broader%2520cross-task%250Aimprovements.%2520This%2520study%2520represents%2520a%2520significant%2520step%2520toward%2520addressing%2520the%250Abiological%2520reasoning%2520blind%2520spot%2520in%2520current%2520LLMs%252C%2520demonstrating%2520that%2520targeted%250Areinforcement%2520learning%2520on%2520carefully%2520curated%2520data%2520can%2520unlock%2520generalizable%250Aperformance%2520in%2520specialized%2520models%252C%2520thereby%2520accelerating%2520AI-driven%2520biological%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OwkinZero%3A%20Accelerating%20Biological%20Discovery%20with%20AI&entry.906535625=Nathan%20Bigaud%20and%20Vincent%20Cabeli%20and%20Meltem%20G%C3%BCrel%20and%20Arthur%20Pignet%20and%20John%20Klein%20and%20Gilles%20Wainrib%20and%20Eric%20Durand&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20are%20rapidly%20advancing%20scientific%20research%2C%0Athey%20continue%20to%20struggle%20with%20core%20biological%20reasoning%20tasks%20essential%20for%0Atranslational%20and%20biomedical%20discovery.%20To%20address%20this%20limitation%2C%20we%20created%0Aand%20curated%20eight%20comprehensive%20benchmark%20datasets%20comprising%20over%20300%2C000%0Averifiable%20question-and-answer%20pairs%2C%20each%20targeting%20critical%20challenges%20in%0Adrug%20discovery%20including%20target%20druggability%2C%20modality%20suitability%2C%20and%20drug%0Aperturbation%20effects.%20Using%20this%20resource%2C%20we%20developed%20the%20OwkinZero%20models%20by%0Apost-training%20open-source%20LLMs%20through%20a%20Reinforcement%20Learning%20from%20Verifiable%0ARewards%20strategy.%20Our%20results%20demonstrate%20that%20specialized%208-32B%20OwkinZero%0Amodels%20substantially%20outperform%20larger%2C%20state-of-the-art%20commercial%20LLMs%20on%0Athese%20biological%20benchmarks.%20Remarkably%2C%20we%20uncover%20evidence%20of%20a%20key%20aspect%20of%0Ageneralization%3A%20specialist%20models%20trained%20on%20a%20single%20task%20consistently%0Aoutperform%20their%20base%20models%20on%20previously%20unseen%20tasks.%20This%20generalization%0Aeffect%20is%20further%20amplified%20in%20our%20comprehensive%20OwkinZero%20models%2C%20which%20were%0Atrained%20on%20a%20mixture%20of%20datasets%20and%20achieve%20even%20broader%20cross-task%0Aimprovements.%20This%20study%20represents%20a%20significant%20step%20toward%20addressing%20the%0Abiological%20reasoning%20blind%20spot%20in%20current%20LLMs%2C%20demonstrating%20that%20targeted%0Areinforcement%20learning%20on%20carefully%20curated%20data%20can%20unlock%20generalizable%0Aperformance%20in%20specialized%20models%2C%20thereby%20accelerating%20AI-driven%20biological%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16315v2&entry.124074799=Read"},
{"title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models", "author": "Qingyue Wang and Yanhe Fu and Yanan Cao and Shuai Wang and Zhiliang Tian and Liang Ding", "abstract": "  Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts are released.\n", "link": "http://arxiv.org/abs/2308.15022v4", "date": "2025-08-25", "relevancy": 2.0356, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20Memory%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20Memory%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Qingyue%20Wang%20and%20Yanhe%20Fu%20and%20Yanan%20Cao%20and%20Shuai%20Wang%20and%20Zhiliang%20Tian%20and%20Liang%20Ding%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT-4%2C%20stand%20out%20remarkable%0Aconversational%20abilities%2C%20enabling%20them%20to%20engage%20in%20dynamic%20and%20contextually%0Arelevant%20dialogues%20across%20a%20wide%20range%20of%20topics.%20However%2C%20given%20a%20long%0Aconversation%2C%20these%20chatbots%20fail%20to%20recall%20past%20information%20and%20tend%20to%0Agenerate%20inconsistent%20responses.%20To%20address%20this%2C%20we%20propose%20to%20recursively%0Agenerate%20summaries/%20memory%20using%20large%20language%20models%20%28LLMs%29%20to%20enhance%0Along-term%20memory%20ability.%20Specifically%2C%20our%20method%20first%20stimulates%20LLMs%20to%0Amemorize%20small%20dialogue%20contexts%20and%20then%20recursively%20produce%20new%20memory%20using%0Aprevious%20memory%20and%20following%20contexts.%20Finally%2C%20the%20chatbot%20can%20easily%0Agenerate%20a%20highly%20consistent%20response%20with%20the%20help%20of%20the%20latest%20memory.%20We%0Aevaluate%20our%20method%20on%20both%20open%20and%20closed%20LLMs%2C%20and%20the%20experiments%20on%20the%0Awidely-used%20public%20dataset%20show%20that%20our%20method%20can%20generate%20more%20consistent%0Aresponses%20in%20a%20long-context%20conversation.%20Also%2C%20we%20show%20that%20our%20strategy%20could%0Anicely%20complement%20both%20long-context%20%28e.g.%2C%208K%20and%2016K%29%20and%20retrieval-enhanced%0ALLMs%2C%20bringing%20further%20long-term%20dialogue%20performance.%20Notably%2C%20our%20method%20is%20a%0Apotential%20solution%20to%20enable%20the%20LLM%20to%20model%20the%20extremely%20long%20context.%20The%0Acode%20and%20scripts%20are%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15022v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursively%2520Summarizing%2520Enables%2520Long-Term%2520Dialogue%2520Memory%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DQingyue%2520Wang%2520and%2520Yanhe%2520Fu%2520and%2520Yanan%2520Cao%2520and%2520Shuai%2520Wang%2520and%2520Zhiliang%2520Tian%2520and%2520Liang%2520Ding%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%2520GPT-4%252C%2520stand%2520out%2520remarkable%250Aconversational%2520abilities%252C%2520enabling%2520them%2520to%2520engage%2520in%2520dynamic%2520and%2520contextually%250Arelevant%2520dialogues%2520across%2520a%2520wide%2520range%2520of%2520topics.%2520However%252C%2520given%2520a%2520long%250Aconversation%252C%2520these%2520chatbots%2520fail%2520to%2520recall%2520past%2520information%2520and%2520tend%2520to%250Agenerate%2520inconsistent%2520responses.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520recursively%250Agenerate%2520summaries/%2520memory%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520enhance%250Along-term%2520memory%2520ability.%2520Specifically%252C%2520our%2520method%2520first%2520stimulates%2520LLMs%2520to%250Amemorize%2520small%2520dialogue%2520contexts%2520and%2520then%2520recursively%2520produce%2520new%2520memory%2520using%250Aprevious%2520memory%2520and%2520following%2520contexts.%2520Finally%252C%2520the%2520chatbot%2520can%2520easily%250Agenerate%2520a%2520highly%2520consistent%2520response%2520with%2520the%2520help%2520of%2520the%2520latest%2520memory.%2520We%250Aevaluate%2520our%2520method%2520on%2520both%2520open%2520and%2520closed%2520LLMs%252C%2520and%2520the%2520experiments%2520on%2520the%250Awidely-used%2520public%2520dataset%2520show%2520that%2520our%2520method%2520can%2520generate%2520more%2520consistent%250Aresponses%2520in%2520a%2520long-context%2520conversation.%2520Also%252C%2520we%2520show%2520that%2520our%2520strategy%2520could%250Anicely%2520complement%2520both%2520long-context%2520%2528e.g.%252C%25208K%2520and%252016K%2529%2520and%2520retrieval-enhanced%250ALLMs%252C%2520bringing%2520further%2520long-term%2520dialogue%2520performance.%2520Notably%252C%2520our%2520method%2520is%2520a%250Apotential%2520solution%2520to%2520enable%2520the%2520LLM%2520to%2520model%2520the%2520extremely%2520long%2520context.%2520The%250Acode%2520and%2520scripts%2520are%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15022v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursively%20Summarizing%20Enables%20Long-Term%20Dialogue%20Memory%20in%20Large%0A%20%20Language%20Models&entry.906535625=Qingyue%20Wang%20and%20Yanhe%20Fu%20and%20Yanan%20Cao%20and%20Shuai%20Wang%20and%20Zhiliang%20Tian%20and%20Liang%20Ding&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT-4%2C%20stand%20out%20remarkable%0Aconversational%20abilities%2C%20enabling%20them%20to%20engage%20in%20dynamic%20and%20contextually%0Arelevant%20dialogues%20across%20a%20wide%20range%20of%20topics.%20However%2C%20given%20a%20long%0Aconversation%2C%20these%20chatbots%20fail%20to%20recall%20past%20information%20and%20tend%20to%0Agenerate%20inconsistent%20responses.%20To%20address%20this%2C%20we%20propose%20to%20recursively%0Agenerate%20summaries/%20memory%20using%20large%20language%20models%20%28LLMs%29%20to%20enhance%0Along-term%20memory%20ability.%20Specifically%2C%20our%20method%20first%20stimulates%20LLMs%20to%0Amemorize%20small%20dialogue%20contexts%20and%20then%20recursively%20produce%20new%20memory%20using%0Aprevious%20memory%20and%20following%20contexts.%20Finally%2C%20the%20chatbot%20can%20easily%0Agenerate%20a%20highly%20consistent%20response%20with%20the%20help%20of%20the%20latest%20memory.%20We%0Aevaluate%20our%20method%20on%20both%20open%20and%20closed%20LLMs%2C%20and%20the%20experiments%20on%20the%0Awidely-used%20public%20dataset%20show%20that%20our%20method%20can%20generate%20more%20consistent%0Aresponses%20in%20a%20long-context%20conversation.%20Also%2C%20we%20show%20that%20our%20strategy%20could%0Anicely%20complement%20both%20long-context%20%28e.g.%2C%208K%20and%2016K%29%20and%20retrieval-enhanced%0ALLMs%2C%20bringing%20further%20long-term%20dialogue%20performance.%20Notably%2C%20our%20method%20is%20a%0Apotential%20solution%20to%20enable%20the%20LLM%20to%20model%20the%20extremely%20long%20context.%20The%0Acode%20and%20scripts%20are%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15022v4&entry.124074799=Read"},
{"title": "Amortized Sampling with Transferable Normalizing Flows", "author": "Charlie B. Tan and Majdi Hassan and Leon Klein and Saifuddin Syed and Dominique Beaini and Michael M. Bronstein and Alexander Tong and Kirill Neklyudov", "abstract": "  Efficient equilibrium sampling of molecular conformations remains a core\nchallenge in computational chemistry and statistical inference. Classical\napproaches such as molecular dynamics or Markov chain Monte Carlo inherently\nlack amortization; the computational cost of sampling must be paid in-full for\neach system of interest. The widespread success of generative models has\ninspired interest into overcoming this limitation through learning sampling\nalgorithms. Despite performing on par with conventional methods when trained on\na single system, learned samplers have so far demonstrated limited ability to\ntransfer across systems. We prove that deep learning enables the design of\nscalable and transferable samplers by introducing Prose, a 280 million\nparameter all-atom transferable normalizing flow trained on a corpus of peptide\nmolecular dynamics trajectories up to 8 residues in length. Prose draws\nzero-shot uncorrelated proposal samples for arbitrary peptide systems,\nachieving the previously intractable transferability across sequence length,\nwhilst retaining the efficient likelihood evaluation of normalizing flows.\nThrough extensive empirical evaluation we demonstrate the efficacy of Prose as\na proposal for a variety of sampling algorithms, finding a simple importance\nsampling-based finetuning procedure to achieve superior performance to\nestablished methods such as sequential Monte Carlo on unseen tetrapeptides. We\nopen-source the Prose codebase, model weights, and training dataset, to further\nstimulate research into amortized sampling methods and finetuning objectives.\n", "link": "http://arxiv.org/abs/2508.18175v1", "date": "2025-08-25", "relevancy": 1.9707, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5312}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4906}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amortized%20Sampling%20with%20Transferable%20Normalizing%20Flows&body=Title%3A%20Amortized%20Sampling%20with%20Transferable%20Normalizing%20Flows%0AAuthor%3A%20Charlie%20B.%20Tan%20and%20Majdi%20Hassan%20and%20Leon%20Klein%20and%20Saifuddin%20Syed%20and%20Dominique%20Beaini%20and%20Michael%20M.%20Bronstein%20and%20Alexander%20Tong%20and%20Kirill%20Neklyudov%0AAbstract%3A%20%20%20Efficient%20equilibrium%20sampling%20of%20molecular%20conformations%20remains%20a%20core%0Achallenge%20in%20computational%20chemistry%20and%20statistical%20inference.%20Classical%0Aapproaches%20such%20as%20molecular%20dynamics%20or%20Markov%20chain%20Monte%20Carlo%20inherently%0Alack%20amortization%3B%20the%20computational%20cost%20of%20sampling%20must%20be%20paid%20in-full%20for%0Aeach%20system%20of%20interest.%20The%20widespread%20success%20of%20generative%20models%20has%0Ainspired%20interest%20into%20overcoming%20this%20limitation%20through%20learning%20sampling%0Aalgorithms.%20Despite%20performing%20on%20par%20with%20conventional%20methods%20when%20trained%20on%0Aa%20single%20system%2C%20learned%20samplers%20have%20so%20far%20demonstrated%20limited%20ability%20to%0Atransfer%20across%20systems.%20We%20prove%20that%20deep%20learning%20enables%20the%20design%20of%0Ascalable%20and%20transferable%20samplers%20by%20introducing%20Prose%2C%20a%20280%20million%0Aparameter%20all-atom%20transferable%20normalizing%20flow%20trained%20on%20a%20corpus%20of%20peptide%0Amolecular%20dynamics%20trajectories%20up%20to%208%20residues%20in%20length.%20Prose%20draws%0Azero-shot%20uncorrelated%20proposal%20samples%20for%20arbitrary%20peptide%20systems%2C%0Aachieving%20the%20previously%20intractable%20transferability%20across%20sequence%20length%2C%0Awhilst%20retaining%20the%20efficient%20likelihood%20evaluation%20of%20normalizing%20flows.%0AThrough%20extensive%20empirical%20evaluation%20we%20demonstrate%20the%20efficacy%20of%20Prose%20as%0Aa%20proposal%20for%20a%20variety%20of%20sampling%20algorithms%2C%20finding%20a%20simple%20importance%0Asampling-based%20finetuning%20procedure%20to%20achieve%20superior%20performance%20to%0Aestablished%20methods%20such%20as%20sequential%20Monte%20Carlo%20on%20unseen%20tetrapeptides.%20We%0Aopen-source%20the%20Prose%20codebase%2C%20model%20weights%2C%20and%20training%20dataset%2C%20to%20further%0Astimulate%20research%20into%20amortized%20sampling%20methods%20and%20finetuning%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmortized%2520Sampling%2520with%2520Transferable%2520Normalizing%2520Flows%26entry.906535625%3DCharlie%2520B.%2520Tan%2520and%2520Majdi%2520Hassan%2520and%2520Leon%2520Klein%2520and%2520Saifuddin%2520Syed%2520and%2520Dominique%2520Beaini%2520and%2520Michael%2520M.%2520Bronstein%2520and%2520Alexander%2520Tong%2520and%2520Kirill%2520Neklyudov%26entry.1292438233%3D%2520%2520Efficient%2520equilibrium%2520sampling%2520of%2520molecular%2520conformations%2520remains%2520a%2520core%250Achallenge%2520in%2520computational%2520chemistry%2520and%2520statistical%2520inference.%2520Classical%250Aapproaches%2520such%2520as%2520molecular%2520dynamics%2520or%2520Markov%2520chain%2520Monte%2520Carlo%2520inherently%250Alack%2520amortization%253B%2520the%2520computational%2520cost%2520of%2520sampling%2520must%2520be%2520paid%2520in-full%2520for%250Aeach%2520system%2520of%2520interest.%2520The%2520widespread%2520success%2520of%2520generative%2520models%2520has%250Ainspired%2520interest%2520into%2520overcoming%2520this%2520limitation%2520through%2520learning%2520sampling%250Aalgorithms.%2520Despite%2520performing%2520on%2520par%2520with%2520conventional%2520methods%2520when%2520trained%2520on%250Aa%2520single%2520system%252C%2520learned%2520samplers%2520have%2520so%2520far%2520demonstrated%2520limited%2520ability%2520to%250Atransfer%2520across%2520systems.%2520We%2520prove%2520that%2520deep%2520learning%2520enables%2520the%2520design%2520of%250Ascalable%2520and%2520transferable%2520samplers%2520by%2520introducing%2520Prose%252C%2520a%2520280%2520million%250Aparameter%2520all-atom%2520transferable%2520normalizing%2520flow%2520trained%2520on%2520a%2520corpus%2520of%2520peptide%250Amolecular%2520dynamics%2520trajectories%2520up%2520to%25208%2520residues%2520in%2520length.%2520Prose%2520draws%250Azero-shot%2520uncorrelated%2520proposal%2520samples%2520for%2520arbitrary%2520peptide%2520systems%252C%250Aachieving%2520the%2520previously%2520intractable%2520transferability%2520across%2520sequence%2520length%252C%250Awhilst%2520retaining%2520the%2520efficient%2520likelihood%2520evaluation%2520of%2520normalizing%2520flows.%250AThrough%2520extensive%2520empirical%2520evaluation%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520Prose%2520as%250Aa%2520proposal%2520for%2520a%2520variety%2520of%2520sampling%2520algorithms%252C%2520finding%2520a%2520simple%2520importance%250Asampling-based%2520finetuning%2520procedure%2520to%2520achieve%2520superior%2520performance%2520to%250Aestablished%2520methods%2520such%2520as%2520sequential%2520Monte%2520Carlo%2520on%2520unseen%2520tetrapeptides.%2520We%250Aopen-source%2520the%2520Prose%2520codebase%252C%2520model%2520weights%252C%2520and%2520training%2520dataset%252C%2520to%2520further%250Astimulate%2520research%2520into%2520amortized%2520sampling%2520methods%2520and%2520finetuning%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amortized%20Sampling%20with%20Transferable%20Normalizing%20Flows&entry.906535625=Charlie%20B.%20Tan%20and%20Majdi%20Hassan%20and%20Leon%20Klein%20and%20Saifuddin%20Syed%20and%20Dominique%20Beaini%20and%20Michael%20M.%20Bronstein%20and%20Alexander%20Tong%20and%20Kirill%20Neklyudov&entry.1292438233=%20%20Efficient%20equilibrium%20sampling%20of%20molecular%20conformations%20remains%20a%20core%0Achallenge%20in%20computational%20chemistry%20and%20statistical%20inference.%20Classical%0Aapproaches%20such%20as%20molecular%20dynamics%20or%20Markov%20chain%20Monte%20Carlo%20inherently%0Alack%20amortization%3B%20the%20computational%20cost%20of%20sampling%20must%20be%20paid%20in-full%20for%0Aeach%20system%20of%20interest.%20The%20widespread%20success%20of%20generative%20models%20has%0Ainspired%20interest%20into%20overcoming%20this%20limitation%20through%20learning%20sampling%0Aalgorithms.%20Despite%20performing%20on%20par%20with%20conventional%20methods%20when%20trained%20on%0Aa%20single%20system%2C%20learned%20samplers%20have%20so%20far%20demonstrated%20limited%20ability%20to%0Atransfer%20across%20systems.%20We%20prove%20that%20deep%20learning%20enables%20the%20design%20of%0Ascalable%20and%20transferable%20samplers%20by%20introducing%20Prose%2C%20a%20280%20million%0Aparameter%20all-atom%20transferable%20normalizing%20flow%20trained%20on%20a%20corpus%20of%20peptide%0Amolecular%20dynamics%20trajectories%20up%20to%208%20residues%20in%20length.%20Prose%20draws%0Azero-shot%20uncorrelated%20proposal%20samples%20for%20arbitrary%20peptide%20systems%2C%0Aachieving%20the%20previously%20intractable%20transferability%20across%20sequence%20length%2C%0Awhilst%20retaining%20the%20efficient%20likelihood%20evaluation%20of%20normalizing%20flows.%0AThrough%20extensive%20empirical%20evaluation%20we%20demonstrate%20the%20efficacy%20of%20Prose%20as%0Aa%20proposal%20for%20a%20variety%20of%20sampling%20algorithms%2C%20finding%20a%20simple%20importance%0Asampling-based%20finetuning%20procedure%20to%20achieve%20superior%20performance%20to%0Aestablished%20methods%20such%20as%20sequential%20Monte%20Carlo%20on%20unseen%20tetrapeptides.%20We%0Aopen-source%20the%20Prose%20codebase%2C%20model%20weights%2C%20and%20training%20dataset%2C%20to%20further%0Astimulate%20research%20into%20amortized%20sampling%20methods%20and%20finetuning%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18175v1&entry.124074799=Read"},
{"title": "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and\n  Opportunities", "author": "Aishik Mandal and Tanmoy Chakraborty and Iryna Gurevych", "abstract": "  Mental health disorders create profound personal and societal burdens, yet\nconventional diagnostics are resource-intensive and limit accessibility.\nAdvances in artificial intelligence, particularly natural language processing\nand multimodal methods, offer promise for detecting and addressing mental\ndisorders, but raise critical privacy risks. This paper examines these\nchallenges and proposes solutions, including anonymization, synthetic data, and\nprivacy-preserving training, while outlining frameworks for privacy-utility\ntrade-offs, aiming to advance reliable, privacy-aware AI tools that support\nclinical decision-making and improve mental health outcomes.\n", "link": "http://arxiv.org/abs/2502.00451v3", "date": "2025-08-25", "relevancy": 0.9045, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4498}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Privacy-aware%20Mental%20Health%20AI%20Models%3A%20Advances%2C%20Challenges%2C%20and%0A%20%20Opportunities&body=Title%3A%20Towards%20Privacy-aware%20Mental%20Health%20AI%20Models%3A%20Advances%2C%20Challenges%2C%20and%0A%20%20Opportunities%0AAuthor%3A%20Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych%0AAbstract%3A%20%20%20Mental%20health%20disorders%20create%20profound%20personal%20and%20societal%20burdens%2C%20yet%0Aconventional%20diagnostics%20are%20resource-intensive%20and%20limit%20accessibility.%0AAdvances%20in%20artificial%20intelligence%2C%20particularly%20natural%20language%20processing%0Aand%20multimodal%20methods%2C%20offer%20promise%20for%20detecting%20and%20addressing%20mental%0Adisorders%2C%20but%20raise%20critical%20privacy%20risks.%20This%20paper%20examines%20these%0Achallenges%20and%20proposes%20solutions%2C%20including%20anonymization%2C%20synthetic%20data%2C%20and%0Aprivacy-preserving%20training%2C%20while%20outlining%20frameworks%20for%20privacy-utility%0Atrade-offs%2C%20aiming%20to%20advance%20reliable%2C%20privacy-aware%20AI%20tools%20that%20support%0Aclinical%20decision-making%20and%20improve%20mental%20health%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00451v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Privacy-aware%2520Mental%2520Health%2520AI%2520Models%253A%2520Advances%252C%2520Challenges%252C%2520and%250A%2520%2520Opportunities%26entry.906535625%3DAishik%2520Mandal%2520and%2520Tanmoy%2520Chakraborty%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3D%2520%2520Mental%2520health%2520disorders%2520create%2520profound%2520personal%2520and%2520societal%2520burdens%252C%2520yet%250Aconventional%2520diagnostics%2520are%2520resource-intensive%2520and%2520limit%2520accessibility.%250AAdvances%2520in%2520artificial%2520intelligence%252C%2520particularly%2520natural%2520language%2520processing%250Aand%2520multimodal%2520methods%252C%2520offer%2520promise%2520for%2520detecting%2520and%2520addressing%2520mental%250Adisorders%252C%2520but%2520raise%2520critical%2520privacy%2520risks.%2520This%2520paper%2520examines%2520these%250Achallenges%2520and%2520proposes%2520solutions%252C%2520including%2520anonymization%252C%2520synthetic%2520data%252C%2520and%250Aprivacy-preserving%2520training%252C%2520while%2520outlining%2520frameworks%2520for%2520privacy-utility%250Atrade-offs%252C%2520aiming%2520to%2520advance%2520reliable%252C%2520privacy-aware%2520AI%2520tools%2520that%2520support%250Aclinical%2520decision-making%2520and%2520improve%2520mental%2520health%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00451v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Privacy-aware%20Mental%20Health%20AI%20Models%3A%20Advances%2C%20Challenges%2C%20and%0A%20%20Opportunities&entry.906535625=Aishik%20Mandal%20and%20Tanmoy%20Chakraborty%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Mental%20health%20disorders%20create%20profound%20personal%20and%20societal%20burdens%2C%20yet%0Aconventional%20diagnostics%20are%20resource-intensive%20and%20limit%20accessibility.%0AAdvances%20in%20artificial%20intelligence%2C%20particularly%20natural%20language%20processing%0Aand%20multimodal%20methods%2C%20offer%20promise%20for%20detecting%20and%20addressing%20mental%0Adisorders%2C%20but%20raise%20critical%20privacy%20risks.%20This%20paper%20examines%20these%0Achallenges%20and%20proposes%20solutions%2C%20including%20anonymization%2C%20synthetic%20data%2C%20and%0Aprivacy-preserving%20training%2C%20while%20outlining%20frameworks%20for%20privacy-utility%0Atrade-offs%2C%20aiming%20to%20advance%20reliable%2C%20privacy-aware%20AI%20tools%20that%20support%0Aclinical%20decision-making%20and%20improve%20mental%20health%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00451v3&entry.124074799=Read"},
{"title": "ReHub: Linear Complexity Graph Transformers with Adaptive Hub-Spoke\n  Reassignment", "author": "Tomer Borreda and Daniel Freedman and Or Litany", "abstract": "  We present ReHub, a novel graph transformer architecture that achieves linear\ncomplexity through an efficient reassignment technique between nodes and\nvirtual nodes. Graph transformers have become increasingly important in graph\nlearning for their ability to utilize long-range node communication explicitly,\naddressing limitations such as oversmoothing and oversquashing found in\nmessage-passing graph networks. However, their dense attention mechanism scales\nquadratically with the number of nodes, limiting their applicability to\nlarge-scale graphs. ReHub draws inspiration from the airline industry's\nhub-and-spoke model, where flights are assigned to optimize operational\nefficiency. In our approach, graph nodes (spokes) are dynamically reassigned to\na fixed number of virtual nodes (hubs) at each model layer. Recent work, Neural\nAtoms (Li et al., 2024), has demonstrated impressive and consistent\nimprovements over GNN baselines by utilizing such virtual nodes; their findings\nsuggest that the number of hubs strongly influences performance. However,\nincreasing the number of hubs typically raises complexity, requiring a\ntrade-off to maintain linear complexity. Our key insight is that each node only\nneeds to interact with a small subset of hubs to achieve linear complexity,\neven when the total number of hubs is large. To leverage all hubs without\nincurring additional computational costs, we propose a simple yet effective\nadaptive reassignment technique based on hub-hub similarity scores, eliminating\nthe need for expensive node-hub computations. Our experiments on LRGB indicate\na consistent improvement in results over the base method, Neural Atoms, while\nmaintaining a linear complexity. Remarkably, our sparse model achieves\nperformance on par with its non-sparse counterpart. Furthermore, ReHub\noutperforms competitive baselines and consistently ranks among top performers\nacross various benchmarks.\n", "link": "http://arxiv.org/abs/2412.01519v2", "date": "2025-08-25", "relevancy": 1.9422, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5324}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4762}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReHub%3A%20Linear%20Complexity%20Graph%20Transformers%20with%20Adaptive%20Hub-Spoke%0A%20%20Reassignment&body=Title%3A%20ReHub%3A%20Linear%20Complexity%20Graph%20Transformers%20with%20Adaptive%20Hub-Spoke%0A%20%20Reassignment%0AAuthor%3A%20Tomer%20Borreda%20and%20Daniel%20Freedman%20and%20Or%20Litany%0AAbstract%3A%20%20%20We%20present%20ReHub%2C%20a%20novel%20graph%20transformer%20architecture%20that%20achieves%20linear%0Acomplexity%20through%20an%20efficient%20reassignment%20technique%20between%20nodes%20and%0Avirtual%20nodes.%20Graph%20transformers%20have%20become%20increasingly%20important%20in%20graph%0Alearning%20for%20their%20ability%20to%20utilize%20long-range%20node%20communication%20explicitly%2C%0Aaddressing%20limitations%20such%20as%20oversmoothing%20and%20oversquashing%20found%20in%0Amessage-passing%20graph%20networks.%20However%2C%20their%20dense%20attention%20mechanism%20scales%0Aquadratically%20with%20the%20number%20of%20nodes%2C%20limiting%20their%20applicability%20to%0Alarge-scale%20graphs.%20ReHub%20draws%20inspiration%20from%20the%20airline%20industry%27s%0Ahub-and-spoke%20model%2C%20where%20flights%20are%20assigned%20to%20optimize%20operational%0Aefficiency.%20In%20our%20approach%2C%20graph%20nodes%20%28spokes%29%20are%20dynamically%20reassigned%20to%0Aa%20fixed%20number%20of%20virtual%20nodes%20%28hubs%29%20at%20each%20model%20layer.%20Recent%20work%2C%20Neural%0AAtoms%20%28Li%20et%20al.%2C%202024%29%2C%20has%20demonstrated%20impressive%20and%20consistent%0Aimprovements%20over%20GNN%20baselines%20by%20utilizing%20such%20virtual%20nodes%3B%20their%20findings%0Asuggest%20that%20the%20number%20of%20hubs%20strongly%20influences%20performance.%20However%2C%0Aincreasing%20the%20number%20of%20hubs%20typically%20raises%20complexity%2C%20requiring%20a%0Atrade-off%20to%20maintain%20linear%20complexity.%20Our%20key%20insight%20is%20that%20each%20node%20only%0Aneeds%20to%20interact%20with%20a%20small%20subset%20of%20hubs%20to%20achieve%20linear%20complexity%2C%0Aeven%20when%20the%20total%20number%20of%20hubs%20is%20large.%20To%20leverage%20all%20hubs%20without%0Aincurring%20additional%20computational%20costs%2C%20we%20propose%20a%20simple%20yet%20effective%0Aadaptive%20reassignment%20technique%20based%20on%20hub-hub%20similarity%20scores%2C%20eliminating%0Athe%20need%20for%20expensive%20node-hub%20computations.%20Our%20experiments%20on%20LRGB%20indicate%0Aa%20consistent%20improvement%20in%20results%20over%20the%20base%20method%2C%20Neural%20Atoms%2C%20while%0Amaintaining%20a%20linear%20complexity.%20Remarkably%2C%20our%20sparse%20model%20achieves%0Aperformance%20on%20par%20with%20its%20non-sparse%20counterpart.%20Furthermore%2C%20ReHub%0Aoutperforms%20competitive%20baselines%20and%20consistently%20ranks%20among%20top%20performers%0Aacross%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01519v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReHub%253A%2520Linear%2520Complexity%2520Graph%2520Transformers%2520with%2520Adaptive%2520Hub-Spoke%250A%2520%2520Reassignment%26entry.906535625%3DTomer%2520Borreda%2520and%2520Daniel%2520Freedman%2520and%2520Or%2520Litany%26entry.1292438233%3D%2520%2520We%2520present%2520ReHub%252C%2520a%2520novel%2520graph%2520transformer%2520architecture%2520that%2520achieves%2520linear%250Acomplexity%2520through%2520an%2520efficient%2520reassignment%2520technique%2520between%2520nodes%2520and%250Avirtual%2520nodes.%2520Graph%2520transformers%2520have%2520become%2520increasingly%2520important%2520in%2520graph%250Alearning%2520for%2520their%2520ability%2520to%2520utilize%2520long-range%2520node%2520communication%2520explicitly%252C%250Aaddressing%2520limitations%2520such%2520as%2520oversmoothing%2520and%2520oversquashing%2520found%2520in%250Amessage-passing%2520graph%2520networks.%2520However%252C%2520their%2520dense%2520attention%2520mechanism%2520scales%250Aquadratically%2520with%2520the%2520number%2520of%2520nodes%252C%2520limiting%2520their%2520applicability%2520to%250Alarge-scale%2520graphs.%2520ReHub%2520draws%2520inspiration%2520from%2520the%2520airline%2520industry%2527s%250Ahub-and-spoke%2520model%252C%2520where%2520flights%2520are%2520assigned%2520to%2520optimize%2520operational%250Aefficiency.%2520In%2520our%2520approach%252C%2520graph%2520nodes%2520%2528spokes%2529%2520are%2520dynamically%2520reassigned%2520to%250Aa%2520fixed%2520number%2520of%2520virtual%2520nodes%2520%2528hubs%2529%2520at%2520each%2520model%2520layer.%2520Recent%2520work%252C%2520Neural%250AAtoms%2520%2528Li%2520et%2520al.%252C%25202024%2529%252C%2520has%2520demonstrated%2520impressive%2520and%2520consistent%250Aimprovements%2520over%2520GNN%2520baselines%2520by%2520utilizing%2520such%2520virtual%2520nodes%253B%2520their%2520findings%250Asuggest%2520that%2520the%2520number%2520of%2520hubs%2520strongly%2520influences%2520performance.%2520However%252C%250Aincreasing%2520the%2520number%2520of%2520hubs%2520typically%2520raises%2520complexity%252C%2520requiring%2520a%250Atrade-off%2520to%2520maintain%2520linear%2520complexity.%2520Our%2520key%2520insight%2520is%2520that%2520each%2520node%2520only%250Aneeds%2520to%2520interact%2520with%2520a%2520small%2520subset%2520of%2520hubs%2520to%2520achieve%2520linear%2520complexity%252C%250Aeven%2520when%2520the%2520total%2520number%2520of%2520hubs%2520is%2520large.%2520To%2520leverage%2520all%2520hubs%2520without%250Aincurring%2520additional%2520computational%2520costs%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Aadaptive%2520reassignment%2520technique%2520based%2520on%2520hub-hub%2520similarity%2520scores%252C%2520eliminating%250Athe%2520need%2520for%2520expensive%2520node-hub%2520computations.%2520Our%2520experiments%2520on%2520LRGB%2520indicate%250Aa%2520consistent%2520improvement%2520in%2520results%2520over%2520the%2520base%2520method%252C%2520Neural%2520Atoms%252C%2520while%250Amaintaining%2520a%2520linear%2520complexity.%2520Remarkably%252C%2520our%2520sparse%2520model%2520achieves%250Aperformance%2520on%2520par%2520with%2520its%2520non-sparse%2520counterpart.%2520Furthermore%252C%2520ReHub%250Aoutperforms%2520competitive%2520baselines%2520and%2520consistently%2520ranks%2520among%2520top%2520performers%250Aacross%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01519v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReHub%3A%20Linear%20Complexity%20Graph%20Transformers%20with%20Adaptive%20Hub-Spoke%0A%20%20Reassignment&entry.906535625=Tomer%20Borreda%20and%20Daniel%20Freedman%20and%20Or%20Litany&entry.1292438233=%20%20We%20present%20ReHub%2C%20a%20novel%20graph%20transformer%20architecture%20that%20achieves%20linear%0Acomplexity%20through%20an%20efficient%20reassignment%20technique%20between%20nodes%20and%0Avirtual%20nodes.%20Graph%20transformers%20have%20become%20increasingly%20important%20in%20graph%0Alearning%20for%20their%20ability%20to%20utilize%20long-range%20node%20communication%20explicitly%2C%0Aaddressing%20limitations%20such%20as%20oversmoothing%20and%20oversquashing%20found%20in%0Amessage-passing%20graph%20networks.%20However%2C%20their%20dense%20attention%20mechanism%20scales%0Aquadratically%20with%20the%20number%20of%20nodes%2C%20limiting%20their%20applicability%20to%0Alarge-scale%20graphs.%20ReHub%20draws%20inspiration%20from%20the%20airline%20industry%27s%0Ahub-and-spoke%20model%2C%20where%20flights%20are%20assigned%20to%20optimize%20operational%0Aefficiency.%20In%20our%20approach%2C%20graph%20nodes%20%28spokes%29%20are%20dynamically%20reassigned%20to%0Aa%20fixed%20number%20of%20virtual%20nodes%20%28hubs%29%20at%20each%20model%20layer.%20Recent%20work%2C%20Neural%0AAtoms%20%28Li%20et%20al.%2C%202024%29%2C%20has%20demonstrated%20impressive%20and%20consistent%0Aimprovements%20over%20GNN%20baselines%20by%20utilizing%20such%20virtual%20nodes%3B%20their%20findings%0Asuggest%20that%20the%20number%20of%20hubs%20strongly%20influences%20performance.%20However%2C%0Aincreasing%20the%20number%20of%20hubs%20typically%20raises%20complexity%2C%20requiring%20a%0Atrade-off%20to%20maintain%20linear%20complexity.%20Our%20key%20insight%20is%20that%20each%20node%20only%0Aneeds%20to%20interact%20with%20a%20small%20subset%20of%20hubs%20to%20achieve%20linear%20complexity%2C%0Aeven%20when%20the%20total%20number%20of%20hubs%20is%20large.%20To%20leverage%20all%20hubs%20without%0Aincurring%20additional%20computational%20costs%2C%20we%20propose%20a%20simple%20yet%20effective%0Aadaptive%20reassignment%20technique%20based%20on%20hub-hub%20similarity%20scores%2C%20eliminating%0Athe%20need%20for%20expensive%20node-hub%20computations.%20Our%20experiments%20on%20LRGB%20indicate%0Aa%20consistent%20improvement%20in%20results%20over%20the%20base%20method%2C%20Neural%20Atoms%2C%20while%0Amaintaining%20a%20linear%20complexity.%20Remarkably%2C%20our%20sparse%20model%20achieves%0Aperformance%20on%20par%20with%20its%20non-sparse%20counterpart.%20Furthermore%2C%20ReHub%0Aoutperforms%20competitive%20baselines%20and%20consistently%20ranks%20among%20top%20performers%0Aacross%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01519v2&entry.124074799=Read"},
{"title": "Egocentric Instruction-oriented Affordance Prediction via Large\n  Multimodal Model", "author": "Bokai Ji and Jie Gu and Xiaokang Ma and Chu Tang and Jingmin Chen and Guangxia Li", "abstract": "  Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.\n", "link": "http://arxiv.org/abs/2508.17922v1", "date": "2025-08-25", "relevancy": 1.7982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6456}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6155}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Egocentric%20Instruction-oriented%20Affordance%20Prediction%20via%20Large%0A%20%20Multimodal%20Model&body=Title%3A%20Egocentric%20Instruction-oriented%20Affordance%20Prediction%20via%20Large%0A%20%20Multimodal%20Model%0AAuthor%3A%20Bokai%20Ji%20and%20Jie%20Gu%20and%20Xiaokang%20Ma%20and%20Chu%20Tang%20and%20Jingmin%20Chen%20and%20Guangxia%20Li%0AAbstract%3A%20%20%20Affordance%20is%20crucial%20for%20intelligent%20robots%20in%20the%20context%20of%20object%0Amanipulation.%20In%20this%20paper%2C%20we%20argue%20that%20affordance%20should%20be%0Atask-/instruction-dependent%2C%20which%20is%20overlooked%20by%20many%20previous%20works.%20That%0Ais%2C%20different%20instructions%20can%20lead%20to%20different%20manipulation%20regions%20and%0Adirections%20even%20for%20the%20same%20object.%20According%20to%20this%20observation%2C%20we%20present%0Aa%20new%20dataset%20comprising%20fifteen%20thousand%20object-instruction-affordance%0Atriplets.%20All%20scenes%20in%20the%20dataset%20are%20from%20an%20egocentric%20viewpoint%2C%20designed%0Ato%20approximate%20the%20perspective%20of%20a%20human-like%20robot.%20Furthermore%2C%20we%0Ainvestigate%20how%20to%20enable%20large%20multimodal%20models%20%28LMMs%29%20to%20serve%20as%20affordance%0Apredictors%20by%20implementing%20a%20%60%60search%20against%20verifiers%27%27%20pipeline.%20An%20LMM%20is%0Aasked%20to%20progressively%20predict%20affordances%2C%20with%20the%20output%20at%20each%20step%20being%0Averified%20by%20itself%20during%20the%20iterative%20process%2C%20imitating%20a%20reasoning%20process.%0AExperiments%20show%20that%20our%20method%20not%20only%20unlocks%20new%20instruction-oriented%0Aaffordance%20prediction%20capabilities%2C%20but%20also%20achieves%20outstanding%20performance%0Abroadly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgocentric%2520Instruction-oriented%2520Affordance%2520Prediction%2520via%2520Large%250A%2520%2520Multimodal%2520Model%26entry.906535625%3DBokai%2520Ji%2520and%2520Jie%2520Gu%2520and%2520Xiaokang%2520Ma%2520and%2520Chu%2520Tang%2520and%2520Jingmin%2520Chen%2520and%2520Guangxia%2520Li%26entry.1292438233%3D%2520%2520Affordance%2520is%2520crucial%2520for%2520intelligent%2520robots%2520in%2520the%2520context%2520of%2520object%250Amanipulation.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520affordance%2520should%2520be%250Atask-/instruction-dependent%252C%2520which%2520is%2520overlooked%2520by%2520many%2520previous%2520works.%2520That%250Ais%252C%2520different%2520instructions%2520can%2520lead%2520to%2520different%2520manipulation%2520regions%2520and%250Adirections%2520even%2520for%2520the%2520same%2520object.%2520According%2520to%2520this%2520observation%252C%2520we%2520present%250Aa%2520new%2520dataset%2520comprising%2520fifteen%2520thousand%2520object-instruction-affordance%250Atriplets.%2520All%2520scenes%2520in%2520the%2520dataset%2520are%2520from%2520an%2520egocentric%2520viewpoint%252C%2520designed%250Ato%2520approximate%2520the%2520perspective%2520of%2520a%2520human-like%2520robot.%2520Furthermore%252C%2520we%250Ainvestigate%2520how%2520to%2520enable%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520to%2520serve%2520as%2520affordance%250Apredictors%2520by%2520implementing%2520a%2520%2560%2560search%2520against%2520verifiers%2527%2527%2520pipeline.%2520An%2520LMM%2520is%250Aasked%2520to%2520progressively%2520predict%2520affordances%252C%2520with%2520the%2520output%2520at%2520each%2520step%2520being%250Averified%2520by%2520itself%2520during%2520the%2520iterative%2520process%252C%2520imitating%2520a%2520reasoning%2520process.%250AExperiments%2520show%2520that%2520our%2520method%2520not%2520only%2520unlocks%2520new%2520instruction-oriented%250Aaffordance%2520prediction%2520capabilities%252C%2520but%2520also%2520achieves%2520outstanding%2520performance%250Abroadly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Egocentric%20Instruction-oriented%20Affordance%20Prediction%20via%20Large%0A%20%20Multimodal%20Model&entry.906535625=Bokai%20Ji%20and%20Jie%20Gu%20and%20Xiaokang%20Ma%20and%20Chu%20Tang%20and%20Jingmin%20Chen%20and%20Guangxia%20Li&entry.1292438233=%20%20Affordance%20is%20crucial%20for%20intelligent%20robots%20in%20the%20context%20of%20object%0Amanipulation.%20In%20this%20paper%2C%20we%20argue%20that%20affordance%20should%20be%0Atask-/instruction-dependent%2C%20which%20is%20overlooked%20by%20many%20previous%20works.%20That%0Ais%2C%20different%20instructions%20can%20lead%20to%20different%20manipulation%20regions%20and%0Adirections%20even%20for%20the%20same%20object.%20According%20to%20this%20observation%2C%20we%20present%0Aa%20new%20dataset%20comprising%20fifteen%20thousand%20object-instruction-affordance%0Atriplets.%20All%20scenes%20in%20the%20dataset%20are%20from%20an%20egocentric%20viewpoint%2C%20designed%0Ato%20approximate%20the%20perspective%20of%20a%20human-like%20robot.%20Furthermore%2C%20we%0Ainvestigate%20how%20to%20enable%20large%20multimodal%20models%20%28LMMs%29%20to%20serve%20as%20affordance%0Apredictors%20by%20implementing%20a%20%60%60search%20against%20verifiers%27%27%20pipeline.%20An%20LMM%20is%0Aasked%20to%20progressively%20predict%20affordances%2C%20with%20the%20output%20at%20each%20step%20being%0Averified%20by%20itself%20during%20the%20iterative%20process%2C%20imitating%20a%20reasoning%20process.%0AExperiments%20show%20that%20our%20method%20not%20only%20unlocks%20new%20instruction-oriented%0Aaffordance%20prediction%20capabilities%2C%20but%20also%20achieves%20outstanding%20performance%0Abroadly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17922v1&entry.124074799=Read"},
{"title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "author": "Dengyang Jiang and Zanyi Wang and Hengzhuang Li and Sizhe Dang and Teli Ma and Wei Wei and Guang Dai and Lei Zhang and Mengmeng Wang", "abstract": "  Building a generalized affordance grounding model to identify actionable\nregions on objects is vital for real-world applications. Existing methods to\ntrain the model can be divided into weakly and fully supervised ways. However,\nthe former method requires a complex training framework design and can not\ninfer new actions without an auxiliary prior. While the latter often struggle\nwith limited annotated data and components trained from scratch despite being\nsimpler. This study focuses on fully supervised affordance grounding and\novercomes its limitations by proposing AffordanceSAM, which extends SAM's\ngeneralization capacity in segmentation to affordance grounding. Specifically,\nwe design an affordance-adaption module and curate a coarse-to-fine annotated\ndataset called C2F-Aff to thoroughly transfer SAM's robust performance to\naffordance in a three-stage training manner. Experimental results confirm that\nAffordanceSAM achieves state-of-the-art (SOTA) performance on the AGD20K\nbenchmark and exhibits strong generalized capacity.\n", "link": "http://arxiv.org/abs/2504.15650v2", "date": "2025-08-25", "relevancy": 1.5522, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AffordanceSAM%3A%20Segment%20Anything%20Once%20More%20in%20Affordance%20Grounding&body=Title%3A%20AffordanceSAM%3A%20Segment%20Anything%20Once%20More%20in%20Affordance%20Grounding%0AAuthor%3A%20Dengyang%20Jiang%20and%20Zanyi%20Wang%20and%20Hengzhuang%20Li%20and%20Sizhe%20Dang%20and%20Teli%20Ma%20and%20Wei%20Wei%20and%20Guang%20Dai%20and%20Lei%20Zhang%20and%20Mengmeng%20Wang%0AAbstract%3A%20%20%20Building%20a%20generalized%20affordance%20grounding%20model%20to%20identify%20actionable%0Aregions%20on%20objects%20is%20vital%20for%20real-world%20applications.%20Existing%20methods%20to%0Atrain%20the%20model%20can%20be%20divided%20into%20weakly%20and%20fully%20supervised%20ways.%20However%2C%0Athe%20former%20method%20requires%20a%20complex%20training%20framework%20design%20and%20can%20not%0Ainfer%20new%20actions%20without%20an%20auxiliary%20prior.%20While%20the%20latter%20often%20struggle%0Awith%20limited%20annotated%20data%20and%20components%20trained%20from%20scratch%20despite%20being%0Asimpler.%20This%20study%20focuses%20on%20fully%20supervised%20affordance%20grounding%20and%0Aovercomes%20its%20limitations%20by%20proposing%20AffordanceSAM%2C%20which%20extends%20SAM%27s%0Ageneralization%20capacity%20in%20segmentation%20to%20affordance%20grounding.%20Specifically%2C%0Awe%20design%20an%20affordance-adaption%20module%20and%20curate%20a%20coarse-to-fine%20annotated%0Adataset%20called%20C2F-Aff%20to%20thoroughly%20transfer%20SAM%27s%20robust%20performance%20to%0Aaffordance%20in%20a%20three-stage%20training%20manner.%20Experimental%20results%20confirm%20that%0AAffordanceSAM%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20the%20AGD20K%0Abenchmark%20and%20exhibits%20strong%20generalized%20capacity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffordanceSAM%253A%2520Segment%2520Anything%2520Once%2520More%2520in%2520Affordance%2520Grounding%26entry.906535625%3DDengyang%2520Jiang%2520and%2520Zanyi%2520Wang%2520and%2520Hengzhuang%2520Li%2520and%2520Sizhe%2520Dang%2520and%2520Teli%2520Ma%2520and%2520Wei%2520Wei%2520and%2520Guang%2520Dai%2520and%2520Lei%2520Zhang%2520and%2520Mengmeng%2520Wang%26entry.1292438233%3D%2520%2520Building%2520a%2520generalized%2520affordance%2520grounding%2520model%2520to%2520identify%2520actionable%250Aregions%2520on%2520objects%2520is%2520vital%2520for%2520real-world%2520applications.%2520Existing%2520methods%2520to%250Atrain%2520the%2520model%2520can%2520be%2520divided%2520into%2520weakly%2520and%2520fully%2520supervised%2520ways.%2520However%252C%250Athe%2520former%2520method%2520requires%2520a%2520complex%2520training%2520framework%2520design%2520and%2520can%2520not%250Ainfer%2520new%2520actions%2520without%2520an%2520auxiliary%2520prior.%2520While%2520the%2520latter%2520often%2520struggle%250Awith%2520limited%2520annotated%2520data%2520and%2520components%2520trained%2520from%2520scratch%2520despite%2520being%250Asimpler.%2520This%2520study%2520focuses%2520on%2520fully%2520supervised%2520affordance%2520grounding%2520and%250Aovercomes%2520its%2520limitations%2520by%2520proposing%2520AffordanceSAM%252C%2520which%2520extends%2520SAM%2527s%250Ageneralization%2520capacity%2520in%2520segmentation%2520to%2520affordance%2520grounding.%2520Specifically%252C%250Awe%2520design%2520an%2520affordance-adaption%2520module%2520and%2520curate%2520a%2520coarse-to-fine%2520annotated%250Adataset%2520called%2520C2F-Aff%2520to%2520thoroughly%2520transfer%2520SAM%2527s%2520robust%2520performance%2520to%250Aaffordance%2520in%2520a%2520three-stage%2520training%2520manner.%2520Experimental%2520results%2520confirm%2520that%250AAffordanceSAM%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520the%2520AGD20K%250Abenchmark%2520and%2520exhibits%2520strong%2520generalized%2520capacity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AffordanceSAM%3A%20Segment%20Anything%20Once%20More%20in%20Affordance%20Grounding&entry.906535625=Dengyang%20Jiang%20and%20Zanyi%20Wang%20and%20Hengzhuang%20Li%20and%20Sizhe%20Dang%20and%20Teli%20Ma%20and%20Wei%20Wei%20and%20Guang%20Dai%20and%20Lei%20Zhang%20and%20Mengmeng%20Wang&entry.1292438233=%20%20Building%20a%20generalized%20affordance%20grounding%20model%20to%20identify%20actionable%0Aregions%20on%20objects%20is%20vital%20for%20real-world%20applications.%20Existing%20methods%20to%0Atrain%20the%20model%20can%20be%20divided%20into%20weakly%20and%20fully%20supervised%20ways.%20However%2C%0Athe%20former%20method%20requires%20a%20complex%20training%20framework%20design%20and%20can%20not%0Ainfer%20new%20actions%20without%20an%20auxiliary%20prior.%20While%20the%20latter%20often%20struggle%0Awith%20limited%20annotated%20data%20and%20components%20trained%20from%20scratch%20despite%20being%0Asimpler.%20This%20study%20focuses%20on%20fully%20supervised%20affordance%20grounding%20and%0Aovercomes%20its%20limitations%20by%20proposing%20AffordanceSAM%2C%20which%20extends%20SAM%27s%0Ageneralization%20capacity%20in%20segmentation%20to%20affordance%20grounding.%20Specifically%2C%0Awe%20design%20an%20affordance-adaption%20module%20and%20curate%20a%20coarse-to-fine%20annotated%0Adataset%20called%20C2F-Aff%20to%20thoroughly%20transfer%20SAM%27s%20robust%20performance%20to%0Aaffordance%20in%20a%20three-stage%20training%20manner.%20Experimental%20results%20confirm%20that%0AAffordanceSAM%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20the%20AGD20K%0Abenchmark%20and%20exhibits%20strong%20generalized%20capacity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15650v2&entry.124074799=Read"},
{"title": "RepoMaster: Autonomous Exploration and Understanding of GitHub\n  Repositories for Complex Task Solving", "author": "Huacan Wang and Ziyi Ni and Shuo Zhang and Shuo Lu and Sen Hu and Ziyang He and Chen Hu and Jiaye Lin and Yifu Guo and Ronghao Chen and Xin Li and Daxin Jiang and Yuntao Du and Pin Lyu", "abstract": "  The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/QuantaAlpha/RepoMaster.\n", "link": "http://arxiv.org/abs/2505.21577v3", "date": "2025-08-25", "relevancy": 1.9701, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepoMaster%3A%20Autonomous%20Exploration%20and%20Understanding%20of%20GitHub%0A%20%20Repositories%20for%20Complex%20Task%20Solving&body=Title%3A%20RepoMaster%3A%20Autonomous%20Exploration%20and%20Understanding%20of%20GitHub%0A%20%20Repositories%20for%20Complex%20Task%20Solving%0AAuthor%3A%20Huacan%20Wang%20and%20Ziyi%20Ni%20and%20Shuo%20Zhang%20and%20Shuo%20Lu%20and%20Sen%20Hu%20and%20Ziyang%20He%20and%20Chen%20Hu%20and%20Jiaye%20Lin%20and%20Yifu%20Guo%20and%20Ronghao%20Chen%20and%20Xin%20Li%20and%20Daxin%20Jiang%20and%20Yuntao%20Du%20and%20Pin%20Lyu%0AAbstract%3A%20%20%20The%20ultimate%20goal%20of%20code%20agents%20is%20to%20solve%20complex%20tasks%20autonomously.%0AAlthough%20large%20language%20models%20%28LLMs%29%20have%20made%20substantial%20progress%20in%20code%0Ageneration%2C%20real-world%20tasks%20typically%20demand%20full-fledged%20code%20repositories%0Arather%20than%20simple%20scripts.%20Building%20such%20repositories%20from%20scratch%20remains%20a%0Amajor%20challenge.%20Fortunately%2C%20GitHub%20hosts%20a%20vast%2C%20evolving%20collection%20of%0Aopen-source%20repositories%2C%20which%20developers%20frequently%20reuse%20as%20modular%0Acomponents%20for%20complex%20tasks.%20Yet%2C%20existing%20frameworks%20like%20OpenHands%20and%0ASWE-Agent%20still%20struggle%20to%20effectively%20leverage%20these%20valuable%20resources.%0ARelying%20solely%20on%20README%20files%20provides%20insufficient%20guidance%2C%20and%20deeper%0Aexploration%20reveals%20two%20core%20obstacles%3A%20overwhelming%20information%20and%20tangled%0Adependencies%20of%20repositories%2C%20both%20constrained%20by%20the%20limited%20context%20windows%0Aof%20current%20LLMs.%20To%20tackle%20these%20issues%2C%20we%20propose%20RepoMaster%2C%20an%20autonomous%0Aagent%20framework%20designed%20to%20explore%20and%20reuse%20GitHub%20repositories%20for%20solving%0Acomplex%20tasks.%20For%20efficient%20understanding%2C%20RepoMaster%20constructs%20function-call%0Agraphs%2C%20module-dependency%20graphs%2C%20and%20hierarchical%20code%20trees%20to%20identify%0Aessential%20components%2C%20providing%20only%20identified%20core%20elements%20to%20the%20LLMs%0Arather%20than%20the%20entire%20repository.%20During%20autonomous%20execution%2C%20it%0Aprogressively%20explores%20related%20components%20using%20our%20exploration%20tools%20and%0Aprunes%20information%20to%20optimize%20context%20usage.%20Evaluated%20on%20the%20adjusted%0AMLE-bench%2C%20RepoMaster%20achieves%20a%20110%25%20relative%20boost%20in%20valid%20submissions%20over%0Athe%20strongest%20baseline%20OpenHands.%20On%20our%20newly%20released%20GitTaskBench%2C%0ARepoMaster%20lifts%20the%20task-pass%20rate%20from%2040.7%25%20to%2062.9%25%20while%20reducing%20token%0Ausage%20by%2095%25.%20Our%20code%20and%20demonstration%20materials%20are%20publicly%20available%20at%0Ahttps%3A//github.com/QuantaAlpha/RepoMaster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21577v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepoMaster%253A%2520Autonomous%2520Exploration%2520and%2520Understanding%2520of%2520GitHub%250A%2520%2520Repositories%2520for%2520Complex%2520Task%2520Solving%26entry.906535625%3DHuacan%2520Wang%2520and%2520Ziyi%2520Ni%2520and%2520Shuo%2520Zhang%2520and%2520Shuo%2520Lu%2520and%2520Sen%2520Hu%2520and%2520Ziyang%2520He%2520and%2520Chen%2520Hu%2520and%2520Jiaye%2520Lin%2520and%2520Yifu%2520Guo%2520and%2520Ronghao%2520Chen%2520and%2520Xin%2520Li%2520and%2520Daxin%2520Jiang%2520and%2520Yuntao%2520Du%2520and%2520Pin%2520Lyu%26entry.1292438233%3D%2520%2520The%2520ultimate%2520goal%2520of%2520code%2520agents%2520is%2520to%2520solve%2520complex%2520tasks%2520autonomously.%250AAlthough%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520substantial%2520progress%2520in%2520code%250Ageneration%252C%2520real-world%2520tasks%2520typically%2520demand%2520full-fledged%2520code%2520repositories%250Arather%2520than%2520simple%2520scripts.%2520Building%2520such%2520repositories%2520from%2520scratch%2520remains%2520a%250Amajor%2520challenge.%2520Fortunately%252C%2520GitHub%2520hosts%2520a%2520vast%252C%2520evolving%2520collection%2520of%250Aopen-source%2520repositories%252C%2520which%2520developers%2520frequently%2520reuse%2520as%2520modular%250Acomponents%2520for%2520complex%2520tasks.%2520Yet%252C%2520existing%2520frameworks%2520like%2520OpenHands%2520and%250ASWE-Agent%2520still%2520struggle%2520to%2520effectively%2520leverage%2520these%2520valuable%2520resources.%250ARelying%2520solely%2520on%2520README%2520files%2520provides%2520insufficient%2520guidance%252C%2520and%2520deeper%250Aexploration%2520reveals%2520two%2520core%2520obstacles%253A%2520overwhelming%2520information%2520and%2520tangled%250Adependencies%2520of%2520repositories%252C%2520both%2520constrained%2520by%2520the%2520limited%2520context%2520windows%250Aof%2520current%2520LLMs.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520RepoMaster%252C%2520an%2520autonomous%250Aagent%2520framework%2520designed%2520to%2520explore%2520and%2520reuse%2520GitHub%2520repositories%2520for%2520solving%250Acomplex%2520tasks.%2520For%2520efficient%2520understanding%252C%2520RepoMaster%2520constructs%2520function-call%250Agraphs%252C%2520module-dependency%2520graphs%252C%2520and%2520hierarchical%2520code%2520trees%2520to%2520identify%250Aessential%2520components%252C%2520providing%2520only%2520identified%2520core%2520elements%2520to%2520the%2520LLMs%250Arather%2520than%2520the%2520entire%2520repository.%2520During%2520autonomous%2520execution%252C%2520it%250Aprogressively%2520explores%2520related%2520components%2520using%2520our%2520exploration%2520tools%2520and%250Aprunes%2520information%2520to%2520optimize%2520context%2520usage.%2520Evaluated%2520on%2520the%2520adjusted%250AMLE-bench%252C%2520RepoMaster%2520achieves%2520a%2520110%2525%2520relative%2520boost%2520in%2520valid%2520submissions%2520over%250Athe%2520strongest%2520baseline%2520OpenHands.%2520On%2520our%2520newly%2520released%2520GitTaskBench%252C%250ARepoMaster%2520lifts%2520the%2520task-pass%2520rate%2520from%252040.7%2525%2520to%252062.9%2525%2520while%2520reducing%2520token%250Ausage%2520by%252095%2525.%2520Our%2520code%2520and%2520demonstration%2520materials%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/QuantaAlpha/RepoMaster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21577v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepoMaster%3A%20Autonomous%20Exploration%20and%20Understanding%20of%20GitHub%0A%20%20Repositories%20for%20Complex%20Task%20Solving&entry.906535625=Huacan%20Wang%20and%20Ziyi%20Ni%20and%20Shuo%20Zhang%20and%20Shuo%20Lu%20and%20Sen%20Hu%20and%20Ziyang%20He%20and%20Chen%20Hu%20and%20Jiaye%20Lin%20and%20Yifu%20Guo%20and%20Ronghao%20Chen%20and%20Xin%20Li%20and%20Daxin%20Jiang%20and%20Yuntao%20Du%20and%20Pin%20Lyu&entry.1292438233=%20%20The%20ultimate%20goal%20of%20code%20agents%20is%20to%20solve%20complex%20tasks%20autonomously.%0AAlthough%20large%20language%20models%20%28LLMs%29%20have%20made%20substantial%20progress%20in%20code%0Ageneration%2C%20real-world%20tasks%20typically%20demand%20full-fledged%20code%20repositories%0Arather%20than%20simple%20scripts.%20Building%20such%20repositories%20from%20scratch%20remains%20a%0Amajor%20challenge.%20Fortunately%2C%20GitHub%20hosts%20a%20vast%2C%20evolving%20collection%20of%0Aopen-source%20repositories%2C%20which%20developers%20frequently%20reuse%20as%20modular%0Acomponents%20for%20complex%20tasks.%20Yet%2C%20existing%20frameworks%20like%20OpenHands%20and%0ASWE-Agent%20still%20struggle%20to%20effectively%20leverage%20these%20valuable%20resources.%0ARelying%20solely%20on%20README%20files%20provides%20insufficient%20guidance%2C%20and%20deeper%0Aexploration%20reveals%20two%20core%20obstacles%3A%20overwhelming%20information%20and%20tangled%0Adependencies%20of%20repositories%2C%20both%20constrained%20by%20the%20limited%20context%20windows%0Aof%20current%20LLMs.%20To%20tackle%20these%20issues%2C%20we%20propose%20RepoMaster%2C%20an%20autonomous%0Aagent%20framework%20designed%20to%20explore%20and%20reuse%20GitHub%20repositories%20for%20solving%0Acomplex%20tasks.%20For%20efficient%20understanding%2C%20RepoMaster%20constructs%20function-call%0Agraphs%2C%20module-dependency%20graphs%2C%20and%20hierarchical%20code%20trees%20to%20identify%0Aessential%20components%2C%20providing%20only%20identified%20core%20elements%20to%20the%20LLMs%0Arather%20than%20the%20entire%20repository.%20During%20autonomous%20execution%2C%20it%0Aprogressively%20explores%20related%20components%20using%20our%20exploration%20tools%20and%0Aprunes%20information%20to%20optimize%20context%20usage.%20Evaluated%20on%20the%20adjusted%0AMLE-bench%2C%20RepoMaster%20achieves%20a%20110%25%20relative%20boost%20in%20valid%20submissions%20over%0Athe%20strongest%20baseline%20OpenHands.%20On%20our%20newly%20released%20GitTaskBench%2C%0ARepoMaster%20lifts%20the%20task-pass%20rate%20from%2040.7%25%20to%2062.9%25%20while%20reducing%20token%0Ausage%20by%2095%25.%20Our%20code%20and%20demonstration%20materials%20are%20publicly%20available%20at%0Ahttps%3A//github.com/QuantaAlpha/RepoMaster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21577v3&entry.124074799=Read"},
{"title": "Why Isn't Relational Learning Taking Over the World?", "author": "David Poole", "abstract": "  Artificial intelligence seems to be taking over the world with systems that\nmodel pixels, words, and phonemes. The world is arguably made up, not of\npixels, words, and phonemes but of entities (objects, things, including events)\nwith properties and relations among them. Surely we should model these, not the\nperception or description of them. You might suspect that concentrating on\nmodeling words and pixels is because all of the (valuable) data in the world is\nin terms of text and images. If you look into almost any company you will find\ntheir most valuable data is in spreadsheets, databases and other relational\nformats. These are not the form that are studied in introductory machine\nlearning, but are full of product numbers, student numbers, transaction numbers\nand other identifiers that can't be interpreted naively as numbers. The field\nthat studies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.\n", "link": "http://arxiv.org/abs/2507.13558v3", "date": "2025-08-25", "relevancy": 1.8968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F&body=Title%3A%20Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F%0AAuthor%3A%20David%20Poole%0AAbstract%3A%20%20%20Artificial%20intelligence%20seems%20to%20be%20taking%20over%20the%20world%20with%20systems%20that%0Amodel%20pixels%2C%20words%2C%20and%20phonemes.%20The%20world%20is%20arguably%20made%20up%2C%20not%20of%0Apixels%2C%20words%2C%20and%20phonemes%20but%20of%20entities%20%28objects%2C%20things%2C%20including%20events%29%0Awith%20properties%20and%20relations%20among%20them.%20Surely%20we%20should%20model%20these%2C%20not%20the%0Aperception%20or%20description%20of%20them.%20You%20might%20suspect%20that%20concentrating%20on%0Amodeling%20words%20and%20pixels%20is%20because%20all%20of%20the%20%28valuable%29%20data%20in%20the%20world%20is%0Ain%20terms%20of%20text%20and%20images.%20If%20you%20look%20into%20almost%20any%20company%20you%20will%20find%0Atheir%20most%20valuable%20data%20is%20in%20spreadsheets%2C%20databases%20and%20other%20relational%0Aformats.%20These%20are%20not%20the%20form%20that%20are%20studied%20in%20introductory%20machine%0Alearning%2C%20but%20are%20full%20of%20product%20numbers%2C%20student%20numbers%2C%20transaction%20numbers%0Aand%20other%20identifiers%20that%20can%27t%20be%20interpreted%20naively%20as%20numbers.%20The%20field%0Athat%20studies%20this%20sort%20of%20data%20has%20various%20names%20including%20relational%20learning%2C%0Astatistical%20relational%20AI%2C%20and%20many%20others.%20This%20paper%20explains%20why%20relational%0Alearning%20is%20not%20taking%20over%20the%20world%20--%20except%20in%20a%20few%20cases%20with%20restricted%0Arelations%20--%20and%20what%20needs%20to%20be%20done%20to%20bring%20it%20to%20it%27s%20rightful%20prominence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13558v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Isn%2527t%2520Relational%2520Learning%2520Taking%2520Over%2520the%2520World%253F%26entry.906535625%3DDavid%2520Poole%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520seems%2520to%2520be%2520taking%2520over%2520the%2520world%2520with%2520systems%2520that%250Amodel%2520pixels%252C%2520words%252C%2520and%2520phonemes.%2520The%2520world%2520is%2520arguably%2520made%2520up%252C%2520not%2520of%250Apixels%252C%2520words%252C%2520and%2520phonemes%2520but%2520of%2520entities%2520%2528objects%252C%2520things%252C%2520including%2520events%2529%250Awith%2520properties%2520and%2520relations%2520among%2520them.%2520Surely%2520we%2520should%2520model%2520these%252C%2520not%2520the%250Aperception%2520or%2520description%2520of%2520them.%2520You%2520might%2520suspect%2520that%2520concentrating%2520on%250Amodeling%2520words%2520and%2520pixels%2520is%2520because%2520all%2520of%2520the%2520%2528valuable%2529%2520data%2520in%2520the%2520world%2520is%250Ain%2520terms%2520of%2520text%2520and%2520images.%2520If%2520you%2520look%2520into%2520almost%2520any%2520company%2520you%2520will%2520find%250Atheir%2520most%2520valuable%2520data%2520is%2520in%2520spreadsheets%252C%2520databases%2520and%2520other%2520relational%250Aformats.%2520These%2520are%2520not%2520the%2520form%2520that%2520are%2520studied%2520in%2520introductory%2520machine%250Alearning%252C%2520but%2520are%2520full%2520of%2520product%2520numbers%252C%2520student%2520numbers%252C%2520transaction%2520numbers%250Aand%2520other%2520identifiers%2520that%2520can%2527t%2520be%2520interpreted%2520naively%2520as%2520numbers.%2520The%2520field%250Athat%2520studies%2520this%2520sort%2520of%2520data%2520has%2520various%2520names%2520including%2520relational%2520learning%252C%250Astatistical%2520relational%2520AI%252C%2520and%2520many%2520others.%2520This%2520paper%2520explains%2520why%2520relational%250Alearning%2520is%2520not%2520taking%2520over%2520the%2520world%2520--%2520except%2520in%2520a%2520few%2520cases%2520with%2520restricted%250Arelations%2520--%2520and%2520what%2520needs%2520to%2520be%2520done%2520to%2520bring%2520it%2520to%2520it%2527s%2520rightful%2520prominence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13558v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Isn%27t%20Relational%20Learning%20Taking%20Over%20the%20World%3F&entry.906535625=David%20Poole&entry.1292438233=%20%20Artificial%20intelligence%20seems%20to%20be%20taking%20over%20the%20world%20with%20systems%20that%0Amodel%20pixels%2C%20words%2C%20and%20phonemes.%20The%20world%20is%20arguably%20made%20up%2C%20not%20of%0Apixels%2C%20words%2C%20and%20phonemes%20but%20of%20entities%20%28objects%2C%20things%2C%20including%20events%29%0Awith%20properties%20and%20relations%20among%20them.%20Surely%20we%20should%20model%20these%2C%20not%20the%0Aperception%20or%20description%20of%20them.%20You%20might%20suspect%20that%20concentrating%20on%0Amodeling%20words%20and%20pixels%20is%20because%20all%20of%20the%20%28valuable%29%20data%20in%20the%20world%20is%0Ain%20terms%20of%20text%20and%20images.%20If%20you%20look%20into%20almost%20any%20company%20you%20will%20find%0Atheir%20most%20valuable%20data%20is%20in%20spreadsheets%2C%20databases%20and%20other%20relational%0Aformats.%20These%20are%20not%20the%20form%20that%20are%20studied%20in%20introductory%20machine%0Alearning%2C%20but%20are%20full%20of%20product%20numbers%2C%20student%20numbers%2C%20transaction%20numbers%0Aand%20other%20identifiers%20that%20can%27t%20be%20interpreted%20naively%20as%20numbers.%20The%20field%0Athat%20studies%20this%20sort%20of%20data%20has%20various%20names%20including%20relational%20learning%2C%0Astatistical%20relational%20AI%2C%20and%20many%20others.%20This%20paper%20explains%20why%20relational%0Alearning%20is%20not%20taking%20over%20the%20world%20--%20except%20in%20a%20few%20cases%20with%20restricted%0Arelations%20--%20and%20what%20needs%20to%20be%20done%20to%20bring%20it%20to%20it%27s%20rightful%20prominence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13558v3&entry.124074799=Read"},
{"title": "Teaching LLMs to Think Mathematically: A Critical Study of\n  Decision-Making via Optimization", "author": "Mohammad J. Abdel-Rahman and Yasmeen Alslman and Dania Refai and Amro Saleh and Malik A. Abu Loha and Mohammad Yahya Hamed", "abstract": "  This paper investigates the capabilities of large language models (LLMs) in\nformulating and solving decision-making problems using mathematical\nprogramming. We first conduct a systematic review and meta-analysis of recent\nliterature to assess how well LLMs understand, structure, and solve\noptimization problems across domains. The analysis is guided by critical review\nquestions focusing on learning approaches, dataset designs, evaluation metrics,\nand prompting strategies. Our systematic evidence is complemented by targeted\nexperiments designed to evaluate the performance of state-of-the-art LLMs in\nautomatically generating optimization models for problems in computer networks.\nUsing a newly constructed dataset, we apply three prompting strategies:\nAct-as-expert, chain-of-thought, and self-consistency, and evaluate the\nobtained outputs based on optimality gap, token-level F1 score, and compilation\naccuracy. Results show promising progress in LLMs' ability to parse natural\nlanguage and represent symbolic formulations, but also reveal key limitations\nin accuracy, scalability, and interpretability. These empirical gaps motivate\nseveral future research directions, including structured datasets,\ndomain-specific fine-tuning, hybrid neuro-symbolic approaches, modular\nmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper\ncontributes a structured roadmap for advancing LLM capabilities in mathematical\nprogramming.\n", "link": "http://arxiv.org/abs/2508.18091v1", "date": "2025-08-25", "relevancy": 2.0287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20LLMs%20to%20Think%20Mathematically%3A%20A%20Critical%20Study%20of%0A%20%20Decision-Making%20via%20Optimization&body=Title%3A%20Teaching%20LLMs%20to%20Think%20Mathematically%3A%20A%20Critical%20Study%20of%0A%20%20Decision-Making%20via%20Optimization%0AAuthor%3A%20Mohammad%20J.%20Abdel-Rahman%20and%20Yasmeen%20Alslman%20and%20Dania%20Refai%20and%20Amro%20Saleh%20and%20Malik%20A.%20Abu%20Loha%20and%20Mohammad%20Yahya%20Hamed%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%0Aformulating%20and%20solving%20decision-making%20problems%20using%20mathematical%0Aprogramming.%20We%20first%20conduct%20a%20systematic%20review%20and%20meta-analysis%20of%20recent%0Aliterature%20to%20assess%20how%20well%20LLMs%20understand%2C%20structure%2C%20and%20solve%0Aoptimization%20problems%20across%20domains.%20The%20analysis%20is%20guided%20by%20critical%20review%0Aquestions%20focusing%20on%20learning%20approaches%2C%20dataset%20designs%2C%20evaluation%20metrics%2C%0Aand%20prompting%20strategies.%20Our%20systematic%20evidence%20is%20complemented%20by%20targeted%0Aexperiments%20designed%20to%20evaluate%20the%20performance%20of%20state-of-the-art%20LLMs%20in%0Aautomatically%20generating%20optimization%20models%20for%20problems%20in%20computer%20networks.%0AUsing%20a%20newly%20constructed%20dataset%2C%20we%20apply%20three%20prompting%20strategies%3A%0AAct-as-expert%2C%20chain-of-thought%2C%20and%20self-consistency%2C%20and%20evaluate%20the%0Aobtained%20outputs%20based%20on%20optimality%20gap%2C%20token-level%20F1%20score%2C%20and%20compilation%0Aaccuracy.%20Results%20show%20promising%20progress%20in%20LLMs%27%20ability%20to%20parse%20natural%0Alanguage%20and%20represent%20symbolic%20formulations%2C%20but%20also%20reveal%20key%20limitations%0Ain%20accuracy%2C%20scalability%2C%20and%20interpretability.%20These%20empirical%20gaps%20motivate%0Aseveral%20future%20research%20directions%2C%20including%20structured%20datasets%2C%0Adomain-specific%20fine-tuning%2C%20hybrid%20neuro-symbolic%20approaches%2C%20modular%0Amulti-agent%20architectures%2C%20and%20dynamic%20retrieval%20via%20chain-of-RAGs.%20This%20paper%0Acontributes%20a%20structured%20roadmap%20for%20advancing%20LLM%20capabilities%20in%20mathematical%0Aprogramming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520LLMs%2520to%2520Think%2520Mathematically%253A%2520A%2520Critical%2520Study%2520of%250A%2520%2520Decision-Making%2520via%2520Optimization%26entry.906535625%3DMohammad%2520J.%2520Abdel-Rahman%2520and%2520Yasmeen%2520Alslman%2520and%2520Dania%2520Refai%2520and%2520Amro%2520Saleh%2520and%2520Malik%2520A.%2520Abu%2520Loha%2520and%2520Mohammad%2520Yahya%2520Hamed%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%250Aformulating%2520and%2520solving%2520decision-making%2520problems%2520using%2520mathematical%250Aprogramming.%2520We%2520first%2520conduct%2520a%2520systematic%2520review%2520and%2520meta-analysis%2520of%2520recent%250Aliterature%2520to%2520assess%2520how%2520well%2520LLMs%2520understand%252C%2520structure%252C%2520and%2520solve%250Aoptimization%2520problems%2520across%2520domains.%2520The%2520analysis%2520is%2520guided%2520by%2520critical%2520review%250Aquestions%2520focusing%2520on%2520learning%2520approaches%252C%2520dataset%2520designs%252C%2520evaluation%2520metrics%252C%250Aand%2520prompting%2520strategies.%2520Our%2520systematic%2520evidence%2520is%2520complemented%2520by%2520targeted%250Aexperiments%2520designed%2520to%2520evaluate%2520the%2520performance%2520of%2520state-of-the-art%2520LLMs%2520in%250Aautomatically%2520generating%2520optimization%2520models%2520for%2520problems%2520in%2520computer%2520networks.%250AUsing%2520a%2520newly%2520constructed%2520dataset%252C%2520we%2520apply%2520three%2520prompting%2520strategies%253A%250AAct-as-expert%252C%2520chain-of-thought%252C%2520and%2520self-consistency%252C%2520and%2520evaluate%2520the%250Aobtained%2520outputs%2520based%2520on%2520optimality%2520gap%252C%2520token-level%2520F1%2520score%252C%2520and%2520compilation%250Aaccuracy.%2520Results%2520show%2520promising%2520progress%2520in%2520LLMs%2527%2520ability%2520to%2520parse%2520natural%250Alanguage%2520and%2520represent%2520symbolic%2520formulations%252C%2520but%2520also%2520reveal%2520key%2520limitations%250Ain%2520accuracy%252C%2520scalability%252C%2520and%2520interpretability.%2520These%2520empirical%2520gaps%2520motivate%250Aseveral%2520future%2520research%2520directions%252C%2520including%2520structured%2520datasets%252C%250Adomain-specific%2520fine-tuning%252C%2520hybrid%2520neuro-symbolic%2520approaches%252C%2520modular%250Amulti-agent%2520architectures%252C%2520and%2520dynamic%2520retrieval%2520via%2520chain-of-RAGs.%2520This%2520paper%250Acontributes%2520a%2520structured%2520roadmap%2520for%2520advancing%2520LLM%2520capabilities%2520in%2520mathematical%250Aprogramming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20LLMs%20to%20Think%20Mathematically%3A%20A%20Critical%20Study%20of%0A%20%20Decision-Making%20via%20Optimization&entry.906535625=Mohammad%20J.%20Abdel-Rahman%20and%20Yasmeen%20Alslman%20and%20Dania%20Refai%20and%20Amro%20Saleh%20and%20Malik%20A.%20Abu%20Loha%20and%20Mohammad%20Yahya%20Hamed&entry.1292438233=%20%20This%20paper%20investigates%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%0Aformulating%20and%20solving%20decision-making%20problems%20using%20mathematical%0Aprogramming.%20We%20first%20conduct%20a%20systematic%20review%20and%20meta-analysis%20of%20recent%0Aliterature%20to%20assess%20how%20well%20LLMs%20understand%2C%20structure%2C%20and%20solve%0Aoptimization%20problems%20across%20domains.%20The%20analysis%20is%20guided%20by%20critical%20review%0Aquestions%20focusing%20on%20learning%20approaches%2C%20dataset%20designs%2C%20evaluation%20metrics%2C%0Aand%20prompting%20strategies.%20Our%20systematic%20evidence%20is%20complemented%20by%20targeted%0Aexperiments%20designed%20to%20evaluate%20the%20performance%20of%20state-of-the-art%20LLMs%20in%0Aautomatically%20generating%20optimization%20models%20for%20problems%20in%20computer%20networks.%0AUsing%20a%20newly%20constructed%20dataset%2C%20we%20apply%20three%20prompting%20strategies%3A%0AAct-as-expert%2C%20chain-of-thought%2C%20and%20self-consistency%2C%20and%20evaluate%20the%0Aobtained%20outputs%20based%20on%20optimality%20gap%2C%20token-level%20F1%20score%2C%20and%20compilation%0Aaccuracy.%20Results%20show%20promising%20progress%20in%20LLMs%27%20ability%20to%20parse%20natural%0Alanguage%20and%20represent%20symbolic%20formulations%2C%20but%20also%20reveal%20key%20limitations%0Ain%20accuracy%2C%20scalability%2C%20and%20interpretability.%20These%20empirical%20gaps%20motivate%0Aseveral%20future%20research%20directions%2C%20including%20structured%20datasets%2C%0Adomain-specific%20fine-tuning%2C%20hybrid%20neuro-symbolic%20approaches%2C%20modular%0Amulti-agent%20architectures%2C%20and%20dynamic%20retrieval%20via%20chain-of-RAGs.%20This%20paper%0Acontributes%20a%20structured%20roadmap%20for%20advancing%20LLM%20capabilities%20in%20mathematical%0Aprogramming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18091v1&entry.124074799=Read"},
{"title": "The AI Data Scientist", "author": "Farkhad Akimov and Munachiso Samuel Nwadike and Zangir Iklassov and Martin Tak\u00e1\u010d", "abstract": "  Imagine decision-makers uploading data and, within minutes, receiving clear,\nactionable insights delivered straight to their fingertips. That is the promise\nof the AI Data Scientist, an autonomous Agent powered by large language models\n(LLMs) that closes the gap between evidence and action. Rather than simply\nwriting code or responding to prompts, it reasons through questions, tests\nideas, and delivers end-to-end insights at a pace far beyond traditional\nworkflows. Guided by the scientific tenet of the hypothesis, this Agent\nuncovers explanatory patterns in data, evaluates their statistical\nsignificance, and uses them to inform predictive modeling. It then translates\nthese results into recommendations that are both rigorous and accessible. At\nthe core of the AI Data Scientist is a team of specialized LLM Subagents, each\nresponsible for a distinct task such as data cleaning, statistical testing,\nvalidation, and plain-language communication. These Subagents write their own\ncode, reason about causality, and identify when additional data is needed to\nsupport sound conclusions. Together, they achieve in minutes what might\notherwise take days or weeks, enabling a new kind of interaction that makes\ndeep data science both accessible and actionable.\n", "link": "http://arxiv.org/abs/2508.18113v1", "date": "2025-08-25", "relevancy": 1.8913, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4837}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.48}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20AI%20Data%20Scientist&body=Title%3A%20The%20AI%20Data%20Scientist%0AAuthor%3A%20Farkhad%20Akimov%20and%20Munachiso%20Samuel%20Nwadike%20and%20Zangir%20Iklassov%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20%20%20Imagine%20decision-makers%20uploading%20data%20and%2C%20within%20minutes%2C%20receiving%20clear%2C%0Aactionable%20insights%20delivered%20straight%20to%20their%20fingertips.%20That%20is%20the%20promise%0Aof%20the%20AI%20Data%20Scientist%2C%20an%20autonomous%20Agent%20powered%20by%20large%20language%20models%0A%28LLMs%29%20that%20closes%20the%20gap%20between%20evidence%20and%20action.%20Rather%20than%20simply%0Awriting%20code%20or%20responding%20to%20prompts%2C%20it%20reasons%20through%20questions%2C%20tests%0Aideas%2C%20and%20delivers%20end-to-end%20insights%20at%20a%20pace%20far%20beyond%20traditional%0Aworkflows.%20Guided%20by%20the%20scientific%20tenet%20of%20the%20hypothesis%2C%20this%20Agent%0Auncovers%20explanatory%20patterns%20in%20data%2C%20evaluates%20their%20statistical%0Asignificance%2C%20and%20uses%20them%20to%20inform%20predictive%20modeling.%20It%20then%20translates%0Athese%20results%20into%20recommendations%20that%20are%20both%20rigorous%20and%20accessible.%20At%0Athe%20core%20of%20the%20AI%20Data%20Scientist%20is%20a%20team%20of%20specialized%20LLM%20Subagents%2C%20each%0Aresponsible%20for%20a%20distinct%20task%20such%20as%20data%20cleaning%2C%20statistical%20testing%2C%0Avalidation%2C%20and%20plain-language%20communication.%20These%20Subagents%20write%20their%20own%0Acode%2C%20reason%20about%20causality%2C%20and%20identify%20when%20additional%20data%20is%20needed%20to%0Asupport%20sound%20conclusions.%20Together%2C%20they%20achieve%20in%20minutes%20what%20might%0Aotherwise%20take%20days%20or%20weeks%2C%20enabling%20a%20new%20kind%20of%20interaction%20that%20makes%0Adeep%20data%20science%20both%20accessible%20and%20actionable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520AI%2520Data%2520Scientist%26entry.906535625%3DFarkhad%2520Akimov%2520and%2520Munachiso%2520Samuel%2520Nwadike%2520and%2520Zangir%2520Iklassov%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%26entry.1292438233%3D%2520%2520Imagine%2520decision-makers%2520uploading%2520data%2520and%252C%2520within%2520minutes%252C%2520receiving%2520clear%252C%250Aactionable%2520insights%2520delivered%2520straight%2520to%2520their%2520fingertips.%2520That%2520is%2520the%2520promise%250Aof%2520the%2520AI%2520Data%2520Scientist%252C%2520an%2520autonomous%2520Agent%2520powered%2520by%2520large%2520language%2520models%250A%2528LLMs%2529%2520that%2520closes%2520the%2520gap%2520between%2520evidence%2520and%2520action.%2520Rather%2520than%2520simply%250Awriting%2520code%2520or%2520responding%2520to%2520prompts%252C%2520it%2520reasons%2520through%2520questions%252C%2520tests%250Aideas%252C%2520and%2520delivers%2520end-to-end%2520insights%2520at%2520a%2520pace%2520far%2520beyond%2520traditional%250Aworkflows.%2520Guided%2520by%2520the%2520scientific%2520tenet%2520of%2520the%2520hypothesis%252C%2520this%2520Agent%250Auncovers%2520explanatory%2520patterns%2520in%2520data%252C%2520evaluates%2520their%2520statistical%250Asignificance%252C%2520and%2520uses%2520them%2520to%2520inform%2520predictive%2520modeling.%2520It%2520then%2520translates%250Athese%2520results%2520into%2520recommendations%2520that%2520are%2520both%2520rigorous%2520and%2520accessible.%2520At%250Athe%2520core%2520of%2520the%2520AI%2520Data%2520Scientist%2520is%2520a%2520team%2520of%2520specialized%2520LLM%2520Subagents%252C%2520each%250Aresponsible%2520for%2520a%2520distinct%2520task%2520such%2520as%2520data%2520cleaning%252C%2520statistical%2520testing%252C%250Avalidation%252C%2520and%2520plain-language%2520communication.%2520These%2520Subagents%2520write%2520their%2520own%250Acode%252C%2520reason%2520about%2520causality%252C%2520and%2520identify%2520when%2520additional%2520data%2520is%2520needed%2520to%250Asupport%2520sound%2520conclusions.%2520Together%252C%2520they%2520achieve%2520in%2520minutes%2520what%2520might%250Aotherwise%2520take%2520days%2520or%2520weeks%252C%2520enabling%2520a%2520new%2520kind%2520of%2520interaction%2520that%2520makes%250Adeep%2520data%2520science%2520both%2520accessible%2520and%2520actionable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20AI%20Data%20Scientist&entry.906535625=Farkhad%20Akimov%20and%20Munachiso%20Samuel%20Nwadike%20and%20Zangir%20Iklassov%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=%20%20Imagine%20decision-makers%20uploading%20data%20and%2C%20within%20minutes%2C%20receiving%20clear%2C%0Aactionable%20insights%20delivered%20straight%20to%20their%20fingertips.%20That%20is%20the%20promise%0Aof%20the%20AI%20Data%20Scientist%2C%20an%20autonomous%20Agent%20powered%20by%20large%20language%20models%0A%28LLMs%29%20that%20closes%20the%20gap%20between%20evidence%20and%20action.%20Rather%20than%20simply%0Awriting%20code%20or%20responding%20to%20prompts%2C%20it%20reasons%20through%20questions%2C%20tests%0Aideas%2C%20and%20delivers%20end-to-end%20insights%20at%20a%20pace%20far%20beyond%20traditional%0Aworkflows.%20Guided%20by%20the%20scientific%20tenet%20of%20the%20hypothesis%2C%20this%20Agent%0Auncovers%20explanatory%20patterns%20in%20data%2C%20evaluates%20their%20statistical%0Asignificance%2C%20and%20uses%20them%20to%20inform%20predictive%20modeling.%20It%20then%20translates%0Athese%20results%20into%20recommendations%20that%20are%20both%20rigorous%20and%20accessible.%20At%0Athe%20core%20of%20the%20AI%20Data%20Scientist%20is%20a%20team%20of%20specialized%20LLM%20Subagents%2C%20each%0Aresponsible%20for%20a%20distinct%20task%20such%20as%20data%20cleaning%2C%20statistical%20testing%2C%0Avalidation%2C%20and%20plain-language%20communication.%20These%20Subagents%20write%20their%20own%0Acode%2C%20reason%20about%20causality%2C%20and%20identify%20when%20additional%20data%20is%20needed%20to%0Asupport%20sound%20conclusions.%20Together%2C%20they%20achieve%20in%20minutes%20what%20might%0Aotherwise%20take%20days%20or%20weeks%2C%20enabling%20a%20new%20kind%20of%20interaction%20that%20makes%0Adeep%20data%20science%20both%20accessible%20and%20actionable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18113v1&entry.124074799=Read"},
{"title": "Analysis of Harpy's Constrained Trotting and Jumping Maneuver", "author": "Prathima Ananda Kumar", "abstract": "  This study presents an analysis of experimental data from Harpy, a\nthruster-assisted bipedal robot developed at Northeastern University. The study\nexamines data sets from trotting and jumping experiments to understand the\nfundamental principles governing hybrid leg-thruster locomotion. Through data\nanalysis across multiple locomotion modes, this research reveals that Harpy\nachieves stable locomotion with bounded trajectories and consistent foot\nplacement through strategic leg-thruster synergy. The results demonstrate\ncontrolled joint behavior with low torques and symmetric tracking, accurate\nfoot placement within kinematic constraints despite phase-transition\nperturbations, and underactuated degree-of-freedom stability without\ndivergence. Energy level analysis reveals that legs provide primary propulsion,\nwhile the thrusters enable additional aerial phase control. The analysis\nidentifies critical body-leg coupling dynamics during aerial phases that\nrequire phase-specific control strategies. Consistent repeatability and\nsymmetry across experiments validate the robustness of the hybrid actuation\napproach.\n", "link": "http://arxiv.org/abs/2508.18139v1", "date": "2025-08-25", "relevancy": 1.4096, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4639}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Harpy%27s%20Constrained%20Trotting%20and%20Jumping%20Maneuver&body=Title%3A%20Analysis%20of%20Harpy%27s%20Constrained%20Trotting%20and%20Jumping%20Maneuver%0AAuthor%3A%20Prathima%20Ananda%20Kumar%0AAbstract%3A%20%20%20This%20study%20presents%20an%20analysis%20of%20experimental%20data%20from%20Harpy%2C%20a%0Athruster-assisted%20bipedal%20robot%20developed%20at%20Northeastern%20University.%20The%20study%0Aexamines%20data%20sets%20from%20trotting%20and%20jumping%20experiments%20to%20understand%20the%0Afundamental%20principles%20governing%20hybrid%20leg-thruster%20locomotion.%20Through%20data%0Aanalysis%20across%20multiple%20locomotion%20modes%2C%20this%20research%20reveals%20that%20Harpy%0Aachieves%20stable%20locomotion%20with%20bounded%20trajectories%20and%20consistent%20foot%0Aplacement%20through%20strategic%20leg-thruster%20synergy.%20The%20results%20demonstrate%0Acontrolled%20joint%20behavior%20with%20low%20torques%20and%20symmetric%20tracking%2C%20accurate%0Afoot%20placement%20within%20kinematic%20constraints%20despite%20phase-transition%0Aperturbations%2C%20and%20underactuated%20degree-of-freedom%20stability%20without%0Adivergence.%20Energy%20level%20analysis%20reveals%20that%20legs%20provide%20primary%20propulsion%2C%0Awhile%20the%20thrusters%20enable%20additional%20aerial%20phase%20control.%20The%20analysis%0Aidentifies%20critical%20body-leg%20coupling%20dynamics%20during%20aerial%20phases%20that%0Arequire%20phase-specific%20control%20strategies.%20Consistent%20repeatability%20and%0Asymmetry%20across%20experiments%20validate%20the%20robustness%20of%20the%20hybrid%20actuation%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Harpy%2527s%2520Constrained%2520Trotting%2520and%2520Jumping%2520Maneuver%26entry.906535625%3DPrathima%2520Ananda%2520Kumar%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520an%2520analysis%2520of%2520experimental%2520data%2520from%2520Harpy%252C%2520a%250Athruster-assisted%2520bipedal%2520robot%2520developed%2520at%2520Northeastern%2520University.%2520The%2520study%250Aexamines%2520data%2520sets%2520from%2520trotting%2520and%2520jumping%2520experiments%2520to%2520understand%2520the%250Afundamental%2520principles%2520governing%2520hybrid%2520leg-thruster%2520locomotion.%2520Through%2520data%250Aanalysis%2520across%2520multiple%2520locomotion%2520modes%252C%2520this%2520research%2520reveals%2520that%2520Harpy%250Aachieves%2520stable%2520locomotion%2520with%2520bounded%2520trajectories%2520and%2520consistent%2520foot%250Aplacement%2520through%2520strategic%2520leg-thruster%2520synergy.%2520The%2520results%2520demonstrate%250Acontrolled%2520joint%2520behavior%2520with%2520low%2520torques%2520and%2520symmetric%2520tracking%252C%2520accurate%250Afoot%2520placement%2520within%2520kinematic%2520constraints%2520despite%2520phase-transition%250Aperturbations%252C%2520and%2520underactuated%2520degree-of-freedom%2520stability%2520without%250Adivergence.%2520Energy%2520level%2520analysis%2520reveals%2520that%2520legs%2520provide%2520primary%2520propulsion%252C%250Awhile%2520the%2520thrusters%2520enable%2520additional%2520aerial%2520phase%2520control.%2520The%2520analysis%250Aidentifies%2520critical%2520body-leg%2520coupling%2520dynamics%2520during%2520aerial%2520phases%2520that%250Arequire%2520phase-specific%2520control%2520strategies.%2520Consistent%2520repeatability%2520and%250Asymmetry%2520across%2520experiments%2520validate%2520the%2520robustness%2520of%2520the%2520hybrid%2520actuation%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Harpy%27s%20Constrained%20Trotting%20and%20Jumping%20Maneuver&entry.906535625=Prathima%20Ananda%20Kumar&entry.1292438233=%20%20This%20study%20presents%20an%20analysis%20of%20experimental%20data%20from%20Harpy%2C%20a%0Athruster-assisted%20bipedal%20robot%20developed%20at%20Northeastern%20University.%20The%20study%0Aexamines%20data%20sets%20from%20trotting%20and%20jumping%20experiments%20to%20understand%20the%0Afundamental%20principles%20governing%20hybrid%20leg-thruster%20locomotion.%20Through%20data%0Aanalysis%20across%20multiple%20locomotion%20modes%2C%20this%20research%20reveals%20that%20Harpy%0Aachieves%20stable%20locomotion%20with%20bounded%20trajectories%20and%20consistent%20foot%0Aplacement%20through%20strategic%20leg-thruster%20synergy.%20The%20results%20demonstrate%0Acontrolled%20joint%20behavior%20with%20low%20torques%20and%20symmetric%20tracking%2C%20accurate%0Afoot%20placement%20within%20kinematic%20constraints%20despite%20phase-transition%0Aperturbations%2C%20and%20underactuated%20degree-of-freedom%20stability%20without%0Adivergence.%20Energy%20level%20analysis%20reveals%20that%20legs%20provide%20primary%20propulsion%2C%0Awhile%20the%20thrusters%20enable%20additional%20aerial%20phase%20control.%20The%20analysis%0Aidentifies%20critical%20body-leg%20coupling%20dynamics%20during%20aerial%20phases%20that%0Arequire%20phase-specific%20control%20strategies.%20Consistent%20repeatability%20and%0Asymmetry%20across%20experiments%20validate%20the%20robustness%20of%20the%20hybrid%20actuation%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18139v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


