<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240728.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UGG: Unified Generative Grasping", "author": "Jiaxin Lu and Hao Kang and Haoxiang Li and Bo Liu and Yiding Yang and Qixing Huang and Gang Hua", "abstract": "  Dexterous grasping aims to produce diverse grasping postures with a high\ngrasping success rate. Regression-based methods that directly predict grasping\nparameters given the object may achieve a high success rate but often lack\ndiversity. Generation-based methods that generate grasping postures conditioned\non the object can often produce diverse grasping, but they are insufficient for\nhigh grasping success due to lack of discriminative information. To mitigate,\nwe introduce a unified diffusion-based dexterous grasp generation model, dubbed\nthe name UGG, which operates within the object point cloud and hand parameter\nspaces. Our all-transformer architecture unifies the information from the\nobject, the hand, and the contacts, introducing a novel representation of\ncontact points for improved contact modeling. The flexibility and quality of\nour model enable the integration of a lightweight discriminator, benefiting\nfrom simulated discriminative data, which pushes for a high success rate while\npreserving high diversity. Beyond grasp generation, our model can also generate\nobjects based on hand information, offering valuable insights into object\ndesign and studying how the generative model perceives objects. Our model\nachieves state-of-the-art dexterous grasping on the large-scale DexGraspNet\ndataset while facilitating human-centric object design, marking a significant\nadvancement in dexterous grasping research. Our project page is\nhttps://jiaxin-lu.github.io/ugg/.\n", "link": "http://arxiv.org/abs/2311.16917v2", "date": "2024-07-26", "relevancy": 3.5037, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.9827}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5636}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UGG%3A%20Unified%20Generative%20Grasping&body=Title%3A%20UGG%3A%20Unified%20Generative%20Grasping%0AAuthor%3A%20Jiaxin%20Lu%20and%20Hao%20Kang%20and%20Haoxiang%20Li%20and%20Bo%20Liu%20and%20Yiding%20Yang%20and%20Qixing%20Huang%20and%20Gang%20Hua%0AAbstract%3A%20%20%20Dexterous%20grasping%20aims%20to%20produce%20diverse%20grasping%20postures%20with%20a%20high%0Agrasping%20success%20rate.%20Regression-based%20methods%20that%20directly%20predict%20grasping%0Aparameters%20given%20the%20object%20may%20achieve%20a%20high%20success%20rate%20but%20often%20lack%0Adiversity.%20Generation-based%20methods%20that%20generate%20grasping%20postures%20conditioned%0Aon%20the%20object%20can%20often%20produce%20diverse%20grasping%2C%20but%20they%20are%20insufficient%20for%0Ahigh%20grasping%20success%20due%20to%20lack%20of%20discriminative%20information.%20To%20mitigate%2C%0Awe%20introduce%20a%20unified%20diffusion-based%20dexterous%20grasp%20generation%20model%2C%20dubbed%0Athe%20name%20UGG%2C%20which%20operates%20within%20the%20object%20point%20cloud%20and%20hand%20parameter%0Aspaces.%20Our%20all-transformer%20architecture%20unifies%20the%20information%20from%20the%0Aobject%2C%20the%20hand%2C%20and%20the%20contacts%2C%20introducing%20a%20novel%20representation%20of%0Acontact%20points%20for%20improved%20contact%20modeling.%20The%20flexibility%20and%20quality%20of%0Aour%20model%20enable%20the%20integration%20of%20a%20lightweight%20discriminator%2C%20benefiting%0Afrom%20simulated%20discriminative%20data%2C%20which%20pushes%20for%20a%20high%20success%20rate%20while%0Apreserving%20high%20diversity.%20Beyond%20grasp%20generation%2C%20our%20model%20can%20also%20generate%0Aobjects%20based%20on%20hand%20information%2C%20offering%20valuable%20insights%20into%20object%0Adesign%20and%20studying%20how%20the%20generative%20model%20perceives%20objects.%20Our%20model%0Aachieves%20state-of-the-art%20dexterous%20grasping%20on%20the%20large-scale%20DexGraspNet%0Adataset%20while%20facilitating%20human-centric%20object%20design%2C%20marking%20a%20significant%0Aadvancement%20in%20dexterous%20grasping%20research.%20Our%20project%20page%20is%0Ahttps%3A//jiaxin-lu.github.io/ugg/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUGG%253A%2520Unified%2520Generative%2520Grasping%26entry.906535625%3DJiaxin%2520Lu%2520and%2520Hao%2520Kang%2520and%2520Haoxiang%2520Li%2520and%2520Bo%2520Liu%2520and%2520Yiding%2520Yang%2520and%2520Qixing%2520Huang%2520and%2520Gang%2520Hua%26entry.1292438233%3D%2520%2520Dexterous%2520grasping%2520aims%2520to%2520produce%2520diverse%2520grasping%2520postures%2520with%2520a%2520high%250Agrasping%2520success%2520rate.%2520Regression-based%2520methods%2520that%2520directly%2520predict%2520grasping%250Aparameters%2520given%2520the%2520object%2520may%2520achieve%2520a%2520high%2520success%2520rate%2520but%2520often%2520lack%250Adiversity.%2520Generation-based%2520methods%2520that%2520generate%2520grasping%2520postures%2520conditioned%250Aon%2520the%2520object%2520can%2520often%2520produce%2520diverse%2520grasping%252C%2520but%2520they%2520are%2520insufficient%2520for%250Ahigh%2520grasping%2520success%2520due%2520to%2520lack%2520of%2520discriminative%2520information.%2520To%2520mitigate%252C%250Awe%2520introduce%2520a%2520unified%2520diffusion-based%2520dexterous%2520grasp%2520generation%2520model%252C%2520dubbed%250Athe%2520name%2520UGG%252C%2520which%2520operates%2520within%2520the%2520object%2520point%2520cloud%2520and%2520hand%2520parameter%250Aspaces.%2520Our%2520all-transformer%2520architecture%2520unifies%2520the%2520information%2520from%2520the%250Aobject%252C%2520the%2520hand%252C%2520and%2520the%2520contacts%252C%2520introducing%2520a%2520novel%2520representation%2520of%250Acontact%2520points%2520for%2520improved%2520contact%2520modeling.%2520The%2520flexibility%2520and%2520quality%2520of%250Aour%2520model%2520enable%2520the%2520integration%2520of%2520a%2520lightweight%2520discriminator%252C%2520benefiting%250Afrom%2520simulated%2520discriminative%2520data%252C%2520which%2520pushes%2520for%2520a%2520high%2520success%2520rate%2520while%250Apreserving%2520high%2520diversity.%2520Beyond%2520grasp%2520generation%252C%2520our%2520model%2520can%2520also%2520generate%250Aobjects%2520based%2520on%2520hand%2520information%252C%2520offering%2520valuable%2520insights%2520into%2520object%250Adesign%2520and%2520studying%2520how%2520the%2520generative%2520model%2520perceives%2520objects.%2520Our%2520model%250Aachieves%2520state-of-the-art%2520dexterous%2520grasping%2520on%2520the%2520large-scale%2520DexGraspNet%250Adataset%2520while%2520facilitating%2520human-centric%2520object%2520design%252C%2520marking%2520a%2520significant%250Aadvancement%2520in%2520dexterous%2520grasping%2520research.%2520Our%2520project%2520page%2520is%250Ahttps%253A//jiaxin-lu.github.io/ugg/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UGG%3A%20Unified%20Generative%20Grasping&entry.906535625=Jiaxin%20Lu%20and%20Hao%20Kang%20and%20Haoxiang%20Li%20and%20Bo%20Liu%20and%20Yiding%20Yang%20and%20Qixing%20Huang%20and%20Gang%20Hua&entry.1292438233=%20%20Dexterous%20grasping%20aims%20to%20produce%20diverse%20grasping%20postures%20with%20a%20high%0Agrasping%20success%20rate.%20Regression-based%20methods%20that%20directly%20predict%20grasping%0Aparameters%20given%20the%20object%20may%20achieve%20a%20high%20success%20rate%20but%20often%20lack%0Adiversity.%20Generation-based%20methods%20that%20generate%20grasping%20postures%20conditioned%0Aon%20the%20object%20can%20often%20produce%20diverse%20grasping%2C%20but%20they%20are%20insufficient%20for%0Ahigh%20grasping%20success%20due%20to%20lack%20of%20discriminative%20information.%20To%20mitigate%2C%0Awe%20introduce%20a%20unified%20diffusion-based%20dexterous%20grasp%20generation%20model%2C%20dubbed%0Athe%20name%20UGG%2C%20which%20operates%20within%20the%20object%20point%20cloud%20and%20hand%20parameter%0Aspaces.%20Our%20all-transformer%20architecture%20unifies%20the%20information%20from%20the%0Aobject%2C%20the%20hand%2C%20and%20the%20contacts%2C%20introducing%20a%20novel%20representation%20of%0Acontact%20points%20for%20improved%20contact%20modeling.%20The%20flexibility%20and%20quality%20of%0Aour%20model%20enable%20the%20integration%20of%20a%20lightweight%20discriminator%2C%20benefiting%0Afrom%20simulated%20discriminative%20data%2C%20which%20pushes%20for%20a%20high%20success%20rate%20while%0Apreserving%20high%20diversity.%20Beyond%20grasp%20generation%2C%20our%20model%20can%20also%20generate%0Aobjects%20based%20on%20hand%20information%2C%20offering%20valuable%20insights%20into%20object%0Adesign%20and%20studying%20how%20the%20generative%20model%20perceives%20objects.%20Our%20model%0Aachieves%20state-of-the-art%20dexterous%20grasping%20on%20the%20large-scale%20DexGraspNet%0Adataset%20while%20facilitating%20human-centric%20object%20design%2C%20marking%20a%20significant%0Aadvancement%20in%20dexterous%20grasping%20research.%20Our%20project%20page%20is%0Ahttps%3A//jiaxin-lu.github.io/ugg/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16917v2&entry.124074799=Read"},
{"title": "PIV3CAMS: a multi-camera dataset for multiple computer vision problems\n  and its application to novel view-point synthesis", "author": "Sohyeong Kim and Martin Danelljan and Radu Timofte and Luc Van Gool and Jean-Philippe Thiran", "abstract": "  The modern approaches for computer vision tasks significantly rely on machine\nlearning, which requires a large number of quality images. While there is a\nplethora of image datasets with a single type of images, there is a lack of\ndatasets collected from multiple cameras. In this thesis, we introduce Paired\nImage and Video data from three CAMeraS, namely PIV3CAMS, aimed at multiple\ncomputer vision tasks. The PIV3CAMS dataset consists of 8385 pairs of images\nand 82 pairs of videos taken from three different cameras: Canon D5 Mark IV,\nHuawei P20, and ZED stereo camera. The dataset includes various indoor and\noutdoor scenes from different locations in Zurich (Switzerland) and Cheonan\n(South Korea). Some of the computer vision applications that can benefit from\nthe PIV3CAMS dataset are image/video enhancement, view interpolation, image\nmatching, and much more. We provide a careful explanation of the data\ncollection process and detailed analysis of the data. The second part of this\nthesis studies the usage of depth information in the view synthesizing task. In\naddition to the regeneration of a current state-of-the-art algorithm, we\ninvestigate several proposed alternative models that integrate depth\ninformation geometrically. Through extensive experiments, we show that the\neffect of depth is crucial in small view changes. Finally, we apply our model\nto the introduced PIV3CAMS dataset to synthesize novel target views as an\nexample application of PIV3CAMS.\n", "link": "http://arxiv.org/abs/2407.18695v1", "date": "2024-07-26", "relevancy": 3.0724, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIV3CAMS%3A%20a%20multi-camera%20dataset%20for%20multiple%20computer%20vision%20problems%0A%20%20and%20its%20application%20to%20novel%20view-point%20synthesis&body=Title%3A%20PIV3CAMS%3A%20a%20multi-camera%20dataset%20for%20multiple%20computer%20vision%20problems%0A%20%20and%20its%20application%20to%20novel%20view-point%20synthesis%0AAuthor%3A%20Sohyeong%20Kim%20and%20Martin%20Danelljan%20and%20Radu%20Timofte%20and%20Luc%20Van%20Gool%20and%20Jean-Philippe%20Thiran%0AAbstract%3A%20%20%20The%20modern%20approaches%20for%20computer%20vision%20tasks%20significantly%20rely%20on%20machine%0Alearning%2C%20which%20requires%20a%20large%20number%20of%20quality%20images.%20While%20there%20is%20a%0Aplethora%20of%20image%20datasets%20with%20a%20single%20type%20of%20images%2C%20there%20is%20a%20lack%20of%0Adatasets%20collected%20from%20multiple%20cameras.%20In%20this%20thesis%2C%20we%20introduce%20Paired%0AImage%20and%20Video%20data%20from%20three%20CAMeraS%2C%20namely%20PIV3CAMS%2C%20aimed%20at%20multiple%0Acomputer%20vision%20tasks.%20The%20PIV3CAMS%20dataset%20consists%20of%208385%20pairs%20of%20images%0Aand%2082%20pairs%20of%20videos%20taken%20from%20three%20different%20cameras%3A%20Canon%20D5%20Mark%20IV%2C%0AHuawei%20P20%2C%20and%20ZED%20stereo%20camera.%20The%20dataset%20includes%20various%20indoor%20and%0Aoutdoor%20scenes%20from%20different%20locations%20in%20Zurich%20%28Switzerland%29%20and%20Cheonan%0A%28South%20Korea%29.%20Some%20of%20the%20computer%20vision%20applications%20that%20can%20benefit%20from%0Athe%20PIV3CAMS%20dataset%20are%20image/video%20enhancement%2C%20view%20interpolation%2C%20image%0Amatching%2C%20and%20much%20more.%20We%20provide%20a%20careful%20explanation%20of%20the%20data%0Acollection%20process%20and%20detailed%20analysis%20of%20the%20data.%20The%20second%20part%20of%20this%0Athesis%20studies%20the%20usage%20of%20depth%20information%20in%20the%20view%20synthesizing%20task.%20In%0Aaddition%20to%20the%20regeneration%20of%20a%20current%20state-of-the-art%20algorithm%2C%20we%0Ainvestigate%20several%20proposed%20alternative%20models%20that%20integrate%20depth%0Ainformation%20geometrically.%20Through%20extensive%20experiments%2C%20we%20show%20that%20the%0Aeffect%20of%20depth%20is%20crucial%20in%20small%20view%20changes.%20Finally%2C%20we%20apply%20our%20model%0Ato%20the%20introduced%20PIV3CAMS%20dataset%20to%20synthesize%20novel%20target%20views%20as%20an%0Aexample%20application%20of%20PIV3CAMS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIV3CAMS%253A%2520a%2520multi-camera%2520dataset%2520for%2520multiple%2520computer%2520vision%2520problems%250A%2520%2520and%2520its%2520application%2520to%2520novel%2520view-point%2520synthesis%26entry.906535625%3DSohyeong%2520Kim%2520and%2520Martin%2520Danelljan%2520and%2520Radu%2520Timofte%2520and%2520Luc%2520Van%2520Gool%2520and%2520Jean-Philippe%2520Thiran%26entry.1292438233%3D%2520%2520The%2520modern%2520approaches%2520for%2520computer%2520vision%2520tasks%2520significantly%2520rely%2520on%2520machine%250Alearning%252C%2520which%2520requires%2520a%2520large%2520number%2520of%2520quality%2520images.%2520While%2520there%2520is%2520a%250Aplethora%2520of%2520image%2520datasets%2520with%2520a%2520single%2520type%2520of%2520images%252C%2520there%2520is%2520a%2520lack%2520of%250Adatasets%2520collected%2520from%2520multiple%2520cameras.%2520In%2520this%2520thesis%252C%2520we%2520introduce%2520Paired%250AImage%2520and%2520Video%2520data%2520from%2520three%2520CAMeraS%252C%2520namely%2520PIV3CAMS%252C%2520aimed%2520at%2520multiple%250Acomputer%2520vision%2520tasks.%2520The%2520PIV3CAMS%2520dataset%2520consists%2520of%25208385%2520pairs%2520of%2520images%250Aand%252082%2520pairs%2520of%2520videos%2520taken%2520from%2520three%2520different%2520cameras%253A%2520Canon%2520D5%2520Mark%2520IV%252C%250AHuawei%2520P20%252C%2520and%2520ZED%2520stereo%2520camera.%2520The%2520dataset%2520includes%2520various%2520indoor%2520and%250Aoutdoor%2520scenes%2520from%2520different%2520locations%2520in%2520Zurich%2520%2528Switzerland%2529%2520and%2520Cheonan%250A%2528South%2520Korea%2529.%2520Some%2520of%2520the%2520computer%2520vision%2520applications%2520that%2520can%2520benefit%2520from%250Athe%2520PIV3CAMS%2520dataset%2520are%2520image/video%2520enhancement%252C%2520view%2520interpolation%252C%2520image%250Amatching%252C%2520and%2520much%2520more.%2520We%2520provide%2520a%2520careful%2520explanation%2520of%2520the%2520data%250Acollection%2520process%2520and%2520detailed%2520analysis%2520of%2520the%2520data.%2520The%2520second%2520part%2520of%2520this%250Athesis%2520studies%2520the%2520usage%2520of%2520depth%2520information%2520in%2520the%2520view%2520synthesizing%2520task.%2520In%250Aaddition%2520to%2520the%2520regeneration%2520of%2520a%2520current%2520state-of-the-art%2520algorithm%252C%2520we%250Ainvestigate%2520several%2520proposed%2520alternative%2520models%2520that%2520integrate%2520depth%250Ainformation%2520geometrically.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520the%250Aeffect%2520of%2520depth%2520is%2520crucial%2520in%2520small%2520view%2520changes.%2520Finally%252C%2520we%2520apply%2520our%2520model%250Ato%2520the%2520introduced%2520PIV3CAMS%2520dataset%2520to%2520synthesize%2520novel%2520target%2520views%2520as%2520an%250Aexample%2520application%2520of%2520PIV3CAMS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIV3CAMS%3A%20a%20multi-camera%20dataset%20for%20multiple%20computer%20vision%20problems%0A%20%20and%20its%20application%20to%20novel%20view-point%20synthesis&entry.906535625=Sohyeong%20Kim%20and%20Martin%20Danelljan%20and%20Radu%20Timofte%20and%20Luc%20Van%20Gool%20and%20Jean-Philippe%20Thiran&entry.1292438233=%20%20The%20modern%20approaches%20for%20computer%20vision%20tasks%20significantly%20rely%20on%20machine%0Alearning%2C%20which%20requires%20a%20large%20number%20of%20quality%20images.%20While%20there%20is%20a%0Aplethora%20of%20image%20datasets%20with%20a%20single%20type%20of%20images%2C%20there%20is%20a%20lack%20of%0Adatasets%20collected%20from%20multiple%20cameras.%20In%20this%20thesis%2C%20we%20introduce%20Paired%0AImage%20and%20Video%20data%20from%20three%20CAMeraS%2C%20namely%20PIV3CAMS%2C%20aimed%20at%20multiple%0Acomputer%20vision%20tasks.%20The%20PIV3CAMS%20dataset%20consists%20of%208385%20pairs%20of%20images%0Aand%2082%20pairs%20of%20videos%20taken%20from%20three%20different%20cameras%3A%20Canon%20D5%20Mark%20IV%2C%0AHuawei%20P20%2C%20and%20ZED%20stereo%20camera.%20The%20dataset%20includes%20various%20indoor%20and%0Aoutdoor%20scenes%20from%20different%20locations%20in%20Zurich%20%28Switzerland%29%20and%20Cheonan%0A%28South%20Korea%29.%20Some%20of%20the%20computer%20vision%20applications%20that%20can%20benefit%20from%0Athe%20PIV3CAMS%20dataset%20are%20image/video%20enhancement%2C%20view%20interpolation%2C%20image%0Amatching%2C%20and%20much%20more.%20We%20provide%20a%20careful%20explanation%20of%20the%20data%0Acollection%20process%20and%20detailed%20analysis%20of%20the%20data.%20The%20second%20part%20of%20this%0Athesis%20studies%20the%20usage%20of%20depth%20information%20in%20the%20view%20synthesizing%20task.%20In%0Aaddition%20to%20the%20regeneration%20of%20a%20current%20state-of-the-art%20algorithm%2C%20we%0Ainvestigate%20several%20proposed%20alternative%20models%20that%20integrate%20depth%0Ainformation%20geometrically.%20Through%20extensive%20experiments%2C%20we%20show%20that%20the%0Aeffect%20of%20depth%20is%20crucial%20in%20small%20view%20changes.%20Finally%2C%20we%20apply%20our%20model%0Ato%20the%20introduced%20PIV3CAMS%20dataset%20to%20synthesize%20novel%20target%20views%20as%20an%0Aexample%20application%20of%20PIV3CAMS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18695v1&entry.124074799=Read"},
{"title": "Relightable Neural Actor with Intrinsic Decomposition and Pose Control", "author": "Diogo Luvizon and Vladislav Golyanik and Adam Kortylewski and Marc Habermann and Christian Theobalt", "abstract": "  Creating a controllable and relightable digital avatar from multi-view video\nwith fixed illumination is a very challenging problem since humans are highly\narticulated, creating pose-dependent appearance effects, and skin as well as\nclothing require space-varying BRDF modeling. Existing works on creating\nanimatible avatars either do not focus on relighting at all, require controlled\nillumination setups, or try to recover a relightable avatar from very low cost\nsetups, i.e. a single RGB video, at the cost of severely limited result\nquality, e.g. shadows not even being modeled. To address this, we propose\nRelightable Neural Actor, a new video-based method for learning a pose-driven\nneural human model that can be relighted, allows appearance editing, and models\npose-dependent effects such as wrinkles and self-shadows. Importantly, for\ntraining, our method solely requires a multi-view recording of the human under\na known, but static lighting condition. To tackle this challenging problem, we\nleverage an implicit geometry representation of the actor with a drivable\ndensity field that models pose-dependent deformations and derive a dynamic\nmapping between 3D and UV spaces, where normal, visibility, and materials are\neffectively encoded. To evaluate our approach in real-world scenarios, we\ncollect a new dataset with four identities recorded under different light\nconditions, indoors and outdoors, providing the first benchmark of its kind for\nhuman relighting, and demonstrating state-of-the-art relighting results for\nnovel human poses.\n", "link": "http://arxiv.org/abs/2312.11587v2", "date": "2024-07-26", "relevancy": 3.0622, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6292}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6041}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relightable%20Neural%20Actor%20with%20Intrinsic%20Decomposition%20and%20Pose%20Control&body=Title%3A%20Relightable%20Neural%20Actor%20with%20Intrinsic%20Decomposition%20and%20Pose%20Control%0AAuthor%3A%20Diogo%20Luvizon%20and%20Vladislav%20Golyanik%20and%20Adam%20Kortylewski%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Creating%20a%20controllable%20and%20relightable%20digital%20avatar%20from%20multi-view%20video%0Awith%20fixed%20illumination%20is%20a%20very%20challenging%20problem%20since%20humans%20are%20highly%0Aarticulated%2C%20creating%20pose-dependent%20appearance%20effects%2C%20and%20skin%20as%20well%20as%0Aclothing%20require%20space-varying%20BRDF%20modeling.%20Existing%20works%20on%20creating%0Aanimatible%20avatars%20either%20do%20not%20focus%20on%20relighting%20at%20all%2C%20require%20controlled%0Aillumination%20setups%2C%20or%20try%20to%20recover%20a%20relightable%20avatar%20from%20very%20low%20cost%0Asetups%2C%20i.e.%20a%20single%20RGB%20video%2C%20at%20the%20cost%20of%20severely%20limited%20result%0Aquality%2C%20e.g.%20shadows%20not%20even%20being%20modeled.%20To%20address%20this%2C%20we%20propose%0ARelightable%20Neural%20Actor%2C%20a%20new%20video-based%20method%20for%20learning%20a%20pose-driven%0Aneural%20human%20model%20that%20can%20be%20relighted%2C%20allows%20appearance%20editing%2C%20and%20models%0Apose-dependent%20effects%20such%20as%20wrinkles%20and%20self-shadows.%20Importantly%2C%20for%0Atraining%2C%20our%20method%20solely%20requires%20a%20multi-view%20recording%20of%20the%20human%20under%0Aa%20known%2C%20but%20static%20lighting%20condition.%20To%20tackle%20this%20challenging%20problem%2C%20we%0Aleverage%20an%20implicit%20geometry%20representation%20of%20the%20actor%20with%20a%20drivable%0Adensity%20field%20that%20models%20pose-dependent%20deformations%20and%20derive%20a%20dynamic%0Amapping%20between%203D%20and%20UV%20spaces%2C%20where%20normal%2C%20visibility%2C%20and%20materials%20are%0Aeffectively%20encoded.%20To%20evaluate%20our%20approach%20in%20real-world%20scenarios%2C%20we%0Acollect%20a%20new%20dataset%20with%20four%20identities%20recorded%20under%20different%20light%0Aconditions%2C%20indoors%20and%20outdoors%2C%20providing%20the%20first%20benchmark%20of%20its%20kind%20for%0Ahuman%20relighting%2C%20and%20demonstrating%20state-of-the-art%20relighting%20results%20for%0Anovel%20human%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelightable%2520Neural%2520Actor%2520with%2520Intrinsic%2520Decomposition%2520and%2520Pose%2520Control%26entry.906535625%3DDiogo%2520Luvizon%2520and%2520Vladislav%2520Golyanik%2520and%2520Adam%2520Kortylewski%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Creating%2520a%2520controllable%2520and%2520relightable%2520digital%2520avatar%2520from%2520multi-view%2520video%250Awith%2520fixed%2520illumination%2520is%2520a%2520very%2520challenging%2520problem%2520since%2520humans%2520are%2520highly%250Aarticulated%252C%2520creating%2520pose-dependent%2520appearance%2520effects%252C%2520and%2520skin%2520as%2520well%2520as%250Aclothing%2520require%2520space-varying%2520BRDF%2520modeling.%2520Existing%2520works%2520on%2520creating%250Aanimatible%2520avatars%2520either%2520do%2520not%2520focus%2520on%2520relighting%2520at%2520all%252C%2520require%2520controlled%250Aillumination%2520setups%252C%2520or%2520try%2520to%2520recover%2520a%2520relightable%2520avatar%2520from%2520very%2520low%2520cost%250Asetups%252C%2520i.e.%2520a%2520single%2520RGB%2520video%252C%2520at%2520the%2520cost%2520of%2520severely%2520limited%2520result%250Aquality%252C%2520e.g.%2520shadows%2520not%2520even%2520being%2520modeled.%2520To%2520address%2520this%252C%2520we%2520propose%250ARelightable%2520Neural%2520Actor%252C%2520a%2520new%2520video-based%2520method%2520for%2520learning%2520a%2520pose-driven%250Aneural%2520human%2520model%2520that%2520can%2520be%2520relighted%252C%2520allows%2520appearance%2520editing%252C%2520and%2520models%250Apose-dependent%2520effects%2520such%2520as%2520wrinkles%2520and%2520self-shadows.%2520Importantly%252C%2520for%250Atraining%252C%2520our%2520method%2520solely%2520requires%2520a%2520multi-view%2520recording%2520of%2520the%2520human%2520under%250Aa%2520known%252C%2520but%2520static%2520lighting%2520condition.%2520To%2520tackle%2520this%2520challenging%2520problem%252C%2520we%250Aleverage%2520an%2520implicit%2520geometry%2520representation%2520of%2520the%2520actor%2520with%2520a%2520drivable%250Adensity%2520field%2520that%2520models%2520pose-dependent%2520deformations%2520and%2520derive%2520a%2520dynamic%250Amapping%2520between%25203D%2520and%2520UV%2520spaces%252C%2520where%2520normal%252C%2520visibility%252C%2520and%2520materials%2520are%250Aeffectively%2520encoded.%2520To%2520evaluate%2520our%2520approach%2520in%2520real-world%2520scenarios%252C%2520we%250Acollect%2520a%2520new%2520dataset%2520with%2520four%2520identities%2520recorded%2520under%2520different%2520light%250Aconditions%252C%2520indoors%2520and%2520outdoors%252C%2520providing%2520the%2520first%2520benchmark%2520of%2520its%2520kind%2520for%250Ahuman%2520relighting%252C%2520and%2520demonstrating%2520state-of-the-art%2520relighting%2520results%2520for%250Anovel%2520human%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relightable%20Neural%20Actor%20with%20Intrinsic%20Decomposition%20and%20Pose%20Control&entry.906535625=Diogo%20Luvizon%20and%20Vladislav%20Golyanik%20and%20Adam%20Kortylewski%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20Creating%20a%20controllable%20and%20relightable%20digital%20avatar%20from%20multi-view%20video%0Awith%20fixed%20illumination%20is%20a%20very%20challenging%20problem%20since%20humans%20are%20highly%0Aarticulated%2C%20creating%20pose-dependent%20appearance%20effects%2C%20and%20skin%20as%20well%20as%0Aclothing%20require%20space-varying%20BRDF%20modeling.%20Existing%20works%20on%20creating%0Aanimatible%20avatars%20either%20do%20not%20focus%20on%20relighting%20at%20all%2C%20require%20controlled%0Aillumination%20setups%2C%20or%20try%20to%20recover%20a%20relightable%20avatar%20from%20very%20low%20cost%0Asetups%2C%20i.e.%20a%20single%20RGB%20video%2C%20at%20the%20cost%20of%20severely%20limited%20result%0Aquality%2C%20e.g.%20shadows%20not%20even%20being%20modeled.%20To%20address%20this%2C%20we%20propose%0ARelightable%20Neural%20Actor%2C%20a%20new%20video-based%20method%20for%20learning%20a%20pose-driven%0Aneural%20human%20model%20that%20can%20be%20relighted%2C%20allows%20appearance%20editing%2C%20and%20models%0Apose-dependent%20effects%20such%20as%20wrinkles%20and%20self-shadows.%20Importantly%2C%20for%0Atraining%2C%20our%20method%20solely%20requires%20a%20multi-view%20recording%20of%20the%20human%20under%0Aa%20known%2C%20but%20static%20lighting%20condition.%20To%20tackle%20this%20challenging%20problem%2C%20we%0Aleverage%20an%20implicit%20geometry%20representation%20of%20the%20actor%20with%20a%20drivable%0Adensity%20field%20that%20models%20pose-dependent%20deformations%20and%20derive%20a%20dynamic%0Amapping%20between%203D%20and%20UV%20spaces%2C%20where%20normal%2C%20visibility%2C%20and%20materials%20are%0Aeffectively%20encoded.%20To%20evaluate%20our%20approach%20in%20real-world%20scenarios%2C%20we%0Acollect%20a%20new%20dataset%20with%20four%20identities%20recorded%20under%20different%20light%0Aconditions%2C%20indoors%20and%20outdoors%2C%20providing%20the%20first%20benchmark%20of%20its%20kind%20for%0Ahuman%20relighting%2C%20and%20demonstrating%20state-of-the-art%20relighting%20results%20for%0Anovel%20human%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11587v2&entry.124074799=Read"},
{"title": "Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud\n  Matching", "author": "Matteo Bastico and Etienne Decenci\u00e8re and Laurent Cort\u00e9 and Yannick Tillier and David Ryckelynck", "abstract": "  Point cloud matching, a crucial technique in computer vision, medical and\nrobotics fields, is primarily concerned with finding correspondences between\npairs of point clouds or voxels. In some practical scenarios, emphasizing local\ndifferences is crucial for accurately identifying a correct match, thereby\nenhancing the overall robustness and reliability of the matching process.\nCommonly used shape descriptors have several limitations and often fail to\nprovide meaningful local insights about the paired geometries. In this work, we\npropose a new technique, based on graph Laplacian eigenmaps, to match point\nclouds by taking into account fine local structures. To deal with the order and\nsign ambiguity of Laplacian eigenmaps, we introduce a new operator, called\nCoupled Laplacian (https://github.com/matteo-bastico/CoupLap), that allows to\neasily generate aligned eigenspaces for multiple registered geometries. We show\nthat the similarity between those aligned high-dimensional spaces provides a\nlocally meaningful score to match shapes. We firstly evaluate the performance\nof the proposed technique in a point-wise manner, focusing on the task of\nobject anomaly localization on the MVTec 3D-AD dataset. Additionally, we define\na new medical task, called automatic Bone Side Estimation (BSE), which we\naddress through a global similarity score derived from coupled eigenspaces. In\norder to test it, we propose a benchmark collecting bone surface structures\nfrom various public datasets. Our matching technique, based on Coupled\nLaplacian, outperforms other methods by reaching an impressive accuracy on both\ntasks.\n", "link": "http://arxiv.org/abs/2402.17372v2", "date": "2024-07-26", "relevancy": 2.9429, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coupled%20Laplacian%20Eigenmaps%20for%20Locally-Aware%203D%20Rigid%20Point%20Cloud%0A%20%20Matching&body=Title%3A%20Coupled%20Laplacian%20Eigenmaps%20for%20Locally-Aware%203D%20Rigid%20Point%20Cloud%0A%20%20Matching%0AAuthor%3A%20Matteo%20Bastico%20and%20Etienne%20Decenci%C3%A8re%20and%20Laurent%20Cort%C3%A9%20and%20Yannick%20Tillier%20and%20David%20Ryckelynck%0AAbstract%3A%20%20%20Point%20cloud%20matching%2C%20a%20crucial%20technique%20in%20computer%20vision%2C%20medical%20and%0Arobotics%20fields%2C%20is%20primarily%20concerned%20with%20finding%20correspondences%20between%0Apairs%20of%20point%20clouds%20or%20voxels.%20In%20some%20practical%20scenarios%2C%20emphasizing%20local%0Adifferences%20is%20crucial%20for%20accurately%20identifying%20a%20correct%20match%2C%20thereby%0Aenhancing%20the%20overall%20robustness%20and%20reliability%20of%20the%20matching%20process.%0ACommonly%20used%20shape%20descriptors%20have%20several%20limitations%20and%20often%20fail%20to%0Aprovide%20meaningful%20local%20insights%20about%20the%20paired%20geometries.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20technique%2C%20based%20on%20graph%20Laplacian%20eigenmaps%2C%20to%20match%20point%0Aclouds%20by%20taking%20into%20account%20fine%20local%20structures.%20To%20deal%20with%20the%20order%20and%0Asign%20ambiguity%20of%20Laplacian%20eigenmaps%2C%20we%20introduce%20a%20new%20operator%2C%20called%0ACoupled%20Laplacian%20%28https%3A//github.com/matteo-bastico/CoupLap%29%2C%20that%20allows%20to%0Aeasily%20generate%20aligned%20eigenspaces%20for%20multiple%20registered%20geometries.%20We%20show%0Athat%20the%20similarity%20between%20those%20aligned%20high-dimensional%20spaces%20provides%20a%0Alocally%20meaningful%20score%20to%20match%20shapes.%20We%20firstly%20evaluate%20the%20performance%0Aof%20the%20proposed%20technique%20in%20a%20point-wise%20manner%2C%20focusing%20on%20the%20task%20of%0Aobject%20anomaly%20localization%20on%20the%20MVTec%203D-AD%20dataset.%20Additionally%2C%20we%20define%0Aa%20new%20medical%20task%2C%20called%20automatic%20Bone%20Side%20Estimation%20%28BSE%29%2C%20which%20we%0Aaddress%20through%20a%20global%20similarity%20score%20derived%20from%20coupled%20eigenspaces.%20In%0Aorder%20to%20test%20it%2C%20we%20propose%20a%20benchmark%20collecting%20bone%20surface%20structures%0Afrom%20various%20public%20datasets.%20Our%20matching%20technique%2C%20based%20on%20Coupled%0ALaplacian%2C%20outperforms%20other%20methods%20by%20reaching%20an%20impressive%20accuracy%20on%20both%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoupled%2520Laplacian%2520Eigenmaps%2520for%2520Locally-Aware%25203D%2520Rigid%2520Point%2520Cloud%250A%2520%2520Matching%26entry.906535625%3DMatteo%2520Bastico%2520and%2520Etienne%2520Decenci%25C3%25A8re%2520and%2520Laurent%2520Cort%25C3%25A9%2520and%2520Yannick%2520Tillier%2520and%2520David%2520Ryckelynck%26entry.1292438233%3D%2520%2520Point%2520cloud%2520matching%252C%2520a%2520crucial%2520technique%2520in%2520computer%2520vision%252C%2520medical%2520and%250Arobotics%2520fields%252C%2520is%2520primarily%2520concerned%2520with%2520finding%2520correspondences%2520between%250Apairs%2520of%2520point%2520clouds%2520or%2520voxels.%2520In%2520some%2520practical%2520scenarios%252C%2520emphasizing%2520local%250Adifferences%2520is%2520crucial%2520for%2520accurately%2520identifying%2520a%2520correct%2520match%252C%2520thereby%250Aenhancing%2520the%2520overall%2520robustness%2520and%2520reliability%2520of%2520the%2520matching%2520process.%250ACommonly%2520used%2520shape%2520descriptors%2520have%2520several%2520limitations%2520and%2520often%2520fail%2520to%250Aprovide%2520meaningful%2520local%2520insights%2520about%2520the%2520paired%2520geometries.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520new%2520technique%252C%2520based%2520on%2520graph%2520Laplacian%2520eigenmaps%252C%2520to%2520match%2520point%250Aclouds%2520by%2520taking%2520into%2520account%2520fine%2520local%2520structures.%2520To%2520deal%2520with%2520the%2520order%2520and%250Asign%2520ambiguity%2520of%2520Laplacian%2520eigenmaps%252C%2520we%2520introduce%2520a%2520new%2520operator%252C%2520called%250ACoupled%2520Laplacian%2520%2528https%253A//github.com/matteo-bastico/CoupLap%2529%252C%2520that%2520allows%2520to%250Aeasily%2520generate%2520aligned%2520eigenspaces%2520for%2520multiple%2520registered%2520geometries.%2520We%2520show%250Athat%2520the%2520similarity%2520between%2520those%2520aligned%2520high-dimensional%2520spaces%2520provides%2520a%250Alocally%2520meaningful%2520score%2520to%2520match%2520shapes.%2520We%2520firstly%2520evaluate%2520the%2520performance%250Aof%2520the%2520proposed%2520technique%2520in%2520a%2520point-wise%2520manner%252C%2520focusing%2520on%2520the%2520task%2520of%250Aobject%2520anomaly%2520localization%2520on%2520the%2520MVTec%25203D-AD%2520dataset.%2520Additionally%252C%2520we%2520define%250Aa%2520new%2520medical%2520task%252C%2520called%2520automatic%2520Bone%2520Side%2520Estimation%2520%2528BSE%2529%252C%2520which%2520we%250Aaddress%2520through%2520a%2520global%2520similarity%2520score%2520derived%2520from%2520coupled%2520eigenspaces.%2520In%250Aorder%2520to%2520test%2520it%252C%2520we%2520propose%2520a%2520benchmark%2520collecting%2520bone%2520surface%2520structures%250Afrom%2520various%2520public%2520datasets.%2520Our%2520matching%2520technique%252C%2520based%2520on%2520Coupled%250ALaplacian%252C%2520outperforms%2520other%2520methods%2520by%2520reaching%2520an%2520impressive%2520accuracy%2520on%2520both%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coupled%20Laplacian%20Eigenmaps%20for%20Locally-Aware%203D%20Rigid%20Point%20Cloud%0A%20%20Matching&entry.906535625=Matteo%20Bastico%20and%20Etienne%20Decenci%C3%A8re%20and%20Laurent%20Cort%C3%A9%20and%20Yannick%20Tillier%20and%20David%20Ryckelynck&entry.1292438233=%20%20Point%20cloud%20matching%2C%20a%20crucial%20technique%20in%20computer%20vision%2C%20medical%20and%0Arobotics%20fields%2C%20is%20primarily%20concerned%20with%20finding%20correspondences%20between%0Apairs%20of%20point%20clouds%20or%20voxels.%20In%20some%20practical%20scenarios%2C%20emphasizing%20local%0Adifferences%20is%20crucial%20for%20accurately%20identifying%20a%20correct%20match%2C%20thereby%0Aenhancing%20the%20overall%20robustness%20and%20reliability%20of%20the%20matching%20process.%0ACommonly%20used%20shape%20descriptors%20have%20several%20limitations%20and%20often%20fail%20to%0Aprovide%20meaningful%20local%20insights%20about%20the%20paired%20geometries.%20In%20this%20work%2C%20we%0Apropose%20a%20new%20technique%2C%20based%20on%20graph%20Laplacian%20eigenmaps%2C%20to%20match%20point%0Aclouds%20by%20taking%20into%20account%20fine%20local%20structures.%20To%20deal%20with%20the%20order%20and%0Asign%20ambiguity%20of%20Laplacian%20eigenmaps%2C%20we%20introduce%20a%20new%20operator%2C%20called%0ACoupled%20Laplacian%20%28https%3A//github.com/matteo-bastico/CoupLap%29%2C%20that%20allows%20to%0Aeasily%20generate%20aligned%20eigenspaces%20for%20multiple%20registered%20geometries.%20We%20show%0Athat%20the%20similarity%20between%20those%20aligned%20high-dimensional%20spaces%20provides%20a%0Alocally%20meaningful%20score%20to%20match%20shapes.%20We%20firstly%20evaluate%20the%20performance%0Aof%20the%20proposed%20technique%20in%20a%20point-wise%20manner%2C%20focusing%20on%20the%20task%20of%0Aobject%20anomaly%20localization%20on%20the%20MVTec%203D-AD%20dataset.%20Additionally%2C%20we%20define%0Aa%20new%20medical%20task%2C%20called%20automatic%20Bone%20Side%20Estimation%20%28BSE%29%2C%20which%20we%0Aaddress%20through%20a%20global%20similarity%20score%20derived%20from%20coupled%20eigenspaces.%20In%0Aorder%20to%20test%20it%2C%20we%20propose%20a%20benchmark%20collecting%20bone%20surface%20structures%0Afrom%20various%20public%20datasets.%20Our%20matching%20technique%2C%20based%20on%20Coupled%0ALaplacian%2C%20outperforms%20other%20methods%20by%20reaching%20an%20impressive%20accuracy%20on%20both%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17372v2&entry.124074799=Read"},
{"title": "Scene Coordinate Reconstruction: Posing of Image Collections via\n  Incremental Learning of a Relocalizer", "author": "Eric Brachmann and Jamie Wynn and Shuai Chen and Tommaso Cavallari and \u00c1ron Monszpart and Daniyar Turmukhambetov and Victor Adrian Prisacariu", "abstract": "  We address the task of estimating camera parameters from a set of images\ndepicting a scene. Popular feature-based structure-from-motion (SfM) tools\nsolve this task by incremental reconstruction: they repeat triangulation of\nsparse 3D points and registration of more camera views to the sparse point\ncloud. We re-interpret incremental structure-from-motion as an iterated\napplication and refinement of a visual relocalizer, that is, of a method that\nregisters new views to the current state of the reconstruction. This\nperspective allows us to investigate alternative visual relocalizers that are\nnot rooted in local feature matching. We show that scene coordinate regression,\na learning-based relocalization approach, allows us to build implicit, neural\nscene representations from unposed images. Different from other learning-based\nreconstruction methods, we do not require pose priors nor sequential inputs,\nand we optimize efficiently over thousands of images. In many cases, our\nmethod, ACE0, estimates camera poses with an accuracy close to feature-based\nSfM, as demonstrated by novel view synthesis. Project page:\nhttps://nianticlabs.github.io/acezero/\n", "link": "http://arxiv.org/abs/2404.14351v2", "date": "2024-07-26", "relevancy": 2.9206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5948}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer&body=Title%3A%20Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer%0AAuthor%3A%20Eric%20Brachmann%20and%20Jamie%20Wynn%20and%20Shuai%20Chen%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Daniyar%20Turmukhambetov%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20We%20address%20the%20task%20of%20estimating%20camera%20parameters%20from%20a%20set%20of%20images%0Adepicting%20a%20scene.%20Popular%20feature-based%20structure-from-motion%20%28SfM%29%20tools%0Asolve%20this%20task%20by%20incremental%20reconstruction%3A%20they%20repeat%20triangulation%20of%0Asparse%203D%20points%20and%20registration%20of%20more%20camera%20views%20to%20the%20sparse%20point%0Acloud.%20We%20re-interpret%20incremental%20structure-from-motion%20as%20an%20iterated%0Aapplication%20and%20refinement%20of%20a%20visual%20relocalizer%2C%20that%20is%2C%20of%20a%20method%20that%0Aregisters%20new%20views%20to%20the%20current%20state%20of%20the%20reconstruction.%20This%0Aperspective%20allows%20us%20to%20investigate%20alternative%20visual%20relocalizers%20that%20are%0Anot%20rooted%20in%20local%20feature%20matching.%20We%20show%20that%20scene%20coordinate%20regression%2C%0Aa%20learning-based%20relocalization%20approach%2C%20allows%20us%20to%20build%20implicit%2C%20neural%0Ascene%20representations%20from%20unposed%20images.%20Different%20from%20other%20learning-based%0Areconstruction%20methods%2C%20we%20do%20not%20require%20pose%20priors%20nor%20sequential%20inputs%2C%0Aand%20we%20optimize%20efficiently%20over%20thousands%20of%20images.%20In%20many%20cases%2C%20our%0Amethod%2C%20ACE0%2C%20estimates%20camera%20poses%20with%20an%20accuracy%20close%20to%20feature-based%0ASfM%2C%20as%20demonstrated%20by%20novel%20view%20synthesis.%20Project%20page%3A%0Ahttps%3A//nianticlabs.github.io/acezero/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Coordinate%2520Reconstruction%253A%2520Posing%2520of%2520Image%2520Collections%2520via%250A%2520%2520Incremental%2520Learning%2520of%2520a%2520Relocalizer%26entry.906535625%3DEric%2520Brachmann%2520and%2520Jamie%2520Wynn%2520and%2520Shuai%2520Chen%2520and%2520Tommaso%2520Cavallari%2520and%2520%25C3%2581ron%2520Monszpart%2520and%2520Daniyar%2520Turmukhambetov%2520and%2520Victor%2520Adrian%2520Prisacariu%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520task%2520of%2520estimating%2520camera%2520parameters%2520from%2520a%2520set%2520of%2520images%250Adepicting%2520a%2520scene.%2520Popular%2520feature-based%2520structure-from-motion%2520%2528SfM%2529%2520tools%250Asolve%2520this%2520task%2520by%2520incremental%2520reconstruction%253A%2520they%2520repeat%2520triangulation%2520of%250Asparse%25203D%2520points%2520and%2520registration%2520of%2520more%2520camera%2520views%2520to%2520the%2520sparse%2520point%250Acloud.%2520We%2520re-interpret%2520incremental%2520structure-from-motion%2520as%2520an%2520iterated%250Aapplication%2520and%2520refinement%2520of%2520a%2520visual%2520relocalizer%252C%2520that%2520is%252C%2520of%2520a%2520method%2520that%250Aregisters%2520new%2520views%2520to%2520the%2520current%2520state%2520of%2520the%2520reconstruction.%2520This%250Aperspective%2520allows%2520us%2520to%2520investigate%2520alternative%2520visual%2520relocalizers%2520that%2520are%250Anot%2520rooted%2520in%2520local%2520feature%2520matching.%2520We%2520show%2520that%2520scene%2520coordinate%2520regression%252C%250Aa%2520learning-based%2520relocalization%2520approach%252C%2520allows%2520us%2520to%2520build%2520implicit%252C%2520neural%250Ascene%2520representations%2520from%2520unposed%2520images.%2520Different%2520from%2520other%2520learning-based%250Areconstruction%2520methods%252C%2520we%2520do%2520not%2520require%2520pose%2520priors%2520nor%2520sequential%2520inputs%252C%250Aand%2520we%2520optimize%2520efficiently%2520over%2520thousands%2520of%2520images.%2520In%2520many%2520cases%252C%2520our%250Amethod%252C%2520ACE0%252C%2520estimates%2520camera%2520poses%2520with%2520an%2520accuracy%2520close%2520to%2520feature-based%250ASfM%252C%2520as%2520demonstrated%2520by%2520novel%2520view%2520synthesis.%2520Project%2520page%253A%250Ahttps%253A//nianticlabs.github.io/acezero/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Coordinate%20Reconstruction%3A%20Posing%20of%20Image%20Collections%20via%0A%20%20Incremental%20Learning%20of%20a%20Relocalizer&entry.906535625=Eric%20Brachmann%20and%20Jamie%20Wynn%20and%20Shuai%20Chen%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Daniyar%20Turmukhambetov%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20We%20address%20the%20task%20of%20estimating%20camera%20parameters%20from%20a%20set%20of%20images%0Adepicting%20a%20scene.%20Popular%20feature-based%20structure-from-motion%20%28SfM%29%20tools%0Asolve%20this%20task%20by%20incremental%20reconstruction%3A%20they%20repeat%20triangulation%20of%0Asparse%203D%20points%20and%20registration%20of%20more%20camera%20views%20to%20the%20sparse%20point%0Acloud.%20We%20re-interpret%20incremental%20structure-from-motion%20as%20an%20iterated%0Aapplication%20and%20refinement%20of%20a%20visual%20relocalizer%2C%20that%20is%2C%20of%20a%20method%20that%0Aregisters%20new%20views%20to%20the%20current%20state%20of%20the%20reconstruction.%20This%0Aperspective%20allows%20us%20to%20investigate%20alternative%20visual%20relocalizers%20that%20are%0Anot%20rooted%20in%20local%20feature%20matching.%20We%20show%20that%20scene%20coordinate%20regression%2C%0Aa%20learning-based%20relocalization%20approach%2C%20allows%20us%20to%20build%20implicit%2C%20neural%0Ascene%20representations%20from%20unposed%20images.%20Different%20from%20other%20learning-based%0Areconstruction%20methods%2C%20we%20do%20not%20require%20pose%20priors%20nor%20sequential%20inputs%2C%0Aand%20we%20optimize%20efficiently%20over%20thousands%20of%20images.%20In%20many%20cases%2C%20our%0Amethod%2C%20ACE0%2C%20estimates%20camera%20poses%20with%20an%20accuracy%20close%20to%20feature-based%0ASfM%2C%20as%20demonstrated%20by%20novel%20view%20synthesis.%20Project%20page%3A%0Ahttps%3A//nianticlabs.github.io/acezero/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14351v2&entry.124074799=Read"},
{"title": "CGGM: A conditional graph generation model with adaptive sparsity for\n  node anomaly detection in IoT networks", "author": "Xianshi Su and Munan Li and Tongbang Jiang and Hao Long", "abstract": "  Dynamic graphs are extensively employed for detecting anomalous behavior in\nnodes within the Internet of Things (IoT). Generative models are often used to\naddress the issue of imbalanced node categories in dynamic graphs.\nNevertheless, the constraints it faces include the monotonicity of adjacency\nrelationships, the difficulty in constructing multi-dimensional features for\nnodes, and the lack of a method for end-to-end generation of multiple\ncategories of nodes. This paper presents a novel graph generation model, called\nCGGM, designed specifically to generate a larger number of nodes belonging to\nthe minority class. The mechanism for generating an adjacency matrix, through\nadaptive sparsity, enhances flexibility in its structure. The feature\ngeneration module, called multidimensional features generator (MFG) to generate\nnode features along with topological information. Labels are transformed into\nembedding vectors, serving as conditional constraints to control the generation\nof synthetic data across multiple categories. Using a multi-stage loss, the\ndistribution of synthetic data is adjusted to closely resemble that of real\ndata. In extensive experiments, we show that CGGM's synthetic data outperforms\nstate-of-the-art methods across various metrics. Our results demonstrate\nefficient generation of diverse data categories, robustly enhancing\nmulti-category classification model performance.\n", "link": "http://arxiv.org/abs/2402.17363v2", "date": "2024-07-26", "relevancy": 2.846, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5806}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5676}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGGM%3A%20A%20conditional%20graph%20generation%20model%20with%20adaptive%20sparsity%20for%0A%20%20node%20anomaly%20detection%20in%20IoT%20networks&body=Title%3A%20CGGM%3A%20A%20conditional%20graph%20generation%20model%20with%20adaptive%20sparsity%20for%0A%20%20node%20anomaly%20detection%20in%20IoT%20networks%0AAuthor%3A%20Xianshi%20Su%20and%20Munan%20Li%20and%20Tongbang%20Jiang%20and%20Hao%20Long%0AAbstract%3A%20%20%20Dynamic%20graphs%20are%20extensively%20employed%20for%20detecting%20anomalous%20behavior%20in%0Anodes%20within%20the%20Internet%20of%20Things%20%28IoT%29.%20Generative%20models%20are%20often%20used%20to%0Aaddress%20the%20issue%20of%20imbalanced%20node%20categories%20in%20dynamic%20graphs.%0ANevertheless%2C%20the%20constraints%20it%20faces%20include%20the%20monotonicity%20of%20adjacency%0Arelationships%2C%20the%20difficulty%20in%20constructing%20multi-dimensional%20features%20for%0Anodes%2C%20and%20the%20lack%20of%20a%20method%20for%20end-to-end%20generation%20of%20multiple%0Acategories%20of%20nodes.%20This%20paper%20presents%20a%20novel%20graph%20generation%20model%2C%20called%0ACGGM%2C%20designed%20specifically%20to%20generate%20a%20larger%20number%20of%20nodes%20belonging%20to%0Athe%20minority%20class.%20The%20mechanism%20for%20generating%20an%20adjacency%20matrix%2C%20through%0Aadaptive%20sparsity%2C%20enhances%20flexibility%20in%20its%20structure.%20The%20feature%0Ageneration%20module%2C%20called%20multidimensional%20features%20generator%20%28MFG%29%20to%20generate%0Anode%20features%20along%20with%20topological%20information.%20Labels%20are%20transformed%20into%0Aembedding%20vectors%2C%20serving%20as%20conditional%20constraints%20to%20control%20the%20generation%0Aof%20synthetic%20data%20across%20multiple%20categories.%20Using%20a%20multi-stage%20loss%2C%20the%0Adistribution%20of%20synthetic%20data%20is%20adjusted%20to%20closely%20resemble%20that%20of%20real%0Adata.%20In%20extensive%20experiments%2C%20we%20show%20that%20CGGM%27s%20synthetic%20data%20outperforms%0Astate-of-the-art%20methods%20across%20various%20metrics.%20Our%20results%20demonstrate%0Aefficient%20generation%20of%20diverse%20data%20categories%2C%20robustly%20enhancing%0Amulti-category%20classification%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGGM%253A%2520A%2520conditional%2520graph%2520generation%2520model%2520with%2520adaptive%2520sparsity%2520for%250A%2520%2520node%2520anomaly%2520detection%2520in%2520IoT%2520networks%26entry.906535625%3DXianshi%2520Su%2520and%2520Munan%2520Li%2520and%2520Tongbang%2520Jiang%2520and%2520Hao%2520Long%26entry.1292438233%3D%2520%2520Dynamic%2520graphs%2520are%2520extensively%2520employed%2520for%2520detecting%2520anomalous%2520behavior%2520in%250Anodes%2520within%2520the%2520Internet%2520of%2520Things%2520%2528IoT%2529.%2520Generative%2520models%2520are%2520often%2520used%2520to%250Aaddress%2520the%2520issue%2520of%2520imbalanced%2520node%2520categories%2520in%2520dynamic%2520graphs.%250ANevertheless%252C%2520the%2520constraints%2520it%2520faces%2520include%2520the%2520monotonicity%2520of%2520adjacency%250Arelationships%252C%2520the%2520difficulty%2520in%2520constructing%2520multi-dimensional%2520features%2520for%250Anodes%252C%2520and%2520the%2520lack%2520of%2520a%2520method%2520for%2520end-to-end%2520generation%2520of%2520multiple%250Acategories%2520of%2520nodes.%2520This%2520paper%2520presents%2520a%2520novel%2520graph%2520generation%2520model%252C%2520called%250ACGGM%252C%2520designed%2520specifically%2520to%2520generate%2520a%2520larger%2520number%2520of%2520nodes%2520belonging%2520to%250Athe%2520minority%2520class.%2520The%2520mechanism%2520for%2520generating%2520an%2520adjacency%2520matrix%252C%2520through%250Aadaptive%2520sparsity%252C%2520enhances%2520flexibility%2520in%2520its%2520structure.%2520The%2520feature%250Ageneration%2520module%252C%2520called%2520multidimensional%2520features%2520generator%2520%2528MFG%2529%2520to%2520generate%250Anode%2520features%2520along%2520with%2520topological%2520information.%2520Labels%2520are%2520transformed%2520into%250Aembedding%2520vectors%252C%2520serving%2520as%2520conditional%2520constraints%2520to%2520control%2520the%2520generation%250Aof%2520synthetic%2520data%2520across%2520multiple%2520categories.%2520Using%2520a%2520multi-stage%2520loss%252C%2520the%250Adistribution%2520of%2520synthetic%2520data%2520is%2520adjusted%2520to%2520closely%2520resemble%2520that%2520of%2520real%250Adata.%2520In%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520CGGM%2527s%2520synthetic%2520data%2520outperforms%250Astate-of-the-art%2520methods%2520across%2520various%2520metrics.%2520Our%2520results%2520demonstrate%250Aefficient%2520generation%2520of%2520diverse%2520data%2520categories%252C%2520robustly%2520enhancing%250Amulti-category%2520classification%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGGM%3A%20A%20conditional%20graph%20generation%20model%20with%20adaptive%20sparsity%20for%0A%20%20node%20anomaly%20detection%20in%20IoT%20networks&entry.906535625=Xianshi%20Su%20and%20Munan%20Li%20and%20Tongbang%20Jiang%20and%20Hao%20Long&entry.1292438233=%20%20Dynamic%20graphs%20are%20extensively%20employed%20for%20detecting%20anomalous%20behavior%20in%0Anodes%20within%20the%20Internet%20of%20Things%20%28IoT%29.%20Generative%20models%20are%20often%20used%20to%0Aaddress%20the%20issue%20of%20imbalanced%20node%20categories%20in%20dynamic%20graphs.%0ANevertheless%2C%20the%20constraints%20it%20faces%20include%20the%20monotonicity%20of%20adjacency%0Arelationships%2C%20the%20difficulty%20in%20constructing%20multi-dimensional%20features%20for%0Anodes%2C%20and%20the%20lack%20of%20a%20method%20for%20end-to-end%20generation%20of%20multiple%0Acategories%20of%20nodes.%20This%20paper%20presents%20a%20novel%20graph%20generation%20model%2C%20called%0ACGGM%2C%20designed%20specifically%20to%20generate%20a%20larger%20number%20of%20nodes%20belonging%20to%0Athe%20minority%20class.%20The%20mechanism%20for%20generating%20an%20adjacency%20matrix%2C%20through%0Aadaptive%20sparsity%2C%20enhances%20flexibility%20in%20its%20structure.%20The%20feature%0Ageneration%20module%2C%20called%20multidimensional%20features%20generator%20%28MFG%29%20to%20generate%0Anode%20features%20along%20with%20topological%20information.%20Labels%20are%20transformed%20into%0Aembedding%20vectors%2C%20serving%20as%20conditional%20constraints%20to%20control%20the%20generation%0Aof%20synthetic%20data%20across%20multiple%20categories.%20Using%20a%20multi-stage%20loss%2C%20the%0Adistribution%20of%20synthetic%20data%20is%20adjusted%20to%20closely%20resemble%20that%20of%20real%0Adata.%20In%20extensive%20experiments%2C%20we%20show%20that%20CGGM%27s%20synthetic%20data%20outperforms%0Astate-of-the-art%20methods%20across%20various%20metrics.%20Our%20results%20demonstrate%0Aefficient%20generation%20of%20diverse%20data%20categories%2C%20robustly%20enhancing%0Amulti-category%20classification%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17363v2&entry.124074799=Read"},
{"title": "Floating No More: Object-Ground Reconstruction from a Single Image", "author": "Yunze Man and Yichen Sheng and Jianming Zhang and Liang-Yan Gui and Yu-Xiong Wang", "abstract": "  Recent advancements in 3D object reconstruction from single images have\nprimarily focused on improving the accuracy of object shapes. Yet, these\ntechniques often fail to accurately capture the inter-relation between the\nobject, ground, and camera. As a result, the reconstructed objects often appear\nfloating or tilted when placed on flat surfaces. This limitation significantly\naffects 3D-aware image editing applications like shadow rendering and object\npose manipulation. To address this issue, we introduce ORG (Object\nReconstruction with Ground), a novel task aimed at reconstructing 3D object\ngeometry in conjunction with the ground surface. Our method uses two compact\npixel-level representations to depict the relationship between camera, object,\nand ground. Experiments show that the proposed ORG model can effectively\nreconstruct object-ground geometry on unseen data, significantly enhancing the\nquality of shadow generation and pose manipulation compared to conventional\nsingle-image 3D reconstruction techniques.\n", "link": "http://arxiv.org/abs/2407.18914v1", "date": "2024-07-26", "relevancy": 2.8148, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5882}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5538}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Floating%20No%20More%3A%20Object-Ground%20Reconstruction%20from%20a%20Single%20Image&body=Title%3A%20Floating%20No%20More%3A%20Object-Ground%20Reconstruction%20from%20a%20Single%20Image%0AAuthor%3A%20Yunze%20Man%20and%20Yichen%20Sheng%20and%20Jianming%20Zhang%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20object%20reconstruction%20from%20single%20images%20have%0Aprimarily%20focused%20on%20improving%20the%20accuracy%20of%20object%20shapes.%20Yet%2C%20these%0Atechniques%20often%20fail%20to%20accurately%20capture%20the%20inter-relation%20between%20the%0Aobject%2C%20ground%2C%20and%20camera.%20As%20a%20result%2C%20the%20reconstructed%20objects%20often%20appear%0Afloating%20or%20tilted%20when%20placed%20on%20flat%20surfaces.%20This%20limitation%20significantly%0Aaffects%203D-aware%20image%20editing%20applications%20like%20shadow%20rendering%20and%20object%0Apose%20manipulation.%20To%20address%20this%20issue%2C%20we%20introduce%20ORG%20%28Object%0AReconstruction%20with%20Ground%29%2C%20a%20novel%20task%20aimed%20at%20reconstructing%203D%20object%0Ageometry%20in%20conjunction%20with%20the%20ground%20surface.%20Our%20method%20uses%20two%20compact%0Apixel-level%20representations%20to%20depict%20the%20relationship%20between%20camera%2C%20object%2C%0Aand%20ground.%20Experiments%20show%20that%20the%20proposed%20ORG%20model%20can%20effectively%0Areconstruct%20object-ground%20geometry%20on%20unseen%20data%2C%20significantly%20enhancing%20the%0Aquality%20of%20shadow%20generation%20and%20pose%20manipulation%20compared%20to%20conventional%0Asingle-image%203D%20reconstruction%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloating%2520No%2520More%253A%2520Object-Ground%2520Reconstruction%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYunze%2520Man%2520and%2520Yichen%2520Sheng%2520and%2520Jianming%2520Zhang%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520object%2520reconstruction%2520from%2520single%2520images%2520have%250Aprimarily%2520focused%2520on%2520improving%2520the%2520accuracy%2520of%2520object%2520shapes.%2520Yet%252C%2520these%250Atechniques%2520often%2520fail%2520to%2520accurately%2520capture%2520the%2520inter-relation%2520between%2520the%250Aobject%252C%2520ground%252C%2520and%2520camera.%2520As%2520a%2520result%252C%2520the%2520reconstructed%2520objects%2520often%2520appear%250Afloating%2520or%2520tilted%2520when%2520placed%2520on%2520flat%2520surfaces.%2520This%2520limitation%2520significantly%250Aaffects%25203D-aware%2520image%2520editing%2520applications%2520like%2520shadow%2520rendering%2520and%2520object%250Apose%2520manipulation.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520ORG%2520%2528Object%250AReconstruction%2520with%2520Ground%2529%252C%2520a%2520novel%2520task%2520aimed%2520at%2520reconstructing%25203D%2520object%250Ageometry%2520in%2520conjunction%2520with%2520the%2520ground%2520surface.%2520Our%2520method%2520uses%2520two%2520compact%250Apixel-level%2520representations%2520to%2520depict%2520the%2520relationship%2520between%2520camera%252C%2520object%252C%250Aand%2520ground.%2520Experiments%2520show%2520that%2520the%2520proposed%2520ORG%2520model%2520can%2520effectively%250Areconstruct%2520object-ground%2520geometry%2520on%2520unseen%2520data%252C%2520significantly%2520enhancing%2520the%250Aquality%2520of%2520shadow%2520generation%2520and%2520pose%2520manipulation%2520compared%2520to%2520conventional%250Asingle-image%25203D%2520reconstruction%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Floating%20No%20More%3A%20Object-Ground%20Reconstruction%20from%20a%20Single%20Image&entry.906535625=Yunze%20Man%20and%20Yichen%20Sheng%20and%20Jianming%20Zhang%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%203D%20object%20reconstruction%20from%20single%20images%20have%0Aprimarily%20focused%20on%20improving%20the%20accuracy%20of%20object%20shapes.%20Yet%2C%20these%0Atechniques%20often%20fail%20to%20accurately%20capture%20the%20inter-relation%20between%20the%0Aobject%2C%20ground%2C%20and%20camera.%20As%20a%20result%2C%20the%20reconstructed%20objects%20often%20appear%0Afloating%20or%20tilted%20when%20placed%20on%20flat%20surfaces.%20This%20limitation%20significantly%0Aaffects%203D-aware%20image%20editing%20applications%20like%20shadow%20rendering%20and%20object%0Apose%20manipulation.%20To%20address%20this%20issue%2C%20we%20introduce%20ORG%20%28Object%0AReconstruction%20with%20Ground%29%2C%20a%20novel%20task%20aimed%20at%20reconstructing%203D%20object%0Ageometry%20in%20conjunction%20with%20the%20ground%20surface.%20Our%20method%20uses%20two%20compact%0Apixel-level%20representations%20to%20depict%20the%20relationship%20between%20camera%2C%20object%2C%0Aand%20ground.%20Experiments%20show%20that%20the%20proposed%20ORG%20model%20can%20effectively%0Areconstruct%20object-ground%20geometry%20on%20unseen%20data%2C%20significantly%20enhancing%20the%0Aquality%20of%20shadow%20generation%20and%20pose%20manipulation%20compared%20to%20conventional%0Asingle-image%203D%20reconstruction%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18914v1&entry.124074799=Read"},
{"title": "DynamicTrack: Advancing Gigapixel Tracking in Crowded Scenes", "author": "Yunqi Zhao and Yuchen Guo and Zheng Cao and Kai Ni and Ruqi Huang and Lu Fang", "abstract": "  Tracking in gigapixel scenarios holds numerous potential applications in\nvideo surveillance and pedestrian analysis. Existing algorithms attempt to\nperform tracking in crowded scenes by utilizing multiple cameras or group\nrelationships. However, their performance significantly degrades when\nconfronted with complex interaction and occlusion inherent in gigapixel images.\nIn this paper, we introduce DynamicTrack, a dynamic tracking framework designed\nto address gigapixel tracking challenges in crowded scenes. In particular, we\npropose a dynamic detector that utilizes contrastive learning to jointly detect\nthe head and body of pedestrians. Building upon this, we design a dynamic\nassociation algorithm that effectively utilizes head and body information for\nmatching purposes. Extensive experiments show that our tracker achieves\nstate-of-the-art performance on widely used tracking benchmarks specifically\ndesigned for gigapixel crowded scenes.\n", "link": "http://arxiv.org/abs/2407.18637v1", "date": "2024-07-26", "relevancy": 2.8097, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5777}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.572}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicTrack%3A%20Advancing%20Gigapixel%20Tracking%20in%20Crowded%20Scenes&body=Title%3A%20DynamicTrack%3A%20Advancing%20Gigapixel%20Tracking%20in%20Crowded%20Scenes%0AAuthor%3A%20Yunqi%20Zhao%20and%20Yuchen%20Guo%20and%20Zheng%20Cao%20and%20Kai%20Ni%20and%20Ruqi%20Huang%20and%20Lu%20Fang%0AAbstract%3A%20%20%20Tracking%20in%20gigapixel%20scenarios%20holds%20numerous%20potential%20applications%20in%0Avideo%20surveillance%20and%20pedestrian%20analysis.%20Existing%20algorithms%20attempt%20to%0Aperform%20tracking%20in%20crowded%20scenes%20by%20utilizing%20multiple%20cameras%20or%20group%0Arelationships.%20However%2C%20their%20performance%20significantly%20degrades%20when%0Aconfronted%20with%20complex%20interaction%20and%20occlusion%20inherent%20in%20gigapixel%20images.%0AIn%20this%20paper%2C%20we%20introduce%20DynamicTrack%2C%20a%20dynamic%20tracking%20framework%20designed%0Ato%20address%20gigapixel%20tracking%20challenges%20in%20crowded%20scenes.%20In%20particular%2C%20we%0Apropose%20a%20dynamic%20detector%20that%20utilizes%20contrastive%20learning%20to%20jointly%20detect%0Athe%20head%20and%20body%20of%20pedestrians.%20Building%20upon%20this%2C%20we%20design%20a%20dynamic%0Aassociation%20algorithm%20that%20effectively%20utilizes%20head%20and%20body%20information%20for%0Amatching%20purposes.%20Extensive%20experiments%20show%20that%20our%20tracker%20achieves%0Astate-of-the-art%20performance%20on%20widely%20used%20tracking%20benchmarks%20specifically%0Adesigned%20for%20gigapixel%20crowded%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicTrack%253A%2520Advancing%2520Gigapixel%2520Tracking%2520in%2520Crowded%2520Scenes%26entry.906535625%3DYunqi%2520Zhao%2520and%2520Yuchen%2520Guo%2520and%2520Zheng%2520Cao%2520and%2520Kai%2520Ni%2520and%2520Ruqi%2520Huang%2520and%2520Lu%2520Fang%26entry.1292438233%3D%2520%2520Tracking%2520in%2520gigapixel%2520scenarios%2520holds%2520numerous%2520potential%2520applications%2520in%250Avideo%2520surveillance%2520and%2520pedestrian%2520analysis.%2520Existing%2520algorithms%2520attempt%2520to%250Aperform%2520tracking%2520in%2520crowded%2520scenes%2520by%2520utilizing%2520multiple%2520cameras%2520or%2520group%250Arelationships.%2520However%252C%2520their%2520performance%2520significantly%2520degrades%2520when%250Aconfronted%2520with%2520complex%2520interaction%2520and%2520occlusion%2520inherent%2520in%2520gigapixel%2520images.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520DynamicTrack%252C%2520a%2520dynamic%2520tracking%2520framework%2520designed%250Ato%2520address%2520gigapixel%2520tracking%2520challenges%2520in%2520crowded%2520scenes.%2520In%2520particular%252C%2520we%250Apropose%2520a%2520dynamic%2520detector%2520that%2520utilizes%2520contrastive%2520learning%2520to%2520jointly%2520detect%250Athe%2520head%2520and%2520body%2520of%2520pedestrians.%2520Building%2520upon%2520this%252C%2520we%2520design%2520a%2520dynamic%250Aassociation%2520algorithm%2520that%2520effectively%2520utilizes%2520head%2520and%2520body%2520information%2520for%250Amatching%2520purposes.%2520Extensive%2520experiments%2520show%2520that%2520our%2520tracker%2520achieves%250Astate-of-the-art%2520performance%2520on%2520widely%2520used%2520tracking%2520benchmarks%2520specifically%250Adesigned%2520for%2520gigapixel%2520crowded%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicTrack%3A%20Advancing%20Gigapixel%20Tracking%20in%20Crowded%20Scenes&entry.906535625=Yunqi%20Zhao%20and%20Yuchen%20Guo%20and%20Zheng%20Cao%20and%20Kai%20Ni%20and%20Ruqi%20Huang%20and%20Lu%20Fang&entry.1292438233=%20%20Tracking%20in%20gigapixel%20scenarios%20holds%20numerous%20potential%20applications%20in%0Avideo%20surveillance%20and%20pedestrian%20analysis.%20Existing%20algorithms%20attempt%20to%0Aperform%20tracking%20in%20crowded%20scenes%20by%20utilizing%20multiple%20cameras%20or%20group%0Arelationships.%20However%2C%20their%20performance%20significantly%20degrades%20when%0Aconfronted%20with%20complex%20interaction%20and%20occlusion%20inherent%20in%20gigapixel%20images.%0AIn%20this%20paper%2C%20we%20introduce%20DynamicTrack%2C%20a%20dynamic%20tracking%20framework%20designed%0Ato%20address%20gigapixel%20tracking%20challenges%20in%20crowded%20scenes.%20In%20particular%2C%20we%0Apropose%20a%20dynamic%20detector%20that%20utilizes%20contrastive%20learning%20to%20jointly%20detect%0Athe%20head%20and%20body%20of%20pedestrians.%20Building%20upon%20this%2C%20we%20design%20a%20dynamic%0Aassociation%20algorithm%20that%20effectively%20utilizes%20head%20and%20body%20information%20for%0Amatching%20purposes.%20Extensive%20experiments%20show%20that%20our%20tracker%20achieves%0Astate-of-the-art%20performance%20on%20widely%20used%20tracking%20benchmarks%20specifically%0Adesigned%20for%20gigapixel%20crowded%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18637v1&entry.124074799=Read"},
{"title": "Divide and Conquer: A Systematic Approach for Industrial Scale\n  High-Definition OpenDRIVE Generation from Sparse Point Clouds", "author": "Leon Eisemann and Johannes Maucher", "abstract": "  High-definition road maps play a crucial role in the functionality and\nverification of highly automated driving functions. These contain precise\ninformation about the road network, geometry, condition, as well as traffic\nsigns. Despite their importance for the development and evaluation of driving\nfunctions, the generation of high-definition maps is still an ongoing research\ntopic. While previous work in this area has primarily focused on the accuracy\nof road geometry, we present a novel approach for automated large-scale map\ngeneration for use in industrial applications. Our proposed method leverages a\nminimal number of external information about the road to process LiDAR data in\nsegments. These segments are subsequently combined, enabling a flexible and\nscalable process that achieves high-definition accuracy. Additionally, we\nshowcase the use of the resulting OpenDRIVE in driving function simulation.\n", "link": "http://arxiv.org/abs/2407.18703v1", "date": "2024-07-26", "relevancy": 2.8039, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5718}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5552}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Conquer%3A%20A%20Systematic%20Approach%20for%20Industrial%20Scale%0A%20%20High-Definition%20OpenDRIVE%20Generation%20from%20Sparse%20Point%20Clouds&body=Title%3A%20Divide%20and%20Conquer%3A%20A%20Systematic%20Approach%20for%20Industrial%20Scale%0A%20%20High-Definition%20OpenDRIVE%20Generation%20from%20Sparse%20Point%20Clouds%0AAuthor%3A%20Leon%20Eisemann%20and%20Johannes%20Maucher%0AAbstract%3A%20%20%20High-definition%20road%20maps%20play%20a%20crucial%20role%20in%20the%20functionality%20and%0Averification%20of%20highly%20automated%20driving%20functions.%20These%20contain%20precise%0Ainformation%20about%20the%20road%20network%2C%20geometry%2C%20condition%2C%20as%20well%20as%20traffic%0Asigns.%20Despite%20their%20importance%20for%20the%20development%20and%20evaluation%20of%20driving%0Afunctions%2C%20the%20generation%20of%20high-definition%20maps%20is%20still%20an%20ongoing%20research%0Atopic.%20While%20previous%20work%20in%20this%20area%20has%20primarily%20focused%20on%20the%20accuracy%0Aof%20road%20geometry%2C%20we%20present%20a%20novel%20approach%20for%20automated%20large-scale%20map%0Ageneration%20for%20use%20in%20industrial%20applications.%20Our%20proposed%20method%20leverages%20a%0Aminimal%20number%20of%20external%20information%20about%20the%20road%20to%20process%20LiDAR%20data%20in%0Asegments.%20These%20segments%20are%20subsequently%20combined%2C%20enabling%20a%20flexible%20and%0Ascalable%20process%20that%20achieves%20high-definition%20accuracy.%20Additionally%2C%20we%0Ashowcase%20the%20use%20of%20the%20resulting%20OpenDRIVE%20in%20driving%20function%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Conquer%253A%2520A%2520Systematic%2520Approach%2520for%2520Industrial%2520Scale%250A%2520%2520High-Definition%2520OpenDRIVE%2520Generation%2520from%2520Sparse%2520Point%2520Clouds%26entry.906535625%3DLeon%2520Eisemann%2520and%2520Johannes%2520Maucher%26entry.1292438233%3D%2520%2520High-definition%2520road%2520maps%2520play%2520a%2520crucial%2520role%2520in%2520the%2520functionality%2520and%250Averification%2520of%2520highly%2520automated%2520driving%2520functions.%2520These%2520contain%2520precise%250Ainformation%2520about%2520the%2520road%2520network%252C%2520geometry%252C%2520condition%252C%2520as%2520well%2520as%2520traffic%250Asigns.%2520Despite%2520their%2520importance%2520for%2520the%2520development%2520and%2520evaluation%2520of%2520driving%250Afunctions%252C%2520the%2520generation%2520of%2520high-definition%2520maps%2520is%2520still%2520an%2520ongoing%2520research%250Atopic.%2520While%2520previous%2520work%2520in%2520this%2520area%2520has%2520primarily%2520focused%2520on%2520the%2520accuracy%250Aof%2520road%2520geometry%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520automated%2520large-scale%2520map%250Ageneration%2520for%2520use%2520in%2520industrial%2520applications.%2520Our%2520proposed%2520method%2520leverages%2520a%250Aminimal%2520number%2520of%2520external%2520information%2520about%2520the%2520road%2520to%2520process%2520LiDAR%2520data%2520in%250Asegments.%2520These%2520segments%2520are%2520subsequently%2520combined%252C%2520enabling%2520a%2520flexible%2520and%250Ascalable%2520process%2520that%2520achieves%2520high-definition%2520accuracy.%2520Additionally%252C%2520we%250Ashowcase%2520the%2520use%2520of%2520the%2520resulting%2520OpenDRIVE%2520in%2520driving%2520function%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Conquer%3A%20A%20Systematic%20Approach%20for%20Industrial%20Scale%0A%20%20High-Definition%20OpenDRIVE%20Generation%20from%20Sparse%20Point%20Clouds&entry.906535625=Leon%20Eisemann%20and%20Johannes%20Maucher&entry.1292438233=%20%20High-definition%20road%20maps%20play%20a%20crucial%20role%20in%20the%20functionality%20and%0Averification%20of%20highly%20automated%20driving%20functions.%20These%20contain%20precise%0Ainformation%20about%20the%20road%20network%2C%20geometry%2C%20condition%2C%20as%20well%20as%20traffic%0Asigns.%20Despite%20their%20importance%20for%20the%20development%20and%20evaluation%20of%20driving%0Afunctions%2C%20the%20generation%20of%20high-definition%20maps%20is%20still%20an%20ongoing%20research%0Atopic.%20While%20previous%20work%20in%20this%20area%20has%20primarily%20focused%20on%20the%20accuracy%0Aof%20road%20geometry%2C%20we%20present%20a%20novel%20approach%20for%20automated%20large-scale%20map%0Ageneration%20for%20use%20in%20industrial%20applications.%20Our%20proposed%20method%20leverages%20a%0Aminimal%20number%20of%20external%20information%20about%20the%20road%20to%20process%20LiDAR%20data%20in%0Asegments.%20These%20segments%20are%20subsequently%20combined%2C%20enabling%20a%20flexible%20and%0Ascalable%20process%20that%20achieves%20high-definition%20accuracy.%20Additionally%2C%20we%0Ashowcase%20the%20use%20of%20the%20resulting%20OpenDRIVE%20in%20driving%20function%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18703v1&entry.124074799=Read"},
{"title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision", "author": "Aleksandar Shtedritski and Christian Rupprecht and Andrea Vedaldi", "abstract": "  Canonical surface mapping generalizes keypoint detection by assigning each\npixel of an object to a corresponding point in a 3D template. Popularised by\nDensePose for the analysis of humans, authors have since attempted to apply the\nconcept to more categories, but with limited success due to the high cost of\nmanual supervision. In this work, we introduce SHIC, a method to learn\ncanonical maps without manual supervision which achieves better results than\nsupervised methods for most categories. Our idea is to leverage foundation\ncomputer vision models such as DINO and Stable Diffusion that are open-ended\nand thus possess excellent priors over natural categories. SHIC reduces the\nproblem of estimating image-to-template correspondences to predicting\nimage-to-image correspondences using features from the foundation models. The\nreduction works by matching images of the object to non-photorealistic renders\nof the template, which emulates the process of collecting manual annotations\nfor this task. These correspondences are then used to supervise high-quality\ncanonical maps for any object of interest. We also show that image generators\ncan further improve the realism of the template views, which provide an\nadditional source of supervision for the model.\n", "link": "http://arxiv.org/abs/2407.18907v1", "date": "2024-07-26", "relevancy": 2.7863, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHIC%3A%20Shape-Image%20Correspondences%20with%20no%20Keypoint%20Supervision&body=Title%3A%20SHIC%3A%20Shape-Image%20Correspondences%20with%20no%20Keypoint%20Supervision%0AAuthor%3A%20Aleksandar%20Shtedritski%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20Canonical%20surface%20mapping%20generalizes%20keypoint%20detection%20by%20assigning%20each%0Apixel%20of%20an%20object%20to%20a%20corresponding%20point%20in%20a%203D%20template.%20Popularised%20by%0ADensePose%20for%20the%20analysis%20of%20humans%2C%20authors%20have%20since%20attempted%20to%20apply%20the%0Aconcept%20to%20more%20categories%2C%20but%20with%20limited%20success%20due%20to%20the%20high%20cost%20of%0Amanual%20supervision.%20In%20this%20work%2C%20we%20introduce%20SHIC%2C%20a%20method%20to%20learn%0Acanonical%20maps%20without%20manual%20supervision%20which%20achieves%20better%20results%20than%0Asupervised%20methods%20for%20most%20categories.%20Our%20idea%20is%20to%20leverage%20foundation%0Acomputer%20vision%20models%20such%20as%20DINO%20and%20Stable%20Diffusion%20that%20are%20open-ended%0Aand%20thus%20possess%20excellent%20priors%20over%20natural%20categories.%20SHIC%20reduces%20the%0Aproblem%20of%20estimating%20image-to-template%20correspondences%20to%20predicting%0Aimage-to-image%20correspondences%20using%20features%20from%20the%20foundation%20models.%20The%0Areduction%20works%20by%20matching%20images%20of%20the%20object%20to%20non-photorealistic%20renders%0Aof%20the%20template%2C%20which%20emulates%20the%20process%20of%20collecting%20manual%20annotations%0Afor%20this%20task.%20These%20correspondences%20are%20then%20used%20to%20supervise%20high-quality%0Acanonical%20maps%20for%20any%20object%20of%20interest.%20We%20also%20show%20that%20image%20generators%0Acan%20further%20improve%20the%20realism%20of%20the%20template%20views%2C%20which%20provide%20an%0Aadditional%20source%20of%20supervision%20for%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHIC%253A%2520Shape-Image%2520Correspondences%2520with%2520no%2520Keypoint%2520Supervision%26entry.906535625%3DAleksandar%2520Shtedritski%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520Canonical%2520surface%2520mapping%2520generalizes%2520keypoint%2520detection%2520by%2520assigning%2520each%250Apixel%2520of%2520an%2520object%2520to%2520a%2520corresponding%2520point%2520in%2520a%25203D%2520template.%2520Popularised%2520by%250ADensePose%2520for%2520the%2520analysis%2520of%2520humans%252C%2520authors%2520have%2520since%2520attempted%2520to%2520apply%2520the%250Aconcept%2520to%2520more%2520categories%252C%2520but%2520with%2520limited%2520success%2520due%2520to%2520the%2520high%2520cost%2520of%250Amanual%2520supervision.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SHIC%252C%2520a%2520method%2520to%2520learn%250Acanonical%2520maps%2520without%2520manual%2520supervision%2520which%2520achieves%2520better%2520results%2520than%250Asupervised%2520methods%2520for%2520most%2520categories.%2520Our%2520idea%2520is%2520to%2520leverage%2520foundation%250Acomputer%2520vision%2520models%2520such%2520as%2520DINO%2520and%2520Stable%2520Diffusion%2520that%2520are%2520open-ended%250Aand%2520thus%2520possess%2520excellent%2520priors%2520over%2520natural%2520categories.%2520SHIC%2520reduces%2520the%250Aproblem%2520of%2520estimating%2520image-to-template%2520correspondences%2520to%2520predicting%250Aimage-to-image%2520correspondences%2520using%2520features%2520from%2520the%2520foundation%2520models.%2520The%250Areduction%2520works%2520by%2520matching%2520images%2520of%2520the%2520object%2520to%2520non-photorealistic%2520renders%250Aof%2520the%2520template%252C%2520which%2520emulates%2520the%2520process%2520of%2520collecting%2520manual%2520annotations%250Afor%2520this%2520task.%2520These%2520correspondences%2520are%2520then%2520used%2520to%2520supervise%2520high-quality%250Acanonical%2520maps%2520for%2520any%2520object%2520of%2520interest.%2520We%2520also%2520show%2520that%2520image%2520generators%250Acan%2520further%2520improve%2520the%2520realism%2520of%2520the%2520template%2520views%252C%2520which%2520provide%2520an%250Aadditional%2520source%2520of%2520supervision%2520for%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHIC%3A%20Shape-Image%20Correspondences%20with%20no%20Keypoint%20Supervision&entry.906535625=Aleksandar%20Shtedritski%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20Canonical%20surface%20mapping%20generalizes%20keypoint%20detection%20by%20assigning%20each%0Apixel%20of%20an%20object%20to%20a%20corresponding%20point%20in%20a%203D%20template.%20Popularised%20by%0ADensePose%20for%20the%20analysis%20of%20humans%2C%20authors%20have%20since%20attempted%20to%20apply%20the%0Aconcept%20to%20more%20categories%2C%20but%20with%20limited%20success%20due%20to%20the%20high%20cost%20of%0Amanual%20supervision.%20In%20this%20work%2C%20we%20introduce%20SHIC%2C%20a%20method%20to%20learn%0Acanonical%20maps%20without%20manual%20supervision%20which%20achieves%20better%20results%20than%0Asupervised%20methods%20for%20most%20categories.%20Our%20idea%20is%20to%20leverage%20foundation%0Acomputer%20vision%20models%20such%20as%20DINO%20and%20Stable%20Diffusion%20that%20are%20open-ended%0Aand%20thus%20possess%20excellent%20priors%20over%20natural%20categories.%20SHIC%20reduces%20the%0Aproblem%20of%20estimating%20image-to-template%20correspondences%20to%20predicting%0Aimage-to-image%20correspondences%20using%20features%20from%20the%20foundation%20models.%20The%0Areduction%20works%20by%20matching%20images%20of%20the%20object%20to%20non-photorealistic%20renders%0Aof%20the%20template%2C%20which%20emulates%20the%20process%20of%20collecting%20manual%20annotations%0Afor%20this%20task.%20These%20correspondences%20are%20then%20used%20to%20supervise%20high-quality%0Acanonical%20maps%20for%20any%20object%20of%20interest.%20We%20also%20show%20that%20image%20generators%0Acan%20further%20improve%20the%20realism%20of%20the%20template%20views%2C%20which%20provide%20an%0Aadditional%20source%20of%20supervision%20for%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18907v1&entry.124074799=Read"},
{"title": "Learn from the Learnt: Source-Free Active Domain Adaptation via\n  Contrastive Sampling and Visual Persistence", "author": "Mengyao Lyu and Tianxiang Hao and Xinhao Xu and Hui Chen and Zijia Lin and Jungong Han and Guiguang Ding", "abstract": "  Domain Adaptation (DA) facilitates knowledge transfer from a source domain to\na related target domain. This paper investigates a practical DA paradigm,\nnamely Source data-Free Active Domain Adaptation (SFADA), where source data\nbecomes inaccessible during adaptation, and a minimum amount of annotation\nbudget is available in the target domain. Without referencing the source data,\nnew challenges emerge in identifying the most informative target samples for\nlabeling, establishing cross-domain alignment during adaptation, and ensuring\ncontinuous performance improvements through the iterative query-and-adaptation\nprocess. In response, we present learn from the learnt (LFTL), a novel paradigm\nfor SFADA to leverage the learnt knowledge from the source pretrained model and\nactively iterated models without extra overhead. We propose Contrastive Active\nSampling to learn from the hypotheses of the preceding model, thereby querying\ntarget samples that are both informative to the current model and persistently\nchallenging throughout active learning. During adaptation, we learn from\nfeatures of actively selected anchors obtained from previous intermediate\nmodels, so that the Visual Persistence-guided Adaptation can facilitate feature\ndistribution alignment and active sample exploitation. Extensive experiments on\nthree widely-used benchmarks show that our LFTL achieves state-of-the-art\nperformance, superior computational efficiency and continuous improvements as\nthe annotation budget increases. Our code is available at\nhttps://github.com/lyumengyao/lftl.\n", "link": "http://arxiv.org/abs/2407.18899v1", "date": "2024-07-26", "relevancy": 2.7063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5647}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20from%20the%20Learnt%3A%20Source-Free%20Active%20Domain%20Adaptation%20via%0A%20%20Contrastive%20Sampling%20and%20Visual%20Persistence&body=Title%3A%20Learn%20from%20the%20Learnt%3A%20Source-Free%20Active%20Domain%20Adaptation%20via%0A%20%20Contrastive%20Sampling%20and%20Visual%20Persistence%0AAuthor%3A%20Mengyao%20Lyu%20and%20Tianxiang%20Hao%20and%20Xinhao%20Xu%20and%20Hui%20Chen%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Domain%20Adaptation%20%28DA%29%20facilitates%20knowledge%20transfer%20from%20a%20source%20domain%20to%0Aa%20related%20target%20domain.%20This%20paper%20investigates%20a%20practical%20DA%20paradigm%2C%0Anamely%20Source%20data-Free%20Active%20Domain%20Adaptation%20%28SFADA%29%2C%20where%20source%20data%0Abecomes%20inaccessible%20during%20adaptation%2C%20and%20a%20minimum%20amount%20of%20annotation%0Abudget%20is%20available%20in%20the%20target%20domain.%20Without%20referencing%20the%20source%20data%2C%0Anew%20challenges%20emerge%20in%20identifying%20the%20most%20informative%20target%20samples%20for%0Alabeling%2C%20establishing%20cross-domain%20alignment%20during%20adaptation%2C%20and%20ensuring%0Acontinuous%20performance%20improvements%20through%20the%20iterative%20query-and-adaptation%0Aprocess.%20In%20response%2C%20we%20present%20learn%20from%20the%20learnt%20%28LFTL%29%2C%20a%20novel%20paradigm%0Afor%20SFADA%20to%20leverage%20the%20learnt%20knowledge%20from%20the%20source%20pretrained%20model%20and%0Aactively%20iterated%20models%20without%20extra%20overhead.%20We%20propose%20Contrastive%20Active%0ASampling%20to%20learn%20from%20the%20hypotheses%20of%20the%20preceding%20model%2C%20thereby%20querying%0Atarget%20samples%20that%20are%20both%20informative%20to%20the%20current%20model%20and%20persistently%0Achallenging%20throughout%20active%20learning.%20During%20adaptation%2C%20we%20learn%20from%0Afeatures%20of%20actively%20selected%20anchors%20obtained%20from%20previous%20intermediate%0Amodels%2C%20so%20that%20the%20Visual%20Persistence-guided%20Adaptation%20can%20facilitate%20feature%0Adistribution%20alignment%20and%20active%20sample%20exploitation.%20Extensive%20experiments%20on%0Athree%20widely-used%20benchmarks%20show%20that%20our%20LFTL%20achieves%20state-of-the-art%0Aperformance%2C%20superior%20computational%20efficiency%20and%20continuous%20improvements%20as%0Athe%20annotation%20budget%20increases.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lyumengyao/lftl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520from%2520the%2520Learnt%253A%2520Source-Free%2520Active%2520Domain%2520Adaptation%2520via%250A%2520%2520Contrastive%2520Sampling%2520and%2520Visual%2520Persistence%26entry.906535625%3DMengyao%2520Lyu%2520and%2520Tianxiang%2520Hao%2520and%2520Xinhao%2520Xu%2520and%2520Hui%2520Chen%2520and%2520Zijia%2520Lin%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Domain%2520Adaptation%2520%2528DA%2529%2520facilitates%2520knowledge%2520transfer%2520from%2520a%2520source%2520domain%2520to%250Aa%2520related%2520target%2520domain.%2520This%2520paper%2520investigates%2520a%2520practical%2520DA%2520paradigm%252C%250Anamely%2520Source%2520data-Free%2520Active%2520Domain%2520Adaptation%2520%2528SFADA%2529%252C%2520where%2520source%2520data%250Abecomes%2520inaccessible%2520during%2520adaptation%252C%2520and%2520a%2520minimum%2520amount%2520of%2520annotation%250Abudget%2520is%2520available%2520in%2520the%2520target%2520domain.%2520Without%2520referencing%2520the%2520source%2520data%252C%250Anew%2520challenges%2520emerge%2520in%2520identifying%2520the%2520most%2520informative%2520target%2520samples%2520for%250Alabeling%252C%2520establishing%2520cross-domain%2520alignment%2520during%2520adaptation%252C%2520and%2520ensuring%250Acontinuous%2520performance%2520improvements%2520through%2520the%2520iterative%2520query-and-adaptation%250Aprocess.%2520In%2520response%252C%2520we%2520present%2520learn%2520from%2520the%2520learnt%2520%2528LFTL%2529%252C%2520a%2520novel%2520paradigm%250Afor%2520SFADA%2520to%2520leverage%2520the%2520learnt%2520knowledge%2520from%2520the%2520source%2520pretrained%2520model%2520and%250Aactively%2520iterated%2520models%2520without%2520extra%2520overhead.%2520We%2520propose%2520Contrastive%2520Active%250ASampling%2520to%2520learn%2520from%2520the%2520hypotheses%2520of%2520the%2520preceding%2520model%252C%2520thereby%2520querying%250Atarget%2520samples%2520that%2520are%2520both%2520informative%2520to%2520the%2520current%2520model%2520and%2520persistently%250Achallenging%2520throughout%2520active%2520learning.%2520During%2520adaptation%252C%2520we%2520learn%2520from%250Afeatures%2520of%2520actively%2520selected%2520anchors%2520obtained%2520from%2520previous%2520intermediate%250Amodels%252C%2520so%2520that%2520the%2520Visual%2520Persistence-guided%2520Adaptation%2520can%2520facilitate%2520feature%250Adistribution%2520alignment%2520and%2520active%2520sample%2520exploitation.%2520Extensive%2520experiments%2520on%250Athree%2520widely-used%2520benchmarks%2520show%2520that%2520our%2520LFTL%2520achieves%2520state-of-the-art%250Aperformance%252C%2520superior%2520computational%2520efficiency%2520and%2520continuous%2520improvements%2520as%250Athe%2520annotation%2520budget%2520increases.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lyumengyao/lftl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20from%20the%20Learnt%3A%20Source-Free%20Active%20Domain%20Adaptation%20via%0A%20%20Contrastive%20Sampling%20and%20Visual%20Persistence&entry.906535625=Mengyao%20Lyu%20and%20Tianxiang%20Hao%20and%20Xinhao%20Xu%20and%20Hui%20Chen%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Domain%20Adaptation%20%28DA%29%20facilitates%20knowledge%20transfer%20from%20a%20source%20domain%20to%0Aa%20related%20target%20domain.%20This%20paper%20investigates%20a%20practical%20DA%20paradigm%2C%0Anamely%20Source%20data-Free%20Active%20Domain%20Adaptation%20%28SFADA%29%2C%20where%20source%20data%0Abecomes%20inaccessible%20during%20adaptation%2C%20and%20a%20minimum%20amount%20of%20annotation%0Abudget%20is%20available%20in%20the%20target%20domain.%20Without%20referencing%20the%20source%20data%2C%0Anew%20challenges%20emerge%20in%20identifying%20the%20most%20informative%20target%20samples%20for%0Alabeling%2C%20establishing%20cross-domain%20alignment%20during%20adaptation%2C%20and%20ensuring%0Acontinuous%20performance%20improvements%20through%20the%20iterative%20query-and-adaptation%0Aprocess.%20In%20response%2C%20we%20present%20learn%20from%20the%20learnt%20%28LFTL%29%2C%20a%20novel%20paradigm%0Afor%20SFADA%20to%20leverage%20the%20learnt%20knowledge%20from%20the%20source%20pretrained%20model%20and%0Aactively%20iterated%20models%20without%20extra%20overhead.%20We%20propose%20Contrastive%20Active%0ASampling%20to%20learn%20from%20the%20hypotheses%20of%20the%20preceding%20model%2C%20thereby%20querying%0Atarget%20samples%20that%20are%20both%20informative%20to%20the%20current%20model%20and%20persistently%0Achallenging%20throughout%20active%20learning.%20During%20adaptation%2C%20we%20learn%20from%0Afeatures%20of%20actively%20selected%20anchors%20obtained%20from%20previous%20intermediate%0Amodels%2C%20so%20that%20the%20Visual%20Persistence-guided%20Adaptation%20can%20facilitate%20feature%0Adistribution%20alignment%20and%20active%20sample%20exploitation.%20Extensive%20experiments%20on%0Athree%20widely-used%20benchmarks%20show%20that%20our%20LFTL%20achieves%20state-of-the-art%0Aperformance%2C%20superior%20computational%20efficiency%20and%20continuous%20improvements%20as%0Athe%20annotation%20budget%20increases.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lyumengyao/lftl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18899v1&entry.124074799=Read"},
{"title": "Real Time Multi Organ Classification on Computed Tomography Images", "author": "Halid Ziya Yerebakan and Yoshihisa Shinagawa and Gerardo Hermosillo Valadez", "abstract": "  Organ segmentation is a fundamental task in medical imaging since it is\nuseful for many clinical automation pipelines. However, some tasks do not\nrequire full segmentation. Instead, a classifier can identify the selected\norgan without segmenting the entire volume. In this study, we demonstrate a\nclassifier based method to obtain organ labels in real time by using a large\ncontext size with a sparse data sampling strategy. Although our method operates\nas an independent classifier at query locations, it can generate full\nsegmentations by querying grid locations at any resolution, offering faster\nperformance than segmentation algorithms. We compared our method with existing\nsegmentation techniques, demonstrating its superior runtime potential for\npractical applications in medical imaging.\n", "link": "http://arxiv.org/abs/2404.18731v2", "date": "2024-07-26", "relevancy": 2.5818, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5293}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images&body=Title%3A%20Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images%0AAuthor%3A%20Halid%20Ziya%20Yerebakan%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez%0AAbstract%3A%20%20%20Organ%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20imaging%20since%20it%20is%0Auseful%20for%20many%20clinical%20automation%20pipelines.%20However%2C%20some%20tasks%20do%20not%0Arequire%20full%20segmentation.%20Instead%2C%20a%20classifier%20can%20identify%20the%20selected%0Aorgan%20without%20segmenting%20the%20entire%20volume.%20In%20this%20study%2C%20we%20demonstrate%20a%0Aclassifier%20based%20method%20to%20obtain%20organ%20labels%20in%20real%20time%20by%20using%20a%20large%0Acontext%20size%20with%20a%20sparse%20data%20sampling%20strategy.%20Although%20our%20method%20operates%0Aas%20an%20independent%20classifier%20at%20query%20locations%2C%20it%20can%20generate%20full%0Asegmentations%20by%20querying%20grid%20locations%20at%20any%20resolution%2C%20offering%20faster%0Aperformance%20than%20segmentation%20algorithms.%20We%20compared%20our%20method%20with%20existing%0Asegmentation%20techniques%2C%20demonstrating%20its%20superior%20runtime%20potential%20for%0Apractical%20applications%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520Time%2520Multi%2520Organ%2520Classification%2520on%2520Computed%2520Tomography%2520Images%26entry.906535625%3DHalid%2520Ziya%2520Yerebakan%2520and%2520Yoshihisa%2520Shinagawa%2520and%2520Gerardo%2520Hermosillo%2520Valadez%26entry.1292438233%3D%2520%2520Organ%2520segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520medical%2520imaging%2520since%2520it%2520is%250Auseful%2520for%2520many%2520clinical%2520automation%2520pipelines.%2520However%252C%2520some%2520tasks%2520do%2520not%250Arequire%2520full%2520segmentation.%2520Instead%252C%2520a%2520classifier%2520can%2520identify%2520the%2520selected%250Aorgan%2520without%2520segmenting%2520the%2520entire%2520volume.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520a%250Aclassifier%2520based%2520method%2520to%2520obtain%2520organ%2520labels%2520in%2520real%2520time%2520by%2520using%2520a%2520large%250Acontext%2520size%2520with%2520a%2520sparse%2520data%2520sampling%2520strategy.%2520Although%2520our%2520method%2520operates%250Aas%2520an%2520independent%2520classifier%2520at%2520query%2520locations%252C%2520it%2520can%2520generate%2520full%250Asegmentations%2520by%2520querying%2520grid%2520locations%2520at%2520any%2520resolution%252C%2520offering%2520faster%250Aperformance%2520than%2520segmentation%2520algorithms.%2520We%2520compared%2520our%2520method%2520with%2520existing%250Asegmentation%2520techniques%252C%2520demonstrating%2520its%2520superior%2520runtime%2520potential%2520for%250Apractical%2520applications%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images&entry.906535625=Halid%20Ziya%20Yerebakan%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez&entry.1292438233=%20%20Organ%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20imaging%20since%20it%20is%0Auseful%20for%20many%20clinical%20automation%20pipelines.%20However%2C%20some%20tasks%20do%20not%0Arequire%20full%20segmentation.%20Instead%2C%20a%20classifier%20can%20identify%20the%20selected%0Aorgan%20without%20segmenting%20the%20entire%20volume.%20In%20this%20study%2C%20we%20demonstrate%20a%0Aclassifier%20based%20method%20to%20obtain%20organ%20labels%20in%20real%20time%20by%20using%20a%20large%0Acontext%20size%20with%20a%20sparse%20data%20sampling%20strategy.%20Although%20our%20method%20operates%0Aas%20an%20independent%20classifier%20at%20query%20locations%2C%20it%20can%20generate%20full%0Asegmentations%20by%20querying%20grid%20locations%20at%20any%20resolution%2C%20offering%20faster%0Aperformance%20than%20segmentation%20algorithms.%20We%20compared%20our%20method%20with%20existing%0Asegmentation%20techniques%2C%20demonstrating%20its%20superior%20runtime%20potential%20for%0Apractical%20applications%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18731v2&entry.124074799=Read"},
{"title": "Generative Adversarial Networks for Imputing Sparse Learning Performance", "author": "Liang Zhang and Mohammed Yeasin and Jionghao Lin and Felix Havugimana and Xiangen Hu", "abstract": "  Learning performance data, such as correct or incorrect responses to\nquestions in Intelligent Tutoring Systems (ITSs) is crucial for tracking and\nassessing the learners' progress and mastery of knowledge. However, the issue\nof data sparsity, characterized by unexplored questions and missing attempts,\nhampers accurate assessment and the provision of tailored, personalized\ninstruction within ITSs. This paper proposes using the Generative Adversarial\nImputation Networks (GAIN) framework to impute sparse learning performance\ndata, reconstructed into a three-dimensional (3D) tensor representation across\nthe dimensions of learners, questions and attempts. Our customized GAIN-based\nmethod computational process imputes sparse data in a 3D tensor space,\nsignificantly enhanced by convolutional neural networks for its input and\noutput layers. This adaptation also includes the use of a least squares loss\nfunction for optimization and aligns the shapes of the input and output with\nthe dimensions of the questions-attempts matrices along the learners'\ndimension. Through extensive experiments on six datasets from various ITSs,\nincluding AutoTutor, ASSISTments and MATHia, we demonstrate that the GAIN\napproach generally outperforms existing methods such as tensor factorization\nand other generative adversarial network (GAN) based approaches in terms of\nimputation accuracy. This finding enhances comprehensive learning data modeling\nand analytics in AI-based education.\n", "link": "http://arxiv.org/abs/2407.18875v1", "date": "2024-07-26", "relevancy": 2.5517, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5249}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5079}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20for%20Imputing%20Sparse%20Learning%20Performance&body=Title%3A%20Generative%20Adversarial%20Networks%20for%20Imputing%20Sparse%20Learning%20Performance%0AAuthor%3A%20Liang%20Zhang%20and%20Mohammed%20Yeasin%20and%20Jionghao%20Lin%20and%20Felix%20Havugimana%20and%20Xiangen%20Hu%0AAbstract%3A%20%20%20Learning%20performance%20data%2C%20such%20as%20correct%20or%20incorrect%20responses%20to%0Aquestions%20in%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20is%20crucial%20for%20tracking%20and%0Aassessing%20the%20learners%27%20progress%20and%20mastery%20of%20knowledge.%20However%2C%20the%20issue%0Aof%20data%20sparsity%2C%20characterized%20by%20unexplored%20questions%20and%20missing%20attempts%2C%0Ahampers%20accurate%20assessment%20and%20the%20provision%20of%20tailored%2C%20personalized%0Ainstruction%20within%20ITSs.%20This%20paper%20proposes%20using%20the%20Generative%20Adversarial%0AImputation%20Networks%20%28GAIN%29%20framework%20to%20impute%20sparse%20learning%20performance%0Adata%2C%20reconstructed%20into%20a%20three-dimensional%20%283D%29%20tensor%20representation%20across%0Athe%20dimensions%20of%20learners%2C%20questions%20and%20attempts.%20Our%20customized%20GAIN-based%0Amethod%20computational%20process%20imputes%20sparse%20data%20in%20a%203D%20tensor%20space%2C%0Asignificantly%20enhanced%20by%20convolutional%20neural%20networks%20for%20its%20input%20and%0Aoutput%20layers.%20This%20adaptation%20also%20includes%20the%20use%20of%20a%20least%20squares%20loss%0Afunction%20for%20optimization%20and%20aligns%20the%20shapes%20of%20the%20input%20and%20output%20with%0Athe%20dimensions%20of%20the%20questions-attempts%20matrices%20along%20the%20learners%27%0Adimension.%20Through%20extensive%20experiments%20on%20six%20datasets%20from%20various%20ITSs%2C%0Aincluding%20AutoTutor%2C%20ASSISTments%20and%20MATHia%2C%20we%20demonstrate%20that%20the%20GAIN%0Aapproach%20generally%20outperforms%20existing%20methods%20such%20as%20tensor%20factorization%0Aand%20other%20generative%20adversarial%20network%20%28GAN%29%20based%20approaches%20in%20terms%20of%0Aimputation%20accuracy.%20This%20finding%20enhances%20comprehensive%20learning%20data%20modeling%0Aand%20analytics%20in%20AI-based%20education.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520for%2520Imputing%2520Sparse%2520Learning%2520Performance%26entry.906535625%3DLiang%2520Zhang%2520and%2520Mohammed%2520Yeasin%2520and%2520Jionghao%2520Lin%2520and%2520Felix%2520Havugimana%2520and%2520Xiangen%2520Hu%26entry.1292438233%3D%2520%2520Learning%2520performance%2520data%252C%2520such%2520as%2520correct%2520or%2520incorrect%2520responses%2520to%250Aquestions%2520in%2520Intelligent%2520Tutoring%2520Systems%2520%2528ITSs%2529%2520is%2520crucial%2520for%2520tracking%2520and%250Aassessing%2520the%2520learners%2527%2520progress%2520and%2520mastery%2520of%2520knowledge.%2520However%252C%2520the%2520issue%250Aof%2520data%2520sparsity%252C%2520characterized%2520by%2520unexplored%2520questions%2520and%2520missing%2520attempts%252C%250Ahampers%2520accurate%2520assessment%2520and%2520the%2520provision%2520of%2520tailored%252C%2520personalized%250Ainstruction%2520within%2520ITSs.%2520This%2520paper%2520proposes%2520using%2520the%2520Generative%2520Adversarial%250AImputation%2520Networks%2520%2528GAIN%2529%2520framework%2520to%2520impute%2520sparse%2520learning%2520performance%250Adata%252C%2520reconstructed%2520into%2520a%2520three-dimensional%2520%25283D%2529%2520tensor%2520representation%2520across%250Athe%2520dimensions%2520of%2520learners%252C%2520questions%2520and%2520attempts.%2520Our%2520customized%2520GAIN-based%250Amethod%2520computational%2520process%2520imputes%2520sparse%2520data%2520in%2520a%25203D%2520tensor%2520space%252C%250Asignificantly%2520enhanced%2520by%2520convolutional%2520neural%2520networks%2520for%2520its%2520input%2520and%250Aoutput%2520layers.%2520This%2520adaptation%2520also%2520includes%2520the%2520use%2520of%2520a%2520least%2520squares%2520loss%250Afunction%2520for%2520optimization%2520and%2520aligns%2520the%2520shapes%2520of%2520the%2520input%2520and%2520output%2520with%250Athe%2520dimensions%2520of%2520the%2520questions-attempts%2520matrices%2520along%2520the%2520learners%2527%250Adimension.%2520Through%2520extensive%2520experiments%2520on%2520six%2520datasets%2520from%2520various%2520ITSs%252C%250Aincluding%2520AutoTutor%252C%2520ASSISTments%2520and%2520MATHia%252C%2520we%2520demonstrate%2520that%2520the%2520GAIN%250Aapproach%2520generally%2520outperforms%2520existing%2520methods%2520such%2520as%2520tensor%2520factorization%250Aand%2520other%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520based%2520approaches%2520in%2520terms%2520of%250Aimputation%2520accuracy.%2520This%2520finding%2520enhances%2520comprehensive%2520learning%2520data%2520modeling%250Aand%2520analytics%2520in%2520AI-based%2520education.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20for%20Imputing%20Sparse%20Learning%20Performance&entry.906535625=Liang%20Zhang%20and%20Mohammed%20Yeasin%20and%20Jionghao%20Lin%20and%20Felix%20Havugimana%20and%20Xiangen%20Hu&entry.1292438233=%20%20Learning%20performance%20data%2C%20such%20as%20correct%20or%20incorrect%20responses%20to%0Aquestions%20in%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20is%20crucial%20for%20tracking%20and%0Aassessing%20the%20learners%27%20progress%20and%20mastery%20of%20knowledge.%20However%2C%20the%20issue%0Aof%20data%20sparsity%2C%20characterized%20by%20unexplored%20questions%20and%20missing%20attempts%2C%0Ahampers%20accurate%20assessment%20and%20the%20provision%20of%20tailored%2C%20personalized%0Ainstruction%20within%20ITSs.%20This%20paper%20proposes%20using%20the%20Generative%20Adversarial%0AImputation%20Networks%20%28GAIN%29%20framework%20to%20impute%20sparse%20learning%20performance%0Adata%2C%20reconstructed%20into%20a%20three-dimensional%20%283D%29%20tensor%20representation%20across%0Athe%20dimensions%20of%20learners%2C%20questions%20and%20attempts.%20Our%20customized%20GAIN-based%0Amethod%20computational%20process%20imputes%20sparse%20data%20in%20a%203D%20tensor%20space%2C%0Asignificantly%20enhanced%20by%20convolutional%20neural%20networks%20for%20its%20input%20and%0Aoutput%20layers.%20This%20adaptation%20also%20includes%20the%20use%20of%20a%20least%20squares%20loss%0Afunction%20for%20optimization%20and%20aligns%20the%20shapes%20of%20the%20input%20and%20output%20with%0Athe%20dimensions%20of%20the%20questions-attempts%20matrices%20along%20the%20learners%27%0Adimension.%20Through%20extensive%20experiments%20on%20six%20datasets%20from%20various%20ITSs%2C%0Aincluding%20AutoTutor%2C%20ASSISTments%20and%20MATHia%2C%20we%20demonstrate%20that%20the%20GAIN%0Aapproach%20generally%20outperforms%20existing%20methods%20such%20as%20tensor%20factorization%0Aand%20other%20generative%20adversarial%20network%20%28GAN%29%20based%20approaches%20in%20terms%20of%0Aimputation%20accuracy.%20This%20finding%20enhances%20comprehensive%20learning%20data%20modeling%0Aand%20analytics%20in%20AI-based%20education.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18875v1&entry.124074799=Read"},
{"title": "Contrastive Learning of Asset Embeddings from Financial Time Series", "author": "Rian Dolphin and Barry Smyth and Ruihai Dong", "abstract": "  Representation learning has emerged as a powerful paradigm for extracting\nvaluable latent features from complex, high-dimensional data. In financial\ndomains, learning informative representations for assets can be used for tasks\nlike sector classification, and risk management. However, the complex and\nstochastic nature of financial markets poses unique challenges. We propose a\nnovel contrastive learning framework to generate asset embeddings from\nfinancial time series data. Our approach leverages the similarity of asset\nreturns over many subwindows to generate informative positive and negative\nsamples, using a statistical sampling strategy based on hypothesis testing to\naddress the noisy nature of financial data. We explore various contrastive loss\nfunctions that capture the relationships between assets in different ways to\nlearn a discriminative representation space. Experiments on real-world datasets\ndemonstrate the effectiveness of the learned asset embeddings on benchmark\nindustry classification and portfolio optimization tasks. In each case our\nnovel approaches significantly outperform existing baselines highlighting the\npotential for contrastive learning to capture meaningful and actionable\nrelationships in financial data.\n", "link": "http://arxiv.org/abs/2407.18645v1", "date": "2024-07-26", "relevancy": 2.5441, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20of%20Asset%20Embeddings%20from%20Financial%20Time%20Series&body=Title%3A%20Contrastive%20Learning%20of%20Asset%20Embeddings%20from%20Financial%20Time%20Series%0AAuthor%3A%20Rian%20Dolphin%20and%20Barry%20Smyth%20and%20Ruihai%20Dong%0AAbstract%3A%20%20%20Representation%20learning%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20extracting%0Avaluable%20latent%20features%20from%20complex%2C%20high-dimensional%20data.%20In%20financial%0Adomains%2C%20learning%20informative%20representations%20for%20assets%20can%20be%20used%20for%20tasks%0Alike%20sector%20classification%2C%20and%20risk%20management.%20However%2C%20the%20complex%20and%0Astochastic%20nature%20of%20financial%20markets%20poses%20unique%20challenges.%20We%20propose%20a%0Anovel%20contrastive%20learning%20framework%20to%20generate%20asset%20embeddings%20from%0Afinancial%20time%20series%20data.%20Our%20approach%20leverages%20the%20similarity%20of%20asset%0Areturns%20over%20many%20subwindows%20to%20generate%20informative%20positive%20and%20negative%0Asamples%2C%20using%20a%20statistical%20sampling%20strategy%20based%20on%20hypothesis%20testing%20to%0Aaddress%20the%20noisy%20nature%20of%20financial%20data.%20We%20explore%20various%20contrastive%20loss%0Afunctions%20that%20capture%20the%20relationships%20between%20assets%20in%20different%20ways%20to%0Alearn%20a%20discriminative%20representation%20space.%20Experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20effectiveness%20of%20the%20learned%20asset%20embeddings%20on%20benchmark%0Aindustry%20classification%20and%20portfolio%20optimization%20tasks.%20In%20each%20case%20our%0Anovel%20approaches%20significantly%20outperform%20existing%20baselines%20highlighting%20the%0Apotential%20for%20contrastive%20learning%20to%20capture%20meaningful%20and%20actionable%0Arelationships%20in%20financial%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520of%2520Asset%2520Embeddings%2520from%2520Financial%2520Time%2520Series%26entry.906535625%3DRian%2520Dolphin%2520and%2520Barry%2520Smyth%2520and%2520Ruihai%2520Dong%26entry.1292438233%3D%2520%2520Representation%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520extracting%250Avaluable%2520latent%2520features%2520from%2520complex%252C%2520high-dimensional%2520data.%2520In%2520financial%250Adomains%252C%2520learning%2520informative%2520representations%2520for%2520assets%2520can%2520be%2520used%2520for%2520tasks%250Alike%2520sector%2520classification%252C%2520and%2520risk%2520management.%2520However%252C%2520the%2520complex%2520and%250Astochastic%2520nature%2520of%2520financial%2520markets%2520poses%2520unique%2520challenges.%2520We%2520propose%2520a%250Anovel%2520contrastive%2520learning%2520framework%2520to%2520generate%2520asset%2520embeddings%2520from%250Afinancial%2520time%2520series%2520data.%2520Our%2520approach%2520leverages%2520the%2520similarity%2520of%2520asset%250Areturns%2520over%2520many%2520subwindows%2520to%2520generate%2520informative%2520positive%2520and%2520negative%250Asamples%252C%2520using%2520a%2520statistical%2520sampling%2520strategy%2520based%2520on%2520hypothesis%2520testing%2520to%250Aaddress%2520the%2520noisy%2520nature%2520of%2520financial%2520data.%2520We%2520explore%2520various%2520contrastive%2520loss%250Afunctions%2520that%2520capture%2520the%2520relationships%2520between%2520assets%2520in%2520different%2520ways%2520to%250Alearn%2520a%2520discriminative%2520representation%2520space.%2520Experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520learned%2520asset%2520embeddings%2520on%2520benchmark%250Aindustry%2520classification%2520and%2520portfolio%2520optimization%2520tasks.%2520In%2520each%2520case%2520our%250Anovel%2520approaches%2520significantly%2520outperform%2520existing%2520baselines%2520highlighting%2520the%250Apotential%2520for%2520contrastive%2520learning%2520to%2520capture%2520meaningful%2520and%2520actionable%250Arelationships%2520in%2520financial%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20of%20Asset%20Embeddings%20from%20Financial%20Time%20Series&entry.906535625=Rian%20Dolphin%20and%20Barry%20Smyth%20and%20Ruihai%20Dong&entry.1292438233=%20%20Representation%20learning%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20extracting%0Avaluable%20latent%20features%20from%20complex%2C%20high-dimensional%20data.%20In%20financial%0Adomains%2C%20learning%20informative%20representations%20for%20assets%20can%20be%20used%20for%20tasks%0Alike%20sector%20classification%2C%20and%20risk%20management.%20However%2C%20the%20complex%20and%0Astochastic%20nature%20of%20financial%20markets%20poses%20unique%20challenges.%20We%20propose%20a%0Anovel%20contrastive%20learning%20framework%20to%20generate%20asset%20embeddings%20from%0Afinancial%20time%20series%20data.%20Our%20approach%20leverages%20the%20similarity%20of%20asset%0Areturns%20over%20many%20subwindows%20to%20generate%20informative%20positive%20and%20negative%0Asamples%2C%20using%20a%20statistical%20sampling%20strategy%20based%20on%20hypothesis%20testing%20to%0Aaddress%20the%20noisy%20nature%20of%20financial%20data.%20We%20explore%20various%20contrastive%20loss%0Afunctions%20that%20capture%20the%20relationships%20between%20assets%20in%20different%20ways%20to%0Alearn%20a%20discriminative%20representation%20space.%20Experiments%20on%20real-world%20datasets%0Ademonstrate%20the%20effectiveness%20of%20the%20learned%20asset%20embeddings%20on%20benchmark%0Aindustry%20classification%20and%20portfolio%20optimization%20tasks.%20In%20each%20case%20our%0Anovel%20approaches%20significantly%20outperform%20existing%20baselines%20highlighting%20the%0Apotential%20for%20contrastive%20learning%20to%20capture%20meaningful%20and%20actionable%0Arelationships%20in%20financial%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18645v1&entry.124074799=Read"},
{"title": "A Physics-Informed Neural Network-Based Approach for the Spatial\n  Upsampling of Spherical Microphone Arrays", "author": "Federico Miotello and Ferdinando Terminiello and Mirco Pezzoli and Alberto Bernardini and Fabio Antonacci and Augusto Sarti", "abstract": "  Spherical microphone arrays are convenient tools for capturing the spatial\ncharacteristics of a sound field. However, achieving superior spatial\nresolution requires arrays with numerous capsules, consequently leading to\nexpensive devices. To address this issue, we present a method for spatially\nupsampling spherical microphone arrays with a limited number of capsules. Our\napproach exploits a physics-informed neural network with Rowdy activation\nfunctions, leveraging physical constraints to provide high-order microphone\narray signals, starting from low-order devices. Results show that, within its\ndomain of application, our approach outperforms a state of the art method based\non signal processing for spherical microphone arrays upsampling.\n", "link": "http://arxiv.org/abs/2407.18732v1", "date": "2024-07-26", "relevancy": 2.5433, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5218}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5041}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics-Informed%20Neural%20Network-Based%20Approach%20for%20the%20Spatial%0A%20%20Upsampling%20of%20Spherical%20Microphone%20Arrays&body=Title%3A%20A%20Physics-Informed%20Neural%20Network-Based%20Approach%20for%20the%20Spatial%0A%20%20Upsampling%20of%20Spherical%20Microphone%20Arrays%0AAuthor%3A%20Federico%20Miotello%20and%20Ferdinando%20Terminiello%20and%20Mirco%20Pezzoli%20and%20Alberto%20Bernardini%20and%20Fabio%20Antonacci%20and%20Augusto%20Sarti%0AAbstract%3A%20%20%20Spherical%20microphone%20arrays%20are%20convenient%20tools%20for%20capturing%20the%20spatial%0Acharacteristics%20of%20a%20sound%20field.%20However%2C%20achieving%20superior%20spatial%0Aresolution%20requires%20arrays%20with%20numerous%20capsules%2C%20consequently%20leading%20to%0Aexpensive%20devices.%20To%20address%20this%20issue%2C%20we%20present%20a%20method%20for%20spatially%0Aupsampling%20spherical%20microphone%20arrays%20with%20a%20limited%20number%20of%20capsules.%20Our%0Aapproach%20exploits%20a%20physics-informed%20neural%20network%20with%20Rowdy%20activation%0Afunctions%2C%20leveraging%20physical%20constraints%20to%20provide%20high-order%20microphone%0Aarray%20signals%2C%20starting%20from%20low-order%20devices.%20Results%20show%20that%2C%20within%20its%0Adomain%20of%20application%2C%20our%20approach%20outperforms%20a%20state%20of%20the%20art%20method%20based%0Aon%20signal%20processing%20for%20spherical%20microphone%20arrays%20upsampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics-Informed%2520Neural%2520Network-Based%2520Approach%2520for%2520the%2520Spatial%250A%2520%2520Upsampling%2520of%2520Spherical%2520Microphone%2520Arrays%26entry.906535625%3DFederico%2520Miotello%2520and%2520Ferdinando%2520Terminiello%2520and%2520Mirco%2520Pezzoli%2520and%2520Alberto%2520Bernardini%2520and%2520Fabio%2520Antonacci%2520and%2520Augusto%2520Sarti%26entry.1292438233%3D%2520%2520Spherical%2520microphone%2520arrays%2520are%2520convenient%2520tools%2520for%2520capturing%2520the%2520spatial%250Acharacteristics%2520of%2520a%2520sound%2520field.%2520However%252C%2520achieving%2520superior%2520spatial%250Aresolution%2520requires%2520arrays%2520with%2520numerous%2520capsules%252C%2520consequently%2520leading%2520to%250Aexpensive%2520devices.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520a%2520method%2520for%2520spatially%250Aupsampling%2520spherical%2520microphone%2520arrays%2520with%2520a%2520limited%2520number%2520of%2520capsules.%2520Our%250Aapproach%2520exploits%2520a%2520physics-informed%2520neural%2520network%2520with%2520Rowdy%2520activation%250Afunctions%252C%2520leveraging%2520physical%2520constraints%2520to%2520provide%2520high-order%2520microphone%250Aarray%2520signals%252C%2520starting%2520from%2520low-order%2520devices.%2520Results%2520show%2520that%252C%2520within%2520its%250Adomain%2520of%2520application%252C%2520our%2520approach%2520outperforms%2520a%2520state%2520of%2520the%2520art%2520method%2520based%250Aon%2520signal%2520processing%2520for%2520spherical%2520microphone%2520arrays%2520upsampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-Informed%20Neural%20Network-Based%20Approach%20for%20the%20Spatial%0A%20%20Upsampling%20of%20Spherical%20Microphone%20Arrays&entry.906535625=Federico%20Miotello%20and%20Ferdinando%20Terminiello%20and%20Mirco%20Pezzoli%20and%20Alberto%20Bernardini%20and%20Fabio%20Antonacci%20and%20Augusto%20Sarti&entry.1292438233=%20%20Spherical%20microphone%20arrays%20are%20convenient%20tools%20for%20capturing%20the%20spatial%0Acharacteristics%20of%20a%20sound%20field.%20However%2C%20achieving%20superior%20spatial%0Aresolution%20requires%20arrays%20with%20numerous%20capsules%2C%20consequently%20leading%20to%0Aexpensive%20devices.%20To%20address%20this%20issue%2C%20we%20present%20a%20method%20for%20spatially%0Aupsampling%20spherical%20microphone%20arrays%20with%20a%20limited%20number%20of%20capsules.%20Our%0Aapproach%20exploits%20a%20physics-informed%20neural%20network%20with%20Rowdy%20activation%0Afunctions%2C%20leveraging%20physical%20constraints%20to%20provide%20high-order%20microphone%0Aarray%20signals%2C%20starting%20from%20low-order%20devices.%20Results%20show%20that%2C%20within%20its%0Adomain%20of%20application%2C%20our%20approach%20outperforms%20a%20state%20of%20the%20art%20method%20based%0Aon%20signal%20processing%20for%20spherical%20microphone%20arrays%20upsampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18732v1&entry.124074799=Read"},
{"title": "Viewpoint Textual Inversion: Discovering Scene Representations and 3D\n  View Control in 2D Diffusion Models", "author": "James Burgess and Kuan-Chieh Wang and Serena Yeung-Levy", "abstract": "  Text-to-image diffusion models generate impressive and realistic images, but\ndo they learn to represent the 3D world from only 2D supervision? We\ndemonstrate that yes, certain 3D scene representations are encoded in the text\nembedding space of models like Stable Diffusion. Our approach, Viewpoint Neural\nTextual Inversion (ViewNeTI), is to discover 3D view tokens; these tokens\ncontrol the 3D viewpoint - the rendering pose in a scene - of generated images.\nSpecifically, we train a small neural mapper to take continuous camera\nviewpoint parameters and predict a view token (a word embedding). This token\nconditions diffusion generation via cross-attention to produce images with the\ndesired camera viewpoint. Using ViewNeTI as an evaluation tool, we report two\nfindings: first, the text latent space has a continuous view-control manifold\nfor particular 3D scenes; second, we find evidence for a generalized\nview-control manifold for all scenes. We conclude that since the view token\ncontrols the 3D `rendering' viewpoint, there is likely a scene representation\nembedded in frozen 2D diffusion models. Finally, we exploit the 3D scene\nrepresentations for 3D vision tasks, namely, view-controlled text-to-image\ngeneration, and novel view synthesis from a single image, where our approach\nsets state-of-the-art for LPIPS. Code available at\nhttps://github.com/jmhb0/view_neti\n", "link": "http://arxiv.org/abs/2309.07986v2", "date": "2024-07-26", "relevancy": 2.5321, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6404}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6404}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Viewpoint%20Textual%20Inversion%3A%20Discovering%20Scene%20Representations%20and%203D%0A%20%20View%20Control%20in%202D%20Diffusion%20Models&body=Title%3A%20Viewpoint%20Textual%20Inversion%3A%20Discovering%20Scene%20Representations%20and%203D%0A%20%20View%20Control%20in%202D%20Diffusion%20Models%0AAuthor%3A%20James%20Burgess%20and%20Kuan-Chieh%20Wang%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20generate%20impressive%20and%20realistic%20images%2C%20but%0Ado%20they%20learn%20to%20represent%20the%203D%20world%20from%20only%202D%20supervision%3F%20We%0Ademonstrate%20that%20yes%2C%20certain%203D%20scene%20representations%20are%20encoded%20in%20the%20text%0Aembedding%20space%20of%20models%20like%20Stable%20Diffusion.%20Our%20approach%2C%20Viewpoint%20Neural%0ATextual%20Inversion%20%28ViewNeTI%29%2C%20is%20to%20discover%203D%20view%20tokens%3B%20these%20tokens%0Acontrol%20the%203D%20viewpoint%20-%20the%20rendering%20pose%20in%20a%20scene%20-%20of%20generated%20images.%0ASpecifically%2C%20we%20train%20a%20small%20neural%20mapper%20to%20take%20continuous%20camera%0Aviewpoint%20parameters%20and%20predict%20a%20view%20token%20%28a%20word%20embedding%29.%20This%20token%0Aconditions%20diffusion%20generation%20via%20cross-attention%20to%20produce%20images%20with%20the%0Adesired%20camera%20viewpoint.%20Using%20ViewNeTI%20as%20an%20evaluation%20tool%2C%20we%20report%20two%0Afindings%3A%20first%2C%20the%20text%20latent%20space%20has%20a%20continuous%20view-control%20manifold%0Afor%20particular%203D%20scenes%3B%20second%2C%20we%20find%20evidence%20for%20a%20generalized%0Aview-control%20manifold%20for%20all%20scenes.%20We%20conclude%20that%20since%20the%20view%20token%0Acontrols%20the%203D%20%60rendering%27%20viewpoint%2C%20there%20is%20likely%20a%20scene%20representation%0Aembedded%20in%20frozen%202D%20diffusion%20models.%20Finally%2C%20we%20exploit%20the%203D%20scene%0Arepresentations%20for%203D%20vision%20tasks%2C%20namely%2C%20view-controlled%20text-to-image%0Ageneration%2C%20and%20novel%20view%20synthesis%20from%20a%20single%20image%2C%20where%20our%20approach%0Asets%20state-of-the-art%20for%20LPIPS.%20Code%20available%20at%0Ahttps%3A//github.com/jmhb0/view_neti%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewpoint%2520Textual%2520Inversion%253A%2520Discovering%2520Scene%2520Representations%2520and%25203D%250A%2520%2520View%2520Control%2520in%25202D%2520Diffusion%2520Models%26entry.906535625%3DJames%2520Burgess%2520and%2520Kuan-Chieh%2520Wang%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520generate%2520impressive%2520and%2520realistic%2520images%252C%2520but%250Ado%2520they%2520learn%2520to%2520represent%2520the%25203D%2520world%2520from%2520only%25202D%2520supervision%253F%2520We%250Ademonstrate%2520that%2520yes%252C%2520certain%25203D%2520scene%2520representations%2520are%2520encoded%2520in%2520the%2520text%250Aembedding%2520space%2520of%2520models%2520like%2520Stable%2520Diffusion.%2520Our%2520approach%252C%2520Viewpoint%2520Neural%250ATextual%2520Inversion%2520%2528ViewNeTI%2529%252C%2520is%2520to%2520discover%25203D%2520view%2520tokens%253B%2520these%2520tokens%250Acontrol%2520the%25203D%2520viewpoint%2520-%2520the%2520rendering%2520pose%2520in%2520a%2520scene%2520-%2520of%2520generated%2520images.%250ASpecifically%252C%2520we%2520train%2520a%2520small%2520neural%2520mapper%2520to%2520take%2520continuous%2520camera%250Aviewpoint%2520parameters%2520and%2520predict%2520a%2520view%2520token%2520%2528a%2520word%2520embedding%2529.%2520This%2520token%250Aconditions%2520diffusion%2520generation%2520via%2520cross-attention%2520to%2520produce%2520images%2520with%2520the%250Adesired%2520camera%2520viewpoint.%2520Using%2520ViewNeTI%2520as%2520an%2520evaluation%2520tool%252C%2520we%2520report%2520two%250Afindings%253A%2520first%252C%2520the%2520text%2520latent%2520space%2520has%2520a%2520continuous%2520view-control%2520manifold%250Afor%2520particular%25203D%2520scenes%253B%2520second%252C%2520we%2520find%2520evidence%2520for%2520a%2520generalized%250Aview-control%2520manifold%2520for%2520all%2520scenes.%2520We%2520conclude%2520that%2520since%2520the%2520view%2520token%250Acontrols%2520the%25203D%2520%2560rendering%2527%2520viewpoint%252C%2520there%2520is%2520likely%2520a%2520scene%2520representation%250Aembedded%2520in%2520frozen%25202D%2520diffusion%2520models.%2520Finally%252C%2520we%2520exploit%2520the%25203D%2520scene%250Arepresentations%2520for%25203D%2520vision%2520tasks%252C%2520namely%252C%2520view-controlled%2520text-to-image%250Ageneration%252C%2520and%2520novel%2520view%2520synthesis%2520from%2520a%2520single%2520image%252C%2520where%2520our%2520approach%250Asets%2520state-of-the-art%2520for%2520LPIPS.%2520Code%2520available%2520at%250Ahttps%253A//github.com/jmhb0/view_neti%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Viewpoint%20Textual%20Inversion%3A%20Discovering%20Scene%20Representations%20and%203D%0A%20%20View%20Control%20in%202D%20Diffusion%20Models&entry.906535625=James%20Burgess%20and%20Kuan-Chieh%20Wang%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20generate%20impressive%20and%20realistic%20images%2C%20but%0Ado%20they%20learn%20to%20represent%20the%203D%20world%20from%20only%202D%20supervision%3F%20We%0Ademonstrate%20that%20yes%2C%20certain%203D%20scene%20representations%20are%20encoded%20in%20the%20text%0Aembedding%20space%20of%20models%20like%20Stable%20Diffusion.%20Our%20approach%2C%20Viewpoint%20Neural%0ATextual%20Inversion%20%28ViewNeTI%29%2C%20is%20to%20discover%203D%20view%20tokens%3B%20these%20tokens%0Acontrol%20the%203D%20viewpoint%20-%20the%20rendering%20pose%20in%20a%20scene%20-%20of%20generated%20images.%0ASpecifically%2C%20we%20train%20a%20small%20neural%20mapper%20to%20take%20continuous%20camera%0Aviewpoint%20parameters%20and%20predict%20a%20view%20token%20%28a%20word%20embedding%29.%20This%20token%0Aconditions%20diffusion%20generation%20via%20cross-attention%20to%20produce%20images%20with%20the%0Adesired%20camera%20viewpoint.%20Using%20ViewNeTI%20as%20an%20evaluation%20tool%2C%20we%20report%20two%0Afindings%3A%20first%2C%20the%20text%20latent%20space%20has%20a%20continuous%20view-control%20manifold%0Afor%20particular%203D%20scenes%3B%20second%2C%20we%20find%20evidence%20for%20a%20generalized%0Aview-control%20manifold%20for%20all%20scenes.%20We%20conclude%20that%20since%20the%20view%20token%0Acontrols%20the%203D%20%60rendering%27%20viewpoint%2C%20there%20is%20likely%20a%20scene%20representation%0Aembedded%20in%20frozen%202D%20diffusion%20models.%20Finally%2C%20we%20exploit%20the%203D%20scene%0Arepresentations%20for%203D%20vision%20tasks%2C%20namely%2C%20view-controlled%20text-to-image%0Ageneration%2C%20and%20novel%20view%20synthesis%20from%20a%20single%20image%2C%20where%20our%20approach%0Asets%20state-of-the-art%20for%20LPIPS.%20Code%20available%20at%0Ahttps%3A//github.com/jmhb0/view_neti%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07986v2&entry.124074799=Read"},
{"title": "Downlink CCM Estimation via Representation Learning with Graph\n  Regularization", "author": "Melih Can Zerin and Elif Vural and Ali \u00d6zg\u00fcr Y\u0131lmaz", "abstract": "  In this paper, we propose an algorithm for downlink (DL) channel covariance\nmatrix (CCM) estimation for frequency division duplexing (FDD) massive\nmultiple-input multiple-output (MIMO) communication systems with base station\n(BS) possessing a uniform linear array (ULA) antenna structure. We make use of\nthe inherent similarity between the uplink (UL) CCM and the DL CCM due to\nangular reciprocity. We consider a setting where the UL CCM is mapped to DL CCM\nby a mapping function. We first present a theoretical error analysis of\nlearning a nonlinear embedding by constructing a mapping function, which points\nto the importance of the Lipschitz regularity of the mapping function for\nachieving high estimation performance. Then, based on the theoretical ground,\nwe propose a representation learning algorithm as a solution for the estimation\nproblem, where Gaussian RBF kernel interpolators are chosen to map UL CCMs to\ntheir DL counterparts. The proposed algorithm is based on the optimization of\nan objective function that fits a regression model between the DL CCM and UL\nCCM samples in the training dataset and preserves the local geometric structure\nof the data in the UL CCM space, while explicitly regulating the Lipschitz\ncontinuity of the mapping function in light of our theoretical findings. The\nproposed algorithm surpasses benchmark methods in terms of three error metrics\nas shown by simulations.\n", "link": "http://arxiv.org/abs/2407.18865v1", "date": "2024-07-26", "relevancy": 2.4658, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.503}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Downlink%20CCM%20Estimation%20via%20Representation%20Learning%20with%20Graph%0A%20%20Regularization&body=Title%3A%20Downlink%20CCM%20Estimation%20via%20Representation%20Learning%20with%20Graph%0A%20%20Regularization%0AAuthor%3A%20Melih%20Can%20Zerin%20and%20Elif%20Vural%20and%20Ali%20%C3%96zg%C3%BCr%20Y%C4%B1lmaz%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20for%20downlink%20%28DL%29%20channel%20covariance%0Amatrix%20%28CCM%29%20estimation%20for%20frequency%20division%20duplexing%20%28FDD%29%20massive%0Amultiple-input%20multiple-output%20%28MIMO%29%20communication%20systems%20with%20base%20station%0A%28BS%29%20possessing%20a%20uniform%20linear%20array%20%28ULA%29%20antenna%20structure.%20We%20make%20use%20of%0Athe%20inherent%20similarity%20between%20the%20uplink%20%28UL%29%20CCM%20and%20the%20DL%20CCM%20due%20to%0Aangular%20reciprocity.%20We%20consider%20a%20setting%20where%20the%20UL%20CCM%20is%20mapped%20to%20DL%20CCM%0Aby%20a%20mapping%20function.%20We%20first%20present%20a%20theoretical%20error%20analysis%20of%0Alearning%20a%20nonlinear%20embedding%20by%20constructing%20a%20mapping%20function%2C%20which%20points%0Ato%20the%20importance%20of%20the%20Lipschitz%20regularity%20of%20the%20mapping%20function%20for%0Aachieving%20high%20estimation%20performance.%20Then%2C%20based%20on%20the%20theoretical%20ground%2C%0Awe%20propose%20a%20representation%20learning%20algorithm%20as%20a%20solution%20for%20the%20estimation%0Aproblem%2C%20where%20Gaussian%20RBF%20kernel%20interpolators%20are%20chosen%20to%20map%20UL%20CCMs%20to%0Atheir%20DL%20counterparts.%20The%20proposed%20algorithm%20is%20based%20on%20the%20optimization%20of%0Aan%20objective%20function%20that%20fits%20a%20regression%20model%20between%20the%20DL%20CCM%20and%20UL%0ACCM%20samples%20in%20the%20training%20dataset%20and%20preserves%20the%20local%20geometric%20structure%0Aof%20the%20data%20in%20the%20UL%20CCM%20space%2C%20while%20explicitly%20regulating%20the%20Lipschitz%0Acontinuity%20of%20the%20mapping%20function%20in%20light%20of%20our%20theoretical%20findings.%20The%0Aproposed%20algorithm%20surpasses%20benchmark%20methods%20in%20terms%20of%20three%20error%20metrics%0Aas%20shown%20by%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDownlink%2520CCM%2520Estimation%2520via%2520Representation%2520Learning%2520with%2520Graph%250A%2520%2520Regularization%26entry.906535625%3DMelih%2520Can%2520Zerin%2520and%2520Elif%2520Vural%2520and%2520Ali%2520%25C3%2596zg%25C3%25BCr%2520Y%25C4%25B1lmaz%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520algorithm%2520for%2520downlink%2520%2528DL%2529%2520channel%2520covariance%250Amatrix%2520%2528CCM%2529%2520estimation%2520for%2520frequency%2520division%2520duplexing%2520%2528FDD%2529%2520massive%250Amultiple-input%2520multiple-output%2520%2528MIMO%2529%2520communication%2520systems%2520with%2520base%2520station%250A%2528BS%2529%2520possessing%2520a%2520uniform%2520linear%2520array%2520%2528ULA%2529%2520antenna%2520structure.%2520We%2520make%2520use%2520of%250Athe%2520inherent%2520similarity%2520between%2520the%2520uplink%2520%2528UL%2529%2520CCM%2520and%2520the%2520DL%2520CCM%2520due%2520to%250Aangular%2520reciprocity.%2520We%2520consider%2520a%2520setting%2520where%2520the%2520UL%2520CCM%2520is%2520mapped%2520to%2520DL%2520CCM%250Aby%2520a%2520mapping%2520function.%2520We%2520first%2520present%2520a%2520theoretical%2520error%2520analysis%2520of%250Alearning%2520a%2520nonlinear%2520embedding%2520by%2520constructing%2520a%2520mapping%2520function%252C%2520which%2520points%250Ato%2520the%2520importance%2520of%2520the%2520Lipschitz%2520regularity%2520of%2520the%2520mapping%2520function%2520for%250Aachieving%2520high%2520estimation%2520performance.%2520Then%252C%2520based%2520on%2520the%2520theoretical%2520ground%252C%250Awe%2520propose%2520a%2520representation%2520learning%2520algorithm%2520as%2520a%2520solution%2520for%2520the%2520estimation%250Aproblem%252C%2520where%2520Gaussian%2520RBF%2520kernel%2520interpolators%2520are%2520chosen%2520to%2520map%2520UL%2520CCMs%2520to%250Atheir%2520DL%2520counterparts.%2520The%2520proposed%2520algorithm%2520is%2520based%2520on%2520the%2520optimization%2520of%250Aan%2520objective%2520function%2520that%2520fits%2520a%2520regression%2520model%2520between%2520the%2520DL%2520CCM%2520and%2520UL%250ACCM%2520samples%2520in%2520the%2520training%2520dataset%2520and%2520preserves%2520the%2520local%2520geometric%2520structure%250Aof%2520the%2520data%2520in%2520the%2520UL%2520CCM%2520space%252C%2520while%2520explicitly%2520regulating%2520the%2520Lipschitz%250Acontinuity%2520of%2520the%2520mapping%2520function%2520in%2520light%2520of%2520our%2520theoretical%2520findings.%2520The%250Aproposed%2520algorithm%2520surpasses%2520benchmark%2520methods%2520in%2520terms%2520of%2520three%2520error%2520metrics%250Aas%2520shown%2520by%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Downlink%20CCM%20Estimation%20via%20Representation%20Learning%20with%20Graph%0A%20%20Regularization&entry.906535625=Melih%20Can%20Zerin%20and%20Elif%20Vural%20and%20Ali%20%C3%96zg%C3%BCr%20Y%C4%B1lmaz&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20for%20downlink%20%28DL%29%20channel%20covariance%0Amatrix%20%28CCM%29%20estimation%20for%20frequency%20division%20duplexing%20%28FDD%29%20massive%0Amultiple-input%20multiple-output%20%28MIMO%29%20communication%20systems%20with%20base%20station%0A%28BS%29%20possessing%20a%20uniform%20linear%20array%20%28ULA%29%20antenna%20structure.%20We%20make%20use%20of%0Athe%20inherent%20similarity%20between%20the%20uplink%20%28UL%29%20CCM%20and%20the%20DL%20CCM%20due%20to%0Aangular%20reciprocity.%20We%20consider%20a%20setting%20where%20the%20UL%20CCM%20is%20mapped%20to%20DL%20CCM%0Aby%20a%20mapping%20function.%20We%20first%20present%20a%20theoretical%20error%20analysis%20of%0Alearning%20a%20nonlinear%20embedding%20by%20constructing%20a%20mapping%20function%2C%20which%20points%0Ato%20the%20importance%20of%20the%20Lipschitz%20regularity%20of%20the%20mapping%20function%20for%0Aachieving%20high%20estimation%20performance.%20Then%2C%20based%20on%20the%20theoretical%20ground%2C%0Awe%20propose%20a%20representation%20learning%20algorithm%20as%20a%20solution%20for%20the%20estimation%0Aproblem%2C%20where%20Gaussian%20RBF%20kernel%20interpolators%20are%20chosen%20to%20map%20UL%20CCMs%20to%0Atheir%20DL%20counterparts.%20The%20proposed%20algorithm%20is%20based%20on%20the%20optimization%20of%0Aan%20objective%20function%20that%20fits%20a%20regression%20model%20between%20the%20DL%20CCM%20and%20UL%0ACCM%20samples%20in%20the%20training%20dataset%20and%20preserves%20the%20local%20geometric%20structure%0Aof%20the%20data%20in%20the%20UL%20CCM%20space%2C%20while%20explicitly%20regulating%20the%20Lipschitz%0Acontinuity%20of%20the%20mapping%20function%20in%20light%20of%20our%20theoretical%20findings.%20The%0Aproposed%20algorithm%20surpasses%20benchmark%20methods%20in%20terms%20of%20three%20error%20metrics%0Aas%20shown%20by%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18865v1&entry.124074799=Read"},
{"title": "HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM", "author": "Zhe Xin and Yufeng Yue and Liangjun Zhang and Chenming Wu", "abstract": "  Simultaneous Localization and Mapping (SLAM) is a fundamental task in\nrobotics, driving numerous applications such as autonomous driving and virtual\nreality. Recent progress on neural implicit SLAM has shown encouraging and\nimpressive results. However, the robustness of neural SLAM, particularly in\nchallenging or data-limited situations, remains an unresolved issue. This paper\npresents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural\nSLAM, which combines the benefits of neural implicit field and feature-metric\noptimization. This hybrid method optimizes a multi-resolution implicit field\nand enhances robustness in challenging environments with sudden viewpoint\nchanges or sparse data collection. Our comprehensive experimental results on\nbenchmarking datasets validate the effectiveness of our hybrid approach,\ndemonstrating its superior performance over existing implicit field-based\nmethods in challenging scenarios. HERO-SLAM provides a new pathway to enhance\nthe stability, performance, and applicability of neural SLAM in real-world\nscenarios. Code is available on the project page: https://hero-slam.github.io.\n", "link": "http://arxiv.org/abs/2407.18813v1", "date": "2024-07-26", "relevancy": 2.4284, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6347}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERO-SLAM%3A%20Hybrid%20Enhanced%20Robust%20Optimization%20of%20Neural%20SLAM&body=Title%3A%20HERO-SLAM%3A%20Hybrid%20Enhanced%20Robust%20Optimization%20of%20Neural%20SLAM%0AAuthor%3A%20Zhe%20Xin%20and%20Yufeng%20Yue%20and%20Liangjun%20Zhang%20and%20Chenming%20Wu%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20fundamental%20task%20in%0Arobotics%2C%20driving%20numerous%20applications%20such%20as%20autonomous%20driving%20and%20virtual%0Areality.%20Recent%20progress%20on%20neural%20implicit%20SLAM%20has%20shown%20encouraging%20and%0Aimpressive%20results.%20However%2C%20the%20robustness%20of%20neural%20SLAM%2C%20particularly%20in%0Achallenging%20or%20data-limited%20situations%2C%20remains%20an%20unresolved%20issue.%20This%20paper%0Apresents%20HERO-SLAM%2C%20a%20Hybrid%20Enhanced%20Robust%20Optimization%20method%20for%20neural%0ASLAM%2C%20which%20combines%20the%20benefits%20of%20neural%20implicit%20field%20and%20feature-metric%0Aoptimization.%20This%20hybrid%20method%20optimizes%20a%20multi-resolution%20implicit%20field%0Aand%20enhances%20robustness%20in%20challenging%20environments%20with%20sudden%20viewpoint%0Achanges%20or%20sparse%20data%20collection.%20Our%20comprehensive%20experimental%20results%20on%0Abenchmarking%20datasets%20validate%20the%20effectiveness%20of%20our%20hybrid%20approach%2C%0Ademonstrating%20its%20superior%20performance%20over%20existing%20implicit%20field-based%0Amethods%20in%20challenging%20scenarios.%20HERO-SLAM%20provides%20a%20new%20pathway%20to%20enhance%0Athe%20stability%2C%20performance%2C%20and%20applicability%20of%20neural%20SLAM%20in%20real-world%0Ascenarios.%20Code%20is%20available%20on%20the%20project%20page%3A%20https%3A//hero-slam.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERO-SLAM%253A%2520Hybrid%2520Enhanced%2520Robust%2520Optimization%2520of%2520Neural%2520SLAM%26entry.906535625%3DZhe%2520Xin%2520and%2520Yufeng%2520Yue%2520and%2520Liangjun%2520Zhang%2520and%2520Chenming%2520Wu%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520a%2520fundamental%2520task%2520in%250Arobotics%252C%2520driving%2520numerous%2520applications%2520such%2520as%2520autonomous%2520driving%2520and%2520virtual%250Areality.%2520Recent%2520progress%2520on%2520neural%2520implicit%2520SLAM%2520has%2520shown%2520encouraging%2520and%250Aimpressive%2520results.%2520However%252C%2520the%2520robustness%2520of%2520neural%2520SLAM%252C%2520particularly%2520in%250Achallenging%2520or%2520data-limited%2520situations%252C%2520remains%2520an%2520unresolved%2520issue.%2520This%2520paper%250Apresents%2520HERO-SLAM%252C%2520a%2520Hybrid%2520Enhanced%2520Robust%2520Optimization%2520method%2520for%2520neural%250ASLAM%252C%2520which%2520combines%2520the%2520benefits%2520of%2520neural%2520implicit%2520field%2520and%2520feature-metric%250Aoptimization.%2520This%2520hybrid%2520method%2520optimizes%2520a%2520multi-resolution%2520implicit%2520field%250Aand%2520enhances%2520robustness%2520in%2520challenging%2520environments%2520with%2520sudden%2520viewpoint%250Achanges%2520or%2520sparse%2520data%2520collection.%2520Our%2520comprehensive%2520experimental%2520results%2520on%250Abenchmarking%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520hybrid%2520approach%252C%250Ademonstrating%2520its%2520superior%2520performance%2520over%2520existing%2520implicit%2520field-based%250Amethods%2520in%2520challenging%2520scenarios.%2520HERO-SLAM%2520provides%2520a%2520new%2520pathway%2520to%2520enhance%250Athe%2520stability%252C%2520performance%252C%2520and%2520applicability%2520of%2520neural%2520SLAM%2520in%2520real-world%250Ascenarios.%2520Code%2520is%2520available%2520on%2520the%2520project%2520page%253A%2520https%253A//hero-slam.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERO-SLAM%3A%20Hybrid%20Enhanced%20Robust%20Optimization%20of%20Neural%20SLAM&entry.906535625=Zhe%20Xin%20and%20Yufeng%20Yue%20and%20Liangjun%20Zhang%20and%20Chenming%20Wu&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20fundamental%20task%20in%0Arobotics%2C%20driving%20numerous%20applications%20such%20as%20autonomous%20driving%20and%20virtual%0Areality.%20Recent%20progress%20on%20neural%20implicit%20SLAM%20has%20shown%20encouraging%20and%0Aimpressive%20results.%20However%2C%20the%20robustness%20of%20neural%20SLAM%2C%20particularly%20in%0Achallenging%20or%20data-limited%20situations%2C%20remains%20an%20unresolved%20issue.%20This%20paper%0Apresents%20HERO-SLAM%2C%20a%20Hybrid%20Enhanced%20Robust%20Optimization%20method%20for%20neural%0ASLAM%2C%20which%20combines%20the%20benefits%20of%20neural%20implicit%20field%20and%20feature-metric%0Aoptimization.%20This%20hybrid%20method%20optimizes%20a%20multi-resolution%20implicit%20field%0Aand%20enhances%20robustness%20in%20challenging%20environments%20with%20sudden%20viewpoint%0Achanges%20or%20sparse%20data%20collection.%20Our%20comprehensive%20experimental%20results%20on%0Abenchmarking%20datasets%20validate%20the%20effectiveness%20of%20our%20hybrid%20approach%2C%0Ademonstrating%20its%20superior%20performance%20over%20existing%20implicit%20field-based%0Amethods%20in%20challenging%20scenarios.%20HERO-SLAM%20provides%20a%20new%20pathway%20to%20enhance%0Athe%20stability%2C%20performance%2C%20and%20applicability%20of%20neural%20SLAM%20in%20real-world%0Ascenarios.%20Code%20is%20available%20on%20the%20project%20page%3A%20https%3A//hero-slam.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18813v1&entry.124074799=Read"},
{"title": "The Role of Temporal Hierarchy in Spiking Neural Networks", "author": "Filippo Moro and Pau Vilimelis Aceituno and Laura Kriener and Melika Payvand", "abstract": "  Spiking Neural Networks (SNNs) have the potential for rich spatio-temporal\nsignal processing thanks to exploiting both spatial and temporal parameters.\nThe temporal dynamics such as time constants of the synapses and neurons and\ndelays have been recently shown to have computational benefits that help reduce\nthe overall number of parameters required in the network and increase the\naccuracy of the SNNs in solving temporal tasks. Optimizing such temporal\nparameters, for example, through gradient descent, gives rise to a temporal\narchitecture for different problems. As has been shown in machine learning, to\nreduce the cost of optimization, architectural biases can be applied, in this\ncase in the temporal domain. Such inductive biases in temporal parameters have\nbeen found in neuroscience studies, highlighting a hierarchy of temporal\nstructure and input representation in different layers of the cortex. Motivated\nby this, we propose to impose a hierarchy of temporal representation in the\nhidden layers of SNNs, highlighting that such an inductive bias improves their\nperformance. We demonstrate the positive effects of temporal hierarchy in the\ntime constants of feed-forward SNNs applied to temporal tasks (Multi-Time-Scale\nXOR and Keyword Spotting, with a benefit of up to 4.1% in classification\naccuracy). Moreover, we show that such architectural biases, i.e. hierarchy of\ntime constants, naturally emerge when optimizing the time constants through\ngradient descent, initialized as homogeneous values. We further pursue this\nproposal in temporal convolutional SNNs, by introducing the hierarchical bias\nin the size and dilation of temporal kernels, giving rise to competitive\nresults in popular temporal spike-based datasets.\n", "link": "http://arxiv.org/abs/2407.18838v1", "date": "2024-07-26", "relevancy": 2.4251, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5041}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Temporal%20Hierarchy%20in%20Spiking%20Neural%20Networks&body=Title%3A%20The%20Role%20of%20Temporal%20Hierarchy%20in%20Spiking%20Neural%20Networks%0AAuthor%3A%20Filippo%20Moro%20and%20Pau%20Vilimelis%20Aceituno%20and%20Laura%20Kriener%20and%20Melika%20Payvand%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20the%20potential%20for%20rich%20spatio-temporal%0Asignal%20processing%20thanks%20to%20exploiting%20both%20spatial%20and%20temporal%20parameters.%0AThe%20temporal%20dynamics%20such%20as%20time%20constants%20of%20the%20synapses%20and%20neurons%20and%0Adelays%20have%20been%20recently%20shown%20to%20have%20computational%20benefits%20that%20help%20reduce%0Athe%20overall%20number%20of%20parameters%20required%20in%20the%20network%20and%20increase%20the%0Aaccuracy%20of%20the%20SNNs%20in%20solving%20temporal%20tasks.%20Optimizing%20such%20temporal%0Aparameters%2C%20for%20example%2C%20through%20gradient%20descent%2C%20gives%20rise%20to%20a%20temporal%0Aarchitecture%20for%20different%20problems.%20As%20has%20been%20shown%20in%20machine%20learning%2C%20to%0Areduce%20the%20cost%20of%20optimization%2C%20architectural%20biases%20can%20be%20applied%2C%20in%20this%0Acase%20in%20the%20temporal%20domain.%20Such%20inductive%20biases%20in%20temporal%20parameters%20have%0Abeen%20found%20in%20neuroscience%20studies%2C%20highlighting%20a%20hierarchy%20of%20temporal%0Astructure%20and%20input%20representation%20in%20different%20layers%20of%20the%20cortex.%20Motivated%0Aby%20this%2C%20we%20propose%20to%20impose%20a%20hierarchy%20of%20temporal%20representation%20in%20the%0Ahidden%20layers%20of%20SNNs%2C%20highlighting%20that%20such%20an%20inductive%20bias%20improves%20their%0Aperformance.%20We%20demonstrate%20the%20positive%20effects%20of%20temporal%20hierarchy%20in%20the%0Atime%20constants%20of%20feed-forward%20SNNs%20applied%20to%20temporal%20tasks%20%28Multi-Time-Scale%0AXOR%20and%20Keyword%20Spotting%2C%20with%20a%20benefit%20of%20up%20to%204.1%25%20in%20classification%0Aaccuracy%29.%20Moreover%2C%20we%20show%20that%20such%20architectural%20biases%2C%20i.e.%20hierarchy%20of%0Atime%20constants%2C%20naturally%20emerge%20when%20optimizing%20the%20time%20constants%20through%0Agradient%20descent%2C%20initialized%20as%20homogeneous%20values.%20We%20further%20pursue%20this%0Aproposal%20in%20temporal%20convolutional%20SNNs%2C%20by%20introducing%20the%20hierarchical%20bias%0Ain%20the%20size%20and%20dilation%20of%20temporal%20kernels%2C%20giving%20rise%20to%20competitive%0Aresults%20in%20popular%20temporal%20spike-based%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Temporal%2520Hierarchy%2520in%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DFilippo%2520Moro%2520and%2520Pau%2520Vilimelis%2520Aceituno%2520and%2520Laura%2520Kriener%2520and%2520Melika%2520Payvand%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520the%2520potential%2520for%2520rich%2520spatio-temporal%250Asignal%2520processing%2520thanks%2520to%2520exploiting%2520both%2520spatial%2520and%2520temporal%2520parameters.%250AThe%2520temporal%2520dynamics%2520such%2520as%2520time%2520constants%2520of%2520the%2520synapses%2520and%2520neurons%2520and%250Adelays%2520have%2520been%2520recently%2520shown%2520to%2520have%2520computational%2520benefits%2520that%2520help%2520reduce%250Athe%2520overall%2520number%2520of%2520parameters%2520required%2520in%2520the%2520network%2520and%2520increase%2520the%250Aaccuracy%2520of%2520the%2520SNNs%2520in%2520solving%2520temporal%2520tasks.%2520Optimizing%2520such%2520temporal%250Aparameters%252C%2520for%2520example%252C%2520through%2520gradient%2520descent%252C%2520gives%2520rise%2520to%2520a%2520temporal%250Aarchitecture%2520for%2520different%2520problems.%2520As%2520has%2520been%2520shown%2520in%2520machine%2520learning%252C%2520to%250Areduce%2520the%2520cost%2520of%2520optimization%252C%2520architectural%2520biases%2520can%2520be%2520applied%252C%2520in%2520this%250Acase%2520in%2520the%2520temporal%2520domain.%2520Such%2520inductive%2520biases%2520in%2520temporal%2520parameters%2520have%250Abeen%2520found%2520in%2520neuroscience%2520studies%252C%2520highlighting%2520a%2520hierarchy%2520of%2520temporal%250Astructure%2520and%2520input%2520representation%2520in%2520different%2520layers%2520of%2520the%2520cortex.%2520Motivated%250Aby%2520this%252C%2520we%2520propose%2520to%2520impose%2520a%2520hierarchy%2520of%2520temporal%2520representation%2520in%2520the%250Ahidden%2520layers%2520of%2520SNNs%252C%2520highlighting%2520that%2520such%2520an%2520inductive%2520bias%2520improves%2520their%250Aperformance.%2520We%2520demonstrate%2520the%2520positive%2520effects%2520of%2520temporal%2520hierarchy%2520in%2520the%250Atime%2520constants%2520of%2520feed-forward%2520SNNs%2520applied%2520to%2520temporal%2520tasks%2520%2528Multi-Time-Scale%250AXOR%2520and%2520Keyword%2520Spotting%252C%2520with%2520a%2520benefit%2520of%2520up%2520to%25204.1%2525%2520in%2520classification%250Aaccuracy%2529.%2520Moreover%252C%2520we%2520show%2520that%2520such%2520architectural%2520biases%252C%2520i.e.%2520hierarchy%2520of%250Atime%2520constants%252C%2520naturally%2520emerge%2520when%2520optimizing%2520the%2520time%2520constants%2520through%250Agradient%2520descent%252C%2520initialized%2520as%2520homogeneous%2520values.%2520We%2520further%2520pursue%2520this%250Aproposal%2520in%2520temporal%2520convolutional%2520SNNs%252C%2520by%2520introducing%2520the%2520hierarchical%2520bias%250Ain%2520the%2520size%2520and%2520dilation%2520of%2520temporal%2520kernels%252C%2520giving%2520rise%2520to%2520competitive%250Aresults%2520in%2520popular%2520temporal%2520spike-based%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Temporal%20Hierarchy%20in%20Spiking%20Neural%20Networks&entry.906535625=Filippo%20Moro%20and%20Pau%20Vilimelis%20Aceituno%20and%20Laura%20Kriener%20and%20Melika%20Payvand&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20have%20the%20potential%20for%20rich%20spatio-temporal%0Asignal%20processing%20thanks%20to%20exploiting%20both%20spatial%20and%20temporal%20parameters.%0AThe%20temporal%20dynamics%20such%20as%20time%20constants%20of%20the%20synapses%20and%20neurons%20and%0Adelays%20have%20been%20recently%20shown%20to%20have%20computational%20benefits%20that%20help%20reduce%0Athe%20overall%20number%20of%20parameters%20required%20in%20the%20network%20and%20increase%20the%0Aaccuracy%20of%20the%20SNNs%20in%20solving%20temporal%20tasks.%20Optimizing%20such%20temporal%0Aparameters%2C%20for%20example%2C%20through%20gradient%20descent%2C%20gives%20rise%20to%20a%20temporal%0Aarchitecture%20for%20different%20problems.%20As%20has%20been%20shown%20in%20machine%20learning%2C%20to%0Areduce%20the%20cost%20of%20optimization%2C%20architectural%20biases%20can%20be%20applied%2C%20in%20this%0Acase%20in%20the%20temporal%20domain.%20Such%20inductive%20biases%20in%20temporal%20parameters%20have%0Abeen%20found%20in%20neuroscience%20studies%2C%20highlighting%20a%20hierarchy%20of%20temporal%0Astructure%20and%20input%20representation%20in%20different%20layers%20of%20the%20cortex.%20Motivated%0Aby%20this%2C%20we%20propose%20to%20impose%20a%20hierarchy%20of%20temporal%20representation%20in%20the%0Ahidden%20layers%20of%20SNNs%2C%20highlighting%20that%20such%20an%20inductive%20bias%20improves%20their%0Aperformance.%20We%20demonstrate%20the%20positive%20effects%20of%20temporal%20hierarchy%20in%20the%0Atime%20constants%20of%20feed-forward%20SNNs%20applied%20to%20temporal%20tasks%20%28Multi-Time-Scale%0AXOR%20and%20Keyword%20Spotting%2C%20with%20a%20benefit%20of%20up%20to%204.1%25%20in%20classification%0Aaccuracy%29.%20Moreover%2C%20we%20show%20that%20such%20architectural%20biases%2C%20i.e.%20hierarchy%20of%0Atime%20constants%2C%20naturally%20emerge%20when%20optimizing%20the%20time%20constants%20through%0Agradient%20descent%2C%20initialized%20as%20homogeneous%20values.%20We%20further%20pursue%20this%0Aproposal%20in%20temporal%20convolutional%20SNNs%2C%20by%20introducing%20the%20hierarchical%20bias%0Ain%20the%20size%20and%20dilation%20of%20temporal%20kernels%2C%20giving%20rise%20to%20competitive%0Aresults%20in%20popular%20temporal%20spike-based%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18838v1&entry.124074799=Read"},
{"title": "AEP$n$P: A Less-constrained EP$n$P Solver for Pose Estimation with\n  Anisotropic Scaling", "author": "Jiaxin Wei and Stefan Leutenegger and Laurent Kneip", "abstract": "  Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for pose\nestimation in various applications. In this paper, we present a new approach to\nthe P$n$P problem with relaxed constraints, eliminating the need for precise 3D\ncoordinates, which is especially suitable for object pose estimation where\ncorresponding object models may not be available in practice. Built upon the\nclassical EP$n$P solver, we refer to it as AEP$n$P due to its ability to handle\nunknown anisotropic scaling factors in addition to the common 6D\ntransformation. Through a few algebraic manipulations and a well-chosen frame\nof reference, this new problem can be boiled down to a simple linear null-space\nproblem followed by point registration-based identification of a similarity\ntransformation. Experimental results on both simulated and real datasets\ndemonstrate the effectiveness of AEP$n$P as a flexible and practical solution\nto object pose estimation. Code: https://github.com/goldoak/AEPnP.\n", "link": "http://arxiv.org/abs/2310.09982v4", "date": "2024-07-26", "relevancy": 2.4047, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4824}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AEP%24n%24P%3A%20A%20Less-constrained%20EP%24n%24P%20Solver%20for%20Pose%20Estimation%20with%0A%20%20Anisotropic%20Scaling&body=Title%3A%20AEP%24n%24P%3A%20A%20Less-constrained%20EP%24n%24P%20Solver%20for%20Pose%20Estimation%20with%0A%20%20Anisotropic%20Scaling%0AAuthor%3A%20Jiaxin%20Wei%20and%20Stefan%20Leutenegger%20and%20Laurent%20Kneip%0AAbstract%3A%20%20%20Perspective-%24n%24-Point%20%28P%24n%24P%29%20stands%20as%20a%20fundamental%20algorithm%20for%20pose%0Aestimation%20in%20various%20applications.%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20to%0Athe%20P%24n%24P%20problem%20with%20relaxed%20constraints%2C%20eliminating%20the%20need%20for%20precise%203D%0Acoordinates%2C%20which%20is%20especially%20suitable%20for%20object%20pose%20estimation%20where%0Acorresponding%20object%20models%20may%20not%20be%20available%20in%20practice.%20Built%20upon%20the%0Aclassical%20EP%24n%24P%20solver%2C%20we%20refer%20to%20it%20as%20AEP%24n%24P%20due%20to%20its%20ability%20to%20handle%0Aunknown%20anisotropic%20scaling%20factors%20in%20addition%20to%20the%20common%206D%0Atransformation.%20Through%20a%20few%20algebraic%20manipulations%20and%20a%20well-chosen%20frame%0Aof%20reference%2C%20this%20new%20problem%20can%20be%20boiled%20down%20to%20a%20simple%20linear%20null-space%0Aproblem%20followed%20by%20point%20registration-based%20identification%20of%20a%20similarity%0Atransformation.%20Experimental%20results%20on%20both%20simulated%20and%20real%20datasets%0Ademonstrate%20the%20effectiveness%20of%20AEP%24n%24P%20as%20a%20flexible%20and%20practical%20solution%0Ato%20object%20pose%20estimation.%20Code%3A%20https%3A//github.com/goldoak/AEPnP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09982v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAEP%2524n%2524P%253A%2520A%2520Less-constrained%2520EP%2524n%2524P%2520Solver%2520for%2520Pose%2520Estimation%2520with%250A%2520%2520Anisotropic%2520Scaling%26entry.906535625%3DJiaxin%2520Wei%2520and%2520Stefan%2520Leutenegger%2520and%2520Laurent%2520Kneip%26entry.1292438233%3D%2520%2520Perspective-%2524n%2524-Point%2520%2528P%2524n%2524P%2529%2520stands%2520as%2520a%2520fundamental%2520algorithm%2520for%2520pose%250Aestimation%2520in%2520various%2520applications.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520approach%2520to%250Athe%2520P%2524n%2524P%2520problem%2520with%2520relaxed%2520constraints%252C%2520eliminating%2520the%2520need%2520for%2520precise%25203D%250Acoordinates%252C%2520which%2520is%2520especially%2520suitable%2520for%2520object%2520pose%2520estimation%2520where%250Acorresponding%2520object%2520models%2520may%2520not%2520be%2520available%2520in%2520practice.%2520Built%2520upon%2520the%250Aclassical%2520EP%2524n%2524P%2520solver%252C%2520we%2520refer%2520to%2520it%2520as%2520AEP%2524n%2524P%2520due%2520to%2520its%2520ability%2520to%2520handle%250Aunknown%2520anisotropic%2520scaling%2520factors%2520in%2520addition%2520to%2520the%2520common%25206D%250Atransformation.%2520Through%2520a%2520few%2520algebraic%2520manipulations%2520and%2520a%2520well-chosen%2520frame%250Aof%2520reference%252C%2520this%2520new%2520problem%2520can%2520be%2520boiled%2520down%2520to%2520a%2520simple%2520linear%2520null-space%250Aproblem%2520followed%2520by%2520point%2520registration-based%2520identification%2520of%2520a%2520similarity%250Atransformation.%2520Experimental%2520results%2520on%2520both%2520simulated%2520and%2520real%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520AEP%2524n%2524P%2520as%2520a%2520flexible%2520and%2520practical%2520solution%250Ato%2520object%2520pose%2520estimation.%2520Code%253A%2520https%253A//github.com/goldoak/AEPnP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09982v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AEP%24n%24P%3A%20A%20Less-constrained%20EP%24n%24P%20Solver%20for%20Pose%20Estimation%20with%0A%20%20Anisotropic%20Scaling&entry.906535625=Jiaxin%20Wei%20and%20Stefan%20Leutenegger%20and%20Laurent%20Kneip&entry.1292438233=%20%20Perspective-%24n%24-Point%20%28P%24n%24P%29%20stands%20as%20a%20fundamental%20algorithm%20for%20pose%0Aestimation%20in%20various%20applications.%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20to%0Athe%20P%24n%24P%20problem%20with%20relaxed%20constraints%2C%20eliminating%20the%20need%20for%20precise%203D%0Acoordinates%2C%20which%20is%20especially%20suitable%20for%20object%20pose%20estimation%20where%0Acorresponding%20object%20models%20may%20not%20be%20available%20in%20practice.%20Built%20upon%20the%0Aclassical%20EP%24n%24P%20solver%2C%20we%20refer%20to%20it%20as%20AEP%24n%24P%20due%20to%20its%20ability%20to%20handle%0Aunknown%20anisotropic%20scaling%20factors%20in%20addition%20to%20the%20common%206D%0Atransformation.%20Through%20a%20few%20algebraic%20manipulations%20and%20a%20well-chosen%20frame%0Aof%20reference%2C%20this%20new%20problem%20can%20be%20boiled%20down%20to%20a%20simple%20linear%20null-space%0Aproblem%20followed%20by%20point%20registration-based%20identification%20of%20a%20similarity%0Atransformation.%20Experimental%20results%20on%20both%20simulated%20and%20real%20datasets%0Ademonstrate%20the%20effectiveness%20of%20AEP%24n%24P%20as%20a%20flexible%20and%20practical%20solution%0Ato%20object%20pose%20estimation.%20Code%3A%20https%3A//github.com/goldoak/AEPnP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09982v4&entry.124074799=Read"},
{"title": "Finite Neural Networks as Mixtures of Gaussian Processes: From Provable\n  Error Bounds to Prior Selection", "author": "Steven Adams and  Patan\u00e8 and Morteza Lahijanian and Luca Laurenti", "abstract": "  Infinitely wide or deep neural networks (NNs) with independent and\nidentically distributed (i.i.d.) parameters have been shown to be equivalent to\nGaussian processes. Because of the favorable properties of Gaussian processes,\nthis equivalence is commonly employed to analyze neural networks and has led to\nvarious breakthroughs over the years. However, neural networks and Gaussian\nprocesses are equivalent only in the limit; in the finite case there are\ncurrently no methods available to approximate a trained neural network with a\nGaussian model with bounds on the approximation error. In this work, we present\nan algorithmic framework to approximate a neural network of finite width and\ndepth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian\nprocesses with error bounds on the approximation error. In particular, we\nconsider the Wasserstein distance to quantify the closeness between\nprobabilistic models and, by relying on tools from optimal transport and\nGaussian processes, we iteratively approximate the output distribution of each\nlayer of the neural network as a mixture of Gaussian processes. Crucially, for\nany NN and $\\epsilon >0$ our approach is able to return a mixture of Gaussian\nprocesses that is $\\epsilon$-close to the NN at a finite set of input points.\nFurthermore, we rely on the differentiability of the resulting error bound to\nshow how our approach can be employed to tune the parameters of a NN to mimic\nthe functional behavior of a given Gaussian process, e.g., for prior selection\nin the context of Bayesian inference. We empirically investigate the\neffectiveness of our results on both regression and classification problems\nwith various neural network architectures. Our experiments highlight how our\nresults can represent an important step towards understanding neural network\npredictions and formally quantifying their uncertainty.\n", "link": "http://arxiv.org/abs/2407.18707v1", "date": "2024-07-26", "relevancy": 2.378, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4996}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4711}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite%20Neural%20Networks%20as%20Mixtures%20of%20Gaussian%20Processes%3A%20From%20Provable%0A%20%20Error%20Bounds%20to%20Prior%20Selection&body=Title%3A%20Finite%20Neural%20Networks%20as%20Mixtures%20of%20Gaussian%20Processes%3A%20From%20Provable%0A%20%20Error%20Bounds%20to%20Prior%20Selection%0AAuthor%3A%20Steven%20Adams%20and%20%20Patan%C3%A8%20and%20Morteza%20Lahijanian%20and%20Luca%20Laurenti%0AAbstract%3A%20%20%20Infinitely%20wide%20or%20deep%20neural%20networks%20%28NNs%29%20with%20independent%20and%0Aidentically%20distributed%20%28i.i.d.%29%20parameters%20have%20been%20shown%20to%20be%20equivalent%20to%0AGaussian%20processes.%20Because%20of%20the%20favorable%20properties%20of%20Gaussian%20processes%2C%0Athis%20equivalence%20is%20commonly%20employed%20to%20analyze%20neural%20networks%20and%20has%20led%20to%0Avarious%20breakthroughs%20over%20the%20years.%20However%2C%20neural%20networks%20and%20Gaussian%0Aprocesses%20are%20equivalent%20only%20in%20the%20limit%3B%20in%20the%20finite%20case%20there%20are%0Acurrently%20no%20methods%20available%20to%20approximate%20a%20trained%20neural%20network%20with%20a%0AGaussian%20model%20with%20bounds%20on%20the%20approximation%20error.%20In%20this%20work%2C%20we%20present%0Aan%20algorithmic%20framework%20to%20approximate%20a%20neural%20network%20of%20finite%20width%20and%0Adepth%2C%20and%20with%20not%20necessarily%20i.i.d.%20parameters%2C%20with%20a%20mixture%20of%20Gaussian%0Aprocesses%20with%20error%20bounds%20on%20the%20approximation%20error.%20In%20particular%2C%20we%0Aconsider%20the%20Wasserstein%20distance%20to%20quantify%20the%20closeness%20between%0Aprobabilistic%20models%20and%2C%20by%20relying%20on%20tools%20from%20optimal%20transport%20and%0AGaussian%20processes%2C%20we%20iteratively%20approximate%20the%20output%20distribution%20of%20each%0Alayer%20of%20the%20neural%20network%20as%20a%20mixture%20of%20Gaussian%20processes.%20Crucially%2C%20for%0Aany%20NN%20and%20%24%5Cepsilon%20%3E0%24%20our%20approach%20is%20able%20to%20return%20a%20mixture%20of%20Gaussian%0Aprocesses%20that%20is%20%24%5Cepsilon%24-close%20to%20the%20NN%20at%20a%20finite%20set%20of%20input%20points.%0AFurthermore%2C%20we%20rely%20on%20the%20differentiability%20of%20the%20resulting%20error%20bound%20to%0Ashow%20how%20our%20approach%20can%20be%20employed%20to%20tune%20the%20parameters%20of%20a%20NN%20to%20mimic%0Athe%20functional%20behavior%20of%20a%20given%20Gaussian%20process%2C%20e.g.%2C%20for%20prior%20selection%0Ain%20the%20context%20of%20Bayesian%20inference.%20We%20empirically%20investigate%20the%0Aeffectiveness%20of%20our%20results%20on%20both%20regression%20and%20classification%20problems%0Awith%20various%20neural%20network%20architectures.%20Our%20experiments%20highlight%20how%20our%0Aresults%20can%20represent%20an%20important%20step%20towards%20understanding%20neural%20network%0Apredictions%20and%20formally%20quantifying%20their%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite%2520Neural%2520Networks%2520as%2520Mixtures%2520of%2520Gaussian%2520Processes%253A%2520From%2520Provable%250A%2520%2520Error%2520Bounds%2520to%2520Prior%2520Selection%26entry.906535625%3DSteven%2520Adams%2520and%2520%2520Patan%25C3%25A8%2520and%2520Morteza%2520Lahijanian%2520and%2520Luca%2520Laurenti%26entry.1292438233%3D%2520%2520Infinitely%2520wide%2520or%2520deep%2520neural%2520networks%2520%2528NNs%2529%2520with%2520independent%2520and%250Aidentically%2520distributed%2520%2528i.i.d.%2529%2520parameters%2520have%2520been%2520shown%2520to%2520be%2520equivalent%2520to%250AGaussian%2520processes.%2520Because%2520of%2520the%2520favorable%2520properties%2520of%2520Gaussian%2520processes%252C%250Athis%2520equivalence%2520is%2520commonly%2520employed%2520to%2520analyze%2520neural%2520networks%2520and%2520has%2520led%2520to%250Avarious%2520breakthroughs%2520over%2520the%2520years.%2520However%252C%2520neural%2520networks%2520and%2520Gaussian%250Aprocesses%2520are%2520equivalent%2520only%2520in%2520the%2520limit%253B%2520in%2520the%2520finite%2520case%2520there%2520are%250Acurrently%2520no%2520methods%2520available%2520to%2520approximate%2520a%2520trained%2520neural%2520network%2520with%2520a%250AGaussian%2520model%2520with%2520bounds%2520on%2520the%2520approximation%2520error.%2520In%2520this%2520work%252C%2520we%2520present%250Aan%2520algorithmic%2520framework%2520to%2520approximate%2520a%2520neural%2520network%2520of%2520finite%2520width%2520and%250Adepth%252C%2520and%2520with%2520not%2520necessarily%2520i.i.d.%2520parameters%252C%2520with%2520a%2520mixture%2520of%2520Gaussian%250Aprocesses%2520with%2520error%2520bounds%2520on%2520the%2520approximation%2520error.%2520In%2520particular%252C%2520we%250Aconsider%2520the%2520Wasserstein%2520distance%2520to%2520quantify%2520the%2520closeness%2520between%250Aprobabilistic%2520models%2520and%252C%2520by%2520relying%2520on%2520tools%2520from%2520optimal%2520transport%2520and%250AGaussian%2520processes%252C%2520we%2520iteratively%2520approximate%2520the%2520output%2520distribution%2520of%2520each%250Alayer%2520of%2520the%2520neural%2520network%2520as%2520a%2520mixture%2520of%2520Gaussian%2520processes.%2520Crucially%252C%2520for%250Aany%2520NN%2520and%2520%2524%255Cepsilon%2520%253E0%2524%2520our%2520approach%2520is%2520able%2520to%2520return%2520a%2520mixture%2520of%2520Gaussian%250Aprocesses%2520that%2520is%2520%2524%255Cepsilon%2524-close%2520to%2520the%2520NN%2520at%2520a%2520finite%2520set%2520of%2520input%2520points.%250AFurthermore%252C%2520we%2520rely%2520on%2520the%2520differentiability%2520of%2520the%2520resulting%2520error%2520bound%2520to%250Ashow%2520how%2520our%2520approach%2520can%2520be%2520employed%2520to%2520tune%2520the%2520parameters%2520of%2520a%2520NN%2520to%2520mimic%250Athe%2520functional%2520behavior%2520of%2520a%2520given%2520Gaussian%2520process%252C%2520e.g.%252C%2520for%2520prior%2520selection%250Ain%2520the%2520context%2520of%2520Bayesian%2520inference.%2520We%2520empirically%2520investigate%2520the%250Aeffectiveness%2520of%2520our%2520results%2520on%2520both%2520regression%2520and%2520classification%2520problems%250Awith%2520various%2520neural%2520network%2520architectures.%2520Our%2520experiments%2520highlight%2520how%2520our%250Aresults%2520can%2520represent%2520an%2520important%2520step%2520towards%2520understanding%2520neural%2520network%250Apredictions%2520and%2520formally%2520quantifying%2520their%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite%20Neural%20Networks%20as%20Mixtures%20of%20Gaussian%20Processes%3A%20From%20Provable%0A%20%20Error%20Bounds%20to%20Prior%20Selection&entry.906535625=Steven%20Adams%20and%20%20Patan%C3%A8%20and%20Morteza%20Lahijanian%20and%20Luca%20Laurenti&entry.1292438233=%20%20Infinitely%20wide%20or%20deep%20neural%20networks%20%28NNs%29%20with%20independent%20and%0Aidentically%20distributed%20%28i.i.d.%29%20parameters%20have%20been%20shown%20to%20be%20equivalent%20to%0AGaussian%20processes.%20Because%20of%20the%20favorable%20properties%20of%20Gaussian%20processes%2C%0Athis%20equivalence%20is%20commonly%20employed%20to%20analyze%20neural%20networks%20and%20has%20led%20to%0Avarious%20breakthroughs%20over%20the%20years.%20However%2C%20neural%20networks%20and%20Gaussian%0Aprocesses%20are%20equivalent%20only%20in%20the%20limit%3B%20in%20the%20finite%20case%20there%20are%0Acurrently%20no%20methods%20available%20to%20approximate%20a%20trained%20neural%20network%20with%20a%0AGaussian%20model%20with%20bounds%20on%20the%20approximation%20error.%20In%20this%20work%2C%20we%20present%0Aan%20algorithmic%20framework%20to%20approximate%20a%20neural%20network%20of%20finite%20width%20and%0Adepth%2C%20and%20with%20not%20necessarily%20i.i.d.%20parameters%2C%20with%20a%20mixture%20of%20Gaussian%0Aprocesses%20with%20error%20bounds%20on%20the%20approximation%20error.%20In%20particular%2C%20we%0Aconsider%20the%20Wasserstein%20distance%20to%20quantify%20the%20closeness%20between%0Aprobabilistic%20models%20and%2C%20by%20relying%20on%20tools%20from%20optimal%20transport%20and%0AGaussian%20processes%2C%20we%20iteratively%20approximate%20the%20output%20distribution%20of%20each%0Alayer%20of%20the%20neural%20network%20as%20a%20mixture%20of%20Gaussian%20processes.%20Crucially%2C%20for%0Aany%20NN%20and%20%24%5Cepsilon%20%3E0%24%20our%20approach%20is%20able%20to%20return%20a%20mixture%20of%20Gaussian%0Aprocesses%20that%20is%20%24%5Cepsilon%24-close%20to%20the%20NN%20at%20a%20finite%20set%20of%20input%20points.%0AFurthermore%2C%20we%20rely%20on%20the%20differentiability%20of%20the%20resulting%20error%20bound%20to%0Ashow%20how%20our%20approach%20can%20be%20employed%20to%20tune%20the%20parameters%20of%20a%20NN%20to%20mimic%0Athe%20functional%20behavior%20of%20a%20given%20Gaussian%20process%2C%20e.g.%2C%20for%20prior%20selection%0Ain%20the%20context%20of%20Bayesian%20inference.%20We%20empirically%20investigate%20the%0Aeffectiveness%20of%20our%20results%20on%20both%20regression%20and%20classification%20problems%0Awith%20various%20neural%20network%20architectures.%20Our%20experiments%20highlight%20how%20our%0Aresults%20can%20represent%20an%20important%20step%20towards%20understanding%20neural%20network%0Apredictions%20and%20formally%20quantifying%20their%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18707v1&entry.124074799=Read"},
{"title": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview", "author": "Yuhang Ming and Xingrui Yang and Weihan Wang and Zheng Chen and Jinglun Feng and Yifan Xing and Guofeng Zhang", "abstract": "  Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D\nscene representation, offering high-fidelity renderings and reconstructions\nfrom a set of sparse and unstructured sensor data. In the context of autonomous\nrobotics, where perception and understanding of the environment are pivotal,\nNeRF holds immense promise for improving performance. In this paper, we present\na comprehensive survey and analysis of the state-of-the-art techniques for\nutilizing NeRF to enhance the capabilities of autonomous robots. We especially\nfocus on the perception, localization and navigation, and decision-making\nmodules of autonomous robots and delve into tasks crucial for autonomous\noperation, including 3D reconstruction, segmentation, pose estimation,\nsimultaneous localization and mapping (SLAM), navigation and planning, and\ninteraction. Our survey meticulously benchmarks existing NeRF-based methods,\nproviding insights into their strengths and limitations. Moreover, we explore\npromising avenues for future research and development in this domain. Notably,\nwe discuss the integration of advanced techniques such as 3D Gaussian splatting\n(3DGS), large language models (LLM), and generative AIs, envisioning enhanced\nreconstruction efficiency, scene understanding, decision-making capabilities.\nThis survey serves as a roadmap for researchers seeking to leverage NeRFs to\nempower autonomous robots, paving the way for innovative solutions that can\nnavigate and interact seamlessly in complex environments.\n", "link": "http://arxiv.org/abs/2405.05526v2", "date": "2024-07-26", "relevancy": 2.3411, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.596}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Neural%20Radiance%20Fields%20for%20Autonomous%20Robots%3A%20An%20Overview&body=Title%3A%20Benchmarking%20Neural%20Radiance%20Fields%20for%20Autonomous%20Robots%3A%20An%20Overview%0AAuthor%3A%20Yuhang%20Ming%20and%20Xingrui%20Yang%20and%20Weihan%20Wang%20and%20Zheng%20Chen%20and%20Jinglun%20Feng%20and%20Yifan%20Xing%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Ascene%20representation%2C%20offering%20high-fidelity%20renderings%20and%20reconstructions%0Afrom%20a%20set%20of%20sparse%20and%20unstructured%20sensor%20data.%20In%20the%20context%20of%20autonomous%0Arobotics%2C%20where%20perception%20and%20understanding%20of%20the%20environment%20are%20pivotal%2C%0ANeRF%20holds%20immense%20promise%20for%20improving%20performance.%20In%20this%20paper%2C%20we%20present%0Aa%20comprehensive%20survey%20and%20analysis%20of%20the%20state-of-the-art%20techniques%20for%0Autilizing%20NeRF%20to%20enhance%20the%20capabilities%20of%20autonomous%20robots.%20We%20especially%0Afocus%20on%20the%20perception%2C%20localization%20and%20navigation%2C%20and%20decision-making%0Amodules%20of%20autonomous%20robots%20and%20delve%20into%20tasks%20crucial%20for%20autonomous%0Aoperation%2C%20including%203D%20reconstruction%2C%20segmentation%2C%20pose%20estimation%2C%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%2C%20navigation%20and%20planning%2C%20and%0Ainteraction.%20Our%20survey%20meticulously%20benchmarks%20existing%20NeRF-based%20methods%2C%0Aproviding%20insights%20into%20their%20strengths%20and%20limitations.%20Moreover%2C%20we%20explore%0Apromising%20avenues%20for%20future%20research%20and%20development%20in%20this%20domain.%20Notably%2C%0Awe%20discuss%20the%20integration%20of%20advanced%20techniques%20such%20as%203D%20Gaussian%20splatting%0A%283DGS%29%2C%20large%20language%20models%20%28LLM%29%2C%20and%20generative%20AIs%2C%20envisioning%20enhanced%0Areconstruction%20efficiency%2C%20scene%20understanding%2C%20decision-making%20capabilities.%0AThis%20survey%20serves%20as%20a%20roadmap%20for%20researchers%20seeking%20to%20leverage%20NeRFs%20to%0Aempower%20autonomous%20robots%2C%20paving%20the%20way%20for%20innovative%20solutions%20that%20can%0Anavigate%20and%20interact%20seamlessly%20in%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Neural%2520Radiance%2520Fields%2520for%2520Autonomous%2520Robots%253A%2520An%2520Overview%26entry.906535625%3DYuhang%2520Ming%2520and%2520Xingrui%2520Yang%2520and%2520Weihan%2520Wang%2520and%2520Zheng%2520Chen%2520and%2520Jinglun%2520Feng%2520and%2520Yifan%2520Xing%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%25203D%250Ascene%2520representation%252C%2520offering%2520high-fidelity%2520renderings%2520and%2520reconstructions%250Afrom%2520a%2520set%2520of%2520sparse%2520and%2520unstructured%2520sensor%2520data.%2520In%2520the%2520context%2520of%2520autonomous%250Arobotics%252C%2520where%2520perception%2520and%2520understanding%2520of%2520the%2520environment%2520are%2520pivotal%252C%250ANeRF%2520holds%2520immense%2520promise%2520for%2520improving%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520comprehensive%2520survey%2520and%2520analysis%2520of%2520the%2520state-of-the-art%2520techniques%2520for%250Autilizing%2520NeRF%2520to%2520enhance%2520the%2520capabilities%2520of%2520autonomous%2520robots.%2520We%2520especially%250Afocus%2520on%2520the%2520perception%252C%2520localization%2520and%2520navigation%252C%2520and%2520decision-making%250Amodules%2520of%2520autonomous%2520robots%2520and%2520delve%2520into%2520tasks%2520crucial%2520for%2520autonomous%250Aoperation%252C%2520including%25203D%2520reconstruction%252C%2520segmentation%252C%2520pose%2520estimation%252C%250Asimultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%252C%2520navigation%2520and%2520planning%252C%2520and%250Ainteraction.%2520Our%2520survey%2520meticulously%2520benchmarks%2520existing%2520NeRF-based%2520methods%252C%250Aproviding%2520insights%2520into%2520their%2520strengths%2520and%2520limitations.%2520Moreover%252C%2520we%2520explore%250Apromising%2520avenues%2520for%2520future%2520research%2520and%2520development%2520in%2520this%2520domain.%2520Notably%252C%250Awe%2520discuss%2520the%2520integration%2520of%2520advanced%2520techniques%2520such%2520as%25203D%2520Gaussian%2520splatting%250A%25283DGS%2529%252C%2520large%2520language%2520models%2520%2528LLM%2529%252C%2520and%2520generative%2520AIs%252C%2520envisioning%2520enhanced%250Areconstruction%2520efficiency%252C%2520scene%2520understanding%252C%2520decision-making%2520capabilities.%250AThis%2520survey%2520serves%2520as%2520a%2520roadmap%2520for%2520researchers%2520seeking%2520to%2520leverage%2520NeRFs%2520to%250Aempower%2520autonomous%2520robots%252C%2520paving%2520the%2520way%2520for%2520innovative%2520solutions%2520that%2520can%250Anavigate%2520and%2520interact%2520seamlessly%2520in%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Neural%20Radiance%20Fields%20for%20Autonomous%20Robots%3A%20An%20Overview&entry.906535625=Yuhang%20Ming%20and%20Xingrui%20Yang%20and%20Weihan%20Wang%20and%20Zheng%20Chen%20and%20Jinglun%20Feng%20and%20Yifan%20Xing%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%0Ascene%20representation%2C%20offering%20high-fidelity%20renderings%20and%20reconstructions%0Afrom%20a%20set%20of%20sparse%20and%20unstructured%20sensor%20data.%20In%20the%20context%20of%20autonomous%0Arobotics%2C%20where%20perception%20and%20understanding%20of%20the%20environment%20are%20pivotal%2C%0ANeRF%20holds%20immense%20promise%20for%20improving%20performance.%20In%20this%20paper%2C%20we%20present%0Aa%20comprehensive%20survey%20and%20analysis%20of%20the%20state-of-the-art%20techniques%20for%0Autilizing%20NeRF%20to%20enhance%20the%20capabilities%20of%20autonomous%20robots.%20We%20especially%0Afocus%20on%20the%20perception%2C%20localization%20and%20navigation%2C%20and%20decision-making%0Amodules%20of%20autonomous%20robots%20and%20delve%20into%20tasks%20crucial%20for%20autonomous%0Aoperation%2C%20including%203D%20reconstruction%2C%20segmentation%2C%20pose%20estimation%2C%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%2C%20navigation%20and%20planning%2C%20and%0Ainteraction.%20Our%20survey%20meticulously%20benchmarks%20existing%20NeRF-based%20methods%2C%0Aproviding%20insights%20into%20their%20strengths%20and%20limitations.%20Moreover%2C%20we%20explore%0Apromising%20avenues%20for%20future%20research%20and%20development%20in%20this%20domain.%20Notably%2C%0Awe%20discuss%20the%20integration%20of%20advanced%20techniques%20such%20as%203D%20Gaussian%20splatting%0A%283DGS%29%2C%20large%20language%20models%20%28LLM%29%2C%20and%20generative%20AIs%2C%20envisioning%20enhanced%0Areconstruction%20efficiency%2C%20scene%20understanding%2C%20decision-making%20capabilities.%0AThis%20survey%20serves%20as%20a%20roadmap%20for%20researchers%20seeking%20to%20leverage%20NeRFs%20to%0Aempower%20autonomous%20robots%2C%20paving%20the%20way%20for%20innovative%20solutions%20that%20can%0Anavigate%20and%20interact%20seamlessly%20in%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05526v2&entry.124074799=Read"},
{"title": "SCB-dataset: A Dataset for Detecting Student Classroom Behavior", "author": "Fan Yang", "abstract": "  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n", "link": "http://arxiv.org/abs/2304.02488v2", "date": "2024-07-26", "relevancy": 2.3083, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior&body=Title%3A%20SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior%0AAuthor%3A%20Fan%20Yang%0AAbstract%3A%20%20%20The%20use%20of%20deep%20learning%20methods%20for%20automatic%20detection%20of%20students%27%0Aclassroom%20behavior%20is%20a%20promising%20approach%20to%20analyze%20their%20class%20performance%0Aand%20enhance%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset%29%20that%20reflects%20real-life%20scenarios.%20Our%20dataset%20includes%2011%2C248%0Alabels%20and%204%2C003%20images%2C%20with%20a%20focus%20on%20hand-raising%20behavior.%20We%20evaluated%0Athe%20dataset%20using%20the%20YOLOv7%20algorithm%2C%20achieving%20a%20mean%20average%20precision%0A%28map%29%20of%20up%20to%2085.3%25.%20We%20believe%20that%20our%20dataset%20can%20serve%20as%20a%20robust%0Afoundation%20for%20future%20research%20in%20the%20field%20of%20student%20behavior%20detection%20and%0Apromote%20further%20advancements%20in%20this%20area.Our%20SCB-dataset%20can%20be%20downloaded%0Afrom%3A%20https%3A//github.com/Whiffe/SCB-dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCB-dataset%253A%2520A%2520Dataset%2520for%2520Detecting%2520Student%2520Classroom%2520Behavior%26entry.906535625%3DFan%2520Yang%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520deep%2520learning%2520methods%2520for%2520automatic%2520detection%2520of%2520students%2527%250Aclassroom%2520behavior%2520is%2520a%2520promising%2520approach%2520to%2520analyze%2520their%2520class%2520performance%250Aand%2520enhance%2520teaching%2520effectiveness.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%250Adatasets%2520on%2520student%2520behavior%2520poses%2520a%2520challenge%2520for%2520researchers%2520in%2520this%2520field.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Student%2520Classroom%2520Behavior%2520dataset%250A%2528SCB-dataset%2529%2520that%2520reflects%2520real-life%2520scenarios.%2520Our%2520dataset%2520includes%252011%252C248%250Alabels%2520and%25204%252C003%2520images%252C%2520with%2520a%2520focus%2520on%2520hand-raising%2520behavior.%2520We%2520evaluated%250Athe%2520dataset%2520using%2520the%2520YOLOv7%2520algorithm%252C%2520achieving%2520a%2520mean%2520average%2520precision%250A%2528map%2529%2520of%2520up%2520to%252085.3%2525.%2520We%2520believe%2520that%2520our%2520dataset%2520can%2520serve%2520as%2520a%2520robust%250Afoundation%2520for%2520future%2520research%2520in%2520the%2520field%2520of%2520student%2520behavior%2520detection%2520and%250Apromote%2520further%2520advancements%2520in%2520this%2520area.Our%2520SCB-dataset%2520can%2520be%2520downloaded%250Afrom%253A%2520https%253A//github.com/Whiffe/SCB-dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior&entry.906535625=Fan%20Yang&entry.1292438233=%20%20The%20use%20of%20deep%20learning%20methods%20for%20automatic%20detection%20of%20students%27%0Aclassroom%20behavior%20is%20a%20promising%20approach%20to%20analyze%20their%20class%20performance%0Aand%20enhance%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset%29%20that%20reflects%20real-life%20scenarios.%20Our%20dataset%20includes%2011%2C248%0Alabels%20and%204%2C003%20images%2C%20with%20a%20focus%20on%20hand-raising%20behavior.%20We%20evaluated%0Athe%20dataset%20using%20the%20YOLOv7%20algorithm%2C%20achieving%20a%20mean%20average%20precision%0A%28map%29%20of%20up%20to%2085.3%25.%20We%20believe%20that%20our%20dataset%20can%20serve%20as%20a%20robust%0Afoundation%20for%20future%20research%20in%20the%20field%20of%20student%20behavior%20detection%20and%0Apromote%20further%20advancements%20in%20this%20area.Our%20SCB-dataset%20can%20be%20downloaded%0Afrom%3A%20https%3A//github.com/Whiffe/SCB-dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02488v2&entry.124074799=Read"},
{"title": "MUVO: A Multimodal World Model with Spatial Representations for\n  Autonomous Driving", "author": "Daniel Bogdoll and Yitian Yang and Tim Joseph and J. Marius Z\u00f6llner", "abstract": "  Learning unsupervised world models for autonomous driving has the potential\nto improve the reasoning capabilities of today's systems dramatically. However,\nmost work neglects the physical attributes of the world and focuses on sensor\ndata alone. We propose MUVO, a MUltimodal World Model with spatial VOxel\nrepresentations, to address this challenge. We utilize raw camera and lidar\ndata to learn a sensor-agnostic geometric representation of the world. We\ndemonstrate multimodal future predictions and show that our spatial\nrepresentation improves the prediction quality of both camera images and lidar\npoint clouds.\n", "link": "http://arxiv.org/abs/2311.11762v3", "date": "2024-07-26", "relevancy": 2.2979, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUVO%3A%20A%20Multimodal%20World%20Model%20with%20Spatial%20Representations%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20MUVO%3A%20A%20Multimodal%20World%20Model%20with%20Spatial%20Representations%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Daniel%20Bogdoll%20and%20Yitian%20Yang%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Learning%20unsupervised%20world%20models%20for%20autonomous%20driving%20has%20the%20potential%0Ato%20improve%20the%20reasoning%20capabilities%20of%20today%27s%20systems%20dramatically.%20However%2C%0Amost%20work%20neglects%20the%20physical%20attributes%20of%20the%20world%20and%20focuses%20on%20sensor%0Adata%20alone.%20We%20propose%20MUVO%2C%20a%20MUltimodal%20World%20Model%20with%20spatial%20VOxel%0Arepresentations%2C%20to%20address%20this%20challenge.%20We%20utilize%20raw%20camera%20and%20lidar%0Adata%20to%20learn%20a%20sensor-agnostic%20geometric%20representation%20of%20the%20world.%20We%0Ademonstrate%20multimodal%20future%20predictions%20and%20show%20that%20our%20spatial%0Arepresentation%20improves%20the%20prediction%20quality%20of%20both%20camera%20images%20and%20lidar%0Apoint%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUVO%253A%2520A%2520Multimodal%2520World%2520Model%2520with%2520Spatial%2520Representations%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DDaniel%2520Bogdoll%2520and%2520Yitian%2520Yang%2520and%2520Tim%2520Joseph%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Learning%2520unsupervised%2520world%2520models%2520for%2520autonomous%2520driving%2520has%2520the%2520potential%250Ato%2520improve%2520the%2520reasoning%2520capabilities%2520of%2520today%2527s%2520systems%2520dramatically.%2520However%252C%250Amost%2520work%2520neglects%2520the%2520physical%2520attributes%2520of%2520the%2520world%2520and%2520focuses%2520on%2520sensor%250Adata%2520alone.%2520We%2520propose%2520MUVO%252C%2520a%2520MUltimodal%2520World%2520Model%2520with%2520spatial%2520VOxel%250Arepresentations%252C%2520to%2520address%2520this%2520challenge.%2520We%2520utilize%2520raw%2520camera%2520and%2520lidar%250Adata%2520to%2520learn%2520a%2520sensor-agnostic%2520geometric%2520representation%2520of%2520the%2520world.%2520We%250Ademonstrate%2520multimodal%2520future%2520predictions%2520and%2520show%2520that%2520our%2520spatial%250Arepresentation%2520improves%2520the%2520prediction%2520quality%2520of%2520both%2520camera%2520images%2520and%2520lidar%250Apoint%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUVO%3A%20A%20Multimodal%20World%20Model%20with%20Spatial%20Representations%20for%0A%20%20Autonomous%20Driving&entry.906535625=Daniel%20Bogdoll%20and%20Yitian%20Yang%20and%20Tim%20Joseph%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Learning%20unsupervised%20world%20models%20for%20autonomous%20driving%20has%20the%20potential%0Ato%20improve%20the%20reasoning%20capabilities%20of%20today%27s%20systems%20dramatically.%20However%2C%0Amost%20work%20neglects%20the%20physical%20attributes%20of%20the%20world%20and%20focuses%20on%20sensor%0Adata%20alone.%20We%20propose%20MUVO%2C%20a%20MUltimodal%20World%20Model%20with%20spatial%20VOxel%0Arepresentations%2C%20to%20address%20this%20challenge.%20We%20utilize%20raw%20camera%20and%20lidar%0Adata%20to%20learn%20a%20sensor-agnostic%20geometric%20representation%20of%20the%20world.%20We%0Ademonstrate%20multimodal%20future%20predictions%20and%20show%20that%20our%20spatial%0Arepresentation%20improves%20the%20prediction%20quality%20of%20both%20camera%20images%20and%20lidar%0Apoint%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11762v3&entry.124074799=Read"},
{"title": "DART: An Automated End-to-End Object Detection Pipeline with Data\n  Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label\n  Review, and Model Training", "author": "Chen Xin and Andreas Hartel and Enkelejda Kasneci", "abstract": "  Accurate real-time object detection is vital across numerous industrial\napplications, from safety monitoring to quality control. Traditional\napproaches, however, are hindered by arduous manual annotation and data\ncollection, struggling to adapt to ever-changing environments and novel target\nobjects. To address these limitations, this paper presents DART, an innovative\nautomated end-to-end pipeline that revolutionizes object detection workflows\nfrom data collection to model evaluation. It eliminates the need for laborious\nhuman labeling and extensive data collection while achieving outstanding\naccuracy across diverse scenarios. DART encompasses four key stages: (1) Data\nDiversification using subject-driven image generation (DreamBooth with SDXL),\n(2) Annotation via open-vocabulary object detection (Grounding DINO) to\ngenerate bounding box and class labels (3) Review of generated images and\npseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee\ncredibility, (4) Training of real-time object detectors (YOLOv8 and YOLOv10)\nusing the verified data as ground truth. We apply DART to a self-collected\ndataset of construction machines named Liebherr Product, which contains over\n15K high-quality images across 23 categories. The current instantiation of DART\nsignificantly increases average precision (AP) from 0.064 to 0.832. Its modular\ndesign ensures easy exchangeability and extensibility, allowing for future\nalgorithm upgrades, seamless integration of new object categories, and\nadaptability to customized environments without manual labeling and additional\ndata collection. The code and dataset are released at\nhttps://github.com/chen-xin-94/DART.\n", "link": "http://arxiv.org/abs/2407.09174v2", "date": "2024-07-26", "relevancy": 2.254, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training&body=Title%3A%20DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training%0AAuthor%3A%20Chen%20Xin%20and%20Andreas%20Hartel%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20Accurate%20real-time%20object%20detection%20is%20vital%20across%20numerous%20industrial%0Aapplications%2C%20from%20safety%20monitoring%20to%20quality%20control.%20Traditional%0Aapproaches%2C%20however%2C%20are%20hindered%20by%20arduous%20manual%20annotation%20and%20data%0Acollection%2C%20struggling%20to%20adapt%20to%20ever-changing%20environments%20and%20novel%20target%0Aobjects.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20DART%2C%20an%20innovative%0Aautomated%20end-to-end%20pipeline%20that%20revolutionizes%20object%20detection%20workflows%0Afrom%20data%20collection%20to%20model%20evaluation.%20It%20eliminates%20the%20need%20for%20laborious%0Ahuman%20labeling%20and%20extensive%20data%20collection%20while%20achieving%20outstanding%0Aaccuracy%20across%20diverse%20scenarios.%20DART%20encompasses%20four%20key%20stages%3A%20%281%29%20Data%0ADiversification%20using%20subject-driven%20image%20generation%20%28DreamBooth%20with%20SDXL%29%2C%0A%282%29%20Annotation%20via%20open-vocabulary%20object%20detection%20%28Grounding%20DINO%29%20to%0Agenerate%20bounding%20box%20and%20class%20labels%20%283%29%20Review%20of%20generated%20images%20and%0Apseudo-labels%20by%20large%20multimodal%20models%20%28InternVL-1.5%20and%20GPT-4o%29%20to%20guarantee%0Acredibility%2C%20%284%29%20Training%20of%20real-time%20object%20detectors%20%28YOLOv8%20and%20YOLOv10%29%0Ausing%20the%20verified%20data%20as%20ground%20truth.%20We%20apply%20DART%20to%20a%20self-collected%0Adataset%20of%20construction%20machines%20named%20Liebherr%20Product%2C%20which%20contains%20over%0A15K%20high-quality%20images%20across%2023%20categories.%20The%20current%20instantiation%20of%20DART%0Asignificantly%20increases%20average%20precision%20%28AP%29%20from%200.064%20to%200.832.%20Its%20modular%0Adesign%20ensures%20easy%20exchangeability%20and%20extensibility%2C%20allowing%20for%20future%0Aalgorithm%20upgrades%2C%20seamless%20integration%20of%20new%20object%20categories%2C%20and%0Aadaptability%20to%20customized%20environments%20without%20manual%20labeling%20and%20additional%0Adata%20collection.%20The%20code%20and%20dataset%20are%20released%20at%0Ahttps%3A//github.com/chen-xin-94/DART.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART%253A%2520An%2520Automated%2520End-to-End%2520Object%2520Detection%2520Pipeline%2520with%2520Data%250A%2520%2520Diversification%252C%2520Open-Vocabulary%2520Bounding%2520Box%2520Annotation%252C%2520Pseudo-Label%250A%2520%2520Review%252C%2520and%2520Model%2520Training%26entry.906535625%3DChen%2520Xin%2520and%2520Andreas%2520Hartel%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520Accurate%2520real-time%2520object%2520detection%2520is%2520vital%2520across%2520numerous%2520industrial%250Aapplications%252C%2520from%2520safety%2520monitoring%2520to%2520quality%2520control.%2520Traditional%250Aapproaches%252C%2520however%252C%2520are%2520hindered%2520by%2520arduous%2520manual%2520annotation%2520and%2520data%250Acollection%252C%2520struggling%2520to%2520adapt%2520to%2520ever-changing%2520environments%2520and%2520novel%2520target%250Aobjects.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520presents%2520DART%252C%2520an%2520innovative%250Aautomated%2520end-to-end%2520pipeline%2520that%2520revolutionizes%2520object%2520detection%2520workflows%250Afrom%2520data%2520collection%2520to%2520model%2520evaluation.%2520It%2520eliminates%2520the%2520need%2520for%2520laborious%250Ahuman%2520labeling%2520and%2520extensive%2520data%2520collection%2520while%2520achieving%2520outstanding%250Aaccuracy%2520across%2520diverse%2520scenarios.%2520DART%2520encompasses%2520four%2520key%2520stages%253A%2520%25281%2529%2520Data%250ADiversification%2520using%2520subject-driven%2520image%2520generation%2520%2528DreamBooth%2520with%2520SDXL%2529%252C%250A%25282%2529%2520Annotation%2520via%2520open-vocabulary%2520object%2520detection%2520%2528Grounding%2520DINO%2529%2520to%250Agenerate%2520bounding%2520box%2520and%2520class%2520labels%2520%25283%2529%2520Review%2520of%2520generated%2520images%2520and%250Apseudo-labels%2520by%2520large%2520multimodal%2520models%2520%2528InternVL-1.5%2520and%2520GPT-4o%2529%2520to%2520guarantee%250Acredibility%252C%2520%25284%2529%2520Training%2520of%2520real-time%2520object%2520detectors%2520%2528YOLOv8%2520and%2520YOLOv10%2529%250Ausing%2520the%2520verified%2520data%2520as%2520ground%2520truth.%2520We%2520apply%2520DART%2520to%2520a%2520self-collected%250Adataset%2520of%2520construction%2520machines%2520named%2520Liebherr%2520Product%252C%2520which%2520contains%2520over%250A15K%2520high-quality%2520images%2520across%252023%2520categories.%2520The%2520current%2520instantiation%2520of%2520DART%250Asignificantly%2520increases%2520average%2520precision%2520%2528AP%2529%2520from%25200.064%2520to%25200.832.%2520Its%2520modular%250Adesign%2520ensures%2520easy%2520exchangeability%2520and%2520extensibility%252C%2520allowing%2520for%2520future%250Aalgorithm%2520upgrades%252C%2520seamless%2520integration%2520of%2520new%2520object%2520categories%252C%2520and%250Aadaptability%2520to%2520customized%2520environments%2520without%2520manual%2520labeling%2520and%2520additional%250Adata%2520collection.%2520The%2520code%2520and%2520dataset%2520are%2520released%2520at%250Ahttps%253A//github.com/chen-xin-94/DART.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20An%20Automated%20End-to-End%20Object%20Detection%20Pipeline%20with%20Data%0A%20%20Diversification%2C%20Open-Vocabulary%20Bounding%20Box%20Annotation%2C%20Pseudo-Label%0A%20%20Review%2C%20and%20Model%20Training&entry.906535625=Chen%20Xin%20and%20Andreas%20Hartel%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Accurate%20real-time%20object%20detection%20is%20vital%20across%20numerous%20industrial%0Aapplications%2C%20from%20safety%20monitoring%20to%20quality%20control.%20Traditional%0Aapproaches%2C%20however%2C%20are%20hindered%20by%20arduous%20manual%20annotation%20and%20data%0Acollection%2C%20struggling%20to%20adapt%20to%20ever-changing%20environments%20and%20novel%20target%0Aobjects.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20DART%2C%20an%20innovative%0Aautomated%20end-to-end%20pipeline%20that%20revolutionizes%20object%20detection%20workflows%0Afrom%20data%20collection%20to%20model%20evaluation.%20It%20eliminates%20the%20need%20for%20laborious%0Ahuman%20labeling%20and%20extensive%20data%20collection%20while%20achieving%20outstanding%0Aaccuracy%20across%20diverse%20scenarios.%20DART%20encompasses%20four%20key%20stages%3A%20%281%29%20Data%0ADiversification%20using%20subject-driven%20image%20generation%20%28DreamBooth%20with%20SDXL%29%2C%0A%282%29%20Annotation%20via%20open-vocabulary%20object%20detection%20%28Grounding%20DINO%29%20to%0Agenerate%20bounding%20box%20and%20class%20labels%20%283%29%20Review%20of%20generated%20images%20and%0Apseudo-labels%20by%20large%20multimodal%20models%20%28InternVL-1.5%20and%20GPT-4o%29%20to%20guarantee%0Acredibility%2C%20%284%29%20Training%20of%20real-time%20object%20detectors%20%28YOLOv8%20and%20YOLOv10%29%0Ausing%20the%20verified%20data%20as%20ground%20truth.%20We%20apply%20DART%20to%20a%20self-collected%0Adataset%20of%20construction%20machines%20named%20Liebherr%20Product%2C%20which%20contains%20over%0A15K%20high-quality%20images%20across%2023%20categories.%20The%20current%20instantiation%20of%20DART%0Asignificantly%20increases%20average%20precision%20%28AP%29%20from%200.064%20to%200.832.%20Its%20modular%0Adesign%20ensures%20easy%20exchangeability%20and%20extensibility%2C%20allowing%20for%20future%0Aalgorithm%20upgrades%2C%20seamless%20integration%20of%20new%20object%20categories%2C%20and%0Aadaptability%20to%20customized%20environments%20without%20manual%20labeling%20and%20additional%0Adata%20collection.%20The%20code%20and%20dataset%20are%20released%20at%0Ahttps%3A//github.com/chen-xin-94/DART.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09174v2&entry.124074799=Read"},
{"title": "Outlier detection by ensembling uncertainty with negative objectness", "author": "Anja Deli\u0107 and Matej Grci\u0107 and Sini\u0161a \u0160egvi\u0107", "abstract": "  Outlier detection is an essential capability in safety-critical applications\nof supervised visual recognition. Most of the existing methods deliver best\nresults by encouraging standard closed-set models to produce low-confidence\npredictions in negative training data. However, that approach conflates\nprediction uncertainty with recognition of the negative class. We therefore\nreconsider direct prediction of K+1 logits that correspond to K groundtruth\nclasses and one outlier class. This setup allows us to formulate a novel\nanomaly score as an ensemble of in-distribution uncertainty and the posterior\nof the outlier class which we term negative objectness. Now outliers can be\nindependently detected due to i) high prediction uncertainty or ii) similarity\nwith negative data. We embed our method into a dense prediction architecture\nwith mask-level recognition over K+2 classes. The training procedure encourages\nthe novel K+2-th class to learn negative objectness at pasted negative\ninstances. Our models outperform the current state-of-the art on standard\nbenchmarks for image-wide and pixel-level outlier detection with and without\ntraining on real negative data.\n", "link": "http://arxiv.org/abs/2402.15374v3", "date": "2024-07-26", "relevancy": 2.2357, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5749}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5713}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness&body=Title%3A%20Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness%0AAuthor%3A%20Anja%20Deli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87%0AAbstract%3A%20%20%20Outlier%20detection%20is%20an%20essential%20capability%20in%20safety-critical%20applications%0Aof%20supervised%20visual%20recognition.%20Most%20of%20the%20existing%20methods%20deliver%20best%0Aresults%20by%20encouraging%20standard%20closed-set%20models%20to%20produce%20low-confidence%0Apredictions%20in%20negative%20training%20data.%20However%2C%20that%20approach%20conflates%0Aprediction%20uncertainty%20with%20recognition%20of%20the%20negative%20class.%20We%20therefore%0Areconsider%20direct%20prediction%20of%20K%2B1%20logits%20that%20correspond%20to%20K%20groundtruth%0Aclasses%20and%20one%20outlier%20class.%20This%20setup%20allows%20us%20to%20formulate%20a%20novel%0Aanomaly%20score%20as%20an%20ensemble%20of%20in-distribution%20uncertainty%20and%20the%20posterior%0Aof%20the%20outlier%20class%20which%20we%20term%20negative%20objectness.%20Now%20outliers%20can%20be%0Aindependently%20detected%20due%20to%20i%29%20high%20prediction%20uncertainty%20or%20ii%29%20similarity%0Awith%20negative%20data.%20We%20embed%20our%20method%20into%20a%20dense%20prediction%20architecture%0Awith%20mask-level%20recognition%20over%20K%2B2%20classes.%20The%20training%20procedure%20encourages%0Athe%20novel%20K%2B2-th%20class%20to%20learn%20negative%20objectness%20at%20pasted%20negative%0Ainstances.%20Our%20models%20outperform%20the%20current%20state-of-the%20art%20on%20standard%0Abenchmarks%20for%20image-wide%20and%20pixel-level%20outlier%20detection%20with%20and%20without%0Atraining%20on%20real%20negative%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15374v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutlier%2520detection%2520by%2520ensembling%2520uncertainty%2520with%2520negative%2520objectness%26entry.906535625%3DAnja%2520Deli%25C4%2587%2520and%2520Matej%2520Grci%25C4%2587%2520and%2520Sini%25C5%25A1a%2520%25C5%25A0egvi%25C4%2587%26entry.1292438233%3D%2520%2520Outlier%2520detection%2520is%2520an%2520essential%2520capability%2520in%2520safety-critical%2520applications%250Aof%2520supervised%2520visual%2520recognition.%2520Most%2520of%2520the%2520existing%2520methods%2520deliver%2520best%250Aresults%2520by%2520encouraging%2520standard%2520closed-set%2520models%2520to%2520produce%2520low-confidence%250Apredictions%2520in%2520negative%2520training%2520data.%2520However%252C%2520that%2520approach%2520conflates%250Aprediction%2520uncertainty%2520with%2520recognition%2520of%2520the%2520negative%2520class.%2520We%2520therefore%250Areconsider%2520direct%2520prediction%2520of%2520K%252B1%2520logits%2520that%2520correspond%2520to%2520K%2520groundtruth%250Aclasses%2520and%2520one%2520outlier%2520class.%2520This%2520setup%2520allows%2520us%2520to%2520formulate%2520a%2520novel%250Aanomaly%2520score%2520as%2520an%2520ensemble%2520of%2520in-distribution%2520uncertainty%2520and%2520the%2520posterior%250Aof%2520the%2520outlier%2520class%2520which%2520we%2520term%2520negative%2520objectness.%2520Now%2520outliers%2520can%2520be%250Aindependently%2520detected%2520due%2520to%2520i%2529%2520high%2520prediction%2520uncertainty%2520or%2520ii%2529%2520similarity%250Awith%2520negative%2520data.%2520We%2520embed%2520our%2520method%2520into%2520a%2520dense%2520prediction%2520architecture%250Awith%2520mask-level%2520recognition%2520over%2520K%252B2%2520classes.%2520The%2520training%2520procedure%2520encourages%250Athe%2520novel%2520K%252B2-th%2520class%2520to%2520learn%2520negative%2520objectness%2520at%2520pasted%2520negative%250Ainstances.%2520Our%2520models%2520outperform%2520the%2520current%2520state-of-the%2520art%2520on%2520standard%250Abenchmarks%2520for%2520image-wide%2520and%2520pixel-level%2520outlier%2520detection%2520with%2520and%2520without%250Atraining%2520on%2520real%2520negative%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15374v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outlier%20detection%20by%20ensembling%20uncertainty%20with%20negative%20objectness&entry.906535625=Anja%20Deli%C4%87%20and%20Matej%20Grci%C4%87%20and%20Sini%C5%A1a%20%C5%A0egvi%C4%87&entry.1292438233=%20%20Outlier%20detection%20is%20an%20essential%20capability%20in%20safety-critical%20applications%0Aof%20supervised%20visual%20recognition.%20Most%20of%20the%20existing%20methods%20deliver%20best%0Aresults%20by%20encouraging%20standard%20closed-set%20models%20to%20produce%20low-confidence%0Apredictions%20in%20negative%20training%20data.%20However%2C%20that%20approach%20conflates%0Aprediction%20uncertainty%20with%20recognition%20of%20the%20negative%20class.%20We%20therefore%0Areconsider%20direct%20prediction%20of%20K%2B1%20logits%20that%20correspond%20to%20K%20groundtruth%0Aclasses%20and%20one%20outlier%20class.%20This%20setup%20allows%20us%20to%20formulate%20a%20novel%0Aanomaly%20score%20as%20an%20ensemble%20of%20in-distribution%20uncertainty%20and%20the%20posterior%0Aof%20the%20outlier%20class%20which%20we%20term%20negative%20objectness.%20Now%20outliers%20can%20be%0Aindependently%20detected%20due%20to%20i%29%20high%20prediction%20uncertainty%20or%20ii%29%20similarity%0Awith%20negative%20data.%20We%20embed%20our%20method%20into%20a%20dense%20prediction%20architecture%0Awith%20mask-level%20recognition%20over%20K%2B2%20classes.%20The%20training%20procedure%20encourages%0Athe%20novel%20K%2B2-th%20class%20to%20learn%20negative%20objectness%20at%20pasted%20negative%0Ainstances.%20Our%20models%20outperform%20the%20current%20state-of-the%20art%20on%20standard%0Abenchmarks%20for%20image-wide%20and%20pixel-level%20outlier%20detection%20with%20and%20without%0Atraining%20on%20real%20negative%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15374v3&entry.124074799=Read"},
{"title": "Selective Vision-Language Subspace Projection for Few-shot CLIP", "author": "Xingyu Zhu and Beier Zhu and Yi Tan and Shuo Wang and Yanbin Hao and Hanwang Zhang", "abstract": "  Vision-language models such as CLIP are capable of mapping the different\nmodality data into a unified feature space, enabling zero/few-shot inference by\nmeasuring the similarity of given images and texts. However, most existing\nmethods overlook modality gaps in CLIP's encoded features, which is shown as\nthe text and image features lie far apart from each other, resulting in limited\nclassification performance. To tackle this issue, we introduce a method called\nSelective Vision-Language Subspace Projection (SSP), which incorporates local\nimage features and utilizes them as a bridge to enhance the alignment between\nimage-text pairs. Specifically, our SSP framework comprises two parallel\nmodules: a vision projector and a language projector. Both projectors utilize\nlocal image features to span the respective subspaces for image and texts,\nthereby projecting the image and text features into their respective subspaces\nto achieve alignment. Moreover, our approach entails only training-free matrix\ncalculations and can be seamlessly integrated into advanced CLIP-based few-shot\nlearning frameworks. Extensive experiments on 11 datasets have demonstrated\nSSP's superior text-image alignment capabilities, outperforming the\nstate-of-the-art alignment methods. The code is available at\nhttps://github.com/zhuhsingyuu/SSP\n", "link": "http://arxiv.org/abs/2407.16977v2", "date": "2024-07-26", "relevancy": 2.2275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.616}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Vision-Language%20Subspace%20Projection%20for%20Few-shot%20CLIP&body=Title%3A%20Selective%20Vision-Language%20Subspace%20Projection%20for%20Few-shot%20CLIP%0AAuthor%3A%20Xingyu%20Zhu%20and%20Beier%20Zhu%20and%20Yi%20Tan%20and%20Shuo%20Wang%20and%20Yanbin%20Hao%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Vision-language%20models%20such%20as%20CLIP%20are%20capable%20of%20mapping%20the%20different%0Amodality%20data%20into%20a%20unified%20feature%20space%2C%20enabling%20zero/few-shot%20inference%20by%0Ameasuring%20the%20similarity%20of%20given%20images%20and%20texts.%20However%2C%20most%20existing%0Amethods%20overlook%20modality%20gaps%20in%20CLIP%27s%20encoded%20features%2C%20which%20is%20shown%20as%0Athe%20text%20and%20image%20features%20lie%20far%20apart%20from%20each%20other%2C%20resulting%20in%20limited%0Aclassification%20performance.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20method%20called%0ASelective%20Vision-Language%20Subspace%20Projection%20%28SSP%29%2C%20which%20incorporates%20local%0Aimage%20features%20and%20utilizes%20them%20as%20a%20bridge%20to%20enhance%20the%20alignment%20between%0Aimage-text%20pairs.%20Specifically%2C%20our%20SSP%20framework%20comprises%20two%20parallel%0Amodules%3A%20a%20vision%20projector%20and%20a%20language%20projector.%20Both%20projectors%20utilize%0Alocal%20image%20features%20to%20span%20the%20respective%20subspaces%20for%20image%20and%20texts%2C%0Athereby%20projecting%20the%20image%20and%20text%20features%20into%20their%20respective%20subspaces%0Ato%20achieve%20alignment.%20Moreover%2C%20our%20approach%20entails%20only%20training-free%20matrix%0Acalculations%20and%20can%20be%20seamlessly%20integrated%20into%20advanced%20CLIP-based%20few-shot%0Alearning%20frameworks.%20Extensive%20experiments%20on%2011%20datasets%20have%20demonstrated%0ASSP%27s%20superior%20text-image%20alignment%20capabilities%2C%20outperforming%20the%0Astate-of-the-art%20alignment%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuhsingyuu/SSP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Vision-Language%2520Subspace%2520Projection%2520for%2520Few-shot%2520CLIP%26entry.906535625%3DXingyu%2520Zhu%2520and%2520Beier%2520Zhu%2520and%2520Yi%2520Tan%2520and%2520Shuo%2520Wang%2520and%2520Yanbin%2520Hao%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520such%2520as%2520CLIP%2520are%2520capable%2520of%2520mapping%2520the%2520different%250Amodality%2520data%2520into%2520a%2520unified%2520feature%2520space%252C%2520enabling%2520zero/few-shot%2520inference%2520by%250Ameasuring%2520the%2520similarity%2520of%2520given%2520images%2520and%2520texts.%2520However%252C%2520most%2520existing%250Amethods%2520overlook%2520modality%2520gaps%2520in%2520CLIP%2527s%2520encoded%2520features%252C%2520which%2520is%2520shown%2520as%250Athe%2520text%2520and%2520image%2520features%2520lie%2520far%2520apart%2520from%2520each%2520other%252C%2520resulting%2520in%2520limited%250Aclassification%2520performance.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520a%2520method%2520called%250ASelective%2520Vision-Language%2520Subspace%2520Projection%2520%2528SSP%2529%252C%2520which%2520incorporates%2520local%250Aimage%2520features%2520and%2520utilizes%2520them%2520as%2520a%2520bridge%2520to%2520enhance%2520the%2520alignment%2520between%250Aimage-text%2520pairs.%2520Specifically%252C%2520our%2520SSP%2520framework%2520comprises%2520two%2520parallel%250Amodules%253A%2520a%2520vision%2520projector%2520and%2520a%2520language%2520projector.%2520Both%2520projectors%2520utilize%250Alocal%2520image%2520features%2520to%2520span%2520the%2520respective%2520subspaces%2520for%2520image%2520and%2520texts%252C%250Athereby%2520projecting%2520the%2520image%2520and%2520text%2520features%2520into%2520their%2520respective%2520subspaces%250Ato%2520achieve%2520alignment.%2520Moreover%252C%2520our%2520approach%2520entails%2520only%2520training-free%2520matrix%250Acalculations%2520and%2520can%2520be%2520seamlessly%2520integrated%2520into%2520advanced%2520CLIP-based%2520few-shot%250Alearning%2520frameworks.%2520Extensive%2520experiments%2520on%252011%2520datasets%2520have%2520demonstrated%250ASSP%2527s%2520superior%2520text-image%2520alignment%2520capabilities%252C%2520outperforming%2520the%250Astate-of-the-art%2520alignment%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zhuhsingyuu/SSP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Vision-Language%20Subspace%20Projection%20for%20Few-shot%20CLIP&entry.906535625=Xingyu%20Zhu%20and%20Beier%20Zhu%20and%20Yi%20Tan%20and%20Shuo%20Wang%20and%20Yanbin%20Hao%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Vision-language%20models%20such%20as%20CLIP%20are%20capable%20of%20mapping%20the%20different%0Amodality%20data%20into%20a%20unified%20feature%20space%2C%20enabling%20zero/few-shot%20inference%20by%0Ameasuring%20the%20similarity%20of%20given%20images%20and%20texts.%20However%2C%20most%20existing%0Amethods%20overlook%20modality%20gaps%20in%20CLIP%27s%20encoded%20features%2C%20which%20is%20shown%20as%0Athe%20text%20and%20image%20features%20lie%20far%20apart%20from%20each%20other%2C%20resulting%20in%20limited%0Aclassification%20performance.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20method%20called%0ASelective%20Vision-Language%20Subspace%20Projection%20%28SSP%29%2C%20which%20incorporates%20local%0Aimage%20features%20and%20utilizes%20them%20as%20a%20bridge%20to%20enhance%20the%20alignment%20between%0Aimage-text%20pairs.%20Specifically%2C%20our%20SSP%20framework%20comprises%20two%20parallel%0Amodules%3A%20a%20vision%20projector%20and%20a%20language%20projector.%20Both%20projectors%20utilize%0Alocal%20image%20features%20to%20span%20the%20respective%20subspaces%20for%20image%20and%20texts%2C%0Athereby%20projecting%20the%20image%20and%20text%20features%20into%20their%20respective%20subspaces%0Ato%20achieve%20alignment.%20Moreover%2C%20our%20approach%20entails%20only%20training-free%20matrix%0Acalculations%20and%20can%20be%20seamlessly%20integrated%20into%20advanced%20CLIP-based%20few-shot%0Alearning%20frameworks.%20Extensive%20experiments%20on%2011%20datasets%20have%20demonstrated%0ASSP%27s%20superior%20text-image%20alignment%20capabilities%2C%20outperforming%20the%0Astate-of-the-art%20alignment%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuhsingyuu/SSP%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16977v2&entry.124074799=Read"},
{"title": "Auto DragGAN: Editing the Generative Image Manifold in an Autoregressive\n  Manner", "author": "Pengxiang Cai and Zhiwei Liu and Guibo Zhu and Yunfang Niu and Jinqiao Wang", "abstract": "  Pixel-level fine-grained image editing remains an open challenge. Previous\nworks fail to achieve an ideal trade-off between control granularity and\ninference speed. They either fail to achieve pixel-level fine-grained control,\nor their inference speed requires optimization. To address this, this paper for\nthe first time employs a regression-based network to learn the variation\npatterns of StyleGAN latent codes during the image dragging process. This\nmethod enables pixel-level precision in dragging editing with little time cost.\nUsers can specify handle points and their corresponding target points on any\nGAN-generated images, and our method will move each handle point to its\ncorresponding target point. Through experimental analysis, we discover that a\nshort movement distance from handle points to target points yields a\nhigh-fidelity edited image, as the model only needs to predict the movement of\na small portion of pixels. To achieve this, we decompose the entire movement\nprocess into multiple sub-processes. Specifically, we develop a transformer\nencoder-decoder based network named 'Latent Predictor' to predict the latent\ncode motion trajectories from handle points to target points in an\nautoregressive manner. Moreover, to enhance the prediction stability, we\nintroduce a component named 'Latent Regularizer', aimed at constraining the\nlatent code motion within the distribution of natural images. Extensive\nexperiments demonstrate that our method achieves state-of-the-art (SOTA)\ninference speed and image editing performance at the pixel-level granularity.\n", "link": "http://arxiv.org/abs/2407.18656v1", "date": "2024-07-26", "relevancy": 2.197, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5626}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5533}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto%20DragGAN%3A%20Editing%20the%20Generative%20Image%20Manifold%20in%20an%20Autoregressive%0A%20%20Manner&body=Title%3A%20Auto%20DragGAN%3A%20Editing%20the%20Generative%20Image%20Manifold%20in%20an%20Autoregressive%0A%20%20Manner%0AAuthor%3A%20Pengxiang%20Cai%20and%20Zhiwei%20Liu%20and%20Guibo%20Zhu%20and%20Yunfang%20Niu%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Pixel-level%20fine-grained%20image%20editing%20remains%20an%20open%20challenge.%20Previous%0Aworks%20fail%20to%20achieve%20an%20ideal%20trade-off%20between%20control%20granularity%20and%0Ainference%20speed.%20They%20either%20fail%20to%20achieve%20pixel-level%20fine-grained%20control%2C%0Aor%20their%20inference%20speed%20requires%20optimization.%20To%20address%20this%2C%20this%20paper%20for%0Athe%20first%20time%20employs%20a%20regression-based%20network%20to%20learn%20the%20variation%0Apatterns%20of%20StyleGAN%20latent%20codes%20during%20the%20image%20dragging%20process.%20This%0Amethod%20enables%20pixel-level%20precision%20in%20dragging%20editing%20with%20little%20time%20cost.%0AUsers%20can%20specify%20handle%20points%20and%20their%20corresponding%20target%20points%20on%20any%0AGAN-generated%20images%2C%20and%20our%20method%20will%20move%20each%20handle%20point%20to%20its%0Acorresponding%20target%20point.%20Through%20experimental%20analysis%2C%20we%20discover%20that%20a%0Ashort%20movement%20distance%20from%20handle%20points%20to%20target%20points%20yields%20a%0Ahigh-fidelity%20edited%20image%2C%20as%20the%20model%20only%20needs%20to%20predict%20the%20movement%20of%0Aa%20small%20portion%20of%20pixels.%20To%20achieve%20this%2C%20we%20decompose%20the%20entire%20movement%0Aprocess%20into%20multiple%20sub-processes.%20Specifically%2C%20we%20develop%20a%20transformer%0Aencoder-decoder%20based%20network%20named%20%27Latent%20Predictor%27%20to%20predict%20the%20latent%0Acode%20motion%20trajectories%20from%20handle%20points%20to%20target%20points%20in%20an%0Aautoregressive%20manner.%20Moreover%2C%20to%20enhance%20the%20prediction%20stability%2C%20we%0Aintroduce%20a%20component%20named%20%27Latent%20Regularizer%27%2C%20aimed%20at%20constraining%20the%0Alatent%20code%20motion%20within%20the%20distribution%20of%20natural%20images.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%0Ainference%20speed%20and%20image%20editing%20performance%20at%20the%20pixel-level%20granularity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto%2520DragGAN%253A%2520Editing%2520the%2520Generative%2520Image%2520Manifold%2520in%2520an%2520Autoregressive%250A%2520%2520Manner%26entry.906535625%3DPengxiang%2520Cai%2520and%2520Zhiwei%2520Liu%2520and%2520Guibo%2520Zhu%2520and%2520Yunfang%2520Niu%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Pixel-level%2520fine-grained%2520image%2520editing%2520remains%2520an%2520open%2520challenge.%2520Previous%250Aworks%2520fail%2520to%2520achieve%2520an%2520ideal%2520trade-off%2520between%2520control%2520granularity%2520and%250Ainference%2520speed.%2520They%2520either%2520fail%2520to%2520achieve%2520pixel-level%2520fine-grained%2520control%252C%250Aor%2520their%2520inference%2520speed%2520requires%2520optimization.%2520To%2520address%2520this%252C%2520this%2520paper%2520for%250Athe%2520first%2520time%2520employs%2520a%2520regression-based%2520network%2520to%2520learn%2520the%2520variation%250Apatterns%2520of%2520StyleGAN%2520latent%2520codes%2520during%2520the%2520image%2520dragging%2520process.%2520This%250Amethod%2520enables%2520pixel-level%2520precision%2520in%2520dragging%2520editing%2520with%2520little%2520time%2520cost.%250AUsers%2520can%2520specify%2520handle%2520points%2520and%2520their%2520corresponding%2520target%2520points%2520on%2520any%250AGAN-generated%2520images%252C%2520and%2520our%2520method%2520will%2520move%2520each%2520handle%2520point%2520to%2520its%250Acorresponding%2520target%2520point.%2520Through%2520experimental%2520analysis%252C%2520we%2520discover%2520that%2520a%250Ashort%2520movement%2520distance%2520from%2520handle%2520points%2520to%2520target%2520points%2520yields%2520a%250Ahigh-fidelity%2520edited%2520image%252C%2520as%2520the%2520model%2520only%2520needs%2520to%2520predict%2520the%2520movement%2520of%250Aa%2520small%2520portion%2520of%2520pixels.%2520To%2520achieve%2520this%252C%2520we%2520decompose%2520the%2520entire%2520movement%250Aprocess%2520into%2520multiple%2520sub-processes.%2520Specifically%252C%2520we%2520develop%2520a%2520transformer%250Aencoder-decoder%2520based%2520network%2520named%2520%2527Latent%2520Predictor%2527%2520to%2520predict%2520the%2520latent%250Acode%2520motion%2520trajectories%2520from%2520handle%2520points%2520to%2520target%2520points%2520in%2520an%250Aautoregressive%2520manner.%2520Moreover%252C%2520to%2520enhance%2520the%2520prediction%2520stability%252C%2520we%250Aintroduce%2520a%2520component%2520named%2520%2527Latent%2520Regularizer%2527%252C%2520aimed%2520at%2520constraining%2520the%250Alatent%2520code%2520motion%2520within%2520the%2520distribution%2520of%2520natural%2520images.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%250Ainference%2520speed%2520and%2520image%2520editing%2520performance%2520at%2520the%2520pixel-level%2520granularity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto%20DragGAN%3A%20Editing%20the%20Generative%20Image%20Manifold%20in%20an%20Autoregressive%0A%20%20Manner&entry.906535625=Pengxiang%20Cai%20and%20Zhiwei%20Liu%20and%20Guibo%20Zhu%20and%20Yunfang%20Niu%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Pixel-level%20fine-grained%20image%20editing%20remains%20an%20open%20challenge.%20Previous%0Aworks%20fail%20to%20achieve%20an%20ideal%20trade-off%20between%20control%20granularity%20and%0Ainference%20speed.%20They%20either%20fail%20to%20achieve%20pixel-level%20fine-grained%20control%2C%0Aor%20their%20inference%20speed%20requires%20optimization.%20To%20address%20this%2C%20this%20paper%20for%0Athe%20first%20time%20employs%20a%20regression-based%20network%20to%20learn%20the%20variation%0Apatterns%20of%20StyleGAN%20latent%20codes%20during%20the%20image%20dragging%20process.%20This%0Amethod%20enables%20pixel-level%20precision%20in%20dragging%20editing%20with%20little%20time%20cost.%0AUsers%20can%20specify%20handle%20points%20and%20their%20corresponding%20target%20points%20on%20any%0AGAN-generated%20images%2C%20and%20our%20method%20will%20move%20each%20handle%20point%20to%20its%0Acorresponding%20target%20point.%20Through%20experimental%20analysis%2C%20we%20discover%20that%20a%0Ashort%20movement%20distance%20from%20handle%20points%20to%20target%20points%20yields%20a%0Ahigh-fidelity%20edited%20image%2C%20as%20the%20model%20only%20needs%20to%20predict%20the%20movement%20of%0Aa%20small%20portion%20of%20pixels.%20To%20achieve%20this%2C%20we%20decompose%20the%20entire%20movement%0Aprocess%20into%20multiple%20sub-processes.%20Specifically%2C%20we%20develop%20a%20transformer%0Aencoder-decoder%20based%20network%20named%20%27Latent%20Predictor%27%20to%20predict%20the%20latent%0Acode%20motion%20trajectories%20from%20handle%20points%20to%20target%20points%20in%20an%0Aautoregressive%20manner.%20Moreover%2C%20to%20enhance%20the%20prediction%20stability%2C%20we%0Aintroduce%20a%20component%20named%20%27Latent%20Regularizer%27%2C%20aimed%20at%20constraining%20the%0Alatent%20code%20motion%20within%20the%20distribution%20of%20natural%20images.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%0Ainference%20speed%20and%20image%20editing%20performance%20at%20the%20pixel-level%20granularity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18656v1&entry.124074799=Read"},
{"title": "Attacks on fairness in Federated Learning", "author": "Joseph Rance and Filip Svoboda", "abstract": "  Federated Learning is an important emerging distributed training paradigm\nthat keeps data private on clients. It is now well understood that by\ncontrolling only a small subset of FL clients, it is possible to introduce a\nbackdoor to a federated learning model, in the presence of certain attributes.\nIn this paper, we present a new type of attack that compromises the fairness of\nthe trained model. Fairness is understood to be the attribute-level performance\ndistribution of a trained model. It is particularly salient in domains where,\nfor example, skewed accuracy discrimination between subpopulations could have\ndisastrous consequences. We find that by employing a threat model similar to\nthat of a backdoor attack, an attacker is able to influence the aggregated\nmodel to have an unfair performance distribution between any given set of\nattributes. Furthermore, we find that this attack is possible by controlling\nonly a single client. While combating naturally induced unfairness in FL has\npreviously been discussed in depth, its artificially induced kind has been\nneglected. We show that defending against attacks on fairness should be a\ncritical consideration in any situation where unfairness in a trained model\ncould benefit a user who participated in its training.\n", "link": "http://arxiv.org/abs/2311.12715v2", "date": "2024-07-26", "relevancy": 2.157, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4359}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4322}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attacks%20on%20fairness%20in%20Federated%20Learning&body=Title%3A%20Attacks%20on%20fairness%20in%20Federated%20Learning%0AAuthor%3A%20Joseph%20Rance%20and%20Filip%20Svoboda%0AAbstract%3A%20%20%20Federated%20Learning%20is%20an%20important%20emerging%20distributed%20training%20paradigm%0Athat%20keeps%20data%20private%20on%20clients.%20It%20is%20now%20well%20understood%20that%20by%0Acontrolling%20only%20a%20small%20subset%20of%20FL%20clients%2C%20it%20is%20possible%20to%20introduce%20a%0Abackdoor%20to%20a%20federated%20learning%20model%2C%20in%20the%20presence%20of%20certain%20attributes.%0AIn%20this%20paper%2C%20we%20present%20a%20new%20type%20of%20attack%20that%20compromises%20the%20fairness%20of%0Athe%20trained%20model.%20Fairness%20is%20understood%20to%20be%20the%20attribute-level%20performance%0Adistribution%20of%20a%20trained%20model.%20It%20is%20particularly%20salient%20in%20domains%20where%2C%0Afor%20example%2C%20skewed%20accuracy%20discrimination%20between%20subpopulations%20could%20have%0Adisastrous%20consequences.%20We%20find%20that%20by%20employing%20a%20threat%20model%20similar%20to%0Athat%20of%20a%20backdoor%20attack%2C%20an%20attacker%20is%20able%20to%20influence%20the%20aggregated%0Amodel%20to%20have%20an%20unfair%20performance%20distribution%20between%20any%20given%20set%20of%0Aattributes.%20Furthermore%2C%20we%20find%20that%20this%20attack%20is%20possible%20by%20controlling%0Aonly%20a%20single%20client.%20While%20combating%20naturally%20induced%20unfairness%20in%20FL%20has%0Apreviously%20been%20discussed%20in%20depth%2C%20its%20artificially%20induced%20kind%20has%20been%0Aneglected.%20We%20show%20that%20defending%20against%20attacks%20on%20fairness%20should%20be%20a%0Acritical%20consideration%20in%20any%20situation%20where%20unfairness%20in%20a%20trained%20model%0Acould%20benefit%20a%20user%20who%20participated%20in%20its%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttacks%2520on%2520fairness%2520in%2520Federated%2520Learning%26entry.906535625%3DJoseph%2520Rance%2520and%2520Filip%2520Svoboda%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520is%2520an%2520important%2520emerging%2520distributed%2520training%2520paradigm%250Athat%2520keeps%2520data%2520private%2520on%2520clients.%2520It%2520is%2520now%2520well%2520understood%2520that%2520by%250Acontrolling%2520only%2520a%2520small%2520subset%2520of%2520FL%2520clients%252C%2520it%2520is%2520possible%2520to%2520introduce%2520a%250Abackdoor%2520to%2520a%2520federated%2520learning%2520model%252C%2520in%2520the%2520presence%2520of%2520certain%2520attributes.%250AIn%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520type%2520of%2520attack%2520that%2520compromises%2520the%2520fairness%2520of%250Athe%2520trained%2520model.%2520Fairness%2520is%2520understood%2520to%2520be%2520the%2520attribute-level%2520performance%250Adistribution%2520of%2520a%2520trained%2520model.%2520It%2520is%2520particularly%2520salient%2520in%2520domains%2520where%252C%250Afor%2520example%252C%2520skewed%2520accuracy%2520discrimination%2520between%2520subpopulations%2520could%2520have%250Adisastrous%2520consequences.%2520We%2520find%2520that%2520by%2520employing%2520a%2520threat%2520model%2520similar%2520to%250Athat%2520of%2520a%2520backdoor%2520attack%252C%2520an%2520attacker%2520is%2520able%2520to%2520influence%2520the%2520aggregated%250Amodel%2520to%2520have%2520an%2520unfair%2520performance%2520distribution%2520between%2520any%2520given%2520set%2520of%250Aattributes.%2520Furthermore%252C%2520we%2520find%2520that%2520this%2520attack%2520is%2520possible%2520by%2520controlling%250Aonly%2520a%2520single%2520client.%2520While%2520combating%2520naturally%2520induced%2520unfairness%2520in%2520FL%2520has%250Apreviously%2520been%2520discussed%2520in%2520depth%252C%2520its%2520artificially%2520induced%2520kind%2520has%2520been%250Aneglected.%2520We%2520show%2520that%2520defending%2520against%2520attacks%2520on%2520fairness%2520should%2520be%2520a%250Acritical%2520consideration%2520in%2520any%2520situation%2520where%2520unfairness%2520in%2520a%2520trained%2520model%250Acould%2520benefit%2520a%2520user%2520who%2520participated%2520in%2520its%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attacks%20on%20fairness%20in%20Federated%20Learning&entry.906535625=Joseph%20Rance%20and%20Filip%20Svoboda&entry.1292438233=%20%20Federated%20Learning%20is%20an%20important%20emerging%20distributed%20training%20paradigm%0Athat%20keeps%20data%20private%20on%20clients.%20It%20is%20now%20well%20understood%20that%20by%0Acontrolling%20only%20a%20small%20subset%20of%20FL%20clients%2C%20it%20is%20possible%20to%20introduce%20a%0Abackdoor%20to%20a%20federated%20learning%20model%2C%20in%20the%20presence%20of%20certain%20attributes.%0AIn%20this%20paper%2C%20we%20present%20a%20new%20type%20of%20attack%20that%20compromises%20the%20fairness%20of%0Athe%20trained%20model.%20Fairness%20is%20understood%20to%20be%20the%20attribute-level%20performance%0Adistribution%20of%20a%20trained%20model.%20It%20is%20particularly%20salient%20in%20domains%20where%2C%0Afor%20example%2C%20skewed%20accuracy%20discrimination%20between%20subpopulations%20could%20have%0Adisastrous%20consequences.%20We%20find%20that%20by%20employing%20a%20threat%20model%20similar%20to%0Athat%20of%20a%20backdoor%20attack%2C%20an%20attacker%20is%20able%20to%20influence%20the%20aggregated%0Amodel%20to%20have%20an%20unfair%20performance%20distribution%20between%20any%20given%20set%20of%0Aattributes.%20Furthermore%2C%20we%20find%20that%20this%20attack%20is%20possible%20by%20controlling%0Aonly%20a%20single%20client.%20While%20combating%20naturally%20induced%20unfairness%20in%20FL%20has%0Apreviously%20been%20discussed%20in%20depth%2C%20its%20artificially%20induced%20kind%20has%20been%0Aneglected.%20We%20show%20that%20defending%20against%20attacks%20on%20fairness%20should%20be%20a%0Acritical%20consideration%20in%20any%20situation%20where%20unfairness%20in%20a%20trained%20model%0Acould%20benefit%20a%20user%20who%20participated%20in%20its%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12715v2&entry.124074799=Read"},
{"title": "Fast and Reliable Probabilistic Reflectometry Inversion with\n  Prior-Amortized Neural Posterior Estimation", "author": "Vladimir Starostin and Maximilian Dax and Alexander Gerlach and Alexander Hinderhofer and \u00c1lvaro Tejero-Cantero and Frank Schreiber", "abstract": "  Reconstructing the structure of thin films and multilayers from measurements\nof scattered X-rays or neutrons is key to progress in physics, chemistry, and\nbiology. However, finding all structures compatible with reflectometry data is\ncomputationally prohibitive for standard algorithms, which typically results in\nunreliable analysis with only a single potential solution identified. We\naddress this lack of reliability with a probabilistic deep learning method that\nidentifies all realistic structures in seconds, setting new standards in\nreflectometry. Our method, Prior-Amortized Neural Posterior Estimation (PANPE),\ncombines simulation-based inference with novel adaptive priors that inform the\ninference network about known structural properties and controllable\nexperimental conditions. PANPE networks support key scenarios such as\nhigh-throughput sample characterization, real-time monitoring of evolving\nstructures, or the co-refinement of several experimental data sets, and can be\nadapted to provide fast, reliable, and flexible inference across many other\ninverse problems.\n", "link": "http://arxiv.org/abs/2407.18648v1", "date": "2024-07-26", "relevancy": 2.1563, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Reliable%20Probabilistic%20Reflectometry%20Inversion%20with%0A%20%20Prior-Amortized%20Neural%20Posterior%20Estimation&body=Title%3A%20Fast%20and%20Reliable%20Probabilistic%20Reflectometry%20Inversion%20with%0A%20%20Prior-Amortized%20Neural%20Posterior%20Estimation%0AAuthor%3A%20Vladimir%20Starostin%20and%20Maximilian%20Dax%20and%20Alexander%20Gerlach%20and%20Alexander%20Hinderhofer%20and%20%C3%81lvaro%20Tejero-Cantero%20and%20Frank%20Schreiber%0AAbstract%3A%20%20%20Reconstructing%20the%20structure%20of%20thin%20films%20and%20multilayers%20from%20measurements%0Aof%20scattered%20X-rays%20or%20neutrons%20is%20key%20to%20progress%20in%20physics%2C%20chemistry%2C%20and%0Abiology.%20However%2C%20finding%20all%20structures%20compatible%20with%20reflectometry%20data%20is%0Acomputationally%20prohibitive%20for%20standard%20algorithms%2C%20which%20typically%20results%20in%0Aunreliable%20analysis%20with%20only%20a%20single%20potential%20solution%20identified.%20We%0Aaddress%20this%20lack%20of%20reliability%20with%20a%20probabilistic%20deep%20learning%20method%20that%0Aidentifies%20all%20realistic%20structures%20in%20seconds%2C%20setting%20new%20standards%20in%0Areflectometry.%20Our%20method%2C%20Prior-Amortized%20Neural%20Posterior%20Estimation%20%28PANPE%29%2C%0Acombines%20simulation-based%20inference%20with%20novel%20adaptive%20priors%20that%20inform%20the%0Ainference%20network%20about%20known%20structural%20properties%20and%20controllable%0Aexperimental%20conditions.%20PANPE%20networks%20support%20key%20scenarios%20such%20as%0Ahigh-throughput%20sample%20characterization%2C%20real-time%20monitoring%20of%20evolving%0Astructures%2C%20or%20the%20co-refinement%20of%20several%20experimental%20data%20sets%2C%20and%20can%20be%0Aadapted%20to%20provide%20fast%2C%20reliable%2C%20and%20flexible%20inference%20across%20many%20other%0Ainverse%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Reliable%2520Probabilistic%2520Reflectometry%2520Inversion%2520with%250A%2520%2520Prior-Amortized%2520Neural%2520Posterior%2520Estimation%26entry.906535625%3DVladimir%2520Starostin%2520and%2520Maximilian%2520Dax%2520and%2520Alexander%2520Gerlach%2520and%2520Alexander%2520Hinderhofer%2520and%2520%25C3%2581lvaro%2520Tejero-Cantero%2520and%2520Frank%2520Schreiber%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520structure%2520of%2520thin%2520films%2520and%2520multilayers%2520from%2520measurements%250Aof%2520scattered%2520X-rays%2520or%2520neutrons%2520is%2520key%2520to%2520progress%2520in%2520physics%252C%2520chemistry%252C%2520and%250Abiology.%2520However%252C%2520finding%2520all%2520structures%2520compatible%2520with%2520reflectometry%2520data%2520is%250Acomputationally%2520prohibitive%2520for%2520standard%2520algorithms%252C%2520which%2520typically%2520results%2520in%250Aunreliable%2520analysis%2520with%2520only%2520a%2520single%2520potential%2520solution%2520identified.%2520We%250Aaddress%2520this%2520lack%2520of%2520reliability%2520with%2520a%2520probabilistic%2520deep%2520learning%2520method%2520that%250Aidentifies%2520all%2520realistic%2520structures%2520in%2520seconds%252C%2520setting%2520new%2520standards%2520in%250Areflectometry.%2520Our%2520method%252C%2520Prior-Amortized%2520Neural%2520Posterior%2520Estimation%2520%2528PANPE%2529%252C%250Acombines%2520simulation-based%2520inference%2520with%2520novel%2520adaptive%2520priors%2520that%2520inform%2520the%250Ainference%2520network%2520about%2520known%2520structural%2520properties%2520and%2520controllable%250Aexperimental%2520conditions.%2520PANPE%2520networks%2520support%2520key%2520scenarios%2520such%2520as%250Ahigh-throughput%2520sample%2520characterization%252C%2520real-time%2520monitoring%2520of%2520evolving%250Astructures%252C%2520or%2520the%2520co-refinement%2520of%2520several%2520experimental%2520data%2520sets%252C%2520and%2520can%2520be%250Aadapted%2520to%2520provide%2520fast%252C%2520reliable%252C%2520and%2520flexible%2520inference%2520across%2520many%2520other%250Ainverse%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Reliable%20Probabilistic%20Reflectometry%20Inversion%20with%0A%20%20Prior-Amortized%20Neural%20Posterior%20Estimation&entry.906535625=Vladimir%20Starostin%20and%20Maximilian%20Dax%20and%20Alexander%20Gerlach%20and%20Alexander%20Hinderhofer%20and%20%C3%81lvaro%20Tejero-Cantero%20and%20Frank%20Schreiber&entry.1292438233=%20%20Reconstructing%20the%20structure%20of%20thin%20films%20and%20multilayers%20from%20measurements%0Aof%20scattered%20X-rays%20or%20neutrons%20is%20key%20to%20progress%20in%20physics%2C%20chemistry%2C%20and%0Abiology.%20However%2C%20finding%20all%20structures%20compatible%20with%20reflectometry%20data%20is%0Acomputationally%20prohibitive%20for%20standard%20algorithms%2C%20which%20typically%20results%20in%0Aunreliable%20analysis%20with%20only%20a%20single%20potential%20solution%20identified.%20We%0Aaddress%20this%20lack%20of%20reliability%20with%20a%20probabilistic%20deep%20learning%20method%20that%0Aidentifies%20all%20realistic%20structures%20in%20seconds%2C%20setting%20new%20standards%20in%0Areflectometry.%20Our%20method%2C%20Prior-Amortized%20Neural%20Posterior%20Estimation%20%28PANPE%29%2C%0Acombines%20simulation-based%20inference%20with%20novel%20adaptive%20priors%20that%20inform%20the%0Ainference%20network%20about%20known%20structural%20properties%20and%20controllable%0Aexperimental%20conditions.%20PANPE%20networks%20support%20key%20scenarios%20such%20as%0Ahigh-throughput%20sample%20characterization%2C%20real-time%20monitoring%20of%20evolving%0Astructures%2C%20or%20the%20co-refinement%20of%20several%20experimental%20data%20sets%2C%20and%20can%20be%0Aadapted%20to%20provide%20fast%2C%20reliable%2C%20and%20flexible%20inference%20across%20many%20other%0Ainverse%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18648v1&entry.124074799=Read"},
{"title": "Geometric Generative Models based on Morphological Equivariant PDEs and\n  GANs", "author": "El Hadji S. Diop and Thierno Fall and Alioune Mbengue and Mohamed Daoudi", "abstract": "  Content and image generation consist in creating or generating data from\nnoisy information by extracting specific features such as texture, edges, and\nother thin image structures. We are interested here in generative models, and\ntwo main problems are addressed. Firstly, the improvements of specific feature\nextraction while accounting at multiscale levels intrinsic geometric features;\nand secondly, the equivariance of the network to reduce its complexity and\nprovide a geometric interpretability. To proceed, we propose a geometric\ngenerative model based on an equivariant partial differential equation (PDE)\nfor group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on\nmorphology operators and generative adversarial networks (GANs). Equivariant\nmorphological PDE layers are composed of multiscale dilations and erosions\nformulated in Riemannian manifolds, while group symmetries are defined on a Lie\ngroup. We take advantage of the Lie group structure to properly integrate the\nequivariance in layers, and are able to use the Riemannian metric to solve the\nmultiscale morphological operations. Each point of the Lie group is associated\nwith a unique point in the manifold, which helps us derive a metric on the\nRiemannian manifold from a tensor field invariant under the Lie group so that\nthe induced metric has the same symmetries. The proposed geometric\nmorphological GAN (GM-GAN) is obtained by using the proposed morphological\nequivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.\nGM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results\nshow that GM-GAN model outperforms classical GAN.\n", "link": "http://arxiv.org/abs/2403.14897v3", "date": "2024-07-26", "relevancy": 2.1563, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5511}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5412}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Generative%20Models%20based%20on%20Morphological%20Equivariant%20PDEs%20and%0A%20%20GANs&body=Title%3A%20Geometric%20Generative%20Models%20based%20on%20Morphological%20Equivariant%20PDEs%20and%0A%20%20GANs%0AAuthor%3A%20El%20Hadji%20S.%20Diop%20and%20Thierno%20Fall%20and%20Alioune%20Mbengue%20and%20Mohamed%20Daoudi%0AAbstract%3A%20%20%20Content%20and%20image%20generation%20consist%20in%20creating%20or%20generating%20data%20from%0Anoisy%20information%20by%20extracting%20specific%20features%20such%20as%20texture%2C%20edges%2C%20and%0Aother%20thin%20image%20structures.%20We%20are%20interested%20here%20in%20generative%20models%2C%20and%0Atwo%20main%20problems%20are%20addressed.%20Firstly%2C%20the%20improvements%20of%20specific%20feature%0Aextraction%20while%20accounting%20at%20multiscale%20levels%20intrinsic%20geometric%20features%3B%0Aand%20secondly%2C%20the%20equivariance%20of%20the%20network%20to%20reduce%20its%20complexity%20and%0Aprovide%20a%20geometric%20interpretability.%20To%20proceed%2C%20we%20propose%20a%20geometric%0Agenerative%20model%20based%20on%20an%20equivariant%20partial%20differential%20equation%20%28PDE%29%0Afor%20group%20convolution%20neural%20networks%20%28G-CNNs%29%2C%20so%20called%20PDE-G-CNNs%2C%20built%20on%0Amorphology%20operators%20and%20generative%20adversarial%20networks%20%28GANs%29.%20Equivariant%0Amorphological%20PDE%20layers%20are%20composed%20of%20multiscale%20dilations%20and%20erosions%0Aformulated%20in%20Riemannian%20manifolds%2C%20while%20group%20symmetries%20are%20defined%20on%20a%20Lie%0Agroup.%20We%20take%20advantage%20of%20the%20Lie%20group%20structure%20to%20properly%20integrate%20the%0Aequivariance%20in%20layers%2C%20and%20are%20able%20to%20use%20the%20Riemannian%20metric%20to%20solve%20the%0Amultiscale%20morphological%20operations.%20Each%20point%20of%20the%20Lie%20group%20is%20associated%0Awith%20a%20unique%20point%20in%20the%20manifold%2C%20which%20helps%20us%20derive%20a%20metric%20on%20the%0ARiemannian%20manifold%20from%20a%20tensor%20field%20invariant%20under%20the%20Lie%20group%20so%20that%0Athe%20induced%20metric%20has%20the%20same%20symmetries.%20The%20proposed%20geometric%0Amorphological%20GAN%20%28GM-GAN%29%20is%20obtained%20by%20using%20the%20proposed%20morphological%0Aequivariant%20convolutions%20in%20PDE-G-CNNs%20to%20bring%20nonlinearity%20in%20classical%20CNNs.%0AGM-GAN%20is%20evaluated%20on%20MNIST%20data%20and%20compared%20with%20GANs.%20Preliminary%20results%0Ashow%20that%20GM-GAN%20model%20outperforms%20classical%20GAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Generative%2520Models%2520based%2520on%2520Morphological%2520Equivariant%2520PDEs%2520and%250A%2520%2520GANs%26entry.906535625%3DEl%2520Hadji%2520S.%2520Diop%2520and%2520Thierno%2520Fall%2520and%2520Alioune%2520Mbengue%2520and%2520Mohamed%2520Daoudi%26entry.1292438233%3D%2520%2520Content%2520and%2520image%2520generation%2520consist%2520in%2520creating%2520or%2520generating%2520data%2520from%250Anoisy%2520information%2520by%2520extracting%2520specific%2520features%2520such%2520as%2520texture%252C%2520edges%252C%2520and%250Aother%2520thin%2520image%2520structures.%2520We%2520are%2520interested%2520here%2520in%2520generative%2520models%252C%2520and%250Atwo%2520main%2520problems%2520are%2520addressed.%2520Firstly%252C%2520the%2520improvements%2520of%2520specific%2520feature%250Aextraction%2520while%2520accounting%2520at%2520multiscale%2520levels%2520intrinsic%2520geometric%2520features%253B%250Aand%2520secondly%252C%2520the%2520equivariance%2520of%2520the%2520network%2520to%2520reduce%2520its%2520complexity%2520and%250Aprovide%2520a%2520geometric%2520interpretability.%2520To%2520proceed%252C%2520we%2520propose%2520a%2520geometric%250Agenerative%2520model%2520based%2520on%2520an%2520equivariant%2520partial%2520differential%2520equation%2520%2528PDE%2529%250Afor%2520group%2520convolution%2520neural%2520networks%2520%2528G-CNNs%2529%252C%2520so%2520called%2520PDE-G-CNNs%252C%2520built%2520on%250Amorphology%2520operators%2520and%2520generative%2520adversarial%2520networks%2520%2528GANs%2529.%2520Equivariant%250Amorphological%2520PDE%2520layers%2520are%2520composed%2520of%2520multiscale%2520dilations%2520and%2520erosions%250Aformulated%2520in%2520Riemannian%2520manifolds%252C%2520while%2520group%2520symmetries%2520are%2520defined%2520on%2520a%2520Lie%250Agroup.%2520We%2520take%2520advantage%2520of%2520the%2520Lie%2520group%2520structure%2520to%2520properly%2520integrate%2520the%250Aequivariance%2520in%2520layers%252C%2520and%2520are%2520able%2520to%2520use%2520the%2520Riemannian%2520metric%2520to%2520solve%2520the%250Amultiscale%2520morphological%2520operations.%2520Each%2520point%2520of%2520the%2520Lie%2520group%2520is%2520associated%250Awith%2520a%2520unique%2520point%2520in%2520the%2520manifold%252C%2520which%2520helps%2520us%2520derive%2520a%2520metric%2520on%2520the%250ARiemannian%2520manifold%2520from%2520a%2520tensor%2520field%2520invariant%2520under%2520the%2520Lie%2520group%2520so%2520that%250Athe%2520induced%2520metric%2520has%2520the%2520same%2520symmetries.%2520The%2520proposed%2520geometric%250Amorphological%2520GAN%2520%2528GM-GAN%2529%2520is%2520obtained%2520by%2520using%2520the%2520proposed%2520morphological%250Aequivariant%2520convolutions%2520in%2520PDE-G-CNNs%2520to%2520bring%2520nonlinearity%2520in%2520classical%2520CNNs.%250AGM-GAN%2520is%2520evaluated%2520on%2520MNIST%2520data%2520and%2520compared%2520with%2520GANs.%2520Preliminary%2520results%250Ashow%2520that%2520GM-GAN%2520model%2520outperforms%2520classical%2520GAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Generative%20Models%20based%20on%20Morphological%20Equivariant%20PDEs%20and%0A%20%20GANs&entry.906535625=El%20Hadji%20S.%20Diop%20and%20Thierno%20Fall%20and%20Alioune%20Mbengue%20and%20Mohamed%20Daoudi&entry.1292438233=%20%20Content%20and%20image%20generation%20consist%20in%20creating%20or%20generating%20data%20from%0Anoisy%20information%20by%20extracting%20specific%20features%20such%20as%20texture%2C%20edges%2C%20and%0Aother%20thin%20image%20structures.%20We%20are%20interested%20here%20in%20generative%20models%2C%20and%0Atwo%20main%20problems%20are%20addressed.%20Firstly%2C%20the%20improvements%20of%20specific%20feature%0Aextraction%20while%20accounting%20at%20multiscale%20levels%20intrinsic%20geometric%20features%3B%0Aand%20secondly%2C%20the%20equivariance%20of%20the%20network%20to%20reduce%20its%20complexity%20and%0Aprovide%20a%20geometric%20interpretability.%20To%20proceed%2C%20we%20propose%20a%20geometric%0Agenerative%20model%20based%20on%20an%20equivariant%20partial%20differential%20equation%20%28PDE%29%0Afor%20group%20convolution%20neural%20networks%20%28G-CNNs%29%2C%20so%20called%20PDE-G-CNNs%2C%20built%20on%0Amorphology%20operators%20and%20generative%20adversarial%20networks%20%28GANs%29.%20Equivariant%0Amorphological%20PDE%20layers%20are%20composed%20of%20multiscale%20dilations%20and%20erosions%0Aformulated%20in%20Riemannian%20manifolds%2C%20while%20group%20symmetries%20are%20defined%20on%20a%20Lie%0Agroup.%20We%20take%20advantage%20of%20the%20Lie%20group%20structure%20to%20properly%20integrate%20the%0Aequivariance%20in%20layers%2C%20and%20are%20able%20to%20use%20the%20Riemannian%20metric%20to%20solve%20the%0Amultiscale%20morphological%20operations.%20Each%20point%20of%20the%20Lie%20group%20is%20associated%0Awith%20a%20unique%20point%20in%20the%20manifold%2C%20which%20helps%20us%20derive%20a%20metric%20on%20the%0ARiemannian%20manifold%20from%20a%20tensor%20field%20invariant%20under%20the%20Lie%20group%20so%20that%0Athe%20induced%20metric%20has%20the%20same%20symmetries.%20The%20proposed%20geometric%0Amorphological%20GAN%20%28GM-GAN%29%20is%20obtained%20by%20using%20the%20proposed%20morphological%0Aequivariant%20convolutions%20in%20PDE-G-CNNs%20to%20bring%20nonlinearity%20in%20classical%20CNNs.%0AGM-GAN%20is%20evaluated%20on%20MNIST%20data%20and%20compared%20with%20GANs.%20Preliminary%20results%0Ashow%20that%20GM-GAN%20model%20outperforms%20classical%20GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14897v3&entry.124074799=Read"},
{"title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation", "author": "Esteban Garces Arias and Julian Rodemann and Meimingwei Li and Christian Heumann and Matthias A\u00dfenmacher", "abstract": "  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n", "link": "http://arxiv.org/abs/2407.18698v1", "date": "2024-07-26", "relevancy": 2.1448, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5524}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5433}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Contrastive%20Search%3A%20Uncertainty-Guided%20Decoding%20for%20Open-Ended%0A%20%20Text%20Generation&body=Title%3A%20Adaptive%20Contrastive%20Search%3A%20Uncertainty-Guided%20Decoding%20for%20Open-Ended%0A%20%20Text%20Generation%0AAuthor%3A%20Esteban%20Garces%20Arias%20and%20Julian%20Rodemann%20and%20Meimingwei%20Li%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher%0AAbstract%3A%20%20%20Decoding%20from%20the%20output%20distributions%20of%20large%20language%20models%20to%20produce%0Ahigh-quality%20text%20is%20a%20complex%20challenge%20in%20language%20modeling.%20Various%0Aapproaches%2C%20such%20as%20beam%20search%2C%20sampling%20with%20temperature%2C%20%24k-%24sampling%2C%0Anucleus%20%24p-%24sampling%2C%20typical%20decoding%2C%20contrastive%20decoding%2C%20and%20contrastive%0Asearch%2C%20have%20been%20proposed%20to%20address%20this%20problem%2C%20aiming%20to%20improve%0Acoherence%2C%20diversity%2C%20as%20well%20as%20resemblance%20to%20human-generated%20text.%20In%20this%0Astudy%2C%20we%20introduce%20adaptive%20contrastive%20search%2C%20a%20novel%20decoding%20strategy%0Aextending%20contrastive%20search%20by%20incorporating%20an%20adaptive%20degeneration%20penalty%2C%0Aguided%20by%20the%20estimated%20uncertainty%20of%20the%20model%20at%20each%20generation%20step.%20This%0Astrategy%20is%20designed%20to%20enhance%20both%20the%20creativity%20and%20diversity%20of%20the%0Alanguage%20modeling%20process%20while%20at%20the%20same%20time%20producing%20coherent%20and%0Ahigh-quality%20generated%20text%20output.%20Our%20findings%20indicate%20performance%0Aenhancement%20in%20both%20aspects%2C%20across%20different%20model%20architectures%20and%20datasets%2C%0Aunderscoring%20the%20effectiveness%20of%20our%20method%20in%20text%20generation%20tasks.%20Our%20code%0Abase%2C%20datasets%2C%20and%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Contrastive%2520Search%253A%2520Uncertainty-Guided%2520Decoding%2520for%2520Open-Ended%250A%2520%2520Text%2520Generation%26entry.906535625%3DEsteban%2520Garces%2520Arias%2520and%2520Julian%2520Rodemann%2520and%2520Meimingwei%2520Li%2520and%2520Christian%2520Heumann%2520and%2520Matthias%2520A%25C3%259Fenmacher%26entry.1292438233%3D%2520%2520Decoding%2520from%2520the%2520output%2520distributions%2520of%2520large%2520language%2520models%2520to%2520produce%250Ahigh-quality%2520text%2520is%2520a%2520complex%2520challenge%2520in%2520language%2520modeling.%2520Various%250Aapproaches%252C%2520such%2520as%2520beam%2520search%252C%2520sampling%2520with%2520temperature%252C%2520%2524k-%2524sampling%252C%250Anucleus%2520%2524p-%2524sampling%252C%2520typical%2520decoding%252C%2520contrastive%2520decoding%252C%2520and%2520contrastive%250Asearch%252C%2520have%2520been%2520proposed%2520to%2520address%2520this%2520problem%252C%2520aiming%2520to%2520improve%250Acoherence%252C%2520diversity%252C%2520as%2520well%2520as%2520resemblance%2520to%2520human-generated%2520text.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520adaptive%2520contrastive%2520search%252C%2520a%2520novel%2520decoding%2520strategy%250Aextending%2520contrastive%2520search%2520by%2520incorporating%2520an%2520adaptive%2520degeneration%2520penalty%252C%250Aguided%2520by%2520the%2520estimated%2520uncertainty%2520of%2520the%2520model%2520at%2520each%2520generation%2520step.%2520This%250Astrategy%2520is%2520designed%2520to%2520enhance%2520both%2520the%2520creativity%2520and%2520diversity%2520of%2520the%250Alanguage%2520modeling%2520process%2520while%2520at%2520the%2520same%2520time%2520producing%2520coherent%2520and%250Ahigh-quality%2520generated%2520text%2520output.%2520Our%2520findings%2520indicate%2520performance%250Aenhancement%2520in%2520both%2520aspects%252C%2520across%2520different%2520model%2520architectures%2520and%2520datasets%252C%250Aunderscoring%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520text%2520generation%2520tasks.%2520Our%2520code%250Abase%252C%2520datasets%252C%2520and%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Contrastive%20Search%3A%20Uncertainty-Guided%20Decoding%20for%20Open-Ended%0A%20%20Text%20Generation&entry.906535625=Esteban%20Garces%20Arias%20and%20Julian%20Rodemann%20and%20Meimingwei%20Li%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher&entry.1292438233=%20%20Decoding%20from%20the%20output%20distributions%20of%20large%20language%20models%20to%20produce%0Ahigh-quality%20text%20is%20a%20complex%20challenge%20in%20language%20modeling.%20Various%0Aapproaches%2C%20such%20as%20beam%20search%2C%20sampling%20with%20temperature%2C%20%24k-%24sampling%2C%0Anucleus%20%24p-%24sampling%2C%20typical%20decoding%2C%20contrastive%20decoding%2C%20and%20contrastive%0Asearch%2C%20have%20been%20proposed%20to%20address%20this%20problem%2C%20aiming%20to%20improve%0Acoherence%2C%20diversity%2C%20as%20well%20as%20resemblance%20to%20human-generated%20text.%20In%20this%0Astudy%2C%20we%20introduce%20adaptive%20contrastive%20search%2C%20a%20novel%20decoding%20strategy%0Aextending%20contrastive%20search%20by%20incorporating%20an%20adaptive%20degeneration%20penalty%2C%0Aguided%20by%20the%20estimated%20uncertainty%20of%20the%20model%20at%20each%20generation%20step.%20This%0Astrategy%20is%20designed%20to%20enhance%20both%20the%20creativity%20and%20diversity%20of%20the%0Alanguage%20modeling%20process%20while%20at%20the%20same%20time%20producing%20coherent%20and%0Ahigh-quality%20generated%20text%20output.%20Our%20findings%20indicate%20performance%0Aenhancement%20in%20both%20aspects%2C%20across%20different%20model%20architectures%20and%20datasets%2C%0Aunderscoring%20the%20effectiveness%20of%20our%20method%20in%20text%20generation%20tasks.%20Our%20code%0Abase%2C%20datasets%2C%20and%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18698v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics", "author": "Mengjie Zhao and Cees Taal and Stephan Baggerohr and Olga Fink", "abstract": "  Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance.\n", "link": "http://arxiv.org/abs/2407.18691v1", "date": "2024-07-26", "relevancy": 2.1369, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5395}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Virtual%20Sensing%20in%20Complex%20Systems%3A%20Addressing%0A%20%20Heterogeneous%20Temporal%20Dynamics&body=Title%3A%20Graph%20Neural%20Networks%20for%20Virtual%20Sensing%20in%20Complex%20Systems%3A%20Addressing%0A%20%20Heterogeneous%20Temporal%20Dynamics%0AAuthor%3A%20Mengjie%20Zhao%20and%20Cees%20Taal%20and%20Stephan%20Baggerohr%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Real-time%20condition%20monitoring%20is%20crucial%20for%20the%20reliable%20and%20efficient%0Aoperation%20of%20complex%20systems.%20However%2C%20relying%20solely%20on%20physical%20sensors%20can%0Abe%20limited%20due%20to%20their%20cost%2C%20placement%20constraints%2C%20or%20inability%20to%20directly%0Ameasure%20certain%20critical%20parameters.%20Virtual%20sensing%20addresses%20these%0Alimitations%20by%20leveraging%20readily%20available%20sensor%20data%20and%20system%20knowledge%20to%0Aestimate%20inaccessible%20parameters%20or%20infer%20system%20states.%20The%20increasing%0Acomplexity%20of%20industrial%20systems%20necessitates%20deployments%20of%20sensors%20with%0Adiverse%20modalities%20to%20provide%20a%20comprehensive%20understanding%20of%20system%20states.%0AThese%20sensors%20capture%20data%20at%20varying%20frequencies%20to%20monitor%20both%20rapid%20and%0Aslowly%20varying%20system%20dynamics%2C%20as%20well%20as%20local%20and%20global%20state%20evolutions%20of%0Athe%20systems.%20This%20leads%20to%20heterogeneous%20temporal%20dynamics%2C%20which%2C%20particularly%0Aunder%20varying%20operational%20end%20environmental%20conditions%2C%20pose%20a%20significant%0Achallenge%20for%20accurate%20virtual%20sensing.%20To%20address%20this%2C%20we%20propose%20a%0AHeterogeneous%20Temporal%20Graph%20Neural%20Network%20%28HTGNN%29%20framework.%20HTGNN%20explicitly%0Amodels%20signals%20from%20diverse%20sensors%20and%20integrates%20operating%20conditions%20into%0Athe%20model%20architecture.%20We%20evaluate%20HTGNN%20using%20two%20newly%20released%20datasets%3A%20a%0Abearing%20dataset%20with%20diverse%20load%20conditions%20for%20bearing%20load%20prediction%20and%20a%0Ayear-long%20simulated%20dataset%20for%20predicting%20bridge%20live%20loads.%20Our%20results%0Ademonstrate%20that%20HTGNN%20significantly%20outperforms%20established%20baseline%20methods%0Ain%20both%20tasks%2C%20particularly%20under%20highly%20varying%20operating%20conditions.%20These%0Aresults%20highlight%20HTGNN%27s%20potential%20as%20a%20robust%20and%20accurate%20virtual%20sensing%0Aapproach%20for%20complex%20systems%2C%20paving%20the%20way%20for%20improved%20monitoring%2C%0Apredictive%20maintenance%2C%20and%20enhanced%20system%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520for%2520Virtual%2520Sensing%2520in%2520Complex%2520Systems%253A%2520Addressing%250A%2520%2520Heterogeneous%2520Temporal%2520Dynamics%26entry.906535625%3DMengjie%2520Zhao%2520and%2520Cees%2520Taal%2520and%2520Stephan%2520Baggerohr%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Real-time%2520condition%2520monitoring%2520is%2520crucial%2520for%2520the%2520reliable%2520and%2520efficient%250Aoperation%2520of%2520complex%2520systems.%2520However%252C%2520relying%2520solely%2520on%2520physical%2520sensors%2520can%250Abe%2520limited%2520due%2520to%2520their%2520cost%252C%2520placement%2520constraints%252C%2520or%2520inability%2520to%2520directly%250Ameasure%2520certain%2520critical%2520parameters.%2520Virtual%2520sensing%2520addresses%2520these%250Alimitations%2520by%2520leveraging%2520readily%2520available%2520sensor%2520data%2520and%2520system%2520knowledge%2520to%250Aestimate%2520inaccessible%2520parameters%2520or%2520infer%2520system%2520states.%2520The%2520increasing%250Acomplexity%2520of%2520industrial%2520systems%2520necessitates%2520deployments%2520of%2520sensors%2520with%250Adiverse%2520modalities%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%2520system%2520states.%250AThese%2520sensors%2520capture%2520data%2520at%2520varying%2520frequencies%2520to%2520monitor%2520both%2520rapid%2520and%250Aslowly%2520varying%2520system%2520dynamics%252C%2520as%2520well%2520as%2520local%2520and%2520global%2520state%2520evolutions%2520of%250Athe%2520systems.%2520This%2520leads%2520to%2520heterogeneous%2520temporal%2520dynamics%252C%2520which%252C%2520particularly%250Aunder%2520varying%2520operational%2520end%2520environmental%2520conditions%252C%2520pose%2520a%2520significant%250Achallenge%2520for%2520accurate%2520virtual%2520sensing.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250AHeterogeneous%2520Temporal%2520Graph%2520Neural%2520Network%2520%2528HTGNN%2529%2520framework.%2520HTGNN%2520explicitly%250Amodels%2520signals%2520from%2520diverse%2520sensors%2520and%2520integrates%2520operating%2520conditions%2520into%250Athe%2520model%2520architecture.%2520We%2520evaluate%2520HTGNN%2520using%2520two%2520newly%2520released%2520datasets%253A%2520a%250Abearing%2520dataset%2520with%2520diverse%2520load%2520conditions%2520for%2520bearing%2520load%2520prediction%2520and%2520a%250Ayear-long%2520simulated%2520dataset%2520for%2520predicting%2520bridge%2520live%2520loads.%2520Our%2520results%250Ademonstrate%2520that%2520HTGNN%2520significantly%2520outperforms%2520established%2520baseline%2520methods%250Ain%2520both%2520tasks%252C%2520particularly%2520under%2520highly%2520varying%2520operating%2520conditions.%2520These%250Aresults%2520highlight%2520HTGNN%2527s%2520potential%2520as%2520a%2520robust%2520and%2520accurate%2520virtual%2520sensing%250Aapproach%2520for%2520complex%2520systems%252C%2520paving%2520the%2520way%2520for%2520improved%2520monitoring%252C%250Apredictive%2520maintenance%252C%2520and%2520enhanced%2520system%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Virtual%20Sensing%20in%20Complex%20Systems%3A%20Addressing%0A%20%20Heterogeneous%20Temporal%20Dynamics&entry.906535625=Mengjie%20Zhao%20and%20Cees%20Taal%20and%20Stephan%20Baggerohr%20and%20Olga%20Fink&entry.1292438233=%20%20Real-time%20condition%20monitoring%20is%20crucial%20for%20the%20reliable%20and%20efficient%0Aoperation%20of%20complex%20systems.%20However%2C%20relying%20solely%20on%20physical%20sensors%20can%0Abe%20limited%20due%20to%20their%20cost%2C%20placement%20constraints%2C%20or%20inability%20to%20directly%0Ameasure%20certain%20critical%20parameters.%20Virtual%20sensing%20addresses%20these%0Alimitations%20by%20leveraging%20readily%20available%20sensor%20data%20and%20system%20knowledge%20to%0Aestimate%20inaccessible%20parameters%20or%20infer%20system%20states.%20The%20increasing%0Acomplexity%20of%20industrial%20systems%20necessitates%20deployments%20of%20sensors%20with%0Adiverse%20modalities%20to%20provide%20a%20comprehensive%20understanding%20of%20system%20states.%0AThese%20sensors%20capture%20data%20at%20varying%20frequencies%20to%20monitor%20both%20rapid%20and%0Aslowly%20varying%20system%20dynamics%2C%20as%20well%20as%20local%20and%20global%20state%20evolutions%20of%0Athe%20systems.%20This%20leads%20to%20heterogeneous%20temporal%20dynamics%2C%20which%2C%20particularly%0Aunder%20varying%20operational%20end%20environmental%20conditions%2C%20pose%20a%20significant%0Achallenge%20for%20accurate%20virtual%20sensing.%20To%20address%20this%2C%20we%20propose%20a%0AHeterogeneous%20Temporal%20Graph%20Neural%20Network%20%28HTGNN%29%20framework.%20HTGNN%20explicitly%0Amodels%20signals%20from%20diverse%20sensors%20and%20integrates%20operating%20conditions%20into%0Athe%20model%20architecture.%20We%20evaluate%20HTGNN%20using%20two%20newly%20released%20datasets%3A%20a%0Abearing%20dataset%20with%20diverse%20load%20conditions%20for%20bearing%20load%20prediction%20and%20a%0Ayear-long%20simulated%20dataset%20for%20predicting%20bridge%20live%20loads.%20Our%20results%0Ademonstrate%20that%20HTGNN%20significantly%20outperforms%20established%20baseline%20methods%0Ain%20both%20tasks%2C%20particularly%20under%20highly%20varying%20operating%20conditions.%20These%0Aresults%20highlight%20HTGNN%27s%20potential%20as%20a%20robust%20and%20accurate%20virtual%20sensing%0Aapproach%20for%20complex%20systems%2C%20paving%20the%20way%20for%20improved%20monitoring%2C%0Apredictive%20maintenance%2C%20and%20enhanced%20system%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18691v1&entry.124074799=Read"},
{"title": "Deep Companion Learning: Enhancing Generalization Through Historical\n  Consistency", "author": "Ruizhao Zhu and Venkatesh Saligrama", "abstract": "  We propose Deep Companion Learning (DCL), a novel training method for Deep\nNeural Networks (DNNs) that enhances generalization by penalizing inconsistent\nmodel predictions compared to its historical performance. To achieve this, we\ntrain a deep-companion model (DCM), by using previous versions of the model to\nprovide forecasts on new inputs. This companion model deciphers a meaningful\nlatent semantic structure within the data, thereby providing targeted\nsupervision that encourages the primary model to address the scenarios it finds\nmost challenging. We validate our approach through both theoretical analysis\nand extensive experimentation, including ablation studies, on a variety of\nbenchmark datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1K) using diverse\narchitectural models (ShuffleNetV2, ResNet, Vision Transformer, etc.),\ndemonstrating state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2407.18821v1", "date": "2024-07-26", "relevancy": 2.1172, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Companion%20Learning%3A%20Enhancing%20Generalization%20Through%20Historical%0A%20%20Consistency&body=Title%3A%20Deep%20Companion%20Learning%3A%20Enhancing%20Generalization%20Through%20Historical%0A%20%20Consistency%0AAuthor%3A%20Ruizhao%20Zhu%20and%20Venkatesh%20Saligrama%0AAbstract%3A%20%20%20We%20propose%20Deep%20Companion%20Learning%20%28DCL%29%2C%20a%20novel%20training%20method%20for%20Deep%0ANeural%20Networks%20%28DNNs%29%20that%20enhances%20generalization%20by%20penalizing%20inconsistent%0Amodel%20predictions%20compared%20to%20its%20historical%20performance.%20To%20achieve%20this%2C%20we%0Atrain%20a%20deep-companion%20model%20%28DCM%29%2C%20by%20using%20previous%20versions%20of%20the%20model%20to%0Aprovide%20forecasts%20on%20new%20inputs.%20This%20companion%20model%20deciphers%20a%20meaningful%0Alatent%20semantic%20structure%20within%20the%20data%2C%20thereby%20providing%20targeted%0Asupervision%20that%20encourages%20the%20primary%20model%20to%20address%20the%20scenarios%20it%20finds%0Amost%20challenging.%20We%20validate%20our%20approach%20through%20both%20theoretical%20analysis%0Aand%20extensive%20experimentation%2C%20including%20ablation%20studies%2C%20on%20a%20variety%20of%0Abenchmark%20datasets%20%28CIFAR-100%2C%20Tiny-ImageNet%2C%20ImageNet-1K%29%20using%20diverse%0Aarchitectural%20models%20%28ShuffleNetV2%2C%20ResNet%2C%20Vision%20Transformer%2C%20etc.%29%2C%0Ademonstrating%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Companion%2520Learning%253A%2520Enhancing%2520Generalization%2520Through%2520Historical%250A%2520%2520Consistency%26entry.906535625%3DRuizhao%2520Zhu%2520and%2520Venkatesh%2520Saligrama%26entry.1292438233%3D%2520%2520We%2520propose%2520Deep%2520Companion%2520Learning%2520%2528DCL%2529%252C%2520a%2520novel%2520training%2520method%2520for%2520Deep%250ANeural%2520Networks%2520%2528DNNs%2529%2520that%2520enhances%2520generalization%2520by%2520penalizing%2520inconsistent%250Amodel%2520predictions%2520compared%2520to%2520its%2520historical%2520performance.%2520To%2520achieve%2520this%252C%2520we%250Atrain%2520a%2520deep-companion%2520model%2520%2528DCM%2529%252C%2520by%2520using%2520previous%2520versions%2520of%2520the%2520model%2520to%250Aprovide%2520forecasts%2520on%2520new%2520inputs.%2520This%2520companion%2520model%2520deciphers%2520a%2520meaningful%250Alatent%2520semantic%2520structure%2520within%2520the%2520data%252C%2520thereby%2520providing%2520targeted%250Asupervision%2520that%2520encourages%2520the%2520primary%2520model%2520to%2520address%2520the%2520scenarios%2520it%2520finds%250Amost%2520challenging.%2520We%2520validate%2520our%2520approach%2520through%2520both%2520theoretical%2520analysis%250Aand%2520extensive%2520experimentation%252C%2520including%2520ablation%2520studies%252C%2520on%2520a%2520variety%2520of%250Abenchmark%2520datasets%2520%2528CIFAR-100%252C%2520Tiny-ImageNet%252C%2520ImageNet-1K%2529%2520using%2520diverse%250Aarchitectural%2520models%2520%2528ShuffleNetV2%252C%2520ResNet%252C%2520Vision%2520Transformer%252C%2520etc.%2529%252C%250Ademonstrating%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Companion%20Learning%3A%20Enhancing%20Generalization%20Through%20Historical%0A%20%20Consistency&entry.906535625=Ruizhao%20Zhu%20and%20Venkatesh%20Saligrama&entry.1292438233=%20%20We%20propose%20Deep%20Companion%20Learning%20%28DCL%29%2C%20a%20novel%20training%20method%20for%20Deep%0ANeural%20Networks%20%28DNNs%29%20that%20enhances%20generalization%20by%20penalizing%20inconsistent%0Amodel%20predictions%20compared%20to%20its%20historical%20performance.%20To%20achieve%20this%2C%20we%0Atrain%20a%20deep-companion%20model%20%28DCM%29%2C%20by%20using%20previous%20versions%20of%20the%20model%20to%0Aprovide%20forecasts%20on%20new%20inputs.%20This%20companion%20model%20deciphers%20a%20meaningful%0Alatent%20semantic%20structure%20within%20the%20data%2C%20thereby%20providing%20targeted%0Asupervision%20that%20encourages%20the%20primary%20model%20to%20address%20the%20scenarios%20it%20finds%0Amost%20challenging.%20We%20validate%20our%20approach%20through%20both%20theoretical%20analysis%0Aand%20extensive%20experimentation%2C%20including%20ablation%20studies%2C%20on%20a%20variety%20of%0Abenchmark%20datasets%20%28CIFAR-100%2C%20Tiny-ImageNet%2C%20ImageNet-1K%29%20using%20diverse%0Aarchitectural%20models%20%28ShuffleNetV2%2C%20ResNet%2C%20Vision%20Transformer%2C%20etc.%29%2C%0Ademonstrating%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18821v1&entry.124074799=Read"},
{"title": "Neurosymbolic AI for Enhancing Instructability in Generative AI", "author": "Amit Sheth and Vishal Pallagani and Kaushik Roy", "abstract": "  Generative AI, especially via Large Language Models (LLMs), has transformed\ncontent creation across text, images, and music, showcasing capabilities in\nfollowing instructions through prompting, largely facilitated by instruction\ntuning. Instruction tuning is a supervised fine-tuning method where LLMs are\ntrained on datasets formatted with specific tasks and corresponding\ninstructions. This method systematically enhances the model's ability to\ncomprehend and execute the provided directives. Despite these advancements,\nLLMs still face challenges in consistently interpreting complex, multi-step\ninstructions and generalizing them to novel tasks, which are essential for\nbroader applicability in real-world scenarios. This article explores why\nneurosymbolic AI offers a better path to enhance the instructability of LLMs.\nWe explore the use a symbolic task planner to decompose high-level instructions\ninto structured tasks, a neural semantic parser to ground these tasks into\nexecutable actions, and a neuro-symbolic executor to implement these actions\nwhile dynamically maintaining an explicit representation of state. We also seek\nto show that neurosymbolic approach enhances the reliability and\ncontext-awareness of task execution, enabling LLMs to dynamically interpret and\nrespond to a wider range of instructional contexts with greater precision and\nflexibility.\n", "link": "http://arxiv.org/abs/2407.18722v1", "date": "2024-07-26", "relevancy": 2.1115, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5669}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neurosymbolic%20AI%20for%20Enhancing%20Instructability%20in%20Generative%20AI&body=Title%3A%20Neurosymbolic%20AI%20for%20Enhancing%20Instructability%20in%20Generative%20AI%0AAuthor%3A%20Amit%20Sheth%20and%20Vishal%20Pallagani%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Generative%20AI%2C%20especially%20via%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20transformed%0Acontent%20creation%20across%20text%2C%20images%2C%20and%20music%2C%20showcasing%20capabilities%20in%0Afollowing%20instructions%20through%20prompting%2C%20largely%20facilitated%20by%20instruction%0Atuning.%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20method%20where%20LLMs%20are%0Atrained%20on%20datasets%20formatted%20with%20specific%20tasks%20and%20corresponding%0Ainstructions.%20This%20method%20systematically%20enhances%20the%20model%27s%20ability%20to%0Acomprehend%20and%20execute%20the%20provided%20directives.%20Despite%20these%20advancements%2C%0ALLMs%20still%20face%20challenges%20in%20consistently%20interpreting%20complex%2C%20multi-step%0Ainstructions%20and%20generalizing%20them%20to%20novel%20tasks%2C%20which%20are%20essential%20for%0Abroader%20applicability%20in%20real-world%20scenarios.%20This%20article%20explores%20why%0Aneurosymbolic%20AI%20offers%20a%20better%20path%20to%20enhance%20the%20instructability%20of%20LLMs.%0AWe%20explore%20the%20use%20a%20symbolic%20task%20planner%20to%20decompose%20high-level%20instructions%0Ainto%20structured%20tasks%2C%20a%20neural%20semantic%20parser%20to%20ground%20these%20tasks%20into%0Aexecutable%20actions%2C%20and%20a%20neuro-symbolic%20executor%20to%20implement%20these%20actions%0Awhile%20dynamically%20maintaining%20an%20explicit%20representation%20of%20state.%20We%20also%20seek%0Ato%20show%20that%20neurosymbolic%20approach%20enhances%20the%20reliability%20and%0Acontext-awareness%20of%20task%20execution%2C%20enabling%20LLMs%20to%20dynamically%20interpret%20and%0Arespond%20to%20a%20wider%20range%20of%20instructional%20contexts%20with%20greater%20precision%20and%0Aflexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurosymbolic%2520AI%2520for%2520Enhancing%2520Instructability%2520in%2520Generative%2520AI%26entry.906535625%3DAmit%2520Sheth%2520and%2520Vishal%2520Pallagani%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Generative%2520AI%252C%2520especially%2520via%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520has%2520transformed%250Acontent%2520creation%2520across%2520text%252C%2520images%252C%2520and%2520music%252C%2520showcasing%2520capabilities%2520in%250Afollowing%2520instructions%2520through%2520prompting%252C%2520largely%2520facilitated%2520by%2520instruction%250Atuning.%2520Instruction%2520tuning%2520is%2520a%2520supervised%2520fine-tuning%2520method%2520where%2520LLMs%2520are%250Atrained%2520on%2520datasets%2520formatted%2520with%2520specific%2520tasks%2520and%2520corresponding%250Ainstructions.%2520This%2520method%2520systematically%2520enhances%2520the%2520model%2527s%2520ability%2520to%250Acomprehend%2520and%2520execute%2520the%2520provided%2520directives.%2520Despite%2520these%2520advancements%252C%250ALLMs%2520still%2520face%2520challenges%2520in%2520consistently%2520interpreting%2520complex%252C%2520multi-step%250Ainstructions%2520and%2520generalizing%2520them%2520to%2520novel%2520tasks%252C%2520which%2520are%2520essential%2520for%250Abroader%2520applicability%2520in%2520real-world%2520scenarios.%2520This%2520article%2520explores%2520why%250Aneurosymbolic%2520AI%2520offers%2520a%2520better%2520path%2520to%2520enhance%2520the%2520instructability%2520of%2520LLMs.%250AWe%2520explore%2520the%2520use%2520a%2520symbolic%2520task%2520planner%2520to%2520decompose%2520high-level%2520instructions%250Ainto%2520structured%2520tasks%252C%2520a%2520neural%2520semantic%2520parser%2520to%2520ground%2520these%2520tasks%2520into%250Aexecutable%2520actions%252C%2520and%2520a%2520neuro-symbolic%2520executor%2520to%2520implement%2520these%2520actions%250Awhile%2520dynamically%2520maintaining%2520an%2520explicit%2520representation%2520of%2520state.%2520We%2520also%2520seek%250Ato%2520show%2520that%2520neurosymbolic%2520approach%2520enhances%2520the%2520reliability%2520and%250Acontext-awareness%2520of%2520task%2520execution%252C%2520enabling%2520LLMs%2520to%2520dynamically%2520interpret%2520and%250Arespond%2520to%2520a%2520wider%2520range%2520of%2520instructional%2520contexts%2520with%2520greater%2520precision%2520and%250Aflexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neurosymbolic%20AI%20for%20Enhancing%20Instructability%20in%20Generative%20AI&entry.906535625=Amit%20Sheth%20and%20Vishal%20Pallagani%20and%20Kaushik%20Roy&entry.1292438233=%20%20Generative%20AI%2C%20especially%20via%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20transformed%0Acontent%20creation%20across%20text%2C%20images%2C%20and%20music%2C%20showcasing%20capabilities%20in%0Afollowing%20instructions%20through%20prompting%2C%20largely%20facilitated%20by%20instruction%0Atuning.%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20method%20where%20LLMs%20are%0Atrained%20on%20datasets%20formatted%20with%20specific%20tasks%20and%20corresponding%0Ainstructions.%20This%20method%20systematically%20enhances%20the%20model%27s%20ability%20to%0Acomprehend%20and%20execute%20the%20provided%20directives.%20Despite%20these%20advancements%2C%0ALLMs%20still%20face%20challenges%20in%20consistently%20interpreting%20complex%2C%20multi-step%0Ainstructions%20and%20generalizing%20them%20to%20novel%20tasks%2C%20which%20are%20essential%20for%0Abroader%20applicability%20in%20real-world%20scenarios.%20This%20article%20explores%20why%0Aneurosymbolic%20AI%20offers%20a%20better%20path%20to%20enhance%20the%20instructability%20of%20LLMs.%0AWe%20explore%20the%20use%20a%20symbolic%20task%20planner%20to%20decompose%20high-level%20instructions%0Ainto%20structured%20tasks%2C%20a%20neural%20semantic%20parser%20to%20ground%20these%20tasks%20into%0Aexecutable%20actions%2C%20and%20a%20neuro-symbolic%20executor%20to%20implement%20these%20actions%0Awhile%20dynamically%20maintaining%20an%20explicit%20representation%20of%20state.%20We%20also%20seek%0Ato%20show%20that%20neurosymbolic%20approach%20enhances%20the%20reliability%20and%0Acontext-awareness%20of%20task%20execution%2C%20enabling%20LLMs%20to%20dynamically%20interpret%20and%0Arespond%20to%20a%20wider%20range%20of%20instructional%20contexts%20with%20greater%20precision%20and%0Aflexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18722v1&entry.124074799=Read"},
{"title": "Lessons from Learning to Spin \"Pens\"", "author": "Jun Wang and Ying Yuan and Haichuan Che and Haozhi Qi and Yi Ma and Jitendra Malik and Xiaolong Wang", "abstract": "  In-hand manipulation of pen-like objects is an important skill in our daily\nlives, as many tools such as hammers and screwdrivers are similarly shaped.\nHowever, current learning-based methods struggle with this task due to a lack\nof high-quality demonstrations and the significant gap between simulation and\nthe real world. In this work, we push the boundaries of learning-based in-hand\nmanipulation systems by demonstrating the capability to spin pen-like objects.\nWe first use reinforcement learning to train an oracle policy with privileged\ninformation and generate a high-fidelity trajectory dataset in simulation. This\nserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)\nconducting open-loop trajectory replay in the real world. We then fine-tune the\nsensorimotor policy using these real-world trajectories to adapt it to the real\nworld dynamics. With less than 50 trajectories, our policy learns to rotate\nmore than ten pen-like objects with different physical properties for multiple\nrevolutions. We present a comprehensive analysis of our design choices and\nshare the lessons learned during development.\n", "link": "http://arxiv.org/abs/2407.18902v1", "date": "2024-07-26", "relevancy": 2.0779, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5186}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lessons%20from%20Learning%20to%20Spin%20%22Pens%22&body=Title%3A%20Lessons%20from%20Learning%20to%20Spin%20%22Pens%22%0AAuthor%3A%20Jun%20Wang%20and%20Ying%20Yuan%20and%20Haichuan%20Che%20and%20Haozhi%20Qi%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20In-hand%20manipulation%20of%20pen-like%20objects%20is%20an%20important%20skill%20in%20our%20daily%0Alives%2C%20as%20many%20tools%20such%20as%20hammers%20and%20screwdrivers%20are%20similarly%20shaped.%0AHowever%2C%20current%20learning-based%20methods%20struggle%20with%20this%20task%20due%20to%20a%20lack%0Aof%20high-quality%20demonstrations%20and%20the%20significant%20gap%20between%20simulation%20and%0Athe%20real%20world.%20In%20this%20work%2C%20we%20push%20the%20boundaries%20of%20learning-based%20in-hand%0Amanipulation%20systems%20by%20demonstrating%20the%20capability%20to%20spin%20pen-like%20objects.%0AWe%20first%20use%20reinforcement%20learning%20to%20train%20an%20oracle%20policy%20with%20privileged%0Ainformation%20and%20generate%20a%20high-fidelity%20trajectory%20dataset%20in%20simulation.%20This%0Aserves%20two%20purposes%3A%201%29%20pre-training%20a%20sensorimotor%20policy%20in%20simulation%3B%202%29%0Aconducting%20open-loop%20trajectory%20replay%20in%20the%20real%20world.%20We%20then%20fine-tune%20the%0Asensorimotor%20policy%20using%20these%20real-world%20trajectories%20to%20adapt%20it%20to%20the%20real%0Aworld%20dynamics.%20With%20less%20than%2050%20trajectories%2C%20our%20policy%20learns%20to%20rotate%0Amore%20than%20ten%20pen-like%20objects%20with%20different%20physical%20properties%20for%20multiple%0Arevolutions.%20We%20present%20a%20comprehensive%20analysis%20of%20our%20design%20choices%20and%0Ashare%20the%20lessons%20learned%20during%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLessons%2520from%2520Learning%2520to%2520Spin%2520%2522Pens%2522%26entry.906535625%3DJun%2520Wang%2520and%2520Ying%2520Yuan%2520and%2520Haichuan%2520Che%2520and%2520Haozhi%2520Qi%2520and%2520Yi%2520Ma%2520and%2520Jitendra%2520Malik%2520and%2520Xiaolong%2520Wang%26entry.1292438233%3D%2520%2520In-hand%2520manipulation%2520of%2520pen-like%2520objects%2520is%2520an%2520important%2520skill%2520in%2520our%2520daily%250Alives%252C%2520as%2520many%2520tools%2520such%2520as%2520hammers%2520and%2520screwdrivers%2520are%2520similarly%2520shaped.%250AHowever%252C%2520current%2520learning-based%2520methods%2520struggle%2520with%2520this%2520task%2520due%2520to%2520a%2520lack%250Aof%2520high-quality%2520demonstrations%2520and%2520the%2520significant%2520gap%2520between%2520simulation%2520and%250Athe%2520real%2520world.%2520In%2520this%2520work%252C%2520we%2520push%2520the%2520boundaries%2520of%2520learning-based%2520in-hand%250Amanipulation%2520systems%2520by%2520demonstrating%2520the%2520capability%2520to%2520spin%2520pen-like%2520objects.%250AWe%2520first%2520use%2520reinforcement%2520learning%2520to%2520train%2520an%2520oracle%2520policy%2520with%2520privileged%250Ainformation%2520and%2520generate%2520a%2520high-fidelity%2520trajectory%2520dataset%2520in%2520simulation.%2520This%250Aserves%2520two%2520purposes%253A%25201%2529%2520pre-training%2520a%2520sensorimotor%2520policy%2520in%2520simulation%253B%25202%2529%250Aconducting%2520open-loop%2520trajectory%2520replay%2520in%2520the%2520real%2520world.%2520We%2520then%2520fine-tune%2520the%250Asensorimotor%2520policy%2520using%2520these%2520real-world%2520trajectories%2520to%2520adapt%2520it%2520to%2520the%2520real%250Aworld%2520dynamics.%2520With%2520less%2520than%252050%2520trajectories%252C%2520our%2520policy%2520learns%2520to%2520rotate%250Amore%2520than%2520ten%2520pen-like%2520objects%2520with%2520different%2520physical%2520properties%2520for%2520multiple%250Arevolutions.%2520We%2520present%2520a%2520comprehensive%2520analysis%2520of%2520our%2520design%2520choices%2520and%250Ashare%2520the%2520lessons%2520learned%2520during%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lessons%20from%20Learning%20to%20Spin%20%22Pens%22&entry.906535625=Jun%20Wang%20and%20Ying%20Yuan%20and%20Haichuan%20Che%20and%20Haozhi%20Qi%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Xiaolong%20Wang&entry.1292438233=%20%20In-hand%20manipulation%20of%20pen-like%20objects%20is%20an%20important%20skill%20in%20our%20daily%0Alives%2C%20as%20many%20tools%20such%20as%20hammers%20and%20screwdrivers%20are%20similarly%20shaped.%0AHowever%2C%20current%20learning-based%20methods%20struggle%20with%20this%20task%20due%20to%20a%20lack%0Aof%20high-quality%20demonstrations%20and%20the%20significant%20gap%20between%20simulation%20and%0Athe%20real%20world.%20In%20this%20work%2C%20we%20push%20the%20boundaries%20of%20learning-based%20in-hand%0Amanipulation%20systems%20by%20demonstrating%20the%20capability%20to%20spin%20pen-like%20objects.%0AWe%20first%20use%20reinforcement%20learning%20to%20train%20an%20oracle%20policy%20with%20privileged%0Ainformation%20and%20generate%20a%20high-fidelity%20trajectory%20dataset%20in%20simulation.%20This%0Aserves%20two%20purposes%3A%201%29%20pre-training%20a%20sensorimotor%20policy%20in%20simulation%3B%202%29%0Aconducting%20open-loop%20trajectory%20replay%20in%20the%20real%20world.%20We%20then%20fine-tune%20the%0Asensorimotor%20policy%20using%20these%20real-world%20trajectories%20to%20adapt%20it%20to%20the%20real%0Aworld%20dynamics.%20With%20less%20than%2050%20trajectories%2C%20our%20policy%20learns%20to%20rotate%0Amore%20than%20ten%20pen-like%20objects%20with%20different%20physical%20properties%20for%20multiple%0Arevolutions.%20We%20present%20a%20comprehensive%20analysis%20of%20our%20design%20choices%20and%0Ashare%20the%20lessons%20learned%20during%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18902v1&entry.124074799=Read"},
{"title": "Pseudo-Prompt Generating in Pre-trained Vision-Language Models for\n  Multi-Label Medical Image Classification", "author": "Yaoqin Ye and Junjie Zhang and Hongwei Shi", "abstract": "  The task of medical image recognition is notably complicated by the presence\nof varied and multiple pathological indications, presenting a unique challenge\nin multi-label classification with unseen labels. This complexity underlines\nthe need for computer-aided diagnosis methods employing multi-label zero-shot\nlearning. Recent advancements in pre-trained vision-language models (VLMs) have\nshowcased notable zero-shot classification abilities on medical images.\nHowever, these methods have limitations on leveraging extensive pre-trained\nknowledge from broader image datasets, and often depend on manual prompt\nconstruction by expert radiologists. By automating the process of prompt\ntuning, prompt learning techniques have emerged as an efficient way to adapt\nVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in\nperforming class-specific prompts on unseen categories, limiting\ngeneralizability in fine-grained scenarios. To overcome these constraints, we\nintroduce a novel prompt generation approach inspirited by text generation in\nnatural language processing (NLP). Our method, named Pseudo-Prompt Generating\n(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring\na RNN-based decoder, PsPG autoregressively generates class-tailored embedding\nvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label\nchest radiograph datasets affirm the superiority of our approach against\nleading medical vision-language and multi-label prompt learning methods. The\nsource code is available at https://github.com/fallingnight/PsPG\n", "link": "http://arxiv.org/abs/2405.06468v2", "date": "2024-07-26", "relevancy": 2.0725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&body=Title%3A%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification%0AAuthor%3A%20Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi%0AAbstract%3A%20%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Prompt%2520Generating%2520in%2520Pre-trained%2520Vision-Language%2520Models%2520for%250A%2520%2520Multi-Label%2520Medical%2520Image%2520Classification%26entry.906535625%3DYaoqin%2520Ye%2520and%2520Junjie%2520Zhang%2520and%2520Hongwei%2520Shi%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520medical%2520image%2520recognition%2520is%2520notably%2520complicated%2520by%2520the%2520presence%250Aof%2520varied%2520and%2520multiple%2520pathological%2520indications%252C%2520presenting%2520a%2520unique%2520challenge%250Ain%2520multi-label%2520classification%2520with%2520unseen%2520labels.%2520This%2520complexity%2520underlines%250Athe%2520need%2520for%2520computer-aided%2520diagnosis%2520methods%2520employing%2520multi-label%2520zero-shot%250Alearning.%2520Recent%2520advancements%2520in%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%250Ashowcased%2520notable%2520zero-shot%2520classification%2520abilities%2520on%2520medical%2520images.%250AHowever%252C%2520these%2520methods%2520have%2520limitations%2520on%2520leveraging%2520extensive%2520pre-trained%250Aknowledge%2520from%2520broader%2520image%2520datasets%252C%2520and%2520often%2520depend%2520on%2520manual%2520prompt%250Aconstruction%2520by%2520expert%2520radiologists.%2520By%2520automating%2520the%2520process%2520of%2520prompt%250Atuning%252C%2520prompt%2520learning%2520techniques%2520have%2520emerged%2520as%2520an%2520efficient%2520way%2520to%2520adapt%250AVLMs%2520to%2520downstream%2520tasks.%2520Yet%252C%2520existing%2520CoOp-based%2520strategies%2520fall%2520short%2520in%250Aperforming%2520class-specific%2520prompts%2520on%2520unseen%2520categories%252C%2520limiting%250Ageneralizability%2520in%2520fine-grained%2520scenarios.%2520To%2520overcome%2520these%2520constraints%252C%2520we%250Aintroduce%2520a%2520novel%2520prompt%2520generation%2520approach%2520inspirited%2520by%2520text%2520generation%2520in%250Anatural%2520language%2520processing%2520%2528NLP%2529.%2520Our%2520method%252C%2520named%2520Pseudo-Prompt%2520Generating%250A%2528PsPG%2529%252C%2520capitalizes%2520on%2520the%2520priori%2520knowledge%2520of%2520multi-modal%2520features.%2520Featuring%250Aa%2520RNN-based%2520decoder%252C%2520PsPG%2520autoregressively%2520generates%2520class-tailored%2520embedding%250Avectors%252C%2520i.e.%252C%2520pseudo-prompts.%2520Comparative%2520evaluations%2520on%2520various%2520multi-label%250Achest%2520radiograph%2520datasets%2520affirm%2520the%2520superiority%2520of%2520our%2520approach%2520against%250Aleading%2520medical%2520vision-language%2520and%2520multi-label%2520prompt%2520learning%2520methods.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/fallingnight/PsPG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&entry.906535625=Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi&entry.1292438233=%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06468v2&entry.124074799=Read"},
{"title": "Target Specific De Novo Design of Drug Candidate Molecules with Graph\n  Transformer-based Generative Adversarial Networks", "author": "Atabey \u00dcnl\u00fc and Elif \u00c7evrim and Ahmet Sar\u0131g\u00fcn and Melih G\u00f6kay Yi\u011fit and Hayriye \u00c7elikbilek and Osman Bayram and Heval Ata\u015f G\u00fcvenilir and Altay Koya\u015f and Deniz Cansen Kahraman and Abdurrahman Ol\u011fa\u00e7 and Ahmet Rifaio\u011flu and Erden Bano\u011flu and Tunca Do\u011fan", "abstract": "  Discovering novel drug candidate molecules is one of the most fundamental and\ncritical steps in drug development. Generative deep learning models, which\ncreate synthetic data given a probability distribution, offer a high potential\nfor designing de novo molecules. However, for them to be useful in real-life\ndrug development pipelines, these models should be able to design drug-like and\ntarget-centric molecules. In this study, we propose an end-to-end generative\nsystem, DrugGEN, for the de novo design of drug candidate molecules that\ninteract with intended target proteins. The proposed method represents\nmolecules as graphs and processes them via a generative adversarial network\ncomprising graph transformer layers. The system is trained using a large\ndataset of drug-like compounds and target-specific bioactive molecules to\ndesign effective inhibitory molecules against the AKT1 protein, which is\ncritically important in developing treatments for various types of cancer. We\nconducted molecular docking and dynamics to assess the target-centric\ngeneration performance of the model, as well as attention score visualisation\nto examine model interpretability. Results indicate that our de novo molecules\nhave a high potential for interacting with the AKT1 protein at the level of its\nnative ligands. Using the open-access DrugGEN codebase, it is possible to\neasily train models for other druggable proteins, given a dataset of\nexperimentally known bioactive molecules.\n", "link": "http://arxiv.org/abs/2302.07868v6", "date": "2024-07-26", "relevancy": 2.0464, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target%20Specific%20De%20Novo%20Design%20of%20Drug%20Candidate%20Molecules%20with%20Graph%0A%20%20Transformer-based%20Generative%20Adversarial%20Networks&body=Title%3A%20Target%20Specific%20De%20Novo%20Design%20of%20Drug%20Candidate%20Molecules%20with%20Graph%0A%20%20Transformer-based%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Atabey%20%C3%9Cnl%C3%BC%20and%20Elif%20%C3%87evrim%20and%20Ahmet%20Sar%C4%B1g%C3%BCn%20and%20Melih%20G%C3%B6kay%20Yi%C4%9Fit%20and%20Hayriye%20%C3%87elikbilek%20and%20Osman%20Bayram%20and%20Heval%20Ata%C5%9F%20G%C3%BCvenilir%20and%20Altay%20Koya%C5%9F%20and%20Deniz%20Cansen%20Kahraman%20and%20Abdurrahman%20Ol%C4%9Fa%C3%A7%20and%20Ahmet%20Rifaio%C4%9Flu%20and%20Erden%20Bano%C4%9Flu%20and%20Tunca%20Do%C4%9Fan%0AAbstract%3A%20%20%20Discovering%20novel%20drug%20candidate%20molecules%20is%20one%20of%20the%20most%20fundamental%20and%0Acritical%20steps%20in%20drug%20development.%20Generative%20deep%20learning%20models%2C%20which%0Acreate%20synthetic%20data%20given%20a%20probability%20distribution%2C%20offer%20a%20high%20potential%0Afor%20designing%20de%20novo%20molecules.%20However%2C%20for%20them%20to%20be%20useful%20in%20real-life%0Adrug%20development%20pipelines%2C%20these%20models%20should%20be%20able%20to%20design%20drug-like%20and%0Atarget-centric%20molecules.%20In%20this%20study%2C%20we%20propose%20an%20end-to-end%20generative%0Asystem%2C%20DrugGEN%2C%20for%20the%20de%20novo%20design%20of%20drug%20candidate%20molecules%20that%0Ainteract%20with%20intended%20target%20proteins.%20The%20proposed%20method%20represents%0Amolecules%20as%20graphs%20and%20processes%20them%20via%20a%20generative%20adversarial%20network%0Acomprising%20graph%20transformer%20layers.%20The%20system%20is%20trained%20using%20a%20large%0Adataset%20of%20drug-like%20compounds%20and%20target-specific%20bioactive%20molecules%20to%0Adesign%20effective%20inhibitory%20molecules%20against%20the%20AKT1%20protein%2C%20which%20is%0Acritically%20important%20in%20developing%20treatments%20for%20various%20types%20of%20cancer.%20We%0Aconducted%20molecular%20docking%20and%20dynamics%20to%20assess%20the%20target-centric%0Ageneration%20performance%20of%20the%20model%2C%20as%20well%20as%20attention%20score%20visualisation%0Ato%20examine%20model%20interpretability.%20Results%20indicate%20that%20our%20de%20novo%20molecules%0Ahave%20a%20high%20potential%20for%20interacting%20with%20the%20AKT1%20protein%20at%20the%20level%20of%20its%0Anative%20ligands.%20Using%20the%20open-access%20DrugGEN%20codebase%2C%20it%20is%20possible%20to%0Aeasily%20train%20models%20for%20other%20druggable%20proteins%2C%20given%20a%20dataset%20of%0Aexperimentally%20known%20bioactive%20molecules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.07868v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget%2520Specific%2520De%2520Novo%2520Design%2520of%2520Drug%2520Candidate%2520Molecules%2520with%2520Graph%250A%2520%2520Transformer-based%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DAtabey%2520%25C3%259Cnl%25C3%25BC%2520and%2520Elif%2520%25C3%2587evrim%2520and%2520Ahmet%2520Sar%25C4%25B1g%25C3%25BCn%2520and%2520Melih%2520G%25C3%25B6kay%2520Yi%25C4%259Fit%2520and%2520Hayriye%2520%25C3%2587elikbilek%2520and%2520Osman%2520Bayram%2520and%2520Heval%2520Ata%25C5%259F%2520G%25C3%25BCvenilir%2520and%2520Altay%2520Koya%25C5%259F%2520and%2520Deniz%2520Cansen%2520Kahraman%2520and%2520Abdurrahman%2520Ol%25C4%259Fa%25C3%25A7%2520and%2520Ahmet%2520Rifaio%25C4%259Flu%2520and%2520Erden%2520Bano%25C4%259Flu%2520and%2520Tunca%2520Do%25C4%259Fan%26entry.1292438233%3D%2520%2520Discovering%2520novel%2520drug%2520candidate%2520molecules%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520and%250Acritical%2520steps%2520in%2520drug%2520development.%2520Generative%2520deep%2520learning%2520models%252C%2520which%250Acreate%2520synthetic%2520data%2520given%2520a%2520probability%2520distribution%252C%2520offer%2520a%2520high%2520potential%250Afor%2520designing%2520de%2520novo%2520molecules.%2520However%252C%2520for%2520them%2520to%2520be%2520useful%2520in%2520real-life%250Adrug%2520development%2520pipelines%252C%2520these%2520models%2520should%2520be%2520able%2520to%2520design%2520drug-like%2520and%250Atarget-centric%2520molecules.%2520In%2520this%2520study%252C%2520we%2520propose%2520an%2520end-to-end%2520generative%250Asystem%252C%2520DrugGEN%252C%2520for%2520the%2520de%2520novo%2520design%2520of%2520drug%2520candidate%2520molecules%2520that%250Ainteract%2520with%2520intended%2520target%2520proteins.%2520The%2520proposed%2520method%2520represents%250Amolecules%2520as%2520graphs%2520and%2520processes%2520them%2520via%2520a%2520generative%2520adversarial%2520network%250Acomprising%2520graph%2520transformer%2520layers.%2520The%2520system%2520is%2520trained%2520using%2520a%2520large%250Adataset%2520of%2520drug-like%2520compounds%2520and%2520target-specific%2520bioactive%2520molecules%2520to%250Adesign%2520effective%2520inhibitory%2520molecules%2520against%2520the%2520AKT1%2520protein%252C%2520which%2520is%250Acritically%2520important%2520in%2520developing%2520treatments%2520for%2520various%2520types%2520of%2520cancer.%2520We%250Aconducted%2520molecular%2520docking%2520and%2520dynamics%2520to%2520assess%2520the%2520target-centric%250Ageneration%2520performance%2520of%2520the%2520model%252C%2520as%2520well%2520as%2520attention%2520score%2520visualisation%250Ato%2520examine%2520model%2520interpretability.%2520Results%2520indicate%2520that%2520our%2520de%2520novo%2520molecules%250Ahave%2520a%2520high%2520potential%2520for%2520interacting%2520with%2520the%2520AKT1%2520protein%2520at%2520the%2520level%2520of%2520its%250Anative%2520ligands.%2520Using%2520the%2520open-access%2520DrugGEN%2520codebase%252C%2520it%2520is%2520possible%2520to%250Aeasily%2520train%2520models%2520for%2520other%2520druggable%2520proteins%252C%2520given%2520a%2520dataset%2520of%250Aexperimentally%2520known%2520bioactive%2520molecules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.07868v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target%20Specific%20De%20Novo%20Design%20of%20Drug%20Candidate%20Molecules%20with%20Graph%0A%20%20Transformer-based%20Generative%20Adversarial%20Networks&entry.906535625=Atabey%20%C3%9Cnl%C3%BC%20and%20Elif%20%C3%87evrim%20and%20Ahmet%20Sar%C4%B1g%C3%BCn%20and%20Melih%20G%C3%B6kay%20Yi%C4%9Fit%20and%20Hayriye%20%C3%87elikbilek%20and%20Osman%20Bayram%20and%20Heval%20Ata%C5%9F%20G%C3%BCvenilir%20and%20Altay%20Koya%C5%9F%20and%20Deniz%20Cansen%20Kahraman%20and%20Abdurrahman%20Ol%C4%9Fa%C3%A7%20and%20Ahmet%20Rifaio%C4%9Flu%20and%20Erden%20Bano%C4%9Flu%20and%20Tunca%20Do%C4%9Fan&entry.1292438233=%20%20Discovering%20novel%20drug%20candidate%20molecules%20is%20one%20of%20the%20most%20fundamental%20and%0Acritical%20steps%20in%20drug%20development.%20Generative%20deep%20learning%20models%2C%20which%0Acreate%20synthetic%20data%20given%20a%20probability%20distribution%2C%20offer%20a%20high%20potential%0Afor%20designing%20de%20novo%20molecules.%20However%2C%20for%20them%20to%20be%20useful%20in%20real-life%0Adrug%20development%20pipelines%2C%20these%20models%20should%20be%20able%20to%20design%20drug-like%20and%0Atarget-centric%20molecules.%20In%20this%20study%2C%20we%20propose%20an%20end-to-end%20generative%0Asystem%2C%20DrugGEN%2C%20for%20the%20de%20novo%20design%20of%20drug%20candidate%20molecules%20that%0Ainteract%20with%20intended%20target%20proteins.%20The%20proposed%20method%20represents%0Amolecules%20as%20graphs%20and%20processes%20them%20via%20a%20generative%20adversarial%20network%0Acomprising%20graph%20transformer%20layers.%20The%20system%20is%20trained%20using%20a%20large%0Adataset%20of%20drug-like%20compounds%20and%20target-specific%20bioactive%20molecules%20to%0Adesign%20effective%20inhibitory%20molecules%20against%20the%20AKT1%20protein%2C%20which%20is%0Acritically%20important%20in%20developing%20treatments%20for%20various%20types%20of%20cancer.%20We%0Aconducted%20molecular%20docking%20and%20dynamics%20to%20assess%20the%20target-centric%0Ageneration%20performance%20of%20the%20model%2C%20as%20well%20as%20attention%20score%20visualisation%0Ato%20examine%20model%20interpretability.%20Results%20indicate%20that%20our%20de%20novo%20molecules%0Ahave%20a%20high%20potential%20for%20interacting%20with%20the%20AKT1%20protein%20at%20the%20level%20of%20its%0Anative%20ligands.%20Using%20the%20open-access%20DrugGEN%20codebase%2C%20it%20is%20possible%20to%0Aeasily%20train%20models%20for%20other%20druggable%20proteins%2C%20given%20a%20dataset%20of%0Aexperimentally%20known%20bioactive%20molecules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07868v6&entry.124074799=Read"},
{"title": "Towards Interactive Autonomous Vehicle Testing:\n  Vehicle-Under-Test-Centered Traffic Simulation", "author": "Yiru Liu and Xiaocong Zhao and Jian Sun", "abstract": "  The simulation-based testing is essential for safely implementing autonomous\nvehicles (AV) on roads, necessitating simulated traffic environments that\ndynamically interact with the Vehicle Under Test (VUT). This study introduces a\nVUT-Centered environmental Dynamics Inference (VCDI) model for realistic,\ninteractive, and diverse background traffic simulation. Serving the purpose of\nAV testing, VCDI employs Transformer-based modules in a conditional trajectory\ninference framework to simulate VUT-centered driving interaction events. First,\nthe VUT future motion is taken as an augmented model input to bridge the action\ndependence between VUT and background objects. Second, to enrich the scenario\ndiversity, a Gaussian-distributional cost function module is designed to\ncapture the uncertainty of the VUT's strategy, triggering various scenario\nevolution. Experimental results validate VCDI's trajectory-level simulation\nprecision which outperforms the state-of-the-art trajectory prediction work.\nThe flexibility of the distributional cost function allows VCDI to provide\ndiverse-yet-realistic scenarios for AV testing. We demonstrate such capability\nby modifying the anticipation to the VUT's cost-based strategy and thus achieve\nmultiple testing scenarios with explainable background traffic evolution. Codes\nare available at https://github.com/YNYSNL/VCDI.\n", "link": "http://arxiv.org/abs/2406.02860v2", "date": "2024-07-26", "relevancy": 2.0407, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5229}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5085}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interactive%20Autonomous%20Vehicle%20Testing%3A%0A%20%20Vehicle-Under-Test-Centered%20Traffic%20Simulation&body=Title%3A%20Towards%20Interactive%20Autonomous%20Vehicle%20Testing%3A%0A%20%20Vehicle-Under-Test-Centered%20Traffic%20Simulation%0AAuthor%3A%20Yiru%20Liu%20and%20Xiaocong%20Zhao%20and%20Jian%20Sun%0AAbstract%3A%20%20%20The%20simulation-based%20testing%20is%20essential%20for%20safely%20implementing%20autonomous%0Avehicles%20%28AV%29%20on%20roads%2C%20necessitating%20simulated%20traffic%20environments%20that%0Adynamically%20interact%20with%20the%20Vehicle%20Under%20Test%20%28VUT%29.%20This%20study%20introduces%20a%0AVUT-Centered%20environmental%20Dynamics%20Inference%20%28VCDI%29%20model%20for%20realistic%2C%0Ainteractive%2C%20and%20diverse%20background%20traffic%20simulation.%20Serving%20the%20purpose%20of%0AAV%20testing%2C%20VCDI%20employs%20Transformer-based%20modules%20in%20a%20conditional%20trajectory%0Ainference%20framework%20to%20simulate%20VUT-centered%20driving%20interaction%20events.%20First%2C%0Athe%20VUT%20future%20motion%20is%20taken%20as%20an%20augmented%20model%20input%20to%20bridge%20the%20action%0Adependence%20between%20VUT%20and%20background%20objects.%20Second%2C%20to%20enrich%20the%20scenario%0Adiversity%2C%20a%20Gaussian-distributional%20cost%20function%20module%20is%20designed%20to%0Acapture%20the%20uncertainty%20of%20the%20VUT%27s%20strategy%2C%20triggering%20various%20scenario%0Aevolution.%20Experimental%20results%20validate%20VCDI%27s%20trajectory-level%20simulation%0Aprecision%20which%20outperforms%20the%20state-of-the-art%20trajectory%20prediction%20work.%0AThe%20flexibility%20of%20the%20distributional%20cost%20function%20allows%20VCDI%20to%20provide%0Adiverse-yet-realistic%20scenarios%20for%20AV%20testing.%20We%20demonstrate%20such%20capability%0Aby%20modifying%20the%20anticipation%20to%20the%20VUT%27s%20cost-based%20strategy%20and%20thus%20achieve%0Amultiple%20testing%20scenarios%20with%20explainable%20background%20traffic%20evolution.%20Codes%0Aare%20available%20at%20https%3A//github.com/YNYSNL/VCDI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interactive%2520Autonomous%2520Vehicle%2520Testing%253A%250A%2520%2520Vehicle-Under-Test-Centered%2520Traffic%2520Simulation%26entry.906535625%3DYiru%2520Liu%2520and%2520Xiaocong%2520Zhao%2520and%2520Jian%2520Sun%26entry.1292438233%3D%2520%2520The%2520simulation-based%2520testing%2520is%2520essential%2520for%2520safely%2520implementing%2520autonomous%250Avehicles%2520%2528AV%2529%2520on%2520roads%252C%2520necessitating%2520simulated%2520traffic%2520environments%2520that%250Adynamically%2520interact%2520with%2520the%2520Vehicle%2520Under%2520Test%2520%2528VUT%2529.%2520This%2520study%2520introduces%2520a%250AVUT-Centered%2520environmental%2520Dynamics%2520Inference%2520%2528VCDI%2529%2520model%2520for%2520realistic%252C%250Ainteractive%252C%2520and%2520diverse%2520background%2520traffic%2520simulation.%2520Serving%2520the%2520purpose%2520of%250AAV%2520testing%252C%2520VCDI%2520employs%2520Transformer-based%2520modules%2520in%2520a%2520conditional%2520trajectory%250Ainference%2520framework%2520to%2520simulate%2520VUT-centered%2520driving%2520interaction%2520events.%2520First%252C%250Athe%2520VUT%2520future%2520motion%2520is%2520taken%2520as%2520an%2520augmented%2520model%2520input%2520to%2520bridge%2520the%2520action%250Adependence%2520between%2520VUT%2520and%2520background%2520objects.%2520Second%252C%2520to%2520enrich%2520the%2520scenario%250Adiversity%252C%2520a%2520Gaussian-distributional%2520cost%2520function%2520module%2520is%2520designed%2520to%250Acapture%2520the%2520uncertainty%2520of%2520the%2520VUT%2527s%2520strategy%252C%2520triggering%2520various%2520scenario%250Aevolution.%2520Experimental%2520results%2520validate%2520VCDI%2527s%2520trajectory-level%2520simulation%250Aprecision%2520which%2520outperforms%2520the%2520state-of-the-art%2520trajectory%2520prediction%2520work.%250AThe%2520flexibility%2520of%2520the%2520distributional%2520cost%2520function%2520allows%2520VCDI%2520to%2520provide%250Adiverse-yet-realistic%2520scenarios%2520for%2520AV%2520testing.%2520We%2520demonstrate%2520such%2520capability%250Aby%2520modifying%2520the%2520anticipation%2520to%2520the%2520VUT%2527s%2520cost-based%2520strategy%2520and%2520thus%2520achieve%250Amultiple%2520testing%2520scenarios%2520with%2520explainable%2520background%2520traffic%2520evolution.%2520Codes%250Aare%2520available%2520at%2520https%253A//github.com/YNYSNL/VCDI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interactive%20Autonomous%20Vehicle%20Testing%3A%0A%20%20Vehicle-Under-Test-Centered%20Traffic%20Simulation&entry.906535625=Yiru%20Liu%20and%20Xiaocong%20Zhao%20and%20Jian%20Sun&entry.1292438233=%20%20The%20simulation-based%20testing%20is%20essential%20for%20safely%20implementing%20autonomous%0Avehicles%20%28AV%29%20on%20roads%2C%20necessitating%20simulated%20traffic%20environments%20that%0Adynamically%20interact%20with%20the%20Vehicle%20Under%20Test%20%28VUT%29.%20This%20study%20introduces%20a%0AVUT-Centered%20environmental%20Dynamics%20Inference%20%28VCDI%29%20model%20for%20realistic%2C%0Ainteractive%2C%20and%20diverse%20background%20traffic%20simulation.%20Serving%20the%20purpose%20of%0AAV%20testing%2C%20VCDI%20employs%20Transformer-based%20modules%20in%20a%20conditional%20trajectory%0Ainference%20framework%20to%20simulate%20VUT-centered%20driving%20interaction%20events.%20First%2C%0Athe%20VUT%20future%20motion%20is%20taken%20as%20an%20augmented%20model%20input%20to%20bridge%20the%20action%0Adependence%20between%20VUT%20and%20background%20objects.%20Second%2C%20to%20enrich%20the%20scenario%0Adiversity%2C%20a%20Gaussian-distributional%20cost%20function%20module%20is%20designed%20to%0Acapture%20the%20uncertainty%20of%20the%20VUT%27s%20strategy%2C%20triggering%20various%20scenario%0Aevolution.%20Experimental%20results%20validate%20VCDI%27s%20trajectory-level%20simulation%0Aprecision%20which%20outperforms%20the%20state-of-the-art%20trajectory%20prediction%20work.%0AThe%20flexibility%20of%20the%20distributional%20cost%20function%20allows%20VCDI%20to%20provide%0Adiverse-yet-realistic%20scenarios%20for%20AV%20testing.%20We%20demonstrate%20such%20capability%0Aby%20modifying%20the%20anticipation%20to%20the%20VUT%27s%20cost-based%20strategy%20and%20thus%20achieve%0Amultiple%20testing%20scenarios%20with%20explainable%20background%20traffic%20evolution.%20Codes%0Aare%20available%20at%20https%3A//github.com/YNYSNL/VCDI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02860v2&entry.124074799=Read"},
{"title": "Wolf: Captioning Everything with a World Summarization Framework", "author": "Boyi Li and Ligeng Zhu and Ran Tian and Shuhan Tan and Yuxiao Chen and Yao Lu and Yin Cui and Sushant Veer and Max Ehrlich and Jonah Philion and Xinshuo Weng and Fuzhao Xue and Andrew Tao and Ming-Yu Liu and Sanja Fidler and Boris Ivanovic and Trevor Darrell and Jitendra Malik and Song Han and Marco Pavone", "abstract": "  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.\n", "link": "http://arxiv.org/abs/2407.18908v1", "date": "2024-07-26", "relevancy": 2.0364, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5198}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wolf%3A%20Captioning%20Everything%20with%20a%20World%20Summarization%20Framework&body=Title%3A%20Wolf%3A%20Captioning%20Everything%20with%20a%20World%20Summarization%20Framework%0AAuthor%3A%20Boyi%20Li%20and%20Ligeng%20Zhu%20and%20Ran%20Tian%20and%20Shuhan%20Tan%20and%20Yuxiao%20Chen%20and%20Yao%20Lu%20and%20Yin%20Cui%20and%20Sushant%20Veer%20and%20Max%20Ehrlich%20and%20Jonah%20Philion%20and%20Xinshuo%20Weng%20and%20Fuzhao%20Xue%20and%20Andrew%20Tao%20and%20Ming-Yu%20Liu%20and%20Sanja%20Fidler%20and%20Boris%20Ivanovic%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Song%20Han%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20We%20propose%20Wolf%2C%20a%20WOrLd%20summarization%20Framework%20for%20accurate%20video%0Acaptioning.%20Wolf%20is%20an%20automated%20captioning%20framework%20that%20adopts%20a%0Amixture-of-experts%20approach%2C%20leveraging%20complementary%20strengths%20of%20Vision%0ALanguage%20Models%20%28VLMs%29.%20By%20utilizing%20both%20image%20and%20video%20models%2C%20our%20framework%0Acaptures%20different%20levels%20of%20information%20and%20summarizes%20them%20efficiently.%20Our%0Aapproach%20can%20be%20applied%20to%20enhance%20video%20understanding%2C%20auto-labeling%2C%20and%0Acaptioning.%20To%20evaluate%20caption%20quality%2C%20we%20introduce%20CapScore%2C%20an%20LLM-based%0Ametric%20to%20assess%20the%20similarity%20and%20quality%20of%20generated%20captions%20compared%20to%0Athe%20ground%20truth%20captions.%20We%20further%20build%20four%20human-annotated%20datasets%20in%0Athree%20domains%3A%20autonomous%20driving%2C%20general%20scenes%2C%20and%20robotics%2C%20to%20facilitate%0Acomprehensive%20comparisons.%20We%20show%20that%20Wolf%20achieves%20superior%20captioning%0Aperformance%20compared%20to%20state-of-the-art%20approaches%20from%20the%20research%20community%0A%28VILA1.5%2C%20CogAgent%29%20and%20commercial%20solutions%20%28Gemini-Pro-1.5%2C%20GPT-4V%29.%20For%0Ainstance%2C%20in%20comparison%20with%20GPT-4V%2C%20Wolf%20improves%20CapScore%20both%20quality-wise%0Aby%2055.6%25%20and%20similarity-wise%20by%2077.4%25%20on%20challenging%20driving%20videos.%20Finally%2C%0Awe%20establish%20a%20benchmark%20for%20video%20captioning%20and%20introduce%20a%20leaderboard%2C%0Aaiming%20to%20accelerate%20advancements%20in%20video%20understanding%2C%20captioning%2C%20and%20data%0Aalignment.%20Leaderboard%3A%20https%3A//wolfv0.github.io/leaderboard.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWolf%253A%2520Captioning%2520Everything%2520with%2520a%2520World%2520Summarization%2520Framework%26entry.906535625%3DBoyi%2520Li%2520and%2520Ligeng%2520Zhu%2520and%2520Ran%2520Tian%2520and%2520Shuhan%2520Tan%2520and%2520Yuxiao%2520Chen%2520and%2520Yao%2520Lu%2520and%2520Yin%2520Cui%2520and%2520Sushant%2520Veer%2520and%2520Max%2520Ehrlich%2520and%2520Jonah%2520Philion%2520and%2520Xinshuo%2520Weng%2520and%2520Fuzhao%2520Xue%2520and%2520Andrew%2520Tao%2520and%2520Ming-Yu%2520Liu%2520and%2520Sanja%2520Fidler%2520and%2520Boris%2520Ivanovic%2520and%2520Trevor%2520Darrell%2520and%2520Jitendra%2520Malik%2520and%2520Song%2520Han%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520We%2520propose%2520Wolf%252C%2520a%2520WOrLd%2520summarization%2520Framework%2520for%2520accurate%2520video%250Acaptioning.%2520Wolf%2520is%2520an%2520automated%2520captioning%2520framework%2520that%2520adopts%2520a%250Amixture-of-experts%2520approach%252C%2520leveraging%2520complementary%2520strengths%2520of%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529.%2520By%2520utilizing%2520both%2520image%2520and%2520video%2520models%252C%2520our%2520framework%250Acaptures%2520different%2520levels%2520of%2520information%2520and%2520summarizes%2520them%2520efficiently.%2520Our%250Aapproach%2520can%2520be%2520applied%2520to%2520enhance%2520video%2520understanding%252C%2520auto-labeling%252C%2520and%250Acaptioning.%2520To%2520evaluate%2520caption%2520quality%252C%2520we%2520introduce%2520CapScore%252C%2520an%2520LLM-based%250Ametric%2520to%2520assess%2520the%2520similarity%2520and%2520quality%2520of%2520generated%2520captions%2520compared%2520to%250Athe%2520ground%2520truth%2520captions.%2520We%2520further%2520build%2520four%2520human-annotated%2520datasets%2520in%250Athree%2520domains%253A%2520autonomous%2520driving%252C%2520general%2520scenes%252C%2520and%2520robotics%252C%2520to%2520facilitate%250Acomprehensive%2520comparisons.%2520We%2520show%2520that%2520Wolf%2520achieves%2520superior%2520captioning%250Aperformance%2520compared%2520to%2520state-of-the-art%2520approaches%2520from%2520the%2520research%2520community%250A%2528VILA1.5%252C%2520CogAgent%2529%2520and%2520commercial%2520solutions%2520%2528Gemini-Pro-1.5%252C%2520GPT-4V%2529.%2520For%250Ainstance%252C%2520in%2520comparison%2520with%2520GPT-4V%252C%2520Wolf%2520improves%2520CapScore%2520both%2520quality-wise%250Aby%252055.6%2525%2520and%2520similarity-wise%2520by%252077.4%2525%2520on%2520challenging%2520driving%2520videos.%2520Finally%252C%250Awe%2520establish%2520a%2520benchmark%2520for%2520video%2520captioning%2520and%2520introduce%2520a%2520leaderboard%252C%250Aaiming%2520to%2520accelerate%2520advancements%2520in%2520video%2520understanding%252C%2520captioning%252C%2520and%2520data%250Aalignment.%2520Leaderboard%253A%2520https%253A//wolfv0.github.io/leaderboard.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wolf%3A%20Captioning%20Everything%20with%20a%20World%20Summarization%20Framework&entry.906535625=Boyi%20Li%20and%20Ligeng%20Zhu%20and%20Ran%20Tian%20and%20Shuhan%20Tan%20and%20Yuxiao%20Chen%20and%20Yao%20Lu%20and%20Yin%20Cui%20and%20Sushant%20Veer%20and%20Max%20Ehrlich%20and%20Jonah%20Philion%20and%20Xinshuo%20Weng%20and%20Fuzhao%20Xue%20and%20Andrew%20Tao%20and%20Ming-Yu%20Liu%20and%20Sanja%20Fidler%20and%20Boris%20Ivanovic%20and%20Trevor%20Darrell%20and%20Jitendra%20Malik%20and%20Song%20Han%20and%20Marco%20Pavone&entry.1292438233=%20%20We%20propose%20Wolf%2C%20a%20WOrLd%20summarization%20Framework%20for%20accurate%20video%0Acaptioning.%20Wolf%20is%20an%20automated%20captioning%20framework%20that%20adopts%20a%0Amixture-of-experts%20approach%2C%20leveraging%20complementary%20strengths%20of%20Vision%0ALanguage%20Models%20%28VLMs%29.%20By%20utilizing%20both%20image%20and%20video%20models%2C%20our%20framework%0Acaptures%20different%20levels%20of%20information%20and%20summarizes%20them%20efficiently.%20Our%0Aapproach%20can%20be%20applied%20to%20enhance%20video%20understanding%2C%20auto-labeling%2C%20and%0Acaptioning.%20To%20evaluate%20caption%20quality%2C%20we%20introduce%20CapScore%2C%20an%20LLM-based%0Ametric%20to%20assess%20the%20similarity%20and%20quality%20of%20generated%20captions%20compared%20to%0Athe%20ground%20truth%20captions.%20We%20further%20build%20four%20human-annotated%20datasets%20in%0Athree%20domains%3A%20autonomous%20driving%2C%20general%20scenes%2C%20and%20robotics%2C%20to%20facilitate%0Acomprehensive%20comparisons.%20We%20show%20that%20Wolf%20achieves%20superior%20captioning%0Aperformance%20compared%20to%20state-of-the-art%20approaches%20from%20the%20research%20community%0A%28VILA1.5%2C%20CogAgent%29%20and%20commercial%20solutions%20%28Gemini-Pro-1.5%2C%20GPT-4V%29.%20For%0Ainstance%2C%20in%20comparison%20with%20GPT-4V%2C%20Wolf%20improves%20CapScore%20both%20quality-wise%0Aby%2055.6%25%20and%20similarity-wise%20by%2077.4%25%20on%20challenging%20driving%20videos.%20Finally%2C%0Awe%20establish%20a%20benchmark%20for%20video%20captioning%20and%20introduce%20a%20leaderboard%2C%0Aaiming%20to%20accelerate%20advancements%20in%20video%20understanding%2C%20captioning%2C%20and%20data%0Aalignment.%20Leaderboard%3A%20https%3A//wolfv0.github.io/leaderboard.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18908v1&entry.124074799=Read"},
{"title": "Differentiable Robust Model Predictive Control", "author": "Alex Oshin and Hassan Almubarak and Evangelos A. Theodorou", "abstract": "  Deterministic model predictive control (MPC), while powerful, is often\ninsufficient for effectively controlling autonomous systems in the real-world.\nFactors such as environmental noise and model error can cause deviations from\nthe expected nominal performance. Robust MPC algorithms aim to bridge this gap\nbetween deterministic and uncertain control. However, these methods are often\nexcessively difficult to tune for robustness due to the nonlinear and\nnon-intuitive effects that controller parameters have on performance. To\naddress this challenge, we first present a unifying perspective on\ndifferentiable optimization for control using the implicit function theorem\n(IFT), from which existing state-of-the art methods can be derived. Drawing\nparallels with differential dynamic programming, the IFT enables the derivation\nof an efficient differentiable optimal control framework. The derived scheme is\nsubsequently paired with a tube-based MPC architecture to facilitate the\nautomatic and real-time tuning of robust controllers in the presence of large\nuncertainties and disturbances. The proposed algorithm is benchmarked on\nmultiple nonlinear robotic systems, including two systems in the MuJoCo\nsimulator environment and one hardware experiment on the Robotarium testbed, to\ndemonstrate its efficacy.\n", "link": "http://arxiv.org/abs/2308.08426v3", "date": "2024-07-26", "relevancy": 2.0309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.562}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Robust%20Model%20Predictive%20Control&body=Title%3A%20Differentiable%20Robust%20Model%20Predictive%20Control%0AAuthor%3A%20Alex%20Oshin%20and%20Hassan%20Almubarak%20and%20Evangelos%20A.%20Theodorou%0AAbstract%3A%20%20%20Deterministic%20model%20predictive%20control%20%28MPC%29%2C%20while%20powerful%2C%20is%20often%0Ainsufficient%20for%20effectively%20controlling%20autonomous%20systems%20in%20the%20real-world.%0AFactors%20such%20as%20environmental%20noise%20and%20model%20error%20can%20cause%20deviations%20from%0Athe%20expected%20nominal%20performance.%20Robust%20MPC%20algorithms%20aim%20to%20bridge%20this%20gap%0Abetween%20deterministic%20and%20uncertain%20control.%20However%2C%20these%20methods%20are%20often%0Aexcessively%20difficult%20to%20tune%20for%20robustness%20due%20to%20the%20nonlinear%20and%0Anon-intuitive%20effects%20that%20controller%20parameters%20have%20on%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20first%20present%20a%20unifying%20perspective%20on%0Adifferentiable%20optimization%20for%20control%20using%20the%20implicit%20function%20theorem%0A%28IFT%29%2C%20from%20which%20existing%20state-of-the%20art%20methods%20can%20be%20derived.%20Drawing%0Aparallels%20with%20differential%20dynamic%20programming%2C%20the%20IFT%20enables%20the%20derivation%0Aof%20an%20efficient%20differentiable%20optimal%20control%20framework.%20The%20derived%20scheme%20is%0Asubsequently%20paired%20with%20a%20tube-based%20MPC%20architecture%20to%20facilitate%20the%0Aautomatic%20and%20real-time%20tuning%20of%20robust%20controllers%20in%20the%20presence%20of%20large%0Auncertainties%20and%20disturbances.%20The%20proposed%20algorithm%20is%20benchmarked%20on%0Amultiple%20nonlinear%20robotic%20systems%2C%20including%20two%20systems%20in%20the%20MuJoCo%0Asimulator%20environment%20and%20one%20hardware%20experiment%20on%20the%20Robotarium%20testbed%2C%20to%0Ademonstrate%20its%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08426v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Robust%2520Model%2520Predictive%2520Control%26entry.906535625%3DAlex%2520Oshin%2520and%2520Hassan%2520Almubarak%2520and%2520Evangelos%2520A.%2520Theodorou%26entry.1292438233%3D%2520%2520Deterministic%2520model%2520predictive%2520control%2520%2528MPC%2529%252C%2520while%2520powerful%252C%2520is%2520often%250Ainsufficient%2520for%2520effectively%2520controlling%2520autonomous%2520systems%2520in%2520the%2520real-world.%250AFactors%2520such%2520as%2520environmental%2520noise%2520and%2520model%2520error%2520can%2520cause%2520deviations%2520from%250Athe%2520expected%2520nominal%2520performance.%2520Robust%2520MPC%2520algorithms%2520aim%2520to%2520bridge%2520this%2520gap%250Abetween%2520deterministic%2520and%2520uncertain%2520control.%2520However%252C%2520these%2520methods%2520are%2520often%250Aexcessively%2520difficult%2520to%2520tune%2520for%2520robustness%2520due%2520to%2520the%2520nonlinear%2520and%250Anon-intuitive%2520effects%2520that%2520controller%2520parameters%2520have%2520on%2520performance.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520first%2520present%2520a%2520unifying%2520perspective%2520on%250Adifferentiable%2520optimization%2520for%2520control%2520using%2520the%2520implicit%2520function%2520theorem%250A%2528IFT%2529%252C%2520from%2520which%2520existing%2520state-of-the%2520art%2520methods%2520can%2520be%2520derived.%2520Drawing%250Aparallels%2520with%2520differential%2520dynamic%2520programming%252C%2520the%2520IFT%2520enables%2520the%2520derivation%250Aof%2520an%2520efficient%2520differentiable%2520optimal%2520control%2520framework.%2520The%2520derived%2520scheme%2520is%250Asubsequently%2520paired%2520with%2520a%2520tube-based%2520MPC%2520architecture%2520to%2520facilitate%2520the%250Aautomatic%2520and%2520real-time%2520tuning%2520of%2520robust%2520controllers%2520in%2520the%2520presence%2520of%2520large%250Auncertainties%2520and%2520disturbances.%2520The%2520proposed%2520algorithm%2520is%2520benchmarked%2520on%250Amultiple%2520nonlinear%2520robotic%2520systems%252C%2520including%2520two%2520systems%2520in%2520the%2520MuJoCo%250Asimulator%2520environment%2520and%2520one%2520hardware%2520experiment%2520on%2520the%2520Robotarium%2520testbed%252C%2520to%250Ademonstrate%2520its%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08426v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Robust%20Model%20Predictive%20Control&entry.906535625=Alex%20Oshin%20and%20Hassan%20Almubarak%20and%20Evangelos%20A.%20Theodorou&entry.1292438233=%20%20Deterministic%20model%20predictive%20control%20%28MPC%29%2C%20while%20powerful%2C%20is%20often%0Ainsufficient%20for%20effectively%20controlling%20autonomous%20systems%20in%20the%20real-world.%0AFactors%20such%20as%20environmental%20noise%20and%20model%20error%20can%20cause%20deviations%20from%0Athe%20expected%20nominal%20performance.%20Robust%20MPC%20algorithms%20aim%20to%20bridge%20this%20gap%0Abetween%20deterministic%20and%20uncertain%20control.%20However%2C%20these%20methods%20are%20often%0Aexcessively%20difficult%20to%20tune%20for%20robustness%20due%20to%20the%20nonlinear%20and%0Anon-intuitive%20effects%20that%20controller%20parameters%20have%20on%20performance.%20To%0Aaddress%20this%20challenge%2C%20we%20first%20present%20a%20unifying%20perspective%20on%0Adifferentiable%20optimization%20for%20control%20using%20the%20implicit%20function%20theorem%0A%28IFT%29%2C%20from%20which%20existing%20state-of-the%20art%20methods%20can%20be%20derived.%20Drawing%0Aparallels%20with%20differential%20dynamic%20programming%2C%20the%20IFT%20enables%20the%20derivation%0Aof%20an%20efficient%20differentiable%20optimal%20control%20framework.%20The%20derived%20scheme%20is%0Asubsequently%20paired%20with%20a%20tube-based%20MPC%20architecture%20to%20facilitate%20the%0Aautomatic%20and%20real-time%20tuning%20of%20robust%20controllers%20in%20the%20presence%20of%20large%0Auncertainties%20and%20disturbances.%20The%20proposed%20algorithm%20is%20benchmarked%20on%0Amultiple%20nonlinear%20robotic%20systems%2C%20including%20two%20systems%20in%20the%20MuJoCo%0Asimulator%20environment%20and%20one%20hardware%20experiment%20on%20the%20Robotarium%20testbed%2C%20to%0Ademonstrate%20its%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08426v3&entry.124074799=Read"},
{"title": "Local Binary Pattern(LBP) Optimization for Feature Extraction", "author": "Zeinab Sedaghatjoo and Hossein Hosseinzadeh and Bahram Sadeghi Bigham", "abstract": "  The rapid growth of image data has led to the development of advanced image\nprocessing and computer vision techniques, which are crucial in various\napplications such as image classification, image segmentation, and pattern\nrecognition. Texture is an important feature that has been widely used in many\nimage processing tasks. Therefore, analyzing and understanding texture plays a\npivotal role in image analysis and understanding.Local binary pattern (LBP) is\na powerful operator that describes the local texture features of images. This\npaper provides a novel mathematical representation of the LBP by separating the\noperator into three matrices, two of which are always fixed and do not depend\non the input data. These fixed matrices are analyzed in depth, and a new\nalgorithm is proposed to optimize them for improved classification performance.\nThe optimization process is based on the singular value decomposition (SVD)\nalgorithm. As a result, the authors present optimal LBPs that effectively\ndescribe the texture of human face images. Several experiment results presented\nin this paper convincingly verify the efficiency and superiority of the\noptimized LBPs for face detection and facial expression recognition tasks.\n", "link": "http://arxiv.org/abs/2407.18665v1", "date": "2024-07-26", "relevancy": 2.0277, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5114}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5111}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Binary%20Pattern%28LBP%29%20Optimization%20for%20Feature%20Extraction&body=Title%3A%20Local%20Binary%20Pattern%28LBP%29%20Optimization%20for%20Feature%20Extraction%0AAuthor%3A%20Zeinab%20Sedaghatjoo%20and%20Hossein%20Hosseinzadeh%20and%20Bahram%20Sadeghi%20Bigham%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20image%20data%20has%20led%20to%20the%20development%20of%20advanced%20image%0Aprocessing%20and%20computer%20vision%20techniques%2C%20which%20are%20crucial%20in%20various%0Aapplications%20such%20as%20image%20classification%2C%20image%20segmentation%2C%20and%20pattern%0Arecognition.%20Texture%20is%20an%20important%20feature%20that%20has%20been%20widely%20used%20in%20many%0Aimage%20processing%20tasks.%20Therefore%2C%20analyzing%20and%20understanding%20texture%20plays%20a%0Apivotal%20role%20in%20image%20analysis%20and%20understanding.Local%20binary%20pattern%20%28LBP%29%20is%0Aa%20powerful%20operator%20that%20describes%20the%20local%20texture%20features%20of%20images.%20This%0Apaper%20provides%20a%20novel%20mathematical%20representation%20of%20the%20LBP%20by%20separating%20the%0Aoperator%20into%20three%20matrices%2C%20two%20of%20which%20are%20always%20fixed%20and%20do%20not%20depend%0Aon%20the%20input%20data.%20These%20fixed%20matrices%20are%20analyzed%20in%20depth%2C%20and%20a%20new%0Aalgorithm%20is%20proposed%20to%20optimize%20them%20for%20improved%20classification%20performance.%0AThe%20optimization%20process%20is%20based%20on%20the%20singular%20value%20decomposition%20%28SVD%29%0Aalgorithm.%20As%20a%20result%2C%20the%20authors%20present%20optimal%20LBPs%20that%20effectively%0Adescribe%20the%20texture%20of%20human%20face%20images.%20Several%20experiment%20results%20presented%0Ain%20this%20paper%20convincingly%20verify%20the%20efficiency%20and%20superiority%20of%20the%0Aoptimized%20LBPs%20for%20face%20detection%20and%20facial%20expression%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Binary%2520Pattern%2528LBP%2529%2520Optimization%2520for%2520Feature%2520Extraction%26entry.906535625%3DZeinab%2520Sedaghatjoo%2520and%2520Hossein%2520Hosseinzadeh%2520and%2520Bahram%2520Sadeghi%2520Bigham%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520image%2520data%2520has%2520led%2520to%2520the%2520development%2520of%2520advanced%2520image%250Aprocessing%2520and%2520computer%2520vision%2520techniques%252C%2520which%2520are%2520crucial%2520in%2520various%250Aapplications%2520such%2520as%2520image%2520classification%252C%2520image%2520segmentation%252C%2520and%2520pattern%250Arecognition.%2520Texture%2520is%2520an%2520important%2520feature%2520that%2520has%2520been%2520widely%2520used%2520in%2520many%250Aimage%2520processing%2520tasks.%2520Therefore%252C%2520analyzing%2520and%2520understanding%2520texture%2520plays%2520a%250Apivotal%2520role%2520in%2520image%2520analysis%2520and%2520understanding.Local%2520binary%2520pattern%2520%2528LBP%2529%2520is%250Aa%2520powerful%2520operator%2520that%2520describes%2520the%2520local%2520texture%2520features%2520of%2520images.%2520This%250Apaper%2520provides%2520a%2520novel%2520mathematical%2520representation%2520of%2520the%2520LBP%2520by%2520separating%2520the%250Aoperator%2520into%2520three%2520matrices%252C%2520two%2520of%2520which%2520are%2520always%2520fixed%2520and%2520do%2520not%2520depend%250Aon%2520the%2520input%2520data.%2520These%2520fixed%2520matrices%2520are%2520analyzed%2520in%2520depth%252C%2520and%2520a%2520new%250Aalgorithm%2520is%2520proposed%2520to%2520optimize%2520them%2520for%2520improved%2520classification%2520performance.%250AThe%2520optimization%2520process%2520is%2520based%2520on%2520the%2520singular%2520value%2520decomposition%2520%2528SVD%2529%250Aalgorithm.%2520As%2520a%2520result%252C%2520the%2520authors%2520present%2520optimal%2520LBPs%2520that%2520effectively%250Adescribe%2520the%2520texture%2520of%2520human%2520face%2520images.%2520Several%2520experiment%2520results%2520presented%250Ain%2520this%2520paper%2520convincingly%2520verify%2520the%2520efficiency%2520and%2520superiority%2520of%2520the%250Aoptimized%2520LBPs%2520for%2520face%2520detection%2520and%2520facial%2520expression%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Binary%20Pattern%28LBP%29%20Optimization%20for%20Feature%20Extraction&entry.906535625=Zeinab%20Sedaghatjoo%20and%20Hossein%20Hosseinzadeh%20and%20Bahram%20Sadeghi%20Bigham&entry.1292438233=%20%20The%20rapid%20growth%20of%20image%20data%20has%20led%20to%20the%20development%20of%20advanced%20image%0Aprocessing%20and%20computer%20vision%20techniques%2C%20which%20are%20crucial%20in%20various%0Aapplications%20such%20as%20image%20classification%2C%20image%20segmentation%2C%20and%20pattern%0Arecognition.%20Texture%20is%20an%20important%20feature%20that%20has%20been%20widely%20used%20in%20many%0Aimage%20processing%20tasks.%20Therefore%2C%20analyzing%20and%20understanding%20texture%20plays%20a%0Apivotal%20role%20in%20image%20analysis%20and%20understanding.Local%20binary%20pattern%20%28LBP%29%20is%0Aa%20powerful%20operator%20that%20describes%20the%20local%20texture%20features%20of%20images.%20This%0Apaper%20provides%20a%20novel%20mathematical%20representation%20of%20the%20LBP%20by%20separating%20the%0Aoperator%20into%20three%20matrices%2C%20two%20of%20which%20are%20always%20fixed%20and%20do%20not%20depend%0Aon%20the%20input%20data.%20These%20fixed%20matrices%20are%20analyzed%20in%20depth%2C%20and%20a%20new%0Aalgorithm%20is%20proposed%20to%20optimize%20them%20for%20improved%20classification%20performance.%0AThe%20optimization%20process%20is%20based%20on%20the%20singular%20value%20decomposition%20%28SVD%29%0Aalgorithm.%20As%20a%20result%2C%20the%20authors%20present%20optimal%20LBPs%20that%20effectively%0Adescribe%20the%20texture%20of%20human%20face%20images.%20Several%20experiment%20results%20presented%0Ain%20this%20paper%20convincingly%20verify%20the%20efficiency%20and%20superiority%20of%20the%0Aoptimized%20LBPs%20for%20face%20detection%20and%20facial%20expression%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18665v1&entry.124074799=Read"},
{"title": "AppWorld: A Controllable World of Apps and People for Benchmarking\n  Interactive Coding Agents", "author": "Harsh Trivedi and Tushar Khot and Mareike Hartmann and Ruskin Manku and Vinty Dong and Edward Li and Shashank Gupta and Ashish Sabharwal and Niranjan Balasubramanian", "abstract": "  Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.\n", "link": "http://arxiv.org/abs/2407.18901v1", "date": "2024-07-26", "relevancy": 2.0254, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5321}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5311}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AppWorld%3A%20A%20Controllable%20World%20of%20Apps%20and%20People%20for%20Benchmarking%0A%20%20Interactive%20Coding%20Agents&body=Title%3A%20AppWorld%3A%20A%20Controllable%20World%20of%20Apps%20and%20People%20for%20Benchmarking%0A%20%20Interactive%20Coding%20Agents%0AAuthor%3A%20Harsh%20Trivedi%20and%20Tushar%20Khot%20and%20Mareike%20Hartmann%20and%20Ruskin%20Manku%20and%20Vinty%20Dong%20and%20Edward%20Li%20and%20Shashank%20Gupta%20and%20Ashish%20Sabharwal%20and%20Niranjan%20Balasubramanian%0AAbstract%3A%20%20%20Autonomous%20agents%20that%20address%20day-to-day%20digital%20tasks%20%28e.g.%2C%20ordering%0Agroceries%20for%20a%20household%29%2C%20must%20not%20only%20operate%20multiple%20apps%20%28e.g.%2C%20notes%2C%0Amessaging%2C%20shopping%20app%29%20via%20APIs%2C%20but%20also%20generate%20rich%20code%20with%20complex%0Acontrol%20flow%20in%20an%20iterative%20manner%20based%20on%20their%20interaction%20with%20the%0Aenvironment.%20However%2C%20existing%20benchmarks%20for%20tool%20use%20are%20inadequate%2C%20as%20they%0Aonly%20cover%20tasks%20that%20require%20a%20simple%20sequence%20of%20API%20calls.%0A%20%20To%20remedy%20this%20gap%2C%20we%20built%20%24%5Ctextbf%7BAppWorld%20Engine%7D%24%2C%20a%20high-quality%0Aexecution%20environment%20%2860K%20lines%20of%20code%29%20of%209%20day-to-day%20apps%20operable%20via%20457%0AAPIs%20and%20populated%20with%20realistic%20digital%20activities%20simulating%20the%20lives%20of%0A~100%20fictitious%20users.%20We%20then%20created%20%24%5Ctextbf%7BAppWorld%20Benchmark%7D%24%20%2840K%20lines%0Aof%20code%29%2C%20a%20suite%20of%20750%20natural%2C%20diverse%2C%20and%20challenging%20autonomous%20agent%0Atasks%20requiring%20rich%20and%20interactive%20code%20generation.%20It%20supports%20robust%0Aprogrammatic%20evaluation%20with%20state-based%20unit%20tests%2C%20allowing%20for%20different%0Aways%20of%20completing%20a%20task%20while%20also%20checking%20for%20unexpected%20changes%2C%20i.e.%2C%0Acollateral%20damage.%20The%20state-of-the-art%20LLM%2C%20GPT-4o%2C%20solves%20only%20~49%25%20of%20our%0A%27normal%27%20tasks%20and%20~30%25%20of%20%27challenge%27%20tasks%2C%20while%20other%20models%20solve%20at%20least%0A16%25%20fewer.%20This%20highlights%20the%20benchmark%27s%20difficulty%20and%20AppWorld%27s%20potential%0Ato%20push%20the%20frontiers%20of%20interactive%20coding%20agents.%20The%20project%20website%20is%0Aavailable%20at%20https%3A//appworld.dev/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAppWorld%253A%2520A%2520Controllable%2520World%2520of%2520Apps%2520and%2520People%2520for%2520Benchmarking%250A%2520%2520Interactive%2520Coding%2520Agents%26entry.906535625%3DHarsh%2520Trivedi%2520and%2520Tushar%2520Khot%2520and%2520Mareike%2520Hartmann%2520and%2520Ruskin%2520Manku%2520and%2520Vinty%2520Dong%2520and%2520Edward%2520Li%2520and%2520Shashank%2520Gupta%2520and%2520Ashish%2520Sabharwal%2520and%2520Niranjan%2520Balasubramanian%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520that%2520address%2520day-to-day%2520digital%2520tasks%2520%2528e.g.%252C%2520ordering%250Agroceries%2520for%2520a%2520household%2529%252C%2520must%2520not%2520only%2520operate%2520multiple%2520apps%2520%2528e.g.%252C%2520notes%252C%250Amessaging%252C%2520shopping%2520app%2529%2520via%2520APIs%252C%2520but%2520also%2520generate%2520rich%2520code%2520with%2520complex%250Acontrol%2520flow%2520in%2520an%2520iterative%2520manner%2520based%2520on%2520their%2520interaction%2520with%2520the%250Aenvironment.%2520However%252C%2520existing%2520benchmarks%2520for%2520tool%2520use%2520are%2520inadequate%252C%2520as%2520they%250Aonly%2520cover%2520tasks%2520that%2520require%2520a%2520simple%2520sequence%2520of%2520API%2520calls.%250A%2520%2520To%2520remedy%2520this%2520gap%252C%2520we%2520built%2520%2524%255Ctextbf%257BAppWorld%2520Engine%257D%2524%252C%2520a%2520high-quality%250Aexecution%2520environment%2520%252860K%2520lines%2520of%2520code%2529%2520of%25209%2520day-to-day%2520apps%2520operable%2520via%2520457%250AAPIs%2520and%2520populated%2520with%2520realistic%2520digital%2520activities%2520simulating%2520the%2520lives%2520of%250A~100%2520fictitious%2520users.%2520We%2520then%2520created%2520%2524%255Ctextbf%257BAppWorld%2520Benchmark%257D%2524%2520%252840K%2520lines%250Aof%2520code%2529%252C%2520a%2520suite%2520of%2520750%2520natural%252C%2520diverse%252C%2520and%2520challenging%2520autonomous%2520agent%250Atasks%2520requiring%2520rich%2520and%2520interactive%2520code%2520generation.%2520It%2520supports%2520robust%250Aprogrammatic%2520evaluation%2520with%2520state-based%2520unit%2520tests%252C%2520allowing%2520for%2520different%250Aways%2520of%2520completing%2520a%2520task%2520while%2520also%2520checking%2520for%2520unexpected%2520changes%252C%2520i.e.%252C%250Acollateral%2520damage.%2520The%2520state-of-the-art%2520LLM%252C%2520GPT-4o%252C%2520solves%2520only%2520~49%2525%2520of%2520our%250A%2527normal%2527%2520tasks%2520and%2520~30%2525%2520of%2520%2527challenge%2527%2520tasks%252C%2520while%2520other%2520models%2520solve%2520at%2520least%250A16%2525%2520fewer.%2520This%2520highlights%2520the%2520benchmark%2527s%2520difficulty%2520and%2520AppWorld%2527s%2520potential%250Ato%2520push%2520the%2520frontiers%2520of%2520interactive%2520coding%2520agents.%2520The%2520project%2520website%2520is%250Aavailable%2520at%2520https%253A//appworld.dev/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AppWorld%3A%20A%20Controllable%20World%20of%20Apps%20and%20People%20for%20Benchmarking%0A%20%20Interactive%20Coding%20Agents&entry.906535625=Harsh%20Trivedi%20and%20Tushar%20Khot%20and%20Mareike%20Hartmann%20and%20Ruskin%20Manku%20and%20Vinty%20Dong%20and%20Edward%20Li%20and%20Shashank%20Gupta%20and%20Ashish%20Sabharwal%20and%20Niranjan%20Balasubramanian&entry.1292438233=%20%20Autonomous%20agents%20that%20address%20day-to-day%20digital%20tasks%20%28e.g.%2C%20ordering%0Agroceries%20for%20a%20household%29%2C%20must%20not%20only%20operate%20multiple%20apps%20%28e.g.%2C%20notes%2C%0Amessaging%2C%20shopping%20app%29%20via%20APIs%2C%20but%20also%20generate%20rich%20code%20with%20complex%0Acontrol%20flow%20in%20an%20iterative%20manner%20based%20on%20their%20interaction%20with%20the%0Aenvironment.%20However%2C%20existing%20benchmarks%20for%20tool%20use%20are%20inadequate%2C%20as%20they%0Aonly%20cover%20tasks%20that%20require%20a%20simple%20sequence%20of%20API%20calls.%0A%20%20To%20remedy%20this%20gap%2C%20we%20built%20%24%5Ctextbf%7BAppWorld%20Engine%7D%24%2C%20a%20high-quality%0Aexecution%20environment%20%2860K%20lines%20of%20code%29%20of%209%20day-to-day%20apps%20operable%20via%20457%0AAPIs%20and%20populated%20with%20realistic%20digital%20activities%20simulating%20the%20lives%20of%0A~100%20fictitious%20users.%20We%20then%20created%20%24%5Ctextbf%7BAppWorld%20Benchmark%7D%24%20%2840K%20lines%0Aof%20code%29%2C%20a%20suite%20of%20750%20natural%2C%20diverse%2C%20and%20challenging%20autonomous%20agent%0Atasks%20requiring%20rich%20and%20interactive%20code%20generation.%20It%20supports%20robust%0Aprogrammatic%20evaluation%20with%20state-based%20unit%20tests%2C%20allowing%20for%20different%0Aways%20of%20completing%20a%20task%20while%20also%20checking%20for%20unexpected%20changes%2C%20i.e.%2C%0Acollateral%20damage.%20The%20state-of-the-art%20LLM%2C%20GPT-4o%2C%20solves%20only%20~49%25%20of%20our%0A%27normal%27%20tasks%20and%20~30%25%20of%20%27challenge%27%20tasks%2C%20while%20other%20models%20solve%20at%20least%0A16%25%20fewer.%20This%20highlights%20the%20benchmark%27s%20difficulty%20and%20AppWorld%27s%20potential%0Ato%20push%20the%20frontiers%20of%20interactive%20coding%20agents.%20The%20project%20website%20is%0Aavailable%20at%20https%3A//appworld.dev/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18901v1&entry.124074799=Read"},
{"title": "Log-Concave Coupling for Sampling Neural Net Posteriors", "author": "Curtis McDonald and Andrew R Barron", "abstract": "  In this work, we present a sampling algorithm for single hidden layer neural\nnetworks. This algorithm is built upon a recursive series of Bayesian\nposteriors using a method we call Greedy Bayes. Sampling of the Bayesian\nposterior for neuron weight vectors $w$ of dimension $d$ is challenging because\nof its multimodality. Our algorithm to tackle this problem is based on a\ncoupling of the posterior density for $w$ with an auxiliary random variable\n$\\xi$.\n  The resulting reverse conditional $w|\\xi$ of neuron weights given auxiliary\nrandom variable is shown to be log concave. In the construction of the\nposterior distributions we provide some freedom in the choice of the prior. In\nparticular, for Gaussian priors on $w$ with suitably small variance, the\nresulting marginal density of the auxiliary variable $\\xi$ is proven to be\nstrictly log concave for all dimensions $d$. For a uniform prior on the unit\n$\\ell_1$ ball, evidence is given that the density of $\\xi$ is again strictly\nlog concave for sufficiently large $d$.\n  The score of the marginal density of the auxiliary random variable $\\xi$ is\ndetermined by an expectation over $w|\\xi$ and thus can be computed by various\nrapidly mixing Markov Chain Monte Carlo methods. Moreover, the computation of\nthe score of $\\xi$ permits methods of sampling $\\xi$ by a stochastic diffusion\n(Langevin dynamics) with drift function built from this score. With such\ndynamics, information-theoretic methods pioneered by Bakry and Emery show that\naccurate sampling of $\\xi$ is obtained rapidly when its density is indeed\nstrictly log-concave. After which, one more draw from $w|\\xi$, produces neuron\nweights $w$ whose marginal distribution is from the desired posterior.\n", "link": "http://arxiv.org/abs/2407.18802v1", "date": "2024-07-26", "relevancy": 2.0221, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Log-Concave%20Coupling%20for%20Sampling%20Neural%20Net%20Posteriors&body=Title%3A%20Log-Concave%20Coupling%20for%20Sampling%20Neural%20Net%20Posteriors%0AAuthor%3A%20Curtis%20McDonald%20and%20Andrew%20R%20Barron%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20sampling%20algorithm%20for%20single%20hidden%20layer%20neural%0Anetworks.%20This%20algorithm%20is%20built%20upon%20a%20recursive%20series%20of%20Bayesian%0Aposteriors%20using%20a%20method%20we%20call%20Greedy%20Bayes.%20Sampling%20of%20the%20Bayesian%0Aposterior%20for%20neuron%20weight%20vectors%20%24w%24%20of%20dimension%20%24d%24%20is%20challenging%20because%0Aof%20its%20multimodality.%20Our%20algorithm%20to%20tackle%20this%20problem%20is%20based%20on%20a%0Acoupling%20of%20the%20posterior%20density%20for%20%24w%24%20with%20an%20auxiliary%20random%20variable%0A%24%5Cxi%24.%0A%20%20The%20resulting%20reverse%20conditional%20%24w%7C%5Cxi%24%20of%20neuron%20weights%20given%20auxiliary%0Arandom%20variable%20is%20shown%20to%20be%20log%20concave.%20In%20the%20construction%20of%20the%0Aposterior%20distributions%20we%20provide%20some%20freedom%20in%20the%20choice%20of%20the%20prior.%20In%0Aparticular%2C%20for%20Gaussian%20priors%20on%20%24w%24%20with%20suitably%20small%20variance%2C%20the%0Aresulting%20marginal%20density%20of%20the%20auxiliary%20variable%20%24%5Cxi%24%20is%20proven%20to%20be%0Astrictly%20log%20concave%20for%20all%20dimensions%20%24d%24.%20For%20a%20uniform%20prior%20on%20the%20unit%0A%24%5Cell_1%24%20ball%2C%20evidence%20is%20given%20that%20the%20density%20of%20%24%5Cxi%24%20is%20again%20strictly%0Alog%20concave%20for%20sufficiently%20large%20%24d%24.%0A%20%20The%20score%20of%20the%20marginal%20density%20of%20the%20auxiliary%20random%20variable%20%24%5Cxi%24%20is%0Adetermined%20by%20an%20expectation%20over%20%24w%7C%5Cxi%24%20and%20thus%20can%20be%20computed%20by%20various%0Arapidly%20mixing%20Markov%20Chain%20Monte%20Carlo%20methods.%20Moreover%2C%20the%20computation%20of%0Athe%20score%20of%20%24%5Cxi%24%20permits%20methods%20of%20sampling%20%24%5Cxi%24%20by%20a%20stochastic%20diffusion%0A%28Langevin%20dynamics%29%20with%20drift%20function%20built%20from%20this%20score.%20With%20such%0Adynamics%2C%20information-theoretic%20methods%20pioneered%20by%20Bakry%20and%20Emery%20show%20that%0Aaccurate%20sampling%20of%20%24%5Cxi%24%20is%20obtained%20rapidly%20when%20its%20density%20is%20indeed%0Astrictly%20log-concave.%20After%20which%2C%20one%20more%20draw%20from%20%24w%7C%5Cxi%24%2C%20produces%20neuron%0Aweights%20%24w%24%20whose%20marginal%20distribution%20is%20from%20the%20desired%20posterior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLog-Concave%2520Coupling%2520for%2520Sampling%2520Neural%2520Net%2520Posteriors%26entry.906535625%3DCurtis%2520McDonald%2520and%2520Andrew%2520R%2520Barron%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520sampling%2520algorithm%2520for%2520single%2520hidden%2520layer%2520neural%250Anetworks.%2520This%2520algorithm%2520is%2520built%2520upon%2520a%2520recursive%2520series%2520of%2520Bayesian%250Aposteriors%2520using%2520a%2520method%2520we%2520call%2520Greedy%2520Bayes.%2520Sampling%2520of%2520the%2520Bayesian%250Aposterior%2520for%2520neuron%2520weight%2520vectors%2520%2524w%2524%2520of%2520dimension%2520%2524d%2524%2520is%2520challenging%2520because%250Aof%2520its%2520multimodality.%2520Our%2520algorithm%2520to%2520tackle%2520this%2520problem%2520is%2520based%2520on%2520a%250Acoupling%2520of%2520the%2520posterior%2520density%2520for%2520%2524w%2524%2520with%2520an%2520auxiliary%2520random%2520variable%250A%2524%255Cxi%2524.%250A%2520%2520The%2520resulting%2520reverse%2520conditional%2520%2524w%257C%255Cxi%2524%2520of%2520neuron%2520weights%2520given%2520auxiliary%250Arandom%2520variable%2520is%2520shown%2520to%2520be%2520log%2520concave.%2520In%2520the%2520construction%2520of%2520the%250Aposterior%2520distributions%2520we%2520provide%2520some%2520freedom%2520in%2520the%2520choice%2520of%2520the%2520prior.%2520In%250Aparticular%252C%2520for%2520Gaussian%2520priors%2520on%2520%2524w%2524%2520with%2520suitably%2520small%2520variance%252C%2520the%250Aresulting%2520marginal%2520density%2520of%2520the%2520auxiliary%2520variable%2520%2524%255Cxi%2524%2520is%2520proven%2520to%2520be%250Astrictly%2520log%2520concave%2520for%2520all%2520dimensions%2520%2524d%2524.%2520For%2520a%2520uniform%2520prior%2520on%2520the%2520unit%250A%2524%255Cell_1%2524%2520ball%252C%2520evidence%2520is%2520given%2520that%2520the%2520density%2520of%2520%2524%255Cxi%2524%2520is%2520again%2520strictly%250Alog%2520concave%2520for%2520sufficiently%2520large%2520%2524d%2524.%250A%2520%2520The%2520score%2520of%2520the%2520marginal%2520density%2520of%2520the%2520auxiliary%2520random%2520variable%2520%2524%255Cxi%2524%2520is%250Adetermined%2520by%2520an%2520expectation%2520over%2520%2524w%257C%255Cxi%2524%2520and%2520thus%2520can%2520be%2520computed%2520by%2520various%250Arapidly%2520mixing%2520Markov%2520Chain%2520Monte%2520Carlo%2520methods.%2520Moreover%252C%2520the%2520computation%2520of%250Athe%2520score%2520of%2520%2524%255Cxi%2524%2520permits%2520methods%2520of%2520sampling%2520%2524%255Cxi%2524%2520by%2520a%2520stochastic%2520diffusion%250A%2528Langevin%2520dynamics%2529%2520with%2520drift%2520function%2520built%2520from%2520this%2520score.%2520With%2520such%250Adynamics%252C%2520information-theoretic%2520methods%2520pioneered%2520by%2520Bakry%2520and%2520Emery%2520show%2520that%250Aaccurate%2520sampling%2520of%2520%2524%255Cxi%2524%2520is%2520obtained%2520rapidly%2520when%2520its%2520density%2520is%2520indeed%250Astrictly%2520log-concave.%2520After%2520which%252C%2520one%2520more%2520draw%2520from%2520%2524w%257C%255Cxi%2524%252C%2520produces%2520neuron%250Aweights%2520%2524w%2524%2520whose%2520marginal%2520distribution%2520is%2520from%2520the%2520desired%2520posterior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Log-Concave%20Coupling%20for%20Sampling%20Neural%20Net%20Posteriors&entry.906535625=Curtis%20McDonald%20and%20Andrew%20R%20Barron&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20sampling%20algorithm%20for%20single%20hidden%20layer%20neural%0Anetworks.%20This%20algorithm%20is%20built%20upon%20a%20recursive%20series%20of%20Bayesian%0Aposteriors%20using%20a%20method%20we%20call%20Greedy%20Bayes.%20Sampling%20of%20the%20Bayesian%0Aposterior%20for%20neuron%20weight%20vectors%20%24w%24%20of%20dimension%20%24d%24%20is%20challenging%20because%0Aof%20its%20multimodality.%20Our%20algorithm%20to%20tackle%20this%20problem%20is%20based%20on%20a%0Acoupling%20of%20the%20posterior%20density%20for%20%24w%24%20with%20an%20auxiliary%20random%20variable%0A%24%5Cxi%24.%0A%20%20The%20resulting%20reverse%20conditional%20%24w%7C%5Cxi%24%20of%20neuron%20weights%20given%20auxiliary%0Arandom%20variable%20is%20shown%20to%20be%20log%20concave.%20In%20the%20construction%20of%20the%0Aposterior%20distributions%20we%20provide%20some%20freedom%20in%20the%20choice%20of%20the%20prior.%20In%0Aparticular%2C%20for%20Gaussian%20priors%20on%20%24w%24%20with%20suitably%20small%20variance%2C%20the%0Aresulting%20marginal%20density%20of%20the%20auxiliary%20variable%20%24%5Cxi%24%20is%20proven%20to%20be%0Astrictly%20log%20concave%20for%20all%20dimensions%20%24d%24.%20For%20a%20uniform%20prior%20on%20the%20unit%0A%24%5Cell_1%24%20ball%2C%20evidence%20is%20given%20that%20the%20density%20of%20%24%5Cxi%24%20is%20again%20strictly%0Alog%20concave%20for%20sufficiently%20large%20%24d%24.%0A%20%20The%20score%20of%20the%20marginal%20density%20of%20the%20auxiliary%20random%20variable%20%24%5Cxi%24%20is%0Adetermined%20by%20an%20expectation%20over%20%24w%7C%5Cxi%24%20and%20thus%20can%20be%20computed%20by%20various%0Arapidly%20mixing%20Markov%20Chain%20Monte%20Carlo%20methods.%20Moreover%2C%20the%20computation%20of%0Athe%20score%20of%20%24%5Cxi%24%20permits%20methods%20of%20sampling%20%24%5Cxi%24%20by%20a%20stochastic%20diffusion%0A%28Langevin%20dynamics%29%20with%20drift%20function%20built%20from%20this%20score.%20With%20such%0Adynamics%2C%20information-theoretic%20methods%20pioneered%20by%20Bakry%20and%20Emery%20show%20that%0Aaccurate%20sampling%20of%20%24%5Cxi%24%20is%20obtained%20rapidly%20when%20its%20density%20is%20indeed%0Astrictly%20log-concave.%20After%20which%2C%20one%20more%20draw%20from%20%24w%7C%5Cxi%24%2C%20produces%20neuron%0Aweights%20%24w%24%20whose%20marginal%20distribution%20is%20from%20the%20desired%20posterior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18802v1&entry.124074799=Read"},
{"title": "Frequency Guidance Matters: Skeletal Action Recognition by\n  Frequency-Aware Mixed Transformer", "author": "Wenhan Wu and Ce Zheng and Zihao Yang and Chen Chen and Srijan Das and Aidong Lu", "abstract": "  Recently, transformers have demonstrated great potential for modeling\nlong-term dependencies from skeleton sequences and thereby gained\never-increasing attention in skeleton action recognition. However, the existing\ntransformer-based approaches heavily rely on the naive attention mechanism for\ncapturing the spatiotemporal features, which falls short in learning\ndiscriminative representations that exhibit similar motion patterns. To address\nthis challenge, we introduce the Frequency-aware Mixed Transformer\n(FreqMixFormer), specifically designed for recognizing similar skeletal actions\nwith subtle discriminative motions. First, we introduce a frequency-aware\nattention module to unweave skeleton frequency representations by embedding\njoint features into frequency attention maps, aiming to distinguish the\ndiscriminative movements based on their frequency coefficients. Subsequently,\nwe develop a mixed transformer architecture to incorporate spatial features\nwith frequency features to model the comprehensive frequency-spatial patterns.\nAdditionally, a temporal transformer is proposed to extract the global\ncorrelations across frames. Extensive experiments show that FreqMiXFormer\noutperforms SOTA on 3 popular skeleton action recognition datasets, including\nNTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\n", "link": "http://arxiv.org/abs/2407.12322v2", "date": "2024-07-26", "relevancy": 2.0202, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5255}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency%20Guidance%20Matters%3A%20Skeletal%20Action%20Recognition%20by%0A%20%20Frequency-Aware%20Mixed%20Transformer&body=Title%3A%20Frequency%20Guidance%20Matters%3A%20Skeletal%20Action%20Recognition%20by%0A%20%20Frequency-Aware%20Mixed%20Transformer%0AAuthor%3A%20Wenhan%20Wu%20and%20Ce%20Zheng%20and%20Zihao%20Yang%20and%20Chen%20Chen%20and%20Srijan%20Das%20and%20Aidong%20Lu%0AAbstract%3A%20%20%20Recently%2C%20transformers%20have%20demonstrated%20great%20potential%20for%20modeling%0Along-term%20dependencies%20from%20skeleton%20sequences%20and%20thereby%20gained%0Aever-increasing%20attention%20in%20skeleton%20action%20recognition.%20However%2C%20the%20existing%0Atransformer-based%20approaches%20heavily%20rely%20on%20the%20naive%20attention%20mechanism%20for%0Acapturing%20the%20spatiotemporal%20features%2C%20which%20falls%20short%20in%20learning%0Adiscriminative%20representations%20that%20exhibit%20similar%20motion%20patterns.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20the%20Frequency-aware%20Mixed%20Transformer%0A%28FreqMixFormer%29%2C%20specifically%20designed%20for%20recognizing%20similar%20skeletal%20actions%0Awith%20subtle%20discriminative%20motions.%20First%2C%20we%20introduce%20a%20frequency-aware%0Aattention%20module%20to%20unweave%20skeleton%20frequency%20representations%20by%20embedding%0Ajoint%20features%20into%20frequency%20attention%20maps%2C%20aiming%20to%20distinguish%20the%0Adiscriminative%20movements%20based%20on%20their%20frequency%20coefficients.%20Subsequently%2C%0Awe%20develop%20a%20mixed%20transformer%20architecture%20to%20incorporate%20spatial%20features%0Awith%20frequency%20features%20to%20model%20the%20comprehensive%20frequency-spatial%20patterns.%0AAdditionally%2C%20a%20temporal%20transformer%20is%20proposed%20to%20extract%20the%20global%0Acorrelations%20across%20frames.%20Extensive%20experiments%20show%20that%20FreqMiXFormer%0Aoutperforms%20SOTA%20on%203%20popular%20skeleton%20action%20recognition%20datasets%2C%20including%0ANTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency%2520Guidance%2520Matters%253A%2520Skeletal%2520Action%2520Recognition%2520by%250A%2520%2520Frequency-Aware%2520Mixed%2520Transformer%26entry.906535625%3DWenhan%2520Wu%2520and%2520Ce%2520Zheng%2520and%2520Zihao%2520Yang%2520and%2520Chen%2520Chen%2520and%2520Srijan%2520Das%2520and%2520Aidong%2520Lu%26entry.1292438233%3D%2520%2520Recently%252C%2520transformers%2520have%2520demonstrated%2520great%2520potential%2520for%2520modeling%250Along-term%2520dependencies%2520from%2520skeleton%2520sequences%2520and%2520thereby%2520gained%250Aever-increasing%2520attention%2520in%2520skeleton%2520action%2520recognition.%2520However%252C%2520the%2520existing%250Atransformer-based%2520approaches%2520heavily%2520rely%2520on%2520the%2520naive%2520attention%2520mechanism%2520for%250Acapturing%2520the%2520spatiotemporal%2520features%252C%2520which%2520falls%2520short%2520in%2520learning%250Adiscriminative%2520representations%2520that%2520exhibit%2520similar%2520motion%2520patterns.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520introduce%2520the%2520Frequency-aware%2520Mixed%2520Transformer%250A%2528FreqMixFormer%2529%252C%2520specifically%2520designed%2520for%2520recognizing%2520similar%2520skeletal%2520actions%250Awith%2520subtle%2520discriminative%2520motions.%2520First%252C%2520we%2520introduce%2520a%2520frequency-aware%250Aattention%2520module%2520to%2520unweave%2520skeleton%2520frequency%2520representations%2520by%2520embedding%250Ajoint%2520features%2520into%2520frequency%2520attention%2520maps%252C%2520aiming%2520to%2520distinguish%2520the%250Adiscriminative%2520movements%2520based%2520on%2520their%2520frequency%2520coefficients.%2520Subsequently%252C%250Awe%2520develop%2520a%2520mixed%2520transformer%2520architecture%2520to%2520incorporate%2520spatial%2520features%250Awith%2520frequency%2520features%2520to%2520model%2520the%2520comprehensive%2520frequency-spatial%2520patterns.%250AAdditionally%252C%2520a%2520temporal%2520transformer%2520is%2520proposed%2520to%2520extract%2520the%2520global%250Acorrelations%2520across%2520frames.%2520Extensive%2520experiments%2520show%2520that%2520FreqMiXFormer%250Aoutperforms%2520SOTA%2520on%25203%2520popular%2520skeleton%2520action%2520recognition%2520datasets%252C%2520including%250ANTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%252C%2520and%2520NW-UCLA%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency%20Guidance%20Matters%3A%20Skeletal%20Action%20Recognition%20by%0A%20%20Frequency-Aware%20Mixed%20Transformer&entry.906535625=Wenhan%20Wu%20and%20Ce%20Zheng%20and%20Zihao%20Yang%20and%20Chen%20Chen%20and%20Srijan%20Das%20and%20Aidong%20Lu&entry.1292438233=%20%20Recently%2C%20transformers%20have%20demonstrated%20great%20potential%20for%20modeling%0Along-term%20dependencies%20from%20skeleton%20sequences%20and%20thereby%20gained%0Aever-increasing%20attention%20in%20skeleton%20action%20recognition.%20However%2C%20the%20existing%0Atransformer-based%20approaches%20heavily%20rely%20on%20the%20naive%20attention%20mechanism%20for%0Acapturing%20the%20spatiotemporal%20features%2C%20which%20falls%20short%20in%20learning%0Adiscriminative%20representations%20that%20exhibit%20similar%20motion%20patterns.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20the%20Frequency-aware%20Mixed%20Transformer%0A%28FreqMixFormer%29%2C%20specifically%20designed%20for%20recognizing%20similar%20skeletal%20actions%0Awith%20subtle%20discriminative%20motions.%20First%2C%20we%20introduce%20a%20frequency-aware%0Aattention%20module%20to%20unweave%20skeleton%20frequency%20representations%20by%20embedding%0Ajoint%20features%20into%20frequency%20attention%20maps%2C%20aiming%20to%20distinguish%20the%0Adiscriminative%20movements%20based%20on%20their%20frequency%20coefficients.%20Subsequently%2C%0Awe%20develop%20a%20mixed%20transformer%20architecture%20to%20incorporate%20spatial%20features%0Awith%20frequency%20features%20to%20model%20the%20comprehensive%20frequency-spatial%20patterns.%0AAdditionally%2C%20a%20temporal%20transformer%20is%20proposed%20to%20extract%20the%20global%0Acorrelations%20across%20frames.%20Extensive%20experiments%20show%20that%20FreqMiXFormer%0Aoutperforms%20SOTA%20on%203%20popular%20skeleton%20action%20recognition%20datasets%2C%20including%0ANTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12322v2&entry.124074799=Read"},
{"title": "A Scalable Quantum Non-local Neural Network for Image Classification", "author": "Sparsh Gupta and Debanjan Konar and Vaneet Aggarwal", "abstract": "  Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.\n", "link": "http://arxiv.org/abs/2407.18906v1", "date": "2024-07-26", "relevancy": 2.0107, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5118}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4965}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Quantum%20Non-local%20Neural%20Network%20for%20Image%20Classification&body=Title%3A%20A%20Scalable%20Quantum%20Non-local%20Neural%20Network%20for%20Image%20Classification%0AAuthor%3A%20Sparsh%20Gupta%20and%20Debanjan%20Konar%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20Non-local%20operations%20play%20a%20crucial%20role%20in%20computer%20vision%20enabling%20the%0Acapture%20of%20long-range%20dependencies%20through%20weighted%20sums%20of%20features%20across%20the%0Ainput%2C%20surpassing%20the%20constraints%20of%20traditional%20convolution%20operations%20that%0Afocus%20solely%20on%20local%20neighborhoods.%20Non-local%20operations%20typically%20require%0Acomputing%20pairwise%20relationships%20between%20all%20elements%20in%20a%20set%2C%20leading%20to%0Aquadratic%20complexity%20in%20terms%20of%20time%20and%20memory.%20Due%20to%20the%20high%20computational%0Aand%20memory%20demands%2C%20scaling%20non-local%20neural%20networks%20to%20large-scale%20problems%0Acan%20be%20challenging.%20This%20article%20introduces%20a%20hybrid%20quantum-classical%20scalable%0Anon-local%20neural%20network%2C%20referred%20to%20as%20Quantum%20Non-Local%20Neural%20Network%0A%28QNL-Net%29%2C%20to%20enhance%20pattern%20recognition.%20The%20proposed%20QNL-Net%20relies%20on%0Ainherent%20quantum%20parallelism%20to%20allow%20the%20simultaneous%20processing%20of%20a%20large%0Anumber%20of%20input%20features%20enabling%20more%20efficient%20computations%20in%0Aquantum-enhanced%20feature%20space%20and%20involving%20pairwise%20relationships%20through%0Aquantum%20entanglement.%20We%20benchmark%20our%20proposed%20QNL-Net%20with%20other%20quantum%0Acounterparts%20to%20binary%20classification%20with%20datasets%20MNIST%20and%20CIFAR-10.%20The%0Asimulation%20findings%20showcase%20our%20QNL-Net%20achieves%20cutting-edge%20accuracy%20levels%0Ain%20binary%20image%20classification%20among%20quantum%20classifiers%20while%20utilizing%20fewer%0Aqubits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Quantum%2520Non-local%2520Neural%2520Network%2520for%2520Image%2520Classification%26entry.906535625%3DSparsh%2520Gupta%2520and%2520Debanjan%2520Konar%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520Non-local%2520operations%2520play%2520a%2520crucial%2520role%2520in%2520computer%2520vision%2520enabling%2520the%250Acapture%2520of%2520long-range%2520dependencies%2520through%2520weighted%2520sums%2520of%2520features%2520across%2520the%250Ainput%252C%2520surpassing%2520the%2520constraints%2520of%2520traditional%2520convolution%2520operations%2520that%250Afocus%2520solely%2520on%2520local%2520neighborhoods.%2520Non-local%2520operations%2520typically%2520require%250Acomputing%2520pairwise%2520relationships%2520between%2520all%2520elements%2520in%2520a%2520set%252C%2520leading%2520to%250Aquadratic%2520complexity%2520in%2520terms%2520of%2520time%2520and%2520memory.%2520Due%2520to%2520the%2520high%2520computational%250Aand%2520memory%2520demands%252C%2520scaling%2520non-local%2520neural%2520networks%2520to%2520large-scale%2520problems%250Acan%2520be%2520challenging.%2520This%2520article%2520introduces%2520a%2520hybrid%2520quantum-classical%2520scalable%250Anon-local%2520neural%2520network%252C%2520referred%2520to%2520as%2520Quantum%2520Non-Local%2520Neural%2520Network%250A%2528QNL-Net%2529%252C%2520to%2520enhance%2520pattern%2520recognition.%2520The%2520proposed%2520QNL-Net%2520relies%2520on%250Ainherent%2520quantum%2520parallelism%2520to%2520allow%2520the%2520simultaneous%2520processing%2520of%2520a%2520large%250Anumber%2520of%2520input%2520features%2520enabling%2520more%2520efficient%2520computations%2520in%250Aquantum-enhanced%2520feature%2520space%2520and%2520involving%2520pairwise%2520relationships%2520through%250Aquantum%2520entanglement.%2520We%2520benchmark%2520our%2520proposed%2520QNL-Net%2520with%2520other%2520quantum%250Acounterparts%2520to%2520binary%2520classification%2520with%2520datasets%2520MNIST%2520and%2520CIFAR-10.%2520The%250Asimulation%2520findings%2520showcase%2520our%2520QNL-Net%2520achieves%2520cutting-edge%2520accuracy%2520levels%250Ain%2520binary%2520image%2520classification%2520among%2520quantum%2520classifiers%2520while%2520utilizing%2520fewer%250Aqubits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Quantum%20Non-local%20Neural%20Network%20for%20Image%20Classification&entry.906535625=Sparsh%20Gupta%20and%20Debanjan%20Konar%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20Non-local%20operations%20play%20a%20crucial%20role%20in%20computer%20vision%20enabling%20the%0Acapture%20of%20long-range%20dependencies%20through%20weighted%20sums%20of%20features%20across%20the%0Ainput%2C%20surpassing%20the%20constraints%20of%20traditional%20convolution%20operations%20that%0Afocus%20solely%20on%20local%20neighborhoods.%20Non-local%20operations%20typically%20require%0Acomputing%20pairwise%20relationships%20between%20all%20elements%20in%20a%20set%2C%20leading%20to%0Aquadratic%20complexity%20in%20terms%20of%20time%20and%20memory.%20Due%20to%20the%20high%20computational%0Aand%20memory%20demands%2C%20scaling%20non-local%20neural%20networks%20to%20large-scale%20problems%0Acan%20be%20challenging.%20This%20article%20introduces%20a%20hybrid%20quantum-classical%20scalable%0Anon-local%20neural%20network%2C%20referred%20to%20as%20Quantum%20Non-Local%20Neural%20Network%0A%28QNL-Net%29%2C%20to%20enhance%20pattern%20recognition.%20The%20proposed%20QNL-Net%20relies%20on%0Ainherent%20quantum%20parallelism%20to%20allow%20the%20simultaneous%20processing%20of%20a%20large%0Anumber%20of%20input%20features%20enabling%20more%20efficient%20computations%20in%0Aquantum-enhanced%20feature%20space%20and%20involving%20pairwise%20relationships%20through%0Aquantum%20entanglement.%20We%20benchmark%20our%20proposed%20QNL-Net%20with%20other%20quantum%0Acounterparts%20to%20binary%20classification%20with%20datasets%20MNIST%20and%20CIFAR-10.%20The%0Asimulation%20findings%20showcase%20our%20QNL-Net%20achieves%20cutting-edge%20accuracy%20levels%0Ain%20binary%20image%20classification%20among%20quantum%20classifiers%20while%20utilizing%20fewer%0Aqubits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18906v1&entry.124074799=Read"},
{"title": "Scalable Group Choreography via Variational Phase Manifold Learning", "author": "Nhat Le and Khoa Do and Xuan Bui and Tuong Do and Erman Tjiputra and Quang D. Tran and Anh Nguyen", "abstract": "  Generating group dance motion from the music is a challenging task with\nseveral industrial applications. Although several methods have been proposed to\ntackle this problem, most of them prioritize optimizing the fidelity in dancing\nmovement, constrained by predetermined dancer counts in datasets. This\nlimitation impedes adaptability to real-world applications. Our study addresses\nthe scalability problem in group choreography while preserving naturalness and\nsynchronization. In particular, we propose a phase-based variational generative\nmodel for group dance generation on learning a generative manifold. Our method\nachieves high-fidelity group dance motion and enables the generation with an\nunlimited number of dancers while consuming only a minimal and constant amount\nof memory. The intensive experiments on two public datasets show that our\nproposed method outperforms recent state-of-the-art approaches by a large\nmargin and is scalable to a great number of dancers beyond the training data.\n", "link": "http://arxiv.org/abs/2407.18839v1", "date": "2024-07-26", "relevancy": 2.0106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5054}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Group%20Choreography%20via%20Variational%20Phase%20Manifold%20Learning&body=Title%3A%20Scalable%20Group%20Choreography%20via%20Variational%20Phase%20Manifold%20Learning%0AAuthor%3A%20Nhat%20Le%20and%20Khoa%20Do%20and%20Xuan%20Bui%20and%20Tuong%20Do%20and%20Erman%20Tjiputra%20and%20Quang%20D.%20Tran%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20Generating%20group%20dance%20motion%20from%20the%20music%20is%20a%20challenging%20task%20with%0Aseveral%20industrial%20applications.%20Although%20several%20methods%20have%20been%20proposed%20to%0Atackle%20this%20problem%2C%20most%20of%20them%20prioritize%20optimizing%20the%20fidelity%20in%20dancing%0Amovement%2C%20constrained%20by%20predetermined%20dancer%20counts%20in%20datasets.%20This%0Alimitation%20impedes%20adaptability%20to%20real-world%20applications.%20Our%20study%20addresses%0Athe%20scalability%20problem%20in%20group%20choreography%20while%20preserving%20naturalness%20and%0Asynchronization.%20In%20particular%2C%20we%20propose%20a%20phase-based%20variational%20generative%0Amodel%20for%20group%20dance%20generation%20on%20learning%20a%20generative%20manifold.%20Our%20method%0Aachieves%20high-fidelity%20group%20dance%20motion%20and%20enables%20the%20generation%20with%20an%0Aunlimited%20number%20of%20dancers%20while%20consuming%20only%20a%20minimal%20and%20constant%20amount%0Aof%20memory.%20The%20intensive%20experiments%20on%20two%20public%20datasets%20show%20that%20our%0Aproposed%20method%20outperforms%20recent%20state-of-the-art%20approaches%20by%20a%20large%0Amargin%20and%20is%20scalable%20to%20a%20great%20number%20of%20dancers%20beyond%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Group%2520Choreography%2520via%2520Variational%2520Phase%2520Manifold%2520Learning%26entry.906535625%3DNhat%2520Le%2520and%2520Khoa%2520Do%2520and%2520Xuan%2520Bui%2520and%2520Tuong%2520Do%2520and%2520Erman%2520Tjiputra%2520and%2520Quang%2520D.%2520Tran%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Generating%2520group%2520dance%2520motion%2520from%2520the%2520music%2520is%2520a%2520challenging%2520task%2520with%250Aseveral%2520industrial%2520applications.%2520Although%2520several%2520methods%2520have%2520been%2520proposed%2520to%250Atackle%2520this%2520problem%252C%2520most%2520of%2520them%2520prioritize%2520optimizing%2520the%2520fidelity%2520in%2520dancing%250Amovement%252C%2520constrained%2520by%2520predetermined%2520dancer%2520counts%2520in%2520datasets.%2520This%250Alimitation%2520impedes%2520adaptability%2520to%2520real-world%2520applications.%2520Our%2520study%2520addresses%250Athe%2520scalability%2520problem%2520in%2520group%2520choreography%2520while%2520preserving%2520naturalness%2520and%250Asynchronization.%2520In%2520particular%252C%2520we%2520propose%2520a%2520phase-based%2520variational%2520generative%250Amodel%2520for%2520group%2520dance%2520generation%2520on%2520learning%2520a%2520generative%2520manifold.%2520Our%2520method%250Aachieves%2520high-fidelity%2520group%2520dance%2520motion%2520and%2520enables%2520the%2520generation%2520with%2520an%250Aunlimited%2520number%2520of%2520dancers%2520while%2520consuming%2520only%2520a%2520minimal%2520and%2520constant%2520amount%250Aof%2520memory.%2520The%2520intensive%2520experiments%2520on%2520two%2520public%2520datasets%2520show%2520that%2520our%250Aproposed%2520method%2520outperforms%2520recent%2520state-of-the-art%2520approaches%2520by%2520a%2520large%250Amargin%2520and%2520is%2520scalable%2520to%2520a%2520great%2520number%2520of%2520dancers%2520beyond%2520the%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Group%20Choreography%20via%20Variational%20Phase%20Manifold%20Learning&entry.906535625=Nhat%20Le%20and%20Khoa%20Do%20and%20Xuan%20Bui%20and%20Tuong%20Do%20and%20Erman%20Tjiputra%20and%20Quang%20D.%20Tran%20and%20Anh%20Nguyen&entry.1292438233=%20%20Generating%20group%20dance%20motion%20from%20the%20music%20is%20a%20challenging%20task%20with%0Aseveral%20industrial%20applications.%20Although%20several%20methods%20have%20been%20proposed%20to%0Atackle%20this%20problem%2C%20most%20of%20them%20prioritize%20optimizing%20the%20fidelity%20in%20dancing%0Amovement%2C%20constrained%20by%20predetermined%20dancer%20counts%20in%20datasets.%20This%0Alimitation%20impedes%20adaptability%20to%20real-world%20applications.%20Our%20study%20addresses%0Athe%20scalability%20problem%20in%20group%20choreography%20while%20preserving%20naturalness%20and%0Asynchronization.%20In%20particular%2C%20we%20propose%20a%20phase-based%20variational%20generative%0Amodel%20for%20group%20dance%20generation%20on%20learning%20a%20generative%20manifold.%20Our%20method%0Aachieves%20high-fidelity%20group%20dance%20motion%20and%20enables%20the%20generation%20with%20an%0Aunlimited%20number%20of%20dancers%20while%20consuming%20only%20a%20minimal%20and%20constant%20amount%0Aof%20memory.%20The%20intensive%20experiments%20on%20two%20public%20datasets%20show%20that%20our%0Aproposed%20method%20outperforms%20recent%20state-of-the-art%20approaches%20by%20a%20large%0Amargin%20and%20is%20scalable%20to%20a%20great%20number%20of%20dancers%20beyond%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18839v1&entry.124074799=Read"},
{"title": "Benchmarking Dependence Measures to Prevent Shortcut Learning in Medical\n  Imaging", "author": "Sarah M\u00fcller and Louisa Fay and Lisa M. Koch and Sergios Gatidis and Thomas K\u00fcstner and Philipp Berens", "abstract": "  Medical imaging cohorts are often confounded by factors such as acquisition\ndevices, hospital sites, patient backgrounds, and many more. As a result, deep\nlearning models tend to learn spurious correlations instead of causally related\nfeatures, limiting their generalizability to new and unseen data. This problem\ncan be addressed by minimizing dependence measures between intermediate\nrepresentations of task-related and non-task-related variables. These measures\ninclude mutual information, distance correlation, and the performance of\nadversarial classifiers. Here, we benchmark such dependence measures for the\ntask of preventing shortcut learning. We study a simplified setting using\nMorpho-MNIST and a medical imaging task with CheXpert chest radiographs. Our\nresults provide insights into how to mitigate confounding factors in medical\nimaging.\n", "link": "http://arxiv.org/abs/2407.18792v1", "date": "2024-07-26", "relevancy": 2.0018, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5094}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Dependence%20Measures%20to%20Prevent%20Shortcut%20Learning%20in%20Medical%0A%20%20Imaging&body=Title%3A%20Benchmarking%20Dependence%20Measures%20to%20Prevent%20Shortcut%20Learning%20in%20Medical%0A%20%20Imaging%0AAuthor%3A%20Sarah%20M%C3%BCller%20and%20Louisa%20Fay%20and%20Lisa%20M.%20Koch%20and%20Sergios%20Gatidis%20and%20Thomas%20K%C3%BCstner%20and%20Philipp%20Berens%0AAbstract%3A%20%20%20Medical%20imaging%20cohorts%20are%20often%20confounded%20by%20factors%20such%20as%20acquisition%0Adevices%2C%20hospital%20sites%2C%20patient%20backgrounds%2C%20and%20many%20more.%20As%20a%20result%2C%20deep%0Alearning%20models%20tend%20to%20learn%20spurious%20correlations%20instead%20of%20causally%20related%0Afeatures%2C%20limiting%20their%20generalizability%20to%20new%20and%20unseen%20data.%20This%20problem%0Acan%20be%20addressed%20by%20minimizing%20dependence%20measures%20between%20intermediate%0Arepresentations%20of%20task-related%20and%20non-task-related%20variables.%20These%20measures%0Ainclude%20mutual%20information%2C%20distance%20correlation%2C%20and%20the%20performance%20of%0Aadversarial%20classifiers.%20Here%2C%20we%20benchmark%20such%20dependence%20measures%20for%20the%0Atask%20of%20preventing%20shortcut%20learning.%20We%20study%20a%20simplified%20setting%20using%0AMorpho-MNIST%20and%20a%20medical%20imaging%20task%20with%20CheXpert%20chest%20radiographs.%20Our%0Aresults%20provide%20insights%20into%20how%20to%20mitigate%20confounding%20factors%20in%20medical%0Aimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Dependence%2520Measures%2520to%2520Prevent%2520Shortcut%2520Learning%2520in%2520Medical%250A%2520%2520Imaging%26entry.906535625%3DSarah%2520M%25C3%25BCller%2520and%2520Louisa%2520Fay%2520and%2520Lisa%2520M.%2520Koch%2520and%2520Sergios%2520Gatidis%2520and%2520Thomas%2520K%25C3%25BCstner%2520and%2520Philipp%2520Berens%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520cohorts%2520are%2520often%2520confounded%2520by%2520factors%2520such%2520as%2520acquisition%250Adevices%252C%2520hospital%2520sites%252C%2520patient%2520backgrounds%252C%2520and%2520many%2520more.%2520As%2520a%2520result%252C%2520deep%250Alearning%2520models%2520tend%2520to%2520learn%2520spurious%2520correlations%2520instead%2520of%2520causally%2520related%250Afeatures%252C%2520limiting%2520their%2520generalizability%2520to%2520new%2520and%2520unseen%2520data.%2520This%2520problem%250Acan%2520be%2520addressed%2520by%2520minimizing%2520dependence%2520measures%2520between%2520intermediate%250Arepresentations%2520of%2520task-related%2520and%2520non-task-related%2520variables.%2520These%2520measures%250Ainclude%2520mutual%2520information%252C%2520distance%2520correlation%252C%2520and%2520the%2520performance%2520of%250Aadversarial%2520classifiers.%2520Here%252C%2520we%2520benchmark%2520such%2520dependence%2520measures%2520for%2520the%250Atask%2520of%2520preventing%2520shortcut%2520learning.%2520We%2520study%2520a%2520simplified%2520setting%2520using%250AMorpho-MNIST%2520and%2520a%2520medical%2520imaging%2520task%2520with%2520CheXpert%2520chest%2520radiographs.%2520Our%250Aresults%2520provide%2520insights%2520into%2520how%2520to%2520mitigate%2520confounding%2520factors%2520in%2520medical%250Aimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Dependence%20Measures%20to%20Prevent%20Shortcut%20Learning%20in%20Medical%0A%20%20Imaging&entry.906535625=Sarah%20M%C3%BCller%20and%20Louisa%20Fay%20and%20Lisa%20M.%20Koch%20and%20Sergios%20Gatidis%20and%20Thomas%20K%C3%BCstner%20and%20Philipp%20Berens&entry.1292438233=%20%20Medical%20imaging%20cohorts%20are%20often%20confounded%20by%20factors%20such%20as%20acquisition%0Adevices%2C%20hospital%20sites%2C%20patient%20backgrounds%2C%20and%20many%20more.%20As%20a%20result%2C%20deep%0Alearning%20models%20tend%20to%20learn%20spurious%20correlations%20instead%20of%20causally%20related%0Afeatures%2C%20limiting%20their%20generalizability%20to%20new%20and%20unseen%20data.%20This%20problem%0Acan%20be%20addressed%20by%20minimizing%20dependence%20measures%20between%20intermediate%0Arepresentations%20of%20task-related%20and%20non-task-related%20variables.%20These%20measures%0Ainclude%20mutual%20information%2C%20distance%20correlation%2C%20and%20the%20performance%20of%0Aadversarial%20classifiers.%20Here%2C%20we%20benchmark%20such%20dependence%20measures%20for%20the%0Atask%20of%20preventing%20shortcut%20learning.%20We%20study%20a%20simplified%20setting%20using%0AMorpho-MNIST%20and%20a%20medical%20imaging%20task%20with%20CheXpert%20chest%20radiographs.%20Our%0Aresults%20provide%20insights%20into%20how%20to%20mitigate%20confounding%20factors%20in%20medical%0Aimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18792v1&entry.124074799=Read"},
{"title": "Distilling Multi-Scale Knowledge for Event Temporal Relation Extraction", "author": "Hao-Ren Yao and Luke Breitfeller and Aakanksha Naik and Chunxiao Zhou and Carolyn Rose", "abstract": "  Event Temporal Relation Extraction (ETRE) is paramount but challenging.\nWithin a discourse, event pairs are situated at different distances or the\nso-called proximity bands. The temporal ordering communicated about event pairs\nwhere at more remote (i.e., ``long'') or less remote (i.e., ``short'')\nproximity bands are encoded differently. SOTA models have tended to perform\nwell on events situated at either short or long proximity bands, but not both.\nNonetheless, real-world, natural texts contain all types of temporal\nevent-pairs. In this paper, we present MulCo: Distilling Multi-Scale Knowledge\nvia Contrastive Learning, a knowledge co-distillation approach that shares\nknowledge across multiple event pair proximity bands to improve performance on\nall types of temporal datasets. Our experimental results show that MulCo\nsuccessfully integrates linguistic cues pertaining to temporal reasoning across\nboth short and long proximity bands and achieves new state-of-the-art results\non several ETRE benchmark datasets.\n", "link": "http://arxiv.org/abs/2209.00568v3", "date": "2024-07-26", "relevancy": 1.9855, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4821}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Multi-Scale%20Knowledge%20for%20Event%20Temporal%20Relation%20Extraction&body=Title%3A%20Distilling%20Multi-Scale%20Knowledge%20for%20Event%20Temporal%20Relation%20Extraction%0AAuthor%3A%20Hao-Ren%20Yao%20and%20Luke%20Breitfeller%20and%20Aakanksha%20Naik%20and%20Chunxiao%20Zhou%20and%20Carolyn%20Rose%0AAbstract%3A%20%20%20Event%20Temporal%20Relation%20Extraction%20%28ETRE%29%20is%20paramount%20but%20challenging.%0AWithin%20a%20discourse%2C%20event%20pairs%20are%20situated%20at%20different%20distances%20or%20the%0Aso-called%20proximity%20bands.%20The%20temporal%20ordering%20communicated%20about%20event%20pairs%0Awhere%20at%20more%20remote%20%28i.e.%2C%20%60%60long%27%27%29%20or%20less%20remote%20%28i.e.%2C%20%60%60short%27%27%29%0Aproximity%20bands%20are%20encoded%20differently.%20SOTA%20models%20have%20tended%20to%20perform%0Awell%20on%20events%20situated%20at%20either%20short%20or%20long%20proximity%20bands%2C%20but%20not%20both.%0ANonetheless%2C%20real-world%2C%20natural%20texts%20contain%20all%20types%20of%20temporal%0Aevent-pairs.%20In%20this%20paper%2C%20we%20present%20MulCo%3A%20Distilling%20Multi-Scale%20Knowledge%0Avia%20Contrastive%20Learning%2C%20a%20knowledge%20co-distillation%20approach%20that%20shares%0Aknowledge%20across%20multiple%20event%20pair%20proximity%20bands%20to%20improve%20performance%20on%0Aall%20types%20of%20temporal%20datasets.%20Our%20experimental%20results%20show%20that%20MulCo%0Asuccessfully%20integrates%20linguistic%20cues%20pertaining%20to%20temporal%20reasoning%20across%0Aboth%20short%20and%20long%20proximity%20bands%20and%20achieves%20new%20state-of-the-art%20results%0Aon%20several%20ETRE%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.00568v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Multi-Scale%2520Knowledge%2520for%2520Event%2520Temporal%2520Relation%2520Extraction%26entry.906535625%3DHao-Ren%2520Yao%2520and%2520Luke%2520Breitfeller%2520and%2520Aakanksha%2520Naik%2520and%2520Chunxiao%2520Zhou%2520and%2520Carolyn%2520Rose%26entry.1292438233%3D%2520%2520Event%2520Temporal%2520Relation%2520Extraction%2520%2528ETRE%2529%2520is%2520paramount%2520but%2520challenging.%250AWithin%2520a%2520discourse%252C%2520event%2520pairs%2520are%2520situated%2520at%2520different%2520distances%2520or%2520the%250Aso-called%2520proximity%2520bands.%2520The%2520temporal%2520ordering%2520communicated%2520about%2520event%2520pairs%250Awhere%2520at%2520more%2520remote%2520%2528i.e.%252C%2520%2560%2560long%2527%2527%2529%2520or%2520less%2520remote%2520%2528i.e.%252C%2520%2560%2560short%2527%2527%2529%250Aproximity%2520bands%2520are%2520encoded%2520differently.%2520SOTA%2520models%2520have%2520tended%2520to%2520perform%250Awell%2520on%2520events%2520situated%2520at%2520either%2520short%2520or%2520long%2520proximity%2520bands%252C%2520but%2520not%2520both.%250ANonetheless%252C%2520real-world%252C%2520natural%2520texts%2520contain%2520all%2520types%2520of%2520temporal%250Aevent-pairs.%2520In%2520this%2520paper%252C%2520we%2520present%2520MulCo%253A%2520Distilling%2520Multi-Scale%2520Knowledge%250Avia%2520Contrastive%2520Learning%252C%2520a%2520knowledge%2520co-distillation%2520approach%2520that%2520shares%250Aknowledge%2520across%2520multiple%2520event%2520pair%2520proximity%2520bands%2520to%2520improve%2520performance%2520on%250Aall%2520types%2520of%2520temporal%2520datasets.%2520Our%2520experimental%2520results%2520show%2520that%2520MulCo%250Asuccessfully%2520integrates%2520linguistic%2520cues%2520pertaining%2520to%2520temporal%2520reasoning%2520across%250Aboth%2520short%2520and%2520long%2520proximity%2520bands%2520and%2520achieves%2520new%2520state-of-the-art%2520results%250Aon%2520several%2520ETRE%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.00568v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Multi-Scale%20Knowledge%20for%20Event%20Temporal%20Relation%20Extraction&entry.906535625=Hao-Ren%20Yao%20and%20Luke%20Breitfeller%20and%20Aakanksha%20Naik%20and%20Chunxiao%20Zhou%20and%20Carolyn%20Rose&entry.1292438233=%20%20Event%20Temporal%20Relation%20Extraction%20%28ETRE%29%20is%20paramount%20but%20challenging.%0AWithin%20a%20discourse%2C%20event%20pairs%20are%20situated%20at%20different%20distances%20or%20the%0Aso-called%20proximity%20bands.%20The%20temporal%20ordering%20communicated%20about%20event%20pairs%0Awhere%20at%20more%20remote%20%28i.e.%2C%20%60%60long%27%27%29%20or%20less%20remote%20%28i.e.%2C%20%60%60short%27%27%29%0Aproximity%20bands%20are%20encoded%20differently.%20SOTA%20models%20have%20tended%20to%20perform%0Awell%20on%20events%20situated%20at%20either%20short%20or%20long%20proximity%20bands%2C%20but%20not%20both.%0ANonetheless%2C%20real-world%2C%20natural%20texts%20contain%20all%20types%20of%20temporal%0Aevent-pairs.%20In%20this%20paper%2C%20we%20present%20MulCo%3A%20Distilling%20Multi-Scale%20Knowledge%0Avia%20Contrastive%20Learning%2C%20a%20knowledge%20co-distillation%20approach%20that%20shares%0Aknowledge%20across%20multiple%20event%20pair%20proximity%20bands%20to%20improve%20performance%20on%0Aall%20types%20of%20temporal%20datasets.%20Our%20experimental%20results%20show%20that%20MulCo%0Asuccessfully%20integrates%20linguistic%20cues%20pertaining%20to%20temporal%20reasoning%20across%0Aboth%20short%20and%20long%20proximity%20bands%20and%20achieves%20new%20state-of-the-art%20results%0Aon%20several%20ETRE%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.00568v3&entry.124074799=Read"},
{"title": "Robust VAEs via Generating Process of Noise Augmented Data", "author": "Hiroo Irobe and Wataru Aoki and Kimihiro Yamazaki and Yuhui Zhang and Takumi Nakagawa and Hiroki Waida and Yuichiro Wada and Takafumi Kanamori", "abstract": "  Advancing defensive mechanisms against adversarial attacks in generative\nmodels is a critical research topic in machine learning. Our study focuses on a\nspecific type of generative models - Variational Auto-Encoders (VAEs). Contrary\nto common beliefs and existing literature which suggest that noise injection\ntowards training data can make models more robust, our preliminary experiments\nrevealed that naive usage of noise augmentation technique did not substantially\nimprove VAE robustness. In fact, it even degraded the quality of learned\nrepresentations, making VAEs more susceptible to adversarial perturbations.\nThis paper introduces a novel framework that enhances robustness by\nregularizing the latent space divergence between original and noise-augmented\ndata. Through incorporating a paired probabilistic prior into the standard\nvariational lower bound, our method significantly boosts defense against\nadversarial attacks. Our empirical evaluations demonstrate that this approach,\ntermed Robust Augmented Variational Auto-ENcoder (RAVEN), yields superior\nperformance in resisting adversarial inputs on widely-recognized benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2407.18632v1", "date": "2024-07-26", "relevancy": 1.9805, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5274}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20VAEs%20via%20Generating%20Process%20of%20Noise%20Augmented%20Data&body=Title%3A%20Robust%20VAEs%20via%20Generating%20Process%20of%20Noise%20Augmented%20Data%0AAuthor%3A%20Hiroo%20Irobe%20and%20Wataru%20Aoki%20and%20Kimihiro%20Yamazaki%20and%20Yuhui%20Zhang%20and%20Takumi%20Nakagawa%20and%20Hiroki%20Waida%20and%20Yuichiro%20Wada%20and%20Takafumi%20Kanamori%0AAbstract%3A%20%20%20Advancing%20defensive%20mechanisms%20against%20adversarial%20attacks%20in%20generative%0Amodels%20is%20a%20critical%20research%20topic%20in%20machine%20learning.%20Our%20study%20focuses%20on%20a%0Aspecific%20type%20of%20generative%20models%20-%20Variational%20Auto-Encoders%20%28VAEs%29.%20Contrary%0Ato%20common%20beliefs%20and%20existing%20literature%20which%20suggest%20that%20noise%20injection%0Atowards%20training%20data%20can%20make%20models%20more%20robust%2C%20our%20preliminary%20experiments%0Arevealed%20that%20naive%20usage%20of%20noise%20augmentation%20technique%20did%20not%20substantially%0Aimprove%20VAE%20robustness.%20In%20fact%2C%20it%20even%20degraded%20the%20quality%20of%20learned%0Arepresentations%2C%20making%20VAEs%20more%20susceptible%20to%20adversarial%20perturbations.%0AThis%20paper%20introduces%20a%20novel%20framework%20that%20enhances%20robustness%20by%0Aregularizing%20the%20latent%20space%20divergence%20between%20original%20and%20noise-augmented%0Adata.%20Through%20incorporating%20a%20paired%20probabilistic%20prior%20into%20the%20standard%0Avariational%20lower%20bound%2C%20our%20method%20significantly%20boosts%20defense%20against%0Aadversarial%20attacks.%20Our%20empirical%20evaluations%20demonstrate%20that%20this%20approach%2C%0Atermed%20Robust%20Augmented%20Variational%20Auto-ENcoder%20%28RAVEN%29%2C%20yields%20superior%0Aperformance%20in%20resisting%20adversarial%20inputs%20on%20widely-recognized%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520VAEs%2520via%2520Generating%2520Process%2520of%2520Noise%2520Augmented%2520Data%26entry.906535625%3DHiroo%2520Irobe%2520and%2520Wataru%2520Aoki%2520and%2520Kimihiro%2520Yamazaki%2520and%2520Yuhui%2520Zhang%2520and%2520Takumi%2520Nakagawa%2520and%2520Hiroki%2520Waida%2520and%2520Yuichiro%2520Wada%2520and%2520Takafumi%2520Kanamori%26entry.1292438233%3D%2520%2520Advancing%2520defensive%2520mechanisms%2520against%2520adversarial%2520attacks%2520in%2520generative%250Amodels%2520is%2520a%2520critical%2520research%2520topic%2520in%2520machine%2520learning.%2520Our%2520study%2520focuses%2520on%2520a%250Aspecific%2520type%2520of%2520generative%2520models%2520-%2520Variational%2520Auto-Encoders%2520%2528VAEs%2529.%2520Contrary%250Ato%2520common%2520beliefs%2520and%2520existing%2520literature%2520which%2520suggest%2520that%2520noise%2520injection%250Atowards%2520training%2520data%2520can%2520make%2520models%2520more%2520robust%252C%2520our%2520preliminary%2520experiments%250Arevealed%2520that%2520naive%2520usage%2520of%2520noise%2520augmentation%2520technique%2520did%2520not%2520substantially%250Aimprove%2520VAE%2520robustness.%2520In%2520fact%252C%2520it%2520even%2520degraded%2520the%2520quality%2520of%2520learned%250Arepresentations%252C%2520making%2520VAEs%2520more%2520susceptible%2520to%2520adversarial%2520perturbations.%250AThis%2520paper%2520introduces%2520a%2520novel%2520framework%2520that%2520enhances%2520robustness%2520by%250Aregularizing%2520the%2520latent%2520space%2520divergence%2520between%2520original%2520and%2520noise-augmented%250Adata.%2520Through%2520incorporating%2520a%2520paired%2520probabilistic%2520prior%2520into%2520the%2520standard%250Avariational%2520lower%2520bound%252C%2520our%2520method%2520significantly%2520boosts%2520defense%2520against%250Aadversarial%2520attacks.%2520Our%2520empirical%2520evaluations%2520demonstrate%2520that%2520this%2520approach%252C%250Atermed%2520Robust%2520Augmented%2520Variational%2520Auto-ENcoder%2520%2528RAVEN%2529%252C%2520yields%2520superior%250Aperformance%2520in%2520resisting%2520adversarial%2520inputs%2520on%2520widely-recognized%2520benchmark%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20VAEs%20via%20Generating%20Process%20of%20Noise%20Augmented%20Data&entry.906535625=Hiroo%20Irobe%20and%20Wataru%20Aoki%20and%20Kimihiro%20Yamazaki%20and%20Yuhui%20Zhang%20and%20Takumi%20Nakagawa%20and%20Hiroki%20Waida%20and%20Yuichiro%20Wada%20and%20Takafumi%20Kanamori&entry.1292438233=%20%20Advancing%20defensive%20mechanisms%20against%20adversarial%20attacks%20in%20generative%0Amodels%20is%20a%20critical%20research%20topic%20in%20machine%20learning.%20Our%20study%20focuses%20on%20a%0Aspecific%20type%20of%20generative%20models%20-%20Variational%20Auto-Encoders%20%28VAEs%29.%20Contrary%0Ato%20common%20beliefs%20and%20existing%20literature%20which%20suggest%20that%20noise%20injection%0Atowards%20training%20data%20can%20make%20models%20more%20robust%2C%20our%20preliminary%20experiments%0Arevealed%20that%20naive%20usage%20of%20noise%20augmentation%20technique%20did%20not%20substantially%0Aimprove%20VAE%20robustness.%20In%20fact%2C%20it%20even%20degraded%20the%20quality%20of%20learned%0Arepresentations%2C%20making%20VAEs%20more%20susceptible%20to%20adversarial%20perturbations.%0AThis%20paper%20introduces%20a%20novel%20framework%20that%20enhances%20robustness%20by%0Aregularizing%20the%20latent%20space%20divergence%20between%20original%20and%20noise-augmented%0Adata.%20Through%20incorporating%20a%20paired%20probabilistic%20prior%20into%20the%20standard%0Avariational%20lower%20bound%2C%20our%20method%20significantly%20boosts%20defense%20against%0Aadversarial%20attacks.%20Our%20empirical%20evaluations%20demonstrate%20that%20this%20approach%2C%0Atermed%20Robust%20Augmented%20Variational%20Auto-ENcoder%20%28RAVEN%29%2C%20yields%20superior%0Aperformance%20in%20resisting%20adversarial%20inputs%20on%20widely-recognized%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18632v1&entry.124074799=Read"},
{"title": "Unsupervised Reservoir Computing for Multivariate Denoising of Severely\n  Contaminated Signals", "author": "Jaesung Choi and Pilwon Kim", "abstract": "  The interdependence and high dimensionality of multivariate signals present\nsignificant challenges for denoising, as conventional univariate methods often\nstruggle to capture the complex interactions between variables. A successful\napproach must consider not only the multivariate dependencies of the desired\nsignal but also the multivariate dependencies of the interfering noise. In our\nprevious research, we introduced a method using machine learning to extract the\nmaximum portion of ``predictable information\" from univariate signal. We extend\nthis approach to multivariate signals, with the key idea being to properly\nincorporate the interdependencies of the noise back into the interdependent\nreconstruction of the signal. The method works successfully for various\nmultivariate signals, including chaotic signals and highly oscillating\nsinusoidal signals which are corrupted by spatially correlated intensive noise.\nIt consistently outperforms other existing multivariate denoising methods\nacross a wide range of scenarios.\n", "link": "http://arxiv.org/abs/2407.18759v1", "date": "2024-07-26", "relevancy": 1.9779, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5162}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4853}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Reservoir%20Computing%20for%20Multivariate%20Denoising%20of%20Severely%0A%20%20Contaminated%20Signals&body=Title%3A%20Unsupervised%20Reservoir%20Computing%20for%20Multivariate%20Denoising%20of%20Severely%0A%20%20Contaminated%20Signals%0AAuthor%3A%20Jaesung%20Choi%20and%20Pilwon%20Kim%0AAbstract%3A%20%20%20The%20interdependence%20and%20high%20dimensionality%20of%20multivariate%20signals%20present%0Asignificant%20challenges%20for%20denoising%2C%20as%20conventional%20univariate%20methods%20often%0Astruggle%20to%20capture%20the%20complex%20interactions%20between%20variables.%20A%20successful%0Aapproach%20must%20consider%20not%20only%20the%20multivariate%20dependencies%20of%20the%20desired%0Asignal%20but%20also%20the%20multivariate%20dependencies%20of%20the%20interfering%20noise.%20In%20our%0Aprevious%20research%2C%20we%20introduced%20a%20method%20using%20machine%20learning%20to%20extract%20the%0Amaximum%20portion%20of%20%60%60predictable%20information%22%20from%20univariate%20signal.%20We%20extend%0Athis%20approach%20to%20multivariate%20signals%2C%20with%20the%20key%20idea%20being%20to%20properly%0Aincorporate%20the%20interdependencies%20of%20the%20noise%20back%20into%20the%20interdependent%0Areconstruction%20of%20the%20signal.%20The%20method%20works%20successfully%20for%20various%0Amultivariate%20signals%2C%20including%20chaotic%20signals%20and%20highly%20oscillating%0Asinusoidal%20signals%20which%20are%20corrupted%20by%20spatially%20correlated%20intensive%20noise.%0AIt%20consistently%20outperforms%20other%20existing%20multivariate%20denoising%20methods%0Aacross%20a%20wide%20range%20of%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Reservoir%2520Computing%2520for%2520Multivariate%2520Denoising%2520of%2520Severely%250A%2520%2520Contaminated%2520Signals%26entry.906535625%3DJaesung%2520Choi%2520and%2520Pilwon%2520Kim%26entry.1292438233%3D%2520%2520The%2520interdependence%2520and%2520high%2520dimensionality%2520of%2520multivariate%2520signals%2520present%250Asignificant%2520challenges%2520for%2520denoising%252C%2520as%2520conventional%2520univariate%2520methods%2520often%250Astruggle%2520to%2520capture%2520the%2520complex%2520interactions%2520between%2520variables.%2520A%2520successful%250Aapproach%2520must%2520consider%2520not%2520only%2520the%2520multivariate%2520dependencies%2520of%2520the%2520desired%250Asignal%2520but%2520also%2520the%2520multivariate%2520dependencies%2520of%2520the%2520interfering%2520noise.%2520In%2520our%250Aprevious%2520research%252C%2520we%2520introduced%2520a%2520method%2520using%2520machine%2520learning%2520to%2520extract%2520the%250Amaximum%2520portion%2520of%2520%2560%2560predictable%2520information%2522%2520from%2520univariate%2520signal.%2520We%2520extend%250Athis%2520approach%2520to%2520multivariate%2520signals%252C%2520with%2520the%2520key%2520idea%2520being%2520to%2520properly%250Aincorporate%2520the%2520interdependencies%2520of%2520the%2520noise%2520back%2520into%2520the%2520interdependent%250Areconstruction%2520of%2520the%2520signal.%2520The%2520method%2520works%2520successfully%2520for%2520various%250Amultivariate%2520signals%252C%2520including%2520chaotic%2520signals%2520and%2520highly%2520oscillating%250Asinusoidal%2520signals%2520which%2520are%2520corrupted%2520by%2520spatially%2520correlated%2520intensive%2520noise.%250AIt%2520consistently%2520outperforms%2520other%2520existing%2520multivariate%2520denoising%2520methods%250Aacross%2520a%2520wide%2520range%2520of%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Reservoir%20Computing%20for%20Multivariate%20Denoising%20of%20Severely%0A%20%20Contaminated%20Signals&entry.906535625=Jaesung%20Choi%20and%20Pilwon%20Kim&entry.1292438233=%20%20The%20interdependence%20and%20high%20dimensionality%20of%20multivariate%20signals%20present%0Asignificant%20challenges%20for%20denoising%2C%20as%20conventional%20univariate%20methods%20often%0Astruggle%20to%20capture%20the%20complex%20interactions%20between%20variables.%20A%20successful%0Aapproach%20must%20consider%20not%20only%20the%20multivariate%20dependencies%20of%20the%20desired%0Asignal%20but%20also%20the%20multivariate%20dependencies%20of%20the%20interfering%20noise.%20In%20our%0Aprevious%20research%2C%20we%20introduced%20a%20method%20using%20machine%20learning%20to%20extract%20the%0Amaximum%20portion%20of%20%60%60predictable%20information%22%20from%20univariate%20signal.%20We%20extend%0Athis%20approach%20to%20multivariate%20signals%2C%20with%20the%20key%20idea%20being%20to%20properly%0Aincorporate%20the%20interdependencies%20of%20the%20noise%20back%20into%20the%20interdependent%0Areconstruction%20of%20the%20signal.%20The%20method%20works%20successfully%20for%20various%0Amultivariate%20signals%2C%20including%20chaotic%20signals%20and%20highly%20oscillating%0Asinusoidal%20signals%20which%20are%20corrupted%20by%20spatially%20correlated%20intensive%20noise.%0AIt%20consistently%20outperforms%20other%20existing%20multivariate%20denoising%20methods%0Aacross%20a%20wide%20range%20of%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18759v1&entry.124074799=Read"},
{"title": "A Labeled Ophthalmic Ultrasound Dataset with Medical Report Generation\n  Based on Cross-modal Deep Learning", "author": "Jing Wang and Junyan Fan and Meng Zhou and Yanzhu Zhang and Mingyu Shi", "abstract": "  Ultrasound imaging reveals eye morphology and aids in diagnosing and treating\neye diseases. However, interpreting diagnostic reports requires specialized\nphysicians. We present a labeled ophthalmic dataset for the precise analysis\nand the automated exploration of medical images along with their associated\nreports. It collects three modal data, including the ultrasound images, blood\nflow information and examination reports from 2,417 patients at an\nophthalmology hospital in Shenyang, China, during the year 2018, in which the\npatient information is de-identified for privacy protection. To the best of our\nknowledge, it is the only ophthalmic dataset that contains the three modal\ninformation simultaneously. It incrementally consists of 4,858 images with the\ncorresponding free-text reports, which describe 15 typical imaging findings of\nintraocular diseases and the corresponding anatomical locations. Each image\nshows three kinds of blood flow indices at three specific arteries, i.e., nine\nparameter values to describe the spectral characteristics of blood flow\ndistribution. The reports were written by ophthalmologists during the clinical\ncare. The proposed dataset is applied to generate medical report based on the\ncross-modal deep learning model. The experimental results demonstrate that our\ndataset is suitable for training supervised models concerning cross-modal\nmedical data.\n", "link": "http://arxiv.org/abs/2407.18667v1", "date": "2024-07-26", "relevancy": 1.968, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Labeled%20Ophthalmic%20Ultrasound%20Dataset%20with%20Medical%20Report%20Generation%0A%20%20Based%20on%20Cross-modal%20Deep%20Learning&body=Title%3A%20A%20Labeled%20Ophthalmic%20Ultrasound%20Dataset%20with%20Medical%20Report%20Generation%0A%20%20Based%20on%20Cross-modal%20Deep%20Learning%0AAuthor%3A%20Jing%20Wang%20and%20Junyan%20Fan%20and%20Meng%20Zhou%20and%20Yanzhu%20Zhang%20and%20Mingyu%20Shi%0AAbstract%3A%20%20%20Ultrasound%20imaging%20reveals%20eye%20morphology%20and%20aids%20in%20diagnosing%20and%20treating%0Aeye%20diseases.%20However%2C%20interpreting%20diagnostic%20reports%20requires%20specialized%0Aphysicians.%20We%20present%20a%20labeled%20ophthalmic%20dataset%20for%20the%20precise%20analysis%0Aand%20the%20automated%20exploration%20of%20medical%20images%20along%20with%20their%20associated%0Areports.%20It%20collects%20three%20modal%20data%2C%20including%20the%20ultrasound%20images%2C%20blood%0Aflow%20information%20and%20examination%20reports%20from%202%2C417%20patients%20at%20an%0Aophthalmology%20hospital%20in%20Shenyang%2C%20China%2C%20during%20the%20year%202018%2C%20in%20which%20the%0Apatient%20information%20is%20de-identified%20for%20privacy%20protection.%20To%20the%20best%20of%20our%0Aknowledge%2C%20it%20is%20the%20only%20ophthalmic%20dataset%20that%20contains%20the%20three%20modal%0Ainformation%20simultaneously.%20It%20incrementally%20consists%20of%204%2C858%20images%20with%20the%0Acorresponding%20free-text%20reports%2C%20which%20describe%2015%20typical%20imaging%20findings%20of%0Aintraocular%20diseases%20and%20the%20corresponding%20anatomical%20locations.%20Each%20image%0Ashows%20three%20kinds%20of%20blood%20flow%20indices%20at%20three%20specific%20arteries%2C%20i.e.%2C%20nine%0Aparameter%20values%20to%20describe%20the%20spectral%20characteristics%20of%20blood%20flow%0Adistribution.%20The%20reports%20were%20written%20by%20ophthalmologists%20during%20the%20clinical%0Acare.%20The%20proposed%20dataset%20is%20applied%20to%20generate%20medical%20report%20based%20on%20the%0Across-modal%20deep%20learning%20model.%20The%20experimental%20results%20demonstrate%20that%20our%0Adataset%20is%20suitable%20for%20training%20supervised%20models%20concerning%20cross-modal%0Amedical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Labeled%2520Ophthalmic%2520Ultrasound%2520Dataset%2520with%2520Medical%2520Report%2520Generation%250A%2520%2520Based%2520on%2520Cross-modal%2520Deep%2520Learning%26entry.906535625%3DJing%2520Wang%2520and%2520Junyan%2520Fan%2520and%2520Meng%2520Zhou%2520and%2520Yanzhu%2520Zhang%2520and%2520Mingyu%2520Shi%26entry.1292438233%3D%2520%2520Ultrasound%2520imaging%2520reveals%2520eye%2520morphology%2520and%2520aids%2520in%2520diagnosing%2520and%2520treating%250Aeye%2520diseases.%2520However%252C%2520interpreting%2520diagnostic%2520reports%2520requires%2520specialized%250Aphysicians.%2520We%2520present%2520a%2520labeled%2520ophthalmic%2520dataset%2520for%2520the%2520precise%2520analysis%250Aand%2520the%2520automated%2520exploration%2520of%2520medical%2520images%2520along%2520with%2520their%2520associated%250Areports.%2520It%2520collects%2520three%2520modal%2520data%252C%2520including%2520the%2520ultrasound%2520images%252C%2520blood%250Aflow%2520information%2520and%2520examination%2520reports%2520from%25202%252C417%2520patients%2520at%2520an%250Aophthalmology%2520hospital%2520in%2520Shenyang%252C%2520China%252C%2520during%2520the%2520year%25202018%252C%2520in%2520which%2520the%250Apatient%2520information%2520is%2520de-identified%2520for%2520privacy%2520protection.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520it%2520is%2520the%2520only%2520ophthalmic%2520dataset%2520that%2520contains%2520the%2520three%2520modal%250Ainformation%2520simultaneously.%2520It%2520incrementally%2520consists%2520of%25204%252C858%2520images%2520with%2520the%250Acorresponding%2520free-text%2520reports%252C%2520which%2520describe%252015%2520typical%2520imaging%2520findings%2520of%250Aintraocular%2520diseases%2520and%2520the%2520corresponding%2520anatomical%2520locations.%2520Each%2520image%250Ashows%2520three%2520kinds%2520of%2520blood%2520flow%2520indices%2520at%2520three%2520specific%2520arteries%252C%2520i.e.%252C%2520nine%250Aparameter%2520values%2520to%2520describe%2520the%2520spectral%2520characteristics%2520of%2520blood%2520flow%250Adistribution.%2520The%2520reports%2520were%2520written%2520by%2520ophthalmologists%2520during%2520the%2520clinical%250Acare.%2520The%2520proposed%2520dataset%2520is%2520applied%2520to%2520generate%2520medical%2520report%2520based%2520on%2520the%250Across-modal%2520deep%2520learning%2520model.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Adataset%2520is%2520suitable%2520for%2520training%2520supervised%2520models%2520concerning%2520cross-modal%250Amedical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Labeled%20Ophthalmic%20Ultrasound%20Dataset%20with%20Medical%20Report%20Generation%0A%20%20Based%20on%20Cross-modal%20Deep%20Learning&entry.906535625=Jing%20Wang%20and%20Junyan%20Fan%20and%20Meng%20Zhou%20and%20Yanzhu%20Zhang%20and%20Mingyu%20Shi&entry.1292438233=%20%20Ultrasound%20imaging%20reveals%20eye%20morphology%20and%20aids%20in%20diagnosing%20and%20treating%0Aeye%20diseases.%20However%2C%20interpreting%20diagnostic%20reports%20requires%20specialized%0Aphysicians.%20We%20present%20a%20labeled%20ophthalmic%20dataset%20for%20the%20precise%20analysis%0Aand%20the%20automated%20exploration%20of%20medical%20images%20along%20with%20their%20associated%0Areports.%20It%20collects%20three%20modal%20data%2C%20including%20the%20ultrasound%20images%2C%20blood%0Aflow%20information%20and%20examination%20reports%20from%202%2C417%20patients%20at%20an%0Aophthalmology%20hospital%20in%20Shenyang%2C%20China%2C%20during%20the%20year%202018%2C%20in%20which%20the%0Apatient%20information%20is%20de-identified%20for%20privacy%20protection.%20To%20the%20best%20of%20our%0Aknowledge%2C%20it%20is%20the%20only%20ophthalmic%20dataset%20that%20contains%20the%20three%20modal%0Ainformation%20simultaneously.%20It%20incrementally%20consists%20of%204%2C858%20images%20with%20the%0Acorresponding%20free-text%20reports%2C%20which%20describe%2015%20typical%20imaging%20findings%20of%0Aintraocular%20diseases%20and%20the%20corresponding%20anatomical%20locations.%20Each%20image%0Ashows%20three%20kinds%20of%20blood%20flow%20indices%20at%20three%20specific%20arteries%2C%20i.e.%2C%20nine%0Aparameter%20values%20to%20describe%20the%20spectral%20characteristics%20of%20blood%20flow%0Adistribution.%20The%20reports%20were%20written%20by%20ophthalmologists%20during%20the%20clinical%0Acare.%20The%20proposed%20dataset%20is%20applied%20to%20generate%20medical%20report%20based%20on%20the%0Across-modal%20deep%20learning%20model.%20The%20experimental%20results%20demonstrate%20that%20our%0Adataset%20is%20suitable%20for%20training%20supervised%20models%20concerning%20cross-modal%0Amedical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18667v1&entry.124074799=Read"},
{"title": "Do We Really Need Graph Convolution During Training? Light Post-Training\n  Graph-ODE for Efficient Recommendation", "author": "Weizhi Zhang and Liangwei Yang and Zihe Song and Henry Peng Zou and Ke Xu and Henry Peng Zou and Liancheng Fang and Philip S. Yu", "abstract": "  The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems (RecSys) have been persistent concerns, hindering\ntheir deployment in real-world applications. This paper presents a critical\nexamination of the necessity of graph convolutions during the training phase\nand introduces an innovative alternative: the Light Post-Training Graph\nOrdinary-Differential-Equation (LightGODE). Our investigation reveals that the\nbenefits of GCNs are more pronounced during testing rather than training.\nMotivated by this, LightGODE utilizes a novel post-training graph convolution\nmethod that bypasses the computation-intensive message passing of GCNs and\nemploys a non-parametric continuous graph ordinary-differential-equation (ODE)\nto dynamically model node representations. This approach drastically reduces\ntraining time while achieving fine-grained post-training graph convolution to\navoid the distortion of the original training embedding space, termed the\nembedding discrepancy issue. We validate our model across several real-world\ndatasets of different scales, demonstrating that LightGODE not only outperforms\nGCN-based models in terms of efficiency and effectiveness but also\nsignificantly mitigates the embedding discrepancy commonly associated with\ndeeper graph convolution layers. Our LightGODE challenges the prevailing\nparadigms in RecSys training and suggests re-evaluating the role of graph\nconvolutions, potentially guiding future developments of efficient large-scale\ngraph-based RecSys.\n", "link": "http://arxiv.org/abs/2407.18910v1", "date": "2024-07-26", "relevancy": 1.9639, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4986}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20We%20Really%20Need%20Graph%20Convolution%20During%20Training%3F%20Light%20Post-Training%0A%20%20Graph-ODE%20for%20Efficient%20Recommendation&body=Title%3A%20Do%20We%20Really%20Need%20Graph%20Convolution%20During%20Training%3F%20Light%20Post-Training%0A%20%20Graph-ODE%20for%20Efficient%20Recommendation%0AAuthor%3A%20Weizhi%20Zhang%20and%20Liangwei%20Yang%20and%20Zihe%20Song%20and%20Henry%20Peng%20Zou%20and%20Ke%20Xu%20and%20Henry%20Peng%20Zou%20and%20Liancheng%20Fang%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20The%20efficiency%20and%20scalability%20of%20graph%20convolution%20networks%20%28GCNs%29%20in%0Atraining%20recommender%20systems%20%28RecSys%29%20have%20been%20persistent%20concerns%2C%20hindering%0Atheir%20deployment%20in%20real-world%20applications.%20This%20paper%20presents%20a%20critical%0Aexamination%20of%20the%20necessity%20of%20graph%20convolutions%20during%20the%20training%20phase%0Aand%20introduces%20an%20innovative%20alternative%3A%20the%20Light%20Post-Training%20Graph%0AOrdinary-Differential-Equation%20%28LightGODE%29.%20Our%20investigation%20reveals%20that%20the%0Abenefits%20of%20GCNs%20are%20more%20pronounced%20during%20testing%20rather%20than%20training.%0AMotivated%20by%20this%2C%20LightGODE%20utilizes%20a%20novel%20post-training%20graph%20convolution%0Amethod%20that%20bypasses%20the%20computation-intensive%20message%20passing%20of%20GCNs%20and%0Aemploys%20a%20non-parametric%20continuous%20graph%20ordinary-differential-equation%20%28ODE%29%0Ato%20dynamically%20model%20node%20representations.%20This%20approach%20drastically%20reduces%0Atraining%20time%20while%20achieving%20fine-grained%20post-training%20graph%20convolution%20to%0Aavoid%20the%20distortion%20of%20the%20original%20training%20embedding%20space%2C%20termed%20the%0Aembedding%20discrepancy%20issue.%20We%20validate%20our%20model%20across%20several%20real-world%0Adatasets%20of%20different%20scales%2C%20demonstrating%20that%20LightGODE%20not%20only%20outperforms%0AGCN-based%20models%20in%20terms%20of%20efficiency%20and%20effectiveness%20but%20also%0Asignificantly%20mitigates%20the%20embedding%20discrepancy%20commonly%20associated%20with%0Adeeper%20graph%20convolution%20layers.%20Our%20LightGODE%20challenges%20the%20prevailing%0Aparadigms%20in%20RecSys%20training%20and%20suggests%20re-evaluating%20the%20role%20of%20graph%0Aconvolutions%2C%20potentially%20guiding%20future%20developments%20of%20efficient%20large-scale%0Agraph-based%20RecSys.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520We%2520Really%2520Need%2520Graph%2520Convolution%2520During%2520Training%253F%2520Light%2520Post-Training%250A%2520%2520Graph-ODE%2520for%2520Efficient%2520Recommendation%26entry.906535625%3DWeizhi%2520Zhang%2520and%2520Liangwei%2520Yang%2520and%2520Zihe%2520Song%2520and%2520Henry%2520Peng%2520Zou%2520and%2520Ke%2520Xu%2520and%2520Henry%2520Peng%2520Zou%2520and%2520Liancheng%2520Fang%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520The%2520efficiency%2520and%2520scalability%2520of%2520graph%2520convolution%2520networks%2520%2528GCNs%2529%2520in%250Atraining%2520recommender%2520systems%2520%2528RecSys%2529%2520have%2520been%2520persistent%2520concerns%252C%2520hindering%250Atheir%2520deployment%2520in%2520real-world%2520applications.%2520This%2520paper%2520presents%2520a%2520critical%250Aexamination%2520of%2520the%2520necessity%2520of%2520graph%2520convolutions%2520during%2520the%2520training%2520phase%250Aand%2520introduces%2520an%2520innovative%2520alternative%253A%2520the%2520Light%2520Post-Training%2520Graph%250AOrdinary-Differential-Equation%2520%2528LightGODE%2529.%2520Our%2520investigation%2520reveals%2520that%2520the%250Abenefits%2520of%2520GCNs%2520are%2520more%2520pronounced%2520during%2520testing%2520rather%2520than%2520training.%250AMotivated%2520by%2520this%252C%2520LightGODE%2520utilizes%2520a%2520novel%2520post-training%2520graph%2520convolution%250Amethod%2520that%2520bypasses%2520the%2520computation-intensive%2520message%2520passing%2520of%2520GCNs%2520and%250Aemploys%2520a%2520non-parametric%2520continuous%2520graph%2520ordinary-differential-equation%2520%2528ODE%2529%250Ato%2520dynamically%2520model%2520node%2520representations.%2520This%2520approach%2520drastically%2520reduces%250Atraining%2520time%2520while%2520achieving%2520fine-grained%2520post-training%2520graph%2520convolution%2520to%250Aavoid%2520the%2520distortion%2520of%2520the%2520original%2520training%2520embedding%2520space%252C%2520termed%2520the%250Aembedding%2520discrepancy%2520issue.%2520We%2520validate%2520our%2520model%2520across%2520several%2520real-world%250Adatasets%2520of%2520different%2520scales%252C%2520demonstrating%2520that%2520LightGODE%2520not%2520only%2520outperforms%250AGCN-based%2520models%2520in%2520terms%2520of%2520efficiency%2520and%2520effectiveness%2520but%2520also%250Asignificantly%2520mitigates%2520the%2520embedding%2520discrepancy%2520commonly%2520associated%2520with%250Adeeper%2520graph%2520convolution%2520layers.%2520Our%2520LightGODE%2520challenges%2520the%2520prevailing%250Aparadigms%2520in%2520RecSys%2520training%2520and%2520suggests%2520re-evaluating%2520the%2520role%2520of%2520graph%250Aconvolutions%252C%2520potentially%2520guiding%2520future%2520developments%2520of%2520efficient%2520large-scale%250Agraph-based%2520RecSys.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20We%20Really%20Need%20Graph%20Convolution%20During%20Training%3F%20Light%20Post-Training%0A%20%20Graph-ODE%20for%20Efficient%20Recommendation&entry.906535625=Weizhi%20Zhang%20and%20Liangwei%20Yang%20and%20Zihe%20Song%20and%20Henry%20Peng%20Zou%20and%20Ke%20Xu%20and%20Henry%20Peng%20Zou%20and%20Liancheng%20Fang%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20The%20efficiency%20and%20scalability%20of%20graph%20convolution%20networks%20%28GCNs%29%20in%0Atraining%20recommender%20systems%20%28RecSys%29%20have%20been%20persistent%20concerns%2C%20hindering%0Atheir%20deployment%20in%20real-world%20applications.%20This%20paper%20presents%20a%20critical%0Aexamination%20of%20the%20necessity%20of%20graph%20convolutions%20during%20the%20training%20phase%0Aand%20introduces%20an%20innovative%20alternative%3A%20the%20Light%20Post-Training%20Graph%0AOrdinary-Differential-Equation%20%28LightGODE%29.%20Our%20investigation%20reveals%20that%20the%0Abenefits%20of%20GCNs%20are%20more%20pronounced%20during%20testing%20rather%20than%20training.%0AMotivated%20by%20this%2C%20LightGODE%20utilizes%20a%20novel%20post-training%20graph%20convolution%0Amethod%20that%20bypasses%20the%20computation-intensive%20message%20passing%20of%20GCNs%20and%0Aemploys%20a%20non-parametric%20continuous%20graph%20ordinary-differential-equation%20%28ODE%29%0Ato%20dynamically%20model%20node%20representations.%20This%20approach%20drastically%20reduces%0Atraining%20time%20while%20achieving%20fine-grained%20post-training%20graph%20convolution%20to%0Aavoid%20the%20distortion%20of%20the%20original%20training%20embedding%20space%2C%20termed%20the%0Aembedding%20discrepancy%20issue.%20We%20validate%20our%20model%20across%20several%20real-world%0Adatasets%20of%20different%20scales%2C%20demonstrating%20that%20LightGODE%20not%20only%20outperforms%0AGCN-based%20models%20in%20terms%20of%20efficiency%20and%20effectiveness%20but%20also%0Asignificantly%20mitigates%20the%20embedding%20discrepancy%20commonly%20associated%20with%0Adeeper%20graph%20convolution%20layers.%20Our%20LightGODE%20challenges%20the%20prevailing%0Aparadigms%20in%20RecSys%20training%20and%20suggests%20re-evaluating%20the%20role%20of%20graph%0Aconvolutions%2C%20potentially%20guiding%20future%20developments%20of%20efficient%20large-scale%0Agraph-based%20RecSys.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18910v1&entry.124074799=Read"},
{"title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI", "author": "Tim R\u00e4dsch and Annika Reinke and Vivienn Weru and Minu D. Tizabi and Nicholas Heller and Fabian Isensee and Annette Kopp-Schneider and Lena Maier-Hein", "abstract": "  This paper does not describe a novel method. Instead, it studies an essential\nfoundation for reliable benchmarking and ultimately real-world application of\nAI-based image analysis: generating high-quality reference annotations.\nPrevious research has focused on crowdsourcing as a means of outsourcing\nannotations. However, little attention has so far been given to annotation\ncompanies, specifically regarding their internal quality assurance (QA)\nprocesses. Therefore, our aim is to evaluate the influence of QA employed by\nannotation companies on annotation quality and devise methodologies for\nmaximizing data annotation efficacy. Based on a total of 57,648 instance\nsegmented images obtained from a total of 924 annotators and 34 QA workers from\nfour annotation companies and Amazon Mechanical Turk (MTurk), we derived the\nfollowing insights: (1) Annotation companies perform better both in terms of\nquantity and quality compared to the widely used platform MTurk. (2) Annotation\ncompanies' internal QA only provides marginal improvements, if any. However,\nimproving labeling instructions instead of investing in QA can substantially\nboost annotation performance. (3) The benefit of internal QA depends on\nspecific image characteristics. Our work could enable researchers to derive\nsubstantially more value from a fixed annotation budget and change the way\nannotation companies conduct internal QA.\n", "link": "http://arxiv.org/abs/2407.17596v2", "date": "2024-07-26", "relevancy": 1.9572, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4898}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality%20Assured%3A%20Rethinking%20Annotation%20Strategies%20in%20Imaging%20AI&body=Title%3A%20Quality%20Assured%3A%20Rethinking%20Annotation%20Strategies%20in%20Imaging%20AI%0AAuthor%3A%20Tim%20R%C3%A4dsch%20and%20Annika%20Reinke%20and%20Vivienn%20Weru%20and%20Minu%20D.%20Tizabi%20and%20Nicholas%20Heller%20and%20Fabian%20Isensee%20and%20Annette%20Kopp-Schneider%20and%20Lena%20Maier-Hein%0AAbstract%3A%20%20%20This%20paper%20does%20not%20describe%20a%20novel%20method.%20Instead%2C%20it%20studies%20an%20essential%0Afoundation%20for%20reliable%20benchmarking%20and%20ultimately%20real-world%20application%20of%0AAI-based%20image%20analysis%3A%20generating%20high-quality%20reference%20annotations.%0APrevious%20research%20has%20focused%20on%20crowdsourcing%20as%20a%20means%20of%20outsourcing%0Aannotations.%20However%2C%20little%20attention%20has%20so%20far%20been%20given%20to%20annotation%0Acompanies%2C%20specifically%20regarding%20their%20internal%20quality%20assurance%20%28QA%29%0Aprocesses.%20Therefore%2C%20our%20aim%20is%20to%20evaluate%20the%20influence%20of%20QA%20employed%20by%0Aannotation%20companies%20on%20annotation%20quality%20and%20devise%20methodologies%20for%0Amaximizing%20data%20annotation%20efficacy.%20Based%20on%20a%20total%20of%2057%2C648%20instance%0Asegmented%20images%20obtained%20from%20a%20total%20of%20924%20annotators%20and%2034%20QA%20workers%20from%0Afour%20annotation%20companies%20and%20Amazon%20Mechanical%20Turk%20%28MTurk%29%2C%20we%20derived%20the%0Afollowing%20insights%3A%20%281%29%20Annotation%20companies%20perform%20better%20both%20in%20terms%20of%0Aquantity%20and%20quality%20compared%20to%20the%20widely%20used%20platform%20MTurk.%20%282%29%20Annotation%0Acompanies%27%20internal%20QA%20only%20provides%20marginal%20improvements%2C%20if%20any.%20However%2C%0Aimproving%20labeling%20instructions%20instead%20of%20investing%20in%20QA%20can%20substantially%0Aboost%20annotation%20performance.%20%283%29%20The%20benefit%20of%20internal%20QA%20depends%20on%0Aspecific%20image%20characteristics.%20Our%20work%20could%20enable%20researchers%20to%20derive%0Asubstantially%20more%20value%20from%20a%20fixed%20annotation%20budget%20and%20change%20the%20way%0Aannotation%20companies%20conduct%20internal%20QA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality%2520Assured%253A%2520Rethinking%2520Annotation%2520Strategies%2520in%2520Imaging%2520AI%26entry.906535625%3DTim%2520R%25C3%25A4dsch%2520and%2520Annika%2520Reinke%2520and%2520Vivienn%2520Weru%2520and%2520Minu%2520D.%2520Tizabi%2520and%2520Nicholas%2520Heller%2520and%2520Fabian%2520Isensee%2520and%2520Annette%2520Kopp-Schneider%2520and%2520Lena%2520Maier-Hein%26entry.1292438233%3D%2520%2520This%2520paper%2520does%2520not%2520describe%2520a%2520novel%2520method.%2520Instead%252C%2520it%2520studies%2520an%2520essential%250Afoundation%2520for%2520reliable%2520benchmarking%2520and%2520ultimately%2520real-world%2520application%2520of%250AAI-based%2520image%2520analysis%253A%2520generating%2520high-quality%2520reference%2520annotations.%250APrevious%2520research%2520has%2520focused%2520on%2520crowdsourcing%2520as%2520a%2520means%2520of%2520outsourcing%250Aannotations.%2520However%252C%2520little%2520attention%2520has%2520so%2520far%2520been%2520given%2520to%2520annotation%250Acompanies%252C%2520specifically%2520regarding%2520their%2520internal%2520quality%2520assurance%2520%2528QA%2529%250Aprocesses.%2520Therefore%252C%2520our%2520aim%2520is%2520to%2520evaluate%2520the%2520influence%2520of%2520QA%2520employed%2520by%250Aannotation%2520companies%2520on%2520annotation%2520quality%2520and%2520devise%2520methodologies%2520for%250Amaximizing%2520data%2520annotation%2520efficacy.%2520Based%2520on%2520a%2520total%2520of%252057%252C648%2520instance%250Asegmented%2520images%2520obtained%2520from%2520a%2520total%2520of%2520924%2520annotators%2520and%252034%2520QA%2520workers%2520from%250Afour%2520annotation%2520companies%2520and%2520Amazon%2520Mechanical%2520Turk%2520%2528MTurk%2529%252C%2520we%2520derived%2520the%250Afollowing%2520insights%253A%2520%25281%2529%2520Annotation%2520companies%2520perform%2520better%2520both%2520in%2520terms%2520of%250Aquantity%2520and%2520quality%2520compared%2520to%2520the%2520widely%2520used%2520platform%2520MTurk.%2520%25282%2529%2520Annotation%250Acompanies%2527%2520internal%2520QA%2520only%2520provides%2520marginal%2520improvements%252C%2520if%2520any.%2520However%252C%250Aimproving%2520labeling%2520instructions%2520instead%2520of%2520investing%2520in%2520QA%2520can%2520substantially%250Aboost%2520annotation%2520performance.%2520%25283%2529%2520The%2520benefit%2520of%2520internal%2520QA%2520depends%2520on%250Aspecific%2520image%2520characteristics.%2520Our%2520work%2520could%2520enable%2520researchers%2520to%2520derive%250Asubstantially%2520more%2520value%2520from%2520a%2520fixed%2520annotation%2520budget%2520and%2520change%2520the%2520way%250Aannotation%2520companies%2520conduct%2520internal%2520QA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality%20Assured%3A%20Rethinking%20Annotation%20Strategies%20in%20Imaging%20AI&entry.906535625=Tim%20R%C3%A4dsch%20and%20Annika%20Reinke%20and%20Vivienn%20Weru%20and%20Minu%20D.%20Tizabi%20and%20Nicholas%20Heller%20and%20Fabian%20Isensee%20and%20Annette%20Kopp-Schneider%20and%20Lena%20Maier-Hein&entry.1292438233=%20%20This%20paper%20does%20not%20describe%20a%20novel%20method.%20Instead%2C%20it%20studies%20an%20essential%0Afoundation%20for%20reliable%20benchmarking%20and%20ultimately%20real-world%20application%20of%0AAI-based%20image%20analysis%3A%20generating%20high-quality%20reference%20annotations.%0APrevious%20research%20has%20focused%20on%20crowdsourcing%20as%20a%20means%20of%20outsourcing%0Aannotations.%20However%2C%20little%20attention%20has%20so%20far%20been%20given%20to%20annotation%0Acompanies%2C%20specifically%20regarding%20their%20internal%20quality%20assurance%20%28QA%29%0Aprocesses.%20Therefore%2C%20our%20aim%20is%20to%20evaluate%20the%20influence%20of%20QA%20employed%20by%0Aannotation%20companies%20on%20annotation%20quality%20and%20devise%20methodologies%20for%0Amaximizing%20data%20annotation%20efficacy.%20Based%20on%20a%20total%20of%2057%2C648%20instance%0Asegmented%20images%20obtained%20from%20a%20total%20of%20924%20annotators%20and%2034%20QA%20workers%20from%0Afour%20annotation%20companies%20and%20Amazon%20Mechanical%20Turk%20%28MTurk%29%2C%20we%20derived%20the%0Afollowing%20insights%3A%20%281%29%20Annotation%20companies%20perform%20better%20both%20in%20terms%20of%0Aquantity%20and%20quality%20compared%20to%20the%20widely%20used%20platform%20MTurk.%20%282%29%20Annotation%0Acompanies%27%20internal%20QA%20only%20provides%20marginal%20improvements%2C%20if%20any.%20However%2C%0Aimproving%20labeling%20instructions%20instead%20of%20investing%20in%20QA%20can%20substantially%0Aboost%20annotation%20performance.%20%283%29%20The%20benefit%20of%20internal%20QA%20depends%20on%0Aspecific%20image%20characteristics.%20Our%20work%20could%20enable%20researchers%20to%20derive%0Asubstantially%20more%20value%20from%20a%20fixed%20annotation%20budget%20and%20change%20the%20way%0Aannotation%20companies%20conduct%20internal%20QA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17596v2&entry.124074799=Read"},
{"title": "Semantic Prototypes: Enhancing Transparency Without Black Boxes", "author": "Orfeas Menis-Mastromichalakis and Giorgos Filandrianos and Jason Liartis and Edmund Dervakos and Giorgos Stamou", "abstract": "  As machine learning (ML) models and datasets increase in complexity, the\ndemand for methods that enhance explainability and interpretability becomes\nparamount. Prototypes, by encapsulating essential characteristics within data,\noffer insights that enable tactical decision-making and enhance transparency.\nTraditional prototype methods often rely on sub-symbolic raw data and opaque\nlatent spaces, reducing explainability and increasing the risk of\nmisinterpretations. This paper presents a novel framework that utilizes\nsemantic descriptions to define prototypes and provide clear explanations,\neffectively addressing the shortcomings of conventional methods. Our approach\nleverages concept-based descriptions to cluster data on the semantic level,\nensuring that prototypes not only represent underlying properties intuitively\nbut are also straightforward to interpret. Our method simplifies the\ninterpretative process and effectively bridges the gap between complex data\nstructures and human cognitive processes, thereby enhancing transparency and\nfostering trust. Our approach outperforms existing widely-used prototype\nmethods in facilitating human understanding and informativeness, as validated\nthrough a user survey.\n", "link": "http://arxiv.org/abs/2407.15871v2", "date": "2024-07-26", "relevancy": 1.9461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes&body=Title%3A%20Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes%0AAuthor%3A%20Orfeas%20Menis-Mastromichalakis%20and%20Giorgos%20Filandrianos%20and%20Jason%20Liartis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20As%20machine%20learning%20%28ML%29%20models%20and%20datasets%20increase%20in%20complexity%2C%20the%0Ademand%20for%20methods%20that%20enhance%20explainability%20and%20interpretability%20becomes%0Aparamount.%20Prototypes%2C%20by%20encapsulating%20essential%20characteristics%20within%20data%2C%0Aoffer%20insights%20that%20enable%20tactical%20decision-making%20and%20enhance%20transparency.%0ATraditional%20prototype%20methods%20often%20rely%20on%20sub-symbolic%20raw%20data%20and%20opaque%0Alatent%20spaces%2C%20reducing%20explainability%20and%20increasing%20the%20risk%20of%0Amisinterpretations.%20This%20paper%20presents%20a%20novel%20framework%20that%20utilizes%0Asemantic%20descriptions%20to%20define%20prototypes%20and%20provide%20clear%20explanations%2C%0Aeffectively%20addressing%20the%20shortcomings%20of%20conventional%20methods.%20Our%20approach%0Aleverages%20concept-based%20descriptions%20to%20cluster%20data%20on%20the%20semantic%20level%2C%0Aensuring%20that%20prototypes%20not%20only%20represent%20underlying%20properties%20intuitively%0Abut%20are%20also%20straightforward%20to%20interpret.%20Our%20method%20simplifies%20the%0Ainterpretative%20process%20and%20effectively%20bridges%20the%20gap%20between%20complex%20data%0Astructures%20and%20human%20cognitive%20processes%2C%20thereby%20enhancing%20transparency%20and%0Afostering%20trust.%20Our%20approach%20outperforms%20existing%20widely-used%20prototype%0Amethods%20in%20facilitating%20human%20understanding%20and%20informativeness%2C%20as%20validated%0Athrough%20a%20user%20survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Prototypes%253A%2520Enhancing%2520Transparency%2520Without%2520Black%2520Boxes%26entry.906535625%3DOrfeas%2520Menis-Mastromichalakis%2520and%2520Giorgos%2520Filandrianos%2520and%2520Jason%2520Liartis%2520and%2520Edmund%2520Dervakos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520%2528ML%2529%2520models%2520and%2520datasets%2520increase%2520in%2520complexity%252C%2520the%250Ademand%2520for%2520methods%2520that%2520enhance%2520explainability%2520and%2520interpretability%2520becomes%250Aparamount.%2520Prototypes%252C%2520by%2520encapsulating%2520essential%2520characteristics%2520within%2520data%252C%250Aoffer%2520insights%2520that%2520enable%2520tactical%2520decision-making%2520and%2520enhance%2520transparency.%250ATraditional%2520prototype%2520methods%2520often%2520rely%2520on%2520sub-symbolic%2520raw%2520data%2520and%2520opaque%250Alatent%2520spaces%252C%2520reducing%2520explainability%2520and%2520increasing%2520the%2520risk%2520of%250Amisinterpretations.%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520that%2520utilizes%250Asemantic%2520descriptions%2520to%2520define%2520prototypes%2520and%2520provide%2520clear%2520explanations%252C%250Aeffectively%2520addressing%2520the%2520shortcomings%2520of%2520conventional%2520methods.%2520Our%2520approach%250Aleverages%2520concept-based%2520descriptions%2520to%2520cluster%2520data%2520on%2520the%2520semantic%2520level%252C%250Aensuring%2520that%2520prototypes%2520not%2520only%2520represent%2520underlying%2520properties%2520intuitively%250Abut%2520are%2520also%2520straightforward%2520to%2520interpret.%2520Our%2520method%2520simplifies%2520the%250Ainterpretative%2520process%2520and%2520effectively%2520bridges%2520the%2520gap%2520between%2520complex%2520data%250Astructures%2520and%2520human%2520cognitive%2520processes%252C%2520thereby%2520enhancing%2520transparency%2520and%250Afostering%2520trust.%2520Our%2520approach%2520outperforms%2520existing%2520widely-used%2520prototype%250Amethods%2520in%2520facilitating%2520human%2520understanding%2520and%2520informativeness%252C%2520as%2520validated%250Athrough%2520a%2520user%2520survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Prototypes%3A%20Enhancing%20Transparency%20Without%20Black%20Boxes&entry.906535625=Orfeas%20Menis-Mastromichalakis%20and%20Giorgos%20Filandrianos%20and%20Jason%20Liartis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20As%20machine%20learning%20%28ML%29%20models%20and%20datasets%20increase%20in%20complexity%2C%20the%0Ademand%20for%20methods%20that%20enhance%20explainability%20and%20interpretability%20becomes%0Aparamount.%20Prototypes%2C%20by%20encapsulating%20essential%20characteristics%20within%20data%2C%0Aoffer%20insights%20that%20enable%20tactical%20decision-making%20and%20enhance%20transparency.%0ATraditional%20prototype%20methods%20often%20rely%20on%20sub-symbolic%20raw%20data%20and%20opaque%0Alatent%20spaces%2C%20reducing%20explainability%20and%20increasing%20the%20risk%20of%0Amisinterpretations.%20This%20paper%20presents%20a%20novel%20framework%20that%20utilizes%0Asemantic%20descriptions%20to%20define%20prototypes%20and%20provide%20clear%20explanations%2C%0Aeffectively%20addressing%20the%20shortcomings%20of%20conventional%20methods.%20Our%20approach%0Aleverages%20concept-based%20descriptions%20to%20cluster%20data%20on%20the%20semantic%20level%2C%0Aensuring%20that%20prototypes%20not%20only%20represent%20underlying%20properties%20intuitively%0Abut%20are%20also%20straightforward%20to%20interpret.%20Our%20method%20simplifies%20the%0Ainterpretative%20process%20and%20effectively%20bridges%20the%20gap%20between%20complex%20data%0Astructures%20and%20human%20cognitive%20processes%2C%20thereby%20enhancing%20transparency%20and%0Afostering%20trust.%20Our%20approach%20outperforms%20existing%20widely-used%20prototype%0Amethods%20in%20facilitating%20human%20understanding%20and%20informativeness%2C%20as%20validated%0Athrough%20a%20user%20survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15871v2&entry.124074799=Read"},
{"title": "Harnessing the Power of Large Language Models for Empathetic Response\n  Generation: Empirical Investigations and Improvements", "author": "Yushan Qian and Wei-Nan Zhang and Ting Liu", "abstract": "  Empathetic dialogue is an indispensable part of building harmonious social\nrelationships and contributes to the development of a helpful AI. Previous\napproaches are mainly based on fine small-scale language models. With the\nadvent of ChatGPT, the application effect of large language models (LLMs) in\nthis field has attracted great attention. This work empirically investigates\nthe performance of LLMs in generating empathetic responses and proposes three\nimprovement methods of semantically similar in-context learning, two-stage\ninteractive generation, and combination with the knowledge base. Extensive\nexperiments show that LLMs can significantly benefit from our proposed methods\nand is able to achieve state-of-the-art performance in both automatic and human\nevaluations. Additionally, we explore the possibility of GPT-4 simulating human\nevaluators.\n", "link": "http://arxiv.org/abs/2310.05140v4", "date": "2024-07-26", "relevancy": 1.944, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5164}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4708}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Power%20of%20Large%20Language%20Models%20for%20Empathetic%20Response%0A%20%20Generation%3A%20Empirical%20Investigations%20and%20Improvements&body=Title%3A%20Harnessing%20the%20Power%20of%20Large%20Language%20Models%20for%20Empathetic%20Response%0A%20%20Generation%3A%20Empirical%20Investigations%20and%20Improvements%0AAuthor%3A%20Yushan%20Qian%20and%20Wei-Nan%20Zhang%20and%20Ting%20Liu%0AAbstract%3A%20%20%20Empathetic%20dialogue%20is%20an%20indispensable%20part%20of%20building%20harmonious%20social%0Arelationships%20and%20contributes%20to%20the%20development%20of%20a%20helpful%20AI.%20Previous%0Aapproaches%20are%20mainly%20based%20on%20fine%20small-scale%20language%20models.%20With%20the%0Aadvent%20of%20ChatGPT%2C%20the%20application%20effect%20of%20large%20language%20models%20%28LLMs%29%20in%0Athis%20field%20has%20attracted%20great%20attention.%20This%20work%20empirically%20investigates%0Athe%20performance%20of%20LLMs%20in%20generating%20empathetic%20responses%20and%20proposes%20three%0Aimprovement%20methods%20of%20semantically%20similar%20in-context%20learning%2C%20two-stage%0Ainteractive%20generation%2C%20and%20combination%20with%20the%20knowledge%20base.%20Extensive%0Aexperiments%20show%20that%20LLMs%20can%20significantly%20benefit%20from%20our%20proposed%20methods%0Aand%20is%20able%20to%20achieve%20state-of-the-art%20performance%20in%20both%20automatic%20and%20human%0Aevaluations.%20Additionally%2C%20we%20explore%20the%20possibility%20of%20GPT-4%20simulating%20human%0Aevaluators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05140v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Power%2520of%2520Large%2520Language%2520Models%2520for%2520Empathetic%2520Response%250A%2520%2520Generation%253A%2520Empirical%2520Investigations%2520and%2520Improvements%26entry.906535625%3DYushan%2520Qian%2520and%2520Wei-Nan%2520Zhang%2520and%2520Ting%2520Liu%26entry.1292438233%3D%2520%2520Empathetic%2520dialogue%2520is%2520an%2520indispensable%2520part%2520of%2520building%2520harmonious%2520social%250Arelationships%2520and%2520contributes%2520to%2520the%2520development%2520of%2520a%2520helpful%2520AI.%2520Previous%250Aapproaches%2520are%2520mainly%2520based%2520on%2520fine%2520small-scale%2520language%2520models.%2520With%2520the%250Aadvent%2520of%2520ChatGPT%252C%2520the%2520application%2520effect%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%250Athis%2520field%2520has%2520attracted%2520great%2520attention.%2520This%2520work%2520empirically%2520investigates%250Athe%2520performance%2520of%2520LLMs%2520in%2520generating%2520empathetic%2520responses%2520and%2520proposes%2520three%250Aimprovement%2520methods%2520of%2520semantically%2520similar%2520in-context%2520learning%252C%2520two-stage%250Ainteractive%2520generation%252C%2520and%2520combination%2520with%2520the%2520knowledge%2520base.%2520Extensive%250Aexperiments%2520show%2520that%2520LLMs%2520can%2520significantly%2520benefit%2520from%2520our%2520proposed%2520methods%250Aand%2520is%2520able%2520to%2520achieve%2520state-of-the-art%2520performance%2520in%2520both%2520automatic%2520and%2520human%250Aevaluations.%2520Additionally%252C%2520we%2520explore%2520the%2520possibility%2520of%2520GPT-4%2520simulating%2520human%250Aevaluators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05140v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Power%20of%20Large%20Language%20Models%20for%20Empathetic%20Response%0A%20%20Generation%3A%20Empirical%20Investigations%20and%20Improvements&entry.906535625=Yushan%20Qian%20and%20Wei-Nan%20Zhang%20and%20Ting%20Liu&entry.1292438233=%20%20Empathetic%20dialogue%20is%20an%20indispensable%20part%20of%20building%20harmonious%20social%0Arelationships%20and%20contributes%20to%20the%20development%20of%20a%20helpful%20AI.%20Previous%0Aapproaches%20are%20mainly%20based%20on%20fine%20small-scale%20language%20models.%20With%20the%0Aadvent%20of%20ChatGPT%2C%20the%20application%20effect%20of%20large%20language%20models%20%28LLMs%29%20in%0Athis%20field%20has%20attracted%20great%20attention.%20This%20work%20empirically%20investigates%0Athe%20performance%20of%20LLMs%20in%20generating%20empathetic%20responses%20and%20proposes%20three%0Aimprovement%20methods%20of%20semantically%20similar%20in-context%20learning%2C%20two-stage%0Ainteractive%20generation%2C%20and%20combination%20with%20the%20knowledge%20base.%20Extensive%0Aexperiments%20show%20that%20LLMs%20can%20significantly%20benefit%20from%20our%20proposed%20methods%0Aand%20is%20able%20to%20achieve%20state-of-the-art%20performance%20in%20both%20automatic%20and%20human%0Aevaluations.%20Additionally%2C%20we%20explore%20the%20possibility%20of%20GPT-4%20simulating%20human%0Aevaluators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05140v4&entry.124074799=Read"},
{"title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks:\n  The Narrow Width Limit", "author": "Zechen Zhang and Haim Sompolinsky", "abstract": "  The infinite width limit of random neural networks is known to result in\nNeural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized\nby task-independent kernels. It is widely accepted that larger network widths\ncontribute to improved generalization (Park et al. [2019]). However, this work\nchallenges this notion by investigating the narrow width limit of the Bayesian\nParallel Branching Graph Neural Network (BPB-GNN), an architecture that\nresembles residual networks. We demonstrate that when the width of a BPB-GNN is\nsignificantly smaller compared to the number of training examples, each branch\nexhibits more robust learning due to a symmetry breaking of branches in kernel\nrenormalization. Surprisingly, the performance of a BPB-GNN in the narrow width\nlimit is generally superior or comparable to that achieved in the wide width\nlimit in bias-limited scenarios. Furthermore, the readout norms of each branch\nin the narrow width limit are mostly independent of the architectural\nhyperparameters but generally reflective of the nature of the data. Our results\ncharacterize a newly defined narrow-width regime for parallel branching\nnetworks in general.\n", "link": "http://arxiv.org/abs/2407.18807v1", "date": "2024-07-26", "relevancy": 1.943, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Learning%20in%20Bayesian%20Parallel%20Branching%20Graph%20Neural%20Networks%3A%0A%20%20The%20Narrow%20Width%20Limit&body=Title%3A%20Robust%20Learning%20in%20Bayesian%20Parallel%20Branching%20Graph%20Neural%20Networks%3A%0A%20%20The%20Narrow%20Width%20Limit%0AAuthor%3A%20Zechen%20Zhang%20and%20Haim%20Sompolinsky%0AAbstract%3A%20%20%20The%20infinite%20width%20limit%20of%20random%20neural%20networks%20is%20known%20to%20result%20in%0ANeural%20Networks%20as%20Gaussian%20Process%20%28NNGP%29%20%28Lee%20et%20al.%20%5B2018%5D%29%2C%20characterized%0Aby%20task-independent%20kernels.%20It%20is%20widely%20accepted%20that%20larger%20network%20widths%0Acontribute%20to%20improved%20generalization%20%28Park%20et%20al.%20%5B2019%5D%29.%20However%2C%20this%20work%0Achallenges%20this%20notion%20by%20investigating%20the%20narrow%20width%20limit%20of%20the%20Bayesian%0AParallel%20Branching%20Graph%20Neural%20Network%20%28BPB-GNN%29%2C%20an%20architecture%20that%0Aresembles%20residual%20networks.%20We%20demonstrate%20that%20when%20the%20width%20of%20a%20BPB-GNN%20is%0Asignificantly%20smaller%20compared%20to%20the%20number%20of%20training%20examples%2C%20each%20branch%0Aexhibits%20more%20robust%20learning%20due%20to%20a%20symmetry%20breaking%20of%20branches%20in%20kernel%0Arenormalization.%20Surprisingly%2C%20the%20performance%20of%20a%20BPB-GNN%20in%20the%20narrow%20width%0Alimit%20is%20generally%20superior%20or%20comparable%20to%20that%20achieved%20in%20the%20wide%20width%0Alimit%20in%20bias-limited%20scenarios.%20Furthermore%2C%20the%20readout%20norms%20of%20each%20branch%0Ain%20the%20narrow%20width%20limit%20are%20mostly%20independent%20of%20the%20architectural%0Ahyperparameters%20but%20generally%20reflective%20of%20the%20nature%20of%20the%20data.%20Our%20results%0Acharacterize%20a%20newly%20defined%20narrow-width%20regime%20for%20parallel%20branching%0Anetworks%20in%20general.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Learning%2520in%2520Bayesian%2520Parallel%2520Branching%2520Graph%2520Neural%2520Networks%253A%250A%2520%2520The%2520Narrow%2520Width%2520Limit%26entry.906535625%3DZechen%2520Zhang%2520and%2520Haim%2520Sompolinsky%26entry.1292438233%3D%2520%2520The%2520infinite%2520width%2520limit%2520of%2520random%2520neural%2520networks%2520is%2520known%2520to%2520result%2520in%250ANeural%2520Networks%2520as%2520Gaussian%2520Process%2520%2528NNGP%2529%2520%2528Lee%2520et%2520al.%2520%255B2018%255D%2529%252C%2520characterized%250Aby%2520task-independent%2520kernels.%2520It%2520is%2520widely%2520accepted%2520that%2520larger%2520network%2520widths%250Acontribute%2520to%2520improved%2520generalization%2520%2528Park%2520et%2520al.%2520%255B2019%255D%2529.%2520However%252C%2520this%2520work%250Achallenges%2520this%2520notion%2520by%2520investigating%2520the%2520narrow%2520width%2520limit%2520of%2520the%2520Bayesian%250AParallel%2520Branching%2520Graph%2520Neural%2520Network%2520%2528BPB-GNN%2529%252C%2520an%2520architecture%2520that%250Aresembles%2520residual%2520networks.%2520We%2520demonstrate%2520that%2520when%2520the%2520width%2520of%2520a%2520BPB-GNN%2520is%250Asignificantly%2520smaller%2520compared%2520to%2520the%2520number%2520of%2520training%2520examples%252C%2520each%2520branch%250Aexhibits%2520more%2520robust%2520learning%2520due%2520to%2520a%2520symmetry%2520breaking%2520of%2520branches%2520in%2520kernel%250Arenormalization.%2520Surprisingly%252C%2520the%2520performance%2520of%2520a%2520BPB-GNN%2520in%2520the%2520narrow%2520width%250Alimit%2520is%2520generally%2520superior%2520or%2520comparable%2520to%2520that%2520achieved%2520in%2520the%2520wide%2520width%250Alimit%2520in%2520bias-limited%2520scenarios.%2520Furthermore%252C%2520the%2520readout%2520norms%2520of%2520each%2520branch%250Ain%2520the%2520narrow%2520width%2520limit%2520are%2520mostly%2520independent%2520of%2520the%2520architectural%250Ahyperparameters%2520but%2520generally%2520reflective%2520of%2520the%2520nature%2520of%2520the%2520data.%2520Our%2520results%250Acharacterize%2520a%2520newly%2520defined%2520narrow-width%2520regime%2520for%2520parallel%2520branching%250Anetworks%2520in%2520general.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Learning%20in%20Bayesian%20Parallel%20Branching%20Graph%20Neural%20Networks%3A%0A%20%20The%20Narrow%20Width%20Limit&entry.906535625=Zechen%20Zhang%20and%20Haim%20Sompolinsky&entry.1292438233=%20%20The%20infinite%20width%20limit%20of%20random%20neural%20networks%20is%20known%20to%20result%20in%0ANeural%20Networks%20as%20Gaussian%20Process%20%28NNGP%29%20%28Lee%20et%20al.%20%5B2018%5D%29%2C%20characterized%0Aby%20task-independent%20kernels.%20It%20is%20widely%20accepted%20that%20larger%20network%20widths%0Acontribute%20to%20improved%20generalization%20%28Park%20et%20al.%20%5B2019%5D%29.%20However%2C%20this%20work%0Achallenges%20this%20notion%20by%20investigating%20the%20narrow%20width%20limit%20of%20the%20Bayesian%0AParallel%20Branching%20Graph%20Neural%20Network%20%28BPB-GNN%29%2C%20an%20architecture%20that%0Aresembles%20residual%20networks.%20We%20demonstrate%20that%20when%20the%20width%20of%20a%20BPB-GNN%20is%0Asignificantly%20smaller%20compared%20to%20the%20number%20of%20training%20examples%2C%20each%20branch%0Aexhibits%20more%20robust%20learning%20due%20to%20a%20symmetry%20breaking%20of%20branches%20in%20kernel%0Arenormalization.%20Surprisingly%2C%20the%20performance%20of%20a%20BPB-GNN%20in%20the%20narrow%20width%0Alimit%20is%20generally%20superior%20or%20comparable%20to%20that%20achieved%20in%20the%20wide%20width%0Alimit%20in%20bias-limited%20scenarios.%20Furthermore%2C%20the%20readout%20norms%20of%20each%20branch%0Ain%20the%20narrow%20width%20limit%20are%20mostly%20independent%20of%20the%20architectural%0Ahyperparameters%20but%20generally%20reflective%20of%20the%20nature%20of%20the%20data.%20Our%20results%0Acharacterize%20a%20newly%20defined%20narrow-width%20regime%20for%20parallel%20branching%0Anetworks%20in%20general.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18807v1&entry.124074799=Read"},
{"title": "Breaking the Global North Stereotype: A Global South-centric Benchmark\n  Dataset for Auditing and Mitigating Biases in Facial Recognition Systems", "author": "Siddharth D Jaiswal and Animesh Ganai and Abhisek Dash and Saptarshi Ghosh and Animesh Mukherjee", "abstract": "  Facial Recognition Systems (FRSs) are being developed and deployed globally\nat unprecedented rates. Most platforms are designed in a limited set of\ncountries but deployed in worldwide, without adequate checkpoints. This is\nespecially problematic for Global South countries which lack strong legislation\nto safeguard persons facing disparate performance of these systems. A\ncombination of unavailability of datasets, lack of understanding of FRS\nfunctionality and low-resource bias mitigation measures accentuate the problem.\nIn this work, we propose a new face dataset composed of 6,579 unique male and\nfemale sportspersons from eight countries around the world. More than 50% of\nthe dataset comprises individuals from the Global South countries and is\ndemographically diverse. To aid adversarial audits and robust model training,\neach image has four adversarial variants, totaling over 40,000 images. We also\nbenchmark five popular FRSs, both commercial and open-source, for the task of\ngender prediction (and country prediction for one of the open-source models as\nan example of red-teaming). Experiments on industrial FRSs reveal accuracies\nranging from 98.2%--38.1%, with a large disparity between males and females in\nthe Global South (max difference of 38.5%). Biases are also observed in all\nFRSs between females of the Global North and South (max difference of ~50%).\nGrad-CAM analysis identifies the nose, forehead and mouth as the regions of\ninterest on one of the open-source FRSs. Utilizing this insight, we design\nsimple, low-resource bias mitigation solutions using few-shot and novel\ncontrastive learning techniques significantly improving the accuracy with\ndisparity between males and females reducing from 50% to 1.5% in one of the\nsettings. In the red-teaming experiment with the open-source Deepface model,\ncontrastive learning proves more effective than simple fine-tuning.\n", "link": "http://arxiv.org/abs/2407.15810v2", "date": "2024-07-26", "relevancy": 1.9369, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4927}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4912}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Global%20North%20Stereotype%3A%20A%20Global%20South-centric%20Benchmark%0A%20%20Dataset%20for%20Auditing%20and%20Mitigating%20Biases%20in%20Facial%20Recognition%20Systems&body=Title%3A%20Breaking%20the%20Global%20North%20Stereotype%3A%20A%20Global%20South-centric%20Benchmark%0A%20%20Dataset%20for%20Auditing%20and%20Mitigating%20Biases%20in%20Facial%20Recognition%20Systems%0AAuthor%3A%20Siddharth%20D%20Jaiswal%20and%20Animesh%20Ganai%20and%20Abhisek%20Dash%20and%20Saptarshi%20Ghosh%20and%20Animesh%20Mukherjee%0AAbstract%3A%20%20%20Facial%20Recognition%20Systems%20%28FRSs%29%20are%20being%20developed%20and%20deployed%20globally%0Aat%20unprecedented%20rates.%20Most%20platforms%20are%20designed%20in%20a%20limited%20set%20of%0Acountries%20but%20deployed%20in%20worldwide%2C%20without%20adequate%20checkpoints.%20This%20is%0Aespecially%20problematic%20for%20Global%20South%20countries%20which%20lack%20strong%20legislation%0Ato%20safeguard%20persons%20facing%20disparate%20performance%20of%20these%20systems.%20A%0Acombination%20of%20unavailability%20of%20datasets%2C%20lack%20of%20understanding%20of%20FRS%0Afunctionality%20and%20low-resource%20bias%20mitigation%20measures%20accentuate%20the%20problem.%0AIn%20this%20work%2C%20we%20propose%20a%20new%20face%20dataset%20composed%20of%206%2C579%20unique%20male%20and%0Afemale%20sportspersons%20from%20eight%20countries%20around%20the%20world.%20More%20than%2050%25%20of%0Athe%20dataset%20comprises%20individuals%20from%20the%20Global%20South%20countries%20and%20is%0Ademographically%20diverse.%20To%20aid%20adversarial%20audits%20and%20robust%20model%20training%2C%0Aeach%20image%20has%20four%20adversarial%20variants%2C%20totaling%20over%2040%2C000%20images.%20We%20also%0Abenchmark%20five%20popular%20FRSs%2C%20both%20commercial%20and%20open-source%2C%20for%20the%20task%20of%0Agender%20prediction%20%28and%20country%20prediction%20for%20one%20of%20the%20open-source%20models%20as%0Aan%20example%20of%20red-teaming%29.%20Experiments%20on%20industrial%20FRSs%20reveal%20accuracies%0Aranging%20from%2098.2%25--38.1%25%2C%20with%20a%20large%20disparity%20between%20males%20and%20females%20in%0Athe%20Global%20South%20%28max%20difference%20of%2038.5%25%29.%20Biases%20are%20also%20observed%20in%20all%0AFRSs%20between%20females%20of%20the%20Global%20North%20and%20South%20%28max%20difference%20of%20~50%25%29.%0AGrad-CAM%20analysis%20identifies%20the%20nose%2C%20forehead%20and%20mouth%20as%20the%20regions%20of%0Ainterest%20on%20one%20of%20the%20open-source%20FRSs.%20Utilizing%20this%20insight%2C%20we%20design%0Asimple%2C%20low-resource%20bias%20mitigation%20solutions%20using%20few-shot%20and%20novel%0Acontrastive%20learning%20techniques%20significantly%20improving%20the%20accuracy%20with%0Adisparity%20between%20males%20and%20females%20reducing%20from%2050%25%20to%201.5%25%20in%20one%20of%20the%0Asettings.%20In%20the%20red-teaming%20experiment%20with%20the%20open-source%20Deepface%20model%2C%0Acontrastive%20learning%20proves%20more%20effective%20than%20simple%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Global%2520North%2520Stereotype%253A%2520A%2520Global%2520South-centric%2520Benchmark%250A%2520%2520Dataset%2520for%2520Auditing%2520and%2520Mitigating%2520Biases%2520in%2520Facial%2520Recognition%2520Systems%26entry.906535625%3DSiddharth%2520D%2520Jaiswal%2520and%2520Animesh%2520Ganai%2520and%2520Abhisek%2520Dash%2520and%2520Saptarshi%2520Ghosh%2520and%2520Animesh%2520Mukherjee%26entry.1292438233%3D%2520%2520Facial%2520Recognition%2520Systems%2520%2528FRSs%2529%2520are%2520being%2520developed%2520and%2520deployed%2520globally%250Aat%2520unprecedented%2520rates.%2520Most%2520platforms%2520are%2520designed%2520in%2520a%2520limited%2520set%2520of%250Acountries%2520but%2520deployed%2520in%2520worldwide%252C%2520without%2520adequate%2520checkpoints.%2520This%2520is%250Aespecially%2520problematic%2520for%2520Global%2520South%2520countries%2520which%2520lack%2520strong%2520legislation%250Ato%2520safeguard%2520persons%2520facing%2520disparate%2520performance%2520of%2520these%2520systems.%2520A%250Acombination%2520of%2520unavailability%2520of%2520datasets%252C%2520lack%2520of%2520understanding%2520of%2520FRS%250Afunctionality%2520and%2520low-resource%2520bias%2520mitigation%2520measures%2520accentuate%2520the%2520problem.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520face%2520dataset%2520composed%2520of%25206%252C579%2520unique%2520male%2520and%250Afemale%2520sportspersons%2520from%2520eight%2520countries%2520around%2520the%2520world.%2520More%2520than%252050%2525%2520of%250Athe%2520dataset%2520comprises%2520individuals%2520from%2520the%2520Global%2520South%2520countries%2520and%2520is%250Ademographically%2520diverse.%2520To%2520aid%2520adversarial%2520audits%2520and%2520robust%2520model%2520training%252C%250Aeach%2520image%2520has%2520four%2520adversarial%2520variants%252C%2520totaling%2520over%252040%252C000%2520images.%2520We%2520also%250Abenchmark%2520five%2520popular%2520FRSs%252C%2520both%2520commercial%2520and%2520open-source%252C%2520for%2520the%2520task%2520of%250Agender%2520prediction%2520%2528and%2520country%2520prediction%2520for%2520one%2520of%2520the%2520open-source%2520models%2520as%250Aan%2520example%2520of%2520red-teaming%2529.%2520Experiments%2520on%2520industrial%2520FRSs%2520reveal%2520accuracies%250Aranging%2520from%252098.2%2525--38.1%2525%252C%2520with%2520a%2520large%2520disparity%2520between%2520males%2520and%2520females%2520in%250Athe%2520Global%2520South%2520%2528max%2520difference%2520of%252038.5%2525%2529.%2520Biases%2520are%2520also%2520observed%2520in%2520all%250AFRSs%2520between%2520females%2520of%2520the%2520Global%2520North%2520and%2520South%2520%2528max%2520difference%2520of%2520~50%2525%2529.%250AGrad-CAM%2520analysis%2520identifies%2520the%2520nose%252C%2520forehead%2520and%2520mouth%2520as%2520the%2520regions%2520of%250Ainterest%2520on%2520one%2520of%2520the%2520open-source%2520FRSs.%2520Utilizing%2520this%2520insight%252C%2520we%2520design%250Asimple%252C%2520low-resource%2520bias%2520mitigation%2520solutions%2520using%2520few-shot%2520and%2520novel%250Acontrastive%2520learning%2520techniques%2520significantly%2520improving%2520the%2520accuracy%2520with%250Adisparity%2520between%2520males%2520and%2520females%2520reducing%2520from%252050%2525%2520to%25201.5%2525%2520in%2520one%2520of%2520the%250Asettings.%2520In%2520the%2520red-teaming%2520experiment%2520with%2520the%2520open-source%2520Deepface%2520model%252C%250Acontrastive%2520learning%2520proves%2520more%2520effective%2520than%2520simple%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Global%20North%20Stereotype%3A%20A%20Global%20South-centric%20Benchmark%0A%20%20Dataset%20for%20Auditing%20and%20Mitigating%20Biases%20in%20Facial%20Recognition%20Systems&entry.906535625=Siddharth%20D%20Jaiswal%20and%20Animesh%20Ganai%20and%20Abhisek%20Dash%20and%20Saptarshi%20Ghosh%20and%20Animesh%20Mukherjee&entry.1292438233=%20%20Facial%20Recognition%20Systems%20%28FRSs%29%20are%20being%20developed%20and%20deployed%20globally%0Aat%20unprecedented%20rates.%20Most%20platforms%20are%20designed%20in%20a%20limited%20set%20of%0Acountries%20but%20deployed%20in%20worldwide%2C%20without%20adequate%20checkpoints.%20This%20is%0Aespecially%20problematic%20for%20Global%20South%20countries%20which%20lack%20strong%20legislation%0Ato%20safeguard%20persons%20facing%20disparate%20performance%20of%20these%20systems.%20A%0Acombination%20of%20unavailability%20of%20datasets%2C%20lack%20of%20understanding%20of%20FRS%0Afunctionality%20and%20low-resource%20bias%20mitigation%20measures%20accentuate%20the%20problem.%0AIn%20this%20work%2C%20we%20propose%20a%20new%20face%20dataset%20composed%20of%206%2C579%20unique%20male%20and%0Afemale%20sportspersons%20from%20eight%20countries%20around%20the%20world.%20More%20than%2050%25%20of%0Athe%20dataset%20comprises%20individuals%20from%20the%20Global%20South%20countries%20and%20is%0Ademographically%20diverse.%20To%20aid%20adversarial%20audits%20and%20robust%20model%20training%2C%0Aeach%20image%20has%20four%20adversarial%20variants%2C%20totaling%20over%2040%2C000%20images.%20We%20also%0Abenchmark%20five%20popular%20FRSs%2C%20both%20commercial%20and%20open-source%2C%20for%20the%20task%20of%0Agender%20prediction%20%28and%20country%20prediction%20for%20one%20of%20the%20open-source%20models%20as%0Aan%20example%20of%20red-teaming%29.%20Experiments%20on%20industrial%20FRSs%20reveal%20accuracies%0Aranging%20from%2098.2%25--38.1%25%2C%20with%20a%20large%20disparity%20between%20males%20and%20females%20in%0Athe%20Global%20South%20%28max%20difference%20of%2038.5%25%29.%20Biases%20are%20also%20observed%20in%20all%0AFRSs%20between%20females%20of%20the%20Global%20North%20and%20South%20%28max%20difference%20of%20~50%25%29.%0AGrad-CAM%20analysis%20identifies%20the%20nose%2C%20forehead%20and%20mouth%20as%20the%20regions%20of%0Ainterest%20on%20one%20of%20the%20open-source%20FRSs.%20Utilizing%20this%20insight%2C%20we%20design%0Asimple%2C%20low-resource%20bias%20mitigation%20solutions%20using%20few-shot%20and%20novel%0Acontrastive%20learning%20techniques%20significantly%20improving%20the%20accuracy%20with%0Adisparity%20between%20males%20and%20females%20reducing%20from%2050%25%20to%201.5%25%20in%20one%20of%20the%0Asettings.%20In%20the%20red-teaming%20experiment%20with%20the%20open-source%20Deepface%20model%2C%0Acontrastive%20learning%20proves%20more%20effective%20than%20simple%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15810v2&entry.124074799=Read"},
{"title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning", "author": "Michael F\u00e4rber and David Lamprecht and Yuni Susanti", "abstract": "  In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF\ndata into data representations tailored for graph machine learning tasks.\nAutoRDF2GML enables, for the first time, the creation of both content-based\nfeatures -- i.e., features based on RDF datatype properties -- and\ntopology-based features -- i.e., features based on RDF object properties.\nCharacterized by automated feature extraction, AutoRDF2GML makes it possible\neven for users less familiar with RDF and SPARQL to generate data\nrepresentations ready for graph machine learning tasks, such as link\nprediction, node classification, and graph classification. Furthermore, we\npresent four new benchmark datasets for graph machine learning, created from\nlarge RDF knowledge graphs using our framework. These datasets serve as\nvaluable resources for evaluating graph machine learning approaches, such as\ngraph neural networks. Overall, our framework effectively bridges the gap\nbetween the Graph Machine Learning and Semantic Web communities, paving the way\nfor RDF-based machine learning applications.\n", "link": "http://arxiv.org/abs/2407.18735v1", "date": "2024-07-26", "relevancy": 1.9367, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoRDF2GML%3A%20Facilitating%20RDF%20Integration%20in%20Graph%20Machine%20Learning&body=Title%3A%20AutoRDF2GML%3A%20Facilitating%20RDF%20Integration%20in%20Graph%20Machine%20Learning%0AAuthor%3A%20Michael%20F%C3%A4rber%20and%20David%20Lamprecht%20and%20Yuni%20Susanti%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20AutoRDF2GML%2C%20a%20framework%20designed%20to%20convert%20RDF%0Adata%20into%20data%20representations%20tailored%20for%20graph%20machine%20learning%20tasks.%0AAutoRDF2GML%20enables%2C%20for%20the%20first%20time%2C%20the%20creation%20of%20both%20content-based%0Afeatures%20--%20i.e.%2C%20features%20based%20on%20RDF%20datatype%20properties%20--%20and%0Atopology-based%20features%20--%20i.e.%2C%20features%20based%20on%20RDF%20object%20properties.%0ACharacterized%20by%20automated%20feature%20extraction%2C%20AutoRDF2GML%20makes%20it%20possible%0Aeven%20for%20users%20less%20familiar%20with%20RDF%20and%20SPARQL%20to%20generate%20data%0Arepresentations%20ready%20for%20graph%20machine%20learning%20tasks%2C%20such%20as%20link%0Aprediction%2C%20node%20classification%2C%20and%20graph%20classification.%20Furthermore%2C%20we%0Apresent%20four%20new%20benchmark%20datasets%20for%20graph%20machine%20learning%2C%20created%20from%0Alarge%20RDF%20knowledge%20graphs%20using%20our%20framework.%20These%20datasets%20serve%20as%0Avaluable%20resources%20for%20evaluating%20graph%20machine%20learning%20approaches%2C%20such%20as%0Agraph%20neural%20networks.%20Overall%2C%20our%20framework%20effectively%20bridges%20the%20gap%0Abetween%20the%20Graph%20Machine%20Learning%20and%20Semantic%20Web%20communities%2C%20paving%20the%20way%0Afor%20RDF-based%20machine%20learning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoRDF2GML%253A%2520Facilitating%2520RDF%2520Integration%2520in%2520Graph%2520Machine%2520Learning%26entry.906535625%3DMichael%2520F%25C3%25A4rber%2520and%2520David%2520Lamprecht%2520and%2520Yuni%2520Susanti%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AutoRDF2GML%252C%2520a%2520framework%2520designed%2520to%2520convert%2520RDF%250Adata%2520into%2520data%2520representations%2520tailored%2520for%2520graph%2520machine%2520learning%2520tasks.%250AAutoRDF2GML%2520enables%252C%2520for%2520the%2520first%2520time%252C%2520the%2520creation%2520of%2520both%2520content-based%250Afeatures%2520--%2520i.e.%252C%2520features%2520based%2520on%2520RDF%2520datatype%2520properties%2520--%2520and%250Atopology-based%2520features%2520--%2520i.e.%252C%2520features%2520based%2520on%2520RDF%2520object%2520properties.%250ACharacterized%2520by%2520automated%2520feature%2520extraction%252C%2520AutoRDF2GML%2520makes%2520it%2520possible%250Aeven%2520for%2520users%2520less%2520familiar%2520with%2520RDF%2520and%2520SPARQL%2520to%2520generate%2520data%250Arepresentations%2520ready%2520for%2520graph%2520machine%2520learning%2520tasks%252C%2520such%2520as%2520link%250Aprediction%252C%2520node%2520classification%252C%2520and%2520graph%2520classification.%2520Furthermore%252C%2520we%250Apresent%2520four%2520new%2520benchmark%2520datasets%2520for%2520graph%2520machine%2520learning%252C%2520created%2520from%250Alarge%2520RDF%2520knowledge%2520graphs%2520using%2520our%2520framework.%2520These%2520datasets%2520serve%2520as%250Avaluable%2520resources%2520for%2520evaluating%2520graph%2520machine%2520learning%2520approaches%252C%2520such%2520as%250Agraph%2520neural%2520networks.%2520Overall%252C%2520our%2520framework%2520effectively%2520bridges%2520the%2520gap%250Abetween%2520the%2520Graph%2520Machine%2520Learning%2520and%2520Semantic%2520Web%2520communities%252C%2520paving%2520the%2520way%250Afor%2520RDF-based%2520machine%2520learning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoRDF2GML%3A%20Facilitating%20RDF%20Integration%20in%20Graph%20Machine%20Learning&entry.906535625=Michael%20F%C3%A4rber%20and%20David%20Lamprecht%20and%20Yuni%20Susanti&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20AutoRDF2GML%2C%20a%20framework%20designed%20to%20convert%20RDF%0Adata%20into%20data%20representations%20tailored%20for%20graph%20machine%20learning%20tasks.%0AAutoRDF2GML%20enables%2C%20for%20the%20first%20time%2C%20the%20creation%20of%20both%20content-based%0Afeatures%20--%20i.e.%2C%20features%20based%20on%20RDF%20datatype%20properties%20--%20and%0Atopology-based%20features%20--%20i.e.%2C%20features%20based%20on%20RDF%20object%20properties.%0ACharacterized%20by%20automated%20feature%20extraction%2C%20AutoRDF2GML%20makes%20it%20possible%0Aeven%20for%20users%20less%20familiar%20with%20RDF%20and%20SPARQL%20to%20generate%20data%0Arepresentations%20ready%20for%20graph%20machine%20learning%20tasks%2C%20such%20as%20link%0Aprediction%2C%20node%20classification%2C%20and%20graph%20classification.%20Furthermore%2C%20we%0Apresent%20four%20new%20benchmark%20datasets%20for%20graph%20machine%20learning%2C%20created%20from%0Alarge%20RDF%20knowledge%20graphs%20using%20our%20framework.%20These%20datasets%20serve%20as%0Avaluable%20resources%20for%20evaluating%20graph%20machine%20learning%20approaches%2C%20such%20as%0Agraph%20neural%20networks.%20Overall%2C%20our%20framework%20effectively%20bridges%20the%20gap%0Abetween%20the%20Graph%20Machine%20Learning%20and%20Semantic%20Web%20communities%2C%20paving%20the%20way%0Afor%20RDF-based%20machine%20learning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18735v1&entry.124074799=Read"},
{"title": "A Minimum-Jerk Approach to Handle Singularities in Virtual Fixtures", "author": "Giovanni Braglia and Sylvain Calinon and Luigi Biagiotti", "abstract": "  Implementing virtual fixtures in guiding tasks constrains the movement of the\nrobot's end effector to specific curves within its workspace. However,\nincorporating guiding frameworks may encounter discontinuities when optimizing\nthe reference target position to the nearest point relative to the current\nrobot position. This article aims to give a geometric interpretation of such\ndiscontinuities, with specific reference to the commonly adopted Gauss-Newton\nalgorithm. The effect of such discontinuities, defined as Euclidean Distance\nSingularities, is experimentally proved. We then propose a solution that is\nbased on a Linear Quadratic Tracking problem with minimum jerk command, then\ncompare and validate the performances of the proposed framework in two\ndifferent human-robot interaction scenarios.\n", "link": "http://arxiv.org/abs/2405.03473v2", "date": "2024-07-26", "relevancy": 1.9317, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4896}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures&body=Title%3A%20A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures%0AAuthor%3A%20Giovanni%20Braglia%20and%20Sylvain%20Calinon%20and%20Luigi%20Biagiotti%0AAbstract%3A%20%20%20Implementing%20virtual%20fixtures%20in%20guiding%20tasks%20constrains%20the%20movement%20of%20the%0Arobot%27s%20end%20effector%20to%20specific%20curves%20within%20its%20workspace.%20However%2C%0Aincorporating%20guiding%20frameworks%20may%20encounter%20discontinuities%20when%20optimizing%0Athe%20reference%20target%20position%20to%20the%20nearest%20point%20relative%20to%20the%20current%0Arobot%20position.%20This%20article%20aims%20to%20give%20a%20geometric%20interpretation%20of%20such%0Adiscontinuities%2C%20with%20specific%20reference%20to%20the%20commonly%20adopted%20Gauss-Newton%0Aalgorithm.%20The%20effect%20of%20such%20discontinuities%2C%20defined%20as%20Euclidean%20Distance%0ASingularities%2C%20is%20experimentally%20proved.%20We%20then%20propose%20a%20solution%20that%20is%0Abased%20on%20a%20Linear%20Quadratic%20Tracking%20problem%20with%20minimum%20jerk%20command%2C%20then%0Acompare%20and%20validate%20the%20performances%20of%20the%20proposed%20framework%20in%20two%0Adifferent%20human-robot%20interaction%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Minimum-Jerk%2520Approach%2520to%2520Handle%2520Singularities%2520in%2520Virtual%2520Fixtures%26entry.906535625%3DGiovanni%2520Braglia%2520and%2520Sylvain%2520Calinon%2520and%2520Luigi%2520Biagiotti%26entry.1292438233%3D%2520%2520Implementing%2520virtual%2520fixtures%2520in%2520guiding%2520tasks%2520constrains%2520the%2520movement%2520of%2520the%250Arobot%2527s%2520end%2520effector%2520to%2520specific%2520curves%2520within%2520its%2520workspace.%2520However%252C%250Aincorporating%2520guiding%2520frameworks%2520may%2520encounter%2520discontinuities%2520when%2520optimizing%250Athe%2520reference%2520target%2520position%2520to%2520the%2520nearest%2520point%2520relative%2520to%2520the%2520current%250Arobot%2520position.%2520This%2520article%2520aims%2520to%2520give%2520a%2520geometric%2520interpretation%2520of%2520such%250Adiscontinuities%252C%2520with%2520specific%2520reference%2520to%2520the%2520commonly%2520adopted%2520Gauss-Newton%250Aalgorithm.%2520The%2520effect%2520of%2520such%2520discontinuities%252C%2520defined%2520as%2520Euclidean%2520Distance%250ASingularities%252C%2520is%2520experimentally%2520proved.%2520We%2520then%2520propose%2520a%2520solution%2520that%2520is%250Abased%2520on%2520a%2520Linear%2520Quadratic%2520Tracking%2520problem%2520with%2520minimum%2520jerk%2520command%252C%2520then%250Acompare%2520and%2520validate%2520the%2520performances%2520of%2520the%2520proposed%2520framework%2520in%2520two%250Adifferent%2520human-robot%2520interaction%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Minimum-Jerk%20Approach%20to%20Handle%20Singularities%20in%20Virtual%20Fixtures&entry.906535625=Giovanni%20Braglia%20and%20Sylvain%20Calinon%20and%20Luigi%20Biagiotti&entry.1292438233=%20%20Implementing%20virtual%20fixtures%20in%20guiding%20tasks%20constrains%20the%20movement%20of%20the%0Arobot%27s%20end%20effector%20to%20specific%20curves%20within%20its%20workspace.%20However%2C%0Aincorporating%20guiding%20frameworks%20may%20encounter%20discontinuities%20when%20optimizing%0Athe%20reference%20target%20position%20to%20the%20nearest%20point%20relative%20to%20the%20current%0Arobot%20position.%20This%20article%20aims%20to%20give%20a%20geometric%20interpretation%20of%20such%0Adiscontinuities%2C%20with%20specific%20reference%20to%20the%20commonly%20adopted%20Gauss-Newton%0Aalgorithm.%20The%20effect%20of%20such%20discontinuities%2C%20defined%20as%20Euclidean%20Distance%0ASingularities%2C%20is%20experimentally%20proved.%20We%20then%20propose%20a%20solution%20that%20is%0Abased%20on%20a%20Linear%20Quadratic%20Tracking%20problem%20with%20minimum%20jerk%20command%2C%20then%0Acompare%20and%20validate%20the%20performances%20of%20the%20proposed%20framework%20in%20two%0Adifferent%20human-robot%20interaction%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03473v2&entry.124074799=Read"},
{"title": "A dual ensemble classifier used to recognise contaminated multi-channel\n  EMG and MMG signals in the control of upper limb bioprosthesis", "author": "Pawel Trajdos and Marek Kurzynski", "abstract": "  Myopotential pattern recognition to decode the intent of the user is the most\nadvanced approach to controlling a powered bioprosthesis. Unfortunately, many\nfactors make this a difficult problem and achieving acceptable recognition\nquality in real-word conditions is a serious challenge. The aim of the paper is\nto develop a recognition system that will mitigate factors related to\nmultimodality and multichannel recording of biosignals and their high\nsusceptibility to contamination. The proposed method involves the use of two\nco-operating multiclassifier systems. The first system is composed of one-class\nclassifiers related to individual electromyographic (EMG) and mechanomyographic\n(MMG) biosignal recording channels, and its task is to recognise contaminated\nchannels. The role of the second system is to recognise the class of movement\nresulting from the patient's intention. The ensemble system consists of base\nclassifiers using the representation (extracted features) of biosignals from\ndifferent channels. The system uses a dynamic selection mechanism, eliminating\nthose base classifiers that are associated with biosignal channels that are\nrecognised by the one-class ensemble system as being contaminated. Experimental\nstudies were conducted using signals from an able-bodied person with simulation\nof amputation. The results obtained allow us to reject the null hypothesis that\nthe application of the dual ensemble foes not lead to improved classification\nquality.\n", "link": "http://arxiv.org/abs/2407.18675v1", "date": "2024-07-26", "relevancy": 1.9189, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5485}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4704}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20dual%20ensemble%20classifier%20used%20to%20recognise%20contaminated%20multi-channel%0A%20%20EMG%20and%20MMG%20signals%20in%20the%20control%20of%20upper%20limb%20bioprosthesis&body=Title%3A%20A%20dual%20ensemble%20classifier%20used%20to%20recognise%20contaminated%20multi-channel%0A%20%20EMG%20and%20MMG%20signals%20in%20the%20control%20of%20upper%20limb%20bioprosthesis%0AAuthor%3A%20Pawel%20Trajdos%20and%20Marek%20Kurzynski%0AAbstract%3A%20%20%20Myopotential%20pattern%20recognition%20to%20decode%20the%20intent%20of%20the%20user%20is%20the%20most%0Aadvanced%20approach%20to%20controlling%20a%20powered%20bioprosthesis.%20Unfortunately%2C%20many%0Afactors%20make%20this%20a%20difficult%20problem%20and%20achieving%20acceptable%20recognition%0Aquality%20in%20real-word%20conditions%20is%20a%20serious%20challenge.%20The%20aim%20of%20the%20paper%20is%0Ato%20develop%20a%20recognition%20system%20that%20will%20mitigate%20factors%20related%20to%0Amultimodality%20and%20multichannel%20recording%20of%20biosignals%20and%20their%20high%0Asusceptibility%20to%20contamination.%20The%20proposed%20method%20involves%20the%20use%20of%20two%0Aco-operating%20multiclassifier%20systems.%20The%20first%20system%20is%20composed%20of%20one-class%0Aclassifiers%20related%20to%20individual%20electromyographic%20%28EMG%29%20and%20mechanomyographic%0A%28MMG%29%20biosignal%20recording%20channels%2C%20and%20its%20task%20is%20to%20recognise%20contaminated%0Achannels.%20The%20role%20of%20the%20second%20system%20is%20to%20recognise%20the%20class%20of%20movement%0Aresulting%20from%20the%20patient%27s%20intention.%20The%20ensemble%20system%20consists%20of%20base%0Aclassifiers%20using%20the%20representation%20%28extracted%20features%29%20of%20biosignals%20from%0Adifferent%20channels.%20The%20system%20uses%20a%20dynamic%20selection%20mechanism%2C%20eliminating%0Athose%20base%20classifiers%20that%20are%20associated%20with%20biosignal%20channels%20that%20are%0Arecognised%20by%20the%20one-class%20ensemble%20system%20as%20being%20contaminated.%20Experimental%0Astudies%20were%20conducted%20using%20signals%20from%20an%20able-bodied%20person%20with%20simulation%0Aof%20amputation.%20The%20results%20obtained%20allow%20us%20to%20reject%20the%20null%20hypothesis%20that%0Athe%20application%20of%20the%20dual%20ensemble%20foes%20not%20lead%20to%20improved%20classification%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520dual%2520ensemble%2520classifier%2520used%2520to%2520recognise%2520contaminated%2520multi-channel%250A%2520%2520EMG%2520and%2520MMG%2520signals%2520in%2520the%2520control%2520of%2520upper%2520limb%2520bioprosthesis%26entry.906535625%3DPawel%2520Trajdos%2520and%2520Marek%2520Kurzynski%26entry.1292438233%3D%2520%2520Myopotential%2520pattern%2520recognition%2520to%2520decode%2520the%2520intent%2520of%2520the%2520user%2520is%2520the%2520most%250Aadvanced%2520approach%2520to%2520controlling%2520a%2520powered%2520bioprosthesis.%2520Unfortunately%252C%2520many%250Afactors%2520make%2520this%2520a%2520difficult%2520problem%2520and%2520achieving%2520acceptable%2520recognition%250Aquality%2520in%2520real-word%2520conditions%2520is%2520a%2520serious%2520challenge.%2520The%2520aim%2520of%2520the%2520paper%2520is%250Ato%2520develop%2520a%2520recognition%2520system%2520that%2520will%2520mitigate%2520factors%2520related%2520to%250Amultimodality%2520and%2520multichannel%2520recording%2520of%2520biosignals%2520and%2520their%2520high%250Asusceptibility%2520to%2520contamination.%2520The%2520proposed%2520method%2520involves%2520the%2520use%2520of%2520two%250Aco-operating%2520multiclassifier%2520systems.%2520The%2520first%2520system%2520is%2520composed%2520of%2520one-class%250Aclassifiers%2520related%2520to%2520individual%2520electromyographic%2520%2528EMG%2529%2520and%2520mechanomyographic%250A%2528MMG%2529%2520biosignal%2520recording%2520channels%252C%2520and%2520its%2520task%2520is%2520to%2520recognise%2520contaminated%250Achannels.%2520The%2520role%2520of%2520the%2520second%2520system%2520is%2520to%2520recognise%2520the%2520class%2520of%2520movement%250Aresulting%2520from%2520the%2520patient%2527s%2520intention.%2520The%2520ensemble%2520system%2520consists%2520of%2520base%250Aclassifiers%2520using%2520the%2520representation%2520%2528extracted%2520features%2529%2520of%2520biosignals%2520from%250Adifferent%2520channels.%2520The%2520system%2520uses%2520a%2520dynamic%2520selection%2520mechanism%252C%2520eliminating%250Athose%2520base%2520classifiers%2520that%2520are%2520associated%2520with%2520biosignal%2520channels%2520that%2520are%250Arecognised%2520by%2520the%2520one-class%2520ensemble%2520system%2520as%2520being%2520contaminated.%2520Experimental%250Astudies%2520were%2520conducted%2520using%2520signals%2520from%2520an%2520able-bodied%2520person%2520with%2520simulation%250Aof%2520amputation.%2520The%2520results%2520obtained%2520allow%2520us%2520to%2520reject%2520the%2520null%2520hypothesis%2520that%250Athe%2520application%2520of%2520the%2520dual%2520ensemble%2520foes%2520not%2520lead%2520to%2520improved%2520classification%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20dual%20ensemble%20classifier%20used%20to%20recognise%20contaminated%20multi-channel%0A%20%20EMG%20and%20MMG%20signals%20in%20the%20control%20of%20upper%20limb%20bioprosthesis&entry.906535625=Pawel%20Trajdos%20and%20Marek%20Kurzynski&entry.1292438233=%20%20Myopotential%20pattern%20recognition%20to%20decode%20the%20intent%20of%20the%20user%20is%20the%20most%0Aadvanced%20approach%20to%20controlling%20a%20powered%20bioprosthesis.%20Unfortunately%2C%20many%0Afactors%20make%20this%20a%20difficult%20problem%20and%20achieving%20acceptable%20recognition%0Aquality%20in%20real-word%20conditions%20is%20a%20serious%20challenge.%20The%20aim%20of%20the%20paper%20is%0Ato%20develop%20a%20recognition%20system%20that%20will%20mitigate%20factors%20related%20to%0Amultimodality%20and%20multichannel%20recording%20of%20biosignals%20and%20their%20high%0Asusceptibility%20to%20contamination.%20The%20proposed%20method%20involves%20the%20use%20of%20two%0Aco-operating%20multiclassifier%20systems.%20The%20first%20system%20is%20composed%20of%20one-class%0Aclassifiers%20related%20to%20individual%20electromyographic%20%28EMG%29%20and%20mechanomyographic%0A%28MMG%29%20biosignal%20recording%20channels%2C%20and%20its%20task%20is%20to%20recognise%20contaminated%0Achannels.%20The%20role%20of%20the%20second%20system%20is%20to%20recognise%20the%20class%20of%20movement%0Aresulting%20from%20the%20patient%27s%20intention.%20The%20ensemble%20system%20consists%20of%20base%0Aclassifiers%20using%20the%20representation%20%28extracted%20features%29%20of%20biosignals%20from%0Adifferent%20channels.%20The%20system%20uses%20a%20dynamic%20selection%20mechanism%2C%20eliminating%0Athose%20base%20classifiers%20that%20are%20associated%20with%20biosignal%20channels%20that%20are%0Arecognised%20by%20the%20one-class%20ensemble%20system%20as%20being%20contaminated.%20Experimental%0Astudies%20were%20conducted%20using%20signals%20from%20an%20able-bodied%20person%20with%20simulation%0Aof%20amputation.%20The%20results%20obtained%20allow%20us%20to%20reject%20the%20null%20hypothesis%20that%0Athe%20application%20of%20the%20dual%20ensemble%20foes%20not%20lead%20to%20improved%20classification%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18675v1&entry.124074799=Read"},
{"title": "Embedding And Clustering Your Data Can Improve Contrastive Pretraining", "author": "Luke Merrick", "abstract": "  Recent studies of large-scale contrastive pretraining in the text embedding\ndomain show that using single-source minibatches, rather than mixed-source\nminibatches, can substantially improve overall model accuracy. In this work, we\nexplore extending training data stratification beyond source granularity by\nleveraging a pretrained text embedding model and the classic k-means clustering\nalgorithm to further split training data apart by the semantic clusters within\neach source. Experimentally, we observe a notable increase in NDCG@10 when\npretraining a BERT-based text embedding model on query-passage pairs from the\nMSMARCO passage retrieval dataset. Additionally, we conceptually connect our\nclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B\nmethodology and the nearest-neighbor-based hard-negative mining aspect of the\nANCE methodology and discuss how this unified view motivates future lines of\nresearch on the organization of contrastive pretraining data.\n", "link": "http://arxiv.org/abs/2407.18887v1", "date": "2024-07-26", "relevancy": 1.9126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4558}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20And%20Clustering%20Your%20Data%20Can%20Improve%20Contrastive%20Pretraining&body=Title%3A%20Embedding%20And%20Clustering%20Your%20Data%20Can%20Improve%20Contrastive%20Pretraining%0AAuthor%3A%20Luke%20Merrick%0AAbstract%3A%20%20%20Recent%20studies%20of%20large-scale%20contrastive%20pretraining%20in%20the%20text%20embedding%0Adomain%20show%20that%20using%20single-source%20minibatches%2C%20rather%20than%20mixed-source%0Aminibatches%2C%20can%20substantially%20improve%20overall%20model%20accuracy.%20In%20this%20work%2C%20we%0Aexplore%20extending%20training%20data%20stratification%20beyond%20source%20granularity%20by%0Aleveraging%20a%20pretrained%20text%20embedding%20model%20and%20the%20classic%20k-means%20clustering%0Aalgorithm%20to%20further%20split%20training%20data%20apart%20by%20the%20semantic%20clusters%20within%0Aeach%20source.%20Experimentally%2C%20we%20observe%20a%20notable%20increase%20in%20NDCG%4010%20when%0Apretraining%20a%20BERT-based%20text%20embedding%20model%20on%20query-passage%20pairs%20from%20the%0AMSMARCO%20passage%20retrieval%20dataset.%20Additionally%2C%20we%20conceptually%20connect%20our%0Aclustering%20approach%20to%20both%20the%20Topic%20Aware%20Sampling%20%28TAS%29%20aspect%20of%20the%20TAS-B%0Amethodology%20and%20the%20nearest-neighbor-based%20hard-negative%20mining%20aspect%20of%20the%0AANCE%20methodology%20and%20discuss%20how%20this%20unified%20view%20motivates%20future%20lines%20of%0Aresearch%20on%20the%20organization%20of%20contrastive%20pretraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520And%2520Clustering%2520Your%2520Data%2520Can%2520Improve%2520Contrastive%2520Pretraining%26entry.906535625%3DLuke%2520Merrick%26entry.1292438233%3D%2520%2520Recent%2520studies%2520of%2520large-scale%2520contrastive%2520pretraining%2520in%2520the%2520text%2520embedding%250Adomain%2520show%2520that%2520using%2520single-source%2520minibatches%252C%2520rather%2520than%2520mixed-source%250Aminibatches%252C%2520can%2520substantially%2520improve%2520overall%2520model%2520accuracy.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520extending%2520training%2520data%2520stratification%2520beyond%2520source%2520granularity%2520by%250Aleveraging%2520a%2520pretrained%2520text%2520embedding%2520model%2520and%2520the%2520classic%2520k-means%2520clustering%250Aalgorithm%2520to%2520further%2520split%2520training%2520data%2520apart%2520by%2520the%2520semantic%2520clusters%2520within%250Aeach%2520source.%2520Experimentally%252C%2520we%2520observe%2520a%2520notable%2520increase%2520in%2520NDCG%254010%2520when%250Apretraining%2520a%2520BERT-based%2520text%2520embedding%2520model%2520on%2520query-passage%2520pairs%2520from%2520the%250AMSMARCO%2520passage%2520retrieval%2520dataset.%2520Additionally%252C%2520we%2520conceptually%2520connect%2520our%250Aclustering%2520approach%2520to%2520both%2520the%2520Topic%2520Aware%2520Sampling%2520%2528TAS%2529%2520aspect%2520of%2520the%2520TAS-B%250Amethodology%2520and%2520the%2520nearest-neighbor-based%2520hard-negative%2520mining%2520aspect%2520of%2520the%250AANCE%2520methodology%2520and%2520discuss%2520how%2520this%2520unified%2520view%2520motivates%2520future%2520lines%2520of%250Aresearch%2520on%2520the%2520organization%2520of%2520contrastive%2520pretraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20And%20Clustering%20Your%20Data%20Can%20Improve%20Contrastive%20Pretraining&entry.906535625=Luke%20Merrick&entry.1292438233=%20%20Recent%20studies%20of%20large-scale%20contrastive%20pretraining%20in%20the%20text%20embedding%0Adomain%20show%20that%20using%20single-source%20minibatches%2C%20rather%20than%20mixed-source%0Aminibatches%2C%20can%20substantially%20improve%20overall%20model%20accuracy.%20In%20this%20work%2C%20we%0Aexplore%20extending%20training%20data%20stratification%20beyond%20source%20granularity%20by%0Aleveraging%20a%20pretrained%20text%20embedding%20model%20and%20the%20classic%20k-means%20clustering%0Aalgorithm%20to%20further%20split%20training%20data%20apart%20by%20the%20semantic%20clusters%20within%0Aeach%20source.%20Experimentally%2C%20we%20observe%20a%20notable%20increase%20in%20NDCG%4010%20when%0Apretraining%20a%20BERT-based%20text%20embedding%20model%20on%20query-passage%20pairs%20from%20the%0AMSMARCO%20passage%20retrieval%20dataset.%20Additionally%2C%20we%20conceptually%20connect%20our%0Aclustering%20approach%20to%20both%20the%20Topic%20Aware%20Sampling%20%28TAS%29%20aspect%20of%20the%20TAS-B%0Amethodology%20and%20the%20nearest-neighbor-based%20hard-negative%20mining%20aspect%20of%20the%0AANCE%20methodology%20and%20discuss%20how%20this%20unified%20view%20motivates%20future%20lines%20of%0Aresearch%20on%20the%20organization%20of%20contrastive%20pretraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18887v1&entry.124074799=Read"},
{"title": "Engaging with Children's Artwork in Mixed Visual-Ability Families", "author": "Arnavi Chheda-Kothary and Jacob O. Wobbrock and Jon E. Froehlich", "abstract": "  We present two studies exploring how blind or low-vision (BLV) family members\nengage with their sighted children's artwork, strategies to support\nunderstanding and interpretation, and the potential role of technology, such as\nAI, therein. Our first study involved 14 BLV individuals, and the second\nincluded five groups of BLV individuals with their children. Through\nsemi-structured interviews with AI descriptions of children's artwork and\nmulti-sensory design probes, we found that BLV family members value artwork\nengagement as a bonding opportunity, preferring the child's storytelling and\ninterpretation over other nonvisual representations. Additionally, despite some\ninaccuracies, BLV family members felt that AI-generated descriptions could\nfacilitate dialogue with their children and aid self-guided art discovery. We\nclose with specific design considerations for supporting artwork engagement in\nmixed visual-ability families, including enabling artwork access through\nvarious methods, supporting children's corrections of AI output, and\ndistinctions in context vs. content and interpretation vs. description of\nchildren's artwork.\n", "link": "http://arxiv.org/abs/2407.18874v1", "date": "2024-07-26", "relevancy": 1.9066, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4873}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.472}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Engaging%20with%20Children%27s%20Artwork%20in%20Mixed%20Visual-Ability%20Families&body=Title%3A%20Engaging%20with%20Children%27s%20Artwork%20in%20Mixed%20Visual-Ability%20Families%0AAuthor%3A%20Arnavi%20Chheda-Kothary%20and%20Jacob%20O.%20Wobbrock%20and%20Jon%20E.%20Froehlich%0AAbstract%3A%20%20%20We%20present%20two%20studies%20exploring%20how%20blind%20or%20low-vision%20%28BLV%29%20family%20members%0Aengage%20with%20their%20sighted%20children%27s%20artwork%2C%20strategies%20to%20support%0Aunderstanding%20and%20interpretation%2C%20and%20the%20potential%20role%20of%20technology%2C%20such%20as%0AAI%2C%20therein.%20Our%20first%20study%20involved%2014%20BLV%20individuals%2C%20and%20the%20second%0Aincluded%20five%20groups%20of%20BLV%20individuals%20with%20their%20children.%20Through%0Asemi-structured%20interviews%20with%20AI%20descriptions%20of%20children%27s%20artwork%20and%0Amulti-sensory%20design%20probes%2C%20we%20found%20that%20BLV%20family%20members%20value%20artwork%0Aengagement%20as%20a%20bonding%20opportunity%2C%20preferring%20the%20child%27s%20storytelling%20and%0Ainterpretation%20over%20other%20nonvisual%20representations.%20Additionally%2C%20despite%20some%0Ainaccuracies%2C%20BLV%20family%20members%20felt%20that%20AI-generated%20descriptions%20could%0Afacilitate%20dialogue%20with%20their%20children%20and%20aid%20self-guided%20art%20discovery.%20We%0Aclose%20with%20specific%20design%20considerations%20for%20supporting%20artwork%20engagement%20in%0Amixed%20visual-ability%20families%2C%20including%20enabling%20artwork%20access%20through%0Avarious%20methods%2C%20supporting%20children%27s%20corrections%20of%20AI%20output%2C%20and%0Adistinctions%20in%20context%20vs.%20content%20and%20interpretation%20vs.%20description%20of%0Achildren%27s%20artwork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngaging%2520with%2520Children%2527s%2520Artwork%2520in%2520Mixed%2520Visual-Ability%2520Families%26entry.906535625%3DArnavi%2520Chheda-Kothary%2520and%2520Jacob%2520O.%2520Wobbrock%2520and%2520Jon%2520E.%2520Froehlich%26entry.1292438233%3D%2520%2520We%2520present%2520two%2520studies%2520exploring%2520how%2520blind%2520or%2520low-vision%2520%2528BLV%2529%2520family%2520members%250Aengage%2520with%2520their%2520sighted%2520children%2527s%2520artwork%252C%2520strategies%2520to%2520support%250Aunderstanding%2520and%2520interpretation%252C%2520and%2520the%2520potential%2520role%2520of%2520technology%252C%2520such%2520as%250AAI%252C%2520therein.%2520Our%2520first%2520study%2520involved%252014%2520BLV%2520individuals%252C%2520and%2520the%2520second%250Aincluded%2520five%2520groups%2520of%2520BLV%2520individuals%2520with%2520their%2520children.%2520Through%250Asemi-structured%2520interviews%2520with%2520AI%2520descriptions%2520of%2520children%2527s%2520artwork%2520and%250Amulti-sensory%2520design%2520probes%252C%2520we%2520found%2520that%2520BLV%2520family%2520members%2520value%2520artwork%250Aengagement%2520as%2520a%2520bonding%2520opportunity%252C%2520preferring%2520the%2520child%2527s%2520storytelling%2520and%250Ainterpretation%2520over%2520other%2520nonvisual%2520representations.%2520Additionally%252C%2520despite%2520some%250Ainaccuracies%252C%2520BLV%2520family%2520members%2520felt%2520that%2520AI-generated%2520descriptions%2520could%250Afacilitate%2520dialogue%2520with%2520their%2520children%2520and%2520aid%2520self-guided%2520art%2520discovery.%2520We%250Aclose%2520with%2520specific%2520design%2520considerations%2520for%2520supporting%2520artwork%2520engagement%2520in%250Amixed%2520visual-ability%2520families%252C%2520including%2520enabling%2520artwork%2520access%2520through%250Avarious%2520methods%252C%2520supporting%2520children%2527s%2520corrections%2520of%2520AI%2520output%252C%2520and%250Adistinctions%2520in%2520context%2520vs.%2520content%2520and%2520interpretation%2520vs.%2520description%2520of%250Achildren%2527s%2520artwork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Engaging%20with%20Children%27s%20Artwork%20in%20Mixed%20Visual-Ability%20Families&entry.906535625=Arnavi%20Chheda-Kothary%20and%20Jacob%20O.%20Wobbrock%20and%20Jon%20E.%20Froehlich&entry.1292438233=%20%20We%20present%20two%20studies%20exploring%20how%20blind%20or%20low-vision%20%28BLV%29%20family%20members%0Aengage%20with%20their%20sighted%20children%27s%20artwork%2C%20strategies%20to%20support%0Aunderstanding%20and%20interpretation%2C%20and%20the%20potential%20role%20of%20technology%2C%20such%20as%0AAI%2C%20therein.%20Our%20first%20study%20involved%2014%20BLV%20individuals%2C%20and%20the%20second%0Aincluded%20five%20groups%20of%20BLV%20individuals%20with%20their%20children.%20Through%0Asemi-structured%20interviews%20with%20AI%20descriptions%20of%20children%27s%20artwork%20and%0Amulti-sensory%20design%20probes%2C%20we%20found%20that%20BLV%20family%20members%20value%20artwork%0Aengagement%20as%20a%20bonding%20opportunity%2C%20preferring%20the%20child%27s%20storytelling%20and%0Ainterpretation%20over%20other%20nonvisual%20representations.%20Additionally%2C%20despite%20some%0Ainaccuracies%2C%20BLV%20family%20members%20felt%20that%20AI-generated%20descriptions%20could%0Afacilitate%20dialogue%20with%20their%20children%20and%20aid%20self-guided%20art%20discovery.%20We%0Aclose%20with%20specific%20design%20considerations%20for%20supporting%20artwork%20engagement%20in%0Amixed%20visual-ability%20families%2C%20including%20enabling%20artwork%20access%20through%0Avarious%20methods%2C%20supporting%20children%27s%20corrections%20of%20AI%20output%2C%20and%0Adistinctions%20in%20context%20vs.%20content%20and%20interpretation%20vs.%20description%20of%0Achildren%27s%20artwork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18874v1&entry.124074799=Read"},
{"title": "Hybrid summary statistics: neural weak lensing inference beyond the\n  power spectrum", "author": "T. Lucas Makinen and Alan Heavens and Natalia Porqueres and Tom Charnock and Axel Lapel and Benjamin D. Wandelt", "abstract": "  In inference problems, we often have domain knowledge which allows us to\ndefine summary statistics that capture most of the information content in a\ndataset. In this paper, we present a hybrid approach, where such physics-based\nsummaries are augmented by a set of compressed neural summary statistics that\nare optimised to extract the extra information that is not captured by the\npredefined summaries. The resulting statistics are very powerful inputs to\nsimulation-based or implicit inference of model parameters. We apply this\ngeneralisation of Information Maximising Neural Networks (IMNNs) to parameter\nconstraints from tomographic weak gravitational lensing convergence maps to\nfind summary statistics that are explicitly optimised to complement angular\npower spectrum estimates. We study several dark matter simulation resolutions\nin low- and high-noise regimes. We show that i) the information-update\nformalism extracts at least $3\\times$ and up to $8\\times$ as much information\nas the angular power spectrum in all noise regimes, ii) the network summaries\nare highly complementary to existing 2-point summaries, and iii) our formalism\nallows for networks with smaller, physically-informed architectures to match\nmuch larger regression networks with far fewer simulations needed to obtain\nasymptotically optimal inference.\n", "link": "http://arxiv.org/abs/2407.18909v1", "date": "2024-07-26", "relevancy": 1.8931, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20summary%20statistics%3A%20neural%20weak%20lensing%20inference%20beyond%20the%0A%20%20power%20spectrum&body=Title%3A%20Hybrid%20summary%20statistics%3A%20neural%20weak%20lensing%20inference%20beyond%20the%0A%20%20power%20spectrum%0AAuthor%3A%20T.%20Lucas%20Makinen%20and%20Alan%20Heavens%20and%20Natalia%20Porqueres%20and%20Tom%20Charnock%20and%20Axel%20Lapel%20and%20Benjamin%20D.%20Wandelt%0AAbstract%3A%20%20%20In%20inference%20problems%2C%20we%20often%20have%20domain%20knowledge%20which%20allows%20us%20to%0Adefine%20summary%20statistics%20that%20capture%20most%20of%20the%20information%20content%20in%20a%0Adataset.%20In%20this%20paper%2C%20we%20present%20a%20hybrid%20approach%2C%20where%20such%20physics-based%0Asummaries%20are%20augmented%20by%20a%20set%20of%20compressed%20neural%20summary%20statistics%20that%0Aare%20optimised%20to%20extract%20the%20extra%20information%20that%20is%20not%20captured%20by%20the%0Apredefined%20summaries.%20The%20resulting%20statistics%20are%20very%20powerful%20inputs%20to%0Asimulation-based%20or%20implicit%20inference%20of%20model%20parameters.%20We%20apply%20this%0Ageneralisation%20of%20Information%20Maximising%20Neural%20Networks%20%28IMNNs%29%20to%20parameter%0Aconstraints%20from%20tomographic%20weak%20gravitational%20lensing%20convergence%20maps%20to%0Afind%20summary%20statistics%20that%20are%20explicitly%20optimised%20to%20complement%20angular%0Apower%20spectrum%20estimates.%20We%20study%20several%20dark%20matter%20simulation%20resolutions%0Ain%20low-%20and%20high-noise%20regimes.%20We%20show%20that%20i%29%20the%20information-update%0Aformalism%20extracts%20at%20least%20%243%5Ctimes%24%20and%20up%20to%20%248%5Ctimes%24%20as%20much%20information%0Aas%20the%20angular%20power%20spectrum%20in%20all%20noise%20regimes%2C%20ii%29%20the%20network%20summaries%0Aare%20highly%20complementary%20to%20existing%202-point%20summaries%2C%20and%20iii%29%20our%20formalism%0Aallows%20for%20networks%20with%20smaller%2C%20physically-informed%20architectures%20to%20match%0Amuch%20larger%20regression%20networks%20with%20far%20fewer%20simulations%20needed%20to%20obtain%0Aasymptotically%20optimal%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520summary%2520statistics%253A%2520neural%2520weak%2520lensing%2520inference%2520beyond%2520the%250A%2520%2520power%2520spectrum%26entry.906535625%3DT.%2520Lucas%2520Makinen%2520and%2520Alan%2520Heavens%2520and%2520Natalia%2520Porqueres%2520and%2520Tom%2520Charnock%2520and%2520Axel%2520Lapel%2520and%2520Benjamin%2520D.%2520Wandelt%26entry.1292438233%3D%2520%2520In%2520inference%2520problems%252C%2520we%2520often%2520have%2520domain%2520knowledge%2520which%2520allows%2520us%2520to%250Adefine%2520summary%2520statistics%2520that%2520capture%2520most%2520of%2520the%2520information%2520content%2520in%2520a%250Adataset.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520hybrid%2520approach%252C%2520where%2520such%2520physics-based%250Asummaries%2520are%2520augmented%2520by%2520a%2520set%2520of%2520compressed%2520neural%2520summary%2520statistics%2520that%250Aare%2520optimised%2520to%2520extract%2520the%2520extra%2520information%2520that%2520is%2520not%2520captured%2520by%2520the%250Apredefined%2520summaries.%2520The%2520resulting%2520statistics%2520are%2520very%2520powerful%2520inputs%2520to%250Asimulation-based%2520or%2520implicit%2520inference%2520of%2520model%2520parameters.%2520We%2520apply%2520this%250Ageneralisation%2520of%2520Information%2520Maximising%2520Neural%2520Networks%2520%2528IMNNs%2529%2520to%2520parameter%250Aconstraints%2520from%2520tomographic%2520weak%2520gravitational%2520lensing%2520convergence%2520maps%2520to%250Afind%2520summary%2520statistics%2520that%2520are%2520explicitly%2520optimised%2520to%2520complement%2520angular%250Apower%2520spectrum%2520estimates.%2520We%2520study%2520several%2520dark%2520matter%2520simulation%2520resolutions%250Ain%2520low-%2520and%2520high-noise%2520regimes.%2520We%2520show%2520that%2520i%2529%2520the%2520information-update%250Aformalism%2520extracts%2520at%2520least%2520%25243%255Ctimes%2524%2520and%2520up%2520to%2520%25248%255Ctimes%2524%2520as%2520much%2520information%250Aas%2520the%2520angular%2520power%2520spectrum%2520in%2520all%2520noise%2520regimes%252C%2520ii%2529%2520the%2520network%2520summaries%250Aare%2520highly%2520complementary%2520to%2520existing%25202-point%2520summaries%252C%2520and%2520iii%2529%2520our%2520formalism%250Aallows%2520for%2520networks%2520with%2520smaller%252C%2520physically-informed%2520architectures%2520to%2520match%250Amuch%2520larger%2520regression%2520networks%2520with%2520far%2520fewer%2520simulations%2520needed%2520to%2520obtain%250Aasymptotically%2520optimal%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20summary%20statistics%3A%20neural%20weak%20lensing%20inference%20beyond%20the%0A%20%20power%20spectrum&entry.906535625=T.%20Lucas%20Makinen%20and%20Alan%20Heavens%20and%20Natalia%20Porqueres%20and%20Tom%20Charnock%20and%20Axel%20Lapel%20and%20Benjamin%20D.%20Wandelt&entry.1292438233=%20%20In%20inference%20problems%2C%20we%20often%20have%20domain%20knowledge%20which%20allows%20us%20to%0Adefine%20summary%20statistics%20that%20capture%20most%20of%20the%20information%20content%20in%20a%0Adataset.%20In%20this%20paper%2C%20we%20present%20a%20hybrid%20approach%2C%20where%20such%20physics-based%0Asummaries%20are%20augmented%20by%20a%20set%20of%20compressed%20neural%20summary%20statistics%20that%0Aare%20optimised%20to%20extract%20the%20extra%20information%20that%20is%20not%20captured%20by%20the%0Apredefined%20summaries.%20The%20resulting%20statistics%20are%20very%20powerful%20inputs%20to%0Asimulation-based%20or%20implicit%20inference%20of%20model%20parameters.%20We%20apply%20this%0Ageneralisation%20of%20Information%20Maximising%20Neural%20Networks%20%28IMNNs%29%20to%20parameter%0Aconstraints%20from%20tomographic%20weak%20gravitational%20lensing%20convergence%20maps%20to%0Afind%20summary%20statistics%20that%20are%20explicitly%20optimised%20to%20complement%20angular%0Apower%20spectrum%20estimates.%20We%20study%20several%20dark%20matter%20simulation%20resolutions%0Ain%20low-%20and%20high-noise%20regimes.%20We%20show%20that%20i%29%20the%20information-update%0Aformalism%20extracts%20at%20least%20%243%5Ctimes%24%20and%20up%20to%20%248%5Ctimes%24%20as%20much%20information%0Aas%20the%20angular%20power%20spectrum%20in%20all%20noise%20regimes%2C%20ii%29%20the%20network%20summaries%0Aare%20highly%20complementary%20to%20existing%202-point%20summaries%2C%20and%20iii%29%20our%20formalism%0Aallows%20for%20networks%20with%20smaller%2C%20physically-informed%20architectures%20to%20match%0Amuch%20larger%20regression%20networks%20with%20far%20fewer%20simulations%20needed%20to%20obtain%0Aasymptotically%20optimal%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18909v1&entry.124074799=Read"},
{"title": "TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on\n  OGD portals", "author": "Kevin Kliimask and Anastasija Nikiforova", "abstract": "  Efforts directed towards promoting Open Government Data (OGD) have gained\nsignificant traction across various governmental tiers since the mid-2000s. As\nmore datasets are published on OGD portals, finding specific data becomes\nharder, leading to information overload. Complete and accurate documentation of\ndatasets, including association of proper tags with datasets is key to\nimproving dataset findability and accessibility. Analysis conducted on the\nEstonian Open Data Portal, revealed that 11% datasets have no associated tags,\nwhile 26% had only one tag assigned to them, which underscores challenges in\ndata findability and accessibility within the portal, which, according to the\nrecent Open Data Maturity Report, is considered trend-setter. The aim of this\nstudy is to propose an automated solution to tagging datasets to improve data\nfindability on OGD portals. This paper presents Tagify - a prototype of tagging\ninterface that employs large language models (LLM) such as GPT-3.5-turbo and\nGPT-4 to automate dataset tagging, generating tags for datasets in English and\nEstonian, thereby augmenting metadata preparation by data publishers and\nimproving data findability on OGD portals by data users. The developed solution\nwas evaluated by users and their feedback was collected to define an agenda for\nfuture prototype improvements.\n", "link": "http://arxiv.org/abs/2407.18764v1", "date": "2024-07-26", "relevancy": 1.8716, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAGIFY%3A%20LLM-powered%20Tagging%20Interface%20for%20Improved%20Data%20Findability%20on%0A%20%20OGD%20portals&body=Title%3A%20TAGIFY%3A%20LLM-powered%20Tagging%20Interface%20for%20Improved%20Data%20Findability%20on%0A%20%20OGD%20portals%0AAuthor%3A%20Kevin%20Kliimask%20and%20Anastasija%20Nikiforova%0AAbstract%3A%20%20%20Efforts%20directed%20towards%20promoting%20Open%20Government%20Data%20%28OGD%29%20have%20gained%0Asignificant%20traction%20across%20various%20governmental%20tiers%20since%20the%20mid-2000s.%20As%0Amore%20datasets%20are%20published%20on%20OGD%20portals%2C%20finding%20specific%20data%20becomes%0Aharder%2C%20leading%20to%20information%20overload.%20Complete%20and%20accurate%20documentation%20of%0Adatasets%2C%20including%20association%20of%20proper%20tags%20with%20datasets%20is%20key%20to%0Aimproving%20dataset%20findability%20and%20accessibility.%20Analysis%20conducted%20on%20the%0AEstonian%20Open%20Data%20Portal%2C%20revealed%20that%2011%25%20datasets%20have%20no%20associated%20tags%2C%0Awhile%2026%25%20had%20only%20one%20tag%20assigned%20to%20them%2C%20which%20underscores%20challenges%20in%0Adata%20findability%20and%20accessibility%20within%20the%20portal%2C%20which%2C%20according%20to%20the%0Arecent%20Open%20Data%20Maturity%20Report%2C%20is%20considered%20trend-setter.%20The%20aim%20of%20this%0Astudy%20is%20to%20propose%20an%20automated%20solution%20to%20tagging%20datasets%20to%20improve%20data%0Afindability%20on%20OGD%20portals.%20This%20paper%20presents%20Tagify%20-%20a%20prototype%20of%20tagging%0Ainterface%20that%20employs%20large%20language%20models%20%28LLM%29%20such%20as%20GPT-3.5-turbo%20and%0AGPT-4%20to%20automate%20dataset%20tagging%2C%20generating%20tags%20for%20datasets%20in%20English%20and%0AEstonian%2C%20thereby%20augmenting%20metadata%20preparation%20by%20data%20publishers%20and%0Aimproving%20data%20findability%20on%20OGD%20portals%20by%20data%20users.%20The%20developed%20solution%0Awas%20evaluated%20by%20users%20and%20their%20feedback%20was%20collected%20to%20define%20an%20agenda%20for%0Afuture%20prototype%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAGIFY%253A%2520LLM-powered%2520Tagging%2520Interface%2520for%2520Improved%2520Data%2520Findability%2520on%250A%2520%2520OGD%2520portals%26entry.906535625%3DKevin%2520Kliimask%2520and%2520Anastasija%2520Nikiforova%26entry.1292438233%3D%2520%2520Efforts%2520directed%2520towards%2520promoting%2520Open%2520Government%2520Data%2520%2528OGD%2529%2520have%2520gained%250Asignificant%2520traction%2520across%2520various%2520governmental%2520tiers%2520since%2520the%2520mid-2000s.%2520As%250Amore%2520datasets%2520are%2520published%2520on%2520OGD%2520portals%252C%2520finding%2520specific%2520data%2520becomes%250Aharder%252C%2520leading%2520to%2520information%2520overload.%2520Complete%2520and%2520accurate%2520documentation%2520of%250Adatasets%252C%2520including%2520association%2520of%2520proper%2520tags%2520with%2520datasets%2520is%2520key%2520to%250Aimproving%2520dataset%2520findability%2520and%2520accessibility.%2520Analysis%2520conducted%2520on%2520the%250AEstonian%2520Open%2520Data%2520Portal%252C%2520revealed%2520that%252011%2525%2520datasets%2520have%2520no%2520associated%2520tags%252C%250Awhile%252026%2525%2520had%2520only%2520one%2520tag%2520assigned%2520to%2520them%252C%2520which%2520underscores%2520challenges%2520in%250Adata%2520findability%2520and%2520accessibility%2520within%2520the%2520portal%252C%2520which%252C%2520according%2520to%2520the%250Arecent%2520Open%2520Data%2520Maturity%2520Report%252C%2520is%2520considered%2520trend-setter.%2520The%2520aim%2520of%2520this%250Astudy%2520is%2520to%2520propose%2520an%2520automated%2520solution%2520to%2520tagging%2520datasets%2520to%2520improve%2520data%250Afindability%2520on%2520OGD%2520portals.%2520This%2520paper%2520presents%2520Tagify%2520-%2520a%2520prototype%2520of%2520tagging%250Ainterface%2520that%2520employs%2520large%2520language%2520models%2520%2528LLM%2529%2520such%2520as%2520GPT-3.5-turbo%2520and%250AGPT-4%2520to%2520automate%2520dataset%2520tagging%252C%2520generating%2520tags%2520for%2520datasets%2520in%2520English%2520and%250AEstonian%252C%2520thereby%2520augmenting%2520metadata%2520preparation%2520by%2520data%2520publishers%2520and%250Aimproving%2520data%2520findability%2520on%2520OGD%2520portals%2520by%2520data%2520users.%2520The%2520developed%2520solution%250Awas%2520evaluated%2520by%2520users%2520and%2520their%2520feedback%2520was%2520collected%2520to%2520define%2520an%2520agenda%2520for%250Afuture%2520prototype%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAGIFY%3A%20LLM-powered%20Tagging%20Interface%20for%20Improved%20Data%20Findability%20on%0A%20%20OGD%20portals&entry.906535625=Kevin%20Kliimask%20and%20Anastasija%20Nikiforova&entry.1292438233=%20%20Efforts%20directed%20towards%20promoting%20Open%20Government%20Data%20%28OGD%29%20have%20gained%0Asignificant%20traction%20across%20various%20governmental%20tiers%20since%20the%20mid-2000s.%20As%0Amore%20datasets%20are%20published%20on%20OGD%20portals%2C%20finding%20specific%20data%20becomes%0Aharder%2C%20leading%20to%20information%20overload.%20Complete%20and%20accurate%20documentation%20of%0Adatasets%2C%20including%20association%20of%20proper%20tags%20with%20datasets%20is%20key%20to%0Aimproving%20dataset%20findability%20and%20accessibility.%20Analysis%20conducted%20on%20the%0AEstonian%20Open%20Data%20Portal%2C%20revealed%20that%2011%25%20datasets%20have%20no%20associated%20tags%2C%0Awhile%2026%25%20had%20only%20one%20tag%20assigned%20to%20them%2C%20which%20underscores%20challenges%20in%0Adata%20findability%20and%20accessibility%20within%20the%20portal%2C%20which%2C%20according%20to%20the%0Arecent%20Open%20Data%20Maturity%20Report%2C%20is%20considered%20trend-setter.%20The%20aim%20of%20this%0Astudy%20is%20to%20propose%20an%20automated%20solution%20to%20tagging%20datasets%20to%20improve%20data%0Afindability%20on%20OGD%20portals.%20This%20paper%20presents%20Tagify%20-%20a%20prototype%20of%20tagging%0Ainterface%20that%20employs%20large%20language%20models%20%28LLM%29%20such%20as%20GPT-3.5-turbo%20and%0AGPT-4%20to%20automate%20dataset%20tagging%2C%20generating%20tags%20for%20datasets%20in%20English%20and%0AEstonian%2C%20thereby%20augmenting%20metadata%20preparation%20by%20data%20publishers%20and%0Aimproving%20data%20findability%20on%20OGD%20portals%20by%20data%20users.%20The%20developed%20solution%0Awas%20evaluated%20by%20users%20and%20their%20feedback%20was%20collected%20to%20define%20an%20agenda%20for%0Afuture%20prototype%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18764v1&entry.124074799=Read"},
{"title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for\n  Enhanced Cross-Modal Alignment", "author": "Yuze Zheng and Zixuan Li and Xiangxian Li and Jinxing Liu and Yuqing Wang and Xiangxu Meng and Lei Meng", "abstract": "  Image classification models often demonstrate unstable performance in\nreal-world applications due to variations in image information, driven by\ndiffering visual perspectives of subject objects and lighting discrepancies. To\nmitigate these challenges, existing studies commonly incorporate additional\nmodal information matching the visual data to regularize the model's learning\nprocess, enabling the extraction of high-quality visual features from complex\nimage regions. Specifically, in the realm of multimodal learning, cross-modal\nalignment is recognized as an effective strategy, harmonizing different modal\ninformation by learning a domain-consistent latent feature space for visual and\nsemantic features. However, this approach may face limitations due to the\nheterogeneity between multimodal information, such as differences in feature\ndistribution and structure. To address this issue, we introduce a Multimodal\nAlignment and Reconstruction Network (MARNet), designed to enhance the model's\nresistance to visual noise. Importantly, MARNet includes a cross-modal\ndiffusion reconstruction module for smoothly and stably blending information\nacross different domains. Experiments conducted on two benchmark datasets,\nVireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves\nthe quality of image information extracted by the model. It is a plug-and-play\nframework that can be rapidly integrated into various image classification\nframeworks, boosting model performance.\n", "link": "http://arxiv.org/abs/2407.18854v1", "date": "2024-07-26", "relevancy": 1.8651, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Visual%20and%20Semantic%20Feature%20Spaces%20with%20Diffusion%20Models%20for%0A%20%20Enhanced%20Cross-Modal%20Alignment&body=Title%3A%20Unifying%20Visual%20and%20Semantic%20Feature%20Spaces%20with%20Diffusion%20Models%20for%0A%20%20Enhanced%20Cross-Modal%20Alignment%0AAuthor%3A%20Yuze%20Zheng%20and%20Zixuan%20Li%20and%20Xiangxian%20Li%20and%20Jinxing%20Liu%20and%20Yuqing%20Wang%20and%20Xiangxu%20Meng%20and%20Lei%20Meng%0AAbstract%3A%20%20%20Image%20classification%20models%20often%20demonstrate%20unstable%20performance%20in%0Areal-world%20applications%20due%20to%20variations%20in%20image%20information%2C%20driven%20by%0Adiffering%20visual%20perspectives%20of%20subject%20objects%20and%20lighting%20discrepancies.%20To%0Amitigate%20these%20challenges%2C%20existing%20studies%20commonly%20incorporate%20additional%0Amodal%20information%20matching%20the%20visual%20data%20to%20regularize%20the%20model%27s%20learning%0Aprocess%2C%20enabling%20the%20extraction%20of%20high-quality%20visual%20features%20from%20complex%0Aimage%20regions.%20Specifically%2C%20in%20the%20realm%20of%20multimodal%20learning%2C%20cross-modal%0Aalignment%20is%20recognized%20as%20an%20effective%20strategy%2C%20harmonizing%20different%20modal%0Ainformation%20by%20learning%20a%20domain-consistent%20latent%20feature%20space%20for%20visual%20and%0Asemantic%20features.%20However%2C%20this%20approach%20may%20face%20limitations%20due%20to%20the%0Aheterogeneity%20between%20multimodal%20information%2C%20such%20as%20differences%20in%20feature%0Adistribution%20and%20structure.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20Multimodal%0AAlignment%20and%20Reconstruction%20Network%20%28MARNet%29%2C%20designed%20to%20enhance%20the%20model%27s%0Aresistance%20to%20visual%20noise.%20Importantly%2C%20MARNet%20includes%20a%20cross-modal%0Adiffusion%20reconstruction%20module%20for%20smoothly%20and%20stably%20blending%20information%0Aacross%20different%20domains.%20Experiments%20conducted%20on%20two%20benchmark%20datasets%2C%0AVireo-Food172%20and%20Ingredient-101%2C%20demonstrate%20that%20MARNet%20effectively%20improves%0Athe%20quality%20of%20image%20information%20extracted%20by%20the%20model.%20It%20is%20a%20plug-and-play%0Aframework%20that%20can%20be%20rapidly%20integrated%20into%20various%20image%20classification%0Aframeworks%2C%20boosting%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Visual%2520and%2520Semantic%2520Feature%2520Spaces%2520with%2520Diffusion%2520Models%2520for%250A%2520%2520Enhanced%2520Cross-Modal%2520Alignment%26entry.906535625%3DYuze%2520Zheng%2520and%2520Zixuan%2520Li%2520and%2520Xiangxian%2520Li%2520and%2520Jinxing%2520Liu%2520and%2520Yuqing%2520Wang%2520and%2520Xiangxu%2520Meng%2520and%2520Lei%2520Meng%26entry.1292438233%3D%2520%2520Image%2520classification%2520models%2520often%2520demonstrate%2520unstable%2520performance%2520in%250Areal-world%2520applications%2520due%2520to%2520variations%2520in%2520image%2520information%252C%2520driven%2520by%250Adiffering%2520visual%2520perspectives%2520of%2520subject%2520objects%2520and%2520lighting%2520discrepancies.%2520To%250Amitigate%2520these%2520challenges%252C%2520existing%2520studies%2520commonly%2520incorporate%2520additional%250Amodal%2520information%2520matching%2520the%2520visual%2520data%2520to%2520regularize%2520the%2520model%2527s%2520learning%250Aprocess%252C%2520enabling%2520the%2520extraction%2520of%2520high-quality%2520visual%2520features%2520from%2520complex%250Aimage%2520regions.%2520Specifically%252C%2520in%2520the%2520realm%2520of%2520multimodal%2520learning%252C%2520cross-modal%250Aalignment%2520is%2520recognized%2520as%2520an%2520effective%2520strategy%252C%2520harmonizing%2520different%2520modal%250Ainformation%2520by%2520learning%2520a%2520domain-consistent%2520latent%2520feature%2520space%2520for%2520visual%2520and%250Asemantic%2520features.%2520However%252C%2520this%2520approach%2520may%2520face%2520limitations%2520due%2520to%2520the%250Aheterogeneity%2520between%2520multimodal%2520information%252C%2520such%2520as%2520differences%2520in%2520feature%250Adistribution%2520and%2520structure.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520Multimodal%250AAlignment%2520and%2520Reconstruction%2520Network%2520%2528MARNet%2529%252C%2520designed%2520to%2520enhance%2520the%2520model%2527s%250Aresistance%2520to%2520visual%2520noise.%2520Importantly%252C%2520MARNet%2520includes%2520a%2520cross-modal%250Adiffusion%2520reconstruction%2520module%2520for%2520smoothly%2520and%2520stably%2520blending%2520information%250Aacross%2520different%2520domains.%2520Experiments%2520conducted%2520on%2520two%2520benchmark%2520datasets%252C%250AVireo-Food172%2520and%2520Ingredient-101%252C%2520demonstrate%2520that%2520MARNet%2520effectively%2520improves%250Athe%2520quality%2520of%2520image%2520information%2520extracted%2520by%2520the%2520model.%2520It%2520is%2520a%2520plug-and-play%250Aframework%2520that%2520can%2520be%2520rapidly%2520integrated%2520into%2520various%2520image%2520classification%250Aframeworks%252C%2520boosting%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Visual%20and%20Semantic%20Feature%20Spaces%20with%20Diffusion%20Models%20for%0A%20%20Enhanced%20Cross-Modal%20Alignment&entry.906535625=Yuze%20Zheng%20and%20Zixuan%20Li%20and%20Xiangxian%20Li%20and%20Jinxing%20Liu%20and%20Yuqing%20Wang%20and%20Xiangxu%20Meng%20and%20Lei%20Meng&entry.1292438233=%20%20Image%20classification%20models%20often%20demonstrate%20unstable%20performance%20in%0Areal-world%20applications%20due%20to%20variations%20in%20image%20information%2C%20driven%20by%0Adiffering%20visual%20perspectives%20of%20subject%20objects%20and%20lighting%20discrepancies.%20To%0Amitigate%20these%20challenges%2C%20existing%20studies%20commonly%20incorporate%20additional%0Amodal%20information%20matching%20the%20visual%20data%20to%20regularize%20the%20model%27s%20learning%0Aprocess%2C%20enabling%20the%20extraction%20of%20high-quality%20visual%20features%20from%20complex%0Aimage%20regions.%20Specifically%2C%20in%20the%20realm%20of%20multimodal%20learning%2C%20cross-modal%0Aalignment%20is%20recognized%20as%20an%20effective%20strategy%2C%20harmonizing%20different%20modal%0Ainformation%20by%20learning%20a%20domain-consistent%20latent%20feature%20space%20for%20visual%20and%0Asemantic%20features.%20However%2C%20this%20approach%20may%20face%20limitations%20due%20to%20the%0Aheterogeneity%20between%20multimodal%20information%2C%20such%20as%20differences%20in%20feature%0Adistribution%20and%20structure.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20Multimodal%0AAlignment%20and%20Reconstruction%20Network%20%28MARNet%29%2C%20designed%20to%20enhance%20the%20model%27s%0Aresistance%20to%20visual%20noise.%20Importantly%2C%20MARNet%20includes%20a%20cross-modal%0Adiffusion%20reconstruction%20module%20for%20smoothly%20and%20stably%20blending%20information%0Aacross%20different%20domains.%20Experiments%20conducted%20on%20two%20benchmark%20datasets%2C%0AVireo-Food172%20and%20Ingredient-101%2C%20demonstrate%20that%20MARNet%20effectively%20improves%0Athe%20quality%20of%20image%20information%20extracted%20by%20the%20model.%20It%20is%20a%20plug-and-play%0Aframework%20that%20can%20be%20rapidly%20integrated%20into%20various%20image%20classification%0Aframeworks%2C%20boosting%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18854v1&entry.124074799=Read"},
{"title": "Learning production functions for supply chains with graph neural\n  networks", "author": "Serina Chang and Zhiyin Lin and Benjamin Yan and Swapnil Bembde and Qi Xiu and Chi Heem Wong and Yu Qin and Frank Kloster and Alex Luo and Raj Palleti and Jure Leskovec", "abstract": "  The global economy relies on the flow of goods over supply chain networks,\nwith nodes as firms and edges as transactions between firms. While we may\nobserve these external transactions, they are governed by unseen production\nfunctions, which determine how firms internally transform the input products\nthey receive into output products that they sell. In this setting, it can be\nextremely valuable to infer these production functions, to better understand\nand improve supply chains, and to forecast future transactions more accurately.\nHowever, existing graph neural networks (GNNs) cannot capture these hidden\nrelationships between nodes' inputs and outputs. Here, we introduce a new class\nof models for this setting, by combining temporal GNNs with a novel inventory\nmodule, which learns production functions via attention weights and a special\nloss function. We evaluate our models extensively on real supply chains data,\nalong with data generated from our new open-source simulator, SupplySim. Our\nmodels successfully infer production functions, with a 6-50% improvement over\nbaselines, and forecast future transactions on real and synthetic data,\noutperforming baselines by 11-62%.\n", "link": "http://arxiv.org/abs/2407.18772v1", "date": "2024-07-26", "relevancy": 1.8543, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4667}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4648}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20production%20functions%20for%20supply%20chains%20with%20graph%20neural%0A%20%20networks&body=Title%3A%20Learning%20production%20functions%20for%20supply%20chains%20with%20graph%20neural%0A%20%20networks%0AAuthor%3A%20Serina%20Chang%20and%20Zhiyin%20Lin%20and%20Benjamin%20Yan%20and%20Swapnil%20Bembde%20and%20Qi%20Xiu%20and%20Chi%20Heem%20Wong%20and%20Yu%20Qin%20and%20Frank%20Kloster%20and%20Alex%20Luo%20and%20Raj%20Palleti%20and%20Jure%20Leskovec%0AAbstract%3A%20%20%20The%20global%20economy%20relies%20on%20the%20flow%20of%20goods%20over%20supply%20chain%20networks%2C%0Awith%20nodes%20as%20firms%20and%20edges%20as%20transactions%20between%20firms.%20While%20we%20may%0Aobserve%20these%20external%20transactions%2C%20they%20are%20governed%20by%20unseen%20production%0Afunctions%2C%20which%20determine%20how%20firms%20internally%20transform%20the%20input%20products%0Athey%20receive%20into%20output%20products%20that%20they%20sell.%20In%20this%20setting%2C%20it%20can%20be%0Aextremely%20valuable%20to%20infer%20these%20production%20functions%2C%20to%20better%20understand%0Aand%20improve%20supply%20chains%2C%20and%20to%20forecast%20future%20transactions%20more%20accurately.%0AHowever%2C%20existing%20graph%20neural%20networks%20%28GNNs%29%20cannot%20capture%20these%20hidden%0Arelationships%20between%20nodes%27%20inputs%20and%20outputs.%20Here%2C%20we%20introduce%20a%20new%20class%0Aof%20models%20for%20this%20setting%2C%20by%20combining%20temporal%20GNNs%20with%20a%20novel%20inventory%0Amodule%2C%20which%20learns%20production%20functions%20via%20attention%20weights%20and%20a%20special%0Aloss%20function.%20We%20evaluate%20our%20models%20extensively%20on%20real%20supply%20chains%20data%2C%0Aalong%20with%20data%20generated%20from%20our%20new%20open-source%20simulator%2C%20SupplySim.%20Our%0Amodels%20successfully%20infer%20production%20functions%2C%20with%20a%206-50%25%20improvement%20over%0Abaselines%2C%20and%20forecast%20future%20transactions%20on%20real%20and%20synthetic%20data%2C%0Aoutperforming%20baselines%20by%2011-62%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520production%2520functions%2520for%2520supply%2520chains%2520with%2520graph%2520neural%250A%2520%2520networks%26entry.906535625%3DSerina%2520Chang%2520and%2520Zhiyin%2520Lin%2520and%2520Benjamin%2520Yan%2520and%2520Swapnil%2520Bembde%2520and%2520Qi%2520Xiu%2520and%2520Chi%2520Heem%2520Wong%2520and%2520Yu%2520Qin%2520and%2520Frank%2520Kloster%2520and%2520Alex%2520Luo%2520and%2520Raj%2520Palleti%2520and%2520Jure%2520Leskovec%26entry.1292438233%3D%2520%2520The%2520global%2520economy%2520relies%2520on%2520the%2520flow%2520of%2520goods%2520over%2520supply%2520chain%2520networks%252C%250Awith%2520nodes%2520as%2520firms%2520and%2520edges%2520as%2520transactions%2520between%2520firms.%2520While%2520we%2520may%250Aobserve%2520these%2520external%2520transactions%252C%2520they%2520are%2520governed%2520by%2520unseen%2520production%250Afunctions%252C%2520which%2520determine%2520how%2520firms%2520internally%2520transform%2520the%2520input%2520products%250Athey%2520receive%2520into%2520output%2520products%2520that%2520they%2520sell.%2520In%2520this%2520setting%252C%2520it%2520can%2520be%250Aextremely%2520valuable%2520to%2520infer%2520these%2520production%2520functions%252C%2520to%2520better%2520understand%250Aand%2520improve%2520supply%2520chains%252C%2520and%2520to%2520forecast%2520future%2520transactions%2520more%2520accurately.%250AHowever%252C%2520existing%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520cannot%2520capture%2520these%2520hidden%250Arelationships%2520between%2520nodes%2527%2520inputs%2520and%2520outputs.%2520Here%252C%2520we%2520introduce%2520a%2520new%2520class%250Aof%2520models%2520for%2520this%2520setting%252C%2520by%2520combining%2520temporal%2520GNNs%2520with%2520a%2520novel%2520inventory%250Amodule%252C%2520which%2520learns%2520production%2520functions%2520via%2520attention%2520weights%2520and%2520a%2520special%250Aloss%2520function.%2520We%2520evaluate%2520our%2520models%2520extensively%2520on%2520real%2520supply%2520chains%2520data%252C%250Aalong%2520with%2520data%2520generated%2520from%2520our%2520new%2520open-source%2520simulator%252C%2520SupplySim.%2520Our%250Amodels%2520successfully%2520infer%2520production%2520functions%252C%2520with%2520a%25206-50%2525%2520improvement%2520over%250Abaselines%252C%2520and%2520forecast%2520future%2520transactions%2520on%2520real%2520and%2520synthetic%2520data%252C%250Aoutperforming%2520baselines%2520by%252011-62%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20production%20functions%20for%20supply%20chains%20with%20graph%20neural%0A%20%20networks&entry.906535625=Serina%20Chang%20and%20Zhiyin%20Lin%20and%20Benjamin%20Yan%20and%20Swapnil%20Bembde%20and%20Qi%20Xiu%20and%20Chi%20Heem%20Wong%20and%20Yu%20Qin%20and%20Frank%20Kloster%20and%20Alex%20Luo%20and%20Raj%20Palleti%20and%20Jure%20Leskovec&entry.1292438233=%20%20The%20global%20economy%20relies%20on%20the%20flow%20of%20goods%20over%20supply%20chain%20networks%2C%0Awith%20nodes%20as%20firms%20and%20edges%20as%20transactions%20between%20firms.%20While%20we%20may%0Aobserve%20these%20external%20transactions%2C%20they%20are%20governed%20by%20unseen%20production%0Afunctions%2C%20which%20determine%20how%20firms%20internally%20transform%20the%20input%20products%0Athey%20receive%20into%20output%20products%20that%20they%20sell.%20In%20this%20setting%2C%20it%20can%20be%0Aextremely%20valuable%20to%20infer%20these%20production%20functions%2C%20to%20better%20understand%0Aand%20improve%20supply%20chains%2C%20and%20to%20forecast%20future%20transactions%20more%20accurately.%0AHowever%2C%20existing%20graph%20neural%20networks%20%28GNNs%29%20cannot%20capture%20these%20hidden%0Arelationships%20between%20nodes%27%20inputs%20and%20outputs.%20Here%2C%20we%20introduce%20a%20new%20class%0Aof%20models%20for%20this%20setting%2C%20by%20combining%20temporal%20GNNs%20with%20a%20novel%20inventory%0Amodule%2C%20which%20learns%20production%20functions%20via%20attention%20weights%20and%20a%20special%0Aloss%20function.%20We%20evaluate%20our%20models%20extensively%20on%20real%20supply%20chains%20data%2C%0Aalong%20with%20data%20generated%20from%20our%20new%20open-source%20simulator%2C%20SupplySim.%20Our%0Amodels%20successfully%20infer%20production%20functions%2C%20with%20a%206-50%25%20improvement%20over%0Abaselines%2C%20and%20forecast%20future%20transactions%20on%20real%20and%20synthetic%20data%2C%0Aoutperforming%20baselines%20by%2011-62%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18772v1&entry.124074799=Read"},
{"title": "Knowledge Graph Structure as Prompt: Improving Small Language Models\n  Capabilities for Knowledge-based Causal Discovery", "author": "Yuni Susanti and Michael F\u00e4rber", "abstract": "  Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.\n", "link": "http://arxiv.org/abs/2407.18752v1", "date": "2024-07-26", "relevancy": 1.8514, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery&body=Title%3A%20Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery%0AAuthor%3A%20Yuni%20Susanti%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20%20%20Causal%20discovery%20aims%20to%20estimate%20causal%20structures%20among%20variables%20based%20on%0Aobservational%20data.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20fresh%20perspective%20to%0Atackle%20the%20causal%20discovery%20problem%20by%20reasoning%20on%20the%20metadata%20associated%0Awith%20variables%20rather%20than%20their%20actual%20data%20values%2C%20an%20approach%20referred%20to%20as%0Aknowledge-based%20causal%20discovery.%20In%20this%20paper%2C%20we%20investigate%20the%0Acapabilities%20of%20Small%20Language%20Models%20%28SLMs%2C%20defined%20as%20LLMs%20with%20fewer%20than%201%0Abillion%20parameters%29%20with%20prompt-based%20learning%20for%20knowledge-based%20causal%0Adiscovery.%20Specifically%2C%20we%20present%20KG%20Structure%20as%20Prompt%2C%20a%20novel%20approach%0Afor%20integrating%20structural%20information%20from%20a%20knowledge%20graph%2C%20such%20as%20common%0Aneighbor%20nodes%20and%20metapaths%2C%20into%20prompt-based%20learning%20to%20enhance%20the%0Acapabilities%20of%20SLMs.%20Experimental%20results%20on%20three%20types%20of%20biomedical%20and%0Aopen-domain%20datasets%20under%20few-shot%20settings%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20surpassing%20most%20baselines%20and%20even%20conventional%20fine-tuning%0Aapproaches%20trained%20on%20full%20datasets.%20Our%20findings%20further%20highlight%20the%20strong%0Acapabilities%20of%20SLMs%3A%20in%20combination%20with%20knowledge%20graphs%20and%20prompt-based%0Alearning%2C%20SLMs%20demonstrate%20the%20potential%20to%20surpass%20LLMs%20with%20larger%20number%20of%0Aparameters.%20Our%20code%20and%20datasets%20are%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph%2520Structure%2520as%2520Prompt%253A%2520Improving%2520Small%2520Language%2520Models%250A%2520%2520Capabilities%2520for%2520Knowledge-based%2520Causal%2520Discovery%26entry.906535625%3DYuni%2520Susanti%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520aims%2520to%2520estimate%2520causal%2520structures%2520among%2520variables%2520based%2520on%250Aobservational%2520data.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520fresh%2520perspective%2520to%250Atackle%2520the%2520causal%2520discovery%2520problem%2520by%2520reasoning%2520on%2520the%2520metadata%2520associated%250Awith%2520variables%2520rather%2520than%2520their%2520actual%2520data%2520values%252C%2520an%2520approach%2520referred%2520to%2520as%250Aknowledge-based%2520causal%2520discovery.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Acapabilities%2520of%2520Small%2520Language%2520Models%2520%2528SLMs%252C%2520defined%2520as%2520LLMs%2520with%2520fewer%2520than%25201%250Abillion%2520parameters%2529%2520with%2520prompt-based%2520learning%2520for%2520knowledge-based%2520causal%250Adiscovery.%2520Specifically%252C%2520we%2520present%2520KG%2520Structure%2520as%2520Prompt%252C%2520a%2520novel%2520approach%250Afor%2520integrating%2520structural%2520information%2520from%2520a%2520knowledge%2520graph%252C%2520such%2520as%2520common%250Aneighbor%2520nodes%2520and%2520metapaths%252C%2520into%2520prompt-based%2520learning%2520to%2520enhance%2520the%250Acapabilities%2520of%2520SLMs.%2520Experimental%2520results%2520on%2520three%2520types%2520of%2520biomedical%2520and%250Aopen-domain%2520datasets%2520under%2520few-shot%2520settings%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%252C%2520surpassing%2520most%2520baselines%2520and%2520even%2520conventional%2520fine-tuning%250Aapproaches%2520trained%2520on%2520full%2520datasets.%2520Our%2520findings%2520further%2520highlight%2520the%2520strong%250Acapabilities%2520of%2520SLMs%253A%2520in%2520combination%2520with%2520knowledge%2520graphs%2520and%2520prompt-based%250Alearning%252C%2520SLMs%2520demonstrate%2520the%2520potential%2520to%2520surpass%2520LLMs%2520with%2520larger%2520number%2520of%250Aparameters.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph%20Structure%20as%20Prompt%3A%20Improving%20Small%20Language%20Models%0A%20%20Capabilities%20for%20Knowledge-based%20Causal%20Discovery&entry.906535625=Yuni%20Susanti%20and%20Michael%20F%C3%A4rber&entry.1292438233=%20%20Causal%20discovery%20aims%20to%20estimate%20causal%20structures%20among%20variables%20based%20on%0Aobservational%20data.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20fresh%20perspective%20to%0Atackle%20the%20causal%20discovery%20problem%20by%20reasoning%20on%20the%20metadata%20associated%0Awith%20variables%20rather%20than%20their%20actual%20data%20values%2C%20an%20approach%20referred%20to%20as%0Aknowledge-based%20causal%20discovery.%20In%20this%20paper%2C%20we%20investigate%20the%0Acapabilities%20of%20Small%20Language%20Models%20%28SLMs%2C%20defined%20as%20LLMs%20with%20fewer%20than%201%0Abillion%20parameters%29%20with%20prompt-based%20learning%20for%20knowledge-based%20causal%0Adiscovery.%20Specifically%2C%20we%20present%20KG%20Structure%20as%20Prompt%2C%20a%20novel%20approach%0Afor%20integrating%20structural%20information%20from%20a%20knowledge%20graph%2C%20such%20as%20common%0Aneighbor%20nodes%20and%20metapaths%2C%20into%20prompt-based%20learning%20to%20enhance%20the%0Acapabilities%20of%20SLMs.%20Experimental%20results%20on%20three%20types%20of%20biomedical%20and%0Aopen-domain%20datasets%20under%20few-shot%20settings%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20surpassing%20most%20baselines%20and%20even%20conventional%20fine-tuning%0Aapproaches%20trained%20on%20full%20datasets.%20Our%20findings%20further%20highlight%20the%20strong%0Acapabilities%20of%20SLMs%3A%20in%20combination%20with%20knowledge%20graphs%20and%20prompt-based%0Alearning%2C%20SLMs%20demonstrate%20the%20potential%20to%20surpass%20LLMs%20with%20larger%20number%20of%0Aparameters.%20Our%20code%20and%20datasets%20are%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18752v1&entry.124074799=Read"},
{"title": "The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks", "author": "Jona te Lintelo and Stefanos Koffas and Stjepan Picek", "abstract": "  Sponge attacks aim to increase the energy consumption and computation time of\nneural networks deployed on hardware accelerators. Existing sponge attacks can\nbe performed during inference via sponge examples or during training via Sponge\nPoisoning. Sponge examples leverage perturbations added to the model's input to\nincrease energy and latency, while Sponge Poisoning alters the objective\nfunction of a model to induce inference-time energy effects. In this work, we\npropose a novel sponge attack called SkipSponge. SkipSponge is the first sponge\nattack that is performed directly on the parameters of a pre-trained model\nusing only a few data samples. Our experiments show that SkipSponge can\nsuccessfully increase the energy consumption of image classification models,\nGANs, and autoencoders with fewer samples required than Sponge Poisoning. We\nshow that poisoning defenses are ineffective if not adjusted specifically for\nthe defense against SkipSponge (i.e., they decrease target layer bias values).\nOur work shows that SkipSponge is more effective on the GANs and the\nautoencoders than the state-of-the-art. Additionally, SkipSponge is stealthier\nthan the previous Sponge Poisoning attack as it does not require significant\nchanges in the victim model's weights. Our experiments indicate that the\nSkipSponge attack can be performed even when an attacker has access to only 1%\nof the entire dataset and reaches up to 13% energy increase.\n", "link": "http://arxiv.org/abs/2402.06357v3", "date": "2024-07-26", "relevancy": 1.8314, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4605}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20SkipSponge%20Attack%3A%20Sponge%20Weight%20Poisoning%20of%20Deep%20Neural%20Networks&body=Title%3A%20The%20SkipSponge%20Attack%3A%20Sponge%20Weight%20Poisoning%20of%20Deep%20Neural%20Networks%0AAuthor%3A%20Jona%20te%20Lintelo%20and%20Stefanos%20Koffas%20and%20Stjepan%20Picek%0AAbstract%3A%20%20%20Sponge%20attacks%20aim%20to%20increase%20the%20energy%20consumption%20and%20computation%20time%20of%0Aneural%20networks%20deployed%20on%20hardware%20accelerators.%20Existing%20sponge%20attacks%20can%0Abe%20performed%20during%20inference%20via%20sponge%20examples%20or%20during%20training%20via%20Sponge%0APoisoning.%20Sponge%20examples%20leverage%20perturbations%20added%20to%20the%20model%27s%20input%20to%0Aincrease%20energy%20and%20latency%2C%20while%20Sponge%20Poisoning%20alters%20the%20objective%0Afunction%20of%20a%20model%20to%20induce%20inference-time%20energy%20effects.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20sponge%20attack%20called%20SkipSponge.%20SkipSponge%20is%20the%20first%20sponge%0Aattack%20that%20is%20performed%20directly%20on%20the%20parameters%20of%20a%20pre-trained%20model%0Ausing%20only%20a%20few%20data%20samples.%20Our%20experiments%20show%20that%20SkipSponge%20can%0Asuccessfully%20increase%20the%20energy%20consumption%20of%20image%20classification%20models%2C%0AGANs%2C%20and%20autoencoders%20with%20fewer%20samples%20required%20than%20Sponge%20Poisoning.%20We%0Ashow%20that%20poisoning%20defenses%20are%20ineffective%20if%20not%20adjusted%20specifically%20for%0Athe%20defense%20against%20SkipSponge%20%28i.e.%2C%20they%20decrease%20target%20layer%20bias%20values%29.%0AOur%20work%20shows%20that%20SkipSponge%20is%20more%20effective%20on%20the%20GANs%20and%20the%0Aautoencoders%20than%20the%20state-of-the-art.%20Additionally%2C%20SkipSponge%20is%20stealthier%0Athan%20the%20previous%20Sponge%20Poisoning%20attack%20as%20it%20does%20not%20require%20significant%0Achanges%20in%20the%20victim%20model%27s%20weights.%20Our%20experiments%20indicate%20that%20the%0ASkipSponge%20attack%20can%20be%20performed%20even%20when%20an%20attacker%20has%20access%20to%20only%201%25%0Aof%20the%20entire%20dataset%20and%20reaches%20up%20to%2013%25%20energy%20increase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06357v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520SkipSponge%2520Attack%253A%2520Sponge%2520Weight%2520Poisoning%2520of%2520Deep%2520Neural%2520Networks%26entry.906535625%3DJona%2520te%2520Lintelo%2520and%2520Stefanos%2520Koffas%2520and%2520Stjepan%2520Picek%26entry.1292438233%3D%2520%2520Sponge%2520attacks%2520aim%2520to%2520increase%2520the%2520energy%2520consumption%2520and%2520computation%2520time%2520of%250Aneural%2520networks%2520deployed%2520on%2520hardware%2520accelerators.%2520Existing%2520sponge%2520attacks%2520can%250Abe%2520performed%2520during%2520inference%2520via%2520sponge%2520examples%2520or%2520during%2520training%2520via%2520Sponge%250APoisoning.%2520Sponge%2520examples%2520leverage%2520perturbations%2520added%2520to%2520the%2520model%2527s%2520input%2520to%250Aincrease%2520energy%2520and%2520latency%252C%2520while%2520Sponge%2520Poisoning%2520alters%2520the%2520objective%250Afunction%2520of%2520a%2520model%2520to%2520induce%2520inference-time%2520energy%2520effects.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520sponge%2520attack%2520called%2520SkipSponge.%2520SkipSponge%2520is%2520the%2520first%2520sponge%250Aattack%2520that%2520is%2520performed%2520directly%2520on%2520the%2520parameters%2520of%2520a%2520pre-trained%2520model%250Ausing%2520only%2520a%2520few%2520data%2520samples.%2520Our%2520experiments%2520show%2520that%2520SkipSponge%2520can%250Asuccessfully%2520increase%2520the%2520energy%2520consumption%2520of%2520image%2520classification%2520models%252C%250AGANs%252C%2520and%2520autoencoders%2520with%2520fewer%2520samples%2520required%2520than%2520Sponge%2520Poisoning.%2520We%250Ashow%2520that%2520poisoning%2520defenses%2520are%2520ineffective%2520if%2520not%2520adjusted%2520specifically%2520for%250Athe%2520defense%2520against%2520SkipSponge%2520%2528i.e.%252C%2520they%2520decrease%2520target%2520layer%2520bias%2520values%2529.%250AOur%2520work%2520shows%2520that%2520SkipSponge%2520is%2520more%2520effective%2520on%2520the%2520GANs%2520and%2520the%250Aautoencoders%2520than%2520the%2520state-of-the-art.%2520Additionally%252C%2520SkipSponge%2520is%2520stealthier%250Athan%2520the%2520previous%2520Sponge%2520Poisoning%2520attack%2520as%2520it%2520does%2520not%2520require%2520significant%250Achanges%2520in%2520the%2520victim%2520model%2527s%2520weights.%2520Our%2520experiments%2520indicate%2520that%2520the%250ASkipSponge%2520attack%2520can%2520be%2520performed%2520even%2520when%2520an%2520attacker%2520has%2520access%2520to%2520only%25201%2525%250Aof%2520the%2520entire%2520dataset%2520and%2520reaches%2520up%2520to%252013%2525%2520energy%2520increase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06357v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20SkipSponge%20Attack%3A%20Sponge%20Weight%20Poisoning%20of%20Deep%20Neural%20Networks&entry.906535625=Jona%20te%20Lintelo%20and%20Stefanos%20Koffas%20and%20Stjepan%20Picek&entry.1292438233=%20%20Sponge%20attacks%20aim%20to%20increase%20the%20energy%20consumption%20and%20computation%20time%20of%0Aneural%20networks%20deployed%20on%20hardware%20accelerators.%20Existing%20sponge%20attacks%20can%0Abe%20performed%20during%20inference%20via%20sponge%20examples%20or%20during%20training%20via%20Sponge%0APoisoning.%20Sponge%20examples%20leverage%20perturbations%20added%20to%20the%20model%27s%20input%20to%0Aincrease%20energy%20and%20latency%2C%20while%20Sponge%20Poisoning%20alters%20the%20objective%0Afunction%20of%20a%20model%20to%20induce%20inference-time%20energy%20effects.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20sponge%20attack%20called%20SkipSponge.%20SkipSponge%20is%20the%20first%20sponge%0Aattack%20that%20is%20performed%20directly%20on%20the%20parameters%20of%20a%20pre-trained%20model%0Ausing%20only%20a%20few%20data%20samples.%20Our%20experiments%20show%20that%20SkipSponge%20can%0Asuccessfully%20increase%20the%20energy%20consumption%20of%20image%20classification%20models%2C%0AGANs%2C%20and%20autoencoders%20with%20fewer%20samples%20required%20than%20Sponge%20Poisoning.%20We%0Ashow%20that%20poisoning%20defenses%20are%20ineffective%20if%20not%20adjusted%20specifically%20for%0Athe%20defense%20against%20SkipSponge%20%28i.e.%2C%20they%20decrease%20target%20layer%20bias%20values%29.%0AOur%20work%20shows%20that%20SkipSponge%20is%20more%20effective%20on%20the%20GANs%20and%20the%0Aautoencoders%20than%20the%20state-of-the-art.%20Additionally%2C%20SkipSponge%20is%20stealthier%0Athan%20the%20previous%20Sponge%20Poisoning%20attack%20as%20it%20does%20not%20require%20significant%0Achanges%20in%20the%20victim%20model%27s%20weights.%20Our%20experiments%20indicate%20that%20the%0ASkipSponge%20attack%20can%20be%20performed%20even%20when%20an%20attacker%20has%20access%20to%20only%201%25%0Aof%20the%20entire%20dataset%20and%20reaches%20up%20to%2013%25%20energy%20increase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06357v3&entry.124074799=Read"},
{"title": "On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs", "author": "Hankz Hankui Zhuo and Xin Chen and Rong Pan", "abstract": "  Plan synthesis aims to generate a course of actions or policies to transit\ngiven initial states to goal states, provided domain models that could be\ndesigned by experts or learnt from training data or interactions with the\nworld. Intrigued by the claims of emergent planning capabilities in large\nlanguage models (LLMs), works have been proposed to investigate the planning\neffectiveness of LLMs, without considering any utilization of off-the-shelf\nplanning techniques in LLMs. In this paper, we aim to further study the insight\nof the planning capability of LLMs by investigating the roles of LLMs in\noff-the-shelf planning frameworks. To do this, we investigate the effectiveness\nof embedding LLMs into one of the well-known planning frameworks, graph-based\nplanning, proposing a novel LLMs-based planning framework with LLMs embedded in\ntwo levels of planning graphs, i.e., mutual constraints generation level and\nconstraints solving level. We empirically exhibit the effectiveness of our\nproposed framework in various planning domains.\n", "link": "http://arxiv.org/abs/2403.00783v2", "date": "2024-07-26", "relevancy": 1.8146, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4818}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4697}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Roles%20of%20LLMs%20in%20Planning%3A%20Embedding%20LLMs%20into%20Planning%20Graphs&body=Title%3A%20On%20the%20Roles%20of%20LLMs%20in%20Planning%3A%20Embedding%20LLMs%20into%20Planning%20Graphs%0AAuthor%3A%20Hankz%20Hankui%20Zhuo%20and%20Xin%20Chen%20and%20Rong%20Pan%0AAbstract%3A%20%20%20Plan%20synthesis%20aims%20to%20generate%20a%20course%20of%20actions%20or%20policies%20to%20transit%0Agiven%20initial%20states%20to%20goal%20states%2C%20provided%20domain%20models%20that%20could%20be%0Adesigned%20by%20experts%20or%20learnt%20from%20training%20data%20or%20interactions%20with%20the%0Aworld.%20Intrigued%20by%20the%20claims%20of%20emergent%20planning%20capabilities%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20works%20have%20been%20proposed%20to%20investigate%20the%20planning%0Aeffectiveness%20of%20LLMs%2C%20without%20considering%20any%20utilization%20of%20off-the-shelf%0Aplanning%20techniques%20in%20LLMs.%20In%20this%20paper%2C%20we%20aim%20to%20further%20study%20the%20insight%0Aof%20the%20planning%20capability%20of%20LLMs%20by%20investigating%20the%20roles%20of%20LLMs%20in%0Aoff-the-shelf%20planning%20frameworks.%20To%20do%20this%2C%20we%20investigate%20the%20effectiveness%0Aof%20embedding%20LLMs%20into%20one%20of%20the%20well-known%20planning%20frameworks%2C%20graph-based%0Aplanning%2C%20proposing%20a%20novel%20LLMs-based%20planning%20framework%20with%20LLMs%20embedded%20in%0Atwo%20levels%20of%20planning%20graphs%2C%20i.e.%2C%20mutual%20constraints%20generation%20level%20and%0Aconstraints%20solving%20level.%20We%20empirically%20exhibit%20the%20effectiveness%20of%20our%0Aproposed%20framework%20in%20various%20planning%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Roles%2520of%2520LLMs%2520in%2520Planning%253A%2520Embedding%2520LLMs%2520into%2520Planning%2520Graphs%26entry.906535625%3DHankz%2520Hankui%2520Zhuo%2520and%2520Xin%2520Chen%2520and%2520Rong%2520Pan%26entry.1292438233%3D%2520%2520Plan%2520synthesis%2520aims%2520to%2520generate%2520a%2520course%2520of%2520actions%2520or%2520policies%2520to%2520transit%250Agiven%2520initial%2520states%2520to%2520goal%2520states%252C%2520provided%2520domain%2520models%2520that%2520could%2520be%250Adesigned%2520by%2520experts%2520or%2520learnt%2520from%2520training%2520data%2520or%2520interactions%2520with%2520the%250Aworld.%2520Intrigued%2520by%2520the%2520claims%2520of%2520emergent%2520planning%2520capabilities%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520works%2520have%2520been%2520proposed%2520to%2520investigate%2520the%2520planning%250Aeffectiveness%2520of%2520LLMs%252C%2520without%2520considering%2520any%2520utilization%2520of%2520off-the-shelf%250Aplanning%2520techniques%2520in%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520further%2520study%2520the%2520insight%250Aof%2520the%2520planning%2520capability%2520of%2520LLMs%2520by%2520investigating%2520the%2520roles%2520of%2520LLMs%2520in%250Aoff-the-shelf%2520planning%2520frameworks.%2520To%2520do%2520this%252C%2520we%2520investigate%2520the%2520effectiveness%250Aof%2520embedding%2520LLMs%2520into%2520one%2520of%2520the%2520well-known%2520planning%2520frameworks%252C%2520graph-based%250Aplanning%252C%2520proposing%2520a%2520novel%2520LLMs-based%2520planning%2520framework%2520with%2520LLMs%2520embedded%2520in%250Atwo%2520levels%2520of%2520planning%2520graphs%252C%2520i.e.%252C%2520mutual%2520constraints%2520generation%2520level%2520and%250Aconstraints%2520solving%2520level.%2520We%2520empirically%2520exhibit%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520framework%2520in%2520various%2520planning%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Roles%20of%20LLMs%20in%20Planning%3A%20Embedding%20LLMs%20into%20Planning%20Graphs&entry.906535625=Hankz%20Hankui%20Zhuo%20and%20Xin%20Chen%20and%20Rong%20Pan&entry.1292438233=%20%20Plan%20synthesis%20aims%20to%20generate%20a%20course%20of%20actions%20or%20policies%20to%20transit%0Agiven%20initial%20states%20to%20goal%20states%2C%20provided%20domain%20models%20that%20could%20be%0Adesigned%20by%20experts%20or%20learnt%20from%20training%20data%20or%20interactions%20with%20the%0Aworld.%20Intrigued%20by%20the%20claims%20of%20emergent%20planning%20capabilities%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20works%20have%20been%20proposed%20to%20investigate%20the%20planning%0Aeffectiveness%20of%20LLMs%2C%20without%20considering%20any%20utilization%20of%20off-the-shelf%0Aplanning%20techniques%20in%20LLMs.%20In%20this%20paper%2C%20we%20aim%20to%20further%20study%20the%20insight%0Aof%20the%20planning%20capability%20of%20LLMs%20by%20investigating%20the%20roles%20of%20LLMs%20in%0Aoff-the-shelf%20planning%20frameworks.%20To%20do%20this%2C%20we%20investigate%20the%20effectiveness%0Aof%20embedding%20LLMs%20into%20one%20of%20the%20well-known%20planning%20frameworks%2C%20graph-based%0Aplanning%2C%20proposing%20a%20novel%20LLMs-based%20planning%20framework%20with%20LLMs%20embedded%20in%0Atwo%20levels%20of%20planning%20graphs%2C%20i.e.%2C%20mutual%20constraints%20generation%20level%20and%0Aconstraints%20solving%20level.%20We%20empirically%20exhibit%20the%20effectiveness%20of%20our%0Aproposed%20framework%20in%20various%20planning%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00783v2&entry.124074799=Read"},
{"title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies", "author": "Chaofan Tao and Qian Liu and Longxu Dou and Niklas Muennighoff and Zhongwei Wan and Ping Luo and Min Lin and Ngai Wong", "abstract": "  Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. We investigate how vocabulary size impacts LLM scaling laws by training\nmodels ranging from 33M to 3B parameters on up to 500B characters with various\nvocabulary configurations. We propose three complementary approaches for\npredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative\nestimation, and parametric fit of the loss function. Our approaches converge on\nthe same result that the optimal vocabulary size depends on the available\ncompute budget and that larger models deserve larger vocabularies. However,\nmost LLMs use too small vocabulary sizes. For example, we predict that the\noptimal vocabulary size of Llama2-70B should have been at least 216K, 7 times\nlarger than its vocabulary of 32K. We validate our predictions empirically by\ntraining models with 3B parameters across different FLOPs budgets. Adopting our\npredicted optimal vocabulary size consistently improves downstream performance\nover commonly used vocabulary sizes. By increasing the vocabulary size from the\nconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to\n32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly\nconsidering model parameters and vocabulary size for efficient scaling.\n", "link": "http://arxiv.org/abs/2407.13623v2", "date": "2024-07-26", "relevancy": 1.8133, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Laws%20with%20Vocabulary%3A%20Larger%20Models%20Deserve%20Larger%20Vocabularies&body=Title%3A%20Scaling%20Laws%20with%20Vocabulary%3A%20Larger%20Models%20Deserve%20Larger%20Vocabularies%0AAuthor%3A%20Chaofan%20Tao%20and%20Qian%20Liu%20and%20Longxu%20Dou%20and%20Niklas%20Muennighoff%20and%20Zhongwei%20Wan%20and%20Ping%20Luo%20and%20Min%20Lin%20and%20Ngai%20Wong%0AAbstract%3A%20%20%20Research%20on%20scaling%20large%20language%20models%20%28LLMs%29%20has%20primarily%20focused%20on%0Amodel%20parameters%20and%20training%20data%20size%2C%20overlooking%20the%20role%20of%20vocabulary%0Asize.%20We%20investigate%20how%20vocabulary%20size%20impacts%20LLM%20scaling%20laws%20by%20training%0Amodels%20ranging%20from%2033M%20to%203B%20parameters%20on%20up%20to%20500B%20characters%20with%20various%0Avocabulary%20configurations.%20We%20propose%20three%20complementary%20approaches%20for%0Apredicting%20the%20compute-optimal%20vocabulary%20size%3A%20IsoFLOPs%20analysis%2C%20derivative%0Aestimation%2C%20and%20parametric%20fit%20of%20the%20loss%20function.%20Our%20approaches%20converge%20on%0Athe%20same%20result%20that%20the%20optimal%20vocabulary%20size%20depends%20on%20the%20available%0Acompute%20budget%20and%20that%20larger%20models%20deserve%20larger%20vocabularies.%20However%2C%0Amost%20LLMs%20use%20too%20small%20vocabulary%20sizes.%20For%20example%2C%20we%20predict%20that%20the%0Aoptimal%20vocabulary%20size%20of%20Llama2-70B%20should%20have%20been%20at%20least%20216K%2C%207%20times%0Alarger%20than%20its%20vocabulary%20of%2032K.%20We%20validate%20our%20predictions%20empirically%20by%0Atraining%20models%20with%203B%20parameters%20across%20different%20FLOPs%20budgets.%20Adopting%20our%0Apredicted%20optimal%20vocabulary%20size%20consistently%20improves%20downstream%20performance%0Aover%20commonly%20used%20vocabulary%20sizes.%20By%20increasing%20the%20vocabulary%20size%20from%20the%0Aconventional%2032K%20to%2043K%2C%20we%20improve%20performance%20on%20ARC-Challenge%20from%2029.1%20to%0A32.0%20with%20the%20same%202.3e21%20FLOPs.%20Our%20work%20emphasizes%20the%20necessity%20of%20jointly%0Aconsidering%20model%20parameters%20and%20vocabulary%20size%20for%20efficient%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Laws%2520with%2520Vocabulary%253A%2520Larger%2520Models%2520Deserve%2520Larger%2520Vocabularies%26entry.906535625%3DChaofan%2520Tao%2520and%2520Qian%2520Liu%2520and%2520Longxu%2520Dou%2520and%2520Niklas%2520Muennighoff%2520and%2520Zhongwei%2520Wan%2520and%2520Ping%2520Luo%2520and%2520Min%2520Lin%2520and%2520Ngai%2520Wong%26entry.1292438233%3D%2520%2520Research%2520on%2520scaling%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520primarily%2520focused%2520on%250Amodel%2520parameters%2520and%2520training%2520data%2520size%252C%2520overlooking%2520the%2520role%2520of%2520vocabulary%250Asize.%2520We%2520investigate%2520how%2520vocabulary%2520size%2520impacts%2520LLM%2520scaling%2520laws%2520by%2520training%250Amodels%2520ranging%2520from%252033M%2520to%25203B%2520parameters%2520on%2520up%2520to%2520500B%2520characters%2520with%2520various%250Avocabulary%2520configurations.%2520We%2520propose%2520three%2520complementary%2520approaches%2520for%250Apredicting%2520the%2520compute-optimal%2520vocabulary%2520size%253A%2520IsoFLOPs%2520analysis%252C%2520derivative%250Aestimation%252C%2520and%2520parametric%2520fit%2520of%2520the%2520loss%2520function.%2520Our%2520approaches%2520converge%2520on%250Athe%2520same%2520result%2520that%2520the%2520optimal%2520vocabulary%2520size%2520depends%2520on%2520the%2520available%250Acompute%2520budget%2520and%2520that%2520larger%2520models%2520deserve%2520larger%2520vocabularies.%2520However%252C%250Amost%2520LLMs%2520use%2520too%2520small%2520vocabulary%2520sizes.%2520For%2520example%252C%2520we%2520predict%2520that%2520the%250Aoptimal%2520vocabulary%2520size%2520of%2520Llama2-70B%2520should%2520have%2520been%2520at%2520least%2520216K%252C%25207%2520times%250Alarger%2520than%2520its%2520vocabulary%2520of%252032K.%2520We%2520validate%2520our%2520predictions%2520empirically%2520by%250Atraining%2520models%2520with%25203B%2520parameters%2520across%2520different%2520FLOPs%2520budgets.%2520Adopting%2520our%250Apredicted%2520optimal%2520vocabulary%2520size%2520consistently%2520improves%2520downstream%2520performance%250Aover%2520commonly%2520used%2520vocabulary%2520sizes.%2520By%2520increasing%2520the%2520vocabulary%2520size%2520from%2520the%250Aconventional%252032K%2520to%252043K%252C%2520we%2520improve%2520performance%2520on%2520ARC-Challenge%2520from%252029.1%2520to%250A32.0%2520with%2520the%2520same%25202.3e21%2520FLOPs.%2520Our%2520work%2520emphasizes%2520the%2520necessity%2520of%2520jointly%250Aconsidering%2520model%2520parameters%2520and%2520vocabulary%2520size%2520for%2520efficient%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Laws%20with%20Vocabulary%3A%20Larger%20Models%20Deserve%20Larger%20Vocabularies&entry.906535625=Chaofan%20Tao%20and%20Qian%20Liu%20and%20Longxu%20Dou%20and%20Niklas%20Muennighoff%20and%20Zhongwei%20Wan%20and%20Ping%20Luo%20and%20Min%20Lin%20and%20Ngai%20Wong&entry.1292438233=%20%20Research%20on%20scaling%20large%20language%20models%20%28LLMs%29%20has%20primarily%20focused%20on%0Amodel%20parameters%20and%20training%20data%20size%2C%20overlooking%20the%20role%20of%20vocabulary%0Asize.%20We%20investigate%20how%20vocabulary%20size%20impacts%20LLM%20scaling%20laws%20by%20training%0Amodels%20ranging%20from%2033M%20to%203B%20parameters%20on%20up%20to%20500B%20characters%20with%20various%0Avocabulary%20configurations.%20We%20propose%20three%20complementary%20approaches%20for%0Apredicting%20the%20compute-optimal%20vocabulary%20size%3A%20IsoFLOPs%20analysis%2C%20derivative%0Aestimation%2C%20and%20parametric%20fit%20of%20the%20loss%20function.%20Our%20approaches%20converge%20on%0Athe%20same%20result%20that%20the%20optimal%20vocabulary%20size%20depends%20on%20the%20available%0Acompute%20budget%20and%20that%20larger%20models%20deserve%20larger%20vocabularies.%20However%2C%0Amost%20LLMs%20use%20too%20small%20vocabulary%20sizes.%20For%20example%2C%20we%20predict%20that%20the%0Aoptimal%20vocabulary%20size%20of%20Llama2-70B%20should%20have%20been%20at%20least%20216K%2C%207%20times%0Alarger%20than%20its%20vocabulary%20of%2032K.%20We%20validate%20our%20predictions%20empirically%20by%0Atraining%20models%20with%203B%20parameters%20across%20different%20FLOPs%20budgets.%20Adopting%20our%0Apredicted%20optimal%20vocabulary%20size%20consistently%20improves%20downstream%20performance%0Aover%20commonly%20used%20vocabulary%20sizes.%20By%20increasing%20the%20vocabulary%20size%20from%20the%0Aconventional%2032K%20to%2043K%2C%20we%20improve%20performance%20on%20ARC-Challenge%20from%2029.1%20to%0A32.0%20with%20the%20same%202.3e21%20FLOPs.%20Our%20work%20emphasizes%20the%20necessity%20of%20jointly%0Aconsidering%20model%20parameters%20and%20vocabulary%20size%20for%20efficient%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13623v2&entry.124074799=Read"},
{"title": "Large Language Model for Table Processing: A Survey", "author": "Weizheng Lu and Jing Zhang and Ju Fan and Zihao Fu and Yueguo Chen and Xiaoyong Du", "abstract": "  Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including processing implicit user intentions and\nextracting information from various table sources.\n", "link": "http://arxiv.org/abs/2402.05121v2", "date": "2024-07-26", "relevancy": 1.7973, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4519}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20for%20Table%20Processing%3A%20A%20Survey&body=Title%3A%20Large%20Language%20Model%20for%20Table%20Processing%3A%20A%20Survey%0AAuthor%3A%20Weizheng%20Lu%20and%20Jing%20Zhang%20and%20Ju%20Fan%20and%20Zihao%20Fu%20and%20Yueguo%20Chen%20and%20Xiaoyong%20Du%0AAbstract%3A%20%20%20Tables%2C%20typically%20two-dimensional%20and%20structured%20to%20store%20large%20amounts%20of%0Adata%2C%20are%20essential%20in%20daily%20activities%20like%20database%20queries%2C%20spreadsheet%0Amanipulations%2C%20web%20table%20question%20answering%2C%20and%20image%20table%20information%0Aextraction.%20Automating%20these%20table-centric%20tasks%20with%20Large%20Language%20Models%0A%28LLMs%29%20or%20Visual%20Language%20Models%20%28VLMs%29%20offers%20significant%20public%20benefits%2C%0Agarnering%20interest%20from%20academia%20and%20industry.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20table-related%20tasks%2C%20examining%20both%20user%20scenarios%0Aand%20technical%20aspects.%20It%20covers%20traditional%20tasks%20like%20table%20question%0Aanswering%20as%20well%20as%20emerging%20fields%20such%20as%20spreadsheet%20manipulation%20and%20table%0Adata%20analysis.%20We%20summarize%20the%20training%20techniques%20for%20LLMs%20and%20VLMs%20tailored%0Afor%20table%20processing.%20Additionally%2C%20we%20discuss%20prompt%20engineering%2C%20particularly%0Athe%20use%20of%20LLM-powered%20agents%2C%20for%20various%20table-related%20tasks.%20Finally%2C%20we%0Ahighlight%20several%20challenges%2C%20including%20processing%20implicit%20user%20intentions%20and%0Aextracting%20information%20from%20various%20table%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05121v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520for%2520Table%2520Processing%253A%2520A%2520Survey%26entry.906535625%3DWeizheng%2520Lu%2520and%2520Jing%2520Zhang%2520and%2520Ju%2520Fan%2520and%2520Zihao%2520Fu%2520and%2520Yueguo%2520Chen%2520and%2520Xiaoyong%2520Du%26entry.1292438233%3D%2520%2520Tables%252C%2520typically%2520two-dimensional%2520and%2520structured%2520to%2520store%2520large%2520amounts%2520of%250Adata%252C%2520are%2520essential%2520in%2520daily%2520activities%2520like%2520database%2520queries%252C%2520spreadsheet%250Amanipulations%252C%2520web%2520table%2520question%2520answering%252C%2520and%2520image%2520table%2520information%250Aextraction.%2520Automating%2520these%2520table-centric%2520tasks%2520with%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520or%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520offers%2520significant%2520public%2520benefits%252C%250Agarnering%2520interest%2520from%2520academia%2520and%2520industry.%2520This%2520survey%2520provides%2520a%250Acomprehensive%2520overview%2520of%2520table-related%2520tasks%252C%2520examining%2520both%2520user%2520scenarios%250Aand%2520technical%2520aspects.%2520It%2520covers%2520traditional%2520tasks%2520like%2520table%2520question%250Aanswering%2520as%2520well%2520as%2520emerging%2520fields%2520such%2520as%2520spreadsheet%2520manipulation%2520and%2520table%250Adata%2520analysis.%2520We%2520summarize%2520the%2520training%2520techniques%2520for%2520LLMs%2520and%2520VLMs%2520tailored%250Afor%2520table%2520processing.%2520Additionally%252C%2520we%2520discuss%2520prompt%2520engineering%252C%2520particularly%250Athe%2520use%2520of%2520LLM-powered%2520agents%252C%2520for%2520various%2520table-related%2520tasks.%2520Finally%252C%2520we%250Ahighlight%2520several%2520challenges%252C%2520including%2520processing%2520implicit%2520user%2520intentions%2520and%250Aextracting%2520information%2520from%2520various%2520table%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05121v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20for%20Table%20Processing%3A%20A%20Survey&entry.906535625=Weizheng%20Lu%20and%20Jing%20Zhang%20and%20Ju%20Fan%20and%20Zihao%20Fu%20and%20Yueguo%20Chen%20and%20Xiaoyong%20Du&entry.1292438233=%20%20Tables%2C%20typically%20two-dimensional%20and%20structured%20to%20store%20large%20amounts%20of%0Adata%2C%20are%20essential%20in%20daily%20activities%20like%20database%20queries%2C%20spreadsheet%0Amanipulations%2C%20web%20table%20question%20answering%2C%20and%20image%20table%20information%0Aextraction.%20Automating%20these%20table-centric%20tasks%20with%20Large%20Language%20Models%0A%28LLMs%29%20or%20Visual%20Language%20Models%20%28VLMs%29%20offers%20significant%20public%20benefits%2C%0Agarnering%20interest%20from%20academia%20and%20industry.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20table-related%20tasks%2C%20examining%20both%20user%20scenarios%0Aand%20technical%20aspects.%20It%20covers%20traditional%20tasks%20like%20table%20question%0Aanswering%20as%20well%20as%20emerging%20fields%20such%20as%20spreadsheet%20manipulation%20and%20table%0Adata%20analysis.%20We%20summarize%20the%20training%20techniques%20for%20LLMs%20and%20VLMs%20tailored%0Afor%20table%20processing.%20Additionally%2C%20we%20discuss%20prompt%20engineering%2C%20particularly%0Athe%20use%20of%20LLM-powered%20agents%2C%20for%20various%20table-related%20tasks.%20Finally%2C%20we%0Ahighlight%20several%20challenges%2C%20including%20processing%20implicit%20user%20intentions%20and%0Aextracting%20information%20from%20various%20table%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05121v2&entry.124074799=Read"},
{"title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI", "author": "Andr\u00e9 Platzer", "abstract": "  This perspective piece calls for the study of the new field of Intersymbolic\nAI, by which we mean the combination of symbolic AI, whose building blocks have\ninherent significance/meaning, with subsymbolic AI, whose entirety creates\nsignificance/effect despite the fact that individual building blocks escape\nmeaning. Canonical kinds of symbolic AI are logic, games and planning.\nCanonical kinds of subsymbolic AI are (un)supervised machine and reinforcement\nlearning. Intersymbolic AI interlinks the worlds of symbolic AI with its\ncompositional symbolic significance and meaning and of subsymbolic AI with its\nsummative significance or effect to enable culminations of insights from both\nworlds by going between and across symbolic AI insights with subsymbolic AI\ntechniques that are being helped by symbolic AI principles. For example,\nIntersymbolic AI may start with symbolic AI to understand a dynamic system,\ncontinue with subsymbolic AI to learn its control, and end with symbolic AI to\nsafely use the outcome of the learned subsymbolic AI controller in the dynamic\nsystem. The way Intersymbolic AI combines both symbolic and subsymbolic AI to\nincrease the effectiveness of AI compared to either kind of AI alone is likened\nto the way that the combination of both conscious and subconscious thought\nincreases the effectiveness of human thought compared to either kind of thought\nalone. Some successful contributions to the Intersymbolic AI paradigm are\nsurveyed here but many more are considered possible by advancing Intersymbolic\nAI.\n", "link": "http://arxiv.org/abs/2406.11563v3", "date": "2024-07-26", "relevancy": 1.7956, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4535}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4519}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intersymbolic%20AI%3A%20Interlinking%20Symbolic%20AI%20and%20Subsymbolic%20AI&body=Title%3A%20Intersymbolic%20AI%3A%20Interlinking%20Symbolic%20AI%20and%20Subsymbolic%20AI%0AAuthor%3A%20Andr%C3%A9%20Platzer%0AAbstract%3A%20%20%20This%20perspective%20piece%20calls%20for%20the%20study%20of%20the%20new%20field%20of%20Intersymbolic%0AAI%2C%20by%20which%20we%20mean%20the%20combination%20of%20symbolic%20AI%2C%20whose%20building%20blocks%20have%0Ainherent%20significance/meaning%2C%20with%20subsymbolic%20AI%2C%20whose%20entirety%20creates%0Asignificance/effect%20despite%20the%20fact%20that%20individual%20building%20blocks%20escape%0Ameaning.%20Canonical%20kinds%20of%20symbolic%20AI%20are%20logic%2C%20games%20and%20planning.%0ACanonical%20kinds%20of%20subsymbolic%20AI%20are%20%28un%29supervised%20machine%20and%20reinforcement%0Alearning.%20Intersymbolic%20AI%20interlinks%20the%20worlds%20of%20symbolic%20AI%20with%20its%0Acompositional%20symbolic%20significance%20and%20meaning%20and%20of%20subsymbolic%20AI%20with%20its%0Asummative%20significance%20or%20effect%20to%20enable%20culminations%20of%20insights%20from%20both%0Aworlds%20by%20going%20between%20and%20across%20symbolic%20AI%20insights%20with%20subsymbolic%20AI%0Atechniques%20that%20are%20being%20helped%20by%20symbolic%20AI%20principles.%20For%20example%2C%0AIntersymbolic%20AI%20may%20start%20with%20symbolic%20AI%20to%20understand%20a%20dynamic%20system%2C%0Acontinue%20with%20subsymbolic%20AI%20to%20learn%20its%20control%2C%20and%20end%20with%20symbolic%20AI%20to%0Asafely%20use%20the%20outcome%20of%20the%20learned%20subsymbolic%20AI%20controller%20in%20the%20dynamic%0Asystem.%20The%20way%20Intersymbolic%20AI%20combines%20both%20symbolic%20and%20subsymbolic%20AI%20to%0Aincrease%20the%20effectiveness%20of%20AI%20compared%20to%20either%20kind%20of%20AI%20alone%20is%20likened%0Ato%20the%20way%20that%20the%20combination%20of%20both%20conscious%20and%20subconscious%20thought%0Aincreases%20the%20effectiveness%20of%20human%20thought%20compared%20to%20either%20kind%20of%20thought%0Aalone.%20Some%20successful%20contributions%20to%20the%20Intersymbolic%20AI%20paradigm%20are%0Asurveyed%20here%20but%20many%20more%20are%20considered%20possible%20by%20advancing%20Intersymbolic%0AAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntersymbolic%2520AI%253A%2520Interlinking%2520Symbolic%2520AI%2520and%2520Subsymbolic%2520AI%26entry.906535625%3DAndr%25C3%25A9%2520Platzer%26entry.1292438233%3D%2520%2520This%2520perspective%2520piece%2520calls%2520for%2520the%2520study%2520of%2520the%2520new%2520field%2520of%2520Intersymbolic%250AAI%252C%2520by%2520which%2520we%2520mean%2520the%2520combination%2520of%2520symbolic%2520AI%252C%2520whose%2520building%2520blocks%2520have%250Ainherent%2520significance/meaning%252C%2520with%2520subsymbolic%2520AI%252C%2520whose%2520entirety%2520creates%250Asignificance/effect%2520despite%2520the%2520fact%2520that%2520individual%2520building%2520blocks%2520escape%250Ameaning.%2520Canonical%2520kinds%2520of%2520symbolic%2520AI%2520are%2520logic%252C%2520games%2520and%2520planning.%250ACanonical%2520kinds%2520of%2520subsymbolic%2520AI%2520are%2520%2528un%2529supervised%2520machine%2520and%2520reinforcement%250Alearning.%2520Intersymbolic%2520AI%2520interlinks%2520the%2520worlds%2520of%2520symbolic%2520AI%2520with%2520its%250Acompositional%2520symbolic%2520significance%2520and%2520meaning%2520and%2520of%2520subsymbolic%2520AI%2520with%2520its%250Asummative%2520significance%2520or%2520effect%2520to%2520enable%2520culminations%2520of%2520insights%2520from%2520both%250Aworlds%2520by%2520going%2520between%2520and%2520across%2520symbolic%2520AI%2520insights%2520with%2520subsymbolic%2520AI%250Atechniques%2520that%2520are%2520being%2520helped%2520by%2520symbolic%2520AI%2520principles.%2520For%2520example%252C%250AIntersymbolic%2520AI%2520may%2520start%2520with%2520symbolic%2520AI%2520to%2520understand%2520a%2520dynamic%2520system%252C%250Acontinue%2520with%2520subsymbolic%2520AI%2520to%2520learn%2520its%2520control%252C%2520and%2520end%2520with%2520symbolic%2520AI%2520to%250Asafely%2520use%2520the%2520outcome%2520of%2520the%2520learned%2520subsymbolic%2520AI%2520controller%2520in%2520the%2520dynamic%250Asystem.%2520The%2520way%2520Intersymbolic%2520AI%2520combines%2520both%2520symbolic%2520and%2520subsymbolic%2520AI%2520to%250Aincrease%2520the%2520effectiveness%2520of%2520AI%2520compared%2520to%2520either%2520kind%2520of%2520AI%2520alone%2520is%2520likened%250Ato%2520the%2520way%2520that%2520the%2520combination%2520of%2520both%2520conscious%2520and%2520subconscious%2520thought%250Aincreases%2520the%2520effectiveness%2520of%2520human%2520thought%2520compared%2520to%2520either%2520kind%2520of%2520thought%250Aalone.%2520Some%2520successful%2520contributions%2520to%2520the%2520Intersymbolic%2520AI%2520paradigm%2520are%250Asurveyed%2520here%2520but%2520many%2520more%2520are%2520considered%2520possible%2520by%2520advancing%2520Intersymbolic%250AAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intersymbolic%20AI%3A%20Interlinking%20Symbolic%20AI%20and%20Subsymbolic%20AI&entry.906535625=Andr%C3%A9%20Platzer&entry.1292438233=%20%20This%20perspective%20piece%20calls%20for%20the%20study%20of%20the%20new%20field%20of%20Intersymbolic%0AAI%2C%20by%20which%20we%20mean%20the%20combination%20of%20symbolic%20AI%2C%20whose%20building%20blocks%20have%0Ainherent%20significance/meaning%2C%20with%20subsymbolic%20AI%2C%20whose%20entirety%20creates%0Asignificance/effect%20despite%20the%20fact%20that%20individual%20building%20blocks%20escape%0Ameaning.%20Canonical%20kinds%20of%20symbolic%20AI%20are%20logic%2C%20games%20and%20planning.%0ACanonical%20kinds%20of%20subsymbolic%20AI%20are%20%28un%29supervised%20machine%20and%20reinforcement%0Alearning.%20Intersymbolic%20AI%20interlinks%20the%20worlds%20of%20symbolic%20AI%20with%20its%0Acompositional%20symbolic%20significance%20and%20meaning%20and%20of%20subsymbolic%20AI%20with%20its%0Asummative%20significance%20or%20effect%20to%20enable%20culminations%20of%20insights%20from%20both%0Aworlds%20by%20going%20between%20and%20across%20symbolic%20AI%20insights%20with%20subsymbolic%20AI%0Atechniques%20that%20are%20being%20helped%20by%20symbolic%20AI%20principles.%20For%20example%2C%0AIntersymbolic%20AI%20may%20start%20with%20symbolic%20AI%20to%20understand%20a%20dynamic%20system%2C%0Acontinue%20with%20subsymbolic%20AI%20to%20learn%20its%20control%2C%20and%20end%20with%20symbolic%20AI%20to%0Asafely%20use%20the%20outcome%20of%20the%20learned%20subsymbolic%20AI%20controller%20in%20the%20dynamic%0Asystem.%20The%20way%20Intersymbolic%20AI%20combines%20both%20symbolic%20and%20subsymbolic%20AI%20to%0Aincrease%20the%20effectiveness%20of%20AI%20compared%20to%20either%20kind%20of%20AI%20alone%20is%20likened%0Ato%20the%20way%20that%20the%20combination%20of%20both%20conscious%20and%20subconscious%20thought%0Aincreases%20the%20effectiveness%20of%20human%20thought%20compared%20to%20either%20kind%20of%20thought%0Aalone.%20Some%20successful%20contributions%20to%20the%20Intersymbolic%20AI%20paradigm%20are%0Asurveyed%20here%20but%20many%20more%20are%20considered%20possible%20by%20advancing%20Intersymbolic%0AAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11563v3&entry.124074799=Read"},
{"title": "Optimizing Design and Control Methods for Using Collaborative Robots in\n  Upper-Limb Rehabilitation", "author": "Dario Onfiani and Marco Caramaschi and Luigi Biagiotti and Fabio Pini", "abstract": "  In this paper, we address the development of a robotic rehabilitation system\nfor the upper limbs based on collaborative end-effector solutions. The use of\ncommercial collaborative robots offers significant advantages for this task, as\nthey are optimized from an engineering perspective and ensure safe physical\ninteraction with humans. However, they also come with noticeable drawbacks,\nsuch as the limited range of sizes available on the market and the standard\ncontrol modes, which are primarily oriented towards industrial or service\napplications. To address these limitations, we propose an optimization-based\ndesign method to fully exploit the capability of the cobot in performing\nrehabilitation tasks. Additionally, we introduce a novel control architecture\nbased on an admittance-type Virtual Fixture method, which constrains the motion\nof the robot along a prescribed path. This approach allows for an intuitive\ndefinition of the task to be performed via Programming by Demonstration and\nenables the system to operate both passively and actively. In passive mode, the\nsystem supports the patient during task execution with additional force, while\nin active mode, it opposes the motion with a braking force. Experimental\nresults demonstrate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2407.18661v1", "date": "2024-07-26", "relevancy": 1.7949, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6309}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Design%20and%20Control%20Methods%20for%20Using%20Collaborative%20Robots%20in%0A%20%20Upper-Limb%20Rehabilitation&body=Title%3A%20Optimizing%20Design%20and%20Control%20Methods%20for%20Using%20Collaborative%20Robots%20in%0A%20%20Upper-Limb%20Rehabilitation%0AAuthor%3A%20Dario%20Onfiani%20and%20Marco%20Caramaschi%20and%20Luigi%20Biagiotti%20and%20Fabio%20Pini%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20development%20of%20a%20robotic%20rehabilitation%20system%0Afor%20the%20upper%20limbs%20based%20on%20collaborative%20end-effector%20solutions.%20The%20use%20of%0Acommercial%20collaborative%20robots%20offers%20significant%20advantages%20for%20this%20task%2C%20as%0Athey%20are%20optimized%20from%20an%20engineering%20perspective%20and%20ensure%20safe%20physical%0Ainteraction%20with%20humans.%20However%2C%20they%20also%20come%20with%20noticeable%20drawbacks%2C%0Asuch%20as%20the%20limited%20range%20of%20sizes%20available%20on%20the%20market%20and%20the%20standard%0Acontrol%20modes%2C%20which%20are%20primarily%20oriented%20towards%20industrial%20or%20service%0Aapplications.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20optimization-based%0Adesign%20method%20to%20fully%20exploit%20the%20capability%20of%20the%20cobot%20in%20performing%0Arehabilitation%20tasks.%20Additionally%2C%20we%20introduce%20a%20novel%20control%20architecture%0Abased%20on%20an%20admittance-type%20Virtual%20Fixture%20method%2C%20which%20constrains%20the%20motion%0Aof%20the%20robot%20along%20a%20prescribed%20path.%20This%20approach%20allows%20for%20an%20intuitive%0Adefinition%20of%20the%20task%20to%20be%20performed%20via%20Programming%20by%20Demonstration%20and%0Aenables%20the%20system%20to%20operate%20both%20passively%20and%20actively.%20In%20passive%20mode%2C%20the%0Asystem%20supports%20the%20patient%20during%20task%20execution%20with%20additional%20force%2C%20while%0Ain%20active%20mode%2C%20it%20opposes%20the%20motion%20with%20a%20braking%20force.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Design%2520and%2520Control%2520Methods%2520for%2520Using%2520Collaborative%2520Robots%2520in%250A%2520%2520Upper-Limb%2520Rehabilitation%26entry.906535625%3DDario%2520Onfiani%2520and%2520Marco%2520Caramaschi%2520and%2520Luigi%2520Biagiotti%2520and%2520Fabio%2520Pini%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520development%2520of%2520a%2520robotic%2520rehabilitation%2520system%250Afor%2520the%2520upper%2520limbs%2520based%2520on%2520collaborative%2520end-effector%2520solutions.%2520The%2520use%2520of%250Acommercial%2520collaborative%2520robots%2520offers%2520significant%2520advantages%2520for%2520this%2520task%252C%2520as%250Athey%2520are%2520optimized%2520from%2520an%2520engineering%2520perspective%2520and%2520ensure%2520safe%2520physical%250Ainteraction%2520with%2520humans.%2520However%252C%2520they%2520also%2520come%2520with%2520noticeable%2520drawbacks%252C%250Asuch%2520as%2520the%2520limited%2520range%2520of%2520sizes%2520available%2520on%2520the%2520market%2520and%2520the%2520standard%250Acontrol%2520modes%252C%2520which%2520are%2520primarily%2520oriented%2520towards%2520industrial%2520or%2520service%250Aapplications.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520optimization-based%250Adesign%2520method%2520to%2520fully%2520exploit%2520the%2520capability%2520of%2520the%2520cobot%2520in%2520performing%250Arehabilitation%2520tasks.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520control%2520architecture%250Abased%2520on%2520an%2520admittance-type%2520Virtual%2520Fixture%2520method%252C%2520which%2520constrains%2520the%2520motion%250Aof%2520the%2520robot%2520along%2520a%2520prescribed%2520path.%2520This%2520approach%2520allows%2520for%2520an%2520intuitive%250Adefinition%2520of%2520the%2520task%2520to%2520be%2520performed%2520via%2520Programming%2520by%2520Demonstration%2520and%250Aenables%2520the%2520system%2520to%2520operate%2520both%2520passively%2520and%2520actively.%2520In%2520passive%2520mode%252C%2520the%250Asystem%2520supports%2520the%2520patient%2520during%2520task%2520execution%2520with%2520additional%2520force%252C%2520while%250Ain%2520active%2520mode%252C%2520it%2520opposes%2520the%2520motion%2520with%2520a%2520braking%2520force.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Design%20and%20Control%20Methods%20for%20Using%20Collaborative%20Robots%20in%0A%20%20Upper-Limb%20Rehabilitation&entry.906535625=Dario%20Onfiani%20and%20Marco%20Caramaschi%20and%20Luigi%20Biagiotti%20and%20Fabio%20Pini&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20development%20of%20a%20robotic%20rehabilitation%20system%0Afor%20the%20upper%20limbs%20based%20on%20collaborative%20end-effector%20solutions.%20The%20use%20of%0Acommercial%20collaborative%20robots%20offers%20significant%20advantages%20for%20this%20task%2C%20as%0Athey%20are%20optimized%20from%20an%20engineering%20perspective%20and%20ensure%20safe%20physical%0Ainteraction%20with%20humans.%20However%2C%20they%20also%20come%20with%20noticeable%20drawbacks%2C%0Asuch%20as%20the%20limited%20range%20of%20sizes%20available%20on%20the%20market%20and%20the%20standard%0Acontrol%20modes%2C%20which%20are%20primarily%20oriented%20towards%20industrial%20or%20service%0Aapplications.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20optimization-based%0Adesign%20method%20to%20fully%20exploit%20the%20capability%20of%20the%20cobot%20in%20performing%0Arehabilitation%20tasks.%20Additionally%2C%20we%20introduce%20a%20novel%20control%20architecture%0Abased%20on%20an%20admittance-type%20Virtual%20Fixture%20method%2C%20which%20constrains%20the%20motion%0Aof%20the%20robot%20along%20a%20prescribed%20path.%20This%20approach%20allows%20for%20an%20intuitive%0Adefinition%20of%20the%20task%20to%20be%20performed%20via%20Programming%20by%20Demonstration%20and%0Aenables%20the%20system%20to%20operate%20both%20passively%20and%20actively.%20In%20passive%20mode%2C%20the%0Asystem%20supports%20the%20patient%20during%20task%20execution%20with%20additional%20force%2C%20while%0Ain%20active%20mode%2C%20it%20opposes%20the%20motion%20with%20a%20braking%20force.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18661v1&entry.124074799=Read"},
{"title": "Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities", "author": "Sven Cattell and Avijit Ghosh and Lucie-Aim\u00e9e Kaffee", "abstract": "  Harm reporting in Artificial Intelligence (AI) currently lacks a structured\nprocess for disclosing and addressing algorithmic flaws, relying largely on an\nad-hoc approach. This contrasts sharply with the well-established Coordinated\nVulnerability Disclosure (CVD) ecosystem in software security. While global\nefforts to establish frameworks for AI transparency and collaboration are\nunderway, the unique challenges presented by machine learning (ML) models\ndemand a specialized approach. To address this gap, we propose implementing a\nCoordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML\nand AI issues. This paper reviews the evolution of ML disclosure practices,\nfrom ad hoc reporting to emerging participatory auditing methods, and compares\nthem with cybersecurity norms. Our framework introduces innovations such as\nextended model cards, dynamic scope expansion, an independent adjudication\npanel, and an automated verification process. We also outline a forthcoming\nreal-world pilot of CFD. We argue that CFD could significantly enhance public\ntrust in AI systems. By balancing organizational and community interests, CFD\naims to improve AI accountability in a rapidly evolving technological\nlandscape.\n", "link": "http://arxiv.org/abs/2402.07039v3", "date": "2024-07-26", "relevancy": 1.7826, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4408}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinated%20Flaw%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities&body=Title%3A%20Coordinated%20Flaw%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities%0AAuthor%3A%20Sven%20Cattell%20and%20Avijit%20Ghosh%20and%20Lucie-Aim%C3%A9e%20Kaffee%0AAbstract%3A%20%20%20Harm%20reporting%20in%20Artificial%20Intelligence%20%28AI%29%20currently%20lacks%20a%20structured%0Aprocess%20for%20disclosing%20and%20addressing%20algorithmic%20flaws%2C%20relying%20largely%20on%20an%0Aad-hoc%20approach.%20This%20contrasts%20sharply%20with%20the%20well-established%20Coordinated%0AVulnerability%20Disclosure%20%28CVD%29%20ecosystem%20in%20software%20security.%20While%20global%0Aefforts%20to%20establish%20frameworks%20for%20AI%20transparency%20and%20collaboration%20are%0Aunderway%2C%20the%20unique%20challenges%20presented%20by%20machine%20learning%20%28ML%29%20models%0Ademand%20a%20specialized%20approach.%20To%20address%20this%20gap%2C%20we%20propose%20implementing%20a%0ACoordinated%20Flaw%20Disclosure%20%28CFD%29%20framework%20tailored%20to%20the%20complexities%20of%20ML%0Aand%20AI%20issues.%20This%20paper%20reviews%20the%20evolution%20of%20ML%20disclosure%20practices%2C%0Afrom%20ad%20hoc%20reporting%20to%20emerging%20participatory%20auditing%20methods%2C%20and%20compares%0Athem%20with%20cybersecurity%20norms.%20Our%20framework%20introduces%20innovations%20such%20as%0Aextended%20model%20cards%2C%20dynamic%20scope%20expansion%2C%20an%20independent%20adjudication%0Apanel%2C%20and%20an%20automated%20verification%20process.%20We%20also%20outline%20a%20forthcoming%0Areal-world%20pilot%20of%20CFD.%20We%20argue%20that%20CFD%20could%20significantly%20enhance%20public%0Atrust%20in%20AI%20systems.%20By%20balancing%20organizational%20and%20community%20interests%2C%20CFD%0Aaims%20to%20improve%20AI%20accountability%20in%20a%20rapidly%20evolving%20technological%0Alandscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinated%2520Flaw%2520Disclosure%2520for%2520AI%253A%2520Beyond%2520Security%2520Vulnerabilities%26entry.906535625%3DSven%2520Cattell%2520and%2520Avijit%2520Ghosh%2520and%2520Lucie-Aim%25C3%25A9e%2520Kaffee%26entry.1292438233%3D%2520%2520Harm%2520reporting%2520in%2520Artificial%2520Intelligence%2520%2528AI%2529%2520currently%2520lacks%2520a%2520structured%250Aprocess%2520for%2520disclosing%2520and%2520addressing%2520algorithmic%2520flaws%252C%2520relying%2520largely%2520on%2520an%250Aad-hoc%2520approach.%2520This%2520contrasts%2520sharply%2520with%2520the%2520well-established%2520Coordinated%250AVulnerability%2520Disclosure%2520%2528CVD%2529%2520ecosystem%2520in%2520software%2520security.%2520While%2520global%250Aefforts%2520to%2520establish%2520frameworks%2520for%2520AI%2520transparency%2520and%2520collaboration%2520are%250Aunderway%252C%2520the%2520unique%2520challenges%2520presented%2520by%2520machine%2520learning%2520%2528ML%2529%2520models%250Ademand%2520a%2520specialized%2520approach.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520implementing%2520a%250ACoordinated%2520Flaw%2520Disclosure%2520%2528CFD%2529%2520framework%2520tailored%2520to%2520the%2520complexities%2520of%2520ML%250Aand%2520AI%2520issues.%2520This%2520paper%2520reviews%2520the%2520evolution%2520of%2520ML%2520disclosure%2520practices%252C%250Afrom%2520ad%2520hoc%2520reporting%2520to%2520emerging%2520participatory%2520auditing%2520methods%252C%2520and%2520compares%250Athem%2520with%2520cybersecurity%2520norms.%2520Our%2520framework%2520introduces%2520innovations%2520such%2520as%250Aextended%2520model%2520cards%252C%2520dynamic%2520scope%2520expansion%252C%2520an%2520independent%2520adjudication%250Apanel%252C%2520and%2520an%2520automated%2520verification%2520process.%2520We%2520also%2520outline%2520a%2520forthcoming%250Areal-world%2520pilot%2520of%2520CFD.%2520We%2520argue%2520that%2520CFD%2520could%2520significantly%2520enhance%2520public%250Atrust%2520in%2520AI%2520systems.%2520By%2520balancing%2520organizational%2520and%2520community%2520interests%252C%2520CFD%250Aaims%2520to%2520improve%2520AI%2520accountability%2520in%2520a%2520rapidly%2520evolving%2520technological%250Alandscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinated%20Flaw%20Disclosure%20for%20AI%3A%20Beyond%20Security%20Vulnerabilities&entry.906535625=Sven%20Cattell%20and%20Avijit%20Ghosh%20and%20Lucie-Aim%C3%A9e%20Kaffee&entry.1292438233=%20%20Harm%20reporting%20in%20Artificial%20Intelligence%20%28AI%29%20currently%20lacks%20a%20structured%0Aprocess%20for%20disclosing%20and%20addressing%20algorithmic%20flaws%2C%20relying%20largely%20on%20an%0Aad-hoc%20approach.%20This%20contrasts%20sharply%20with%20the%20well-established%20Coordinated%0AVulnerability%20Disclosure%20%28CVD%29%20ecosystem%20in%20software%20security.%20While%20global%0Aefforts%20to%20establish%20frameworks%20for%20AI%20transparency%20and%20collaboration%20are%0Aunderway%2C%20the%20unique%20challenges%20presented%20by%20machine%20learning%20%28ML%29%20models%0Ademand%20a%20specialized%20approach.%20To%20address%20this%20gap%2C%20we%20propose%20implementing%20a%0ACoordinated%20Flaw%20Disclosure%20%28CFD%29%20framework%20tailored%20to%20the%20complexities%20of%20ML%0Aand%20AI%20issues.%20This%20paper%20reviews%20the%20evolution%20of%20ML%20disclosure%20practices%2C%0Afrom%20ad%20hoc%20reporting%20to%20emerging%20participatory%20auditing%20methods%2C%20and%20compares%0Athem%20with%20cybersecurity%20norms.%20Our%20framework%20introduces%20innovations%20such%20as%0Aextended%20model%20cards%2C%20dynamic%20scope%20expansion%2C%20an%20independent%20adjudication%0Apanel%2C%20and%20an%20automated%20verification%20process.%20We%20also%20outline%20a%20forthcoming%0Areal-world%20pilot%20of%20CFD.%20We%20argue%20that%20CFD%20could%20significantly%20enhance%20public%0Atrust%20in%20AI%20systems.%20By%20balancing%20organizational%20and%20community%20interests%2C%20CFD%0Aaims%20to%20improve%20AI%20accountability%20in%20a%20rapidly%20evolving%20technological%0Alandscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07039v3&entry.124074799=Read"},
{"title": "Physics-Guided Actor-Critic Reinforcement Learning for Swimming in\n  Turbulence", "author": "Christopher Koh and Laurent Pagnier and Michael Chertkov", "abstract": "  Turbulent diffusion causes particles placed in proximity to separate. We\ninvestigate the required swimming efforts to maintain a particle close to its\npassively advected counterpart. We explore optimally balancing these efforts\nwith the intended goal by developing and comparing a novel Physics-Informed\nReinforcement Learning (PIRL) strategy with prescribed control (PC) and\nstandard physics-agnostic Reinforcement Learning strategies. Our PIRL scheme,\ncoined the Actor-Physicist, is an adaptation of the Actor-Critic algorithm in\nwhich the Neural Network parameterized Critic is replaced with an analytically\nderived physical heuristic function (the physicist). This strategy is then\ncompared with an analytically computed optimal PC policy derived from a\nstochastic optimal control formulation and standard physics-agnostic\nActor-Critic type algorithms.\n", "link": "http://arxiv.org/abs/2406.10242v2", "date": "2024-07-26", "relevancy": 1.7799, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4466}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Guided%20Actor-Critic%20Reinforcement%20Learning%20for%20Swimming%20in%0A%20%20Turbulence&body=Title%3A%20Physics-Guided%20Actor-Critic%20Reinforcement%20Learning%20for%20Swimming%20in%0A%20%20Turbulence%0AAuthor%3A%20Christopher%20Koh%20and%20Laurent%20Pagnier%20and%20Michael%20Chertkov%0AAbstract%3A%20%20%20Turbulent%20diffusion%20causes%20particles%20placed%20in%20proximity%20to%20separate.%20We%0Ainvestigate%20the%20required%20swimming%20efforts%20to%20maintain%20a%20particle%20close%20to%20its%0Apassively%20advected%20counterpart.%20We%20explore%20optimally%20balancing%20these%20efforts%0Awith%20the%20intended%20goal%20by%20developing%20and%20comparing%20a%20novel%20Physics-Informed%0AReinforcement%20Learning%20%28PIRL%29%20strategy%20with%20prescribed%20control%20%28PC%29%20and%0Astandard%20physics-agnostic%20Reinforcement%20Learning%20strategies.%20Our%20PIRL%20scheme%2C%0Acoined%20the%20Actor-Physicist%2C%20is%20an%20adaptation%20of%20the%20Actor-Critic%20algorithm%20in%0Awhich%20the%20Neural%20Network%20parameterized%20Critic%20is%20replaced%20with%20an%20analytically%0Aderived%20physical%20heuristic%20function%20%28the%20physicist%29.%20This%20strategy%20is%20then%0Acompared%20with%20an%20analytically%20computed%20optimal%20PC%20policy%20derived%20from%20a%0Astochastic%20optimal%20control%20formulation%20and%20standard%20physics-agnostic%0AActor-Critic%20type%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Guided%2520Actor-Critic%2520Reinforcement%2520Learning%2520for%2520Swimming%2520in%250A%2520%2520Turbulence%26entry.906535625%3DChristopher%2520Koh%2520and%2520Laurent%2520Pagnier%2520and%2520Michael%2520Chertkov%26entry.1292438233%3D%2520%2520Turbulent%2520diffusion%2520causes%2520particles%2520placed%2520in%2520proximity%2520to%2520separate.%2520We%250Ainvestigate%2520the%2520required%2520swimming%2520efforts%2520to%2520maintain%2520a%2520particle%2520close%2520to%2520its%250Apassively%2520advected%2520counterpart.%2520We%2520explore%2520optimally%2520balancing%2520these%2520efforts%250Awith%2520the%2520intended%2520goal%2520by%2520developing%2520and%2520comparing%2520a%2520novel%2520Physics-Informed%250AReinforcement%2520Learning%2520%2528PIRL%2529%2520strategy%2520with%2520prescribed%2520control%2520%2528PC%2529%2520and%250Astandard%2520physics-agnostic%2520Reinforcement%2520Learning%2520strategies.%2520Our%2520PIRL%2520scheme%252C%250Acoined%2520the%2520Actor-Physicist%252C%2520is%2520an%2520adaptation%2520of%2520the%2520Actor-Critic%2520algorithm%2520in%250Awhich%2520the%2520Neural%2520Network%2520parameterized%2520Critic%2520is%2520replaced%2520with%2520an%2520analytically%250Aderived%2520physical%2520heuristic%2520function%2520%2528the%2520physicist%2529.%2520This%2520strategy%2520is%2520then%250Acompared%2520with%2520an%2520analytically%2520computed%2520optimal%2520PC%2520policy%2520derived%2520from%2520a%250Astochastic%2520optimal%2520control%2520formulation%2520and%2520standard%2520physics-agnostic%250AActor-Critic%2520type%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Guided%20Actor-Critic%20Reinforcement%20Learning%20for%20Swimming%20in%0A%20%20Turbulence&entry.906535625=Christopher%20Koh%20and%20Laurent%20Pagnier%20and%20Michael%20Chertkov&entry.1292438233=%20%20Turbulent%20diffusion%20causes%20particles%20placed%20in%20proximity%20to%20separate.%20We%0Ainvestigate%20the%20required%20swimming%20efforts%20to%20maintain%20a%20particle%20close%20to%20its%0Apassively%20advected%20counterpart.%20We%20explore%20optimally%20balancing%20these%20efforts%0Awith%20the%20intended%20goal%20by%20developing%20and%20comparing%20a%20novel%20Physics-Informed%0AReinforcement%20Learning%20%28PIRL%29%20strategy%20with%20prescribed%20control%20%28PC%29%20and%0Astandard%20physics-agnostic%20Reinforcement%20Learning%20strategies.%20Our%20PIRL%20scheme%2C%0Acoined%20the%20Actor-Physicist%2C%20is%20an%20adaptation%20of%20the%20Actor-Critic%20algorithm%20in%0Awhich%20the%20Neural%20Network%20parameterized%20Critic%20is%20replaced%20with%20an%20analytically%0Aderived%20physical%20heuristic%20function%20%28the%20physicist%29.%20This%20strategy%20is%20then%0Acompared%20with%20an%20analytically%20computed%20optimal%20PC%20policy%20derived%20from%20a%0Astochastic%20optimal%20control%20formulation%20and%20standard%20physics-agnostic%0AActor-Critic%20type%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10242v2&entry.124074799=Read"},
{"title": "An Accelerated Multi-level Monte Carlo Approach for Average Reward\n  Reinforcement Learning with General Policy Parametrization", "author": "Swetha Ganesh and Vaneet Aggarwal", "abstract": "  In our study, we delve into average-reward reinforcement learning with\ngeneral policy parametrization. Within this domain, current guarantees either\nfall short with suboptimal guarantees or demand prior knowledge of mixing time.\nTo address these issues, we introduce Randomized Accelerated Natural Actor\nCritic, a method that integrates Multi-level Monte-Carlo and Natural Actor\nCritic. Our approach is the first to achieve global convergence rate of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ without requiring knowledge of mixing time,\nsignificantly surpassing the state-of-the-art bound of\n$\\tilde{\\mathcal{O}}(1/T^{1/4})$.\n", "link": "http://arxiv.org/abs/2407.18878v1", "date": "2024-07-26", "relevancy": 1.7796, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4403}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Accelerated%20Multi-level%20Monte%20Carlo%20Approach%20for%20Average%20Reward%0A%20%20Reinforcement%20Learning%20with%20General%20Policy%20Parametrization&body=Title%3A%20An%20Accelerated%20Multi-level%20Monte%20Carlo%20Approach%20for%20Average%20Reward%0A%20%20Reinforcement%20Learning%20with%20General%20Policy%20Parametrization%0AAuthor%3A%20Swetha%20Ganesh%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20In%20our%20study%2C%20we%20delve%20into%20average-reward%20reinforcement%20learning%20with%0Ageneral%20policy%20parametrization.%20Within%20this%20domain%2C%20current%20guarantees%20either%0Afall%20short%20with%20suboptimal%20guarantees%20or%20demand%20prior%20knowledge%20of%20mixing%20time.%0ATo%20address%20these%20issues%2C%20we%20introduce%20Randomized%20Accelerated%20Natural%20Actor%0ACritic%2C%20a%20method%20that%20integrates%20Multi-level%20Monte-Carlo%20and%20Natural%20Actor%0ACritic.%20Our%20approach%20is%20the%20first%20to%20achieve%20global%20convergence%20rate%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%24%20without%20requiring%20knowledge%20of%20mixing%20time%2C%0Asignificantly%20surpassing%20the%20state-of-the-art%20bound%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/T%5E%7B1/4%7D%29%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Accelerated%2520Multi-level%2520Monte%2520Carlo%2520Approach%2520for%2520Average%2520Reward%250A%2520%2520Reinforcement%2520Learning%2520with%2520General%2520Policy%2520Parametrization%26entry.906535625%3DSwetha%2520Ganesh%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520In%2520our%2520study%252C%2520we%2520delve%2520into%2520average-reward%2520reinforcement%2520learning%2520with%250Ageneral%2520policy%2520parametrization.%2520Within%2520this%2520domain%252C%2520current%2520guarantees%2520either%250Afall%2520short%2520with%2520suboptimal%2520guarantees%2520or%2520demand%2520prior%2520knowledge%2520of%2520mixing%2520time.%250ATo%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Randomized%2520Accelerated%2520Natural%2520Actor%250ACritic%252C%2520a%2520method%2520that%2520integrates%2520Multi-level%2520Monte-Carlo%2520and%2520Natural%2520Actor%250ACritic.%2520Our%2520approach%2520is%2520the%2520first%2520to%2520achieve%2520global%2520convergence%2520rate%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%25281/%255Csqrt%257BT%257D%2529%2524%2520without%2520requiring%2520knowledge%2520of%2520mixing%2520time%252C%250Asignificantly%2520surpassing%2520the%2520state-of-the-art%2520bound%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%25281/T%255E%257B1/4%257D%2529%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Accelerated%20Multi-level%20Monte%20Carlo%20Approach%20for%20Average%20Reward%0A%20%20Reinforcement%20Learning%20with%20General%20Policy%20Parametrization&entry.906535625=Swetha%20Ganesh%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20In%20our%20study%2C%20we%20delve%20into%20average-reward%20reinforcement%20learning%20with%0Ageneral%20policy%20parametrization.%20Within%20this%20domain%2C%20current%20guarantees%20either%0Afall%20short%20with%20suboptimal%20guarantees%20or%20demand%20prior%20knowledge%20of%20mixing%20time.%0ATo%20address%20these%20issues%2C%20we%20introduce%20Randomized%20Accelerated%20Natural%20Actor%0ACritic%2C%20a%20method%20that%20integrates%20Multi-level%20Monte-Carlo%20and%20Natural%20Actor%0ACritic.%20Our%20approach%20is%20the%20first%20to%20achieve%20global%20convergence%20rate%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/%5Csqrt%7BT%7D%29%24%20without%20requiring%20knowledge%20of%20mixing%20time%2C%0Asignificantly%20surpassing%20the%20state-of-the-art%20bound%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%281/T%5E%7B1/4%7D%29%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18878v1&entry.124074799=Read"},
{"title": "Learning a Shape-Conditioned Agent for Purely Tactile In-Hand\n  Manipulation of Various Objects", "author": "Johannes Pitz and Lennart R\u00f6stel and Leon Sievers and Darius Burschka and Berthold B\u00e4uml", "abstract": "  Reorienting diverse objects with a multi-fingered hand is a challenging task.\nCurrent methods in robotic in-hand manipulation are either object-specific or\nrequire permanent supervision of the object state from visual sensors. This is\nfar from human capabilities and from what is needed in real-world applications.\nIn this work, we address this gap by training shape-conditioned agents to\nreorient diverse objects in hand, relying purely on tactile feedback (via\ntorque and position measurements of the fingers' joints). To achieve this, we\npropose a learning framework that exploits shape information in a reinforcement\nlearning policy and a learned state estimator. We find that representing 3D\nshapes by vectors from a fixed set of basis points to the shape's surface,\ntransformed by its predicted 3D pose, is especially helpful for learning\ndexterous in-hand manipulation. In simulation and real-world experiments, we\nshow the reorientation of many objects with high success rates, on par with\nstate-of-the-art results obtained with specialized single-object agents.\nMoreover, we show generalization to novel objects, achieving success rates of\n$\\sim$90% even for non-convex shapes.\n", "link": "http://arxiv.org/abs/2407.18834v1", "date": "2024-07-26", "relevancy": 1.7677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6061}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Shape-Conditioned%20Agent%20for%20Purely%20Tactile%20In-Hand%0A%20%20Manipulation%20of%20Various%20Objects&body=Title%3A%20Learning%20a%20Shape-Conditioned%20Agent%20for%20Purely%20Tactile%20In-Hand%0A%20%20Manipulation%20of%20Various%20Objects%0AAuthor%3A%20Johannes%20Pitz%20and%20Lennart%20R%C3%B6stel%20and%20Leon%20Sievers%20and%20Darius%20Burschka%20and%20Berthold%20B%C3%A4uml%0AAbstract%3A%20%20%20Reorienting%20diverse%20objects%20with%20a%20multi-fingered%20hand%20is%20a%20challenging%20task.%0ACurrent%20methods%20in%20robotic%20in-hand%20manipulation%20are%20either%20object-specific%20or%0Arequire%20permanent%20supervision%20of%20the%20object%20state%20from%20visual%20sensors.%20This%20is%0Afar%20from%20human%20capabilities%20and%20from%20what%20is%20needed%20in%20real-world%20applications.%0AIn%20this%20work%2C%20we%20address%20this%20gap%20by%20training%20shape-conditioned%20agents%20to%0Areorient%20diverse%20objects%20in%20hand%2C%20relying%20purely%20on%20tactile%20feedback%20%28via%0Atorque%20and%20position%20measurements%20of%20the%20fingers%27%20joints%29.%20To%20achieve%20this%2C%20we%0Apropose%20a%20learning%20framework%20that%20exploits%20shape%20information%20in%20a%20reinforcement%0Alearning%20policy%20and%20a%20learned%20state%20estimator.%20We%20find%20that%20representing%203D%0Ashapes%20by%20vectors%20from%20a%20fixed%20set%20of%20basis%20points%20to%20the%20shape%27s%20surface%2C%0Atransformed%20by%20its%20predicted%203D%20pose%2C%20is%20especially%20helpful%20for%20learning%0Adexterous%20in-hand%20manipulation.%20In%20simulation%20and%20real-world%20experiments%2C%20we%0Ashow%20the%20reorientation%20of%20many%20objects%20with%20high%20success%20rates%2C%20on%20par%20with%0Astate-of-the-art%20results%20obtained%20with%20specialized%20single-object%20agents.%0AMoreover%2C%20we%20show%20generalization%20to%20novel%20objects%2C%20achieving%20success%20rates%20of%0A%24%5Csim%2490%25%20even%20for%20non-convex%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Shape-Conditioned%2520Agent%2520for%2520Purely%2520Tactile%2520In-Hand%250A%2520%2520Manipulation%2520of%2520Various%2520Objects%26entry.906535625%3DJohannes%2520Pitz%2520and%2520Lennart%2520R%25C3%25B6stel%2520and%2520Leon%2520Sievers%2520and%2520Darius%2520Burschka%2520and%2520Berthold%2520B%25C3%25A4uml%26entry.1292438233%3D%2520%2520Reorienting%2520diverse%2520objects%2520with%2520a%2520multi-fingered%2520hand%2520is%2520a%2520challenging%2520task.%250ACurrent%2520methods%2520in%2520robotic%2520in-hand%2520manipulation%2520are%2520either%2520object-specific%2520or%250Arequire%2520permanent%2520supervision%2520of%2520the%2520object%2520state%2520from%2520visual%2520sensors.%2520This%2520is%250Afar%2520from%2520human%2520capabilities%2520and%2520from%2520what%2520is%2520needed%2520in%2520real-world%2520applications.%250AIn%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520training%2520shape-conditioned%2520agents%2520to%250Areorient%2520diverse%2520objects%2520in%2520hand%252C%2520relying%2520purely%2520on%2520tactile%2520feedback%2520%2528via%250Atorque%2520and%2520position%2520measurements%2520of%2520the%2520fingers%2527%2520joints%2529.%2520To%2520achieve%2520this%252C%2520we%250Apropose%2520a%2520learning%2520framework%2520that%2520exploits%2520shape%2520information%2520in%2520a%2520reinforcement%250Alearning%2520policy%2520and%2520a%2520learned%2520state%2520estimator.%2520We%2520find%2520that%2520representing%25203D%250Ashapes%2520by%2520vectors%2520from%2520a%2520fixed%2520set%2520of%2520basis%2520points%2520to%2520the%2520shape%2527s%2520surface%252C%250Atransformed%2520by%2520its%2520predicted%25203D%2520pose%252C%2520is%2520especially%2520helpful%2520for%2520learning%250Adexterous%2520in-hand%2520manipulation.%2520In%2520simulation%2520and%2520real-world%2520experiments%252C%2520we%250Ashow%2520the%2520reorientation%2520of%2520many%2520objects%2520with%2520high%2520success%2520rates%252C%2520on%2520par%2520with%250Astate-of-the-art%2520results%2520obtained%2520with%2520specialized%2520single-object%2520agents.%250AMoreover%252C%2520we%2520show%2520generalization%2520to%2520novel%2520objects%252C%2520achieving%2520success%2520rates%2520of%250A%2524%255Csim%252490%2525%2520even%2520for%2520non-convex%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Shape-Conditioned%20Agent%20for%20Purely%20Tactile%20In-Hand%0A%20%20Manipulation%20of%20Various%20Objects&entry.906535625=Johannes%20Pitz%20and%20Lennart%20R%C3%B6stel%20and%20Leon%20Sievers%20and%20Darius%20Burschka%20and%20Berthold%20B%C3%A4uml&entry.1292438233=%20%20Reorienting%20diverse%20objects%20with%20a%20multi-fingered%20hand%20is%20a%20challenging%20task.%0ACurrent%20methods%20in%20robotic%20in-hand%20manipulation%20are%20either%20object-specific%20or%0Arequire%20permanent%20supervision%20of%20the%20object%20state%20from%20visual%20sensors.%20This%20is%0Afar%20from%20human%20capabilities%20and%20from%20what%20is%20needed%20in%20real-world%20applications.%0AIn%20this%20work%2C%20we%20address%20this%20gap%20by%20training%20shape-conditioned%20agents%20to%0Areorient%20diverse%20objects%20in%20hand%2C%20relying%20purely%20on%20tactile%20feedback%20%28via%0Atorque%20and%20position%20measurements%20of%20the%20fingers%27%20joints%29.%20To%20achieve%20this%2C%20we%0Apropose%20a%20learning%20framework%20that%20exploits%20shape%20information%20in%20a%20reinforcement%0Alearning%20policy%20and%20a%20learned%20state%20estimator.%20We%20find%20that%20representing%203D%0Ashapes%20by%20vectors%20from%20a%20fixed%20set%20of%20basis%20points%20to%20the%20shape%27s%20surface%2C%0Atransformed%20by%20its%20predicted%203D%20pose%2C%20is%20especially%20helpful%20for%20learning%0Adexterous%20in-hand%20manipulation.%20In%20simulation%20and%20real-world%20experiments%2C%20we%0Ashow%20the%20reorientation%20of%20many%20objects%20with%20high%20success%20rates%2C%20on%20par%20with%0Astate-of-the-art%20results%20obtained%20with%20specialized%20single-object%20agents.%0AMoreover%2C%20we%20show%20generalization%20to%20novel%20objects%2C%20achieving%20success%20rates%20of%0A%24%5Csim%2490%25%20even%20for%20non-convex%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18834v1&entry.124074799=Read"},
{"title": "Particip-AI: A Democratic Surveying Framework for Anticipating Future AI\n  Use Cases, Harms and Benefits", "author": "Jimin Mun and Liwei Jiang and Jenny Liang and Inyoung Cheong and Nicole DeCario and Yejin Choi and Tadayoshi Kohno and Maarten Sap", "abstract": "  General purpose AI, such as ChatGPT, seems to have lowered the barriers for\nthe public to use AI and harness its power. However, the governance and\ndevelopment of AI still remain in the hands of a few, and the pace of\ndevelopment is accelerating without a comprehensive assessment of risks. As a\nfirst step towards democratic risk assessment and design of general purpose AI,\nwe introduce PARTICIP-AI, a carefully designed framework for laypeople to\nspeculate and assess AI use cases and their impacts. Our framework allows us to\nstudy more nuanced and detailed public opinions on AI through collecting use\ncases, surfacing diverse harms through risk assessment under alternate\nscenarios (i.e., developing and not developing a use case), and illuminating\ntensions over AI development through making a concluding choice on its\ndevelopment. To showcase the promise of our framework towards informing\ndemocratic AI development, we run a medium-scale study with inputs from 295\ndemographically diverse participants. Our analyses show that participants'\nresponses emphasize applications for personal life and society, contrasting\nwith most current AI development's business focus. We also surface diverse set\nof envisioned harms such as distrust in AI and institutions, complementary to\nthose defined by experts. Furthermore, we found that perceived impact of not\ndeveloping use cases significantly predicted participants' judgements of\nwhether AI use cases should be developed, and highlighted lay users' concerns\nof techno-solutionism. We conclude with a discussion on how frameworks like\nPARTICIP-AI can further guide democratic AI development and governance.\n", "link": "http://arxiv.org/abs/2403.14791v2", "date": "2024-07-26", "relevancy": 1.7546, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4437}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4413}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particip-AI%3A%20A%20Democratic%20Surveying%20Framework%20for%20Anticipating%20Future%20AI%0A%20%20Use%20Cases%2C%20Harms%20and%20Benefits&body=Title%3A%20Particip-AI%3A%20A%20Democratic%20Surveying%20Framework%20for%20Anticipating%20Future%20AI%0A%20%20Use%20Cases%2C%20Harms%20and%20Benefits%0AAuthor%3A%20Jimin%20Mun%20and%20Liwei%20Jiang%20and%20Jenny%20Liang%20and%20Inyoung%20Cheong%20and%20Nicole%20DeCario%20and%20Yejin%20Choi%20and%20Tadayoshi%20Kohno%20and%20Maarten%20Sap%0AAbstract%3A%20%20%20General%20purpose%20AI%2C%20such%20as%20ChatGPT%2C%20seems%20to%20have%20lowered%20the%20barriers%20for%0Athe%20public%20to%20use%20AI%20and%20harness%20its%20power.%20However%2C%20the%20governance%20and%0Adevelopment%20of%20AI%20still%20remain%20in%20the%20hands%20of%20a%20few%2C%20and%20the%20pace%20of%0Adevelopment%20is%20accelerating%20without%20a%20comprehensive%20assessment%20of%20risks.%20As%20a%0Afirst%20step%20towards%20democratic%20risk%20assessment%20and%20design%20of%20general%20purpose%20AI%2C%0Awe%20introduce%20PARTICIP-AI%2C%20a%20carefully%20designed%20framework%20for%20laypeople%20to%0Aspeculate%20and%20assess%20AI%20use%20cases%20and%20their%20impacts.%20Our%20framework%20allows%20us%20to%0Astudy%20more%20nuanced%20and%20detailed%20public%20opinions%20on%20AI%20through%20collecting%20use%0Acases%2C%20surfacing%20diverse%20harms%20through%20risk%20assessment%20under%20alternate%0Ascenarios%20%28i.e.%2C%20developing%20and%20not%20developing%20a%20use%20case%29%2C%20and%20illuminating%0Atensions%20over%20AI%20development%20through%20making%20a%20concluding%20choice%20on%20its%0Adevelopment.%20To%20showcase%20the%20promise%20of%20our%20framework%20towards%20informing%0Ademocratic%20AI%20development%2C%20we%20run%20a%20medium-scale%20study%20with%20inputs%20from%20295%0Ademographically%20diverse%20participants.%20Our%20analyses%20show%20that%20participants%27%0Aresponses%20emphasize%20applications%20for%20personal%20life%20and%20society%2C%20contrasting%0Awith%20most%20current%20AI%20development%27s%20business%20focus.%20We%20also%20surface%20diverse%20set%0Aof%20envisioned%20harms%20such%20as%20distrust%20in%20AI%20and%20institutions%2C%20complementary%20to%0Athose%20defined%20by%20experts.%20Furthermore%2C%20we%20found%20that%20perceived%20impact%20of%20not%0Adeveloping%20use%20cases%20significantly%20predicted%20participants%27%20judgements%20of%0Awhether%20AI%20use%20cases%20should%20be%20developed%2C%20and%20highlighted%20lay%20users%27%20concerns%0Aof%20techno-solutionism.%20We%20conclude%20with%20a%20discussion%20on%20how%20frameworks%20like%0APARTICIP-AI%20can%20further%20guide%20democratic%20AI%20development%20and%20governance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticip-AI%253A%2520A%2520Democratic%2520Surveying%2520Framework%2520for%2520Anticipating%2520Future%2520AI%250A%2520%2520Use%2520Cases%252C%2520Harms%2520and%2520Benefits%26entry.906535625%3DJimin%2520Mun%2520and%2520Liwei%2520Jiang%2520and%2520Jenny%2520Liang%2520and%2520Inyoung%2520Cheong%2520and%2520Nicole%2520DeCario%2520and%2520Yejin%2520Choi%2520and%2520Tadayoshi%2520Kohno%2520and%2520Maarten%2520Sap%26entry.1292438233%3D%2520%2520General%2520purpose%2520AI%252C%2520such%2520as%2520ChatGPT%252C%2520seems%2520to%2520have%2520lowered%2520the%2520barriers%2520for%250Athe%2520public%2520to%2520use%2520AI%2520and%2520harness%2520its%2520power.%2520However%252C%2520the%2520governance%2520and%250Adevelopment%2520of%2520AI%2520still%2520remain%2520in%2520the%2520hands%2520of%2520a%2520few%252C%2520and%2520the%2520pace%2520of%250Adevelopment%2520is%2520accelerating%2520without%2520a%2520comprehensive%2520assessment%2520of%2520risks.%2520As%2520a%250Afirst%2520step%2520towards%2520democratic%2520risk%2520assessment%2520and%2520design%2520of%2520general%2520purpose%2520AI%252C%250Awe%2520introduce%2520PARTICIP-AI%252C%2520a%2520carefully%2520designed%2520framework%2520for%2520laypeople%2520to%250Aspeculate%2520and%2520assess%2520AI%2520use%2520cases%2520and%2520their%2520impacts.%2520Our%2520framework%2520allows%2520us%2520to%250Astudy%2520more%2520nuanced%2520and%2520detailed%2520public%2520opinions%2520on%2520AI%2520through%2520collecting%2520use%250Acases%252C%2520surfacing%2520diverse%2520harms%2520through%2520risk%2520assessment%2520under%2520alternate%250Ascenarios%2520%2528i.e.%252C%2520developing%2520and%2520not%2520developing%2520a%2520use%2520case%2529%252C%2520and%2520illuminating%250Atensions%2520over%2520AI%2520development%2520through%2520making%2520a%2520concluding%2520choice%2520on%2520its%250Adevelopment.%2520To%2520showcase%2520the%2520promise%2520of%2520our%2520framework%2520towards%2520informing%250Ademocratic%2520AI%2520development%252C%2520we%2520run%2520a%2520medium-scale%2520study%2520with%2520inputs%2520from%2520295%250Ademographically%2520diverse%2520participants.%2520Our%2520analyses%2520show%2520that%2520participants%2527%250Aresponses%2520emphasize%2520applications%2520for%2520personal%2520life%2520and%2520society%252C%2520contrasting%250Awith%2520most%2520current%2520AI%2520development%2527s%2520business%2520focus.%2520We%2520also%2520surface%2520diverse%2520set%250Aof%2520envisioned%2520harms%2520such%2520as%2520distrust%2520in%2520AI%2520and%2520institutions%252C%2520complementary%2520to%250Athose%2520defined%2520by%2520experts.%2520Furthermore%252C%2520we%2520found%2520that%2520perceived%2520impact%2520of%2520not%250Adeveloping%2520use%2520cases%2520significantly%2520predicted%2520participants%2527%2520judgements%2520of%250Awhether%2520AI%2520use%2520cases%2520should%2520be%2520developed%252C%2520and%2520highlighted%2520lay%2520users%2527%2520concerns%250Aof%2520techno-solutionism.%2520We%2520conclude%2520with%2520a%2520discussion%2520on%2520how%2520frameworks%2520like%250APARTICIP-AI%2520can%2520further%2520guide%2520democratic%2520AI%2520development%2520and%2520governance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particip-AI%3A%20A%20Democratic%20Surveying%20Framework%20for%20Anticipating%20Future%20AI%0A%20%20Use%20Cases%2C%20Harms%20and%20Benefits&entry.906535625=Jimin%20Mun%20and%20Liwei%20Jiang%20and%20Jenny%20Liang%20and%20Inyoung%20Cheong%20and%20Nicole%20DeCario%20and%20Yejin%20Choi%20and%20Tadayoshi%20Kohno%20and%20Maarten%20Sap&entry.1292438233=%20%20General%20purpose%20AI%2C%20such%20as%20ChatGPT%2C%20seems%20to%20have%20lowered%20the%20barriers%20for%0Athe%20public%20to%20use%20AI%20and%20harness%20its%20power.%20However%2C%20the%20governance%20and%0Adevelopment%20of%20AI%20still%20remain%20in%20the%20hands%20of%20a%20few%2C%20and%20the%20pace%20of%0Adevelopment%20is%20accelerating%20without%20a%20comprehensive%20assessment%20of%20risks.%20As%20a%0Afirst%20step%20towards%20democratic%20risk%20assessment%20and%20design%20of%20general%20purpose%20AI%2C%0Awe%20introduce%20PARTICIP-AI%2C%20a%20carefully%20designed%20framework%20for%20laypeople%20to%0Aspeculate%20and%20assess%20AI%20use%20cases%20and%20their%20impacts.%20Our%20framework%20allows%20us%20to%0Astudy%20more%20nuanced%20and%20detailed%20public%20opinions%20on%20AI%20through%20collecting%20use%0Acases%2C%20surfacing%20diverse%20harms%20through%20risk%20assessment%20under%20alternate%0Ascenarios%20%28i.e.%2C%20developing%20and%20not%20developing%20a%20use%20case%29%2C%20and%20illuminating%0Atensions%20over%20AI%20development%20through%20making%20a%20concluding%20choice%20on%20its%0Adevelopment.%20To%20showcase%20the%20promise%20of%20our%20framework%20towards%20informing%0Ademocratic%20AI%20development%2C%20we%20run%20a%20medium-scale%20study%20with%20inputs%20from%20295%0Ademographically%20diverse%20participants.%20Our%20analyses%20show%20that%20participants%27%0Aresponses%20emphasize%20applications%20for%20personal%20life%20and%20society%2C%20contrasting%0Awith%20most%20current%20AI%20development%27s%20business%20focus.%20We%20also%20surface%20diverse%20set%0Aof%20envisioned%20harms%20such%20as%20distrust%20in%20AI%20and%20institutions%2C%20complementary%20to%0Athose%20defined%20by%20experts.%20Furthermore%2C%20we%20found%20that%20perceived%20impact%20of%20not%0Adeveloping%20use%20cases%20significantly%20predicted%20participants%27%20judgements%20of%0Awhether%20AI%20use%20cases%20should%20be%20developed%2C%20and%20highlighted%20lay%20users%27%20concerns%0Aof%20techno-solutionism.%20We%20conclude%20with%20a%20discussion%20on%20how%20frameworks%20like%0APARTICIP-AI%20can%20further%20guide%20democratic%20AI%20development%20and%20governance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14791v2&entry.124074799=Read"},
{"title": "Cluster-norm for Unsupervised Probing of Knowledge", "author": "Walter Laurito and Sharan Maiya and Gr\u00e9goire Dhimo\u00efla and  Owen and  Yeung and Kaarel H\u00e4nni", "abstract": "  The deployment of language models brings challenges in generating reliable\ninformation, especially when these models are fine-tuned using human\npreferences. To extract encoded knowledge without (potentially) biased human\nlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)\nhave been developed (Burns et al., 2022). However, salient but unrelated\nfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).\nAddressing this, we propose a cluster normalization method to minimize the\nimpact of such features by clustering and normalizing activations of contrast\npairs before applying unsupervised probing techniques. While this approach does\nnot address the issue of differentiating between knowledge in general and\nsimulated knowledge - a major issue in the literature of latent knowledge\nelicitation (Christiano et al., 2021) - it significantly improves the ability\nof unsupervised probes to identify the intended knowledge amidst distractions.\n", "link": "http://arxiv.org/abs/2407.18712v1", "date": "2024-07-26", "relevancy": 1.7483, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4301}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster-norm%20for%20Unsupervised%20Probing%20of%20Knowledge&body=Title%3A%20Cluster-norm%20for%20Unsupervised%20Probing%20of%20Knowledge%0AAuthor%3A%20Walter%20Laurito%20and%20Sharan%20Maiya%20and%20Gr%C3%A9goire%20Dhimo%C3%AFla%20and%20%20Owen%20and%20%20Yeung%20and%20Kaarel%20H%C3%A4nni%0AAbstract%3A%20%20%20The%20deployment%20of%20language%20models%20brings%20challenges%20in%20generating%20reliable%0Ainformation%2C%20especially%20when%20these%20models%20are%20fine-tuned%20using%20human%0Apreferences.%20To%20extract%20encoded%20knowledge%20without%20%28potentially%29%20biased%20human%0Alabels%2C%20unsupervised%20probing%20techniques%20like%20Contrast-Consistent%20Search%20%28CCS%29%0Ahave%20been%20developed%20%28Burns%20et%20al.%2C%202022%29.%20However%2C%20salient%20but%20unrelated%0Afeatures%20in%20a%20given%20dataset%20can%20mislead%20these%20probes%20%28Farquhar%20et%20al.%2C%202023%29.%0AAddressing%20this%2C%20we%20propose%20a%20cluster%20normalization%20method%20to%20minimize%20the%0Aimpact%20of%20such%20features%20by%20clustering%20and%20normalizing%20activations%20of%20contrast%0Apairs%20before%20applying%20unsupervised%20probing%20techniques.%20While%20this%20approach%20does%0Anot%20address%20the%20issue%20of%20differentiating%20between%20knowledge%20in%20general%20and%0Asimulated%20knowledge%20-%20a%20major%20issue%20in%20the%20literature%20of%20latent%20knowledge%0Aelicitation%20%28Christiano%20et%20al.%2C%202021%29%20-%20it%20significantly%20improves%20the%20ability%0Aof%20unsupervised%20probes%20to%20identify%20the%20intended%20knowledge%20amidst%20distractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster-norm%2520for%2520Unsupervised%2520Probing%2520of%2520Knowledge%26entry.906535625%3DWalter%2520Laurito%2520and%2520Sharan%2520Maiya%2520and%2520Gr%25C3%25A9goire%2520Dhimo%25C3%25AFla%2520and%2520%2520Owen%2520and%2520%2520Yeung%2520and%2520Kaarel%2520H%25C3%25A4nni%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520language%2520models%2520brings%2520challenges%2520in%2520generating%2520reliable%250Ainformation%252C%2520especially%2520when%2520these%2520models%2520are%2520fine-tuned%2520using%2520human%250Apreferences.%2520To%2520extract%2520encoded%2520knowledge%2520without%2520%2528potentially%2529%2520biased%2520human%250Alabels%252C%2520unsupervised%2520probing%2520techniques%2520like%2520Contrast-Consistent%2520Search%2520%2528CCS%2529%250Ahave%2520been%2520developed%2520%2528Burns%2520et%2520al.%252C%25202022%2529.%2520However%252C%2520salient%2520but%2520unrelated%250Afeatures%2520in%2520a%2520given%2520dataset%2520can%2520mislead%2520these%2520probes%2520%2528Farquhar%2520et%2520al.%252C%25202023%2529.%250AAddressing%2520this%252C%2520we%2520propose%2520a%2520cluster%2520normalization%2520method%2520to%2520minimize%2520the%250Aimpact%2520of%2520such%2520features%2520by%2520clustering%2520and%2520normalizing%2520activations%2520of%2520contrast%250Apairs%2520before%2520applying%2520unsupervised%2520probing%2520techniques.%2520While%2520this%2520approach%2520does%250Anot%2520address%2520the%2520issue%2520of%2520differentiating%2520between%2520knowledge%2520in%2520general%2520and%250Asimulated%2520knowledge%2520-%2520a%2520major%2520issue%2520in%2520the%2520literature%2520of%2520latent%2520knowledge%250Aelicitation%2520%2528Christiano%2520et%2520al.%252C%25202021%2529%2520-%2520it%2520significantly%2520improves%2520the%2520ability%250Aof%2520unsupervised%2520probes%2520to%2520identify%2520the%2520intended%2520knowledge%2520amidst%2520distractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster-norm%20for%20Unsupervised%20Probing%20of%20Knowledge&entry.906535625=Walter%20Laurito%20and%20Sharan%20Maiya%20and%20Gr%C3%A9goire%20Dhimo%C3%AFla%20and%20%20Owen%20and%20%20Yeung%20and%20Kaarel%20H%C3%A4nni&entry.1292438233=%20%20The%20deployment%20of%20language%20models%20brings%20challenges%20in%20generating%20reliable%0Ainformation%2C%20especially%20when%20these%20models%20are%20fine-tuned%20using%20human%0Apreferences.%20To%20extract%20encoded%20knowledge%20without%20%28potentially%29%20biased%20human%0Alabels%2C%20unsupervised%20probing%20techniques%20like%20Contrast-Consistent%20Search%20%28CCS%29%0Ahave%20been%20developed%20%28Burns%20et%20al.%2C%202022%29.%20However%2C%20salient%20but%20unrelated%0Afeatures%20in%20a%20given%20dataset%20can%20mislead%20these%20probes%20%28Farquhar%20et%20al.%2C%202023%29.%0AAddressing%20this%2C%20we%20propose%20a%20cluster%20normalization%20method%20to%20minimize%20the%0Aimpact%20of%20such%20features%20by%20clustering%20and%20normalizing%20activations%20of%20contrast%0Apairs%20before%20applying%20unsupervised%20probing%20techniques.%20While%20this%20approach%20does%0Anot%20address%20the%20issue%20of%20differentiating%20between%20knowledge%20in%20general%20and%0Asimulated%20knowledge%20-%20a%20major%20issue%20in%20the%20literature%20of%20latent%20knowledge%0Aelicitation%20%28Christiano%20et%20al.%2C%202021%29%20-%20it%20significantly%20improves%20the%20ability%0Aof%20unsupervised%20probes%20to%20identify%20the%20intended%20knowledge%20amidst%20distractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18712v1&entry.124074799=Read"},
{"title": "Towards Generalized Offensive Language Identification", "author": "Alphaeus Dmonte and Tejas Arya and Tharindu Ranasinghe and Marcos Zampieri", "abstract": "  The prevalence of offensive content on the internet, encompassing hate speech\nand cyberbullying, is a pervasive issue worldwide. Consequently, it has\ngarnered significant attention from the machine learning (ML) and natural\nlanguage processing (NLP) communities. As a result, numerous systems have been\ndeveloped to automatically identify potentially harmful content and mitigate\nits impact. These systems can follow two approaches; (1) Use publicly available\nmodels and application endpoints, including prompting large language models\n(LLMs) (2) Annotate datasets and train ML models on them. However, both\napproaches lack an understanding of how generalizable they are. Furthermore,\nthe applicability of these systems is often questioned in off-domain and\npractical environments. This paper empirically evaluates the generalizability\nof offensive language detection models and datasets across a novel generalized\nbenchmark. We answer three research questions on generalizability. Our findings\nwill be useful in creating robust real-world offensive language detection\nsystems.\n", "link": "http://arxiv.org/abs/2407.18738v1", "date": "2024-07-26", "relevancy": 1.7475, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4622}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4383}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalized%20Offensive%20Language%20Identification&body=Title%3A%20Towards%20Generalized%20Offensive%20Language%20Identification%0AAuthor%3A%20Alphaeus%20Dmonte%20and%20Tejas%20Arya%20and%20Tharindu%20Ranasinghe%20and%20Marcos%20Zampieri%0AAbstract%3A%20%20%20The%20prevalence%20of%20offensive%20content%20on%20the%20internet%2C%20encompassing%20hate%20speech%0Aand%20cyberbullying%2C%20is%20a%20pervasive%20issue%20worldwide.%20Consequently%2C%20it%20has%0Agarnered%20significant%20attention%20from%20the%20machine%20learning%20%28ML%29%20and%20natural%0Alanguage%20processing%20%28NLP%29%20communities.%20As%20a%20result%2C%20numerous%20systems%20have%20been%0Adeveloped%20to%20automatically%20identify%20potentially%20harmful%20content%20and%20mitigate%0Aits%20impact.%20These%20systems%20can%20follow%20two%20approaches%3B%20%281%29%20Use%20publicly%20available%0Amodels%20and%20application%20endpoints%2C%20including%20prompting%20large%20language%20models%0A%28LLMs%29%20%282%29%20Annotate%20datasets%20and%20train%20ML%20models%20on%20them.%20However%2C%20both%0Aapproaches%20lack%20an%20understanding%20of%20how%20generalizable%20they%20are.%20Furthermore%2C%0Athe%20applicability%20of%20these%20systems%20is%20often%20questioned%20in%20off-domain%20and%0Apractical%20environments.%20This%20paper%20empirically%20evaluates%20the%20generalizability%0Aof%20offensive%20language%20detection%20models%20and%20datasets%20across%20a%20novel%20generalized%0Abenchmark.%20We%20answer%20three%20research%20questions%20on%20generalizability.%20Our%20findings%0Awill%20be%20useful%20in%20creating%20robust%20real-world%20offensive%20language%20detection%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalized%2520Offensive%2520Language%2520Identification%26entry.906535625%3DAlphaeus%2520Dmonte%2520and%2520Tejas%2520Arya%2520and%2520Tharindu%2520Ranasinghe%2520and%2520Marcos%2520Zampieri%26entry.1292438233%3D%2520%2520The%2520prevalence%2520of%2520offensive%2520content%2520on%2520the%2520internet%252C%2520encompassing%2520hate%2520speech%250Aand%2520cyberbullying%252C%2520is%2520a%2520pervasive%2520issue%2520worldwide.%2520Consequently%252C%2520it%2520has%250Agarnered%2520significant%2520attention%2520from%2520the%2520machine%2520learning%2520%2528ML%2529%2520and%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520communities.%2520As%2520a%2520result%252C%2520numerous%2520systems%2520have%2520been%250Adeveloped%2520to%2520automatically%2520identify%2520potentially%2520harmful%2520content%2520and%2520mitigate%250Aits%2520impact.%2520These%2520systems%2520can%2520follow%2520two%2520approaches%253B%2520%25281%2529%2520Use%2520publicly%2520available%250Amodels%2520and%2520application%2520endpoints%252C%2520including%2520prompting%2520large%2520language%2520models%250A%2528LLMs%2529%2520%25282%2529%2520Annotate%2520datasets%2520and%2520train%2520ML%2520models%2520on%2520them.%2520However%252C%2520both%250Aapproaches%2520lack%2520an%2520understanding%2520of%2520how%2520generalizable%2520they%2520are.%2520Furthermore%252C%250Athe%2520applicability%2520of%2520these%2520systems%2520is%2520often%2520questioned%2520in%2520off-domain%2520and%250Apractical%2520environments.%2520This%2520paper%2520empirically%2520evaluates%2520the%2520generalizability%250Aof%2520offensive%2520language%2520detection%2520models%2520and%2520datasets%2520across%2520a%2520novel%2520generalized%250Abenchmark.%2520We%2520answer%2520three%2520research%2520questions%2520on%2520generalizability.%2520Our%2520findings%250Awill%2520be%2520useful%2520in%2520creating%2520robust%2520real-world%2520offensive%2520language%2520detection%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalized%20Offensive%20Language%20Identification&entry.906535625=Alphaeus%20Dmonte%20and%20Tejas%20Arya%20and%20Tharindu%20Ranasinghe%20and%20Marcos%20Zampieri&entry.1292438233=%20%20The%20prevalence%20of%20offensive%20content%20on%20the%20internet%2C%20encompassing%20hate%20speech%0Aand%20cyberbullying%2C%20is%20a%20pervasive%20issue%20worldwide.%20Consequently%2C%20it%20has%0Agarnered%20significant%20attention%20from%20the%20machine%20learning%20%28ML%29%20and%20natural%0Alanguage%20processing%20%28NLP%29%20communities.%20As%20a%20result%2C%20numerous%20systems%20have%20been%0Adeveloped%20to%20automatically%20identify%20potentially%20harmful%20content%20and%20mitigate%0Aits%20impact.%20These%20systems%20can%20follow%20two%20approaches%3B%20%281%29%20Use%20publicly%20available%0Amodels%20and%20application%20endpoints%2C%20including%20prompting%20large%20language%20models%0A%28LLMs%29%20%282%29%20Annotate%20datasets%20and%20train%20ML%20models%20on%20them.%20However%2C%20both%0Aapproaches%20lack%20an%20understanding%20of%20how%20generalizable%20they%20are.%20Furthermore%2C%0Athe%20applicability%20of%20these%20systems%20is%20often%20questioned%20in%20off-domain%20and%0Apractical%20environments.%20This%20paper%20empirically%20evaluates%20the%20generalizability%0Aof%20offensive%20language%20detection%20models%20and%20datasets%20across%20a%20novel%20generalized%0Abenchmark.%20We%20answer%20three%20research%20questions%20on%20generalizability.%20Our%20findings%0Awill%20be%20useful%20in%20creating%20robust%20real-world%20offensive%20language%20detection%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18738v1&entry.124074799=Read"},
{"title": "Merit-based Fair Combinatorial Semi-Bandit with Unrestricted Feedback\n  Delays", "author": "Ziqun Chen and Kechao Cai and Zhuoyue Chen and Jinbei Zhang and John C. S. Lui", "abstract": "  We study the stochastic combinatorial semi-bandit problem with unrestricted\nfeedback delays under merit-based fairness constraints. This is motivated by\napplications such as crowdsourcing, and online advertising, where immediate\nfeedback is not immediately available and fairness among different choices (or\narms) is crucial. We consider two types of unrestricted feedback delays:\nreward-independent delays where the feedback delays are independent of the\nrewards, and reward-dependent delays where the feedback delays are correlated\nwith the rewards. Furthermore, we introduce merit-based fairness constraints to\nensure a fair selection of the arms. We define the reward regret and the\nfairness regret and present new bandit algorithms to select arms under\nunrestricted feedback delays based on their merits. We prove that our\nalgorithms all achieve sublinear expected reward regret and expected fairness\nregret, with a dependence on the quantiles of the delay distribution. We also\nconduct extensive experiments using synthetic and real-world data and show that\nour algorithms can fairly select arms with different feedback delays.\n", "link": "http://arxiv.org/abs/2407.15439v2", "date": "2024-07-26", "relevancy": 1.7416, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4647}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4422}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merit-based%20Fair%20Combinatorial%20Semi-Bandit%20with%20Unrestricted%20Feedback%0A%20%20Delays&body=Title%3A%20Merit-based%20Fair%20Combinatorial%20Semi-Bandit%20with%20Unrestricted%20Feedback%0A%20%20Delays%0AAuthor%3A%20Ziqun%20Chen%20and%20Kechao%20Cai%20and%20Zhuoyue%20Chen%20and%20Jinbei%20Zhang%20and%20John%20C.%20S.%20Lui%0AAbstract%3A%20%20%20We%20study%20the%20stochastic%20combinatorial%20semi-bandit%20problem%20with%20unrestricted%0Afeedback%20delays%20under%20merit-based%20fairness%20constraints.%20This%20is%20motivated%20by%0Aapplications%20such%20as%20crowdsourcing%2C%20and%20online%20advertising%2C%20where%20immediate%0Afeedback%20is%20not%20immediately%20available%20and%20fairness%20among%20different%20choices%20%28or%0Aarms%29%20is%20crucial.%20We%20consider%20two%20types%20of%20unrestricted%20feedback%20delays%3A%0Areward-independent%20delays%20where%20the%20feedback%20delays%20are%20independent%20of%20the%0Arewards%2C%20and%20reward-dependent%20delays%20where%20the%20feedback%20delays%20are%20correlated%0Awith%20the%20rewards.%20Furthermore%2C%20we%20introduce%20merit-based%20fairness%20constraints%20to%0Aensure%20a%20fair%20selection%20of%20the%20arms.%20We%20define%20the%20reward%20regret%20and%20the%0Afairness%20regret%20and%20present%20new%20bandit%20algorithms%20to%20select%20arms%20under%0Aunrestricted%20feedback%20delays%20based%20on%20their%20merits.%20We%20prove%20that%20our%0Aalgorithms%20all%20achieve%20sublinear%20expected%20reward%20regret%20and%20expected%20fairness%0Aregret%2C%20with%20a%20dependence%20on%20the%20quantiles%20of%20the%20delay%20distribution.%20We%20also%0Aconduct%20extensive%20experiments%20using%20synthetic%20and%20real-world%20data%20and%20show%20that%0Aour%20algorithms%20can%20fairly%20select%20arms%20with%20different%20feedback%20delays.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerit-based%2520Fair%2520Combinatorial%2520Semi-Bandit%2520with%2520Unrestricted%2520Feedback%250A%2520%2520Delays%26entry.906535625%3DZiqun%2520Chen%2520and%2520Kechao%2520Cai%2520and%2520Zhuoyue%2520Chen%2520and%2520Jinbei%2520Zhang%2520and%2520John%2520C.%2520S.%2520Lui%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520stochastic%2520combinatorial%2520semi-bandit%2520problem%2520with%2520unrestricted%250Afeedback%2520delays%2520under%2520merit-based%2520fairness%2520constraints.%2520This%2520is%2520motivated%2520by%250Aapplications%2520such%2520as%2520crowdsourcing%252C%2520and%2520online%2520advertising%252C%2520where%2520immediate%250Afeedback%2520is%2520not%2520immediately%2520available%2520and%2520fairness%2520among%2520different%2520choices%2520%2528or%250Aarms%2529%2520is%2520crucial.%2520We%2520consider%2520two%2520types%2520of%2520unrestricted%2520feedback%2520delays%253A%250Areward-independent%2520delays%2520where%2520the%2520feedback%2520delays%2520are%2520independent%2520of%2520the%250Arewards%252C%2520and%2520reward-dependent%2520delays%2520where%2520the%2520feedback%2520delays%2520are%2520correlated%250Awith%2520the%2520rewards.%2520Furthermore%252C%2520we%2520introduce%2520merit-based%2520fairness%2520constraints%2520to%250Aensure%2520a%2520fair%2520selection%2520of%2520the%2520arms.%2520We%2520define%2520the%2520reward%2520regret%2520and%2520the%250Afairness%2520regret%2520and%2520present%2520new%2520bandit%2520algorithms%2520to%2520select%2520arms%2520under%250Aunrestricted%2520feedback%2520delays%2520based%2520on%2520their%2520merits.%2520We%2520prove%2520that%2520our%250Aalgorithms%2520all%2520achieve%2520sublinear%2520expected%2520reward%2520regret%2520and%2520expected%2520fairness%250Aregret%252C%2520with%2520a%2520dependence%2520on%2520the%2520quantiles%2520of%2520the%2520delay%2520distribution.%2520We%2520also%250Aconduct%2520extensive%2520experiments%2520using%2520synthetic%2520and%2520real-world%2520data%2520and%2520show%2520that%250Aour%2520algorithms%2520can%2520fairly%2520select%2520arms%2520with%2520different%2520feedback%2520delays.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merit-based%20Fair%20Combinatorial%20Semi-Bandit%20with%20Unrestricted%20Feedback%0A%20%20Delays&entry.906535625=Ziqun%20Chen%20and%20Kechao%20Cai%20and%20Zhuoyue%20Chen%20and%20Jinbei%20Zhang%20and%20John%20C.%20S.%20Lui&entry.1292438233=%20%20We%20study%20the%20stochastic%20combinatorial%20semi-bandit%20problem%20with%20unrestricted%0Afeedback%20delays%20under%20merit-based%20fairness%20constraints.%20This%20is%20motivated%20by%0Aapplications%20such%20as%20crowdsourcing%2C%20and%20online%20advertising%2C%20where%20immediate%0Afeedback%20is%20not%20immediately%20available%20and%20fairness%20among%20different%20choices%20%28or%0Aarms%29%20is%20crucial.%20We%20consider%20two%20types%20of%20unrestricted%20feedback%20delays%3A%0Areward-independent%20delays%20where%20the%20feedback%20delays%20are%20independent%20of%20the%0Arewards%2C%20and%20reward-dependent%20delays%20where%20the%20feedback%20delays%20are%20correlated%0Awith%20the%20rewards.%20Furthermore%2C%20we%20introduce%20merit-based%20fairness%20constraints%20to%0Aensure%20a%20fair%20selection%20of%20the%20arms.%20We%20define%20the%20reward%20regret%20and%20the%0Afairness%20regret%20and%20present%20new%20bandit%20algorithms%20to%20select%20arms%20under%0Aunrestricted%20feedback%20delays%20based%20on%20their%20merits.%20We%20prove%20that%20our%0Aalgorithms%20all%20achieve%20sublinear%20expected%20reward%20regret%20and%20expected%20fairness%0Aregret%2C%20with%20a%20dependence%20on%20the%20quantiles%20of%20the%20delay%20distribution.%20We%20also%0Aconduct%20extensive%20experiments%20using%20synthetic%20and%20real-world%20data%20and%20show%20that%0Aour%20algorithms%20can%20fairly%20select%20arms%20with%20different%20feedback%20delays.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15439v2&entry.124074799=Read"},
{"title": "HRP: Human Affordances for Robotic Pre-Training", "author": "Mohan Kumar Srirama and Sudeep Dasari and Shikhar Bahl and Abhinav Gupta", "abstract": "  In order to *generalize* to various tasks in the wild, robotic agents will\nneed a suitable representation (i.e., vision network) that enables the robot to\npredict optimal actions given high dimensional vision inputs. However, learning\nsuch a representation requires an extreme amount of diverse training data,\nwhich is prohibitively expensive to collect on a real robot. How can we\novercome this problem? Instead of collecting more robot data, this paper\nproposes using internet-scale, human videos to extract \"affordances,\" both at\nthe environment and agent level, and distill them into a pre-trained\nrepresentation. We present a simple framework for pre-training representations\non hand, object, and contact \"affordance labels\" that highlight relevant\nobjects in images and how to interact with them. These affordances are\nautomatically extracted from human video data (with the help of off-the-shelf\ncomputer vision modules) and used to fine-tune existing representations. Our\napproach can efficiently fine-tune *any* existing representation, and results\nin models with stronger downstream robotic performance across the board. We\nexperimentally demonstrate (using 3000+ robot trials) that this affordance\npre-training scheme boosts performance by a minimum of 15% on 5 real-world\ntasks, which consider three diverse robot morphologies (including a dexterous\nhand). Unlike prior works in the space, these representations improve\nperformance across 3 different camera views. Quantitatively, we find that our\napproach leads to higher levels of generalization in out-of-distribution\nsettings. For code, weights, and data check: https://hrp-robot.github.io\n", "link": "http://arxiv.org/abs/2407.18911v1", "date": "2024-07-26", "relevancy": 1.7201, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6075}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6024}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRP%3A%20Human%20Affordances%20for%20Robotic%20Pre-Training&body=Title%3A%20HRP%3A%20Human%20Affordances%20for%20Robotic%20Pre-Training%0AAuthor%3A%20Mohan%20Kumar%20Srirama%20and%20Sudeep%20Dasari%20and%20Shikhar%20Bahl%20and%20Abhinav%20Gupta%0AAbstract%3A%20%20%20In%20order%20to%20%2Ageneralize%2A%20to%20various%20tasks%20in%20the%20wild%2C%20robotic%20agents%20will%0Aneed%20a%20suitable%20representation%20%28i.e.%2C%20vision%20network%29%20that%20enables%20the%20robot%20to%0Apredict%20optimal%20actions%20given%20high%20dimensional%20vision%20inputs.%20However%2C%20learning%0Asuch%20a%20representation%20requires%20an%20extreme%20amount%20of%20diverse%20training%20data%2C%0Awhich%20is%20prohibitively%20expensive%20to%20collect%20on%20a%20real%20robot.%20How%20can%20we%0Aovercome%20this%20problem%3F%20Instead%20of%20collecting%20more%20robot%20data%2C%20this%20paper%0Aproposes%20using%20internet-scale%2C%20human%20videos%20to%20extract%20%22affordances%2C%22%20both%20at%0Athe%20environment%20and%20agent%20level%2C%20and%20distill%20them%20into%20a%20pre-trained%0Arepresentation.%20We%20present%20a%20simple%20framework%20for%20pre-training%20representations%0Aon%20hand%2C%20object%2C%20and%20contact%20%22affordance%20labels%22%20that%20highlight%20relevant%0Aobjects%20in%20images%20and%20how%20to%20interact%20with%20them.%20These%20affordances%20are%0Aautomatically%20extracted%20from%20human%20video%20data%20%28with%20the%20help%20of%20off-the-shelf%0Acomputer%20vision%20modules%29%20and%20used%20to%20fine-tune%20existing%20representations.%20Our%0Aapproach%20can%20efficiently%20fine-tune%20%2Aany%2A%20existing%20representation%2C%20and%20results%0Ain%20models%20with%20stronger%20downstream%20robotic%20performance%20across%20the%20board.%20We%0Aexperimentally%20demonstrate%20%28using%203000%2B%20robot%20trials%29%20that%20this%20affordance%0Apre-training%20scheme%20boosts%20performance%20by%20a%20minimum%20of%2015%25%20on%205%20real-world%0Atasks%2C%20which%20consider%20three%20diverse%20robot%20morphologies%20%28including%20a%20dexterous%0Ahand%29.%20Unlike%20prior%20works%20in%20the%20space%2C%20these%20representations%20improve%0Aperformance%20across%203%20different%20camera%20views.%20Quantitatively%2C%20we%20find%20that%20our%0Aapproach%20leads%20to%20higher%20levels%20of%20generalization%20in%20out-of-distribution%0Asettings.%20For%20code%2C%20weights%2C%20and%20data%20check%3A%20https%3A//hrp-robot.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRP%253A%2520Human%2520Affordances%2520for%2520Robotic%2520Pre-Training%26entry.906535625%3DMohan%2520Kumar%2520Srirama%2520and%2520Sudeep%2520Dasari%2520and%2520Shikhar%2520Bahl%2520and%2520Abhinav%2520Gupta%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520%252Ageneralize%252A%2520to%2520various%2520tasks%2520in%2520the%2520wild%252C%2520robotic%2520agents%2520will%250Aneed%2520a%2520suitable%2520representation%2520%2528i.e.%252C%2520vision%2520network%2529%2520that%2520enables%2520the%2520robot%2520to%250Apredict%2520optimal%2520actions%2520given%2520high%2520dimensional%2520vision%2520inputs.%2520However%252C%2520learning%250Asuch%2520a%2520representation%2520requires%2520an%2520extreme%2520amount%2520of%2520diverse%2520training%2520data%252C%250Awhich%2520is%2520prohibitively%2520expensive%2520to%2520collect%2520on%2520a%2520real%2520robot.%2520How%2520can%2520we%250Aovercome%2520this%2520problem%253F%2520Instead%2520of%2520collecting%2520more%2520robot%2520data%252C%2520this%2520paper%250Aproposes%2520using%2520internet-scale%252C%2520human%2520videos%2520to%2520extract%2520%2522affordances%252C%2522%2520both%2520at%250Athe%2520environment%2520and%2520agent%2520level%252C%2520and%2520distill%2520them%2520into%2520a%2520pre-trained%250Arepresentation.%2520We%2520present%2520a%2520simple%2520framework%2520for%2520pre-training%2520representations%250Aon%2520hand%252C%2520object%252C%2520and%2520contact%2520%2522affordance%2520labels%2522%2520that%2520highlight%2520relevant%250Aobjects%2520in%2520images%2520and%2520how%2520to%2520interact%2520with%2520them.%2520These%2520affordances%2520are%250Aautomatically%2520extracted%2520from%2520human%2520video%2520data%2520%2528with%2520the%2520help%2520of%2520off-the-shelf%250Acomputer%2520vision%2520modules%2529%2520and%2520used%2520to%2520fine-tune%2520existing%2520representations.%2520Our%250Aapproach%2520can%2520efficiently%2520fine-tune%2520%252Aany%252A%2520existing%2520representation%252C%2520and%2520results%250Ain%2520models%2520with%2520stronger%2520downstream%2520robotic%2520performance%2520across%2520the%2520board.%2520We%250Aexperimentally%2520demonstrate%2520%2528using%25203000%252B%2520robot%2520trials%2529%2520that%2520this%2520affordance%250Apre-training%2520scheme%2520boosts%2520performance%2520by%2520a%2520minimum%2520of%252015%2525%2520on%25205%2520real-world%250Atasks%252C%2520which%2520consider%2520three%2520diverse%2520robot%2520morphologies%2520%2528including%2520a%2520dexterous%250Ahand%2529.%2520Unlike%2520prior%2520works%2520in%2520the%2520space%252C%2520these%2520representations%2520improve%250Aperformance%2520across%25203%2520different%2520camera%2520views.%2520Quantitatively%252C%2520we%2520find%2520that%2520our%250Aapproach%2520leads%2520to%2520higher%2520levels%2520of%2520generalization%2520in%2520out-of-distribution%250Asettings.%2520For%2520code%252C%2520weights%252C%2520and%2520data%2520check%253A%2520https%253A//hrp-robot.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRP%3A%20Human%20Affordances%20for%20Robotic%20Pre-Training&entry.906535625=Mohan%20Kumar%20Srirama%20and%20Sudeep%20Dasari%20and%20Shikhar%20Bahl%20and%20Abhinav%20Gupta&entry.1292438233=%20%20In%20order%20to%20%2Ageneralize%2A%20to%20various%20tasks%20in%20the%20wild%2C%20robotic%20agents%20will%0Aneed%20a%20suitable%20representation%20%28i.e.%2C%20vision%20network%29%20that%20enables%20the%20robot%20to%0Apredict%20optimal%20actions%20given%20high%20dimensional%20vision%20inputs.%20However%2C%20learning%0Asuch%20a%20representation%20requires%20an%20extreme%20amount%20of%20diverse%20training%20data%2C%0Awhich%20is%20prohibitively%20expensive%20to%20collect%20on%20a%20real%20robot.%20How%20can%20we%0Aovercome%20this%20problem%3F%20Instead%20of%20collecting%20more%20robot%20data%2C%20this%20paper%0Aproposes%20using%20internet-scale%2C%20human%20videos%20to%20extract%20%22affordances%2C%22%20both%20at%0Athe%20environment%20and%20agent%20level%2C%20and%20distill%20them%20into%20a%20pre-trained%0Arepresentation.%20We%20present%20a%20simple%20framework%20for%20pre-training%20representations%0Aon%20hand%2C%20object%2C%20and%20contact%20%22affordance%20labels%22%20that%20highlight%20relevant%0Aobjects%20in%20images%20and%20how%20to%20interact%20with%20them.%20These%20affordances%20are%0Aautomatically%20extracted%20from%20human%20video%20data%20%28with%20the%20help%20of%20off-the-shelf%0Acomputer%20vision%20modules%29%20and%20used%20to%20fine-tune%20existing%20representations.%20Our%0Aapproach%20can%20efficiently%20fine-tune%20%2Aany%2A%20existing%20representation%2C%20and%20results%0Ain%20models%20with%20stronger%20downstream%20robotic%20performance%20across%20the%20board.%20We%0Aexperimentally%20demonstrate%20%28using%203000%2B%20robot%20trials%29%20that%20this%20affordance%0Apre-training%20scheme%20boosts%20performance%20by%20a%20minimum%20of%2015%25%20on%205%20real-world%0Atasks%2C%20which%20consider%20three%20diverse%20robot%20morphologies%20%28including%20a%20dexterous%0Ahand%29.%20Unlike%20prior%20works%20in%20the%20space%2C%20these%20representations%20improve%0Aperformance%20across%203%20different%20camera%20views.%20Quantitatively%2C%20we%20find%20that%20our%0Aapproach%20leads%20to%20higher%20levels%20of%20generalization%20in%20out-of-distribution%0Asettings.%20For%20code%2C%20weights%2C%20and%20data%20check%3A%20https%3A//hrp-robot.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18911v1&entry.124074799=Read"},
{"title": "Aspects of importance sampling in parameter selection for neural\n  networks using ridgelet transform", "author": "Hikaru Homma and Jun Ohkubo", "abstract": "  The choice of parameters in neural networks is crucial in the performance,\nand an oracle distribution derived from the ridgelet transform enables us to\nobtain suitable initial parameters. In other words, the distribution of\nparameters is connected to the integral representation of target functions. The\noracle distribution allows us to avoid the conventional backpropagation\nlearning process; only a linear regression is enough to construct the neural\nnetwork in simple cases. This study provides a new look at the oracle\ndistributions and ridgelet transforms, i.e., an aspect of importance sampling.\nIn addition, we propose extensions of the parameter sampling methods. We\ndemonstrate the aspect of importance sampling and the proposed sampling\nalgorithms via one-dimensional and high-dimensional examples; the results imply\nthat the magnitude of weight parameters could be more crucial than the\nintercept parameters.\n", "link": "http://arxiv.org/abs/2407.18655v1", "date": "2024-07-26", "relevancy": 1.7165, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4384}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4326}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aspects%20of%20importance%20sampling%20in%20parameter%20selection%20for%20neural%0A%20%20networks%20using%20ridgelet%20transform&body=Title%3A%20Aspects%20of%20importance%20sampling%20in%20parameter%20selection%20for%20neural%0A%20%20networks%20using%20ridgelet%20transform%0AAuthor%3A%20Hikaru%20Homma%20and%20Jun%20Ohkubo%0AAbstract%3A%20%20%20The%20choice%20of%20parameters%20in%20neural%20networks%20is%20crucial%20in%20the%20performance%2C%0Aand%20an%20oracle%20distribution%20derived%20from%20the%20ridgelet%20transform%20enables%20us%20to%0Aobtain%20suitable%20initial%20parameters.%20In%20other%20words%2C%20the%20distribution%20of%0Aparameters%20is%20connected%20to%20the%20integral%20representation%20of%20target%20functions.%20The%0Aoracle%20distribution%20allows%20us%20to%20avoid%20the%20conventional%20backpropagation%0Alearning%20process%3B%20only%20a%20linear%20regression%20is%20enough%20to%20construct%20the%20neural%0Anetwork%20in%20simple%20cases.%20This%20study%20provides%20a%20new%20look%20at%20the%20oracle%0Adistributions%20and%20ridgelet%20transforms%2C%20i.e.%2C%20an%20aspect%20of%20importance%20sampling.%0AIn%20addition%2C%20we%20propose%20extensions%20of%20the%20parameter%20sampling%20methods.%20We%0Ademonstrate%20the%20aspect%20of%20importance%20sampling%20and%20the%20proposed%20sampling%0Aalgorithms%20via%20one-dimensional%20and%20high-dimensional%20examples%3B%20the%20results%20imply%0Athat%20the%20magnitude%20of%20weight%20parameters%20could%20be%20more%20crucial%20than%20the%0Aintercept%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAspects%2520of%2520importance%2520sampling%2520in%2520parameter%2520selection%2520for%2520neural%250A%2520%2520networks%2520using%2520ridgelet%2520transform%26entry.906535625%3DHikaru%2520Homma%2520and%2520Jun%2520Ohkubo%26entry.1292438233%3D%2520%2520The%2520choice%2520of%2520parameters%2520in%2520neural%2520networks%2520is%2520crucial%2520in%2520the%2520performance%252C%250Aand%2520an%2520oracle%2520distribution%2520derived%2520from%2520the%2520ridgelet%2520transform%2520enables%2520us%2520to%250Aobtain%2520suitable%2520initial%2520parameters.%2520In%2520other%2520words%252C%2520the%2520distribution%2520of%250Aparameters%2520is%2520connected%2520to%2520the%2520integral%2520representation%2520of%2520target%2520functions.%2520The%250Aoracle%2520distribution%2520allows%2520us%2520to%2520avoid%2520the%2520conventional%2520backpropagation%250Alearning%2520process%253B%2520only%2520a%2520linear%2520regression%2520is%2520enough%2520to%2520construct%2520the%2520neural%250Anetwork%2520in%2520simple%2520cases.%2520This%2520study%2520provides%2520a%2520new%2520look%2520at%2520the%2520oracle%250Adistributions%2520and%2520ridgelet%2520transforms%252C%2520i.e.%252C%2520an%2520aspect%2520of%2520importance%2520sampling.%250AIn%2520addition%252C%2520we%2520propose%2520extensions%2520of%2520the%2520parameter%2520sampling%2520methods.%2520We%250Ademonstrate%2520the%2520aspect%2520of%2520importance%2520sampling%2520and%2520the%2520proposed%2520sampling%250Aalgorithms%2520via%2520one-dimensional%2520and%2520high-dimensional%2520examples%253B%2520the%2520results%2520imply%250Athat%2520the%2520magnitude%2520of%2520weight%2520parameters%2520could%2520be%2520more%2520crucial%2520than%2520the%250Aintercept%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aspects%20of%20importance%20sampling%20in%20parameter%20selection%20for%20neural%0A%20%20networks%20using%20ridgelet%20transform&entry.906535625=Hikaru%20Homma%20and%20Jun%20Ohkubo&entry.1292438233=%20%20The%20choice%20of%20parameters%20in%20neural%20networks%20is%20crucial%20in%20the%20performance%2C%0Aand%20an%20oracle%20distribution%20derived%20from%20the%20ridgelet%20transform%20enables%20us%20to%0Aobtain%20suitable%20initial%20parameters.%20In%20other%20words%2C%20the%20distribution%20of%0Aparameters%20is%20connected%20to%20the%20integral%20representation%20of%20target%20functions.%20The%0Aoracle%20distribution%20allows%20us%20to%20avoid%20the%20conventional%20backpropagation%0Alearning%20process%3B%20only%20a%20linear%20regression%20is%20enough%20to%20construct%20the%20neural%0Anetwork%20in%20simple%20cases.%20This%20study%20provides%20a%20new%20look%20at%20the%20oracle%0Adistributions%20and%20ridgelet%20transforms%2C%20i.e.%2C%20an%20aspect%20of%20importance%20sampling.%0AIn%20addition%2C%20we%20propose%20extensions%20of%20the%20parameter%20sampling%20methods.%20We%0Ademonstrate%20the%20aspect%20of%20importance%20sampling%20and%20the%20proposed%20sampling%0Aalgorithms%20via%20one-dimensional%20and%20high-dimensional%20examples%3B%20the%20results%20imply%0Athat%20the%20magnitude%20of%20weight%20parameters%20could%20be%20more%20crucial%20than%20the%0Aintercept%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18655v1&entry.124074799=Read"},
{"title": "Atmospheric Density-Compensating Model Predictive Control for Targeted\n  Reentry of Drag-Modulated Spacecraft", "author": "Alex D. Hayes and Ryan J. Caverly", "abstract": "  This paper presents an estimation and control framework that enables the\ntargeted reentry of a drag-modulated spacecraft in the presence of atmospheric\ndensity uncertainty. In particular, an extended Kalman filter (EKF) is used to\nestimate the in-flight density errors relative to the atmospheric density used\nto generate the nominal guidance trajectory. This information is leveraged\nwithin a model predictive control (MPC) strategy to improve tracking\nperformance, reduce control effort, and increase robustness to actuator\nsaturation compared to the state-of-the-art approach. The estimation and\ncontrol framework is tested in a Monte Carlo simulation campaign with\nhistorical space weather data. These simulation efforts demonstrate that the\nproposed framework is able to stay within 100 km of the guidance trajectory at\nall points in time for 98.4% of cases. The remaining 1.6% of cases were pushed\naway from the guidance by large density errors, many due to significant solar\nstorms and flares, that could not physically be compensated for by the drag\ncontrol device. For the successful cases, the proposed framework was able to\nguide the spacecraft to the desired location at the entry interface altitude\nwith a mean error of 12.1 km and 99.7% of cases below 100 km.\n", "link": "http://arxiv.org/abs/2407.18762v1", "date": "2024-07-26", "relevancy": 1.7148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4447}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atmospheric%20Density-Compensating%20Model%20Predictive%20Control%20for%20Targeted%0A%20%20Reentry%20of%20Drag-Modulated%20Spacecraft&body=Title%3A%20Atmospheric%20Density-Compensating%20Model%20Predictive%20Control%20for%20Targeted%0A%20%20Reentry%20of%20Drag-Modulated%20Spacecraft%0AAuthor%3A%20Alex%20D.%20Hayes%20and%20Ryan%20J.%20Caverly%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20estimation%20and%20control%20framework%20that%20enables%20the%0Atargeted%20reentry%20of%20a%20drag-modulated%20spacecraft%20in%20the%20presence%20of%20atmospheric%0Adensity%20uncertainty.%20In%20particular%2C%20an%20extended%20Kalman%20filter%20%28EKF%29%20is%20used%20to%0Aestimate%20the%20in-flight%20density%20errors%20relative%20to%20the%20atmospheric%20density%20used%0Ato%20generate%20the%20nominal%20guidance%20trajectory.%20This%20information%20is%20leveraged%0Awithin%20a%20model%20predictive%20control%20%28MPC%29%20strategy%20to%20improve%20tracking%0Aperformance%2C%20reduce%20control%20effort%2C%20and%20increase%20robustness%20to%20actuator%0Asaturation%20compared%20to%20the%20state-of-the-art%20approach.%20The%20estimation%20and%0Acontrol%20framework%20is%20tested%20in%20a%20Monte%20Carlo%20simulation%20campaign%20with%0Ahistorical%20space%20weather%20data.%20These%20simulation%20efforts%20demonstrate%20that%20the%0Aproposed%20framework%20is%20able%20to%20stay%20within%20100%20km%20of%20the%20guidance%20trajectory%20at%0Aall%20points%20in%20time%20for%2098.4%25%20of%20cases.%20The%20remaining%201.6%25%20of%20cases%20were%20pushed%0Aaway%20from%20the%20guidance%20by%20large%20density%20errors%2C%20many%20due%20to%20significant%20solar%0Astorms%20and%20flares%2C%20that%20could%20not%20physically%20be%20compensated%20for%20by%20the%20drag%0Acontrol%20device.%20For%20the%20successful%20cases%2C%20the%20proposed%20framework%20was%20able%20to%0Aguide%20the%20spacecraft%20to%20the%20desired%20location%20at%20the%20entry%20interface%20altitude%0Awith%20a%20mean%20error%20of%2012.1%20km%20and%2099.7%25%20of%20cases%20below%20100%20km.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtmospheric%2520Density-Compensating%2520Model%2520Predictive%2520Control%2520for%2520Targeted%250A%2520%2520Reentry%2520of%2520Drag-Modulated%2520Spacecraft%26entry.906535625%3DAlex%2520D.%2520Hayes%2520and%2520Ryan%2520J.%2520Caverly%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520estimation%2520and%2520control%2520framework%2520that%2520enables%2520the%250Atargeted%2520reentry%2520of%2520a%2520drag-modulated%2520spacecraft%2520in%2520the%2520presence%2520of%2520atmospheric%250Adensity%2520uncertainty.%2520In%2520particular%252C%2520an%2520extended%2520Kalman%2520filter%2520%2528EKF%2529%2520is%2520used%2520to%250Aestimate%2520the%2520in-flight%2520density%2520errors%2520relative%2520to%2520the%2520atmospheric%2520density%2520used%250Ato%2520generate%2520the%2520nominal%2520guidance%2520trajectory.%2520This%2520information%2520is%2520leveraged%250Awithin%2520a%2520model%2520predictive%2520control%2520%2528MPC%2529%2520strategy%2520to%2520improve%2520tracking%250Aperformance%252C%2520reduce%2520control%2520effort%252C%2520and%2520increase%2520robustness%2520to%2520actuator%250Asaturation%2520compared%2520to%2520the%2520state-of-the-art%2520approach.%2520The%2520estimation%2520and%250Acontrol%2520framework%2520is%2520tested%2520in%2520a%2520Monte%2520Carlo%2520simulation%2520campaign%2520with%250Ahistorical%2520space%2520weather%2520data.%2520These%2520simulation%2520efforts%2520demonstrate%2520that%2520the%250Aproposed%2520framework%2520is%2520able%2520to%2520stay%2520within%2520100%2520km%2520of%2520the%2520guidance%2520trajectory%2520at%250Aall%2520points%2520in%2520time%2520for%252098.4%2525%2520of%2520cases.%2520The%2520remaining%25201.6%2525%2520of%2520cases%2520were%2520pushed%250Aaway%2520from%2520the%2520guidance%2520by%2520large%2520density%2520errors%252C%2520many%2520due%2520to%2520significant%2520solar%250Astorms%2520and%2520flares%252C%2520that%2520could%2520not%2520physically%2520be%2520compensated%2520for%2520by%2520the%2520drag%250Acontrol%2520device.%2520For%2520the%2520successful%2520cases%252C%2520the%2520proposed%2520framework%2520was%2520able%2520to%250Aguide%2520the%2520spacecraft%2520to%2520the%2520desired%2520location%2520at%2520the%2520entry%2520interface%2520altitude%250Awith%2520a%2520mean%2520error%2520of%252012.1%2520km%2520and%252099.7%2525%2520of%2520cases%2520below%2520100%2520km.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atmospheric%20Density-Compensating%20Model%20Predictive%20Control%20for%20Targeted%0A%20%20Reentry%20of%20Drag-Modulated%20Spacecraft&entry.906535625=Alex%20D.%20Hayes%20and%20Ryan%20J.%20Caverly&entry.1292438233=%20%20This%20paper%20presents%20an%20estimation%20and%20control%20framework%20that%20enables%20the%0Atargeted%20reentry%20of%20a%20drag-modulated%20spacecraft%20in%20the%20presence%20of%20atmospheric%0Adensity%20uncertainty.%20In%20particular%2C%20an%20extended%20Kalman%20filter%20%28EKF%29%20is%20used%20to%0Aestimate%20the%20in-flight%20density%20errors%20relative%20to%20the%20atmospheric%20density%20used%0Ato%20generate%20the%20nominal%20guidance%20trajectory.%20This%20information%20is%20leveraged%0Awithin%20a%20model%20predictive%20control%20%28MPC%29%20strategy%20to%20improve%20tracking%0Aperformance%2C%20reduce%20control%20effort%2C%20and%20increase%20robustness%20to%20actuator%0Asaturation%20compared%20to%20the%20state-of-the-art%20approach.%20The%20estimation%20and%0Acontrol%20framework%20is%20tested%20in%20a%20Monte%20Carlo%20simulation%20campaign%20with%0Ahistorical%20space%20weather%20data.%20These%20simulation%20efforts%20demonstrate%20that%20the%0Aproposed%20framework%20is%20able%20to%20stay%20within%20100%20km%20of%20the%20guidance%20trajectory%20at%0Aall%20points%20in%20time%20for%2098.4%25%20of%20cases.%20The%20remaining%201.6%25%20of%20cases%20were%20pushed%0Aaway%20from%20the%20guidance%20by%20large%20density%20errors%2C%20many%20due%20to%20significant%20solar%0Astorms%20and%20flares%2C%20that%20could%20not%20physically%20be%20compensated%20for%20by%20the%20drag%0Acontrol%20device.%20For%20the%20successful%20cases%2C%20the%20proposed%20framework%20was%20able%20to%0Aguide%20the%20spacecraft%20to%20the%20desired%20location%20at%20the%20entry%20interface%20altitude%0Awith%20a%20mean%20error%20of%2012.1%20km%20and%2099.7%25%20of%20cases%20below%20100%20km.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18762v1&entry.124074799=Read"},
{"title": "Modeling the Lane-Change Reactions to Merging Vehicles for Highway\n  On-Ramp Simulations", "author": "Dustin Holley and Jovin Dsa and Hossein Nourkhiz Mahjoub and Gibran Ali and Tyler Naes and Ehsan Moradi-Pari and Pawan Sai Kallepalli", "abstract": "  Enhancing simulation environments to replicate real-world driver behavior is\nessential for developing Autonomous Vehicle technology. While some previous\nworks have studied the yielding reaction of lag vehicles in response to a\nmerging car at highway on-ramps, the possible lane-change reaction of the lag\ncar has not been widely studied. In this work we aim to improve the simulation\nof the highway merge scenario by including the lane-change reaction in addition\nto yielding behavior of main-lane lag vehicles, and we evaluate two different\nmodels for their ability to capture this reactive lane-change behavior. To tune\nthe payoff functions of these models, a novel naturalistic dataset was\ncollected on U.S. highways that provided several hours of merge-specific data\nto learn the lane change behavior of U.S. drivers. To make sure that we are\ncollecting a representative set of different U.S. highway geometries in our\ndata, we surveyed 50,000 U.S. highway on-ramps and then selected eight\nrepresentative sites. The data were collected using roadside-mounted lidar\nsensors to capture various merge driver interactions. The models were\ndemonstrated to be configurable for both keep-straight and lane-change\nbehavior. The models were finally integrated into a high-fidelity simulation\nenvironment and confirmed to have adequate computation time efficiency for use\nin large-scale simulations to support autonomous vehicle development.\n", "link": "http://arxiv.org/abs/2404.09851v2", "date": "2024-07-26", "relevancy": 1.7097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4418}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20the%20Lane-Change%20Reactions%20to%20Merging%20Vehicles%20for%20Highway%0A%20%20On-Ramp%20Simulations&body=Title%3A%20Modeling%20the%20Lane-Change%20Reactions%20to%20Merging%20Vehicles%20for%20Highway%0A%20%20On-Ramp%20Simulations%0AAuthor%3A%20Dustin%20Holley%20and%20Jovin%20Dsa%20and%20Hossein%20Nourkhiz%20Mahjoub%20and%20Gibran%20Ali%20and%20Tyler%20Naes%20and%20Ehsan%20Moradi-Pari%20and%20Pawan%20Sai%20Kallepalli%0AAbstract%3A%20%20%20Enhancing%20simulation%20environments%20to%20replicate%20real-world%20driver%20behavior%20is%0Aessential%20for%20developing%20Autonomous%20Vehicle%20technology.%20While%20some%20previous%0Aworks%20have%20studied%20the%20yielding%20reaction%20of%20lag%20vehicles%20in%20response%20to%20a%0Amerging%20car%20at%20highway%20on-ramps%2C%20the%20possible%20lane-change%20reaction%20of%20the%20lag%0Acar%20has%20not%20been%20widely%20studied.%20In%20this%20work%20we%20aim%20to%20improve%20the%20simulation%0Aof%20the%20highway%20merge%20scenario%20by%20including%20the%20lane-change%20reaction%20in%20addition%0Ato%20yielding%20behavior%20of%20main-lane%20lag%20vehicles%2C%20and%20we%20evaluate%20two%20different%0Amodels%20for%20their%20ability%20to%20capture%20this%20reactive%20lane-change%20behavior.%20To%20tune%0Athe%20payoff%20functions%20of%20these%20models%2C%20a%20novel%20naturalistic%20dataset%20was%0Acollected%20on%20U.S.%20highways%20that%20provided%20several%20hours%20of%20merge-specific%20data%0Ato%20learn%20the%20lane%20change%20behavior%20of%20U.S.%20drivers.%20To%20make%20sure%20that%20we%20are%0Acollecting%20a%20representative%20set%20of%20different%20U.S.%20highway%20geometries%20in%20our%0Adata%2C%20we%20surveyed%2050%2C000%20U.S.%20highway%20on-ramps%20and%20then%20selected%20eight%0Arepresentative%20sites.%20The%20data%20were%20collected%20using%20roadside-mounted%20lidar%0Asensors%20to%20capture%20various%20merge%20driver%20interactions.%20The%20models%20were%0Ademonstrated%20to%20be%20configurable%20for%20both%20keep-straight%20and%20lane-change%0Abehavior.%20The%20models%20were%20finally%20integrated%20into%20a%20high-fidelity%20simulation%0Aenvironment%20and%20confirmed%20to%20have%20adequate%20computation%20time%20efficiency%20for%20use%0Ain%20large-scale%20simulations%20to%20support%20autonomous%20vehicle%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520the%2520Lane-Change%2520Reactions%2520to%2520Merging%2520Vehicles%2520for%2520Highway%250A%2520%2520On-Ramp%2520Simulations%26entry.906535625%3DDustin%2520Holley%2520and%2520Jovin%2520Dsa%2520and%2520Hossein%2520Nourkhiz%2520Mahjoub%2520and%2520Gibran%2520Ali%2520and%2520Tyler%2520Naes%2520and%2520Ehsan%2520Moradi-Pari%2520and%2520Pawan%2520Sai%2520Kallepalli%26entry.1292438233%3D%2520%2520Enhancing%2520simulation%2520environments%2520to%2520replicate%2520real-world%2520driver%2520behavior%2520is%250Aessential%2520for%2520developing%2520Autonomous%2520Vehicle%2520technology.%2520While%2520some%2520previous%250Aworks%2520have%2520studied%2520the%2520yielding%2520reaction%2520of%2520lag%2520vehicles%2520in%2520response%2520to%2520a%250Amerging%2520car%2520at%2520highway%2520on-ramps%252C%2520the%2520possible%2520lane-change%2520reaction%2520of%2520the%2520lag%250Acar%2520has%2520not%2520been%2520widely%2520studied.%2520In%2520this%2520work%2520we%2520aim%2520to%2520improve%2520the%2520simulation%250Aof%2520the%2520highway%2520merge%2520scenario%2520by%2520including%2520the%2520lane-change%2520reaction%2520in%2520addition%250Ato%2520yielding%2520behavior%2520of%2520main-lane%2520lag%2520vehicles%252C%2520and%2520we%2520evaluate%2520two%2520different%250Amodels%2520for%2520their%2520ability%2520to%2520capture%2520this%2520reactive%2520lane-change%2520behavior.%2520To%2520tune%250Athe%2520payoff%2520functions%2520of%2520these%2520models%252C%2520a%2520novel%2520naturalistic%2520dataset%2520was%250Acollected%2520on%2520U.S.%2520highways%2520that%2520provided%2520several%2520hours%2520of%2520merge-specific%2520data%250Ato%2520learn%2520the%2520lane%2520change%2520behavior%2520of%2520U.S.%2520drivers.%2520To%2520make%2520sure%2520that%2520we%2520are%250Acollecting%2520a%2520representative%2520set%2520of%2520different%2520U.S.%2520highway%2520geometries%2520in%2520our%250Adata%252C%2520we%2520surveyed%252050%252C000%2520U.S.%2520highway%2520on-ramps%2520and%2520then%2520selected%2520eight%250Arepresentative%2520sites.%2520The%2520data%2520were%2520collected%2520using%2520roadside-mounted%2520lidar%250Asensors%2520to%2520capture%2520various%2520merge%2520driver%2520interactions.%2520The%2520models%2520were%250Ademonstrated%2520to%2520be%2520configurable%2520for%2520both%2520keep-straight%2520and%2520lane-change%250Abehavior.%2520The%2520models%2520were%2520finally%2520integrated%2520into%2520a%2520high-fidelity%2520simulation%250Aenvironment%2520and%2520confirmed%2520to%2520have%2520adequate%2520computation%2520time%2520efficiency%2520for%2520use%250Ain%2520large-scale%2520simulations%2520to%2520support%2520autonomous%2520vehicle%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20the%20Lane-Change%20Reactions%20to%20Merging%20Vehicles%20for%20Highway%0A%20%20On-Ramp%20Simulations&entry.906535625=Dustin%20Holley%20and%20Jovin%20Dsa%20and%20Hossein%20Nourkhiz%20Mahjoub%20and%20Gibran%20Ali%20and%20Tyler%20Naes%20and%20Ehsan%20Moradi-Pari%20and%20Pawan%20Sai%20Kallepalli&entry.1292438233=%20%20Enhancing%20simulation%20environments%20to%20replicate%20real-world%20driver%20behavior%20is%0Aessential%20for%20developing%20Autonomous%20Vehicle%20technology.%20While%20some%20previous%0Aworks%20have%20studied%20the%20yielding%20reaction%20of%20lag%20vehicles%20in%20response%20to%20a%0Amerging%20car%20at%20highway%20on-ramps%2C%20the%20possible%20lane-change%20reaction%20of%20the%20lag%0Acar%20has%20not%20been%20widely%20studied.%20In%20this%20work%20we%20aim%20to%20improve%20the%20simulation%0Aof%20the%20highway%20merge%20scenario%20by%20including%20the%20lane-change%20reaction%20in%20addition%0Ato%20yielding%20behavior%20of%20main-lane%20lag%20vehicles%2C%20and%20we%20evaluate%20two%20different%0Amodels%20for%20their%20ability%20to%20capture%20this%20reactive%20lane-change%20behavior.%20To%20tune%0Athe%20payoff%20functions%20of%20these%20models%2C%20a%20novel%20naturalistic%20dataset%20was%0Acollected%20on%20U.S.%20highways%20that%20provided%20several%20hours%20of%20merge-specific%20data%0Ato%20learn%20the%20lane%20change%20behavior%20of%20U.S.%20drivers.%20To%20make%20sure%20that%20we%20are%0Acollecting%20a%20representative%20set%20of%20different%20U.S.%20highway%20geometries%20in%20our%0Adata%2C%20we%20surveyed%2050%2C000%20U.S.%20highway%20on-ramps%20and%20then%20selected%20eight%0Arepresentative%20sites.%20The%20data%20were%20collected%20using%20roadside-mounted%20lidar%0Asensors%20to%20capture%20various%20merge%20driver%20interactions.%20The%20models%20were%0Ademonstrated%20to%20be%20configurable%20for%20both%20keep-straight%20and%20lane-change%0Abehavior.%20The%20models%20were%20finally%20integrated%20into%20a%20high-fidelity%20simulation%0Aenvironment%20and%20confirmed%20to%20have%20adequate%20computation%20time%20efficiency%20for%20use%0Ain%20large-scale%20simulations%20to%20support%20autonomous%20vehicle%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09851v2&entry.124074799=Read"},
{"title": "Unsqueeze [CLS] Bottleneck to Learn Rich Representations", "author": "Qing Su and Shihao Ji", "abstract": "  Distillation-based self-supervised learning typically leads to more\ncompressed representations due to its radical clustering process and the\nimplementation of a sharper target distribution. To overcome this limitation\nand preserve more information from input, we introduce UDI, conceptualized as\nUnsqueezed Distillation-based self-supervised learning (SSL). UDI enriches the\nlearned representation by encouraging multimodal prediction distilled from a\nconsolidated profile of local predictions that are derived via stratified\nsampling. Our evaluations show that UDI not only promotes semantically\nmeaningful representations at instance level, delivering superior or\ncompetitive results to state-of-the-art SSL methods in image classification,\nbut also effectively preserves the nuisance of input, which yields significant\nimprovement in dense prediction tasks, including object detection and\nsegmentation. Additionally, UDI performs competitively in low-shot image\nclassification, improving the scalability of joint-embedding pipelines. Various\nvisualizations and ablation studies are presented to further elucidate the\nmechanisms behind UDI. Our source code is available at\nhttps://github.com/ISL-CV/udi.\n", "link": "http://arxiv.org/abs/2407.17671v2", "date": "2024-07-26", "relevancy": 1.6956, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5858}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5617}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsqueeze%20%5BCLS%5D%20Bottleneck%20to%20Learn%20Rich%20Representations&body=Title%3A%20Unsqueeze%20%5BCLS%5D%20Bottleneck%20to%20Learn%20Rich%20Representations%0AAuthor%3A%20Qing%20Su%20and%20Shihao%20Ji%0AAbstract%3A%20%20%20Distillation-based%20self-supervised%20learning%20typically%20leads%20to%20more%0Acompressed%20representations%20due%20to%20its%20radical%20clustering%20process%20and%20the%0Aimplementation%20of%20a%20sharper%20target%20distribution.%20To%20overcome%20this%20limitation%0Aand%20preserve%20more%20information%20from%20input%2C%20we%20introduce%20UDI%2C%20conceptualized%20as%0AUnsqueezed%20Distillation-based%20self-supervised%20learning%20%28SSL%29.%20UDI%20enriches%20the%0Alearned%20representation%20by%20encouraging%20multimodal%20prediction%20distilled%20from%20a%0Aconsolidated%20profile%20of%20local%20predictions%20that%20are%20derived%20via%20stratified%0Asampling.%20Our%20evaluations%20show%20that%20UDI%20not%20only%20promotes%20semantically%0Ameaningful%20representations%20at%20instance%20level%2C%20delivering%20superior%20or%0Acompetitive%20results%20to%20state-of-the-art%20SSL%20methods%20in%20image%20classification%2C%0Abut%20also%20effectively%20preserves%20the%20nuisance%20of%20input%2C%20which%20yields%20significant%0Aimprovement%20in%20dense%20prediction%20tasks%2C%20including%20object%20detection%20and%0Asegmentation.%20Additionally%2C%20UDI%20performs%20competitively%20in%20low-shot%20image%0Aclassification%2C%20improving%20the%20scalability%20of%20joint-embedding%20pipelines.%20Various%0Avisualizations%20and%20ablation%20studies%20are%20presented%20to%20further%20elucidate%20the%0Amechanisms%20behind%20UDI.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ISL-CV/udi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsqueeze%2520%255BCLS%255D%2520Bottleneck%2520to%2520Learn%2520Rich%2520Representations%26entry.906535625%3DQing%2520Su%2520and%2520Shihao%2520Ji%26entry.1292438233%3D%2520%2520Distillation-based%2520self-supervised%2520learning%2520typically%2520leads%2520to%2520more%250Acompressed%2520representations%2520due%2520to%2520its%2520radical%2520clustering%2520process%2520and%2520the%250Aimplementation%2520of%2520a%2520sharper%2520target%2520distribution.%2520To%2520overcome%2520this%2520limitation%250Aand%2520preserve%2520more%2520information%2520from%2520input%252C%2520we%2520introduce%2520UDI%252C%2520conceptualized%2520as%250AUnsqueezed%2520Distillation-based%2520self-supervised%2520learning%2520%2528SSL%2529.%2520UDI%2520enriches%2520the%250Alearned%2520representation%2520by%2520encouraging%2520multimodal%2520prediction%2520distilled%2520from%2520a%250Aconsolidated%2520profile%2520of%2520local%2520predictions%2520that%2520are%2520derived%2520via%2520stratified%250Asampling.%2520Our%2520evaluations%2520show%2520that%2520UDI%2520not%2520only%2520promotes%2520semantically%250Ameaningful%2520representations%2520at%2520instance%2520level%252C%2520delivering%2520superior%2520or%250Acompetitive%2520results%2520to%2520state-of-the-art%2520SSL%2520methods%2520in%2520image%2520classification%252C%250Abut%2520also%2520effectively%2520preserves%2520the%2520nuisance%2520of%2520input%252C%2520which%2520yields%2520significant%250Aimprovement%2520in%2520dense%2520prediction%2520tasks%252C%2520including%2520object%2520detection%2520and%250Asegmentation.%2520Additionally%252C%2520UDI%2520performs%2520competitively%2520in%2520low-shot%2520image%250Aclassification%252C%2520improving%2520the%2520scalability%2520of%2520joint-embedding%2520pipelines.%2520Various%250Avisualizations%2520and%2520ablation%2520studies%2520are%2520presented%2520to%2520further%2520elucidate%2520the%250Amechanisms%2520behind%2520UDI.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ISL-CV/udi.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsqueeze%20%5BCLS%5D%20Bottleneck%20to%20Learn%20Rich%20Representations&entry.906535625=Qing%20Su%20and%20Shihao%20Ji&entry.1292438233=%20%20Distillation-based%20self-supervised%20learning%20typically%20leads%20to%20more%0Acompressed%20representations%20due%20to%20its%20radical%20clustering%20process%20and%20the%0Aimplementation%20of%20a%20sharper%20target%20distribution.%20To%20overcome%20this%20limitation%0Aand%20preserve%20more%20information%20from%20input%2C%20we%20introduce%20UDI%2C%20conceptualized%20as%0AUnsqueezed%20Distillation-based%20self-supervised%20learning%20%28SSL%29.%20UDI%20enriches%20the%0Alearned%20representation%20by%20encouraging%20multimodal%20prediction%20distilled%20from%20a%0Aconsolidated%20profile%20of%20local%20predictions%20that%20are%20derived%20via%20stratified%0Asampling.%20Our%20evaluations%20show%20that%20UDI%20not%20only%20promotes%20semantically%0Ameaningful%20representations%20at%20instance%20level%2C%20delivering%20superior%20or%0Acompetitive%20results%20to%20state-of-the-art%20SSL%20methods%20in%20image%20classification%2C%0Abut%20also%20effectively%20preserves%20the%20nuisance%20of%20input%2C%20which%20yields%20significant%0Aimprovement%20in%20dense%20prediction%20tasks%2C%20including%20object%20detection%20and%0Asegmentation.%20Additionally%2C%20UDI%20performs%20competitively%20in%20low-shot%20image%0Aclassification%2C%20improving%20the%20scalability%20of%20joint-embedding%20pipelines.%20Various%0Avisualizations%20and%20ablation%20studies%20are%20presented%20to%20further%20elucidate%20the%0Amechanisms%20behind%20UDI.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ISL-CV/udi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17671v2&entry.124074799=Read"},
{"title": "Model Composition for Multimodal Large Language Models", "author": "Chi Chen and Yiyang Du and Zheng Fang and Ziyue Wang and Fuwen Luo and Peng Li and Ming Yan and Ji Zhang and Fei Huang and Maosong Sun and Yang Liu", "abstract": "  Recent developments in Multimodal Large Language Models (MLLMs) have shown\nrapid progress, moving towards the goal of creating versatile MLLMs that\nunderstand inputs from various modalities. However, existing methods typically\nrely on joint training with paired multimodal instruction data, which is\nresource-intensive and challenging to extend to new modalities. In this paper,\nwe propose a new paradigm through the model composition of existing MLLMs to\ncreate a new model that retains the modal understanding capabilities of each\noriginal model. Our basic implementation, NaiveMC, demonstrates the\neffectiveness of this paradigm by reusing modality encoders and merging LLM\nparameters. Furthermore, we introduce DAMC to address parameter interference\nand mismatch issues during the merging process, thereby enhancing the model\nperformance. To facilitate research in this area, we propose MCUB, a benchmark\nfor assessing ability of MLLMs to understand inputs from diverse modalities.\nExperiments on this benchmark and four other multimodal understanding tasks\nshow significant improvements over baselines, proving that model composition\ncan create a versatile model capable of processing inputs from multiple\nmodalities.\n", "link": "http://arxiv.org/abs/2402.12750v2", "date": "2024-07-26", "relevancy": 1.6851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5814}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5451}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Composition%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Model%20Composition%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Chi%20Chen%20and%20Yiyang%20Du%20and%20Zheng%20Fang%20and%20Ziyue%20Wang%20and%20Fuwen%20Luo%20and%20Peng%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang%20and%20Maosong%20Sun%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Recent%20developments%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%0Arapid%20progress%2C%20moving%20towards%20the%20goal%20of%20creating%20versatile%20MLLMs%20that%0Aunderstand%20inputs%20from%20various%20modalities.%20However%2C%20existing%20methods%20typically%0Arely%20on%20joint%20training%20with%20paired%20multimodal%20instruction%20data%2C%20which%20is%0Aresource-intensive%20and%20challenging%20to%20extend%20to%20new%20modalities.%20In%20this%20paper%2C%0Awe%20propose%20a%20new%20paradigm%20through%20the%20model%20composition%20of%20existing%20MLLMs%20to%0Acreate%20a%20new%20model%20that%20retains%20the%20modal%20understanding%20capabilities%20of%20each%0Aoriginal%20model.%20Our%20basic%20implementation%2C%20NaiveMC%2C%20demonstrates%20the%0Aeffectiveness%20of%20this%20paradigm%20by%20reusing%20modality%20encoders%20and%20merging%20LLM%0Aparameters.%20Furthermore%2C%20we%20introduce%20DAMC%20to%20address%20parameter%20interference%0Aand%20mismatch%20issues%20during%20the%20merging%20process%2C%20thereby%20enhancing%20the%20model%0Aperformance.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20propose%20MCUB%2C%20a%20benchmark%0Afor%20assessing%20ability%20of%20MLLMs%20to%20understand%20inputs%20from%20diverse%20modalities.%0AExperiments%20on%20this%20benchmark%20and%20four%20other%20multimodal%20understanding%20tasks%0Ashow%20significant%20improvements%20over%20baselines%2C%20proving%20that%20model%20composition%0Acan%20create%20a%20versatile%20model%20capable%20of%20processing%20inputs%20from%20multiple%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Composition%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DChi%2520Chen%2520and%2520Yiyang%2520Du%2520and%2520Zheng%2520Fang%2520and%2520Ziyue%2520Wang%2520and%2520Fuwen%2520Luo%2520and%2520Peng%2520Li%2520and%2520Ming%2520Yan%2520and%2520Ji%2520Zhang%2520and%2520Fei%2520Huang%2520and%2520Maosong%2520Sun%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%250Arapid%2520progress%252C%2520moving%2520towards%2520the%2520goal%2520of%2520creating%2520versatile%2520MLLMs%2520that%250Aunderstand%2520inputs%2520from%2520various%2520modalities.%2520However%252C%2520existing%2520methods%2520typically%250Arely%2520on%2520joint%2520training%2520with%2520paired%2520multimodal%2520instruction%2520data%252C%2520which%2520is%250Aresource-intensive%2520and%2520challenging%2520to%2520extend%2520to%2520new%2520modalities.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520new%2520paradigm%2520through%2520the%2520model%2520composition%2520of%2520existing%2520MLLMs%2520to%250Acreate%2520a%2520new%2520model%2520that%2520retains%2520the%2520modal%2520understanding%2520capabilities%2520of%2520each%250Aoriginal%2520model.%2520Our%2520basic%2520implementation%252C%2520NaiveMC%252C%2520demonstrates%2520the%250Aeffectiveness%2520of%2520this%2520paradigm%2520by%2520reusing%2520modality%2520encoders%2520and%2520merging%2520LLM%250Aparameters.%2520Furthermore%252C%2520we%2520introduce%2520DAMC%2520to%2520address%2520parameter%2520interference%250Aand%2520mismatch%2520issues%2520during%2520the%2520merging%2520process%252C%2520thereby%2520enhancing%2520the%2520model%250Aperformance.%2520To%2520facilitate%2520research%2520in%2520this%2520area%252C%2520we%2520propose%2520MCUB%252C%2520a%2520benchmark%250Afor%2520assessing%2520ability%2520of%2520MLLMs%2520to%2520understand%2520inputs%2520from%2520diverse%2520modalities.%250AExperiments%2520on%2520this%2520benchmark%2520and%2520four%2520other%2520multimodal%2520understanding%2520tasks%250Ashow%2520significant%2520improvements%2520over%2520baselines%252C%2520proving%2520that%2520model%2520composition%250Acan%2520create%2520a%2520versatile%2520model%2520capable%2520of%2520processing%2520inputs%2520from%2520multiple%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Composition%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Chi%20Chen%20and%20Yiyang%20Du%20and%20Zheng%20Fang%20and%20Ziyue%20Wang%20and%20Fuwen%20Luo%20and%20Peng%20Li%20and%20Ming%20Yan%20and%20Ji%20Zhang%20and%20Fei%20Huang%20and%20Maosong%20Sun%20and%20Yang%20Liu&entry.1292438233=%20%20Recent%20developments%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%0Arapid%20progress%2C%20moving%20towards%20the%20goal%20of%20creating%20versatile%20MLLMs%20that%0Aunderstand%20inputs%20from%20various%20modalities.%20However%2C%20existing%20methods%20typically%0Arely%20on%20joint%20training%20with%20paired%20multimodal%20instruction%20data%2C%20which%20is%0Aresource-intensive%20and%20challenging%20to%20extend%20to%20new%20modalities.%20In%20this%20paper%2C%0Awe%20propose%20a%20new%20paradigm%20through%20the%20model%20composition%20of%20existing%20MLLMs%20to%0Acreate%20a%20new%20model%20that%20retains%20the%20modal%20understanding%20capabilities%20of%20each%0Aoriginal%20model.%20Our%20basic%20implementation%2C%20NaiveMC%2C%20demonstrates%20the%0Aeffectiveness%20of%20this%20paradigm%20by%20reusing%20modality%20encoders%20and%20merging%20LLM%0Aparameters.%20Furthermore%2C%20we%20introduce%20DAMC%20to%20address%20parameter%20interference%0Aand%20mismatch%20issues%20during%20the%20merging%20process%2C%20thereby%20enhancing%20the%20model%0Aperformance.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20propose%20MCUB%2C%20a%20benchmark%0Afor%20assessing%20ability%20of%20MLLMs%20to%20understand%20inputs%20from%20diverse%20modalities.%0AExperiments%20on%20this%20benchmark%20and%20four%20other%20multimodal%20understanding%20tasks%0Ashow%20significant%20improvements%20over%20baselines%2C%20proving%20that%20model%20composition%0Acan%20create%20a%20versatile%20model%20capable%20of%20processing%20inputs%20from%20multiple%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12750v2&entry.124074799=Read"},
{"title": "BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation", "author": "Peng Hao and Xiaobing Wang and Yingying Jiang and Hanchao Jia and Xiaoshuai Hao", "abstract": "  Scene Graph Generation (SGG) remains a challenging task due to its\ncompositional property. Previous approaches improve prediction efficiency by\nlearning in an end-to-end manner. However, these methods exhibit limited\nperformance as they assume unidirectional conditioning between entities and\npredicates, leading to insufficient information interaction. To address this\nlimitation, we propose a novel bidirectional conditioning factorization for\nSGG, introducing efficient interaction between entities and predicates.\nSpecifically, we develop an end-to-end scene graph generation model,\nBidirectional Conditioning Transformer (BCTR), to implement our factorization.\nBCTR consists of two key modules. First, the Bidirectional Conditioning\nGenerator (BCG) facilitates multi-stage interactive feature augmentation\nbetween entities and predicates, enabling mutual benefits between the two\npredictions. Second, Random Feature Alignment (RFA) regularizes the feature\nspace by distilling multi-modal knowledge from pre-trained models, enhancing\nBCTR's ability on tailed categories without relying on statistical priors. We\nconduct a series of experiments on Visual Genome and Open Image V6,\ndemonstrating that BCTR achieves state-of-the-art performance on both\nbenchmarks. The code will be available upon acceptance of the paper.\n", "link": "http://arxiv.org/abs/2407.18715v1", "date": "2024-07-26", "relevancy": 1.6732, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5669}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BCTR%3A%20Bidirectional%20Conditioning%20Transformer%20for%20Scene%20Graph%20Generation&body=Title%3A%20BCTR%3A%20Bidirectional%20Conditioning%20Transformer%20for%20Scene%20Graph%20Generation%0AAuthor%3A%20Peng%20Hao%20and%20Xiaobing%20Wang%20and%20Yingying%20Jiang%20and%20Hanchao%20Jia%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20remains%20a%20challenging%20task%20due%20to%20its%0Acompositional%20property.%20Previous%20approaches%20improve%20prediction%20efficiency%20by%0Alearning%20in%20an%20end-to-end%20manner.%20However%2C%20these%20methods%20exhibit%20limited%0Aperformance%20as%20they%20assume%20unidirectional%20conditioning%20between%20entities%20and%0Apredicates%2C%20leading%20to%20insufficient%20information%20interaction.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20bidirectional%20conditioning%20factorization%20for%0ASGG%2C%20introducing%20efficient%20interaction%20between%20entities%20and%20predicates.%0ASpecifically%2C%20we%20develop%20an%20end-to-end%20scene%20graph%20generation%20model%2C%0ABidirectional%20Conditioning%20Transformer%20%28BCTR%29%2C%20to%20implement%20our%20factorization.%0ABCTR%20consists%20of%20two%20key%20modules.%20First%2C%20the%20Bidirectional%20Conditioning%0AGenerator%20%28BCG%29%20facilitates%20multi-stage%20interactive%20feature%20augmentation%0Abetween%20entities%20and%20predicates%2C%20enabling%20mutual%20benefits%20between%20the%20two%0Apredictions.%20Second%2C%20Random%20Feature%20Alignment%20%28RFA%29%20regularizes%20the%20feature%0Aspace%20by%20distilling%20multi-modal%20knowledge%20from%20pre-trained%20models%2C%20enhancing%0ABCTR%27s%20ability%20on%20tailed%20categories%20without%20relying%20on%20statistical%20priors.%20We%0Aconduct%20a%20series%20of%20experiments%20on%20Visual%20Genome%20and%20Open%20Image%20V6%2C%0Ademonstrating%20that%20BCTR%20achieves%20state-of-the-art%20performance%20on%20both%0Abenchmarks.%20The%20code%20will%20be%20available%20upon%20acceptance%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBCTR%253A%2520Bidirectional%2520Conditioning%2520Transformer%2520for%2520Scene%2520Graph%2520Generation%26entry.906535625%3DPeng%2520Hao%2520and%2520Xiaobing%2520Wang%2520and%2520Yingying%2520Jiang%2520and%2520Hanchao%2520Jia%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Scene%2520Graph%2520Generation%2520%2528SGG%2529%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520its%250Acompositional%2520property.%2520Previous%2520approaches%2520improve%2520prediction%2520efficiency%2520by%250Alearning%2520in%2520an%2520end-to-end%2520manner.%2520However%252C%2520these%2520methods%2520exhibit%2520limited%250Aperformance%2520as%2520they%2520assume%2520unidirectional%2520conditioning%2520between%2520entities%2520and%250Apredicates%252C%2520leading%2520to%2520insufficient%2520information%2520interaction.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520bidirectional%2520conditioning%2520factorization%2520for%250ASGG%252C%2520introducing%2520efficient%2520interaction%2520between%2520entities%2520and%2520predicates.%250ASpecifically%252C%2520we%2520develop%2520an%2520end-to-end%2520scene%2520graph%2520generation%2520model%252C%250ABidirectional%2520Conditioning%2520Transformer%2520%2528BCTR%2529%252C%2520to%2520implement%2520our%2520factorization.%250ABCTR%2520consists%2520of%2520two%2520key%2520modules.%2520First%252C%2520the%2520Bidirectional%2520Conditioning%250AGenerator%2520%2528BCG%2529%2520facilitates%2520multi-stage%2520interactive%2520feature%2520augmentation%250Abetween%2520entities%2520and%2520predicates%252C%2520enabling%2520mutual%2520benefits%2520between%2520the%2520two%250Apredictions.%2520Second%252C%2520Random%2520Feature%2520Alignment%2520%2528RFA%2529%2520regularizes%2520the%2520feature%250Aspace%2520by%2520distilling%2520multi-modal%2520knowledge%2520from%2520pre-trained%2520models%252C%2520enhancing%250ABCTR%2527s%2520ability%2520on%2520tailed%2520categories%2520without%2520relying%2520on%2520statistical%2520priors.%2520We%250Aconduct%2520a%2520series%2520of%2520experiments%2520on%2520Visual%2520Genome%2520and%2520Open%2520Image%2520V6%252C%250Ademonstrating%2520that%2520BCTR%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%250Abenchmarks.%2520The%2520code%2520will%2520be%2520available%2520upon%2520acceptance%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BCTR%3A%20Bidirectional%20Conditioning%20Transformer%20for%20Scene%20Graph%20Generation&entry.906535625=Peng%20Hao%20and%20Xiaobing%20Wang%20and%20Yingying%20Jiang%20and%20Hanchao%20Jia%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20remains%20a%20challenging%20task%20due%20to%20its%0Acompositional%20property.%20Previous%20approaches%20improve%20prediction%20efficiency%20by%0Alearning%20in%20an%20end-to-end%20manner.%20However%2C%20these%20methods%20exhibit%20limited%0Aperformance%20as%20they%20assume%20unidirectional%20conditioning%20between%20entities%20and%0Apredicates%2C%20leading%20to%20insufficient%20information%20interaction.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20bidirectional%20conditioning%20factorization%20for%0ASGG%2C%20introducing%20efficient%20interaction%20between%20entities%20and%20predicates.%0ASpecifically%2C%20we%20develop%20an%20end-to-end%20scene%20graph%20generation%20model%2C%0ABidirectional%20Conditioning%20Transformer%20%28BCTR%29%2C%20to%20implement%20our%20factorization.%0ABCTR%20consists%20of%20two%20key%20modules.%20First%2C%20the%20Bidirectional%20Conditioning%0AGenerator%20%28BCG%29%20facilitates%20multi-stage%20interactive%20feature%20augmentation%0Abetween%20entities%20and%20predicates%2C%20enabling%20mutual%20benefits%20between%20the%20two%0Apredictions.%20Second%2C%20Random%20Feature%20Alignment%20%28RFA%29%20regularizes%20the%20feature%0Aspace%20by%20distilling%20multi-modal%20knowledge%20from%20pre-trained%20models%2C%20enhancing%0ABCTR%27s%20ability%20on%20tailed%20categories%20without%20relying%20on%20statistical%20priors.%20We%0Aconduct%20a%20series%20of%20experiments%20on%20Visual%20Genome%20and%20Open%20Image%20V6%2C%0Ademonstrating%20that%20BCTR%20achieves%20state-of-the-art%20performance%20on%20both%0Abenchmarks.%20The%20code%20will%20be%20available%20upon%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18715v1&entry.124074799=Read"},
{"title": "AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination\n  Datasets based Recommendation System", "author": "Shakshi Sharma and Anwitaman Datta and Rajesh Sharma", "abstract": "  Misinformation has emerged as a major societal threat in recent years in\ngeneral; specifically in the context of the COVID-19 pandemic, it has wrecked\nhavoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable\nsolutions for combating misinformation are the need of the hour. This work\nexplored how existing information obtained from social media and augmented with\nmore curated fact checked data repositories can be harnessed to facilitate\nautomated rebuttal of misinformation at scale. While the ideas herein can be\ngeneralized and reapplied in the broader context of misinformation mitigation\nusing a multitude of information sources and catering to the spectrum of social\nmedia platforms, this work serves as a proof of concept, and as such, it is\nconfined in its scope to only rebuttal of tweets, and in the specific context\nof misinformation regarding COVID-19. It leverages two publicly available\ndatasets, viz. FaCov (fact-checked articles) and misleading (social media\nTwitter) data on COVID-19 Vaccination.\n", "link": "http://arxiv.org/abs/2310.19834v2", "date": "2024-07-26", "relevancy": 1.6635, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4111}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMIR%3A%20Automated%20MisInformation%20Rebuttal%20--%20A%20COVID-19%20Vaccination%0A%20%20Datasets%20based%20Recommendation%20System&body=Title%3A%20AMIR%3A%20Automated%20MisInformation%20Rebuttal%20--%20A%20COVID-19%20Vaccination%0A%20%20Datasets%20based%20Recommendation%20System%0AAuthor%3A%20Shakshi%20Sharma%20and%20Anwitaman%20Datta%20and%20Rajesh%20Sharma%0AAbstract%3A%20%20%20Misinformation%20has%20emerged%20as%20a%20major%20societal%20threat%20in%20recent%20years%20in%0Ageneral%3B%20specifically%20in%20the%20context%20of%20the%20COVID-19%20pandemic%2C%20it%20has%20wrecked%0Ahavoc%2C%20for%20instance%2C%20by%20fuelling%20vaccine%20hesitancy.%20Cost-effective%2C%20scalable%0Asolutions%20for%20combating%20misinformation%20are%20the%20need%20of%20the%20hour.%20This%20work%0Aexplored%20how%20existing%20information%20obtained%20from%20social%20media%20and%20augmented%20with%0Amore%20curated%20fact%20checked%20data%20repositories%20can%20be%20harnessed%20to%20facilitate%0Aautomated%20rebuttal%20of%20misinformation%20at%20scale.%20While%20the%20ideas%20herein%20can%20be%0Ageneralized%20and%20reapplied%20in%20the%20broader%20context%20of%20misinformation%20mitigation%0Ausing%20a%20multitude%20of%20information%20sources%20and%20catering%20to%20the%20spectrum%20of%20social%0Amedia%20platforms%2C%20this%20work%20serves%20as%20a%20proof%20of%20concept%2C%20and%20as%20such%2C%20it%20is%0Aconfined%20in%20its%20scope%20to%20only%20rebuttal%20of%20tweets%2C%20and%20in%20the%20specific%20context%0Aof%20misinformation%20regarding%20COVID-19.%20It%20leverages%20two%20publicly%20available%0Adatasets%2C%20viz.%20FaCov%20%28fact-checked%20articles%29%20and%20misleading%20%28social%20media%0ATwitter%29%20data%20on%20COVID-19%20Vaccination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMIR%253A%2520Automated%2520MisInformation%2520Rebuttal%2520--%2520A%2520COVID-19%2520Vaccination%250A%2520%2520Datasets%2520based%2520Recommendation%2520System%26entry.906535625%3DShakshi%2520Sharma%2520and%2520Anwitaman%2520Datta%2520and%2520Rajesh%2520Sharma%26entry.1292438233%3D%2520%2520Misinformation%2520has%2520emerged%2520as%2520a%2520major%2520societal%2520threat%2520in%2520recent%2520years%2520in%250Ageneral%253B%2520specifically%2520in%2520the%2520context%2520of%2520the%2520COVID-19%2520pandemic%252C%2520it%2520has%2520wrecked%250Ahavoc%252C%2520for%2520instance%252C%2520by%2520fuelling%2520vaccine%2520hesitancy.%2520Cost-effective%252C%2520scalable%250Asolutions%2520for%2520combating%2520misinformation%2520are%2520the%2520need%2520of%2520the%2520hour.%2520This%2520work%250Aexplored%2520how%2520existing%2520information%2520obtained%2520from%2520social%2520media%2520and%2520augmented%2520with%250Amore%2520curated%2520fact%2520checked%2520data%2520repositories%2520can%2520be%2520harnessed%2520to%2520facilitate%250Aautomated%2520rebuttal%2520of%2520misinformation%2520at%2520scale.%2520While%2520the%2520ideas%2520herein%2520can%2520be%250Ageneralized%2520and%2520reapplied%2520in%2520the%2520broader%2520context%2520of%2520misinformation%2520mitigation%250Ausing%2520a%2520multitude%2520of%2520information%2520sources%2520and%2520catering%2520to%2520the%2520spectrum%2520of%2520social%250Amedia%2520platforms%252C%2520this%2520work%2520serves%2520as%2520a%2520proof%2520of%2520concept%252C%2520and%2520as%2520such%252C%2520it%2520is%250Aconfined%2520in%2520its%2520scope%2520to%2520only%2520rebuttal%2520of%2520tweets%252C%2520and%2520in%2520the%2520specific%2520context%250Aof%2520misinformation%2520regarding%2520COVID-19.%2520It%2520leverages%2520two%2520publicly%2520available%250Adatasets%252C%2520viz.%2520FaCov%2520%2528fact-checked%2520articles%2529%2520and%2520misleading%2520%2528social%2520media%250ATwitter%2529%2520data%2520on%2520COVID-19%2520Vaccination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMIR%3A%20Automated%20MisInformation%20Rebuttal%20--%20A%20COVID-19%20Vaccination%0A%20%20Datasets%20based%20Recommendation%20System&entry.906535625=Shakshi%20Sharma%20and%20Anwitaman%20Datta%20and%20Rajesh%20Sharma&entry.1292438233=%20%20Misinformation%20has%20emerged%20as%20a%20major%20societal%20threat%20in%20recent%20years%20in%0Ageneral%3B%20specifically%20in%20the%20context%20of%20the%20COVID-19%20pandemic%2C%20it%20has%20wrecked%0Ahavoc%2C%20for%20instance%2C%20by%20fuelling%20vaccine%20hesitancy.%20Cost-effective%2C%20scalable%0Asolutions%20for%20combating%20misinformation%20are%20the%20need%20of%20the%20hour.%20This%20work%0Aexplored%20how%20existing%20information%20obtained%20from%20social%20media%20and%20augmented%20with%0Amore%20curated%20fact%20checked%20data%20repositories%20can%20be%20harnessed%20to%20facilitate%0Aautomated%20rebuttal%20of%20misinformation%20at%20scale.%20While%20the%20ideas%20herein%20can%20be%0Ageneralized%20and%20reapplied%20in%20the%20broader%20context%20of%20misinformation%20mitigation%0Ausing%20a%20multitude%20of%20information%20sources%20and%20catering%20to%20the%20spectrum%20of%20social%0Amedia%20platforms%2C%20this%20work%20serves%20as%20a%20proof%20of%20concept%2C%20and%20as%20such%2C%20it%20is%0Aconfined%20in%20its%20scope%20to%20only%20rebuttal%20of%20tweets%2C%20and%20in%20the%20specific%20context%0Aof%20misinformation%20regarding%20COVID-19.%20It%20leverages%20two%20publicly%20available%0Adatasets%2C%20viz.%20FaCov%20%28fact-checked%20articles%29%20and%20misleading%20%28social%20media%0ATwitter%29%20data%20on%20COVID-19%20Vaccination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19834v2&entry.124074799=Read"},
{"title": "Recursive Introspection: Teaching Language Model Agents How to\n  Self-Improve", "author": "Yuxiao Qu and Tianjun Zhang and Naman Garg and Aviral Kumar", "abstract": "  A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.\n", "link": "http://arxiv.org/abs/2407.18219v2", "date": "2024-07-26", "relevancy": 1.5378, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5197}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Introspection%3A%20Teaching%20Language%20Model%20Agents%20How%20to%0A%20%20Self-Improve&body=Title%3A%20Recursive%20Introspection%3A%20Teaching%20Language%20Model%20Agents%20How%20to%0A%20%20Self-Improve%0AAuthor%3A%20Yuxiao%20Qu%20and%20Tianjun%20Zhang%20and%20Naman%20Garg%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20A%20central%20piece%20in%20enabling%20intelligent%20agentic%20behavior%20in%20foundation%20models%0Ais%20to%20make%20them%20capable%20of%20introspecting%20upon%20their%20behavior%2C%20reasoning%2C%20and%0Acorrecting%20their%20mistakes%20as%20more%20computation%20or%20interaction%20is%20available.%20Even%0Athe%20strongest%20proprietary%20large%20language%20models%20%28LLMs%29%20do%20not%20quite%20exhibit%20the%0Aability%20of%20continually%20improving%20their%20responses%20sequentially%2C%20even%20in%0Ascenarios%20where%20they%20are%20explicitly%20told%20that%20they%20are%20making%20a%20mistake.%20In%0Athis%20paper%2C%20we%20develop%20RISE%3A%20Recursive%20IntroSpEction%2C%20an%20approach%20for%0Afine-tuning%20LLMs%20to%20introduce%20this%20capability%2C%20despite%20prior%20work%20hypothesizing%0Athat%20this%20capability%20may%20not%20be%20possible%20to%20attain.%20Our%20approach%20prescribes%20an%0Aiterative%20fine-tuning%20procedure%2C%20which%20attempts%20to%20teach%20the%20model%20how%20to%20alter%0Aits%20response%20after%20having%20executed%20previously%20unsuccessful%20attempts%20to%20solve%20a%0Ahard%20test-time%20problem%2C%20with%20optionally%20additional%20environment%20feedback.%20RISE%0Aposes%20fine-tuning%20for%20a%20single-turn%20prompt%20as%20solving%20a%20multi-turn%20Markov%0Adecision%20process%20%28MDP%29%2C%20where%20the%20initial%20state%20is%20the%20prompt.%20Inspired%20by%0Aprinciples%20in%20online%20imitation%20learning%20and%20reinforcement%20learning%2C%20we%20propose%0Astrategies%20for%20multi-turn%20data%20collection%20and%20training%20so%20as%20to%20imbue%20an%20LLM%0Awith%20the%20capability%20to%20recursively%20detect%20and%20correct%20its%20previous%20mistakes%20in%0Asubsequent%20iterations.%20Our%20experiments%20show%20that%20RISE%20enables%20Llama2%2C%20Llama3%2C%0Aand%20Mistral%20models%20to%20improve%20themselves%20with%20more%20turns%20on%20math%20reasoning%0Atasks%2C%20outperforming%20several%20single-turn%20strategies%20given%20an%20equal%20amount%20of%0Ainference-time%20computation.%20We%20also%20find%20that%20RISE%20scales%20well%2C%20often%20attaining%0Alarger%20benefits%20with%20more%20capable%20models.%20Our%20analysis%20shows%20that%20RISE%20makes%0Ameaningful%20improvements%20to%20responses%20to%20arrive%20at%20the%20correct%20solution%20for%0Achallenging%20prompts%2C%20without%20disrupting%20one-turn%20abilities%20as%20a%20result%20of%0Aexpressing%20more%20complex%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Introspection%253A%2520Teaching%2520Language%2520Model%2520Agents%2520How%2520to%250A%2520%2520Self-Improve%26entry.906535625%3DYuxiao%2520Qu%2520and%2520Tianjun%2520Zhang%2520and%2520Naman%2520Garg%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520A%2520central%2520piece%2520in%2520enabling%2520intelligent%2520agentic%2520behavior%2520in%2520foundation%2520models%250Ais%2520to%2520make%2520them%2520capable%2520of%2520introspecting%2520upon%2520their%2520behavior%252C%2520reasoning%252C%2520and%250Acorrecting%2520their%2520mistakes%2520as%2520more%2520computation%2520or%2520interaction%2520is%2520available.%2520Even%250Athe%2520strongest%2520proprietary%2520large%2520language%2520models%2520%2528LLMs%2529%2520do%2520not%2520quite%2520exhibit%2520the%250Aability%2520of%2520continually%2520improving%2520their%2520responses%2520sequentially%252C%2520even%2520in%250Ascenarios%2520where%2520they%2520are%2520explicitly%2520told%2520that%2520they%2520are%2520making%2520a%2520mistake.%2520In%250Athis%2520paper%252C%2520we%2520develop%2520RISE%253A%2520Recursive%2520IntroSpEction%252C%2520an%2520approach%2520for%250Afine-tuning%2520LLMs%2520to%2520introduce%2520this%2520capability%252C%2520despite%2520prior%2520work%2520hypothesizing%250Athat%2520this%2520capability%2520may%2520not%2520be%2520possible%2520to%2520attain.%2520Our%2520approach%2520prescribes%2520an%250Aiterative%2520fine-tuning%2520procedure%252C%2520which%2520attempts%2520to%2520teach%2520the%2520model%2520how%2520to%2520alter%250Aits%2520response%2520after%2520having%2520executed%2520previously%2520unsuccessful%2520attempts%2520to%2520solve%2520a%250Ahard%2520test-time%2520problem%252C%2520with%2520optionally%2520additional%2520environment%2520feedback.%2520RISE%250Aposes%2520fine-tuning%2520for%2520a%2520single-turn%2520prompt%2520as%2520solving%2520a%2520multi-turn%2520Markov%250Adecision%2520process%2520%2528MDP%2529%252C%2520where%2520the%2520initial%2520state%2520is%2520the%2520prompt.%2520Inspired%2520by%250Aprinciples%2520in%2520online%2520imitation%2520learning%2520and%2520reinforcement%2520learning%252C%2520we%2520propose%250Astrategies%2520for%2520multi-turn%2520data%2520collection%2520and%2520training%2520so%2520as%2520to%2520imbue%2520an%2520LLM%250Awith%2520the%2520capability%2520to%2520recursively%2520detect%2520and%2520correct%2520its%2520previous%2520mistakes%2520in%250Asubsequent%2520iterations.%2520Our%2520experiments%2520show%2520that%2520RISE%2520enables%2520Llama2%252C%2520Llama3%252C%250Aand%2520Mistral%2520models%2520to%2520improve%2520themselves%2520with%2520more%2520turns%2520on%2520math%2520reasoning%250Atasks%252C%2520outperforming%2520several%2520single-turn%2520strategies%2520given%2520an%2520equal%2520amount%2520of%250Ainference-time%2520computation.%2520We%2520also%2520find%2520that%2520RISE%2520scales%2520well%252C%2520often%2520attaining%250Alarger%2520benefits%2520with%2520more%2520capable%2520models.%2520Our%2520analysis%2520shows%2520that%2520RISE%2520makes%250Ameaningful%2520improvements%2520to%2520responses%2520to%2520arrive%2520at%2520the%2520correct%2520solution%2520for%250Achallenging%2520prompts%252C%2520without%2520disrupting%2520one-turn%2520abilities%2520as%2520a%2520result%2520of%250Aexpressing%2520more%2520complex%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Introspection%3A%20Teaching%20Language%20Model%20Agents%20How%20to%0A%20%20Self-Improve&entry.906535625=Yuxiao%20Qu%20and%20Tianjun%20Zhang%20and%20Naman%20Garg%20and%20Aviral%20Kumar&entry.1292438233=%20%20A%20central%20piece%20in%20enabling%20intelligent%20agentic%20behavior%20in%20foundation%20models%0Ais%20to%20make%20them%20capable%20of%20introspecting%20upon%20their%20behavior%2C%20reasoning%2C%20and%0Acorrecting%20their%20mistakes%20as%20more%20computation%20or%20interaction%20is%20available.%20Even%0Athe%20strongest%20proprietary%20large%20language%20models%20%28LLMs%29%20do%20not%20quite%20exhibit%20the%0Aability%20of%20continually%20improving%20their%20responses%20sequentially%2C%20even%20in%0Ascenarios%20where%20they%20are%20explicitly%20told%20that%20they%20are%20making%20a%20mistake.%20In%0Athis%20paper%2C%20we%20develop%20RISE%3A%20Recursive%20IntroSpEction%2C%20an%20approach%20for%0Afine-tuning%20LLMs%20to%20introduce%20this%20capability%2C%20despite%20prior%20work%20hypothesizing%0Athat%20this%20capability%20may%20not%20be%20possible%20to%20attain.%20Our%20approach%20prescribes%20an%0Aiterative%20fine-tuning%20procedure%2C%20which%20attempts%20to%20teach%20the%20model%20how%20to%20alter%0Aits%20response%20after%20having%20executed%20previously%20unsuccessful%20attempts%20to%20solve%20a%0Ahard%20test-time%20problem%2C%20with%20optionally%20additional%20environment%20feedback.%20RISE%0Aposes%20fine-tuning%20for%20a%20single-turn%20prompt%20as%20solving%20a%20multi-turn%20Markov%0Adecision%20process%20%28MDP%29%2C%20where%20the%20initial%20state%20is%20the%20prompt.%20Inspired%20by%0Aprinciples%20in%20online%20imitation%20learning%20and%20reinforcement%20learning%2C%20we%20propose%0Astrategies%20for%20multi-turn%20data%20collection%20and%20training%20so%20as%20to%20imbue%20an%20LLM%0Awith%20the%20capability%20to%20recursively%20detect%20and%20correct%20its%20previous%20mistakes%20in%0Asubsequent%20iterations.%20Our%20experiments%20show%20that%20RISE%20enables%20Llama2%2C%20Llama3%2C%0Aand%20Mistral%20models%20to%20improve%20themselves%20with%20more%20turns%20on%20math%20reasoning%0Atasks%2C%20outperforming%20several%20single-turn%20strategies%20given%20an%20equal%20amount%20of%0Ainference-time%20computation.%20We%20also%20find%20that%20RISE%20scales%20well%2C%20often%20attaining%0Alarger%20benefits%20with%20more%20capable%20models.%20Our%20analysis%20shows%20that%20RISE%20makes%0Ameaningful%20improvements%20to%20responses%20to%20arrive%20at%20the%20correct%20solution%20for%0Achallenging%20prompts%2C%20without%20disrupting%20one-turn%20abilities%20as%20a%20result%20of%0Aexpressing%20more%20complex%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18219v2&entry.124074799=Read"},
{"title": "Collaborative Evolving Strategy for Automatic Data-Centric Development", "author": "Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian", "abstract": "  Artificial Intelligence (AI) significantly influences many fields, largely\nthanks to the vast amounts of high-quality data for machine learning models.\nThe emphasis is now on a data-centric AI strategy, prioritizing data\ndevelopment over model design progress. Automating this process is crucial. In\nthis paper, we serve as the first work to introduce the automatic data-centric\ndevelopment (AD^2) task and outline its core challenges, which require\ndomain-experts-like task scheduling and implementation capability, largely\nunexplored by previous work.\n  By leveraging the strong complex problem-solving capabilities of large\nlanguage models (LLMs), we propose an LLM-based autonomous agent, equipped with\na strategy named Collaborative Knowledge-STudying-Enhanced Evolution by\nRetrieval (Co-STEER), to simultaneously address all the challenges.\nSpecifically, our proposed Co-STEER agent enriches its domain knowledge through\nour proposed evolving strategy and develops both its scheduling and\nimplementation skills by accumulating and retrieving domain-specific practical\nexperience. With an improved schedule, the capability for implementation\naccelerates. Simultaneously, as implementation feedback becomes more thorough,\nthe scheduling accuracy increases. These two capabilities evolve together\nthrough practical feedback, enabling a collaborative evolution process.\n  Extensive experimental results demonstrate that our Co-STEER agent breaks new\nground in AD^2 research, possesses strong evolvable schedule and implementation\nability, and demonstrates the significant effectiveness of its components. Our\nCo-STEER paves the way for AD^2 advancements.\n", "link": "http://arxiv.org/abs/2407.18690v1", "date": "2024-07-26", "relevancy": 1.5458, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5314}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Evolving%20Strategy%20for%20Automatic%20Data-Centric%20Development&body=Title%3A%20Collaborative%20Evolving%20Strategy%20for%20Automatic%20Data-Centric%20Development%0AAuthor%3A%20Xu%20Yang%20and%20Haotian%20Chen%20and%20Wenjun%20Feng%20and%20Haoxue%20Wang%20and%20Zeqi%20Ye%20and%20Xinjie%20Shen%20and%20Xiao%20Yang%20and%20Shizhao%20Sun%20and%20Weiqing%20Liu%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20significantly%20influences%20many%20fields%2C%20largely%0Athanks%20to%20the%20vast%20amounts%20of%20high-quality%20data%20for%20machine%20learning%20models.%0AThe%20emphasis%20is%20now%20on%20a%20data-centric%20AI%20strategy%2C%20prioritizing%20data%0Adevelopment%20over%20model%20design%20progress.%20Automating%20this%20process%20is%20crucial.%20In%0Athis%20paper%2C%20we%20serve%20as%20the%20first%20work%20to%20introduce%20the%20automatic%20data-centric%0Adevelopment%20%28AD%5E2%29%20task%20and%20outline%20its%20core%20challenges%2C%20which%20require%0Adomain-experts-like%20task%20scheduling%20and%20implementation%20capability%2C%20largely%0Aunexplored%20by%20previous%20work.%0A%20%20By%20leveraging%20the%20strong%20complex%20problem-solving%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20we%20propose%20an%20LLM-based%20autonomous%20agent%2C%20equipped%20with%0Aa%20strategy%20named%20Collaborative%20Knowledge-STudying-Enhanced%20Evolution%20by%0ARetrieval%20%28Co-STEER%29%2C%20to%20simultaneously%20address%20all%20the%20challenges.%0ASpecifically%2C%20our%20proposed%20Co-STEER%20agent%20enriches%20its%20domain%20knowledge%20through%0Aour%20proposed%20evolving%20strategy%20and%20develops%20both%20its%20scheduling%20and%0Aimplementation%20skills%20by%20accumulating%20and%20retrieving%20domain-specific%20practical%0Aexperience.%20With%20an%20improved%20schedule%2C%20the%20capability%20for%20implementation%0Aaccelerates.%20Simultaneously%2C%20as%20implementation%20feedback%20becomes%20more%20thorough%2C%0Athe%20scheduling%20accuracy%20increases.%20These%20two%20capabilities%20evolve%20together%0Athrough%20practical%20feedback%2C%20enabling%20a%20collaborative%20evolution%20process.%0A%20%20Extensive%20experimental%20results%20demonstrate%20that%20our%20Co-STEER%20agent%20breaks%20new%0Aground%20in%20AD%5E2%20research%2C%20possesses%20strong%20evolvable%20schedule%20and%20implementation%0Aability%2C%20and%20demonstrates%20the%20significant%20effectiveness%20of%20its%20components.%20Our%0ACo-STEER%20paves%20the%20way%20for%20AD%5E2%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Evolving%2520Strategy%2520for%2520Automatic%2520Data-Centric%2520Development%26entry.906535625%3DXu%2520Yang%2520and%2520Haotian%2520Chen%2520and%2520Wenjun%2520Feng%2520and%2520Haoxue%2520Wang%2520and%2520Zeqi%2520Ye%2520and%2520Xinjie%2520Shen%2520and%2520Xiao%2520Yang%2520and%2520Shizhao%2520Sun%2520and%2520Weiqing%2520Liu%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520significantly%2520influences%2520many%2520fields%252C%2520largely%250Athanks%2520to%2520the%2520vast%2520amounts%2520of%2520high-quality%2520data%2520for%2520machine%2520learning%2520models.%250AThe%2520emphasis%2520is%2520now%2520on%2520a%2520data-centric%2520AI%2520strategy%252C%2520prioritizing%2520data%250Adevelopment%2520over%2520model%2520design%2520progress.%2520Automating%2520this%2520process%2520is%2520crucial.%2520In%250Athis%2520paper%252C%2520we%2520serve%2520as%2520the%2520first%2520work%2520to%2520introduce%2520the%2520automatic%2520data-centric%250Adevelopment%2520%2528AD%255E2%2529%2520task%2520and%2520outline%2520its%2520core%2520challenges%252C%2520which%2520require%250Adomain-experts-like%2520task%2520scheduling%2520and%2520implementation%2520capability%252C%2520largely%250Aunexplored%2520by%2520previous%2520work.%250A%2520%2520By%2520leveraging%2520the%2520strong%2520complex%2520problem-solving%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520we%2520propose%2520an%2520LLM-based%2520autonomous%2520agent%252C%2520equipped%2520with%250Aa%2520strategy%2520named%2520Collaborative%2520Knowledge-STudying-Enhanced%2520Evolution%2520by%250ARetrieval%2520%2528Co-STEER%2529%252C%2520to%2520simultaneously%2520address%2520all%2520the%2520challenges.%250ASpecifically%252C%2520our%2520proposed%2520Co-STEER%2520agent%2520enriches%2520its%2520domain%2520knowledge%2520through%250Aour%2520proposed%2520evolving%2520strategy%2520and%2520develops%2520both%2520its%2520scheduling%2520and%250Aimplementation%2520skills%2520by%2520accumulating%2520and%2520retrieving%2520domain-specific%2520practical%250Aexperience.%2520With%2520an%2520improved%2520schedule%252C%2520the%2520capability%2520for%2520implementation%250Aaccelerates.%2520Simultaneously%252C%2520as%2520implementation%2520feedback%2520becomes%2520more%2520thorough%252C%250Athe%2520scheduling%2520accuracy%2520increases.%2520These%2520two%2520capabilities%2520evolve%2520together%250Athrough%2520practical%2520feedback%252C%2520enabling%2520a%2520collaborative%2520evolution%2520process.%250A%2520%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520Co-STEER%2520agent%2520breaks%2520new%250Aground%2520in%2520AD%255E2%2520research%252C%2520possesses%2520strong%2520evolvable%2520schedule%2520and%2520implementation%250Aability%252C%2520and%2520demonstrates%2520the%2520significant%2520effectiveness%2520of%2520its%2520components.%2520Our%250ACo-STEER%2520paves%2520the%2520way%2520for%2520AD%255E2%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Evolving%20Strategy%20for%20Automatic%20Data-Centric%20Development&entry.906535625=Xu%20Yang%20and%20Haotian%20Chen%20and%20Wenjun%20Feng%20and%20Haoxue%20Wang%20and%20Zeqi%20Ye%20and%20Xinjie%20Shen%20and%20Xiao%20Yang%20and%20Shizhao%20Sun%20and%20Weiqing%20Liu%20and%20Jiang%20Bian&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20significantly%20influences%20many%20fields%2C%20largely%0Athanks%20to%20the%20vast%20amounts%20of%20high-quality%20data%20for%20machine%20learning%20models.%0AThe%20emphasis%20is%20now%20on%20a%20data-centric%20AI%20strategy%2C%20prioritizing%20data%0Adevelopment%20over%20model%20design%20progress.%20Automating%20this%20process%20is%20crucial.%20In%0Athis%20paper%2C%20we%20serve%20as%20the%20first%20work%20to%20introduce%20the%20automatic%20data-centric%0Adevelopment%20%28AD%5E2%29%20task%20and%20outline%20its%20core%20challenges%2C%20which%20require%0Adomain-experts-like%20task%20scheduling%20and%20implementation%20capability%2C%20largely%0Aunexplored%20by%20previous%20work.%0A%20%20By%20leveraging%20the%20strong%20complex%20problem-solving%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20we%20propose%20an%20LLM-based%20autonomous%20agent%2C%20equipped%20with%0Aa%20strategy%20named%20Collaborative%20Knowledge-STudying-Enhanced%20Evolution%20by%0ARetrieval%20%28Co-STEER%29%2C%20to%20simultaneously%20address%20all%20the%20challenges.%0ASpecifically%2C%20our%20proposed%20Co-STEER%20agent%20enriches%20its%20domain%20knowledge%20through%0Aour%20proposed%20evolving%20strategy%20and%20develops%20both%20its%20scheduling%20and%0Aimplementation%20skills%20by%20accumulating%20and%20retrieving%20domain-specific%20practical%0Aexperience.%20With%20an%20improved%20schedule%2C%20the%20capability%20for%20implementation%0Aaccelerates.%20Simultaneously%2C%20as%20implementation%20feedback%20becomes%20more%20thorough%2C%0Athe%20scheduling%20accuracy%20increases.%20These%20two%20capabilities%20evolve%20together%0Athrough%20practical%20feedback%2C%20enabling%20a%20collaborative%20evolution%20process.%0A%20%20Extensive%20experimental%20results%20demonstrate%20that%20our%20Co-STEER%20agent%20breaks%20new%0Aground%20in%20AD%5E2%20research%2C%20possesses%20strong%20evolvable%20schedule%20and%20implementation%0Aability%2C%20and%20demonstrates%20the%20significant%20effectiveness%20of%20its%20components.%20Our%0ACo-STEER%20paves%20the%20way%20for%20AD%5E2%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18690v1&entry.124074799=Read"},
{"title": "LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text\n  Translation", "author": "Bunyamin Keles and Murat Gunay and Serdar I. Caglar", "abstract": "  Machine translation is indispensable in healthcare for enabling the global\ndissemination of medical knowledge across languages. However, complex medical\nterminology poses unique challenges to achieving adequate translation quality\nand accuracy. This study introduces a novel \"LLMs-in-the-loop\" approach to\ndevelop supervised neural machine translation models optimized specifically for\nmedical texts. While large language models (LLMs) have demonstrated powerful\ncapabilities, this research shows that small, specialized models trained on\nhigh-quality in-domain (mostly synthetic) data can outperform even vastly\nlarger LLMs.\n  Custom parallel corpora in six languages were compiled from scientific\narticles, synthetically generated clinical documents, and medical texts. Our\nLLMs-in-the-loop methodology employs synthetic data generation, rigorous\nevaluation, and agent orchestration to enhance performance. We developed small\nmedical translation models using the MarianMT base model. We introduce a new\nmedical translation test dataset to standardize evaluation in this domain.\nAssessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our\nMarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo.\n  Results demonstrate that our LLMs-in-the-loop approach, combined with\nfine-tuning high-quality, domain-specific data, enables specialized models to\noutperform general-purpose and some larger systems. This research, part of a\nbroader series on expert small models, paves the way for future\nhealthcare-related AI developments, including deidentification and bio-medical\nentity extraction models. Our study underscores the potential of tailored\nneural translation models and the LLMs-in-the-loop methodology to advance the\nfield through improved data generation, evaluation, agent, and modeling\ntechniques.\n", "link": "http://arxiv.org/abs/2407.12126v2", "date": "2024-07-26", "relevancy": 1.5176, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs-in-the-loop%20Part-1%3A%20Expert%20Small%20AI%20Models%20for%20Bio-Medical%20Text%0A%20%20Translation&body=Title%3A%20LLMs-in-the-loop%20Part-1%3A%20Expert%20Small%20AI%20Models%20for%20Bio-Medical%20Text%0A%20%20Translation%0AAuthor%3A%20Bunyamin%20Keles%20and%20Murat%20Gunay%20and%20Serdar%20I.%20Caglar%0AAbstract%3A%20%20%20Machine%20translation%20is%20indispensable%20in%20healthcare%20for%20enabling%20the%20global%0Adissemination%20of%20medical%20knowledge%20across%20languages.%20However%2C%20complex%20medical%0Aterminology%20poses%20unique%20challenges%20to%20achieving%20adequate%20translation%20quality%0Aand%20accuracy.%20This%20study%20introduces%20a%20novel%20%22LLMs-in-the-loop%22%20approach%20to%0Adevelop%20supervised%20neural%20machine%20translation%20models%20optimized%20specifically%20for%0Amedical%20texts.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20powerful%0Acapabilities%2C%20this%20research%20shows%20that%20small%2C%20specialized%20models%20trained%20on%0Ahigh-quality%20in-domain%20%28mostly%20synthetic%29%20data%20can%20outperform%20even%20vastly%0Alarger%20LLMs.%0A%20%20Custom%20parallel%20corpora%20in%20six%20languages%20were%20compiled%20from%20scientific%0Aarticles%2C%20synthetically%20generated%20clinical%20documents%2C%20and%20medical%20texts.%20Our%0ALLMs-in-the-loop%20methodology%20employs%20synthetic%20data%20generation%2C%20rigorous%0Aevaluation%2C%20and%20agent%20orchestration%20to%20enhance%20performance.%20We%20developed%20small%0Amedical%20translation%20models%20using%20the%20MarianMT%20base%20model.%20We%20introduce%20a%20new%0Amedical%20translation%20test%20dataset%20to%20standardize%20evaluation%20in%20this%20domain.%0AAssessed%20using%20BLEU%2C%20METEOR%2C%20ROUGE%2C%20and%20BERT%20scores%20on%20this%20test%20set%2C%20our%0AMarianMT-based%20models%20outperform%20Google%20Translate%2C%20DeepL%2C%20and%20GPT-4-Turbo.%0A%20%20Results%20demonstrate%20that%20our%20LLMs-in-the-loop%20approach%2C%20combined%20with%0Afine-tuning%20high-quality%2C%20domain-specific%20data%2C%20enables%20specialized%20models%20to%0Aoutperform%20general-purpose%20and%20some%20larger%20systems.%20This%20research%2C%20part%20of%20a%0Abroader%20series%20on%20expert%20small%20models%2C%20paves%20the%20way%20for%20future%0Ahealthcare-related%20AI%20developments%2C%20including%20deidentification%20and%20bio-medical%0Aentity%20extraction%20models.%20Our%20study%20underscores%20the%20potential%20of%20tailored%0Aneural%20translation%20models%20and%20the%20LLMs-in-the-loop%20methodology%20to%20advance%20the%0Afield%20through%20improved%20data%20generation%2C%20evaluation%2C%20agent%2C%20and%20modeling%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs-in-the-loop%2520Part-1%253A%2520Expert%2520Small%2520AI%2520Models%2520for%2520Bio-Medical%2520Text%250A%2520%2520Translation%26entry.906535625%3DBunyamin%2520Keles%2520and%2520Murat%2520Gunay%2520and%2520Serdar%2520I.%2520Caglar%26entry.1292438233%3D%2520%2520Machine%2520translation%2520is%2520indispensable%2520in%2520healthcare%2520for%2520enabling%2520the%2520global%250Adissemination%2520of%2520medical%2520knowledge%2520across%2520languages.%2520However%252C%2520complex%2520medical%250Aterminology%2520poses%2520unique%2520challenges%2520to%2520achieving%2520adequate%2520translation%2520quality%250Aand%2520accuracy.%2520This%2520study%2520introduces%2520a%2520novel%2520%2522LLMs-in-the-loop%2522%2520approach%2520to%250Adevelop%2520supervised%2520neural%2520machine%2520translation%2520models%2520optimized%2520specifically%2520for%250Amedical%2520texts.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520powerful%250Acapabilities%252C%2520this%2520research%2520shows%2520that%2520small%252C%2520specialized%2520models%2520trained%2520on%250Ahigh-quality%2520in-domain%2520%2528mostly%2520synthetic%2529%2520data%2520can%2520outperform%2520even%2520vastly%250Alarger%2520LLMs.%250A%2520%2520Custom%2520parallel%2520corpora%2520in%2520six%2520languages%2520were%2520compiled%2520from%2520scientific%250Aarticles%252C%2520synthetically%2520generated%2520clinical%2520documents%252C%2520and%2520medical%2520texts.%2520Our%250ALLMs-in-the-loop%2520methodology%2520employs%2520synthetic%2520data%2520generation%252C%2520rigorous%250Aevaluation%252C%2520and%2520agent%2520orchestration%2520to%2520enhance%2520performance.%2520We%2520developed%2520small%250Amedical%2520translation%2520models%2520using%2520the%2520MarianMT%2520base%2520model.%2520We%2520introduce%2520a%2520new%250Amedical%2520translation%2520test%2520dataset%2520to%2520standardize%2520evaluation%2520in%2520this%2520domain.%250AAssessed%2520using%2520BLEU%252C%2520METEOR%252C%2520ROUGE%252C%2520and%2520BERT%2520scores%2520on%2520this%2520test%2520set%252C%2520our%250AMarianMT-based%2520models%2520outperform%2520Google%2520Translate%252C%2520DeepL%252C%2520and%2520GPT-4-Turbo.%250A%2520%2520Results%2520demonstrate%2520that%2520our%2520LLMs-in-the-loop%2520approach%252C%2520combined%2520with%250Afine-tuning%2520high-quality%252C%2520domain-specific%2520data%252C%2520enables%2520specialized%2520models%2520to%250Aoutperform%2520general-purpose%2520and%2520some%2520larger%2520systems.%2520This%2520research%252C%2520part%2520of%2520a%250Abroader%2520series%2520on%2520expert%2520small%2520models%252C%2520paves%2520the%2520way%2520for%2520future%250Ahealthcare-related%2520AI%2520developments%252C%2520including%2520deidentification%2520and%2520bio-medical%250Aentity%2520extraction%2520models.%2520Our%2520study%2520underscores%2520the%2520potential%2520of%2520tailored%250Aneural%2520translation%2520models%2520and%2520the%2520LLMs-in-the-loop%2520methodology%2520to%2520advance%2520the%250Afield%2520through%2520improved%2520data%2520generation%252C%2520evaluation%252C%2520agent%252C%2520and%2520modeling%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs-in-the-loop%20Part-1%3A%20Expert%20Small%20AI%20Models%20for%20Bio-Medical%20Text%0A%20%20Translation&entry.906535625=Bunyamin%20Keles%20and%20Murat%20Gunay%20and%20Serdar%20I.%20Caglar&entry.1292438233=%20%20Machine%20translation%20is%20indispensable%20in%20healthcare%20for%20enabling%20the%20global%0Adissemination%20of%20medical%20knowledge%20across%20languages.%20However%2C%20complex%20medical%0Aterminology%20poses%20unique%20challenges%20to%20achieving%20adequate%20translation%20quality%0Aand%20accuracy.%20This%20study%20introduces%20a%20novel%20%22LLMs-in-the-loop%22%20approach%20to%0Adevelop%20supervised%20neural%20machine%20translation%20models%20optimized%20specifically%20for%0Amedical%20texts.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20powerful%0Acapabilities%2C%20this%20research%20shows%20that%20small%2C%20specialized%20models%20trained%20on%0Ahigh-quality%20in-domain%20%28mostly%20synthetic%29%20data%20can%20outperform%20even%20vastly%0Alarger%20LLMs.%0A%20%20Custom%20parallel%20corpora%20in%20six%20languages%20were%20compiled%20from%20scientific%0Aarticles%2C%20synthetically%20generated%20clinical%20documents%2C%20and%20medical%20texts.%20Our%0ALLMs-in-the-loop%20methodology%20employs%20synthetic%20data%20generation%2C%20rigorous%0Aevaluation%2C%20and%20agent%20orchestration%20to%20enhance%20performance.%20We%20developed%20small%0Amedical%20translation%20models%20using%20the%20MarianMT%20base%20model.%20We%20introduce%20a%20new%0Amedical%20translation%20test%20dataset%20to%20standardize%20evaluation%20in%20this%20domain.%0AAssessed%20using%20BLEU%2C%20METEOR%2C%20ROUGE%2C%20and%20BERT%20scores%20on%20this%20test%20set%2C%20our%0AMarianMT-based%20models%20outperform%20Google%20Translate%2C%20DeepL%2C%20and%20GPT-4-Turbo.%0A%20%20Results%20demonstrate%20that%20our%20LLMs-in-the-loop%20approach%2C%20combined%20with%0Afine-tuning%20high-quality%2C%20domain-specific%20data%2C%20enables%20specialized%20models%20to%0Aoutperform%20general-purpose%20and%20some%20larger%20systems.%20This%20research%2C%20part%20of%20a%0Abroader%20series%20on%20expert%20small%20models%2C%20paves%20the%20way%20for%20future%0Ahealthcare-related%20AI%20developments%2C%20including%20deidentification%20and%20bio-medical%0Aentity%20extraction%20models.%20Our%20study%20underscores%20the%20potential%20of%20tailored%0Aneural%20translation%20models%20and%20the%20LLMs-in-the-loop%20methodology%20to%20advance%20the%0Afield%20through%20improved%20data%20generation%2C%20evaluation%2C%20agent%2C%20and%20modeling%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12126v2&entry.124074799=Read"},
{"title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents", "author": "Elizaveta Tennant and Stephen Hailes and Mirco Musolesi", "abstract": "  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n", "link": "http://arxiv.org/abs/2403.04202v4", "date": "2024-07-26", "relevancy": 1.4524, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4883}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4857}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&body=Title%3A%20Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents%0AAuthor%3A%20Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents%3A%20a%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents%3B%20however%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20focused%20on%20maximizing%20outcomes%0Aover%20time%29%2C%20norm-based%20%28i.e.%2C%20conforming%20to%20specific%20norms%29%2C%20or%20virtue-based%0A%28i.e.%2C%20considering%20a%20combination%20of%20different%20virtues%29.%20The%20extent%20to%20which%0Aagents%27%20co-development%20may%20be%20impacted%20by%20such%20moral%20heterogeneity%20in%0Apopulations%20is%20not%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20the%0Alearning%20dynamics%20of%20morally%20heterogeneous%20populations%20interacting%20in%20a%20social%0Adilemma%20setting.%20Using%20an%20Iterated%20Prisoner%27s%20Dilemma%20environment%20with%20a%0Apartner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%20which%20the%20prevalence%0Aof%20diverse%20moral%20agents%20in%20populations%20affects%20individual%20agents%27%20learning%0Abehaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%20several%20types%20of%0Anon-trivial%20interactions%20between%20pro-social%20and%20anti-social%20agents%2C%20and%20find%0Athat%20certain%20types%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%20agents%20towards%0Amore%20cooperative%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04202v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics%2520of%2520Moral%2520Behavior%2520in%2520Heterogeneous%2520Populations%2520of%2520Learning%250A%2520%2520Agents%26entry.906535625%3DElizaveta%2520Tennant%2520and%2520Stephen%2520Hailes%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Growing%2520concerns%2520about%2520safety%2520and%2520alignment%2520of%2520AI%2520systems%2520highlight%2520the%250Aimportance%2520of%2520embedding%2520moral%2520capabilities%2520in%2520artificial%2520agents%253A%2520a%2520promising%250Asolution%2520is%2520the%2520use%2520of%2520learning%2520from%2520experience%252C%2520i.e.%252C%2520Reinforcement%2520Learning.%250AIn%2520multi-agent%2520%2528social%2529%2520environments%252C%2520complex%2520population-level%2520phenomena%2520may%250Aemerge%2520from%2520interactions%2520between%2520individual%2520learning%2520agents.%2520Many%2520of%2520the%250Aexisting%2520studies%2520rely%2520on%2520simulated%2520social%2520dilemma%2520environments%2520to%2520study%2520the%250Ainteractions%2520of%2520independent%2520learning%2520agents%253B%2520however%252C%2520they%2520tend%2520to%2520ignore%2520the%250Amoral%2520heterogeneity%2520that%2520is%2520likely%2520to%2520be%2520present%2520in%2520societies%2520of%2520agents%2520in%250Apractice.%2520For%2520example%252C%2520at%2520different%2520points%2520in%2520time%2520a%2520single%2520learning%2520agent%2520may%250Aface%2520opponents%2520who%2520are%2520consequentialist%2520%2528i.e.%252C%2520focused%2520on%2520maximizing%2520outcomes%250Aover%2520time%2529%252C%2520norm-based%2520%2528i.e.%252C%2520conforming%2520to%2520specific%2520norms%2529%252C%2520or%2520virtue-based%250A%2528i.e.%252C%2520considering%2520a%2520combination%2520of%2520different%2520virtues%2529.%2520The%2520extent%2520to%2520which%250Aagents%2527%2520co-development%2520may%2520be%2520impacted%2520by%2520such%2520moral%2520heterogeneity%2520in%250Apopulations%2520is%2520not%2520well%2520understood.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520study%2520of%2520the%250Alearning%2520dynamics%2520of%2520morally%2520heterogeneous%2520populations%2520interacting%2520in%2520a%2520social%250Adilemma%2520setting.%2520Using%2520an%2520Iterated%2520Prisoner%2527s%2520Dilemma%2520environment%2520with%2520a%250Apartner%2520selection%2520mechanism%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520the%2520prevalence%250Aof%2520diverse%2520moral%2520agents%2520in%2520populations%2520affects%2520individual%2520agents%2527%2520learning%250Abehaviors%2520and%2520emergent%2520population-level%2520outcomes.%2520We%2520observe%2520several%2520types%2520of%250Anon-trivial%2520interactions%2520between%2520pro-social%2520and%2520anti-social%2520agents%252C%2520and%2520find%250Athat%2520certain%2520types%2520of%2520moral%2520agents%2520are%2520able%2520to%2520steer%2520selfish%2520agents%2520towards%250Amore%2520cooperative%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04202v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics%20of%20Moral%20Behavior%20in%20Heterogeneous%20Populations%20of%20Learning%0A%20%20Agents&entry.906535625=Elizaveta%20Tennant%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Growing%20concerns%20about%20safety%20and%20alignment%20of%20AI%20systems%20highlight%20the%0Aimportance%20of%20embedding%20moral%20capabilities%20in%20artificial%20agents%3A%20a%20promising%0Asolution%20is%20the%20use%20of%20learning%20from%20experience%2C%20i.e.%2C%20Reinforcement%20Learning.%0AIn%20multi-agent%20%28social%29%20environments%2C%20complex%20population-level%20phenomena%20may%0Aemerge%20from%20interactions%20between%20individual%20learning%20agents.%20Many%20of%20the%0Aexisting%20studies%20rely%20on%20simulated%20social%20dilemma%20environments%20to%20study%20the%0Ainteractions%20of%20independent%20learning%20agents%3B%20however%2C%20they%20tend%20to%20ignore%20the%0Amoral%20heterogeneity%20that%20is%20likely%20to%20be%20present%20in%20societies%20of%20agents%20in%0Apractice.%20For%20example%2C%20at%20different%20points%20in%20time%20a%20single%20learning%20agent%20may%0Aface%20opponents%20who%20are%20consequentialist%20%28i.e.%2C%20focused%20on%20maximizing%20outcomes%0Aover%20time%29%2C%20norm-based%20%28i.e.%2C%20conforming%20to%20specific%20norms%29%2C%20or%20virtue-based%0A%28i.e.%2C%20considering%20a%20combination%20of%20different%20virtues%29.%20The%20extent%20to%20which%0Aagents%27%20co-development%20may%20be%20impacted%20by%20such%20moral%20heterogeneity%20in%0Apopulations%20is%20not%20well%20understood.%20In%20this%20paper%2C%20we%20present%20a%20study%20of%20the%0Alearning%20dynamics%20of%20morally%20heterogeneous%20populations%20interacting%20in%20a%20social%0Adilemma%20setting.%20Using%20an%20Iterated%20Prisoner%27s%20Dilemma%20environment%20with%20a%0Apartner%20selection%20mechanism%2C%20we%20investigate%20the%20extent%20to%20which%20the%20prevalence%0Aof%20diverse%20moral%20agents%20in%20populations%20affects%20individual%20agents%27%20learning%0Abehaviors%20and%20emergent%20population-level%20outcomes.%20We%20observe%20several%20types%20of%0Anon-trivial%20interactions%20between%20pro-social%20and%20anti-social%20agents%2C%20and%20find%0Athat%20certain%20types%20of%20moral%20agents%20are%20able%20to%20steer%20selfish%20agents%20towards%0Amore%20cooperative%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04202v4&entry.124074799=Read"},
{"title": "On the Pros and Cons of Active Learning for Moral Preference Elicitation", "author": "Vijay Keswani and Vincent Conitzer and Hoda Heidari and Jana Schaich Borg and Walter Sinnott-Armstrong", "abstract": "  Computational preference elicitation methods are tools used to learn people's\npreferences quantitatively in a given context. Recent works on preference\nelicitation advocate for active learning as an efficient method to iteratively\nconstruct queries (framed as comparisons between context-specific cases) that\nare likely to be most informative about an agent's underlying preferences. In\nthis work, we argue that the use of active learning for moral preference\nelicitation relies on certain assumptions about the underlying moral\npreferences, which can be violated in practice. Specifically, we highlight the\nfollowing common assumptions (a) preferences are stable over time and not\nsensitive to the sequence of presented queries, (b) the appropriate hypothesis\nclass is chosen to model moral preferences, and (c) noise in the agent's\nresponses is limited. While these assumptions can be appropriate for preference\nelicitation in certain domains, prior research on moral psychology suggests\nthey may not be valid for moral judgments. Through a synthetic simulation of\npreferences that violate the above assumptions, we observe that active learning\ncan have similar or worse performance than a basic random query selection\nmethod in certain settings. Yet, simulation results also demonstrate that\nactive learning can still be viable if the degree of instability or noise is\nrelatively small and when the agent's preferences can be approximately\nrepresented with the hypothesis class used for learning. Our study highlights\nthe nuances associated with effective moral preference elicitation in practice\nand advocates for the cautious use of active learning as a methodology to learn\nmoral preferences.\n", "link": "http://arxiv.org/abs/2407.18889v1", "date": "2024-07-26", "relevancy": 0.913, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Pros%20and%20Cons%20of%20Active%20Learning%20for%20Moral%20Preference%20Elicitation&body=Title%3A%20On%20the%20Pros%20and%20Cons%20of%20Active%20Learning%20for%20Moral%20Preference%20Elicitation%0AAuthor%3A%20Vijay%20Keswani%20and%20Vincent%20Conitzer%20and%20Hoda%20Heidari%20and%20Jana%20Schaich%20Borg%20and%20Walter%20Sinnott-Armstrong%0AAbstract%3A%20%20%20Computational%20preference%20elicitation%20methods%20are%20tools%20used%20to%20learn%20people%27s%0Apreferences%20quantitatively%20in%20a%20given%20context.%20Recent%20works%20on%20preference%0Aelicitation%20advocate%20for%20active%20learning%20as%20an%20efficient%20method%20to%20iteratively%0Aconstruct%20queries%20%28framed%20as%20comparisons%20between%20context-specific%20cases%29%20that%0Aare%20likely%20to%20be%20most%20informative%20about%20an%20agent%27s%20underlying%20preferences.%20In%0Athis%20work%2C%20we%20argue%20that%20the%20use%20of%20active%20learning%20for%20moral%20preference%0Aelicitation%20relies%20on%20certain%20assumptions%20about%20the%20underlying%20moral%0Apreferences%2C%20which%20can%20be%20violated%20in%20practice.%20Specifically%2C%20we%20highlight%20the%0Afollowing%20common%20assumptions%20%28a%29%20preferences%20are%20stable%20over%20time%20and%20not%0Asensitive%20to%20the%20sequence%20of%20presented%20queries%2C%20%28b%29%20the%20appropriate%20hypothesis%0Aclass%20is%20chosen%20to%20model%20moral%20preferences%2C%20and%20%28c%29%20noise%20in%20the%20agent%27s%0Aresponses%20is%20limited.%20While%20these%20assumptions%20can%20be%20appropriate%20for%20preference%0Aelicitation%20in%20certain%20domains%2C%20prior%20research%20on%20moral%20psychology%20suggests%0Athey%20may%20not%20be%20valid%20for%20moral%20judgments.%20Through%20a%20synthetic%20simulation%20of%0Apreferences%20that%20violate%20the%20above%20assumptions%2C%20we%20observe%20that%20active%20learning%0Acan%20have%20similar%20or%20worse%20performance%20than%20a%20basic%20random%20query%20selection%0Amethod%20in%20certain%20settings.%20Yet%2C%20simulation%20results%20also%20demonstrate%20that%0Aactive%20learning%20can%20still%20be%20viable%20if%20the%20degree%20of%20instability%20or%20noise%20is%0Arelatively%20small%20and%20when%20the%20agent%27s%20preferences%20can%20be%20approximately%0Arepresented%20with%20the%20hypothesis%20class%20used%20for%20learning.%20Our%20study%20highlights%0Athe%20nuances%20associated%20with%20effective%20moral%20preference%20elicitation%20in%20practice%0Aand%20advocates%20for%20the%20cautious%20use%20of%20active%20learning%20as%20a%20methodology%20to%20learn%0Amoral%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Pros%2520and%2520Cons%2520of%2520Active%2520Learning%2520for%2520Moral%2520Preference%2520Elicitation%26entry.906535625%3DVijay%2520Keswani%2520and%2520Vincent%2520Conitzer%2520and%2520Hoda%2520Heidari%2520and%2520Jana%2520Schaich%2520Borg%2520and%2520Walter%2520Sinnott-Armstrong%26entry.1292438233%3D%2520%2520Computational%2520preference%2520elicitation%2520methods%2520are%2520tools%2520used%2520to%2520learn%2520people%2527s%250Apreferences%2520quantitatively%2520in%2520a%2520given%2520context.%2520Recent%2520works%2520on%2520preference%250Aelicitation%2520advocate%2520for%2520active%2520learning%2520as%2520an%2520efficient%2520method%2520to%2520iteratively%250Aconstruct%2520queries%2520%2528framed%2520as%2520comparisons%2520between%2520context-specific%2520cases%2529%2520that%250Aare%2520likely%2520to%2520be%2520most%2520informative%2520about%2520an%2520agent%2527s%2520underlying%2520preferences.%2520In%250Athis%2520work%252C%2520we%2520argue%2520that%2520the%2520use%2520of%2520active%2520learning%2520for%2520moral%2520preference%250Aelicitation%2520relies%2520on%2520certain%2520assumptions%2520about%2520the%2520underlying%2520moral%250Apreferences%252C%2520which%2520can%2520be%2520violated%2520in%2520practice.%2520Specifically%252C%2520we%2520highlight%2520the%250Afollowing%2520common%2520assumptions%2520%2528a%2529%2520preferences%2520are%2520stable%2520over%2520time%2520and%2520not%250Asensitive%2520to%2520the%2520sequence%2520of%2520presented%2520queries%252C%2520%2528b%2529%2520the%2520appropriate%2520hypothesis%250Aclass%2520is%2520chosen%2520to%2520model%2520moral%2520preferences%252C%2520and%2520%2528c%2529%2520noise%2520in%2520the%2520agent%2527s%250Aresponses%2520is%2520limited.%2520While%2520these%2520assumptions%2520can%2520be%2520appropriate%2520for%2520preference%250Aelicitation%2520in%2520certain%2520domains%252C%2520prior%2520research%2520on%2520moral%2520psychology%2520suggests%250Athey%2520may%2520not%2520be%2520valid%2520for%2520moral%2520judgments.%2520Through%2520a%2520synthetic%2520simulation%2520of%250Apreferences%2520that%2520violate%2520the%2520above%2520assumptions%252C%2520we%2520observe%2520that%2520active%2520learning%250Acan%2520have%2520similar%2520or%2520worse%2520performance%2520than%2520a%2520basic%2520random%2520query%2520selection%250Amethod%2520in%2520certain%2520settings.%2520Yet%252C%2520simulation%2520results%2520also%2520demonstrate%2520that%250Aactive%2520learning%2520can%2520still%2520be%2520viable%2520if%2520the%2520degree%2520of%2520instability%2520or%2520noise%2520is%250Arelatively%2520small%2520and%2520when%2520the%2520agent%2527s%2520preferences%2520can%2520be%2520approximately%250Arepresented%2520with%2520the%2520hypothesis%2520class%2520used%2520for%2520learning.%2520Our%2520study%2520highlights%250Athe%2520nuances%2520associated%2520with%2520effective%2520moral%2520preference%2520elicitation%2520in%2520practice%250Aand%2520advocates%2520for%2520the%2520cautious%2520use%2520of%2520active%2520learning%2520as%2520a%2520methodology%2520to%2520learn%250Amoral%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Pros%20and%20Cons%20of%20Active%20Learning%20for%20Moral%20Preference%20Elicitation&entry.906535625=Vijay%20Keswani%20and%20Vincent%20Conitzer%20and%20Hoda%20Heidari%20and%20Jana%20Schaich%20Borg%20and%20Walter%20Sinnott-Armstrong&entry.1292438233=%20%20Computational%20preference%20elicitation%20methods%20are%20tools%20used%20to%20learn%20people%27s%0Apreferences%20quantitatively%20in%20a%20given%20context.%20Recent%20works%20on%20preference%0Aelicitation%20advocate%20for%20active%20learning%20as%20an%20efficient%20method%20to%20iteratively%0Aconstruct%20queries%20%28framed%20as%20comparisons%20between%20context-specific%20cases%29%20that%0Aare%20likely%20to%20be%20most%20informative%20about%20an%20agent%27s%20underlying%20preferences.%20In%0Athis%20work%2C%20we%20argue%20that%20the%20use%20of%20active%20learning%20for%20moral%20preference%0Aelicitation%20relies%20on%20certain%20assumptions%20about%20the%20underlying%20moral%0Apreferences%2C%20which%20can%20be%20violated%20in%20practice.%20Specifically%2C%20we%20highlight%20the%0Afollowing%20common%20assumptions%20%28a%29%20preferences%20are%20stable%20over%20time%20and%20not%0Asensitive%20to%20the%20sequence%20of%20presented%20queries%2C%20%28b%29%20the%20appropriate%20hypothesis%0Aclass%20is%20chosen%20to%20model%20moral%20preferences%2C%20and%20%28c%29%20noise%20in%20the%20agent%27s%0Aresponses%20is%20limited.%20While%20these%20assumptions%20can%20be%20appropriate%20for%20preference%0Aelicitation%20in%20certain%20domains%2C%20prior%20research%20on%20moral%20psychology%20suggests%0Athey%20may%20not%20be%20valid%20for%20moral%20judgments.%20Through%20a%20synthetic%20simulation%20of%0Apreferences%20that%20violate%20the%20above%20assumptions%2C%20we%20observe%20that%20active%20learning%0Acan%20have%20similar%20or%20worse%20performance%20than%20a%20basic%20random%20query%20selection%0Amethod%20in%20certain%20settings.%20Yet%2C%20simulation%20results%20also%20demonstrate%20that%0Aactive%20learning%20can%20still%20be%20viable%20if%20the%20degree%20of%20instability%20or%20noise%20is%0Arelatively%20small%20and%20when%20the%20agent%27s%20preferences%20can%20be%20approximately%0Arepresented%20with%20the%20hypothesis%20class%20used%20for%20learning.%20Our%20study%20highlights%0Athe%20nuances%20associated%20with%20effective%20moral%20preference%20elicitation%20in%20practice%0Aand%20advocates%20for%20the%20cautious%20use%20of%20active%20learning%20as%20a%20methodology%20to%20learn%0Amoral%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18889v1&entry.124074799=Read"},
{"title": "Exploring Scaling Trends in LLM Robustness", "author": "Nikolaus Howe and Micha\u0142 Zajac and Ian McKenzie and Oskar Hollinsworth and Tom Tseng and Pierre-Luc Bacon and Adam Gleave", "abstract": "  Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses.\n", "link": "http://arxiv.org/abs/2407.18213v2", "date": "2024-07-26", "relevancy": 1.4126, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4608}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Scaling%20Trends%20in%20LLM%20Robustness&body=Title%3A%20Exploring%20Scaling%20Trends%20in%20LLM%20Robustness%0AAuthor%3A%20Nikolaus%20Howe%20and%20Micha%C5%82%20Zajac%20and%20Ian%20McKenzie%20and%20Oskar%20Hollinsworth%20and%20Tom%20Tseng%20and%20Pierre-Luc%20Bacon%20and%20Adam%20Gleave%0AAbstract%3A%20%20%20Language%20model%20capabilities%20predictably%20improve%20from%20scaling%20a%20model%27s%20size%0Aand%20training%20data.%20Motivated%20by%20this%2C%20increasingly%20large%20language%20models%20have%0Abeen%20trained%2C%20yielding%20an%20array%20of%20impressive%20capabilities.%20Yet%20these%20models%0Aare%20vulnerable%20to%20adversarial%20prompts%2C%20such%20as%20%22jailbreaks%22%20that%20hijack%20models%0Ato%20perform%20undesired%20behaviors%2C%20posing%20a%20significant%20risk%20of%20misuse.%20Prior%20work%0Aindicates%20that%20computer%20vision%20models%20become%20more%20robust%20with%20model%20and%20data%0Ascaling%2C%20raising%20the%20question%3A%20does%20language%20model%20robustness%20also%20improve%20with%0Ascale%3F%20We%20study%20this%20question%20empirically%2C%20finding%20that%20larger%20models%20respond%0Asubstantially%20better%20to%20adversarial%20training%2C%20but%20there%20is%20little%20to%20no%20benefit%0Afrom%20model%20scale%20in%20the%20absence%20of%20explicit%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18213v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Scaling%2520Trends%2520in%2520LLM%2520Robustness%26entry.906535625%3DNikolaus%2520Howe%2520and%2520Micha%25C5%2582%2520Zajac%2520and%2520Ian%2520McKenzie%2520and%2520Oskar%2520Hollinsworth%2520and%2520Tom%2520Tseng%2520and%2520Pierre-Luc%2520Bacon%2520and%2520Adam%2520Gleave%26entry.1292438233%3D%2520%2520Language%2520model%2520capabilities%2520predictably%2520improve%2520from%2520scaling%2520a%2520model%2527s%2520size%250Aand%2520training%2520data.%2520Motivated%2520by%2520this%252C%2520increasingly%2520large%2520language%2520models%2520have%250Abeen%2520trained%252C%2520yielding%2520an%2520array%2520of%2520impressive%2520capabilities.%2520Yet%2520these%2520models%250Aare%2520vulnerable%2520to%2520adversarial%2520prompts%252C%2520such%2520as%2520%2522jailbreaks%2522%2520that%2520hijack%2520models%250Ato%2520perform%2520undesired%2520behaviors%252C%2520posing%2520a%2520significant%2520risk%2520of%2520misuse.%2520Prior%2520work%250Aindicates%2520that%2520computer%2520vision%2520models%2520become%2520more%2520robust%2520with%2520model%2520and%2520data%250Ascaling%252C%2520raising%2520the%2520question%253A%2520does%2520language%2520model%2520robustness%2520also%2520improve%2520with%250Ascale%253F%2520We%2520study%2520this%2520question%2520empirically%252C%2520finding%2520that%2520larger%2520models%2520respond%250Asubstantially%2520better%2520to%2520adversarial%2520training%252C%2520but%2520there%2520is%2520little%2520to%2520no%2520benefit%250Afrom%2520model%2520scale%2520in%2520the%2520absence%2520of%2520explicit%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18213v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Scaling%20Trends%20in%20LLM%20Robustness&entry.906535625=Nikolaus%20Howe%20and%20Micha%C5%82%20Zajac%20and%20Ian%20McKenzie%20and%20Oskar%20Hollinsworth%20and%20Tom%20Tseng%20and%20Pierre-Luc%20Bacon%20and%20Adam%20Gleave&entry.1292438233=%20%20Language%20model%20capabilities%20predictably%20improve%20from%20scaling%20a%20model%27s%20size%0Aand%20training%20data.%20Motivated%20by%20this%2C%20increasingly%20large%20language%20models%20have%0Abeen%20trained%2C%20yielding%20an%20array%20of%20impressive%20capabilities.%20Yet%20these%20models%0Aare%20vulnerable%20to%20adversarial%20prompts%2C%20such%20as%20%22jailbreaks%22%20that%20hijack%20models%0Ato%20perform%20undesired%20behaviors%2C%20posing%20a%20significant%20risk%20of%20misuse.%20Prior%20work%0Aindicates%20that%20computer%20vision%20models%20become%20more%20robust%20with%20model%20and%20data%0Ascaling%2C%20raising%20the%20question%3A%20does%20language%20model%20robustness%20also%20improve%20with%0Ascale%3F%20We%20study%20this%20question%20empirically%2C%20finding%20that%20larger%20models%20respond%0Asubstantially%20better%20to%20adversarial%20training%2C%20but%20there%20is%20little%20to%20no%20benefit%0Afrom%20model%20scale%20in%20the%20absence%20of%20explicit%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18213v2&entry.124074799=Read"},
{"title": "FLUE: Federated Learning with Un-Encrypted model weights", "author": "Elie Atallah", "abstract": "  Federated Learning enables diverse devices to collaboratively train a shared\nmodel while keeping training data locally stored, avoiding the need for\ncentralized cloud storage. Despite existing privacy measures, concerns arise\nfrom potential reverse engineering of gradients, even with added noise,\nrevealing private data. To address this, recent research emphasizes using\nencrypted model parameters during training. This paper introduces a novel\nfederated learning algorithm, leveraging coded local gradients without\nencryption, exchanging coded proxies for model parameters, and injecting\nsurplus noise for enhanced privacy. Two algorithm variants are presented,\nshowcasing convergence and learning rates adaptable to coding schemes and raw\ndata characteristics. Two encryption-free implementations with fixed and random\ncoding matrices are provided, demonstrating promising simulation results from\nboth federated optimization and machine learning perspectives.\n", "link": "http://arxiv.org/abs/2407.18750v1", "date": "2024-07-26", "relevancy": 1.4303, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4834}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4822}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLUE%3A%20Federated%20Learning%20with%20Un-Encrypted%20model%20weights&body=Title%3A%20FLUE%3A%20Federated%20Learning%20with%20Un-Encrypted%20model%20weights%0AAuthor%3A%20Elie%20Atallah%0AAbstract%3A%20%20%20Federated%20Learning%20enables%20diverse%20devices%20to%20collaboratively%20train%20a%20shared%0Amodel%20while%20keeping%20training%20data%20locally%20stored%2C%20avoiding%20the%20need%20for%0Acentralized%20cloud%20storage.%20Despite%20existing%20privacy%20measures%2C%20concerns%20arise%0Afrom%20potential%20reverse%20engineering%20of%20gradients%2C%20even%20with%20added%20noise%2C%0Arevealing%20private%20data.%20To%20address%20this%2C%20recent%20research%20emphasizes%20using%0Aencrypted%20model%20parameters%20during%20training.%20This%20paper%20introduces%20a%20novel%0Afederated%20learning%20algorithm%2C%20leveraging%20coded%20local%20gradients%20without%0Aencryption%2C%20exchanging%20coded%20proxies%20for%20model%20parameters%2C%20and%20injecting%0Asurplus%20noise%20for%20enhanced%20privacy.%20Two%20algorithm%20variants%20are%20presented%2C%0Ashowcasing%20convergence%20and%20learning%20rates%20adaptable%20to%20coding%20schemes%20and%20raw%0Adata%20characteristics.%20Two%20encryption-free%20implementations%20with%20fixed%20and%20random%0Acoding%20matrices%20are%20provided%2C%20demonstrating%20promising%20simulation%20results%20from%0Aboth%20federated%20optimization%20and%20machine%20learning%20perspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLUE%253A%2520Federated%2520Learning%2520with%2520Un-Encrypted%2520model%2520weights%26entry.906535625%3DElie%2520Atallah%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520enables%2520diverse%2520devices%2520to%2520collaboratively%2520train%2520a%2520shared%250Amodel%2520while%2520keeping%2520training%2520data%2520locally%2520stored%252C%2520avoiding%2520the%2520need%2520for%250Acentralized%2520cloud%2520storage.%2520Despite%2520existing%2520privacy%2520measures%252C%2520concerns%2520arise%250Afrom%2520potential%2520reverse%2520engineering%2520of%2520gradients%252C%2520even%2520with%2520added%2520noise%252C%250Arevealing%2520private%2520data.%2520To%2520address%2520this%252C%2520recent%2520research%2520emphasizes%2520using%250Aencrypted%2520model%2520parameters%2520during%2520training.%2520This%2520paper%2520introduces%2520a%2520novel%250Afederated%2520learning%2520algorithm%252C%2520leveraging%2520coded%2520local%2520gradients%2520without%250Aencryption%252C%2520exchanging%2520coded%2520proxies%2520for%2520model%2520parameters%252C%2520and%2520injecting%250Asurplus%2520noise%2520for%2520enhanced%2520privacy.%2520Two%2520algorithm%2520variants%2520are%2520presented%252C%250Ashowcasing%2520convergence%2520and%2520learning%2520rates%2520adaptable%2520to%2520coding%2520schemes%2520and%2520raw%250Adata%2520characteristics.%2520Two%2520encryption-free%2520implementations%2520with%2520fixed%2520and%2520random%250Acoding%2520matrices%2520are%2520provided%252C%2520demonstrating%2520promising%2520simulation%2520results%2520from%250Aboth%2520federated%2520optimization%2520and%2520machine%2520learning%2520perspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLUE%3A%20Federated%20Learning%20with%20Un-Encrypted%20model%20weights&entry.906535625=Elie%20Atallah&entry.1292438233=%20%20Federated%20Learning%20enables%20diverse%20devices%20to%20collaboratively%20train%20a%20shared%0Amodel%20while%20keeping%20training%20data%20locally%20stored%2C%20avoiding%20the%20need%20for%0Acentralized%20cloud%20storage.%20Despite%20existing%20privacy%20measures%2C%20concerns%20arise%0Afrom%20potential%20reverse%20engineering%20of%20gradients%2C%20even%20with%20added%20noise%2C%0Arevealing%20private%20data.%20To%20address%20this%2C%20recent%20research%20emphasizes%20using%0Aencrypted%20model%20parameters%20during%20training.%20This%20paper%20introduces%20a%20novel%0Afederated%20learning%20algorithm%2C%20leveraging%20coded%20local%20gradients%20without%0Aencryption%2C%20exchanging%20coded%20proxies%20for%20model%20parameters%2C%20and%20injecting%0Asurplus%20noise%20for%20enhanced%20privacy.%20Two%20algorithm%20variants%20are%20presented%2C%0Ashowcasing%20convergence%20and%20learning%20rates%20adaptable%20to%20coding%20schemes%20and%20raw%0Adata%20characteristics.%20Two%20encryption-free%20implementations%20with%20fixed%20and%20random%0Acoding%20matrices%20are%20provided%2C%20demonstrating%20promising%20simulation%20results%20from%0Aboth%20federated%20optimization%20and%20machine%20learning%20perspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18750v1&entry.124074799=Read"},
{"title": "Adversarial Robustification via Text-to-Image Diffusion Models", "author": "Daewon Choi and Jongheon Jeong and Huiwon Jang and Jinwoo Shin", "abstract": "  Adversarial robustness has been conventionally believed as a challenging\nproperty to encode for neural networks, requiring plenty of training data. In\nthe recent paradigm of adopting off-the-shelf models, however, access to their\ntraining data is often infeasible or not practical, while most of such models\nare not originally trained concerning adversarial robustness. In this paper, we\ndevelop a scalable and model-agnostic solution to achieve adversarial\nrobustness without using any data. Our intuition is to view recent\ntext-to-image diffusion models as \"adaptable\" denoisers that can be optimized\nto specify target tasks. Based on this, we propose: (a) to initiate a\ndenoise-and-classify pipeline that offers provable guarantees against\nadversarial attacks, and (b) to leverage a few synthetic reference images\ngenerated from the text-to-image model that enables novel adaptation schemes.\nOur experiments show that our data-free scheme applied to the pre-trained CLIP\ncould improve the (provable) adversarial robustness of its diverse zero-shot\nclassification derivatives (while maintaining their accuracy), significantly\nsurpassing prior approaches that utilize the full training data. Not only for\nCLIP, we also demonstrate that our framework is easily applicable for\nrobustifying other visual classifiers efficiently.\n", "link": "http://arxiv.org/abs/2407.18658v1", "date": "2024-07-26", "relevancy": 1.2326, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6166}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustification%20via%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Adversarial%20Robustification%20via%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Daewon%20Choi%20and%20Jongheon%20Jeong%20and%20Huiwon%20Jang%20and%20Jinwoo%20Shin%0AAbstract%3A%20%20%20Adversarial%20robustness%20has%20been%20conventionally%20believed%20as%20a%20challenging%0Aproperty%20to%20encode%20for%20neural%20networks%2C%20requiring%20plenty%20of%20training%20data.%20In%0Athe%20recent%20paradigm%20of%20adopting%20off-the-shelf%20models%2C%20however%2C%20access%20to%20their%0Atraining%20data%20is%20often%20infeasible%20or%20not%20practical%2C%20while%20most%20of%20such%20models%0Aare%20not%20originally%20trained%20concerning%20adversarial%20robustness.%20In%20this%20paper%2C%20we%0Adevelop%20a%20scalable%20and%20model-agnostic%20solution%20to%20achieve%20adversarial%0Arobustness%20without%20using%20any%20data.%20Our%20intuition%20is%20to%20view%20recent%0Atext-to-image%20diffusion%20models%20as%20%22adaptable%22%20denoisers%20that%20can%20be%20optimized%0Ato%20specify%20target%20tasks.%20Based%20on%20this%2C%20we%20propose%3A%20%28a%29%20to%20initiate%20a%0Adenoise-and-classify%20pipeline%20that%20offers%20provable%20guarantees%20against%0Aadversarial%20attacks%2C%20and%20%28b%29%20to%20leverage%20a%20few%20synthetic%20reference%20images%0Agenerated%20from%20the%20text-to-image%20model%20that%20enables%20novel%20adaptation%20schemes.%0AOur%20experiments%20show%20that%20our%20data-free%20scheme%20applied%20to%20the%20pre-trained%20CLIP%0Acould%20improve%20the%20%28provable%29%20adversarial%20robustness%20of%20its%20diverse%20zero-shot%0Aclassification%20derivatives%20%28while%20maintaining%20their%20accuracy%29%2C%20significantly%0Asurpassing%20prior%20approaches%20that%20utilize%20the%20full%20training%20data.%20Not%20only%20for%0ACLIP%2C%20we%20also%20demonstrate%20that%20our%20framework%20is%20easily%20applicable%20for%0Arobustifying%20other%20visual%20classifiers%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustification%2520via%2520Text-to-Image%2520Diffusion%2520Models%26entry.906535625%3DDaewon%2520Choi%2520and%2520Jongheon%2520Jeong%2520and%2520Huiwon%2520Jang%2520and%2520Jinwoo%2520Shin%26entry.1292438233%3D%2520%2520Adversarial%2520robustness%2520has%2520been%2520conventionally%2520believed%2520as%2520a%2520challenging%250Aproperty%2520to%2520encode%2520for%2520neural%2520networks%252C%2520requiring%2520plenty%2520of%2520training%2520data.%2520In%250Athe%2520recent%2520paradigm%2520of%2520adopting%2520off-the-shelf%2520models%252C%2520however%252C%2520access%2520to%2520their%250Atraining%2520data%2520is%2520often%2520infeasible%2520or%2520not%2520practical%252C%2520while%2520most%2520of%2520such%2520models%250Aare%2520not%2520originally%2520trained%2520concerning%2520adversarial%2520robustness.%2520In%2520this%2520paper%252C%2520we%250Adevelop%2520a%2520scalable%2520and%2520model-agnostic%2520solution%2520to%2520achieve%2520adversarial%250Arobustness%2520without%2520using%2520any%2520data.%2520Our%2520intuition%2520is%2520to%2520view%2520recent%250Atext-to-image%2520diffusion%2520models%2520as%2520%2522adaptable%2522%2520denoisers%2520that%2520can%2520be%2520optimized%250Ato%2520specify%2520target%2520tasks.%2520Based%2520on%2520this%252C%2520we%2520propose%253A%2520%2528a%2529%2520to%2520initiate%2520a%250Adenoise-and-classify%2520pipeline%2520that%2520offers%2520provable%2520guarantees%2520against%250Aadversarial%2520attacks%252C%2520and%2520%2528b%2529%2520to%2520leverage%2520a%2520few%2520synthetic%2520reference%2520images%250Agenerated%2520from%2520the%2520text-to-image%2520model%2520that%2520enables%2520novel%2520adaptation%2520schemes.%250AOur%2520experiments%2520show%2520that%2520our%2520data-free%2520scheme%2520applied%2520to%2520the%2520pre-trained%2520CLIP%250Acould%2520improve%2520the%2520%2528provable%2529%2520adversarial%2520robustness%2520of%2520its%2520diverse%2520zero-shot%250Aclassification%2520derivatives%2520%2528while%2520maintaining%2520their%2520accuracy%2529%252C%2520significantly%250Asurpassing%2520prior%2520approaches%2520that%2520utilize%2520the%2520full%2520training%2520data.%2520Not%2520only%2520for%250ACLIP%252C%2520we%2520also%2520demonstrate%2520that%2520our%2520framework%2520is%2520easily%2520applicable%2520for%250Arobustifying%2520other%2520visual%2520classifiers%2520efficiently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustification%20via%20Text-to-Image%20Diffusion%20Models&entry.906535625=Daewon%20Choi%20and%20Jongheon%20Jeong%20and%20Huiwon%20Jang%20and%20Jinwoo%20Shin&entry.1292438233=%20%20Adversarial%20robustness%20has%20been%20conventionally%20believed%20as%20a%20challenging%0Aproperty%20to%20encode%20for%20neural%20networks%2C%20requiring%20plenty%20of%20training%20data.%20In%0Athe%20recent%20paradigm%20of%20adopting%20off-the-shelf%20models%2C%20however%2C%20access%20to%20their%0Atraining%20data%20is%20often%20infeasible%20or%20not%20practical%2C%20while%20most%20of%20such%20models%0Aare%20not%20originally%20trained%20concerning%20adversarial%20robustness.%20In%20this%20paper%2C%20we%0Adevelop%20a%20scalable%20and%20model-agnostic%20solution%20to%20achieve%20adversarial%0Arobustness%20without%20using%20any%20data.%20Our%20intuition%20is%20to%20view%20recent%0Atext-to-image%20diffusion%20models%20as%20%22adaptable%22%20denoisers%20that%20can%20be%20optimized%0Ato%20specify%20target%20tasks.%20Based%20on%20this%2C%20we%20propose%3A%20%28a%29%20to%20initiate%20a%0Adenoise-and-classify%20pipeline%20that%20offers%20provable%20guarantees%20against%0Aadversarial%20attacks%2C%20and%20%28b%29%20to%20leverage%20a%20few%20synthetic%20reference%20images%0Agenerated%20from%20the%20text-to-image%20model%20that%20enables%20novel%20adaptation%20schemes.%0AOur%20experiments%20show%20that%20our%20data-free%20scheme%20applied%20to%20the%20pre-trained%20CLIP%0Acould%20improve%20the%20%28provable%29%20adversarial%20robustness%20of%20its%20diverse%20zero-shot%0Aclassification%20derivatives%20%28while%20maintaining%20their%20accuracy%29%2C%20significantly%0Asurpassing%20prior%20approaches%20that%20utilize%20the%20full%20training%20data.%20Not%20only%20for%0ACLIP%2C%20we%20also%20demonstrate%20that%20our%20framework%20is%20easily%20applicable%20for%0Arobustifying%20other%20visual%20classifiers%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18658v1&entry.124074799=Read"},
{"title": "Multi-Robot System Architecture design in SysML and BPMN", "author": "Ahmed R. Sadik and Christian Goerick", "abstract": "  Multi-Robot System (MRS) is a complex system that contains many different\nsoftware and hardware components. This main problem addressed in this article\nis the MRS design complexity. The proposed solution provides a modular modeling\nand simulation technique that is based on formal system engineering method,\ntherefore the MRS design complexity is decomposed and reduced. Modeling the MRS\nhas been achieved via two formal Architecture Description Languages (ADLs),\nwhich are Systems Modeling Language (SysML) and Business Process Model and\nNotation (BPMN), to design the system blueprints. By using those abstract\ndesign ADLs, the implementation of the project becomes technology agnostic.\nThis allows to transfer the design concept from on programming language to\nanother. During the simulation phase, a multi-agent environment is used to\nsimulate the MRS blueprints. The simulation has been implemented in Java Agent\nDevelopment (JADE) middleware. Therefore, its results can be used to analysis\nand verify the proposed MRS model in form of performance evaluation matrix.\n", "link": "http://arxiv.org/abs/2407.18749v1", "date": "2024-07-26", "relevancy": 1.3781, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5322}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5296}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Robot%20System%20Architecture%20design%20in%20SysML%20and%20BPMN&body=Title%3A%20Multi-Robot%20System%20Architecture%20design%20in%20SysML%20and%20BPMN%0AAuthor%3A%20Ahmed%20R.%20Sadik%20and%20Christian%20Goerick%0AAbstract%3A%20%20%20Multi-Robot%20System%20%28MRS%29%20is%20a%20complex%20system%20that%20contains%20many%20different%0Asoftware%20and%20hardware%20components.%20This%20main%20problem%20addressed%20in%20this%20article%0Ais%20the%20MRS%20design%20complexity.%20The%20proposed%20solution%20provides%20a%20modular%20modeling%0Aand%20simulation%20technique%20that%20is%20based%20on%20formal%20system%20engineering%20method%2C%0Atherefore%20the%20MRS%20design%20complexity%20is%20decomposed%20and%20reduced.%20Modeling%20the%20MRS%0Ahas%20been%20achieved%20via%20two%20formal%20Architecture%20Description%20Languages%20%28ADLs%29%2C%0Awhich%20are%20Systems%20Modeling%20Language%20%28SysML%29%20and%20Business%20Process%20Model%20and%0ANotation%20%28BPMN%29%2C%20to%20design%20the%20system%20blueprints.%20By%20using%20those%20abstract%0Adesign%20ADLs%2C%20the%20implementation%20of%20the%20project%20becomes%20technology%20agnostic.%0AThis%20allows%20to%20transfer%20the%20design%20concept%20from%20on%20programming%20language%20to%0Aanother.%20During%20the%20simulation%20phase%2C%20a%20multi-agent%20environment%20is%20used%20to%0Asimulate%20the%20MRS%20blueprints.%20The%20simulation%20has%20been%20implemented%20in%20Java%20Agent%0ADevelopment%20%28JADE%29%20middleware.%20Therefore%2C%20its%20results%20can%20be%20used%20to%20analysis%0Aand%20verify%20the%20proposed%20MRS%20model%20in%20form%20of%20performance%20evaluation%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Robot%2520System%2520Architecture%2520design%2520in%2520SysML%2520and%2520BPMN%26entry.906535625%3DAhmed%2520R.%2520Sadik%2520and%2520Christian%2520Goerick%26entry.1292438233%3D%2520%2520Multi-Robot%2520System%2520%2528MRS%2529%2520is%2520a%2520complex%2520system%2520that%2520contains%2520many%2520different%250Asoftware%2520and%2520hardware%2520components.%2520This%2520main%2520problem%2520addressed%2520in%2520this%2520article%250Ais%2520the%2520MRS%2520design%2520complexity.%2520The%2520proposed%2520solution%2520provides%2520a%2520modular%2520modeling%250Aand%2520simulation%2520technique%2520that%2520is%2520based%2520on%2520formal%2520system%2520engineering%2520method%252C%250Atherefore%2520the%2520MRS%2520design%2520complexity%2520is%2520decomposed%2520and%2520reduced.%2520Modeling%2520the%2520MRS%250Ahas%2520been%2520achieved%2520via%2520two%2520formal%2520Architecture%2520Description%2520Languages%2520%2528ADLs%2529%252C%250Awhich%2520are%2520Systems%2520Modeling%2520Language%2520%2528SysML%2529%2520and%2520Business%2520Process%2520Model%2520and%250ANotation%2520%2528BPMN%2529%252C%2520to%2520design%2520the%2520system%2520blueprints.%2520By%2520using%2520those%2520abstract%250Adesign%2520ADLs%252C%2520the%2520implementation%2520of%2520the%2520project%2520becomes%2520technology%2520agnostic.%250AThis%2520allows%2520to%2520transfer%2520the%2520design%2520concept%2520from%2520on%2520programming%2520language%2520to%250Aanother.%2520During%2520the%2520simulation%2520phase%252C%2520a%2520multi-agent%2520environment%2520is%2520used%2520to%250Asimulate%2520the%2520MRS%2520blueprints.%2520The%2520simulation%2520has%2520been%2520implemented%2520in%2520Java%2520Agent%250ADevelopment%2520%2528JADE%2529%2520middleware.%2520Therefore%252C%2520its%2520results%2520can%2520be%2520used%2520to%2520analysis%250Aand%2520verify%2520the%2520proposed%2520MRS%2520model%2520in%2520form%2520of%2520performance%2520evaluation%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Robot%20System%20Architecture%20design%20in%20SysML%20and%20BPMN&entry.906535625=Ahmed%20R.%20Sadik%20and%20Christian%20Goerick&entry.1292438233=%20%20Multi-Robot%20System%20%28MRS%29%20is%20a%20complex%20system%20that%20contains%20many%20different%0Asoftware%20and%20hardware%20components.%20This%20main%20problem%20addressed%20in%20this%20article%0Ais%20the%20MRS%20design%20complexity.%20The%20proposed%20solution%20provides%20a%20modular%20modeling%0Aand%20simulation%20technique%20that%20is%20based%20on%20formal%20system%20engineering%20method%2C%0Atherefore%20the%20MRS%20design%20complexity%20is%20decomposed%20and%20reduced.%20Modeling%20the%20MRS%0Ahas%20been%20achieved%20via%20two%20formal%20Architecture%20Description%20Languages%20%28ADLs%29%2C%0Awhich%20are%20Systems%20Modeling%20Language%20%28SysML%29%20and%20Business%20Process%20Model%20and%0ANotation%20%28BPMN%29%2C%20to%20design%20the%20system%20blueprints.%20By%20using%20those%20abstract%0Adesign%20ADLs%2C%20the%20implementation%20of%20the%20project%20becomes%20technology%20agnostic.%0AThis%20allows%20to%20transfer%20the%20design%20concept%20from%20on%20programming%20language%20to%0Aanother.%20During%20the%20simulation%20phase%2C%20a%20multi-agent%20environment%20is%20used%20to%0Asimulate%20the%20MRS%20blueprints.%20The%20simulation%20has%20been%20implemented%20in%20Java%20Agent%0ADevelopment%20%28JADE%29%20middleware.%20Therefore%2C%20its%20results%20can%20be%20used%20to%20analysis%0Aand%20verify%20the%20proposed%20MRS%20model%20in%20form%20of%20performance%20evaluation%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18749v1&entry.124074799=Read"},
{"title": "Understanding XAI Through the Philosopher's Lens: A Historical\n  Perspective", "author": "Martina Mattioli and Antonio Emanuele Cin\u00e0 and Marcello Pelillo", "abstract": "  Despite explainable AI (XAI) has recently become a hot topic and several\ndifferent approaches have been developed, there is still a widespread belief\nthat it lacks a convincing unifying foundation. On the other hand, over the\npast centuries, the very concept of explanation has been the subject of\nextensive philosophical analysis in an attempt to address the fundamental\nquestion of \"why\" in the context of scientific law. However, this discussion\nhas rarely been connected with XAI. This paper tries to fill in this gap and\naims to explore the concept of explanation in AI through an epistemological\nlens. By comparing the historical development of both the philosophy of science\nand AI, an intriguing picture emerges. Specifically, we show that a gradual\nprogression has independently occurred in both domains from logical-deductive\nto statistical models of explanation, thereby experiencing in both cases a\nparadigm shift from deterministic to nondeterministic and probabilistic\ncausality. Interestingly, we also notice that similar concepts have\nindependently emerged in both realms such as, for example, the relation between\nexplanation and understanding and the importance of pragmatic factors. Our\nstudy aims to be the first step towards understanding the philosophical\nunderpinnings of the notion of explanation in AI, and we hope that our findings\nwill shed some fresh light on the elusive nature of XAI.\n", "link": "http://arxiv.org/abs/2407.18782v1", "date": "2024-07-26", "relevancy": 1.2255, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4129}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20XAI%20Through%20the%20Philosopher%27s%20Lens%3A%20A%20Historical%0A%20%20Perspective&body=Title%3A%20Understanding%20XAI%20Through%20the%20Philosopher%27s%20Lens%3A%20A%20Historical%0A%20%20Perspective%0AAuthor%3A%20Martina%20Mattioli%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Marcello%20Pelillo%0AAbstract%3A%20%20%20Despite%20explainable%20AI%20%28XAI%29%20has%20recently%20become%20a%20hot%20topic%20and%20several%0Adifferent%20approaches%20have%20been%20developed%2C%20there%20is%20still%20a%20widespread%20belief%0Athat%20it%20lacks%20a%20convincing%20unifying%20foundation.%20On%20the%20other%20hand%2C%20over%20the%0Apast%20centuries%2C%20the%20very%20concept%20of%20explanation%20has%20been%20the%20subject%20of%0Aextensive%20philosophical%20analysis%20in%20an%20attempt%20to%20address%20the%20fundamental%0Aquestion%20of%20%22why%22%20in%20the%20context%20of%20scientific%20law.%20However%2C%20this%20discussion%0Ahas%20rarely%20been%20connected%20with%20XAI.%20This%20paper%20tries%20to%20fill%20in%20this%20gap%20and%0Aaims%20to%20explore%20the%20concept%20of%20explanation%20in%20AI%20through%20an%20epistemological%0Alens.%20By%20comparing%20the%20historical%20development%20of%20both%20the%20philosophy%20of%20science%0Aand%20AI%2C%20an%20intriguing%20picture%20emerges.%20Specifically%2C%20we%20show%20that%20a%20gradual%0Aprogression%20has%20independently%20occurred%20in%20both%20domains%20from%20logical-deductive%0Ato%20statistical%20models%20of%20explanation%2C%20thereby%20experiencing%20in%20both%20cases%20a%0Aparadigm%20shift%20from%20deterministic%20to%20nondeterministic%20and%20probabilistic%0Acausality.%20Interestingly%2C%20we%20also%20notice%20that%20similar%20concepts%20have%0Aindependently%20emerged%20in%20both%20realms%20such%20as%2C%20for%20example%2C%20the%20relation%20between%0Aexplanation%20and%20understanding%20and%20the%20importance%20of%20pragmatic%20factors.%20Our%0Astudy%20aims%20to%20be%20the%20first%20step%20towards%20understanding%20the%20philosophical%0Aunderpinnings%20of%20the%20notion%20of%20explanation%20in%20AI%2C%20and%20we%20hope%20that%20our%20findings%0Awill%20shed%20some%20fresh%20light%20on%20the%20elusive%20nature%20of%20XAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520XAI%2520Through%2520the%2520Philosopher%2527s%2520Lens%253A%2520A%2520Historical%250A%2520%2520Perspective%26entry.906535625%3DMartina%2520Mattioli%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Marcello%2520Pelillo%26entry.1292438233%3D%2520%2520Despite%2520explainable%2520AI%2520%2528XAI%2529%2520has%2520recently%2520become%2520a%2520hot%2520topic%2520and%2520several%250Adifferent%2520approaches%2520have%2520been%2520developed%252C%2520there%2520is%2520still%2520a%2520widespread%2520belief%250Athat%2520it%2520lacks%2520a%2520convincing%2520unifying%2520foundation.%2520On%2520the%2520other%2520hand%252C%2520over%2520the%250Apast%2520centuries%252C%2520the%2520very%2520concept%2520of%2520explanation%2520has%2520been%2520the%2520subject%2520of%250Aextensive%2520philosophical%2520analysis%2520in%2520an%2520attempt%2520to%2520address%2520the%2520fundamental%250Aquestion%2520of%2520%2522why%2522%2520in%2520the%2520context%2520of%2520scientific%2520law.%2520However%252C%2520this%2520discussion%250Ahas%2520rarely%2520been%2520connected%2520with%2520XAI.%2520This%2520paper%2520tries%2520to%2520fill%2520in%2520this%2520gap%2520and%250Aaims%2520to%2520explore%2520the%2520concept%2520of%2520explanation%2520in%2520AI%2520through%2520an%2520epistemological%250Alens.%2520By%2520comparing%2520the%2520historical%2520development%2520of%2520both%2520the%2520philosophy%2520of%2520science%250Aand%2520AI%252C%2520an%2520intriguing%2520picture%2520emerges.%2520Specifically%252C%2520we%2520show%2520that%2520a%2520gradual%250Aprogression%2520has%2520independently%2520occurred%2520in%2520both%2520domains%2520from%2520logical-deductive%250Ato%2520statistical%2520models%2520of%2520explanation%252C%2520thereby%2520experiencing%2520in%2520both%2520cases%2520a%250Aparadigm%2520shift%2520from%2520deterministic%2520to%2520nondeterministic%2520and%2520probabilistic%250Acausality.%2520Interestingly%252C%2520we%2520also%2520notice%2520that%2520similar%2520concepts%2520have%250Aindependently%2520emerged%2520in%2520both%2520realms%2520such%2520as%252C%2520for%2520example%252C%2520the%2520relation%2520between%250Aexplanation%2520and%2520understanding%2520and%2520the%2520importance%2520of%2520pragmatic%2520factors.%2520Our%250Astudy%2520aims%2520to%2520be%2520the%2520first%2520step%2520towards%2520understanding%2520the%2520philosophical%250Aunderpinnings%2520of%2520the%2520notion%2520of%2520explanation%2520in%2520AI%252C%2520and%2520we%2520hope%2520that%2520our%2520findings%250Awill%2520shed%2520some%2520fresh%2520light%2520on%2520the%2520elusive%2520nature%2520of%2520XAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20XAI%20Through%20the%20Philosopher%27s%20Lens%3A%20A%20Historical%0A%20%20Perspective&entry.906535625=Martina%20Mattioli%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Marcello%20Pelillo&entry.1292438233=%20%20Despite%20explainable%20AI%20%28XAI%29%20has%20recently%20become%20a%20hot%20topic%20and%20several%0Adifferent%20approaches%20have%20been%20developed%2C%20there%20is%20still%20a%20widespread%20belief%0Athat%20it%20lacks%20a%20convincing%20unifying%20foundation.%20On%20the%20other%20hand%2C%20over%20the%0Apast%20centuries%2C%20the%20very%20concept%20of%20explanation%20has%20been%20the%20subject%20of%0Aextensive%20philosophical%20analysis%20in%20an%20attempt%20to%20address%20the%20fundamental%0Aquestion%20of%20%22why%22%20in%20the%20context%20of%20scientific%20law.%20However%2C%20this%20discussion%0Ahas%20rarely%20been%20connected%20with%20XAI.%20This%20paper%20tries%20to%20fill%20in%20this%20gap%20and%0Aaims%20to%20explore%20the%20concept%20of%20explanation%20in%20AI%20through%20an%20epistemological%0Alens.%20By%20comparing%20the%20historical%20development%20of%20both%20the%20philosophy%20of%20science%0Aand%20AI%2C%20an%20intriguing%20picture%20emerges.%20Specifically%2C%20we%20show%20that%20a%20gradual%0Aprogression%20has%20independently%20occurred%20in%20both%20domains%20from%20logical-deductive%0Ato%20statistical%20models%20of%20explanation%2C%20thereby%20experiencing%20in%20both%20cases%20a%0Aparadigm%20shift%20from%20deterministic%20to%20nondeterministic%20and%20probabilistic%0Acausality.%20Interestingly%2C%20we%20also%20notice%20that%20similar%20concepts%20have%0Aindependently%20emerged%20in%20both%20realms%20such%20as%2C%20for%20example%2C%20the%20relation%20between%0Aexplanation%20and%20understanding%20and%20the%20importance%20of%20pragmatic%20factors.%20Our%0Astudy%20aims%20to%20be%20the%20first%20step%20towards%20understanding%20the%20philosophical%0Aunderpinnings%20of%20the%20notion%20of%20explanation%20in%20AI%2C%20and%20we%20hope%20that%20our%20findings%0Awill%20shed%20some%20fresh%20light%20on%20the%20elusive%20nature%20of%20XAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18782v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


