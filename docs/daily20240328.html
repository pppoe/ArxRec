<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Self-supervised co-salient object detection via feature correspondence\n  at multiple scales", "author": "Souradeep Chakraborty and Dimitris Samaras", "abstract": "  Our paper introduces a novel two-stage self-supervised approach for detecting\nco-occurring salient objects (CoSOD) in image groups without requiring\nsegmentation annotations. Unlike existing unsupervised methods that rely solely\non patch-level information (e.g. clustering patch descriptors) or on\ncomputation heavy off-the-shelf components for CoSOD, our lightweight model\nleverages feature correspondences at both patch and region levels,\nsignificantly improving prediction performance. In the first stage, we train a\nself-supervised network that detects co-salient regions by computing local\npatch-level feature correspondences across images. We obtain the segmentation\npredictions using confidence-based adaptive thresholding. In the next stage, we\nrefine these intermediate segmentations by eliminating the detected regions\n(within each image) whose averaged feature representations are dissimilar to\nthe foreground feature representation averaged across all the cross-attention\nmaps (from the previous stage). Extensive experiments on three CoSOD benchmark\ndatasets show that our self-supervised model outperforms the corresponding\nstate-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model\nhas a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,\nour self-supervised model also outperforms several recent fully supervised\nCoSOD models on the three test datasets (e.g., on the CoCA dataset, our model\nhas a 4.6% F-measure gain over a recent supervised CoSOD model).\n", "link": "http://arxiv.org/abs/2403.11107v2", "date": "2024-03-27", "relevancy": 2.8084, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5749}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5587}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20co-salient%20object%20detection%20via%20feature%20correspondence%0A%20%20at%20multiple%20scales&body=Title%3A%20Self-supervised%20co-salient%20object%20detection%20via%20feature%20correspondence%0A%20%20at%20multiple%20scales%0AAuthor%3A%20Souradeep%20Chakraborty%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20Our%20paper%20introduces%20a%20novel%20two-stage%20self-supervised%20approach%20for%20detecting%0Aco-occurring%20salient%20objects%20%28CoSOD%29%20in%20image%20groups%20without%20requiring%0Asegmentation%20annotations.%20Unlike%20existing%20unsupervised%20methods%20that%20rely%20solely%0Aon%20patch-level%20information%20%28e.g.%20clustering%20patch%20descriptors%29%20or%20on%0Acomputation%20heavy%20off-the-shelf%20components%20for%20CoSOD%2C%20our%20lightweight%20model%0Aleverages%20feature%20correspondences%20at%20both%20patch%20and%20region%20levels%2C%0Asignificantly%20improving%20prediction%20performance.%20In%20the%20first%20stage%2C%20we%20train%20a%0Aself-supervised%20network%20that%20detects%20co-salient%20regions%20by%20computing%20local%0Apatch-level%20feature%20correspondences%20across%20images.%20We%20obtain%20the%20segmentation%0Apredictions%20using%20confidence-based%20adaptive%20thresholding.%20In%20the%20next%20stage%2C%20we%0Arefine%20these%20intermediate%20segmentations%20by%20eliminating%20the%20detected%20regions%0A%28within%20each%20image%29%20whose%20averaged%20feature%20representations%20are%20dissimilar%20to%0Athe%20foreground%20feature%20representation%20averaged%20across%20all%20the%20cross-attention%0Amaps%20%28from%20the%20previous%20stage%29.%20Extensive%20experiments%20on%20three%20CoSOD%20benchmark%0Adatasets%20show%20that%20our%20self-supervised%20model%20outperforms%20the%20corresponding%0Astate-of-the-art%20models%20by%20a%20huge%20margin%20%28e.g.%20on%20the%20CoCA%20dataset%2C%20our%20model%0Ahas%20a%2013.7%25%20F-measure%20gain%20over%20the%20SOTA%20unsupervised%20CoSOD%20model%29.%20Notably%2C%0Aour%20self-supervised%20model%20also%20outperforms%20several%20recent%20fully%20supervised%0ACoSOD%20models%20on%20the%20three%20test%20datasets%20%28e.g.%2C%20on%20the%20CoCA%20dataset%2C%20our%20model%0Ahas%20a%204.6%25%20F-measure%20gain%20over%20a%20recent%20supervised%20CoSOD%20model%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11107v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20co-salient%20object%20detection%20via%20feature%20correspondence%0A%20%20at%20multiple%20scales&entry.906535625=Souradeep%20Chakraborty%20and%20Dimitris%20Samaras&entry.1292438233=%20%20Our%20paper%20introduces%20a%20novel%20two-stage%20self-supervised%20approach%20for%20detecting%0Aco-occurring%20salient%20objects%20%28CoSOD%29%20in%20image%20groups%20without%20requiring%0Asegmentation%20annotations.%20Unlike%20existing%20unsupervised%20methods%20that%20rely%20solely%0Aon%20patch-level%20information%20%28e.g.%20clustering%20patch%20descriptors%29%20or%20on%0Acomputation%20heavy%20off-the-shelf%20components%20for%20CoSOD%2C%20our%20lightweight%20model%0Aleverages%20feature%20correspondences%20at%20both%20patch%20and%20region%20levels%2C%0Asignificantly%20improving%20prediction%20performance.%20In%20the%20first%20stage%2C%20we%20train%20a%0Aself-supervised%20network%20that%20detects%20co-salient%20regions%20by%20computing%20local%0Apatch-level%20feature%20correspondences%20across%20images.%20We%20obtain%20the%20segmentation%0Apredictions%20using%20confidence-based%20adaptive%20thresholding.%20In%20the%20next%20stage%2C%20we%0Arefine%20these%20intermediate%20segmentations%20by%20eliminating%20the%20detected%20regions%0A%28within%20each%20image%29%20whose%20averaged%20feature%20representations%20are%20dissimilar%20to%0Athe%20foreground%20feature%20representation%20averaged%20across%20all%20the%20cross-attention%0Amaps%20%28from%20the%20previous%20stage%29.%20Extensive%20experiments%20on%20three%20CoSOD%20benchmark%0Adatasets%20show%20that%20our%20self-supervised%20model%20outperforms%20the%20corresponding%0Astate-of-the-art%20models%20by%20a%20huge%20margin%20%28e.g.%20on%20the%20CoCA%20dataset%2C%20our%20model%0Ahas%20a%2013.7%25%20F-measure%20gain%20over%20the%20SOTA%20unsupervised%20CoSOD%20model%29.%20Notably%2C%0Aour%20self-supervised%20model%20also%20outperforms%20several%20recent%20fully%20supervised%0ACoSOD%20models%20on%20the%20three%20test%20datasets%20%28e.g.%2C%20on%20the%20CoCA%20dataset%2C%20our%20model%0Ahas%20a%204.6%25%20F-measure%20gain%20over%20a%20recent%20supervised%20CoSOD%20model%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11107v2&entry.124074799=Read"},
{"title": "CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary\n  semantic segmentation", "author": "Monika Wysocza\u0144ska and Oriane Sim\u00e9oni and Micha\u00ebl Ramamonjisoa and Andrei Bursuc and Tomasz Trzci\u0144ski and Patrick P\u00e9rez", "abstract": "  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose an\nopen-vocabulary semantic segmentation method, which does not require any\nannotations. We propose to locally improve dense MaskCLIP features, which are\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features. Our method CLIP-DINOiser needs only a\nsingle forward pass of CLIP and two light convolutional layers at inference, no\nextra supervision nor extra memory and reaches state-of-the-art results on\nchallenging and fine-grained benchmarks such as COCO, Pascal Context,\nCityscapes and ADE20k. The code to reproduce our results is available at\nhttps://github.com/wysoczanska/clip_dinoiser.\n", "link": "http://arxiv.org/abs/2312.12359v2", "date": "2024-03-27", "relevancy": 2.7682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLIP-DINOiser%3A%20Teaching%20CLIP%20a%20few%20DINO%20tricks%20for%20open-vocabulary%0A%20%20semantic%20segmentation&body=Title%3A%20CLIP-DINOiser%3A%20Teaching%20CLIP%20a%20few%20DINO%20tricks%20for%20open-vocabulary%0A%20%20semantic%20segmentation%0AAuthor%3A%20Monika%20Wysocza%C5%84ska%20and%20Oriane%20Sim%C3%A9oni%20and%20Micha%C3%ABl%20Ramamonjisoa%20and%20Andrei%20Bursuc%20and%20Tomasz%20Trzci%C5%84ski%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20%20%20The%20popular%20CLIP%20model%20displays%20impressive%20zero-shot%20capabilities%20thanks%20to%0Aits%20seamless%20interaction%20with%20arbitrary%20text%20prompts.%20However%2C%20its%20lack%20of%0Aspatial%20awareness%20makes%20it%20unsuitable%20for%20dense%20computer%20vision%20tasks%2C%20e.g.%2C%0Asemantic%20segmentation%2C%20without%20an%20additional%20fine-tuning%20step%20that%20often%20uses%0Aannotations%20and%20can%20potentially%20suppress%20its%20original%20open-vocabulary%0Aproperties.%20Meanwhile%2C%20self-supervised%20representation%20methods%20have%20demonstrated%0Agood%20localization%20properties%20without%20human-made%20annotations%20nor%20explicit%0Asupervision.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%20worlds%20and%20propose%20an%0Aopen-vocabulary%20semantic%20segmentation%20method%2C%20which%20does%20not%20require%20any%0Aannotations.%20We%20propose%20to%20locally%20improve%20dense%20MaskCLIP%20features%2C%20which%20are%0Acomputed%20with%20a%20simple%20modification%20of%20CLIP%27s%20last%20pooling%20layer%2C%20by%0Aintegrating%20localization%20priors%20extracted%20from%20self-supervised%20features.%20By%0Adoing%20so%2C%20we%20greatly%20improve%20the%20performance%20of%20MaskCLIP%20and%20produce%20smooth%0Aoutputs.%20Moreover%2C%20we%20show%20that%20the%20used%20self-supervised%20feature%20properties%20can%0Adirectly%20be%20learnt%20from%20CLIP%20features.%20Our%20method%20CLIP-DINOiser%20needs%20only%20a%0Asingle%20forward%20pass%20of%20CLIP%20and%20two%20light%20convolutional%20layers%20at%20inference%2C%20no%0Aextra%20supervision%20nor%20extra%20memory%20and%20reaches%20state-of-the-art%20results%20on%0Achallenging%20and%20fine-grained%20benchmarks%20such%20as%20COCO%2C%20Pascal%20Context%2C%0ACityscapes%20and%20ADE20k.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttps%3A//github.com/wysoczanska/clip_dinoiser.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12359v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-DINOiser%3A%20Teaching%20CLIP%20a%20few%20DINO%20tricks%20for%20open-vocabulary%0A%20%20semantic%20segmentation&entry.906535625=Monika%20Wysocza%C5%84ska%20and%20Oriane%20Sim%C3%A9oni%20and%20Micha%C3%ABl%20Ramamonjisoa%20and%20Andrei%20Bursuc%20and%20Tomasz%20Trzci%C5%84ski%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20The%20popular%20CLIP%20model%20displays%20impressive%20zero-shot%20capabilities%20thanks%20to%0Aits%20seamless%20interaction%20with%20arbitrary%20text%20prompts.%20However%2C%20its%20lack%20of%0Aspatial%20awareness%20makes%20it%20unsuitable%20for%20dense%20computer%20vision%20tasks%2C%20e.g.%2C%0Asemantic%20segmentation%2C%20without%20an%20additional%20fine-tuning%20step%20that%20often%20uses%0Aannotations%20and%20can%20potentially%20suppress%20its%20original%20open-vocabulary%0Aproperties.%20Meanwhile%2C%20self-supervised%20representation%20methods%20have%20demonstrated%0Agood%20localization%20properties%20without%20human-made%20annotations%20nor%20explicit%0Asupervision.%20In%20this%20work%2C%20we%20take%20the%20best%20of%20both%20worlds%20and%20propose%20an%0Aopen-vocabulary%20semantic%20segmentation%20method%2C%20which%20does%20not%20require%20any%0Aannotations.%20We%20propose%20to%20locally%20improve%20dense%20MaskCLIP%20features%2C%20which%20are%0Acomputed%20with%20a%20simple%20modification%20of%20CLIP%27s%20last%20pooling%20layer%2C%20by%0Aintegrating%20localization%20priors%20extracted%20from%20self-supervised%20features.%20By%0Adoing%20so%2C%20we%20greatly%20improve%20the%20performance%20of%20MaskCLIP%20and%20produce%20smooth%0Aoutputs.%20Moreover%2C%20we%20show%20that%20the%20used%20self-supervised%20feature%20properties%20can%0Adirectly%20be%20learnt%20from%20CLIP%20features.%20Our%20method%20CLIP-DINOiser%20needs%20only%20a%0Asingle%20forward%20pass%20of%20CLIP%20and%20two%20light%20convolutional%20layers%20at%20inference%2C%20no%0Aextra%20supervision%20nor%20extra%20memory%20and%20reaches%20state-of-the-art%20results%20on%0Achallenging%20and%20fine-grained%20benchmarks%20such%20as%20COCO%2C%20Pascal%20Context%2C%0ACityscapes%20and%20ADE20k.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttps%3A//github.com/wysoczanska/clip_dinoiser.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12359v2&entry.124074799=Read"},
{"title": "Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation", "author": "Xingqun Qi and Jiahao Pan and Peng Li and Ruibin Yuan and Xiaowei Chi and Mengfei Li and Wenhan Luo and Wei Xue and Shanghang Zhang and Qifeng Liu and Yike Guo", "abstract": "  Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets. Our\ncode and dataset will be released on the project page:\nhttps://xingqunqi-lab.github.io/Emo-Transition-Gesture/.\n", "link": "http://arxiv.org/abs/2311.17532v3", "date": "2024-03-27", "relevancy": 2.737, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.572}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5361}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Weakly-Supervised%20Emotion%20Transition%20Learning%20for%20Diverse%203D%20Co-speech%0A%20%20Gesture%20Generation&body=Title%3A%20Weakly-Supervised%20Emotion%20Transition%20Learning%20for%20Diverse%203D%20Co-speech%0A%20%20Gesture%20Generation%0AAuthor%3A%20Xingqun%20Qi%20and%20Jiahao%20Pan%20and%20Peng%20Li%20and%20Ruibin%20Yuan%20and%20Xiaowei%20Chi%20and%20Mengfei%20Li%20and%20Wenhan%20Luo%20and%20Wei%20Xue%20and%20Shanghang%20Zhang%20and%20Qifeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Generating%20vivid%20and%20emotional%203D%20co-speech%20gestures%20is%20crucial%20for%20virtual%0Aavatar%20animation%20in%20human-machine%20interaction%20applications.%20While%20the%20existing%0Amethods%20enable%20generating%20the%20gestures%20to%20follow%20a%20single%20emotion%20label%2C%20they%0Aoverlook%20that%20long%20gesture%20sequence%20modeling%20with%20emotion%20transition%20is%20more%0Apractical%20in%20real%20scenes.%20In%20addition%2C%20the%20lack%20of%20large-scale%20available%0Adatasets%20with%20emotional%20transition%20speech%20and%20corresponding%203D%20human%20gestures%0Aalso%20limits%20the%20addressing%20of%20this%20task.%20To%20fulfill%20this%20goal%2C%20we%20first%0Aincorporate%20the%20ChatGPT-4%20and%20an%20audio%20inpainting%20approach%20to%20construct%20the%0Ahigh-fidelity%20emotion%20transition%20human%20speeches.%20Considering%20obtaining%20the%0Arealistic%203D%20pose%20annotations%20corresponding%20to%20the%20dynamically%20inpainted%0Aemotion%20transition%20audio%20is%20extremely%20difficult%2C%20we%20propose%20a%20novel%20weakly%0Asupervised%20training%20strategy%20to%20encourage%20authority%20gesture%20transitions.%0ASpecifically%2C%20to%20enhance%20the%20coordination%20of%20transition%20gestures%20w.r.t%0Adifferent%20emotional%20ones%2C%20we%20model%20the%20temporal%20association%20representation%0Abetween%20two%20different%20emotional%20gesture%20sequences%20as%20style%20guidance%20and%20infuse%0Ait%20into%20the%20transition%20generation.%20We%20further%20devise%20an%20emotion%20mixture%0Amechanism%20that%20provides%20weak%20supervision%20based%20on%20a%20learnable%20mixed%20emotion%0Alabel%20for%20transition%20gestures.%20Last%2C%20we%20present%20a%20keyframe%20sampler%20to%20supply%0Aeffective%20initial%20posture%20cues%20in%20long%20sequences%2C%20enabling%20us%20to%20generate%0Adiverse%20gestures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Athe%20state-of-the-art%20models%20constructed%20by%20adapting%20single%20emotion-conditioned%0Acounterparts%20on%20our%20newly%20defined%20emotion%20transition%20task%20and%20datasets.%20Our%0Acode%20and%20dataset%20will%20be%20released%20on%20the%20project%20page%3A%0Ahttps%3A//xingqunqi-lab.github.io/Emo-Transition-Gesture/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17532v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-Supervised%20Emotion%20Transition%20Learning%20for%20Diverse%203D%20Co-speech%0A%20%20Gesture%20Generation&entry.906535625=Xingqun%20Qi%20and%20Jiahao%20Pan%20and%20Peng%20Li%20and%20Ruibin%20Yuan%20and%20Xiaowei%20Chi%20and%20Mengfei%20Li%20and%20Wenhan%20Luo%20and%20Wei%20Xue%20and%20Shanghang%20Zhang%20and%20Qifeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20Generating%20vivid%20and%20emotional%203D%20co-speech%20gestures%20is%20crucial%20for%20virtual%0Aavatar%20animation%20in%20human-machine%20interaction%20applications.%20While%20the%20existing%0Amethods%20enable%20generating%20the%20gestures%20to%20follow%20a%20single%20emotion%20label%2C%20they%0Aoverlook%20that%20long%20gesture%20sequence%20modeling%20with%20emotion%20transition%20is%20more%0Apractical%20in%20real%20scenes.%20In%20addition%2C%20the%20lack%20of%20large-scale%20available%0Adatasets%20with%20emotional%20transition%20speech%20and%20corresponding%203D%20human%20gestures%0Aalso%20limits%20the%20addressing%20of%20this%20task.%20To%20fulfill%20this%20goal%2C%20we%20first%0Aincorporate%20the%20ChatGPT-4%20and%20an%20audio%20inpainting%20approach%20to%20construct%20the%0Ahigh-fidelity%20emotion%20transition%20human%20speeches.%20Considering%20obtaining%20the%0Arealistic%203D%20pose%20annotations%20corresponding%20to%20the%20dynamically%20inpainted%0Aemotion%20transition%20audio%20is%20extremely%20difficult%2C%20we%20propose%20a%20novel%20weakly%0Asupervised%20training%20strategy%20to%20encourage%20authority%20gesture%20transitions.%0ASpecifically%2C%20to%20enhance%20the%20coordination%20of%20transition%20gestures%20w.r.t%0Adifferent%20emotional%20ones%2C%20we%20model%20the%20temporal%20association%20representation%0Abetween%20two%20different%20emotional%20gesture%20sequences%20as%20style%20guidance%20and%20infuse%0Ait%20into%20the%20transition%20generation.%20We%20further%20devise%20an%20emotion%20mixture%0Amechanism%20that%20provides%20weak%20supervision%20based%20on%20a%20learnable%20mixed%20emotion%0Alabel%20for%20transition%20gestures.%20Last%2C%20we%20present%20a%20keyframe%20sampler%20to%20supply%0Aeffective%20initial%20posture%20cues%20in%20long%20sequences%2C%20enabling%20us%20to%20generate%0Adiverse%20gestures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Athe%20state-of-the-art%20models%20constructed%20by%20adapting%20single%20emotion-conditioned%0Acounterparts%20on%20our%20newly%20defined%20emotion%20transition%20task%20and%20datasets.%20Our%0Acode%20and%20dataset%20will%20be%20released%20on%20the%20project%20page%3A%0Ahttps%3A//xingqunqi-lab.github.io/Emo-Transition-Gesture/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17532v3&entry.124074799=Read"},
{"title": "SAR-Net: Multi-scale Direction-aware SAR Network via Global Information\n  Fusion", "author": "Mingxiang Cao and Jie Lei and Weiying Xie and Jiaqing Zhang and Daixun Li and Yunsong Li", "abstract": "  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion. Among them, Multi-scale Alignment Module (MAM) and distinct\nMulti-level Fusion Module (MFM) enhance feature integration by capturing both\ntexture detail and semantic information. Then, Multi-feature Embedding Module\n(MEM) feeds back global features into the primary branches, further improving\ninformation transmission. Additionally, DAM, through bidirectional attention\npolymerization, captures direction-aware information, effectively eliminating\nbackground interference. Extensive experiments demonstrate the effectiveness of\nSAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and\nship datasets (SSDD, HRSID), confirming its generalization capability and\nrobustness.\n", "link": "http://arxiv.org/abs/2312.16943v2", "date": "2024-03-27", "relevancy": 2.7087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SAR-Net%3A%20Multi-scale%20Direction-aware%20SAR%20Network%20via%20Global%20Information%0A%20%20Fusion&body=Title%3A%20SAR-Net%3A%20Multi-scale%20Direction-aware%20SAR%20Network%20via%20Global%20Information%0A%20%20Fusion%0AAuthor%3A%20Mingxiang%20Cao%20and%20Jie%20Lei%20and%20Weiying%20Xie%20and%20Jiaqing%20Zhang%20and%20Daixun%20Li%20and%20Yunsong%20Li%0AAbstract%3A%20%20%20Deep%20learning%20has%20driven%20significant%20progress%20in%20object%20detection%20using%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20imagery.%20Existing%20methods%2C%20while%20achieving%0Apromising%20results%2C%20often%20struggle%20to%20effectively%20integrate%20local%20and%20global%0Ainformation%2C%20particularly%20direction-aware%20features.%20This%20paper%20proposes%0ASAR-Net%2C%20a%20novel%20framework%20specifically%20designed%20for%20global%20fusion%20of%0Adirection-aware%20information%20in%20SAR%20object%20detection.%20SAR-Net%20leverages%20two%20key%0Ainnovations%3A%20the%20Unity%20Compensation%20Mechanism%20%28UCM%29%20and%20the%20Direction-aware%0AAttention%20Module%20%28DAM%29.%20UCM%20facilitates%20the%20establishment%20of%20complementary%0Arelationships%20among%20features%20across%20different%20scales%2C%20enabling%20efficient%20global%0Ainformation%20fusion.%20Among%20them%2C%20Multi-scale%20Alignment%20Module%20%28MAM%29%20and%20distinct%0AMulti-level%20Fusion%20Module%20%28MFM%29%20enhance%20feature%20integration%20by%20capturing%20both%0Atexture%20detail%20and%20semantic%20information.%20Then%2C%20Multi-feature%20Embedding%20Module%0A%28MEM%29%20feeds%20back%20global%20features%20into%20the%20primary%20branches%2C%20further%20improving%0Ainformation%20transmission.%20Additionally%2C%20DAM%2C%20through%20bidirectional%20attention%0Apolymerization%2C%20captures%20direction-aware%20information%2C%20effectively%20eliminating%0Abackground%20interference.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0ASAR-Net%2C%20achieving%20state-of-the-art%20results%20on%20aircraft%20%28SAR-AIRcraft-1.0%29%20and%0Aship%20datasets%20%28SSDD%2C%20HRSID%29%2C%20confirming%20its%20generalization%20capability%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16943v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAR-Net%3A%20Multi-scale%20Direction-aware%20SAR%20Network%20via%20Global%20Information%0A%20%20Fusion&entry.906535625=Mingxiang%20Cao%20and%20Jie%20Lei%20and%20Weiying%20Xie%20and%20Jiaqing%20Zhang%20and%20Daixun%20Li%20and%20Yunsong%20Li&entry.1292438233=%20%20Deep%20learning%20has%20driven%20significant%20progress%20in%20object%20detection%20using%0ASynthetic%20Aperture%20Radar%20%28SAR%29%20imagery.%20Existing%20methods%2C%20while%20achieving%0Apromising%20results%2C%20often%20struggle%20to%20effectively%20integrate%20local%20and%20global%0Ainformation%2C%20particularly%20direction-aware%20features.%20This%20paper%20proposes%0ASAR-Net%2C%20a%20novel%20framework%20specifically%20designed%20for%20global%20fusion%20of%0Adirection-aware%20information%20in%20SAR%20object%20detection.%20SAR-Net%20leverages%20two%20key%0Ainnovations%3A%20the%20Unity%20Compensation%20Mechanism%20%28UCM%29%20and%20the%20Direction-aware%0AAttention%20Module%20%28DAM%29.%20UCM%20facilitates%20the%20establishment%20of%20complementary%0Arelationships%20among%20features%20across%20different%20scales%2C%20enabling%20efficient%20global%0Ainformation%20fusion.%20Among%20them%2C%20Multi-scale%20Alignment%20Module%20%28MAM%29%20and%20distinct%0AMulti-level%20Fusion%20Module%20%28MFM%29%20enhance%20feature%20integration%20by%20capturing%20both%0Atexture%20detail%20and%20semantic%20information.%20Then%2C%20Multi-feature%20Embedding%20Module%0A%28MEM%29%20feeds%20back%20global%20features%20into%20the%20primary%20branches%2C%20further%20improving%0Ainformation%20transmission.%20Additionally%2C%20DAM%2C%20through%20bidirectional%20attention%0Apolymerization%2C%20captures%20direction-aware%20information%2C%20effectively%20eliminating%0Abackground%20interference.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0ASAR-Net%2C%20achieving%20state-of-the-art%20results%20on%20aircraft%20%28SAR-AIRcraft-1.0%29%20and%0Aship%20datasets%20%28SSDD%2C%20HRSID%29%2C%20confirming%20its%20generalization%20capability%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16943v2&entry.124074799=Read"},
{"title": "Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding", "author": "Run Shao and Zhaoyang Zhang and Chao Tao and Yunsheng Zhang and Chengli Peng and Haifeng Li", "abstract": "  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n", "link": "http://arxiv.org/abs/2403.18593v1", "date": "2024-03-27", "relevancy": 2.7017, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Homogeneous%20Tokenizer%20Matters%3A%20Homogeneous%20Visual%20Tokenizer%20for%20Remote%0A%20%20Sensing%20Image%20Understanding&body=Title%3A%20Homogeneous%20Tokenizer%20Matters%3A%20Homogeneous%20Visual%20Tokenizer%20for%20Remote%0A%20%20Sensing%20Image%20Understanding%0AAuthor%3A%20Run%20Shao%20and%20Zhaoyang%20Zhang%20and%20Chao%20Tao%20and%20Yunsheng%20Zhang%20and%20Chengli%20Peng%20and%20Haifeng%20Li%0AAbstract%3A%20%20%20The%20tokenizer%2C%20as%20one%20of%20the%20fundamental%20components%20of%20large%20models%2C%20has%20long%0Abeen%20overlooked%20or%20even%20misunderstood%20in%20visual%20tasks.%20One%20key%20factor%20of%20the%0Agreat%20comprehension%20power%20of%20the%20large%20language%20model%20is%20that%20natural%20language%0Atokenizers%20utilize%20meaningful%20words%20or%20subwords%20as%20the%20basic%20elements%20of%0Alanguage.%20In%20contrast%2C%20mainstream%20visual%20tokenizers%2C%20represented%20by%20patch-based%0Amethods%20such%20as%20Patch%20Embed%2C%20rely%20on%20meaningless%20rectangular%20patches%20as%20basic%0Aelements%20of%20vision%2C%20which%20cannot%20serve%20as%20effectively%20as%20words%20or%20subwords%20in%0Alanguage.%20Starting%20from%20the%20essence%20of%20the%20tokenizer%2C%20we%20defined%20semantically%0Aindependent%20regions%20%28SIRs%29%20for%20vision.%20We%20designed%20a%20simple%20HOmogeneous%20visual%0AtOKenizer%3A%20HOOK.%20HOOK%20mainly%20consists%20of%20two%20modules%3A%20the%20Object%20Perception%0AModule%20%28OPM%29%20and%20the%20Object%20Vectorization%20Module%20%28OVM%29.%20To%20achieve%20homogeneity%2C%0Athe%20OPM%20splits%20the%20image%20into%204%2A4%20pixel%20seeds%20and%20then%20utilizes%20the%20attention%0Amechanism%20to%20perceive%20SIRs.%20The%20OVM%20employs%20cross-attention%20to%20merge%20seeds%0Awithin%20the%20same%20SIR.%20To%20achieve%20adaptability%2C%20the%20OVM%20defines%20a%20variable%20number%0Aof%20learnable%20vectors%20as%20cross-attention%20queries%2C%20allowing%20for%20the%20adjustment%20of%0Atoken%20quantity.%20We%20conducted%20experiments%20on%20the%20NWPU-RESISC45%2C%20WHU-RS19%0Aclassification%20dataset%2C%20and%20GID5%20segmentation%20dataset%20for%20sparse%20and%20dense%0Atasks.%20The%20results%20demonstrate%20that%20the%20visual%20tokens%20obtained%20by%20HOOK%0Acorrespond%20to%20individual%20objects%2C%20which%20demonstrates%20homogeneity.%20HOOK%0Aoutperformed%20Patch%20Embed%20by%206%5C%25%20and%2010%5C%25%20in%20the%20two%20tasks%20and%20achieved%0Astate-of-the-art%20performance%20compared%20to%20the%20baselines%20used%20for%20comparison.%0ACompared%20to%20Patch%20Embed%2C%20which%20requires%20more%20than%20one%20hundred%20tokens%20for%20one%0Aimage%2C%20HOOK%20requires%20only%206%20and%208%20tokens%20for%20sparse%20and%20dense%20tasks%2C%0Arespectively%2C%20resulting%20in%20efficiency%20improvements%20of%201.5%20to%202.8%20times.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GeoX-Lab/Hook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18593v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homogeneous%20Tokenizer%20Matters%3A%20Homogeneous%20Visual%20Tokenizer%20for%20Remote%0A%20%20Sensing%20Image%20Understanding&entry.906535625=Run%20Shao%20and%20Zhaoyang%20Zhang%20and%20Chao%20Tao%20and%20Yunsheng%20Zhang%20and%20Chengli%20Peng%20and%20Haifeng%20Li&entry.1292438233=%20%20The%20tokenizer%2C%20as%20one%20of%20the%20fundamental%20components%20of%20large%20models%2C%20has%20long%0Abeen%20overlooked%20or%20even%20misunderstood%20in%20visual%20tasks.%20One%20key%20factor%20of%20the%0Agreat%20comprehension%20power%20of%20the%20large%20language%20model%20is%20that%20natural%20language%0Atokenizers%20utilize%20meaningful%20words%20or%20subwords%20as%20the%20basic%20elements%20of%0Alanguage.%20In%20contrast%2C%20mainstream%20visual%20tokenizers%2C%20represented%20by%20patch-based%0Amethods%20such%20as%20Patch%20Embed%2C%20rely%20on%20meaningless%20rectangular%20patches%20as%20basic%0Aelements%20of%20vision%2C%20which%20cannot%20serve%20as%20effectively%20as%20words%20or%20subwords%20in%0Alanguage.%20Starting%20from%20the%20essence%20of%20the%20tokenizer%2C%20we%20defined%20semantically%0Aindependent%20regions%20%28SIRs%29%20for%20vision.%20We%20designed%20a%20simple%20HOmogeneous%20visual%0AtOKenizer%3A%20HOOK.%20HOOK%20mainly%20consists%20of%20two%20modules%3A%20the%20Object%20Perception%0AModule%20%28OPM%29%20and%20the%20Object%20Vectorization%20Module%20%28OVM%29.%20To%20achieve%20homogeneity%2C%0Athe%20OPM%20splits%20the%20image%20into%204%2A4%20pixel%20seeds%20and%20then%20utilizes%20the%20attention%0Amechanism%20to%20perceive%20SIRs.%20The%20OVM%20employs%20cross-attention%20to%20merge%20seeds%0Awithin%20the%20same%20SIR.%20To%20achieve%20adaptability%2C%20the%20OVM%20defines%20a%20variable%20number%0Aof%20learnable%20vectors%20as%20cross-attention%20queries%2C%20allowing%20for%20the%20adjustment%20of%0Atoken%20quantity.%20We%20conducted%20experiments%20on%20the%20NWPU-RESISC45%2C%20WHU-RS19%0Aclassification%20dataset%2C%20and%20GID5%20segmentation%20dataset%20for%20sparse%20and%20dense%0Atasks.%20The%20results%20demonstrate%20that%20the%20visual%20tokens%20obtained%20by%20HOOK%0Acorrespond%20to%20individual%20objects%2C%20which%20demonstrates%20homogeneity.%20HOOK%0Aoutperformed%20Patch%20Embed%20by%206%5C%25%20and%2010%5C%25%20in%20the%20two%20tasks%20and%20achieved%0Astate-of-the-art%20performance%20compared%20to%20the%20baselines%20used%20for%20comparison.%0ACompared%20to%20Patch%20Embed%2C%20which%20requires%20more%20than%20one%20hundred%20tokens%20for%20one%0Aimage%2C%20HOOK%20requires%20only%206%20and%208%20tokens%20for%20sparse%20and%20dense%20tasks%2C%0Arespectively%2C%20resulting%20in%20efficiency%20improvements%20of%201.5%20to%202.8%20times.%20The%0Acode%20is%20available%20at%20https%3A//github.com/GeoX-Lab/Hook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18593v1&entry.124074799=Read"},
{"title": "HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional\n  Synthesis and Sampling of Hand-Object Interactions", "author": "Hao Xu and Haipeng Li and Yinqiao Wang and Shuaicheng Liu and Chi-Wing Fu", "abstract": "  Reconstructing 3D hand mesh robustly from a single image is very challenging,\ndue to the lack of diversity in existing real-world datasets. While data\nsynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.\nIn this work, we present HandBooster, a new approach to uplift the data\ndiversity and boost the 3D hand-mesh reconstruction performance by training a\nconditional generative space on hand-object interactions and purposely sampling\nthe space to synthesize effective data samples. First, we construct versatile\ncontent-aware conditions to guide a diffusion model to produce realistic images\nwith diverse hand appearances, poses, views, and backgrounds; favorably,\naccurate 3D annotations are obtained for free. Then, we design a novel\ncondition creator based on our similarity-aware distribution sampling\nstrategies to deliberately find novel and realistic interaction poses that are\ndistinctive from the training set. Equipped with our method, several baselines\ncan be significantly improved beyond the SOTA on the HO3D and DexYCB\nbenchmarks. Our code will be released on\nhttps://github.com/hxwork/HandBooster_Pytorch.\n", "link": "http://arxiv.org/abs/2403.18575v1", "date": "2024-03-27", "relevancy": 2.6697, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5853}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HandBooster%3A%20Boosting%203D%20Hand-Mesh%20Reconstruction%20by%20Conditional%0A%20%20Synthesis%20and%20Sampling%20of%20Hand-Object%20Interactions&body=Title%3A%20HandBooster%3A%20Boosting%203D%20Hand-Mesh%20Reconstruction%20by%20Conditional%0A%20%20Synthesis%20and%20Sampling%20of%20Hand-Object%20Interactions%0AAuthor%3A%20Hao%20Xu%20and%20Haipeng%20Li%20and%20Yinqiao%20Wang%20and%20Shuaicheng%20Liu%20and%20Chi-Wing%20Fu%0AAbstract%3A%20%20%20Reconstructing%203D%20hand%20mesh%20robustly%20from%20a%20single%20image%20is%20very%20challenging%2C%0Adue%20to%20the%20lack%20of%20diversity%20in%20existing%20real-world%20datasets.%20While%20data%0Asynthesis%20helps%20relieve%20the%20issue%2C%20the%20syn-to-real%20gap%20still%20hinders%20its%20usage.%0AIn%20this%20work%2C%20we%20present%20HandBooster%2C%20a%20new%20approach%20to%20uplift%20the%20data%0Adiversity%20and%20boost%20the%203D%20hand-mesh%20reconstruction%20performance%20by%20training%20a%0Aconditional%20generative%20space%20on%20hand-object%20interactions%20and%20purposely%20sampling%0Athe%20space%20to%20synthesize%20effective%20data%20samples.%20First%2C%20we%20construct%20versatile%0Acontent-aware%20conditions%20to%20guide%20a%20diffusion%20model%20to%20produce%20realistic%20images%0Awith%20diverse%20hand%20appearances%2C%20poses%2C%20views%2C%20and%20backgrounds%3B%20favorably%2C%0Aaccurate%203D%20annotations%20are%20obtained%20for%20free.%20Then%2C%20we%20design%20a%20novel%0Acondition%20creator%20based%20on%20our%20similarity-aware%20distribution%20sampling%0Astrategies%20to%20deliberately%20find%20novel%20and%20realistic%20interaction%20poses%20that%20are%0Adistinctive%20from%20the%20training%20set.%20Equipped%20with%20our%20method%2C%20several%20baselines%0Acan%20be%20significantly%20improved%20beyond%20the%20SOTA%20on%20the%20HO3D%20and%20DexYCB%0Abenchmarks.%20Our%20code%20will%20be%20released%20on%0Ahttps%3A//github.com/hxwork/HandBooster_Pytorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18575v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandBooster%3A%20Boosting%203D%20Hand-Mesh%20Reconstruction%20by%20Conditional%0A%20%20Synthesis%20and%20Sampling%20of%20Hand-Object%20Interactions&entry.906535625=Hao%20Xu%20and%20Haipeng%20Li%20and%20Yinqiao%20Wang%20and%20Shuaicheng%20Liu%20and%20Chi-Wing%20Fu&entry.1292438233=%20%20Reconstructing%203D%20hand%20mesh%20robustly%20from%20a%20single%20image%20is%20very%20challenging%2C%0Adue%20to%20the%20lack%20of%20diversity%20in%20existing%20real-world%20datasets.%20While%20data%0Asynthesis%20helps%20relieve%20the%20issue%2C%20the%20syn-to-real%20gap%20still%20hinders%20its%20usage.%0AIn%20this%20work%2C%20we%20present%20HandBooster%2C%20a%20new%20approach%20to%20uplift%20the%20data%0Adiversity%20and%20boost%20the%203D%20hand-mesh%20reconstruction%20performance%20by%20training%20a%0Aconditional%20generative%20space%20on%20hand-object%20interactions%20and%20purposely%20sampling%0Athe%20space%20to%20synthesize%20effective%20data%20samples.%20First%2C%20we%20construct%20versatile%0Acontent-aware%20conditions%20to%20guide%20a%20diffusion%20model%20to%20produce%20realistic%20images%0Awith%20diverse%20hand%20appearances%2C%20poses%2C%20views%2C%20and%20backgrounds%3B%20favorably%2C%0Aaccurate%203D%20annotations%20are%20obtained%20for%20free.%20Then%2C%20we%20design%20a%20novel%0Acondition%20creator%20based%20on%20our%20similarity-aware%20distribution%20sampling%0Astrategies%20to%20deliberately%20find%20novel%20and%20realistic%20interaction%20poses%20that%20are%0Adistinctive%20from%20the%20training%20set.%20Equipped%20with%20our%20method%2C%20several%20baselines%0Acan%20be%20significantly%20improved%20beyond%20the%20SOTA%20on%20the%20HO3D%20and%20DexYCB%0Abenchmarks.%20Our%20code%20will%20be%20released%20on%0Ahttps%3A//github.com/hxwork/HandBooster_Pytorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18575v1&entry.124074799=Read"},
{"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models", "author": "Yanwei Li and Yuechen Zhang and Chengyao Wang and Zhisheng Zhong and Yixin Chen and Ruihang Chu and Shaoteng Liu and Jiaya Jia", "abstract": "  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n", "link": "http://arxiv.org/abs/2403.18814v1", "date": "2024-03-27", "relevancy": 2.6684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5331}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5299}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mini-Gemini%3A%20Mining%20the%20Potential%20of%20Multi-modality%20Vision%20Language%0A%20%20Models&body=Title%3A%20Mini-Gemini%3A%20Mining%20the%20Potential%20of%20Multi-modality%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Yanwei%20Li%20and%20Yuechen%20Zhang%20and%20Chengyao%20Wang%20and%20Zhisheng%20Zhong%20and%20Yixin%20Chen%20and%20Ruihang%20Chu%20and%20Shaoteng%20Liu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Mini-Gemini%2C%20a%20simple%20and%20effective%20framework%0Aenhancing%20multi-modality%20Vision%20Language%20Models%20%28VLMs%29.%20Despite%20the%0Aadvancements%20in%20VLMs%20facilitating%20basic%20visual%20dialog%20and%20reasoning%2C%20a%0Aperformance%20gap%20persists%20compared%20to%20advanced%20models%20like%20GPT-4%20and%20Gemini.%20We%0Atry%20to%20narrow%20the%20gap%20by%20mining%20the%20potential%20of%20VLMs%20for%20better%20performance%0Aand%20any-to-any%20workflow%20from%20three%20aspects%2C%20i.e.%2C%20high-resolution%20visual%0Atokens%2C%20high-quality%20data%2C%20and%20VLM-guided%20generation.%20To%20enhance%20visual%20tokens%2C%0Awe%20propose%20to%20utilize%20an%20additional%20visual%20encoder%20for%20high-resolution%0Arefinement%20without%20increasing%20the%20visual%20token%20count.%20We%20further%20construct%20a%0Ahigh-quality%20dataset%20that%20promotes%20precise%20image%20comprehension%20and%0Areasoning-based%20generation%2C%20expanding%20the%20operational%20scope%20of%20current%20VLMs.%20In%0Ageneral%2C%20Mini-Gemini%20further%20mines%20the%20potential%20of%20VLMs%20and%20empowers%20current%0Aframeworks%20with%20image%20understanding%2C%20reasoning%2C%20and%20generation%20simultaneously.%0AMini-Gemini%20supports%20a%20series%20of%20dense%20and%20MoE%20Large%20Language%20Models%20%28LLMs%29%0Afrom%202B%20to%2034B.%20It%20is%20demonstrated%20to%20achieve%20leading%20performance%20in%20several%0Azero-shot%20benchmarks%20and%20even%20surpasses%20the%20developed%20private%20models.%20Code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/dvlab-research/MiniGemini.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18814v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mini-Gemini%3A%20Mining%20the%20Potential%20of%20Multi-modality%20Vision%20Language%0A%20%20Models&entry.906535625=Yanwei%20Li%20and%20Yuechen%20Zhang%20and%20Chengyao%20Wang%20and%20Zhisheng%20Zhong%20and%20Yixin%20Chen%20and%20Ruihang%20Chu%20and%20Shaoteng%20Liu%20and%20Jiaya%20Jia&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Mini-Gemini%2C%20a%20simple%20and%20effective%20framework%0Aenhancing%20multi-modality%20Vision%20Language%20Models%20%28VLMs%29.%20Despite%20the%0Aadvancements%20in%20VLMs%20facilitating%20basic%20visual%20dialog%20and%20reasoning%2C%20a%0Aperformance%20gap%20persists%20compared%20to%20advanced%20models%20like%20GPT-4%20and%20Gemini.%20We%0Atry%20to%20narrow%20the%20gap%20by%20mining%20the%20potential%20of%20VLMs%20for%20better%20performance%0Aand%20any-to-any%20workflow%20from%20three%20aspects%2C%20i.e.%2C%20high-resolution%20visual%0Atokens%2C%20high-quality%20data%2C%20and%20VLM-guided%20generation.%20To%20enhance%20visual%20tokens%2C%0Awe%20propose%20to%20utilize%20an%20additional%20visual%20encoder%20for%20high-resolution%0Arefinement%20without%20increasing%20the%20visual%20token%20count.%20We%20further%20construct%20a%0Ahigh-quality%20dataset%20that%20promotes%20precise%20image%20comprehension%20and%0Areasoning-based%20generation%2C%20expanding%20the%20operational%20scope%20of%20current%20VLMs.%20In%0Ageneral%2C%20Mini-Gemini%20further%20mines%20the%20potential%20of%20VLMs%20and%20empowers%20current%0Aframeworks%20with%20image%20understanding%2C%20reasoning%2C%20and%20generation%20simultaneously.%0AMini-Gemini%20supports%20a%20series%20of%20dense%20and%20MoE%20Large%20Language%20Models%20%28LLMs%29%0Afrom%202B%20to%2034B.%20It%20is%20demonstrated%20to%20achieve%20leading%20performance%20in%20several%0Azero-shot%20benchmarks%20and%20even%20surpasses%20the%20developed%20private%20models.%20Code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/dvlab-research/MiniGemini.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18814v1&entry.124074799=Read"},
{"title": "Contrastive Learning with Orthonormal Anchors (CLOA)", "author": "Huanran Li and Daniel Pimentel-Alarc\u00f3n", "abstract": "  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n", "link": "http://arxiv.org/abs/2403.18699v1", "date": "2024-03-27", "relevancy": 2.5495, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5028}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20with%20Orthonormal%20Anchors%20%28CLOA%29&body=Title%3A%20Contrastive%20Learning%20with%20Orthonormal%20Anchors%20%28CLOA%29%0AAuthor%3A%20Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n%0AAbstract%3A%20%20%20This%20study%20focuses%20on%20addressing%20the%20instability%20issues%20prevalent%20in%0Acontrastive%20learning%2C%20specifically%20examining%20the%20InfoNCE%20loss%20function%20and%20its%0Aderivatives.%20We%20reveal%20a%20critical%20observation%20that%20these%20loss%20functions%20exhibit%0Aa%20restrictive%20behavior%2C%20leading%20to%20a%20convergence%20phenomenon%20where%20embeddings%0Atend%20to%20merge%20into%20a%20singular%20point.%20This%20%22over-fusion%22%20effect%20detrimentally%0Aaffects%20classification%20accuracy%20in%20subsequent%20supervised-learning%20tasks.%0AThrough%20theoretical%20analysis%2C%20we%20demonstrate%20that%20embeddings%2C%20when%20equalized%20or%0Aconfined%20to%20a%20rank-1%20linear%20subspace%2C%20represent%20a%20local%20minimum%20for%20InfoNCE.%20In%0Aresponse%20to%20this%20challenge%2C%20our%20research%20introduces%20an%20innovative%20strategy%20that%0Aleverages%20the%20same%20or%20fewer%20labeled%20data%20than%20typically%20used%20in%20the%20fine-tuning%0Aphase.%20The%20loss%20we%20proposed%2C%20Orthonormal%20Anchor%20Regression%20Loss%2C%20is%20designed%20to%0Adisentangle%20embedding%20clusters%2C%20significantly%20enhancing%20the%20distinctiveness%20of%0Aeach%20embedding%20while%20simultaneously%20ensuring%20their%20aggregation%20into%20dense%2C%0Awell-defined%20clusters.%20Our%20method%20demonstrates%20remarkable%20improvements%20with%0Ajust%20a%20fraction%20of%20the%20conventional%20label%20requirements%2C%20as%20evidenced%20by%20our%0Aresults%20on%20CIFAR10%20and%20CIFAR100%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18699v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20with%20Orthonormal%20Anchors%20%28CLOA%29&entry.906535625=Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n&entry.1292438233=%20%20This%20study%20focuses%20on%20addressing%20the%20instability%20issues%20prevalent%20in%0Acontrastive%20learning%2C%20specifically%20examining%20the%20InfoNCE%20loss%20function%20and%20its%0Aderivatives.%20We%20reveal%20a%20critical%20observation%20that%20these%20loss%20functions%20exhibit%0Aa%20restrictive%20behavior%2C%20leading%20to%20a%20convergence%20phenomenon%20where%20embeddings%0Atend%20to%20merge%20into%20a%20singular%20point.%20This%20%22over-fusion%22%20effect%20detrimentally%0Aaffects%20classification%20accuracy%20in%20subsequent%20supervised-learning%20tasks.%0AThrough%20theoretical%20analysis%2C%20we%20demonstrate%20that%20embeddings%2C%20when%20equalized%20or%0Aconfined%20to%20a%20rank-1%20linear%20subspace%2C%20represent%20a%20local%20minimum%20for%20InfoNCE.%20In%0Aresponse%20to%20this%20challenge%2C%20our%20research%20introduces%20an%20innovative%20strategy%20that%0Aleverages%20the%20same%20or%20fewer%20labeled%20data%20than%20typically%20used%20in%20the%20fine-tuning%0Aphase.%20The%20loss%20we%20proposed%2C%20Orthonormal%20Anchor%20Regression%20Loss%2C%20is%20designed%20to%0Adisentangle%20embedding%20clusters%2C%20significantly%20enhancing%20the%20distinctiveness%20of%0Aeach%20embedding%20while%20simultaneously%20ensuring%20their%20aggregation%20into%20dense%2C%0Awell-defined%20clusters.%20Our%20method%20demonstrates%20remarkable%20improvements%20with%0Ajust%20a%20fraction%20of%20the%20conventional%20label%20requirements%2C%20as%20evidenced%20by%20our%0Aresults%20on%20CIFAR10%20and%20CIFAR100%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18699v1&entry.124074799=Read"},
{"title": "VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification", "author": "Yulan Hu and Sheng Ouyang and Zhirui Yang and Yong Liu", "abstract": "  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n", "link": "http://arxiv.org/abs/2311.01191v2", "date": "2024-03-27", "relevancy": 2.5485, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4938}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VIGraph%3A%20Generative%20Self-supervised%20Learning%20for%20Class-Imbalanced%20Node%0A%20%20Classification&body=Title%3A%20VIGraph%3A%20Generative%20Self-supervised%20Learning%20for%20Class-Imbalanced%20Node%0A%20%20Classification%0AAuthor%3A%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Zhirui%20Yang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Class%20imbalance%20in%20graph%20data%20presents%20significant%20challenges%20for%20node%0Aclassification.%20While%20existing%20methods%2C%20such%20as%20SMOTE-based%20approaches%2C%0Apartially%20mitigate%20this%20issue%2C%20they%20still%20exhibit%20limitations%20in%20constructing%0Aimbalanced%20graphs.%20Generative%20self-supervised%20learning%20%28SSL%29%20methods%2C%0Aexemplified%20by%20graph%20autoencoders%20%28GAEs%29%2C%20offer%20a%20promising%20solution%20by%0Adirectly%20generating%20minority%20nodes%20from%20the%20data%20itself%2C%20yet%20their%20potential%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20delve%20into%20the%20shortcomings%20of%0ASMOTE-based%20approaches%20in%20the%20construction%20of%20imbalanced%20graphs.%20Furthermore%2C%0Awe%20introduce%20VIGraph%2C%20a%20simple%20yet%20effective%20generative%20SSL%20approach%20that%0Arelies%20on%20the%20Variational%20GAE%20as%20the%20fundamental%20model.%20VIGraph%20strictly%0Aadheres%20to%20the%20concept%20of%20imbalance%20when%20constructing%20imbalanced%20graphs%20and%0Ainnovatively%20leverages%20the%20variational%20inference%20%28VI%29%20ability%20of%20Variational%0AGAE%20to%20generate%20nodes%20for%20minority%20classes.%20VIGraph%20introduces%20comprehensive%0Atraining%20strategies%2C%20including%20cross-view%20contrastive%20learning%20at%20the%20decoding%0Aphase%20to%20capture%20semantic%20knowledge%2C%20adjacency%20matrix%20reconstruction%20to%0Apreserve%20graph%20structure%2C%20and%20alignment%20strategy%20to%20ensure%20stable%20training.%0AVIGraph%20can%20generate%20high-quality%20nodes%20directly%20usable%20for%20classification%2C%0Aeliminating%20the%20need%20to%20integrate%20the%20generated%20nodes%20back%20to%20the%20graph%20as%20well%0Aas%20additional%20retraining%20found%20in%20SMOTE-based%20methods.%20We%20conduct%20extensive%0Aexperiments%2C%20results%20from%20which%20demonstrate%20the%20superiority%20and%20generality%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01191v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIGraph%3A%20Generative%20Self-supervised%20Learning%20for%20Class-Imbalanced%20Node%0A%20%20Classification&entry.906535625=Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Zhirui%20Yang%20and%20Yong%20Liu&entry.1292438233=%20%20Class%20imbalance%20in%20graph%20data%20presents%20significant%20challenges%20for%20node%0Aclassification.%20While%20existing%20methods%2C%20such%20as%20SMOTE-based%20approaches%2C%0Apartially%20mitigate%20this%20issue%2C%20they%20still%20exhibit%20limitations%20in%20constructing%0Aimbalanced%20graphs.%20Generative%20self-supervised%20learning%20%28SSL%29%20methods%2C%0Aexemplified%20by%20graph%20autoencoders%20%28GAEs%29%2C%20offer%20a%20promising%20solution%20by%0Adirectly%20generating%20minority%20nodes%20from%20the%20data%20itself%2C%20yet%20their%20potential%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20delve%20into%20the%20shortcomings%20of%0ASMOTE-based%20approaches%20in%20the%20construction%20of%20imbalanced%20graphs.%20Furthermore%2C%0Awe%20introduce%20VIGraph%2C%20a%20simple%20yet%20effective%20generative%20SSL%20approach%20that%0Arelies%20on%20the%20Variational%20GAE%20as%20the%20fundamental%20model.%20VIGraph%20strictly%0Aadheres%20to%20the%20concept%20of%20imbalance%20when%20constructing%20imbalanced%20graphs%20and%0Ainnovatively%20leverages%20the%20variational%20inference%20%28VI%29%20ability%20of%20Variational%0AGAE%20to%20generate%20nodes%20for%20minority%20classes.%20VIGraph%20introduces%20comprehensive%0Atraining%20strategies%2C%20including%20cross-view%20contrastive%20learning%20at%20the%20decoding%0Aphase%20to%20capture%20semantic%20knowledge%2C%20adjacency%20matrix%20reconstruction%20to%0Apreserve%20graph%20structure%2C%20and%20alignment%20strategy%20to%20ensure%20stable%20training.%0AVIGraph%20can%20generate%20high-quality%20nodes%20directly%20usable%20for%20classification%2C%0Aeliminating%20the%20need%20to%20integrate%20the%20generated%20nodes%20back%20to%20the%20graph%20as%20well%0Aas%20additional%20retraining%20found%20in%20SMOTE-based%20methods.%20We%20conduct%20extensive%0Aexperiments%2C%20results%20from%20which%20demonstrate%20the%20superiority%20and%20generality%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01191v2&entry.124074799=Read"},
{"title": "Attention Calibration for Disentangled Text-to-Image Personalization", "author": "Yanbing Zhang and Mengping Yang and Qin Zhou and Zhe Wang", "abstract": "  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n", "link": "http://arxiv.org/abs/2403.18551v1", "date": "2024-03-27", "relevancy": 2.5472, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6482}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6432}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6258}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization&body=Title%3A%20Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization%0AAuthor%3A%20Yanbing%20Zhang%20and%20Mengping%20Yang%20and%20Qin%20Zhou%20and%20Zhe%20Wang%0AAbstract%3A%20%20%20Recent%20thrilling%20progress%20in%20large-scale%20text-to-image%20%28T2I%29%20models%20has%0Aunlocked%20unprecedented%20synthesis%20quality%20of%20AI-generated%20content%20%28AIGC%29%0Aincluding%20image%20generation%2C%203D%20and%20video%20composition.%20Further%2C%20personalized%0Atechniques%20enable%20appealing%20customized%20production%20of%20a%20novel%20concept%20given%20only%0Aseveral%20images%20as%20reference.%20However%2C%20an%20intriguing%20problem%20persists%3A%20Is%20it%0Apossible%20to%20capture%20multiple%2C%20novel%20concepts%20from%20one%20single%20reference%20image%3F%0AIn%20this%20paper%2C%20we%20identify%20that%20existing%20approaches%20fail%20to%20preserve%20visual%0Aconsistency%20with%20the%20reference%20image%20and%20eliminate%20cross-influence%20from%0Aconcepts.%20To%20alleviate%20this%2C%20we%20propose%20an%20attention%20calibration%20mechanism%20to%0Aimprove%20the%20concept-level%20understanding%20of%20the%20T2I%20model.%20Specifically%2C%20we%0Afirst%20introduce%20new%20learnable%20modifiers%20bound%20with%20classes%20to%20capture%0Aattributes%20of%20multiple%20concepts.%20Then%2C%20the%20classes%20are%20separated%20and%0Astrengthened%20following%20the%20activation%20of%20the%20cross-attention%20operation%2C%0Aensuring%20comprehensive%20and%20self-contained%20concepts.%20Additionally%2C%20we%20suppress%0Athe%20attention%20activation%20of%20different%20classes%20to%20mitigate%20mutual%20influence%0Aamong%20concepts.%20Together%2C%20our%20proposed%20method%2C%20dubbed%20DisenDiff%2C%20can%20learn%0Adisentangled%20multiple%20concepts%20from%20one%20single%20image%20and%20produce%20novel%0Acustomized%20images%20with%20learned%20concepts.%20We%20demonstrate%20that%20our%20method%0Aoutperforms%20the%20current%20state%20of%20the%20art%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%20More%20importantly%2C%20our%20proposed%20techniques%20are%20compatible%20with%20LoRA%0Aand%20inpainting%20pipelines%2C%20enabling%20more%20interactive%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18551v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Calibration%20for%20Disentangled%20Text-to-Image%20Personalization&entry.906535625=Yanbing%20Zhang%20and%20Mengping%20Yang%20and%20Qin%20Zhou%20and%20Zhe%20Wang&entry.1292438233=%20%20Recent%20thrilling%20progress%20in%20large-scale%20text-to-image%20%28T2I%29%20models%20has%0Aunlocked%20unprecedented%20synthesis%20quality%20of%20AI-generated%20content%20%28AIGC%29%0Aincluding%20image%20generation%2C%203D%20and%20video%20composition.%20Further%2C%20personalized%0Atechniques%20enable%20appealing%20customized%20production%20of%20a%20novel%20concept%20given%20only%0Aseveral%20images%20as%20reference.%20However%2C%20an%20intriguing%20problem%20persists%3A%20Is%20it%0Apossible%20to%20capture%20multiple%2C%20novel%20concepts%20from%20one%20single%20reference%20image%3F%0AIn%20this%20paper%2C%20we%20identify%20that%20existing%20approaches%20fail%20to%20preserve%20visual%0Aconsistency%20with%20the%20reference%20image%20and%20eliminate%20cross-influence%20from%0Aconcepts.%20To%20alleviate%20this%2C%20we%20propose%20an%20attention%20calibration%20mechanism%20to%0Aimprove%20the%20concept-level%20understanding%20of%20the%20T2I%20model.%20Specifically%2C%20we%0Afirst%20introduce%20new%20learnable%20modifiers%20bound%20with%20classes%20to%20capture%0Aattributes%20of%20multiple%20concepts.%20Then%2C%20the%20classes%20are%20separated%20and%0Astrengthened%20following%20the%20activation%20of%20the%20cross-attention%20operation%2C%0Aensuring%20comprehensive%20and%20self-contained%20concepts.%20Additionally%2C%20we%20suppress%0Athe%20attention%20activation%20of%20different%20classes%20to%20mitigate%20mutual%20influence%0Aamong%20concepts.%20Together%2C%20our%20proposed%20method%2C%20dubbed%20DisenDiff%2C%20can%20learn%0Adisentangled%20multiple%20concepts%20from%20one%20single%20image%20and%20produce%20novel%0Acustomized%20images%20with%20learned%20concepts.%20We%20demonstrate%20that%20our%20method%0Aoutperforms%20the%20current%20state%20of%20the%20art%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%20More%20importantly%2C%20our%20proposed%20techniques%20are%20compatible%20with%20LoRA%0Aand%20inpainting%20pipelines%2C%20enabling%20more%20interactive%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18551v1&entry.124074799=Read"},
{"title": "SteinGen: Generating Fidelitous and Diverse Graph Samples", "author": "Gesine Reinert and Wenkai Xu", "abstract": "  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n", "link": "http://arxiv.org/abs/2403.18578v1", "date": "2024-03-27", "relevancy": 2.5157, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5304}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4913}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SteinGen%3A%20Generating%20Fidelitous%20and%20Diverse%20Graph%20Samples&body=Title%3A%20SteinGen%3A%20Generating%20Fidelitous%20and%20Diverse%20Graph%20Samples%0AAuthor%3A%20Gesine%20Reinert%20and%20Wenkai%20Xu%0AAbstract%3A%20%20%20Generating%20graphs%20that%20preserve%20characteristic%20structures%20while%20promoting%0Asample%20diversity%20can%20be%20challenging%2C%20especially%20when%20the%20number%20of%20graph%0Aobservations%20is%20small.%20Here%2C%20we%20tackle%20the%20problem%20of%20graph%20generation%20from%0Aonly%20one%20observed%20graph.%20The%20classical%20approach%20of%20graph%20generation%20from%0Aparametric%20models%20relies%20on%20the%20estimation%20of%20parameters%2C%20which%20can%20be%0Ainconsistent%20or%20expensive%20to%20compute%20due%20to%20intractable%20normalisation%0Aconstants.%20Generative%20modelling%20based%20on%20machine%20learning%20techniques%20to%0Agenerate%20high-quality%20graph%20samples%20avoids%20parameter%20estimation%20but%20usually%0Arequires%20abundant%20training%20samples.%20Our%20proposed%20generating%20procedure%2C%0ASteinGen%2C%20which%20is%20phrased%20in%20the%20setting%20of%20graphs%20as%20realisations%20of%0Aexponential%20random%20graph%20models%2C%20combines%20ideas%20from%20Stein%27s%20method%20and%20MCMC%20by%0Aemploying%20Markovian%20dynamics%20which%20are%20based%20on%20a%20Stein%20operator%20for%20the%20target%0Amodel.%20SteinGen%20uses%20the%20Glauber%20dynamics%20associated%20with%20an%20estimated%20Stein%0Aoperator%20to%20generate%20a%20sample%2C%20and%20re-estimates%20the%20Stein%20operator%20from%20the%0Asample%20after%20every%20sampling%20step.%20We%20show%20that%20on%20a%20class%20of%20exponential%20random%0Agraph%20models%20this%20novel%20%22estimation%20and%20re-estimation%22%20generation%20strategy%0Ayields%20high%20distributional%20similarity%20%28high%20fidelity%29%20to%20the%20original%20data%2C%0Acombined%20with%20high%20sample%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18578v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteinGen%3A%20Generating%20Fidelitous%20and%20Diverse%20Graph%20Samples&entry.906535625=Gesine%20Reinert%20and%20Wenkai%20Xu&entry.1292438233=%20%20Generating%20graphs%20that%20preserve%20characteristic%20structures%20while%20promoting%0Asample%20diversity%20can%20be%20challenging%2C%20especially%20when%20the%20number%20of%20graph%0Aobservations%20is%20small.%20Here%2C%20we%20tackle%20the%20problem%20of%20graph%20generation%20from%0Aonly%20one%20observed%20graph.%20The%20classical%20approach%20of%20graph%20generation%20from%0Aparametric%20models%20relies%20on%20the%20estimation%20of%20parameters%2C%20which%20can%20be%0Ainconsistent%20or%20expensive%20to%20compute%20due%20to%20intractable%20normalisation%0Aconstants.%20Generative%20modelling%20based%20on%20machine%20learning%20techniques%20to%0Agenerate%20high-quality%20graph%20samples%20avoids%20parameter%20estimation%20but%20usually%0Arequires%20abundant%20training%20samples.%20Our%20proposed%20generating%20procedure%2C%0ASteinGen%2C%20which%20is%20phrased%20in%20the%20setting%20of%20graphs%20as%20realisations%20of%0Aexponential%20random%20graph%20models%2C%20combines%20ideas%20from%20Stein%27s%20method%20and%20MCMC%20by%0Aemploying%20Markovian%20dynamics%20which%20are%20based%20on%20a%20Stein%20operator%20for%20the%20target%0Amodel.%20SteinGen%20uses%20the%20Glauber%20dynamics%20associated%20with%20an%20estimated%20Stein%0Aoperator%20to%20generate%20a%20sample%2C%20and%20re-estimates%20the%20Stein%20operator%20from%20the%0Asample%20after%20every%20sampling%20step.%20We%20show%20that%20on%20a%20class%20of%20exponential%20random%0Agraph%20models%20this%20novel%20%22estimation%20and%20re-estimation%22%20generation%20strategy%0Ayields%20high%20distributional%20similarity%20%28high%20fidelity%29%20to%20the%20original%20data%2C%0Acombined%20with%20high%20sample%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18578v1&entry.124074799=Read"},
{"title": "OrCo: Towards Better Generalization via Orthogonality and Contrast for\n  Few-Shot Class-Incremental Learning", "author": "Noor Ahmed and Anna Kukleva and Bernt Schiele", "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which\nthe problem space expands with limited data. FSCIL methods inherently face the\nchallenge of catastrophic forgetting as data arrives incrementally, making\nmodels susceptible to overwriting previously acquired knowledge. Moreover,\ngiven the scarcity of labeled samples available at any given time, models may\nbe prone to overfitting and find it challenging to strike a balance between\nextensive pretraining and the limited incremental data. To address these\nchallenges, we propose the OrCo framework built on two core principles:\nfeatures' orthogonality in the representation space, and contrastive learning.\nIn particular, we improve the generalization of the embedding space by\nemploying a combination of supervised and self-supervised contrastive losses\nduring the pretraining phase. Additionally, we introduce OrCo loss to address\nchallenges arising from data limitations during incremental sessions. Through\nfeature space perturbations and orthogonality between classes, the OrCo loss\nmaximizes margins and reserves space for the following incremental data. This,\nin turn, ensures the accommodation of incoming classes in the feature space\nwithout compromising previously acquired knowledge. Our experimental results\nshowcase state-of-the-art performance across three benchmark datasets,\nincluding mini-ImageNet, CIFAR100, and CUB datasets. Code is available at\nhttps://github.com/noorahmedds/OrCo\n", "link": "http://arxiv.org/abs/2403.18550v1", "date": "2024-03-27", "relevancy": 2.4973, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4843}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OrCo%3A%20Towards%20Better%20Generalization%20via%20Orthogonality%20and%20Contrast%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20OrCo%3A%20Towards%20Better%20Generalization%20via%20Orthogonality%20and%20Contrast%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Noor%20Ahmed%20and%20Anna%20Kukleva%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20introduces%20a%20paradigm%20in%20which%0Athe%20problem%20space%20expands%20with%20limited%20data.%20FSCIL%20methods%20inherently%20face%20the%0Achallenge%20of%20catastrophic%20forgetting%20as%20data%20arrives%20incrementally%2C%20making%0Amodels%20susceptible%20to%20overwriting%20previously%20acquired%20knowledge.%20Moreover%2C%0Agiven%20the%20scarcity%20of%20labeled%20samples%20available%20at%20any%20given%20time%2C%20models%20may%0Abe%20prone%20to%20overfitting%20and%20find%20it%20challenging%20to%20strike%20a%20balance%20between%0Aextensive%20pretraining%20and%20the%20limited%20incremental%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20OrCo%20framework%20built%20on%20two%20core%20principles%3A%0Afeatures%27%20orthogonality%20in%20the%20representation%20space%2C%20and%20contrastive%20learning.%0AIn%20particular%2C%20we%20improve%20the%20generalization%20of%20the%20embedding%20space%20by%0Aemploying%20a%20combination%20of%20supervised%20and%20self-supervised%20contrastive%20losses%0Aduring%20the%20pretraining%20phase.%20Additionally%2C%20we%20introduce%20OrCo%20loss%20to%20address%0Achallenges%20arising%20from%20data%20limitations%20during%20incremental%20sessions.%20Through%0Afeature%20space%20perturbations%20and%20orthogonality%20between%20classes%2C%20the%20OrCo%20loss%0Amaximizes%20margins%20and%20reserves%20space%20for%20the%20following%20incremental%20data.%20This%2C%0Ain%20turn%2C%20ensures%20the%20accommodation%20of%20incoming%20classes%20in%20the%20feature%20space%0Awithout%20compromising%20previously%20acquired%20knowledge.%20Our%20experimental%20results%0Ashowcase%20state-of-the-art%20performance%20across%20three%20benchmark%20datasets%2C%0Aincluding%20mini-ImageNet%2C%20CIFAR100%2C%20and%20CUB%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/noorahmedds/OrCo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18550v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OrCo%3A%20Towards%20Better%20Generalization%20via%20Orthogonality%20and%20Contrast%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Noor%20Ahmed%20and%20Anna%20Kukleva%20and%20Bernt%20Schiele&entry.1292438233=%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20introduces%20a%20paradigm%20in%20which%0Athe%20problem%20space%20expands%20with%20limited%20data.%20FSCIL%20methods%20inherently%20face%20the%0Achallenge%20of%20catastrophic%20forgetting%20as%20data%20arrives%20incrementally%2C%20making%0Amodels%20susceptible%20to%20overwriting%20previously%20acquired%20knowledge.%20Moreover%2C%0Agiven%20the%20scarcity%20of%20labeled%20samples%20available%20at%20any%20given%20time%2C%20models%20may%0Abe%20prone%20to%20overfitting%20and%20find%20it%20challenging%20to%20strike%20a%20balance%20between%0Aextensive%20pretraining%20and%20the%20limited%20incremental%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20OrCo%20framework%20built%20on%20two%20core%20principles%3A%0Afeatures%27%20orthogonality%20in%20the%20representation%20space%2C%20and%20contrastive%20learning.%0AIn%20particular%2C%20we%20improve%20the%20generalization%20of%20the%20embedding%20space%20by%0Aemploying%20a%20combination%20of%20supervised%20and%20self-supervised%20contrastive%20losses%0Aduring%20the%20pretraining%20phase.%20Additionally%2C%20we%20introduce%20OrCo%20loss%20to%20address%0Achallenges%20arising%20from%20data%20limitations%20during%20incremental%20sessions.%20Through%0Afeature%20space%20perturbations%20and%20orthogonality%20between%20classes%2C%20the%20OrCo%20loss%0Amaximizes%20margins%20and%20reserves%20space%20for%20the%20following%20incremental%20data.%20This%2C%0Ain%20turn%2C%20ensures%20the%20accommodation%20of%20incoming%20classes%20in%20the%20feature%20space%0Awithout%20compromising%20previously%20acquired%20knowledge.%20Our%20experimental%20results%0Ashowcase%20state-of-the-art%20performance%20across%20three%20benchmark%20datasets%2C%0Aincluding%20mini-ImageNet%2C%20CIFAR100%2C%20and%20CUB%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/noorahmedds/OrCo%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18550v1&entry.124074799=Read"},
{"title": "Point, Segment and Count: A Generalized Framework for Object Counting", "author": "Zhizhong Huang and Mingliang Dai and Yi Zhang and Junping Zhang and Hongming Shan", "abstract": "  Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. In this paper, we propose a generalized framework for both few-shot\nand zero-shot object counting based on detection. Our framework combines the\nsuperior advantages of two foundation models without compromising their\nzero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask\nproposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate\nobject counts. However, this strategy meets the obstacles of efficiency\noverhead and the small crowded objects that cannot be localized and\ndistinguished. To address these issues, our framework, termed PseCo, follows\nthree steps: point, segment, and count. Specifically, we first propose a\nclass-agnostic object localization to provide accurate but least point prompts\nfor SAM, which consequently not only reduces computation costs but also avoids\nmissing small objects. Furthermore, we propose a generalized object\nclassification that leverages CLIP image/text embeddings as the classifier,\nfollowing a hierarchical knowledge distillation to obtain discriminative\nclassifications among hierarchical mask proposals. Extensive experimental\nresults on FSC-147, COCO, and LVIS demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection. Code: https://github.com/Hzzone/PseCo\n", "link": "http://arxiv.org/abs/2311.12386v3", "date": "2024-03-27", "relevancy": 2.4792, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point%2C%20Segment%20and%20Count%3A%20A%20Generalized%20Framework%20for%20Object%20Counting&body=Title%3A%20Point%2C%20Segment%20and%20Count%3A%20A%20Generalized%20Framework%20for%20Object%20Counting%0AAuthor%3A%20Zhizhong%20Huang%20and%20Mingliang%20Dai%20and%20Yi%20Zhang%20and%20Junping%20Zhang%20and%20Hongming%20Shan%0AAbstract%3A%20%20%20Class-agnostic%20object%20counting%20aims%20to%20count%20all%20objects%20in%20an%20image%20with%0Arespect%20to%20example%20boxes%20or%20class%20names%2C%20%5Cemph%7Ba.k.a%7D%20few-shot%20and%20zero-shot%0Acounting.%20In%20this%20paper%2C%20we%20propose%20a%20generalized%20framework%20for%20both%20few-shot%0Aand%20zero-shot%20object%20counting%20based%20on%20detection.%20Our%20framework%20combines%20the%0Asuperior%20advantages%20of%20two%20foundation%20models%20without%20compromising%20their%0Azero-shot%20capability%3A%20%28%5Ctextbf%7Bi%7D%29%20SAM%20to%20segment%20all%20possible%20objects%20as%20mask%0Aproposals%2C%20and%20%28%5Ctextbf%7Bii%7D%29%20CLIP%20to%20classify%20proposals%20to%20obtain%20accurate%0Aobject%20counts.%20However%2C%20this%20strategy%20meets%20the%20obstacles%20of%20efficiency%0Aoverhead%20and%20the%20small%20crowded%20objects%20that%20cannot%20be%20localized%20and%0Adistinguished.%20To%20address%20these%20issues%2C%20our%20framework%2C%20termed%20PseCo%2C%20follows%0Athree%20steps%3A%20point%2C%20segment%2C%20and%20count.%20Specifically%2C%20we%20first%20propose%20a%0Aclass-agnostic%20object%20localization%20to%20provide%20accurate%20but%20least%20point%20prompts%0Afor%20SAM%2C%20which%20consequently%20not%20only%20reduces%20computation%20costs%20but%20also%20avoids%0Amissing%20small%20objects.%20Furthermore%2C%20we%20propose%20a%20generalized%20object%0Aclassification%20that%20leverages%20CLIP%20image/text%20embeddings%20as%20the%20classifier%2C%0Afollowing%20a%20hierarchical%20knowledge%20distillation%20to%20obtain%20discriminative%0Aclassifications%20among%20hierarchical%20mask%20proposals.%20Extensive%20experimental%0Aresults%20on%20FSC-147%2C%20COCO%2C%20and%20LVIS%20demonstrate%20that%20PseCo%20achieves%0Astate-of-the-art%20performance%20in%20both%20few-shot/zero-shot%20object%0Acounting/detection.%20Code%3A%20https%3A//github.com/Hzzone/PseCo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12386v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%2C%20Segment%20and%20Count%3A%20A%20Generalized%20Framework%20for%20Object%20Counting&entry.906535625=Zhizhong%20Huang%20and%20Mingliang%20Dai%20and%20Yi%20Zhang%20and%20Junping%20Zhang%20and%20Hongming%20Shan&entry.1292438233=%20%20Class-agnostic%20object%20counting%20aims%20to%20count%20all%20objects%20in%20an%20image%20with%0Arespect%20to%20example%20boxes%20or%20class%20names%2C%20%5Cemph%7Ba.k.a%7D%20few-shot%20and%20zero-shot%0Acounting.%20In%20this%20paper%2C%20we%20propose%20a%20generalized%20framework%20for%20both%20few-shot%0Aand%20zero-shot%20object%20counting%20based%20on%20detection.%20Our%20framework%20combines%20the%0Asuperior%20advantages%20of%20two%20foundation%20models%20without%20compromising%20their%0Azero-shot%20capability%3A%20%28%5Ctextbf%7Bi%7D%29%20SAM%20to%20segment%20all%20possible%20objects%20as%20mask%0Aproposals%2C%20and%20%28%5Ctextbf%7Bii%7D%29%20CLIP%20to%20classify%20proposals%20to%20obtain%20accurate%0Aobject%20counts.%20However%2C%20this%20strategy%20meets%20the%20obstacles%20of%20efficiency%0Aoverhead%20and%20the%20small%20crowded%20objects%20that%20cannot%20be%20localized%20and%0Adistinguished.%20To%20address%20these%20issues%2C%20our%20framework%2C%20termed%20PseCo%2C%20follows%0Athree%20steps%3A%20point%2C%20segment%2C%20and%20count.%20Specifically%2C%20we%20first%20propose%20a%0Aclass-agnostic%20object%20localization%20to%20provide%20accurate%20but%20least%20point%20prompts%0Afor%20SAM%2C%20which%20consequently%20not%20only%20reduces%20computation%20costs%20but%20also%20avoids%0Amissing%20small%20objects.%20Furthermore%2C%20we%20propose%20a%20generalized%20object%0Aclassification%20that%20leverages%20CLIP%20image/text%20embeddings%20as%20the%20classifier%2C%0Afollowing%20a%20hierarchical%20knowledge%20distillation%20to%20obtain%20discriminative%0Aclassifications%20among%20hierarchical%20mask%20proposals.%20Extensive%20experimental%0Aresults%20on%20FSC-147%2C%20COCO%2C%20and%20LVIS%20demonstrate%20that%20PseCo%20achieves%0Astate-of-the-art%20performance%20in%20both%20few-shot/zero-shot%20object%0Acounting/detection.%20Code%3A%20https%3A//github.com/Hzzone/PseCo%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12386v3&entry.124074799=Read"},
{"title": "LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition", "author": "Sayanton V. Dibbo and Juston S. Moore and Garrett T. Kenyon and Michael A. Teti", "abstract": "  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n", "link": "http://arxiv.org/abs/2308.12882v2", "date": "2024-03-27", "relevancy": 2.4537, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5029}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4738}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LCANets%2B%2B%3A%20Robust%20Audio%20Classification%20using%20Multi-layer%20Neural%20Networks%0A%20%20with%20Lateral%20Competition&body=Title%3A%20LCANets%2B%2B%3A%20Robust%20Audio%20Classification%20using%20Multi-layer%20Neural%20Networks%0A%20%20with%20Lateral%20Competition%0AAuthor%3A%20Sayanton%20V.%20Dibbo%20and%20Juston%20S.%20Moore%20and%20Garrett%20T.%20Kenyon%20and%20Michael%20A.%20Teti%0AAbstract%3A%20%20%20Audio%20classification%20aims%20at%20recognizing%20audio%20signals%2C%20including%20speech%0Acommands%20or%20sound%20events.%20However%2C%20current%20audio%20classifiers%20are%20susceptible%20to%0Aperturbations%20and%20adversarial%20attacks.%20In%20addition%2C%20real-world%20audio%0Aclassification%20tasks%20often%20suffer%20from%20limited%20labeled%20data.%20To%20help%20bridge%0Athese%20gaps%2C%20previous%20work%20developed%20neuro-inspired%20convolutional%20neural%0Anetworks%20%28CNNs%29%20with%20sparse%20coding%20via%20the%20Locally%20Competitive%20Algorithm%20%28LCA%29%0Ain%20the%20first%20layer%20%28i.e.%2C%20LCANets%29%20for%20computer%20vision.%20LCANets%20learn%20in%20a%0Acombination%20of%20supervised%20and%20unsupervised%20learning%2C%20reducing%20dependency%20on%0Alabeled%20samples.%20Motivated%20by%20the%20fact%20that%20auditory%20cortex%20is%20also%20sparse%2C%20we%0Aextend%20LCANets%20to%20audio%20recognition%20tasks%20and%20introduce%20LCANets%2B%2B%2C%20which%20are%0ACNNs%20that%20perform%20sparse%20coding%20in%20multiple%20layers%20via%20LCA.%20We%20demonstrate%20that%0ALCANets%2B%2B%20are%20more%20robust%20than%20standard%20CNNs%20and%20LCANets%20against%20perturbations%2C%0Ae.g.%2C%20background%20noise%2C%20as%20well%20as%20black-box%20and%20white-box%20attacks%2C%20e.g.%2C%0Aevasion%20and%20fast%20gradient%20sign%20%28FGSM%29%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12882v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCANets%2B%2B%3A%20Robust%20Audio%20Classification%20using%20Multi-layer%20Neural%20Networks%0A%20%20with%20Lateral%20Competition&entry.906535625=Sayanton%20V.%20Dibbo%20and%20Juston%20S.%20Moore%20and%20Garrett%20T.%20Kenyon%20and%20Michael%20A.%20Teti&entry.1292438233=%20%20Audio%20classification%20aims%20at%20recognizing%20audio%20signals%2C%20including%20speech%0Acommands%20or%20sound%20events.%20However%2C%20current%20audio%20classifiers%20are%20susceptible%20to%0Aperturbations%20and%20adversarial%20attacks.%20In%20addition%2C%20real-world%20audio%0Aclassification%20tasks%20often%20suffer%20from%20limited%20labeled%20data.%20To%20help%20bridge%0Athese%20gaps%2C%20previous%20work%20developed%20neuro-inspired%20convolutional%20neural%0Anetworks%20%28CNNs%29%20with%20sparse%20coding%20via%20the%20Locally%20Competitive%20Algorithm%20%28LCA%29%0Ain%20the%20first%20layer%20%28i.e.%2C%20LCANets%29%20for%20computer%20vision.%20LCANets%20learn%20in%20a%0Acombination%20of%20supervised%20and%20unsupervised%20learning%2C%20reducing%20dependency%20on%0Alabeled%20samples.%20Motivated%20by%20the%20fact%20that%20auditory%20cortex%20is%20also%20sparse%2C%20we%0Aextend%20LCANets%20to%20audio%20recognition%20tasks%20and%20introduce%20LCANets%2B%2B%2C%20which%20are%0ACNNs%20that%20perform%20sparse%20coding%20in%20multiple%20layers%20via%20LCA.%20We%20demonstrate%20that%0ALCANets%2B%2B%20are%20more%20robust%20than%20standard%20CNNs%20and%20LCANets%20against%20perturbations%2C%0Ae.g.%2C%20background%20noise%2C%20as%20well%20as%20black-box%20and%20white-box%20attacks%2C%20e.g.%2C%0Aevasion%20and%20fast%20gradient%20sign%20%28FGSM%29%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12882v2&entry.124074799=Read"},
{"title": "FedSN: A Novel Federated Learning Framework over LEO Satellite Networks", "author": "Zheng Lin and Zhe Chen and Zihan Fang and Xianhao Chen and Xiong Wang and Yue Gao", "abstract": "  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n", "link": "http://arxiv.org/abs/2311.01483v3", "date": "2024-03-27", "relevancy": 2.4183, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5003}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4789}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4718}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedSN%3A%20A%20Novel%20Federated%20Learning%20Framework%20over%20LEO%20Satellite%20Networks&body=Title%3A%20FedSN%3A%20A%20Novel%20Federated%20Learning%20Framework%20over%20LEO%20Satellite%20Networks%0AAuthor%3A%20Zheng%20Lin%20and%20Zhe%20Chen%20and%20Zihan%20Fang%20and%20Xianhao%20Chen%20and%20Xiong%20Wang%20and%20Yue%20Gao%0AAbstract%3A%20%20%20Recently%2C%20a%20large%20number%20of%20Low%20Earth%20Orbit%20%28LEO%29%20satellites%20have%20been%0Alaunched%20and%20deployed%20successfully%20in%20space%20by%20commercial%20companies%2C%20such%20as%0ASpaceX.%20Due%20to%20multimodal%20sensors%20equipped%20by%20the%20LEO%20satellites%2C%20they%20serve%0Anot%20only%20for%20communication%20but%20also%20for%20various%20machine%20learning%20applications%2C%0Asuch%20as%20space%20modulation%20recognition%2C%20remote%20sensing%20image%20classification%2C%20etc.%0AHowever%2C%20the%20ground%20station%20%28GS%29%20may%20be%20incapable%20of%20downloading%20such%20a%20large%0Avolume%20of%20raw%20sensing%20data%20for%20centralized%20model%20training%20due%20to%20the%20limited%0Acontact%20time%20with%20LEO%20satellites%20%28e.g.%205%20minutes%29.%20Therefore%2C%20federated%0Alearning%20%28FL%29%20has%20emerged%20as%20the%20promising%20solution%20to%20address%20this%20problem%20via%0Aon-device%20training.%20Unfortunately%2C%20to%20enable%20FL%20on%20LEO%20satellites%2C%20we%20still%0Aface%20three%20critical%20challenges%20that%20are%20i%29%20heterogeneous%20computing%20and%20memory%0Acapabilities%2C%20ii%29%20limited%20uplink%20rate%2C%20and%20iii%29%20model%20staleness.%20To%20this%20end%2C%0Awe%20propose%20FedSN%20as%20a%20general%20FL%20framework%20to%20tackle%20the%20above%20challenges%2C%20and%0Afully%20explore%20data%20diversity%20on%20LEO%20satellites.%20Specifically%2C%20we%20first%20present%0Aa%20novel%20sub-structure%20scheme%20to%20enable%20heterogeneous%20local%20model%20training%0Aconsidering%20different%20computing%2C%20memory%2C%20and%20communication%20constraints%20on%20LEO%0Asatellites.%20Additionally%2C%20we%20propose%20a%20pseudo-synchronous%20model%20aggregation%0Astrategy%20to%20dynamically%20schedule%20model%20aggregation%20for%20compensating%20model%0Astaleness.%20To%20further%20demonstrate%20the%20effectiveness%20of%20the%20FedSN%2C%20we%20evaluate%0Ait%20using%20space%20modulation%20recognition%20and%20remote%20sensing%20image%20classification%0Atasks%20by%20leveraging%20the%20data%20from%20real-world%20satellite%20networks.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20FedSN%20framework%20achieves%20higher%20accuracy%2C%0Alower%20computing%2C%20and%20communication%20overhead%20than%20the%20state-of-the-art%0Abenchmarks%20and%20the%20effectiveness%20of%20each%20components%20in%20FedSN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01483v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSN%3A%20A%20Novel%20Federated%20Learning%20Framework%20over%20LEO%20Satellite%20Networks&entry.906535625=Zheng%20Lin%20and%20Zhe%20Chen%20and%20Zihan%20Fang%20and%20Xianhao%20Chen%20and%20Xiong%20Wang%20and%20Yue%20Gao&entry.1292438233=%20%20Recently%2C%20a%20large%20number%20of%20Low%20Earth%20Orbit%20%28LEO%29%20satellites%20have%20been%0Alaunched%20and%20deployed%20successfully%20in%20space%20by%20commercial%20companies%2C%20such%20as%0ASpaceX.%20Due%20to%20multimodal%20sensors%20equipped%20by%20the%20LEO%20satellites%2C%20they%20serve%0Anot%20only%20for%20communication%20but%20also%20for%20various%20machine%20learning%20applications%2C%0Asuch%20as%20space%20modulation%20recognition%2C%20remote%20sensing%20image%20classification%2C%20etc.%0AHowever%2C%20the%20ground%20station%20%28GS%29%20may%20be%20incapable%20of%20downloading%20such%20a%20large%0Avolume%20of%20raw%20sensing%20data%20for%20centralized%20model%20training%20due%20to%20the%20limited%0Acontact%20time%20with%20LEO%20satellites%20%28e.g.%205%20minutes%29.%20Therefore%2C%20federated%0Alearning%20%28FL%29%20has%20emerged%20as%20the%20promising%20solution%20to%20address%20this%20problem%20via%0Aon-device%20training.%20Unfortunately%2C%20to%20enable%20FL%20on%20LEO%20satellites%2C%20we%20still%0Aface%20three%20critical%20challenges%20that%20are%20i%29%20heterogeneous%20computing%20and%20memory%0Acapabilities%2C%20ii%29%20limited%20uplink%20rate%2C%20and%20iii%29%20model%20staleness.%20To%20this%20end%2C%0Awe%20propose%20FedSN%20as%20a%20general%20FL%20framework%20to%20tackle%20the%20above%20challenges%2C%20and%0Afully%20explore%20data%20diversity%20on%20LEO%20satellites.%20Specifically%2C%20we%20first%20present%0Aa%20novel%20sub-structure%20scheme%20to%20enable%20heterogeneous%20local%20model%20training%0Aconsidering%20different%20computing%2C%20memory%2C%20and%20communication%20constraints%20on%20LEO%0Asatellites.%20Additionally%2C%20we%20propose%20a%20pseudo-synchronous%20model%20aggregation%0Astrategy%20to%20dynamically%20schedule%20model%20aggregation%20for%20compensating%20model%0Astaleness.%20To%20further%20demonstrate%20the%20effectiveness%20of%20the%20FedSN%2C%20we%20evaluate%0Ait%20using%20space%20modulation%20recognition%20and%20remote%20sensing%20image%20classification%0Atasks%20by%20leveraging%20the%20data%20from%20real-world%20satellite%20networks.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20FedSN%20framework%20achieves%20higher%20accuracy%2C%0Alower%20computing%2C%20and%20communication%20overhead%20than%20the%20state-of-the-art%0Abenchmarks%20and%20the%20effectiveness%20of%20each%20components%20in%20FedSN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01483v3&entry.124074799=Read"},
{"title": "Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes", "author": "Siang Chen and Wei Tang and Pengwei Xie and Wenming Yang and Guijin Wang", "abstract": "  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n", "link": "http://arxiv.org/abs/2403.18546v1", "date": "2024-03-27", "relevancy": 2.4133, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Heatmap-Guided%206-Dof%20Grasp%20Detection%20in%20Cluttered%20Scenes&body=Title%3A%20Efficient%20Heatmap-Guided%206-Dof%20Grasp%20Detection%20in%20Cluttered%20Scenes%0AAuthor%3A%20Siang%20Chen%20and%20Wei%20Tang%20and%20Pengwei%20Xie%20and%20Wenming%20Yang%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20Fast%20and%20robust%20object%20grasping%20in%20clutter%20is%20a%20crucial%20component%20of%0Arobotics.%20Most%20current%20works%20resort%20to%20the%20whole%20observed%20point%20cloud%20for%206-Dof%0Agrasp%20generation%2C%20ignoring%20the%20guidance%20information%20excavated%20from%20global%0Asemantics%2C%20thus%20limiting%20high-quality%20grasp%20generation%20and%20real-time%0Aperformance.%20In%20this%20work%2C%20we%20show%20that%20the%20widely%20used%20heatmaps%20are%0Aunderestimated%20in%20the%20efficiency%20of%206-Dof%20grasp%20generation.%20Therefore%2C%20we%0Apropose%20an%20effective%20local%20grasp%20generator%20combined%20with%20grasp%20heatmaps%20as%0Aguidance%2C%20which%20infers%20in%20a%20global-to-local%20semantic-to-point%20way.%0ASpecifically%2C%20Gaussian%20encoding%20and%20the%20grid-based%20strategy%20are%20applied%20to%0Apredict%20grasp%20heatmaps%20as%20guidance%20to%20aggregate%20local%20points%20into%20graspable%0Aregions%20and%20provide%20global%20semantic%20information.%20Further%2C%20a%20novel%20non-uniform%0Aanchor%20sampling%20mechanism%20is%20designed%20to%20improve%20grasp%20accuracy%20and%20diversity.%0ABenefiting%20from%20the%20high-efficiency%20encoding%20in%20the%20image%20space%20and%20focusing%20on%0Apoints%20in%20local%20graspable%20regions%2C%20our%20framework%20can%20perform%20high-quality%20grasp%0Adetection%20in%20real-time%20and%20achieve%20state-of-the-art%20results.%20In%20addition%2C%20real%0Arobot%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20a%20success%0Arate%20of%2094%25%20and%20a%20clutter%20completion%20rate%20of%20100%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/THU-VCLab/HGGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18546v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Heatmap-Guided%206-Dof%20Grasp%20Detection%20in%20Cluttered%20Scenes&entry.906535625=Siang%20Chen%20and%20Wei%20Tang%20and%20Pengwei%20Xie%20and%20Wenming%20Yang%20and%20Guijin%20Wang&entry.1292438233=%20%20Fast%20and%20robust%20object%20grasping%20in%20clutter%20is%20a%20crucial%20component%20of%0Arobotics.%20Most%20current%20works%20resort%20to%20the%20whole%20observed%20point%20cloud%20for%206-Dof%0Agrasp%20generation%2C%20ignoring%20the%20guidance%20information%20excavated%20from%20global%0Asemantics%2C%20thus%20limiting%20high-quality%20grasp%20generation%20and%20real-time%0Aperformance.%20In%20this%20work%2C%20we%20show%20that%20the%20widely%20used%20heatmaps%20are%0Aunderestimated%20in%20the%20efficiency%20of%206-Dof%20grasp%20generation.%20Therefore%2C%20we%0Apropose%20an%20effective%20local%20grasp%20generator%20combined%20with%20grasp%20heatmaps%20as%0Aguidance%2C%20which%20infers%20in%20a%20global-to-local%20semantic-to-point%20way.%0ASpecifically%2C%20Gaussian%20encoding%20and%20the%20grid-based%20strategy%20are%20applied%20to%0Apredict%20grasp%20heatmaps%20as%20guidance%20to%20aggregate%20local%20points%20into%20graspable%0Aregions%20and%20provide%20global%20semantic%20information.%20Further%2C%20a%20novel%20non-uniform%0Aanchor%20sampling%20mechanism%20is%20designed%20to%20improve%20grasp%20accuracy%20and%20diversity.%0ABenefiting%20from%20the%20high-efficiency%20encoding%20in%20the%20image%20space%20and%20focusing%20on%0Apoints%20in%20local%20graspable%20regions%2C%20our%20framework%20can%20perform%20high-quality%20grasp%0Adetection%20in%20real-time%20and%20achieve%20state-of-the-art%20results.%20In%20addition%2C%20real%0Arobot%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20a%20success%0Arate%20of%2094%25%20and%20a%20clutter%20completion%20rate%20of%20100%25.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/THU-VCLab/HGGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18546v1&entry.124074799=Read"},
{"title": "Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds", "author": "Zhimin Yuan and Wankang Zeng and Yanfei Su and Weiquan Liu and Ming Cheng and Yulan Guo and Cheng Wang", "abstract": "  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n", "link": "http://arxiv.org/abs/2403.18469v1", "date": "2024-03-27", "relevancy": 2.4094, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6124}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5996}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Density-guided%20Translator%20Boosts%20Synthetic-to-Real%20Unsupervised%20Domain%0A%20%20Adaptive%20Segmentation%20of%203D%20Point%20Clouds&body=Title%3A%20Density-guided%20Translator%20Boosts%20Synthetic-to-Real%20Unsupervised%20Domain%0A%20%20Adaptive%20Segmentation%20of%203D%20Point%20Clouds%0AAuthor%3A%20Zhimin%20Yuan%20and%20Wankang%20Zeng%20and%20Yanfei%20Su%20and%20Weiquan%20Liu%20and%20Ming%20Cheng%20and%20Yulan%20Guo%20and%20Cheng%20Wang%0AAbstract%3A%20%20%203D%20synthetic-to-real%20unsupervised%20domain%20adaptive%20segmentation%20is%20crucial%20to%0Aannotating%20new%20domains.%20Self-training%20is%20a%20competitive%20approach%20for%20this%20task%2C%0Abut%20its%20performance%20is%20limited%20by%20different%20sensor%20sampling%20patterns%20%28i.e.%2C%0Avariations%20in%20point%20density%29%20and%20incomplete%20training%20strategies.%20In%20this%20work%2C%0Awe%20propose%20a%20density-guided%20translator%20%28DGT%29%2C%20which%20translates%20point%20density%0Abetween%20domains%2C%20and%20integrates%20it%20into%20a%20two-stage%20self-training%20pipeline%0Anamed%20DGT-ST.%20First%2C%20in%20contrast%20to%20existing%20works%20that%20simultaneously%20conduct%0Adata%20generation%20and%20feature/output%20alignment%20within%20unstable%20adversarial%0Atraining%2C%20we%20employ%20the%20non-learnable%20DGT%20to%20bridge%20the%20domain%20gap%20at%20the%20input%0Alevel.%20Second%2C%20to%20provide%20a%20well-initialized%20model%20for%20self-training%2C%20we%0Apropose%20a%20category-level%20adversarial%20network%20in%20stage%20one%20that%20utilizes%20the%0Aprototype%20to%20prevent%20negative%20transfer.%20Finally%2C%20by%20leveraging%20the%20designs%0Aabove%2C%20a%20domain-mixed%20self-training%20method%20with%20source-aware%20consistency%20loss%0Ais%20proposed%20in%20stage%20two%20to%20narrow%20the%20domain%20gap%20further.%20Experiments%20on%20two%0Asynthetic-to-real%20segmentation%20tasks%20%28SynLiDAR%20%24%5Crightarrow%24%20semanticKITTI%20and%0ASynLiDAR%20%24%5Crightarrow%24%20semanticPOSS%29%20demonstrate%20that%20DGT-ST%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%209.4%24%5C%25%24%20and%204.3%24%5C%25%24%20mIoU%20improvements%2C%0Arespectively.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/yuan-zm/DGT-ST%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18469v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Density-guided%20Translator%20Boosts%20Synthetic-to-Real%20Unsupervised%20Domain%0A%20%20Adaptive%20Segmentation%20of%203D%20Point%20Clouds&entry.906535625=Zhimin%20Yuan%20and%20Wankang%20Zeng%20and%20Yanfei%20Su%20and%20Weiquan%20Liu%20and%20Ming%20Cheng%20and%20Yulan%20Guo%20and%20Cheng%20Wang&entry.1292438233=%20%203D%20synthetic-to-real%20unsupervised%20domain%20adaptive%20segmentation%20is%20crucial%20to%0Aannotating%20new%20domains.%20Self-training%20is%20a%20competitive%20approach%20for%20this%20task%2C%0Abut%20its%20performance%20is%20limited%20by%20different%20sensor%20sampling%20patterns%20%28i.e.%2C%0Avariations%20in%20point%20density%29%20and%20incomplete%20training%20strategies.%20In%20this%20work%2C%0Awe%20propose%20a%20density-guided%20translator%20%28DGT%29%2C%20which%20translates%20point%20density%0Abetween%20domains%2C%20and%20integrates%20it%20into%20a%20two-stage%20self-training%20pipeline%0Anamed%20DGT-ST.%20First%2C%20in%20contrast%20to%20existing%20works%20that%20simultaneously%20conduct%0Adata%20generation%20and%20feature/output%20alignment%20within%20unstable%20adversarial%0Atraining%2C%20we%20employ%20the%20non-learnable%20DGT%20to%20bridge%20the%20domain%20gap%20at%20the%20input%0Alevel.%20Second%2C%20to%20provide%20a%20well-initialized%20model%20for%20self-training%2C%20we%0Apropose%20a%20category-level%20adversarial%20network%20in%20stage%20one%20that%20utilizes%20the%0Aprototype%20to%20prevent%20negative%20transfer.%20Finally%2C%20by%20leveraging%20the%20designs%0Aabove%2C%20a%20domain-mixed%20self-training%20method%20with%20source-aware%20consistency%20loss%0Ais%20proposed%20in%20stage%20two%20to%20narrow%20the%20domain%20gap%20further.%20Experiments%20on%20two%0Asynthetic-to-real%20segmentation%20tasks%20%28SynLiDAR%20%24%5Crightarrow%24%20semanticKITTI%20and%0ASynLiDAR%20%24%5Crightarrow%24%20semanticPOSS%29%20demonstrate%20that%20DGT-ST%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%209.4%24%5C%25%24%20and%204.3%24%5C%25%24%20mIoU%20improvements%2C%0Arespectively.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/yuan-zm/DGT-ST%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18469v1&entry.124074799=Read"},
{"title": "Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features", "author": "Thomas Wimmer and Peter Wonka and Maks Ovsjanikov", "abstract": "  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n", "link": "http://arxiv.org/abs/2311.18113v2", "date": "2024-03-27", "relevancy": 2.3842, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Back%20to%203D%3A%20Few-Shot%203D%20Keypoint%20Detection%20with%20Back-Projected%202D%0A%20%20Features&body=Title%3A%20Back%20to%203D%3A%20Few-Shot%203D%20Keypoint%20Detection%20with%20Back-Projected%202D%0A%20%20Features%0AAuthor%3A%20Thomas%20Wimmer%20and%20Peter%20Wonka%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20With%20the%20immense%20growth%20of%20dataset%20sizes%20and%20computing%20resources%20in%20recent%0Ayears%2C%20so-called%20foundation%20models%20have%20become%20popular%20in%20NLP%20and%20vision%20tasks.%0AIn%20this%20work%2C%20we%20propose%20to%20explore%20foundation%20models%20for%20the%20task%20of%20keypoint%0Adetection%20on%203D%20shapes.%20A%20unique%20characteristic%20of%20keypoint%20detection%20is%20that%0Ait%20requires%20semantic%20and%20geometric%20awareness%20while%20demanding%20high%20localization%0Aaccuracy.%20To%20address%20this%20problem%2C%20we%20propose%2C%20first%2C%20to%20back-project%20features%0Afrom%20large%20pre-trained%202D%20vision%20models%20onto%203D%20shapes%20and%20employ%20them%20for%20this%0Atask.%20We%20show%20that%20we%20obtain%20robust%203D%20features%20that%20contain%20rich%20semantic%0Ainformation%20and%20analyze%20multiple%20candidate%20features%20stemming%20from%20different%202D%0Afoundation%20models.%20Second%2C%20we%20employ%20a%20keypoint%20candidate%20optimization%20module%0Awhich%20aims%20to%20match%20the%20average%20observed%20distribution%20of%20keypoints%20on%20the%20shape%0Aand%20is%20guided%20by%20the%20back-projected%20features.%20The%20resulting%20approach%20achieves%20a%0Anew%20state%20of%20the%20art%20for%20few-shot%20keypoint%20detection%20on%20the%20KeyPointNet%0Adataset%2C%20almost%20doubling%20the%20performance%20of%20the%20previous%20best%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18113v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%203D%3A%20Few-Shot%203D%20Keypoint%20Detection%20with%20Back-Projected%202D%0A%20%20Features&entry.906535625=Thomas%20Wimmer%20and%20Peter%20Wonka%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20With%20the%20immense%20growth%20of%20dataset%20sizes%20and%20computing%20resources%20in%20recent%0Ayears%2C%20so-called%20foundation%20models%20have%20become%20popular%20in%20NLP%20and%20vision%20tasks.%0AIn%20this%20work%2C%20we%20propose%20to%20explore%20foundation%20models%20for%20the%20task%20of%20keypoint%0Adetection%20on%203D%20shapes.%20A%20unique%20characteristic%20of%20keypoint%20detection%20is%20that%0Ait%20requires%20semantic%20and%20geometric%20awareness%20while%20demanding%20high%20localization%0Aaccuracy.%20To%20address%20this%20problem%2C%20we%20propose%2C%20first%2C%20to%20back-project%20features%0Afrom%20large%20pre-trained%202D%20vision%20models%20onto%203D%20shapes%20and%20employ%20them%20for%20this%0Atask.%20We%20show%20that%20we%20obtain%20robust%203D%20features%20that%20contain%20rich%20semantic%0Ainformation%20and%20analyze%20multiple%20candidate%20features%20stemming%20from%20different%202D%0Afoundation%20models.%20Second%2C%20we%20employ%20a%20keypoint%20candidate%20optimization%20module%0Awhich%20aims%20to%20match%20the%20average%20observed%20distribution%20of%20keypoints%20on%20the%20shape%0Aand%20is%20guided%20by%20the%20back-projected%20features.%20The%20resulting%20approach%20achieves%20a%0Anew%20state%20of%20the%20art%20for%20few-shot%20keypoint%20detection%20on%20the%20KeyPointNet%0Adataset%2C%20almost%20doubling%20the%20performance%20of%20the%20previous%20best%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18113v2&entry.124074799=Read"},
{"title": "BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D\n  Object Detection", "author": "Jiaming Liu and Rongyu Zhang and Xiaoqi Li and Xiaowei Chi and Zehui Chen and Ming Lu and Yandong Guo and Shanghang Zhang", "abstract": "  Vision-centric bird-eye-view (BEV) perception has shown promising potential\nin autonomous driving. Recent works mainly focus on improving efficiency or\naccuracy but neglect the challenges when facing environment changing, resulting\nin severe degradation of transfer performance. For BEV perception, we figure\nout the significant domain gaps existing in typical real-world cross-domain\nscenarios and comprehensively solve the Domain Adaption (DA) problem for\nmulti-view 3D object detection. Since BEV perception approaches are complicated\nand contain several components, the domain shift accumulation on multiple\ngeometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In\nthis paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework\nto ease the domain shift accumulation, which consists of a Depth-Aware Teacher\n(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines\ntarget lidar and reliable depth prediction to construct depth-aware\ninformation, extracting target domain-specific knowledge in Voxel and BEV\nfeature spaces. It then transfers the sufficient domain knowledge of multiple\nspaces to the student model. In order to jointly alleviate the domain shift,\nGAS projects multi-geometric space features to a shared geometric embedding\nspace and decreases data distribution distance between two domains. To verify\nthe effectiveness of our method, we conduct BEV 3D object detection experiments\non three cross-domain scenarios and achieve state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2211.17126v2", "date": "2024-03-27", "relevancy": 2.3519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5938}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5886}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5819}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BEVUDA%3A%20Multi-geometric%20Space%20Alignments%20for%20Domain%20Adaptive%20BEV%203D%0A%20%20Object%20Detection&body=Title%3A%20BEVUDA%3A%20Multi-geometric%20Space%20Alignments%20for%20Domain%20Adaptive%20BEV%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Jiaming%20Liu%20and%20Rongyu%20Zhang%20and%20Xiaoqi%20Li%20and%20Xiaowei%20Chi%20and%20Zehui%20Chen%20and%20Ming%20Lu%20and%20Yandong%20Guo%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Vision-centric%20bird-eye-view%20%28BEV%29%20perception%20has%20shown%20promising%20potential%0Ain%20autonomous%20driving.%20Recent%20works%20mainly%20focus%20on%20improving%20efficiency%20or%0Aaccuracy%20but%20neglect%20the%20challenges%20when%20facing%20environment%20changing%2C%20resulting%0Ain%20severe%20degradation%20of%20transfer%20performance.%20For%20BEV%20perception%2C%20we%20figure%0Aout%20the%20significant%20domain%20gaps%20existing%20in%20typical%20real-world%20cross-domain%0Ascenarios%20and%20comprehensively%20solve%20the%20Domain%20Adaption%20%28DA%29%20problem%20for%0Amulti-view%203D%20object%20detection.%20Since%20BEV%20perception%20approaches%20are%20complicated%0Aand%20contain%20several%20components%2C%20the%20domain%20shift%20accumulation%20on%20multiple%0Ageometric%20spaces%20%28i.e.%2C%202D%2C%203D%20Voxel%2C%20BEV%29%20makes%20BEV%20DA%20even%20challenging.%20In%0Athis%20paper%2C%20we%20propose%20a%20Multi-space%20Alignment%20Teacher-Student%20%28MATS%29%20framework%0Ato%20ease%20the%20domain%20shift%20accumulation%2C%20which%20consists%20of%20a%20Depth-Aware%20Teacher%0A%28DAT%29%20and%20a%20Geometric-space%20Aligned%20Student%20%28GAS%29%20model.%20DAT%20tactfully%20combines%0Atarget%20lidar%20and%20reliable%20depth%20prediction%20to%20construct%20depth-aware%0Ainformation%2C%20extracting%20target%20domain-specific%20knowledge%20in%20Voxel%20and%20BEV%0Afeature%20spaces.%20It%20then%20transfers%20the%20sufficient%20domain%20knowledge%20of%20multiple%0Aspaces%20to%20the%20student%20model.%20In%20order%20to%20jointly%20alleviate%20the%20domain%20shift%2C%0AGAS%20projects%20multi-geometric%20space%20features%20to%20a%20shared%20geometric%20embedding%0Aspace%20and%20decreases%20data%20distribution%20distance%20between%20two%20domains.%20To%20verify%0Athe%20effectiveness%20of%20our%20method%2C%20we%20conduct%20BEV%203D%20object%20detection%20experiments%0Aon%20three%20cross-domain%20scenarios%20and%20achieve%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.17126v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVUDA%3A%20Multi-geometric%20Space%20Alignments%20for%20Domain%20Adaptive%20BEV%203D%0A%20%20Object%20Detection&entry.906535625=Jiaming%20Liu%20and%20Rongyu%20Zhang%20and%20Xiaoqi%20Li%20and%20Xiaowei%20Chi%20and%20Zehui%20Chen%20and%20Ming%20Lu%20and%20Yandong%20Guo%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Vision-centric%20bird-eye-view%20%28BEV%29%20perception%20has%20shown%20promising%20potential%0Ain%20autonomous%20driving.%20Recent%20works%20mainly%20focus%20on%20improving%20efficiency%20or%0Aaccuracy%20but%20neglect%20the%20challenges%20when%20facing%20environment%20changing%2C%20resulting%0Ain%20severe%20degradation%20of%20transfer%20performance.%20For%20BEV%20perception%2C%20we%20figure%0Aout%20the%20significant%20domain%20gaps%20existing%20in%20typical%20real-world%20cross-domain%0Ascenarios%20and%20comprehensively%20solve%20the%20Domain%20Adaption%20%28DA%29%20problem%20for%0Amulti-view%203D%20object%20detection.%20Since%20BEV%20perception%20approaches%20are%20complicated%0Aand%20contain%20several%20components%2C%20the%20domain%20shift%20accumulation%20on%20multiple%0Ageometric%20spaces%20%28i.e.%2C%202D%2C%203D%20Voxel%2C%20BEV%29%20makes%20BEV%20DA%20even%20challenging.%20In%0Athis%20paper%2C%20we%20propose%20a%20Multi-space%20Alignment%20Teacher-Student%20%28MATS%29%20framework%0Ato%20ease%20the%20domain%20shift%20accumulation%2C%20which%20consists%20of%20a%20Depth-Aware%20Teacher%0A%28DAT%29%20and%20a%20Geometric-space%20Aligned%20Student%20%28GAS%29%20model.%20DAT%20tactfully%20combines%0Atarget%20lidar%20and%20reliable%20depth%20prediction%20to%20construct%20depth-aware%0Ainformation%2C%20extracting%20target%20domain-specific%20knowledge%20in%20Voxel%20and%20BEV%0Afeature%20spaces.%20It%20then%20transfers%20the%20sufficient%20domain%20knowledge%20of%20multiple%0Aspaces%20to%20the%20student%20model.%20In%20order%20to%20jointly%20alleviate%20the%20domain%20shift%2C%0AGAS%20projects%20multi-geometric%20space%20features%20to%20a%20shared%20geometric%20embedding%0Aspace%20and%20decreases%20data%20distribution%20distance%20between%20two%20domains.%20To%20verify%0Athe%20effectiveness%20of%20our%20method%2C%20we%20conduct%20BEV%203D%20object%20detection%20experiments%0Aon%20three%20cross-domain%20scenarios%20and%20achieve%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.17126v2&entry.124074799=Read"},
{"title": "ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition", "author": "Weidong Xie and Lun Luo and Nanfei Ye and Yi Ren and Shaoyi Du and Minhang Wang and Jintao Xu and Rui Ai and Weihao Gu and Xieyuanli Chen", "abstract": "  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n", "link": "http://arxiv.org/abs/2403.18762v1", "date": "2024-03-27", "relevancy": 2.3513, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5945}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5851}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ModaLink%3A%20Unifying%20Modalities%20for%20Efficient%20Image-to-PointCloud%20Place%0A%20%20Recognition&body=Title%3A%20ModaLink%3A%20Unifying%20Modalities%20for%20Efficient%20Image-to-PointCloud%20Place%0A%20%20Recognition%0AAuthor%3A%20Weidong%20Xie%20and%20Lun%20Luo%20and%20Nanfei%20Ye%20and%20Yi%20Ren%20and%20Shaoyi%20Du%20and%20Minhang%20Wang%20and%20Jintao%20Xu%20and%20Rui%20Ai%20and%20Weihao%20Gu%20and%20Xieyuanli%20Chen%0AAbstract%3A%20%20%20Place%20recognition%20is%20an%20important%20task%20for%20robots%20and%20autonomous%20cars%20to%0Alocalize%20themselves%20and%20close%20loops%20in%20pre-built%20maps.%20While%20single-modal%0Asensor-based%20methods%20have%20shown%20satisfactory%20performance%2C%20cross-modal%20place%0Arecognition%20that%20retrieving%20images%20from%20a%20point-cloud%20database%20remains%20a%0Achallenging%20problem.%20Current%20cross-modal%20methods%20transform%20images%20into%203D%0Apoints%20using%20depth%20estimation%20for%20modality%20conversion%2C%20which%20are%20usually%0Acomputationally%20intensive%20and%20need%20expensive%20labeled%20data%20for%20depth%0Asupervision.%20In%20this%20work%2C%20we%20introduce%20a%20fast%20and%20lightweight%20framework%20to%0Aencode%20images%20and%20point%20clouds%20into%20place-distinctive%20descriptors.%20We%20propose%0Aan%20effective%20Field%20of%20View%20%28FoV%29%20transformation%20module%20to%20convert%20point%20clouds%0Ainto%20an%20analogous%20modality%20as%20images.%20This%20module%20eliminates%20the%20necessity%20for%0Adepth%20estimation%20and%20helps%20subsequent%20modules%20achieve%20real-time%20performance.%20We%0Afurther%20design%20a%20non-negative%20factorization-based%20encoder%20to%20extract%20mutually%0Aconsistent%20semantic%20features%20between%20point%20clouds%20and%20images.%20This%20encoder%0Ayields%20more%20distinctive%20global%20descriptors%20for%20retrieval.%20Experimental%20results%0Aon%20the%20KITTI%20dataset%20show%20that%20our%20proposed%20methods%20achieve%20state-of-the-art%0Aperformance%20while%20running%20in%20real%20time.%20Additional%20evaluation%20on%20the%20HAOMO%0Adataset%20covering%20a%2017%20km%20trajectory%20further%20shows%20the%20practical%20generalization%0Acapabilities.%20We%20have%20released%20the%20implementation%20of%20our%20methods%20as%20open%20source%0Aat%3A%20https%3A//github.com/haomo-ai/ModaLink.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18762v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModaLink%3A%20Unifying%20Modalities%20for%20Efficient%20Image-to-PointCloud%20Place%0A%20%20Recognition&entry.906535625=Weidong%20Xie%20and%20Lun%20Luo%20and%20Nanfei%20Ye%20and%20Yi%20Ren%20and%20Shaoyi%20Du%20and%20Minhang%20Wang%20and%20Jintao%20Xu%20and%20Rui%20Ai%20and%20Weihao%20Gu%20and%20Xieyuanli%20Chen&entry.1292438233=%20%20Place%20recognition%20is%20an%20important%20task%20for%20robots%20and%20autonomous%20cars%20to%0Alocalize%20themselves%20and%20close%20loops%20in%20pre-built%20maps.%20While%20single-modal%0Asensor-based%20methods%20have%20shown%20satisfactory%20performance%2C%20cross-modal%20place%0Arecognition%20that%20retrieving%20images%20from%20a%20point-cloud%20database%20remains%20a%0Achallenging%20problem.%20Current%20cross-modal%20methods%20transform%20images%20into%203D%0Apoints%20using%20depth%20estimation%20for%20modality%20conversion%2C%20which%20are%20usually%0Acomputationally%20intensive%20and%20need%20expensive%20labeled%20data%20for%20depth%0Asupervision.%20In%20this%20work%2C%20we%20introduce%20a%20fast%20and%20lightweight%20framework%20to%0Aencode%20images%20and%20point%20clouds%20into%20place-distinctive%20descriptors.%20We%20propose%0Aan%20effective%20Field%20of%20View%20%28FoV%29%20transformation%20module%20to%20convert%20point%20clouds%0Ainto%20an%20analogous%20modality%20as%20images.%20This%20module%20eliminates%20the%20necessity%20for%0Adepth%20estimation%20and%20helps%20subsequent%20modules%20achieve%20real-time%20performance.%20We%0Afurther%20design%20a%20non-negative%20factorization-based%20encoder%20to%20extract%20mutually%0Aconsistent%20semantic%20features%20between%20point%20clouds%20and%20images.%20This%20encoder%0Ayields%20more%20distinctive%20global%20descriptors%20for%20retrieval.%20Experimental%20results%0Aon%20the%20KITTI%20dataset%20show%20that%20our%20proposed%20methods%20achieve%20state-of-the-art%0Aperformance%20while%20running%20in%20real%20time.%20Additional%20evaluation%20on%20the%20HAOMO%0Adataset%20covering%20a%2017%20km%20trajectory%20further%20shows%20the%20practical%20generalization%0Acapabilities.%20We%20have%20released%20the%20implementation%20of%20our%20methods%20as%20open%20source%0Aat%3A%20https%3A//github.com/haomo-ai/ModaLink.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18762v1&entry.124074799=Read"},
{"title": "ViDA: Homeostatic Visual Domain Adapter for Continual Test Time\n  Adaptation", "author": "Jiaming Liu and Senqiao Yang and Peidong Jia and Renrui Zhang and Ming Lu and Yandong Guo and Wei Xue and Shanghang Zhang", "abstract": "  Since real-world machine systems are running in non-stationary environments,\nContinual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained\nmodel to continually changing target domains. Recently, existing methods mainly\nfocus on model-based adaptation, which aims to leverage a self-training manner\nto extract the target domain knowledge. However, pseudo labels can be noisy and\nthe updated model parameters are unreliable under dynamic data distributions,\nleading to error accumulation and catastrophic forgetting in the continual\nadaptation process. To tackle these challenges and maintain the model\nplasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly\nhandling both domain-specific and domain-shared knowledge. Specifically, we\nfirst comprehensively explore the different domain representations of the\nadapters with trainable high-rank or low-rank embedding spaces. Then we inject\nViDAs into the pre-trained model, which leverages high-rank and low-rank\nfeatures to adapt the current domain distribution and maintain the continual\ndomain-shared knowledge, respectively. To exploit the low-rank and high-rank\nViDAs more effectively, we further propose a Homeostatic Knowledge Allotment\n(HKA) strategy, which adaptively combines different knowledge from each ViDA.\nExtensive experiments conducted on four widely used benchmarks demonstrate that\nour proposed method achieves state-of-the-art performance in both\nclassification and segmentation CTTA tasks. Note that, our method can be\nregarded as a novel transfer paradigm for large-scale models, delivering\npromising results in adaptation to continually changing distributions. Project\npage: https://sites.google.com/view/iclr2024-vida/home.\n", "link": "http://arxiv.org/abs/2306.04344v3", "date": "2024-03-27", "relevancy": 2.338, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6197}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5639}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5479}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ViDA%3A%20Homeostatic%20Visual%20Domain%20Adapter%20for%20Continual%20Test%20Time%0A%20%20Adaptation&body=Title%3A%20ViDA%3A%20Homeostatic%20Visual%20Domain%20Adapter%20for%20Continual%20Test%20Time%0A%20%20Adaptation%0AAuthor%3A%20Jiaming%20Liu%20and%20Senqiao%20Yang%20and%20Peidong%20Jia%20and%20Renrui%20Zhang%20and%20Ming%20Lu%20and%20Yandong%20Guo%20and%20Wei%20Xue%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Since%20real-world%20machine%20systems%20are%20running%20in%20non-stationary%20environments%2C%0AContinual%20Test-Time%20Adaptation%20%28CTTA%29%20task%20is%20proposed%20to%20adapt%20the%20pre-trained%0Amodel%20to%20continually%20changing%20target%20domains.%20Recently%2C%20existing%20methods%20mainly%0Afocus%20on%20model-based%20adaptation%2C%20which%20aims%20to%20leverage%20a%20self-training%20manner%0Ato%20extract%20the%20target%20domain%20knowledge.%20However%2C%20pseudo%20labels%20can%20be%20noisy%20and%0Athe%20updated%20model%20parameters%20are%20unreliable%20under%20dynamic%20data%20distributions%2C%0Aleading%20to%20error%20accumulation%20and%20catastrophic%20forgetting%20in%20the%20continual%0Aadaptation%20process.%20To%20tackle%20these%20challenges%20and%20maintain%20the%20model%0Aplasticity%2C%20we%20design%20a%20Visual%20Domain%20Adapter%20%28ViDA%29%20for%20CTTA%2C%20explicitly%0Ahandling%20both%20domain-specific%20and%20domain-shared%20knowledge.%20Specifically%2C%20we%0Afirst%20comprehensively%20explore%20the%20different%20domain%20representations%20of%20the%0Aadapters%20with%20trainable%20high-rank%20or%20low-rank%20embedding%20spaces.%20Then%20we%20inject%0AViDAs%20into%20the%20pre-trained%20model%2C%20which%20leverages%20high-rank%20and%20low-rank%0Afeatures%20to%20adapt%20the%20current%20domain%20distribution%20and%20maintain%20the%20continual%0Adomain-shared%20knowledge%2C%20respectively.%20To%20exploit%20the%20low-rank%20and%20high-rank%0AViDAs%20more%20effectively%2C%20we%20further%20propose%20a%20Homeostatic%20Knowledge%20Allotment%0A%28HKA%29%20strategy%2C%20which%20adaptively%20combines%20different%20knowledge%20from%20each%20ViDA.%0AExtensive%20experiments%20conducted%20on%20four%20widely%20used%20benchmarks%20demonstrate%20that%0Aour%20proposed%20method%20achieves%20state-of-the-art%20performance%20in%20both%0Aclassification%20and%20segmentation%20CTTA%20tasks.%20Note%20that%2C%20our%20method%20can%20be%0Aregarded%20as%20a%20novel%20transfer%20paradigm%20for%20large-scale%20models%2C%20delivering%0Apromising%20results%20in%20adaptation%20to%20continually%20changing%20distributions.%20Project%0Apage%3A%20https%3A//sites.google.com/view/iclr2024-vida/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04344v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViDA%3A%20Homeostatic%20Visual%20Domain%20Adapter%20for%20Continual%20Test%20Time%0A%20%20Adaptation&entry.906535625=Jiaming%20Liu%20and%20Senqiao%20Yang%20and%20Peidong%20Jia%20and%20Renrui%20Zhang%20and%20Ming%20Lu%20and%20Yandong%20Guo%20and%20Wei%20Xue%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Since%20real-world%20machine%20systems%20are%20running%20in%20non-stationary%20environments%2C%0AContinual%20Test-Time%20Adaptation%20%28CTTA%29%20task%20is%20proposed%20to%20adapt%20the%20pre-trained%0Amodel%20to%20continually%20changing%20target%20domains.%20Recently%2C%20existing%20methods%20mainly%0Afocus%20on%20model-based%20adaptation%2C%20which%20aims%20to%20leverage%20a%20self-training%20manner%0Ato%20extract%20the%20target%20domain%20knowledge.%20However%2C%20pseudo%20labels%20can%20be%20noisy%20and%0Athe%20updated%20model%20parameters%20are%20unreliable%20under%20dynamic%20data%20distributions%2C%0Aleading%20to%20error%20accumulation%20and%20catastrophic%20forgetting%20in%20the%20continual%0Aadaptation%20process.%20To%20tackle%20these%20challenges%20and%20maintain%20the%20model%0Aplasticity%2C%20we%20design%20a%20Visual%20Domain%20Adapter%20%28ViDA%29%20for%20CTTA%2C%20explicitly%0Ahandling%20both%20domain-specific%20and%20domain-shared%20knowledge.%20Specifically%2C%20we%0Afirst%20comprehensively%20explore%20the%20different%20domain%20representations%20of%20the%0Aadapters%20with%20trainable%20high-rank%20or%20low-rank%20embedding%20spaces.%20Then%20we%20inject%0AViDAs%20into%20the%20pre-trained%20model%2C%20which%20leverages%20high-rank%20and%20low-rank%0Afeatures%20to%20adapt%20the%20current%20domain%20distribution%20and%20maintain%20the%20continual%0Adomain-shared%20knowledge%2C%20respectively.%20To%20exploit%20the%20low-rank%20and%20high-rank%0AViDAs%20more%20effectively%2C%20we%20further%20propose%20a%20Homeostatic%20Knowledge%20Allotment%0A%28HKA%29%20strategy%2C%20which%20adaptively%20combines%20different%20knowledge%20from%20each%20ViDA.%0AExtensive%20experiments%20conducted%20on%20four%20widely%20used%20benchmarks%20demonstrate%20that%0Aour%20proposed%20method%20achieves%20state-of-the-art%20performance%20in%20both%0Aclassification%20and%20segmentation%20CTTA%20tasks.%20Note%20that%2C%20our%20method%20can%20be%0Aregarded%20as%20a%20novel%20transfer%20paradigm%20for%20large-scale%20models%2C%20delivering%0Apromising%20results%20in%20adaptation%20to%20continually%20changing%20distributions.%20Project%0Apage%3A%20https%3A//sites.google.com/view/iclr2024-vida/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04344v3&entry.124074799=Read"},
{"title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation", "author": "Suraj Patni and Aradhye Agarwal and Chetan Arora", "abstract": "  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n", "link": "http://arxiv.org/abs/2403.18807v1", "date": "2024-03-27", "relevancy": 2.3373, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6324}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECoDepth%3A%20Effective%20Conditioning%20of%20Diffusion%20Models%20for%20Monocular%20Depth%0A%20%20Estimation&body=Title%3A%20ECoDepth%3A%20Effective%20Conditioning%20of%20Diffusion%20Models%20for%20Monocular%20Depth%0A%20%20Estimation%0AAuthor%3A%20Suraj%20Patni%20and%20Aradhye%20Agarwal%20and%20Chetan%20Arora%0AAbstract%3A%20%20%20In%20the%20absence%20of%20parallax%20cues%2C%20a%20learning-based%20single%20image%20depth%0Aestimation%20%28SIDE%29%20model%20relies%20heavily%20on%20shading%20and%20contextual%20cues%20in%20the%0Aimage.%20While%20this%20simplicity%20is%20attractive%2C%20it%20is%20necessary%20to%20train%20such%0Amodels%20on%20large%20and%20varied%20datasets%2C%20which%20are%20difficult%20to%20capture.%20It%20has%0Abeen%20shown%20that%20using%20embeddings%20from%20pre-trained%20foundational%20models%2C%20such%20as%0ACLIP%2C%20improves%20zero%20shot%20transfer%20in%20several%20applications.%20Taking%20inspiration%0Afrom%20this%2C%20in%20our%20paper%20we%20explore%20the%20use%20of%20global%20image%20priors%20generated%0Afrom%20a%20pre-trained%20ViT%20model%20to%20provide%20more%20detailed%20contextual%20information.%0AWe%20argue%20that%20the%20embedding%20vector%20from%20a%20ViT%20model%2C%20pre-trained%20on%20a%20large%0Adataset%2C%20captures%20greater%20relevant%20information%20for%20SIDE%20than%20the%20usual%20route%20of%0Agenerating%20pseudo%20image%20captions%2C%20followed%20by%20CLIP%20based%20text%20embeddings.%20Based%0Aon%20this%20idea%2C%20we%20propose%20a%20new%20SIDE%20model%20using%20a%20diffusion%20backbone%20which%20is%0Aconditioned%20on%20ViT%20embeddings.%20Our%20proposed%20design%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20SIDE%20on%20NYUv2%20dataset%2C%20achieving%20Abs%20Rel%20error%20of%0A0.059%2814%25%20improvement%29%20compared%20to%200.069%20by%20the%20current%20SOTA%20%28VPD%29.%20And%20on%0AKITTI%20dataset%2C%20achieving%20Sq%20Rel%20error%20of%200.139%20%282%25%20improvement%29%20compared%20to%0A0.142%20by%20the%20current%20SOTA%20%28GEDepth%29.%20For%20zero-shot%20transfer%20with%20a%20model%0Atrained%20on%20NYUv2%2C%20we%20report%20mean%20relative%20improvement%20of%20%2820%25%2C%2023%25%2C%2081%25%2C%2025%25%29%0Aover%20NeWCRFs%20on%20%28Sun-RGBD%2C%20iBims1%2C%20DIODE%2C%20HyperSim%29%20datasets%2C%20compared%20to%20%2816%25%2C%0A18%25%2C%2045%25%2C%209%25%29%20by%20ZoeDepth.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Aradhye2002/EcoDepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18807v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECoDepth%3A%20Effective%20Conditioning%20of%20Diffusion%20Models%20for%20Monocular%20Depth%0A%20%20Estimation&entry.906535625=Suraj%20Patni%20and%20Aradhye%20Agarwal%20and%20Chetan%20Arora&entry.1292438233=%20%20In%20the%20absence%20of%20parallax%20cues%2C%20a%20learning-based%20single%20image%20depth%0Aestimation%20%28SIDE%29%20model%20relies%20heavily%20on%20shading%20and%20contextual%20cues%20in%20the%0Aimage.%20While%20this%20simplicity%20is%20attractive%2C%20it%20is%20necessary%20to%20train%20such%0Amodels%20on%20large%20and%20varied%20datasets%2C%20which%20are%20difficult%20to%20capture.%20It%20has%0Abeen%20shown%20that%20using%20embeddings%20from%20pre-trained%20foundational%20models%2C%20such%20as%0ACLIP%2C%20improves%20zero%20shot%20transfer%20in%20several%20applications.%20Taking%20inspiration%0Afrom%20this%2C%20in%20our%20paper%20we%20explore%20the%20use%20of%20global%20image%20priors%20generated%0Afrom%20a%20pre-trained%20ViT%20model%20to%20provide%20more%20detailed%20contextual%20information.%0AWe%20argue%20that%20the%20embedding%20vector%20from%20a%20ViT%20model%2C%20pre-trained%20on%20a%20large%0Adataset%2C%20captures%20greater%20relevant%20information%20for%20SIDE%20than%20the%20usual%20route%20of%0Agenerating%20pseudo%20image%20captions%2C%20followed%20by%20CLIP%20based%20text%20embeddings.%20Based%0Aon%20this%20idea%2C%20we%20propose%20a%20new%20SIDE%20model%20using%20a%20diffusion%20backbone%20which%20is%0Aconditioned%20on%20ViT%20embeddings.%20Our%20proposed%20design%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20SIDE%20on%20NYUv2%20dataset%2C%20achieving%20Abs%20Rel%20error%20of%0A0.059%2814%25%20improvement%29%20compared%20to%200.069%20by%20the%20current%20SOTA%20%28VPD%29.%20And%20on%0AKITTI%20dataset%2C%20achieving%20Sq%20Rel%20error%20of%200.139%20%282%25%20improvement%29%20compared%20to%0A0.142%20by%20the%20current%20SOTA%20%28GEDepth%29.%20For%20zero-shot%20transfer%20with%20a%20model%0Atrained%20on%20NYUv2%2C%20we%20report%20mean%20relative%20improvement%20of%20%2820%25%2C%2023%25%2C%2081%25%2C%2025%25%29%0Aover%20NeWCRFs%20on%20%28Sun-RGBD%2C%20iBims1%2C%20DIODE%2C%20HyperSim%29%20datasets%2C%20compared%20to%20%2816%25%2C%0A18%25%2C%2045%25%2C%209%25%29%20by%20ZoeDepth.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Aradhye2002/EcoDepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18807v1&entry.124074799=Read"},
{"title": "Nonlinear model reduction for operator learning", "author": "Hamidreza Eivazi and Stefan Wittek and Andreas Rausch", "abstract": "  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n", "link": "http://arxiv.org/abs/2403.18735v1", "date": "2024-03-27", "relevancy": 2.3116, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4648}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4618}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20model%20reduction%20for%20operator%20learning&body=Title%3A%20Nonlinear%20model%20reduction%20for%20operator%20learning%0AAuthor%3A%20Hamidreza%20Eivazi%20and%20Stefan%20Wittek%20and%20Andreas%20Rausch%0AAbstract%3A%20%20%20Operator%20learning%20provides%20methods%20to%20approximate%20mappings%20between%0Ainfinite-dimensional%20function%20spaces.%20Deep%20operator%20networks%20%28DeepONets%29%20are%20a%0Anotable%20architecture%20in%20this%20field.%20Recently%2C%20an%20extension%20of%20DeepONet%20based%20on%0Amodel%20reduction%20and%20neural%20networks%2C%20proper%20orthogonal%20decomposition%0A%28POD%29-DeepONet%2C%20has%20been%20able%20to%20outperform%20other%20architectures%20in%20terms%20of%0Aaccuracy%20for%20several%20benchmark%20tests.%20We%20extend%20this%20idea%20towards%20nonlinear%0Amodel%20order%20reduction%20by%20proposing%20an%20efficient%20framework%20that%20combines%20neural%0Anetworks%20with%20kernel%20principal%20component%20analysis%20%28KPCA%29%20for%20operator%20learning.%0AOur%20results%20demonstrate%20the%20superior%20performance%20of%20KPCA-DeepONet%20over%0APOD-DeepONet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18735v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20model%20reduction%20for%20operator%20learning&entry.906535625=Hamidreza%20Eivazi%20and%20Stefan%20Wittek%20and%20Andreas%20Rausch&entry.1292438233=%20%20Operator%20learning%20provides%20methods%20to%20approximate%20mappings%20between%0Ainfinite-dimensional%20function%20spaces.%20Deep%20operator%20networks%20%28DeepONets%29%20are%20a%0Anotable%20architecture%20in%20this%20field.%20Recently%2C%20an%20extension%20of%20DeepONet%20based%20on%0Amodel%20reduction%20and%20neural%20networks%2C%20proper%20orthogonal%20decomposition%0A%28POD%29-DeepONet%2C%20has%20been%20able%20to%20outperform%20other%20architectures%20in%20terms%20of%0Aaccuracy%20for%20several%20benchmark%20tests.%20We%20extend%20this%20idea%20towards%20nonlinear%0Amodel%20order%20reduction%20by%20proposing%20an%20efficient%20framework%20that%20combines%20neural%0Anetworks%20with%20kernel%20principal%20component%20analysis%20%28KPCA%29%20for%20operator%20learning.%0AOur%20results%20demonstrate%20the%20superior%20performance%20of%20KPCA-DeepONet%20over%0APOD-DeepONet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18735v1&entry.124074799=Read"},
{"title": "Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives", "author": "Wenqi Zhang and Yongliang Shen and Linjuan Wu and Qiuying Peng and Jun Wang and Yueting Zhuang and Weiming Lu", "abstract": "  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n", "link": "http://arxiv.org/abs/2401.02009v2", "date": "2024-03-27", "relevancy": 2.3084, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4745}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.465}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4454}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Contrast%3A%20Better%20Reflection%20Through%20Inconsistent%20Solving%0A%20%20Perspectives&body=Title%3A%20Self-Contrast%3A%20Better%20Reflection%20Through%20Inconsistent%20Solving%0A%20%20Perspectives%0AAuthor%3A%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Linjuan%20Wu%20and%20Qiuying%20Peng%20and%20Jun%20Wang%20and%20Yueting%20Zhuang%20and%20Weiming%20Lu%0AAbstract%3A%20%20%20The%20reflection%20capacity%20of%20Large%20Language%20Model%20%28LLM%29%20has%20garnered%20extensive%0Aattention.%20A%20post-hoc%20prompting%20strategy%2C%20e.g.%2C%20reflexion%20and%20self-refine%2C%0Arefines%20LLM%27s%20response%20based%20on%20self-evaluated%20or%20external%20feedback.%20However%2C%0Arecent%20research%20indicates%20without%20external%20feedback%2C%20LLM%27s%20intrinsic%20reflection%0Ais%20unstable.%20Our%20investigation%20unveils%20that%20the%20key%20bottleneck%20is%20the%20quality%0Aof%20the%20self-evaluated%20feedback.%20We%20find%20LLMs%20often%20exhibit%20overconfidence%20or%0Ahigh%20randomness%20when%20self-evaluate%2C%20offering%20stubborn%20or%20inconsistent%20feedback%2C%0Awhich%20causes%20poor%20reflection.%20To%20remedy%20this%2C%20we%20advocate%20Self-Contrast%3A%20It%0Aadaptively%20explores%20diverse%20solving%20perspectives%20tailored%20to%20the%20request%2C%0Acontrasts%20the%20differences%2C%20and%20summarizes%20these%20discrepancies%20into%20a%20checklist%0Awhich%20could%20be%20used%20to%20re-examine%20and%20eliminate%20discrepancies.%20Our%20method%0Aendows%20LLM%20with%20diverse%20perspectives%20to%20alleviate%20stubborn%20biases.%20Moreover%2C%0Atheir%20discrepancies%20indicate%20potential%20errors%20or%20inherent%20uncertainties%20that%0ALLM%20often%20overlooks.%20Reflecting%20upon%20these%20can%20catalyze%20more%20accurate%20and%0Astable%20reflection.%20Experiments%20conducted%20on%20a%20series%20of%20reasoning%20and%0Atranslation%20tasks%20with%20different%20LLMs%20serve%20to%20underscore%20the%20effectiveness%20and%0Agenerality%20of%20our%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02009v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Contrast%3A%20Better%20Reflection%20Through%20Inconsistent%20Solving%0A%20%20Perspectives&entry.906535625=Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Linjuan%20Wu%20and%20Qiuying%20Peng%20and%20Jun%20Wang%20and%20Yueting%20Zhuang%20and%20Weiming%20Lu&entry.1292438233=%20%20The%20reflection%20capacity%20of%20Large%20Language%20Model%20%28LLM%29%20has%20garnered%20extensive%0Aattention.%20A%20post-hoc%20prompting%20strategy%2C%20e.g.%2C%20reflexion%20and%20self-refine%2C%0Arefines%20LLM%27s%20response%20based%20on%20self-evaluated%20or%20external%20feedback.%20However%2C%0Arecent%20research%20indicates%20without%20external%20feedback%2C%20LLM%27s%20intrinsic%20reflection%0Ais%20unstable.%20Our%20investigation%20unveils%20that%20the%20key%20bottleneck%20is%20the%20quality%0Aof%20the%20self-evaluated%20feedback.%20We%20find%20LLMs%20often%20exhibit%20overconfidence%20or%0Ahigh%20randomness%20when%20self-evaluate%2C%20offering%20stubborn%20or%20inconsistent%20feedback%2C%0Awhich%20causes%20poor%20reflection.%20To%20remedy%20this%2C%20we%20advocate%20Self-Contrast%3A%20It%0Aadaptively%20explores%20diverse%20solving%20perspectives%20tailored%20to%20the%20request%2C%0Acontrasts%20the%20differences%2C%20and%20summarizes%20these%20discrepancies%20into%20a%20checklist%0Awhich%20could%20be%20used%20to%20re-examine%20and%20eliminate%20discrepancies.%20Our%20method%0Aendows%20LLM%20with%20diverse%20perspectives%20to%20alleviate%20stubborn%20biases.%20Moreover%2C%0Atheir%20discrepancies%20indicate%20potential%20errors%20or%20inherent%20uncertainties%20that%0ALLM%20often%20overlooks.%20Reflecting%20upon%20these%20can%20catalyze%20more%20accurate%20and%0Astable%20reflection.%20Experiments%20conducted%20on%20a%20series%20of%20reasoning%20and%0Atranslation%20tasks%20with%20different%20LLMs%20serve%20to%20underscore%20the%20effectiveness%20and%0Agenerality%20of%20our%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02009v2&entry.124074799=Read"},
{"title": "Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence", "author": "Zhiming Mao and Haoli Bai and Lu Hou and Jiansheng Wei and Xin Jiang and Qun Liu and Kam-Fai Wong", "abstract": "  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n", "link": "http://arxiv.org/abs/2403.16516v2", "date": "2024-03-27", "relevancy": 2.2883, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visually%20Guided%20Generative%20Text-Layout%20Pre-training%20for%20Document%0A%20%20Intelligence&body=Title%3A%20Visually%20Guided%20Generative%20Text-Layout%20Pre-training%20for%20Document%0A%20%20Intelligence%0AAuthor%3A%20Zhiming%20Mao%20and%20Haoli%20Bai%20and%20Lu%20Hou%20and%20Jiansheng%20Wei%20and%20Xin%20Jiang%20and%20Qun%20Liu%20and%20Kam-Fai%20Wong%0AAbstract%3A%20%20%20Prior%20study%20shows%20that%20pre-training%20techniques%20can%20boost%20the%20performance%20of%0Avisual%20document%20understanding%20%28VDU%29%2C%20which%20typically%20requires%20models%20to%20gain%0Aabilities%20to%20perceive%20and%20reason%20both%20document%20texts%20and%20layouts%20%28e.g.%2C%0Alocations%20of%20texts%20and%20table-cells%29.%20To%20this%20end%2C%20we%20propose%20visually%20guided%0Agenerative%20text-layout%20pre-training%2C%20named%20ViTLP.%20Given%20a%20document%20image%2C%20the%0Amodel%20optimizes%20hierarchical%20language%20and%20layout%20modeling%20objectives%20to%0Agenerate%20the%20interleaved%20text%20and%20layout%20sequence.%20In%20addition%2C%20to%20address%20the%0Alimitation%20of%20processing%20long%20documents%20by%20Transformers%2C%20we%20introduce%20a%0Astraightforward%20yet%20effective%20multi-segment%20generative%20pre-training%20scheme%2C%0Afacilitating%20ViTLP%20to%20process%20word-intensive%20documents%20of%20any%20length.%20ViTLP%20can%0Afunction%20as%20a%20native%20OCR%20model%20to%20localize%20and%20recognize%20texts%20of%20document%0Aimages.%20Besides%2C%20ViTLP%20can%20be%20effectively%20applied%20to%20various%20downstream%20VDU%0Atasks.%20Extensive%20experiments%20show%20that%20ViTLP%20achieves%20competitive%20performance%0Aover%20existing%20baselines%20on%20benchmark%20VDU%20tasks%2C%20including%20information%0Aextraction%2C%20document%20classification%2C%20and%20document%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16516v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visually%20Guided%20Generative%20Text-Layout%20Pre-training%20for%20Document%0A%20%20Intelligence&entry.906535625=Zhiming%20Mao%20and%20Haoli%20Bai%20and%20Lu%20Hou%20and%20Jiansheng%20Wei%20and%20Xin%20Jiang%20and%20Qun%20Liu%20and%20Kam-Fai%20Wong&entry.1292438233=%20%20Prior%20study%20shows%20that%20pre-training%20techniques%20can%20boost%20the%20performance%20of%0Avisual%20document%20understanding%20%28VDU%29%2C%20which%20typically%20requires%20models%20to%20gain%0Aabilities%20to%20perceive%20and%20reason%20both%20document%20texts%20and%20layouts%20%28e.g.%2C%0Alocations%20of%20texts%20and%20table-cells%29.%20To%20this%20end%2C%20we%20propose%20visually%20guided%0Agenerative%20text-layout%20pre-training%2C%20named%20ViTLP.%20Given%20a%20document%20image%2C%20the%0Amodel%20optimizes%20hierarchical%20language%20and%20layout%20modeling%20objectives%20to%0Agenerate%20the%20interleaved%20text%20and%20layout%20sequence.%20In%20addition%2C%20to%20address%20the%0Alimitation%20of%20processing%20long%20documents%20by%20Transformers%2C%20we%20introduce%20a%0Astraightforward%20yet%20effective%20multi-segment%20generative%20pre-training%20scheme%2C%0Afacilitating%20ViTLP%20to%20process%20word-intensive%20documents%20of%20any%20length.%20ViTLP%20can%0Afunction%20as%20a%20native%20OCR%20model%20to%20localize%20and%20recognize%20texts%20of%20document%0Aimages.%20Besides%2C%20ViTLP%20can%20be%20effectively%20applied%20to%20various%20downstream%20VDU%0Atasks.%20Extensive%20experiments%20show%20that%20ViTLP%20achieves%20competitive%20performance%0Aover%20existing%20baselines%20on%20benchmark%20VDU%20tasks%2C%20including%20information%0Aextraction%2C%20document%20classification%2C%20and%20document%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16516v2&entry.124074799=Read"},
{"title": "Neural Architecture Search for Sentence Classification with BERT", "author": "Philip Kenneweg and Sarah Schr\u00f6der and Barbara Hammer", "abstract": "  Pre training of language models on large text corpora is common practice in\nNatural Language Processing. Following, fine tuning of these models is\nperformed to achieve the best results on a variety of tasks. In this paper we\nquestion the common practice of only adding a single output layer as a\nclassification head on top of the network. We perform an AutoML search to find\narchitectures that outperform the current single layer at only a small compute\ncost. We validate our classification architecture on a variety of NLP\nbenchmarks from the GLUE dataset.\n", "link": "http://arxiv.org/abs/2403.18547v1", "date": "2024-03-27", "relevancy": 2.2774, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4535}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Architecture%20Search%20for%20Sentence%20Classification%20with%20BERT&body=Title%3A%20Neural%20Architecture%20Search%20for%20Sentence%20Classification%20with%20BERT%0AAuthor%3A%20Philip%20Kenneweg%20and%20Sarah%20Schr%C3%B6der%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Pre%20training%20of%20language%20models%20on%20large%20text%20corpora%20is%20common%20practice%20in%0ANatural%20Language%20Processing.%20Following%2C%20fine%20tuning%20of%20these%20models%20is%0Aperformed%20to%20achieve%20the%20best%20results%20on%20a%20variety%20of%20tasks.%20In%20this%20paper%20we%0Aquestion%20the%20common%20practice%20of%20only%20adding%20a%20single%20output%20layer%20as%20a%0Aclassification%20head%20on%20top%20of%20the%20network.%20We%20perform%20an%20AutoML%20search%20to%20find%0Aarchitectures%20that%20outperform%20the%20current%20single%20layer%20at%20only%20a%20small%20compute%0Acost.%20We%20validate%20our%20classification%20architecture%20on%20a%20variety%20of%20NLP%0Abenchmarks%20from%20the%20GLUE%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18547v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Architecture%20Search%20for%20Sentence%20Classification%20with%20BERT&entry.906535625=Philip%20Kenneweg%20and%20Sarah%20Schr%C3%B6der%20and%20Barbara%20Hammer&entry.1292438233=%20%20Pre%20training%20of%20language%20models%20on%20large%20text%20corpora%20is%20common%20practice%20in%0ANatural%20Language%20Processing.%20Following%2C%20fine%20tuning%20of%20these%20models%20is%0Aperformed%20to%20achieve%20the%20best%20results%20on%20a%20variety%20of%20tasks.%20In%20this%20paper%20we%0Aquestion%20the%20common%20practice%20of%20only%20adding%20a%20single%20output%20layer%20as%20a%0Aclassification%20head%20on%20top%20of%20the%20network.%20We%20perform%20an%20AutoML%20search%20to%20find%0Aarchitectures%20that%20outperform%20the%20current%20single%20layer%20at%20only%20a%20small%20compute%0Acost.%20We%20validate%20our%20classification%20architecture%20on%20a%20variety%20of%20NLP%0Abenchmarks%20from%20the%20GLUE%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18547v1&entry.124074799=Read"},
{"title": "DRIVE: Data-driven Robot Input Vector Exploration", "author": "Dominic Baril and Simon-Pierre Desch\u00eanes and Luc Coupal and Cyril Goffin and Julien L\u00e9pine and Philippe Gigu\u00e8re and Fran\u00e7ois Pomerleau", "abstract": "  An accurate motion model is a fundamental component of most autonomous\nnavigation systems. While much work has been done on improving model\nformulation, no standard protocol exists for gathering empirical data required\nto train models. In this work, we address this issue by proposing Data-driven\nRobot Input Vector Exploration (DRIVE), a protocol that enables characterizing\nuncrewed ground vehicles (UGVs) input limits and gathering empirical model\ntraining data. We also propose a novel learned slip approach outperforming\nsimilar acceleration learning approaches. Our contributions are validated\nthrough an extensive experimental evaluation, cumulating over 7 km and 1.8 h of\ndriving data over three distinct UGVs and four terrain types. We show that our\nprotocol offers increased predictive performance over common human-driven\ndata-gathering protocols. Furthermore, our protocol converges with 46 s of\ntraining data, almost four times less than the shortest human dataset gathering\nprotocol. We show that the operational limit for our model is reached in\nextreme slip conditions encountered on surfaced ice. DRIVE is an efficient way\nof characterizing UGV motion in its operational conditions. Our code and\ndataset are both available online at this link:\nhttps://github.com/norlab-ulaval/DRIVE.\n", "link": "http://arxiv.org/abs/2309.10718v2", "date": "2024-03-27", "relevancy": 2.2768, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5718}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.569}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5667}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DRIVE%3A%20Data-driven%20Robot%20Input%20Vector%20Exploration&body=Title%3A%20DRIVE%3A%20Data-driven%20Robot%20Input%20Vector%20Exploration%0AAuthor%3A%20Dominic%20Baril%20and%20Simon-Pierre%20Desch%C3%AAnes%20and%20Luc%20Coupal%20and%20Cyril%20Goffin%20and%20Julien%20L%C3%A9pine%20and%20Philippe%20Gigu%C3%A8re%20and%20Fran%C3%A7ois%20Pomerleau%0AAbstract%3A%20%20%20An%20accurate%20motion%20model%20is%20a%20fundamental%20component%20of%20most%20autonomous%0Anavigation%20systems.%20While%20much%20work%20has%20been%20done%20on%20improving%20model%0Aformulation%2C%20no%20standard%20protocol%20exists%20for%20gathering%20empirical%20data%20required%0Ato%20train%20models.%20In%20this%20work%2C%20we%20address%20this%20issue%20by%20proposing%20Data-driven%0ARobot%20Input%20Vector%20Exploration%20%28DRIVE%29%2C%20a%20protocol%20that%20enables%20characterizing%0Auncrewed%20ground%20vehicles%20%28UGVs%29%20input%20limits%20and%20gathering%20empirical%20model%0Atraining%20data.%20We%20also%20propose%20a%20novel%20learned%20slip%20approach%20outperforming%0Asimilar%20acceleration%20learning%20approaches.%20Our%20contributions%20are%20validated%0Athrough%20an%20extensive%20experimental%20evaluation%2C%20cumulating%20over%207%20km%20and%201.8%20h%20of%0Adriving%20data%20over%20three%20distinct%20UGVs%20and%20four%20terrain%20types.%20We%20show%20that%20our%0Aprotocol%20offers%20increased%20predictive%20performance%20over%20common%20human-driven%0Adata-gathering%20protocols.%20Furthermore%2C%20our%20protocol%20converges%20with%2046%20s%20of%0Atraining%20data%2C%20almost%20four%20times%20less%20than%20the%20shortest%20human%20dataset%20gathering%0Aprotocol.%20We%20show%20that%20the%20operational%20limit%20for%20our%20model%20is%20reached%20in%0Aextreme%20slip%20conditions%20encountered%20on%20surfaced%20ice.%20DRIVE%20is%20an%20efficient%20way%0Aof%20characterizing%20UGV%20motion%20in%20its%20operational%20conditions.%20Our%20code%20and%0Adataset%20are%20both%20available%20online%20at%20this%20link%3A%0Ahttps%3A//github.com/norlab-ulaval/DRIVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10718v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRIVE%3A%20Data-driven%20Robot%20Input%20Vector%20Exploration&entry.906535625=Dominic%20Baril%20and%20Simon-Pierre%20Desch%C3%AAnes%20and%20Luc%20Coupal%20and%20Cyril%20Goffin%20and%20Julien%20L%C3%A9pine%20and%20Philippe%20Gigu%C3%A8re%20and%20Fran%C3%A7ois%20Pomerleau&entry.1292438233=%20%20An%20accurate%20motion%20model%20is%20a%20fundamental%20component%20of%20most%20autonomous%0Anavigation%20systems.%20While%20much%20work%20has%20been%20done%20on%20improving%20model%0Aformulation%2C%20no%20standard%20protocol%20exists%20for%20gathering%20empirical%20data%20required%0Ato%20train%20models.%20In%20this%20work%2C%20we%20address%20this%20issue%20by%20proposing%20Data-driven%0ARobot%20Input%20Vector%20Exploration%20%28DRIVE%29%2C%20a%20protocol%20that%20enables%20characterizing%0Auncrewed%20ground%20vehicles%20%28UGVs%29%20input%20limits%20and%20gathering%20empirical%20model%0Atraining%20data.%20We%20also%20propose%20a%20novel%20learned%20slip%20approach%20outperforming%0Asimilar%20acceleration%20learning%20approaches.%20Our%20contributions%20are%20validated%0Athrough%20an%20extensive%20experimental%20evaluation%2C%20cumulating%20over%207%20km%20and%201.8%20h%20of%0Adriving%20data%20over%20three%20distinct%20UGVs%20and%20four%20terrain%20types.%20We%20show%20that%20our%0Aprotocol%20offers%20increased%20predictive%20performance%20over%20common%20human-driven%0Adata-gathering%20protocols.%20Furthermore%2C%20our%20protocol%20converges%20with%2046%20s%20of%0Atraining%20data%2C%20almost%20four%20times%20less%20than%20the%20shortest%20human%20dataset%20gathering%0Aprotocol.%20We%20show%20that%20the%20operational%20limit%20for%20our%20model%20is%20reached%20in%0Aextreme%20slip%20conditions%20encountered%20on%20surfaced%20ice.%20DRIVE%20is%20an%20efficient%20way%0Aof%20characterizing%20UGV%20motion%20in%20its%20operational%20conditions.%20Our%20code%20and%0Adataset%20are%20both%20available%20online%20at%20this%20link%3A%0Ahttps%3A//github.com/norlab-ulaval/DRIVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10718v2&entry.124074799=Read"},
{"title": "Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual\n  Test-Time Adaptation", "author": "Jiaming Liu and Ran Xu and Senqiao Yang and Renrui Zhang and Qizhe Zhang and Zehui Chen and Yandong Guo and Shanghang Zhang", "abstract": "  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\npre-trained model to continually changing target distributions, addressing\nreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\nor teacher-student pseudo-labeling schemes for knowledge extraction in\nunlabeled target domains. However, dynamic data distributions cause\nmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\nlearning methods, hindering the effective mitigation of error accumulation and\ncatastrophic forgetting problems during the continual adaptation process. To\ntackle these issues, we propose a continual self-supervised method, Adaptive\nDistribution Masked Autoencoders (ADMA), which enhances the extraction of\ntarget domain knowledge while mitigating the accumulation of distribution\nshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\nto adaptively sample masked positions, followed by establishing consistency\nconstraints between the masked target samples and the original target samples.\nAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\na hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\nleveraging its invariant properties to boost task-relevant representations.\nThrough conducting extensive experiments on four widely recognized benchmarks,\nour proposed method attains state-of-the-art performance in both classification\nand segmentation CTTA tasks. Our project page:\nhttps://sites.google.com/view/continual-mae/home.\n", "link": "http://arxiv.org/abs/2312.12480v2", "date": "2024-03-27", "relevancy": 2.2756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5661}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual-MAE%3A%20Adaptive%20Distribution%20Masked%20Autoencoders%20for%20Continual%0A%20%20Test-Time%20Adaptation&body=Title%3A%20Continual-MAE%3A%20Adaptive%20Distribution%20Masked%20Autoencoders%20for%20Continual%0A%20%20Test-Time%20Adaptation%0AAuthor%3A%20Jiaming%20Liu%20and%20Ran%20Xu%20and%20Senqiao%20Yang%20and%20Renrui%20Zhang%20and%20Qizhe%20Zhang%20and%20Zehui%20Chen%20and%20Yandong%20Guo%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20is%20proposed%20to%20migrate%20a%20source%0Apre-trained%20model%20to%20continually%20changing%20target%20distributions%2C%20addressing%0Areal-world%20dynamism.%20Existing%20CTTA%20methods%20mainly%20rely%20on%20entropy%20minimization%0Aor%20teacher-student%20pseudo-labeling%20schemes%20for%20knowledge%20extraction%20in%0Aunlabeled%20target%20domains.%20However%2C%20dynamic%20data%20distributions%20cause%0Amiscalibrated%20predictions%20and%20noisy%20pseudo-labels%20in%20existing%20self-supervised%0Alearning%20methods%2C%20hindering%20the%20effective%20mitigation%20of%20error%20accumulation%20and%0Acatastrophic%20forgetting%20problems%20during%20the%20continual%20adaptation%20process.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20continual%20self-supervised%20method%2C%20Adaptive%0ADistribution%20Masked%20Autoencoders%20%28ADMA%29%2C%20which%20enhances%20the%20extraction%20of%0Atarget%20domain%20knowledge%20while%20mitigating%20the%20accumulation%20of%20distribution%0Ashifts.%20Specifically%2C%20we%20propose%20a%20Distribution-aware%20Masking%20%28DaM%29%20mechanism%0Ato%20adaptively%20sample%20masked%20positions%2C%20followed%20by%20establishing%20consistency%0Aconstraints%20between%20the%20masked%20target%20samples%20and%20the%20original%20target%20samples.%0AAdditionally%2C%20for%20masked%20tokens%2C%20we%20utilize%20an%20efficient%20decoder%20to%20reconstruct%0Aa%20hand-crafted%20feature%20descriptor%20%28e.g.%2C%20Histograms%20of%20Oriented%20Gradients%29%2C%0Aleveraging%20its%20invariant%20properties%20to%20boost%20task-relevant%20representations.%0AThrough%20conducting%20extensive%20experiments%20on%20four%20widely%20recognized%20benchmarks%2C%0Aour%20proposed%20method%20attains%20state-of-the-art%20performance%20in%20both%20classification%0Aand%20segmentation%20CTTA%20tasks.%20Our%20project%20page%3A%0Ahttps%3A//sites.google.com/view/continual-mae/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12480v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual-MAE%3A%20Adaptive%20Distribution%20Masked%20Autoencoders%20for%20Continual%0A%20%20Test-Time%20Adaptation&entry.906535625=Jiaming%20Liu%20and%20Ran%20Xu%20and%20Senqiao%20Yang%20and%20Renrui%20Zhang%20and%20Qizhe%20Zhang%20and%20Zehui%20Chen%20and%20Yandong%20Guo%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20is%20proposed%20to%20migrate%20a%20source%0Apre-trained%20model%20to%20continually%20changing%20target%20distributions%2C%20addressing%0Areal-world%20dynamism.%20Existing%20CTTA%20methods%20mainly%20rely%20on%20entropy%20minimization%0Aor%20teacher-student%20pseudo-labeling%20schemes%20for%20knowledge%20extraction%20in%0Aunlabeled%20target%20domains.%20However%2C%20dynamic%20data%20distributions%20cause%0Amiscalibrated%20predictions%20and%20noisy%20pseudo-labels%20in%20existing%20self-supervised%0Alearning%20methods%2C%20hindering%20the%20effective%20mitigation%20of%20error%20accumulation%20and%0Acatastrophic%20forgetting%20problems%20during%20the%20continual%20adaptation%20process.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20continual%20self-supervised%20method%2C%20Adaptive%0ADistribution%20Masked%20Autoencoders%20%28ADMA%29%2C%20which%20enhances%20the%20extraction%20of%0Atarget%20domain%20knowledge%20while%20mitigating%20the%20accumulation%20of%20distribution%0Ashifts.%20Specifically%2C%20we%20propose%20a%20Distribution-aware%20Masking%20%28DaM%29%20mechanism%0Ato%20adaptively%20sample%20masked%20positions%2C%20followed%20by%20establishing%20consistency%0Aconstraints%20between%20the%20masked%20target%20samples%20and%20the%20original%20target%20samples.%0AAdditionally%2C%20for%20masked%20tokens%2C%20we%20utilize%20an%20efficient%20decoder%20to%20reconstruct%0Aa%20hand-crafted%20feature%20descriptor%20%28e.g.%2C%20Histograms%20of%20Oriented%20Gradients%29%2C%0Aleveraging%20its%20invariant%20properties%20to%20boost%20task-relevant%20representations.%0AThrough%20conducting%20extensive%20experiments%20on%20four%20widely%20recognized%20benchmarks%2C%0Aour%20proposed%20method%20attains%20state-of-the-art%20performance%20in%20both%20classification%0Aand%20segmentation%20CTTA%20tasks.%20Our%20project%20page%3A%0Ahttps%3A//sites.google.com/view/continual-mae/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12480v2&entry.124074799=Read"},
{"title": "Backpropagation-free Network for 3D Test-time Adaptation", "author": "Yanshuo Wang and Ali Cheraghian and Zeeshan Hayder and Jie Hong and Sameera Ramasinghe and Shafin Rahman and David Ahmedt-Aristizabal and Xuesong Li and Lars Petersson and Mehrtash Harandi", "abstract": "  Real-world systems often encounter new data over time, which leads to\nexperiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods\ntend to apply computationally heavy and memory-intensive backpropagation-based\napproaches to handle this. Here, we propose a novel method that uses a\nbackpropagation-free approach for TTA for the specific case of 3D data. Our\nmodel uses a two-stream architecture to maintain knowledge about the source\ndomain as well as complementary target-domain-specific information. The\nbackpropagation-free property of our model helps address the well-known\nforgetting problem and mitigates the error accumulation issue. The proposed\nmethod also eliminates the need for the usually noisy process of\npseudo-labeling and reliance on costly self-supervised training. Moreover, our\nmethod leverages subspace learning, effectively reducing the distribution\nvariance between the two domains. Furthermore, the source-domain-specific and\nthe target-domain-specific streams are aligned using a novel entropy-based\nadaptive fusion strategy. Extensive experiments on popular benchmarks\ndemonstrate the effectiveness of our method. The code will be available at\nhttps://github.com/abie-e/BFTT3D.\n", "link": "http://arxiv.org/abs/2403.18442v1", "date": "2024-03-27", "relevancy": 2.2588, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5335}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Backpropagation-free%20Network%20for%203D%20Test-time%20Adaptation&body=Title%3A%20Backpropagation-free%20Network%20for%203D%20Test-time%20Adaptation%0AAuthor%3A%20Yanshuo%20Wang%20and%20Ali%20Cheraghian%20and%20Zeeshan%20Hayder%20and%20Jie%20Hong%20and%20Sameera%20Ramasinghe%20and%20Shafin%20Rahman%20and%20David%20Ahmedt-Aristizabal%20and%20Xuesong%20Li%20and%20Lars%20Petersson%20and%20Mehrtash%20Harandi%0AAbstract%3A%20%20%20Real-world%20systems%20often%20encounter%20new%20data%20over%20time%2C%20which%20leads%20to%0Aexperiencing%20target%20domain%20shifts.%20Existing%20Test-Time%20Adaptation%20%28TTA%29%20methods%0Atend%20to%20apply%20computationally%20heavy%20and%20memory-intensive%20backpropagation-based%0Aapproaches%20to%20handle%20this.%20Here%2C%20we%20propose%20a%20novel%20method%20that%20uses%20a%0Abackpropagation-free%20approach%20for%20TTA%20for%20the%20specific%20case%20of%203D%20data.%20Our%0Amodel%20uses%20a%20two-stream%20architecture%20to%20maintain%20knowledge%20about%20the%20source%0Adomain%20as%20well%20as%20complementary%20target-domain-specific%20information.%20The%0Abackpropagation-free%20property%20of%20our%20model%20helps%20address%20the%20well-known%0Aforgetting%20problem%20and%20mitigates%20the%20error%20accumulation%20issue.%20The%20proposed%0Amethod%20also%20eliminates%20the%20need%20for%20the%20usually%20noisy%20process%20of%0Apseudo-labeling%20and%20reliance%20on%20costly%20self-supervised%20training.%20Moreover%2C%20our%0Amethod%20leverages%20subspace%20learning%2C%20effectively%20reducing%20the%20distribution%0Avariance%20between%20the%20two%20domains.%20Furthermore%2C%20the%20source-domain-specific%20and%0Athe%20target-domain-specific%20streams%20are%20aligned%20using%20a%20novel%20entropy-based%0Aadaptive%20fusion%20strategy.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/abie-e/BFTT3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18442v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backpropagation-free%20Network%20for%203D%20Test-time%20Adaptation&entry.906535625=Yanshuo%20Wang%20and%20Ali%20Cheraghian%20and%20Zeeshan%20Hayder%20and%20Jie%20Hong%20and%20Sameera%20Ramasinghe%20and%20Shafin%20Rahman%20and%20David%20Ahmedt-Aristizabal%20and%20Xuesong%20Li%20and%20Lars%20Petersson%20and%20Mehrtash%20Harandi&entry.1292438233=%20%20Real-world%20systems%20often%20encounter%20new%20data%20over%20time%2C%20which%20leads%20to%0Aexperiencing%20target%20domain%20shifts.%20Existing%20Test-Time%20Adaptation%20%28TTA%29%20methods%0Atend%20to%20apply%20computationally%20heavy%20and%20memory-intensive%20backpropagation-based%0Aapproaches%20to%20handle%20this.%20Here%2C%20we%20propose%20a%20novel%20method%20that%20uses%20a%0Abackpropagation-free%20approach%20for%20TTA%20for%20the%20specific%20case%20of%203D%20data.%20Our%0Amodel%20uses%20a%20two-stream%20architecture%20to%20maintain%20knowledge%20about%20the%20source%0Adomain%20as%20well%20as%20complementary%20target-domain-specific%20information.%20The%0Abackpropagation-free%20property%20of%20our%20model%20helps%20address%20the%20well-known%0Aforgetting%20problem%20and%20mitigates%20the%20error%20accumulation%20issue.%20The%20proposed%0Amethod%20also%20eliminates%20the%20need%20for%20the%20usually%20noisy%20process%20of%0Apseudo-labeling%20and%20reliance%20on%20costly%20self-supervised%20training.%20Moreover%2C%20our%0Amethod%20leverages%20subspace%20learning%2C%20effectively%20reducing%20the%20distribution%0Avariance%20between%20the%20two%20domains.%20Furthermore%2C%20the%20source-domain-specific%20and%0Athe%20target-domain-specific%20streams%20are%20aligned%20using%20a%20novel%20entropy-based%0Aadaptive%20fusion%20strategy.%20Extensive%20experiments%20on%20popular%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20our%20method.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/abie-e/BFTT3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18442v1&entry.124074799=Read"},
{"title": "A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation", "author": "Francesco Galati and Daniele Falcetta and Rosa Cortese and Barbara Casolla and Ferran Prados and Ninon Burgos and Maria A. Zuluaga", "abstract": "  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n", "link": "http://arxiv.org/abs/2309.06075v2", "date": "2024-03-27", "relevancy": 2.2532, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5978}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation&body=Title%3A%20A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation%0AAuthor%3A%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Rosa%20Cortese%20and%20Barbara%20Casolla%20and%20Ferran%20Prados%20and%20Ninon%20Burgos%20and%20Maria%20A.%20Zuluaga%0AAbstract%3A%20%20%20We%20present%20a%20semi-supervised%20domain%20adaptation%20framework%20for%20brain%20vessel%0Asegmentation%20from%20different%20image%20modalities.%20Existing%20state-of-the-art%20methods%0Afocus%20on%20a%20single%20modality%2C%20despite%20the%20wide%20range%20of%20available%20cerebrovascular%0Aimaging%20techniques.%20This%20can%20lead%20to%20significant%20distribution%20shifts%20that%0Anegatively%20impact%20the%20generalization%20across%20modalities.%20By%20relying%20on%20annotated%0Aangiographies%20and%20a%20limited%20number%20of%20annotated%20venographies%2C%20our%20framework%0Aaccomplishes%20image-to-image%20translation%20and%20semantic%20segmentation%2C%20leveraging%20a%0Adisentangled%20and%20semantically%20rich%20latent%20space%20to%20represent%20heterogeneous%20data%0Aand%20perform%20image-level%20adaptation%20from%20source%20to%20target%20domains.%20Moreover%2C%20we%0Areduce%20the%20typical%20complexity%20of%20cycle-based%20architectures%20and%20minimize%20the%20use%0Aof%20adversarial%20training%2C%20which%20allows%20us%20to%20build%20an%20efficient%20and%20intuitive%0Amodel%20with%20stable%20training.%20We%20evaluate%20our%20method%20on%20magnetic%20resonance%0Aangiographies%20and%20venographies.%20While%20achieving%20state-of-the-art%20performance%20in%0Athe%20source%20domain%2C%20our%20method%20attains%20a%20Dice%20score%20coefficient%20in%20the%20target%0Adomain%20that%20is%20only%208.9%25%20lower%2C%20highlighting%20its%20promising%20potential%20for%20robust%0Acerebrovascular%20image%20segmentation%20across%20different%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06075v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation&entry.906535625=Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Rosa%20Cortese%20and%20Barbara%20Casolla%20and%20Ferran%20Prados%20and%20Ninon%20Burgos%20and%20Maria%20A.%20Zuluaga&entry.1292438233=%20%20We%20present%20a%20semi-supervised%20domain%20adaptation%20framework%20for%20brain%20vessel%0Asegmentation%20from%20different%20image%20modalities.%20Existing%20state-of-the-art%20methods%0Afocus%20on%20a%20single%20modality%2C%20despite%20the%20wide%20range%20of%20available%20cerebrovascular%0Aimaging%20techniques.%20This%20can%20lead%20to%20significant%20distribution%20shifts%20that%0Anegatively%20impact%20the%20generalization%20across%20modalities.%20By%20relying%20on%20annotated%0Aangiographies%20and%20a%20limited%20number%20of%20annotated%20venographies%2C%20our%20framework%0Aaccomplishes%20image-to-image%20translation%20and%20semantic%20segmentation%2C%20leveraging%20a%0Adisentangled%20and%20semantically%20rich%20latent%20space%20to%20represent%20heterogeneous%20data%0Aand%20perform%20image-level%20adaptation%20from%20source%20to%20target%20domains.%20Moreover%2C%20we%0Areduce%20the%20typical%20complexity%20of%20cycle-based%20architectures%20and%20minimize%20the%20use%0Aof%20adversarial%20training%2C%20which%20allows%20us%20to%20build%20an%20efficient%20and%20intuitive%0Amodel%20with%20stable%20training.%20We%20evaluate%20our%20method%20on%20magnetic%20resonance%0Aangiographies%20and%20venographies.%20While%20achieving%20state-of-the-art%20performance%20in%0Athe%20source%20domain%2C%20our%20method%20attains%20a%20Dice%20score%20coefficient%20in%20the%20target%0Adomain%20that%20is%20only%208.9%25%20lower%2C%20highlighting%20its%20promising%20potential%20for%20robust%0Acerebrovascular%20image%20segmentation%20across%20different%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06075v2&entry.124074799=Read"},
{"title": "A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation", "author": "Francesco Galati and Daniele Falcetta and Rosa Cortese and Barbara Casolla and Ferran Prados and Ninon Burgos and Maria A. Zuluaga", "abstract": "  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n", "link": "http://arxiv.org/abs/2309.06075v2", "date": "2024-03-27", "relevancy": 2.253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5977}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation&body=Title%3A%20A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation%0AAuthor%3A%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Rosa%20Cortese%20and%20Barbara%20Casolla%20and%20Ferran%20Prados%20and%20Ninon%20Burgos%20and%20Maria%20A.%20Zuluaga%0AAbstract%3A%20%20%20We%20present%20a%20semi-supervised%20domain%20adaptation%20framework%20for%20brain%20vessel%0Asegmentation%20from%20different%20image%20modalities.%20Existing%20state-of-the-art%20methods%0Afocus%20on%20a%20single%20modality%2C%20despite%20the%20wide%20range%20of%20available%20cerebrovascular%0Aimaging%20techniques.%20This%20can%20lead%20to%20significant%20distribution%20shifts%20that%0Anegatively%20impact%20the%20generalization%20across%20modalities.%20By%20relying%20on%20annotated%0Aangiographies%20and%20a%20limited%20number%20of%20annotated%20venographies%2C%20our%20framework%0Aaccomplishes%20image-to-image%20translation%20and%20semantic%20segmentation%2C%20leveraging%20a%0Adisentangled%20and%20semantically%20rich%20latent%20space%20to%20represent%20heterogeneous%20data%0Aand%20perform%20image-level%20adaptation%20from%20source%20to%20target%20domains.%20Moreover%2C%20we%0Areduce%20the%20typical%20complexity%20of%20cycle-based%20architectures%20and%20minimize%20the%20use%0Aof%20adversarial%20training%2C%20which%20allows%20us%20to%20build%20an%20efficient%20and%20intuitive%0Amodel%20with%20stable%20training.%20We%20evaluate%20our%20method%20on%20magnetic%20resonance%0Aangiographies%20and%20venographies.%20While%20achieving%20state-of-the-art%20performance%20in%0Athe%20source%20domain%2C%20our%20method%20attains%20a%20Dice%20score%20coefficient%20in%20the%20target%0Adomain%20that%20is%20only%208.9%25%20lower%2C%20highlighting%20its%20promising%20potential%20for%20robust%0Acerebrovascular%20image%20segmentation%20across%20different%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06075v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2V%3A%20A%20Semi-Supervised%20Domain%20Adaptation%20Framework%20for%20Brain%20Vessel%0A%20%20Segmentation%20via%20Two-Phase%20Training%20Angiography-to-Venography%20Translation&entry.906535625=Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Rosa%20Cortese%20and%20Barbara%20Casolla%20and%20Ferran%20Prados%20and%20Ninon%20Burgos%20and%20Maria%20A.%20Zuluaga&entry.1292438233=%20%20We%20present%20a%20semi-supervised%20domain%20adaptation%20framework%20for%20brain%20vessel%0Asegmentation%20from%20different%20image%20modalities.%20Existing%20state-of-the-art%20methods%0Afocus%20on%20a%20single%20modality%2C%20despite%20the%20wide%20range%20of%20available%20cerebrovascular%0Aimaging%20techniques.%20This%20can%20lead%20to%20significant%20distribution%20shifts%20that%0Anegatively%20impact%20the%20generalization%20across%20modalities.%20By%20relying%20on%20annotated%0Aangiographies%20and%20a%20limited%20number%20of%20annotated%20venographies%2C%20our%20framework%0Aaccomplishes%20image-to-image%20translation%20and%20semantic%20segmentation%2C%20leveraging%20a%0Adisentangled%20and%20semantically%20rich%20latent%20space%20to%20represent%20heterogeneous%20data%0Aand%20perform%20image-level%20adaptation%20from%20source%20to%20target%20domains.%20Moreover%2C%20we%0Areduce%20the%20typical%20complexity%20of%20cycle-based%20architectures%20and%20minimize%20the%20use%0Aof%20adversarial%20training%2C%20which%20allows%20us%20to%20build%20an%20efficient%20and%20intuitive%0Amodel%20with%20stable%20training.%20We%20evaluate%20our%20method%20on%20magnetic%20resonance%0Aangiographies%20and%20venographies.%20While%20achieving%20state-of-the-art%20performance%20in%0Athe%20source%20domain%2C%20our%20method%20attains%20a%20Dice%20score%20coefficient%20in%20the%20target%0Adomain%20that%20is%20only%208.9%25%20lower%2C%20highlighting%20its%20promising%20potential%20for%20robust%0Acerebrovascular%20image%20segmentation%20across%20different%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06075v2&entry.124074799=Read"},
{"title": "Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning", "author": "Yang Yu and Danruo Deng and Furui Liu and Yueming Jin and Qi Dou and Guangyong Chen and Pheng-Ann Heng", "abstract": "  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n", "link": "http://arxiv.org/abs/2303.12091v3", "date": "2024-03-27", "relevancy": 2.2466, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6645}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5432}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Negative%20Evidential%20Deep%20Learning%20for%20Open-set%20Semi-supervised%0A%20%20Learning&body=Title%3A%20Adaptive%20Negative%20Evidential%20Deep%20Learning%20for%20Open-set%20Semi-supervised%0A%20%20Learning%0AAuthor%3A%20Yang%20Yu%20and%20Danruo%20Deng%20and%20Furui%20Liu%20and%20Yueming%20Jin%20and%20Qi%20Dou%20and%20Guangyong%20Chen%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20methods%20assume%20that%20labeled%20data%2C%20unlabeled%0Adata%20and%20test%20data%20are%20from%20the%20same%20distribution.%20Open-set%20semi-supervised%0Alearning%20%28Open-set%20SSL%29%20considers%20a%20more%20practical%20scenario%2C%20where%20unlabeled%0Adata%20and%20test%20data%20contain%20new%20categories%20%28outliers%29%20not%20observed%20in%20labeled%0Adata%20%28inliers%29.%20Most%20previous%20works%20focused%20on%20outlier%20detection%20via%20binary%0Aclassifiers%2C%20which%20suffer%20from%20insufficient%20scalability%20and%20inability%20to%0Adistinguish%20different%20types%20of%20uncertainty.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20Adaptive%20Negative%20Evidential%20Deep%20Learning%20%28ANEDL%29%20to%20tackle%20these%0Alimitations.%20Concretely%2C%20we%20first%20introduce%20evidential%20deep%20learning%20%28EDL%29%20as%0Aan%20outlier%20detector%20to%20quantify%20different%20types%20of%20uncertainty%2C%20and%20design%0Adifferent%20uncertainty%20metrics%20for%20self-training%20and%20inference.%20Furthermore%2C%20we%0Apropose%20a%20novel%20adaptive%20negative%20optimization%20strategy%2C%20making%20EDL%20more%0Atailored%20to%20the%20unlabeled%20dataset%20containing%20both%20inliers%20and%20outliers.%20As%0Ademonstrated%20empirically%2C%20our%20proposed%20method%20outperforms%20existing%0Astate-of-the-art%20methods%20across%20four%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12091v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Negative%20Evidential%20Deep%20Learning%20for%20Open-set%20Semi-supervised%0A%20%20Learning&entry.906535625=Yang%20Yu%20and%20Danruo%20Deng%20and%20Furui%20Liu%20and%20Yueming%20Jin%20and%20Qi%20Dou%20and%20Guangyong%20Chen%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20methods%20assume%20that%20labeled%20data%2C%20unlabeled%0Adata%20and%20test%20data%20are%20from%20the%20same%20distribution.%20Open-set%20semi-supervised%0Alearning%20%28Open-set%20SSL%29%20considers%20a%20more%20practical%20scenario%2C%20where%20unlabeled%0Adata%20and%20test%20data%20contain%20new%20categories%20%28outliers%29%20not%20observed%20in%20labeled%0Adata%20%28inliers%29.%20Most%20previous%20works%20focused%20on%20outlier%20detection%20via%20binary%0Aclassifiers%2C%20which%20suffer%20from%20insufficient%20scalability%20and%20inability%20to%0Adistinguish%20different%20types%20of%20uncertainty.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2C%20Adaptive%20Negative%20Evidential%20Deep%20Learning%20%28ANEDL%29%20to%20tackle%20these%0Alimitations.%20Concretely%2C%20we%20first%20introduce%20evidential%20deep%20learning%20%28EDL%29%20as%0Aan%20outlier%20detector%20to%20quantify%20different%20types%20of%20uncertainty%2C%20and%20design%0Adifferent%20uncertainty%20metrics%20for%20self-training%20and%20inference.%20Furthermore%2C%20we%0Apropose%20a%20novel%20adaptive%20negative%20optimization%20strategy%2C%20making%20EDL%20more%0Atailored%20to%20the%20unlabeled%20dataset%20containing%20both%20inliers%20and%20outliers.%20As%0Ademonstrated%20empirically%2C%20our%20proposed%20method%20outperforms%20existing%0Astate-of-the-art%20methods%20across%20four%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12091v3&entry.124074799=Read"},
{"title": "Garment3DGen: 3D Garment Stylization and Texture Generation", "author": "Nikolaos Sarafianos and Tuur Stuyck and Xiaoyu Xiang and Yilei Li and Jovan Popovic and Rakesh Ranjan", "abstract": "  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.\n", "link": "http://arxiv.org/abs/2403.18816v1", "date": "2024-03-27", "relevancy": 2.2355, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6142}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5624}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5332}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation&body=Title%3A%20Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation%0AAuthor%3A%20Nikolaos%20Sarafianos%20and%20Tuur%20Stuyck%20and%20Xiaoyu%20Xiang%20and%20Yilei%20Li%20and%20Jovan%20Popovic%20and%20Rakesh%20Ranjan%0AAbstract%3A%20%20%20We%20introduce%20Garment3DGen%20a%20new%20method%20to%20synthesize%203D%20garment%20assets%20from%20a%0Abase%20mesh%20given%20a%20single%20input%20image%20as%20guidance.%20Our%20proposed%20approach%20allows%0Ausers%20to%20generate%203D%20textured%20clothes%20based%20on%20both%20real%20and%20synthetic%20images%2C%0Asuch%20as%20those%20generated%20by%20text%20prompts.%20The%20generated%20assets%20can%20be%20directly%0Adraped%20and%20simulated%20on%20human%20bodies.%20First%2C%20we%20leverage%20the%20recent%20progress%20of%0Aimage%20to%203D%20diffusion%20methods%20to%20generate%203D%20garment%20geometries.%20However%2C%20since%0Athese%20geometries%20cannot%20be%20utilized%20directly%20for%20downstream%20tasks%2C%20we%20propose%0Ato%20use%20them%20as%20pseudo%20ground-truth%20and%20set%20up%20a%20mesh%20deformation%20optimization%0Aprocedure%20that%20deforms%20a%20base%20template%20mesh%20to%20match%20the%20generated%203D%20target.%0ASecond%2C%20we%20introduce%20carefully%20designed%20losses%20that%20allow%20the%20input%20base%20mesh%0Ato%20freely%20deform%20towards%20the%20desired%20target%2C%20yet%20preserve%20mesh%20quality%20and%0Atopology%20such%20that%20they%20can%20be%20simulated.%20Finally%2C%20a%20texture%20estimation%20module%0Agenerates%20high-fidelity%20texture%20maps%20that%20are%20globally%20and%20locally%20consistent%0Aand%20faithfully%20capture%20the%20input%20guidance%2C%20allowing%20us%20to%20render%20the%20generated%0A3D%20assets.%20With%20Garment3DGen%20users%20can%20generate%20the%20textured%203D%20garment%20of%0Atheir%20choice%20without%20the%20need%20of%20artist%20intervention.%20One%20can%20provide%20a%20textual%0Aprompt%20describing%20the%20garment%20they%20desire%20to%20generate%20a%20simulation-ready%203D%0Aasset.%20We%20present%20a%20plethora%20of%20quantitative%20and%20qualitative%20comparisons%20on%0Avarious%20assets%20both%20real%20and%20generated%20and%20provide%20use-cases%20of%20how%20one%20can%0Agenerate%20simulation-ready%203D%20garments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18816v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Garment3DGen%3A%203D%20Garment%20Stylization%20and%20Texture%20Generation&entry.906535625=Nikolaos%20Sarafianos%20and%20Tuur%20Stuyck%20and%20Xiaoyu%20Xiang%20and%20Yilei%20Li%20and%20Jovan%20Popovic%20and%20Rakesh%20Ranjan&entry.1292438233=%20%20We%20introduce%20Garment3DGen%20a%20new%20method%20to%20synthesize%203D%20garment%20assets%20from%20a%0Abase%20mesh%20given%20a%20single%20input%20image%20as%20guidance.%20Our%20proposed%20approach%20allows%0Ausers%20to%20generate%203D%20textured%20clothes%20based%20on%20both%20real%20and%20synthetic%20images%2C%0Asuch%20as%20those%20generated%20by%20text%20prompts.%20The%20generated%20assets%20can%20be%20directly%0Adraped%20and%20simulated%20on%20human%20bodies.%20First%2C%20we%20leverage%20the%20recent%20progress%20of%0Aimage%20to%203D%20diffusion%20methods%20to%20generate%203D%20garment%20geometries.%20However%2C%20since%0Athese%20geometries%20cannot%20be%20utilized%20directly%20for%20downstream%20tasks%2C%20we%20propose%0Ato%20use%20them%20as%20pseudo%20ground-truth%20and%20set%20up%20a%20mesh%20deformation%20optimization%0Aprocedure%20that%20deforms%20a%20base%20template%20mesh%20to%20match%20the%20generated%203D%20target.%0ASecond%2C%20we%20introduce%20carefully%20designed%20losses%20that%20allow%20the%20input%20base%20mesh%0Ato%20freely%20deform%20towards%20the%20desired%20target%2C%20yet%20preserve%20mesh%20quality%20and%0Atopology%20such%20that%20they%20can%20be%20simulated.%20Finally%2C%20a%20texture%20estimation%20module%0Agenerates%20high-fidelity%20texture%20maps%20that%20are%20globally%20and%20locally%20consistent%0Aand%20faithfully%20capture%20the%20input%20guidance%2C%20allowing%20us%20to%20render%20the%20generated%0A3D%20assets.%20With%20Garment3DGen%20users%20can%20generate%20the%20textured%203D%20garment%20of%0Atheir%20choice%20without%20the%20need%20of%20artist%20intervention.%20One%20can%20provide%20a%20textual%0Aprompt%20describing%20the%20garment%20they%20desire%20to%20generate%20a%20simulation-ready%203D%0Aasset.%20We%20present%20a%20plethora%20of%20quantitative%20and%20qualitative%20comparisons%20on%0Avarious%20assets%20both%20real%20and%20generated%20and%20provide%20use-cases%20of%20how%20one%20can%0Agenerate%20simulation-ready%203D%20garments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18816v1&entry.124074799=Read"},
{"title": "An Efficient Risk-aware Branch MPC for Automated Driving that is Robust\n  to Uncertain Vehicle Behaviors", "author": "Luyao Zhang and George Pantazis and Shaohang Han and Sergio Grammatico", "abstract": "  One of the critical challenges in automated driving is ensuring safety of\nautomated vehicles despite the unknown behavior of the other vehicles. Although\nmotion prediction modules are able to generate a probability distribution\nassociated with various behavior modes, their probabilistic estimates are often\ninaccurate, thus leading to a possibly unsafe trajectory. To overcome this\nchallenge, we propose a risk-aware motion planning framework that appropriately\naccounts for the ambiguity in the estimated probability distribution. We\nformulate the risk-aware motion planning problem as a min-max optimization\nproblem and develop an efficient iterative method by incorporating a\nregularization term in the probability update step. Via extensive numerical\nstudies, we validate the convergence of our method and demonstrate its\nadvantages compared to the state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2403.18695v1", "date": "2024-03-27", "relevancy": 2.231, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5976}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5784}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5212}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Risk-aware%20Branch%20MPC%20for%20Automated%20Driving%20that%20is%20Robust%0A%20%20to%20Uncertain%20Vehicle%20Behaviors&body=Title%3A%20An%20Efficient%20Risk-aware%20Branch%20MPC%20for%20Automated%20Driving%20that%20is%20Robust%0A%20%20to%20Uncertain%20Vehicle%20Behaviors%0AAuthor%3A%20Luyao%20Zhang%20and%20George%20Pantazis%20and%20Shaohang%20Han%20and%20Sergio%20Grammatico%0AAbstract%3A%20%20%20One%20of%20the%20critical%20challenges%20in%20automated%20driving%20is%20ensuring%20safety%20of%0Aautomated%20vehicles%20despite%20the%20unknown%20behavior%20of%20the%20other%20vehicles.%20Although%0Amotion%20prediction%20modules%20are%20able%20to%20generate%20a%20probability%20distribution%0Aassociated%20with%20various%20behavior%20modes%2C%20their%20probabilistic%20estimates%20are%20often%0Ainaccurate%2C%20thus%20leading%20to%20a%20possibly%20unsafe%20trajectory.%20To%20overcome%20this%0Achallenge%2C%20we%20propose%20a%20risk-aware%20motion%20planning%20framework%20that%20appropriately%0Aaccounts%20for%20the%20ambiguity%20in%20the%20estimated%20probability%20distribution.%20We%0Aformulate%20the%20risk-aware%20motion%20planning%20problem%20as%20a%20min-max%20optimization%0Aproblem%20and%20develop%20an%20efficient%20iterative%20method%20by%20incorporating%20a%0Aregularization%20term%20in%20the%20probability%20update%20step.%20Via%20extensive%20numerical%0Astudies%2C%20we%20validate%20the%20convergence%20of%20our%20method%20and%20demonstrate%20its%0Aadvantages%20compared%20to%20the%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Risk-aware%20Branch%20MPC%20for%20Automated%20Driving%20that%20is%20Robust%0A%20%20to%20Uncertain%20Vehicle%20Behaviors&entry.906535625=Luyao%20Zhang%20and%20George%20Pantazis%20and%20Shaohang%20Han%20and%20Sergio%20Grammatico&entry.1292438233=%20%20One%20of%20the%20critical%20challenges%20in%20automated%20driving%20is%20ensuring%20safety%20of%0Aautomated%20vehicles%20despite%20the%20unknown%20behavior%20of%20the%20other%20vehicles.%20Although%0Amotion%20prediction%20modules%20are%20able%20to%20generate%20a%20probability%20distribution%0Aassociated%20with%20various%20behavior%20modes%2C%20their%20probabilistic%20estimates%20are%20often%0Ainaccurate%2C%20thus%20leading%20to%20a%20possibly%20unsafe%20trajectory.%20To%20overcome%20this%0Achallenge%2C%20we%20propose%20a%20risk-aware%20motion%20planning%20framework%20that%20appropriately%0Aaccounts%20for%20the%20ambiguity%20in%20the%20estimated%20probability%20distribution.%20We%0Aformulate%20the%20risk-aware%20motion%20planning%20problem%20as%20a%20min-max%20optimization%0Aproblem%20and%20develop%20an%20efficient%20iterative%20method%20by%20incorporating%20a%0Aregularization%20term%20in%20the%20probability%20update%20step.%20Via%20extensive%20numerical%0Astudies%2C%20we%20validate%20the%20convergence%20of%20our%20method%20and%20demonstrate%20its%0Aadvantages%20compared%20to%20the%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18695v1&entry.124074799=Read"},
{"title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering", "author": "Guoxing Sun and Rishabh Dabral and Pascal Fua and Christian Theobalt and Marc Habermann", "abstract": "  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.\n", "link": "http://arxiv.org/abs/2403.18820v1", "date": "2024-03-27", "relevancy": 2.2307, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5585}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering&body=Title%3A%20MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering%0AAuthor%3A%20Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann%0AAbstract%3A%20%20%20Faithful%20human%20performance%20capture%20and%20free-view%20rendering%20from%20sparse%20RGB%0Aobservations%20is%20a%20long-standing%20problem%20in%20Vision%20and%20Graphics.%20The%20main%0Achallenges%20are%20the%20lack%20of%20observations%20and%20the%20inherent%20ambiguities%20of%20the%0Asetting%2C%20e.g.%20occlusions%20and%20depth%20ambiguity.%20As%20a%20result%2C%20radiance%20fields%2C%0Awhich%20have%20shown%20great%20promise%20in%20capturing%20high-frequency%20appearance%20and%0Ageometry%20details%20in%20dense%20setups%2C%20perform%20poorly%20when%20na%5C%22ively%20supervising%0Athem%20on%20sparse%20camera%20views%2C%20as%20the%20field%20simply%20overfits%20to%20the%20sparse-view%0Ainputs.%20To%20address%20this%2C%20we%20propose%20MetaCap%2C%20a%20method%20for%20efficient%20and%0Ahigh-quality%20geometry%20recovery%20and%20novel%20view%20synthesis%20given%20very%20sparse%20or%0Aeven%20a%20single%20view%20of%20the%20human.%20Our%20key%20idea%20is%20to%20meta-learn%20the%20radiance%0Afield%20weights%20solely%20from%20potentially%20sparse%20multi-view%20videos%2C%20which%20can%20serve%0Aas%20a%20prior%20when%20fine-tuning%20them%20on%20sparse%20imagery%20depicting%20the%20human.%20This%0Aprior%20provides%20a%20good%20network%20weight%20initialization%2C%20thereby%20effectively%0Aaddressing%20ambiguities%20in%20sparse-view%20capture.%20Due%20to%20the%20articulated%20structure%0Aof%20the%20human%20body%20and%20motion-induced%20surface%20deformations%2C%20learning%20such%20a%0Aprior%20is%20non-trivial.%20Therefore%2C%20we%20propose%20to%20meta-learn%20the%20field%20weights%20in%0Aa%20pose-canonicalized%20space%2C%20which%20reduces%20the%20spatial%20feature%20range%20and%20makes%0Afeature%20learning%20more%20effective.%20Consequently%2C%20one%20can%20fine-tune%20our%20field%0Aparameters%20to%20quickly%20generalize%20to%20unseen%20poses%2C%20novel%20illumination%20conditions%0Aas%20well%20as%20novel%20and%20sparse%20%28even%20monocular%29%20camera%20views.%20For%20evaluating%20our%0Amethod%20under%20different%20scenarios%2C%20we%20collect%20a%20new%20dataset%2C%20WildDynaCap%2C%20which%0Acontains%20subjects%20captured%20in%2C%20both%2C%20a%20dense%20camera%20dome%20and%20in-the-wild%20sparse%0Acamera%20rigs%2C%20and%20demonstrate%20superior%20results%20compared%20to%20recent%0Astate-of-the-art%20methods%20on%20both%20public%20and%20WildDynaCap%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18820v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCap%3A%20Meta-learning%20Priors%20from%20Multi-View%20Imagery%20for%20Sparse-view%0A%20%20Human%20Performance%20Capture%20and%20Rendering&entry.906535625=Guoxing%20Sun%20and%20Rishabh%20Dabral%20and%20Pascal%20Fua%20and%20Christian%20Theobalt%20and%20Marc%20Habermann&entry.1292438233=%20%20Faithful%20human%20performance%20capture%20and%20free-view%20rendering%20from%20sparse%20RGB%0Aobservations%20is%20a%20long-standing%20problem%20in%20Vision%20and%20Graphics.%20The%20main%0Achallenges%20are%20the%20lack%20of%20observations%20and%20the%20inherent%20ambiguities%20of%20the%0Asetting%2C%20e.g.%20occlusions%20and%20depth%20ambiguity.%20As%20a%20result%2C%20radiance%20fields%2C%0Awhich%20have%20shown%20great%20promise%20in%20capturing%20high-frequency%20appearance%20and%0Ageometry%20details%20in%20dense%20setups%2C%20perform%20poorly%20when%20na%5C%22ively%20supervising%0Athem%20on%20sparse%20camera%20views%2C%20as%20the%20field%20simply%20overfits%20to%20the%20sparse-view%0Ainputs.%20To%20address%20this%2C%20we%20propose%20MetaCap%2C%20a%20method%20for%20efficient%20and%0Ahigh-quality%20geometry%20recovery%20and%20novel%20view%20synthesis%20given%20very%20sparse%20or%0Aeven%20a%20single%20view%20of%20the%20human.%20Our%20key%20idea%20is%20to%20meta-learn%20the%20radiance%0Afield%20weights%20solely%20from%20potentially%20sparse%20multi-view%20videos%2C%20which%20can%20serve%0Aas%20a%20prior%20when%20fine-tuning%20them%20on%20sparse%20imagery%20depicting%20the%20human.%20This%0Aprior%20provides%20a%20good%20network%20weight%20initialization%2C%20thereby%20effectively%0Aaddressing%20ambiguities%20in%20sparse-view%20capture.%20Due%20to%20the%20articulated%20structure%0Aof%20the%20human%20body%20and%20motion-induced%20surface%20deformations%2C%20learning%20such%20a%0Aprior%20is%20non-trivial.%20Therefore%2C%20we%20propose%20to%20meta-learn%20the%20field%20weights%20in%0Aa%20pose-canonicalized%20space%2C%20which%20reduces%20the%20spatial%20feature%20range%20and%20makes%0Afeature%20learning%20more%20effective.%20Consequently%2C%20one%20can%20fine-tune%20our%20field%0Aparameters%20to%20quickly%20generalize%20to%20unseen%20poses%2C%20novel%20illumination%20conditions%0Aas%20well%20as%20novel%20and%20sparse%20%28even%20monocular%29%20camera%20views.%20For%20evaluating%20our%0Amethod%20under%20different%20scenarios%2C%20we%20collect%20a%20new%20dataset%2C%20WildDynaCap%2C%20which%0Acontains%20subjects%20captured%20in%2C%20both%2C%20a%20dense%20camera%20dome%20and%20in-the-wild%20sparse%0Acamera%20rigs%2C%20and%20demonstrate%20superior%20results%20compared%20to%20recent%0Astate-of-the-art%20methods%20on%20both%20public%20and%20WildDynaCap%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18820v1&entry.124074799=Read"},
{"title": "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation", "author": "Wenhao Li and Mengyuan Liu and Hong Liu and Pichao Wang and Jialun Cai and Nicu Sebe", "abstract": "  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n", "link": "http://arxiv.org/abs/2311.12028v2", "date": "2024-03-27", "relevancy": 2.2097, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5673}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5249}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hourglass%20Tokenizer%20for%20Efficient%20Transformer-Based%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20Hourglass%20Tokenizer%20for%20Efficient%20Transformer-Based%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Wenhao%20Li%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Pichao%20Wang%20and%20Jialun%20Cai%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Transformers%20have%20been%20successfully%20applied%20in%20the%20field%20of%20video-based%203D%0Ahuman%20pose%20estimation.%20However%2C%20the%20high%20computational%20costs%20of%20these%20video%0Apose%20transformers%20%28VPTs%29%20make%20them%20impractical%20on%20resource-constrained%20devices.%0AIn%20this%20paper%2C%20we%20present%20a%20plug-and-play%20pruning-and-recovering%20framework%2C%0Acalled%20Hourglass%20Tokenizer%20%28HoT%29%2C%20for%20efficient%20transformer-based%203D%20human%20pose%0Aestimation%20from%20videos.%20Our%20HoT%20begins%20with%20pruning%20pose%20tokens%20of%20redundant%0Aframes%20and%20ends%20with%20recovering%20full-length%20tokens%2C%20resulting%20in%20a%20few%20pose%0Atokens%20in%20the%20intermediate%20transformer%20blocks%20and%20thus%20improving%20the%20model%0Aefficiency.%20To%20effectively%20achieve%20this%2C%20we%20propose%20a%20token%20pruning%20cluster%0A%28TPC%29%20that%20dynamically%20selects%20a%20few%20representative%20tokens%20with%20high%20semantic%0Adiversity%20while%20eliminating%20the%20redundancy%20of%20video%20frames.%20In%20addition%2C%20we%0Adevelop%20a%20token%20recovering%20attention%20%28TRA%29%20to%20restore%20the%20detailed%0Aspatio-temporal%20information%20based%20on%20the%20selected%20tokens%2C%20thereby%20expanding%20the%0Anetwork%20output%20to%20the%20original%20full-length%20temporal%20resolution%20for%20fast%0Ainference.%20Extensive%20experiments%20on%20two%20benchmark%20datasets%20%28i.e.%2C%20Human3.6M%20and%0AMPI-INF-3DHP%29%20demonstrate%20that%20our%20method%20can%20achieve%20both%20high%20efficiency%20and%0Aestimation%20accuracy%20compared%20to%20the%20original%20VPT%20models.%20For%20instance%2C%20applying%0Ato%20MotionBERT%20and%20MixSTE%20on%20Human3.6M%2C%20our%20HoT%20can%20save%20nearly%2050%25%20FLOPs%0Awithout%20sacrificing%20accuracy%20and%20nearly%2040%25%20FLOPs%20with%20only%200.2%25%20accuracy%20drop%2C%0Arespectively.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/NationalGAILab/HoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12028v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hourglass%20Tokenizer%20for%20Efficient%20Transformer-Based%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Wenhao%20Li%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Pichao%20Wang%20and%20Jialun%20Cai%20and%20Nicu%20Sebe&entry.1292438233=%20%20Transformers%20have%20been%20successfully%20applied%20in%20the%20field%20of%20video-based%203D%0Ahuman%20pose%20estimation.%20However%2C%20the%20high%20computational%20costs%20of%20these%20video%0Apose%20transformers%20%28VPTs%29%20make%20them%20impractical%20on%20resource-constrained%20devices.%0AIn%20this%20paper%2C%20we%20present%20a%20plug-and-play%20pruning-and-recovering%20framework%2C%0Acalled%20Hourglass%20Tokenizer%20%28HoT%29%2C%20for%20efficient%20transformer-based%203D%20human%20pose%0Aestimation%20from%20videos.%20Our%20HoT%20begins%20with%20pruning%20pose%20tokens%20of%20redundant%0Aframes%20and%20ends%20with%20recovering%20full-length%20tokens%2C%20resulting%20in%20a%20few%20pose%0Atokens%20in%20the%20intermediate%20transformer%20blocks%20and%20thus%20improving%20the%20model%0Aefficiency.%20To%20effectively%20achieve%20this%2C%20we%20propose%20a%20token%20pruning%20cluster%0A%28TPC%29%20that%20dynamically%20selects%20a%20few%20representative%20tokens%20with%20high%20semantic%0Adiversity%20while%20eliminating%20the%20redundancy%20of%20video%20frames.%20In%20addition%2C%20we%0Adevelop%20a%20token%20recovering%20attention%20%28TRA%29%20to%20restore%20the%20detailed%0Aspatio-temporal%20information%20based%20on%20the%20selected%20tokens%2C%20thereby%20expanding%20the%0Anetwork%20output%20to%20the%20original%20full-length%20temporal%20resolution%20for%20fast%0Ainference.%20Extensive%20experiments%20on%20two%20benchmark%20datasets%20%28i.e.%2C%20Human3.6M%20and%0AMPI-INF-3DHP%29%20demonstrate%20that%20our%20method%20can%20achieve%20both%20high%20efficiency%20and%0Aestimation%20accuracy%20compared%20to%20the%20original%20VPT%20models.%20For%20instance%2C%20applying%0Ato%20MotionBERT%20and%20MixSTE%20on%20Human3.6M%2C%20our%20HoT%20can%20save%20nearly%2050%25%20FLOPs%0Awithout%20sacrificing%20accuracy%20and%20nearly%2040%25%20FLOPs%20with%20only%200.2%25%20accuracy%20drop%2C%0Arespectively.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/NationalGAILab/HoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12028v2&entry.124074799=Read"},
{"title": "SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields", "author": "Quentin Herau and Nathan Piasco and Moussab Bennehar and Luis Rold\u00e3o and Dzmitry Tsishkou and Cyrille Migniot and Pascal Vasseur and C\u00e9dric Demonceaux", "abstract": "  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n", "link": "http://arxiv.org/abs/2311.15803v3", "date": "2024-03-27", "relevancy": 2.2028, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SOAC%3A%20Spatio-Temporal%20Overlap-Aware%20Multi-Sensor%20Calibration%20using%0A%20%20Neural%20Radiance%20Fields&body=Title%3A%20SOAC%3A%20Spatio-Temporal%20Overlap-Aware%20Multi-Sensor%20Calibration%20using%0A%20%20Neural%20Radiance%20Fields%0AAuthor%3A%20Quentin%20Herau%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20Cyrille%20Migniot%20and%20Pascal%20Vasseur%20and%20C%C3%A9dric%20Demonceaux%0AAbstract%3A%20%20%20In%20rapidly-evolving%20domains%20such%20as%20autonomous%20driving%2C%20the%20use%20of%20multiple%0Asensors%20with%20different%20modalities%20is%20crucial%20to%20ensure%20high%20operational%0Aprecision%20and%20stability.%20To%20correctly%20exploit%20the%20provided%20information%20by%20each%0Asensor%20in%20a%20single%20common%20frame%2C%20it%20is%20essential%20for%20these%20sensors%20to%20be%0Aaccurately%20calibrated.%20In%20this%20paper%2C%20we%20leverage%20the%20ability%20of%20Neural%0ARadiance%20Fields%20%28NeRF%29%20to%20represent%20different%20sensors%20modalities%20in%20a%20common%0Avolumetric%20representation%20to%20achieve%20robust%20and%20accurate%20spatio-temporal%20sensor%0Acalibration.%20By%20designing%20a%20partitioning%20approach%20based%20on%20the%20visible%20part%20of%0Athe%20scene%20for%20each%20sensor%2C%20we%20formulate%20the%20calibration%20problem%20using%20only%20the%0Aoverlapping%20areas.%20This%20strategy%20results%20in%20a%20more%20robust%20and%20accurate%0Acalibration%20that%20is%20less%20prone%20to%20failure.%20We%20demonstrate%20that%20our%20approach%0Aworks%20on%20outdoor%20urban%20scenes%20by%20validating%20it%20on%20multiple%20established%20driving%0Adatasets.%20Results%20show%20that%20our%20method%20is%20able%20to%20get%20better%20accuracy%20and%0Arobustness%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15803v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOAC%3A%20Spatio-Temporal%20Overlap-Aware%20Multi-Sensor%20Calibration%20using%0A%20%20Neural%20Radiance%20Fields&entry.906535625=Quentin%20Herau%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20Cyrille%20Migniot%20and%20Pascal%20Vasseur%20and%20C%C3%A9dric%20Demonceaux&entry.1292438233=%20%20In%20rapidly-evolving%20domains%20such%20as%20autonomous%20driving%2C%20the%20use%20of%20multiple%0Asensors%20with%20different%20modalities%20is%20crucial%20to%20ensure%20high%20operational%0Aprecision%20and%20stability.%20To%20correctly%20exploit%20the%20provided%20information%20by%20each%0Asensor%20in%20a%20single%20common%20frame%2C%20it%20is%20essential%20for%20these%20sensors%20to%20be%0Aaccurately%20calibrated.%20In%20this%20paper%2C%20we%20leverage%20the%20ability%20of%20Neural%0ARadiance%20Fields%20%28NeRF%29%20to%20represent%20different%20sensors%20modalities%20in%20a%20common%0Avolumetric%20representation%20to%20achieve%20robust%20and%20accurate%20spatio-temporal%20sensor%0Acalibration.%20By%20designing%20a%20partitioning%20approach%20based%20on%20the%20visible%20part%20of%0Athe%20scene%20for%20each%20sensor%2C%20we%20formulate%20the%20calibration%20problem%20using%20only%20the%0Aoverlapping%20areas.%20This%20strategy%20results%20in%20a%20more%20robust%20and%20accurate%0Acalibration%20that%20is%20less%20prone%20to%20failure.%20We%20demonstrate%20that%20our%20approach%0Aworks%20on%20outdoor%20urban%20scenes%20by%20validating%20it%20on%20multiple%20established%20driving%0Adatasets.%20Results%20show%20that%20our%20method%20is%20able%20to%20get%20better%20accuracy%20and%0Arobustness%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15803v3&entry.124074799=Read"},
{"title": "PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction", "author": "Yuxiang Zhao and Zhuomin Chai and Xun Jiang and Yibo Lin and Runsheng Wang and Ru Huang", "abstract": "  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n", "link": "http://arxiv.org/abs/2403.18569v1", "date": "2024-03-27", "relevancy": 2.1873, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6022}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5272}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4993}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PDNNet%3A%20PDN-Aware%20GNN-CNN%20Heterogeneous%20Network%20for%20Dynamic%20IR%20Drop%0A%20%20Prediction&body=Title%3A%20PDNNet%3A%20PDN-Aware%20GNN-CNN%20Heterogeneous%20Network%20for%20Dynamic%20IR%20Drop%0A%20%20Prediction%0AAuthor%3A%20Yuxiang%20Zhao%20and%20Zhuomin%20Chai%20and%20Xun%20Jiang%20and%20Yibo%20Lin%20and%20Runsheng%20Wang%20and%20Ru%20Huang%0AAbstract%3A%20%20%20IR%20drop%20on%20the%20power%20delivery%20network%20%28PDN%29%20is%20closely%20related%20to%20PDN%27s%0Aconfiguration%20and%20cell%20current%20consumption.%20As%20the%20integrated%20circuit%20%28IC%29%0Adesign%20is%20growing%20larger%2C%20dynamic%20IR%20drop%20simulation%20becomes%20computationally%0Aunaffordable%20and%20machine%20learning%20based%20IR%20drop%20prediction%20has%20been%20explored%20as%0Aa%20promising%20solution.%20Although%20CNN-based%20methods%20have%20been%20adapted%20to%20IR%20drop%0Aprediction%20task%20in%20several%20works%2C%20the%20shortcomings%20of%20overlooking%20PDN%0Aconfiguration%20is%20non-negligible.%20In%20this%20paper%2C%20we%20consider%20not%20only%20how%20to%0Aproperly%20represent%20cell-PDN%20relation%2C%20but%20also%20how%20to%20model%20IR%20drop%20following%0Aits%20physical%20nature%20in%20the%20feature%20aggregation%20procedure.%20Thus%2C%20we%20propose%20a%0Anovel%20graph%20structure%2C%20PDNGraph%2C%20to%20unify%20the%20representations%20of%20the%20PDN%0Astructure%20and%20the%20fine-grained%20cell-PDN%20relation.%20We%20further%20propose%20a%0Adual-branch%20heterogeneous%20network%2C%20PDNNet%2C%20incorporating%20two%20parallel%20GNN-CNN%0Abranches%20to%20favorably%20capture%20the%20above%20features%20during%20the%20learning%20process.%0ASeveral%20key%20designs%20are%20presented%20to%20make%20the%20dynamic%20IR%20drop%20prediction%20highly%0Aeffective%20and%20interpretable.%20We%20are%20the%20first%20work%20to%20apply%20graph%20structure%20to%0Adeep-learning%20based%20dynamic%20IR%20drop%20prediction%20method.%20Experiments%20show%20that%0APDNNet%20outperforms%20the%20state-of-the-art%20CNN-based%20methods%20by%20up%20to%2039.3%25%0Areduction%20in%20prediction%20error%20and%20achieves%20545x%20speedup%20compared%20to%20the%0Acommercial%20tool%2C%20which%20demonstrates%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18569v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDNNet%3A%20PDN-Aware%20GNN-CNN%20Heterogeneous%20Network%20for%20Dynamic%20IR%20Drop%0A%20%20Prediction&entry.906535625=Yuxiang%20Zhao%20and%20Zhuomin%20Chai%20and%20Xun%20Jiang%20and%20Yibo%20Lin%20and%20Runsheng%20Wang%20and%20Ru%20Huang&entry.1292438233=%20%20IR%20drop%20on%20the%20power%20delivery%20network%20%28PDN%29%20is%20closely%20related%20to%20PDN%27s%0Aconfiguration%20and%20cell%20current%20consumption.%20As%20the%20integrated%20circuit%20%28IC%29%0Adesign%20is%20growing%20larger%2C%20dynamic%20IR%20drop%20simulation%20becomes%20computationally%0Aunaffordable%20and%20machine%20learning%20based%20IR%20drop%20prediction%20has%20been%20explored%20as%0Aa%20promising%20solution.%20Although%20CNN-based%20methods%20have%20been%20adapted%20to%20IR%20drop%0Aprediction%20task%20in%20several%20works%2C%20the%20shortcomings%20of%20overlooking%20PDN%0Aconfiguration%20is%20non-negligible.%20In%20this%20paper%2C%20we%20consider%20not%20only%20how%20to%0Aproperly%20represent%20cell-PDN%20relation%2C%20but%20also%20how%20to%20model%20IR%20drop%20following%0Aits%20physical%20nature%20in%20the%20feature%20aggregation%20procedure.%20Thus%2C%20we%20propose%20a%0Anovel%20graph%20structure%2C%20PDNGraph%2C%20to%20unify%20the%20representations%20of%20the%20PDN%0Astructure%20and%20the%20fine-grained%20cell-PDN%20relation.%20We%20further%20propose%20a%0Adual-branch%20heterogeneous%20network%2C%20PDNNet%2C%20incorporating%20two%20parallel%20GNN-CNN%0Abranches%20to%20favorably%20capture%20the%20above%20features%20during%20the%20learning%20process.%0ASeveral%20key%20designs%20are%20presented%20to%20make%20the%20dynamic%20IR%20drop%20prediction%20highly%0Aeffective%20and%20interpretable.%20We%20are%20the%20first%20work%20to%20apply%20graph%20structure%20to%0Adeep-learning%20based%20dynamic%20IR%20drop%20prediction%20method.%20Experiments%20show%20that%0APDNNet%20outperforms%20the%20state-of-the-art%20CNN-based%20methods%20by%20up%20to%2039.3%25%0Areduction%20in%20prediction%20error%20and%20achieves%20545x%20speedup%20compared%20to%20the%0Acommercial%20tool%2C%20which%20demonstrates%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18569v1&entry.124074799=Read"},
{"title": "Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP", "author": "Reza Abbasi and Mohammad Samiei and Mohammad Hossein Rohban and Mahdieh Soleymani Baghshah", "abstract": "  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n", "link": "http://arxiv.org/abs/2403.18525v1", "date": "2024-03-27", "relevancy": 2.1864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4764}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Language%20Plays%20a%20Pivotal%20Role%20in%20the%20Object-Attribute%20Compositional%0A%20%20Generalization%20of%20CLIP&body=Title%3A%20Language%20Plays%20a%20Pivotal%20Role%20in%20the%20Object-Attribute%20Compositional%0A%20%20Generalization%20of%20CLIP%0AAuthor%3A%20Reza%20Abbasi%20and%20Mohammad%20Samiei%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20shown%20promising%0AOut-of-Distribution%20%28OoD%29%20generalization%20under%20various%20types%20of%20distribution%0Ashifts.%20Recent%20studies%20attempted%20to%20investigate%20the%20leading%20cause%20of%20this%0Acapability.%20In%20this%20work%2C%20we%20follow%20the%20same%20path%2C%20but%20focus%20on%20a%20specific%20type%0Aof%20OoD%20data%20-%20images%20with%20novel%20compositions%20of%20attribute-object%20pairs%20-%20and%0Astudy%20whether%20such%20models%20can%20successfully%20classify%20those%20images%20into%0Acomposition%20classes.%20We%20carefully%20designed%20an%20authentic%20image%20test%20dataset%0Acalled%20ImageNet-AO%2C%20consisting%20of%20attributes%20for%20objects%20that%20are%20unlikely%0Aencountered%20in%20the%20CLIP%20training%20sets.%20We%20found%20that%20CLIPs%20trained%20with%20large%0Adatasets%20such%20as%20OpenAI%20CLIP%2C%20LAION-400M%2C%20and%20LAION-2B%20show%20orders-of-magnitude%0Aimprovement%20in%20effective%20compositional%20OoD%20generalization%20compared%20to%20both%0Asupervised%20models%20and%20CLIPs%20trained%20with%20smaller%20datasets%2C%20such%20as%20CC-12M%20and%0AYFCC-15M.%20Our%20results%20provide%20evidence%20that%20the%20scale%20and%20diversity%20of%20training%0Adata%20and%20language%20supervision%20play%20a%20key%20role%20in%20unlocking%20the%20compositional%0Ageneralization%20abilities%20of%20vision-language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18525v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Plays%20a%20Pivotal%20Role%20in%20the%20Object-Attribute%20Compositional%0A%20%20Generalization%20of%20CLIP&entry.906535625=Reza%20Abbasi%20and%20Mohammad%20Samiei%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20shown%20promising%0AOut-of-Distribution%20%28OoD%29%20generalization%20under%20various%20types%20of%20distribution%0Ashifts.%20Recent%20studies%20attempted%20to%20investigate%20the%20leading%20cause%20of%20this%0Acapability.%20In%20this%20work%2C%20we%20follow%20the%20same%20path%2C%20but%20focus%20on%20a%20specific%20type%0Aof%20OoD%20data%20-%20images%20with%20novel%20compositions%20of%20attribute-object%20pairs%20-%20and%0Astudy%20whether%20such%20models%20can%20successfully%20classify%20those%20images%20into%0Acomposition%20classes.%20We%20carefully%20designed%20an%20authentic%20image%20test%20dataset%0Acalled%20ImageNet-AO%2C%20consisting%20of%20attributes%20for%20objects%20that%20are%20unlikely%0Aencountered%20in%20the%20CLIP%20training%20sets.%20We%20found%20that%20CLIPs%20trained%20with%20large%0Adatasets%20such%20as%20OpenAI%20CLIP%2C%20LAION-400M%2C%20and%20LAION-2B%20show%20orders-of-magnitude%0Aimprovement%20in%20effective%20compositional%20OoD%20generalization%20compared%20to%20both%0Asupervised%20models%20and%20CLIPs%20trained%20with%20smaller%20datasets%2C%20such%20as%20CC-12M%20and%0AYFCC-15M.%20Our%20results%20provide%20evidence%20that%20the%20scale%20and%20diversity%20of%20training%0Adata%20and%20language%20supervision%20play%20a%20key%20role%20in%20unlocking%20the%20compositional%0Ageneralization%20abilities%20of%20vision-language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18525v1&entry.124074799=Read"},
{"title": "Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation", "author": "Zhipeng Du and Miaojing Shi and Jiankang Deng", "abstract": "  Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to low visibility. Previous methods mitigate this issue\nby exploring image enhancement or object detection techniques with real\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties about collecting and annotating low-light images. To address this\nchallenge, we propose to boost low-light object detection with zero-shot\nday-night domain adaptation, which aims to generalize a detector from well-lit\nscenarios to low-light ones without requiring real low-light data. Revisiting\nRetinex theory in the low-level vision, we first design a reflectance\nrepresentation learning module to learn Retinex-based illumination invariance\nin images with a carefully designed illumination invariance reinforcement\nstrategy. Next, an interchange-redecomposition-coherence procedure is\nintroduced to improve over the vanilla Retinex image decomposition process by\nperforming two sequential image decompositions and introducing a\nredecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and\nCODaN datasets show strong low-light generalizability of our method. Our code\nis available at https://github.com/ZPDu/DAI-Net.\n", "link": "http://arxiv.org/abs/2312.01220v2", "date": "2024-03-27", "relevancy": 2.1711, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5468}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5413}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5365}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Object%20Detection%20with%20Zero-Shot%20Day-Night%20Domain%20Adaptation&body=Title%3A%20Boosting%20Object%20Detection%20with%20Zero-Shot%20Day-Night%20Domain%20Adaptation%0AAuthor%3A%20Zhipeng%20Du%20and%20Miaojing%20Shi%20and%20Jiankang%20Deng%0AAbstract%3A%20%20%20Detecting%20objects%20in%20low-light%20scenarios%20presents%20a%20persistent%20challenge%2C%20as%0Adetectors%20trained%20on%20well-lit%20data%20exhibit%20significant%20performance%20degradation%0Aon%20low-light%20data%20due%20to%20low%20visibility.%20Previous%20methods%20mitigate%20this%20issue%0Aby%20exploring%20image%20enhancement%20or%20object%20detection%20techniques%20with%20real%0Alow-light%20image%20datasets.%20However%2C%20the%20progress%20is%20impeded%20by%20the%20inherent%0Adifficulties%20about%20collecting%20and%20annotating%20low-light%20images.%20To%20address%20this%0Achallenge%2C%20we%20propose%20to%20boost%20low-light%20object%20detection%20with%20zero-shot%0Aday-night%20domain%20adaptation%2C%20which%20aims%20to%20generalize%20a%20detector%20from%20well-lit%0Ascenarios%20to%20low-light%20ones%20without%20requiring%20real%20low-light%20data.%20Revisiting%0ARetinex%20theory%20in%20the%20low-level%20vision%2C%20we%20first%20design%20a%20reflectance%0Arepresentation%20learning%20module%20to%20learn%20Retinex-based%20illumination%20invariance%0Ain%20images%20with%20a%20carefully%20designed%20illumination%20invariance%20reinforcement%0Astrategy.%20Next%2C%20an%20interchange-redecomposition-coherence%20procedure%20is%0Aintroduced%20to%20improve%20over%20the%20vanilla%20Retinex%20image%20decomposition%20process%20by%0Aperforming%20two%20sequential%20image%20decompositions%20and%20introducing%20a%0Aredecomposition%20cohering%20loss.%20Extensive%20experiments%20on%20ExDark%2C%20DARK%20FACE%2C%20and%0ACODaN%20datasets%20show%20strong%20low-light%20generalizability%20of%20our%20method.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/ZPDu/DAI-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01220v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Object%20Detection%20with%20Zero-Shot%20Day-Night%20Domain%20Adaptation&entry.906535625=Zhipeng%20Du%20and%20Miaojing%20Shi%20and%20Jiankang%20Deng&entry.1292438233=%20%20Detecting%20objects%20in%20low-light%20scenarios%20presents%20a%20persistent%20challenge%2C%20as%0Adetectors%20trained%20on%20well-lit%20data%20exhibit%20significant%20performance%20degradation%0Aon%20low-light%20data%20due%20to%20low%20visibility.%20Previous%20methods%20mitigate%20this%20issue%0Aby%20exploring%20image%20enhancement%20or%20object%20detection%20techniques%20with%20real%0Alow-light%20image%20datasets.%20However%2C%20the%20progress%20is%20impeded%20by%20the%20inherent%0Adifficulties%20about%20collecting%20and%20annotating%20low-light%20images.%20To%20address%20this%0Achallenge%2C%20we%20propose%20to%20boost%20low-light%20object%20detection%20with%20zero-shot%0Aday-night%20domain%20adaptation%2C%20which%20aims%20to%20generalize%20a%20detector%20from%20well-lit%0Ascenarios%20to%20low-light%20ones%20without%20requiring%20real%20low-light%20data.%20Revisiting%0ARetinex%20theory%20in%20the%20low-level%20vision%2C%20we%20first%20design%20a%20reflectance%0Arepresentation%20learning%20module%20to%20learn%20Retinex-based%20illumination%20invariance%0Ain%20images%20with%20a%20carefully%20designed%20illumination%20invariance%20reinforcement%0Astrategy.%20Next%2C%20an%20interchange-redecomposition-coherence%20procedure%20is%0Aintroduced%20to%20improve%20over%20the%20vanilla%20Retinex%20image%20decomposition%20process%20by%0Aperforming%20two%20sequential%20image%20decompositions%20and%20introducing%20a%0Aredecomposition%20cohering%20loss.%20Extensive%20experiments%20on%20ExDark%2C%20DARK%20FACE%2C%20and%0ACODaN%20datasets%20show%20strong%20low-light%20generalizability%20of%20our%20method.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/ZPDu/DAI-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01220v2&entry.124074799=Read"},
{"title": "$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation\n  via Optical Flow Consistency and Feature Map Synthesis", "author": "Xiaotong Guo and Huijie Zhao and Shuwei Shao and Xudong Li and Baochang Zhang", "abstract": "  Self-supervised monocular depth estimation methods have been increasingly\ngiven much attention due to the benefit of not requiring large, labelled\ndatasets. Such self-supervised methods require high-quality salient features\nand consequently suffer from severe performance drop for indoor scenes, where\nlow-textured regions dominant in the scenes are almost indiscriminative. To\naddress the issue, we propose a self-supervised indoor monocular depth\nestimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow\nestimation network is introduced to supervise depth learning. To improve\noptical flow estimation performance in low-textured areas, only some patches of\npoints with more discriminative features are adopted for finetuning based on\nour well-designed patch-based photometric loss. The finetuned optical flow\nestimation network generates high-accuracy optical flow as a supervisory signal\nfor depth estimation. Correspondingly, an optical flow consistency loss is\ndesigned. Multi-scale feature maps produced by finetuned optical flow\nestimation network perform warping to compute feature map synthesis loss as\nanother supervisory signal for depth learning. Experimental results on the NYU\nDepth V2 dataset demonstrate the effectiveness of the framework and our\nproposed losses. To evaluate the generalization ability of our\n$\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of\napproximately 1500 points selected from 99 images in 18 scenes. Zero-shot\ngeneralization experiments on 7-Scenes dataset and Campus Indoor achieve\n$\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show\nthat our model can generalize well to monocular images captured in unknown\nindoor scenes.\n", "link": "http://arxiv.org/abs/2403.18443v1", "date": "2024-03-27", "relevancy": 2.1632, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5427}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20%24%5Cmathrm%7BF%5E2Depth%7D%24%3A%20Self-supervised%20Indoor%20Monocular%20Depth%20Estimation%0A%20%20via%20Optical%20Flow%20Consistency%20and%20Feature%20Map%20Synthesis&body=Title%3A%20%24%5Cmathrm%7BF%5E2Depth%7D%24%3A%20Self-supervised%20Indoor%20Monocular%20Depth%20Estimation%0A%20%20via%20Optical%20Flow%20Consistency%20and%20Feature%20Map%20Synthesis%0AAuthor%3A%20Xiaotong%20Guo%20and%20Huijie%20Zhao%20and%20Shuwei%20Shao%20and%20Xudong%20Li%20and%20Baochang%20Zhang%0AAbstract%3A%20%20%20Self-supervised%20monocular%20depth%20estimation%20methods%20have%20been%20increasingly%0Agiven%20much%20attention%20due%20to%20the%20benefit%20of%20not%20requiring%20large%2C%20labelled%0Adatasets.%20Such%20self-supervised%20methods%20require%20high-quality%20salient%20features%0Aand%20consequently%20suffer%20from%20severe%20performance%20drop%20for%20indoor%20scenes%2C%20where%0Alow-textured%20regions%20dominant%20in%20the%20scenes%20are%20almost%20indiscriminative.%20To%0Aaddress%20the%20issue%2C%20we%20propose%20a%20self-supervised%20indoor%20monocular%20depth%0Aestimation%20framework%20called%20%24%5Cmathrm%7BF%5E2Depth%7D%24.%20A%20self-supervised%20optical%20flow%0Aestimation%20network%20is%20introduced%20to%20supervise%20depth%20learning.%20To%20improve%0Aoptical%20flow%20estimation%20performance%20in%20low-textured%20areas%2C%20only%20some%20patches%20of%0Apoints%20with%20more%20discriminative%20features%20are%20adopted%20for%20finetuning%20based%20on%0Aour%20well-designed%20patch-based%20photometric%20loss.%20The%20finetuned%20optical%20flow%0Aestimation%20network%20generates%20high-accuracy%20optical%20flow%20as%20a%20supervisory%20signal%0Afor%20depth%20estimation.%20Correspondingly%2C%20an%20optical%20flow%20consistency%20loss%20is%0Adesigned.%20Multi-scale%20feature%20maps%20produced%20by%20finetuned%20optical%20flow%0Aestimation%20network%20perform%20warping%20to%20compute%20feature%20map%20synthesis%20loss%20as%0Aanother%20supervisory%20signal%20for%20depth%20learning.%20Experimental%20results%20on%20the%20NYU%0ADepth%20V2%20dataset%20demonstrate%20the%20effectiveness%20of%20the%20framework%20and%20our%0Aproposed%20losses.%20To%20evaluate%20the%20generalization%20ability%20of%20our%0A%24%5Cmathrm%7BF%5E2Depth%7D%24%2C%20we%20collect%20a%20Campus%20Indoor%20depth%20dataset%20composed%20of%0Aapproximately%201500%20points%20selected%20from%2099%20images%20in%2018%20scenes.%20Zero-shot%0Ageneralization%20experiments%20on%207-Scenes%20dataset%20and%20Campus%20Indoor%20achieve%0A%24%5Cdelta_1%24%20accuracy%20of%2075.8%25%20and%2076.0%25%20respectively.%20The%20accuracy%20results%20show%0Athat%20our%20model%20can%20generalize%20well%20to%20monocular%20images%20captured%20in%20unknown%0Aindoor%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cmathrm%7BF%5E2Depth%7D%24%3A%20Self-supervised%20Indoor%20Monocular%20Depth%20Estimation%0A%20%20via%20Optical%20Flow%20Consistency%20and%20Feature%20Map%20Synthesis&entry.906535625=Xiaotong%20Guo%20and%20Huijie%20Zhao%20and%20Shuwei%20Shao%20and%20Xudong%20Li%20and%20Baochang%20Zhang&entry.1292438233=%20%20Self-supervised%20monocular%20depth%20estimation%20methods%20have%20been%20increasingly%0Agiven%20much%20attention%20due%20to%20the%20benefit%20of%20not%20requiring%20large%2C%20labelled%0Adatasets.%20Such%20self-supervised%20methods%20require%20high-quality%20salient%20features%0Aand%20consequently%20suffer%20from%20severe%20performance%20drop%20for%20indoor%20scenes%2C%20where%0Alow-textured%20regions%20dominant%20in%20the%20scenes%20are%20almost%20indiscriminative.%20To%0Aaddress%20the%20issue%2C%20we%20propose%20a%20self-supervised%20indoor%20monocular%20depth%0Aestimation%20framework%20called%20%24%5Cmathrm%7BF%5E2Depth%7D%24.%20A%20self-supervised%20optical%20flow%0Aestimation%20network%20is%20introduced%20to%20supervise%20depth%20learning.%20To%20improve%0Aoptical%20flow%20estimation%20performance%20in%20low-textured%20areas%2C%20only%20some%20patches%20of%0Apoints%20with%20more%20discriminative%20features%20are%20adopted%20for%20finetuning%20based%20on%0Aour%20well-designed%20patch-based%20photometric%20loss.%20The%20finetuned%20optical%20flow%0Aestimation%20network%20generates%20high-accuracy%20optical%20flow%20as%20a%20supervisory%20signal%0Afor%20depth%20estimation.%20Correspondingly%2C%20an%20optical%20flow%20consistency%20loss%20is%0Adesigned.%20Multi-scale%20feature%20maps%20produced%20by%20finetuned%20optical%20flow%0Aestimation%20network%20perform%20warping%20to%20compute%20feature%20map%20synthesis%20loss%20as%0Aanother%20supervisory%20signal%20for%20depth%20learning.%20Experimental%20results%20on%20the%20NYU%0ADepth%20V2%20dataset%20demonstrate%20the%20effectiveness%20of%20the%20framework%20and%20our%0Aproposed%20losses.%20To%20evaluate%20the%20generalization%20ability%20of%20our%0A%24%5Cmathrm%7BF%5E2Depth%7D%24%2C%20we%20collect%20a%20Campus%20Indoor%20depth%20dataset%20composed%20of%0Aapproximately%201500%20points%20selected%20from%2099%20images%20in%2018%20scenes.%20Zero-shot%0Ageneralization%20experiments%20on%207-Scenes%20dataset%20and%20Campus%20Indoor%20achieve%0A%24%5Cdelta_1%24%20accuracy%20of%2075.8%25%20and%2076.0%25%20respectively.%20The%20accuracy%20results%20show%0Athat%20our%20model%20can%20generalize%20well%20to%20monocular%20images%20captured%20in%20unknown%0Aindoor%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18443v1&entry.124074799=Read"},
{"title": "Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models", "author": "Lizhen Zhu and James Z. Wang and Wonseuk Lee and Brad Wyble", "abstract": "  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n", "link": "http://arxiv.org/abs/2401.15120v2", "date": "2024-03-27", "relevancy": 2.1609, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5539}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5349}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Incorporating%20simulated%20spatial%20context%20information%20improves%20the%0A%20%20effectiveness%20of%20contrastive%20learning%20models&body=Title%3A%20Incorporating%20simulated%20spatial%20context%20information%20improves%20the%0A%20%20effectiveness%20of%20contrastive%20learning%20models%0AAuthor%3A%20Lizhen%20Zhu%20and%20James%20Z.%20Wang%20and%20Wonseuk%20Lee%20and%20Brad%20Wyble%0AAbstract%3A%20%20%20Visual%20learning%20often%20occurs%20in%20a%20specific%20context%2C%20where%20an%20agent%20acquires%0Askills%20through%20exploration%20and%20tracking%20of%20its%20location%20in%20a%20consistent%0Aenvironment.%20The%20historical%20spatial%20context%20of%20the%20agent%20provides%20a%20similarity%0Asignal%20for%20self-supervised%20contrastive%20learning.%20We%20present%20a%20unique%20approach%2C%0Atermed%20Environmental%20Spatial%20Similarity%20%28ESS%29%2C%20that%20complements%20existing%0Acontrastive%20learning%20methods.%20Using%20images%20from%20simulated%2C%20photorealistic%0Aenvironments%20as%20an%20experimental%20setting%2C%20we%20demonstrate%20that%20ESS%20outperforms%0Atraditional%20instance%20discrimination%20approaches.%20Moreover%2C%20sampling%20additional%0Adata%20from%20the%20same%20environment%20substantially%20improves%20accuracy%20and%20provides%20new%0Aaugmentations.%20ESS%20allows%20remarkable%20proficiency%20in%20room%20classification%20and%0Aspatial%20prediction%20tasks%2C%20especially%20in%20unfamiliar%20environments.%20This%20learning%0Aparadigm%20has%20the%20potential%20to%20enable%20rapid%20visual%20learning%20in%20agents%20operating%0Ain%20new%20environments%20with%20unique%20visual%20characteristics.%20Potentially%0Atransformative%20applications%20span%20from%20robotics%20to%20space%20exploration.%20Our%20proof%0Aof%20concept%20demonstrates%20improved%20efficiency%20over%20methods%20that%20rely%20on%0Aextensive%2C%20disconnected%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15120v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20simulated%20spatial%20context%20information%20improves%20the%0A%20%20effectiveness%20of%20contrastive%20learning%20models&entry.906535625=Lizhen%20Zhu%20and%20James%20Z.%20Wang%20and%20Wonseuk%20Lee%20and%20Brad%20Wyble&entry.1292438233=%20%20Visual%20learning%20often%20occurs%20in%20a%20specific%20context%2C%20where%20an%20agent%20acquires%0Askills%20through%20exploration%20and%20tracking%20of%20its%20location%20in%20a%20consistent%0Aenvironment.%20The%20historical%20spatial%20context%20of%20the%20agent%20provides%20a%20similarity%0Asignal%20for%20self-supervised%20contrastive%20learning.%20We%20present%20a%20unique%20approach%2C%0Atermed%20Environmental%20Spatial%20Similarity%20%28ESS%29%2C%20that%20complements%20existing%0Acontrastive%20learning%20methods.%20Using%20images%20from%20simulated%2C%20photorealistic%0Aenvironments%20as%20an%20experimental%20setting%2C%20we%20demonstrate%20that%20ESS%20outperforms%0Atraditional%20instance%20discrimination%20approaches.%20Moreover%2C%20sampling%20additional%0Adata%20from%20the%20same%20environment%20substantially%20improves%20accuracy%20and%20provides%20new%0Aaugmentations.%20ESS%20allows%20remarkable%20proficiency%20in%20room%20classification%20and%0Aspatial%20prediction%20tasks%2C%20especially%20in%20unfamiliar%20environments.%20This%20learning%0Aparadigm%20has%20the%20potential%20to%20enable%20rapid%20visual%20learning%20in%20agents%20operating%0Ain%20new%20environments%20with%20unique%20visual%20characteristics.%20Potentially%0Atransformative%20applications%20span%20from%20robotics%20to%20space%20exploration.%20Our%20proof%0Aof%20concept%20demonstrates%20improved%20efficiency%20over%20methods%20that%20rely%20on%0Aextensive%2C%20disconnected%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15120v2&entry.124074799=Read"},
{"title": "TransFusion: Contrastive Learning with Transformers", "author": "Huanran Li and Daniel Pimentel-Alarc\u00f3n", "abstract": "  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n", "link": "http://arxiv.org/abs/2403.18681v1", "date": "2024-03-27", "relevancy": 2.1472, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5146}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TransFusion%3A%20Contrastive%20Learning%20with%20Transformers&body=Title%3A%20TransFusion%3A%20Contrastive%20Learning%20with%20Transformers%0AAuthor%3A%20Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20framework%2C%20TransFusion%2C%20designed%20to%20make%20the%0Aprocess%20of%20contrastive%20learning%20more%20analytical%20and%20explainable.%20TransFusion%0Aconsists%20of%20attention%20blocks%20whose%20softmax%20being%20replaced%20by%20ReLU%2C%20and%20its%0Afinal%20block%27s%20weighted-sum%20operation%20is%20truncated%20to%20leave%20the%20adjacency%20matrix%0Aas%20the%20output.%20The%20model%20is%20trained%20by%20minimizing%20the%20Jensen-Shannon%20Divergence%0Abetween%20its%20output%20and%20the%20target%20affinity%20matrix%2C%20which%20indicates%20whether%20each%0Apair%20of%20samples%20belongs%20to%20the%20same%20or%20different%20classes.%20The%20main%20contribution%0Aof%20TransFusion%20lies%20in%20defining%20a%20theoretical%20limit%20for%20answering%20two%0Afundamental%20questions%20in%20the%20field%3A%20the%20maximum%20level%20of%20data%20augmentation%20and%0Athe%20minimum%20batch%20size%20required%20for%20effective%20contrastive%20learning.%0AFurthermore%2C%20experimental%20results%20indicate%20that%20TransFusion%20successfully%0Aextracts%20features%20that%20isolate%20clusters%20from%20complex%20real-world%20data%2C%20leading%0Ato%20improved%20classification%20accuracy%20in%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18681v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransFusion%3A%20Contrastive%20Learning%20with%20Transformers&entry.906535625=Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20framework%2C%20TransFusion%2C%20designed%20to%20make%20the%0Aprocess%20of%20contrastive%20learning%20more%20analytical%20and%20explainable.%20TransFusion%0Aconsists%20of%20attention%20blocks%20whose%20softmax%20being%20replaced%20by%20ReLU%2C%20and%20its%0Afinal%20block%27s%20weighted-sum%20operation%20is%20truncated%20to%20leave%20the%20adjacency%20matrix%0Aas%20the%20output.%20The%20model%20is%20trained%20by%20minimizing%20the%20Jensen-Shannon%20Divergence%0Abetween%20its%20output%20and%20the%20target%20affinity%20matrix%2C%20which%20indicates%20whether%20each%0Apair%20of%20samples%20belongs%20to%20the%20same%20or%20different%20classes.%20The%20main%20contribution%0Aof%20TransFusion%20lies%20in%20defining%20a%20theoretical%20limit%20for%20answering%20two%0Afundamental%20questions%20in%20the%20field%3A%20the%20maximum%20level%20of%20data%20augmentation%20and%0Athe%20minimum%20batch%20size%20required%20for%20effective%20contrastive%20learning.%0AFurthermore%2C%20experimental%20results%20indicate%20that%20TransFusion%20successfully%0Aextracts%20features%20that%20isolate%20clusters%20from%20complex%20real-world%20data%2C%20leading%0Ato%20improved%20classification%20accuracy%20in%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18681v1&entry.124074799=Read"},
{"title": "Bridging the Gap: Regularized Reinforcement Learning for Improved\n  Classical Motion Planning with Safety Modules", "author": "Elias Goldsztejn and Ronen I. Brafman", "abstract": "  Classical navigation planners can provide safe navigation, albeit often\nsuboptimally and with hindered human norm compliance. ML-based, contemporary\nautonomous navigation algorithms can imitate more natural and humancompliant\nnavigation, but usually require large and realistic datasets and do not always\nprovide safety guarantees. We present an approach that leverages a classical\nalgorithm to guide reinforcement learning. This greatly improves the results\nand convergence rate of the underlying RL algorithm and requires no\nhuman-expert demonstrations to jump-start the process. Additionally, we\nincorporate a practical fallback system that can switch back to a classical\nplanner to ensure safety. The outcome is a sample efficient ML approach for\nmobile navigation that builds on classical algorithms, improves them to ensure\nhuman compliance, and guarantees safety.\n", "link": "http://arxiv.org/abs/2403.18524v1", "date": "2024-03-27", "relevancy": 2.147, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.55}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5375}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5307}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%3A%20Regularized%20Reinforcement%20Learning%20for%20Improved%0A%20%20Classical%20Motion%20Planning%20with%20Safety%20Modules&body=Title%3A%20Bridging%20the%20Gap%3A%20Regularized%20Reinforcement%20Learning%20for%20Improved%0A%20%20Classical%20Motion%20Planning%20with%20Safety%20Modules%0AAuthor%3A%20Elias%20Goldsztejn%20and%20Ronen%20I.%20Brafman%0AAbstract%3A%20%20%20Classical%20navigation%20planners%20can%20provide%20safe%20navigation%2C%20albeit%20often%0Asuboptimally%20and%20with%20hindered%20human%20norm%20compliance.%20ML-based%2C%20contemporary%0Aautonomous%20navigation%20algorithms%20can%20imitate%20more%20natural%20and%20humancompliant%0Anavigation%2C%20but%20usually%20require%20large%20and%20realistic%20datasets%20and%20do%20not%20always%0Aprovide%20safety%20guarantees.%20We%20present%20an%20approach%20that%20leverages%20a%20classical%0Aalgorithm%20to%20guide%20reinforcement%20learning.%20This%20greatly%20improves%20the%20results%0Aand%20convergence%20rate%20of%20the%20underlying%20RL%20algorithm%20and%20requires%20no%0Ahuman-expert%20demonstrations%20to%20jump-start%20the%20process.%20Additionally%2C%20we%0Aincorporate%20a%20practical%20fallback%20system%20that%20can%20switch%20back%20to%20a%20classical%0Aplanner%20to%20ensure%20safety.%20The%20outcome%20is%20a%20sample%20efficient%20ML%20approach%20for%0Amobile%20navigation%20that%20builds%20on%20classical%20algorithms%2C%20improves%20them%20to%20ensure%0Ahuman%20compliance%2C%20and%20guarantees%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18524v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%3A%20Regularized%20Reinforcement%20Learning%20for%20Improved%0A%20%20Classical%20Motion%20Planning%20with%20Safety%20Modules&entry.906535625=Elias%20Goldsztejn%20and%20Ronen%20I.%20Brafman&entry.1292438233=%20%20Classical%20navigation%20planners%20can%20provide%20safe%20navigation%2C%20albeit%20often%0Asuboptimally%20and%20with%20hindered%20human%20norm%20compliance.%20ML-based%2C%20contemporary%0Aautonomous%20navigation%20algorithms%20can%20imitate%20more%20natural%20and%20humancompliant%0Anavigation%2C%20but%20usually%20require%20large%20and%20realistic%20datasets%20and%20do%20not%20always%0Aprovide%20safety%20guarantees.%20We%20present%20an%20approach%20that%20leverages%20a%20classical%0Aalgorithm%20to%20guide%20reinforcement%20learning.%20This%20greatly%20improves%20the%20results%0Aand%20convergence%20rate%20of%20the%20underlying%20RL%20algorithm%20and%20requires%20no%0Ahuman-expert%20demonstrations%20to%20jump-start%20the%20process.%20Additionally%2C%20we%0Aincorporate%20a%20practical%20fallback%20system%20that%20can%20switch%20back%20to%20a%20classical%0Aplanner%20to%20ensure%20safety.%20The%20outcome%20is%20a%20sample%20efficient%20ML%20approach%20for%0Amobile%20navigation%20that%20builds%20on%20classical%20algorithms%2C%20improves%20them%20to%20ensure%0Ahuman%20compliance%2C%20and%20guarantees%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18524v1&entry.124074799=Read"},
{"title": "SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery", "author": "Camille Billouard and Dawa Derksen and Emmanuelle Sarrazin and Bruno Vallet", "abstract": "  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n", "link": "http://arxiv.org/abs/2403.18711v1", "date": "2024-03-27", "relevancy": 2.1301, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5839}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5261}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SAT-NGP%20%3A%20Unleashing%20Neural%20Graphics%20Primitives%20for%20Fast%20Relightable%0A%20%20Transient-Free%203D%20reconstruction%20from%20Satellite%20Imagery&body=Title%3A%20SAT-NGP%20%3A%20Unleashing%20Neural%20Graphics%20Primitives%20for%20Fast%20Relightable%0A%20%20Transient-Free%203D%20reconstruction%20from%20Satellite%20Imagery%0AAuthor%3A%20Camille%20Billouard%20and%20Dawa%20Derksen%20and%20Emmanuelle%20Sarrazin%20and%20Bruno%20Vallet%0AAbstract%3A%20%20%20Current%20stereo-vision%20pipelines%20produce%20high%20accuracy%203D%20reconstruction%20when%0Ausing%20multiple%20pairs%20or%20triplets%20of%20satellite%20images.%20However%2C%20these%20pipelines%0Aare%20sensitive%20to%20the%20changes%20between%20images%20that%20can%20occur%20as%20a%20result%20of%0Amulti-date%20acquisitions.%20Such%20variations%20are%20mainly%20due%20to%20variable%20shadows%2C%0Areflexions%20and%20transient%20objects%20%28cars%2C%20vegetation%29.%20To%20take%20such%20changes%20into%0Aaccount%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20recently%20been%20applied%20to%20multi-date%0Asatellite%20imagery.%20However%2C%20Neural%20methods%20are%20very%20compute-intensive%2C%20taking%0Adozens%20of%20hours%20to%20learn%2C%20compared%20with%20minutes%20for%20standard%20stereo-vision%0Apipelines.%20Following%20the%20ideas%20of%20Instant%20Neural%20Graphics%20Primitives%20we%20propose%0Ato%20use%20an%20efficient%20sampling%20strategy%20and%20multi-resolution%20hash%20encoding%20to%0Aaccelerate%20the%20learning.%20Our%20model%2C%20Satellite%20Neural%20Graphics%20Primitives%0A%28SAT-NGP%29%20decreases%20the%20learning%20time%20to%2015%20minutes%20while%20maintaining%20the%0Aquality%20of%20the%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18711v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAT-NGP%20%3A%20Unleashing%20Neural%20Graphics%20Primitives%20for%20Fast%20Relightable%0A%20%20Transient-Free%203D%20reconstruction%20from%20Satellite%20Imagery&entry.906535625=Camille%20Billouard%20and%20Dawa%20Derksen%20and%20Emmanuelle%20Sarrazin%20and%20Bruno%20Vallet&entry.1292438233=%20%20Current%20stereo-vision%20pipelines%20produce%20high%20accuracy%203D%20reconstruction%20when%0Ausing%20multiple%20pairs%20or%20triplets%20of%20satellite%20images.%20However%2C%20these%20pipelines%0Aare%20sensitive%20to%20the%20changes%20between%20images%20that%20can%20occur%20as%20a%20result%20of%0Amulti-date%20acquisitions.%20Such%20variations%20are%20mainly%20due%20to%20variable%20shadows%2C%0Areflexions%20and%20transient%20objects%20%28cars%2C%20vegetation%29.%20To%20take%20such%20changes%20into%0Aaccount%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20recently%20been%20applied%20to%20multi-date%0Asatellite%20imagery.%20However%2C%20Neural%20methods%20are%20very%20compute-intensive%2C%20taking%0Adozens%20of%20hours%20to%20learn%2C%20compared%20with%20minutes%20for%20standard%20stereo-vision%0Apipelines.%20Following%20the%20ideas%20of%20Instant%20Neural%20Graphics%20Primitives%20we%20propose%0Ato%20use%20an%20efficient%20sampling%20strategy%20and%20multi-resolution%20hash%20encoding%20to%0Aaccelerate%20the%20learning.%20Our%20model%2C%20Satellite%20Neural%20Graphics%20Primitives%0A%28SAT-NGP%29%20decreases%20the%20learning%20time%20to%2015%20minutes%20while%20maintaining%20the%0Aquality%20of%20the%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18711v1&entry.124074799=Read"},
{"title": "Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection", "author": "Meng Xing and Zhiyong Feng and Yong Su and Changjae Oh", "abstract": "  Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n", "link": "http://arxiv.org/abs/2204.11041v3", "date": "2024-03-27", "relevancy": 2.1229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5378}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5194}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20by%20Erasing%3A%20Conditional%20Entropy%20based%20Transferable%0A%20%20Out-Of-Distribution%20Detection&body=Title%3A%20Learning%20by%20Erasing%3A%20Conditional%20Entropy%20based%20Transferable%0A%20%20Out-Of-Distribution%20Detection%0AAuthor%3A%20Meng%20Xing%20and%20Zhiyong%20Feng%20and%20Yong%20Su%20and%20Changjae%20Oh%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20essential%20to%20handle%20the%20distribution%0Ashifts%20between%20training%20and%20test%20scenarios.%20For%20a%20new%20in-distribution%20%28ID%29%0Adataset%2C%20existing%20methods%20require%20retraining%20to%20capture%20the%20dataset-specific%0Afeature%20representation%20or%20data%20distribution.%20In%20this%20paper%2C%20we%20propose%20a%20deep%0Agenerative%20models%20%28DGM%29%20based%20transferable%20OOD%20detection%20method%2C%20which%20is%0Aunnecessary%20to%20retrain%20on%20a%20new%20ID%20dataset.%20We%20design%20an%20image%20erasing%20strategy%0Ato%20equip%20exclusive%20conditional%20entropy%20distribution%20for%20each%20ID%20dataset%2C%20which%0Adetermines%20the%20discrepancy%20of%20DGM%27s%20posteriori%20ucertainty%20distribution%20on%0Adifferent%20ID%20datasets.%20Owing%20to%20the%20powerful%20representation%20capacity%20of%0Aconvolutional%20neural%20networks%2C%20the%20proposed%20model%20trained%20on%20complex%20dataset%0Acan%20capture%20the%20above%20discrepancy%20between%20ID%20datasets%20without%20retraining%20and%0Athus%20achieve%20transferable%20OOD%20detection.%20We%20validate%20the%20proposed%20method%20on%0Afive%20datasets%20and%20verity%20that%20ours%20achieves%20comparable%20performance%20to%20the%0Astate-of-the-art%20group%20based%20OOD%20detection%20methods%20that%20need%20to%20be%20retrained%20to%0Adeploy%20on%20new%20ID%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/oOHCIOo/CETOOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.11041v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20by%20Erasing%3A%20Conditional%20Entropy%20based%20Transferable%0A%20%20Out-Of-Distribution%20Detection&entry.906535625=Meng%20Xing%20and%20Zhiyong%20Feng%20and%20Yong%20Su%20and%20Changjae%20Oh&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20is%20essential%20to%20handle%20the%20distribution%0Ashifts%20between%20training%20and%20test%20scenarios.%20For%20a%20new%20in-distribution%20%28ID%29%0Adataset%2C%20existing%20methods%20require%20retraining%20to%20capture%20the%20dataset-specific%0Afeature%20representation%20or%20data%20distribution.%20In%20this%20paper%2C%20we%20propose%20a%20deep%0Agenerative%20models%20%28DGM%29%20based%20transferable%20OOD%20detection%20method%2C%20which%20is%0Aunnecessary%20to%20retrain%20on%20a%20new%20ID%20dataset.%20We%20design%20an%20image%20erasing%20strategy%0Ato%20equip%20exclusive%20conditional%20entropy%20distribution%20for%20each%20ID%20dataset%2C%20which%0Adetermines%20the%20discrepancy%20of%20DGM%27s%20posteriori%20ucertainty%20distribution%20on%0Adifferent%20ID%20datasets.%20Owing%20to%20the%20powerful%20representation%20capacity%20of%0Aconvolutional%20neural%20networks%2C%20the%20proposed%20model%20trained%20on%20complex%20dataset%0Acan%20capture%20the%20above%20discrepancy%20between%20ID%20datasets%20without%20retraining%20and%0Athus%20achieve%20transferable%20OOD%20detection.%20We%20validate%20the%20proposed%20method%20on%0Afive%20datasets%20and%20verity%20that%20ours%20achieves%20comparable%20performance%20to%20the%0Astate-of-the-art%20group%20based%20OOD%20detection%20methods%20that%20need%20to%20be%20retrained%20to%0Adeploy%20on%20new%20ID%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/oOHCIOo/CETOOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.11041v3&entry.124074799=Read"},
{"title": "UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction", "author": "Lan Feng and Mohammadhossein Bahari and Kaouther Messaoud Ben Amor and \u00c9loi Zablocki and Matthieu Cord and Alexandre Alahi", "abstract": "  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n", "link": "http://arxiv.org/abs/2403.15098v2", "date": "2024-03-27", "relevancy": 2.1177, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&body=Title%3A%20UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction%0AAuthor%3A%20Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20e.g.%2C%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/vita-epfl/UniTraj%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15098v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTraj%3A%20A%20Unified%20Framework%20for%20Scalable%20Vehicle%20Trajectory%20Prediction&entry.906535625=Lan%20Feng%20and%20Mohammadhossein%20Bahari%20and%20Kaouther%20Messaoud%20Ben%20Amor%20and%20%C3%89loi%20Zablocki%20and%20Matthieu%20Cord%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Vehicle%20trajectory%20prediction%20has%20increasingly%20relied%20on%20data-driven%0Asolutions%2C%20but%20their%20ability%20to%20scale%20to%20different%20data%20domains%20and%20the%20impact%0Aof%20larger%20dataset%20sizes%20on%20their%20generalization%20remain%20under-explored.%20While%0Athese%20questions%20can%20be%20studied%20by%20employing%20multiple%20datasets%2C%20it%20is%0Achallenging%20due%20to%20several%20discrepancies%2C%20e.g.%2C%20in%20data%20formats%2C%20map%0Aresolution%2C%20and%20semantic%20annotation%20types.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20UniTraj%2C%20a%20comprehensive%20framework%20that%20unifies%20various%20datasets%2C%0Amodels%2C%20and%20evaluation%20criteria%2C%20presenting%20new%20opportunities%20for%20the%20vehicle%0Atrajectory%20prediction%20field.%20In%20particular%2C%20using%20UniTraj%2C%20we%20conduct%20extensive%0Aexperiments%20and%20find%20that%20model%20performance%20significantly%20drops%20when%0Atransferred%20to%20other%20datasets.%20However%2C%20enlarging%20data%20size%20and%20diversity%20can%0Asubstantially%20improve%20performance%2C%20leading%20to%20a%20new%20state-of-the-art%20result%20for%0Athe%20nuScenes%20dataset.%20We%20provide%20insights%20into%20dataset%20characteristics%20to%0Aexplain%20these%20findings.%20The%20code%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/vita-epfl/UniTraj%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15098v2&entry.124074799=Read"},
{"title": "Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification", "author": "Pranav Singh and Raviteja Chukkapalli and Shravan Chaudhari and Luoyao Chen and Mei Chen and Jinqian Pan and Craig Smuda and Jacopo Cirrone", "abstract": "  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n", "link": "http://arxiv.org/abs/2311.10319v4", "date": "2024-03-27", "relevancy": 2.1072, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification&body=Title%3A%20Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification%0AAuthor%3A%20Pranav%20Singh%20and%20Raviteja%20Chukkapalli%20and%20Shravan%20Chaudhari%20and%20Luoyao%20Chen%20and%20Mei%20Chen%20and%20Jinqian%20Pan%20and%20Craig%20Smuda%20and%20Jacopo%20Cirrone%0AAbstract%3A%20%20%20Advancements%20in%20clinical%20treatment%20are%20increasingly%20constrained%20by%20the%0Alimitations%20of%20supervised%20learning%20techniques%2C%20which%20depend%20heavily%20on%20large%0Avolumes%20of%20annotated%20data.%20The%20annotation%20process%20is%20not%20only%20costly%20but%20also%0Ademands%20substantial%20time%20from%20clinical%20specialists.%20Addressing%20this%20issue%2C%20we%0Aintroduce%20the%20S4MI%20%28Self-Supervision%20and%20Semi-Supervision%20for%20Medical%20Imaging%29%0Apipeline%2C%20a%20novel%20approach%20that%20leverages%20advancements%20in%20self-supervised%20and%0Asemi-supervised%20learning.%20These%20techniques%20engage%20in%20auxiliary%20tasks%20that%20do%0Anot%20require%20labeling%2C%20thus%20simplifying%20the%20scaling%20of%20machine%20supervision%0Acompared%20to%20fully-supervised%20methods.%20Our%20study%20benchmarks%20these%20techniques%20on%0Athree%20distinct%20medical%20imaging%20datasets%20to%20evaluate%20their%20effectiveness%20in%0Aclassification%20and%20segmentation%20tasks.%20Notably%2C%20we%20observed%20that%20self%0Asupervised%20learning%20significantly%20surpassed%20the%20performance%20of%20supervised%0Amethods%20in%20the%20classification%20of%20all%20evaluated%20datasets.%20Remarkably%2C%20the%0Asemi-supervised%20approach%20demonstrated%20superior%20outcomes%20in%20segmentation%2C%0Aoutperforming%20fully-supervised%20methods%20while%20using%2050%25%20fewer%20labels%20across%20all%0Adatasets.%20In%20line%20with%20our%20commitment%20to%20contributing%20to%20the%20scientific%0Acommunity%2C%20we%20have%20made%20the%20S4MI%20code%20openly%20accessible%2C%20allowing%20for%20broader%0Aapplication%20and%20further%20development%20of%20these%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10319v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification&entry.906535625=Pranav%20Singh%20and%20Raviteja%20Chukkapalli%20and%20Shravan%20Chaudhari%20and%20Luoyao%20Chen%20and%20Mei%20Chen%20and%20Jinqian%20Pan%20and%20Craig%20Smuda%20and%20Jacopo%20Cirrone&entry.1292438233=%20%20Advancements%20in%20clinical%20treatment%20are%20increasingly%20constrained%20by%20the%0Alimitations%20of%20supervised%20learning%20techniques%2C%20which%20depend%20heavily%20on%20large%0Avolumes%20of%20annotated%20data.%20The%20annotation%20process%20is%20not%20only%20costly%20but%20also%0Ademands%20substantial%20time%20from%20clinical%20specialists.%20Addressing%20this%20issue%2C%20we%0Aintroduce%20the%20S4MI%20%28Self-Supervision%20and%20Semi-Supervision%20for%20Medical%20Imaging%29%0Apipeline%2C%20a%20novel%20approach%20that%20leverages%20advancements%20in%20self-supervised%20and%0Asemi-supervised%20learning.%20These%20techniques%20engage%20in%20auxiliary%20tasks%20that%20do%0Anot%20require%20labeling%2C%20thus%20simplifying%20the%20scaling%20of%20machine%20supervision%0Acompared%20to%20fully-supervised%20methods.%20Our%20study%20benchmarks%20these%20techniques%20on%0Athree%20distinct%20medical%20imaging%20datasets%20to%20evaluate%20their%20effectiveness%20in%0Aclassification%20and%20segmentation%20tasks.%20Notably%2C%20we%20observed%20that%20self%0Asupervised%20learning%20significantly%20surpassed%20the%20performance%20of%20supervised%0Amethods%20in%20the%20classification%20of%20all%20evaluated%20datasets.%20Remarkably%2C%20the%0Asemi-supervised%20approach%20demonstrated%20superior%20outcomes%20in%20segmentation%2C%0Aoutperforming%20fully-supervised%20methods%20while%20using%2050%25%20fewer%20labels%20across%20all%0Adatasets.%20In%20line%20with%20our%20commitment%20to%20contributing%20to%20the%20scientific%0Acommunity%2C%20we%20have%20made%20the%20S4MI%20code%20openly%20accessible%2C%20allowing%20for%20broader%0Aapplication%20and%20further%20development%20of%20these%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10319v4&entry.124074799=Read"},
{"title": "InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing", "author": "Ruoyu Zhao and Qingnan Fan and Fei Kou and Shuai Qin and Hong Gu and Wei Wu and Pengcheng Xu and Mingrui Zhu and Nannan Wang and Xinbo Gao", "abstract": "  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n", "link": "http://arxiv.org/abs/2403.18660v1", "date": "2024-03-27", "relevancy": 2.1013, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5511}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5414}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4931}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstructBrush%3A%20Learning%20Attention-based%20Instruction%20Optimization%20for%0A%20%20Image%20Editing&body=Title%3A%20InstructBrush%3A%20Learning%20Attention-based%20Instruction%20Optimization%20for%0A%20%20Image%20Editing%0AAuthor%3A%20Ruoyu%20Zhao%20and%20Qingnan%20Fan%20and%20Fei%20Kou%20and%20Shuai%20Qin%20and%20Hong%20Gu%20and%20Wei%20Wu%20and%20Pengcheng%20Xu%20and%20Mingrui%20Zhu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20instruction-based%20image%20editing%20methods%20have%20garnered%0Asignificant%20attention%20in%20image%20editing.%20However%2C%20despite%20encompassing%20a%20wide%0Arange%20of%20editing%20priors%2C%20these%20methods%20are%20helpless%20when%20handling%20editing%20tasks%0Athat%20are%20challenging%20to%20accurately%20describe%20through%20language.%20We%20propose%0AInstructBrush%2C%20an%20inversion%20method%20for%20instruction-based%20image%20editing%20methods%0Ato%20bridge%20this%20gap.%20It%20extracts%20editing%20effects%20from%20exemplar%20image%20pairs%20as%0Aediting%20instructions%2C%20which%20are%20further%20applied%20for%20image%20editing.%20Two%20key%0Atechniques%20are%20introduced%20into%20InstructBrush%2C%20Attention-based%20Instruction%0AOptimization%20and%20Transformation-oriented%20Instruction%20Initialization%2C%20to%20address%0Athe%20limitations%20of%20the%20previous%20method%20in%20terms%20of%20inversion%20effects%20and%0Ainstruction%20generalization.%20To%20explore%20the%20ability%20of%20instruction%20inversion%0Amethods%20to%20guide%20image%20editing%20in%20open%20scenarios%2C%20we%20establish%20a%0ATransformationOriented%20Paired%20Benchmark%20%28TOP-Bench%29%2C%20which%20contains%20a%20rich%20set%0Aof%20scenes%20and%20editing%20types.%20The%20creation%20of%20this%20benchmark%20paves%20the%20way%20for%0Afurther%20exploration%20of%20instruction%20inversion.%20Quantitatively%20and%20qualitatively%2C%0Aour%20approach%20achieves%20superior%20performance%20in%20editing%20and%20is%20more%20semantically%0Aconsistent%20with%20the%20target%20editing%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18660v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructBrush%3A%20Learning%20Attention-based%20Instruction%20Optimization%20for%0A%20%20Image%20Editing&entry.906535625=Ruoyu%20Zhao%20and%20Qingnan%20Fan%20and%20Fei%20Kou%20and%20Shuai%20Qin%20and%20Hong%20Gu%20and%20Wei%20Wu%20and%20Pengcheng%20Xu%20and%20Mingrui%20Zhu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20In%20recent%20years%2C%20instruction-based%20image%20editing%20methods%20have%20garnered%0Asignificant%20attention%20in%20image%20editing.%20However%2C%20despite%20encompassing%20a%20wide%0Arange%20of%20editing%20priors%2C%20these%20methods%20are%20helpless%20when%20handling%20editing%20tasks%0Athat%20are%20challenging%20to%20accurately%20describe%20through%20language.%20We%20propose%0AInstructBrush%2C%20an%20inversion%20method%20for%20instruction-based%20image%20editing%20methods%0Ato%20bridge%20this%20gap.%20It%20extracts%20editing%20effects%20from%20exemplar%20image%20pairs%20as%0Aediting%20instructions%2C%20which%20are%20further%20applied%20for%20image%20editing.%20Two%20key%0Atechniques%20are%20introduced%20into%20InstructBrush%2C%20Attention-based%20Instruction%0AOptimization%20and%20Transformation-oriented%20Instruction%20Initialization%2C%20to%20address%0Athe%20limitations%20of%20the%20previous%20method%20in%20terms%20of%20inversion%20effects%20and%0Ainstruction%20generalization.%20To%20explore%20the%20ability%20of%20instruction%20inversion%0Amethods%20to%20guide%20image%20editing%20in%20open%20scenarios%2C%20we%20establish%20a%0ATransformationOriented%20Paired%20Benchmark%20%28TOP-Bench%29%2C%20which%20contains%20a%20rich%20set%0Aof%20scenes%20and%20editing%20types.%20The%20creation%20of%20this%20benchmark%20paves%20the%20way%20for%0Afurther%20exploration%20of%20instruction%20inversion.%20Quantitatively%20and%20qualitatively%2C%0Aour%20approach%20achieves%20superior%20performance%20in%20editing%20and%20is%20more%20semantically%0Aconsistent%20with%20the%20target%20editing%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18660v1&entry.124074799=Read"},
{"title": "SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks", "author": "Brian Formento and Wenjie Feng and Chuan Sheng Foo and Luu Anh Tuan and See-Kiong Ng", "abstract": "  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n", "link": "http://arxiv.org/abs/2403.18423v1", "date": "2024-03-27", "relevancy": 2.0892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SemRoDe%3A%20Macro%20Adversarial%20Training%20to%20Learn%20Representations%20That%20are%0A%20%20Robust%20to%20Word-Level%20Attacks&body=Title%3A%20SemRoDe%3A%20Macro%20Adversarial%20Training%20to%20Learn%20Representations%20That%20are%0A%20%20Robust%20to%20Word-Level%20Attacks%0AAuthor%3A%20Brian%20Formento%20and%20Wenjie%20Feng%20and%20Chuan%20Sheng%20Foo%20and%20Luu%20Anh%20Tuan%20and%20See-Kiong%20Ng%0AAbstract%3A%20%20%20Language%20models%20%28LMs%29%20are%20indispensable%20tools%20for%20natural%20language%20processing%0Atasks%2C%20but%20their%20vulnerability%20to%20adversarial%20attacks%20remains%20a%20concern.%20While%0Acurrent%20research%20has%20explored%20adversarial%20training%20techniques%2C%20their%0Aimprovements%20to%20defend%20against%20word-level%20attacks%20have%20been%20limited.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20approach%20called%20Semantic%20Robust%20Defence%20%28SemRoDe%29%2C%20a%0AMacro%20Adversarial%20Training%20strategy%20to%20enhance%20the%20robustness%20of%20LMs.%20Drawing%0Ainspiration%20from%20recent%20studies%20in%20the%20image%20domain%2C%20we%20investigate%20and%20later%0Aconfirm%20that%20in%20a%20discrete%20data%20setting%20such%20as%20language%2C%20adversarial%20samples%0Agenerated%20via%20word%20substitutions%20do%20indeed%20belong%20to%20an%20adversarial%20domain%0Aexhibiting%20a%20high%20Wasserstein%20distance%20from%20the%20base%20domain.%20Our%20method%20learns%0Aa%20robust%20representation%20that%20bridges%20these%20two%20domains.%20We%20hypothesize%20that%20if%0Asamples%20were%20not%20projected%20into%20an%20adversarial%20domain%2C%20but%20instead%20to%20a%20domain%0Awith%20minimal%20shift%2C%20it%20would%20improve%20attack%20robustness.%20We%20align%20the%20domains%20by%0Aincorporating%20a%20new%20distance-based%20objective.%20With%20this%2C%20our%20model%20is%20able%20to%0Alearn%20more%20generalized%20representations%20by%20aligning%20the%20model%27s%20high-level%0Aoutput%20features%20and%20therefore%20better%20handling%20unseen%20adversarial%20samples.%20This%0Amethod%20can%20be%20generalized%20across%20word%20embeddings%2C%20even%20when%20they%20share%20minimal%0Aoverlap%20at%20both%20vocabulary%20and%20word-substitution%20levels.%20To%20evaluate%20the%0Aeffectiveness%20of%20our%20approach%2C%20we%20conduct%20experiments%20on%20BERT%20and%20RoBERTa%0Amodels%20on%20three%20datasets.%20The%20results%20demonstrate%20promising%20state-of-the-art%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18423v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemRoDe%3A%20Macro%20Adversarial%20Training%20to%20Learn%20Representations%20That%20are%0A%20%20Robust%20to%20Word-Level%20Attacks&entry.906535625=Brian%20Formento%20and%20Wenjie%20Feng%20and%20Chuan%20Sheng%20Foo%20and%20Luu%20Anh%20Tuan%20and%20See-Kiong%20Ng&entry.1292438233=%20%20Language%20models%20%28LMs%29%20are%20indispensable%20tools%20for%20natural%20language%20processing%0Atasks%2C%20but%20their%20vulnerability%20to%20adversarial%20attacks%20remains%20a%20concern.%20While%0Acurrent%20research%20has%20explored%20adversarial%20training%20techniques%2C%20their%0Aimprovements%20to%20defend%20against%20word-level%20attacks%20have%20been%20limited.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20approach%20called%20Semantic%20Robust%20Defence%20%28SemRoDe%29%2C%20a%0AMacro%20Adversarial%20Training%20strategy%20to%20enhance%20the%20robustness%20of%20LMs.%20Drawing%0Ainspiration%20from%20recent%20studies%20in%20the%20image%20domain%2C%20we%20investigate%20and%20later%0Aconfirm%20that%20in%20a%20discrete%20data%20setting%20such%20as%20language%2C%20adversarial%20samples%0Agenerated%20via%20word%20substitutions%20do%20indeed%20belong%20to%20an%20adversarial%20domain%0Aexhibiting%20a%20high%20Wasserstein%20distance%20from%20the%20base%20domain.%20Our%20method%20learns%0Aa%20robust%20representation%20that%20bridges%20these%20two%20domains.%20We%20hypothesize%20that%20if%0Asamples%20were%20not%20projected%20into%20an%20adversarial%20domain%2C%20but%20instead%20to%20a%20domain%0Awith%20minimal%20shift%2C%20it%20would%20improve%20attack%20robustness.%20We%20align%20the%20domains%20by%0Aincorporating%20a%20new%20distance-based%20objective.%20With%20this%2C%20our%20model%20is%20able%20to%0Alearn%20more%20generalized%20representations%20by%20aligning%20the%20model%27s%20high-level%0Aoutput%20features%20and%20therefore%20better%20handling%20unseen%20adversarial%20samples.%20This%0Amethod%20can%20be%20generalized%20across%20word%20embeddings%2C%20even%20when%20they%20share%20minimal%0Aoverlap%20at%20both%20vocabulary%20and%20word-substitution%20levels.%20To%20evaluate%20the%0Aeffectiveness%20of%20our%20approach%2C%20we%20conduct%20experiments%20on%20BERT%20and%20RoBERTa%0Amodels%20on%20three%20datasets.%20The%20results%20demonstrate%20promising%20state-of-the-art%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18423v1&entry.124074799=Read"},
{"title": "Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs", "author": "Yichi Zhang and Zhihao Duan and Yuning Huang and Fengqing Zhu", "abstract": "  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n", "link": "http://arxiv.org/abs/2403.18535v1", "date": "2024-03-27", "relevancy": 2.0854, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5055}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Bound-Guided%20Hierarchical%20VAE%20for%20Neural%20Image%20Codecs&body=Title%3A%20Theoretical%20Bound-Guided%20Hierarchical%20VAE%20for%20Neural%20Image%20Codecs%0AAuthor%3A%20Yichi%20Zhang%20and%20Zhihao%20Duan%20and%20Yuning%20Huang%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Recent%20studies%20reveal%20a%20significant%20theoretical%20link%20between%20variational%0Aautoencoders%20%28VAEs%29%20and%20rate-distortion%20theory%2C%20notably%20in%20utilizing%20VAEs%20to%0Aestimate%20the%20theoretical%20upper%20bound%20of%20the%20information%20rate-distortion%0Afunction%20of%20images.%20Such%20estimated%20theoretical%20bounds%20substantially%20exceed%20the%0Aperformance%20of%20existing%20neural%20image%20codecs%20%28NICs%29.%20To%20narrow%20this%20gap%2C%20we%0Apropose%20a%20theoretical%20bound-guided%20hierarchical%20VAE%20%28BG-VAE%29%20for%20NIC.%20The%0Aproposed%20BG-VAE%20leverages%20the%20theoretical%20bound%20to%20guide%20the%20NIC%20model%20towards%0Aenhanced%20performance.%20We%20implement%20the%20BG-VAE%20using%20Hierarchical%20VAEs%20and%0Ademonstrate%20its%20effectiveness%20through%20extensive%20experiments.%20Along%20with%0Aadvanced%20neural%20network%20blocks%2C%20we%20provide%20a%20versatile%2C%20variable-rate%20NIC%20that%0Aoutperforms%20existing%20methods%20when%20considering%20both%20rate-distortion%20performance%0Aand%20computational%20complexity.%20The%20code%20is%20available%20at%20BG-VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18535v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Bound-Guided%20Hierarchical%20VAE%20for%20Neural%20Image%20Codecs&entry.906535625=Yichi%20Zhang%20and%20Zhihao%20Duan%20and%20Yuning%20Huang%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Recent%20studies%20reveal%20a%20significant%20theoretical%20link%20between%20variational%0Aautoencoders%20%28VAEs%29%20and%20rate-distortion%20theory%2C%20notably%20in%20utilizing%20VAEs%20to%0Aestimate%20the%20theoretical%20upper%20bound%20of%20the%20information%20rate-distortion%0Afunction%20of%20images.%20Such%20estimated%20theoretical%20bounds%20substantially%20exceed%20the%0Aperformance%20of%20existing%20neural%20image%20codecs%20%28NICs%29.%20To%20narrow%20this%20gap%2C%20we%0Apropose%20a%20theoretical%20bound-guided%20hierarchical%20VAE%20%28BG-VAE%29%20for%20NIC.%20The%0Aproposed%20BG-VAE%20leverages%20the%20theoretical%20bound%20to%20guide%20the%20NIC%20model%20towards%0Aenhanced%20performance.%20We%20implement%20the%20BG-VAE%20using%20Hierarchical%20VAEs%20and%0Ademonstrate%20its%20effectiveness%20through%20extensive%20experiments.%20Along%20with%0Aadvanced%20neural%20network%20blocks%2C%20we%20provide%20a%20versatile%2C%20variable-rate%20NIC%20that%0Aoutperforms%20existing%20methods%20when%20considering%20both%20rate-distortion%20performance%0Aand%20computational%20complexity.%20The%20code%20is%20available%20at%20BG-VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18535v1&entry.124074799=Read"},
{"title": "Automated Construction of Time-Space Diagrams for Traffic Analysis Using\n  Street-View Video Sequence", "author": "Tanay Rastogi and M\u00e5rten Bj\u00f6rkman", "abstract": "  Time-space diagrams are essential tools for analyzing traffic patterns and\noptimizing transportation infrastructure and traffic management strategies.\nTraditional data collection methods for these diagrams have limitations in\nterms of temporal and spatial coverage. Recent advancements in camera\ntechnology have overcome these limitations and provided extensive urban data.\nIn this study, we propose an innovative approach to constructing time-space\ndiagrams by utilizing street-view video sequences captured by cameras mounted\non moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and\nphotogrammetry techniques for distance calculation, we can infer vehicle\ntrajectories from the video data and generate time-space diagrams. To evaluate\nthe effectiveness of our proposed method, we utilized datasets from the KITTI\ncomputer vision benchmark suite. The evaluation results demonstrate that our\napproach can generate trajectories from video data, although there are some\nerrors that can be mitigated by improving the performance of the detector,\ntracker, and distance calculation components. In conclusion, the utilization of\nstreet-view video sequences captured by cameras mounted on moving vehicles,\ncombined with state-of-the-art computer vision techniques, has immense\npotential for constructing comprehensive time-space diagrams. These diagrams\noffer valuable insights into traffic patterns and contribute to the design of\ntransportation infrastructure and traffic management strategies.\n", "link": "http://arxiv.org/abs/2308.06098v2", "date": "2024-03-27", "relevancy": 2.0837, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5275}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automated%20Construction%20of%20Time-Space%20Diagrams%20for%20Traffic%20Analysis%20Using%0A%20%20Street-View%20Video%20Sequence&body=Title%3A%20Automated%20Construction%20of%20Time-Space%20Diagrams%20for%20Traffic%20Analysis%20Using%0A%20%20Street-View%20Video%20Sequence%0AAuthor%3A%20Tanay%20Rastogi%20and%20M%C3%A5rten%20Bj%C3%B6rkman%0AAbstract%3A%20%20%20Time-space%20diagrams%20are%20essential%20tools%20for%20analyzing%20traffic%20patterns%20and%0Aoptimizing%20transportation%20infrastructure%20and%20traffic%20management%20strategies.%0ATraditional%20data%20collection%20methods%20for%20these%20diagrams%20have%20limitations%20in%0Aterms%20of%20temporal%20and%20spatial%20coverage.%20Recent%20advancements%20in%20camera%0Atechnology%20have%20overcome%20these%20limitations%20and%20provided%20extensive%20urban%20data.%0AIn%20this%20study%2C%20we%20propose%20an%20innovative%20approach%20to%20constructing%20time-space%0Adiagrams%20by%20utilizing%20street-view%20video%20sequences%20captured%20by%20cameras%20mounted%0Aon%20moving%20vehicles.%20Using%20the%20state-of-the-art%20YOLOv5%2C%20StrongSORT%2C%20and%0Aphotogrammetry%20techniques%20for%20distance%20calculation%2C%20we%20can%20infer%20vehicle%0Atrajectories%20from%20the%20video%20data%20and%20generate%20time-space%20diagrams.%20To%20evaluate%0Athe%20effectiveness%20of%20our%20proposed%20method%2C%20we%20utilized%20datasets%20from%20the%20KITTI%0Acomputer%20vision%20benchmark%20suite.%20The%20evaluation%20results%20demonstrate%20that%20our%0Aapproach%20can%20generate%20trajectories%20from%20video%20data%2C%20although%20there%20are%20some%0Aerrors%20that%20can%20be%20mitigated%20by%20improving%20the%20performance%20of%20the%20detector%2C%0Atracker%2C%20and%20distance%20calculation%20components.%20In%20conclusion%2C%20the%20utilization%20of%0Astreet-view%20video%20sequences%20captured%20by%20cameras%20mounted%20on%20moving%20vehicles%2C%0Acombined%20with%20state-of-the-art%20computer%20vision%20techniques%2C%20has%20immense%0Apotential%20for%20constructing%20comprehensive%20time-space%20diagrams.%20These%20diagrams%0Aoffer%20valuable%20insights%20into%20traffic%20patterns%20and%20contribute%20to%20the%20design%20of%0Atransportation%20infrastructure%20and%20traffic%20management%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.06098v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Construction%20of%20Time-Space%20Diagrams%20for%20Traffic%20Analysis%20Using%0A%20%20Street-View%20Video%20Sequence&entry.906535625=Tanay%20Rastogi%20and%20M%C3%A5rten%20Bj%C3%B6rkman&entry.1292438233=%20%20Time-space%20diagrams%20are%20essential%20tools%20for%20analyzing%20traffic%20patterns%20and%0Aoptimizing%20transportation%20infrastructure%20and%20traffic%20management%20strategies.%0ATraditional%20data%20collection%20methods%20for%20these%20diagrams%20have%20limitations%20in%0Aterms%20of%20temporal%20and%20spatial%20coverage.%20Recent%20advancements%20in%20camera%0Atechnology%20have%20overcome%20these%20limitations%20and%20provided%20extensive%20urban%20data.%0AIn%20this%20study%2C%20we%20propose%20an%20innovative%20approach%20to%20constructing%20time-space%0Adiagrams%20by%20utilizing%20street-view%20video%20sequences%20captured%20by%20cameras%20mounted%0Aon%20moving%20vehicles.%20Using%20the%20state-of-the-art%20YOLOv5%2C%20StrongSORT%2C%20and%0Aphotogrammetry%20techniques%20for%20distance%20calculation%2C%20we%20can%20infer%20vehicle%0Atrajectories%20from%20the%20video%20data%20and%20generate%20time-space%20diagrams.%20To%20evaluate%0Athe%20effectiveness%20of%20our%20proposed%20method%2C%20we%20utilized%20datasets%20from%20the%20KITTI%0Acomputer%20vision%20benchmark%20suite.%20The%20evaluation%20results%20demonstrate%20that%20our%0Aapproach%20can%20generate%20trajectories%20from%20video%20data%2C%20although%20there%20are%20some%0Aerrors%20that%20can%20be%20mitigated%20by%20improving%20the%20performance%20of%20the%20detector%2C%0Atracker%2C%20and%20distance%20calculation%20components.%20In%20conclusion%2C%20the%20utilization%20of%0Astreet-view%20video%20sequences%20captured%20by%20cameras%20mounted%20on%20moving%20vehicles%2C%0Acombined%20with%20state-of-the-art%20computer%20vision%20techniques%2C%20has%20immense%0Apotential%20for%20constructing%20comprehensive%20time-space%20diagrams.%20These%20diagrams%0Aoffer%20valuable%20insights%20into%20traffic%20patterns%20and%20contribute%20to%20the%20design%20of%0Atransportation%20infrastructure%20and%20traffic%20management%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.06098v2&entry.124074799=Read"},
{"title": "Deep Learning for Robust and Explainable Models in Computer Vision", "author": "Mohammadreza Amirian", "abstract": "  Recent breakthroughs in machine and deep learning (ML and DL) research have\nprovided excellent tools for leveraging enormous amounts of data and optimizing\nhuge models with millions of parameters to obtain accurate networks for image\nprocessing. These developments open up tremendous opportunities for using\nartificial intelligence (AI) in the automation and human assisted AI industry.\nHowever, as more and more models are deployed and used in practice, many\nchallenges have emerged. This thesis presents various approaches that address\nrobustness and explainability challenges for using ML and DL in practice.\n  Robustness and reliability are the critical components of any model before\ncertification and deployment in practice. Deep convolutional neural networks\n(CNNs) exhibit vulnerability to transformations of their inputs, such as\nrotation and scaling, or intentional manipulations as described in the\nadversarial attack literature. In addition, building trust in AI-based models\nrequires a better understanding of current models and developing methods that\nare more explainable and interpretable a priori.\n  This thesis presents developments in computer vision models' robustness and\nexplainability. Furthermore, this thesis offers an example of using vision\nmodels' feature response visualization (models' interpretations) to improve\nrobustness despite interpretability and robustness being seemingly unrelated in\nthe related research. Besides methodological developments for robust and\nexplainable vision models, a key message of this thesis is introducing model\ninterpretation techniques as a tool for understanding vision models and\nimproving their design and robustness. In addition to the theoretical\ndevelopments, this thesis demonstrates several applications of ML and DL in\ndifferent contexts, such as medical imaging and affective computing.\n", "link": "http://arxiv.org/abs/2403.18674v1", "date": "2024-03-27", "relevancy": 2.0756, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5343}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4985}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Robust%20and%20Explainable%20Models%20in%20Computer%20Vision&body=Title%3A%20Deep%20Learning%20for%20Robust%20and%20Explainable%20Models%20in%20Computer%20Vision%0AAuthor%3A%20Mohammadreza%20Amirian%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20machine%20and%20deep%20learning%20%28ML%20and%20DL%29%20research%20have%0Aprovided%20excellent%20tools%20for%20leveraging%20enormous%20amounts%20of%20data%20and%20optimizing%0Ahuge%20models%20with%20millions%20of%20parameters%20to%20obtain%20accurate%20networks%20for%20image%0Aprocessing.%20These%20developments%20open%20up%20tremendous%20opportunities%20for%20using%0Aartificial%20intelligence%20%28AI%29%20in%20the%20automation%20and%20human%20assisted%20AI%20industry.%0AHowever%2C%20as%20more%20and%20more%20models%20are%20deployed%20and%20used%20in%20practice%2C%20many%0Achallenges%20have%20emerged.%20This%20thesis%20presents%20various%20approaches%20that%20address%0Arobustness%20and%20explainability%20challenges%20for%20using%20ML%20and%20DL%20in%20practice.%0A%20%20Robustness%20and%20reliability%20are%20the%20critical%20components%20of%20any%20model%20before%0Acertification%20and%20deployment%20in%20practice.%20Deep%20convolutional%20neural%20networks%0A%28CNNs%29%20exhibit%20vulnerability%20to%20transformations%20of%20their%20inputs%2C%20such%20as%0Arotation%20and%20scaling%2C%20or%20intentional%20manipulations%20as%20described%20in%20the%0Aadversarial%20attack%20literature.%20In%20addition%2C%20building%20trust%20in%20AI-based%20models%0Arequires%20a%20better%20understanding%20of%20current%20models%20and%20developing%20methods%20that%0Aare%20more%20explainable%20and%20interpretable%20a%20priori.%0A%20%20This%20thesis%20presents%20developments%20in%20computer%20vision%20models%27%20robustness%20and%0Aexplainability.%20Furthermore%2C%20this%20thesis%20offers%20an%20example%20of%20using%20vision%0Amodels%27%20feature%20response%20visualization%20%28models%27%20interpretations%29%20to%20improve%0Arobustness%20despite%20interpretability%20and%20robustness%20being%20seemingly%20unrelated%20in%0Athe%20related%20research.%20Besides%20methodological%20developments%20for%20robust%20and%0Aexplainable%20vision%20models%2C%20a%20key%20message%20of%20this%20thesis%20is%20introducing%20model%0Ainterpretation%20techniques%20as%20a%20tool%20for%20understanding%20vision%20models%20and%0Aimproving%20their%20design%20and%20robustness.%20In%20addition%20to%20the%20theoretical%0Adevelopments%2C%20this%20thesis%20demonstrates%20several%20applications%20of%20ML%20and%20DL%20in%0Adifferent%20contexts%2C%20such%20as%20medical%20imaging%20and%20affective%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Robust%20and%20Explainable%20Models%20in%20Computer%20Vision&entry.906535625=Mohammadreza%20Amirian&entry.1292438233=%20%20Recent%20breakthroughs%20in%20machine%20and%20deep%20learning%20%28ML%20and%20DL%29%20research%20have%0Aprovided%20excellent%20tools%20for%20leveraging%20enormous%20amounts%20of%20data%20and%20optimizing%0Ahuge%20models%20with%20millions%20of%20parameters%20to%20obtain%20accurate%20networks%20for%20image%0Aprocessing.%20These%20developments%20open%20up%20tremendous%20opportunities%20for%20using%0Aartificial%20intelligence%20%28AI%29%20in%20the%20automation%20and%20human%20assisted%20AI%20industry.%0AHowever%2C%20as%20more%20and%20more%20models%20are%20deployed%20and%20used%20in%20practice%2C%20many%0Achallenges%20have%20emerged.%20This%20thesis%20presents%20various%20approaches%20that%20address%0Arobustness%20and%20explainability%20challenges%20for%20using%20ML%20and%20DL%20in%20practice.%0A%20%20Robustness%20and%20reliability%20are%20the%20critical%20components%20of%20any%20model%20before%0Acertification%20and%20deployment%20in%20practice.%20Deep%20convolutional%20neural%20networks%0A%28CNNs%29%20exhibit%20vulnerability%20to%20transformations%20of%20their%20inputs%2C%20such%20as%0Arotation%20and%20scaling%2C%20or%20intentional%20manipulations%20as%20described%20in%20the%0Aadversarial%20attack%20literature.%20In%20addition%2C%20building%20trust%20in%20AI-based%20models%0Arequires%20a%20better%20understanding%20of%20current%20models%20and%20developing%20methods%20that%0Aare%20more%20explainable%20and%20interpretable%20a%20priori.%0A%20%20This%20thesis%20presents%20developments%20in%20computer%20vision%20models%27%20robustness%20and%0Aexplainability.%20Furthermore%2C%20this%20thesis%20offers%20an%20example%20of%20using%20vision%0Amodels%27%20feature%20response%20visualization%20%28models%27%20interpretations%29%20to%20improve%0Arobustness%20despite%20interpretability%20and%20robustness%20being%20seemingly%20unrelated%20in%0Athe%20related%20research.%20Besides%20methodological%20developments%20for%20robust%20and%0Aexplainable%20vision%20models%2C%20a%20key%20message%20of%20this%20thesis%20is%20introducing%20model%0Ainterpretation%20techniques%20as%20a%20tool%20for%20understanding%20vision%20models%20and%0Aimproving%20their%20design%20and%20robustness.%20In%20addition%20to%20the%20theoretical%0Adevelopments%2C%20this%20thesis%20demonstrates%20several%20applications%20of%20ML%20and%20DL%20in%0Adifferent%20contexts%2C%20such%20as%20medical%20imaging%20and%20affective%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18674v1&entry.124074799=Read"},
{"title": "E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion", "author": "Maomao Li and Ge Yuan and Cairong Wang and Zhian Liu and Yong Zhang and Yongwei Nie and Jue Wang and Dong Xu", "abstract": "  This paper proposes a novel approach to face swapping from the perspective of\nfine-grained facial editing, dubbed \"editing for swapping\" (E4S). The\ntraditional face swapping methods rely on global feature extraction and fail to\npreserve the detailed source identity. In contrast, we propose a Regional GAN\nInversion (RGI) method, which allows the explicit disentanglement of shape and\ntexture. Specifically, our E4S performs face swapping in the latent space of a\npretrained StyleGAN, where a multi-scale mask-guided encoder is applied to\nproject the texture of each facial component into regional style codes and a\nmask-guided injection module manipulating feature maps with the style codes.\nBased on this disentanglement, face swapping can be simplified as style and\nmask swapping. Besides, due to the large lighting condition gap, transferring\nthe source skin into the target image may lead to disharmony lighting. We\npropose a re-coloring network to make the swapped face maintain the target\nlighting condition while preserving the source skin. Further, to deal with the\npotential mismatch areas during mask exchange, we design a face inpainting\nmodule to refine the face shape. The extensive comparisons with\nstate-of-the-art methods demonstrate that our E4S outperforms existing methods\nin preserving texture, shape, and lighting. Our implementation is available at\nhttps://github.com/e4s2024/E4S2024.\n", "link": "http://arxiv.org/abs/2310.15081v3", "date": "2024-03-27", "relevancy": 2.0749, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5371}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5054}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20E4S%3A%20Fine-grained%20Face%20Swapping%20via%20Editing%20With%20Regional%20GAN%20Inversion&body=Title%3A%20E4S%3A%20Fine-grained%20Face%20Swapping%20via%20Editing%20With%20Regional%20GAN%20Inversion%0AAuthor%3A%20Maomao%20Li%20and%20Ge%20Yuan%20and%20Cairong%20Wang%20and%20Zhian%20Liu%20and%20Yong%20Zhang%20and%20Yongwei%20Nie%20and%20Jue%20Wang%20and%20Dong%20Xu%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20face%20swapping%20from%20the%20perspective%20of%0Afine-grained%20facial%20editing%2C%20dubbed%20%22editing%20for%20swapping%22%20%28E4S%29.%20The%0Atraditional%20face%20swapping%20methods%20rely%20on%20global%20feature%20extraction%20and%20fail%20to%0Apreserve%20the%20detailed%20source%20identity.%20In%20contrast%2C%20we%20propose%20a%20Regional%20GAN%0AInversion%20%28RGI%29%20method%2C%20which%20allows%20the%20explicit%20disentanglement%20of%20shape%20and%0Atexture.%20Specifically%2C%20our%20E4S%20performs%20face%20swapping%20in%20the%20latent%20space%20of%20a%0Apretrained%20StyleGAN%2C%20where%20a%20multi-scale%20mask-guided%20encoder%20is%20applied%20to%0Aproject%20the%20texture%20of%20each%20facial%20component%20into%20regional%20style%20codes%20and%20a%0Amask-guided%20injection%20module%20manipulating%20feature%20maps%20with%20the%20style%20codes.%0ABased%20on%20this%20disentanglement%2C%20face%20swapping%20can%20be%20simplified%20as%20style%20and%0Amask%20swapping.%20Besides%2C%20due%20to%20the%20large%20lighting%20condition%20gap%2C%20transferring%0Athe%20source%20skin%20into%20the%20target%20image%20may%20lead%20to%20disharmony%20lighting.%20We%0Apropose%20a%20re-coloring%20network%20to%20make%20the%20swapped%20face%20maintain%20the%20target%0Alighting%20condition%20while%20preserving%20the%20source%20skin.%20Further%2C%20to%20deal%20with%20the%0Apotential%20mismatch%20areas%20during%20mask%20exchange%2C%20we%20design%20a%20face%20inpainting%0Amodule%20to%20refine%20the%20face%20shape.%20The%20extensive%20comparisons%20with%0Astate-of-the-art%20methods%20demonstrate%20that%20our%20E4S%20outperforms%20existing%20methods%0Ain%20preserving%20texture%2C%20shape%2C%20and%20lighting.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/e4s2024/E4S2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15081v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E4S%3A%20Fine-grained%20Face%20Swapping%20via%20Editing%20With%20Regional%20GAN%20Inversion&entry.906535625=Maomao%20Li%20and%20Ge%20Yuan%20and%20Cairong%20Wang%20and%20Zhian%20Liu%20and%20Yong%20Zhang%20and%20Yongwei%20Nie%20and%20Jue%20Wang%20and%20Dong%20Xu&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20face%20swapping%20from%20the%20perspective%20of%0Afine-grained%20facial%20editing%2C%20dubbed%20%22editing%20for%20swapping%22%20%28E4S%29.%20The%0Atraditional%20face%20swapping%20methods%20rely%20on%20global%20feature%20extraction%20and%20fail%20to%0Apreserve%20the%20detailed%20source%20identity.%20In%20contrast%2C%20we%20propose%20a%20Regional%20GAN%0AInversion%20%28RGI%29%20method%2C%20which%20allows%20the%20explicit%20disentanglement%20of%20shape%20and%0Atexture.%20Specifically%2C%20our%20E4S%20performs%20face%20swapping%20in%20the%20latent%20space%20of%20a%0Apretrained%20StyleGAN%2C%20where%20a%20multi-scale%20mask-guided%20encoder%20is%20applied%20to%0Aproject%20the%20texture%20of%20each%20facial%20component%20into%20regional%20style%20codes%20and%20a%0Amask-guided%20injection%20module%20manipulating%20feature%20maps%20with%20the%20style%20codes.%0ABased%20on%20this%20disentanglement%2C%20face%20swapping%20can%20be%20simplified%20as%20style%20and%0Amask%20swapping.%20Besides%2C%20due%20to%20the%20large%20lighting%20condition%20gap%2C%20transferring%0Athe%20source%20skin%20into%20the%20target%20image%20may%20lead%20to%20disharmony%20lighting.%20We%0Apropose%20a%20re-coloring%20network%20to%20make%20the%20swapped%20face%20maintain%20the%20target%0Alighting%20condition%20while%20preserving%20the%20source%20skin.%20Further%2C%20to%20deal%20with%20the%0Apotential%20mismatch%20areas%20during%20mask%20exchange%2C%20we%20design%20a%20face%20inpainting%0Amodule%20to%20refine%20the%20face%20shape.%20The%20extensive%20comparisons%20with%0Astate-of-the-art%20methods%20demonstrate%20that%20our%20E4S%20outperforms%20existing%20methods%0Ain%20preserving%20texture%2C%20shape%2C%20and%20lighting.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/e4s2024/E4S2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15081v3&entry.124074799=Read"},
{"title": "Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction", "author": "Qiuhong Shen and Xuanyu Yi and Zike Wu and Pan Zhou and Hanwang Zhang and Shuicheng Yan and Xinchao Wang", "abstract": "  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n", "link": "http://arxiv.org/abs/2403.18795v1", "date": "2024-03-27", "relevancy": 2.0671, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5296}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gamba%3A%20Marry%20Gaussian%20Splatting%20with%20Mamba%20for%20single%20view%203D%0A%20%20reconstruction&body=Title%3A%20Gamba%3A%20Marry%20Gaussian%20Splatting%20with%20Mamba%20for%20single%20view%203D%0A%20%20reconstruction%0AAuthor%3A%20Qiuhong%20Shen%20and%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Pan%20Zhou%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20We%20tackle%20the%20challenge%20of%20efficiently%20reconstructing%20a%203D%20asset%20from%20a%0Asingle%20image%20with%20growing%20demands%20for%20automated%203D%20content%20creation%20pipelines.%0APrevious%20methods%20primarily%20rely%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20Neural%0ARadiance%20Fields%20%28NeRF%29.%20Despite%20their%20significant%20success%2C%20these%20approaches%0Aencounter%20practical%20limitations%20due%20to%20lengthy%20optimization%20and%20considerable%0Amemory%20usage.%20In%20this%20report%2C%20we%20introduce%20Gamba%2C%20an%20end-to-end%20amortized%203D%0Areconstruction%20model%20from%20single-view%20images%2C%20emphasizing%20two%20main%20insights%3A%0A%281%29%203D%20representation%3A%20leveraging%20a%20large%20number%20of%203D%20Gaussians%20for%20an%0Aefficient%203D%20Gaussian%20splatting%20process%3B%20%282%29%20Backbone%20design%3A%20introducing%20a%0AMamba-based%20sequential%20network%20that%20facilitates%20context-dependent%20reasoning%20and%0Alinear%20scalability%20with%20the%20sequence%20%28token%29%20length%2C%20accommodating%20a%0Asubstantial%20number%20of%20Gaussians.%20Gamba%20incorporates%20significant%20advancements%20in%0Adata%20preprocessing%2C%20regularization%20design%2C%20and%20training%20methodologies.%20We%0Aassessed%20Gamba%20against%20existing%20optimization-based%20and%20feed-forward%203D%0Ageneration%20approaches%20using%20the%20real-world%20scanned%20OmniObject3D%20dataset.%20Here%2C%0AGamba%20demonstrates%20competitive%20generation%20capabilities%2C%20both%20qualitatively%20and%0Aquantitatively%2C%20while%20achieving%20remarkable%20speed%2C%20approximately%200.6%20second%20on%20a%0Asingle%20NVIDIA%20A100%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18795v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gamba%3A%20Marry%20Gaussian%20Splatting%20with%20Mamba%20for%20single%20view%203D%0A%20%20reconstruction&entry.906535625=Qiuhong%20Shen%20and%20Xuanyu%20Yi%20and%20Zike%20Wu%20and%20Pan%20Zhou%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang&entry.1292438233=%20%20We%20tackle%20the%20challenge%20of%20efficiently%20reconstructing%20a%203D%20asset%20from%20a%0Asingle%20image%20with%20growing%20demands%20for%20automated%203D%20content%20creation%20pipelines.%0APrevious%20methods%20primarily%20rely%20on%20Score%20Distillation%20Sampling%20%28SDS%29%20and%20Neural%0ARadiance%20Fields%20%28NeRF%29.%20Despite%20their%20significant%20success%2C%20these%20approaches%0Aencounter%20practical%20limitations%20due%20to%20lengthy%20optimization%20and%20considerable%0Amemory%20usage.%20In%20this%20report%2C%20we%20introduce%20Gamba%2C%20an%20end-to-end%20amortized%203D%0Areconstruction%20model%20from%20single-view%20images%2C%20emphasizing%20two%20main%20insights%3A%0A%281%29%203D%20representation%3A%20leveraging%20a%20large%20number%20of%203D%20Gaussians%20for%20an%0Aefficient%203D%20Gaussian%20splatting%20process%3B%20%282%29%20Backbone%20design%3A%20introducing%20a%0AMamba-based%20sequential%20network%20that%20facilitates%20context-dependent%20reasoning%20and%0Alinear%20scalability%20with%20the%20sequence%20%28token%29%20length%2C%20accommodating%20a%0Asubstantial%20number%20of%20Gaussians.%20Gamba%20incorporates%20significant%20advancements%20in%0Adata%20preprocessing%2C%20regularization%20design%2C%20and%20training%20methodologies.%20We%0Aassessed%20Gamba%20against%20existing%20optimization-based%20and%20feed-forward%203D%0Ageneration%20approaches%20using%20the%20real-world%20scanned%20OmniObject3D%20dataset.%20Here%2C%0AGamba%20demonstrates%20competitive%20generation%20capabilities%2C%20both%20qualitatively%20and%0Aquantitatively%2C%20while%20achieving%20remarkable%20speed%2C%20approximately%200.6%20second%20on%20a%0Asingle%20NVIDIA%20A100%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18795v1&entry.124074799=Read"},
{"title": "Collaborative Active Learning in Conditional Trust Environment", "author": "Zan-Kai Chong and Hiroyuki Ohsaki and Bryan Ng", "abstract": "  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n", "link": "http://arxiv.org/abs/2403.18436v1", "date": "2024-03-27", "relevancy": 2.0651, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Active%20Learning%20in%20Conditional%20Trust%20Environment&body=Title%3A%20Collaborative%20Active%20Learning%20in%20Conditional%20Trust%20Environment%0AAuthor%3A%20Zan-Kai%20Chong%20and%20Hiroyuki%20Ohsaki%20and%20Bryan%20Ng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20collaborative%20active%20learning%2C%20a%20paradigm%20in%0Awhich%20multiple%20collaborators%20explore%20a%20new%20domain%20by%20leveraging%20their%20combined%0Amachine%20learning%20capabilities%20without%20disclosing%20their%20existing%20data%20and%0Amodels.%20Instead%2C%20the%20collaborators%20share%20prediction%20results%20from%20the%20new%20domain%0Aand%20newly%20acquired%20labels.%20This%20collaboration%20offers%20several%20advantages%3A%20%28a%29%20it%0Aaddresses%20privacy%20and%20security%20concerns%20by%20eliminating%20the%20need%20for%20direct%0Amodel%20and%20data%20disclosure%3B%20%28b%29%20it%20enables%20the%20use%20of%20different%20data%20sources%20and%0Ainsights%20without%20direct%20data%20exchange%3B%20and%20%28c%29%20it%20promotes%20cost-effectiveness%0Aand%20resource%20efficiency%20through%20shared%20labeling%20costs.%20To%20realize%20these%0Abenefits%2C%20we%20introduce%20a%20collaborative%20active%20learning%20framework%20designed%20to%0Afulfill%20the%20aforementioned%20objectives.%20We%20validate%20the%20effectiveness%20of%20the%0Aproposed%20framework%20through%20simulations.%20The%20results%20demonstrate%20that%0Acollaboration%20leads%20to%20higher%20AUC%20scores%20compared%20to%20independent%20efforts%2C%0Ahighlighting%20the%20framework%27s%20ability%20to%20overcome%20the%20limitations%20of%20individual%0Amodels.%20These%20findings%20support%20the%20use%20of%20collaborative%20approaches%20in%20active%0Alearning%2C%20emphasizing%20their%20potential%20to%20enhance%20outcomes%20through%20collective%0Aexpertise%20and%20shared%20resources.%20Our%20work%20provides%20a%20foundation%20for%20further%0Aresearch%20on%20collaborative%20active%20learning%20and%20its%20practical%20applications%20in%0Avarious%20domains%20where%20data%20privacy%2C%20cost%20efficiency%2C%20and%20model%20performance%20are%0Acritical%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18436v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Active%20Learning%20in%20Conditional%20Trust%20Environment&entry.906535625=Zan-Kai%20Chong%20and%20Hiroyuki%20Ohsaki%20and%20Bryan%20Ng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20collaborative%20active%20learning%2C%20a%20paradigm%20in%0Awhich%20multiple%20collaborators%20explore%20a%20new%20domain%20by%20leveraging%20their%20combined%0Amachine%20learning%20capabilities%20without%20disclosing%20their%20existing%20data%20and%0Amodels.%20Instead%2C%20the%20collaborators%20share%20prediction%20results%20from%20the%20new%20domain%0Aand%20newly%20acquired%20labels.%20This%20collaboration%20offers%20several%20advantages%3A%20%28a%29%20it%0Aaddresses%20privacy%20and%20security%20concerns%20by%20eliminating%20the%20need%20for%20direct%0Amodel%20and%20data%20disclosure%3B%20%28b%29%20it%20enables%20the%20use%20of%20different%20data%20sources%20and%0Ainsights%20without%20direct%20data%20exchange%3B%20and%20%28c%29%20it%20promotes%20cost-effectiveness%0Aand%20resource%20efficiency%20through%20shared%20labeling%20costs.%20To%20realize%20these%0Abenefits%2C%20we%20introduce%20a%20collaborative%20active%20learning%20framework%20designed%20to%0Afulfill%20the%20aforementioned%20objectives.%20We%20validate%20the%20effectiveness%20of%20the%0Aproposed%20framework%20through%20simulations.%20The%20results%20demonstrate%20that%0Acollaboration%20leads%20to%20higher%20AUC%20scores%20compared%20to%20independent%20efforts%2C%0Ahighlighting%20the%20framework%27s%20ability%20to%20overcome%20the%20limitations%20of%20individual%0Amodels.%20These%20findings%20support%20the%20use%20of%20collaborative%20approaches%20in%20active%0Alearning%2C%20emphasizing%20their%20potential%20to%20enhance%20outcomes%20through%20collective%0Aexpertise%20and%20shared%20resources.%20Our%20work%20provides%20a%20foundation%20for%20further%0Aresearch%20on%20collaborative%20active%20learning%20and%20its%20practical%20applications%20in%0Avarious%20domains%20where%20data%20privacy%2C%20cost%20efficiency%2C%20and%20model%20performance%20are%0Acritical%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18436v1&entry.124074799=Read"},
{"title": "LION: Implicit Vision Prompt Tuning", "author": "Haixin Wang and Jianlong Chang and Xiao Luo and Jinan Sun and Zhouchen Lin and Qi Tian", "abstract": "  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n", "link": "http://arxiv.org/abs/2303.09992v3", "date": "2024-03-27", "relevancy": 2.0566, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5376}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5101}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5088}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LION%3A%20Implicit%20Vision%20Prompt%20Tuning&body=Title%3A%20LION%3A%20Implicit%20Vision%20Prompt%20Tuning%0AAuthor%3A%20Haixin%20Wang%20and%20Jianlong%20Chang%20and%20Xiao%20Luo%20and%20Jinan%20Sun%20and%20Zhouchen%20Lin%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Despite%20recent%20competitive%20performance%20across%20a%20range%20of%20vision%20tasks%2C%20vision%0ATransformers%20still%20have%20an%20issue%20of%20heavy%20computational%20costs.%20Recently%2C%20vision%0Aprompt%20learning%20has%20provided%20an%20economic%20solution%20to%20this%20problem%20without%0Afine-tuning%20the%20whole%20large-scale%20models.%20However%2C%20the%20efficiency%20of%20existing%0Amodels%20are%20still%20far%20from%20satisfactory%20due%20to%20insertion%20of%20extensive%20prompts%0Ablocks%20and%20trick%20prompt%20designs.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20vision%0Amodel%20named%20impLicit%20vIsion%20prOmpt%20tuNing%20%28LION%29%2C%20which%20is%20motivated%20by%20deep%0Aimplicit%20models%20with%20stable%20memory%20costs%20for%20various%20complex%20tasks.%20In%0Aparticular%2C%20we%20merely%20insect%20two%20equilibrium%20implicit%20layers%20in%20two%20ends%20of%20the%0Apre-trained%20main%20backbone%20with%20parameters%20in%20the%20backbone%20frozen.%20Moreover%2C%20we%0Aprune%20the%20parameters%20in%20these%20two%20layers%20according%20to%20lottery%20hypothesis.%20The%0Aperformance%20obtained%20by%20our%20LION%20are%20promising%20on%20a%20wide%20range%20of%20datasets.%20In%0Aparticular%2C%20our%20LION%20reduces%20up%20to%2011.5%25%20of%20training%20parameter%20numbers%20while%0Aobtaining%20higher%20performance%20compared%20with%20the%20state-of-the-art%20baseline%20VPT%2C%0Aespecially%20under%20challenging%20scenes.%20Furthermore%2C%20we%20find%20that%20our%20proposed%0ALION%20had%20a%20good%20generalization%20performance%2C%20making%20it%20an%20easy%20way%20to%20boost%0Atransfer%20learning%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09992v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LION%3A%20Implicit%20Vision%20Prompt%20Tuning&entry.906535625=Haixin%20Wang%20and%20Jianlong%20Chang%20and%20Xiao%20Luo%20and%20Jinan%20Sun%20and%20Zhouchen%20Lin%20and%20Qi%20Tian&entry.1292438233=%20%20Despite%20recent%20competitive%20performance%20across%20a%20range%20of%20vision%20tasks%2C%20vision%0ATransformers%20still%20have%20an%20issue%20of%20heavy%20computational%20costs.%20Recently%2C%20vision%0Aprompt%20learning%20has%20provided%20an%20economic%20solution%20to%20this%20problem%20without%0Afine-tuning%20the%20whole%20large-scale%20models.%20However%2C%20the%20efficiency%20of%20existing%0Amodels%20are%20still%20far%20from%20satisfactory%20due%20to%20insertion%20of%20extensive%20prompts%0Ablocks%20and%20trick%20prompt%20designs.%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20vision%0Amodel%20named%20impLicit%20vIsion%20prOmpt%20tuNing%20%28LION%29%2C%20which%20is%20motivated%20by%20deep%0Aimplicit%20models%20with%20stable%20memory%20costs%20for%20various%20complex%20tasks.%20In%0Aparticular%2C%20we%20merely%20insect%20two%20equilibrium%20implicit%20layers%20in%20two%20ends%20of%20the%0Apre-trained%20main%20backbone%20with%20parameters%20in%20the%20backbone%20frozen.%20Moreover%2C%20we%0Aprune%20the%20parameters%20in%20these%20two%20layers%20according%20to%20lottery%20hypothesis.%20The%0Aperformance%20obtained%20by%20our%20LION%20are%20promising%20on%20a%20wide%20range%20of%20datasets.%20In%0Aparticular%2C%20our%20LION%20reduces%20up%20to%2011.5%25%20of%20training%20parameter%20numbers%20while%0Aobtaining%20higher%20performance%20compared%20with%20the%20state-of-the-art%20baseline%20VPT%2C%0Aespecially%20under%20challenging%20scenes.%20Furthermore%2C%20we%20find%20that%20our%20proposed%0ALION%20had%20a%20good%20generalization%20performance%2C%20making%20it%20an%20easy%20way%20to%20boost%0Atransfer%20learning%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09992v3&entry.124074799=Read"},
{"title": "Transformers-based architectures for stroke segmentation: A review", "author": "Yalda Zafari-Ghadim and Essam A. Rashed and Mohamed Mabrok", "abstract": "  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n", "link": "http://arxiv.org/abs/2403.18637v1", "date": "2024-03-27", "relevancy": 2.0511, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transformers-based%20architectures%20for%20stroke%20segmentation%3A%20A%20review&body=Title%3A%20Transformers-based%20architectures%20for%20stroke%20segmentation%3A%20A%20review%0AAuthor%3A%20Yalda%20Zafari-Ghadim%20and%20Essam%20A.%20Rashed%20and%20Mohamed%20Mabrok%0AAbstract%3A%20%20%20Stroke%20remains%20a%20significant%20global%20health%20concern%2C%20necessitating%20precise%20and%0Aefficient%20diagnostic%20tools%20for%20timely%20intervention%20and%20improved%20patient%0Aoutcomes.%20The%20emergence%20of%20deep%20learning%20methodologies%20has%20transformed%20the%0Alandscape%20of%20medical%20image%20analysis.%20Recently%2C%20Transformers%2C%20initially%20designed%0Afor%20natural%20language%20processing%2C%20have%20exhibited%20remarkable%20capabilities%20in%0Avarious%20computer%20vision%20applications%2C%20including%20medical%20image%20analysis.%20This%0Acomprehensive%20review%20aims%20to%20provide%20an%20in-depth%20exploration%20of%20the%0Acutting-edge%20Transformer-based%20architectures%20applied%20in%20the%20context%20of%20stroke%0Asegmentation.%20It%20commences%20with%20an%20exploration%20of%20stroke%20pathology%2C%20imaging%0Amodalities%2C%20and%20the%20challenges%20associated%20with%20accurate%20diagnosis%20and%0Asegmentation.%20Subsequently%2C%20the%20review%20delves%20into%20the%20fundamental%20ideas%20of%0ATransformers%2C%20offering%20detailed%20insights%20into%20their%20architectural%20intricacies%0Aand%20the%20underlying%20mechanisms%20that%20empower%20them%20to%20effectively%20capture%20complex%0Aspatial%20information%20within%20medical%20images.%20The%20existing%20literature%20is%0Asystematically%20categorized%20and%20analyzed%2C%20discussing%20various%20approaches%20that%0Aleverage%20Transformers%20for%20stroke%20segmentation.%20A%20critical%20assessment%20is%0Aprovided%2C%20highlighting%20the%20strengths%20and%20limitations%20of%20these%20methods%2C%0Aincluding%20considerations%20of%20performance%20and%20computational%20efficiency.%0AAdditionally%2C%20this%20review%20explores%20potential%20avenues%20for%20future%20research%20and%0Adevelopment%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18637v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers-based%20architectures%20for%20stroke%20segmentation%3A%20A%20review&entry.906535625=Yalda%20Zafari-Ghadim%20and%20Essam%20A.%20Rashed%20and%20Mohamed%20Mabrok&entry.1292438233=%20%20Stroke%20remains%20a%20significant%20global%20health%20concern%2C%20necessitating%20precise%20and%0Aefficient%20diagnostic%20tools%20for%20timely%20intervention%20and%20improved%20patient%0Aoutcomes.%20The%20emergence%20of%20deep%20learning%20methodologies%20has%20transformed%20the%0Alandscape%20of%20medical%20image%20analysis.%20Recently%2C%20Transformers%2C%20initially%20designed%0Afor%20natural%20language%20processing%2C%20have%20exhibited%20remarkable%20capabilities%20in%0Avarious%20computer%20vision%20applications%2C%20including%20medical%20image%20analysis.%20This%0Acomprehensive%20review%20aims%20to%20provide%20an%20in-depth%20exploration%20of%20the%0Acutting-edge%20Transformer-based%20architectures%20applied%20in%20the%20context%20of%20stroke%0Asegmentation.%20It%20commences%20with%20an%20exploration%20of%20stroke%20pathology%2C%20imaging%0Amodalities%2C%20and%20the%20challenges%20associated%20with%20accurate%20diagnosis%20and%0Asegmentation.%20Subsequently%2C%20the%20review%20delves%20into%20the%20fundamental%20ideas%20of%0ATransformers%2C%20offering%20detailed%20insights%20into%20their%20architectural%20intricacies%0Aand%20the%20underlying%20mechanisms%20that%20empower%20them%20to%20effectively%20capture%20complex%0Aspatial%20information%20within%20medical%20images.%20The%20existing%20literature%20is%0Asystematically%20categorized%20and%20analyzed%2C%20discussing%20various%20approaches%20that%0Aleverage%20Transformers%20for%20stroke%20segmentation.%20A%20critical%20assessment%20is%0Aprovided%2C%20highlighting%20the%20strengths%20and%20limitations%20of%20these%20methods%2C%0Aincluding%20considerations%20of%20performance%20and%20computational%20efficiency.%0AAdditionally%2C%20this%20review%20explores%20potential%20avenues%20for%20future%20research%20and%0Adevelopment%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18637v1&entry.124074799=Read"},
{"title": "Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images\n  with Deep Learning -- A Review", "author": "Mohammadreza Amirian and Daniel Barco and Ivo Herzig and Frank-Peter Schilling", "abstract": "  Deep learning based approaches have been used to improve image quality in\ncone-beam computed tomography (CBCT), a medical imaging technique often used in\napplications such as image-guided radiation therapy, implant dentistry or\northopaedics. In particular, while deep learning methods have been applied to\nreduce various types of CBCT image artifacts arising from motion, metal\nobjects, or low-dose acquisition, a comprehensive review summarizing the\nsuccesses and shortcomings of these approaches, with a primary focus on the\ntype of artifacts rather than the architecture of neural networks, is lacking\nin the literature. In this review, the data generation and simulation\npipelines, and artifact reduction techniques are specifically investigated for\neach type of artifact. We provide an overview of deep learning techniques that\nhave successfully been shown to reduce artifacts in 3D, as well as in\ntime-resolved (4D) CBCT through the use of projection- and/or volume-domain\noptimizations, or by introducing neural networks directly within the CBCT\nreconstruction algorithms. Research gaps are identified to suggest avenues for\nfuture exploration. One of the key findings of this work is an observed trend\ntowards the use of generative models including GANs and score-based or\ndiffusion models, accompanied with the need for more diverse and open training\ndatasets and simulations.\n", "link": "http://arxiv.org/abs/2403.18565v1", "date": "2024-03-27", "relevancy": 2.0489, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4978}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Artifact%20Reduction%20in%203D%20and%204D%20Cone-beam%20Computed%20Tomography%20Images%0A%20%20with%20Deep%20Learning%20--%20A%20Review&body=Title%3A%20Artifact%20Reduction%20in%203D%20and%204D%20Cone-beam%20Computed%20Tomography%20Images%0A%20%20with%20Deep%20Learning%20--%20A%20Review%0AAuthor%3A%20Mohammadreza%20Amirian%20and%20Daniel%20Barco%20and%20Ivo%20Herzig%20and%20Frank-Peter%20Schilling%0AAbstract%3A%20%20%20Deep%20learning%20based%20approaches%20have%20been%20used%20to%20improve%20image%20quality%20in%0Acone-beam%20computed%20tomography%20%28CBCT%29%2C%20a%20medical%20imaging%20technique%20often%20used%20in%0Aapplications%20such%20as%20image-guided%20radiation%20therapy%2C%20implant%20dentistry%20or%0Aorthopaedics.%20In%20particular%2C%20while%20deep%20learning%20methods%20have%20been%20applied%20to%0Areduce%20various%20types%20of%20CBCT%20image%20artifacts%20arising%20from%20motion%2C%20metal%0Aobjects%2C%20or%20low-dose%20acquisition%2C%20a%20comprehensive%20review%20summarizing%20the%0Asuccesses%20and%20shortcomings%20of%20these%20approaches%2C%20with%20a%20primary%20focus%20on%20the%0Atype%20of%20artifacts%20rather%20than%20the%20architecture%20of%20neural%20networks%2C%20is%20lacking%0Ain%20the%20literature.%20In%20this%20review%2C%20the%20data%20generation%20and%20simulation%0Apipelines%2C%20and%20artifact%20reduction%20techniques%20are%20specifically%20investigated%20for%0Aeach%20type%20of%20artifact.%20We%20provide%20an%20overview%20of%20deep%20learning%20techniques%20that%0Ahave%20successfully%20been%20shown%20to%20reduce%20artifacts%20in%203D%2C%20as%20well%20as%20in%0Atime-resolved%20%284D%29%20CBCT%20through%20the%20use%20of%20projection-%20and/or%20volume-domain%0Aoptimizations%2C%20or%20by%20introducing%20neural%20networks%20directly%20within%20the%20CBCT%0Areconstruction%20algorithms.%20Research%20gaps%20are%20identified%20to%20suggest%20avenues%20for%0Afuture%20exploration.%20One%20of%20the%20key%20findings%20of%20this%20work%20is%20an%20observed%20trend%0Atowards%20the%20use%20of%20generative%20models%20including%20GANs%20and%20score-based%20or%0Adiffusion%20models%2C%20accompanied%20with%20the%20need%20for%20more%20diverse%20and%20open%20training%0Adatasets%20and%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18565v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artifact%20Reduction%20in%203D%20and%204D%20Cone-beam%20Computed%20Tomography%20Images%0A%20%20with%20Deep%20Learning%20--%20A%20Review&entry.906535625=Mohammadreza%20Amirian%20and%20Daniel%20Barco%20and%20Ivo%20Herzig%20and%20Frank-Peter%20Schilling&entry.1292438233=%20%20Deep%20learning%20based%20approaches%20have%20been%20used%20to%20improve%20image%20quality%20in%0Acone-beam%20computed%20tomography%20%28CBCT%29%2C%20a%20medical%20imaging%20technique%20often%20used%20in%0Aapplications%20such%20as%20image-guided%20radiation%20therapy%2C%20implant%20dentistry%20or%0Aorthopaedics.%20In%20particular%2C%20while%20deep%20learning%20methods%20have%20been%20applied%20to%0Areduce%20various%20types%20of%20CBCT%20image%20artifacts%20arising%20from%20motion%2C%20metal%0Aobjects%2C%20or%20low-dose%20acquisition%2C%20a%20comprehensive%20review%20summarizing%20the%0Asuccesses%20and%20shortcomings%20of%20these%20approaches%2C%20with%20a%20primary%20focus%20on%20the%0Atype%20of%20artifacts%20rather%20than%20the%20architecture%20of%20neural%20networks%2C%20is%20lacking%0Ain%20the%20literature.%20In%20this%20review%2C%20the%20data%20generation%20and%20simulation%0Apipelines%2C%20and%20artifact%20reduction%20techniques%20are%20specifically%20investigated%20for%0Aeach%20type%20of%20artifact.%20We%20provide%20an%20overview%20of%20deep%20learning%20techniques%20that%0Ahave%20successfully%20been%20shown%20to%20reduce%20artifacts%20in%203D%2C%20as%20well%20as%20in%0Atime-resolved%20%284D%29%20CBCT%20through%20the%20use%20of%20projection-%20and/or%20volume-domain%0Aoptimizations%2C%20or%20by%20introducing%20neural%20networks%20directly%20within%20the%20CBCT%0Areconstruction%20algorithms.%20Research%20gaps%20are%20identified%20to%20suggest%20avenues%20for%0Afuture%20exploration.%20One%20of%20the%20key%20findings%20of%20this%20work%20is%20an%20observed%20trend%0Atowards%20the%20use%20of%20generative%20models%20including%20GANs%20and%20score-based%20or%0Adiffusion%20models%2C%20accompanied%20with%20the%20need%20for%20more%20diverse%20and%20open%20training%0Adatasets%20and%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18565v1&entry.124074799=Read"},
{"title": "HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with\n  Dual-Branch Pix2pix Generator", "author": "Chang Bian and Beth Philips and Tim Cootes and Martin Fergie", "abstract": "  Computational analysis of multiplexed immunofluorescence histology data is\nemerging as an important method for understanding the tumour micro-environment\nin cancer. This work presents HEMIT, a dataset designed for translating\nHematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)\nimages, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC\nimages are multi-component and cellular-level aligned with H&E, enriching\nsupervised stain translation tasks. To our knowledge, HEMIT is the first\npublicly available cellular-level aligned dataset that enables H&E to\nmulti-target mIHC image translation. This dataset provides the computer vision\ncommunity with a valuable resource to develop novel computational methods which\nhave the potential to gain new insights from H&E slide archives.\n  We also propose a new dual-branch generator architecture, using residual\nConvolutional Neural Networks (CNNs) and Swin Transformers which achieves\nbetter translation outcomes than other popular algorithms. When evaluated on\nHEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the\nhighest overall score on key metrics including the Structural Similarity Index\nMeasure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio\n(PSNR). Additionally, downstream analysis has been used to further validate the\nquality of the generated mIHC images. These results set a new benchmark in the\nfield of stain translation tasks.\n", "link": "http://arxiv.org/abs/2403.18501v1", "date": "2024-03-27", "relevancy": 2.0455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4879}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HEMIT%3A%20H%26E%20to%20Multiplex-immunohistochemistry%20Image%20Translation%20with%0A%20%20Dual-Branch%20Pix2pix%20Generator&body=Title%3A%20HEMIT%3A%20H%26E%20to%20Multiplex-immunohistochemistry%20Image%20Translation%20with%0A%20%20Dual-Branch%20Pix2pix%20Generator%0AAuthor%3A%20Chang%20Bian%20and%20Beth%20Philips%20and%20Tim%20Cootes%20and%20Martin%20Fergie%0AAbstract%3A%20%20%20Computational%20analysis%20of%20multiplexed%20immunofluorescence%20histology%20data%20is%0Aemerging%20as%20an%20important%20method%20for%20understanding%20the%20tumour%20micro-environment%0Ain%20cancer.%20This%20work%20presents%20HEMIT%2C%20a%20dataset%20designed%20for%20translating%0AHematoxylin%20and%20Eosin%20%28H%26E%29%20sections%20to%20multiplex-immunohistochemistry%20%28mIHC%29%0Aimages%2C%20featuring%20DAPI%2C%20CD3%2C%20and%20panCK%20markers.%20Distinctively%2C%20HEMIT%27s%20mIHC%0Aimages%20are%20multi-component%20and%20cellular-level%20aligned%20with%20H%26E%2C%20enriching%0Asupervised%20stain%20translation%20tasks.%20To%20our%20knowledge%2C%20HEMIT%20is%20the%20first%0Apublicly%20available%20cellular-level%20aligned%20dataset%20that%20enables%20H%26E%20to%0Amulti-target%20mIHC%20image%20translation.%20This%20dataset%20provides%20the%20computer%20vision%0Acommunity%20with%20a%20valuable%20resource%20to%20develop%20novel%20computational%20methods%20which%0Ahave%20the%20potential%20to%20gain%20new%20insights%20from%20H%26E%20slide%20archives.%0A%20%20We%20also%20propose%20a%20new%20dual-branch%20generator%20architecture%2C%20using%20residual%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Swin%20Transformers%20which%20achieves%0Abetter%20translation%20outcomes%20than%20other%20popular%20algorithms.%20When%20evaluated%20on%0AHEMIT%2C%20it%20outperforms%20pix2pixHD%2C%20pix2pix%2C%20U-Net%2C%20and%20ResNet%2C%20achieving%20the%0Ahighest%20overall%20score%20on%20key%20metrics%20including%20the%20Structural%20Similarity%20Index%0AMeasure%20%28SSIM%29%2C%20Pearson%20correlation%20score%20%28R%29%2C%20and%20Peak%20signal-to-noise%20Ratio%0A%28PSNR%29.%20Additionally%2C%20downstream%20analysis%20has%20been%20used%20to%20further%20validate%20the%0Aquality%20of%20the%20generated%20mIHC%20images.%20These%20results%20set%20a%20new%20benchmark%20in%20the%0Afield%20of%20stain%20translation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18501v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEMIT%3A%20H%26E%20to%20Multiplex-immunohistochemistry%20Image%20Translation%20with%0A%20%20Dual-Branch%20Pix2pix%20Generator&entry.906535625=Chang%20Bian%20and%20Beth%20Philips%20and%20Tim%20Cootes%20and%20Martin%20Fergie&entry.1292438233=%20%20Computational%20analysis%20of%20multiplexed%20immunofluorescence%20histology%20data%20is%0Aemerging%20as%20an%20important%20method%20for%20understanding%20the%20tumour%20micro-environment%0Ain%20cancer.%20This%20work%20presents%20HEMIT%2C%20a%20dataset%20designed%20for%20translating%0AHematoxylin%20and%20Eosin%20%28H%26E%29%20sections%20to%20multiplex-immunohistochemistry%20%28mIHC%29%0Aimages%2C%20featuring%20DAPI%2C%20CD3%2C%20and%20panCK%20markers.%20Distinctively%2C%20HEMIT%27s%20mIHC%0Aimages%20are%20multi-component%20and%20cellular-level%20aligned%20with%20H%26E%2C%20enriching%0Asupervised%20stain%20translation%20tasks.%20To%20our%20knowledge%2C%20HEMIT%20is%20the%20first%0Apublicly%20available%20cellular-level%20aligned%20dataset%20that%20enables%20H%26E%20to%0Amulti-target%20mIHC%20image%20translation.%20This%20dataset%20provides%20the%20computer%20vision%0Acommunity%20with%20a%20valuable%20resource%20to%20develop%20novel%20computational%20methods%20which%0Ahave%20the%20potential%20to%20gain%20new%20insights%20from%20H%26E%20slide%20archives.%0A%20%20We%20also%20propose%20a%20new%20dual-branch%20generator%20architecture%2C%20using%20residual%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Swin%20Transformers%20which%20achieves%0Abetter%20translation%20outcomes%20than%20other%20popular%20algorithms.%20When%20evaluated%20on%0AHEMIT%2C%20it%20outperforms%20pix2pixHD%2C%20pix2pix%2C%20U-Net%2C%20and%20ResNet%2C%20achieving%20the%0Ahighest%20overall%20score%20on%20key%20metrics%20including%20the%20Structural%20Similarity%20Index%0AMeasure%20%28SSIM%29%2C%20Pearson%20correlation%20score%20%28R%29%2C%20and%20Peak%20signal-to-noise%20Ratio%0A%28PSNR%29.%20Additionally%2C%20downstream%20analysis%20has%20been%20used%20to%20further%20validate%20the%0Aquality%20of%20the%20generated%20mIHC%20images.%20These%20results%20set%20a%20new%20benchmark%20in%20the%0Afield%20of%20stain%20translation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18501v1&entry.124074799=Read"},
{"title": "Semi-Supervised Learning for Deep Causal Generative Models", "author": "Yasin Ibrahim and Hermione Warr and Konstantinos Kamnitsas", "abstract": "  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n", "link": "http://arxiv.org/abs/2403.18717v1", "date": "2024-03-27", "relevancy": 2.0409, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5366}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4945}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models&body=Title%3A%20Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models%0AAuthor%3A%20Yasin%20Ibrahim%20and%20Hermione%20Warr%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20Developing%20models%20that%20can%20answer%20questions%20of%20the%20form%20%22How%20would%20%24x%24%20change%0Aif%20%24y%24%20had%20been%20%24z%24%3F%22%20is%20fundamental%20for%20advancing%20medical%20image%20analysis.%0ATraining%20causal%20generative%20models%20that%20address%20such%20counterfactual%20questions%2C%0Athough%2C%20currently%20requires%20that%20all%20relevant%20variables%20have%20been%20observed%20and%0Athat%20corresponding%20labels%20are%20available%20in%20training%20data.%20However%2C%20clinical%0Adata%20may%20not%20have%20complete%20records%20for%20all%20patients%20and%20state%20of%20the%20art%20causal%0Agenerative%20models%20are%20unable%20to%20take%20full%20advantage%20of%20this.%20We%20thus%20develop%2C%0Afor%20the%20first%20time%2C%20a%20semi-supervised%20deep%20causal%20generative%20model%20that%0Aexploits%20the%20causal%20relationships%20between%20variables%20to%20maximise%20the%20use%20of%20all%0Aavailable%20data.%20We%20explore%20this%20in%20the%20setting%20where%20each%20sample%20is%20either%0Afully%20labelled%20or%20fully%20unlabelled%2C%20as%20well%20as%20the%20more%20clinically%20realistic%0Acase%20of%20having%20different%20labels%20missing%20for%20each%20sample.%20We%20leverage%20techniques%0Afrom%20causal%20inference%20to%20infer%20missing%20values%20and%20subsequently%20generate%0Arealistic%20counterfactuals%2C%20even%20for%20samples%20with%20incomplete%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18717v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Learning%20for%20Deep%20Causal%20Generative%20Models&entry.906535625=Yasin%20Ibrahim%20and%20Hermione%20Warr%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20Developing%20models%20that%20can%20answer%20questions%20of%20the%20form%20%22How%20would%20%24x%24%20change%0Aif%20%24y%24%20had%20been%20%24z%24%3F%22%20is%20fundamental%20for%20advancing%20medical%20image%20analysis.%0ATraining%20causal%20generative%20models%20that%20address%20such%20counterfactual%20questions%2C%0Athough%2C%20currently%20requires%20that%20all%20relevant%20variables%20have%20been%20observed%20and%0Athat%20corresponding%20labels%20are%20available%20in%20training%20data.%20However%2C%20clinical%0Adata%20may%20not%20have%20complete%20records%20for%20all%20patients%20and%20state%20of%20the%20art%20causal%0Agenerative%20models%20are%20unable%20to%20take%20full%20advantage%20of%20this.%20We%20thus%20develop%2C%0Afor%20the%20first%20time%2C%20a%20semi-supervised%20deep%20causal%20generative%20model%20that%0Aexploits%20the%20causal%20relationships%20between%20variables%20to%20maximise%20the%20use%20of%20all%0Aavailable%20data.%20We%20explore%20this%20in%20the%20setting%20where%20each%20sample%20is%20either%0Afully%20labelled%20or%20fully%20unlabelled%2C%20as%20well%20as%20the%20more%20clinically%20realistic%0Acase%20of%20having%20different%20labels%20missing%20for%20each%20sample.%20We%20leverage%20techniques%0Afrom%20causal%20inference%20to%20infer%20missing%20values%20and%20subsequently%20generate%0Arealistic%20counterfactuals%2C%20even%20for%20samples%20with%20incomplete%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18717v1&entry.124074799=Read"},
{"title": "Scalable Lipschitz Estimation for CNNs", "author": "Yusuf Sulehman and Tingting Mu", "abstract": "  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n", "link": "http://arxiv.org/abs/2403.18613v1", "date": "2024-03-27", "relevancy": 2.0187, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5042}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4968}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scalable%20Lipschitz%20Estimation%20for%20CNNs&body=Title%3A%20Scalable%20Lipschitz%20Estimation%20for%20CNNs%0AAuthor%3A%20Yusuf%20Sulehman%20and%20Tingting%20Mu%0AAbstract%3A%20%20%20Estimating%20the%20Lipschitz%20constant%20of%20deep%20neural%20networks%20is%20of%20growing%0Ainterest%20as%20it%20is%20useful%20for%20informing%20on%20generalisability%20and%20adversarial%0Arobustness.%20Convolutional%20neural%20networks%20%28CNNs%29%20in%20particular%2C%20underpin%20much%0Aof%20the%20recent%20success%20in%20computer%20vision%20related%20applications.%20However%2C%0Aalthough%20existing%20methods%20for%20estimating%20the%20Lipschitz%20constant%20can%20be%20tight%2C%0Athey%20have%20limited%20scalability%20when%20applied%20to%20CNNs.%20To%20tackle%20this%2C%20we%20propose%0Aa%20novel%20method%20to%20accelerate%20Lipschitz%20constant%20estimation%20for%20CNNs.%20The%20core%0Aidea%20is%20to%20divide%20a%20large%20convolutional%20block%20via%20a%20joint%20layer%20and%20width-wise%0Apartition%2C%20into%20a%20collection%20of%20smaller%20blocks.%20We%20prove%20an%20upper-bound%20on%20the%0ALipschitz%20constant%20of%20the%20larger%20block%20in%20terms%20of%20the%20Lipschitz%20constants%20of%0Athe%20smaller%20blocks.%20Through%20varying%20the%20partition%20factor%2C%20the%20resulting%20method%0Acan%20be%20adjusted%20to%20prioritise%20either%20accuracy%20or%20scalability%20and%20permits%0Aparallelisation.%20We%20demonstrate%20an%20enhanced%20scalability%20and%20comparable%20accuracy%0Ato%20existing%20baselines%20through%20a%20range%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Lipschitz%20Estimation%20for%20CNNs&entry.906535625=Yusuf%20Sulehman%20and%20Tingting%20Mu&entry.1292438233=%20%20Estimating%20the%20Lipschitz%20constant%20of%20deep%20neural%20networks%20is%20of%20growing%0Ainterest%20as%20it%20is%20useful%20for%20informing%20on%20generalisability%20and%20adversarial%0Arobustness.%20Convolutional%20neural%20networks%20%28CNNs%29%20in%20particular%2C%20underpin%20much%0Aof%20the%20recent%20success%20in%20computer%20vision%20related%20applications.%20However%2C%0Aalthough%20existing%20methods%20for%20estimating%20the%20Lipschitz%20constant%20can%20be%20tight%2C%0Athey%20have%20limited%20scalability%20when%20applied%20to%20CNNs.%20To%20tackle%20this%2C%20we%20propose%0Aa%20novel%20method%20to%20accelerate%20Lipschitz%20constant%20estimation%20for%20CNNs.%20The%20core%0Aidea%20is%20to%20divide%20a%20large%20convolutional%20block%20via%20a%20joint%20layer%20and%20width-wise%0Apartition%2C%20into%20a%20collection%20of%20smaller%20blocks.%20We%20prove%20an%20upper-bound%20on%20the%0ALipschitz%20constant%20of%20the%20larger%20block%20in%20terms%20of%20the%20Lipschitz%20constants%20of%0Athe%20smaller%20blocks.%20Through%20varying%20the%20partition%20factor%2C%20the%20resulting%20method%0Acan%20be%20adjusted%20to%20prioritise%20either%20accuracy%20or%20scalability%20and%20permits%0Aparallelisation.%20We%20demonstrate%20an%20enhanced%20scalability%20and%20comparable%20accuracy%0Ato%20existing%20baselines%20through%20a%20range%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18613v1&entry.124074799=Read"},
{"title": "Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation\n  in cluttered environments", "author": "Manan Tayal and Shishir Kolathaya", "abstract": "  In fields such as mining, search and rescue, and archaeological exploration,\nensuring real-time, collision-free navigation of robots in confined, cluttered\nenvironments is imperative. Despite the value of established path planning\nalgorithms, they often face challenges in convergence rates and handling\ndynamic infeasibilities. Alternative techniques like collision cones struggle\nto accurately represent complex obstacle geometries. This paper introduces a\nnovel category of control barrier functions, known as Polygonal Cone Control\nBarrier Function (PolyC2BF), which addresses overestimation and computational\ncomplexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming\n(QP) problem, proves effective in facilitating collision-free movement of\nmultiple robots in complex environments. The efficacy of this approach is\nfurther demonstrated through PyBullet simulations on quadruped (unicycle\nmodel), and crazyflie 2.1 (quadrotor model) in cluttered environments.\n", "link": "http://arxiv.org/abs/2311.08787v2", "date": "2024-03-27", "relevancy": 2.0121, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Polygonal%20Cone%20Control%20Barrier%20Functions%20%28PolyC2BF%29%20for%20safe%20navigation%0A%20%20in%20cluttered%20environments&body=Title%3A%20Polygonal%20Cone%20Control%20Barrier%20Functions%20%28PolyC2BF%29%20for%20safe%20navigation%0A%20%20in%20cluttered%20environments%0AAuthor%3A%20Manan%20Tayal%20and%20Shishir%20Kolathaya%0AAbstract%3A%20%20%20In%20fields%20such%20as%20mining%2C%20search%20and%20rescue%2C%20and%20archaeological%20exploration%2C%0Aensuring%20real-time%2C%20collision-free%20navigation%20of%20robots%20in%20confined%2C%20cluttered%0Aenvironments%20is%20imperative.%20Despite%20the%20value%20of%20established%20path%20planning%0Aalgorithms%2C%20they%20often%20face%20challenges%20in%20convergence%20rates%20and%20handling%0Adynamic%20infeasibilities.%20Alternative%20techniques%20like%20collision%20cones%20struggle%0Ato%20accurately%20represent%20complex%20obstacle%20geometries.%20This%20paper%20introduces%20a%0Anovel%20category%20of%20control%20barrier%20functions%2C%20known%20as%20Polygonal%20Cone%20Control%0ABarrier%20Function%20%28PolyC2BF%29%2C%20which%20addresses%20overestimation%20and%20computational%0Acomplexity%20issues.%20The%20proposed%20PolyC2BF%2C%20formulated%20as%20a%20Quadratic%20Programming%0A%28QP%29%20problem%2C%20proves%20effective%20in%20facilitating%20collision-free%20movement%20of%0Amultiple%20robots%20in%20complex%20environments.%20The%20efficacy%20of%20this%20approach%20is%0Afurther%20demonstrated%20through%20PyBullet%20simulations%20on%20quadruped%20%28unicycle%0Amodel%29%2C%20and%20crazyflie%202.1%20%28quadrotor%20model%29%20in%20cluttered%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08787v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polygonal%20Cone%20Control%20Barrier%20Functions%20%28PolyC2BF%29%20for%20safe%20navigation%0A%20%20in%20cluttered%20environments&entry.906535625=Manan%20Tayal%20and%20Shishir%20Kolathaya&entry.1292438233=%20%20In%20fields%20such%20as%20mining%2C%20search%20and%20rescue%2C%20and%20archaeological%20exploration%2C%0Aensuring%20real-time%2C%20collision-free%20navigation%20of%20robots%20in%20confined%2C%20cluttered%0Aenvironments%20is%20imperative.%20Despite%20the%20value%20of%20established%20path%20planning%0Aalgorithms%2C%20they%20often%20face%20challenges%20in%20convergence%20rates%20and%20handling%0Adynamic%20infeasibilities.%20Alternative%20techniques%20like%20collision%20cones%20struggle%0Ato%20accurately%20represent%20complex%20obstacle%20geometries.%20This%20paper%20introduces%20a%0Anovel%20category%20of%20control%20barrier%20functions%2C%20known%20as%20Polygonal%20Cone%20Control%0ABarrier%20Function%20%28PolyC2BF%29%2C%20which%20addresses%20overestimation%20and%20computational%0Acomplexity%20issues.%20The%20proposed%20PolyC2BF%2C%20formulated%20as%20a%20Quadratic%20Programming%0A%28QP%29%20problem%2C%20proves%20effective%20in%20facilitating%20collision-free%20movement%20of%0Amultiple%20robots%20in%20complex%20environments.%20The%20efficacy%20of%20this%20approach%20is%0Afurther%20demonstrated%20through%20PyBullet%20simulations%20on%20quadruped%20%28unicycle%0Amodel%29%2C%20and%20crazyflie%202.1%20%28quadrotor%20model%29%20in%20cluttered%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08787v2&entry.124074799=Read"},
{"title": "Understanding the Learning Dynamics of Alignment with Human Feedback", "author": "Shawn Im and Yixuan Li", "abstract": "  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n", "link": "http://arxiv.org/abs/2403.18742v1", "date": "2024-03-27", "relevancy": 2.0107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&body=Title%3A%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback%0AAuthor%3A%20Shawn%20Im%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18742v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&entry.906535625=Shawn%20Im%20and%20Yixuan%20Li&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18742v1&entry.124074799=Read"},
{"title": "Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI", "author": "Alex Glinsky and Alexey Sokolsky", "abstract": "  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n", "link": "http://arxiv.org/abs/2403.09700v2", "date": "2024-03-27", "relevancy": 2.0061, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.506}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4849}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shapley%20Values-Powered%20Framework%20for%20Fair%20Reward%20Split%20in%20Content%0A%20%20Produced%20by%20GenAI&body=Title%3A%20Shapley%20Values-Powered%20Framework%20for%20Fair%20Reward%20Split%20in%20Content%0A%20%20Produced%20by%20GenAI%0AAuthor%3A%20Alex%20Glinsky%20and%20Alexey%20Sokolsky%0AAbstract%3A%20%20%20It%20is%20evident%20that%2C%20currently%2C%20generative%20models%20are%20surpassed%20in%20quality%20by%0Ahuman%20professionals.%20However%2C%20with%20the%20advancements%20in%20Artificial%20Intelligence%2C%0Athis%20gap%20will%20narrow%2C%20leading%20to%20scenarios%20where%20individuals%20who%20have%20dedicated%0Ayears%20of%20their%20lives%20to%20mastering%20a%20skill%20become%20obsolete%20due%20to%20their%20high%0Acosts%2C%20which%20are%20inherently%20linked%20to%20the%20time%20they%20require%20to%20complete%20a%20task%0A--%20a%20task%20that%20AI%20could%20accomplish%20in%20minutes%20or%20seconds.%20To%20avoid%20future%0Asocial%20upheavals%2C%20we%20must%2C%20even%20now%2C%20contemplate%20how%20to%20fairly%20assess%20the%0Acontributions%20of%20such%20individuals%20in%20training%20generative%20models%20and%20how%20to%0Acompensate%20them%20for%20the%20reduction%20or%20complete%20loss%20of%20their%20incomes.%20In%20this%0Awork%2C%20we%20propose%20a%20method%20to%20structure%20collaboration%20between%20model%20developers%0Aand%20data%20providers.%20To%20achieve%20this%2C%20we%20employ%20Shapley%20Values%20to%20quantify%20the%0Acontribution%20of%20artist%28s%29%20in%20an%20image%20generated%20by%20the%20Stable%20Diffusion-v1.5%0Amodel%20and%20to%20equitably%20allocate%20the%20reward%20among%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09700v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shapley%20Values-Powered%20Framework%20for%20Fair%20Reward%20Split%20in%20Content%0A%20%20Produced%20by%20GenAI&entry.906535625=Alex%20Glinsky%20and%20Alexey%20Sokolsky&entry.1292438233=%20%20It%20is%20evident%20that%2C%20currently%2C%20generative%20models%20are%20surpassed%20in%20quality%20by%0Ahuman%20professionals.%20However%2C%20with%20the%20advancements%20in%20Artificial%20Intelligence%2C%0Athis%20gap%20will%20narrow%2C%20leading%20to%20scenarios%20where%20individuals%20who%20have%20dedicated%0Ayears%20of%20their%20lives%20to%20mastering%20a%20skill%20become%20obsolete%20due%20to%20their%20high%0Acosts%2C%20which%20are%20inherently%20linked%20to%20the%20time%20they%20require%20to%20complete%20a%20task%0A--%20a%20task%20that%20AI%20could%20accomplish%20in%20minutes%20or%20seconds.%20To%20avoid%20future%0Asocial%20upheavals%2C%20we%20must%2C%20even%20now%2C%20contemplate%20how%20to%20fairly%20assess%20the%0Acontributions%20of%20such%20individuals%20in%20training%20generative%20models%20and%20how%20to%0Acompensate%20them%20for%20the%20reduction%20or%20complete%20loss%20of%20their%20incomes.%20In%20this%0Awork%2C%20we%20propose%20a%20method%20to%20structure%20collaboration%20between%20model%20developers%0Aand%20data%20providers.%20To%20achieve%20this%2C%20we%20employ%20Shapley%20Values%20to%20quantify%20the%0Acontribution%20of%20artist%28s%29%20in%20an%20image%20generated%20by%20the%20Stable%20Diffusion-v1.5%0Amodel%20and%20to%20equitably%20allocate%20the%20reward%20among%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09700v2&entry.124074799=Read"},
{"title": "CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning", "author": "Shiyu Tian and Hongxin Wei and Yiqun Wang and Lei Feng", "abstract": "  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n", "link": "http://arxiv.org/abs/2303.10365v3", "date": "2024-03-27", "relevancy": 2.0014, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4967}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CroSel%3A%20Cross%20Selection%20of%20Confident%20Pseudo%20Labels%20for%20Partial-Label%0A%20%20Learning&body=Title%3A%20CroSel%3A%20Cross%20Selection%20of%20Confident%20Pseudo%20Labels%20for%20Partial-Label%0A%20%20Learning%0AAuthor%3A%20Shiyu%20Tian%20and%20Hongxin%20Wei%20and%20Yiqun%20Wang%20and%20Lei%20Feng%0AAbstract%3A%20%20%20Partial-label%20learning%20%28PLL%29%20is%20an%20important%20weakly%20supervised%20learning%0Aproblem%2C%20which%20allows%20each%20training%20example%20to%20have%20a%20candidate%20label%20set%0Ainstead%20of%20a%20single%20ground-truth%20label.%20Identification-based%20methods%20have%20been%0Awidely%20explored%20to%20tackle%20label%20ambiguity%20issues%20in%20PLL%2C%20which%20regard%20the%20true%0Alabel%20as%20a%20latent%20variable%20to%20be%20identified.%20However%2C%20identifying%20the%20true%0Alabels%20accurately%20and%20completely%20remains%20challenging%2C%20causing%20noise%20in%20pseudo%0Alabels%20during%20model%20training.%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%0ACroSel%2C%20which%20leverages%20historical%20predictions%20from%20the%20model%20to%20identify%20true%0Alabels%20for%20most%20training%20examples.%20First%2C%20we%20introduce%20a%20cross%20selection%0Astrategy%2C%20which%20enables%20two%20deep%20models%20to%20select%20true%20labels%20of%20partially%0Alabeled%20data%20for%20each%20other.%20Besides%2C%20we%20propose%20a%20novel%20consistency%0Aregularization%20term%20called%20co-mix%20to%20avoid%20sample%20waste%20and%20tiny%20noise%20caused%0Aby%20false%20selection.%20In%20this%20way%2C%20CroSel%20can%20pick%20out%20the%20true%20labels%20of%20most%0Aexamples%20with%20high%20precision.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20CroSel%2C%20which%20consistently%20outperforms%20previous%20state-of-the-art%20methods%20on%0Abenchmark%20datasets.%20Additionally%2C%20our%20method%20achieves%20over%2090%5C%25%20accuracy%20and%0Aquantity%20for%20selecting%20true%20labels%20on%20CIFAR-type%20datasets%20under%20various%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10365v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CroSel%3A%20Cross%20Selection%20of%20Confident%20Pseudo%20Labels%20for%20Partial-Label%0A%20%20Learning&entry.906535625=Shiyu%20Tian%20and%20Hongxin%20Wei%20and%20Yiqun%20Wang%20and%20Lei%20Feng&entry.1292438233=%20%20Partial-label%20learning%20%28PLL%29%20is%20an%20important%20weakly%20supervised%20learning%0Aproblem%2C%20which%20allows%20each%20training%20example%20to%20have%20a%20candidate%20label%20set%0Ainstead%20of%20a%20single%20ground-truth%20label.%20Identification-based%20methods%20have%20been%0Awidely%20explored%20to%20tackle%20label%20ambiguity%20issues%20in%20PLL%2C%20which%20regard%20the%20true%0Alabel%20as%20a%20latent%20variable%20to%20be%20identified.%20However%2C%20identifying%20the%20true%0Alabels%20accurately%20and%20completely%20remains%20challenging%2C%20causing%20noise%20in%20pseudo%0Alabels%20during%20model%20training.%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%0ACroSel%2C%20which%20leverages%20historical%20predictions%20from%20the%20model%20to%20identify%20true%0Alabels%20for%20most%20training%20examples.%20First%2C%20we%20introduce%20a%20cross%20selection%0Astrategy%2C%20which%20enables%20two%20deep%20models%20to%20select%20true%20labels%20of%20partially%0Alabeled%20data%20for%20each%20other.%20Besides%2C%20we%20propose%20a%20novel%20consistency%0Aregularization%20term%20called%20co-mix%20to%20avoid%20sample%20waste%20and%20tiny%20noise%20caused%0Aby%20false%20selection.%20In%20this%20way%2C%20CroSel%20can%20pick%20out%20the%20true%20labels%20of%20most%0Aexamples%20with%20high%20precision.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20CroSel%2C%20which%20consistently%20outperforms%20previous%20state-of-the-art%20methods%20on%0Abenchmark%20datasets.%20Additionally%2C%20our%20method%20achieves%20over%2090%5C%25%20accuracy%20and%0Aquantity%20for%20selecting%20true%20labels%20on%20CIFAR-type%20datasets%20under%20various%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10365v3&entry.124074799=Read"},
{"title": "I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation", "author": "Ayoub Karine and Thibault Napol\u00e9on and Maher Jridi", "abstract": "  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2403.18490v1", "date": "2024-03-27", "relevancy": 1.9971, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5039}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4874}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Ayoub%20Karine%20and%20Thibault%20Napol%C3%A9on%20and%20Maher%20Jridi%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20new%20knowledge%20distillation%20method%20tailored%20for%20image%0Asemantic%20segmentation%2C%20termed%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%0A%28I2CKD%29.%20The%20focus%20of%20this%20method%20is%20on%20capturing%20and%20transferring%20knowledge%0Abetween%20the%20intermediate%20layers%20of%20teacher%20%28cumbersome%20model%29%20and%20student%0A%28compact%20model%29.%20For%20knowledge%20extraction%2C%20we%20exploit%20class%20prototypes%20derived%0Afrom%20feature%20maps.%20To%20facilitate%20knowledge%20transfer%2C%20we%20employ%20a%20triplet%20loss%0Ain%20order%20to%20minimize%20intra-class%20variances%20and%20maximize%20inter-class%20variances%0Abetween%20teacher%20and%20student%20prototypes.%20Consequently%2C%20I2CKD%20enables%20the%20student%0Ato%20better%20mimic%20the%20feature%20representation%20of%20the%20teacher%20for%20each%20class%2C%0Athereby%20enhancing%20the%20segmentation%20performance%20of%20the%20compact%20network.%0AExtensive%20experiments%20on%20three%20segmentation%20datasets%2C%20i.e.%2C%20Cityscapes%2C%20Pascal%0AVOC%20and%20CamVid%2C%20using%20various%20teacher-student%20network%20pairs%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18490v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2CKD%20%3A%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Ayoub%20Karine%20and%20Thibault%20Napol%C3%A9on%20and%20Maher%20Jridi&entry.1292438233=%20%20This%20paper%20proposes%20a%20new%20knowledge%20distillation%20method%20tailored%20for%20image%0Asemantic%20segmentation%2C%20termed%20Intra-%20and%20Inter-Class%20Knowledge%20Distillation%0A%28I2CKD%29.%20The%20focus%20of%20this%20method%20is%20on%20capturing%20and%20transferring%20knowledge%0Abetween%20the%20intermediate%20layers%20of%20teacher%20%28cumbersome%20model%29%20and%20student%0A%28compact%20model%29.%20For%20knowledge%20extraction%2C%20we%20exploit%20class%20prototypes%20derived%0Afrom%20feature%20maps.%20To%20facilitate%20knowledge%20transfer%2C%20we%20employ%20a%20triplet%20loss%0Ain%20order%20to%20minimize%20intra-class%20variances%20and%20maximize%20inter-class%20variances%0Abetween%20teacher%20and%20student%20prototypes.%20Consequently%2C%20I2CKD%20enables%20the%20student%0Ato%20better%20mimic%20the%20feature%20representation%20of%20the%20teacher%20for%20each%20class%2C%0Athereby%20enhancing%20the%20segmentation%20performance%20of%20the%20compact%20network.%0AExtensive%20experiments%20on%20three%20segmentation%20datasets%2C%20i.e.%2C%20Cityscapes%2C%20Pascal%0AVOC%20and%20CamVid%2C%20using%20various%20teacher-student%20network%20pairs%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18490v1&entry.124074799=Read"},
{"title": "Dual Structure-Aware Image Filterings for Semi-supervised Medical Image\n  Segmentation", "author": "Yuliang Gu and Zhichao Sun and Tian Chen and Xin Xiao and Yepeng Liu and Yongchao Xu and Laurent Najman", "abstract": "  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-aware image filterings (DSAIF) as\nthe image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying the\nproposed DSAIF to mutually supervised networks decreases the consensus of their\nerroneous predictions on unlabeled images. This helps to alleviate the\nconfirmation bias issue of overfitting to noisy pseudo labels of unlabeled\nimages, and thus effectively improves the segmentation performance. Extensive\nexperimental results on three benchmark datasets demonstrate that the proposed\nmethod significantly/consistently outperforms some state-of-the-art methods.\nThe source codes will be publicly available.\n", "link": "http://arxiv.org/abs/2312.07264v2", "date": "2024-03-27", "relevancy": 1.9943, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4954}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Structure-Aware%20Image%20Filterings%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Dual%20Structure-Aware%20Image%20Filterings%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Yuliang%20Gu%20and%20Zhichao%20Sun%20and%20Tian%20Chen%20and%20Xin%20Xiao%20and%20Yepeng%20Liu%20and%20Yongchao%20Xu%20and%20Laurent%20Najman%0AAbstract%3A%20%20%20Semi-supervised%20image%20segmentation%20has%20attracted%20great%20attention%20recently.%0AThe%20key%20is%20how%20to%20leverage%20unlabeled%20images%20in%20the%20training%20process.%20Most%0Amethods%20maintain%20consistent%20predictions%20of%20the%20unlabeled%20images%20under%0Avariations%20%28e.g.%2C%20adding%20noise/perturbations%2C%20or%20creating%20alternative%20versions%29%0Ain%20the%20image%20and/or%20model%20level.%20In%20most%20image-level%20variation%2C%20medical%20images%0Aoften%20have%20prior%20structure%20information%2C%20which%20has%20not%20been%20well%20explored.%20In%0Athis%20paper%2C%20we%20propose%20novel%20dual%20structure-aware%20image%20filterings%20%28DSAIF%29%20as%0Athe%20image-level%20variations%20for%20semi-supervised%20medical%20image%20segmentation.%0AMotivated%20by%20connected%20filtering%20that%20simplifies%20image%20via%20filtering%20in%0Astructure-aware%20tree-based%20image%20representation%2C%20we%20resort%20to%20the%20dual%20contrast%0Ainvariant%20Max-tree%20and%20Min-tree%20representation.%20Specifically%2C%20we%20propose%20a%0Anovel%20connected%20filtering%20that%20removes%20topologically%20equivalent%20nodes%20%28i.e.%0Aconnected%20components%29%20having%20no%20siblings%20in%20the%20Max/Min-tree.%20This%20results%20in%0Atwo%20filtered%20images%20preserving%20topologically%20critical%20structure.%20Applying%20the%0Aproposed%20DSAIF%20to%20mutually%20supervised%20networks%20decreases%20the%20consensus%20of%20their%0Aerroneous%20predictions%20on%20unlabeled%20images.%20This%20helps%20to%20alleviate%20the%0Aconfirmation%20bias%20issue%20of%20overfitting%20to%20noisy%20pseudo%20labels%20of%20unlabeled%0Aimages%2C%20and%20thus%20effectively%20improves%20the%20segmentation%20performance.%20Extensive%0Aexperimental%20results%20on%20three%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%0Amethod%20significantly/consistently%20outperforms%20some%20state-of-the-art%20methods.%0AThe%20source%20codes%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07264v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Structure-Aware%20Image%20Filterings%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Yuliang%20Gu%20and%20Zhichao%20Sun%20and%20Tian%20Chen%20and%20Xin%20Xiao%20and%20Yepeng%20Liu%20and%20Yongchao%20Xu%20and%20Laurent%20Najman&entry.1292438233=%20%20Semi-supervised%20image%20segmentation%20has%20attracted%20great%20attention%20recently.%0AThe%20key%20is%20how%20to%20leverage%20unlabeled%20images%20in%20the%20training%20process.%20Most%0Amethods%20maintain%20consistent%20predictions%20of%20the%20unlabeled%20images%20under%0Avariations%20%28e.g.%2C%20adding%20noise/perturbations%2C%20or%20creating%20alternative%20versions%29%0Ain%20the%20image%20and/or%20model%20level.%20In%20most%20image-level%20variation%2C%20medical%20images%0Aoften%20have%20prior%20structure%20information%2C%20which%20has%20not%20been%20well%20explored.%20In%0Athis%20paper%2C%20we%20propose%20novel%20dual%20structure-aware%20image%20filterings%20%28DSAIF%29%20as%0Athe%20image-level%20variations%20for%20semi-supervised%20medical%20image%20segmentation.%0AMotivated%20by%20connected%20filtering%20that%20simplifies%20image%20via%20filtering%20in%0Astructure-aware%20tree-based%20image%20representation%2C%20we%20resort%20to%20the%20dual%20contrast%0Ainvariant%20Max-tree%20and%20Min-tree%20representation.%20Specifically%2C%20we%20propose%20a%0Anovel%20connected%20filtering%20that%20removes%20topologically%20equivalent%20nodes%20%28i.e.%0Aconnected%20components%29%20having%20no%20siblings%20in%20the%20Max/Min-tree.%20This%20results%20in%0Atwo%20filtered%20images%20preserving%20topologically%20critical%20structure.%20Applying%20the%0Aproposed%20DSAIF%20to%20mutually%20supervised%20networks%20decreases%20the%20consensus%20of%20their%0Aerroneous%20predictions%20on%20unlabeled%20images.%20This%20helps%20to%20alleviate%20the%0Aconfirmation%20bias%20issue%20of%20overfitting%20to%20noisy%20pseudo%20labels%20of%20unlabeled%0Aimages%2C%20and%20thus%20effectively%20improves%20the%20segmentation%20performance.%20Extensive%0Aexperimental%20results%20on%20three%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%0Amethod%20significantly/consistently%20outperforms%20some%20state-of-the-art%20methods.%0AThe%20source%20codes%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07264v2&entry.124074799=Read"},
{"title": "Direct mineral content prediction from drill core images via transfer\n  learning", "author": "Romana Boiger and Sergey V. Churakov and Ignacio Ballester Llagaria and Georg Kosakowski and Raphael W\u00fcst and Nikolaos I. Prasianakis", "abstract": "  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n", "link": "http://arxiv.org/abs/2403.18495v1", "date": "2024-03-27", "relevancy": 1.994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4966}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Direct%20mineral%20content%20prediction%20from%20drill%20core%20images%20via%20transfer%0A%20%20learning&body=Title%3A%20Direct%20mineral%20content%20prediction%20from%20drill%20core%20images%20via%20transfer%0A%20%20learning%0AAuthor%3A%20Romana%20Boiger%20and%20Sergey%20V.%20Churakov%20and%20Ignacio%20Ballester%20Llagaria%20and%20Georg%20Kosakowski%20and%20Raphael%20W%C3%BCst%20and%20Nikolaos%20I.%20Prasianakis%0AAbstract%3A%20%20%20Deep%20subsurface%20exploration%20is%20important%20for%20mining%2C%20oil%20and%20gas%20industries%2C%0Aas%20well%20as%20in%20the%20assessment%20of%20geological%20units%20for%20the%20disposal%20of%20chemical%0Aor%20nuclear%20waste%2C%20or%20the%20viability%20of%20geothermal%20energy%20systems.%20Typically%2C%0Adetailed%20examinations%20of%20subsurface%20formations%20or%20units%20are%20performed%20on%0Acuttings%20or%20core%20materials%20extracted%20during%20drilling%20campaigns%2C%20as%20well%20as%20on%0Ageophysical%20borehole%20data%2C%20which%20provide%20detailed%20information%20about%20the%0Apetrophysical%20properties%20of%20the%20rocks.%20Depending%20on%20the%20volume%20of%20rock%20samples%0Aand%20the%20analytical%20program%2C%20the%20laboratory%20analysis%20and%20diagnostics%20can%20be%20very%0Atime-consuming.%20This%20study%20investigates%20the%20potential%20of%20utilizing%20machine%0Alearning%2C%20specifically%20convolutional%20neural%20networks%20%28CNN%29%2C%20to%20assess%20the%0Alithology%20and%20mineral%20content%20solely%20from%20analysis%20of%20drill%20core%20images%2C%20aiming%0Ato%20support%20and%20expedite%20the%20subsurface%20geological%20exploration.%20The%20paper%0Aoutlines%20a%20comprehensive%20methodology%2C%20encompassing%20data%20preprocessing%2C%20machine%0Alearning%20methods%2C%20and%20transfer%20learning%20techniques.%20The%20outcome%20reveals%20a%0Aremarkable%2096.7%25%20accuracy%20in%20the%20classification%20of%20drill%20core%20segments%20into%0Adistinct%20formation%20classes.%20Furthermore%2C%20a%20CNN%20model%20was%20trained%20for%20the%0Aevaluation%20of%20mineral%20content%20using%20a%20learning%20data%20set%20from%20multidimensional%0Alog%20analysis%20data%20%28silicate%2C%20total%20clay%2C%20carbonate%29.%20When%20benchmarked%20against%0Alaboratory%20XRD%20measurements%20on%20samples%20from%20the%20cores%2C%20both%20the%20advanced%0Amultidimensional%20log%20analysis%20model%20and%20the%20neural%20network%20approach%20developed%0Ahere%20provide%20equally%20good%20performance.%20This%20work%20demonstrates%20that%20deep%0Alearning%20and%20particularly%20transfer%20learning%20can%20support%20extracting%0Apetrophysical%20properties%2C%20including%20mineral%20content%20and%20formation%0Aclassification%2C%20from%20drill%20core%20images%2C%20thus%20offering%20a%20road%20map%20for%20enhancing%0Amodel%20performance%20and%20data%20set%20quality%20in%20image-based%20analysis%20of%20drill%20cores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20mineral%20content%20prediction%20from%20drill%20core%20images%20via%20transfer%0A%20%20learning&entry.906535625=Romana%20Boiger%20and%20Sergey%20V.%20Churakov%20and%20Ignacio%20Ballester%20Llagaria%20and%20Georg%20Kosakowski%20and%20Raphael%20W%C3%BCst%20and%20Nikolaos%20I.%20Prasianakis&entry.1292438233=%20%20Deep%20subsurface%20exploration%20is%20important%20for%20mining%2C%20oil%20and%20gas%20industries%2C%0Aas%20well%20as%20in%20the%20assessment%20of%20geological%20units%20for%20the%20disposal%20of%20chemical%0Aor%20nuclear%20waste%2C%20or%20the%20viability%20of%20geothermal%20energy%20systems.%20Typically%2C%0Adetailed%20examinations%20of%20subsurface%20formations%20or%20units%20are%20performed%20on%0Acuttings%20or%20core%20materials%20extracted%20during%20drilling%20campaigns%2C%20as%20well%20as%20on%0Ageophysical%20borehole%20data%2C%20which%20provide%20detailed%20information%20about%20the%0Apetrophysical%20properties%20of%20the%20rocks.%20Depending%20on%20the%20volume%20of%20rock%20samples%0Aand%20the%20analytical%20program%2C%20the%20laboratory%20analysis%20and%20diagnostics%20can%20be%20very%0Atime-consuming.%20This%20study%20investigates%20the%20potential%20of%20utilizing%20machine%0Alearning%2C%20specifically%20convolutional%20neural%20networks%20%28CNN%29%2C%20to%20assess%20the%0Alithology%20and%20mineral%20content%20solely%20from%20analysis%20of%20drill%20core%20images%2C%20aiming%0Ato%20support%20and%20expedite%20the%20subsurface%20geological%20exploration.%20The%20paper%0Aoutlines%20a%20comprehensive%20methodology%2C%20encompassing%20data%20preprocessing%2C%20machine%0Alearning%20methods%2C%20and%20transfer%20learning%20techniques.%20The%20outcome%20reveals%20a%0Aremarkable%2096.7%25%20accuracy%20in%20the%20classification%20of%20drill%20core%20segments%20into%0Adistinct%20formation%20classes.%20Furthermore%2C%20a%20CNN%20model%20was%20trained%20for%20the%0Aevaluation%20of%20mineral%20content%20using%20a%20learning%20data%20set%20from%20multidimensional%0Alog%20analysis%20data%20%28silicate%2C%20total%20clay%2C%20carbonate%29.%20When%20benchmarked%20against%0Alaboratory%20XRD%20measurements%20on%20samples%20from%20the%20cores%2C%20both%20the%20advanced%0Amultidimensional%20log%20analysis%20model%20and%20the%20neural%20network%20approach%20developed%0Ahere%20provide%20equally%20good%20performance.%20This%20work%20demonstrates%20that%20deep%0Alearning%20and%20particularly%20transfer%20learning%20can%20support%20extracting%0Apetrophysical%20properties%2C%20including%20mineral%20content%20and%20formation%0Aclassification%2C%20from%20drill%20core%20images%2C%20thus%20offering%20a%20road%20map%20for%20enhancing%0Amodel%20performance%20and%20data%20set%20quality%20in%20image-based%20analysis%20of%20drill%20cores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18495v1&entry.124074799=Read"},
{"title": "Noise-Robust Keyword Spotting through Self-supervised Pretraining", "author": "Jacob M\u00f8rk and Holger Severin Bovbjerg and Gergely Kiss and Zheng-Hua Tan", "abstract": "  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n", "link": "http://arxiv.org/abs/2403.18560v1", "date": "2024-03-27", "relevancy": 1.9869, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5576}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4808}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4421}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Noise-Robust%20Keyword%20Spotting%20through%20Self-supervised%20Pretraining&body=Title%3A%20Noise-Robust%20Keyword%20Spotting%20through%20Self-supervised%20Pretraining%0AAuthor%3A%20Jacob%20M%C3%B8rk%20and%20Holger%20Severin%20Bovbjerg%20and%20Gergely%20Kiss%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Voice%20assistants%20are%20now%20widely%20available%2C%20and%20to%20activate%20them%20a%20keyword%0Aspotting%20%28KWS%29%20algorithm%20is%20used.%20Modern%20KWS%20systems%20are%20mainly%20trained%20using%0Asupervised%20learning%20methods%20and%20require%20a%20large%20amount%20of%20labelled%20data%20to%0Aachieve%20a%20good%20performance.%20Leveraging%20unlabelled%20data%20through%20self-supervised%0Alearning%20%28SSL%29%20has%20been%20shown%20to%20increase%20the%20accuracy%20in%20clean%20conditions.%0AThis%20paper%20explores%20how%20SSL%20pretraining%20such%20as%20Data2Vec%20can%20be%20used%20to%20enhance%0Athe%20robustness%20of%20KWS%20models%20in%20noisy%20conditions%2C%20which%20is%20under-explored.%0A%20%20Models%20of%20three%20different%20sizes%20are%20pretrained%20using%20different%20pretraining%0Aapproaches%20and%20then%20fine-tuned%20for%20KWS.%20These%20models%20are%20then%20tested%20and%0Acompared%20to%20models%20trained%20using%20two%20baseline%20supervised%20learning%20methods%2C%20one%0Abeing%20standard%20training%20using%20clean%20data%20and%20the%20other%20one%20being%20multi-style%0Atraining%20%28MTR%29.%20The%20results%20show%20that%20pretraining%20and%20fine-tuning%20on%20clean%20data%0Ais%20superior%20to%20supervised%20learning%20on%20clean%20data%20across%20all%20testing%20conditions%2C%0Aand%20superior%20to%20supervised%20MTR%20for%20testing%20conditions%20of%20SNR%20above%205%20dB.%20This%0Aindicates%20that%20pretraining%20alone%20can%20increase%20the%20model%27s%20robustness.%20Finally%2C%0Ait%20is%20found%20that%20using%20noisy%20data%20for%20pretraining%20models%2C%20especially%20with%20the%0AData2Vec-denoising%20approach%2C%20significantly%20enhances%20the%20robustness%20of%20KWS%0Amodels%20in%20noisy%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18560v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-Robust%20Keyword%20Spotting%20through%20Self-supervised%20Pretraining&entry.906535625=Jacob%20M%C3%B8rk%20and%20Holger%20Severin%20Bovbjerg%20and%20Gergely%20Kiss%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Voice%20assistants%20are%20now%20widely%20available%2C%20and%20to%20activate%20them%20a%20keyword%0Aspotting%20%28KWS%29%20algorithm%20is%20used.%20Modern%20KWS%20systems%20are%20mainly%20trained%20using%0Asupervised%20learning%20methods%20and%20require%20a%20large%20amount%20of%20labelled%20data%20to%0Aachieve%20a%20good%20performance.%20Leveraging%20unlabelled%20data%20through%20self-supervised%0Alearning%20%28SSL%29%20has%20been%20shown%20to%20increase%20the%20accuracy%20in%20clean%20conditions.%0AThis%20paper%20explores%20how%20SSL%20pretraining%20such%20as%20Data2Vec%20can%20be%20used%20to%20enhance%0Athe%20robustness%20of%20KWS%20models%20in%20noisy%20conditions%2C%20which%20is%20under-explored.%0A%20%20Models%20of%20three%20different%20sizes%20are%20pretrained%20using%20different%20pretraining%0Aapproaches%20and%20then%20fine-tuned%20for%20KWS.%20These%20models%20are%20then%20tested%20and%0Acompared%20to%20models%20trained%20using%20two%20baseline%20supervised%20learning%20methods%2C%20one%0Abeing%20standard%20training%20using%20clean%20data%20and%20the%20other%20one%20being%20multi-style%0Atraining%20%28MTR%29.%20The%20results%20show%20that%20pretraining%20and%20fine-tuning%20on%20clean%20data%0Ais%20superior%20to%20supervised%20learning%20on%20clean%20data%20across%20all%20testing%20conditions%2C%0Aand%20superior%20to%20supervised%20MTR%20for%20testing%20conditions%20of%20SNR%20above%205%20dB.%20This%0Aindicates%20that%20pretraining%20alone%20can%20increase%20the%20model%27s%20robustness.%20Finally%2C%0Ait%20is%20found%20that%20using%20noisy%20data%20for%20pretraining%20models%2C%20especially%20with%20the%0AData2Vec-denoising%20approach%2C%20significantly%20enhances%20the%20robustness%20of%20KWS%0Amodels%20in%20noisy%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18560v1&entry.124074799=Read"},
{"title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment", "author": "Li Siyao and Tianpei Gu and Zhitao Yang and Zhengyu Lin and Ziwei Liu and Henghui Ding and Lei Yang and Chen Change Loy", "abstract": "  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n", "link": "http://arxiv.org/abs/2403.18811v1", "date": "2024-03-27", "relevancy": 1.966, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Duolando%3A%20Follower%20GPT%20with%20Off-Policy%20Reinforcement%20Learning%20for%20Dance%0A%20%20Accompaniment&body=Title%3A%20Duolando%3A%20Follower%20GPT%20with%20Off-Policy%20Reinforcement%20Learning%20for%20Dance%0A%20%20Accompaniment%0AAuthor%3A%20Li%20Siyao%20and%20Tianpei%20Gu%20and%20Zhitao%20Yang%20and%20Zhengyu%20Lin%20and%20Ziwei%20Liu%20and%20Henghui%20Ding%20and%20Lei%20Yang%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20task%20within%20the%20field%20of%203D%20dance%20generation%2C%20termed%0Adance%20accompaniment%2C%20which%20necessitates%20the%20generation%20of%20responsive%20movements%0Afrom%20a%20dance%20partner%2C%20the%20%22follower%22%2C%20synchronized%20with%20the%20lead%20dancer%27s%0Amovements%20and%20the%20underlying%20musical%20rhythm.%20Unlike%20existing%20solo%20or%20group%0Adance%20generation%20tasks%2C%20a%20duet%20dance%20scenario%20entails%20a%20heightened%20degree%20of%0Ainteraction%20between%20the%20two%20participants%2C%20requiring%20delicate%20coordination%20in%0Aboth%20pose%20and%20position.%20To%20support%20this%20task%2C%20we%20first%20build%20a%20large-scale%20and%0Adiverse%20duet%20interactive%20dance%20dataset%2C%20DD100%2C%20by%20recording%20about%20117%20minutes%0Aof%20professional%20dancers%27%20performances.%20To%20address%20the%20challenges%20inherent%20in%0Athis%20task%2C%20we%20propose%20a%20GPT-based%20model%2C%20Duolando%2C%20which%20autoregressively%0Apredicts%20the%20subsequent%20tokenized%20motion%20conditioned%20on%20the%20coordinated%0Ainformation%20of%20the%20music%2C%20the%20leader%27s%20and%20the%20follower%27s%20movements.%20To%20further%0Aenhance%20the%20GPT%27s%20capabilities%20of%20generating%20stable%20results%20on%20unseen%0Aconditions%20%28music%20and%20leader%20motions%29%2C%20we%20devise%20an%20off-policy%20reinforcement%0Alearning%20strategy%20that%20allows%20the%20model%20to%20explore%20viable%20trajectories%20from%0Aout-of-distribution%20samplings%2C%20guided%20by%20human-defined%20rewards.%20Based%20on%20the%0Acollected%20dataset%20and%20proposed%20method%2C%20we%20establish%20a%20benchmark%20with%20several%0Acarefully%20designed%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18811v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Duolando%3A%20Follower%20GPT%20with%20Off-Policy%20Reinforcement%20Learning%20for%20Dance%0A%20%20Accompaniment&entry.906535625=Li%20Siyao%20and%20Tianpei%20Gu%20and%20Zhitao%20Yang%20and%20Zhengyu%20Lin%20and%20Ziwei%20Liu%20and%20Henghui%20Ding%20and%20Lei%20Yang%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20We%20introduce%20a%20novel%20task%20within%20the%20field%20of%203D%20dance%20generation%2C%20termed%0Adance%20accompaniment%2C%20which%20necessitates%20the%20generation%20of%20responsive%20movements%0Afrom%20a%20dance%20partner%2C%20the%20%22follower%22%2C%20synchronized%20with%20the%20lead%20dancer%27s%0Amovements%20and%20the%20underlying%20musical%20rhythm.%20Unlike%20existing%20solo%20or%20group%0Adance%20generation%20tasks%2C%20a%20duet%20dance%20scenario%20entails%20a%20heightened%20degree%20of%0Ainteraction%20between%20the%20two%20participants%2C%20requiring%20delicate%20coordination%20in%0Aboth%20pose%20and%20position.%20To%20support%20this%20task%2C%20we%20first%20build%20a%20large-scale%20and%0Adiverse%20duet%20interactive%20dance%20dataset%2C%20DD100%2C%20by%20recording%20about%20117%20minutes%0Aof%20professional%20dancers%27%20performances.%20To%20address%20the%20challenges%20inherent%20in%0Athis%20task%2C%20we%20propose%20a%20GPT-based%20model%2C%20Duolando%2C%20which%20autoregressively%0Apredicts%20the%20subsequent%20tokenized%20motion%20conditioned%20on%20the%20coordinated%0Ainformation%20of%20the%20music%2C%20the%20leader%27s%20and%20the%20follower%27s%20movements.%20To%20further%0Aenhance%20the%20GPT%27s%20capabilities%20of%20generating%20stable%20results%20on%20unseen%0Aconditions%20%28music%20and%20leader%20motions%29%2C%20we%20devise%20an%20off-policy%20reinforcement%0Alearning%20strategy%20that%20allows%20the%20model%20to%20explore%20viable%20trajectories%20from%0Aout-of-distribution%20samplings%2C%20guided%20by%20human-defined%20rewards.%20Based%20on%20the%0Acollected%20dataset%20and%20proposed%20method%2C%20we%20establish%20a%20benchmark%20with%20several%0Acarefully%20designed%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18811v1&entry.124074799=Read"},
{"title": "Neural Fields for Interactive Visualization of Statistical Dependencies\n  in 3D Simulation Ensembles", "author": "Fatemeh Farokhmanesh and Kevin H\u00f6hlein and Christoph Neuhauser and Tobias Necker and Martin Weissmann and Takemasa Miyoshi and R\u00fcdiger Westermann", "abstract": "  We present the first neural network that has learned to compactly represent\nand can efficiently reconstruct the statistical dependencies between the values\nof physical variables at different spatial locations in large 3D simulation\nensembles. Going beyond linear dependencies, we consider mutual information as\na measure of non-linear dependence. We demonstrate learning and reconstruction\nwith a large weather forecast ensemble comprising 1000 members, each storing\nmultiple physical variables at a 250 x 352 x 20 simulation grid. By\ncircumventing compute-intensive statistical estimators at runtime, we\ndemonstrate significantly reduced memory and computation requirements for\nreconstructing the major dependence structures. This enables embedding the\nestimator into a GPU-accelerated direct volume renderer and interactively\nvisualizing all mutual dependencies for a selected domain point.\n", "link": "http://arxiv.org/abs/2307.02203v5", "date": "2024-03-27", "relevancy": 1.9563, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4887}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4814}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Fields%20for%20Interactive%20Visualization%20of%20Statistical%20Dependencies%0A%20%20in%203D%20Simulation%20Ensembles&body=Title%3A%20Neural%20Fields%20for%20Interactive%20Visualization%20of%20Statistical%20Dependencies%0A%20%20in%203D%20Simulation%20Ensembles%0AAuthor%3A%20Fatemeh%20Farokhmanesh%20and%20Kevin%20H%C3%B6hlein%20and%20Christoph%20Neuhauser%20and%20Tobias%20Necker%20and%20Martin%20Weissmann%20and%20Takemasa%20Miyoshi%20and%20R%C3%BCdiger%20Westermann%0AAbstract%3A%20%20%20We%20present%20the%20first%20neural%20network%20that%20has%20learned%20to%20compactly%20represent%0Aand%20can%20efficiently%20reconstruct%20the%20statistical%20dependencies%20between%20the%20values%0Aof%20physical%20variables%20at%20different%20spatial%20locations%20in%20large%203D%20simulation%0Aensembles.%20Going%20beyond%20linear%20dependencies%2C%20we%20consider%20mutual%20information%20as%0Aa%20measure%20of%20non-linear%20dependence.%20We%20demonstrate%20learning%20and%20reconstruction%0Awith%20a%20large%20weather%20forecast%20ensemble%20comprising%201000%20members%2C%20each%20storing%0Amultiple%20physical%20variables%20at%20a%20250%20x%20352%20x%2020%20simulation%20grid.%20By%0Acircumventing%20compute-intensive%20statistical%20estimators%20at%20runtime%2C%20we%0Ademonstrate%20significantly%20reduced%20memory%20and%20computation%20requirements%20for%0Areconstructing%20the%20major%20dependence%20structures.%20This%20enables%20embedding%20the%0Aestimator%20into%20a%20GPU-accelerated%20direct%20volume%20renderer%20and%20interactively%0Avisualizing%20all%20mutual%20dependencies%20for%20a%20selected%20domain%20point.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.02203v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Fields%20for%20Interactive%20Visualization%20of%20Statistical%20Dependencies%0A%20%20in%203D%20Simulation%20Ensembles&entry.906535625=Fatemeh%20Farokhmanesh%20and%20Kevin%20H%C3%B6hlein%20and%20Christoph%20Neuhauser%20and%20Tobias%20Necker%20and%20Martin%20Weissmann%20and%20Takemasa%20Miyoshi%20and%20R%C3%BCdiger%20Westermann&entry.1292438233=%20%20We%20present%20the%20first%20neural%20network%20that%20has%20learned%20to%20compactly%20represent%0Aand%20can%20efficiently%20reconstruct%20the%20statistical%20dependencies%20between%20the%20values%0Aof%20physical%20variables%20at%20different%20spatial%20locations%20in%20large%203D%20simulation%0Aensembles.%20Going%20beyond%20linear%20dependencies%2C%20we%20consider%20mutual%20information%20as%0Aa%20measure%20of%20non-linear%20dependence.%20We%20demonstrate%20learning%20and%20reconstruction%0Awith%20a%20large%20weather%20forecast%20ensemble%20comprising%201000%20members%2C%20each%20storing%0Amultiple%20physical%20variables%20at%20a%20250%20x%20352%20x%2020%20simulation%20grid.%20By%0Acircumventing%20compute-intensive%20statistical%20estimators%20at%20runtime%2C%20we%0Ademonstrate%20significantly%20reduced%20memory%20and%20computation%20requirements%20for%0Areconstructing%20the%20major%20dependence%20structures.%20This%20enables%20embedding%20the%0Aestimator%20into%20a%20GPU-accelerated%20direct%20volume%20renderer%20and%20interactively%0Avisualizing%20all%20mutual%20dependencies%20for%20a%20selected%20domain%20point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.02203v5&entry.124074799=Read"},
{"title": "Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing", "author": "Sewoong Lee and JinKyou Choi and Min Su Kim", "abstract": "  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n", "link": "http://arxiv.org/abs/2309.11427v2", "date": "2024-03-27", "relevancy": 1.9539, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5378}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4567}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Pre-Training%20of%20Time-Series%20Data%20for%20Unsupervised%20Fault%0A%20%20Detection%20in%20Semiconductor%20Manufacturing&body=Title%3A%20Generative%20Pre-Training%20of%20Time-Series%20Data%20for%20Unsupervised%20Fault%0A%20%20Detection%20in%20Semiconductor%20Manufacturing%0AAuthor%3A%20Sewoong%20Lee%20and%20JinKyou%20Choi%20and%20Min%20Su%20Kim%0AAbstract%3A%20%20%20This%20paper%20introduces%20TRACE-GPT%2C%20which%20stands%20for%20Time-seRies%0AAnomaly-detection%20with%20Convolutional%20Embedding%20and%20Generative%20Pre-trained%0ATransformers.%20TRACE-GPT%20is%20designed%20to%20pre-train%20univariate%20time-series%20sensor%0Adata%20and%20detect%20faults%20on%20unlabeled%20datasets%20in%20semiconductor%20manufacturing.%20In%0Asemiconductor%20industry%2C%20classifying%20abnormal%20time-series%20sensor%20data%20from%0Anormal%20data%20is%20important%20because%20it%20is%20directly%20related%20to%20wafer%20defect.%0AHowever%2C%20small%2C%20unlabeled%2C%20and%20even%20mixed%20training%20data%20without%20enough%0Aanomalies%20make%20classification%20tasks%20difficult.%20In%20this%20research%2C%20we%20capture%0Afeatures%20of%20time-series%20data%20with%20temporal%20convolutional%20embedding%20and%0AGenerative%20Pre-trained%20Transformer%20%28GPT%29%20to%20classify%20abnormal%20sequences%20from%0Anormal%20sequences%20using%20cross%20entropy%20loss.%20We%20prove%20that%20our%20model%20shows%20better%0Aperformance%20than%20previous%20unsupervised%20models%20with%20both%20an%20open%20dataset%2C%20the%0AUniversity%20of%20California%20Riverside%20%28UCR%29%20time-series%20classification%20archive%2C%0Aand%20the%20process%20log%20of%20our%20Chemical%20Vapor%20Deposition%20%28CVD%29%20equipment.%20Our%20model%0Ahas%20the%20highest%20F1%20score%20at%20Equal%20Error%20Rate%20%28EER%29%20across%20all%20datasets%20and%20is%0Aonly%200.026%20below%20the%20supervised%20state-of-the-art%20baseline%20on%20the%20open%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11427v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Pre-Training%20of%20Time-Series%20Data%20for%20Unsupervised%20Fault%0A%20%20Detection%20in%20Semiconductor%20Manufacturing&entry.906535625=Sewoong%20Lee%20and%20JinKyou%20Choi%20and%20Min%20Su%20Kim&entry.1292438233=%20%20This%20paper%20introduces%20TRACE-GPT%2C%20which%20stands%20for%20Time-seRies%0AAnomaly-detection%20with%20Convolutional%20Embedding%20and%20Generative%20Pre-trained%0ATransformers.%20TRACE-GPT%20is%20designed%20to%20pre-train%20univariate%20time-series%20sensor%0Adata%20and%20detect%20faults%20on%20unlabeled%20datasets%20in%20semiconductor%20manufacturing.%20In%0Asemiconductor%20industry%2C%20classifying%20abnormal%20time-series%20sensor%20data%20from%0Anormal%20data%20is%20important%20because%20it%20is%20directly%20related%20to%20wafer%20defect.%0AHowever%2C%20small%2C%20unlabeled%2C%20and%20even%20mixed%20training%20data%20without%20enough%0Aanomalies%20make%20classification%20tasks%20difficult.%20In%20this%20research%2C%20we%20capture%0Afeatures%20of%20time-series%20data%20with%20temporal%20convolutional%20embedding%20and%0AGenerative%20Pre-trained%20Transformer%20%28GPT%29%20to%20classify%20abnormal%20sequences%20from%0Anormal%20sequences%20using%20cross%20entropy%20loss.%20We%20prove%20that%20our%20model%20shows%20better%0Aperformance%20than%20previous%20unsupervised%20models%20with%20both%20an%20open%20dataset%2C%20the%0AUniversity%20of%20California%20Riverside%20%28UCR%29%20time-series%20classification%20archive%2C%0Aand%20the%20process%20log%20of%20our%20Chemical%20Vapor%20Deposition%20%28CVD%29%20equipment.%20Our%20model%0Ahas%20the%20highest%20F1%20score%20at%20Equal%20Error%20Rate%20%28EER%29%20across%20all%20datasets%20and%20is%0Aonly%200.026%20below%20the%20supervised%20state-of-the-art%20baseline%20on%20the%20open%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11427v2&entry.124074799=Read"},
{"title": "Safe Control for Soft-Rigid Robots with Self-Contact using Control\n  Barrier Functions", "author": "Zach J. Patterson and Wei Xiao and Emily Sologuren and Daniela Rus", "abstract": "  Incorporating both flexible and rigid components in robot designs offers a\nunique solution to the limitations of traditional rigid robotics by enabling\nboth compliance and strength. This paper explores the challenges and solutions\nfor controlling soft-rigid hybrid robots, particularly addressing the issue of\nself-contact. Conventional control methods prioritize precise state tracking,\ninadvertently increasing the system's overall stiffness, which is not always\ndesirable in interactions with the environment or within the robot itself. To\naddress this, we investigate the application of Control Barrier Functions\n(CBFs) and High Order CBFs to manage self-contact scenarios in serially\nconnected soft-rigid hybrid robots. Through an analysis based on Piecewise\nConstant Curvature (PCC) kinematics, we establish CBFs within a classical\ncontrol framework for self-contact dynamics. Our methodology is rigorously\nevaluated in both simulation environments and physical hardware systems. The\nfindings demonstrate that our proposed control strategy effectively regulates\nself-contact in soft-rigid hybrid robotic systems, marking a significant\nadvancement in the field of robotics.\n", "link": "http://arxiv.org/abs/2311.03189v2", "date": "2024-03-27", "relevancy": 1.9471, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4911}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4748}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20Control%20for%20Soft-Rigid%20Robots%20with%20Self-Contact%20using%20Control%0A%20%20Barrier%20Functions&body=Title%3A%20Safe%20Control%20for%20Soft-Rigid%20Robots%20with%20Self-Contact%20using%20Control%0A%20%20Barrier%20Functions%0AAuthor%3A%20Zach%20J.%20Patterson%20and%20Wei%20Xiao%20and%20Emily%20Sologuren%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20Incorporating%20both%20flexible%20and%20rigid%20components%20in%20robot%20designs%20offers%20a%0Aunique%20solution%20to%20the%20limitations%20of%20traditional%20rigid%20robotics%20by%20enabling%0Aboth%20compliance%20and%20strength.%20This%20paper%20explores%20the%20challenges%20and%20solutions%0Afor%20controlling%20soft-rigid%20hybrid%20robots%2C%20particularly%20addressing%20the%20issue%20of%0Aself-contact.%20Conventional%20control%20methods%20prioritize%20precise%20state%20tracking%2C%0Ainadvertently%20increasing%20the%20system%27s%20overall%20stiffness%2C%20which%20is%20not%20always%0Adesirable%20in%20interactions%20with%20the%20environment%20or%20within%20the%20robot%20itself.%20To%0Aaddress%20this%2C%20we%20investigate%20the%20application%20of%20Control%20Barrier%20Functions%0A%28CBFs%29%20and%20High%20Order%20CBFs%20to%20manage%20self-contact%20scenarios%20in%20serially%0Aconnected%20soft-rigid%20hybrid%20robots.%20Through%20an%20analysis%20based%20on%20Piecewise%0AConstant%20Curvature%20%28PCC%29%20kinematics%2C%20we%20establish%20CBFs%20within%20a%20classical%0Acontrol%20framework%20for%20self-contact%20dynamics.%20Our%20methodology%20is%20rigorously%0Aevaluated%20in%20both%20simulation%20environments%20and%20physical%20hardware%20systems.%20The%0Afindings%20demonstrate%20that%20our%20proposed%20control%20strategy%20effectively%20regulates%0Aself-contact%20in%20soft-rigid%20hybrid%20robotic%20systems%2C%20marking%20a%20significant%0Aadvancement%20in%20the%20field%20of%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03189v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Control%20for%20Soft-Rigid%20Robots%20with%20Self-Contact%20using%20Control%0A%20%20Barrier%20Functions&entry.906535625=Zach%20J.%20Patterson%20and%20Wei%20Xiao%20and%20Emily%20Sologuren%20and%20Daniela%20Rus&entry.1292438233=%20%20Incorporating%20both%20flexible%20and%20rigid%20components%20in%20robot%20designs%20offers%20a%0Aunique%20solution%20to%20the%20limitations%20of%20traditional%20rigid%20robotics%20by%20enabling%0Aboth%20compliance%20and%20strength.%20This%20paper%20explores%20the%20challenges%20and%20solutions%0Afor%20controlling%20soft-rigid%20hybrid%20robots%2C%20particularly%20addressing%20the%20issue%20of%0Aself-contact.%20Conventional%20control%20methods%20prioritize%20precise%20state%20tracking%2C%0Ainadvertently%20increasing%20the%20system%27s%20overall%20stiffness%2C%20which%20is%20not%20always%0Adesirable%20in%20interactions%20with%20the%20environment%20or%20within%20the%20robot%20itself.%20To%0Aaddress%20this%2C%20we%20investigate%20the%20application%20of%20Control%20Barrier%20Functions%0A%28CBFs%29%20and%20High%20Order%20CBFs%20to%20manage%20self-contact%20scenarios%20in%20serially%0Aconnected%20soft-rigid%20hybrid%20robots.%20Through%20an%20analysis%20based%20on%20Piecewise%0AConstant%20Curvature%20%28PCC%29%20kinematics%2C%20we%20establish%20CBFs%20within%20a%20classical%0Acontrol%20framework%20for%20self-contact%20dynamics.%20Our%20methodology%20is%20rigorously%0Aevaluated%20in%20both%20simulation%20environments%20and%20physical%20hardware%20systems.%20The%0Afindings%20demonstrate%20that%20our%20proposed%20control%20strategy%20effectively%20regulates%0Aself-contact%20in%20soft-rigid%20hybrid%20robotic%20systems%2C%20marking%20a%20significant%0Aadvancement%20in%20the%20field%20of%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03189v2&entry.124074799=Read"},
{"title": "Bringing Textual Prompt to AI-Generated Image Quality Assessment", "author": "Bowen Qu and Haohui Li and Wei Gao", "abstract": "  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n", "link": "http://arxiv.org/abs/2403.18714v1", "date": "2024-03-27", "relevancy": 1.9442, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4963}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4858}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4759}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bringing%20Textual%20Prompt%20to%20AI-Generated%20Image%20Quality%20Assessment&body=Title%3A%20Bringing%20Textual%20Prompt%20to%20AI-Generated%20Image%20Quality%20Assessment%0AAuthor%3A%20Bowen%20Qu%20and%20Haohui%20Li%20and%20Wei%20Gao%0AAbstract%3A%20%20%20AI-Generated%20Images%20%28AGIs%29%20have%20inherent%20multimodal%20nature.%20Unlike%0Atraditional%20image%20quality%20assessment%20%28IQA%29%20on%20natural%20scenarios%2C%20AGIs%20quality%0Aassessment%20%28AGIQA%29%20takes%20the%20correspondence%20of%20image%20and%20its%20textual%20prompt%0Ainto%20consideration.%20This%20is%20coupled%20in%20the%20ground%20truth%20score%2C%20which%20confuses%0Athe%20unimodal%20IQA%20methods.%20To%20solve%20this%20problem%2C%20we%20introduce%20IP-IQA%20%28AGIs%0AQuality%20Assessment%20via%20Image%20and%20Prompt%29%2C%20a%20multimodal%20framework%20for%20AGIQA%20via%0Acorresponding%20image%20and%20prompt%20incorporation.%20Specifically%2C%20we%20propose%20a%20novel%0Aincremental%20pretraining%20task%20named%20Image2Prompt%20for%20better%20understanding%20of%0AAGIs%20and%20their%20corresponding%20textual%20prompts.%20An%20effective%20and%20efficient%0Aimage-prompt%20fusion%20module%2C%20along%20with%20a%20novel%20special%20%5BQA%5D%20token%2C%20are%20also%0Aapplied.%20Both%20are%20plug-and-play%20and%20beneficial%20for%20the%20cooperation%20of%20image%20and%0Aits%20corresponding%20prompt.%20Experiments%20demonstrate%20that%20our%20IP-IQA%20achieves%20the%0Astate-of-the-art%20on%20AGIQA-1k%20and%20AGIQA-3k%20datasets.%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18714v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20Textual%20Prompt%20to%20AI-Generated%20Image%20Quality%20Assessment&entry.906535625=Bowen%20Qu%20and%20Haohui%20Li%20and%20Wei%20Gao&entry.1292438233=%20%20AI-Generated%20Images%20%28AGIs%29%20have%20inherent%20multimodal%20nature.%20Unlike%0Atraditional%20image%20quality%20assessment%20%28IQA%29%20on%20natural%20scenarios%2C%20AGIs%20quality%0Aassessment%20%28AGIQA%29%20takes%20the%20correspondence%20of%20image%20and%20its%20textual%20prompt%0Ainto%20consideration.%20This%20is%20coupled%20in%20the%20ground%20truth%20score%2C%20which%20confuses%0Athe%20unimodal%20IQA%20methods.%20To%20solve%20this%20problem%2C%20we%20introduce%20IP-IQA%20%28AGIs%0AQuality%20Assessment%20via%20Image%20and%20Prompt%29%2C%20a%20multimodal%20framework%20for%20AGIQA%20via%0Acorresponding%20image%20and%20prompt%20incorporation.%20Specifically%2C%20we%20propose%20a%20novel%0Aincremental%20pretraining%20task%20named%20Image2Prompt%20for%20better%20understanding%20of%0AAGIs%20and%20their%20corresponding%20textual%20prompts.%20An%20effective%20and%20efficient%0Aimage-prompt%20fusion%20module%2C%20along%20with%20a%20novel%20special%20%5BQA%5D%20token%2C%20are%20also%0Aapplied.%20Both%20are%20plug-and-play%20and%20beneficial%20for%20the%20cooperation%20of%20image%20and%0Aits%20corresponding%20prompt.%20Experiments%20demonstrate%20that%20our%20IP-IQA%20achieves%20the%0Astate-of-the-art%20on%20AGIQA-1k%20and%20AGIQA-3k%20datasets.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18714v1&entry.124074799=Read"},
{"title": "Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models", "author": "Jeremy E. Cohen and Valentin Leplat", "abstract": "  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n", "link": "http://arxiv.org/abs/2403.18517v1", "date": "2024-03-27", "relevancy": 1.9429, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.488}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4842}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Algorithms%20for%20Regularized%20Nonnegative%20Scale-invariant%0A%20%20Low-rank%20Approximation%20Models&body=Title%3A%20Efficient%20Algorithms%20for%20Regularized%20Nonnegative%20Scale-invariant%0A%20%20Low-rank%20Approximation%20Models%0AAuthor%3A%20Jeremy%20E.%20Cohen%20and%20Valentin%20Leplat%0AAbstract%3A%20%20%20Regularized%20nonnegative%20low-rank%20approximations%20such%20as%20sparse%20Nonnegative%0AMatrix%20Factorization%20or%20sparse%20Nonnegative%20Tucker%20Decomposition%20are%20an%0Aimportant%20branch%20of%20dimensionality%20reduction%20models%20with%20enhanced%0Ainterpretability.%20However%2C%20from%20a%20practical%20perspective%2C%20the%20choice%20of%0Aregularizers%20and%20regularization%20coefficients%2C%20as%20well%20as%20the%20design%20of%0Aefficient%20algorithms%2C%20is%20challenging%20because%20of%20the%20multifactor%20nature%20of%20these%0Amodels%20and%20the%20lack%20of%20theory%20to%20back%20these%20choices.%20This%20paper%20aims%20at%0Aimproving%20upon%20these%20issues.%20By%20studying%20a%20more%20general%20model%20called%20the%0AHomogeneous%20Regularized%20Scale-Invariant%2C%20we%20prove%20that%20the%20scale-invariance%0Ainherent%20to%20low-rank%20approximation%20models%20causes%20an%20implicit%20regularization%0Awith%20both%20unexpected%20beneficial%20and%20detrimental%20effects.%20This%20observation%0Aallows%20to%20better%20understand%20the%20effect%20of%20regularization%20functions%20in%20low-rank%0Aapproximation%20models%2C%20to%20guide%20the%20choice%20of%20the%20regularization%0Ahyperparameters%2C%20and%20to%20design%20balancing%20strategies%20to%20enhance%20the%20convergence%0Aspeed%20of%20dedicated%20optimization%20algorithms.%20Some%20of%20these%20results%20were%20already%0Aknown%20but%20restricted%20to%20specific%20instances%20of%20regularized%20low-rank%0Aapproximations.%20We%20also%20derive%20a%20generic%20Majorization%20Minimization%20algorithm%0Athat%20handles%20many%20regularized%20nonnegative%20low-rank%20approximations%2C%20with%0Aconvergence%20guarantees.%20We%20showcase%20our%20contributions%20on%20sparse%20Nonnegative%0AMatrix%20Factorization%2C%20ridge-regularized%20Canonical%20Polyadic%20decomposition%20and%0Asparse%20Nonnegative%20Tucker%20Decomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18517v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Algorithms%20for%20Regularized%20Nonnegative%20Scale-invariant%0A%20%20Low-rank%20Approximation%20Models&entry.906535625=Jeremy%20E.%20Cohen%20and%20Valentin%20Leplat&entry.1292438233=%20%20Regularized%20nonnegative%20low-rank%20approximations%20such%20as%20sparse%20Nonnegative%0AMatrix%20Factorization%20or%20sparse%20Nonnegative%20Tucker%20Decomposition%20are%20an%0Aimportant%20branch%20of%20dimensionality%20reduction%20models%20with%20enhanced%0Ainterpretability.%20However%2C%20from%20a%20practical%20perspective%2C%20the%20choice%20of%0Aregularizers%20and%20regularization%20coefficients%2C%20as%20well%20as%20the%20design%20of%0Aefficient%20algorithms%2C%20is%20challenging%20because%20of%20the%20multifactor%20nature%20of%20these%0Amodels%20and%20the%20lack%20of%20theory%20to%20back%20these%20choices.%20This%20paper%20aims%20at%0Aimproving%20upon%20these%20issues.%20By%20studying%20a%20more%20general%20model%20called%20the%0AHomogeneous%20Regularized%20Scale-Invariant%2C%20we%20prove%20that%20the%20scale-invariance%0Ainherent%20to%20low-rank%20approximation%20models%20causes%20an%20implicit%20regularization%0Awith%20both%20unexpected%20beneficial%20and%20detrimental%20effects.%20This%20observation%0Aallows%20to%20better%20understand%20the%20effect%20of%20regularization%20functions%20in%20low-rank%0Aapproximation%20models%2C%20to%20guide%20the%20choice%20of%20the%20regularization%0Ahyperparameters%2C%20and%20to%20design%20balancing%20strategies%20to%20enhance%20the%20convergence%0Aspeed%20of%20dedicated%20optimization%20algorithms.%20Some%20of%20these%20results%20were%20already%0Aknown%20but%20restricted%20to%20specific%20instances%20of%20regularized%20low-rank%0Aapproximations.%20We%20also%20derive%20a%20generic%20Majorization%20Minimization%20algorithm%0Athat%20handles%20many%20regularized%20nonnegative%20low-rank%20approximations%2C%20with%0Aconvergence%20guarantees.%20We%20showcase%20our%20contributions%20on%20sparse%20Nonnegative%0AMatrix%20Factorization%2C%20ridge-regularized%20Canonical%20Polyadic%20decomposition%20and%0Asparse%20Nonnegative%20Tucker%20Decomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18517v1&entry.124074799=Read"},
{"title": "Faster Convergence for Transformer Fine-tuning with Line Search Methods", "author": "Philip Kenneweg and Leonardo Galli and Tristan Kenneweg and Barbara Hammer", "abstract": "  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n", "link": "http://arxiv.org/abs/2403.18506v1", "date": "2024-03-27", "relevancy": 1.9426, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4736}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faster%20Convergence%20for%20Transformer%20Fine-tuning%20with%20Line%20Search%20Methods&body=Title%3A%20Faster%20Convergence%20for%20Transformer%20Fine-tuning%20with%20Line%20Search%20Methods%0AAuthor%3A%20Philip%20Kenneweg%20and%20Leonardo%20Galli%20and%20Tristan%20Kenneweg%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20that%20line%20search%20methods%20greatly%20increase%20performance%0Aof%20traditional%20stochastic%20gradient%20descent%20methods%20on%20a%20variety%20of%20datasets%20and%0Aarchitectures%20%5B1%5D%2C%20%5B2%5D.%20In%20this%20work%20we%20succeed%20in%20extending%20line%20search%0Amethods%20to%20the%20novel%20and%20highly%20popular%20Transformer%20architecture%20and%20dataset%0Adomains%20in%20natural%20language%20processing.%20More%20specifically%2C%20we%20combine%20the%0AArmijo%20line%20search%20with%20the%20Adam%20optimizer%20and%20extend%20it%20by%20subdividing%20the%0Anetworks%20architecture%20into%20sensible%20units%20and%20perform%20the%20line%20search%0Aseparately%20on%20these%20local%20units.%20Our%20optimization%20method%20outperforms%20the%0Atraditional%20Adam%20optimizer%20and%20achieves%20significant%20performance%20improvements%0Afor%20small%20data%20sets%20or%20small%20training%20budgets%2C%20while%20performing%20equal%20or%20better%0Afor%20other%20tested%20cases.%20Our%20work%20is%20publicly%20available%20as%20a%20python%20package%2C%0Awhich%20provides%20a%20hyperparameter-free%20pytorch%20optimizer%20that%20is%20compatible%20with%0Aarbitrary%20network%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18506v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Convergence%20for%20Transformer%20Fine-tuning%20with%20Line%20Search%20Methods&entry.906535625=Philip%20Kenneweg%20and%20Leonardo%20Galli%20and%20Tristan%20Kenneweg%20and%20Barbara%20Hammer&entry.1292438233=%20%20Recent%20works%20have%20shown%20that%20line%20search%20methods%20greatly%20increase%20performance%0Aof%20traditional%20stochastic%20gradient%20descent%20methods%20on%20a%20variety%20of%20datasets%20and%0Aarchitectures%20%5B1%5D%2C%20%5B2%5D.%20In%20this%20work%20we%20succeed%20in%20extending%20line%20search%0Amethods%20to%20the%20novel%20and%20highly%20popular%20Transformer%20architecture%20and%20dataset%0Adomains%20in%20natural%20language%20processing.%20More%20specifically%2C%20we%20combine%20the%0AArmijo%20line%20search%20with%20the%20Adam%20optimizer%20and%20extend%20it%20by%20subdividing%20the%0Anetworks%20architecture%20into%20sensible%20units%20and%20perform%20the%20line%20search%0Aseparately%20on%20these%20local%20units.%20Our%20optimization%20method%20outperforms%20the%0Atraditional%20Adam%20optimizer%20and%20achieves%20significant%20performance%20improvements%0Afor%20small%20data%20sets%20or%20small%20training%20budgets%2C%20while%20performing%20equal%20or%20better%0Afor%20other%20tested%20cases.%20Our%20work%20is%20publicly%20available%20as%20a%20python%20package%2C%0Awhich%20provides%20a%20hyperparameter-free%20pytorch%20optimizer%20that%20is%20compatible%20with%0Aarbitrary%20network%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18506v1&entry.124074799=Read"},
{"title": "Nonlinear Control Allocation: A Learning Based Approach", "author": "Hafiz Zeeshan Iqbal Khan and Surrayya Mobeen and Jahanzeb Rajput and Jamshed Riaz", "abstract": "  Modern aircraft are designed with redundant control effectors to cater for\nfault tolerance and maneuverability requirements. This leads to aircraft being\nover-actuated and requires control allocation schemes to distribute the control\ncommands among control effectors. Traditionally, optimization-based control\nallocation schemes are used; however, for nonlinear allocation problems, these\nmethods require large computational resources. In this work, an artificial\nneural network (ANN) based nonlinear control allocation scheme is proposed. The\nproposed scheme is composed of learning the inverse of the control\neffectiveness map through ANN, and then implementing it as an allocator instead\nof solving an online optimization problem. Stability conditions are presented\nfor closed-loop systems incorporating the allocator, and computational\nchallenges are explored with piece-wise linear effectiveness functions and\nANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is\ncompared with a standard quadratic programming-based method for control\nallocation.\n", "link": "http://arxiv.org/abs/2201.06180v2", "date": "2024-03-27", "relevancy": 1.9332, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5303}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4593}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20Control%20Allocation%3A%20A%20Learning%20Based%20Approach&body=Title%3A%20Nonlinear%20Control%20Allocation%3A%20A%20Learning%20Based%20Approach%0AAuthor%3A%20Hafiz%20Zeeshan%20Iqbal%20Khan%20and%20Surrayya%20Mobeen%20and%20Jahanzeb%20Rajput%20and%20Jamshed%20Riaz%0AAbstract%3A%20%20%20Modern%20aircraft%20are%20designed%20with%20redundant%20control%20effectors%20to%20cater%20for%0Afault%20tolerance%20and%20maneuverability%20requirements.%20This%20leads%20to%20aircraft%20being%0Aover-actuated%20and%20requires%20control%20allocation%20schemes%20to%20distribute%20the%20control%0Acommands%20among%20control%20effectors.%20Traditionally%2C%20optimization-based%20control%0Aallocation%20schemes%20are%20used%3B%20however%2C%20for%20nonlinear%20allocation%20problems%2C%20these%0Amethods%20require%20large%20computational%20resources.%20In%20this%20work%2C%20an%20artificial%0Aneural%20network%20%28ANN%29%20based%20nonlinear%20control%20allocation%20scheme%20is%20proposed.%20The%0Aproposed%20scheme%20is%20composed%20of%20learning%20the%20inverse%20of%20the%20control%0Aeffectiveness%20map%20through%20ANN%2C%20and%20then%20implementing%20it%20as%20an%20allocator%20instead%0Aof%20solving%20an%20online%20optimization%20problem.%20Stability%20conditions%20are%20presented%0Afor%20closed-loop%20systems%20incorporating%20the%20allocator%2C%20and%20computational%0Achallenges%20are%20explored%20with%20piece-wise%20linear%20effectiveness%20functions%20and%0AANN-based%20allocators.%20To%20demonstrate%20the%20efficacy%20of%20the%20proposed%20scheme%2C%20it%20is%0Acompared%20with%20a%20standard%20quadratic%20programming-based%20method%20for%20control%0Aallocation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.06180v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20Control%20Allocation%3A%20A%20Learning%20Based%20Approach&entry.906535625=Hafiz%20Zeeshan%20Iqbal%20Khan%20and%20Surrayya%20Mobeen%20and%20Jahanzeb%20Rajput%20and%20Jamshed%20Riaz&entry.1292438233=%20%20Modern%20aircraft%20are%20designed%20with%20redundant%20control%20effectors%20to%20cater%20for%0Afault%20tolerance%20and%20maneuverability%20requirements.%20This%20leads%20to%20aircraft%20being%0Aover-actuated%20and%20requires%20control%20allocation%20schemes%20to%20distribute%20the%20control%0Acommands%20among%20control%20effectors.%20Traditionally%2C%20optimization-based%20control%0Aallocation%20schemes%20are%20used%3B%20however%2C%20for%20nonlinear%20allocation%20problems%2C%20these%0Amethods%20require%20large%20computational%20resources.%20In%20this%20work%2C%20an%20artificial%0Aneural%20network%20%28ANN%29%20based%20nonlinear%20control%20allocation%20scheme%20is%20proposed.%20The%0Aproposed%20scheme%20is%20composed%20of%20learning%20the%20inverse%20of%20the%20control%0Aeffectiveness%20map%20through%20ANN%2C%20and%20then%20implementing%20it%20as%20an%20allocator%20instead%0Aof%20solving%20an%20online%20optimization%20problem.%20Stability%20conditions%20are%20presented%0Afor%20closed-loop%20systems%20incorporating%20the%20allocator%2C%20and%20computational%0Achallenges%20are%20explored%20with%20piece-wise%20linear%20effectiveness%20functions%20and%0AANN-based%20allocators.%20To%20demonstrate%20the%20efficacy%20of%20the%20proposed%20scheme%2C%20it%20is%0Acompared%20with%20a%20standard%20quadratic%20programming-based%20method%20for%20control%0Aallocation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.06180v2&entry.124074799=Read"},
{"title": "SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface", "author": "Jiahao Luo and Jing Liu and James Davis", "abstract": "  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n", "link": "http://arxiv.org/abs/2403.18784v1", "date": "2024-03-27", "relevancy": 1.929, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4841}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.48}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SplatFace%3A%20Gaussian%20Splat%20Face%20Reconstruction%20Leveraging%20an%20Optimizable%0A%20%20Surface&body=Title%3A%20SplatFace%3A%20Gaussian%20Splat%20Face%20Reconstruction%20Leveraging%20an%20Optimizable%0A%20%20Surface%0AAuthor%3A%20Jiahao%20Luo%20and%20Jing%20Liu%20and%20James%20Davis%0AAbstract%3A%20%20%20We%20present%20SplatFace%2C%20a%20novel%20Gaussian%20splatting%20framework%20designed%20for%203D%0Ahuman%20face%20reconstruction%20without%20reliance%20on%20accurate%20pre-determined%20geometry.%0AOur%20method%20is%20designed%20to%20simultaneously%20deliver%20both%20high-quality%20novel%20view%0Arendering%20and%20accurate%203D%20mesh%20reconstructions.%20We%20incorporate%20a%20generic%203D%0AMorphable%20Model%20%283DMM%29%20to%20provide%20a%20surface%20geometric%20structure%2C%20making%20it%0Apossible%20to%20reconstruct%20faces%20with%20a%20limited%20set%20of%20input%20images.%20We%20introduce%0Aa%20joint%20optimization%20strategy%20that%20refines%20both%20the%20Gaussians%20and%20the%20morphable%0Asurface%20through%20a%20synergistic%20non-rigid%20alignment%20process.%20A%20novel%20distance%0Ametric%2C%20splat-to-surface%2C%20is%20proposed%20to%20improve%20alignment%20by%20considering%20both%0Athe%20Gaussian%20position%20and%20covariance.%20The%20surface%20information%20is%20also%20utilized%0Ato%20incorporate%20a%20world-space%20densification%20process%2C%20resulting%20in%20superior%0Areconstruction%20quality.%20Our%20experimental%20analysis%20demonstrates%20that%20the%0Aproposed%20method%20is%20competitive%20with%20both%20other%20Gaussian%20splatting%20techniques%20in%0Anovel%20view%20synthesis%20and%20other%203D%20reconstruction%20methods%20in%20producing%203D%20face%0Ameshes%20with%20high%20geometric%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18784v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatFace%3A%20Gaussian%20Splat%20Face%20Reconstruction%20Leveraging%20an%20Optimizable%0A%20%20Surface&entry.906535625=Jiahao%20Luo%20and%20Jing%20Liu%20and%20James%20Davis&entry.1292438233=%20%20We%20present%20SplatFace%2C%20a%20novel%20Gaussian%20splatting%20framework%20designed%20for%203D%0Ahuman%20face%20reconstruction%20without%20reliance%20on%20accurate%20pre-determined%20geometry.%0AOur%20method%20is%20designed%20to%20simultaneously%20deliver%20both%20high-quality%20novel%20view%0Arendering%20and%20accurate%203D%20mesh%20reconstructions.%20We%20incorporate%20a%20generic%203D%0AMorphable%20Model%20%283DMM%29%20to%20provide%20a%20surface%20geometric%20structure%2C%20making%20it%0Apossible%20to%20reconstruct%20faces%20with%20a%20limited%20set%20of%20input%20images.%20We%20introduce%0Aa%20joint%20optimization%20strategy%20that%20refines%20both%20the%20Gaussians%20and%20the%20morphable%0Asurface%20through%20a%20synergistic%20non-rigid%20alignment%20process.%20A%20novel%20distance%0Ametric%2C%20splat-to-surface%2C%20is%20proposed%20to%20improve%20alignment%20by%20considering%20both%0Athe%20Gaussian%20position%20and%20covariance.%20The%20surface%20information%20is%20also%20utilized%0Ato%20incorporate%20a%20world-space%20densification%20process%2C%20resulting%20in%20superior%0Areconstruction%20quality.%20Our%20experimental%20analysis%20demonstrates%20that%20the%0Aproposed%20method%20is%20competitive%20with%20both%20other%20Gaussian%20splatting%20techniques%20in%0Anovel%20view%20synthesis%20and%20other%203D%20reconstruction%20methods%20in%20producing%203D%20face%0Ameshes%20with%20high%20geometric%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18784v1&entry.124074799=Read"},
{"title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models", "author": "Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You", "abstract": "  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n", "link": "http://arxiv.org/abs/2402.01739v2", "date": "2024-03-27", "relevancy": 1.9035, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4827}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4608}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenMoE%3A%20An%20Early%20Effort%20on%20Open%20Mixture-of-Experts%20Language%20Models&body=Title%3A%20OpenMoE%3A%20An%20Early%20Effort%20on%20Open%20Mixture-of-Experts%20Language%20Models%0AAuthor%3A%20Fuzhao%20Xue%20and%20Zian%20Zheng%20and%20Yao%20Fu%20and%20Jinjie%20Ni%20and%20Zangwei%20Zheng%20and%20Wangchunshu%20Zhou%20and%20Yang%20You%0AAbstract%3A%20%20%20To%20help%20the%20open-source%20community%20have%20a%20better%20understanding%20of%0AMixture-of-Experts%20%28MoE%29%20based%20large%20language%20models%20%28LLMs%29%2C%20we%20train%20and%0Arelease%20OpenMoE%2C%20a%20series%20of%20fully%20open-sourced%20and%20reproducible%20decoder-only%0AMoE%20LLMs%2C%20ranging%20from%20650M%20to%2034B%20parameters%20and%20trained%20on%20up%20to%20over%201T%0Atokens.%20Our%20investigation%20confirms%20that%20MoE-based%20LLMs%20can%20offer%20a%20more%0Afavorable%20cost-effectiveness%20trade-off%20than%20dense%20LLMs%2C%20highlighting%20the%0Apotential%20effectiveness%20for%20future%20LLM%20development.%0A%20%20One%20more%20important%20contribution%20of%20this%20study%20is%20an%20in-depth%20analysis%20of%20the%0Arouting%20mechanisms%20within%20our%20OpenMoE%20models%2C%20leading%20to%20three%20significant%0Afindings%3A%20Context-Independent%20Specialization%2C%20Early%20Routing%20Learning%2C%20and%0ADrop-towards-the-End.%20We%20discovered%20that%20routing%20decisions%20in%20MoE%20models%20are%0Apredominantly%20based%20on%20token%20IDs%2C%20with%20minimal%20context%20relevance.%20The%0Atoken-to-expert%20assignments%20are%20determined%20early%20in%20the%20pre-training%20phase%20and%0Aremain%20largely%20unchanged.%20This%20imperfect%20routing%20can%20result%20in%20performance%0Adegradation%2C%20particularly%20in%20sequential%20tasks%20like%20multi-turn%20conversations%2C%0Awhere%20tokens%20appearing%20later%20in%20a%20sequence%20are%20more%20likely%20to%20be%20dropped.%0AFinally%2C%20we%20rethink%20our%20design%20based%20on%20the%20above-mentioned%20observations%20and%0Aanalysis.%20To%20facilitate%20future%20MoE%20LLM%20development%2C%20we%20propose%20potential%0Astrategies%20for%20mitigating%20the%20issues%20we%20found%20and%20further%20improving%0Aoff-the-shelf%20MoE%20LLM%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01739v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenMoE%3A%20An%20Early%20Effort%20on%20Open%20Mixture-of-Experts%20Language%20Models&entry.906535625=Fuzhao%20Xue%20and%20Zian%20Zheng%20and%20Yao%20Fu%20and%20Jinjie%20Ni%20and%20Zangwei%20Zheng%20and%20Wangchunshu%20Zhou%20and%20Yang%20You&entry.1292438233=%20%20To%20help%20the%20open-source%20community%20have%20a%20better%20understanding%20of%0AMixture-of-Experts%20%28MoE%29%20based%20large%20language%20models%20%28LLMs%29%2C%20we%20train%20and%0Arelease%20OpenMoE%2C%20a%20series%20of%20fully%20open-sourced%20and%20reproducible%20decoder-only%0AMoE%20LLMs%2C%20ranging%20from%20650M%20to%2034B%20parameters%20and%20trained%20on%20up%20to%20over%201T%0Atokens.%20Our%20investigation%20confirms%20that%20MoE-based%20LLMs%20can%20offer%20a%20more%0Afavorable%20cost-effectiveness%20trade-off%20than%20dense%20LLMs%2C%20highlighting%20the%0Apotential%20effectiveness%20for%20future%20LLM%20development.%0A%20%20One%20more%20important%20contribution%20of%20this%20study%20is%20an%20in-depth%20analysis%20of%20the%0Arouting%20mechanisms%20within%20our%20OpenMoE%20models%2C%20leading%20to%20three%20significant%0Afindings%3A%20Context-Independent%20Specialization%2C%20Early%20Routing%20Learning%2C%20and%0ADrop-towards-the-End.%20We%20discovered%20that%20routing%20decisions%20in%20MoE%20models%20are%0Apredominantly%20based%20on%20token%20IDs%2C%20with%20minimal%20context%20relevance.%20The%0Atoken-to-expert%20assignments%20are%20determined%20early%20in%20the%20pre-training%20phase%20and%0Aremain%20largely%20unchanged.%20This%20imperfect%20routing%20can%20result%20in%20performance%0Adegradation%2C%20particularly%20in%20sequential%20tasks%20like%20multi-turn%20conversations%2C%0Awhere%20tokens%20appearing%20later%20in%20a%20sequence%20are%20more%20likely%20to%20be%20dropped.%0AFinally%2C%20we%20rethink%20our%20design%20based%20on%20the%20above-mentioned%20observations%20and%0Aanalysis.%20To%20facilitate%20future%20MoE%20LLM%20development%2C%20we%20propose%20potential%0Astrategies%20for%20mitigating%20the%20issues%20we%20found%20and%20further%20improving%0Aoff-the-shelf%20MoE%20LLM%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01739v2&entry.124074799=Read"},
{"title": "Improving Line Search Methods for Large Scale Neural Network Training", "author": "Philip Kenneweg and Tristan Kenneweg and Barbara Hammer", "abstract": "  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n", "link": "http://arxiv.org/abs/2403.18519v1", "date": "2024-03-27", "relevancy": 1.903, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4533}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Line%20Search%20Methods%20for%20Large%20Scale%20Neural%20Network%20Training&body=Title%3A%20Improving%20Line%20Search%20Methods%20for%20Large%20Scale%20Neural%20Network%20Training%0AAuthor%3A%20Philip%20Kenneweg%20and%20Tristan%20Kenneweg%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20In%20recent%20studies%2C%20line%20search%20methods%20have%20shown%20significant%20improvements%20in%0Athe%20performance%20of%20traditional%20stochastic%20gradient%20descent%20techniques%2C%0Aeliminating%20the%20need%20for%20a%20specific%20learning%20rate%20schedule.%20In%20this%20paper%2C%20we%0Aidentify%20existing%20issues%20in%20state-of-the-art%20line%20search%20methods%2C%20propose%0Aenhancements%2C%20and%20rigorously%20evaluate%20their%20effectiveness.%20We%20test%20these%0Amethods%20on%20larger%20datasets%20and%20more%20complex%20data%20domains%20than%20before.%0ASpecifically%2C%20we%20improve%20the%20Armijo%20line%20search%20by%20integrating%20the%20momentum%0Aterm%20from%20ADAM%20in%20its%20search%20direction%2C%20enabling%20efficient%20large-scale%0Atraining%2C%20a%20task%20that%20was%20previously%20prone%20to%20failure%20using%20Armijo%20line%20search%0Amethods.%20Our%20optimization%20approach%20outperforms%20both%20the%20previous%20Armijo%0Aimplementation%20and%20tuned%20learning%20rate%20schedules%20for%20Adam.%20Our%20evaluation%0Afocuses%20on%20Transformers%20and%20CNNs%20in%20the%20domains%20of%20NLP%20and%20image%20data.%20Our%20work%0Ais%20publicly%20available%20as%20a%20Python%20package%2C%20which%20provides%20a%20hyperparameter%20free%0APytorch%20optimizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18519v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Line%20Search%20Methods%20for%20Large%20Scale%20Neural%20Network%20Training&entry.906535625=Philip%20Kenneweg%20and%20Tristan%20Kenneweg%20and%20Barbara%20Hammer&entry.1292438233=%20%20In%20recent%20studies%2C%20line%20search%20methods%20have%20shown%20significant%20improvements%20in%0Athe%20performance%20of%20traditional%20stochastic%20gradient%20descent%20techniques%2C%0Aeliminating%20the%20need%20for%20a%20specific%20learning%20rate%20schedule.%20In%20this%20paper%2C%20we%0Aidentify%20existing%20issues%20in%20state-of-the-art%20line%20search%20methods%2C%20propose%0Aenhancements%2C%20and%20rigorously%20evaluate%20their%20effectiveness.%20We%20test%20these%0Amethods%20on%20larger%20datasets%20and%20more%20complex%20data%20domains%20than%20before.%0ASpecifically%2C%20we%20improve%20the%20Armijo%20line%20search%20by%20integrating%20the%20momentum%0Aterm%20from%20ADAM%20in%20its%20search%20direction%2C%20enabling%20efficient%20large-scale%0Atraining%2C%20a%20task%20that%20was%20previously%20prone%20to%20failure%20using%20Armijo%20line%20search%0Amethods.%20Our%20optimization%20approach%20outperforms%20both%20the%20previous%20Armijo%0Aimplementation%20and%20tuned%20learning%20rate%20schedules%20for%20Adam.%20Our%20evaluation%0Afocuses%20on%20Transformers%20and%20CNNs%20in%20the%20domains%20of%20NLP%20and%20image%20data.%20Our%20work%0Ais%20publicly%20available%20as%20a%20Python%20package%2C%20which%20provides%20a%20hyperparameter%20free%0APytorch%20optimizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18519v1&entry.124074799=Read"},
{"title": "FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization", "author": "Nicolas Mauricio Cuadrado and Roberto Alejandro Gutierrez and Martin Tak\u00e1\u010d", "abstract": "  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n", "link": "http://arxiv.org/abs/2403.18444v1", "date": "2024-03-27", "relevancy": 1.8918, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5004}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4914}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4382}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FRESCO%3A%20Federated%20Reinforcement%20Energy%20System%20for%20Cooperative%0A%20%20Optimization&body=Title%3A%20FRESCO%3A%20Federated%20Reinforcement%20Energy%20System%20for%20Cooperative%0A%20%20Optimization%0AAuthor%3A%20Nicolas%20Mauricio%20Cuadrado%20and%20Roberto%20Alejandro%20Gutierrez%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20%20%20The%20rise%20in%20renewable%20energy%20is%20creating%20new%20dynamics%20in%20the%20energy%20grid%20that%0Apromise%20to%20create%20a%20cleaner%20and%20more%20participative%20energy%20grid%2C%20where%0Atechnology%20plays%20a%20crucial%20part%20in%20making%20the%20required%20flexibility%20to%20achieve%0Athe%20vision%20of%20the%20next-generation%20grid.%20This%20work%20presents%20FRESCO%2C%20a%20framework%0Athat%20aims%20to%20ease%20the%20implementation%20of%20energy%20markets%20using%20a%20hierarchical%0Acontrol%20architecture%20of%20reinforcement%20learning%20agents%20trained%20using%20federated%0Alearning.%20The%20core%20concept%20we%20are%20proving%20is%20that%20having%20greedy%20agents%20subject%0Ato%20changing%20conditions%20from%20a%20higher%20level%20agent%20creates%20a%20cooperative%20setup%0Athat%20will%20allow%20for%20fulfilling%20all%20the%20individual%20objectives.%20This%20paper%0Apresents%20a%20general%20overview%20of%20the%20framework%2C%20the%20current%20progress%2C%20and%20some%0Ainsights%20we%20obtained%20from%20the%20recent%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18444v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRESCO%3A%20Federated%20Reinforcement%20Energy%20System%20for%20Cooperative%0A%20%20Optimization&entry.906535625=Nicolas%20Mauricio%20Cuadrado%20and%20Roberto%20Alejandro%20Gutierrez%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=%20%20The%20rise%20in%20renewable%20energy%20is%20creating%20new%20dynamics%20in%20the%20energy%20grid%20that%0Apromise%20to%20create%20a%20cleaner%20and%20more%20participative%20energy%20grid%2C%20where%0Atechnology%20plays%20a%20crucial%20part%20in%20making%20the%20required%20flexibility%20to%20achieve%0Athe%20vision%20of%20the%20next-generation%20grid.%20This%20work%20presents%20FRESCO%2C%20a%20framework%0Athat%20aims%20to%20ease%20the%20implementation%20of%20energy%20markets%20using%20a%20hierarchical%0Acontrol%20architecture%20of%20reinforcement%20learning%20agents%20trained%20using%20federated%0Alearning.%20The%20core%20concept%20we%20are%20proving%20is%20that%20having%20greedy%20agents%20subject%0Ato%20changing%20conditions%20from%20a%20higher%20level%20agent%20creates%20a%20cooperative%20setup%0Athat%20will%20allow%20for%20fulfilling%20all%20the%20individual%20objectives.%20This%20paper%0Apresents%20a%20general%20overview%20of%20the%20framework%2C%20the%20current%20progress%2C%20and%20some%0Ainsights%20we%20obtained%20from%20the%20recent%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18444v1&entry.124074799=Read"},
{"title": "Attention-aware semantic relevance predicting Chinese sentence reading", "author": "Kun Sun", "abstract": "  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n", "link": "http://arxiv.org/abs/2403.18542v1", "date": "2024-03-27", "relevancy": 1.8881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4634}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4588}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention-aware%20semantic%20relevance%20predicting%20Chinese%20sentence%20reading&body=Title%3A%20Attention-aware%20semantic%20relevance%20predicting%20Chinese%20sentence%20reading%0AAuthor%3A%20Kun%20Sun%0AAbstract%3A%20%20%20In%20recent%20years%2C%20several%20influential%20computational%20models%20and%20metrics%20have%0Abeen%20proposed%20to%20predict%20how%20humans%20comprehend%20and%20process%20sentence.%20One%0Aparticularly%20promising%20approach%20is%20contextual%20semantic%20similarity.%20Inspired%20by%0Athe%20attention%20algorithm%20in%20Transformer%20and%20human%20memory%20mechanisms%2C%20this%20study%0Aproposes%20an%20%60%60attention-aware%27%27%20approach%20for%20computing%20contextual%20semantic%0Arelevance.%20This%20new%20approach%20takes%20into%20account%20the%20different%20contributions%20of%0Acontextual%20parts%20and%20the%20expectation%20effect%2C%20allowing%20it%20to%20incorporate%0Acontextual%20information%20fully.%20The%20attention-aware%20approach%20also%20facilitates%20the%0Asimulation%20of%20existing%20reading%20models%20and%20evaluate%20them.%20The%20resulting%0A%60%60attention-aware%27%27%20metrics%20of%20semantic%20relevance%20can%20more%20accurately%20predict%0Afixation%20durations%20in%20Chinese%20reading%20tasks%20recorded%20in%20an%20eye-tracking%20corpus%0Athan%20those%20calculated%20by%20existing%20approaches.%20The%20study%27s%20findings%20further%0Aprovide%20strong%20support%20for%20the%20presence%20of%20semantic%20preview%20benefits%20in%20Chinese%0Anaturalistic%20reading.%20Furthermore%2C%20the%20attention-aware%20metrics%20of%20semantic%0Arelevance%2C%20being%20memory-based%2C%20possess%20high%20interpretability%20from%20both%0Alinguistic%20and%20cognitive%20standpoints%2C%20making%20them%20a%20valuable%20computational%20tool%0Afor%20modeling%20eye-movements%20in%20reading%20and%20further%20gaining%20insight%20into%20the%0Aprocess%20of%20language%20comprehension.%20Our%20approach%20underscores%20the%20potential%20of%0Athese%20metrics%20to%20advance%20our%20comprehension%20of%20how%20humans%20understand%20and%20process%0Alanguage%2C%20ultimately%20leading%20to%20a%20better%20understanding%20of%20language%0Acomprehension%20and%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18542v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-aware%20semantic%20relevance%20predicting%20Chinese%20sentence%20reading&entry.906535625=Kun%20Sun&entry.1292438233=%20%20In%20recent%20years%2C%20several%20influential%20computational%20models%20and%20metrics%20have%0Abeen%20proposed%20to%20predict%20how%20humans%20comprehend%20and%20process%20sentence.%20One%0Aparticularly%20promising%20approach%20is%20contextual%20semantic%20similarity.%20Inspired%20by%0Athe%20attention%20algorithm%20in%20Transformer%20and%20human%20memory%20mechanisms%2C%20this%20study%0Aproposes%20an%20%60%60attention-aware%27%27%20approach%20for%20computing%20contextual%20semantic%0Arelevance.%20This%20new%20approach%20takes%20into%20account%20the%20different%20contributions%20of%0Acontextual%20parts%20and%20the%20expectation%20effect%2C%20allowing%20it%20to%20incorporate%0Acontextual%20information%20fully.%20The%20attention-aware%20approach%20also%20facilitates%20the%0Asimulation%20of%20existing%20reading%20models%20and%20evaluate%20them.%20The%20resulting%0A%60%60attention-aware%27%27%20metrics%20of%20semantic%20relevance%20can%20more%20accurately%20predict%0Afixation%20durations%20in%20Chinese%20reading%20tasks%20recorded%20in%20an%20eye-tracking%20corpus%0Athan%20those%20calculated%20by%20existing%20approaches.%20The%20study%27s%20findings%20further%0Aprovide%20strong%20support%20for%20the%20presence%20of%20semantic%20preview%20benefits%20in%20Chinese%0Anaturalistic%20reading.%20Furthermore%2C%20the%20attention-aware%20metrics%20of%20semantic%0Arelevance%2C%20being%20memory-based%2C%20possess%20high%20interpretability%20from%20both%0Alinguistic%20and%20cognitive%20standpoints%2C%20making%20them%20a%20valuable%20computational%20tool%0Afor%20modeling%20eye-movements%20in%20reading%20and%20further%20gaining%20insight%20into%20the%0Aprocess%20of%20language%20comprehension.%20Our%20approach%20underscores%20the%20potential%20of%0Athese%20metrics%20to%20advance%20our%20comprehension%20of%20how%20humans%20understand%20and%20process%0Alanguage%2C%20ultimately%20leading%20to%20a%20better%20understanding%20of%20language%0Acomprehension%20and%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18542v1&entry.124074799=Read"},
{"title": "Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation", "author": "Hannes Waclawek and Stefan Huber", "abstract": "  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n", "link": "http://arxiv.org/abs/2403.08579v2", "date": "2024-03-27", "relevancy": 1.8793, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4295}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation&body=Title%3A%20Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation%0AAuthor%3A%20Hannes%20Waclawek%20and%20Stefan%20Huber%0AAbstract%3A%20%20%20Piecewise%20Polynomials%20%28PPs%29%20are%20utilized%20in%20several%20engineering%20disciplines%2C%0Alike%20trajectory%20planning%2C%20to%20approximate%20position%20profiles%20given%20in%20the%20form%20of%0Aa%20set%20of%20points.%20While%20the%20approximation%20target%20along%20with%20domain-specific%0Arequirements%2C%20like%20Ck%20-continuity%2C%20can%20be%20formulated%20as%20a%20system%20of%20equations%0Aand%20a%20result%20can%20be%20computed%20directly%2C%20such%20closed-form%20solutions%20posses%0Alimited%20flexibility%20with%20respect%20to%20polynomial%20degrees%2C%20polynomial%20bases%20or%0Aadding%20further%20domain-specific%20requirements.%20Sufficiently%20complex%20optimization%0Agoals%20soon%20call%20for%20the%20use%20of%20numerical%20methods%2C%20like%20gradient%20descent.%20Since%0Agradient%20descent%20lies%20at%20the%20heart%20of%20training%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20modern%20Machine%20Learning%20%28ML%29%20frameworks%20like%20TensorFlow%20come%20with%20a%20set%0Aof%20gradient-based%20optimizers%20potentially%20suitable%20for%20a%20wide%20range%20of%0Aoptimization%20problems%20beyond%20the%20training%20task%20for%20ANNs.%20Our%20approach%20is%20to%0Autilize%20the%20versatility%20of%20PP%20models%20and%20combine%20it%20with%20the%20potential%20of%0Amodern%20ML%20optimizers%20for%20the%20use%20in%20function%20approximation%20in%201D%20trajectory%0Aplanning%20in%20the%20context%20of%20electronic%20cam%20design.%20We%20utilize%20available%0Aoptimizers%20of%20the%20ML%20framework%20TensorFlow%20directly%2C%20outside%20of%20the%20scope%20of%0AANNs%2C%20to%20optimize%20model%20parameters%20of%20our%20PP%20model.%20In%20this%20paper%2C%20we%20show%20how%0Aan%20orthogonal%20polynomial%20basis%20contributes%20to%20improving%20approximation%20and%0Acontinuity%20optimization%20performance.%20Utilizing%20Chebyshev%20polynomials%20of%20the%0Afirst%20kind%2C%20we%20develop%20a%20novel%20regularization%20approach%20enabling%20clearly%0Aimproved%20convergence%20behavior.%20We%20show%20that%2C%20using%20this%20regularization%0Aapproach%2C%20Chebyshev%20basis%20performs%20better%20than%20power%20basis%20for%20all%20relevant%0Aoptimizers%20in%20the%20combined%20approximation%20and%20continuity%20optimization%20setting%0Aand%20demonstrate%20usability%20of%20the%20presented%20approach%20within%20the%20electronic%20cam%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08579v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Optimized%20Orthogonal%20Basis%20Piecewise%20Polynomial%0A%20%20Approximation&entry.906535625=Hannes%20Waclawek%20and%20Stefan%20Huber&entry.1292438233=%20%20Piecewise%20Polynomials%20%28PPs%29%20are%20utilized%20in%20several%20engineering%20disciplines%2C%0Alike%20trajectory%20planning%2C%20to%20approximate%20position%20profiles%20given%20in%20the%20form%20of%0Aa%20set%20of%20points.%20While%20the%20approximation%20target%20along%20with%20domain-specific%0Arequirements%2C%20like%20Ck%20-continuity%2C%20can%20be%20formulated%20as%20a%20system%20of%20equations%0Aand%20a%20result%20can%20be%20computed%20directly%2C%20such%20closed-form%20solutions%20posses%0Alimited%20flexibility%20with%20respect%20to%20polynomial%20degrees%2C%20polynomial%20bases%20or%0Aadding%20further%20domain-specific%20requirements.%20Sufficiently%20complex%20optimization%0Agoals%20soon%20call%20for%20the%20use%20of%20numerical%20methods%2C%20like%20gradient%20descent.%20Since%0Agradient%20descent%20lies%20at%20the%20heart%20of%20training%20Artificial%20Neural%20Networks%0A%28ANNs%29%2C%20modern%20Machine%20Learning%20%28ML%29%20frameworks%20like%20TensorFlow%20come%20with%20a%20set%0Aof%20gradient-based%20optimizers%20potentially%20suitable%20for%20a%20wide%20range%20of%0Aoptimization%20problems%20beyond%20the%20training%20task%20for%20ANNs.%20Our%20approach%20is%20to%0Autilize%20the%20versatility%20of%20PP%20models%20and%20combine%20it%20with%20the%20potential%20of%0Amodern%20ML%20optimizers%20for%20the%20use%20in%20function%20approximation%20in%201D%20trajectory%0Aplanning%20in%20the%20context%20of%20electronic%20cam%20design.%20We%20utilize%20available%0Aoptimizers%20of%20the%20ML%20framework%20TensorFlow%20directly%2C%20outside%20of%20the%20scope%20of%0AANNs%2C%20to%20optimize%20model%20parameters%20of%20our%20PP%20model.%20In%20this%20paper%2C%20we%20show%20how%0Aan%20orthogonal%20polynomial%20basis%20contributes%20to%20improving%20approximation%20and%0Acontinuity%20optimization%20performance.%20Utilizing%20Chebyshev%20polynomials%20of%20the%0Afirst%20kind%2C%20we%20develop%20a%20novel%20regularization%20approach%20enabling%20clearly%0Aimproved%20convergence%20behavior.%20We%20show%20that%2C%20using%20this%20regularization%0Aapproach%2C%20Chebyshev%20basis%20performs%20better%20than%20power%20basis%20for%20all%20relevant%0Aoptimizers%20in%20the%20combined%20approximation%20and%20continuity%20optimization%20setting%0Aand%20demonstrate%20usability%20of%20the%20presented%20approach%20within%20the%20electronic%20cam%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08579v2&entry.124074799=Read"},
{"title": "Benchmarking Object Detectors with COCO: A New Path Forward", "author": "Shweta Singh and Aayan Yadav and Jitesh Jain and Humphrey Shi and Justin Johnson and Karan Desai", "abstract": "  The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz\n", "link": "http://arxiv.org/abs/2403.18819v1", "date": "2024-03-27", "relevancy": 1.8685, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4588}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Object%20Detectors%20with%20COCO%3A%20A%20New%20Path%20Forward&body=Title%3A%20Benchmarking%20Object%20Detectors%20with%20COCO%3A%20A%20New%20Path%20Forward%0AAuthor%3A%20Shweta%20Singh%20and%20Aayan%20Yadav%20and%20Jitesh%20Jain%20and%20Humphrey%20Shi%20and%20Justin%20Johnson%20and%20Karan%20Desai%0AAbstract%3A%20%20%20The%20Common%20Objects%20in%20Context%20%28COCO%29%20dataset%20has%20been%20instrumental%20in%0Abenchmarking%20object%20detectors%20over%20the%20past%20decade.%20Like%20every%20dataset%2C%20COCO%0Acontains%20subtle%20errors%20and%20imperfections%20stemming%20from%20its%20annotation%0Aprocedure.%20With%20the%20advent%20of%20high-performing%20models%2C%20we%20ask%20whether%20these%0Aerrors%20of%20COCO%20are%20hindering%20its%20utility%20in%20reliably%20benchmarking%20further%0Aprogress.%20In%20search%20for%20an%20answer%2C%20we%20inspect%20thousands%20of%20masks%20from%20COCO%0A%282017%20version%29%20and%20uncover%20different%20types%20of%20errors%20such%20as%20imprecise%20mask%0Aboundaries%2C%20non-exhaustively%20annotated%20instances%2C%20and%20mislabeled%20masks.%20Due%20to%0Athe%20prevalence%20of%20COCO%2C%20we%20choose%20to%20correct%20these%20errors%20to%20maintain%0Acontinuity%20with%20prior%20research.%20We%20develop%20COCO-ReM%20%28Refined%20Masks%29%2C%20a%20cleaner%0Aset%20of%20annotations%20with%20visibly%20better%20mask%20quality%20than%20COCO-2017.%20We%20evaluate%0Afifty%20object%20detectors%20and%20find%20that%20models%20that%20predict%20visually%20sharper%20masks%0Ascore%20higher%20on%20COCO-ReM%2C%20affirming%20that%20they%20were%20being%20incorrectly%20penalized%0Adue%20to%20errors%20in%20COCO-2017.%20Moreover%2C%20our%20models%20trained%20using%20COCO-ReM%0Aconverge%20faster%20and%20score%20higher%20than%20their%20larger%20variants%20trained%20using%0ACOCO-2017%2C%20highlighting%20the%20importance%20of%20data%20quality%20in%20improving%20object%0Adetectors.%20With%20these%20findings%2C%20we%20advocate%20using%20COCO-ReM%20for%20future%20object%0Adetection%20research.%20Our%20dataset%20is%20available%20at%20https%3A//cocorem.xyz%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18819v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Object%20Detectors%20with%20COCO%3A%20A%20New%20Path%20Forward&entry.906535625=Shweta%20Singh%20and%20Aayan%20Yadav%20and%20Jitesh%20Jain%20and%20Humphrey%20Shi%20and%20Justin%20Johnson%20and%20Karan%20Desai&entry.1292438233=%20%20The%20Common%20Objects%20in%20Context%20%28COCO%29%20dataset%20has%20been%20instrumental%20in%0Abenchmarking%20object%20detectors%20over%20the%20past%20decade.%20Like%20every%20dataset%2C%20COCO%0Acontains%20subtle%20errors%20and%20imperfections%20stemming%20from%20its%20annotation%0Aprocedure.%20With%20the%20advent%20of%20high-performing%20models%2C%20we%20ask%20whether%20these%0Aerrors%20of%20COCO%20are%20hindering%20its%20utility%20in%20reliably%20benchmarking%20further%0Aprogress.%20In%20search%20for%20an%20answer%2C%20we%20inspect%20thousands%20of%20masks%20from%20COCO%0A%282017%20version%29%20and%20uncover%20different%20types%20of%20errors%20such%20as%20imprecise%20mask%0Aboundaries%2C%20non-exhaustively%20annotated%20instances%2C%20and%20mislabeled%20masks.%20Due%20to%0Athe%20prevalence%20of%20COCO%2C%20we%20choose%20to%20correct%20these%20errors%20to%20maintain%0Acontinuity%20with%20prior%20research.%20We%20develop%20COCO-ReM%20%28Refined%20Masks%29%2C%20a%20cleaner%0Aset%20of%20annotations%20with%20visibly%20better%20mask%20quality%20than%20COCO-2017.%20We%20evaluate%0Afifty%20object%20detectors%20and%20find%20that%20models%20that%20predict%20visually%20sharper%20masks%0Ascore%20higher%20on%20COCO-ReM%2C%20affirming%20that%20they%20were%20being%20incorrectly%20penalized%0Adue%20to%20errors%20in%20COCO-2017.%20Moreover%2C%20our%20models%20trained%20using%20COCO-ReM%0Aconverge%20faster%20and%20score%20higher%20than%20their%20larger%20variants%20trained%20using%0ACOCO-2017%2C%20highlighting%20the%20importance%20of%20data%20quality%20in%20improving%20object%0Adetectors.%20With%20these%20findings%2C%20we%20advocate%20using%20COCO-ReM%20for%20future%20object%0Adetection%20research.%20Our%20dataset%20is%20available%20at%20https%3A//cocorem.xyz%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18819v1&entry.124074799=Read"},
{"title": "Sampling-Based Motion Planning with Online Racing Line Generation for\n  Autonomous Driving on Three-Dimensional Race Tracks", "author": "Levent \u00d6gretmen and Matthias Rowold and Boris Lohmann", "abstract": "  Existing approaches to trajectory planning for autonomous racing employ\nsampling-based methods, generating numerous jerk-optimal trajectories and\nselecting the most favorable feasible trajectory based on a cost function\npenalizing deviations from an offline-calculated racing line. While successful\non oval tracks, these methods face limitations on complex circuits due to the\nsimplistic geometry of jerk-optimal edges failing to capture the complexity of\nthe racing line. Additionally, they only consider two-dimensional tracks,\npotentially neglecting or surpassing the actual dynamic potential. In this\npaper, we present a sampling-based local trajectory planning approach for\nautonomous racing that can maintain the lap time of the racing line even on\ncomplex race tracks and consider the race track's three-dimensional effects. In\nsimulative experiments, we demonstrate that our approach achieves lower lap\ntimes and improved utilization of dynamic limits compared to existing\napproaches. We also investigate the impact of online racing line generation, in\nwhich the time-optimal solution is planned from the current vehicle state for a\nlimited spatial horizon, in contrast to a closed racing line calculated\noffline. We show that combining the sampling-based planner with the online\nracing line generation can significantly reduce lap times in multi-vehicle\nscenarios.\n", "link": "http://arxiv.org/abs/2403.18643v1", "date": "2024-03-27", "relevancy": 1.8671, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4671}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks&body=Title%3A%20Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks%0AAuthor%3A%20Levent%20%C3%96gretmen%20and%20Matthias%20Rowold%20and%20Boris%20Lohmann%0AAbstract%3A%20%20%20Existing%20approaches%20to%20trajectory%20planning%20for%20autonomous%20racing%20employ%0Asampling-based%20methods%2C%20generating%20numerous%20jerk-optimal%20trajectories%20and%0Aselecting%20the%20most%20favorable%20feasible%20trajectory%20based%20on%20a%20cost%20function%0Apenalizing%20deviations%20from%20an%20offline-calculated%20racing%20line.%20While%20successful%0Aon%20oval%20tracks%2C%20these%20methods%20face%20limitations%20on%20complex%20circuits%20due%20to%20the%0Asimplistic%20geometry%20of%20jerk-optimal%20edges%20failing%20to%20capture%20the%20complexity%20of%0Athe%20racing%20line.%20Additionally%2C%20they%20only%20consider%20two-dimensional%20tracks%2C%0Apotentially%20neglecting%20or%20surpassing%20the%20actual%20dynamic%20potential.%20In%20this%0Apaper%2C%20we%20present%20a%20sampling-based%20local%20trajectory%20planning%20approach%20for%0Aautonomous%20racing%20that%20can%20maintain%20the%20lap%20time%20of%20the%20racing%20line%20even%20on%0Acomplex%20race%20tracks%20and%20consider%20the%20race%20track%27s%20three-dimensional%20effects.%20In%0Asimulative%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20lower%20lap%0Atimes%20and%20improved%20utilization%20of%20dynamic%20limits%20compared%20to%20existing%0Aapproaches.%20We%20also%20investigate%20the%20impact%20of%20online%20racing%20line%20generation%2C%20in%0Awhich%20the%20time-optimal%20solution%20is%20planned%20from%20the%20current%20vehicle%20state%20for%20a%0Alimited%20spatial%20horizon%2C%20in%20contrast%20to%20a%20closed%20racing%20line%20calculated%0Aoffline.%20We%20show%20that%20combining%20the%20sampling-based%20planner%20with%20the%20online%0Aracing%20line%20generation%20can%20significantly%20reduce%20lap%20times%20in%20multi-vehicle%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18643v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks&entry.906535625=Levent%20%C3%96gretmen%20and%20Matthias%20Rowold%20and%20Boris%20Lohmann&entry.1292438233=%20%20Existing%20approaches%20to%20trajectory%20planning%20for%20autonomous%20racing%20employ%0Asampling-based%20methods%2C%20generating%20numerous%20jerk-optimal%20trajectories%20and%0Aselecting%20the%20most%20favorable%20feasible%20trajectory%20based%20on%20a%20cost%20function%0Apenalizing%20deviations%20from%20an%20offline-calculated%20racing%20line.%20While%20successful%0Aon%20oval%20tracks%2C%20these%20methods%20face%20limitations%20on%20complex%20circuits%20due%20to%20the%0Asimplistic%20geometry%20of%20jerk-optimal%20edges%20failing%20to%20capture%20the%20complexity%20of%0Athe%20racing%20line.%20Additionally%2C%20they%20only%20consider%20two-dimensional%20tracks%2C%0Apotentially%20neglecting%20or%20surpassing%20the%20actual%20dynamic%20potential.%20In%20this%0Apaper%2C%20we%20present%20a%20sampling-based%20local%20trajectory%20planning%20approach%20for%0Aautonomous%20racing%20that%20can%20maintain%20the%20lap%20time%20of%20the%20racing%20line%20even%20on%0Acomplex%20race%20tracks%20and%20consider%20the%20race%20track%27s%20three-dimensional%20effects.%20In%0Asimulative%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20lower%20lap%0Atimes%20and%20improved%20utilization%20of%20dynamic%20limits%20compared%20to%20existing%0Aapproaches.%20We%20also%20investigate%20the%20impact%20of%20online%20racing%20line%20generation%2C%20in%0Awhich%20the%20time-optimal%20solution%20is%20planned%20from%20the%20current%20vehicle%20state%20for%20a%0Alimited%20spatial%20horizon%2C%20in%20contrast%20to%20a%20closed%20racing%20line%20calculated%0Aoffline.%20We%20show%20that%20combining%20the%20sampling-based%20planner%20with%20the%20online%0Aracing%20line%20generation%20can%20significantly%20reduce%20lap%20times%20in%20multi-vehicle%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18643v1&entry.124074799=Read"},
{"title": "Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features", "author": "Leonardo Pepino and Pablo Riera and Luciana Ferrer and Agustin Gravano", "abstract": "  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n", "link": "http://arxiv.org/abs/2403.18635v1", "date": "2024-03-27", "relevancy": 1.864, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4683}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4551}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fusion%20approaches%20for%20emotion%20recognition%20from%20speech%20using%20acoustic%20and%0A%20%20text-based%20features&body=Title%3A%20Fusion%20approaches%20for%20emotion%20recognition%20from%20speech%20using%20acoustic%20and%0A%20%20text-based%20features%0AAuthor%3A%20Leonardo%20Pepino%20and%20Pablo%20Riera%20and%20Luciana%20Ferrer%20and%20Agustin%20Gravano%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20different%20approaches%20for%20classifying%20emotions%20from%0Aspeech%20using%20acoustic%20and%20text-based%20features.%20We%20propose%20to%20obtain%0Acontextualized%20word%20embeddings%20with%20BERT%20to%20represent%20the%20information%20contained%0Ain%20speech%20transcriptions%20and%20show%20that%20this%20results%20in%20better%20performance%20than%0Ausing%20Glove%20embeddings.%20We%20also%20propose%20and%20compare%20different%20strategies%20to%0Acombine%20the%20audio%20and%20text%20modalities%2C%20evaluating%20them%20on%20IEMOCAP%20and%0AMSP-PODCAST%20datasets.%20We%20find%20that%20fusing%20acoustic%20and%20text-based%20systems%20is%0Abeneficial%20on%20both%20datasets%2C%20though%20only%20subtle%20differences%20are%20observed%20across%0Athe%20evaluated%20fusion%20approaches.%20Finally%2C%20for%20IEMOCAP%2C%20we%20show%20the%20large%20effect%0Athat%20the%20criteria%20used%20to%20define%20the%20cross-validation%20folds%20have%20on%20results.%20In%0Aparticular%2C%20the%20standard%20way%20of%20creating%20folds%20for%20this%20dataset%20results%20in%20a%0Ahighly%20optimistic%20estimation%20of%20performance%20for%20the%20text-based%20system%2C%0Asuggesting%20that%20some%20previous%20works%20may%20overestimate%20the%20advantage%20of%0Aincorporating%20transcriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18635v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20approaches%20for%20emotion%20recognition%20from%20speech%20using%20acoustic%20and%0A%20%20text-based%20features&entry.906535625=Leonardo%20Pepino%20and%20Pablo%20Riera%20and%20Luciana%20Ferrer%20and%20Agustin%20Gravano&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20different%20approaches%20for%20classifying%20emotions%20from%0Aspeech%20using%20acoustic%20and%20text-based%20features.%20We%20propose%20to%20obtain%0Acontextualized%20word%20embeddings%20with%20BERT%20to%20represent%20the%20information%20contained%0Ain%20speech%20transcriptions%20and%20show%20that%20this%20results%20in%20better%20performance%20than%0Ausing%20Glove%20embeddings.%20We%20also%20propose%20and%20compare%20different%20strategies%20to%0Acombine%20the%20audio%20and%20text%20modalities%2C%20evaluating%20them%20on%20IEMOCAP%20and%0AMSP-PODCAST%20datasets.%20We%20find%20that%20fusing%20acoustic%20and%20text-based%20systems%20is%0Abeneficial%20on%20both%20datasets%2C%20though%20only%20subtle%20differences%20are%20observed%20across%0Athe%20evaluated%20fusion%20approaches.%20Finally%2C%20for%20IEMOCAP%2C%20we%20show%20the%20large%20effect%0Athat%20the%20criteria%20used%20to%20define%20the%20cross-validation%20folds%20have%20on%20results.%20In%0Aparticular%2C%20the%20standard%20way%20of%20creating%20folds%20for%20this%20dataset%20results%20in%20a%0Ahighly%20optimistic%20estimation%20of%20performance%20for%20the%20text-based%20system%2C%0Asuggesting%20that%20some%20previous%20works%20may%20overestimate%20the%20advantage%20of%0Aincorporating%20transcriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18635v1&entry.124074799=Read"},
{"title": "U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models", "author": "Ilias Mitsouras and Eleftherios Tsonis and Paraskevi Tzouveli and Athanasios Voulodimos", "abstract": "  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n", "link": "http://arxiv.org/abs/2403.18425v1", "date": "2024-03-27", "relevancy": 1.862, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6703}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6107}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5959}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20U-Sketch%3A%20An%20Efficient%20Approach%20for%20Sketch%20to%20Image%20Diffusion%20Models&body=Title%3A%20U-Sketch%3A%20An%20Efficient%20Approach%20for%20Sketch%20to%20Image%20Diffusion%20Models%0AAuthor%3A%20Ilias%20Mitsouras%20and%20Eleftherios%20Tsonis%20and%20Paraskevi%20Tzouveli%20and%20Athanasios%20Voulodimos%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20performance%20in%20text-to-image%0Asynthesis%2C%20producing%20realistic%20and%20high%20resolution%20images%20that%20faithfully%0Aadhere%20to%20the%20corresponding%20text-prompts.%20Despite%20their%20great%20success%2C%20they%0Astill%20fall%20behind%20in%20sketch-to-image%20synthesis%20tasks%2C%20where%20in%20addition%20to%0Atext-prompts%2C%20the%20spatial%20layout%20of%20the%20generated%20images%20has%20to%20closely%20follow%0Athe%20outlines%20of%20certain%20reference%20sketches.%20Employing%20an%20MLP%20latent%20edge%0Apredictor%20to%20guide%20the%20spatial%20layout%20of%20the%20synthesized%20image%20by%20predicting%0Aedge%20maps%20at%20each%20denoising%20step%20has%20been%20recently%20proposed.%20Despite%20yielding%0Apromising%20results%2C%20the%20pixel-wise%20operation%20of%20the%20MLP%20does%20not%20take%20into%0Aaccount%20the%20spatial%20layout%20as%20a%20whole%2C%20and%20demands%20numerous%20denoising%0Aiterations%20to%20produce%20satisfactory%20images%2C%20leading%20to%20time%20inefficiency.%20To%0Athis%20end%2C%20we%20introduce%20U-Sketch%2C%20a%20framework%20featuring%20a%20U-Net%20type%20latent%20edge%0Apredictor%2C%20which%20is%20capable%20of%20efficiently%20capturing%20both%20local%20and%20global%0Afeatures%2C%20as%20well%20as%20spatial%20correlations%20between%20pixels.%20Moreover%2C%20we%20propose%0Athe%20addition%20of%20a%20sketch%20simplification%20network%20that%20offers%20the%20user%20the%20choice%0Aof%20preprocessing%20and%20simplifying%20input%20sketches%20for%20enhanced%20outputs.%20The%0Aexperimental%20results%2C%20corroborated%20by%20user%20feedback%2C%20demonstrate%20that%20our%0Aproposed%20U-Net%20latent%20edge%20predictor%20leads%20to%20more%20realistic%20results%2C%20that%20are%0Abetter%20aligned%20with%20the%20spatial%20outlines%20of%20the%20reference%20sketches%2C%20while%0Adrastically%20reducing%20the%20number%20of%20required%20denoising%20steps%20and%2C%20consequently%2C%0Athe%20overall%20execution%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18425v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-Sketch%3A%20An%20Efficient%20Approach%20for%20Sketch%20to%20Image%20Diffusion%20Models&entry.906535625=Ilias%20Mitsouras%20and%20Eleftherios%20Tsonis%20and%20Paraskevi%20Tzouveli%20and%20Athanasios%20Voulodimos&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20remarkable%20performance%20in%20text-to-image%0Asynthesis%2C%20producing%20realistic%20and%20high%20resolution%20images%20that%20faithfully%0Aadhere%20to%20the%20corresponding%20text-prompts.%20Despite%20their%20great%20success%2C%20they%0Astill%20fall%20behind%20in%20sketch-to-image%20synthesis%20tasks%2C%20where%20in%20addition%20to%0Atext-prompts%2C%20the%20spatial%20layout%20of%20the%20generated%20images%20has%20to%20closely%20follow%0Athe%20outlines%20of%20certain%20reference%20sketches.%20Employing%20an%20MLP%20latent%20edge%0Apredictor%20to%20guide%20the%20spatial%20layout%20of%20the%20synthesized%20image%20by%20predicting%0Aedge%20maps%20at%20each%20denoising%20step%20has%20been%20recently%20proposed.%20Despite%20yielding%0Apromising%20results%2C%20the%20pixel-wise%20operation%20of%20the%20MLP%20does%20not%20take%20into%0Aaccount%20the%20spatial%20layout%20as%20a%20whole%2C%20and%20demands%20numerous%20denoising%0Aiterations%20to%20produce%20satisfactory%20images%2C%20leading%20to%20time%20inefficiency.%20To%0Athis%20end%2C%20we%20introduce%20U-Sketch%2C%20a%20framework%20featuring%20a%20U-Net%20type%20latent%20edge%0Apredictor%2C%20which%20is%20capable%20of%20efficiently%20capturing%20both%20local%20and%20global%0Afeatures%2C%20as%20well%20as%20spatial%20correlations%20between%20pixels.%20Moreover%2C%20we%20propose%0Athe%20addition%20of%20a%20sketch%20simplification%20network%20that%20offers%20the%20user%20the%20choice%0Aof%20preprocessing%20and%20simplifying%20input%20sketches%20for%20enhanced%20outputs.%20The%0Aexperimental%20results%2C%20corroborated%20by%20user%20feedback%2C%20demonstrate%20that%20our%0Aproposed%20U-Net%20latent%20edge%20predictor%20leads%20to%20more%20realistic%20results%2C%20that%20are%0Abetter%20aligned%20with%20the%20spatial%20outlines%20of%20the%20reference%20sketches%2C%20while%0Adrastically%20reducing%20the%20number%20of%20required%20denoising%20steps%20and%2C%20consequently%2C%0Athe%20overall%20execution%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18425v1&entry.124074799=Read"},
{"title": "Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity", "author": "Shiyuan Zuo and Xingrun Yan and Rongfei Fan and Han Hu and Hangguan Shan and Tony Q. S. Quek", "abstract": "  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n", "link": "http://arxiv.org/abs/2403.13374v3", "date": "2024-03-27", "relevancy": 1.8588, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Byzantine-resilient%20Federated%20Learning%20With%20Adaptivity%20to%20Data%0A%20%20Heterogeneity&body=Title%3A%20Byzantine-resilient%20Federated%20Learning%20With%20Adaptivity%20to%20Data%0A%20%20Heterogeneity%0AAuthor%3A%20Shiyuan%20Zuo%20and%20Xingrun%20Yan%20and%20Rongfei%20Fan%20and%20Han%20Hu%20and%20Hangguan%20Shan%20and%20Tony%20Q.%20S.%20Quek%0AAbstract%3A%20%20%20This%20paper%20deals%20with%20federated%20learning%20%28FL%29%20in%20the%20presence%20of%20malicious%0AByzantine%20attacks%20and%20data%20heterogeneity.%20A%20novel%20Robust%20Average%20Gradient%0AAlgorithm%20%28RAGA%29%20is%20proposed%2C%20which%20leverages%20the%20geometric%20median%20for%0Aaggregation%20and%20can%20freely%20select%20the%20round%20number%20for%20local%20updating.%0ADifferent%20from%20most%20existing%20resilient%20approaches%2C%20which%20perform%20convergence%0Aanalysis%20based%20on%20strongly-convex%20loss%20function%20or%20homogeneously%20distributed%0Adataset%2C%20we%20conduct%20convergence%20analysis%20for%20not%20only%20strongly-convex%20but%20also%0Anon-convex%20loss%20function%20over%20heterogeneous%20dataset.%20According%20to%20our%0Atheoretical%20analysis%2C%20as%20long%20as%20the%20fraction%20of%20dataset%20from%20malicious%20users%0Ais%20less%20than%20half%2C%20RAGA%20can%20achieve%20convergence%20at%20rate%0A%24%5Cmathcal%7BO%7D%28%7B1%7D/%7BT%5E%7B2/3-%20%5Cdelta%7D%7D%29%24%20where%20%24T%24%20is%20the%20iteration%20number%20and%0A%24%5Cdelta%20%5Cin%20%280%2C%202/3%29%24%20for%20non-convex%20loss%20function%2C%20and%20at%20linear%20rate%20for%0Astrongly-convex%20loss%20function.%20Moreover%2C%20stationary%20point%20or%20global%20optimal%0Asolution%20is%20proved%20to%20obtainable%20as%20data%20heterogeneity%20vanishes.%20Experimental%0Aresults%20corroborate%20the%20robustness%20of%20RAGA%20to%20Byzantine%20attacks%20and%20verifies%0Athe%20advantage%20of%20RAGA%20over%20baselines%20on%20convergence%20performance%20under%20various%0Aintensity%20of%20Byzantine%20attacks%2C%20for%20heterogeneous%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13374v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Byzantine-resilient%20Federated%20Learning%20With%20Adaptivity%20to%20Data%0A%20%20Heterogeneity&entry.906535625=Shiyuan%20Zuo%20and%20Xingrun%20Yan%20and%20Rongfei%20Fan%20and%20Han%20Hu%20and%20Hangguan%20Shan%20and%20Tony%20Q.%20S.%20Quek&entry.1292438233=%20%20This%20paper%20deals%20with%20federated%20learning%20%28FL%29%20in%20the%20presence%20of%20malicious%0AByzantine%20attacks%20and%20data%20heterogeneity.%20A%20novel%20Robust%20Average%20Gradient%0AAlgorithm%20%28RAGA%29%20is%20proposed%2C%20which%20leverages%20the%20geometric%20median%20for%0Aaggregation%20and%20can%20freely%20select%20the%20round%20number%20for%20local%20updating.%0ADifferent%20from%20most%20existing%20resilient%20approaches%2C%20which%20perform%20convergence%0Aanalysis%20based%20on%20strongly-convex%20loss%20function%20or%20homogeneously%20distributed%0Adataset%2C%20we%20conduct%20convergence%20analysis%20for%20not%20only%20strongly-convex%20but%20also%0Anon-convex%20loss%20function%20over%20heterogeneous%20dataset.%20According%20to%20our%0Atheoretical%20analysis%2C%20as%20long%20as%20the%20fraction%20of%20dataset%20from%20malicious%20users%0Ais%20less%20than%20half%2C%20RAGA%20can%20achieve%20convergence%20at%20rate%0A%24%5Cmathcal%7BO%7D%28%7B1%7D/%7BT%5E%7B2/3-%20%5Cdelta%7D%7D%29%24%20where%20%24T%24%20is%20the%20iteration%20number%20and%0A%24%5Cdelta%20%5Cin%20%280%2C%202/3%29%24%20for%20non-convex%20loss%20function%2C%20and%20at%20linear%20rate%20for%0Astrongly-convex%20loss%20function.%20Moreover%2C%20stationary%20point%20or%20global%20optimal%0Asolution%20is%20proved%20to%20obtainable%20as%20data%20heterogeneity%20vanishes.%20Experimental%0Aresults%20corroborate%20the%20robustness%20of%20RAGA%20to%20Byzantine%20attacks%20and%20verifies%0Athe%20advantage%20of%20RAGA%20over%20baselines%20on%20convergence%20performance%20under%20various%0Aintensity%20of%20Byzantine%20attacks%2C%20for%20heterogeneous%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13374v3&entry.124074799=Read"},
{"title": "Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration", "author": "Xiang Li and Soo Min Kwon and Ismail R. Alkhouri and Saiprasad Ravishankar and Qing Qu", "abstract": "  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n", "link": "http://arxiv.org/abs/2403.06054v4", "date": "2024-03-27", "relevancy": 1.8473, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6578}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6044}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6022}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Data%20Consistency%20with%20Diffusion%20Purification%20for%20Image%0A%20%20Restoration&body=Title%3A%20Decoupled%20Data%20Consistency%20with%20Diffusion%20Purification%20for%20Image%0A%20%20Restoration%0AAuthor%3A%20Xiang%20Li%20and%20Soo%20Min%20Kwon%20and%20Ismail%20R.%20Alkhouri%20and%20Saiprasad%20Ravishankar%20and%20Qing%20Qu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20gained%20traction%20as%20a%20powerful%20class%20of%20deep%0Agenerative%20priors%2C%20excelling%20in%20a%20wide%20range%20of%20image%20restoration%20tasks%20due%20to%0Atheir%20exceptional%20ability%20to%20model%20data%20distributions.%20To%20solve%20image%0Arestoration%20problems%2C%20many%20existing%20techniques%20achieve%20data%20consistency%20by%0Aincorporating%20additional%20likelihood%20gradient%20steps%20into%20the%20reverse%20sampling%0Aprocess%20of%20diffusion%20models.%20However%2C%20the%20additional%20gradient%20steps%20pose%20a%0Achallenge%20for%20real-world%20practical%20applications%20as%20they%20incur%20a%20large%0Acomputational%20overhead%2C%20thereby%20increasing%20inference%20time.%20They%20also%20present%0Aadditional%20difficulties%20when%20using%20accelerated%20diffusion%20model%20samplers%2C%20as%20the%0Anumber%20of%20data%20consistency%20steps%20is%20limited%20by%20the%20number%20of%20reverse%20sampling%0Asteps.%20In%20this%20work%2C%20we%20propose%20a%20novel%20diffusion-based%20image%20restoration%0Asolver%20that%20addresses%20these%20issues%20by%20decoupling%20the%20reverse%20process%20from%20the%0Adata%20consistency%20steps.%20Our%20method%20involves%20alternating%20between%20a%0Areconstruction%20phase%20to%20maintain%20data%20consistency%20and%20a%20refinement%20phase%20that%0Aenforces%20the%20prior%20via%20diffusion%20purification.%20Our%20approach%20demonstrates%0Aversatility%2C%20making%20it%20highly%20adaptable%20for%20efficient%20problem-solving%20in%20latent%0Aspace.%20Additionally%2C%20it%20reduces%20the%20necessity%20for%20numerous%20sampling%20steps%0Athrough%20the%20integration%20of%20consistency%20models.%20The%20efficacy%20of%20our%20approach%20is%0Avalidated%20through%20comprehensive%20experiments%20across%20various%20image%20restoration%0Atasks%2C%20including%20image%20denoising%2C%20deblurring%2C%20inpainting%2C%20and%20super-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06054v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Data%20Consistency%20with%20Diffusion%20Purification%20for%20Image%0A%20%20Restoration&entry.906535625=Xiang%20Li%20and%20Soo%20Min%20Kwon%20and%20Ismail%20R.%20Alkhouri%20and%20Saiprasad%20Ravishankar%20and%20Qing%20Qu&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20gained%20traction%20as%20a%20powerful%20class%20of%20deep%0Agenerative%20priors%2C%20excelling%20in%20a%20wide%20range%20of%20image%20restoration%20tasks%20due%20to%0Atheir%20exceptional%20ability%20to%20model%20data%20distributions.%20To%20solve%20image%0Arestoration%20problems%2C%20many%20existing%20techniques%20achieve%20data%20consistency%20by%0Aincorporating%20additional%20likelihood%20gradient%20steps%20into%20the%20reverse%20sampling%0Aprocess%20of%20diffusion%20models.%20However%2C%20the%20additional%20gradient%20steps%20pose%20a%0Achallenge%20for%20real-world%20practical%20applications%20as%20they%20incur%20a%20large%0Acomputational%20overhead%2C%20thereby%20increasing%20inference%20time.%20They%20also%20present%0Aadditional%20difficulties%20when%20using%20accelerated%20diffusion%20model%20samplers%2C%20as%20the%0Anumber%20of%20data%20consistency%20steps%20is%20limited%20by%20the%20number%20of%20reverse%20sampling%0Asteps.%20In%20this%20work%2C%20we%20propose%20a%20novel%20diffusion-based%20image%20restoration%0Asolver%20that%20addresses%20these%20issues%20by%20decoupling%20the%20reverse%20process%20from%20the%0Adata%20consistency%20steps.%20Our%20method%20involves%20alternating%20between%20a%0Areconstruction%20phase%20to%20maintain%20data%20consistency%20and%20a%20refinement%20phase%20that%0Aenforces%20the%20prior%20via%20diffusion%20purification.%20Our%20approach%20demonstrates%0Aversatility%2C%20making%20it%20highly%20adaptable%20for%20efficient%20problem-solving%20in%20latent%0Aspace.%20Additionally%2C%20it%20reduces%20the%20necessity%20for%20numerous%20sampling%20steps%0Athrough%20the%20integration%20of%20consistency%20models.%20The%20efficacy%20of%20our%20approach%20is%0Avalidated%20through%20comprehensive%20experiments%20across%20various%20image%20restoration%0Atasks%2C%20including%20image%20denoising%2C%20deblurring%2C%20inpainting%2C%20and%20super-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06054v4&entry.124074799=Read"},
{"title": "Long-form factuality in large language models", "author": "Jerry Wei and Chengrun Yang and Xinying Song and Yifeng Lu and Nathan Hu and Dustin Tran and Daiyi Peng and Ruibo Liu and Da Huang and Cosmo Du and Quoc V. Le", "abstract": "  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n", "link": "http://arxiv.org/abs/2403.18802v1", "date": "2024-03-27", "relevancy": 0.9237, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4495}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.439}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Long-form%20factuality%20in%20large%20language%20models&body=Title%3A%20Long-form%20factuality%20in%20large%20language%20models%0AAuthor%3A%20Jerry%20Wei%20and%20Chengrun%20Yang%20and%20Xinying%20Song%20and%20Yifeng%20Lu%20and%20Nathan%20Hu%20and%20Dustin%20Tran%20and%20Daiyi%20Peng%20and%20Ruibo%20Liu%20and%20Da%20Huang%20and%20Cosmo%20Du%20and%20Quoc%20V.%20Le%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20content%20that%20contains%20factual%0Aerrors%20when%20responding%20to%20fact-seeking%20prompts%20on%20open-ended%20topics.%20To%0Abenchmark%20a%20model%27s%20long-form%20factuality%20in%20open%20domains%2C%20we%20first%20use%20GPT-4%20to%0Agenerate%20LongFact%2C%20a%20prompt%20set%20comprising%20thousands%20of%20questions%20spanning%2038%0Atopics.%20We%20then%20propose%20that%20LLM%20agents%20can%20be%20used%20as%20automated%20evaluators%20for%0Along-form%20factuality%20through%20a%20method%20which%20we%20call%20Search-Augmented%20Factuality%0AEvaluator%20%28SAFE%29.%20SAFE%20utilizes%20an%20LLM%20to%20break%20down%20a%20long-form%20response%20into%0Aa%20set%20of%20individual%20facts%20and%20to%20evaluate%20the%20accuracy%20of%20each%20fact%20using%20a%0Amulti-step%20reasoning%20process%20comprising%20sending%20search%20queries%20to%20Google%20Search%0Aand%20determining%20whether%20a%20fact%20is%20supported%20by%20the%20search%20results.%20Furthermore%2C%0Awe%20propose%20extending%20F1%20score%20as%20an%20aggregated%20metric%20for%20long-form%20factuality.%0ATo%20do%20so%2C%20we%20balance%20the%20percentage%20of%20supported%20facts%20in%20a%20response%0A%28precision%29%20with%20the%20percentage%20of%20provided%20facts%20relative%20to%20a%20hyperparameter%0Arepresenting%20a%20user%27s%20preferred%20response%20length%20%28recall%29.%0A%20%20Empirically%2C%20we%20demonstrate%20that%20LLM%20agents%20can%20achieve%20superhuman%20rating%0Aperformance%20-%20on%20a%20set%20of%20~16k%20individual%20facts%2C%20SAFE%20agrees%20with%20crowdsourced%0Ahuman%20annotators%2072%25%20of%20the%20time%2C%20and%20on%20a%20random%20subset%20of%20100%20disagreement%0Acases%2C%20SAFE%20wins%2076%25%20of%20the%20time.%20At%20the%20same%20time%2C%20SAFE%20is%20more%20than%2020%20times%0Acheaper%20than%20human%20annotators.%20We%20also%20benchmark%20thirteen%20language%20models%20on%0ALongFact%20across%20four%20model%20families%20%28Gemini%2C%20GPT%2C%20Claude%2C%20and%20PaLM-2%29%2C%20finding%0Athat%20larger%20language%20models%20generally%20achieve%20better%20long-form%20factuality.%0ALongFact%2C%20SAFE%2C%20and%20all%20experimental%20code%20are%20available%20at%0Ahttps%3A//github.com/google-deepmind/long-form-factuality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18802v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-form%20factuality%20in%20large%20language%20models&entry.906535625=Jerry%20Wei%20and%20Chengrun%20Yang%20and%20Xinying%20Song%20and%20Yifeng%20Lu%20and%20Nathan%20Hu%20and%20Dustin%20Tran%20and%20Daiyi%20Peng%20and%20Ruibo%20Liu%20and%20Da%20Huang%20and%20Cosmo%20Du%20and%20Quoc%20V.%20Le&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20generate%20content%20that%20contains%20factual%0Aerrors%20when%20responding%20to%20fact-seeking%20prompts%20on%20open-ended%20topics.%20To%0Abenchmark%20a%20model%27s%20long-form%20factuality%20in%20open%20domains%2C%20we%20first%20use%20GPT-4%20to%0Agenerate%20LongFact%2C%20a%20prompt%20set%20comprising%20thousands%20of%20questions%20spanning%2038%0Atopics.%20We%20then%20propose%20that%20LLM%20agents%20can%20be%20used%20as%20automated%20evaluators%20for%0Along-form%20factuality%20through%20a%20method%20which%20we%20call%20Search-Augmented%20Factuality%0AEvaluator%20%28SAFE%29.%20SAFE%20utilizes%20an%20LLM%20to%20break%20down%20a%20long-form%20response%20into%0Aa%20set%20of%20individual%20facts%20and%20to%20evaluate%20the%20accuracy%20of%20each%20fact%20using%20a%0Amulti-step%20reasoning%20process%20comprising%20sending%20search%20queries%20to%20Google%20Search%0Aand%20determining%20whether%20a%20fact%20is%20supported%20by%20the%20search%20results.%20Furthermore%2C%0Awe%20propose%20extending%20F1%20score%20as%20an%20aggregated%20metric%20for%20long-form%20factuality.%0ATo%20do%20so%2C%20we%20balance%20the%20percentage%20of%20supported%20facts%20in%20a%20response%0A%28precision%29%20with%20the%20percentage%20of%20provided%20facts%20relative%20to%20a%20hyperparameter%0Arepresenting%20a%20user%27s%20preferred%20response%20length%20%28recall%29.%0A%20%20Empirically%2C%20we%20demonstrate%20that%20LLM%20agents%20can%20achieve%20superhuman%20rating%0Aperformance%20-%20on%20a%20set%20of%20~16k%20individual%20facts%2C%20SAFE%20agrees%20with%20crowdsourced%0Ahuman%20annotators%2072%25%20of%20the%20time%2C%20and%20on%20a%20random%20subset%20of%20100%20disagreement%0Acases%2C%20SAFE%20wins%2076%25%20of%20the%20time.%20At%20the%20same%20time%2C%20SAFE%20is%20more%20than%2020%20times%0Acheaper%20than%20human%20annotators.%20We%20also%20benchmark%20thirteen%20language%20models%20on%0ALongFact%20across%20four%20model%20families%20%28Gemini%2C%20GPT%2C%20Claude%2C%20and%20PaLM-2%29%2C%20finding%0Athat%20larger%20language%20models%20generally%20achieve%20better%20long-form%20factuality.%0ALongFact%2C%20SAFE%2C%20and%20all%20experimental%20code%20are%20available%20at%0Ahttps%3A//github.com/google-deepmind/long-form-factuality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18802v1&entry.124074799=Read"},
{"title": "One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch", "author": "Caio Cesar Daumann and Mauro Donega and Johannes Erdmann and Massimiliano Galli and Jan Lukas Sp\u00e4h and Davide Valsecchi", "abstract": "  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n", "link": "http://arxiv.org/abs/2403.18582v1", "date": "2024-03-27", "relevancy": 1.4684, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5801}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4652}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4595}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One%20flow%20to%20correct%20them%20all%3A%20improving%20simulations%20in%20high-energy%0A%20%20physics%20with%20a%20single%20normalising%20flow%20and%20a%20switch&body=Title%3A%20One%20flow%20to%20correct%20them%20all%3A%20improving%20simulations%20in%20high-energy%0A%20%20physics%20with%20a%20single%20normalising%20flow%20and%20a%20switch%0AAuthor%3A%20Caio%20Cesar%20Daumann%20and%20Mauro%20Donega%20and%20Johannes%20Erdmann%20and%20Massimiliano%20Galli%20and%20Jan%20Lukas%20Sp%C3%A4h%20and%20Davide%20Valsecchi%0AAbstract%3A%20%20%20Simulated%20events%20are%20key%20ingredients%20in%20almost%20all%20high-energy%20physics%0Aanalyses.%20However%2C%20imperfections%20in%20the%20simulation%20can%20lead%20to%20sizeable%0Adifferences%20between%20the%20observed%20data%20and%20simulated%20events.%20The%20effects%20of%20such%0Amismodelling%20on%20relevant%20observables%20must%20be%20corrected%20either%20effectively%20via%0Ascale%20factors%2C%20with%20weights%20or%20by%20modifying%20the%20distributions%20of%20the%0Aobservables%20and%20their%20correlations.%20We%20introduce%20a%20correction%20method%20that%0Atransforms%20one%20multidimensional%20distribution%20%28simulation%29%20into%20another%20one%0A%28data%29%20using%20a%20simple%20architecture%20based%20on%20a%20single%20normalising%20flow%20with%20a%0Aboolean%20condition.%20We%20demonstrate%20the%20effectiveness%20of%20the%20method%20on%20a%0Aphysics-inspired%20toy%20dataset%20with%20non-trivial%20mismodelling%20of%20several%0Aobservables%20and%20their%20correlations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18582v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20flow%20to%20correct%20them%20all%3A%20improving%20simulations%20in%20high-energy%0A%20%20physics%20with%20a%20single%20normalising%20flow%20and%20a%20switch&entry.906535625=Caio%20Cesar%20Daumann%20and%20Mauro%20Donega%20and%20Johannes%20Erdmann%20and%20Massimiliano%20Galli%20and%20Jan%20Lukas%20Sp%C3%A4h%20and%20Davide%20Valsecchi&entry.1292438233=%20%20Simulated%20events%20are%20key%20ingredients%20in%20almost%20all%20high-energy%20physics%0Aanalyses.%20However%2C%20imperfections%20in%20the%20simulation%20can%20lead%20to%20sizeable%0Adifferences%20between%20the%20observed%20data%20and%20simulated%20events.%20The%20effects%20of%20such%0Amismodelling%20on%20relevant%20observables%20must%20be%20corrected%20either%20effectively%20via%0Ascale%20factors%2C%20with%20weights%20or%20by%20modifying%20the%20distributions%20of%20the%0Aobservables%20and%20their%20correlations.%20We%20introduce%20a%20correction%20method%20that%0Atransforms%20one%20multidimensional%20distribution%20%28simulation%29%20into%20another%20one%0A%28data%29%20using%20a%20simple%20architecture%20based%20on%20a%20single%20normalising%20flow%20with%20a%0Aboolean%20condition.%20We%20demonstrate%20the%20effectiveness%20of%20the%20method%20on%20a%0Aphysics-inspired%20toy%20dataset%20with%20non-trivial%20mismodelling%20of%20several%0Aobservables%20and%20their%20correlations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18582v1&entry.124074799=Read"},
{"title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text", "author": "Elliot Bolton and Abhinav Venigalla and Michihiro Yasunaga and David Hall and Betty Xiong and Tony Lee and Roxana Daneshjou and Jonathan Frankle and Percy Liang and Michael Carbin and Christopher D. Manning", "abstract": "  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n", "link": "http://arxiv.org/abs/2403.18421v1", "date": "2024-03-27", "relevancy": 1.4286, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5004}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BioMedLM%3A%20A%202.7B%20Parameter%20Language%20Model%20Trained%20On%20Biomedical%20Text&body=Title%3A%20BioMedLM%3A%20A%202.7B%20Parameter%20Language%20Model%20Trained%20On%20Biomedical%20Text%0AAuthor%3A%20Elliot%20Bolton%20and%20Abhinav%20Venigalla%20and%20Michihiro%20Yasunaga%20and%20David%20Hall%20and%20Betty%20Xiong%20and%20Tony%20Lee%20and%20Roxana%20Daneshjou%20and%20Jonathan%20Frankle%20and%20Percy%20Liang%20and%20Michael%20Carbin%20and%20Christopher%20D.%20Manning%0AAbstract%3A%20%20%20Models%20such%20as%20GPT-4%20and%20Med-PaLM%202%20have%20demonstrated%20impressive%20performance%0Aon%20a%20wide%20variety%20of%20biomedical%20NLP%20tasks.%20However%2C%20these%20models%20have%20hundreds%0Aof%20billions%20of%20parameters%2C%20are%20computationally%20expensive%20to%20run%2C%20require%20users%0Ato%20send%20their%20input%20data%20over%20the%20internet%2C%20and%20are%20trained%20on%20unknown%20data%0Asources.%20Can%20smaller%2C%20more%20targeted%20models%20compete%3F%20To%20address%20this%20question%2C%0Awe%20build%20and%20release%20BioMedLM%2C%20a%202.7%20billion%20parameter%20GPT-style%20autoregressive%0Amodel%20trained%20exclusively%20on%20PubMed%20abstracts%20and%20full%20articles.%20When%0Afine-tuned%2C%20BioMedLM%20can%20produce%20strong%20multiple-choice%20biomedical%0Aquestion-answering%20results%20competitive%20with%20much%20larger%20models%2C%20such%20as%0Aachieving%20a%20score%20of%2057.3%25%20on%20MedMCQA%20%28dev%29%20and%2069.0%25%20on%20the%20MMLU%20Medical%0AGenetics%20exam.%20BioMedLM%20can%20also%20be%20fine-tuned%20to%20produce%20useful%20answers%20to%0Apatient%20questions%20on%20medical%20topics.%20This%20demonstrates%20that%20smaller%20models%20can%0Apotentially%20serve%20as%20transparent%2C%20privacy-preserving%2C%20economical%20and%0Aenvironmentally%20friendly%20foundations%20for%20particular%20NLP%20applications%2C%20such%20as%0Ain%20biomedicine.%20The%20model%20is%20available%20on%20the%20Hugging%20Face%20Hub%3A%0Ahttps%3A//huggingface.co/stanford-crfm/BioMedLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18421v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioMedLM%3A%20A%202.7B%20Parameter%20Language%20Model%20Trained%20On%20Biomedical%20Text&entry.906535625=Elliot%20Bolton%20and%20Abhinav%20Venigalla%20and%20Michihiro%20Yasunaga%20and%20David%20Hall%20and%20Betty%20Xiong%20and%20Tony%20Lee%20and%20Roxana%20Daneshjou%20and%20Jonathan%20Frankle%20and%20Percy%20Liang%20and%20Michael%20Carbin%20and%20Christopher%20D.%20Manning&entry.1292438233=%20%20Models%20such%20as%20GPT-4%20and%20Med-PaLM%202%20have%20demonstrated%20impressive%20performance%0Aon%20a%20wide%20variety%20of%20biomedical%20NLP%20tasks.%20However%2C%20these%20models%20have%20hundreds%0Aof%20billions%20of%20parameters%2C%20are%20computationally%20expensive%20to%20run%2C%20require%20users%0Ato%20send%20their%20input%20data%20over%20the%20internet%2C%20and%20are%20trained%20on%20unknown%20data%0Asources.%20Can%20smaller%2C%20more%20targeted%20models%20compete%3F%20To%20address%20this%20question%2C%0Awe%20build%20and%20release%20BioMedLM%2C%20a%202.7%20billion%20parameter%20GPT-style%20autoregressive%0Amodel%20trained%20exclusively%20on%20PubMed%20abstracts%20and%20full%20articles.%20When%0Afine-tuned%2C%20BioMedLM%20can%20produce%20strong%20multiple-choice%20biomedical%0Aquestion-answering%20results%20competitive%20with%20much%20larger%20models%2C%20such%20as%0Aachieving%20a%20score%20of%2057.3%25%20on%20MedMCQA%20%28dev%29%20and%2069.0%25%20on%20the%20MMLU%20Medical%0AGenetics%20exam.%20BioMedLM%20can%20also%20be%20fine-tuned%20to%20produce%20useful%20answers%20to%0Apatient%20questions%20on%20medical%20topics.%20This%20demonstrates%20that%20smaller%20models%20can%0Apotentially%20serve%20as%20transparent%2C%20privacy-preserving%2C%20economical%20and%0Aenvironmentally%20friendly%20foundations%20for%20particular%20NLP%20applications%2C%20such%20as%0Ain%20biomedicine.%20The%20model%20is%20available%20on%20the%20Hugging%20Face%20Hub%3A%0Ahttps%3A//huggingface.co/stanford-crfm/BioMedLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18421v1&entry.124074799=Read"},
{"title": "Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG", "author": "Martin Bruse and Luca Versari and Zoltan Szabadka and Jyrki Alakuijala", "abstract": "  We performed pairwise comparisons by human raters of JPEG images from\nMozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a\nquality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely\nto be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8\nbits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits\nper pixel respectively. The raw ratings and source images are publicly\navailable for further analysis and study.\n", "link": "http://arxiv.org/abs/2403.18589v1", "date": "2024-03-27", "relevancy": 1.4289, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3758}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.365}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.342}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Users%20prefer%20Jpegli%20over%20same-sized%20libjpeg-turbo%20or%20MozJPEG&body=Title%3A%20Users%20prefer%20Jpegli%20over%20same-sized%20libjpeg-turbo%20or%20MozJPEG%0AAuthor%3A%20Martin%20Bruse%20and%20Luca%20Versari%20and%20Zoltan%20Szabadka%20and%20Jyrki%20Alakuijala%0AAbstract%3A%20%20%20We%20performed%20pairwise%20comparisons%20by%20human%20raters%20of%20JPEG%20images%20from%0AMozJPEG%2C%20libjpeg-turbo%20and%20our%20new%20Jpegli%20encoder.%20When%20compressing%20images%20at%20a%0Aquality%20similar%20to%20libjpeg-turbo%20quality%2095%2C%20the%20Jpegli%20images%20were%2054%25%20likely%0Ato%20be%20preferred%20over%20both%20libjpeg-turbo%20and%20MozJPEG%20images%2C%20but%20used%20only%202.8%0Abits%20per%20pixel%20compared%20to%20libjpeg-turbo%20and%20MozJPEG%20that%20used%203.8%20and%203.5%20bits%0Aper%20pixel%20respectively.%20The%20raw%20ratings%20and%20source%20images%20are%20publicly%0Aavailable%20for%20further%20analysis%20and%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Users%20prefer%20Jpegli%20over%20same-sized%20libjpeg-turbo%20or%20MozJPEG&entry.906535625=Martin%20Bruse%20and%20Luca%20Versari%20and%20Zoltan%20Szabadka%20and%20Jyrki%20Alakuijala&entry.1292438233=%20%20We%20performed%20pairwise%20comparisons%20by%20human%20raters%20of%20JPEG%20images%20from%0AMozJPEG%2C%20libjpeg-turbo%20and%20our%20new%20Jpegli%20encoder.%20When%20compressing%20images%20at%20a%0Aquality%20similar%20to%20libjpeg-turbo%20quality%2095%2C%20the%20Jpegli%20images%20were%2054%25%20likely%0Ato%20be%20preferred%20over%20both%20libjpeg-turbo%20and%20MozJPEG%20images%2C%20but%20used%20only%202.8%0Abits%20per%20pixel%20compared%20to%20libjpeg-turbo%20and%20MozJPEG%20that%20used%203.8%20and%203.5%20bits%0Aper%20pixel%20respectively.%20The%20raw%20ratings%20and%20source%20images%20are%20publicly%0Aavailable%20for%20further%20analysis%20and%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18589v1&entry.124074799=Read"},
{"title": "ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object", "author": "Chenshuang Zhang and Fei Pan and Junmo Kim and In So Kweon and Chengzhi Mao", "abstract": "  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n", "link": "http://arxiv.org/abs/2403.18775v1", "date": "2024-03-27", "relevancy": 1.2155, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6152}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ImageNet-D%3A%20Benchmarking%20Neural%20Network%20Robustness%20on%20Diffusion%0A%20%20Synthetic%20Object&body=Title%3A%20ImageNet-D%3A%20Benchmarking%20Neural%20Network%20Robustness%20on%20Diffusion%0A%20%20Synthetic%20Object%0AAuthor%3A%20Chenshuang%20Zhang%20and%20Fei%20Pan%20and%20Junmo%20Kim%20and%20In%20So%20Kweon%20and%20Chengzhi%20Mao%0AAbstract%3A%20%20%20We%20establish%20rigorous%20benchmarks%20for%20visual%20perception%20robustness.%20Synthetic%0Aimages%20such%20as%20ImageNet-C%2C%20ImageNet-9%2C%20and%20Stylized%20ImageNet%20provide%20specific%0Atype%20of%20evaluation%20over%20synthetic%20corruptions%2C%20backgrounds%2C%20and%20textures%2C%20yet%0Athose%20robustness%20benchmarks%20are%20restricted%20in%20specified%20variations%20and%20have%20low%0Asynthetic%20quality.%20In%20this%20work%2C%20we%20introduce%20generative%20model%20as%20a%20data%20source%0Afor%20synthesizing%20hard%20images%20that%20benchmark%20deep%20models%27%20robustness.%20Leveraging%0Adiffusion%20models%2C%20we%20are%20able%20to%20generate%20images%20with%20more%20diversified%0Abackgrounds%2C%20textures%2C%20and%20materials%20than%20any%20prior%20work%2C%20where%20we%20term%20this%0Abenchmark%20as%20ImageNet-D.%20Experimental%20results%20show%20that%20ImageNet-D%20results%20in%20a%0Asignificant%20accuracy%20drop%20to%20a%20range%20of%20vision%20models%2C%20from%20the%20standard%20ResNet%0Avisual%20classifier%20to%20the%20latest%20foundation%20models%20like%20CLIP%20and%20MiniGPT-4%2C%0Asignificantly%20reducing%20their%20accuracy%20by%20up%20to%2060%5C%25.%20Our%20work%20suggests%20that%0Adiffusion%20models%20can%20be%20an%20effective%20source%20to%20test%20vision%20models.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/chenshuang-zhang/imagenet_d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18775v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageNet-D%3A%20Benchmarking%20Neural%20Network%20Robustness%20on%20Diffusion%0A%20%20Synthetic%20Object&entry.906535625=Chenshuang%20Zhang%20and%20Fei%20Pan%20and%20Junmo%20Kim%20and%20In%20So%20Kweon%20and%20Chengzhi%20Mao&entry.1292438233=%20%20We%20establish%20rigorous%20benchmarks%20for%20visual%20perception%20robustness.%20Synthetic%0Aimages%20such%20as%20ImageNet-C%2C%20ImageNet-9%2C%20and%20Stylized%20ImageNet%20provide%20specific%0Atype%20of%20evaluation%20over%20synthetic%20corruptions%2C%20backgrounds%2C%20and%20textures%2C%20yet%0Athose%20robustness%20benchmarks%20are%20restricted%20in%20specified%20variations%20and%20have%20low%0Asynthetic%20quality.%20In%20this%20work%2C%20we%20introduce%20generative%20model%20as%20a%20data%20source%0Afor%20synthesizing%20hard%20images%20that%20benchmark%20deep%20models%27%20robustness.%20Leveraging%0Adiffusion%20models%2C%20we%20are%20able%20to%20generate%20images%20with%20more%20diversified%0Abackgrounds%2C%20textures%2C%20and%20materials%20than%20any%20prior%20work%2C%20where%20we%20term%20this%0Abenchmark%20as%20ImageNet-D.%20Experimental%20results%20show%20that%20ImageNet-D%20results%20in%20a%0Asignificant%20accuracy%20drop%20to%20a%20range%20of%20vision%20models%2C%20from%20the%20standard%20ResNet%0Avisual%20classifier%20to%20the%20latest%20foundation%20models%20like%20CLIP%20and%20MiniGPT-4%2C%0Asignificantly%20reducing%20their%20accuracy%20by%20up%20to%2060%5C%25.%20Our%20work%20suggests%20that%0Adiffusion%20models%20can%20be%20an%20effective%20source%20to%20test%20vision%20models.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/chenshuang-zhang/imagenet_d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18775v1&entry.124074799=Read"},
{"title": "Teaching Introductory HRI: UChicago Course \"Human-Robot Interaction:\n  Research and Practice\"", "author": "Sarah Sebo", "abstract": "  In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction:\nResearch and Practice as a hands-on introduction to human-robot interaction\n(HRI) research for both undergraduate and graduate students at the University\nof Chicago. Since 2020, I have taught and refined this course each academic\nyear. Human-Robot Interaction: Research and Practice focuses on the core\nconcepts and cutting-edge research in the field of human-robot interaction\n(HRI), covering topics that include: nonverbal robot behavior, verbal robot\nbehavior, social dynamics, norms & ethics, collaboration & learning, group\ninteractions, applications, and future challenges of HRI. Course meetings\ninvolve students in the class leading discussions about cutting-edge\npeer-reviewed research HRI publications. Students also participate in a\nquarter-long collaborative research project, where they pursue an HRI research\nquestion that often involves conducing their own human-subjects research study\nwhere they recruit human subjects to interact with a robot. In this paper, I\ndetail the structure of the course and its learning goals as well as my\nreflections and student feedback on the course.\n", "link": "http://arxiv.org/abs/2403.18692v1", "date": "2024-03-27", "relevancy": 1.3648, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4628}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4325}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Teaching%20Introductory%20HRI%3A%20UChicago%20Course%20%22Human-Robot%20Interaction%3A%0A%20%20Research%20and%20Practice%22&body=Title%3A%20Teaching%20Introductory%20HRI%3A%20UChicago%20Course%20%22Human-Robot%20Interaction%3A%0A%20%20Research%20and%20Practice%22%0AAuthor%3A%20Sarah%20Sebo%0AAbstract%3A%20%20%20In%202020%2C%20I%20designed%20the%20course%20CMSC%2020630/30630%20Human-Robot%20Interaction%3A%0AResearch%20and%20Practice%20as%20a%20hands-on%20introduction%20to%20human-robot%20interaction%0A%28HRI%29%20research%20for%20both%20undergraduate%20and%20graduate%20students%20at%20the%20University%0Aof%20Chicago.%20Since%202020%2C%20I%20have%20taught%20and%20refined%20this%20course%20each%20academic%0Ayear.%20Human-Robot%20Interaction%3A%20Research%20and%20Practice%20focuses%20on%20the%20core%0Aconcepts%20and%20cutting-edge%20research%20in%20the%20field%20of%20human-robot%20interaction%0A%28HRI%29%2C%20covering%20topics%20that%20include%3A%20nonverbal%20robot%20behavior%2C%20verbal%20robot%0Abehavior%2C%20social%20dynamics%2C%20norms%20%26%20ethics%2C%20collaboration%20%26%20learning%2C%20group%0Ainteractions%2C%20applications%2C%20and%20future%20challenges%20of%20HRI.%20Course%20meetings%0Ainvolve%20students%20in%20the%20class%20leading%20discussions%20about%20cutting-edge%0Apeer-reviewed%20research%20HRI%20publications.%20Students%20also%20participate%20in%20a%0Aquarter-long%20collaborative%20research%20project%2C%20where%20they%20pursue%20an%20HRI%20research%0Aquestion%20that%20often%20involves%20conducing%20their%20own%20human-subjects%20research%20study%0Awhere%20they%20recruit%20human%20subjects%20to%20interact%20with%20a%20robot.%20In%20this%20paper%2C%20I%0Adetail%20the%20structure%20of%20the%20course%20and%20its%20learning%20goals%20as%20well%20as%20my%0Areflections%20and%20student%20feedback%20on%20the%20course.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18692v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Introductory%20HRI%3A%20UChicago%20Course%20%22Human-Robot%20Interaction%3A%0A%20%20Research%20and%20Practice%22&entry.906535625=Sarah%20Sebo&entry.1292438233=%20%20In%202020%2C%20I%20designed%20the%20course%20CMSC%2020630/30630%20Human-Robot%20Interaction%3A%0AResearch%20and%20Practice%20as%20a%20hands-on%20introduction%20to%20human-robot%20interaction%0A%28HRI%29%20research%20for%20both%20undergraduate%20and%20graduate%20students%20at%20the%20University%0Aof%20Chicago.%20Since%202020%2C%20I%20have%20taught%20and%20refined%20this%20course%20each%20academic%0Ayear.%20Human-Robot%20Interaction%3A%20Research%20and%20Practice%20focuses%20on%20the%20core%0Aconcepts%20and%20cutting-edge%20research%20in%20the%20field%20of%20human-robot%20interaction%0A%28HRI%29%2C%20covering%20topics%20that%20include%3A%20nonverbal%20robot%20behavior%2C%20verbal%20robot%0Abehavior%2C%20social%20dynamics%2C%20norms%20%26%20ethics%2C%20collaboration%20%26%20learning%2C%20group%0Ainteractions%2C%20applications%2C%20and%20future%20challenges%20of%20HRI.%20Course%20meetings%0Ainvolve%20students%20in%20the%20class%20leading%20discussions%20about%20cutting-edge%0Apeer-reviewed%20research%20HRI%20publications.%20Students%20also%20participate%20in%20a%0Aquarter-long%20collaborative%20research%20project%2C%20where%20they%20pursue%20an%20HRI%20research%0Aquestion%20that%20often%20involves%20conducing%20their%20own%20human-subjects%20research%20study%0Awhere%20they%20recruit%20human%20subjects%20to%20interact%20with%20a%20robot.%20In%20this%20paper%2C%20I%0Adetail%20the%20structure%20of%20the%20course%20and%20its%20learning%20goals%20as%20well%20as%20my%0Areflections%20and%20student%20feedback%20on%20the%20course.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18692v1&entry.124074799=Read"},
{"title": "$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models", "author": "Yue Xu and Wenjie Wang", "abstract": "  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n", "link": "http://arxiv.org/abs/2403.16432v2", "date": "2024-03-27", "relevancy": 1.3473, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4434}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4316}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20%24%5Ctextit%7BLinkPrompt%7D%24%3A%20Natural%20and%20Universal%20Adversarial%20Attacks%20on%0A%20%20Prompt-based%20Language%20Models&body=Title%3A%20%24%5Ctextit%7BLinkPrompt%7D%24%3A%20Natural%20and%20Universal%20Adversarial%20Attacks%20on%0A%20%20Prompt-based%20Language%20Models%0AAuthor%3A%20Yue%20Xu%20and%20Wenjie%20Wang%0AAbstract%3A%20%20%20Prompt-based%20learning%20is%20a%20new%20language%20model%20training%20paradigm%20that%20adapts%0Athe%20Pre-trained%20Language%20Models%20%28PLMs%29%20to%20downstream%20tasks%2C%20which%20revitalizes%0Athe%20performance%20benchmarks%20across%20various%20natural%20language%20processing%20%28NLP%29%0Atasks.%20Instead%20of%20using%20a%20fixed%20prompt%20template%20to%20fine-tune%20the%20model%2C%20some%0Aresearch%20demonstrates%20the%20effectiveness%20of%20searching%20for%20the%20prompt%20via%0Aoptimization.%20Such%20prompt%20optimization%20process%20of%20prompt-based%20learning%20on%20PLMs%0Aalso%20gives%20insight%20into%20generating%20adversarial%20prompts%20to%20mislead%20the%20model%2C%0Araising%20concerns%20about%20the%20adversarial%20vulnerability%20of%20this%20paradigm.%20Recent%0Astudies%20have%20shown%20that%20universal%20adversarial%20triggers%20%28UATs%29%20can%20be%20generated%0Ato%20alter%20not%20only%20the%20predictions%20of%20the%20target%20PLMs%20but%20also%20the%20prediction%20of%0Acorresponding%20Prompt-based%20Fine-tuning%20Models%20%28PFMs%29%20under%20the%20prompt-based%0Alearning%20paradigm.%20However%2C%20UATs%20found%20in%20previous%20works%20are%20often%20unreadable%0Atokens%20or%20characters%20and%20can%20be%20easily%20distinguished%20from%20natural%20texts%20with%0Aadaptive%20defenses.%20In%20this%20work%2C%20we%20consider%20the%20naturalness%20of%20the%20UATs%20and%0Adevelop%20%24%5Ctextit%7BLinkPrompt%7D%24%2C%20an%20adversarial%20attack%20algorithm%20to%20generate%20UATs%0Aby%20a%20gradient-based%20beam%20search%20algorithm%20that%20not%20only%20effectively%20attacks%20the%0Atarget%20PLMs%20and%20PFMs%20but%20also%20maintains%20the%20naturalness%20among%20the%20trigger%0Atokens.%20Extensive%20results%20demonstrate%20the%20effectiveness%20of%0A%24%5Ctextit%7BLinkPrompt%7D%24%2C%20as%20well%20as%20the%20transferability%20of%20UATs%20generated%20by%0A%24%5Ctextit%7BLinkPrompt%7D%24%20to%20open-sourced%20Large%20Language%20Model%20%28LLM%29%20Llama2%20and%0AAPI-accessed%20LLM%20GPT-3.5-turbo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16432v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctextit%7BLinkPrompt%7D%24%3A%20Natural%20and%20Universal%20Adversarial%20Attacks%20on%0A%20%20Prompt-based%20Language%20Models&entry.906535625=Yue%20Xu%20and%20Wenjie%20Wang&entry.1292438233=%20%20Prompt-based%20learning%20is%20a%20new%20language%20model%20training%20paradigm%20that%20adapts%0Athe%20Pre-trained%20Language%20Models%20%28PLMs%29%20to%20downstream%20tasks%2C%20which%20revitalizes%0Athe%20performance%20benchmarks%20across%20various%20natural%20language%20processing%20%28NLP%29%0Atasks.%20Instead%20of%20using%20a%20fixed%20prompt%20template%20to%20fine-tune%20the%20model%2C%20some%0Aresearch%20demonstrates%20the%20effectiveness%20of%20searching%20for%20the%20prompt%20via%0Aoptimization.%20Such%20prompt%20optimization%20process%20of%20prompt-based%20learning%20on%20PLMs%0Aalso%20gives%20insight%20into%20generating%20adversarial%20prompts%20to%20mislead%20the%20model%2C%0Araising%20concerns%20about%20the%20adversarial%20vulnerability%20of%20this%20paradigm.%20Recent%0Astudies%20have%20shown%20that%20universal%20adversarial%20triggers%20%28UATs%29%20can%20be%20generated%0Ato%20alter%20not%20only%20the%20predictions%20of%20the%20target%20PLMs%20but%20also%20the%20prediction%20of%0Acorresponding%20Prompt-based%20Fine-tuning%20Models%20%28PFMs%29%20under%20the%20prompt-based%0Alearning%20paradigm.%20However%2C%20UATs%20found%20in%20previous%20works%20are%20often%20unreadable%0Atokens%20or%20characters%20and%20can%20be%20easily%20distinguished%20from%20natural%20texts%20with%0Aadaptive%20defenses.%20In%20this%20work%2C%20we%20consider%20the%20naturalness%20of%20the%20UATs%20and%0Adevelop%20%24%5Ctextit%7BLinkPrompt%7D%24%2C%20an%20adversarial%20attack%20algorithm%20to%20generate%20UATs%0Aby%20a%20gradient-based%20beam%20search%20algorithm%20that%20not%20only%20effectively%20attacks%20the%0Atarget%20PLMs%20and%20PFMs%20but%20also%20maintains%20the%20naturalness%20among%20the%20trigger%0Atokens.%20Extensive%20results%20demonstrate%20the%20effectiveness%20of%0A%24%5Ctextit%7BLinkPrompt%7D%24%2C%20as%20well%20as%20the%20transferability%20of%20UATs%20generated%20by%0A%24%5Ctextit%7BLinkPrompt%7D%24%20to%20open-sourced%20Large%20Language%20Model%20%28LLM%29%20Llama2%20and%0AAPI-accessed%20LLM%20GPT-3.5-turbo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16432v2&entry.124074799=Read"},
{"title": "INEXA: Interactive and Explainable Process Model Abstraction Through\n  Object-Centric Process Mining", "author": "Janik-Vasily Benzin and Gyunam Park and Juergen Mangler and Stefanie Rinderle-Ma", "abstract": "  Process events are recorded by multiple information systems at different\ngranularity levels. Based on the resulting event logs, process models are\ndiscovered at different granularity levels, as well. Events stored at a\nfine-grained granularity level, for example, may hinder the discovered process\nmodel to be displayed due the high number of resulting model elements. The\ndiscovered process model of a real-world manufacturing process, for example,\nconsists of 1,489 model elements and over 2,000 arcs. Existing process model\nabstraction techniques could help reducing the size of the model, but would\ndisconnect it from the underlying event log. Existing event abstraction\ntechniques do neither support the analysis of mixed granularity levels, nor\ninteractive exploration of a suitable granularity level. To enable the\nexploration of discovered process models at different granularity levels, we\npropose INEXA, an interactive, explainable process model abstraction method\nthat keeps the link to the event log. As a starting point, INEXA aggregates\nlarge process models to a \"displayable\" size, e.g., for the manufacturing use\ncase to a process model with 58 model elements. Then, the process analyst can\nexplore granularity levels interactively, while applied abstractions are\nautomatically traced in the event log for explainability.\n", "link": "http://arxiv.org/abs/2403.18659v1", "date": "2024-03-27", "relevancy": 0.8552, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5069}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3905}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3853}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20INEXA%3A%20Interactive%20and%20Explainable%20Process%20Model%20Abstraction%20Through%0A%20%20Object-Centric%20Process%20Mining&body=Title%3A%20INEXA%3A%20Interactive%20and%20Explainable%20Process%20Model%20Abstraction%20Through%0A%20%20Object-Centric%20Process%20Mining%0AAuthor%3A%20Janik-Vasily%20Benzin%20and%20Gyunam%20Park%20and%20Juergen%20Mangler%20and%20Stefanie%20Rinderle-Ma%0AAbstract%3A%20%20%20Process%20events%20are%20recorded%20by%20multiple%20information%20systems%20at%20different%0Agranularity%20levels.%20Based%20on%20the%20resulting%20event%20logs%2C%20process%20models%20are%0Adiscovered%20at%20different%20granularity%20levels%2C%20as%20well.%20Events%20stored%20at%20a%0Afine-grained%20granularity%20level%2C%20for%20example%2C%20may%20hinder%20the%20discovered%20process%0Amodel%20to%20be%20displayed%20due%20the%20high%20number%20of%20resulting%20model%20elements.%20The%0Adiscovered%20process%20model%20of%20a%20real-world%20manufacturing%20process%2C%20for%20example%2C%0Aconsists%20of%201%2C489%20model%20elements%20and%20over%202%2C000%20arcs.%20Existing%20process%20model%0Aabstraction%20techniques%20could%20help%20reducing%20the%20size%20of%20the%20model%2C%20but%20would%0Adisconnect%20it%20from%20the%20underlying%20event%20log.%20Existing%20event%20abstraction%0Atechniques%20do%20neither%20support%20the%20analysis%20of%20mixed%20granularity%20levels%2C%20nor%0Ainteractive%20exploration%20of%20a%20suitable%20granularity%20level.%20To%20enable%20the%0Aexploration%20of%20discovered%20process%20models%20at%20different%20granularity%20levels%2C%20we%0Apropose%20INEXA%2C%20an%20interactive%2C%20explainable%20process%20model%20abstraction%20method%0Athat%20keeps%20the%20link%20to%20the%20event%20log.%20As%20a%20starting%20point%2C%20INEXA%20aggregates%0Alarge%20process%20models%20to%20a%20%22displayable%22%20size%2C%20e.g.%2C%20for%20the%20manufacturing%20use%0Acase%20to%20a%20process%20model%20with%2058%20model%20elements.%20Then%2C%20the%20process%20analyst%20can%0Aexplore%20granularity%20levels%20interactively%2C%20while%20applied%20abstractions%20are%0Aautomatically%20traced%20in%20the%20event%20log%20for%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18659v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INEXA%3A%20Interactive%20and%20Explainable%20Process%20Model%20Abstraction%20Through%0A%20%20Object-Centric%20Process%20Mining&entry.906535625=Janik-Vasily%20Benzin%20and%20Gyunam%20Park%20and%20Juergen%20Mangler%20and%20Stefanie%20Rinderle-Ma&entry.1292438233=%20%20Process%20events%20are%20recorded%20by%20multiple%20information%20systems%20at%20different%0Agranularity%20levels.%20Based%20on%20the%20resulting%20event%20logs%2C%20process%20models%20are%0Adiscovered%20at%20different%20granularity%20levels%2C%20as%20well.%20Events%20stored%20at%20a%0Afine-grained%20granularity%20level%2C%20for%20example%2C%20may%20hinder%20the%20discovered%20process%0Amodel%20to%20be%20displayed%20due%20the%20high%20number%20of%20resulting%20model%20elements.%20The%0Adiscovered%20process%20model%20of%20a%20real-world%20manufacturing%20process%2C%20for%20example%2C%0Aconsists%20of%201%2C489%20model%20elements%20and%20over%202%2C000%20arcs.%20Existing%20process%20model%0Aabstraction%20techniques%20could%20help%20reducing%20the%20size%20of%20the%20model%2C%20but%20would%0Adisconnect%20it%20from%20the%20underlying%20event%20log.%20Existing%20event%20abstraction%0Atechniques%20do%20neither%20support%20the%20analysis%20of%20mixed%20granularity%20levels%2C%20nor%0Ainteractive%20exploration%20of%20a%20suitable%20granularity%20level.%20To%20enable%20the%0Aexploration%20of%20discovered%20process%20models%20at%20different%20granularity%20levels%2C%20we%0Apropose%20INEXA%2C%20an%20interactive%2C%20explainable%20process%20model%20abstraction%20method%0Athat%20keeps%20the%20link%20to%20the%20event%20log.%20As%20a%20starting%20point%2C%20INEXA%20aggregates%0Alarge%20process%20models%20to%20a%20%22displayable%22%20size%2C%20e.g.%2C%20for%20the%20manufacturing%20use%0Acase%20to%20a%20process%20model%20with%2058%20model%20elements.%20Then%2C%20the%20process%20analyst%20can%0Aexplore%20granularity%20levels%20interactively%2C%20while%20applied%20abstractions%20are%0Aautomatically%20traced%20in%20the%20event%20log%20for%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18659v1&entry.124074799=Read"},
{"title": "Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets", "author": "Ajinkya Khoche and Aron Asefaw and Alejandro Gonzalez and Bogdan Timus and Sina Sharif Mansouri and Patric Jensfelt", "abstract": "  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n", "link": "http://arxiv.org/abs/2403.18649v1", "date": "2024-03-27", "relevancy": 1.6576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5611}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Addressing%20Data%20Annotation%20Challenges%20in%20Multiple%20Sensors%3A%20A%20Solution%0A%20%20for%20Scania%20Collected%20Datasets&body=Title%3A%20Addressing%20Data%20Annotation%20Challenges%20in%20Multiple%20Sensors%3A%20A%20Solution%0A%20%20for%20Scania%20Collected%20Datasets%0AAuthor%3A%20Ajinkya%20Khoche%20and%20Aron%20Asefaw%20and%20Alejandro%20Gonzalez%20and%20Bogdan%20Timus%20and%20Sina%20Sharif%20Mansouri%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20Data%20annotation%20in%20autonomous%20vehicles%20is%20a%20critical%20step%20in%20the%20development%0Aof%20Deep%20Neural%20Network%20%28DNN%29%20based%20models%20or%20the%20performance%20evaluation%20of%20the%0Aperception%20system.%20This%20often%20takes%20the%20form%20of%20adding%203D%20bounding%20boxes%20on%0Atime-sequential%20and%20registered%20series%20of%20point-sets%20captured%20from%20active%0Asensors%20like%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20and%20Radio%20Detection%20and%0ARanging%20%28RADAR%29.%20When%20annotating%20multiple%20active%20sensors%2C%20there%20is%20a%20need%20to%0Amotion%20compensate%20and%20translate%20the%20points%20to%20a%20consistent%20coordinate%20frame%20and%0Atimestamp%20respectively.%20However%2C%20highly%20dynamic%20objects%20pose%20a%20unique%0Achallenge%2C%20as%20they%20can%20appear%20at%20different%20timestamps%20in%20each%20sensor%27s%20data.%0AWithout%20knowing%20the%20speed%20of%20the%20objects%2C%20their%20position%20appears%20to%20be%0Adifferent%20in%20different%20sensor%20outputs.%20Thus%2C%20even%20after%20motion%20compensation%2C%0Ahighly%20dynamic%20objects%20are%20not%20matched%20from%20multiple%20sensors%20in%20the%20same%20frame%2C%0Aand%20human%20annotators%20struggle%20to%20add%20unique%20bounding%20boxes%20that%20capture%20all%0Aobjects.%20This%20article%20focuses%20on%20addressing%20this%20challenge%2C%20primarily%20within%0Athe%20context%20of%20Scania%20collected%20datasets.%20The%20proposed%20solution%20takes%20a%20track%0Aof%20an%20annotated%20object%20as%20input%20and%20uses%20the%20Moving%20Horizon%20Estimation%20%28MHE%29%20to%0Arobustly%20estimate%20its%20speed.%20The%20estimated%20speed%20profile%20is%20utilized%20to%20correct%0Athe%20position%20of%20the%20annotated%20box%20and%20add%20boxes%20to%20object%20clusters%20missed%20by%0Athe%20original%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Data%20Annotation%20Challenges%20in%20Multiple%20Sensors%3A%20A%20Solution%0A%20%20for%20Scania%20Collected%20Datasets&entry.906535625=Ajinkya%20Khoche%20and%20Aron%20Asefaw%20and%20Alejandro%20Gonzalez%20and%20Bogdan%20Timus%20and%20Sina%20Sharif%20Mansouri%20and%20Patric%20Jensfelt&entry.1292438233=%20%20Data%20annotation%20in%20autonomous%20vehicles%20is%20a%20critical%20step%20in%20the%20development%0Aof%20Deep%20Neural%20Network%20%28DNN%29%20based%20models%20or%20the%20performance%20evaluation%20of%20the%0Aperception%20system.%20This%20often%20takes%20the%20form%20of%20adding%203D%20bounding%20boxes%20on%0Atime-sequential%20and%20registered%20series%20of%20point-sets%20captured%20from%20active%0Asensors%20like%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20and%20Radio%20Detection%20and%0ARanging%20%28RADAR%29.%20When%20annotating%20multiple%20active%20sensors%2C%20there%20is%20a%20need%20to%0Amotion%20compensate%20and%20translate%20the%20points%20to%20a%20consistent%20coordinate%20frame%20and%0Atimestamp%20respectively.%20However%2C%20highly%20dynamic%20objects%20pose%20a%20unique%0Achallenge%2C%20as%20they%20can%20appear%20at%20different%20timestamps%20in%20each%20sensor%27s%20data.%0AWithout%20knowing%20the%20speed%20of%20the%20objects%2C%20their%20position%20appears%20to%20be%0Adifferent%20in%20different%20sensor%20outputs.%20Thus%2C%20even%20after%20motion%20compensation%2C%0Ahighly%20dynamic%20objects%20are%20not%20matched%20from%20multiple%20sensors%20in%20the%20same%20frame%2C%0Aand%20human%20annotators%20struggle%20to%20add%20unique%20bounding%20boxes%20that%20capture%20all%0Aobjects.%20This%20article%20focuses%20on%20addressing%20this%20challenge%2C%20primarily%20within%0Athe%20context%20of%20Scania%20collected%20datasets.%20The%20proposed%20solution%20takes%20a%20track%0Aof%20an%20annotated%20object%20as%20input%20and%20uses%20the%20Moving%20Horizon%20Estimation%20%28MHE%29%20to%0Arobustly%20estimate%20its%20speed.%20The%20estimated%20speed%20profile%20is%20utilized%20to%20correct%0Athe%20position%20of%20the%20annotated%20box%20and%20add%20boxes%20to%20object%20clusters%20missed%20by%0Athe%20original%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18649v1&entry.124074799=Read"},
{"title": "CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT", "author": "Yi Hu and Jinhang Zuo and Alanis Zhao and Bob Iannucci and Carlee Joe-Wong", "abstract": "  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n", "link": "http://arxiv.org/abs/2403.18451v1", "date": "2024-03-27", "relevancy": 0.9142, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4505}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoRAST%3A%20Towards%20Foundation%20Model-Powered%20Correlated%20Data%20Analysis%20in%0A%20%20Resource-Constrained%20CPS%20and%20IoT&body=Title%3A%20CoRAST%3A%20Towards%20Foundation%20Model-Powered%20Correlated%20Data%20Analysis%20in%0A%20%20Resource-Constrained%20CPS%20and%20IoT%0AAuthor%3A%20Yi%20Hu%20and%20Jinhang%20Zuo%20and%20Alanis%20Zhao%20and%20Bob%20Iannucci%20and%20Carlee%20Joe-Wong%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20emerge%20as%20a%20promising%20solution%20to%20harness%20distributed%0Aand%20diverse%20environmental%20data%20by%20leveraging%20prior%20knowledge%20to%20understand%20the%0Acomplicated%20temporal%20and%20spatial%20correlations%20within%20heterogeneous%20datasets.%0AUnlike%20distributed%20learning%20frameworks%20such%20as%20federated%20learning%2C%20which%20often%0Astruggle%20with%20multimodal%20data%2C%20FMs%20can%20transform%20diverse%20inputs%20into%0Aembeddings.%20This%20process%20facilitates%20the%20integration%20of%20information%20from%0Avarious%20modalities%20and%20the%20application%20of%20prior%20learning%20to%20new%20domains.%0AHowever%2C%20deploying%20FMs%20in%20resource-constrained%20edge%20systems%20poses%20significant%0Achallenges.%20To%20this%20end%2C%20we%20introduce%20CoRAST%2C%20a%20novel%20learning%20framework%20that%0Autilizes%20FMs%20for%20enhanced%20analysis%20of%20distributed%2C%20correlated%20heterogeneous%0Adata.%20Utilizing%20a%20server-based%20FM%2C%20CoRAST%20can%20exploit%20existing%20environment%0Ainformation%20to%20extract%20temporal%2C%20spatial%2C%20and%20cross-modal%20correlations%20among%0Asensor%20data.%20This%20enables%20CoRAST%20to%20offer%20context-aware%20insights%20for%20localized%0Aclient%20tasks%20through%20FM-powered%20global%20representation%20learning.%20Our%20evaluation%0Aon%20real-world%20weather%20dataset%20demonstrates%20CoRAST%27s%20ability%20to%20exploit%0Acorrelated%20heterogeneous%20data%20through%20environmental%20representation%20learning%20to%0Areduce%20the%20forecast%20errors%20by%20up%20to%2050.3%25%20compared%20to%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18451v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRAST%3A%20Towards%20Foundation%20Model-Powered%20Correlated%20Data%20Analysis%20in%0A%20%20Resource-Constrained%20CPS%20and%20IoT&entry.906535625=Yi%20Hu%20and%20Jinhang%20Zuo%20and%20Alanis%20Zhao%20and%20Bob%20Iannucci%20and%20Carlee%20Joe-Wong&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20emerge%20as%20a%20promising%20solution%20to%20harness%20distributed%0Aand%20diverse%20environmental%20data%20by%20leveraging%20prior%20knowledge%20to%20understand%20the%0Acomplicated%20temporal%20and%20spatial%20correlations%20within%20heterogeneous%20datasets.%0AUnlike%20distributed%20learning%20frameworks%20such%20as%20federated%20learning%2C%20which%20often%0Astruggle%20with%20multimodal%20data%2C%20FMs%20can%20transform%20diverse%20inputs%20into%0Aembeddings.%20This%20process%20facilitates%20the%20integration%20of%20information%20from%0Avarious%20modalities%20and%20the%20application%20of%20prior%20learning%20to%20new%20domains.%0AHowever%2C%20deploying%20FMs%20in%20resource-constrained%20edge%20systems%20poses%20significant%0Achallenges.%20To%20this%20end%2C%20we%20introduce%20CoRAST%2C%20a%20novel%20learning%20framework%20that%0Autilizes%20FMs%20for%20enhanced%20analysis%20of%20distributed%2C%20correlated%20heterogeneous%0Adata.%20Utilizing%20a%20server-based%20FM%2C%20CoRAST%20can%20exploit%20existing%20environment%0Ainformation%20to%20extract%20temporal%2C%20spatial%2C%20and%20cross-modal%20correlations%20among%0Asensor%20data.%20This%20enables%20CoRAST%20to%20offer%20context-aware%20insights%20for%20localized%0Aclient%20tasks%20through%20FM-powered%20global%20representation%20learning.%20Our%20evaluation%0Aon%20real-world%20weather%20dataset%20demonstrates%20CoRAST%27s%20ability%20to%20exploit%0Acorrelated%20heterogeneous%20data%20through%20environmental%20representation%20learning%20to%0Areduce%20the%20forecast%20errors%20by%20up%20to%2050.3%25%20compared%20to%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18451v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


